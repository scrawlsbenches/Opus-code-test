{
  "generated": "2025-12-17T00:01:54.287308Z",
  "version": "1.0.0",
  "chapters": [
    {
      "path": "00-preface/how-this-book-works.md",
      "title": "How This Book Works",
      "section": "preface",
      "tags": [
        "meta",
        "introduction",
        "self-reference"
      ],
      "source_files": [
        "scripts/generate_book.py",
        "docs/VISION.md"
      ],
      "excerpt": "> *\"The best documentation is the kind that writes itself.\"* The Cortical Chronicles is a **self-documenting book**. It uses the Cortical Text Processor\u2014the very system it documents\u2014to generate its...",
      "keywords": [
        "book",
        "chapter",
        "cortical",
        "generate_book",
        "vision",
        "architecture",
        "search",
        "index",
        "scripts",
        "itself"
      ],
      "full_content": "# How This Book Works\n\n> *\"The best documentation is the kind that writes itself.\"*\n\n## Overview\n\nThe Cortical Chronicles is a **self-documenting book**. It uses the Cortical Text Processor\u2014the very system it documents\u2014to generate its own content. This creates a fascinating recursive property: the book understands itself through the same algorithms it explains.\n\n## The Generation Process\n\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    BOOK GENERATION                           \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                              \u2502\n\u2502  Source Files           Generators           Chapters        \u2502\n\u2502  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500           \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500           \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500        \u2502\n\u2502                                                              \u2502\n\u2502  docs/VISION.md    \u2192   AlgorithmGen    \u2192   01-foundations/  \u2502\n\u2502  cortical/*.ai_meta \u2192  ModuleDocGen    \u2192   02-architecture/ \u2502\n\u2502  samples/decisions/ \u2192  DecisionGen     \u2192   03-decisions/    \u2502\n\u2502  git log           \u2192   NarrativeGen    \u2192   04-evolution/    \u2502\n\u2502  tasks/            \u2192   RoadmapGen      \u2192   05-future/       \u2502\n\u2502                                                              \u2502\n\u2502                    \u2193                                         \u2502\n\u2502              search-index.json                               \u2502\n\u2502                    \u2193                                         \u2502\n\u2502               index.html (searchable)                        \u2502\n\u2502                                                              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n## Chapter Types\n\n### 01-foundations/\nAlgorithm deep-dives extracted from `docs/VISION.md`. Each algorithm (PageRank, BM25, Louvain, etc.) gets its own chapter with:\n- Purpose and intuition\n- Mathematical formulation\n- Implementation details\n- Why it matters for code search\n\n### 02-architecture/\nModule documentation generated from `.ai_meta` files. Includes:\n- Module purpose and dependencies\n- Key functions and classes\n- Mermaid dependency graphs\n\n### 03-decisions/\nArchitecture Decision Records from `samples/decisions/`. Documents the \"why\" behind design choices.\n\n### 04-evolution/\nA narrative of project history generated from git commits. Transforms raw commit logs into a readable story of how the project evolved.\n\n### 05-future/\nRoadmap and vision from task files and `VISION.md`. Shows where the project is heading.\n\n## The Self-Reference Loop\n\nHere's what makes this book special:\n\n1. **The processor indexes its own code** \u2192 Creates a semantic graph\n2. **The generators query that graph** \u2192 Find relevant content\n3. **The book explains those algorithms** \u2192 Reader understands the system\n4. **The system processes those explanations** \u2192 Understands itself better\n\nThis isn't just cute\u2014it's a powerful test of the system's capabilities. If the Cortical Text Processor can understand and explain itself, it can understand any codebase.\n\n## Regenerating the Book\n\nThe book regenerates automatically on every push to `main`:\n\n```bash\n# Manual regeneration\npython scripts/generate_book.py\n\n# Generate specific chapter\npython scripts/generate_book.py --chapter foundations\n\n# Preview without writing\npython scripts/generate_book.py --dry-run\n```\n\n## Searching the Book\n\nThe book includes a semantic search interface. Open `index.html` to:\n- Search by keyword or concept\n- Browse by chapter\n- Follow cross-references\n\nThe search uses the same algorithms described in the book\u2014query expansion, BM25 scoring, PageRank boosting.\n\n## See Also\n\n- [Algorithm Analysis](../01-foundations/index.md) - Deep dive into the algorithms\n- [Architecture](../02-architecture/index.md) - How the code is organized\n- [Source: generate_book.py](../../scripts/generate_book.py) - The generation script\n\n## Source Files\n\nThis chapter was written manually as the seed for the book. Future chapters are auto-generated from:\n- `scripts/generate_book.py` - The orchestrator\n- `docs/VISION.md:185-430` - Algorithm documentation source\n\n---\n\n*This chapter is part of [The Cortical Chronicles](../README.md),\na self-documenting book generated by the Cortical Text Processor.*\n"
    },
    {
      "path": "00-preface/the-living-book-vision.md",
      "title": "The Living Book Vision",
      "section": "preface",
      "tags": [
        "meta",
        "vision",
        "narrative",
        "self-reference"
      ],
      "source_files": [
        "docs/BOOK-GENERATION-VISION.md",
        "scripts/generate_book.py",
        ".git-ml/"
      ],
      "excerpt": "> *\"Code tells you what. Comments tell you why. A living book tells you the journey.\"* What if documentation wrote itself\u2014not as an afterthought, but as a natural byproduct of development? This isn't...",
      "keywords": [
        "book",
        "document",
        "living",
        "vision",
        "code",
        "data",
        "learning",
        "system",
        "documentation",
        "itself"
      ],
      "full_content": "# The Living Book Vision\n\n> *\"Code tells you what. Comments tell you why. A living book tells you the journey.\"*\n\n## The Idea\n\nWhat if documentation wrote itself\u2014not as an afterthought, but as a natural byproduct of development?\n\nThis isn't science fiction. It's what you're reading right now.\n\nThe Cortical Chronicles is a **living book**: it grows with the codebase, captures the stories behind decisions, and transforms raw development artifacts into narrative chapters. Every debugging session becomes a case study. Every bugfix becomes a lesson. Every commit becomes a paragraph in an ongoing story.\n\n## How It Works\n\n### The Data We Already Capture\n\nDuring development, we're already collecting everything a book needs:\n\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                 THE RAW MATERIALS OF A BOOK                      \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                                  \u2502\n\u2502  Developer Questions    \u2192 \"Why isn't search finding X?\"          \u2502\n\u2502  Investigation Traces   \u2192 Files read, tools used, paths tried    \u2502\n\u2502  Breakthroughs         \u2192 \"Aha! The bottleneck is in bigrams!\"    \u2502\n\u2502  Solutions             \u2192 Commits with diffs and context          \u2502\n\u2502  Decisions             \u2192 ADRs with rationale                     \u2502\n\u2502  Outcomes              \u2192 CI results, test coverage               \u2502\n\u2502                                                                  \u2502\n\u2502  Together, these form NARRATIVE ARCS:                           \u2502\n\u2502                                                                  \u2502\n\u2502  Problem \u2192 Investigation \u2192 Discovery \u2192 Solution \u2192 Lesson         \u2502\n\u2502                                                                  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n### From Data to Story\n\nThe generators transform structured data into readable chapters:\n\n| Data Source | Generator | Chapter Type |\n|-------------|-----------|--------------|\n| ML Sessions | CaseStudyGenerator | Problem-solving narratives |\n| Bugfix Commits | LessonExtractor | Distilled wisdom |\n| ADRs + Context | DecisionStoryGenerator | The \"why\" behind choices |\n| Concept Clusters | ConceptEvolutionGenerator | How ideas grew over time |\n| Test Cases | ExerciseGenerator | Reader engagement |\n| PageRank Scores | ReaderJourneyGenerator | Progressive learning paths |\n\n## What Makes This Different\n\n### Traditional Documentation\n\n```\nprocess_document(doc_id, text)\n\nProcess a document and add it to the corpus.\n\nParameters:\n  doc_id: Unique document identifier\n  text: Document content\n\nReturns:\n  None\n```\n\n### A Living Book\n\n> **Chapter 5: Processing Your First Document**\n>\n> Before the system can search, it needs to understand. That understanding begins with `process_document()`.\n>\n> Think of it as reading a book for the first time. You don't just see words\u2014you build a mental map: which ideas connect, which concepts are central, which phrases recur.\n>\n> The processor does the same thing, but algorithmically. When you call:\n>\n> ```python\n> processor.process_document(\"readme\", open(\"README.md\").read())\n> ```\n>\n> ...a cascade of analysis begins. Tokens are extracted. Bigrams form. Lateral connections strengthen between co-occurring terms. The document joins a growing graph of semantic relationships.\n>\n> **Try It Yourself:** Process the CLAUDE.md file and examine what concepts emerge. Which terms have the highest PageRank? What does that tell you about the document's focus?\n\n## The Chapter Types\n\n### Case Studies: Learning from Real Problems\n\nEvery debugging session is a story waiting to be told:\n\n> **Case Study: The Great Performance Hunt**\n>\n> It started with a timeout. The `compute_all()` function was hanging on just 125 documents.\n>\n> The obvious suspect was Louvain clustering\u2014our most complex algorithm. But we profiled first...\n\n### Lessons: Wisdom Distilled from Experience\n\n600+ commits contain patterns worth preserving:\n\n> **Lesson #23: Profile Before Optimizing**\n>\n> **The Mistake:** Assumed Louvain was slow because it's complex.\n> **The Reality:** 99% of time was in `bigram_connections()`.\n> **The Principle:** The obvious culprit is often innocent. Data beats intuition.\n\n### Concept Evolution: Watching Ideas Grow\n\nTrack how key concepts emerged and strengthened:\n\n> **How \"Importance\" Became a First-Class Concept**\n>\n> Week 1: First mention in `analysis.py`\u2014a simple PageRank score.\n> Week 3: Connected to \"relevance\", \"ranking\", \"boost\".\n> Week 6: Cluster of 15 related terms. Central to search quality.\n\n### Exercises: Active Learning\n\nTest cases become teaching moments:\n\n> **Exercise: Query Expansion**\n>\n> Given the query \"neural networks\", write code to expand it with related terms.\n>\n> *Hint 1:* Use `processor.expand_query()`\n> *Hint 2:* Consider lateral connections\n> *Solution:* [Reveal]\n\n## The Self-Reference Loop\n\nHere's the beautiful recursion:\n\n1. **We write code** \u2192 Creates development artifacts\n2. **ML captures the process** \u2192 Structured session data\n3. **Generators synthesize narratives** \u2192 Book chapters\n4. **The book explains the system** \u2192 Readers understand\n5. **Understanding leads to better code** \u2192 Loop continues\n\nThe Cortical Text Processor documents itself using its own algorithms. If it can understand and explain itself, it can understand any codebase.\n\n## Why This Matters\n\n### For Developers\n\n- Documentation stays current automatically\n- Lessons are captured, not forgotten\n- Onboarding accelerates via structured learning paths\n\n### For Teams\n\n- Institutional knowledge persists across turnover\n- Decisions are documented with full context\n- Best practices emerge from analyzed patterns\n\n### For Publishers\n\n- Authentic problem-solving narratives\n- Genuine lessons from real development\n- A unique angle: the book that writes itself\n\n## The Vision\n\nImagine every software project generating its own living book:\n\n- New team members read the story of how the system evolved\n- Debugging sessions become teaching materials\n- Architecture decisions carry their full context\n- The gap between \"code\" and \"understanding\" closes\n\nThis isn't just documentation. It's **computational autobiography**\u2014a system telling its own story through the act of being built.\n\n---\n\n## What You'll Find in This Book\n\n| Section | Content |\n|---------|---------|\n| **Foundations** | The algorithms that power semantic search |\n| **Architecture** | How the code is organized and why |\n| **Decisions** | The choices that shaped the system |\n| **Evolution** | Timeline of how we got here |\n| **Case Studies** | Problem-solving narratives |\n| **Lessons** | Distilled wisdom from 600+ commits |\n| **Concepts** | How key ideas emerged and grew |\n| **Exercises** | Hands-on learning opportunities |\n| **Journey** | Your personalized learning path |\n\nEach section is generated from the codebase itself\u2014living documentation that grows with the system.\n\n---\n\n## See Also\n\n- [How This Book Works](./how-this-book-works.md) - Technical details of generation\n- [Full Vision Document](../../docs/BOOK-GENERATION-VISION.md) - Complete specification\n- [Product Vision](../../docs/VISION.md) - Overall product direction\n\n---\n\n*This chapter is part of [The Cortical Chronicles](../README.md),\na self-documenting book generated by the Cortical Text Processor.*\n"
    },
    {
      "path": "01-foundations/alg-bm25.md",
      "title": "BM25/TF-IDF \u2014 Distinctiveness Scoring",
      "section": "foundations",
      "tags": [
        "algorithms",
        "foundations",
        "ir-theory"
      ],
      "source_files": [
        "docs/VISION.md",
        "cortical/analysis/tfidf.py"
      ],
      "excerpt": "``` BM25(t, d) = IDF(t) \u00d7 (tf(t,d) \u00d7 (k1 + 1)) / (tf(t,d) + k1 \u00d7 (1 - b + b \u00d7 |d|/avgdl)) ``` Where: This dual approach allows:",
      "keywords": [
        "idf",
        "term",
        "document",
        "bm25",
        "frequency",
        "corpus",
        "cortical",
        "importance",
        "scoring",
        "specific"
      ],
      "full_content": "# BM25/TF-IDF \u2014 Distinctiveness Scoring\n\n**Purpose:** Score how well a term distinguishes a specific document from the rest of the corpus.\n\n**Implementation:** `cortical/analysis/tfidf.py`\n\n**BM25 Formula (Default):**\n```\nBM25(t, d) = IDF(t) \u00d7 (tf(t,d) \u00d7 (k1 + 1)) / (tf(t,d) + k1 \u00d7 (1 - b + b \u00d7 |d|/avgdl))\n```\n\nWhere:\n- `IDF(t) = log((N - df(t) + 0.5) / (df(t) + 0.5) + 1)` \u2014 Inverse document frequency with smoothing\n- `tf(t,d)` \u2014 Term frequency in document d\n- `k1 = 1.2` \u2014 Term frequency saturation (diminishing returns after ~12 occurrences)\n- `b = 0.75` \u2014 Length normalization factor\n\n**Why BM25 Over TF-IDF:**\n- Non-negative IDF even for terms appearing in most documents\n- Length normalization prevents long files from unfairly dominating\n- Term frequency saturation models realistic relevance (saying \"API\" 100 times doesn't make a doc 100\u00d7 more relevant than saying it once)\n\n**Dual Storage Strategy:**\n- **Global TF-IDF** (`col.tfidf`): Term importance to entire corpus\n- **Per-Document TF-IDF** (`col.tfidf_per_doc[doc_id]`): Term importance within specific document\n\nThis dual approach allows:\n- Fast corpus-wide importance filtering\n- Accurate per-document relevance scoring for search\n\n---\n\n*This chapter is part of [The Cortical Chronicles](../README.md), a self-documenting book generated by the Cortical Text Processor.*\n"
    },
    {
      "path": "01-foundations/alg-graph-boosted-search.md",
      "title": "Graph-Boosted Search (GB-BM25) \u2014 Hybrid Ranking",
      "section": "foundations",
      "tags": [
        "algorithms",
        "foundations",
        "ir-theory"
      ],
      "source_files": [
        "docs/VISION.md",
        "cortical/query/search.py:425-564"
      ],
      "excerpt": "``` final_score = (0.5 \u00d7 normalized_bm25) + (0.3 \u00d7 normalized_pagerank) + (0.2 \u00d7 normalized_proximity) \u00d7 coverage_multiplier (0.5 to 1.5) ``` 1. **BM25 Base Score (50%):** 2. **PageRank Boost...",
      "keywords": [
        "documents",
        "terms",
        "query",
        "matching",
        "graph",
        "bm25",
        "cortical",
        "term",
        "boost",
        "multiplier"
      ],
      "full_content": "# Graph-Boosted Search (GB-BM25) \u2014 Hybrid Ranking\n\n**Purpose:** Combine BM25 relevance with graph structure signals.\n\n**Implementation:** `cortical/query/search.py:425-564`\n\n**Scoring Formula:**\n```\nfinal_score = (0.5 \u00d7 normalized_bm25) + (0.3 \u00d7 normalized_pagerank) + (0.2 \u00d7 normalized_proximity)\n            \u00d7 coverage_multiplier (0.5 to 1.5)\n```\n\n**Three Signal Sources:**\n\n1. **BM25 Base Score (50%):**\n   - Standard term frequency \u00d7 inverse document frequency\n   - Per-document scoring using `col.tfidf_per_doc`\n\n2. **PageRank Boost (30%):**\n   - Sum of matched term PageRanks\n   - Rewards documents containing important terms\n\n3. **Proximity Boost (20%):**\n   - For each pair of original query terms:\n     - Check if they're connected in the co-occurrence graph\n     - If connected, boost documents containing both\n   - Rewards documents where query terms appear together\n\n**Coverage Multiplier:**\n- Documents matching 1/5 query terms: 0.7\u00d7 multiplier\n- Documents matching all 5 query terms: 1.5\u00d7 multiplier\n- Prevents documents matching one rare term from outranking documents matching many terms\n\n---\n\n*This chapter is part of [The Cortical Chronicles](../README.md), a self-documenting book generated by the Cortical Text Processor.*\n"
    },
    {
      "path": "01-foundations/alg-louvain.md",
      "title": "Louvain Community Detection \u2014 Concept Discovery",
      "section": "foundations",
      "tags": [
        "algorithms",
        "foundations",
        "ir-theory"
      ],
      "source_files": [
        "docs/VISION.md",
        "cortical/analysis/clustering.py"
      ],
      "excerpt": "``` for each node: find neighboring communities calculate modularity gain for moving to each move to best community if gain > 0 repeat until no nodes move ``` ``` collapse each community into a...",
      "keywords": [
        "concepts",
        "clusters",
        "phase",
        "resolution",
        "community",
        "concept",
        "cortical",
        "communities",
        "semantic",
        "algorithm"
      ],
      "full_content": "# Louvain Community Detection \u2014 Concept Discovery\n\n**Purpose:** Discover semantic clusters (concepts) from the term co-occurrence graph.\n\n**Implementation:** `cortical/analysis/clustering.py`\n\n**Two-Phase Algorithm:**\n\n**Phase 1 \u2014 Local Optimization:**\n```\nfor each node:\n    find neighboring communities\n    calculate modularity gain for moving to each\n    move to best community if gain > 0\nrepeat until no nodes move\n```\n\n**Phase 2 \u2014 Network Aggregation:**\n```\ncollapse each community into a single super-node\nedges between communities become edges between super-nodes\nrepeat Phase 1 on the aggregated network\n```\n\n**Modularity Formula:**\n```\nQ = (1/2m) \u00d7 \u03a3 [A_ij - (k_i \u00d7 k_j)/(2m)] \u00d7 \u03b4(c_i, c_j)\n```\n\nThe algorithm optimizes Q, which measures how much edge weight falls within communities versus what would be expected by random chance.\n\n**Resolution Parameter:**\n- `resolution = 1.0` (default): Balanced clusters, ~32 concepts\n- `resolution = 0.5`: Coarse clusters, ~38 concepts (max cluster 64% of tokens)\n- `resolution = 2.0`: Fine-grained clusters, ~79 concepts (max cluster 4.2% of tokens)\n\n**Concept Naming:**\n```python\ntop_members = sorted(cluster_members, key=lambda m: m.pagerank, reverse=True)[:3]\nconcept_name = '/'.join(top_members)  # e.g., \"neural/learning/networks\"\n```\n\n**Why This Matters:**\n- Enables concept-level search (\"find documents about authentication\")\n- Reduces dimensionality while preserving semantic structure\n- Creates Layer 2 (Concepts) that bridges raw terms and documents\n\n---\n\n*This chapter is part of [The Cortical Chronicles](../README.md), a self-documenting book generated by the Cortical Text Processor.*\n"
    },
    {
      "path": "01-foundations/alg-pagerank.md",
      "title": "PageRank \u2014 Importance Discovery",
      "section": "foundations",
      "tags": [
        "algorithms",
        "foundations",
        "ir-theory"
      ],
      "source_files": [
        "docs/VISION.md",
        "cortical/analysis/pagerank.py"
      ],
      "excerpt": "``` importance[term] = (1 - damping) / N + damping \u00d7 \u03a3 (neighbor_importance \u00d7 edge_weight / neighbor_outgoing_sum) ``` The algorithm iteratively propagates importance through the term co-occurrence...",
      "keywords": [
        "pagerank",
        "importance",
        "damping",
        "terms",
        "cortical",
        "term",
        "iterations",
        "layer",
        "propagates",
        "referenced"
      ],
      "full_content": "# PageRank \u2014 Importance Discovery\n\n**Purpose:** Identify which terms matter most in the corpus, independent of raw frequency.\n\n**Implementation:** `cortical/analysis/pagerank.py`\n\n**How It Works:**\n```\nimportance[term] = (1 - damping) / N + damping \u00d7 \u03a3 (neighbor_importance \u00d7 edge_weight / neighbor_outgoing_sum)\n```\n\nThe algorithm iteratively propagates importance through the term co-occurrence graph. Terms that are referenced by many important terms become important themselves\u2014a recursive definition that converges to stable values.\n\n**Key Parameters:**\n- `damping = 0.85`: The probability of following a link vs. jumping to a random node\n- `tolerance = 1e-6`: Convergence threshold (stops when no term changes by more than this)\n- `max_iterations = 20`: Upper bound on iterations\n\n**Three Variants:**\n1. **Standard PageRank**: Applied to Layer 0 (tokens) and Layer 1 (bigrams)\n2. **Semantic PageRank**: Adjusts edge weights by relation type (IsA connections count 1.5\u00d7 more than CoOccurs)\n3. **Hierarchical PageRank**: Propagates importance across all 4 layers with separate cross-layer damping\n\n**Why This Matters for Code Search:**\n- Common utility functions referenced everywhere get high PageRank\n- Core abstractions that everything depends on surface naturally\n- Prevents over-emphasis on boilerplate code that appears frequently but isn't semantically central\n\n**Performance:** O(iterations \u00d7 edges), typically 100-500ms for 10K tokens with early convergence usually at 5-10 iterations.\n\n---\n\n*This chapter is part of [The Cortical Chronicles](../README.md), a self-documenting book generated by the Cortical Text Processor.*\n"
    },
    {
      "path": "01-foundations/alg-query-expansion.md",
      "title": "Query Expansion \u2014 Semantic Bridging",
      "section": "foundations",
      "tags": [
        "algorithms",
        "foundations",
        "ir-theory"
      ],
      "source_files": [
        "docs/VISION.md",
        "cortical/query/expansion.py"
      ],
      "excerpt": "1. **Lateral Connection Expansion:** 2. **Concept Cluster Membership:** 3. **Code Concept Synonyms:** ``` Query: \"neural\" Hop 0: neural (1.0) Hop 1: networks (0.4), learning (0.35) Hop 2: deep...",
      "keywords": [
        "query",
        "expansion",
        "hop",
        "terms",
        "term",
        "cortical",
        "concept",
        "score",
        "per",
        "cluster"
      ],
      "full_content": "# Query Expansion \u2014 Semantic Bridging\n\n**Purpose:** Transform literal query terms into semantically enriched term sets.\n\n**Implementation:** `cortical/query/expansion.py`\n\n**Three Expansion Methods:**\n\n1. **Lateral Connection Expansion:**\n   - Follow co-occurrence edges from query terms\n   - Score: `edge_weight \u00d7 neighbor_score \u00d7 0.6`\n   - Takes top 5 neighbors per query term\n\n2. **Concept Cluster Membership:**\n   - Find concepts containing query terms\n   - Add other cluster members as expansions\n   - Score: `concept.pagerank \u00d7 member.pagerank \u00d7 0.4`\n\n3. **Code Concept Synonyms:**\n   - Programming-specific synonym groups (get/fetch/load, create/make/build)\n   - Limited to 3 synonyms per term to prevent drift\n\n**Multi-Hop Inference:**\n```\nQuery: \"neural\"\n  Hop 0: neural (1.0)\n  Hop 1: networks (0.4), learning (0.35)\n  Hop 2: deep (0.098) \u2014 via learning with decay\n```\n\nChain validity is scored by relation type pairs:\n- `(IsA, IsA)`: 1.0 \u2014 fully transitive (dog\u2192animal\u2192living_thing)\n- `(RelatedTo, RelatedTo)`: 0.6 \u2014 weaker transitivity\n- `(Antonym, Antonym)`: 0.3 \u2014 double negation, avoid\n\n---\n\n*This chapter is part of [The Cortical Chronicles](../README.md), a self-documenting book generated by the Cortical Text Processor.*\n"
    },
    {
      "path": "01-foundations/alg-semantic-extraction.md",
      "title": "Semantic Relation Extraction \u2014 Knowledge Graph Construction",
      "section": "foundations",
      "tags": [
        "algorithms",
        "foundations",
        "ir-theory"
      ],
      "source_files": [
        "docs/VISION.md",
        "cortical/semantics.py"
      ],
      "excerpt": "24 regex patterns detect 10+ relation types: ```python r'(\\w+)\\s+(?:is|are)\\s+(?:a|an)\\s+(?:type\\s+of\\s+)?(\\w+)' \u2192 IsA (0.9 confidence) r'(\\w+)\\s+(?:is|are)\\s+(?:a\\s+)?part\\s+of' \u2192 PartOf (0.95...",
      "keywords": [
        "relation",
        "semantic",
        "isa",
        "partof",
        "causes",
        "cortical",
        "confidence",
        "extraction",
        "knowledge",
        "text"
      ],
      "full_content": "# Semantic Relation Extraction \u2014 Knowledge Graph Construction\n\n**Purpose:** Extract typed relationships (IsA, PartOf, Causes) from document text.\n\n**Implementation:** `cortical/semantics.py`\n\n**Pattern-Based Extraction:**\n24 regex patterns detect 10+ relation types:\n```python\nr'(\\w+)\\s+(?:is|are)\\s+(?:a|an)\\s+(?:type\\s+of\\s+)?(\\w+)' \u2192 IsA (0.9 confidence)\nr'(\\w+)\\s+(?:is|are)\\s+(?:a\\s+)?part\\s+of' \u2192 PartOf (0.95 confidence)\nr'(\\w+)\\s+(?:causes|leads?\\s+to)' \u2192 Causes (0.9 confidence)\n```\n\n**Semantic Retrofitting:**\nBlends co-occurrence weights with semantic relation knowledge:\n```\nnew_weight = \u03b1 \u00d7 original_weight + (1-\u03b1) \u00d7 semantic_target_weight\n```\nWith \u03b1 = 0.3, semantic signals dominate (70%) while preserving some corpus statistics (30%).\n\n**Relation Weight Multipliers:**\n| Relation | Weight | Semantics |\n|----------|--------|-----------|\n| SameAs | 2.0 | Strongest synonymy |\n| IsA | 1.5 | Hypernymy |\n| PartOf | 1.3 | Meronymy |\n| RelatedTo | 0.8 | Generic |\n| Antonym | -0.5 | Opposition |\n\n---\n\n*This chapter is part of [The Cortical Chronicles](../README.md), a self-documenting book generated by the Cortical Text Processor.*\n"
    },
    {
      "path": "02-architecture/index.md",
      "title": "Architecture Overview",
      "section": "architecture",
      "tags": [
        "architecture",
        "index",
        "modules"
      ],
      "source_files": [
        "/home/user/Opus-code-test/cortical/patterns.py",
        "/home/user/Opus-code-test/cortical/code_concepts.py",
        "/home/user/Opus-code-test/cortical/persistence.py",
        "/home/user/Opus-code-test/cortical/semantics.py",
        "/home/user/Opus-code-test/cortical/constants.py",
        "/home/user/Opus-code-test/cortical/embeddings.py",
        "/home/user/Opus-code-test/cortical/chunk_index.py",
        "/home/user/Opus-code-test/cortical/results.py",
        "/home/user/Opus-code-test/cortical/tokenizer.py",
        "/home/user/Opus-code-test/cortical/observability.py",
        "/home/user/Opus-code-test/cortical/diff.py",
        "/home/user/Opus-code-test/cortical/progress.py",
        "/home/user/Opus-code-test/cortical/gaps.py",
        "/home/user/Opus-code-test/cortical/minicolumn.py",
        "/home/user/Opus-code-test/cortical/config.py",
        "/home/user/Opus-code-test/cortical/fingerprint.py",
        "/home/user/Opus-code-test/cortical/layers.py",
        "/home/user/Opus-code-test/cortical/mcp_server.py",
        "/home/user/Opus-code-test/cortical/fluent.py",
        "/home/user/Opus-code-test/cortical/cli_wrapper.py",
        "/home/user/Opus-code-test/cortical/types.py",
        "/home/user/Opus-code-test/cortical/state_storage.py",
        "/home/user/Opus-code-test/cortical/validation.py",
        "/home/user/Opus-code-test/cortical/analysis/clustering.py",
        "/home/user/Opus-code-test/cortical/analysis/quality.py",
        "/home/user/Opus-code-test/cortical/analysis/pagerank.py",
        "/home/user/Opus-code-test/cortical/analysis/__init__.py",
        "/home/user/Opus-code-test/cortical/analysis/tfidf.py",
        "/home/user/Opus-code-test/cortical/analysis/activation.py",
        "/home/user/Opus-code-test/cortical/analysis/connections.py",
        "/home/user/Opus-code-test/cortical/analysis/utils.py",
        "/home/user/Opus-code-test/cortical/query/ranking.py",
        "/home/user/Opus-code-test/cortical/query/intent.py",
        "/home/user/Opus-code-test/cortical/query/passages.py",
        "/home/user/Opus-code-test/cortical/query/__init__.py",
        "/home/user/Opus-code-test/cortical/query/search.py",
        "/home/user/Opus-code-test/cortical/query/definitions.py",
        "/home/user/Opus-code-test/cortical/query/chunking.py",
        "/home/user/Opus-code-test/cortical/query/expansion.py",
        "/home/user/Opus-code-test/cortical/processor/persistence_api.py",
        "/home/user/Opus-code-test/cortical/processor/documents.py",
        "/home/user/Opus-code-test/cortical/processor/__init__.py",
        "/home/user/Opus-code-test/cortical/processor/core.py",
        "/home/user/Opus-code-test/cortical/processor/introspection.py",
        "/home/user/Opus-code-test/cortical/processor/compute.py"
      ],
      "excerpt": "This section documents the architecture of the Cortical Text Processor through automatically extracted module metadata. 8 modules: 3 modules: 3 modules: 3 modules: 3 modules: 3 modules: 6 modules: 8...",
      "keywords": [
        "modules",
        "mod",
        "processor",
        "cortical",
        "module",
        "__init__",
        "observability",
        "persistence",
        "architecture",
        "documents"
      ],
      "full_content": "# Architecture Overview\n\nThis section documents the architecture of the Cortical Text Processor through automatically extracted module metadata.\n\n## Statistics\n\n- **Total Modules**: 45\n- **Module Groups**: 9\n- **Classes**: 50\n- **Functions**: 422\n\n## Module Groups\n\n### [Analysis](mod-analysis.md)\n\n8 modules:\n\n- `__init__.py`\n- `activation.py`\n- `clustering.py`\n- `connections.py`\n- `pagerank.py`\n- `quality.py`\n- `tfidf.py`\n- `utils.py`\n\n### [Configuration](mod-configuration.md)\n\n3 modules:\n\n- `config.py`\n- `constants.py`\n- `validation.py`\n\n### [Data Structures](mod-data-structures.md)\n\n3 modules:\n\n- `layers.py`\n- `minicolumn.py`\n- `types.py`\n\n### [Nlp](mod-nlp.md)\n\n3 modules:\n\n- `embeddings.py`\n- `semantics.py`\n- `tokenizer.py`\n\n### [Observability](mod-observability.md)\n\n3 modules:\n\n- `observability.py`\n- `progress.py`\n- `results.py`\n\n### [Persistence](mod-persistence.md)\n\n3 modules:\n\n- `chunk_index.py`\n- `persistence.py`\n- `state_storage.py`\n\n### [Processor](mod-processor.md)\n\n6 modules:\n\n- `__init__.py`\n- `compute.py`\n- `core.py`\n- `documents.py`\n- `introspection.py`\n- `persistence_api.py`\n\n### [Query](mod-query.md)\n\n8 modules:\n\n- `__init__.py`\n- `chunking.py`\n- `definitions.py`\n- `expansion.py`\n- `intent.py`\n- `passages.py`\n- `ranking.py`\n- `search.py`\n\n### [Utilities](mod-utilities.md)\n\n8 modules:\n\n- `cli_wrapper.py`\n- `code_concepts.py`\n- `diff.py`\n- `fingerprint.py`\n- `fluent.py`\n- `gaps.py`\n- `mcp_server.py`\n- `patterns.py`\n\n---\n\n*This chapter is part of [The Cortical Chronicles](../README.md), a self-documenting book generated by the Cortical Text Processor.*\n"
    },
    {
      "path": "02-architecture/mod-analysis.md",
      "title": "Graph Algorithms",
      "section": "architecture",
      "tags": [
        "architecture",
        "modules",
        "analysis"
      ],
      "source_files": [
        "/home/user/Opus-code-test/cortical/analysis/clustering.py",
        "/home/user/Opus-code-test/cortical/analysis/quality.py",
        "/home/user/Opus-code-test/cortical/analysis/pagerank.py",
        "/home/user/Opus-code-test/cortical/analysis/__init__.py",
        "/home/user/Opus-code-test/cortical/analysis/tfidf.py",
        "/home/user/Opus-code-test/cortical/analysis/activation.py",
        "/home/user/Opus-code-test/cortical/analysis/connections.py",
        "/home/user/Opus-code-test/cortical/analysis/utils.py"
      ],
      "excerpt": "Graph algorithms for computing importance, relevance, and clusters. Analysis Module =============== Graph analysis algorithms for the cortical network. Contains implementations of: Activation...",
      "keywords": [
        "float",
        "dict",
        "int",
        "str",
        "layers",
        "hierarchicallayer",
        "python",
        "corticallayer",
        "algorithms",
        "clustering"
      ],
      "full_content": "# Graph Algorithms\n\nGraph algorithms for computing importance, relevance, and clusters.\n\n## Modules\n\n- **__init__.py**: Analysis Module\n- **activation.py**: Activation propagation algorithm.\n- **clustering.py**: Clustering algorithms for community detection.\n- **connections.py**: Connection building algorithms for network layers.\n- **pagerank.py**: PageRank algorithms for importance scoring.\n- **quality.py**: Clustering quality metrics.\n- **tfidf.py**: TF-IDF and BM25 scoring algorithms.\n- **utils.py**: Utility functions and classes for analysis algorithms.\n\n\n## __init__.py\n\nAnalysis Module\n===============\n\nGraph analysis algorithms for the cortical network.\n\nContains implementations of:\n- PageRank for importance scoring\n- TF-IDF for term weighting\n- Louvain community det...\n\n\n### Dependencies\n\n**Standard Library:**\n\n- `activation.propagate_activation`\n- `clustering._louvain_core`\n- `clustering.build_concept_clusters`\n- `clustering.cluster_by_label_propagation`\n- `clustering.cluster_by_louvain`\n- ... and 22 more\n\n\n\n## activation.py\n\nActivation propagation algorithm.\n\nContains:\n- propagate_activation: Spread activation through the network layers\n\n\n### Functions\n\n#### propagate_activation\n\n```python\npropagate_activation(layers: Dict[CorticalLayer, HierarchicalLayer], iterations: int = 3, decay: float = 0.8, lateral_weight: float = 0.3) -> None\n```\n\nPropagate activation through the network.\n\n### Dependencies\n\n**Standard Library:**\n\n- `layers.CorticalLayer`\n- `layers.HierarchicalLayer`\n- `typing.Dict`\n\n\n\n## clustering.py\n\nClustering algorithms for community detection.\n\nContains:\n- cluster_by_louvain: Louvain modularity optimization (recommended)\n- cluster_by_label_propagation: Label propagation clustering (legacy)\n- bu...\n\n\n### Functions\n\n#### cluster_by_label_propagation\n\n```python\ncluster_by_label_propagation(layer: HierarchicalLayer, min_cluster_size: int = 3, max_iterations: int = 20, cluster_strictness: float = 1.0, bridge_weight: float = 0.0) -> Dict[int, List[str]]\n```\n\nCluster minicolumns using label propagation.\n\n#### cluster_by_louvain\n\n```python\ncluster_by_louvain(layer: HierarchicalLayer, min_cluster_size: int = 3, resolution: float = 1.0, max_iterations: int = 10) -> Dict[int, List[str]]\n```\n\nCluster minicolumns using Louvain community detection.\n\n#### build_concept_clusters\n\n```python\nbuild_concept_clusters(layers: Dict[CorticalLayer, HierarchicalLayer], clusters: Dict[int, List[str]], doc_vote_threshold: float = 0.1) -> None\n```\n\nBuild concept layer from token clusters.\n\n### Dependencies\n\n**Standard Library:**\n\n- `collections.defaultdict`\n- `layers.CorticalLayer`\n- `layers.HierarchicalLayer`\n- `typing.Dict`\n- `typing.List`\n- ... and 2 more\n\n\n\n## connections.py\n\nConnection building algorithms for network layers.\n\nContains:\n- compute_document_connections: Build document-to-document similarity connections\n- compute_bigram_connections: Build lateral connections ...\n\n\n### Functions\n\n#### compute_concept_connections\n\n```python\ncompute_concept_connections(layers: Dict[CorticalLayer, HierarchicalLayer], semantic_relations: List[Tuple[str, str, str, float]] = None, min_shared_docs: int = 1, min_jaccard: float = 0.1, use_member_semantics: bool = False, use_embedding_similarity: bool = False, embedding_threshold: float = 0.3, embeddings: Dict[str, List[float]] = None) -> Dict[str, Any]\n```\n\nBuild lateral connections between concepts in Layer 2.\n\n#### compute_bigram_connections\n\n```python\ncompute_bigram_connections(layers: Dict[CorticalLayer, HierarchicalLayer], min_shared_docs: int = 1, component_weight: float = 0.5, chain_weight: float = 0.7, cooccurrence_weight: float = 0.3, max_bigrams_per_term: int = 100, max_bigrams_per_doc: int = 500, max_connections_per_bigram: int = 50) -> Dict[str, Any]\n```\n\nBuild lateral connections between bigrams in Layer 1.\n\n#### compute_document_connections\n\n```python\ncompute_document_connections(layers: Dict[CorticalLayer, HierarchicalLayer], documents: Dict[str, str], min_shared_terms: int = 3) -> None\n```\n\nBuild lateral connections between documents.\n\n### Dependencies\n\n**Standard Library:**\n\n- `collections.defaultdict`\n- `layers.CorticalLayer`\n- `layers.HierarchicalLayer`\n- `minicolumn.Minicolumn`\n- `typing.Any`\n- ... and 5 more\n\n\n\n## pagerank.py\n\nPageRank algorithms for importance scoring.\n\nContains:\n- compute_pagerank: Standard PageRank for a single layer\n- compute_semantic_pagerank: PageRank with semantic relation weighting\n- compute_hierarc...\n\n\n### Functions\n\n#### compute_pagerank\n\n```python\ncompute_pagerank(layer: HierarchicalLayer, damping: float = 0.85, iterations: int = 20, tolerance: float = 1e-06) -> Dict[str, float]\n```\n\nCompute PageRank scores for minicolumns in a layer.\n\n#### compute_semantic_pagerank\n\n```python\ncompute_semantic_pagerank(layer: HierarchicalLayer, semantic_relations: List[Tuple[str, str, str, float]], relation_weights: Optional[Dict[str, float]] = None, damping: float = 0.85, iterations: int = 20, tolerance: float = 1e-06) -> Dict[str, Any]\n```\n\nCompute PageRank with semantic relation type weighting.\n\n#### compute_hierarchical_pagerank\n\n```python\ncompute_hierarchical_pagerank(layers: Dict[CorticalLayer, HierarchicalLayer], layer_iterations: int = 10, global_iterations: int = 5, damping: float = 0.85, cross_layer_damping: float = 0.7, tolerance: float = 0.0001) -> Dict[str, Any]\n```\n\nCompute PageRank with cross-layer propagation.\n\n### Dependencies\n\n**Standard Library:**\n\n- `collections.defaultdict`\n- `constants.RELATION_WEIGHTS`\n- `layers.CorticalLayer`\n- `layers.HierarchicalLayer`\n- `typing.Any`\n- ... and 4 more\n\n\n\n## quality.py\n\nClustering quality metrics.\n\nContains:\n- compute_clustering_quality: Comprehensive quality evaluation (modularity, silhouette, balance)\n- _compute_modularity: Modularity Q metric\n- _compute_silhouette...\n\n\n### Functions\n\n#### compute_clustering_quality\n\n```python\ncompute_clustering_quality(layers: Dict[CorticalLayer, HierarchicalLayer], sample_size: int = 500) -> Dict[str, Any]\n```\n\nCompute clustering quality metrics for the concept layer.\n\n### Dependencies\n\n**Standard Library:**\n\n- `collections.defaultdict`\n- `layers.CorticalLayer`\n- `layers.HierarchicalLayer`\n- `random`\n- `typing.Any`\n- ... and 3 more\n\n\n\n## tfidf.py\n\nTF-IDF and BM25 scoring algorithms.\n\nContains:\n- compute_tfidf: Traditional TF-IDF scoring\n- compute_bm25: Okapi BM25 scoring with length normalization\n- _tfidf_core: Pure TF-IDF algorithm for unit te...\n\n\n### Functions\n\n#### compute_tfidf\n\n```python\ncompute_tfidf(layers: Dict[CorticalLayer, HierarchicalLayer], documents: Dict[str, str]) -> None\n```\n\nCompute TF-IDF scores for tokens.\n\n#### compute_bm25\n\n```python\ncompute_bm25(layers: Dict[CorticalLayer, HierarchicalLayer], documents: Dict[str, str], doc_lengths: Dict[str, int], avg_doc_length: float, k1: float = 1.2, b: float = 0.75) -> None\n```\n\nCompute BM25 scores for tokens.\n\n### Dependencies\n\n**Standard Library:**\n\n- `layers.CorticalLayer`\n- `layers.HierarchicalLayer`\n- `math`\n- `typing.Dict`\n- `typing.Tuple`\n\n\n\n## utils.py\n\nUtility functions and classes for analysis algorithms.\n\nContains:\n- SparseMatrix: Zero-dependency sparse matrix for bigram connections\n- Similarity functions: cosine_similarity, _doc_similarity, _vect...\n\n\n### Classes\n\n#### SparseMatrix\n\nSimple sparse matrix implementation using dictionary of keys (DOK) format.\n\n**Methods:**\n\n- `set`\n- `get`\n- `multiply_transpose`\n- `get_nonzero`\n\n### Functions\n\n#### cosine_similarity\n\n```python\ncosine_similarity(vec1: Dict[str, float], vec2: Dict[str, float]) -> float\n```\n\nCompute cosine similarity between two sparse vectors.\n\n#### SparseMatrix.set\n\n```python\nSparseMatrix.set(self, row: int, col: int, value: float) -> None\n```\n\nSet value at (row, col).\n\n#### SparseMatrix.get\n\n```python\nSparseMatrix.get(self, row: int, col: int) -> float\n```\n\nGet value at (row, col).\n\n#### SparseMatrix.multiply_transpose\n\n```python\nSparseMatrix.multiply_transpose(self) -> 'SparseMatrix'\n```\n\nMultiply this matrix by its transpose: M * M^T\n\n#### SparseMatrix.get_nonzero\n\n```python\nSparseMatrix.get_nonzero(self) -> List[Tuple[int, int, float]]\n```\n\nGet all non-zero entries.\n\n### Dependencies\n\n**Standard Library:**\n\n- `collections.defaultdict`\n- `math`\n- `typing.Dict`\n- `typing.List`\n- `typing.Tuple`\n\n\n\n---\n\n*This chapter is part of [The Cortical Chronicles](../README.md), a self-documenting book generated by the Cortical Text Processor.*\n"
    },
    {
      "path": "02-architecture/mod-configuration.md",
      "title": "Configuration",
      "section": "architecture",
      "tags": [
        "architecture",
        "modules",
        "configuration"
      ],
      "source_files": [
        "/home/user/Opus-code-test/cortical/constants.py",
        "/home/user/Opus-code-test/cortical/config.py",
        "/home/user/Opus-code-test/cortical/validation.py"
      ],
      "excerpt": "Configuration management and validation. Configuration Module ==================== Centralized configuration for the Cortical Text Processor. This module provides a dataclass-based configuration...",
      "keywords": [
        "configuration",
        "python",
        "corticalconfig",
        "value",
        "module",
        "cortical",
        "typing",
        "none",
        "validation",
        "text"
      ],
      "full_content": "# Configuration\n\nConfiguration management and validation.\n\n## Modules\n\n- **config.py**: Configuration Module\n- **constants.py**: Centralized constants for the Cortical Text Processor.\n- **validation.py**: Validation Module\n\n\n## config.py\n\nConfiguration Module\n====================\n\nCentralized configuration for the Cortical Text Processor.\n\nThis module provides a dataclass-based configuration system that allows\nusers to customize algori...\n\n\n### Classes\n\n#### CorticalConfig\n\nConfiguration settings for the Cortical Text Processor.\n\n**Methods:**\n\n- `copy`\n- `to_dict`\n- `from_dict`\n\n### Functions\n\n#### get_default_config\n\n```python\nget_default_config() -> CorticalConfig\n```\n\nGet a new instance of the default configuration.\n\n#### CorticalConfig.copy\n\n```python\nCorticalConfig.copy(self) -> 'CorticalConfig'\n```\n\nCreate a copy of this configuration.\n\n#### CorticalConfig.to_dict\n\n```python\nCorticalConfig.to_dict(self) -> Dict\n```\n\nConvert configuration to a dictionary for serialization.\n\n#### CorticalConfig.from_dict\n\n```python\nCorticalConfig.from_dict(cls, data: Dict) -> 'CorticalConfig'\n```\n\nCreate configuration from a dictionary.\n\n### Dependencies\n\n**Standard Library:**\n\n- `dataclasses.dataclass`\n- `dataclasses.field`\n- `math`\n- `typing.Dict`\n- `typing.FrozenSet`\n- ... and 1 more\n\n\n\n## constants.py\n\nCentralized constants for the Cortical Text Processor.\n\nThis module provides a single source of truth for constants used across\nmultiple modules, preventing drift and inconsistencies.\n\nTask #96: Centr...\n\n\n### Dependencies\n\n**Standard Library:**\n\n- `typing.Dict`\n- `typing.FrozenSet`\n\n\n\n## validation.py\n\nValidation Module\n=================\n\nInput validation utilities and decorators for the Cortical Text Processor.\n\nThis module provides reusable validators and decorators to ensure\nparameters are valid ...\n\n\n### Functions\n\n#### validate_non_empty_string\n\n```python\nvalidate_non_empty_string(value: Any, param_name: str) -> None\n```\n\nValidate that a value is a non-empty string.\n\n#### validate_positive_int\n\n```python\nvalidate_positive_int(value: Any, param_name: str) -> None\n```\n\nValidate that a value is a positive integer.\n\n#### validate_non_negative_int\n\n```python\nvalidate_non_negative_int(value: Any, param_name: str) -> None\n```\n\nValidate that a value is a non-negative integer.\n\n#### validate_range\n\n```python\nvalidate_range(value: Any, param_name: str, min_val: Optional[float] = None, max_val: Optional[float] = None, inclusive: bool = True) -> None\n```\n\nValidate that a numeric value is within a specified range.\n\n#### validate_params\n\n```python\nvalidate_params(**validators: Callable[[Any], None]) -> Callable[[F], F]\n```\n\nDecorator to validate function parameters.\n\n#### marks_stale\n\n```python\nmarks_stale(*computation_types: str) -> Callable[[F], F]\n```\n\nDecorator to mark computations as stale after method execution.\n\n#### marks_fresh\n\n```python\nmarks_fresh(*computation_types: str) -> Callable[[F], F]\n```\n\nDecorator to mark computations as fresh after method execution.\n\n### Dependencies\n\n**Standard Library:**\n\n- `functools.wraps`\n- `inspect`\n- `typing.Any`\n- `typing.Callable`\n- `typing.Optional`\n- ... and 2 more\n\n\n\n---\n\n*This chapter is part of [The Cortical Chronicles](../README.md), a self-documenting book generated by the Cortical Text Processor.*\n"
    },
    {
      "path": "02-architecture/mod-data-structures.md",
      "title": "Data Structures",
      "section": "architecture",
      "tags": [
        "architecture",
        "modules",
        "data-structures"
      ],
      "source_files": [
        "/home/user/Opus-code-test/cortical/minicolumn.py",
        "/home/user/Opus-code-test/cortical/layers.py",
        "/home/user/Opus-code-test/cortical/types.py"
      ],
      "excerpt": "Fundamental data structures used throughout the system. Layers Module ============= Defines the hierarchical layer structure inspired by the visual cortex. The neocortex processes information through...",
      "keywords": [
        "minicolumn",
        "python",
        "hierarchicallayer",
        "self",
        "str",
        "layer",
        "float",
        "dict",
        "edge",
        "get"
      ],
      "full_content": "# Data Structures\n\nFundamental data structures used throughout the system.\n\n## Modules\n\n- **layers.py**: Layers Module\n- **minicolumn.py**: Minicolumn Module\n- **types.py**: Type Aliases for the Cortical Text Processor.\n\n\n## layers.py\n\nLayers Module\n=============\n\nDefines the hierarchical layer structure inspired by the visual cortex.\n\nThe neocortex processes information through a hierarchy of layers,\neach extracting progressively m...\n\n\n### Classes\n\n#### CorticalLayer\n\nEnumeration of cortical processing layers.\n\n**Methods:**\n\n- `description`\n- `analogy`\n\n#### HierarchicalLayer\n\nA layer in the cortical hierarchy containing minicolumns.\n\n**Methods:**\n\n- `get_or_create_minicolumn`\n- `get_minicolumn`\n- `get_by_id`\n- `remove_minicolumn`\n- `column_count`\n- `total_connections`\n- `average_activation`\n- `activation_range`\n- `sparsity`\n- `top_by_pagerank`\n- `top_by_tfidf`\n- `top_by_activation`\n- `to_dict`\n- `from_dict`\n\n### Functions\n\n#### CorticalLayer.description\n\n```python\nCorticalLayer.description(self) -> str\n```\n\nHuman-readable description of this layer.\n\n#### CorticalLayer.analogy\n\n```python\nCorticalLayer.analogy(self) -> str\n```\n\nVisual cortex analogy for this layer.\n\n#### HierarchicalLayer.get_or_create_minicolumn\n\n```python\nHierarchicalLayer.get_or_create_minicolumn(self, content: str) -> Minicolumn\n```\n\nGet existing minicolumn or create new one.\n\n#### HierarchicalLayer.get_minicolumn\n\n```python\nHierarchicalLayer.get_minicolumn(self, content: str) -> Optional[Minicolumn]\n```\n\nGet a minicolumn by content, or None if not found.\n\n#### HierarchicalLayer.get_by_id\n\n```python\nHierarchicalLayer.get_by_id(self, col_id: str) -> Optional[Minicolumn]\n```\n\nGet a minicolumn by its ID in O(1) time.\n\n#### HierarchicalLayer.remove_minicolumn\n\n```python\nHierarchicalLayer.remove_minicolumn(self, content: str) -> bool\n```\n\nRemove a minicolumn from this layer.\n\n#### HierarchicalLayer.column_count\n\n```python\nHierarchicalLayer.column_count(self) -> int\n```\n\nReturn the number of minicolumns in this layer.\n\n#### HierarchicalLayer.total_connections\n\n```python\nHierarchicalLayer.total_connections(self) -> int\n```\n\nReturn total number of lateral connections in this layer.\n\n#### HierarchicalLayer.average_activation\n\n```python\nHierarchicalLayer.average_activation(self) -> float\n```\n\nCalculate average activation across all minicolumns.\n\n#### HierarchicalLayer.activation_range\n\n```python\nHierarchicalLayer.activation_range(self) -> tuple\n```\n\nReturn (min, max) activation values.\n\n#### HierarchicalLayer.sparsity\n\n```python\nHierarchicalLayer.sparsity(self, threshold_fraction: float = 0.5) -> float\n```\n\nCalculate sparsity (fraction of columns with below-average activation).\n\n#### HierarchicalLayer.top_by_pagerank\n\n```python\nHierarchicalLayer.top_by_pagerank(self, n: int = 10) -> list\n```\n\nGet top minicolumns by PageRank score.\n\n#### HierarchicalLayer.top_by_tfidf\n\n```python\nHierarchicalLayer.top_by_tfidf(self, n: int = 10) -> list\n```\n\nGet top minicolumns by TF-IDF score.\n\n#### HierarchicalLayer.top_by_activation\n\n```python\nHierarchicalLayer.top_by_activation(self, n: int = 10) -> list\n```\n\nGet top minicolumns by activation level.\n\n#### HierarchicalLayer.to_dict\n\n```python\nHierarchicalLayer.to_dict(self) -> Dict\n```\n\nConvert layer to dictionary for serialization.\n\n#### HierarchicalLayer.from_dict\n\n```python\nHierarchicalLayer.from_dict(cls, data: Dict) -> 'HierarchicalLayer'\n```\n\nCreate a layer from dictionary representation.\n\n### Dependencies\n\n**Standard Library:**\n\n- `enum.IntEnum`\n- `minicolumn.Minicolumn`\n- `typing.Dict`\n- `typing.Iterator`\n- `typing.Optional`\n\n\n\n## minicolumn.py\n\nMinicolumn Module\n=================\n\nCore data structure representing a cortical minicolumn.\n\nIn the neocortex, minicolumns are vertical structures containing\n~80-100 neurons that respond to similar f...\n\n\n### Classes\n\n#### Edge\n\nTyped edge with metadata for ConceptNet-style graph representation.\n\n**Methods:**\n\n- `to_dict`\n- `from_dict`\n\n#### Minicolumn\n\nA minicolumn represents a single concept/feature at a given hierarchy level.\n\n**Methods:**\n\n- `lateral_connections`\n- `lateral_connections`\n- `add_lateral_connection`\n- `add_lateral_connections_batch`\n- `set_lateral_connection_weight`\n- `add_typed_connection`\n- `get_typed_connection`\n- `get_connections_by_type`\n- `get_connections_by_source`\n- `add_feedforward_connection`\n- `add_feedback_connection`\n- `connection_count`\n- `top_connections`\n- `to_dict`\n- `from_dict`\n\n### Functions\n\n#### Edge.to_dict\n\n```python\nEdge.to_dict(self) -> Dict\n```\n\nConvert to dictionary for serialization.\n\n#### Edge.from_dict\n\n```python\nEdge.from_dict(cls, data: Dict) -> 'Edge'\n```\n\nCreate an Edge from dictionary representation.\n\n#### Minicolumn.lateral_connections\n\n```python\nMinicolumn.lateral_connections(self, value: Dict[str, float]) -> None\n```\n\nSet lateral connections from a dictionary (for deserialization).\n\n#### Minicolumn.add_lateral_connection\n\n```python\nMinicolumn.add_lateral_connection(self, target_id: str, weight: float = 1.0) -> None\n```\n\nAdd or strengthen a lateral connection to another column.\n\n#### Minicolumn.add_lateral_connections_batch\n\n```python\nMinicolumn.add_lateral_connections_batch(self, connections: Dict[str, float]) -> None\n```\n\nAdd or strengthen multiple lateral connections at once.\n\n#### Minicolumn.set_lateral_connection_weight\n\n```python\nMinicolumn.set_lateral_connection_weight(self, target_id: str, weight: float) -> None\n```\n\nSet the weight of a lateral connection directly (not additive).\n\n#### Minicolumn.add_typed_connection\n\n```python\nMinicolumn.add_typed_connection(self, target_id: str, weight: float = 1.0, relation_type: str = 'co_occurrence', confidence: float = 1.0, source: str = 'corpus') -> None\n```\n\nAdd or update a typed connection with metadata.\n\n#### Minicolumn.get_typed_connection\n\n```python\nMinicolumn.get_typed_connection(self, target_id: str) -> Optional[Edge]\n```\n\nGet a typed connection by target ID.\n\n#### Minicolumn.get_connections_by_type\n\n```python\nMinicolumn.get_connections_by_type(self, relation_type: str) -> List[Edge]\n```\n\nGet all typed connections with a specific relation type.\n\n#### Minicolumn.get_connections_by_source\n\n```python\nMinicolumn.get_connections_by_source(self, source: str) -> List[Edge]\n```\n\nGet all typed connections from a specific source.\n\n#### Minicolumn.add_feedforward_connection\n\n```python\nMinicolumn.add_feedforward_connection(self, target_id: str, weight: float = 1.0) -> None\n```\n\nAdd or strengthen a feedforward connection to a lower layer column.\n\n#### Minicolumn.add_feedback_connection\n\n```python\nMinicolumn.add_feedback_connection(self, target_id: str, weight: float = 1.0) -> None\n```\n\nAdd or strengthen a feedback connection to a higher layer column.\n\n#### Minicolumn.connection_count\n\n```python\nMinicolumn.connection_count(self) -> int\n```\n\nReturn the number of lateral connections.\n\n#### Minicolumn.top_connections\n\n```python\nMinicolumn.top_connections(self, n: int = 5) -> list\n```\n\nGet the strongest lateral connections.\n\n#### Minicolumn.to_dict\n\n```python\nMinicolumn.to_dict(self) -> Dict\n```\n\nConvert to dictionary for serialization.\n\n#### Minicolumn.from_dict\n\n```python\nMinicolumn.from_dict(cls, data: Dict) -> 'Minicolumn'\n```\n\nCreate a minicolumn from dictionary representation.\n\n### Dependencies\n\n**Standard Library:**\n\n- `dataclasses.asdict`\n- `dataclasses.dataclass`\n- `dataclasses.field`\n- `typing.Dict`\n- `typing.List`\n- ... and 2 more\n\n\n\n## types.py\n\nType Aliases for the Cortical Text Processor.\n\nThis module provides type aliases for complex return types used throughout\nthe library, making function signatures more readable and maintainable.\n\nTask ...\n\n\n### Dependencies\n\n**Standard Library:**\n\n- `typing.Any`\n- `typing.Dict`\n- `typing.List`\n- `typing.Optional`\n- `typing.Tuple`\n\n\n\n---\n\n*This chapter is part of [The Cortical Chronicles](../README.md), a self-documenting book generated by the Cortical Text Processor.*\n"
    },
    {
      "path": "02-architecture/mod-nlp.md",
      "title": "NLP Components",
      "section": "architecture",
      "tags": [
        "architecture",
        "modules",
        "nlp"
      ],
      "source_files": [
        "/home/user/Opus-code-test/cortical/semantics.py",
        "/home/user/Opus-code-test/cortical/embeddings.py",
        "/home/user/Opus-code-test/cortical/tokenizer.py"
      ],
      "excerpt": "Natural language processing components for tokenization and semantics. Embeddings Module ================= Graph-based embeddings for the cortical network. Implements three methods for computing term...",
      "keywords": [
        "str",
        "dict",
        "float",
        "python",
        "list",
        "tokenizer",
        "int",
        "tuple",
        "embeddings",
        "term"
      ],
      "full_content": "# NLP Components\n\nNatural language processing components for tokenization and semantics.\n\n## Modules\n\n- **embeddings.py**: Embeddings Module\n- **semantics.py**: Semantics Module\n- **tokenizer.py**: Tokenizer Module\n\n\n## embeddings.py\n\nEmbeddings Module\n=================\n\nGraph-based embeddings for the cortical network.\n\nImplements three methods for computing term embeddings from the\nconnection graph structure:\n1. Adjacency: Direct ...\n\n\n### Functions\n\n#### compute_graph_embeddings\n\n```python\ncompute_graph_embeddings(layers: Dict[CorticalLayer, HierarchicalLayer], dimensions: int = 64, method: str = 'adjacency', max_terms: Optional[int] = None) -> Tuple[Dict[str, List[float]], Dict[str, Any]]\n```\n\nCompute embeddings for tokens based on graph structure.\n\n#### embedding_similarity\n\n```python\nembedding_similarity(embeddings: Dict[str, List[float]], term1: str, term2: str) -> float\n```\n\nCompute cosine similarity between two term embeddings.\n\n#### find_similar_by_embedding\n\n```python\nfind_similar_by_embedding(embeddings: Dict[str, List[float]], term: str, top_n: int = 10) -> List[Tuple[str, float]]\n```\n\nFind terms most similar to a given term by embedding.\n\n### Dependencies\n\n**Standard Library:**\n\n- `collections.defaultdict`\n- `layers.CorticalLayer`\n- `layers.HierarchicalLayer`\n- `math`\n- `random`\n- ... and 5 more\n\n\n\n## semantics.py\n\nSemantics Module\n================\n\nCorpus-derived semantic relations and retrofitting.\n\nExtracts semantic relationships from co-occurrence patterns,\nthen uses them to adjust connection weights (retrof...\n\n\n### Functions\n\n#### extract_pattern_relations\n\n```python\nextract_pattern_relations(documents: Dict[str, str], valid_terms: Set[str], min_confidence: float = 0.5) -> List[Tuple[str, str, str, float]]\n```\n\nExtract semantic relations using pattern matching on document text.\n\n#### get_pattern_statistics\n\n```python\nget_pattern_statistics(relations: List[Tuple[str, str, str, float]]) -> Dict[str, Any]\n```\n\nGet statistics about extracted pattern-based relations.\n\n#### extract_corpus_semantics\n\n```python\nextract_corpus_semantics(layers: Dict[CorticalLayer, HierarchicalLayer], documents: Dict[str, str], tokenizer, window_size: int = 5, min_cooccurrence: int = 2, use_pattern_extraction: bool = True, min_pattern_confidence: float = 0.6, max_similarity_pairs: int = 100000, min_context_keys: int = 3) -> List[Tuple[str, str, str, float]]\n```\n\nExtract semantic relations from corpus co-occurrence patterns.\n\n#### retrofit_connections\n\n```python\nretrofit_connections(layers: Dict[CorticalLayer, HierarchicalLayer], semantic_relations: List[Tuple[str, str, str, float]], iterations: int = 10, alpha: float = 0.3) -> Dict[str, Any]\n```\n\nRetrofit lateral connections using semantic relations.\n\n#### retrofit_embeddings\n\n```python\nretrofit_embeddings(embeddings: Dict[str, List[float]], semantic_relations: List[Tuple[str, str, str, float]], iterations: int = 10, alpha: float = 0.4) -> Dict[str, Any]\n```\n\nRetrofit embeddings using semantic relations.\n\n#### get_relation_type_weight\n\n```python\nget_relation_type_weight(relation_type: str) -> float\n```\n\nGet the weight for a relation type.\n\n#### build_isa_hierarchy\n\n```python\nbuild_isa_hierarchy(semantic_relations: List[Tuple[str, str, str, float]]) -> Tuple[Dict[str, Set[str]], Dict[str, Set[str]]]\n```\n\nBuild IsA parent-child hierarchy from semantic relations.\n\n#### get_ancestors\n\n```python\nget_ancestors(term: str, parents: Dict[str, Set[str]], max_depth: int = 10) -> Dict[str, int]\n```\n\nGet all ancestors of a term with their depth in the hierarchy.\n\n#### get_descendants\n\n```python\nget_descendants(term: str, children: Dict[str, Set[str]], max_depth: int = 10) -> Dict[str, int]\n```\n\nGet all descendants of a term with their depth in the hierarchy.\n\n#### inherit_properties\n\n```python\ninherit_properties(semantic_relations: List[Tuple[str, str, str, float]], decay_factor: float = 0.7, max_depth: int = 5) -> Dict[str, Dict[str, Tuple[float, str, int]]]\n```\n\nCompute inherited properties for all terms based on IsA hierarchy.\n\n#### compute_property_similarity\n\n```python\ncompute_property_similarity(term1: str, term2: str, inherited_properties: Dict[str, Dict[str, Tuple[float, str, int]]], direct_properties: Optional[Dict[str, Dict[str, float]]] = None) -> float\n```\n\nCompute similarity between terms based on shared properties (direct + inherited).\n\n#### apply_inheritance_to_connections\n\n```python\napply_inheritance_to_connections(layers: Dict[CorticalLayer, HierarchicalLayer], inherited_properties: Dict[str, Dict[str, Tuple[float, str, int]]], boost_factor: float = 0.3) -> Dict[str, Any]\n```\n\nBoost lateral connections between terms that share inherited properties.\n\n### Dependencies\n\n**Standard Library:**\n\n- `collections.defaultdict`\n- `constants.RELATION_WEIGHTS`\n- `copy`\n- `layers.CorticalLayer`\n- `layers.HierarchicalLayer`\n- ... and 10 more\n\n\n\n## tokenizer.py\n\nTokenizer Module\n================\n\nText tokenization with stemming and word variant support.\n\nLike early visual processing, the tokenizer extracts basic features\n(words) from raw input, filtering nois...\n\n\n### Classes\n\n#### Tokenizer\n\nText tokenizer with stemming and word variant support.\n\n**Methods:**\n\n- `tokenize`\n- `extract_ngrams`\n- `stem`\n- `get_word_variants`\n- `add_word_mapping`\n\n### Functions\n\n#### split_identifier\n\n```python\nsplit_identifier(identifier: str) -> List[str]\n```\n\nSplit a code identifier into component words.\n\n#### Tokenizer.tokenize\n\n```python\nTokenizer.tokenize(self, text: str, split_identifiers: Optional[bool] = None) -> List[str]\n```\n\nExtract tokens from text.\n\n#### Tokenizer.extract_ngrams\n\n```python\nTokenizer.extract_ngrams(self, tokens: List[str], n: int = 2) -> List[str]\n```\n\nExtract n-grams from token list.\n\n#### Tokenizer.stem\n\n```python\nTokenizer.stem(self, word: str) -> str\n```\n\nApply simple suffix stripping (Porter-lite stemming).\n\n#### Tokenizer.get_word_variants\n\n```python\nTokenizer.get_word_variants(self, word: str) -> List[str]\n```\n\nGet related words/variants for query expansion.\n\n#### Tokenizer.add_word_mapping\n\n```python\nTokenizer.add_word_mapping(self, word: str, variants: List[str]) -> None\n```\n\nAdd a custom word mapping for query expansion.\n\n### Dependencies\n\n**Standard Library:**\n\n- `re`\n- `typing.Dict`\n- `typing.List`\n- `typing.Optional`\n- `typing.Set`\n- ... and 1 more\n\n\n\n---\n\n*This chapter is part of [The Cortical Chronicles](../README.md), a self-documenting book generated by the Cortical Text Processor.*\n"
    },
    {
      "path": "02-architecture/mod-observability.md",
      "title": "Observability",
      "section": "architecture",
      "tags": [
        "architecture",
        "modules",
        "observability"
      ],
      "source_files": [
        "/home/user/Opus-code-test/cortical/results.py",
        "/home/user/Opus-code-test/cortical/observability.py",
        "/home/user/Opus-code-test/cortical/progress.py"
      ],
      "excerpt": "Metrics collection and progress tracking. Observability Module ==================== Provides timing hooks, metrics collection, and trace context for monitoring the Cortical Text Processor's...",
      "keywords": [
        "str",
        "python",
        "none",
        "self",
        "metricscollector",
        "update",
        "passagematch",
        "optional",
        "dict",
        "progress"
      ],
      "full_content": "# Observability\n\nMetrics collection and progress tracking.\n\n## Modules\n\n- **observability.py**: Observability Module\n- **progress.py**: Progress reporting infrastructure for long-running operations.\n- **results.py**: Result Dataclasses for Cortical Text Processor\n\n\n## observability.py\n\nObservability Module\n====================\n\nProvides timing hooks, metrics collection, and trace context for monitoring\nthe Cortical Text Processor's performance and operations.\n\nThis module follows th...\n\n\n### Classes\n\n#### MetricsCollector\n\nCollects and aggregates timing and count metrics for operations.\n\n**Methods:**\n\n- `record_timing`\n- `record_count`\n- `get_operation_stats`\n- `get_all_stats`\n- `get_trace`\n- `reset`\n- `enable`\n- `disable`\n- `trace_context`\n- `get_summary`\n\n#### TraceContext\n\nContext for request tracing across operations.\n\n**Methods:**\n\n- `elapsed_ms`\n\n### Functions\n\n#### timed\n\n```python\ntimed(operation_name: Optional[str] = None, include_args: bool = False)\n```\n\nDecorator for timing method calls and recording to metrics.\n\n#### measure_time\n\n```python\nmeasure_time(func: Callable) -> Callable\n```\n\nSimple timing decorator that logs execution time.\n\n#### get_global_metrics\n\n```python\nget_global_metrics() -> MetricsCollector\n```\n\nGet the global metrics collector instance.\n\n#### enable_global_metrics\n\n```python\nenable_global_metrics() -> None\n```\n\nEnable global metrics collection.\n\n#### disable_global_metrics\n\n```python\ndisable_global_metrics() -> None\n```\n\nDisable global metrics collection.\n\n#### reset_global_metrics\n\n```python\nreset_global_metrics() -> None\n```\n\nReset global metrics.\n\n#### MetricsCollector.record_timing\n\n```python\nMetricsCollector.record_timing(self, operation: str, duration_ms: float, trace_id: Optional[str] = None, context: Optional[Dict[str, Any]] = None) -> None\n```\n\nRecord a timing measurement for an operation.\n\n#### MetricsCollector.record_count\n\n```python\nMetricsCollector.record_count(self, metric_name: str, count: int = 1) -> None\n```\n\nRecord a simple count metric.\n\n#### MetricsCollector.get_operation_stats\n\n```python\nMetricsCollector.get_operation_stats(self, operation: str) -> Dict[str, Any]\n```\n\nGet statistics for a specific operation.\n\n#### MetricsCollector.get_all_stats\n\n```python\nMetricsCollector.get_all_stats(self) -> Dict[str, Dict[str, Any]]\n```\n\nGet statistics for all operations.\n\n#### MetricsCollector.get_trace\n\n```python\nMetricsCollector.get_trace(self, trace_id: str) -> List[tuple]\n```\n\nGet all operations recorded for a trace ID.\n\n#### MetricsCollector.reset\n\n```python\nMetricsCollector.reset(self) -> None\n```\n\nClear all collected metrics.\n\n#### MetricsCollector.enable\n\n```python\nMetricsCollector.enable(self) -> None\n```\n\nEnable metrics collection.\n\n#### MetricsCollector.disable\n\n```python\nMetricsCollector.disable(self) -> None\n```\n\nDisable metrics collection.\n\n#### MetricsCollector.trace_context\n\n```python\nMetricsCollector.trace_context(self, trace_id: str)\n```\n\nContext manager for tracing a block of operations.\n\n#### MetricsCollector.get_summary\n\n```python\nMetricsCollector.get_summary(self) -> str\n```\n\nGet a human-readable summary of all metrics.\n\n#### TraceContext.elapsed_ms\n\n```python\nTraceContext.elapsed_ms(self) -> float\n```\n\nGet elapsed time since trace started in milliseconds.\n\n### Dependencies\n\n**Standard Library:**\n\n- `collections.defaultdict`\n- `contextlib.contextmanager`\n- `functools`\n- `logging`\n- `time`\n- ... and 5 more\n\n\n\n## progress.py\n\nProgress reporting infrastructure for long-running operations.\n\nThis module provides a flexible progress reporting system that supports:\n- Console output with nice formatting\n- Custom callbacks for in...\n\n\n### Classes\n\n#### ProgressReporter\n\nProtocol for progress reporters.\n\n**Methods:**\n\n- `update`\n- `complete`\n\n#### ConsoleProgressReporter\n\nConsole-based progress reporter with nice formatting.\n\n**Methods:**\n\n- `update`\n- `complete`\n\n#### CallbackProgressReporter\n\nProgress reporter that calls a custom callback function.\n\n**Methods:**\n\n- `update`\n- `complete`\n\n#### SilentProgressReporter\n\nNo-op progress reporter for silent operation.\n\n**Methods:**\n\n- `update`\n- `complete`\n\n#### MultiPhaseProgress\n\nHelper for tracking progress across multiple sequential phases.\n\n**Methods:**\n\n- `start_phase`\n- `update`\n- `complete_phase`\n- `overall_progress`\n\n### Functions\n\n#### ProgressReporter.update\n\n```python\nProgressReporter.update(self, phase: str, percent: float, message: Optional[str] = None) -> None\n```\n\nUpdate progress for a specific phase.\n\n#### ProgressReporter.complete\n\n```python\nProgressReporter.complete(self, phase: str, message: Optional[str] = None) -> None\n```\n\nMark a phase as complete.\n\n#### ConsoleProgressReporter.update\n\n```python\nConsoleProgressReporter.update(self, phase: str, percent: float, message: Optional[str] = None) -> None\n```\n\nUpdate progress display.\n\n#### ConsoleProgressReporter.complete\n\n```python\nConsoleProgressReporter.complete(self, phase: str, message: Optional[str] = None) -> None\n```\n\nMark phase as complete and move to new line.\n\n#### CallbackProgressReporter.update\n\n```python\nCallbackProgressReporter.update(self, phase: str, percent: float, message: Optional[str] = None) -> None\n```\n\nCall callback with progress update.\n\n#### CallbackProgressReporter.complete\n\n```python\nCallbackProgressReporter.complete(self, phase: str, message: Optional[str] = None) -> None\n```\n\nCall callback with completion notification.\n\n#### SilentProgressReporter.update\n\n```python\nSilentProgressReporter.update(self, phase: str, percent: float, message: Optional[str] = None) -> None\n```\n\nDo nothing.\n\n#### SilentProgressReporter.complete\n\n```python\nSilentProgressReporter.complete(self, phase: str, message: Optional[str] = None) -> None\n```\n\nDo nothing.\n\n#### MultiPhaseProgress.start_phase\n\n```python\nMultiPhaseProgress.start_phase(self, phase: str) -> None\n```\n\nStart a new phase.\n\n#### MultiPhaseProgress.update\n\n```python\nMultiPhaseProgress.update(self, percent: float, message: Optional[str] = None) -> None\n```\n\nUpdate progress within current phase.\n\n#### MultiPhaseProgress.complete_phase\n\n```python\nMultiPhaseProgress.complete_phase(self, message: Optional[str] = None) -> None\n```\n\nMark current phase as complete.\n\n#### MultiPhaseProgress.overall_progress\n\n```python\nMultiPhaseProgress.overall_progress(self) -> float\n```\n\nGet overall progress across all phases (0-100).\n\n### Dependencies\n\n**Standard Library:**\n\n- `abc.ABC`\n- `abc.abstractmethod`\n- `sys`\n- `time`\n- `typing.Any`\n- ... and 4 more\n\n\n\n## results.py\n\nResult Dataclasses for Cortical Text Processor\n===============================================\n\nStrongly-typed result containers for query operations that provide\nIDE autocomplete and type checking su...\n\n\n### Classes\n\n#### DocumentMatch\n\nA document search result with relevance score.\n\n**Methods:**\n\n- `to_dict`\n- `to_tuple`\n- `from_tuple`\n- `from_dict`\n\n#### PassageMatch\n\nA passage retrieval result with text, location, and relevance score.\n\n**Methods:**\n\n- `to_dict`\n- `to_tuple`\n- `location`\n- `length`\n- `from_tuple`\n- `from_dict`\n\n#### QueryResult\n\nComplete query result with matches and metadata.\n\n**Methods:**\n\n- `to_dict`\n- `top_match`\n- `match_count`\n- `average_score`\n- `from_dict`\n\n### Functions\n\n#### convert_document_matches\n\n```python\nconvert_document_matches(results: List[tuple], metadata: Optional[Dict[str, Dict[str, Any]]] = None) -> List[DocumentMatch]\n```\n\nConvert list of (doc_id, score) tuples to DocumentMatch objects.\n\n#### convert_passage_matches\n\n```python\nconvert_passage_matches(results: List[tuple], metadata: Optional[Dict[str, Dict[str, Any]]] = None) -> List[PassageMatch]\n```\n\nConvert list of (doc_id, text, start, end, score) tuples to PassageMatch objects.\n\n#### DocumentMatch.to_dict\n\n```python\nDocumentMatch.to_dict(self) -> Dict[str, Any]\n```\n\nConvert to dictionary.\n\n#### DocumentMatch.to_tuple\n\n```python\nDocumentMatch.to_tuple(self) -> tuple\n```\n\nConvert to tuple format (doc_id, score).\n\n#### DocumentMatch.from_tuple\n\n```python\nDocumentMatch.from_tuple(cls, doc_id: str, score: float, metadata: Optional[Dict[str, Any]] = None) -> 'DocumentMatch'\n```\n\nCreate from tuple format (doc_id, score).\n\n#### DocumentMatch.from_dict\n\n```python\nDocumentMatch.from_dict(cls, data: Dict[str, Any]) -> 'DocumentMatch'\n```\n\nCreate from dictionary.\n\n#### PassageMatch.to_dict\n\n```python\nPassageMatch.to_dict(self) -> Dict[str, Any]\n```\n\nConvert to dictionary.\n\n#### PassageMatch.to_tuple\n\n```python\nPassageMatch.to_tuple(self) -> tuple\n```\n\nConvert to tuple format (doc_id, text, start, end, score).\n\n#### PassageMatch.location\n\n```python\nPassageMatch.location(self) -> str\n```\n\nGet citation-style location string.\n\n#### PassageMatch.length\n\n```python\nPassageMatch.length(self) -> int\n```\n\nGet passage length in characters.\n\n#### PassageMatch.from_tuple\n\n```python\nPassageMatch.from_tuple(cls, doc_id: str, text: str, start: int, end: int, score: float, metadata: Optional[Dict[str, Any]] = None) -> 'PassageMatch'\n```\n\nCreate from tuple format (doc_id, text, start, end, score).\n\n#### PassageMatch.from_dict\n\n```python\nPassageMatch.from_dict(cls, data: Dict[str, Any]) -> 'PassageMatch'\n```\n\nCreate from dictionary.\n\n#### QueryResult.to_dict\n\n```python\nQueryResult.to_dict(self) -> Dict[str, Any]\n```\n\nConvert to dictionary with nested match dicts.\n\n#### QueryResult.top_match\n\n```python\nQueryResult.top_match(self) -> Union[DocumentMatch, PassageMatch, None]\n```\n\nGet the highest-scoring match.\n\n#### QueryResult.match_count\n\n```python\nQueryResult.match_count(self) -> int\n```\n\nGet number of matches.\n\n#### QueryResult.average_score\n\n```python\nQueryResult.average_score(self) -> float\n```\n\nGet average relevance score across all matches.\n\n#### QueryResult.from_dict\n\n```python\nQueryResult.from_dict(cls, data: Dict[str, Any]) -> 'QueryResult'\n```\n\nCreate from dictionary.\n\n### Dependencies\n\n**Standard Library:**\n\n- `dataclasses.asdict`\n- `dataclasses.dataclass`\n- `dataclasses.field`\n- `typing.Any`\n- `typing.Dict`\n- ... and 3 more\n\n\n\n---\n\n*This chapter is part of [The Cortical Chronicles](../README.md), a self-documenting book generated by the Cortical Text Processor.*\n"
    },
    {
      "path": "02-architecture/mod-persistence.md",
      "title": "Persistence Layer",
      "section": "architecture",
      "tags": [
        "architecture",
        "modules",
        "persistence"
      ],
      "source_files": [
        "/home/user/Opus-code-test/cortical/persistence.py",
        "/home/user/Opus-code-test/cortical/chunk_index.py",
        "/home/user/Opus-code-test/cortical/state_storage.py"
      ],
      "excerpt": "Save and load functionality for maintaining processor state. Chunk-based indexing for git-compatible corpus storage. This module provides append-only, time-stamped JSON chunks that can be safely...",
      "keywords": [
        "str",
        "dict",
        "python",
        "self",
        "bool",
        "none",
        "chunkloader",
        "optional",
        "chunk",
        "list"
      ],
      "full_content": "# Persistence Layer\n\nSave and load functionality for maintaining processor state.\n\n## Modules\n\n- **chunk_index.py**: Chunk-based indexing for git-compatible corpus storage.\n- **persistence.py**: Persistence Module\n- **state_storage.py**: Git-friendly State Storage Module\n\n\n## chunk_index.py\n\nChunk-based indexing for git-compatible corpus storage.\n\nThis module provides append-only, time-stamped JSON chunks that can be\nsafely committed to git without merge conflicts. Each indexing session\nc...\n\n\n### Classes\n\n#### ChunkOperation\n\nA single operation in a chunk (add, modify, or delete).\n\n**Methods:**\n\n- `to_dict`\n- `from_dict`\n\n#### Chunk\n\nA chunk containing operations from a single indexing session.\n\n**Methods:**\n\n- `to_dict`\n- `from_dict`\n- `get_filename`\n\n#### ChunkWriter\n\nWrites indexing session changes to timestamped JSON chunks.\n\n**Methods:**\n\n- `add_document`\n- `modify_document`\n- `delete_document`\n- `has_operations`\n- `save`\n\n#### ChunkLoader\n\nLoads and combines chunks to rebuild document state.\n\n**Methods:**\n\n- `get_chunk_files`\n- `load_chunk`\n- `load_all`\n- `get_documents`\n- `get_mtimes`\n- `get_metadata`\n- `get_chunks`\n- `compute_hash`\n- `is_cache_valid`\n- `save_cache_hash`\n- `get_stats`\n\n#### ChunkCompactor\n\nCompacts multiple chunk files into a single file.\n\n**Methods:**\n\n- `compact`\n\n### Functions\n\n#### get_changes_from_manifest\n\n```python\nget_changes_from_manifest(current_files: Dict[str, float], manifest: Dict[str, float]) -> Tuple[List[str], List[str], List[str]]\n```\n\nCompare current files to manifest to find changes.\n\n#### ChunkOperation.to_dict\n\n```python\nChunkOperation.to_dict(self) -> Dict[str, Any]\n```\n\nConvert to dictionary for JSON serialization.\n\n#### ChunkOperation.from_dict\n\n```python\nChunkOperation.from_dict(cls, d: Dict[str, Any]) -> 'ChunkOperation'\n```\n\nCreate from dictionary.\n\n#### Chunk.to_dict\n\n```python\nChunk.to_dict(self) -> Dict[str, Any]\n```\n\nConvert to dictionary for JSON serialization.\n\n#### Chunk.from_dict\n\n```python\nChunk.from_dict(cls, d: Dict[str, Any]) -> 'Chunk'\n```\n\nCreate from dictionary.\n\n#### Chunk.get_filename\n\n```python\nChunk.get_filename(self) -> str\n```\n\nGenerate filename for this chunk.\n\n#### ChunkWriter.add_document\n\n```python\nChunkWriter.add_document(self, doc_id: str, content: str, mtime: Optional[float] = None, metadata: Optional[Dict[str, Any]] = None)\n```\n\nRecord an add operation.\n\n#### ChunkWriter.modify_document\n\n```python\nChunkWriter.modify_document(self, doc_id: str, content: str, mtime: Optional[float] = None, metadata: Optional[Dict[str, Any]] = None)\n```\n\nRecord a modify operation.\n\n#### ChunkWriter.delete_document\n\n```python\nChunkWriter.delete_document(self, doc_id: str)\n```\n\nRecord a delete operation.\n\n#### ChunkWriter.has_operations\n\n```python\nChunkWriter.has_operations(self) -> bool\n```\n\nCheck if any operations were recorded.\n\n#### ChunkWriter.save\n\n```python\nChunkWriter.save(self, warn_size_kb: int = DEFAULT_WARN_SIZE_KB) -> Optional[Path]\n```\n\nSave chunk to file.\n\n#### ChunkLoader.get_chunk_files\n\n```python\nChunkLoader.get_chunk_files(self) -> List[Path]\n```\n\nGet all chunk files sorted by timestamp.\n\n#### ChunkLoader.load_chunk\n\n```python\nChunkLoader.load_chunk(self, filepath: Path) -> Chunk\n```\n\nLoad a single chunk file.\n\n#### ChunkLoader.load_all\n\n```python\nChunkLoader.load_all(self) -> Dict[str, str]\n```\n\nLoad all chunks and replay operations to get current document state.\n\n#### ChunkLoader.get_documents\n\n```python\nChunkLoader.get_documents(self) -> Dict[str, str]\n```\n\nGet loaded documents (calls load_all if needed).\n\n#### ChunkLoader.get_mtimes\n\n```python\nChunkLoader.get_mtimes(self) -> Dict[str, float]\n```\n\nGet document modification times.\n\n#### ChunkLoader.get_metadata\n\n```python\nChunkLoader.get_metadata(self) -> Dict[str, Dict[str, Any]]\n```\n\nGet document metadata (doc_type, headings, etc.).\n\n#### ChunkLoader.get_chunks\n\n```python\nChunkLoader.get_chunks(self) -> List[Chunk]\n```\n\nGet loaded chunks.\n\n#### ChunkLoader.compute_hash\n\n```python\nChunkLoader.compute_hash(self) -> str\n```\n\nCompute hash of current document state.\n\n#### ChunkLoader.is_cache_valid\n\n```python\nChunkLoader.is_cache_valid(self, cache_path: str, cache_hash_path: Optional[str] = None) -> bool\n```\n\nCheck if pkl cache is valid for current chunk state.\n\n#### ChunkLoader.save_cache_hash\n\n```python\nChunkLoader.save_cache_hash(self, cache_path: str, cache_hash_path: Optional[str] = None)\n```\n\nSave current document hash for cache validation.\n\n#### ChunkLoader.get_stats\n\n```python\nChunkLoader.get_stats(self) -> Dict[str, Any]\n```\n\nGet statistics about loaded chunks.\n\n#### ChunkCompactor.compact\n\n```python\nChunkCompactor.compact(self, before: Optional[str] = None, keep_recent: int = 0, dry_run: bool = False) -> Dict[str, Any]\n```\n\nCompact chunks into a single chunk.\n\n### Dependencies\n\n**Standard Library:**\n\n- `dataclasses.asdict`\n- `dataclasses.dataclass`\n- `dataclasses.field`\n- `datetime.datetime`\n- `hashlib`\n- ... and 11 more\n\n\n\n## persistence.py\n\nPersistence Module\n==================\n\nSave and load functionality for the cortical processor.\n\nSupports:\n- Pickle serialization for full state\n- JSON export for graph visualization\n- Incremental upda...\n\n\n### Classes\n\n#### SignatureVerificationError\n\nRaised when HMAC signature verification fails.\n\n### Functions\n\n#### save_processor\n\n```python\nsave_processor(filepath: str, layers: Dict[CorticalLayer, HierarchicalLayer], documents: Dict[str, str], document_metadata: Optional[Dict[str, Dict[str, Any]]] = None, embeddings: Optional[Dict[str, list]] = None, semantic_relations: Optional[list] = None, metadata: Optional[Dict] = None, verbose: bool = True, format: str = 'pickle', signing_key: Optional[bytes] = None) -> None\n```\n\nSave processor state to a file.\n\n#### load_processor\n\n```python\nload_processor(filepath: str, verbose: bool = True, format: Optional[str] = None, verify_key: Optional[bytes] = None) -> tuple\n```\n\nLoad processor state from a file.\n\n#### export_graph_json\n\n```python\nexport_graph_json(filepath: str, layers: Dict[CorticalLayer, HierarchicalLayer], layer_filter: Optional[CorticalLayer] = None, min_weight: float = 0.0, max_nodes: int = 500, verbose: bool = True) -> Dict\n```\n\nExport graph structure as JSON for visualization.\n\n#### export_embeddings_json\n\n```python\nexport_embeddings_json(filepath: str, embeddings: Dict[str, list], metadata: Optional[Dict] = None) -> None\n```\n\nExport embeddings as JSON.\n\n#### load_embeddings_json\n\n```python\nload_embeddings_json(filepath: str) -> Dict[str, list]\n```\n\nLoad embeddings from JSON.\n\n#### export_semantic_relations_json\n\n```python\nexport_semantic_relations_json(filepath: str, relations: list) -> None\n```\n\nExport semantic relations as JSON.\n\n#### load_semantic_relations_json\n\n```python\nload_semantic_relations_json(filepath: str) -> list\n```\n\nLoad semantic relations from JSON.\n\n#### get_state_summary\n\n```python\nget_state_summary(layers: Dict[CorticalLayer, HierarchicalLayer], documents: Dict[str, str]) -> Dict\n```\n\nGet a summary of the current processor state.\n\n#### export_conceptnet_json\n\n```python\nexport_conceptnet_json(filepath: str, layers: Dict[CorticalLayer, HierarchicalLayer], semantic_relations: Optional[list] = None, include_cross_layer: bool = True, include_typed_edges: bool = True, min_weight: float = 0.0, min_confidence: float = 0.0, max_nodes_per_layer: int = 100, verbose: bool = True) -> Dict[str, Any]\n```\n\nExport ConceptNet-style graph for visualization.\n\n### Dependencies\n\n**Standard Library:**\n\n- `hashlib`\n- `hmac`\n- `json`\n- `layers.CorticalLayer`\n- `layers.HierarchicalLayer`\n- ... and 9 more\n\n\n\n## state_storage.py\n\nGit-friendly State Storage Module\n=================================\n\nReplaces pickle-based persistence with JSON files that:\n- Can be diff'd and reviewed in git\n- Won't cause merge conflicts\n- Support...\n\n\n### Classes\n\n#### StateManifest\n\nManifest file tracking state version and component checksums.\n\n**Methods:**\n\n- `to_dict`\n- `from_dict`\n- `update_checksum`\n\n#### StateWriter\n\nWrites processor state to git-friendly JSON files.\n\n**Methods:**\n\n- `save_layer`\n- `save_documents`\n- `save_semantic_relations`\n- `save_embeddings`\n- `save_manifest`\n- `save_all`\n\n#### StateLoader\n\nLoads processor state from git-friendly JSON files.\n\n**Methods:**\n\n- `exists`\n- `load_manifest`\n- `validate_checksum`\n- `load_layer`\n- `load_documents`\n- `load_semantic_relations`\n- `load_embeddings`\n- `load_all`\n- `get_stats`\n\n### Functions\n\n#### migrate_pkl_to_json\n\n```python\nmigrate_pkl_to_json(pkl_path: str, json_dir: str, verbose: bool = True) -> bool\n```\n\nMigrate a pickle file to git-friendly JSON format.\n\n#### StateManifest.to_dict\n\n```python\nStateManifest.to_dict(self) -> Dict[str, Any]\n```\n\nConvert to dictionary for JSON serialization.\n\n#### StateManifest.from_dict\n\n```python\nStateManifest.from_dict(cls, data: Dict[str, Any]) -> 'StateManifest'\n```\n\nCreate manifest from dictionary.\n\n#### StateManifest.update_checksum\n\n```python\nStateManifest.update_checksum(self, component: str, content: str) -> bool\n```\n\nUpdate checksum for a component.\n\n#### StateWriter.save_layer\n\n```python\nStateWriter.save_layer(self, layer: HierarchicalLayer, force: bool = False) -> bool\n```\n\nSave a single layer to its JSON file.\n\n#### StateWriter.save_documents\n\n```python\nStateWriter.save_documents(self, documents: Dict[str, str], document_metadata: Optional[Dict[str, Dict[str, Any]]] = None, force: bool = False) -> bool\n```\n\nSave documents and metadata.\n\n#### StateWriter.save_semantic_relations\n\n```python\nStateWriter.save_semantic_relations(self, relations: List[Tuple], force: bool = False) -> bool\n```\n\nSave semantic relations.\n\n#### StateWriter.save_embeddings\n\n```python\nStateWriter.save_embeddings(self, embeddings: Dict[str, List[float]], force: bool = False) -> bool\n```\n\nSave graph embeddings.\n\n#### StateWriter.save_manifest\n\n```python\nStateWriter.save_manifest(self) -> None\n```\n\nSave the manifest file.\n\n#### StateWriter.save_all\n\n```python\nStateWriter.save_all(self, layers: Dict[CorticalLayer, HierarchicalLayer], documents: Dict[str, str], document_metadata: Optional[Dict[str, Dict[str, Any]]] = None, embeddings: Optional[Dict[str, List[float]]] = None, semantic_relations: Optional[List[Tuple]] = None, stale_computations: Optional[Set[str]] = None, force: bool = False, verbose: bool = True) -> Dict[str, bool]\n```\n\nSave all processor state.\n\n#### StateLoader.exists\n\n```python\nStateLoader.exists(self) -> bool\n```\n\nCheck if state directory exists and has manifest.\n\n#### StateLoader.load_manifest\n\n```python\nStateLoader.load_manifest(self) -> StateManifest\n```\n\nLoad the manifest file.\n\n#### StateLoader.validate_checksum\n\n```python\nStateLoader.validate_checksum(self, component: str, filepath: Path) -> bool\n```\n\nValidate a component's checksum.\n\n#### StateLoader.load_layer\n\n```python\nStateLoader.load_layer(self, level: int) -> HierarchicalLayer\n```\n\nLoad a single layer.\n\n#### StateLoader.load_documents\n\n```python\nStateLoader.load_documents(self) -> Tuple[Dict[str, str], Dict[str, Dict[str, Any]]]\n```\n\nLoad documents and metadata.\n\n#### StateLoader.load_semantic_relations\n\n```python\nStateLoader.load_semantic_relations(self) -> List[Tuple]\n```\n\nLoad semantic relations.\n\n#### StateLoader.load_embeddings\n\n```python\nStateLoader.load_embeddings(self) -> Dict[str, List[float]]\n```\n\nLoad graph embeddings.\n\n#### StateLoader.load_all\n\n```python\nStateLoader.load_all(self, validate: bool = True, verbose: bool = True) -> Tuple[Dict[CorticalLayer, HierarchicalLayer], Dict[str, str], Dict[str, Dict[str, Any]], Dict[str, List[float]], List[Tuple], Dict[str, Any]]\n```\n\nLoad all processor state.\n\n#### StateLoader.get_stats\n\n```python\nStateLoader.get_stats(self) -> Dict[str, Any]\n```\n\nGet statistics about stored state without loading everything.\n\n### Dependencies\n\n**Standard Library:**\n\n- `dataclasses.asdict`\n- `dataclasses.dataclass`\n- `dataclasses.field`\n- `datetime.datetime`\n- `hashlib`\n- ... and 13 more\n\n\n\n---\n\n*This chapter is part of [The Cortical Chronicles](../README.md), a self-documenting book generated by the Cortical Text Processor.*\n"
    },
    {
      "path": "02-architecture/mod-processor.md",
      "title": "Core Processor",
      "section": "architecture",
      "tags": [
        "architecture",
        "modules",
        "processor"
      ],
      "source_files": [
        "/home/user/Opus-code-test/cortical/processor/persistence_api.py",
        "/home/user/Opus-code-test/cortical/processor/documents.py",
        "/home/user/Opus-code-test/cortical/processor/__init__.py",
        "/home/user/Opus-code-test/cortical/processor/core.py",
        "/home/user/Opus-code-test/cortical/processor/introspection.py",
        "/home/user/Opus-code-test/cortical/processor/compute.py"
      ],
      "excerpt": "The core processor orchestrates all text processing operations. Cortical Text Processor - Main processor package. This package splits the monolithic processor.py into focused modules:...",
      "keywords": [
        "str",
        "python",
        "self",
        "dict",
        "computemixin",
        "bool",
        "introspectionmixin",
        "none",
        "int",
        "true"
      ],
      "full_content": "# Core Processor\n\nThe core processor orchestrates all text processing operations.\n\n## Modules\n\n- **__init__.py**: Cortical Text Processor - Main processor package.\n- **compute.py**: Compute methods: analysis, clustering, embeddings, semantic extraction.\n- **core.py**: Core processor functionality: initialization, staleness tracking, and layer management.\n- **documents.py**: Document management: processing, adding, removing, and metadata handling.\n- **introspection.py**: Introspection: state inspection, fingerprints, gaps, and summaries.\n- **persistence_api.py**: Persistence API: save, load, export, and migration methods.\n\n\n## __init__.py\n\nCortical Text Processor - Main processor package.\n\nThis package splits the monolithic processor.py into focused modules:\n- core.py: Initialization, staleness tracking, layer management\n- documents.py:...\n\n\n### Classes\n\n#### CorticalTextProcessor\n\nNeocortex-inspired text processing system.\n\n### Dependencies\n\n**Standard Library:**\n\n- `compute.ComputeMixin`\n- `core.CoreMixin`\n- `documents.DocumentsMixin`\n- `introspection.IntrospectionMixin`\n- `persistence_api.PersistenceMixin`\n- ... and 1 more\n\n\n\n## compute.py\n\nCompute methods: analysis, clustering, embeddings, semantic extraction.\n\nThis module contains all methods that perform computational analysis on the corpus,\nincluding PageRank, TF-IDF, clustering, and...\n\n\n### Classes\n\n#### ComputeMixin\n\nMixin providing computation functionality.\n\n**Methods:**\n\n- `recompute`\n- `compute_all`\n- `resume_from_checkpoint`\n- `propagate_activation`\n- `compute_importance`\n- `compute_semantic_importance`\n- `compute_hierarchical_importance`\n- `compute_tfidf`\n- `compute_bm25`\n- `compute_document_connections`\n- `compute_bigram_connections`\n- `build_concept_clusters`\n- `compute_clustering_quality`\n- `compute_concept_connections`\n- `extract_corpus_semantics`\n- `extract_pattern_relations`\n- `retrofit_connections`\n- `compute_property_inheritance`\n- `compute_property_similarity`\n- `compute_graph_embeddings`\n- `retrofit_embeddings`\n- `embedding_similarity`\n- `find_similar_by_embedding`\n\n### Functions\n\n#### ComputeMixin.recompute\n\n```python\nComputeMixin.recompute(self, level: str = 'stale', verbose: bool = True) -> Dict[str, bool]\n```\n\nRecompute specified analysis levels.\n\n#### ComputeMixin.compute_all\n\n```python\nComputeMixin.compute_all(self, verbose: bool = True, build_concepts: bool = True, pagerank_method: str = 'standard', connection_strategy: str = 'document_overlap', cluster_strictness: float = 1.0, bridge_weight: float = 0.0, progress_callback: Optional[ProgressReporter] = None, show_progress: bool = False, checkpoint_dir: Optional[str] = None, resume: bool = False) -> Dict[str, Any]\n```\n\nRun all computation steps.\n\n#### ComputeMixin.resume_from_checkpoint\n\n```python\nComputeMixin.resume_from_checkpoint(cls, checkpoint_dir: str, config: Optional['CorticalConfig'] = None, verbose: bool = True) -> 'CorticalTextProcessor'\n```\n\nResume processing from a checkpoint directory.\n\n#### ComputeMixin.propagate_activation\n\n```python\nComputeMixin.propagate_activation(self, iterations: int = 3, decay: float = 0.8, verbose: bool = True) -> None\n```\n\nNone\n\n#### ComputeMixin.compute_importance\n\n```python\nComputeMixin.compute_importance(self, verbose: bool = True) -> None\n```\n\nNone\n\n#### ComputeMixin.compute_semantic_importance\n\n```python\nComputeMixin.compute_semantic_importance(self, relation_weights: Optional[Dict[str, float]] = None, verbose: bool = True) -> Dict[str, Any]\n```\n\nCompute PageRank with semantic relation weighting.\n\n#### ComputeMixin.compute_hierarchical_importance\n\n```python\nComputeMixin.compute_hierarchical_importance(self, layer_iterations: int = 10, global_iterations: int = 5, cross_layer_damping: Optional[float] = None, verbose: bool = True) -> Dict[str, Any]\n```\n\nCompute PageRank with cross-layer propagation.\n\n#### ComputeMixin.compute_tfidf\n\n```python\nComputeMixin.compute_tfidf(self, verbose: bool = True) -> None\n```\n\nCompute document relevance scores using the configured algorithm.\n\n#### ComputeMixin.compute_bm25\n\n```python\nComputeMixin.compute_bm25(self, k1: float = None, b: float = None, verbose: bool = True) -> None\n```\n\nCompute BM25 scores for document relevance ranking.\n\n#### ComputeMixin.compute_document_connections\n\n```python\nComputeMixin.compute_document_connections(self, min_shared_terms: int = 3, verbose: bool = True) -> None\n```\n\nNone\n\n#### ComputeMixin.compute_bigram_connections\n\n```python\nComputeMixin.compute_bigram_connections(self, min_shared_docs: int = 1, component_weight: float = 0.5, chain_weight: float = 0.7, cooccurrence_weight: float = 0.3, max_bigrams_per_term: int = 100, max_bigrams_per_doc: int = 500, max_connections_per_bigram: int = 50, verbose: bool = True) -> Dict[str, Any]\n```\n\nBuild lateral connections between bigrams based on shared components and co-occurrence.\n\n#### ComputeMixin.build_concept_clusters\n\n```python\nComputeMixin.build_concept_clusters(self, min_cluster_size: Optional[int] = None, clustering_method: str = 'louvain', cluster_strictness: Optional[float] = None, bridge_weight: float = 0.0, resolution: Optional[float] = None, verbose: bool = True) -> Dict[int, List[str]]\n```\n\nBuild concept clusters from token layer.\n\n#### ComputeMixin.compute_clustering_quality\n\n```python\nComputeMixin.compute_clustering_quality(self, sample_size: int = 500) -> Dict[str, Any]\n```\n\nCompute clustering quality metrics for the concept layer.\n\n#### ComputeMixin.compute_concept_connections\n\n```python\nComputeMixin.compute_concept_connections(self, use_semantics: bool = True, min_shared_docs: int = 1, min_jaccard: float = 0.1, use_member_semantics: bool = False, use_embedding_similarity: bool = False, embedding_threshold: float = 0.3, verbose: bool = True) -> Dict[str, Any]\n```\n\nBuild lateral connections between concepts based on document overlap and semantics.\n\n#### ComputeMixin.extract_corpus_semantics\n\n```python\nComputeMixin.extract_corpus_semantics(self, use_pattern_extraction: bool = True, min_pattern_confidence: float = 0.6, max_similarity_pairs: int = 100000, min_context_keys: int = 3, verbose: bool = True) -> int\n```\n\nExtract semantic relations from the corpus.\n\n#### ComputeMixin.extract_pattern_relations\n\n```python\nComputeMixin.extract_pattern_relations(self, min_confidence: float = 0.6, verbose: bool = True) -> List[Tuple[str, str, str, float]]\n```\n\nExtract semantic relations using pattern matching only.\n\n#### ComputeMixin.retrofit_connections\n\n```python\nComputeMixin.retrofit_connections(self, iterations: int = 10, alpha: float = 0.3, verbose: bool = True) -> Dict\n```\n\nNone\n\n#### ComputeMixin.compute_property_inheritance\n\n```python\nComputeMixin.compute_property_inheritance(self, decay_factor: float = 0.7, max_depth: int = 5, apply_to_connections: bool = True, boost_factor: float = 0.3, verbose: bool = True) -> Dict[str, Any]\n```\n\nCompute property inheritance based on IsA hierarchy.\n\n#### ComputeMixin.compute_property_similarity\n\n```python\nComputeMixin.compute_property_similarity(self, term1: str, term2: str) -> float\n```\n\nCompute similarity between terms based on shared properties.\n\n#### ComputeMixin.compute_graph_embeddings\n\n```python\nComputeMixin.compute_graph_embeddings(self, dimensions: int = 64, method: str = 'fast', max_terms: Optional[int] = None, verbose: bool = True) -> Dict\n```\n\nCompute graph embeddings for tokens.\n\n#### ComputeMixin.retrofit_embeddings\n\n```python\nComputeMixin.retrofit_embeddings(self, iterations: int = 10, alpha: float = 0.4, verbose: bool = True) -> Dict\n```\n\nNone\n\n#### ComputeMixin.embedding_similarity\n\n```python\nComputeMixin.embedding_similarity(self, term1: str, term2: str) -> float\n```\n\nNone\n\n#### ComputeMixin.find_similar_by_embedding\n\n```python\nComputeMixin.find_similar_by_embedding(self, term: str, top_n: int = 10) -> List[Tuple[str, float]]\n```\n\nNone\n\n### Dependencies\n\n**Standard Library:**\n\n- `datetime.datetime`\n- `json`\n- `layers.CorticalLayer`\n- `logging`\n- `observability.timed`\n- ... and 11 more\n\n**Local Imports:**\n\n- `.analysis`\n- `.embeddings`\n- `.semantics`\n\n\n\n## core.py\n\nCore processor functionality: initialization, staleness tracking, and layer management.\n\nThis module contains the base class definition and core infrastructure that all\nother processor mixins depend o...\n\n\n### Classes\n\n#### CoreMixin\n\nCore mixin providing initialization and staleness tracking.\n\n**Methods:**\n\n- `is_stale`\n- `get_stale_computations`\n- `get_layer`\n- `get_metrics`\n- `get_metrics_summary`\n- `reset_metrics`\n- `enable_metrics`\n- `disable_metrics`\n- `record_metric`\n\n### Functions\n\n#### CoreMixin.is_stale\n\n```python\nCoreMixin.is_stale(self, computation_type: str) -> bool\n```\n\nCheck if a specific computation is stale.\n\n#### CoreMixin.get_stale_computations\n\n```python\nCoreMixin.get_stale_computations(self) -> set\n```\n\nGet the set of computations that are currently stale.\n\n#### CoreMixin.get_layer\n\n```python\nCoreMixin.get_layer(self, layer: CorticalLayer) -> HierarchicalLayer\n```\n\nGet a specific layer by enum.\n\n#### CoreMixin.get_metrics\n\n```python\nCoreMixin.get_metrics(self) -> Dict[str, Dict[str, Any]]\n```\n\nGet all collected metrics.\n\n#### CoreMixin.get_metrics_summary\n\n```python\nCoreMixin.get_metrics_summary(self) -> str\n```\n\nGet a human-readable summary of all metrics.\n\n#### CoreMixin.reset_metrics\n\n```python\nCoreMixin.reset_metrics(self) -> None\n```\n\nClear all collected metrics.\n\n#### CoreMixin.enable_metrics\n\n```python\nCoreMixin.enable_metrics(self) -> None\n```\n\nEnable metrics collection.\n\n#### CoreMixin.disable_metrics\n\n```python\nCoreMixin.disable_metrics(self) -> None\n```\n\nDisable metrics collection.\n\n#### CoreMixin.record_metric\n\n```python\nCoreMixin.record_metric(self, metric_name: str, count: int = 1) -> None\n```\n\nRecord a custom count metric.\n\n### Dependencies\n\n**Standard Library:**\n\n- `config.CorticalConfig`\n- `layers.CorticalLayer`\n- `layers.HierarchicalLayer`\n- `logging`\n- `minicolumn.Minicolumn`\n- ... and 5 more\n\n\n\n## documents.py\n\nDocument management: processing, adding, removing, and metadata handling.\n\nThis module contains all methods related to managing documents in the corpus.\n\n\n### Classes\n\n#### DocumentsMixin\n\nMixin providing document management functionality.\n\n**Methods:**\n\n- `process_document`\n- `set_document_metadata`\n- `get_document_metadata`\n- `get_all_document_metadata`\n- `add_document_incremental`\n- `add_documents_batch`\n- `remove_document`\n- `remove_documents_batch`\n\n### Functions\n\n#### DocumentsMixin.process_document\n\n```python\nDocumentsMixin.process_document(self, doc_id: str, content: str, metadata: Optional[Dict[str, Any]] = None) -> Dict[str, int]\n```\n\nProcess a document and add it to the corpus.\n\n#### DocumentsMixin.set_document_metadata\n\n```python\nDocumentsMixin.set_document_metadata(self, doc_id: str, **kwargs) -> None\n```\n\nSet or update metadata for a document.\n\n#### DocumentsMixin.get_document_metadata\n\n```python\nDocumentsMixin.get_document_metadata(self, doc_id: str) -> Dict[str, Any]\n```\n\nGet metadata for a document.\n\n#### DocumentsMixin.get_all_document_metadata\n\n```python\nDocumentsMixin.get_all_document_metadata(self) -> Dict[str, Dict[str, Any]]\n```\n\nGet metadata for all documents.\n\n#### DocumentsMixin.add_document_incremental\n\n```python\nDocumentsMixin.add_document_incremental(self, doc_id: str, content: str, metadata: Optional[Dict[str, Any]] = None, recompute: str = 'tfidf') -> Dict[str, int]\n```\n\nAdd a document with selective recomputation for efficiency.\n\n#### DocumentsMixin.add_documents_batch\n\n```python\nDocumentsMixin.add_documents_batch(self, documents: List[Tuple[str, str, Optional[Dict[str, Any]]]], recompute: str = 'full', verbose: bool = True) -> Dict[str, Any]\n```\n\nAdd multiple documents with a single recomputation.\n\n#### DocumentsMixin.remove_document\n\n```python\nDocumentsMixin.remove_document(self, doc_id: str, verbose: bool = False) -> Dict[str, Any]\n```\n\nRemove a document from the corpus.\n\n#### DocumentsMixin.remove_documents_batch\n\n```python\nDocumentsMixin.remove_documents_batch(self, doc_ids: List[str], recompute: str = 'none', verbose: bool = True) -> Dict[str, Any]\n```\n\nRemove multiple documents efficiently with single recomputation.\n\n### Dependencies\n\n**Standard Library:**\n\n- `copy`\n- `layers.CorticalLayer`\n- `logging`\n- `observability.timed`\n- `typing.Any`\n- ... and 4 more\n\n\n\n## introspection.py\n\nIntrospection: state inspection, fingerprints, gaps, and summaries.\n\nThis module contains methods for examining the processor state and\ncomparing texts/documents.\n\n\n### Classes\n\n#### IntrospectionMixin\n\nMixin providing introspection functionality.\n\n**Methods:**\n\n- `get_document_signature`\n- `get_corpus_summary`\n- `analyze_knowledge_gaps`\n- `detect_anomalies`\n- `get_fingerprint`\n- `compare_fingerprints`\n- `explain_fingerprint`\n- `explain_similarity`\n- `find_similar_texts`\n- `compare_with`\n- `compare_documents`\n- `what_changed`\n- `summarize_document`\n- `detect_patterns`\n- `detect_patterns_in_corpus`\n- `get_pattern_summary`\n- `get_corpus_pattern_statistics`\n- `format_pattern_report`\n- `list_available_patterns`\n- `list_pattern_categories`\n\n### Functions\n\n#### IntrospectionMixin.get_document_signature\n\n```python\nIntrospectionMixin.get_document_signature(self, doc_id: str, n: int = 10) -> List[Tuple[str, float]]\n```\n\nGet the top-n TF-IDF terms for a document.\n\n#### IntrospectionMixin.get_corpus_summary\n\n```python\nIntrospectionMixin.get_corpus_summary(self) -> Dict\n```\n\nGet summary statistics about the corpus.\n\n#### IntrospectionMixin.analyze_knowledge_gaps\n\n```python\nIntrospectionMixin.analyze_knowledge_gaps(self) -> Dict\n```\n\nAnalyze the corpus for knowledge gaps.\n\n#### IntrospectionMixin.detect_anomalies\n\n```python\nIntrospectionMixin.detect_anomalies(self, threshold: float = 0.3) -> List[Dict]\n```\n\nDetect anomalous patterns in the corpus.\n\n#### IntrospectionMixin.get_fingerprint\n\n```python\nIntrospectionMixin.get_fingerprint(self, text: str, top_n: int = 20) -> Dict\n```\n\nCompute the semantic fingerprint of a text.\n\n#### IntrospectionMixin.compare_fingerprints\n\n```python\nIntrospectionMixin.compare_fingerprints(self, fp1: Dict, fp2: Dict) -> Dict\n```\n\nCompare two fingerprints and compute similarity metrics.\n\n#### IntrospectionMixin.explain_fingerprint\n\n```python\nIntrospectionMixin.explain_fingerprint(self, fp: Dict, top_n: int = 10) -> Dict\n```\n\nGenerate a human-readable explanation of a fingerprint.\n\n#### IntrospectionMixin.explain_similarity\n\n```python\nIntrospectionMixin.explain_similarity(self, fp1: Dict, fp2: Dict) -> str\n```\n\nGenerate a human-readable explanation of fingerprint similarity.\n\n#### IntrospectionMixin.find_similar_texts\n\n```python\nIntrospectionMixin.find_similar_texts(self, text: str, candidates: List[Tuple[str, str]], top_n: int = 5) -> List[Tuple[str, float, Dict]]\n```\n\nFind texts most similar to the given text.\n\n#### IntrospectionMixin.compare_with\n\n```python\nIntrospectionMixin.compare_with(self, other: 'CorticalTextProcessor', top_movers: int = 20, min_pagerank_delta: float = 0.0001) -> 'diff_module.SemanticDiff'\n```\n\nCompare this processor state with another to find semantic differences.\n\n#### IntrospectionMixin.compare_documents\n\n```python\nIntrospectionMixin.compare_documents(self, doc_id_1: str, doc_id_2: str) -> Dict\n```\n\nCompare two documents within this corpus.\n\n#### IntrospectionMixin.what_changed\n\n```python\nIntrospectionMixin.what_changed(self, old_content: str, new_content: str) -> Dict\n```\n\nCompare two text contents to show what changed semantically.\n\n#### IntrospectionMixin.summarize_document\n\n```python\nIntrospectionMixin.summarize_document(self, doc_id: str, num_sentences: int = 3) -> str\n```\n\nGenerate a summary of a document using extractive summarization.\n\n#### IntrospectionMixin.detect_patterns\n\n```python\nIntrospectionMixin.detect_patterns(self, doc_id: str, patterns: Optional[List[str]] = None) -> Dict[str, List[int]]\n```\n\nDetect programming patterns in a specific document.\n\n#### IntrospectionMixin.detect_patterns_in_corpus\n\n```python\nIntrospectionMixin.detect_patterns_in_corpus(self, patterns: Optional[List[str]] = None) -> Dict[str, Dict[str, List[int]]]\n```\n\nDetect patterns across all documents in the corpus.\n\n#### IntrospectionMixin.get_pattern_summary\n\n```python\nIntrospectionMixin.get_pattern_summary(self, doc_id: str) -> Dict[str, int]\n```\n\nGet a summary of pattern occurrences in a document.\n\n#### IntrospectionMixin.get_corpus_pattern_statistics\n\n```python\nIntrospectionMixin.get_corpus_pattern_statistics(self) -> Dict[str, Any]\n```\n\nGet pattern statistics across the entire corpus.\n\n#### IntrospectionMixin.format_pattern_report\n\n```python\nIntrospectionMixin.format_pattern_report(self, doc_id: str, show_lines: bool = False) -> str\n```\n\nFormat pattern detection results as a human-readable report.\n\n#### IntrospectionMixin.list_available_patterns\n\n```python\nIntrospectionMixin.list_available_patterns(self) -> List[str]\n```\n\nList all available pattern names that can be detected.\n\n#### IntrospectionMixin.list_pattern_categories\n\n```python\nIntrospectionMixin.list_pattern_categories(self) -> List[str]\n```\n\nList all pattern categories.\n\n### Dependencies\n\n**Standard Library:**\n\n- `layers.CorticalLayer`\n- `logging`\n- `re`\n- `typing.Any`\n- `typing.Dict`\n- ... and 4 more\n\n**Local Imports:**\n\n- `.fingerprint`\n- `.gaps`\n- `.patterns`\n- `.persistence`\n\n\n\n## persistence_api.py\n\nPersistence API: save, load, export, and migration methods.\n\nThis module contains all methods related to saving and loading processor state.\n\n\n### Classes\n\n#### PersistenceMixin\n\nMixin providing persistence functionality.\n\n**Methods:**\n\n- `save`\n- `load`\n- `save_json`\n- `load_json`\n- `migrate_to_json`\n- `export_graph`\n- `export_conceptnet_json`\n\n### Functions\n\n#### PersistenceMixin.save\n\n```python\nPersistenceMixin.save(self, filepath: str, verbose: bool = True, signing_key: Optional[bytes] = None) -> None\n```\n\nSave processor state to a file.\n\n#### PersistenceMixin.load\n\n```python\nPersistenceMixin.load(cls, filepath: str, verbose: bool = True, verify_key: Optional[bytes] = None) -> 'CorticalTextProcessor'\n```\n\nLoad processor state from a file.\n\n#### PersistenceMixin.save_json\n\n```python\nPersistenceMixin.save_json(self, state_dir: str, force: bool = False, verbose: bool = True) -> Dict[str, bool]\n```\n\nSave processor state to git-friendly JSON format.\n\n#### PersistenceMixin.load_json\n\n```python\nPersistenceMixin.load_json(cls, state_dir: str, config: Optional[CorticalConfig] = None, verbose: bool = True) -> 'CorticalTextProcessor'\n```\n\nLoad processor from git-friendly JSON format.\n\n#### PersistenceMixin.migrate_to_json\n\n```python\nPersistenceMixin.migrate_to_json(self, pkl_path: str, json_dir: str, verbose: bool = True) -> bool\n```\n\nMigrate existing pickle file to git-friendly JSON format.\n\n#### PersistenceMixin.export_graph\n\n```python\nPersistenceMixin.export_graph(self, filepath: str, layer: Optional[CorticalLayer] = None, max_nodes: int = 500) -> Dict\n```\n\nExport graph to JSON for visualization.\n\n#### PersistenceMixin.export_conceptnet_json\n\n```python\nPersistenceMixin.export_conceptnet_json(self, filepath: str, include_cross_layer: bool = True, include_typed_edges: bool = True, min_weight: float = 0.0, min_confidence: float = 0.0, max_nodes_per_layer: int = 100, verbose: bool = True) -> Dict[str, Any]\n```\n\nExport ConceptNet-style graph for visualization.\n\n### Dependencies\n\n**Standard Library:**\n\n- `config.CorticalConfig`\n- `layers.CorticalLayer`\n- `logging`\n- `observability.timed`\n- `typing.Any`\n- ... and 3 more\n\n**Local Imports:**\n\n- `.persistence`\n- `.state_storage`\n\n\n\n---\n\n*This chapter is part of [The Cortical Chronicles](../README.md), a self-documenting book generated by the Cortical Text Processor.*\n"
    },
    {
      "path": "02-architecture/mod-query.md",
      "title": "Search & Retrieval",
      "section": "architecture",
      "tags": [
        "architecture",
        "modules",
        "query"
      ],
      "source_files": [
        "/home/user/Opus-code-test/cortical/query/ranking.py",
        "/home/user/Opus-code-test/cortical/query/intent.py",
        "/home/user/Opus-code-test/cortical/query/passages.py",
        "/home/user/Opus-code-test/cortical/query/__init__.py",
        "/home/user/Opus-code-test/cortical/query/search.py",
        "/home/user/Opus-code-test/cortical/query/definitions.py",
        "/home/user/Opus-code-test/cortical/query/chunking.py",
        "/home/user/Opus-code-test/cortical/query/expansion.py"
      ],
      "excerpt": "Search and retrieval components for finding relevant documents and passages. Query Module ============ Query expansion and search functionality. This package provides methods for expanding queries...",
      "keywords": [
        "str",
        "float",
        "dict",
        "int",
        "list",
        "python",
        "bool",
        "tuple",
        "tokenizer",
        "query"
      ],
      "full_content": "# Search & Retrieval\n\nSearch and retrieval components for finding relevant documents and passages.\n\n## Modules\n\n- **__init__.py**: Query Module\n- **chunking.py**: Chunking Module\n- **definitions.py**: Definition Search Module\n- **expansion.py**: Query Expansion Module\n- **intent.py**: Intent Query Module\n- **passages.py**: Passage Retrieval Module\n- **ranking.py**: Ranking Module\n- **search.py**: Document Search Module\n\n\n## __init__.py\n\nQuery Module\n============\n\nQuery expansion and search functionality.\n\nThis package provides methods for expanding queries using lateral connections,\nconcept clusters, and word variants, then searching...\n\n\n### Dependencies\n\n**Standard Library:**\n\n- `analogy.complete_analogy`\n- `analogy.complete_analogy_simple`\n- `analogy.find_relation_between`\n- `analogy.find_terms_with_relation`\n- `chunking.CODE_BOUNDARY_PATTERN`\n- ... and 49 more\n\n\n\n## chunking.py\n\nChunking Module\n==============\n\nFunctions for splitting documents into chunks for passage retrieval.\n\nThis module provides:\n- Fixed-size text chunking with overlap\n- Code-aware chunking aligned to sem...\n\n\n### Functions\n\n#### create_chunks\n\n```python\ncreate_chunks(text: str, chunk_size: int = 512, overlap: int = 128) -> List[Tuple[str, int, int]]\n```\n\nSplit text into overlapping chunks.\n\n#### find_code_boundaries\n\n```python\nfind_code_boundaries(text: str) -> List[int]\n```\n\nFind semantic boundaries in code (class/function definitions, decorators).\n\n#### create_code_aware_chunks\n\n```python\ncreate_code_aware_chunks(text: str, target_size: int = 512, min_size: int = 100, max_size: int = 1024) -> List[Tuple[str, int, int]]\n```\n\nCreate chunks aligned to code structure boundaries.\n\n#### is_code_file\n\n```python\nis_code_file(doc_id: str) -> bool\n```\n\nDetermine if a document is a code file based on its path/extension.\n\n#### precompute_term_cols\n\n```python\nprecompute_term_cols(query_terms: Dict[str, float], layer0: HierarchicalLayer) -> Dict[str, 'Minicolumn']\n```\n\nPre-compute minicolumn lookups for query terms.\n\n#### score_chunk_fast\n\n```python\nscore_chunk_fast(chunk_tokens: List[str], query_terms: Dict[str, float], term_cols: Dict[str, 'Minicolumn'], doc_id: Optional[str] = None) -> float\n```\n\nFast chunk scoring using pre-computed minicolumn lookups.\n\n#### score_chunk\n\n```python\nscore_chunk(chunk_text: str, query_terms: Dict[str, float], layer0: HierarchicalLayer, tokenizer: Tokenizer, doc_id: Optional[str] = None) -> float\n```\n\nScore a chunk against query terms using TF-IDF.\n\n### Dependencies\n\n**Standard Library:**\n\n- `layers.HierarchicalLayer`\n- `re`\n- `tokenizer.Tokenizer`\n- `typing.Dict`\n- `typing.List`\n- ... and 3 more\n\n\n\n## definitions.py\n\nDefinition Search Module\n========================\n\nFunctions for finding and boosting code definitions (classes, functions, methods).\n\nThis module handles:\n- Detection of definition-seeking queries (\"...\n\n\n### Classes\n\n#### DefinitionQuery\n\nInfo about a definition-seeking query.\n\n### Functions\n\n#### is_definition_query\n\n```python\nis_definition_query(query_text: str) -> Tuple[bool, Optional[str], Optional[str]]\n```\n\nDetect if a query is looking for a code definition.\n\n#### find_definition_in_text\n\n```python\nfind_definition_in_text(text: str, identifier: str, def_type: str, context_chars: int = 500) -> Optional[Tuple[str, int, int]]\n```\n\nFind a definition in source text and extract surrounding context.\n\n#### find_definition_passages\n\n```python\nfind_definition_passages(query_text: str, documents: Dict[str, str], context_chars: int = 500, boost: float = DEFINITION_BOOST) -> List[Tuple[str, str, int, int, float]]\n```\n\nFind definition passages for a definition query.\n\n#### detect_definition_query\n\n```python\ndetect_definition_query(query_text: str) -> DefinitionQuery\n```\n\nDetect if a query is searching for a code definition.\n\n#### apply_definition_boost\n\n```python\napply_definition_boost(passages: List[Tuple[str, str, int, int, float]], query_text: str, boost_factor: float = 3.0) -> List[Tuple[str, str, int, int, float]]\n```\n\nBoost passages that contain actual code definitions matching the query.\n\n#### is_test_file\n\n```python\nis_test_file(doc_id: str) -> bool\n```\n\nDetect if a document ID represents a test file.\n\n#### boost_definition_documents\n\n```python\nboost_definition_documents(doc_results: List[Tuple[str, float]], query_text: str, documents: Dict[str, str], boost_factor: float = 2.0, test_with_definition_penalty: float = 0.5, test_without_definition_penalty: float = 0.7) -> List[Tuple[str, float]]\n```\n\nBoost documents that contain the actual definition being searched for.\n\n### Dependencies\n\n**Standard Library:**\n\n- `re`\n- `typing.Any`\n- `typing.Dict`\n- `typing.List`\n- `typing.Optional`\n- ... and 2 more\n\n\n\n## expansion.py\n\nQuery Expansion Module\n=====================\n\nFunctions for expanding query terms using lateral connections,\nsemantic relations, and code concept synonyms.\n\nThis module provides:\n- Basic query expansi...\n\n\n### Functions\n\n#### score_relation_path\n\n```python\nscore_relation_path(path: List[str]) -> float\n```\n\nScore a relation path by its semantic coherence.\n\n#### expand_query\n\n```python\nexpand_query(query_text: str, layers: Dict[CorticalLayer, HierarchicalLayer], tokenizer: Tokenizer, max_expansions: int = 10, use_lateral: bool = True, use_concepts: bool = True, use_variants: bool = True, use_code_concepts: bool = False, filter_code_stop_words: bool = False, tfidf_weight: float = 0.7, max_expansion_weight: float = 2.0) -> Dict[str, float]\n```\n\nExpand a query using lateral connections and concept clusters.\n\n#### expand_query_semantic\n\n```python\nexpand_query_semantic(query_text: str, layers: Dict[CorticalLayer, HierarchicalLayer], tokenizer: Tokenizer, semantic_relations: List[Tuple[str, str, str, float]], max_expansions: int = 10) -> Dict[str, float]\n```\n\nExpand query using semantic relations extracted from corpus.\n\n#### expand_query_multihop\n\n```python\nexpand_query_multihop(query_text: str, layers: Dict[CorticalLayer, HierarchicalLayer], tokenizer: Tokenizer, semantic_relations: List[Tuple[str, str, str, float]], max_hops: int = 2, max_expansions: int = 15, decay_factor: float = 0.5, min_path_score: float = 0.2) -> Dict[str, float]\n```\n\nExpand query using multi-hop semantic inference.\n\n#### get_expanded_query_terms\n\n```python\nget_expanded_query_terms(query_text: str, layers: Dict[CorticalLayer, HierarchicalLayer], tokenizer: Tokenizer, use_expansion: bool = True, semantic_relations: Optional[List[Tuple[str, str, str, float]]] = None, use_semantic: bool = True, max_expansions: int = 5, semantic_discount: float = 0.8, filter_code_stop_words: bool = False) -> Dict[str, float]\n```\n\nGet expanded query terms with optional semantic expansion.\n\n### Dependencies\n\n**Standard Library:**\n\n- `code_concepts.expand_code_concepts`\n- `collections.defaultdict`\n- `config.DEFAULT_CHAIN_VALIDITY`\n- `layers.CorticalLayer`\n- `layers.HierarchicalLayer`\n- ... and 6 more\n\n\n\n## intent.py\n\nIntent Query Module\n==================\n\nIntent-based query understanding for natural language code search.\n\nThis module handles:\n- Parsing natural language queries to extract intent (where, how, what,...\n\n\n### Classes\n\n#### ParsedIntent\n\nStructured representation of a parsed query intent.\n\n### Functions\n\n#### parse_intent_query\n\n```python\nparse_intent_query(query_text: str) -> ParsedIntent\n```\n\nParse a natural language query to extract intent and searchable terms.\n\n#### search_by_intent\n\n```python\nsearch_by_intent(query_text: str, layers: Dict[CorticalLayer, HierarchicalLayer], tokenizer: 'Tokenizer', top_n: int = 5) -> List[Tuple[str, float, ParsedIntent]]\n```\n\nSearch the corpus using intent-based query understanding.\n\n### Dependencies\n\n**Standard Library:**\n\n- `code_concepts.get_related_terms`\n- `collections.defaultdict`\n- `layers.CorticalLayer`\n- `layers.HierarchicalLayer`\n- `typing.Dict`\n- ... and 4 more\n\n\n\n## passages.py\n\nPassage Retrieval Module\n========================\n\nFunctions for retrieving relevant passages from documents.\n\nThis module provides:\n- Passage retrieval for RAG systems\n- Batch passage retrieval\n- Int...\n\n\n### Functions\n\n#### find_passages_for_query\n\n```python\nfind_passages_for_query(query_text: str, layers: Dict[CorticalLayer, HierarchicalLayer], tokenizer: Tokenizer, documents: Dict[str, str], top_n: int = 5, chunk_size: int = 512, overlap: int = 128, use_expansion: bool = True, doc_filter: Optional[List[str]] = None, semantic_relations: Optional[List[Tuple[str, str, str, float]]] = None, use_semantic: bool = True, use_definition_search: bool = True, definition_boost: float = DEFINITION_BOOST, apply_doc_boost: bool = True, doc_metadata: Optional[Dict[str, Dict[str, Any]]] = None, auto_detect_intent: bool = True, prefer_docs: bool = False, custom_boosts: Optional[Dict[str, float]] = None, use_code_aware_chunks: bool = True, filter_code_stop_words: bool = True, test_file_penalty: float = 0.8) -> List[Tuple[str, str, int, int, float]]\n```\n\nFind text passages most relevant to a query.\n\n#### find_documents_batch\n\n```python\nfind_documents_batch(queries: List[str], layers: Dict[CorticalLayer, HierarchicalLayer], tokenizer: Tokenizer, top_n: int = 5, use_expansion: bool = True, semantic_relations: Optional[List[Tuple[str, str, str, float]]] = None, use_semantic: bool = True) -> List[List[Tuple[str, float]]]\n```\n\nFind documents for multiple queries efficiently.\n\n#### find_passages_batch\n\n```python\nfind_passages_batch(queries: List[str], layers: Dict[CorticalLayer, HierarchicalLayer], tokenizer: Tokenizer, documents: Dict[str, str], top_n: int = 5, chunk_size: int = 512, overlap: int = 128, use_expansion: bool = True, doc_filter: Optional[List[str]] = None, semantic_relations: Optional[List[Tuple[str, str, str, float]]] = None, use_semantic: bool = True) -> List[List[Tuple[str, str, int, int, float]]]\n```\n\nFind passages for multiple queries efficiently.\n\n### Dependencies\n\n**Standard Library:**\n\n- `chunking.CODE_BOUNDARY_PATTERN`\n- `chunking.create_chunks`\n- `chunking.create_code_aware_chunks`\n- `chunking.find_code_boundaries`\n- `chunking.is_code_file`\n- ... and 18 more\n\n\n\n## ranking.py\n\nRanking Module\n=============\n\nMulti-stage ranking and document type boosting for search results.\n\nThis module provides:\n- Document type boosting (docs, code, tests)\n- Conceptual vs implementation quer...\n\n\n### Functions\n\n#### is_conceptual_query\n\n```python\nis_conceptual_query(query_text: str) -> bool\n```\n\nDetermine if a query is conceptual (should boost documentation).\n\n#### get_doc_type_boost\n\n```python\nget_doc_type_boost(doc_id: str, doc_metadata: Optional[Dict[str, Dict[str, Any]]] = None, custom_boosts: Optional[Dict[str, float]] = None) -> float\n```\n\nGet the boost factor for a document based on its type.\n\n#### apply_doc_type_boost\n\n```python\napply_doc_type_boost(results: List[Tuple[str, float]], doc_metadata: Optional[Dict[str, Dict[str, Any]]] = None, boost_docs: bool = True, custom_boosts: Optional[Dict[str, float]] = None) -> List[Tuple[str, float]]\n```\n\nApply document type boosting to search results.\n\n#### find_documents_with_boost\n\n```python\nfind_documents_with_boost(query_text: str, layers: Dict[CorticalLayer, HierarchicalLayer], tokenizer: Tokenizer, top_n: int = 5, doc_metadata: Optional[Dict[str, Dict[str, Any]]] = None, auto_detect_intent: bool = True, prefer_docs: bool = False, custom_boosts: Optional[Dict[str, float]] = None, use_expansion: bool = True, semantic_relations: Optional[List[Tuple[str, str, str, float]]] = None, use_semantic: bool = True) -> List[Tuple[str, float]]\n```\n\nFind documents with optional document-type boosting.\n\n#### find_relevant_concepts\n\n```python\nfind_relevant_concepts(query_terms: Dict[str, float], layers: Dict[CorticalLayer, HierarchicalLayer], top_n: int = 5) -> List[Tuple[str, float, set]]\n```\n\nStage 1: Find concepts relevant to query terms.\n\n#### multi_stage_rank\n\n```python\nmulti_stage_rank(query_text: str, layers: Dict[CorticalLayer, HierarchicalLayer], tokenizer: Tokenizer, documents: Dict[str, str], top_n: int = 5, chunk_size: int = 512, overlap: int = 128, concept_boost: float = 0.3, use_expansion: bool = True, semantic_relations: Optional[List[Tuple[str, str, str, float]]] = None, use_semantic: bool = True) -> List[Tuple[str, str, int, int, float, Dict[str, float]]]\n```\n\nMulti-stage ranking pipeline for improved RAG performance.\n\n#### multi_stage_rank_documents\n\n```python\nmulti_stage_rank_documents(query_text: str, layers: Dict[CorticalLayer, HierarchicalLayer], tokenizer: Tokenizer, top_n: int = 5, concept_boost: float = 0.3, use_expansion: bool = True, semantic_relations: Optional[List[Tuple[str, str, str, float]]] = None, use_semantic: bool = True) -> List[Tuple[str, float, Dict[str, float]]]\n```\n\nMulti-stage ranking for documents (without chunk scoring).\n\n### Dependencies\n\n**Standard Library:**\n\n- `collections.defaultdict`\n- `constants.CONCEPTUAL_KEYWORDS`\n- `constants.DOC_TYPE_BOOSTS`\n- `constants.IMPLEMENTATION_KEYWORDS`\n- `expansion.get_expanded_query_terms`\n- ... and 9 more\n\n\n\n## search.py\n\nDocument Search Module\n=====================\n\nFunctions for searching and retrieving documents from the corpus.\n\nThis module provides:\n- Basic document search using TF-IDF scoring\n- Fast document sear...\n\n\n### Functions\n\n#### find_documents_for_query\n\n```python\nfind_documents_for_query(query_text: str, layers: Dict[CorticalLayer, HierarchicalLayer], tokenizer: Tokenizer, top_n: int = 5, use_expansion: bool = True, semantic_relations: Optional[List[Tuple[str, str, str, float]]] = None, use_semantic: bool = True, doc_name_boost: float = 2.0, filter_code_stop_words: bool = True, test_file_penalty: float = 0.8) -> List[Tuple[str, float]]\n```\n\nFind documents most relevant to a query using TF-IDF and optional expansion.\n\n#### fast_find_documents\n\n```python\nfast_find_documents(query_text: str, layers: Dict[CorticalLayer, HierarchicalLayer], tokenizer: Tokenizer, top_n: int = 5, candidate_multiplier: int = 3, use_code_concepts: bool = True, doc_name_boost: float = 2.0) -> List[Tuple[str, float]]\n```\n\nFast document search using candidate filtering.\n\n#### build_document_index\n\n```python\nbuild_document_index(layers: Dict[CorticalLayer, HierarchicalLayer]) -> Dict[str, Dict[str, float]]\n```\n\nBuild an optimized inverted index for fast querying.\n\n#### search_with_index\n\n```python\nsearch_with_index(query_text: str, index: Dict[str, Dict[str, float]], tokenizer: Tokenizer, top_n: int = 5) -> List[Tuple[str, float]]\n```\n\nSearch using a pre-built inverted index.\n\n#### query_with_spreading_activation\n\n```python\nquery_with_spreading_activation(query_text: str, layers: Dict[CorticalLayer, HierarchicalLayer], tokenizer: Tokenizer, top_n: int = 10, max_expansions: int = 8) -> List[Tuple[str, float]]\n```\n\nQuery with automatic expansion using spreading activation.\n\n#### find_related_documents\n\n```python\nfind_related_documents(doc_id: str, layers: Dict[CorticalLayer, HierarchicalLayer]) -> List[Tuple[str, float]]\n```\n\nFind documents related to a given document via lateral connections.\n\n#### graph_boosted_search\n\n```python\ngraph_boosted_search(query_text: str, layers: Dict[CorticalLayer, HierarchicalLayer], tokenizer: Tokenizer, top_n: int = 5, pagerank_weight: float = 0.3, proximity_weight: float = 0.2, use_expansion: bool = True, semantic_relations: Optional[List[Tuple[str, str, str, float]]] = None) -> List[Tuple[str, float]]\n```\n\nGraph-Boosted BM25 (GB-BM25): Hybrid scoring combining BM25 with graph signals.\n\n### Dependencies\n\n**Standard Library:**\n\n- `code_concepts.get_related_terms`\n- `collections.defaultdict`\n- `expansion.expand_query`\n- `expansion.get_expanded_query_terms`\n- `layers.CorticalLayer`\n- ... and 6 more\n\n\n\n---\n\n*This chapter is part of [The Cortical Chronicles](../README.md), a self-documenting book generated by the Cortical Text Processor.*\n"
    },
    {
      "path": "02-architecture/mod-utilities.md",
      "title": "Utilities",
      "section": "architecture",
      "tags": [
        "architecture",
        "modules",
        "utilities"
      ],
      "source_files": [
        "/home/user/Opus-code-test/cortical/patterns.py",
        "/home/user/Opus-code-test/cortical/code_concepts.py",
        "/home/user/Opus-code-test/cortical/diff.py",
        "/home/user/Opus-code-test/cortical/gaps.py",
        "/home/user/Opus-code-test/cortical/fingerprint.py",
        "/home/user/Opus-code-test/cortical/mcp_server.py",
        "/home/user/Opus-code-test/cortical/fluent.py",
        "/home/user/Opus-code-test/cortical/cli_wrapper.py"
      ],
      "excerpt": "Utility modules supporting various features. CLI wrapper framework for collecting context and triggering actions. Design philosophy: QUIET BY DEFAULT, POWERFUL WHEN NEEDED. Most of the time you just...",
      "keywords": [
        "str",
        "python",
        "self",
        "list",
        "fluentprocessor",
        "dict",
        "none",
        "optional",
        "session",
        "int"
      ],
      "full_content": "# Utilities\n\nUtility modules supporting various features.\n\n## Modules\n\n- **cli_wrapper.py**: CLI wrapper framework for collecting context and triggering actions.\n- **code_concepts.py**: Code Concepts Module\n- **diff.py**: Semantic Diff Module\n- **fingerprint.py**: Fingerprint Module\n- **fluent.py**: Fluent API for CorticalTextProcessor - chainable method interface.\n- **gaps.py**: Gaps Module\n- **mcp_server.py**: MCP (Model Context Protocol) Server for Cortical Text Processor.\n- **patterns.py**: Code Pattern Detection Module\n\n\n## cli_wrapper.py\n\nCLI wrapper framework for collecting context and triggering actions.\n\nDesign philosophy: QUIET BY DEFAULT, POWERFUL WHEN NEEDED.\n\nMost of the time you just want to run a command and check if it worked...\n\n\n### Classes\n\n#### GitContext\n\nGit repository context information.\n\n**Methods:**\n\n- `collect`\n- `to_dict`\n\n#### ExecutionContext\n\nComplete context for a CLI command execution.\n\n**Methods:**\n\n- `to_dict`\n- `to_json`\n- `summary`\n\n#### HookType\n\nTypes of hooks that can be registered.\n\n#### HookRegistry\n\nRegistry for CLI execution hooks.\n\n**Methods:**\n\n- `register`\n- `register_pre`\n- `register_post`\n- `register_success`\n- `register_error`\n- `get_hooks`\n- `trigger`\n\n#### CLIWrapper\n\nWrapper for CLI command execution with context collection and hooks.\n\n**Methods:**\n\n- `run`\n- `on_success`\n- `on_error`\n- `on_complete`\n\n#### TaskCompletionManager\n\nManager for task completion triggers and context window management.\n\n**Methods:**\n\n- `on_task_complete`\n- `on_any_complete`\n- `handle_completion`\n- `get_session_summary`\n- `should_trigger_reindex`\n\n#### ContextWindowManager\n\nManages context window state based on CLI execution history.\n\n**Methods:**\n\n- `add_execution`\n- `add_file_read`\n- `get_recent_files`\n- `get_context_summary`\n- `suggest_pruning`\n\n#### Session\n\nTrack a sequence of commands as a session.\n\n**Methods:**\n\n- `run`\n- `should_reindex`\n- `summary`\n- `results`\n- `success_rate`\n- `all_passed`\n- `modified_files`\n\n#### TaskCheckpoint\n\nSave/restore context state when switching between tasks.\n\n**Methods:**\n\n- `save`\n- `load`\n- `list_tasks`\n- `delete`\n- `summarize`\n\n### Functions\n\n#### create_wrapper_with_completion_manager\n\n```python\ncreate_wrapper_with_completion_manager() -> Tuple[CLIWrapper, TaskCompletionManager]\n```\n\nCreate a CLIWrapper with an attached TaskCompletionManager.\n\n#### run_with_context\n\n```python\nrun_with_context(command: Union[str, List[str]], **kwargs) -> ExecutionContext\n```\n\nConvenience function to run a command with full context collection.\n\n#### run\n\n```python\nrun(command: Union[str, List[str]], git: bool = False, timeout: Optional[float] = None, cwd: Optional[str] = None) -> ExecutionContext\n```\n\nRun a command. That's it.\n\n#### test_then_commit\n\n```python\ntest_then_commit(test_cmd: Union[str, List[str]] = 'python -m unittest discover -s tests', message: str = 'Update', add_all: bool = True) -> Tuple[bool, List[ExecutionContext]]\n```\n\nRun tests, commit only if they pass.\n\n#### commit_and_push\n\n```python\ncommit_and_push(message: str, add_all: bool = True, branch: Optional[str] = None) -> Tuple[bool, List[ExecutionContext]]\n```\n\nAdd, commit, and push in one go.\n\n#### sync_with_main\n\n```python\nsync_with_main(main_branch: str = 'main') -> Tuple[bool, List[ExecutionContext]]\n```\n\nFetch and rebase current branch on main.\n\n#### GitContext.collect\n\n```python\nGitContext.collect(cls, cwd: Optional[str] = None) -> 'GitContext'\n```\n\nCollect git context from current directory.\n\n#### GitContext.to_dict\n\n```python\nGitContext.to_dict(self) -> Dict[str, Any]\n```\n\nConvert to dictionary.\n\n#### ExecutionContext.to_dict\n\n```python\nExecutionContext.to_dict(self) -> Dict[str, Any]\n```\n\nConvert to dictionary for serialization.\n\n#### ExecutionContext.to_json\n\n```python\nExecutionContext.to_json(self, indent: int = 2) -> str\n```\n\nConvert to JSON string.\n\n#### ExecutionContext.summary\n\n```python\nExecutionContext.summary(self) -> str\n```\n\nReturn a concise summary string.\n\n#### HookRegistry.register\n\n```python\nHookRegistry.register(self, hook_type: HookType, callback: HookCallback, pattern: Optional[str] = None) -> None\n```\n\nRegister a hook callback.\n\n#### HookRegistry.register_pre\n\n```python\nHookRegistry.register_pre(self, pattern: Optional[str], callback: HookCallback) -> None\n```\n\nConvenience method for pre-execution hooks.\n\n#### HookRegistry.register_post\n\n```python\nHookRegistry.register_post(self, pattern: Optional[str], callback: HookCallback) -> None\n```\n\nConvenience method for post-execution hooks.\n\n#### HookRegistry.register_success\n\n```python\nHookRegistry.register_success(self, pattern: Optional[str], callback: HookCallback) -> None\n```\n\nConvenience method for success hooks.\n\n#### HookRegistry.register_error\n\n```python\nHookRegistry.register_error(self, pattern: Optional[str], callback: HookCallback) -> None\n```\n\nConvenience method for error hooks.\n\n#### HookRegistry.get_hooks\n\n```python\nHookRegistry.get_hooks(self, hook_type: HookType, command: List[str]) -> List[HookCallback]\n```\n\nGet all hooks that should be triggered for a command.\n\n#### HookRegistry.trigger\n\n```python\nHookRegistry.trigger(self, hook_type: HookType, context: ExecutionContext) -> None\n```\n\nTrigger all matching hooks.\n\n#### CLIWrapper.run\n\n```python\nCLIWrapper.run(self, command: Union[str, List[str]], cwd: Optional[str] = None, timeout: Optional[float] = None, env: Optional[Dict[str, str]] = None, **kwargs) -> ExecutionContext\n```\n\nExecute a command with context collection and hooks.\n\n#### CLIWrapper.on_success\n\n```python\nCLIWrapper.on_success(self, pattern: Optional[str] = None)\n```\n\nDecorator to register a success hook.\n\n#### CLIWrapper.on_error\n\n```python\nCLIWrapper.on_error(self, pattern: Optional[str] = None)\n```\n\nDecorator to register an error hook.\n\n#### CLIWrapper.on_complete\n\n```python\nCLIWrapper.on_complete(self, pattern: Optional[str] = None)\n```\n\nDecorator to register a completion hook (success or failure).\n\n#### TaskCompletionManager.on_task_complete\n\n```python\nTaskCompletionManager.on_task_complete(self, task_type: str, callback: HookCallback) -> None\n```\n\nRegister a callback for when a specific task type completes.\n\n#### TaskCompletionManager.on_any_complete\n\n```python\nTaskCompletionManager.on_any_complete(self, callback: HookCallback) -> None\n```\n\nRegister a callback for any task completion.\n\n#### TaskCompletionManager.handle_completion\n\n```python\nTaskCompletionManager.handle_completion(self, context: ExecutionContext) -> None\n```\n\nHandle task completion and trigger appropriate callbacks.\n\n#### TaskCompletionManager.get_session_summary\n\n```python\nTaskCompletionManager.get_session_summary(self) -> Dict[str, Any]\n```\n\nGet summary of all tasks completed in this session.\n\n#### TaskCompletionManager.should_trigger_reindex\n\n```python\nTaskCompletionManager.should_trigger_reindex(self) -> bool\n```\n\nDetermine if corpus should be re-indexed based on session activity.\n\n#### ContextWindowManager.add_execution\n\n```python\nContextWindowManager.add_execution(self, context: ExecutionContext) -> None\n```\n\nAdd an execution to the context window.\n\n#### ContextWindowManager.add_file_read\n\n```python\nContextWindowManager.add_file_read(self, filepath: str) -> None\n```\n\nTrack that a file was read.\n\n#### ContextWindowManager.get_recent_files\n\n```python\nContextWindowManager.get_recent_files(self, limit: int = 10) -> List[str]\n```\n\nGet most recently accessed files.\n\n#### ContextWindowManager.get_context_summary\n\n```python\nContextWindowManager.get_context_summary(self) -> Dict[str, Any]\n```\n\nGet a summary of current context window state.\n\n#### ContextWindowManager.suggest_pruning\n\n```python\nContextWindowManager.suggest_pruning(self) -> List[str]\n```\n\nSuggest files that could be pruned from context.\n\n#### Session.run\n\n```python\nSession.run(self, command: Union[str, List[str]], **kwargs) -> ExecutionContext\n```\n\nRun a command within this session.\n\n#### Session.should_reindex\n\n```python\nSession.should_reindex(self) -> bool\n```\n\nCheck if corpus re-indexing is recommended based on session activity.\n\n#### Session.summary\n\n```python\nSession.summary(self) -> Dict[str, Any]\n```\n\nGet a summary of this session's activity.\n\n#### Session.results\n\n```python\nSession.results(self) -> List[ExecutionContext]\n```\n\nAll command results from this session.\n\n#### Session.success_rate\n\n```python\nSession.success_rate(self) -> float\n```\n\nFraction of commands that succeeded (0.0 to 1.0).\n\n#### Session.all_passed\n\n```python\nSession.all_passed(self) -> bool\n```\n\nTrue if all commands in this session succeeded.\n\n#### Session.modified_files\n\n```python\nSession.modified_files(self) -> List[str]\n```\n\nList of files modified during this session (from git context).\n\n#### TaskCheckpoint.save\n\n```python\nTaskCheckpoint.save(self, task_name: str, context: Dict[str, Any]) -> None\n```\n\nSave context for a task.\n\n#### TaskCheckpoint.load\n\n```python\nTaskCheckpoint.load(self, task_name: str) -> Optional[Dict[str, Any]]\n```\n\nLoad context for a task. Returns None if not found.\n\n#### TaskCheckpoint.list_tasks\n\n```python\nTaskCheckpoint.list_tasks(self) -> List[str]\n```\n\nList all saved task checkpoints.\n\n#### TaskCheckpoint.delete\n\n```python\nTaskCheckpoint.delete(self, task_name: str) -> bool\n```\n\nDelete a checkpoint. Returns True if deleted.\n\n#### TaskCheckpoint.summarize\n\n```python\nTaskCheckpoint.summarize(self, task_name: str) -> Optional[str]\n```\n\nGet a one-line summary of a task checkpoint.\n\n### Dependencies\n\n**Standard Library:**\n\n- `dataclasses.asdict`\n- `dataclasses.dataclass`\n- `dataclasses.field`\n- `datetime.datetime`\n- `enum.Enum`\n- ... and 15 more\n\n\n\n## code_concepts.py\n\nCode Concepts Module\n====================\n\nProgramming concept groups for semantic code search.\n\nMaps common programming synonyms and related terms to enable\nintent-based code retrieval. When a develo...\n\n\n### Functions\n\n#### get_related_terms\n\n```python\nget_related_terms(term: str, max_terms: int = 5) -> List[str]\n```\n\nGet programming terms related to the given term.\n\n#### expand_code_concepts\n\n```python\nexpand_code_concepts(terms: List[str], max_expansions_per_term: int = 3, weight: float = 0.6) -> Dict[str, float]\n```\n\nExpand a list of terms using code concept groups.\n\n#### get_concept_group\n\n```python\nget_concept_group(term: str) -> List[str]\n```\n\nGet the concept group names a term belongs to.\n\n#### list_concept_groups\n\n```python\nlist_concept_groups() -> List[str]\n```\n\nList all available concept group names.\n\n#### get_group_terms\n\n```python\nget_group_terms(group_name: str) -> List[str]\n```\n\nGet all terms in a concept group.\n\n### Dependencies\n\n**Standard Library:**\n\n- `typing.Dict`\n- `typing.FrozenSet`\n- `typing.List`\n- `typing.Set`\n\n\n\n## diff.py\n\nSemantic Diff Module\n====================\n\nProvides \"What Changed?\" functionality for comparing:\n- Two versions of a document\n- Two processor states\n- Before/after states of a corpus\n\nThis goes beyond...\n\n\n### Classes\n\n#### TermChange\n\nRepresents a change to a term/concept.\n\n**Methods:**\n\n- `pagerank_delta`\n- `tfidf_delta`\n- `documents_added`\n- `documents_removed`\n\n#### RelationChange\n\nRepresents a change to a semantic relation.\n\n#### ClusterChange\n\nRepresents a change to concept clustering.\n\n#### SemanticDiff\n\nComplete semantic diff between two states.\n\n**Methods:**\n\n- `summary`\n- `to_dict`\n\n### Functions\n\n#### compare_processors\n\n```python\ncompare_processors(old_processor: 'CorticalTextProcessor', new_processor: 'CorticalTextProcessor', top_movers: int = 20, min_pagerank_delta: float = 0.0001) -> SemanticDiff\n```\n\nCompare two processor states to find semantic differences.\n\n#### compare_documents\n\n```python\ncompare_documents(processor: 'CorticalTextProcessor', doc_id_old: str, doc_id_new: str) -> Dict[str, Any]\n```\n\nCompare two documents within the same corpus.\n\n#### what_changed\n\n```python\nwhat_changed(processor: 'CorticalTextProcessor', old_content: str, new_content: str, temp_doc_prefix: str = '_diff_temp_') -> Dict[str, Any]\n```\n\nCompare two text contents to show what changed semantically.\n\n#### TermChange.pagerank_delta\n\n```python\nTermChange.pagerank_delta(self) -> Optional[float]\n```\n\nChange in PageRank importance.\n\n#### TermChange.tfidf_delta\n\n```python\nTermChange.tfidf_delta(self) -> Optional[float]\n```\n\nChange in TF-IDF score.\n\n#### TermChange.documents_added\n\n```python\nTermChange.documents_added(self) -> Set[str]\n```\n\nDocuments where this term newly appears.\n\n#### TermChange.documents_removed\n\n```python\nTermChange.documents_removed(self) -> Set[str]\n```\n\nDocuments where this term no longer appears.\n\n#### SemanticDiff.summary\n\n```python\nSemanticDiff.summary(self) -> str\n```\n\nGenerate a human-readable summary of changes.\n\n#### SemanticDiff.to_dict\n\n```python\nSemanticDiff.to_dict(self) -> Dict[str, Any]\n```\n\nConvert to dictionary for serialization.\n\n### Dependencies\n\n**Standard Library:**\n\n- `collections.defaultdict`\n- `dataclasses.dataclass`\n- `dataclasses.field`\n- `layers.CorticalLayer`\n- `layers.HierarchicalLayer`\n- ... and 7 more\n\n\n\n## fingerprint.py\n\nFingerprint Module\n==================\n\nSemantic fingerprinting for code comparison and similarity analysis.\n\nA fingerprint is an interpretable representation of a text's semantic\ncontent, including te...\n\n\n### Classes\n\n#### SemanticFingerprint\n\nStructured representation of a text's semantic fingerprint.\n\n### Functions\n\n#### compute_fingerprint\n\n```python\ncompute_fingerprint(text: str, tokenizer: Tokenizer, layers: Optional[Dict[CorticalLayer, HierarchicalLayer]] = None, top_n: int = 20) -> SemanticFingerprint\n```\n\nCompute the semantic fingerprint of a text.\n\n#### compare_fingerprints\n\n```python\ncompare_fingerprints(fp1: SemanticFingerprint, fp2: SemanticFingerprint) -> Dict[str, Any]\n```\n\nCompare two fingerprints and compute similarity metrics.\n\n#### explain_fingerprint\n\n```python\nexplain_fingerprint(fp: SemanticFingerprint, top_n: int = 10) -> Dict[str, Any]\n```\n\nGenerate a human-readable explanation of a fingerprint.\n\n#### explain_similarity\n\n```python\nexplain_similarity(fp1: SemanticFingerprint, fp2: SemanticFingerprint, comparison: Optional[Dict[str, Any]] = None) -> str\n```\n\nGenerate a human-readable explanation of why two fingerprints are similar.\n\n### Dependencies\n\n**Standard Library:**\n\n- `code_concepts.get_concept_group`\n- `collections.defaultdict`\n- `layers.CorticalLayer`\n- `layers.HierarchicalLayer`\n- `math`\n- ... and 7 more\n\n\n\n## fluent.py\n\nFluent API for CorticalTextProcessor - chainable method interface.\n\nExample:\n    from cortical import FluentProcessor\n\n    # Simple usage\n    results = (FluentProcessor()\n        .add_document(\"doc1\",...\n\n\n### Classes\n\n#### FluentProcessor\n\nFluent/chainable API wrapper for CorticalTextProcessor.\n\n**Methods:**\n\n- `from_existing`\n- `from_files`\n- `from_directory`\n- `load`\n- `add_document`\n- `add_documents`\n- `with_config`\n- `with_tokenizer`\n- `build`\n- `save`\n- `search`\n- `fast_search`\n- `search_passages`\n- `expand`\n- `processor`\n- `is_built`\n\n### Functions\n\n#### FluentProcessor.from_existing\n\n```python\nFluentProcessor.from_existing(cls, processor: CorticalTextProcessor) -> 'FluentProcessor'\n```\n\nCreate a FluentProcessor from an existing CorticalTextProcessor.\n\n#### FluentProcessor.from_files\n\n```python\nFluentProcessor.from_files(cls, file_paths: List[Union[str, Path]], tokenizer: Optional[Tokenizer] = None, config: Optional[CorticalConfig] = None) -> 'FluentProcessor'\n```\n\nCreate a processor from a list of files.\n\n#### FluentProcessor.from_directory\n\n```python\nFluentProcessor.from_directory(cls, directory: Union[str, Path], pattern: str = '*.txt', recursive: bool = False, tokenizer: Optional[Tokenizer] = None, config: Optional[CorticalConfig] = None) -> 'FluentProcessor'\n```\n\nCreate a processor from all files in a directory.\n\n#### FluentProcessor.load\n\n```python\nFluentProcessor.load(cls, path: Union[str, Path]) -> 'FluentProcessor'\n```\n\nLoad a processor from a saved file.\n\n#### FluentProcessor.add_document\n\n```python\nFluentProcessor.add_document(self, doc_id: str, content: str, metadata: Optional[Dict[str, Any]] = None) -> 'FluentProcessor'\n```\n\nAdd a document to the processor (chainable).\n\n#### FluentProcessor.add_documents\n\n```python\nFluentProcessor.add_documents(self, documents: Union[Dict[str, str], List[Tuple[str, str]], List[Tuple[str, str, Dict]]]) -> 'FluentProcessor'\n```\n\nAdd multiple documents at once (chainable).\n\n#### FluentProcessor.with_config\n\n```python\nFluentProcessor.with_config(self, config: CorticalConfig) -> 'FluentProcessor'\n```\n\nSet configuration (chainable).\n\n#### FluentProcessor.with_tokenizer\n\n```python\nFluentProcessor.with_tokenizer(self, tokenizer: Tokenizer) -> 'FluentProcessor'\n```\n\nSet custom tokenizer (chainable).\n\n#### FluentProcessor.build\n\n```python\nFluentProcessor.build(self, verbose: bool = True, build_concepts: bool = True, pagerank_method: str = 'standard', connection_strategy: str = 'document_overlap', cluster_strictness: float = 1.0, bridge_weight: float = 0.0, show_progress: bool = False) -> 'FluentProcessor'\n```\n\nBuild the processor by computing all analysis phases (chainable).\n\n#### FluentProcessor.save\n\n```python\nFluentProcessor.save(self, path: Union[str, Path]) -> 'FluentProcessor'\n```\n\nSave the processor to disk (chainable).\n\n#### FluentProcessor.search\n\n```python\nFluentProcessor.search(self, query: str, top_n: int = 5, use_expansion: bool = True, use_semantic: bool = True) -> List[Tuple[str, float]]\n```\n\nSearch for documents matching the query.\n\n#### FluentProcessor.fast_search\n\n```python\nFluentProcessor.fast_search(self, query: str, top_n: int = 5, candidate_multiplier: int = 3, use_code_concepts: bool = True) -> List[Tuple[str, float]]\n```\n\nFast document search with pre-filtering.\n\n#### FluentProcessor.search_passages\n\n```python\nFluentProcessor.search_passages(self, query: str, top_n: int = 5, chunk_size: Optional[int] = None, overlap: Optional[int] = None, use_expansion: bool = True) -> List[Tuple[str, str, int, int, float]]\n```\n\nSearch for passage chunks matching the query.\n\n#### FluentProcessor.expand\n\n```python\nFluentProcessor.expand(self, query: str, max_expansions: Optional[int] = None, use_variants: bool = True, use_code_concepts: bool = False) -> Dict[str, float]\n```\n\nExpand a query with related terms.\n\n#### FluentProcessor.processor\n\n```python\nFluentProcessor.processor(self) -> CorticalTextProcessor\n```\n\nAccess the underlying CorticalTextProcessor instance.\n\n#### FluentProcessor.is_built\n\n```python\nFluentProcessor.is_built(self) -> bool\n```\n\nCheck if the processor has been built.\n\n### Dependencies\n\n**Standard Library:**\n\n- `config.CorticalConfig`\n- `os`\n- `pathlib.Path`\n- `processor.CorticalTextProcessor`\n- `tokenizer.Tokenizer`\n- ... and 6 more\n\n\n\n## gaps.py\n\nGaps Module\n===========\n\nKnowledge gap detection and anomaly analysis.\n\nIdentifies:\n- Isolated documents that don't connect well to the corpus\n- Weakly covered topics (few documents)\n- Bridge opportun...\n\n\n### Functions\n\n#### analyze_knowledge_gaps\n\n```python\nanalyze_knowledge_gaps(layers: Dict[CorticalLayer, HierarchicalLayer], documents: Dict[str, str]) -> Dict\n```\n\nAnalyze the corpus to identify potential knowledge gaps.\n\n#### detect_anomalies\n\n```python\ndetect_anomalies(layers: Dict[CorticalLayer, HierarchicalLayer], documents: Dict[str, str], threshold: float = 0.3) -> List[Dict]\n```\n\nDetect documents that don't fit well with the rest of the corpus.\n\n### Dependencies\n\n**Standard Library:**\n\n- `analysis.cosine_similarity`\n- `collections.defaultdict`\n- `layers.CorticalLayer`\n- `layers.HierarchicalLayer`\n- `math`\n- ... and 5 more\n\n\n\n## mcp_server.py\n\nMCP (Model Context Protocol) Server for Cortical Text Processor.\n\nProvides an MCP server interface for AI agents to integrate with the\nCortical Text Processor, enabling semantic search, query expansio...\n\n\n### Classes\n\n#### CorticalMCPServer\n\nMCP Server wrapper for CorticalTextProcessor.\n\n**Methods:**\n\n- `run`\n\n### Functions\n\n#### create_mcp_server\n\n```python\ncreate_mcp_server(corpus_path: Optional[str] = None, config: Optional[CorticalConfig] = None) -> CorticalMCPServer\n```\n\nCreate a Cortical MCP Server instance.\n\n#### main\n\n```python\nmain()\n```\n\nMain entry point for running the MCP server from command line.\n\n#### CorticalMCPServer.run\n\n```python\nCorticalMCPServer.run(self, transport: str = 'stdio')\n```\n\nRun the MCP server.\n\n### Dependencies\n\n**Standard Library:**\n\n- `config.CorticalConfig`\n- `logging`\n- `mcp.server.FastMCP`\n- `os`\n- `pathlib.Path`\n- ... and 5 more\n\n\n\n## patterns.py\n\nCode Pattern Detection Module\n==============================\n\nDetects common programming patterns in indexed code.\n\nIdentifies design patterns, idioms, and code structures including:\n- Singleton patte...\n\n\n### Functions\n\n#### detect_patterns_in_text\n\n```python\ndetect_patterns_in_text(text: str, patterns: Optional[List[str]] = None) -> Dict[str, List[int]]\n```\n\nDetect programming patterns in a text string.\n\n#### detect_patterns_in_documents\n\n```python\ndetect_patterns_in_documents(documents: Dict[str, str], patterns: Optional[List[str]] = None) -> Dict[str, Dict[str, List[int]]]\n```\n\nDetect patterns across multiple documents.\n\n#### get_pattern_summary\n\n```python\nget_pattern_summary(pattern_results: Dict[str, List[int]]) -> Dict[str, int]\n```\n\nSummarize pattern detection results by counting occurrences.\n\n#### get_patterns_by_category\n\n```python\nget_patterns_by_category(pattern_results: Dict[str, List[int]]) -> Dict[str, Dict[str, int]]\n```\n\nGroup pattern results by category.\n\n#### get_pattern_description\n\n```python\nget_pattern_description(pattern_name: str) -> Optional[str]\n```\n\nGet the description for a pattern.\n\n#### get_pattern_category\n\n```python\nget_pattern_category(pattern_name: str) -> Optional[str]\n```\n\nGet the category for a pattern.\n\n#### list_all_patterns\n\n```python\nlist_all_patterns() -> List[str]\n```\n\nList all available pattern names.\n\n#### list_patterns_by_category\n\n```python\nlist_patterns_by_category(category: str) -> List[str]\n```\n\nList all patterns in a specific category.\n\n#### list_all_categories\n\n```python\nlist_all_categories() -> List[str]\n```\n\nList all pattern categories.\n\n#### format_pattern_report\n\n```python\nformat_pattern_report(pattern_results: Dict[str, List[int]], show_lines: bool = False) -> str\n```\n\nFormat pattern detection results as a human-readable report.\n\n#### get_corpus_pattern_statistics\n\n```python\nget_corpus_pattern_statistics(doc_patterns: Dict[str, Dict[str, List[int]]]) -> Dict[str, any]\n```\n\nCompute statistics across all documents.\n\n### Dependencies\n\n**Standard Library:**\n\n- `collections.defaultdict`\n- `re`\n- `typing.Dict`\n- `typing.List`\n- `typing.Optional`\n- ... and 2 more\n\n\n\n---\n\n*This chapter is part of [The Cortical Chronicles](../README.md), a self-documenting book generated by the Cortical Text Processor.*\n"
    },
    {
      "path": "03-decisions/decision-adr-microseconds-task-id.md",
      "title": "ADR-001: Add Microseconds to Task ID Generation",
      "section": "decisions",
      "tags": [
        "decisions",
        "adr",
        "adr-microseconds-task-id"
      ],
      "source_files": [
        "samples/decisions/adr-microseconds-task-id.md"
      ],
      "excerpt": "",
      "keywords": [
        "low",
        "task",
        "add",
        "ids",
        "microseconds",
        "tasks",
        "second",
        "implement",
        "thresholds",
        "hex"
      ],
      "full_content": "# ADR-001: Add Microseconds to Task ID Generation\n\n**Status:** Accepted  \n**Date:** 2025-12-14  \n**Tags:** `task-management`, `uniqueness`, `concurrency`  \n\n---\n\n## The Question\n\nTask IDs were generated with second-precision timestamps plus a 4-character hex suffix:\n\n```\nT-YYYYMMDD-HHMMSS-XXXX\nExample: T-20251214-163052-a1b2\n```\n\nDuring CI testing, the `test_unique_task_ids` test was intermittently failing:\n\n```\nAssertionError: 99 != 100\n```\n\nWhen generating 100 task IDs in a tight loop (same second), collisions occurred because:\n- Same timestamp for all IDs in that second\n- Only 4 hex chars = 65,536 possible suffixes\n- Birthday paradox: P(collision) \u2248 7% for 100 items from 65,536\n\n## The Conversation\n\n*This decision emerged from 12 recorded discussion(s).*\n\n### Discussion 1\n\n**When:** 2025-12-16  \n**Matched Keywords:** task, add  \n\n**Query:**\n\n> Please deeply think about the best way to implement these tasks in batches dispatched inteligently to sub Agents and tracked/verifyed by enabling director mode with a backup plan that covers potential failure points intelligently.\n\nTasks:\nT-002\tDocument ML milestone thresholds derivation\tLow\nT-003\tMake CSV export truncation configurable\tLow\nT-004\tRefactor session_logger.py duplication\tLow\nT-010\tImplement confidence scoring thresholds\tMedium\nT-011\tAdd semantic similarity to ML predictions\tLow\nT-0\n\n**Files Explored:** `tasks/*.json`, `scripts/task_utils.py`\n\n### Discussion 2\n\n**When:** 2025-12-16  \n**Matched Keywords:** task, add  \n\n**Query:**\n\n> Please deeply think about the best way to implement these tasks in batches dispatched inteligently to sub Agents and tracked/verifyed by enabling director mode with a backup plan that covers potential failure points intelligently.\n\nTasks:\nT-002\tDocument ML milestone thresholds derivation\tLow\nT-003\tMake CSV export truncation configurable\tLow\nT-004\tRefactor session_logger.py duplication\tLow\nT-010\tImplement confidence scoring thresholds\tMedium\nT-011\tAdd semantic similarity to ML predictions\tLow\nT-0\n\n**Files Explored:** `scripts/task_utils.py`, `tasks/*.json`, `/home/user/Opus-code-test/tests/unit/test_ml_export.py`\n\n### Discussion 3\n\n**When:** 2025-12-16  \n**Matched Keywords:** task, add  \n\n**Query:**\n\n> Please deeply think about the best way to implement these tasks in batches dispatched inteligently to sub Agents and tracked/verifyed by enabling director mode with a backup plan that covers potential failure points intelligently.\n\nTasks:\nT-002\tDocument ML milestone thresholds derivation\tLow\nT-003\tMake CSV export truncation configurable\tLow\nT-004\tRefactor session_logger.py duplication\tLow\nT-010\tImplement confidence scoring thresholds\tMedium\nT-011\tAdd semantic similarity to ML predictions\tLow\nT-0\n\n**Files Explored:** `tasks/*.json`, `/home/user/Opus-code-test/tests/unit/test_ml_export.py`, `scripts/ml_data_collector.py`, `scripts/task_utils.py`\n\n## Options Considered\n\n### Option 1: Increase Random Suffix Length\n\n```\nT-YYYYMMDD-HHMMSS-XXXXXXXX  (8 hex chars)\n```\n\n**Pros:**\n- Simple change\n- 4 billion possibilities per second\n\n**Cons:**\n- Longer IDs\n- Doesn't leverage timestamp ordering\n\n### Option 2: Add Microseconds to Timestamp\n\n```\nT-YYYYMMDD-HHMMSSffffff-XXXX\nExample: T-20251214-163052123456-a1b2\n```\n\n**Pros:**\n- Timestamps remain sortable\n- 1 million unique timestamps per second\n- Combined with 4 hex suffix = practically unlimited uniqueness\n\n**Cons:**\n- IDs are 6 characters longer\n- Existing code parsing IDs needs update\n\n### Option 3: Use UUID Only\n\n```\nT-a1b2c3d4-e5f6-7890-abcd-ef1234567890\n```\n\n**Pros:**\n- Guaranteed uniqueness\n- Standard format\n\n**Cons:**\n- Not human-readable\n- Loses temporal ordering\n- Much longer\n\n## The Decision\n\n**Chosen Option:** Option 2 - Add Microseconds to Timestamp\n\n**Rationale:**\n- Preserves temporal ordering (IDs sort chronologically)\n- Microseconds provide 1M unique slots per second\n- Combined with 4 hex chars: virtually collision-proof\n- Minimal change to existing format\n\n## Implementation\n\n```python\ndef generate_task_id(session_id: Optional[str] = None) -> str:\n    now = datetime.now()\n    date_str = now.strftime(\"%Y%m%d\")\n    time_str = now.strftime(\"%H%M%S%f\")  # Added %f for microseconds\n    suffix = session_id or generate_session_id()\n    return f\"T-{date_str}-{time_str}-{suffix}\"\n```\n\n## Consequences\n\n### Positive\n- Tests no longer flaky\n- IDs unique even under heavy concurrent generation\n- Temporal ordering preserved\n\n### Negative\n- IDs 6 characters longer\n- Tests checking ID format needed update\n\n### Neutral\n- Existing IDs continue to work (no migration needed)\n- No performance impact\n\n## In Hindsight\n\n*This decision has been referenced in 2 subsequent commit(s).*\n\n- **2025-12-14** (`53c7985`): test: Update task ID format test to expect microseconds\n- **2025-12-14** (`5970006`): fix: Add microseconds to task ID to prevent collisions\n\n---\n\n*This decision story was enriched with conversation context from 12 chat session(s). Source: [adr-microseconds-task-id.md](../../samples/decisions/adr-microseconds-task-id.md)*\n"
    },
    {
      "path": "03-decisions/index.md",
      "title": "Architectural Decision Records",
      "section": "decisions",
      "tags": [
        "decisions",
        "adr",
        "index"
      ],
      "source_files": [
        "samples/decisions/"
      ],
      "excerpt": "",
      "keywords": [
        "decision",
        "task",
        "accepted",
        "adr",
        "microseconds",
        "generated",
        "cortical",
        "architectural",
        "records",
        "enriched"
      ],
      "full_content": "# Architectural Decision Records\n\n*Enriched with conversation context and implementation history.*\n\n---\n\n## Overview\n\n**Total Decisions:** 1  \n**Accepted:** 1  \n\n## Decision Catalog\n\n### [ADR-001: Add Microseconds to Task ID Generation](decision-adr-microseconds-task-id.md)\n\n**Status:** Accepted | **Date:** 2025-12-14\n\nTask IDs were generated with second-precision timestamps plus a 4-character hex suffix:...\n\n---\n\n*This chapter is part of [The Cortical Chronicles](../README.md), a self-documenting book generated by the Cortical Text Processor.*\n"
    },
    {
      "path": "04-evolution/bugfixes.md",
      "title": "Bug Fixes and Lessons",
      "section": "evolution",
      "tags": [
        "bugs",
        "fixes",
        "lessons-learned"
      ],
      "source_files": [
        "git log --grep=fix:"
      ],
      "excerpt": "",
      "keywords": [
        "commit",
        "date",
        "git",
        "json",
        "chats",
        "chat",
        "files",
        "changed",
        "session",
        "fix"
      ],
      "full_content": "# Bug Fixes and Lessons\n\n*What broke, how we fixed it, and what we learned.*\n\n---\n\n## Overview\n\n**13 bugs** have been identified and resolved. Each fix taught us something about the system.\n\n## Bug Fix History\n\n### Archive ML session after transcript processing (T-003 16f3)\n\n**Commit:** `59072c8`  \n**Date:** 2025-12-16  \n**Files Changed:** scripts/ml_data_collector.py  \n\n### Update CSV truncation test for new defaults (input=500, output=2000)\n\n**Commit:** `ca94a01`  \n**Date:** 2025-12-16  \n\n### Fix ML data collection milestone counting and add session/action capture\n\n**Commit:** `273baef`  \n**Date:** 2025-12-16  \n**Files Changed:** .git-ml/chats/2025-12-15/chat-20251216-121720-30c3c1.json, .git-ml/chats/2025-12-16/chat-20251216-121720-01077d.json, .git-ml/chats/2025-12-16/chat-20251216-121720-306450.json, .git-ml/chats/2025-12-16/chat-20251216-121720-5ef95b.json, .git-ml/chats/2025-12-16/chat-20251216-121720-8a1e7b.json  \n*(and 6 more)*  \n\n### Address critical ML data collection and prediction issues\n\n**Commit:** `fead1c1`  \n**Date:** 2025-12-16  \n**Files Changed:** .git-ml/chats/2025-12-15/chat-20251216-115057-b5bb48.json, .git-ml/chats/2025-12-16/chat-20251216-115057-3617f9.json, .git-ml/chats/2025-12-16/chat-20251216-115057-9502fd.json, .git-ml/chats/2025-12-16/chat-20251216-115057-cbbe64.json, .git-ml/chats/2025-12-16/chat-20251216-115057-f65b7a.json  \n*(and 4 more)*  \n\n### Add missing imports in validate command\n\n**Commit:** `172ad8f`  \n**Date:** 2025-12-16  \n**Files Changed:** scripts/ml_data_collector.py  \n\n### Clean up gitignore pattern for .git-ml/commits/\n\n**Commit:** `a65d54f`  \n**Date:** 2025-12-16  \n**Files Changed:** .gitignore  \n\n### Prevent infinite commit loop in ML data collection hooks\n\n**Commit:** `66ad656`  \n**Date:** 2025-12-16  \n**Files Changed:** .git-ml/chats/2025-12-16/chat-20251216-004054-78b531.json, .git-ml/tracked/commits.jsonl, scripts/ml_data_collector.py  \n\n### Correct hook format in settings.local.json\n\n**Commit:** `19ac02a`  \n**Date:** 2025-12-16  \n**Files Changed:** .claude/settings.local.json  \n\n### Use filename-based sorting for deterministic session ordering\n\n**Commit:** `61d502d`  \n**Date:** 2025-12-15  \n\n### Increase ID suffix length to prevent collisions\n\n**Commit:** `8ac4b6b`  \n**Date:** 2025-12-15  \n\n### Add import guards for optional test dependencies\n\n**Commit:** `91ffb04`  \n**Date:** 2025-12-15  \n\n### Make session file sorting stable for deterministic ordering\n\n**Commit:** `7433b36`  \n**Date:** 2025-12-15  \n\n### Cap query expansion weights to prevent term domination\n\n**Commit:** `fecd6dc`  \n**Date:** 2025-12-15  \n\n"
    },
    {
      "path": "04-evolution/features.md",
      "title": "Feature Evolution",
      "section": "evolution",
      "tags": [
        "features",
        "capabilities",
        "growth"
      ],
      "source_files": [
        "git log --grep=feat:"
      ],
      "excerpt": "",
      "keywords": [
        "commit",
        "date",
        "add",
        "files",
        "modified",
        "capabilities",
        "system",
        "wave",
        "orchestration",
        "legacy"
      ],
      "full_content": "# Feature Evolution\n\n*How the Cortical Text Processor gained its capabilities.*\n\n---\n\n## Overview\n\nThe system has evolved through **22 feature additions**. Below is the narrative of how each capability came to be.\n\n## Other Capabilities\n\n### Add smart caching to markdown book generation\n\n**Commit:** `afd3c5b`  \n**Date:** 2025-12-16  \n**Files Modified:** 8  \n\n### Add consolidated markdown book generation\n\n**Commit:** `f8a2ad6`  \n**Date:** 2025-12-16  \n**Files Modified:** 3  \n\n### Add content generators for Cortical Chronicles (Wave 2)\n\n**Commit:** `3022110`  \n**Date:** 2025-12-16  \n**Files Modified:** 23  \n\n### Add Cortical Chronicles book infrastructure (Wave 1)\n\n**Commit:** `c730057`  \n**Date:** 2025-12-16  \n**Files Modified:** 13  \n\n### Batch task distribution implementation via Director orchestration\n\n**Commit:** `4f915c3`  \n**Date:** 2025-12-16  \n**Files Modified:** 8  \n\n### Add orchestration extraction for director sub-agent tracking\n\n**Commit:** `4eaeb37`  \n**Date:** 2025-12-15  \n\n### Add stunning animated ASCII codebase visualizer\n\n**Commit:** `e085a0b`  \n**Date:** 2025-12-15  \n\n### Add ASCII art codebase visualization script\n\n**Commit:** `43aae33`  \n**Date:** 2025-12-15  \n\n### Complete legacy task system migration\n\n**Commit:** `33dc8b2`  \n**Date:** 2025-12-15  \n\n### Add director orchestration execution tracking system\n\n**Commit:** `4976c58`  \n**Date:** 2025-12-15  \n\n## Search Capabilities\n\n### Add chunked parallel processing for TF-IDF/BM25 (LEGACY-135)\n\n**Commit:** `5665839`  \n**Date:** 2025-12-16  \n\n### Add search integration and web interface (Wave 3)\n\n**Commit:** `0022466`  \n**Date:** 2025-12-16  \n**Files Modified:** 11  \n\n### Add security concept group and TF-IDF weighted query expansion\n\n**Commit:** `af3a7e0`  \n**Date:** 2025-12-15  \n\n## Data Capabilities\n\n### Implement WAL + Snapshot persistence system (LEGACY-133)\n\n**Commit:** `c7e662a`  \n**Date:** 2025-12-16  \n\n### Add git-tracked JSONL storage for orchestration data\n\n**Commit:** `fb30e38`  \n**Date:** 2025-12-15  \n\n## Ml Capabilities\n\n### Implement top priorities (ML capture, state storage, legacy cleanup)\n\n**Commit:** `4820c64`  \n**Date:** 2025-12-16  \n\n### Add file existence filter to ML predictions\n\n**Commit:** `3cab2ba`  \n**Date:** 2025-12-16  \n**Files Modified:** 1  \n\n### Add ML file prediction model\n\n**Commit:** `ac549dd`  \n**Date:** 2025-12-16  \n**Files Modified:** 2  \n\n### Add chunked storage for git-friendly ML data\n\n**Commit:** `0754540`  \n**Date:** 2025-12-16  \n**Files Modified:** 4  \n\n### Add ML stats report to CI pipeline\n\n**Commit:** `3e05a70`  \n**Date:** 2025-12-16  \n**Files Modified:** 9  \n\n## Documentation Capabilities\n\n### Add CI workflow and documentation (Wave 4)\n\n**Commit:** `940fdf2`  \n**Date:** 2025-12-16  \n**Files Modified:** 5  \n\n### Add animated GIF visualizations to README\n\n**Commit:** `b4d7c82`  \n**Date:** 2025-12-15  \n\n"
    },
    {
      "path": "04-evolution/refactors.md",
      "title": "Refactorings and Architecture Evolution",
      "section": "evolution",
      "tags": [
        "refactoring",
        "architecture",
        "design"
      ],
      "source_files": [
        "git log --grep=refactor:"
      ],
      "excerpt": "",
      "keywords": [
        "commit",
        "date",
        "refactorings",
        "codebase",
        "improved",
        "files",
        "architecture",
        "evolution",
        "structure",
        "time"
      ],
      "full_content": "# Refactorings and Architecture Evolution\n\n*How the codebase structure improved over time.*\n\n---\n\n## Overview\n\nThe codebase has undergone **3 refactorings**. Each improved code quality, maintainability, or performance.\n\n## Refactoring History\n\n### Complete legacy task system cleanup\n\n**Commit:** `8dedda6`  \n**Date:** 2025-12-16  \n\n### Remove unused protobuf serialization (T-013 f0ff)\n\n**Commit:** `d7a98ae`  \n**Date:** 2025-12-16  \n**Changes:** +100/-1460 lines  \n**Scope:** 6 files affected  \n\n### Split large files exceeding 25000 token limit\n\n**Commit:** `21ec5ea`  \n**Date:** 2025-12-15  \n\n"
    },
    {
      "path": "04-evolution/test_timeline.md",
      "title": "Test",
      "section": "evolution",
      "tags": [],
      "source_files": [],
      "excerpt": "",
      "keywords": [
        "add",
        "timeline",
        "december",
        "week",
        "dec",
        "feat",
        "book",
        "docs",
        "vision"
      ],
      "full_content": "# Timeline\n\n---\n\n## December 2025\n\n### Week of Dec 15\n\n- **2025-12-16**: feat: Add book\n- **2025-12-16**: docs: Add vision\n\n"
    },
    {
      "path": "04-evolution/timeline.md",
      "title": "Project Timeline",
      "section": "evolution",
      "tags": [
        "timeline",
        "chronology",
        "evolution"
      ],
      "source_files": [
        "git log"
      ],
      "excerpt": "",
      "keywords": [
        "feat",
        "capture",
        "add",
        "legacy",
        "session",
        "data",
        "book",
        "generation",
        "markdown",
        "implement"
      ],
      "full_content": "# Project Timeline\n\n*A chronological journey through the Cortical Text Processor's development.*\n\n---\n\n## December 2025\n\n### Week of Dec 15\n\n- **2025-12-16**: ml: Capture session data\n- **2025-12-16**: docs: Add living book generation vision\n- **2025-12-16**: ml: Capture session data\n- **2025-12-16**: feat: Add smart caching to markdown book generation\n- **2025-12-16**: ml: Capture session data\n- **2025-12-16**: feat: Add consolidated markdown book generation\n- **2025-12-16**: feat: Add chunked parallel processing for TF-IDF/BM25 (LEGACY-135)\n- **2025-12-16**: feat: Implement WAL + Snapshot persistence system (LEGACY-133)\n- **2025-12-16**: refactor: Complete legacy task system cleanup\n- **2025-12-16**: feat: Implement top priorities (ML capture, state storage, legacy cleanup)\n\n"
    },
    {
      "path": "05-case-studies/index.md",
      "title": "index",
      "section": "case-studies",
      "tags": [],
      "source_files": [],
      "excerpt": "No case studies available yet. Case studies are generated from ML session data when sessions demonstrate significant problem-solving narratives.",
      "keywords": [
        "case",
        "studies",
        "problem",
        "solving",
        "real",
        "sessions",
        "development",
        "generated",
        "session",
        "data"
      ],
      "full_content": "# Case Studies\n\n*Real problem-solving sessions from the development of the Cortical Text Processor*\n\nNo case studies available yet. Case studies are generated from ML session data when sessions demonstrate significant problem-solving narratives.\n\n**What makes a good case study?**\n\n- At least 5 exchanges (substantial investigation)\n- Multiple tools used (shows exploration)\n- Resulted in commits (concrete outcome)\n- Clear problem statement (queries starting with 'fix', 'why', 'how do', etc.)\n\n*These case studies are automatically generated from ML session data collected during development. They demonstrate real problem-solving workflows and serve as both documentation and learning material.*\n"
    },
    {
      "path": "05-future/index.md",
      "title": "Future Chapter",
      "section": "future",
      "tags": [
        "future",
        "placeholder"
      ],
      "source_files": [
        "(to be implemented)"
      ],
      "excerpt": "",
      "keywords": [
        "future",
        "chapter",
        "auto",
        "generated",
        "update"
      ],
      "full_content": "# Future\n\n*This chapter will be auto-generated in a future update.*\n"
    },
    {
      "path": "06-lessons/index.md",
      "title": "Lessons Learned",
      "section": "lessons",
      "tags": [
        "lessons",
        "wisdom",
        "experience"
      ],
      "source_files": [
        "git log"
      ],
      "excerpt": "",
      "keywords": [
        "lessons",
        "development",
        "history",
        "learned",
        "correctness",
        "cortical",
        "performance",
        "architecture",
        "testing",
        "text"
      ],
      "full_content": "# Lessons Learned\n\n*What the Cortical Text Processor taught us about building IR systems.*\n\n---\n\n## Overview\n\nThrough **51 lessons** extracted from development history, we've learned how to build better search systems. Each bug fixed, each optimization made, and each refactoring completed taught us something valuable.\n\n## Statistics\n\n- **Total Commits Analyzed**: 300\n- **Lessons Extracted**: 51\n\n### By Category\n\n- **Performance**: 1 lessons\n- **Correctness**: 24 lessons\n- **Architecture**: 10 lessons\n- **Testing**: 16 lessons\n\n## Lesson Categories\n\n### [Performance Lessons](lessons-performance.md)\n\nHow we learned to optimize search and graph algorithms\n\n**1 lessons** from development history.\n\n### [Correctness Lessons](lessons-correctness.md)\n\nBugs we fixed and edge cases we discovered\n\n**24 lessons** from development history.\n\n### [Architecture Lessons](lessons-architecture.md)\n\nHow we evolved the codebase structure\n\n**10 lessons** from development history.\n\n### [Testing Lessons](lessons-testing.md)\n\nWhat we learned about verifying correctness\n\n**16 lessons** from development history.\n\n---\n\n*This chapter is part of [The Cortical Chronicles](../README.md), a self-documenting book generated by the Cortical Text Processor.*\n"
    },
    {
      "path": "06-lessons/lessons-architecture.md",
      "title": "Architecture Lessons",
      "section": "lessons",
      "tags": [
        "lessons",
        "architecture",
        "experience"
      ],
      "source_files": [
        "git log --grep=architecture"
      ],
      "excerpt": "",
      "keywords": [
        "lesson",
        "files",
        "data",
        "commit",
        "date",
        "changed",
        "add",
        "structure",
        "orchestration",
        "cortical"
      ],
      "full_content": "# Architecture Lessons\n\n*How we evolved the code structure over time.*\n\n---\n\n## Overview\n\nThis chapter captures **10 lessons** from architecture work. Each entry shows the problem, the solution, and the principle we extracted.\n\n### Complete legacy task system cleanup\n\n**Commit:** `8dedda6`  \n**Date:** 2025-12-16  \n**Files Changed:** 4  \n  - `docs/archive/migrate_legacy_tasks.py`\n  - `scripts/select_task.py`\n  - `scripts/task_graph.py`\n  - *(and 1 more)*\n\n**The Lesson:** Maintain clear structure. The lesson? Complete legacy task system cleanup\n\n### Feat: Add Cortical Chronicles book infrastructure (Wave 1)\n\n**Commit:** `c730057`  \n**Date:** 2025-12-16  \n**Files Changed:** 13  \n  - `.git-ml/current_session.json`\n  - `.git-ml/tracked/commits.jsonl`\n  - `book/00-preface/.gitkeep`\n  - *(and 10 more)*\n**Changes:** +434/-0 lines  \n\n**The Lesson:** Maintain clear structure. The lesson? feat: Add Cortical Chronicles book infrastructure (Wave 1)\n\n### Remove unused protobuf serialization (T-013 f0ff)\n\n**Commit:** `d7a98ae`  \n**Date:** 2025-12-16  \n**Files Changed:** 6  \n  - `cortical/persistence.py`\n  - `cortical/proto/__init__.py`\n  - `cortical/proto/schema.proto`\n  - *(and 3 more)*\n**Changes:** +100/-1460 lines  \n\n**The Lesson:** Maintain clear structure. The lesson? Remove unused protobuf serialization (T-013 f0ff)\n\n### Data: Add orchestration extraction data from quality audit session\n\n**Commit:** `c85c668`  \n**Date:** 2025-12-16  \n**Files Changed:** 1  \n  - `.git-ml/tracked/orchestration.jsonl`\n**Changes:** +1/-0 lines  \n\n**The Lesson:** Keep modules focused. The lesson? data: Add orchestration extraction data from quality audit session\n\n### Data: Add orchestration extraction data for ML training\n\n**Commit:** `bb75148`  \n**Date:** 2025-12-15  \n**Files Changed:** 1  \n  - `.git-ml/orchestration/05c8c5d9-75c6-4267-9fef-1d5573ba201b_orchestration.json`\n**Changes:** +66/-0 lines  \n\n**The Lesson:** Keep modules focused. The lesson? data: Add orchestration extraction data for ML training\n\n### Feat: Add orchestration extraction for director sub-agent tracking\n\n**Commit:** `4eaeb37`  \n**Date:** 2025-12-15  \n**Files Changed:** 4  \n  - `.gitignore`\n  - `scripts/ml_collector/__init__.py`\n  - `scripts/ml_collector/orchestration.py`\n  - *(and 1 more)*\n\n**The Lesson:** Keep modules focused. The lesson? feat: Add orchestration extraction for director sub-agent tracking\n\n### Split large files exceeding 25000 token limit\n\n**Commit:** `21ec5ea`  \n**Date:** 2025-12-15  \n**Files Changed:** 36  \n  - `.refactor-backup/BACKUP_PLAN.md`\n  - `.refactor-backup/analysis.py`\n  - `.refactor-backup/ml_data_collector.py`\n  - *(and 33 more)*\n\n**The Lesson:** Keep modules focused. The lesson? Split large files exceeding 25000 token limit\n\n### Consolidate ML data to single JSONL files\n\n**Commit:** `205fe34`  \n**Date:** 2025-12-15  \n**Files Changed:** 486  \n  - `.git-ml/commits-lite/0039ad5b13fb_2025-12-11.json`\n  - `.git-ml/commits-lite/00f88d48ab42_2025-12-14.json`\n  - `.git-ml/commits-lite/051d20028ddd_2025-12-13.json`\n  - *(and 483 more)*\n**Changes:** +658/-12208 lines  \n\n**The Lesson:** Maintain clear structure. The lesson? Consolidate ML data to single JSONL files\n\n### Feat: Add ML data collection infrastructure for project-specific micro-model\n\n**Commit:** `1568f3c`  \n**Date:** 2025-12-15  \n**Files Changed:** 4  \n  - `.claude/hooks/session_logger.py`\n  - `.claude/skills/ml-logger/SKILL.md`\n  - `.gitignore`\n  - *(and 1 more)*\n**Changes:** +1039/-0 lines  \n\n**The Lesson:** Maintain clear structure. The lesson? feat: Add ML data collection infrastructure for project-specific micro-model\n\n### Clean up directory structure and queue search relevance fixes\n\n**Commit:** `cd8b9f5`  \n**Date:** 2025-12-14  \n**Files Changed:** 5  \n  - `IMPLEMENTATION_SUMMARY.md`\n  - `docs/PATTERN_DETECTION_GUIDE.md`\n  - `examples/demo_pattern_detection.py`\n  - *(and 2 more)*\n**Changes:** +68/-293 lines  \n\n**The Lesson:** Maintain clear structure. The lesson? Clean up directory structure and queue search relevance fixes\n\n---\n\n*This chapter is part of [The Cortical Chronicles](../README.md), a self-documenting book generated by the Cortical Text Processor.*\n"
    },
    {
      "path": "06-lessons/lessons-correctness.md",
      "title": "Correctness Lessons",
      "section": "lessons",
      "tags": [
        "lessons",
        "correctness",
        "experience"
      ],
      "source_files": [
        "git log --grep=correctness"
      ],
      "excerpt": "",
      "keywords": [
        "tests",
        "commit",
        "files",
        "lesson",
        "date",
        "changed",
        "verify",
        "assumptions",
        "wisdom",
        "changes"
      ],
      "full_content": "# Correctness Lessons\n\n*Bugs we encountered and how we fixed them.*\n\n---\n\n## Overview\n\nThis chapter captures **24 lessons** from correctness work. Each entry shows the problem, the solution, and the principle we extracted.\n\n### Archive ML session after transcript processing (T-003 16f3)\n\n**Commit:** `59072c8`  \n**Date:** 2025-12-16  \n**Files Changed:** 1  \n  - `scripts/ml_data_collector.py`\n**Changes:** +12/-0 lines  \n\n**The Lesson:** Verify assumptions with tests. The wisdom: Archive ML session after transcript processing (T-003 16f3)\n\n### Update CSV truncation test for new defaults (input=500, output=2000)\n\n**Commit:** `ca94a01`  \n**Date:** 2025-12-16  \n**Files Changed:** 4  \n  - `.git-ml/chats/2025-12-16/chat-20251216-125311-0ce6d9.json`\n  - `.git-ml/chats/2025-12-16/chat-20251216-132048-ba08bf.json`\n  - `.git-ml/tracked/commits.jsonl`\n  - *(and 1 more)*\n\n**The Lesson:** Verify assumptions with tests. The wisdom: Update CSV truncation test for new defaults (input=500, output=2000)\n\n### Fix ML data collection milestone counting and add session/action capture\n\n**Commit:** `273baef`  \n**Date:** 2025-12-16  \n**Files Changed:** 11  \n  - `.git-ml/chats/2025-12-15/chat-20251216-121720-30c3c1.json`\n  - `.git-ml/chats/2025-12-16/chat-20251216-121720-01077d.json`\n  - `.git-ml/chats/2025-12-16/chat-20251216-121720-306450.json`\n  - *(and 8 more)*\n**Changes:** +95/-29 lines  \n\n**The Lesson:** Verify assumptions with tests. The wisdom: Fix ML data collection milestone counting and add session/action capture\n\n### Address critical ML data collection and prediction issues\n\n**Commit:** `fead1c1`  \n**Date:** 2025-12-16  \n**Files Changed:** 9  \n  - `.git-ml/chats/2025-12-15/chat-20251216-115057-b5bb48.json`\n  - `.git-ml/chats/2025-12-16/chat-20251216-115057-3617f9.json`\n  - `.git-ml/chats/2025-12-16/chat-20251216-115057-9502fd.json`\n  - *(and 6 more)*\n**Changes:** +148/-17 lines  \n\n**The Lesson:** Verify assumptions with tests. The wisdom: Address critical ML data collection and prediction issues\n\n### Fix(proto): Make protobuf loading lazy to fix CI smoke test failures\n\n**Commit:** `a93518f`  \n**Date:** 2025-12-16  \n**Files Changed:** 2  \n  - `cortical/proto/__init__.py`\n  - `cortical/proto/serialization.py`\n**Changes:** +53/-19 lines  \n\n**The Lesson:** Verify assumptions with tests. The wisdom: fix(proto): Make protobuf loading lazy to fix CI smoke test failures\n\n### Add missing imports in validate command\n\n**Commit:** `172ad8f`  \n**Date:** 2025-12-16  \n**Files Changed:** 1  \n  - `scripts/ml_data_collector.py`\n**Changes:** +5/-0 lines  \n\n**The Lesson:** Verify assumptions with tests. The wisdom: Add missing imports in validate command\n\n### Clean up gitignore pattern for .git-ml/commits/\n\n**Commit:** `a65d54f`  \n**Date:** 2025-12-16  \n**Files Changed:** 1  \n  - `.gitignore`\n**Changes:** +2/-1 lines  \n\n**The Lesson:** Verify assumptions with tests. The wisdom: Clean up gitignore pattern for .git-ml/commits/\n\n### Prevent infinite commit loop in ML data collection hooks\n\n**Commit:** `66ad656`  \n**Date:** 2025-12-16  \n**Files Changed:** 3  \n  - `.git-ml/chats/2025-12-16/chat-20251216-004054-78b531.json`\n  - `.git-ml/tracked/commits.jsonl`\n  - `scripts/ml_data_collector.py`\n**Changes:** +9/-1 lines  \n\n**The Lesson:** Verify assumptions with tests. The wisdom: Prevent infinite commit loop in ML data collection hooks\n\n### Correct hook format in settings.local.json\n\n**Commit:** `19ac02a`  \n**Date:** 2025-12-16  \n**Files Changed:** 1  \n  - `.claude/settings.local.json`\n**Changes:** +14/-4 lines  \n\n**The Lesson:** Verify assumptions with tests. The wisdom: Correct hook format in settings.local.json\n\n### Use filename-based sorting for deterministic session ordering\n\n**Commit:** `61d502d`  \n**Date:** 2025-12-15  \n**Files Changed:** 2  \n  - `scripts/session_context.py`\n  - `tests/unit/test_session_context.py`\n\n**The Lesson:** Verify assumptions with tests. The wisdom: Use filename-based sorting for deterministic session ordering\n\n### Increase ID suffix length to prevent collisions\n\n**Commit:** `8ac4b6b`  \n**Date:** 2025-12-15  \n**Files Changed:** 2  \n  - `scripts/orchestration_utils.py`\n  - `tests/unit/test_orchestration_utils.py`\n\n**The Lesson:** Verify assumptions with tests. The wisdom: Increase ID suffix length to prevent collisions\n\n### Add import guards for optional test dependencies\n\n**Commit:** `91ffb04`  \n**Date:** 2025-12-15  \n**Files Changed:** 2  \n  - `tests/security/test_fuzzing.py`\n  - `tests/test_mcp_server.py`\n\n**The Lesson:** Verify assumptions with tests. The wisdom: Add import guards for optional test dependencies\n\n### Make session file sorting stable for deterministic ordering\n\n**Commit:** `7433b36`  \n**Date:** 2025-12-15  \n**Files Changed:** 1  \n  - `scripts/session_context.py`\n\n**The Lesson:** Verify assumptions with tests. The wisdom: Make session file sorting stable for deterministic ordering\n\n### Feat(LEGACY-130): Expand customer service corpus and fix xfailed tests\n\n**Commit:** `7f9664d`  \n**Date:** 2025-12-15  \n**Files Changed:** 6  \n  - `samples/customer_service/complaint_escalation_procedures.txt`\n  - `samples/customer_service/empathy_and_active_listening.txt`\n  - `samples/customer_service/refund_request_handling.txt`\n  - *(and 3 more)*\n\n**The Lesson:** Verify assumptions with tests. The wisdom: feat(LEGACY-130): Expand customer service corpus and fix xfailed tests\n\n### Cap query expansion weights to prevent term domination\n\n**Commit:** `fecd6dc`  \n**Date:** 2025-12-15  \n**Files Changed:** 3  \n  - `cortical/query/expansion.py`\n  - `tests/behavioral/test_customer_service_quality.py`\n  - `tests/unit/test_query_expansion.py`\n\n**The Lesson:** Verify assumptions with tests. The wisdom: Cap query expansion weights to prevent term domination\n\n### Add YAML frontmatter to slash commands for discovery\n\n**Commit:** `5b52da2`  \n**Date:** 2025-12-15  \n**Files Changed:** 7  \n  - `.claude/commands/delegate.md`\n  - `.claude/commands/director.md`\n  - `.claude/commands/knowledge-transfer.md`\n  - *(and 4 more)*\n\n**The Lesson:** Verify assumptions with tests. The wisdom: Add YAML frontmatter to slash commands for discovery\n\n### Stop tracking ML commit data files (too large for GitHub)\n\n**Commit:** `a6f39e0`  \n**Date:** 2025-12-15  \n**Files Changed:** 472  \n  - `.git-ml/commits/0039ad5b_2025-12-11_24b1b10a.json`\n  - `.git-ml/commits/00f88d48_2025-12-14_8749d448.json`\n  - `.git-ml/commits/051d2002_2025-12-13_7896a312.json`\n  - *(and 469 more)*\n**Changes:** +4/-2263268 lines  \n\n**The Lesson:** Verify assumptions with tests. The wisdom: Stop tracking ML commit data files (too large for GitHub)\n\n### Increase ML data retention to 2 years for training milestones\n\n**Commit:** `95e9f06`  \n**Date:** 2025-12-15  \n**Files Changed:** 2  \n  - `README.md`\n  - `scripts/ml_data_collector.py`\n**Changes:** +7/-5 lines  \n\n**The Lesson:** Verify assumptions with tests. The wisdom: Increase ML data retention to 2 years for training milestones\n\n### Update tests for BM25 default and stop word tokenization\n\n**Commit:** `9dc7268`  \n**Date:** 2025-12-15  \n**Files Changed:** 2  \n  - `tests/unit/test_processor_core.py`\n  - `tests/unit/test_query_search.py`\n**Changes:** +23/-10 lines  \n\n**The Lesson:** Verify assumptions with tests. The wisdom: Update tests for BM25 default and stop word tokenization\n\n### Address audit findings and add documentation\n\n**Commit:** `36be3a1`  \n**Date:** 2025-12-15  \n**Files Changed:** 4  \n  - `.claude/commands/ml-log.md`\n  - `.claude/commands/ml-stats.md`\n  - `CLAUDE.md`\n  - *(and 1 more)*\n**Changes:** +201/-15 lines  \n\n**The Lesson:** Verify assumptions with tests. The wisdom: Address audit findings and add documentation\n\n### Harden ML data collector with critical fixes\n\n**Commit:** `4438d60`  \n**Date:** 2025-12-15  \n**Files Changed:** 1  \n  - `scripts/ml_data_collector.py`\n**Changes:** +151/-54 lines  \n\n**The Lesson:** Verify assumptions with tests. The wisdom: Harden ML data collector with critical fixes\n\n### Correct line number assertions in pattern detection tests\n\n**Commit:** `1b9901d`  \n**Date:** 2025-12-15  \n**Files Changed:** 1  \n  - `tests/unit/test_patterns.py`\n**Changes:** +5/-5 lines  \n\n**The Lesson:** Verify assumptions with tests. The wisdom: Correct line number assertions in pattern detection tests\n\n### Add test file penalty and code stop word filtering to search\n\n**Commit:** `1fafc8b`  \n**Date:** 2025-12-14  \n**Files Changed:** 3  \n  - `cortical/processor/query_api.py`\n  - `cortical/query/passages.py`\n  - `cortical/query/search.py`\n**Changes:** +51/-9 lines  \n\n**The Lesson:** Verify assumptions with tests. The wisdom: Add test file penalty and code stop word filtering to search\n\n### Replace external action with native Python link checker\n\n**Commit:** `901a181`  \n**Date:** 2025-12-14  \n**Files Changed:** 5  \n  - `.github/workflows/ci.yml`\n  - `.markdown-link-check.json`\n  - `scripts/resolve_wiki_links.py`\n  - *(and 2 more)*\n**Changes:** +172/-34 lines  \n\n**The Lesson:** Validate inputs early. The lesson? Replace external action with native Python link checker\n\n---\n\n*This chapter is part of [The Cortical Chronicles](../README.md), a self-documenting book generated by the Cortical Text Processor.*\n"
    },
    {
      "path": "06-lessons/lessons-performance.md",
      "title": "Performance Lessons",
      "section": "lessons",
      "tags": [
        "lessons",
        "performance",
        "experience"
      ],
      "source_files": [
        "git log --grep=performance"
      ],
      "excerpt": "",
      "keywords": [
        "cortical",
        "optimize",
        "performance",
        "lessons",
        "chapter",
        "feat",
        "compute_all",
        "add",
        "graph",
        "boosted"
      ],
      "full_content": "# Performance Lessons\n\n*What we learned about making the system fast and efficient.*\n\n---\n\n## Overview\n\nThis chapter captures **1 lessons** from performance work. Each entry shows the problem, the solution, and the principle we extracted.\n\n### Feat: Optimize compute_all and add Graph-Boosted search (GB-BM25)\n\n**Commit:** `fcce0c2`  \n**Date:** 2025-12-15  \n**Files Changed:** 5  \n  - `cortical/analysis.py`\n  - `cortical/processor/query_api.py`\n  - `cortical/query/__init__.py`\n  - *(and 2 more)*\n**Changes:** +244/-62 lines  \n\n**The Lesson:** Optimize based on evidence. The lesson? feat: Optimize compute_all and add Graph-Boosted search (GB-BM25)\n\n---\n\n*This chapter is part of [The Cortical Chronicles](../README.md), a self-documenting book generated by the Cortical Text Processor.*\n"
    },
    {
      "path": "06-lessons/lessons-testing.md",
      "title": "Testing Lessons",
      "section": "lessons",
      "tags": [
        "lessons",
        "testing",
        "experience"
      ],
      "source_files": [
        "git log --grep=testing"
      ],
      "excerpt": "",
      "keywords": [
        "tests",
        "coverage",
        "lesson",
        "unit",
        "add",
        "commit",
        "date",
        "files",
        "changed",
        "test"
      ],
      "full_content": "# Testing Lessons\n\n*Insights from writing and maintaining tests.*\n\n---\n\n## Overview\n\nThis chapter captures **16 lessons** from testing work. Each entry shows the problem, the solution, and the principle we extracted.\n\n### Add unit tests for Cortical Chronicles generators\n\n**Commit:** `a09bd89`  \n**Date:** 2025-12-16  \n**Files Changed:** 2  \n  - `.git-ml/tracked/commits.jsonl`\n  - `tests/unit/test_generate_book.py`\n**Changes:** +1372/-0 lines  \n\n**The Lesson:** Test what you build. The lesson? Add unit tests for Cortical Chronicles generators\n\n### Fix(tests): Mock file existence in ML prediction tests\n\n**Commit:** `ec8db7a`  \n**Date:** 2025-12-16  \n**Files Changed:** 2  \n  - `tests/unit/test_ml_file_prediction.py`\n  - `tests/unit/test_protobuf_serialization.py`\n**Changes:** +21/-8 lines  \n\n**The Lesson:** Mock external dependencies. The wisdom: fix(tests): Mock file existence in ML prediction tests\n\n### Fix(tests): Mock file existence in ML prediction tests\n\n**Commit:** `4f7e195`  \n**Date:** 2025-12-16  \n**Files Changed:** 1  \n  - `tests/unit/test_ml_file_prediction.py`\n**Changes:** +10/-6 lines  \n\n**The Lesson:** Mock external dependencies. The wisdom: fix(tests): Mock file existence in ML prediction tests\n\n### Add comprehensive test suite for orchestration.py (33 tests)\n\n**Commit:** `d999c84`  \n**Date:** 2025-12-15  \n**Files Changed:** 2  \n  - `CLAUDE.md`\n  - `tests/unit/test_ml_orchestration.py`\n**Changes:** +801/-1 lines  \n\n**The Lesson:** Test what you build. The lesson? Add comprehensive test suite for orchestration.py (33 tests)\n\n### Update README test count (3800+) and coverage badge (>90%)\n\n**Commit:** `4ec93d5`  \n**Date:** 2025-12-15  \n**Files Changed:** 1  \n  - `README.md`\n\n**The Lesson:** Measure coverage to find gaps. The lesson? Update README test count (3800+) and coverage badge (>90%)\n\n### Update task status for Wave 4 completed coverage tests (ALL COMPLETE)\n\n**Commit:** `3b9a071`  \n**Date:** 2025-12-15  \n**Files Changed:** 1  \n  - `tasks/2025-12-15_05-23-36_ceac.json`\n\n**The Lesson:** Measure coverage to find gaps. The lesson? Update task status for Wave 4 completed coverage tests (ALL COMPLETE)\n\n### Feat: Add comprehensive test coverage for Wave 4 modules (FINAL)\n\n**Commit:** `73d6da8`  \n**Date:** 2025-12-15  \n**Files Changed:** 4  \n  - `tests/unit/test_code_concepts_coverage.py`\n  - `tests/unit/test_diff_coverage.py`\n  - `tests/unit/test_fluent_coverage.py`\n  - *(and 1 more)*\n\n**The Lesson:** Measure coverage to find gaps. The lesson? feat: Add comprehensive test coverage for Wave 4 modules (FINAL)\n\n### Update task status for Wave 3 completed coverage tests\n\n**Commit:** `0b2eaf2`  \n**Date:** 2025-12-15  \n**Files Changed:** 1  \n  - `tasks/2025-12-15_05-23-36_ceac.json`\n\n**The Lesson:** Measure coverage to find gaps. The lesson? Update task status for Wave 3 completed coverage tests\n\n### Feat: Add comprehensive test coverage for Wave 3 modules\n\n**Commit:** `036f830`  \n**Date:** 2025-12-15  \n**Files Changed:** 4  \n  - `tests/unit/test_config_coverage.py`\n  - `tests/unit/test_fingerprint_coverage.py`\n  - `tests/unit/test_query_chunking.py`\n  - *(and 1 more)*\n\n**The Lesson:** Measure coverage to find gaps. The lesson? feat: Add comprehensive test coverage for Wave 3 modules\n\n### Update task status for Wave 2 completed coverage tests\n\n**Commit:** `66f7df2`  \n**Date:** 2025-12-15  \n**Files Changed:** 1  \n  - `tasks/2025-12-15_05-23-36_ceac.json`\n\n**The Lesson:** Measure coverage to find gaps. The lesson? Update task status for Wave 2 completed coverage tests\n\n### Feat: Add comprehensive test coverage for Wave 2 modules\n\n**Commit:** `5a6bb26`  \n**Date:** 2025-12-15  \n**Files Changed:** 4  \n  - `tests/unit/test_embeddings_coverage.py`\n  - `tests/unit/test_query_definitions.py`\n  - `tests/unit/test_query_passages.py`\n  - *(and 1 more)*\n\n**The Lesson:** Measure coverage to find gaps. The lesson? feat: Add comprehensive test coverage for Wave 2 modules\n\n### Update task status for completed coverage tests\n\n**Commit:** `cc147fd`  \n**Date:** 2025-12-15  \n**Files Changed:** 1  \n  - `tasks/2025-12-15_05-23-36_ceac.json`\n\n**The Lesson:** Measure coverage to find gaps. The lesson? Update task status for completed coverage tests\n\n### Feat: Add comprehensive test coverage for query and analysis modules\n\n**Commit:** `70a4b1b`  \n**Date:** 2025-12-15  \n**Files Changed:** 7  \n  - `requirements.txt`\n  - `tasks/2025-12-14_01-53-45_7b60.json`\n  - `tasks/2025-12-14_11-11-44_legacy-migration.json`\n  - *(and 4 more)*\n\n**The Lesson:** Measure coverage to find gaps. The lesson? feat: Add comprehensive test coverage for query and analysis modules\n\n### Add unit tests for ML collector export, feedback, quality commands\n\n**Commit:** `1899ed8`  \n**Date:** 2025-12-15  \n**Files Changed:** 3  \n  - `tests/unit/test_ml_export.py`\n  - `tests/unit/test_ml_feedback.py`\n  - `tests/unit/test_ml_quality.py`\n**Changes:** +1842/-0 lines  \n\n**The Lesson:** Test what you build. The lesson? Add unit tests for ML collector export, feedback, quality commands\n\n### Add 16 code coverage improvement tasks\n\n**Commit:** `d0732b4`  \n**Date:** 2025-12-15  \n**Files Changed:** 1  \n  - `tasks/2025-12-15_05-23-36_ceac.json`\n**Changes:** +248/-0 lines  \n\n**The Lesson:** Measure coverage to find gaps. The lesson? Add 16 code coverage improvement tasks\n\n### Merge pull request #85 from scrawlsbenches/claude/fix-coverage-module-82miT\n\n**Commit:** `d09bbce`  \n**Date:** 2025-12-15  \n**Changes:** +23/-0 lines  \n\n**The Lesson:** Measure coverage to find gaps. The lesson? Merge pull request #85 from scrawlsbenches/claude/fix-coverage-module-82miT\n\n---\n\n*This chapter is part of [The Cortical Chronicles](../README.md), a self-documenting book generated by the Cortical Text Processor.*\n"
    },
    {
      "path": "07-concepts/bigram.md",
      "title": "Concept Evolution: Bigram",
      "section": "concepts",
      "tags": [
        "concept",
        "evolution",
        "bigram"
      ],
      "source_files": [
        "git log"
      ],
      "excerpt": "",
      "keywords": [
        "bigram",
        "concept",
        "add",
        "connections",
        "code",
        "review",
        "commits",
        "lateral",
        "first",
        "task"
      ],
      "full_content": "# Concept Evolution: Bigram\n\n*Tracking the emergence and growth of 'bigram' through commit history.*\n\n---\n\n## Birth\n\n**First Appearance:** 2025-12-09\n\nThe concept of 'bigram' first emerged in commit `50450d1`:\n\n> Add bigram lateral connections (Task 21) and code review concerns\n\n*By Claude*\n\n## Growth Timeline\n\nThe concept has been mentioned in **7 commits** across **1 months** of development.\n\n### December 2025: Emergence\n\n**7 commits** mentioning this concept.\n\n- `50450d1`: Add bigram lateral connections (Task 21) and code review concerns\n- `18f45ef`: Optimize semantic extraction and bigram connections (2x speedup)\n- `0f578c3`: Add expert code review: identify critical bigram bug and verify fixes\n- *(and 4 more)*\n\n## Peak Activity\n\nThe concept saw its most intensive development during week **2025-W49** with **7 commits**.\n\n## Related Concepts\n\nThe 'bigram' concept frequently appears alongside:\n\n- **Lateral Connections** (1 co-occurrences)\n- **Semantic** (1 co-occurrences)\n\n## The Concept Today\n\nMost recent mention was on 2025-12-11:\n\n> Add tests for bigram connection parameters and improve coverage to 90%\n\nThis concept has evolved from its initial appearance to become an **emerging aspect** of the design.\n\n## Key Commits\n\nNotable commits that shaped this concept:\n\n### First: `50450d1`\n\n**Date:** 2025-12-09\n\n**Message:** Add bigram lateral connections (Task 21) and code review concerns\n\n### Midpoint: `17e8147`\n\n**Date:** 2025-12-10\n\n**Message:** Add code review findings: critical bigram separator bugs\n\n### Latest: `1b9438e`\n\n**Date:** 2025-12-11\n\n**Message:** Add tests for bigram connection parameters and improve coverage to 90%\n\n"
    },
    {
      "path": "07-concepts/bm25.md",
      "title": "Concept Evolution: Bm25",
      "section": "concepts",
      "tags": [
        "concept",
        "evolution",
        "bm25"
      ],
      "source_files": [
        "git log"
      ],
      "excerpt": "",
      "keywords": [
        "bm25",
        "concept",
        "feat",
        "commits",
        "default",
        "add",
        "first",
        "implement",
        "scoring",
        "algorithm"
      ],
      "full_content": "# Concept Evolution: Bm25\n\n*Tracking the emergence and growth of 'bm25' through commit history.*\n\n---\n\n## Birth\n\n**First Appearance:** 2025-12-15\n\nThe concept of 'bm25' first emerged in commit `0a52858`:\n\n> feat: Implement BM25 scoring algorithm as default\n\n*By Claude*\n\n## Growth Timeline\n\nThe concept has been mentioned in **9 commits** across **1 months** of development.\n\n### December 2025: Emergence\n\n**9 commits** mentioning this concept.\n\n- `0a52858`: feat: Implement BM25 scoring algorithm as default\n- `fcce0c2`: feat: Optimize compute_all and add Graph-Boosted search (GB-BM25)\n- `63064c7`: docs: Add BM25/GB-BM25 documentation and tests\n- *(and 6 more)*\n\n## Peak Activity\n\nThe concept saw its most intensive development during week **2025-W50** with **9 commits**.\n\n## Related Concepts\n\nThe 'bm25' concept frequently appears alongside:\n\n- **Search** (2 co-occurrences)\n- **Graph** (1 co-occurrences)\n- **Tokenization** (1 co-occurrences)\n\n## The Concept Today\n\nMost recent mention was on 2025-12-16:\n\n> feat: Add chunked parallel processing for TF-IDF/BM25 (LEGACY-135)\n\nThis concept has evolved from its initial appearance to become an **emerging aspect** of the design.\n\n## Key Commits\n\nNotable commits that shaped this concept:\n\n### First: `0a52858`\n\n**Date:** 2025-12-15\n\n**Message:** feat: Implement BM25 scoring algorithm as default\n\n### Midpoint: `9dc7268`\n\n**Date:** 2025-12-15\n\n**Message:** fix: Update tests for BM25 default and stop word tokenization\n\n### Latest: `5665839`\n\n**Date:** 2025-12-16\n\n**Message:** feat: Add chunked parallel processing for TF-IDF/BM25 (LEGACY-135)\n\n"
    },
    {
      "path": "07-concepts/clustering.md",
      "title": "Concept Evolution: Clustering",
      "section": "concepts",
      "tags": [
        "concept",
        "evolution",
        "clustering"
      ],
      "source_files": [
        "git log"
      ],
      "excerpt": "",
      "keywords": [
        "concept",
        "clustering",
        "task",
        "commits",
        "first",
        "bf75e5d",
        "activate",
        "layer",
        "default",
        "date"
      ],
      "full_content": "# Concept Evolution: Clustering\n\n*Tracking the emergence and growth of 'clustering' through commit history.*\n\n---\n\n## Birth\n\n**First Appearance:** 2025-12-09\n\nThe concept of 'clustering' first emerged in commit `bf75e5d`:\n\n> Activate Layer 2 concept clustering by default (Task 10)\n\n*By Claude*\n\n## Growth Timeline\n\nThe concept has been mentioned in **7 commits** across **1 months** of development.\n\n### December 2025: Emergence\n\n**7 commits** mentioning this concept.\n\n- `bf75e5d`: Activate Layer 2 concept clustering by default (Task 10)\n- `e7933b6`: Implement Task 4: Improve clustering to reduce topic isolation\n- `0d24482`: Fix tests to not skip - provide sufficient data for concept clustering\n- *(and 4 more)*\n\n## Peak Activity\n\nThe concept saw its most intensive development during week **2025-W49** with **7 commits**.\n\n## The Concept Today\n\nMost recent mention was on 2025-12-12:\n\n> Task #143: Investigate negative silhouette score in clustering\n\nThis concept has evolved from its initial appearance to become an **emerging aspect** of the design.\n\n## Key Commits\n\nNotable commits that shaped this concept:\n\n### First: `bf75e5d`\n\n**Date:** 2025-12-09\n\n**Message:** Activate Layer 2 concept clustering by default (Task 10)\n\n### Midpoint: `bda9504`\n\n**Date:** 2025-12-11\n\n**Message:** Add critical clustering tasks #123-125 and regression tests (Task #124)\n\n### Latest: `2753132`\n\n**Date:** 2025-12-12\n\n**Message:** Task #143: Investigate negative silhouette score in clustering\n\n"
    },
    {
      "path": "07-concepts/context.md",
      "title": "Concept Evolution: Context",
      "section": "concepts",
      "tags": [
        "concept",
        "evolution",
        "context"
      ],
      "source_files": [
        "git log"
      ],
      "excerpt": "",
      "keywords": [
        "context",
        "concept",
        "add",
        "cli",
        "wrapper",
        "commits",
        "first",
        "a530bea",
        "framework",
        "collection"
      ],
      "full_content": "# Concept Evolution: Context\n\n*Tracking the emergence and growth of 'context' through commit history.*\n\n---\n\n## Birth\n\n**First Appearance:** 2025-12-11\n\nThe concept of 'context' first emerged in commit `a530bea`:\n\n> Add CLI wrapper framework for context collection and task triggers\n\n*By Claude*\n\n## Growth Timeline\n\nThe concept has been mentioned in **3 commits** across **1 months** of development.\n\n### December 2025: Emergence\n\n**3 commits** mentioning this concept.\n\n- `a530bea`: Add CLI wrapper framework for context collection and task triggers\n- `4e10104`: Merge pull request #37 from scrawlsbenches/claude/cli-wrapper-context-01JScUxQPSb4rGC2XhtXPSYB\n- `9bd4067`: feat: Add session handoff generator for context preservation\n\n## Peak Activity\n\nThe concept saw its most intensive development during week **2025-W49** with **2 commits**.\n\n## The Concept Today\n\nMost recent mention was on 2025-12-15:\n\n> feat: Add session handoff generator for context preservation\n\nThis concept has evolved from its initial appearance to become an **emerging aspect** of the design.\n\n## Key Commits\n\nNotable commits that shaped this concept:\n\n### First: `a530bea`\n\n**Date:** 2025-12-11\n\n**Message:** Add CLI wrapper framework for context collection and task triggers\n\n### Midpoint: `4e10104`\n\n**Date:** 2025-12-11\n\n**Message:** Merge pull request #37 from scrawlsbenches/claude/cli-wrapper-context-01JScUxQPSb4rGC2XhtXPSYB\n\n### Latest: `9bd4067`\n\n**Date:** 2025-12-15\n\n**Message:** feat: Add session handoff generator for context preservation\n\n"
    },
    {
      "path": "07-concepts/definition.md",
      "title": "Concept Evolution: Definition",
      "section": "concepts",
      "tags": [
        "concept",
        "evolution",
        "definition"
      ],
      "source_files": [
        "git log"
      ],
      "excerpt": "",
      "keywords": [
        "definition",
        "concept",
        "search",
        "task",
        "commits",
        "boost",
        "add",
        "first",
        "direct",
        "pattern"
      ],
      "full_content": "# Concept Evolution: Definition\n\n*Tracking the emergence and growth of 'definition' through commit history.*\n\n---\n\n## Birth\n\n**First Appearance:** 2025-12-11\n\nThe concept of 'definition' first emerged in commit `60c3483`:\n\n> Add direct definition pattern search for code search (Task #84)\n\n*By Claude*\n\n## Growth Timeline\n\nThe concept has been mentioned in **4 commits** across **1 months** of development.\n\n### December 2025: Emergence\n\n**4 commits** mentioning this concept.\n\n- `60c3483`: Add direct definition pattern search for code search (Task #84)\n- `66a4078`: Merge main, add task #128 for definition boost search quality issue\n- `d85cc90`: Fix definition boost to deprioritize test files over real implementations (Task #128)\n- *(and 1 more)*\n\n## Peak Activity\n\nThe concept saw its most intensive development during week **2025-W49** with **4 commits**.\n\n## Related Concepts\n\nThe 'definition' concept frequently appears alongside:\n\n- **Search** (2 co-occurrences)\n\n## The Concept Today\n\nMost recent mention was on 2025-12-11:\n\n> Mark Tasks #128, #132, #136 complete - definition boost and O(n\u00b2) fixes\n\nThis concept has evolved from its initial appearance to become an **emerging aspect** of the design.\n\n## Key Commits\n\nNotable commits that shaped this concept:\n\n### First: `60c3483`\n\n**Date:** 2025-12-11\n\n**Message:** Add direct definition pattern search for code search (Task #84)\n\n### Midpoint: `d85cc90`\n\n**Date:** 2025-12-11\n\n**Message:** Fix definition boost to deprioritize test files over real implementations (Task #128)\n\n### Latest: `0689785`\n\n**Date:** 2025-12-11\n\n**Message:** Mark Tasks #128, #132, #136 complete - definition boost and O(n\u00b2) fixes\n\n"
    },
    {
      "path": "07-concepts/embeddings.md",
      "title": "Concept Evolution: Embeddings",
      "section": "concepts",
      "tags": [
        "concept",
        "evolution",
        "embeddings"
      ],
      "source_files": [
        "git log"
      ],
      "excerpt": "",
      "keywords": [
        "concept",
        "embeddings",
        "task",
        "commits",
        "fix",
        "first",
        "persist",
        "full",
        "computed",
        "state"
      ],
      "full_content": "# Concept Evolution: Embeddings\n\n*Tracking the emergence and growth of 'embeddings' through commit history.*\n\n---\n\n## Birth\n\n**First Appearance:** 2025-12-09\n\nThe concept of 'embeddings' first emerged in commit `8f862b0`:\n\n> Persist full computed state including embeddings (Task 12)\n\n*By Claude*\n\n## Growth Timeline\n\nThe concept has been mentioned in **5 commits** across **1 months** of development.\n\n### December 2025: Emergence\n\n**5 commits** mentioning this concept.\n\n- `8f862b0`: Persist full computed state including embeddings (Task 12)\n- `6cb35d6`: Add critical task #122: Investigate Concept Layer & Embeddings regressions\n- `919e8a7`: Fix cluster_strictness inversion and improve embeddings (Task #122)\n- *(and 2 more)*\n\n## Peak Activity\n\nThe concept saw its most intensive development during week **2025-W49** with **5 commits**.\n\n## The Concept Today\n\nMost recent mention was on 2025-12-13:\n\n> Fix test_retrofit_embeddings_invalid_alpha_zero to match new validation\n\nThis concept has evolved from its initial appearance to become an **emerging aspect** of the design.\n\n## Key Commits\n\nNotable commits that shaped this concept:\n\n### First: `8f862b0`\n\n**Date:** 2025-12-09\n\n**Message:** Persist full computed state including embeddings (Task 12)\n\n### Midpoint: `919e8a7`\n\n**Date:** 2025-12-11\n\n**Message:** Fix cluster_strictness inversion and improve embeddings (Task #122)\n\n### Latest: `69c206b`\n\n**Date:** 2025-12-13\n\n**Message:** Fix test_retrofit_embeddings_invalid_alpha_zero to match new validation\n\n"
    },
    {
      "path": "07-concepts/graph.md",
      "title": "Concept Evolution: Graph",
      "section": "concepts",
      "tags": [
        "concept",
        "evolution",
        "graph"
      ],
      "source_files": [
        "git log"
      ],
      "excerpt": "",
      "keywords": [
        "graph",
        "concept",
        "add",
        "task",
        "commits",
        "search",
        "bm25",
        "first",
        "e40a80c",
        "conceptnet"
      ],
      "full_content": "# Concept Evolution: Graph\n\n*Tracking the emergence and growth of 'graph' through commit history.*\n\n---\n\n## Birth\n\n**First Appearance:** 2025-12-10\n\nThe concept of 'graph' first emerged in commit `e40a80c`:\n\n> Add ConceptNet-style graph visualization export (Task 29)\n\n*By Claude*\n\n## Growth Timeline\n\nThe concept has been mentioned in **3 commits** across **1 months** of development.\n\n### December 2025: Emergence\n\n**3 commits** mentioning this concept.\n\n- `e40a80c`: Add ConceptNet-style graph visualization export (Task 29)\n- `e5dd3d5`: Task #145: Improve graph embedding quality for common terms\n- `fcce0c2`: feat: Optimize compute_all and add Graph-Boosted search (GB-BM25)\n\n## Peak Activity\n\nThe concept saw its most intensive development during week **2025-W49** with **2 commits**.\n\n## Related Concepts\n\nThe 'graph' concept frequently appears alongside:\n\n- **Bm25** (1 co-occurrences)\n- **Search** (1 co-occurrences)\n\n## The Concept Today\n\nMost recent mention was on 2025-12-15:\n\n> feat: Optimize compute_all and add Graph-Boosted search (GB-BM25)\n\nThis concept has evolved from its initial appearance to become an **emerging aspect** of the design.\n\n## Key Commits\n\nNotable commits that shaped this concept:\n\n### First: `e40a80c`\n\n**Date:** 2025-12-10\n\n**Message:** Add ConceptNet-style graph visualization export (Task 29)\n\n### Midpoint: `e5dd3d5`\n\n**Date:** 2025-12-12\n\n**Message:** Task #145: Improve graph embedding quality for common terms\n\n### Latest: `fcce0c2`\n\n**Date:** 2025-12-15\n\n**Message:** feat: Optimize compute_all and add Graph-Boosted search (GB-BM25)\n\n"
    },
    {
      "path": "07-concepts/incremental.md",
      "title": "Concept Evolution: Incremental",
      "section": "concepts",
      "tags": [
        "concept",
        "evolution",
        "incremental"
      ],
      "source_files": [
        "git log"
      ],
      "excerpt": "",
      "keywords": [
        "incremental",
        "concept",
        "add",
        "indexing",
        "commits",
        "first",
        "document",
        "task",
        "date",
        "message"
      ],
      "full_content": "# Concept Evolution: Incremental\n\n*Tracking the emergence and growth of 'incremental' through commit history.*\n\n---\n\n## Birth\n\n**First Appearance:** 2025-12-09\n\nThe concept of 'incremental' first emerged in commit `38fb4f7`:\n\n> Add incremental document indexing (Task 15)\n\n*By Claude*\n\n## Growth Timeline\n\nThe concept has been mentioned in **5 commits** across **1 months** of development.\n\n### December 2025: Emergence\n\n**5 commits** mentioning this concept.\n\n- `38fb4f7`: Add incremental document indexing (Task 15)\n- `3682739`: Add incremental codebase indexing with progress tracking\n- `b360793`: Update documentation for incremental indexing features\n- *(and 2 more)*\n\n## Peak Activity\n\nThe concept saw its most intensive development during week **2025-W49** with **5 commits**.\n\n## The Concept Today\n\nMost recent mention was on 2025-12-11:\n\n> Add incremental batch mode for full analysis\n\nThis concept has evolved from its initial appearance to become an **emerging aspect** of the design.\n\n## Key Commits\n\nNotable commits that shaped this concept:\n\n### First: `38fb4f7`\n\n**Date:** 2025-12-09\n\n**Message:** Add incremental document indexing (Task 15)\n\n### Midpoint: `b360793`\n\n**Date:** 2025-12-10\n\n**Message:** Update documentation for incremental indexing features\n\n### Latest: `256e842`\n\n**Date:** 2025-12-11\n\n**Message:** Add incremental batch mode for full analysis\n\n"
    },
    {
      "path": "07-concepts/index.md",
      "title": "Concept Evolution Index",
      "section": "concepts",
      "tags": [
        "concepts",
        "index",
        "evolution"
      ],
      "source_files": [
        "git log"
      ],
      "excerpt": "",
      "keywords": [
        "concept",
        "first",
        "commits",
        "mentions",
        "seen",
        "emerging",
        "recent",
        "development",
        "related",
        "search"
      ],
      "full_content": "# Concept Evolution Index\n\n*A guide to how key concepts emerged and grew in the Cortical Text Processor.*\n\n---\n\n## Overview\n\nThis section tracks the evolution of **14 core concepts** through the project's commit history. Each concept chapter shows:\n\n- When the concept first appeared\n- How it grew over time\n- Related concepts and connections\n- Current state and importance\n\n## Concepts by Importance\n\n### [Search](search.md)\n\n**Mentions:** 26 commits\n\n**First seen:** 2025-12-10\n\n**Related to:** Passage, Definition, Louvain\n\n*A core component of the system architecture.*\n\n### [Semantic](semantic.md)\n\n**Mentions:** 9 commits\n\n**First seen:** 2025-12-09\n\n**Related to:** Retrieval, Bigram, Fingerprint\n\n*An emerging concept in recent development.*\n\n### [Bm25](bm25.md)\n\n**Mentions:** 9 commits\n\n**First seen:** 2025-12-15\n\n**Related to:** Search, Graph, Tokenization\n\n*An emerging concept in recent development.*\n\n### [Clustering](clustering.md)\n\n**Mentions:** 7 commits\n\n**First seen:** 2025-12-09\n\n*An emerging concept in recent development.*\n\n### [Bigram](bigram.md)\n\n**Mentions:** 7 commits\n\n**First seen:** 2025-12-09\n\n**Related to:** Lateral Connections, Semantic\n\n*An emerging concept in recent development.*\n\n### [Louvain](louvain.md)\n\n**Mentions:** 7 commits\n\n**First seen:** 2025-12-11\n\n**Related to:** Search\n\n*An emerging concept in recent development.*\n\n### [Embeddings](embeddings.md)\n\n**Mentions:** 5 commits\n\n**First seen:** 2025-12-09\n\n*An emerging concept in recent development.*\n\n### [Incremental](incremental.md)\n\n**Mentions:** 5 commits\n\n**First seen:** 2025-12-09\n\n*An emerging concept in recent development.*\n\n### [Pagerank](pagerank.md)\n\n**Mentions:** 4 commits\n\n**First seen:** 2025-12-09\n\n*An emerging concept in recent development.*\n\n### [Query Expansion](query-expansion.md)\n\n**Mentions:** 4 commits\n\n**First seen:** 2025-12-10\n\n*An emerging concept in recent development.*\n\n### [Definition](definition.md)\n\n**Mentions:** 4 commits\n\n**First seen:** 2025-12-11\n\n**Related to:** Search\n\n*An emerging concept in recent development.*\n\n### [Graph](graph.md)\n\n**Mentions:** 3 commits\n\n**First seen:** 2025-12-10\n\n**Related to:** Bm25, Search\n\n*An emerging concept in recent development.*\n\n### [Tokenization](tokenization.md)\n\n**Mentions:** 3 commits\n\n**First seen:** 2025-12-10\n\n**Related to:** Bm25\n\n*An emerging concept in recent development.*\n\n### [Context](context.md)\n\n**Mentions:** 3 commits\n\n**First seen:** 2025-12-11\n\n*An emerging concept in recent development.*\n\n---\n\n*Each concept chapter provides detailed evolution timeline and key commits.*\n"
    },
    {
      "path": "07-concepts/louvain.md",
      "title": "Concept Evolution: Louvain",
      "section": "concepts",
      "tags": [
        "concept",
        "evolution",
        "louvain"
      ],
      "source_files": [
        "git log"
      ],
      "excerpt": "",
      "keywords": [
        "louvain",
        "concept",
        "task",
        "commits",
        "community",
        "detection",
        "first",
        "implement",
        "add",
        "parameter"
      ],
      "full_content": "# Concept Evolution: Louvain\n\n*Tracking the emergence and growth of 'louvain' through commit history.*\n\n---\n\n## Birth\n\n**First Appearance:** 2025-12-11\n\nThe concept of 'louvain' first emerged in commit `62c7fdf`:\n\n> Implement Louvain community detection (Task #123)\n\n*By Claude*\n\n## Growth Timeline\n\nThe concept has been mentioned in **7 commits** across **1 months** of development.\n\n### December 2025: Emergence\n\n**7 commits** mentioning this concept.\n\n- `62c7fdf`: Implement Louvain community detection (Task #123)\n- `b2b7f92`: Add Task #126: Investigate optimal Louvain resolution\n- `e85c299`: Merge pull request #34 from scrawlsbenches/claude/louvain-community-detection-01FsvWk3GKjFLpEiPwQT4sBc\n- *(and 4 more)*\n\n## Peak Activity\n\nThe concept saw its most intensive development during week **2025-W49** with **7 commits**.\n\n## Related Concepts\n\nThe 'louvain' concept frequently appears alongside:\n\n- **Search** (2 co-occurrences)\n\n## The Concept Today\n\nMost recent mention was on 2025-12-13:\n\n> Add louvain_resolution parameter to CorticalConfig\n\nThis concept has evolved from its initial appearance to become an **emerging aspect** of the design.\n\n## Key Commits\n\nNotable commits that shaped this concept:\n\n### First: `62c7fdf`\n\n**Date:** 2025-12-11\n\n**Message:** Implement Louvain community detection (Task #123)\n\n### Midpoint: `dda7d0c`\n\n**Date:** 2025-12-11\n\n**Message:** Complete Task #126: Louvain resolution parameter research\n\n### Latest: `a47bb61`\n\n**Date:** 2025-12-13\n\n**Message:** Add louvain_resolution parameter to CorticalConfig\n\n"
    },
    {
      "path": "07-concepts/pagerank.md",
      "title": "Concept Evolution: Pagerank",
      "section": "concepts",
      "tags": [
        "concept",
        "evolution",
        "pagerank"
      ],
      "source_files": [
        "git log"
      ],
      "excerpt": "",
      "keywords": [
        "pagerank",
        "concept",
        "add",
        "task",
        "conceptnet",
        "commits",
        "first",
        "c6eefdc",
        "enhanced",
        "list"
      ],
      "full_content": "# Concept Evolution: Pagerank\n\n*Tracking the emergence and growth of 'pagerank' through commit history.*\n\n---\n\n## Birth\n\n**First Appearance:** 2025-12-09\n\nThe concept of 'pagerank' first emerged in commit `c6eefdc`:\n\n> Add ConceptNet-enhanced PageRank task list (Tasks 19-30)\n\n*By Claude*\n\n## Growth Timeline\n\nThe concept has been mentioned in **4 commits** across **1 months** of development.\n\n### December 2025: Emergence\n\n**4 commits** mentioning this concept.\n\n- `c6eefdc`: Add ConceptNet-enhanced PageRank task list (Tasks 19-30)\n- `f6b8389`: Add relation-weighted PageRank (Task 22)\n- `cc57677`: Add cross-layer PageRank propagation (Task 23)\n- *(and 1 more)*\n\n## Peak Activity\n\nThe concept saw its most intensive development during week **2025-W49** with **4 commits**.\n\n## The Concept Today\n\nMost recent mention was on 2025-12-10:\n\n> Add overlapping PageRank, ConceptNet, and Neocortex samples\n\nThis concept has evolved from its initial appearance to become an **emerging aspect** of the design.\n\n## Key Commits\n\nNotable commits that shaped this concept:\n\n### First: `c6eefdc`\n\n**Date:** 2025-12-09\n\n**Message:** Add ConceptNet-enhanced PageRank task list (Tasks 19-30)\n\n### Midpoint: `cc57677`\n\n**Date:** 2025-12-10\n\n**Message:** Add cross-layer PageRank propagation (Task 23)\n\n### Latest: `1e4d35d`\n\n**Date:** 2025-12-10\n\n**Message:** Add overlapping PageRank, ConceptNet, and Neocortex samples\n\n"
    },
    {
      "path": "07-concepts/query-expansion.md",
      "title": "Concept Evolution: Query Expansion",
      "section": "concepts",
      "tags": [
        "concept",
        "evolution",
        "query-expansion"
      ],
      "source_files": [
        "git log"
      ],
      "excerpt": "",
      "keywords": [
        "query",
        "expansion",
        "concept",
        "commits",
        "first",
        "document",
        "magic",
        "numbers",
        "extract",
        "helper"
      ],
      "full_content": "# Concept Evolution: Query Expansion\n\n*Tracking the emergence and growth of 'query expansion' through commit history.*\n\n---\n\n## Birth\n\n**First Appearance:** 2025-12-10\n\nThe concept of 'query expansion' first emerged in commit `16c13a0`:\n\n> Document magic numbers and extract query expansion helper\n\n*By Claude*\n\n## Growth Timeline\n\nThe concept has been mentioned in **4 commits** across **1 months** of development.\n\n### December 2025: Emergence\n\n**4 commits** mentioning this concept.\n\n- `16c13a0`: Document magic numbers and extract query expansion helper\n- `a819131`: Add LRU cache for query expansion results (Task #45)\n- `af3a7e0`: feat: Add security concept group and TF-IDF weighted query expansion\n- *(and 1 more)*\n\n## Peak Activity\n\nThe concept saw its most intensive development during week **2025-W49** with **2 commits**.\n\n## The Concept Today\n\nMost recent mention was on 2025-12-15:\n\n> fix: Cap query expansion weights to prevent term domination\n\nThis concept has evolved from its initial appearance to become an **emerging aspect** of the design.\n\n## Key Commits\n\nNotable commits that shaped this concept:\n\n### First: `16c13a0`\n\n**Date:** 2025-12-10\n\n**Message:** Document magic numbers and extract query expansion helper\n\n### Midpoint: `af3a7e0`\n\n**Date:** 2025-12-15\n\n**Message:** feat: Add security concept group and TF-IDF weighted query expansion\n\n### Latest: `fecd6dc`\n\n**Date:** 2025-12-15\n\n**Message:** fix: Cap query expansion weights to prevent term domination\n\n"
    },
    {
      "path": "07-concepts/search.md",
      "title": "Concept Evolution: Search",
      "section": "concepts",
      "tags": [
        "concept",
        "evolution",
        "search"
      ],
      "source_files": [
        "git log"
      ],
      "excerpt": "",
      "keywords": [
        "search",
        "concept",
        "commits",
        "add",
        "occurrences",
        "system",
        "task",
        "code",
        "first",
        "dc6db89"
      ],
      "full_content": "# Concept Evolution: Search\n\n*Tracking the emergence and growth of 'search' through commit history.*\n\n---\n\n## Birth\n\n**First Appearance:** 2025-12-10\n\nThe concept of 'search' first emerged in commit `dc6db89`:\n\n> Implement dog-fooding: search codebase with its own IR system (Task #47)\n\n*By Claude*\n\n## Growth Timeline\n\nThe concept has been mentioned in **26 commits** across **1 months** of development.\n\n### December 2025: Emergence\n\n**26 commits** mentioning this concept.\n\n- `dc6db89`: Implement dog-fooding: search codebase with its own IR system (Task #47)\n- `975fc91`: Add intent-based code search enhancement tasks (#48-52)\n- `2ad03ed`: Add query optimization for faster code search (Task #52)\n- *(and 23 more)*\n\n## Peak Activity\n\nThe concept saw its most intensive development during week **2025-W49** with **20 commits**.\n\n## Related Concepts\n\nThe 'search' concept frequently appears alongside:\n\n- **Passage** (2 co-occurrences)\n- **Definition** (2 co-occurrences)\n- **Louvain** (2 co-occurrences)\n- **Bm25** (2 co-occurrences)\n- **Intent** (1 co-occurrences)\n\n## The Concept Today\n\nMost recent mention was on 2025-12-16:\n\n> feat: Add search integration and web interface (Wave 3)\n\nThis concept has evolved from its initial appearance to become a **core component** of the system.\n\n## Key Commits\n\nNotable commits that shaped this concept:\n\n### First: `dc6db89`\n\n**Date:** 2025-12-10\n\n**Message:** Implement dog-fooding: search codebase with its own IR system (Task #47)\n\n### Midpoint: `0f75675`\n\n**Date:** 2025-12-11\n\n**Message:** Add Python code samples and update showcase for code search features\n\n### Latest: `0022466`\n\n**Date:** 2025-12-16\n\n**Message:** feat: Add search integration and web interface (Wave 3)\n\n"
    },
    {
      "path": "07-concepts/semantic.md",
      "title": "Concept Evolution: Semantic",
      "section": "concepts",
      "tags": [
        "concept",
        "evolution",
        "semantic"
      ],
      "source_files": [
        "git log"
      ],
      "excerpt": "",
      "keywords": [
        "semantic",
        "concept",
        "commits",
        "retrieval",
        "task",
        "add",
        "occurrences",
        "first",
        "f27d18e",
        "integrate"
      ],
      "full_content": "# Concept Evolution: Semantic\n\n*Tracking the emergence and growth of 'semantic' through commit history.*\n\n---\n\n## Birth\n\n**First Appearance:** 2025-12-09\n\nThe concept of 'semantic' first emerged in commit `f27d18e`:\n\n> Integrate semantic relations into retrieval (Task 11)\n\n*By Claude*\n\n## Growth Timeline\n\nThe concept has been mentioned in **9 commits** across **1 months** of development.\n\n### December 2025: Emergence\n\n**9 commits** mentioning this concept.\n\n- `f27d18e`: Integrate semantic relations into retrieval (Task 11)\n- `4e113e7`: Add multi-hop semantic inference (Tasks 25-26)\n- `18f45ef`: Optimize semantic extraction and bigram connections (2x speedup)\n- *(and 6 more)*\n\n## Peak Activity\n\nThe concept saw its most intensive development during week **2025-W49** with **7 commits**.\n\n## Related Concepts\n\nThe 'semantic' concept frequently appears alongside:\n\n- **Retrieval** (1 co-occurrences)\n- **Bigram** (1 co-occurrences)\n- **Fingerprint** (1 co-occurrences)\n- **Similarity** (1 co-occurrences)\n\n## The Concept Today\n\nMost recent mention was on 2025-12-14:\n\n> feat: Add \"What Changed?\" semantic diff (LEGACY-075)\n\nThis concept has evolved from its initial appearance to become an **emerging aspect** of the design.\n\n## Key Commits\n\nNotable commits that shaped this concept:\n\n### First: `f27d18e`\n\n**Date:** 2025-12-09\n\n**Message:** Integrate semantic relations into retrieval (Task 11)\n\n### Midpoint: `626c008`\n\n**Date:** 2025-12-11\n\n**Message:** Add semantic chunk boundaries for code (Task #86)\n\n### Latest: `a31d1c7`\n\n**Date:** 2025-12-14\n\n**Message:** feat: Add \"What Changed?\" semantic diff (LEGACY-075)\n\n"
    },
    {
      "path": "07-concepts/tokenization.md",
      "title": "Concept Evolution: Tokenization",
      "section": "concepts",
      "tags": [
        "concept",
        "evolution",
        "tokenization"
      ],
      "source_files": [
        "git log"
      ],
      "excerpt": "",
      "keywords": [
        "tokenization",
        "concept",
        "fix",
        "polysemy",
        "commits",
        "bm25",
        "first",
        "d2b42d9",
        "section",
        "clarify"
      ],
      "full_content": "# Concept Evolution: Tokenization\n\n*Tracking the emergence and growth of 'tokenization' through commit history.*\n\n---\n\n## Birth\n\n**First Appearance:** 2025-12-10\n\nThe concept of 'tokenization' first emerged in commit `d2b42d9`:\n\n> Fix polysemy section: clarify tokenization vs actual polysemy\n\n*By Claude*\n\n## Growth Timeline\n\nThe concept has been mentioned in **3 commits** across **1 months** of development.\n\n### December 2025: Emergence\n\n**3 commits** mentioning this concept.\n\n- `d2b42d9`: Fix polysemy section: clarify tokenization vs actual polysemy\n- `2571bb8`: Add code-aware tokenization with identifier splitting (Task #48)\n- `9dc7268`: fix: Update tests for BM25 default and stop word tokenization\n\n## Peak Activity\n\nThe concept saw its most intensive development during week **2025-W49** with **2 commits**.\n\n## Related Concepts\n\nThe 'tokenization' concept frequently appears alongside:\n\n- **Bm25** (1 co-occurrences)\n\n## The Concept Today\n\nMost recent mention was on 2025-12-15:\n\n> fix: Update tests for BM25 default and stop word tokenization\n\nThis concept has evolved from its initial appearance to become an **emerging aspect** of the design.\n\n## Key Commits\n\nNotable commits that shaped this concept:\n\n### First: `d2b42d9`\n\n**Date:** 2025-12-10\n\n**Message:** Fix polysemy section: clarify tokenization vs actual polysemy\n\n### Midpoint: `2571bb8`\n\n**Date:** 2025-12-10\n\n**Message:** Add code-aware tokenization with identifier splitting (Task #48)\n\n### Latest: `9dc7268`\n\n**Date:** 2025-12-15\n\n**Message:** fix: Update tests for BM25 default and stop word tokenization\n\n"
    },
    {
      "path": "08-exercises/ex-advanced.md",
      "title": "Exercises: Advanced",
      "section": "exercises",
      "tags": [
        "exercises",
        "advanced",
        "advanced"
      ],
      "source_files": [
        "test_semantics.py",
        "test_fingerprint.py",
        "test_embeddings.py",
        "test_gaps.py",
        "test_patterns.py"
      ],
      "excerpt": "",
      "keywords": [
        "details",
        "summary",
        "empty",
        "relations",
        "python",
        "assert",
        "hint",
        "result",
        "dog",
        "isa"
      ],
      "full_content": "# Advanced Exercises\n\n*Hands-on coding exercises to master advanced concepts.*\n\n**Difficulty Level:** Advanced\n\n---\n\n## Introduction\n\nChallenge yourself with advanced features:\n\n- Semantic relation extraction\n- Fingerprint-based similarity\n- Graph embeddings\n- Knowledge gap detection\n\n## Exercise: Empty Documents\n\n**Concept:** Empty documents return no relations\n\n**Difficulty:** Advanced\n\n**Time:** ~20 minutes\n\n**Source:** `test_semantics.py`\n\n### Setup\n\n```python\nfrom cortical import CorticalTextProcessor\n```\n\n### Your Task\n\nEmpty documents return no relations.\n\n### Hints\n\n<details>\n<summary>Hint 1</summary>\n\nStart by considering the edge case of empty input.\n\n</details>\n\n<details>\n<summary>Hint 2</summary>\n\nCheck the expected value and comparison in the assertion\n\n</details>\n\n### Solution\n\n<details>\n<summary>Click to reveal</summary>\n\n```python\ndef test_empty_documents(self):\n        \"\"\"Empty documents return no relations.\"\"\"\n        result = extract_pattern_relations({}, {\"term1\", \"term2\"})\n        assert result == []\n```\n\n</details>\n\n---\n\n## Exercise: Empty Valid Terms\n\n**Concept:** No valid terms means no relations extracted\n\n**Difficulty:** Advanced\n\n**Time:** ~20 minutes\n\n**Source:** `test_semantics.py`\n\n### Setup\n\n```python\nfrom cortical import CorticalTextProcessor\n```\n\n### Your Task\n\nNo valid terms means no relations extracted.\n\n### Hints\n\n<details>\n<summary>Hint 1</summary>\n\nStart by considering the edge case of empty input.\n\n</details>\n\n<details>\n<summary>Hint 2</summary>\n\nCheck the expected value and comparison in the assertion\n\n</details>\n\n### Solution\n\n<details>\n<summary>Click to reveal</summary>\n\n```python\ndef test_empty_valid_terms(self):\n        \"\"\"No valid terms means no relations extracted.\"\"\"\n        docs = {\"doc1\": \"A dog is an animal.\"}\n        result = extract_pattern_relations(docs, set())\n        assert result == []\n```\n\n</details>\n\n---\n\n## Exercise: Empty Relations\n\n**Concept:** Empty relations list\n\n**Difficulty:** Advanced\n\n**Time:** ~20 minutes\n\n**Source:** `test_semantics.py`\n\n### Setup\n\n```python\nfrom cortical import CorticalTextProcessor\n```\n\n### Your Task\n\nEmpty relations list.\n\n### Hints\n\n<details>\n<summary>Hint 1</summary>\n\nStart by considering the edge case of empty input.\n\n</details>\n\n<details>\n<summary>Hint 2</summary>\n\nCheck the expected value and comparison in the assertion\n\n</details>\n\n### Solution\n\n<details>\n<summary>Click to reveal</summary>\n\n```python\ndef test_empty_relations(self):\n        \"\"\"Empty relations list.\"\"\"\n        result = get_pattern_statistics([])\n        assert result[\"total_relations\"] == 0\n        assert result[\"relation_type_counts\"] == {}\n```\n\n</details>\n\n---\n\n## Exercise: Single Relation\n\n**Concept:** Single relation statistics\n\n**Difficulty:** Advanced\n\n**Time:** ~20 minutes\n\n**Source:** `test_semantics.py`\n\n### Setup\n\n```python\nfrom cortical import CorticalTextProcessor\n```\n\n### Your Task\n\nSingle relation statistics.\n\n### Hints\n\n<details>\n<summary>Hint 1</summary>\n\nFocus on the simplest case with just one element.\n\n</details>\n\n<details>\n<summary>Hint 2</summary>\n\nCheck the expected value and comparison in the assertion\n\n</details>\n\n### Solution\n\n<details>\n<summary>Click to reveal</summary>\n\n```python\ndef test_single_relation(self):\n        \"\"\"Single relation statistics.\"\"\"\n        relations = [(\"dog\", \"IsA\", \"animal\", 0.9)]\n        result = get_pattern_statistics(relations)\n        assert result[\"total_relations\"] == 1\n        assert result[\"relation_type_counts\"][\"IsA\"] == 1\n```\n\n</details>\n\n---\n\n## Exercise: Empty Relations\n\n**Concept:** Empty relations produce empty hierarchy\n\n**Difficulty:** Advanced\n\n**Time:** ~20 minutes\n\n**Source:** `test_semantics.py`\n\n### Setup\n\n```python\nfrom cortical import CorticalTextProcessor\n```\n\n### Your Task\n\nEmpty relations produce empty hierarchy.\n\n### Hints\n\n<details>\n<summary>Hint 1</summary>\n\nStart by considering the edge case of empty input.\n\n</details>\n\n<details>\n<summary>Hint 2</summary>\n\nCheck the expected value and comparison in the assertion\n\n</details>\n\n### Solution\n\n<details>\n<summary>Click to reveal</summary>\n\n```python\ndef test_empty_relations(self):\n        \"\"\"Empty relations produce empty hierarchy.\"\"\"\n        parents, children = build_isa_hierarchy([])\n        assert parents == {}\n        assert children == {}\n```\n\n</details>\n\n---\n\n## Exercise: Single Isa\n\n**Concept:** Single IsA relation creates parent-child\n\n**Difficulty:** Advanced\n\n**Time:** ~20 minutes\n\n**Source:** `test_semantics.py`\n\n### Setup\n\n```python\nfrom cortical import CorticalTextProcessor\n```\n\n### Your Task\n\nSingle IsA relation creates parent-child.\n\n### Hints\n\n<details>\n<summary>Hint 1</summary>\n\nFocus on the simplest case with just one element.\n\n</details>\n\n### Solution\n\n<details>\n<summary>Click to reveal</summary>\n\n```python\ndef test_single_isa(self):\n        \"\"\"Single IsA relation creates parent-child.\"\"\"\n        relations = [(\"dog\", \"IsA\", \"animal\", 0.9)]\n        parents, children = build_isa_hierarchy(relations)\n        assert \"dog\" in parents\n        assert \"animal\" in parents[\"dog\"]\n        assert \"animal\" in children\n        assert \"dog\" in children[\"animal\"]\n```\n\n</details>\n\n---\n\n## Exercise: Hierarchy Chain\n\n**Concept:** Chain: poodle IsA dog IsA animal\n\n**Difficulty:** Advanced\n\n**Time:** ~20 minutes\n\n**Source:** `test_semantics.py`\n\n### Setup\n\n```python\nfrom cortical import CorticalTextProcessor\n```\n\n### Your Task\n\nChain: poodle IsA dog IsA animal.\n\n### Hints\n\n<details>\n<summary>Hint 1</summary>\n\nThink about how elements connect in sequence.\n\n</details>\n\n### Solution\n\n<details>\n<summary>Click to reveal</summary>\n\n```python\ndef test_hierarchy_chain(self):\n        \"\"\"Chain: poodle IsA dog IsA animal.\"\"\"\n        relations = [\n            (\"poodle\", \"IsA\", \"dog\", 0.9),\n            (\"dog\", \"IsA\", \"animal\", 0.9)\n        ]\n        parents, children = build_isa_hierarchy(relations)\n        assert \"poodle\" in parents\n        assert \"dog\" in parents[\"poodle\"]\n        assert \"dog\" in parents\n        assert \"animal\" in parents[\"dog\"]\n```\n\n</details>\n\n---\n\n## Exercise: Empty Hierarchy\n\n**Concept:** Empty hierarchy returns empty ancestors\n\n**Difficulty:** Advanced\n\n**Time:** ~20 minutes\n\n**Source:** `test_semantics.py`\n\n### Setup\n\n```python\nfrom cortical import CorticalTextProcessor\n```\n\n### Your Task\n\nEmpty hierarchy returns empty ancestors.\n\n### Hints\n\n<details>\n<summary>Hint 1</summary>\n\nStart by considering the edge case of empty input.\n\n</details>\n\n<details>\n<summary>Hint 2</summary>\n\nCheck the expected value and comparison in the assertion\n\n</details>\n\n### Solution\n\n<details>\n<summary>Click to reveal</summary>\n\n```python\ndef test_empty_hierarchy(self):\n        \"\"\"Empty hierarchy returns empty ancestors.\"\"\"\n        result = get_ancestors(\"dog\", {})\n        assert result == {}\n```\n\n</details>\n\n---\n\n## Exercise: Empty Hierarchy\n\n**Concept:** Empty children dict returns empty descendants\n\n**Difficulty:** Advanced\n\n**Time:** ~20 minutes\n\n**Source:** `test_semantics.py`\n\n### Setup\n\n```python\nfrom cortical import CorticalTextProcessor\n```\n\n### Your Task\n\nEmpty children dict returns empty descendants.\n\n### Hints\n\n<details>\n<summary>Hint 1</summary>\n\nStart by considering the edge case of empty input.\n\n</details>\n\n<details>\n<summary>Hint 2</summary>\n\nCheck the expected value and comparison in the assertion\n\n</details>\n\n### Solution\n\n<details>\n<summary>Click to reveal</summary>\n\n```python\ndef test_empty_hierarchy(self):\n        \"\"\"Empty children dict returns empty descendants.\"\"\"\n        result = get_descendants(\"animal\", {})\n        assert result == {}\n```\n\n</details>\n\n---\n\n## Exercise: Empty Corpus\n\n**Concept:** Empty corpus returns no relations\n\n**Difficulty:** Advanced\n\n**Time:** ~20 minutes\n\n**Source:** `test_semantics.py`\n\n### Setup\n\n```python\nfrom cortical import CorticalTextProcessor\nfrom cortical.tokenizer import Tokenizer\n```\n\n### Your Task\n\nEmpty corpus returns no relations.\n\n### Hints\n\n<details>\n<summary>Hint 1</summary>\n\nStart by considering the edge case of empty input.\n\n</details>\n\n<details>\n<summary>Hint 2</summary>\n\nCheck the expected value and comparison in the assertion\n\n</details>\n\n### Solution\n\n<details>\n<summary>Click to reveal</summary>\n\n```python\ndef test_empty_corpus(self):\n        \"\"\"Empty corpus returns no relations.\"\"\"\n        layers = {CorticalLayer.TOKENS: HierarchicalLayer(CorticalLayer.TOKENS)}\n        tokenizer = Tokenizer()\n        result = extract_corpus_semantics(layers, {}, tokenizer)\n        assert result == []\n```\n\n</details>\n\n---\n\n---\n\n*Completed 10 exercises? Check out the other topics for more challenges!*\n"
    },
    {
      "path": "08-exercises/ex-foundations.md",
      "title": "Exercises: Foundations",
      "section": "exercises",
      "tags": [
        "exercises",
        "foundations",
        "beginner"
      ],
      "source_files": [
        "test_analysis.py",
        "test_layers.py",
        "test_tokenizer.py",
        "test_minicolumn.py"
      ],
      "excerpt": "",
      "keywords": [
        "details",
        "summary",
        "empty",
        "result",
        "hint",
        "graph",
        "python",
        "single",
        "self",
        "assert"
      ],
      "full_content": "# Foundations Exercises\n\n*Hands-on coding exercises to master foundations concepts.*\n\n**Difficulty Level:** Beginner\n\n---\n\n## Introduction\n\nThese exercises cover the fundamental algorithms and data structures of the Cortical Text Processor:\n\n- PageRank for term importance\n- TF-IDF for relevance scoring\n- Graph structures and connections\n- Tokenization and text processing\n\n## Exercise: Empty Graph\n\n**Concept:** Empty graph returns empty dict\n\n**Difficulty:** Beginner\n\n**Time:** ~10 minutes\n\n**Source:** `test_analysis.py`\n\n### Setup\n\n```python\nfrom cortical import CorticalTextProcessor\n```\n\n### Your Task\n\nEmpty graph returns empty dict.\n\n### Hints\n\n<details>\n<summary>Hint 1</summary>\n\nStart by considering the edge case of empty input.\n\n</details>\n\n<details>\n<summary>Hint 2</summary>\n\nPageRank is computed with `compute_pagerank()` or `compute_importance()`\n\n</details>\n\n<details>\n<summary>Hint 3</summary>\n\nCheck the expected value and comparison in the assertion\n\n</details>\n\n### Solution\n\n<details>\n<summary>Click to reveal</summary>\n\n```python\ndef test_empty_graph(self):\n        \"\"\"Empty graph returns empty dict.\"\"\"\n        result = _pagerank_core({})\n        assert result == {}\n```\n\n</details>\n\n---\n\n## Exercise: Single Node No Edges\n\n**Concept:** Single node with no edges gets base rank from damping\n\n**Difficulty:** Beginner\n\n**Time:** ~20 minutes\n\n**Source:** `test_analysis.py`\n\n### Setup\n\n```python\nfrom cortical import CorticalTextProcessor\n```\n\n### Your Task\n\nSingle node with no edges gets base rank from damping.\n\n### Hints\n\n<details>\n<summary>Hint 1</summary>\n\nFocus on the simplest case with just one element.\n\n</details>\n\n<details>\n<summary>Hint 2</summary>\n\nPageRank is computed with `compute_pagerank()` or `compute_importance()`\n\n</details>\n\n<details>\n<summary>Hint 3</summary>\n\nCheck the expected value and comparison in the assertion\n\n</details>\n\n### Solution\n\n<details>\n<summary>Click to reveal</summary>\n\n```python\ndef test_single_node_no_edges(self):\n        \"\"\"Single node with no edges gets base rank from damping.\"\"\"\n        graph = {\"a\": []}\n        result = _pagerank_core(graph, damping=0.85)\n        assert \"a\" in result\n        # With no incoming edges, rank = (1-d)/n = 0.15/1 = 0.15\n        assert result[\"a\"] == pytest.approx(0.15)\n```\n\n</details>\n\n---\n\n## Exercise: Single Node Self Loop\n\n**Concept:** Single node with self-loop still gets rank 1.0\n\n**Difficulty:** Beginner\n\n**Time:** ~20 minutes\n\n**Source:** `test_analysis.py`\n\n### Setup\n\n```python\nfrom cortical import CorticalTextProcessor\n```\n\n### Your Task\n\nSingle node with self-loop still gets rank 1.0.\n\n### Hints\n\n<details>\n<summary>Hint 1</summary>\n\nFocus on the simplest case with just one element.\n\n</details>\n\n<details>\n<summary>Hint 2</summary>\n\nPageRank is computed with `compute_pagerank()` or `compute_importance()`\n\n</details>\n\n<details>\n<summary>Hint 3</summary>\n\nCheck the expected value and comparison in the assertion\n\n</details>\n\n### Solution\n\n<details>\n<summary>Click to reveal</summary>\n\n```python\ndef test_single_node_self_loop(self):\n        \"\"\"Single node with self-loop still gets rank 1.0.\"\"\"\n        graph = {\"a\": [(\"a\", 1.0)]}\n        result = _pagerank_core(graph)\n        assert result[\"a\"] == pytest.approx(1.0)\n```\n\n</details>\n\n---\n\n## Exercise: Three Node Chain\n\n**Concept:** Chain: a -> b -> c. C should have highest rank\n\n**Difficulty:** Beginner\n\n**Time:** ~20 minutes\n\n**Source:** `test_analysis.py`\n\n### Setup\n\n```python\nfrom cortical import CorticalTextProcessor\n```\n\n### Your Task\n\nChain: a -> b -> c. C should have highest rank.\n\n### Hints\n\n<details>\n<summary>Hint 1</summary>\n\nThink about how elements connect in sequence.\n\n</details>\n\n<details>\n<summary>Hint 2</summary>\n\nPageRank is computed with `compute_pagerank()` or `compute_importance()`\n\n</details>\n\n### Solution\n\n<details>\n<summary>Click to reveal</summary>\n\n```python\ndef test_three_node_chain(self):\n        \"\"\"Chain: a -> b -> c. C should have highest rank.\"\"\"\n        graph = {\n            \"a\": [(\"b\", 1.0)],\n            \"b\": [(\"c\", 1.0)],\n            \"c\": []\n        }\n        result = _pagerank_core(graph)\n        # c receives transitively, b receives from a\n        assert result[\"c\"] >= result[\"b\"]\n        assert result[\"b\"] >= result[\"a\"]\n```\n\n</details>\n\n---\n\n## Exercise: Cycle\n\n**Concept:** Cycle: a -> b -> c -> a. All should have equal rank\n\n**Difficulty:** Beginner\n\n**Time:** ~20 minutes\n\n**Source:** `test_analysis.py`\n\n### Setup\n\n```python\nfrom cortical import CorticalTextProcessor\n```\n\n### Your Task\n\nCycle: a -> b -> c -> a. All should have equal rank.\n\n### Hints\n\n<details>\n<summary>Hint 1</summary>\n\nThink about how elements connect in sequence.\n\n</details>\n\n<details>\n<summary>Hint 2</summary>\n\nPageRank is computed with `compute_pagerank()` or `compute_importance()`\n\n</details>\n\n<details>\n<summary>Hint 3</summary>\n\nCheck the expected value and comparison in the assertion\n\n</details>\n\n### Solution\n\n<details>\n<summary>Click to reveal</summary>\n\n```python\ndef test_cycle(self):\n        \"\"\"Cycle: a -> b -> c -> a. All should have equal rank.\"\"\"\n        graph = {\n            \"a\": [(\"b\", 1.0)],\n            \"b\": [(\"c\", 1.0)],\n            \"c\": [(\"a\", 1.0)]\n        }\n        result = _pagerank_core(graph)\n        # All nodes in cycle should have equal rank\n        assert result[\"a\"] == pytest.approx(result[\"b\"], rel=0.01)\n        assert result[\"b\"] == pytest.approx(result[\"c\"], rel=0.01)\n```\n\n</details>\n\n---\n\n## Exercise: Empty Corpus\n\n**Concept:** Empty corpus returns empty dict\n\n**Difficulty:** Beginner\n\n**Time:** ~10 minutes\n\n**Source:** `test_analysis.py`\n\n### Setup\n\n```python\nfrom cortical import CorticalTextProcessor\n```\n\n### Your Task\n\nEmpty corpus returns empty dict.\n\n### Hints\n\n<details>\n<summary>Hint 1</summary>\n\nStart by considering the edge case of empty input.\n\n</details>\n\n<details>\n<summary>Hint 2</summary>\n\nCheck the expected value and comparison in the assertion\n\n</details>\n\n### Solution\n\n<details>\n<summary>Click to reveal</summary>\n\n```python\ndef test_empty_corpus(self):\n        \"\"\"Empty corpus returns empty dict.\"\"\"\n        result = _tfidf_core({}, num_docs=0)\n        assert result == {}\n```\n\n</details>\n\n---\n\n## Exercise: Single Term Single Doc\n\n**Concept:** Single term in single doc has IDF of 0\n\n**Difficulty:** Beginner\n\n**Time:** ~20 minutes\n\n**Source:** `test_analysis.py`\n\n### Setup\n\n```python\nfrom cortical import CorticalTextProcessor\n```\n\n### Your Task\n\nSingle term in single doc has IDF of 0.\n\n### Hints\n\n<details>\n<summary>Hint 1</summary>\n\nFocus on the simplest case with just one element.\n\n</details>\n\n<details>\n<summary>Hint 2</summary>\n\nCheck the expected value and comparison in the assertion\n\n</details>\n\n### Solution\n\n<details>\n<summary>Click to reveal</summary>\n\n```python\ndef test_single_term_single_doc(self):\n        \"\"\"Single term in single doc has IDF of 0.\"\"\"\n        stats = {\n            \"term\": (5, 1, {\"doc1\": 5})\n        }\n        result = _tfidf_core(stats, num_docs=1)\n        # IDF = log(1/1) = 0, so TF-IDF = 0\n        assert result[\"term\"][0] == pytest.approx(0.0)\n```\n\n</details>\n\n---\n\n## Exercise: Empty Graph\n\n**Concept:** Empty graph returns empty dict\n\n**Difficulty:** Beginner\n\n**Time:** ~10 minutes\n\n**Source:** `test_analysis.py`\n\n### Setup\n\n```python\nfrom cortical import CorticalTextProcessor\n```\n\n### Your Task\n\nEmpty graph returns empty dict.\n\n### Hints\n\n<details>\n<summary>Hint 1</summary>\n\nStart by considering the edge case of empty input.\n\n</details>\n\n<details>\n<summary>Hint 2</summary>\n\nCheck the expected value and comparison in the assertion\n\n</details>\n\n### Solution\n\n<details>\n<summary>Click to reveal</summary>\n\n```python\ndef test_empty_graph(self):\n        \"\"\"Empty graph returns empty dict.\"\"\"\n        result = _louvain_core({})\n        assert result == {}\n```\n\n</details>\n\n---\n\n## Exercise: Single Node\n\n**Concept:** Single node is its own community\n\n**Difficulty:** Beginner\n\n**Time:** ~10 minutes\n\n**Source:** `test_analysis.py`\n\n### Setup\n\n```python\nfrom cortical import CorticalTextProcessor\n```\n\n### Your Task\n\nSingle node is its own community.\n\n### Hints\n\n<details>\n<summary>Hint 1</summary>\n\nFocus on the simplest case with just one element.\n\n</details>\n\n<details>\n<summary>Hint 2</summary>\n\nCheck the expected value and comparison in the assertion\n\n</details>\n\n### Solution\n\n<details>\n<summary>Click to reveal</summary>\n\n```python\ndef test_single_node(self):\n        \"\"\"Single node is its own community.\"\"\"\n        result = _louvain_core({\"a\": {}})\n        assert \"a\" in result\n        assert result[\"a\"] == 0\n```\n\n</details>\n\n---\n\n## Exercise: Empty Graph\n\n**Concept:** Empty graph has zero modularity\n\n**Difficulty:** Beginner\n\n**Time:** ~10 minutes\n\n**Source:** `test_analysis.py`\n\n### Setup\n\n```python\nfrom cortical import CorticalTextProcessor\n```\n\n### Your Task\n\nEmpty graph has zero modularity.\n\n### Hints\n\n<details>\n<summary>Hint 1</summary>\n\nStart by considering the edge case of empty input.\n\n</details>\n\n<details>\n<summary>Hint 2</summary>\n\nCheck the expected value and comparison in the assertion\n\n</details>\n\n### Solution\n\n<details>\n<summary>Click to reveal</summary>\n\n```python\ndef test_empty_graph(self):\n        \"\"\"Empty graph has zero modularity.\"\"\"\n        result = _modularity_core({}, {})\n        assert result == 0.0\n```\n\n</details>\n\n---\n\n---\n\n*Completed 10 exercises? Check out the other topics for more challenges!*\n"
    },
    {
      "path": "08-exercises/ex-search.md",
      "title": "Exercises: Search",
      "section": "exercises",
      "tags": [
        "exercises",
        "search",
        "intermediate"
      ],
      "source_files": [
        "test_query_search.py",
        "test_query_expansion.py",
        "test_query_ranking.py",
        "test_query_passages.py",
        "test_query_definitions.py"
      ],
      "excerpt": "",
      "keywords": [
        "details",
        "summary",
        "empty",
        "tokenizer",
        "layers",
        "result",
        "hint",
        "term",
        "single",
        "python"
      ],
      "full_content": "# Search Exercises\n\n*Hands-on coding exercises to master search concepts.*\n\n**Difficulty Level:** Intermediate\n\n---\n\n## Introduction\n\nMaster the search and retrieval capabilities:\n\n- Query expansion techniques\n- Document ranking algorithms\n- Passage retrieval\n- Definition extraction\n\n## Exercise: Empty Query\n\n**Concept:** Empty query returns empty results\n\n**Difficulty:** Intermediate\n\n**Time:** ~20 minutes\n\n**Source:** `test_query_search.py`\n\n### Setup\n\n```python\nfrom cortical import CorticalTextProcessor\nfrom cortical.tokenizer import Tokenizer\n```\n\n### Your Task\n\nEmpty query returns empty results.\n\n### Hints\n\n<details>\n<summary>Hint 1</summary>\n\nStart by considering the edge case of empty input.\n\n</details>\n\n<details>\n<summary>Hint 2</summary>\n\nYou may need to create mock layers or minicolumns for testing\n\n</details>\n\n<details>\n<summary>Hint 3</summary>\n\nCheck the expected value and comparison in the assertion\n\n</details>\n\n### Solution\n\n<details>\n<summary>Click to reveal</summary>\n\n```python\ndef test_empty_query(self):\n        \"\"\"Empty query returns empty results.\"\"\"\n        layers = MockLayers.single_term(\"term\", tfidf=1.0, doc_ids=[\"doc1\"])\n        tokenizer = Tokenizer()\n\n        # Tokenizer will return empty list for empty string\n        result = find_documents_for_query(\"\", layers, tokenizer)\n        assert result == []\n```\n\n</details>\n\n---\n\n## Exercise: Single Term Single Doc\n\n**Concept:** Single term matching single document\n\n**Difficulty:** Intermediate\n\n**Time:** ~20 minutes\n\n**Source:** `test_query_search.py`\n\n### Setup\n\n```python\nfrom cortical import CorticalTextProcessor\nfrom cortical.tokenizer import Tokenizer\n```\n\n### Your Task\n\nSingle term matching single document.\n\n### Hints\n\n<details>\n<summary>Hint 1</summary>\n\nFocus on the simplest case with just one element.\n\n</details>\n\n<details>\n<summary>Hint 2</summary>\n\nYou may need to create mock layers or minicolumns for testing\n\n</details>\n\n<details>\n<summary>Hint 3</summary>\n\nCheck the expected value and comparison in the assertion\n\n</details>\n\n### Solution\n\n<details>\n<summary>Click to reveal</summary>\n\n```python\ndef test_single_term_single_doc(self):\n        \"\"\"Single term matching single document.\"\"\"\n        # Create layer with term in doc1\n        col = MockMinicolumn(\n            content=\"neural\",\n            tfidf=2.5,\n            document_ids={\"doc1\"},\n            tfidf_per_doc={\"doc1\": 2.5}\n        )\n        layers = MockLayers.empty()\n        layers[MockLayers.TOKENS] = MockHierarchicalLayer([col])\n\n        tokenizer = Tokenizer()\n        result = find_documents_for_query(\n            \"neural\", layers, tokenizer, use_expansion=False\n        )\n\n        assert len(result) == 1\n        assert result[0][0] == \"doc1\"\n        assert result[0][1] > 0\n```\n\n</details>\n\n---\n\n## Exercise: Single Term Multiple Docs\n\n**Concept:** Single term in multiple documents ranked by TF-IDF\n\n**Difficulty:** Intermediate\n\n**Time:** ~20 minutes\n\n**Source:** `test_query_search.py`\n\n### Setup\n\n```python\nfrom cortical import CorticalTextProcessor\nfrom cortical.tokenizer import Tokenizer\n```\n\n### Your Task\n\nSingle term in multiple documents ranked by TF-IDF.\n\n### Hints\n\n<details>\n<summary>Hint 1</summary>\n\nFocus on the simplest case with just one element.\n\n</details>\n\n<details>\n<summary>Hint 2</summary>\n\nYou may need to create mock layers or minicolumns for testing\n\n</details>\n\n<details>\n<summary>Hint 3</summary>\n\nCheck the expected value and comparison in the assertion\n\n</details>\n\n### Solution\n\n<details>\n<summary>Click to reveal</summary>\n\n```python\ndef test_single_term_multiple_docs(self):\n        \"\"\"Single term in multiple documents ranked by TF-IDF.\"\"\"\n        col = MockMinicolumn(\n            content=\"algorithm\",\n            tfidf=3.0,\n            document_ids={\"doc1\", \"doc2\", \"doc3\"},\n            tfidf_per_doc={\"doc1\": 5.0, \"doc2\": 3.0, \"doc3\": 1.0}\n        )\n        layers = MockLayers.empty()\n        layers[MockLayers.TOKENS] = MockHierarchicalLayer([col])\n\n        tokenizer = Tokenizer()\n        result = find_documents_for_query(\n            \"algorithm\", layers, tokenizer, use_expansion=False\n        )\n\n        assert len(result) == 3\n        # Should be sorted by TF-IDF score\n        assert result[0][0] == \"doc1\"  # Highest score\n        assert result[1][0] == \"doc2\"\n        assert result[2][0] == \"doc3\"  # Lowest score\n        assert result[0][1] > result[1][1] > result[2][1]\n```\n\n</details>\n\n---\n\n## Exercise: Query Expansion Disabled\n\n**Concept:** use_expansion=False uses only query terms\n\n**Difficulty:** Intermediate\n\n**Time:** ~20 minutes\n\n**Source:** `test_query_search.py`\n\n### Setup\n\n```python\nfrom cortical import CorticalTextProcessor\nfrom cortical.tokenizer import Tokenizer\n```\n\n### Your Task\n\nuse_expansion=False uses only query terms.\n\n### Hints\n\n<details>\n<summary>Hint 1</summary>\n\nBreak down the problem into smaller steps.\n\n</details>\n\n<details>\n<summary>Hint 2</summary>\n\nPageRank is computed with `compute_pagerank()` or `compute_importance()`\n\n</details>\n\n<details>\n<summary>Hint 3</summary>\n\nCheck the expected value and comparison in the assertion\n\n</details>\n\n### Solution\n\n<details>\n<summary>Click to reveal</summary>\n\n```python\ndef test_query_expansion_disabled(self):\n        \"\"\"use_expansion=False uses only query terms.\"\"\"\n        # Create connected terms\n        layers = (\n            LayerBuilder()\n            .with_term(\"neural\", tfidf=2.0, pagerank=0.8)\n            .with_term(\"network\", tfidf=2.0, pagerank=0.6)\n            .with_connection(\"neural\", \"network\", weight=5.0)\n            .with_document(\"doc1\", [\"neural\"])\n            .with_document(\"doc2\", [\"network\"])\n            .build()\n        )\n\n        layer0 = layers[MockLayers.TOKENS]\n        layer0.get_minicolumn(\"neural\").tfidf_per_doc = {\"doc1\": 2.0}\n        layer0.get_minicolumn(\"network\").tfidf_per_doc = {\"doc2\": 2.0}\n\n        tokenizer = Tokenizer()\n        result = find_documents_for_query(\n            \"neural\", layers, tokenizer,\n            use_expansion=False\n        )\n\n        # Should only find doc1 (contains \"neural\")\n        assert len(result) == 1\n        assert result[0][0] == \"doc1\"\n```\n\n</details>\n\n---\n\n## Exercise: Empty Corpus\n\n**Concept:** Empty corpus returns empty results\n\n**Difficulty:** Intermediate\n\n**Time:** ~20 minutes\n\n**Source:** `test_query_search.py`\n\n### Setup\n\n```python\nfrom cortical import CorticalTextProcessor\nfrom cortical.tokenizer import Tokenizer\n```\n\n### Your Task\n\nEmpty corpus returns empty results.\n\n### Hints\n\n<details>\n<summary>Hint 1</summary>\n\nStart by considering the edge case of empty input.\n\n</details>\n\n<details>\n<summary>Hint 2</summary>\n\nYou may need to create mock layers or minicolumns for testing\n\n</details>\n\n<details>\n<summary>Hint 3</summary>\n\nCheck the expected value and comparison in the assertion\n\n</details>\n\n### Solution\n\n<details>\n<summary>Click to reveal</summary>\n\n```python\ndef test_empty_corpus(self):\n        \"\"\"Empty corpus returns empty results.\"\"\"\n        layers = MockLayers.empty()\n        tokenizer = Tokenizer()\n\n        result = find_documents_for_query(\"query\", layers, tokenizer)\n\n        assert result == []\n```\n\n</details>\n\n---\n\n## Exercise: Single Term Match\n\n**Concept:** Fast search finds document with matching term\n\n**Difficulty:** Intermediate\n\n**Time:** ~20 minutes\n\n**Source:** `test_query_search.py`\n\n### Setup\n\n```python\nfrom cortical import CorticalTextProcessor\nfrom cortical.tokenizer import Tokenizer\n```\n\n### Your Task\n\nFast search finds document with matching term.\n\n### Hints\n\n<details>\n<summary>Hint 1</summary>\n\nFocus on the simplest case with just one element.\n\n</details>\n\n<details>\n<summary>Hint 2</summary>\n\nYou may need to create mock layers or minicolumns for testing\n\n</details>\n\n<details>\n<summary>Hint 3</summary>\n\nCheck the expected value and comparison in the assertion\n\n</details>\n\n### Solution\n\n<details>\n<summary>Click to reveal</summary>\n\n```python\ndef test_single_term_match(self):\n        \"\"\"Fast search finds document with matching term.\"\"\"\n        col = MockMinicolumn(\n            content=\"algorithm\",\n            document_ids={\"doc1\"},\n            tfidf_per_doc={\"doc1\": 3.0}\n        )\n        layers = MockLayers.empty()\n        layers[MockLayers.TOKENS] = MockHierarchicalLayer([col])\n\n        tokenizer = Tokenizer()\n        result = fast_find_documents(\"algorithm\", layers, tokenizer)\n\n        assert len(result) == 1\n        assert result[0][0] == \"doc1\"\n```\n\n</details>\n\n---\n\n## Exercise: Empty Query\n\n**Concept:** Empty query returns empty results\n\n**Difficulty:** Intermediate\n\n**Time:** ~20 minutes\n\n**Source:** `test_query_search.py`\n\n### Setup\n\n```python\nfrom cortical import CorticalTextProcessor\nfrom cortical.tokenizer import Tokenizer\n```\n\n### Your Task\n\nEmpty query returns empty results.\n\n### Hints\n\n<details>\n<summary>Hint 1</summary>\n\nStart by considering the edge case of empty input.\n\n</details>\n\n<details>\n<summary>Hint 2</summary>\n\nYou may need to create mock layers or minicolumns for testing\n\n</details>\n\n<details>\n<summary>Hint 3</summary>\n\nCheck the expected value and comparison in the assertion\n\n</details>\n\n### Solution\n\n<details>\n<summary>Click to reveal</summary>\n\n```python\ndef test_empty_query(self):\n        \"\"\"Empty query returns empty results.\"\"\"\n        layers = MockLayers.single_term(\"term\", doc_ids=[\"doc1\"])\n        tokenizer = Tokenizer()\n\n        result = fast_find_documents(\"\", layers, tokenizer)\n\n        assert result == []\n```\n\n</details>\n\n---\n\n## Exercise: No Candidates Returns Empty\n\n**Concept:** No matching candidates returns empty\n\n**Difficulty:** Intermediate\n\n**Time:** ~20 minutes\n\n**Source:** `test_query_search.py`\n\n### Setup\n\n```python\nfrom cortical import CorticalTextProcessor\nfrom cortical.tokenizer import Tokenizer\n```\n\n### Your Task\n\nNo matching candidates returns empty.\n\n### Hints\n\n<details>\n<summary>Hint 1</summary>\n\nStart by considering the edge case of empty input.\n\n</details>\n\n<details>\n<summary>Hint 2</summary>\n\nYou may need to create mock layers or minicolumns for testing\n\n</details>\n\n<details>\n<summary>Hint 3</summary>\n\nCheck the expected value and comparison in the assertion\n\n</details>\n\n### Solution\n\n<details>\n<summary>Click to reveal</summary>\n\n```python\ndef test_no_candidates_returns_empty(self):\n        \"\"\"No matching candidates returns empty.\"\"\"\n        layers = MockLayers.single_term(\"existing\", doc_ids=[\"doc1\"])\n        tokenizer = Tokenizer()\n\n        result = fast_find_documents(\"nonexistent\", layers, tokenizer)\n\n        assert result == []\n```\n\n</details>\n\n---\n\n## Exercise: Empty Layer\n\n**Concept:** Empty layer returns empty index\n\n**Difficulty:** Intermediate\n\n**Time:** ~10 minutes\n\n**Source:** `test_query_search.py`\n\n### Setup\n\n```python\nfrom cortical import CorticalTextProcessor\n```\n\n### Your Task\n\nEmpty layer returns empty index.\n\n### Hints\n\n<details>\n<summary>Hint 1</summary>\n\nStart by considering the edge case of empty input.\n\n</details>\n\n<details>\n<summary>Hint 2</summary>\n\nYou may need to create mock layers or minicolumns for testing\n\n</details>\n\n<details>\n<summary>Hint 3</summary>\n\nCheck the expected value and comparison in the assertion\n\n</details>\n\n### Solution\n\n<details>\n<summary>Click to reveal</summary>\n\n```python\ndef test_empty_layer(self):\n        \"\"\"Empty layer returns empty index.\"\"\"\n        layers = MockLayers.empty()\n        result = build_document_index(layers)\n        assert result == {}\n```\n\n</details>\n\n---\n\n## Exercise: Single Term Single Doc\n\n**Concept:** Single term in single document\n\n**Difficulty:** Intermediate\n\n**Time:** ~20 minutes\n\n**Source:** `test_query_search.py`\n\n### Setup\n\n```python\nfrom cortical import CorticalTextProcessor\n```\n\n### Your Task\n\nSingle term in single document.\n\n### Hints\n\n<details>\n<summary>Hint 1</summary>\n\nFocus on the simplest case with just one element.\n\n</details>\n\n<details>\n<summary>Hint 2</summary>\n\nYou may need to create mock layers or minicolumns for testing\n\n</details>\n\n<details>\n<summary>Hint 3</summary>\n\nCheck the expected value and comparison in the assertion\n\n</details>\n\n### Solution\n\n<details>\n<summary>Click to reveal</summary>\n\n```python\ndef test_single_term_single_doc(self):\n        \"\"\"Single term in single document.\"\"\"\n        col = MockMinicolumn(\n            content=\"term\",\n            document_ids={\"doc1\"},\n            tfidf_per_doc={\"doc1\": 2.5}\n        )\n        layers = MockLayers.empty()\n        layers[MockLayers.TOKENS] = MockHierarchicalLayer([col])\n\n        result = build_document_index(layers)\n\n        assert \"term\" in result\n        assert result[\"term\"] == {\"doc1\": 2.5}\n```\n\n</details>\n\n---\n\n---\n\n*Completed 10 exercises? Check out the other topics for more challenges!*\n"
    },
    {
      "path": "08-exercises/index.md",
      "title": "Exercise Index",
      "section": "exercises",
      "tags": [
        "exercises",
        "index",
        "learning"
      ],
      "source_files": [
        "tests/unit/*.py"
      ],
      "excerpt": "",
      "keywords": [
        "exercises",
        "advanced",
        "foundations",
        "search",
        "test",
        "hints",
        "difficulty",
        "suite",
        "learning",
        "exercise"
      ],
      "full_content": "# Exercises\n\n*Hands-on coding exercises derived from the test suite.*\n\n---\n\n## Overview\n\nLearn by doing! These exercises are extracted from the Cortical Text Processor's test suite and transformed into learning challenges.\n\nEach exercise includes:\n\n- **Clear task description** - What you need to implement\n- **Progressive hints** - Guidance without spoilers\n- **Complete solution** - Reference implementation from tests\n- **Verification** - How to check your answer\n\n## Exercise Topics\n\n### [Foundations](ex-foundations.md)\n\n**Difficulty:** Beginner\n\n**Exercises:** 10\n\nCore algorithms and data structures. Start here if you're new!\n\n### [Search](ex-search.md)\n\n**Difficulty:** Intermediate\n\n**Exercises:** 10\n\nSearch, ranking, and retrieval techniques. Build on foundations.\n\n### [Advanced](ex-advanced.md)\n\n**Difficulty:** Advanced\n\n**Exercises:** 10\n\nAdvanced features and complex algorithms. For experienced users.\n\n## Learning Path\n\n**Recommended progression:**\n\n1. Start with **Foundations** exercises\n2. Move to **Search** once comfortable\n3. Challenge yourself with **Advanced** topics\n\n## Tips for Success\n\n- **Read the test carefully** - The docstring explains what's being tested\n- **Use hints progressively** - Try solving first, then reveal hints as needed\n- **Run the solution** - Verify your understanding by executing the code\n- **Experiment** - Modify parameters and see how behavior changes\n\n---\n\n*Total exercises: 30*\n"
    },
    {
      "path": "09-journey/index.md",
      "title": "Your Learning Journey",
      "section": "journey",
      "tags": [
        "journey",
        "learning-path",
        "index"
      ],
      "source_files": [
        "git log",
        "CLAUDE.md"
      ],
      "excerpt": "",
      "keywords": [
        "min",
        "concepts",
        "time",
        "minutes",
        "journey",
        "week",
        "path",
        "semantic",
        "goal",
        "total"
      ],
      "full_content": "# Your Learning Journey\n\n*A progressive path through the Cortical Text Processor, designed for learners at all levels.*\n\n---\n\n## Overview\n\nThis learning journey is organized into three progressive stages, each building on the previous one. The concepts are ordered based on:\n\n- **Dependencies:** What you need to know first\n- **Complexity:** From simple to sophisticated\n- **Historical emergence:** When concepts first appeared in development\n\n## Learning Paths\n\n### [Beginner Path](journey-beginner.md)\n\n**Concepts:** 3  \n**Time:** ~45 minutes  \n**Preview:** Tokenization, Stop Words, Tf-Idf\n\n### [Intermediate Path](journey-intermediate.md)\n\n**Concepts:** 6  \n**Time:** ~150 minutes  \n**Preview:** Pagerank, Lateral Connections, Query Expansion, and 3 more\n\n### [Advanced Path](journey-advanced.md)\n\n**Concepts:** 3  \n**Time:** ~105 minutes  \n**Preview:** Concept Clustering, Semantic Relations, Louvain\n\n---\n\n# Suggested Study Schedule\n\n*A practical 4-week plan to master the Cortical Text Processor.*\n\n---\n\n## Week 1: Foundations\n\n**Goal:** Understand the core building blocks\n\n**Concepts:**\n- Tokenization (~15 min)\n- Stop Words (~15 min)\n- Tf-Idf (~15 min)\n\n**Total Time:** ~45 minutes\n\n**Activities:**\n- Read foundation chapters\n- Run `showcase.py` to see concepts in action\n- Experiment with basic tokenization and TF-IDF\n\n## Week 2: Building Complexity\n\n**Goal:** Add graph-based features to your mental model\n\n**Concepts:**\n- Pagerank (~25 min)\n- Lateral Connections (~25 min)\n- Query Expansion (~25 min)\n\n**Total Time:** ~75 minutes\n\n**Activities:**\n- Study PageRank and BM25 implementations\n- Index a sample corpus with `scripts/index_codebase.py`\n- Explore query expansion behavior\n\n## Week 3: Advanced Structures\n\n**Goal:** Understand hierarchical organization and persistence\n\n**Concepts:**\n- Incremental Indexing (~25 min)\n- Bm25 (~25 min)\n- Persistence (~25 min)\n\n**Total Time:** ~75 minutes\n\n**Activities:**\n- Review minicolumn and layer architecture\n- Practice save/load operations\n- Test incremental indexing\n\n## Week 4: Mastery\n\n**Goal:** Master sophisticated algorithms and optimization\n\n**Concepts:**\n- Concept Clustering (~35 min)\n- Semantic Relations (~35 min)\n- Louvain (~35 min)\n\n**Total Time:** ~105 minutes\n\n**Activities:**\n- Study Louvain clustering implementation\n- Experiment with semantic relations extraction\n- Profile performance with `scripts/profile_full_analysis.py`\n- Implement a custom feature using the library\n\n## Tips for Success\n\n1. **Follow the order** - Prerequisites build on each other\n2. **Code along** - Run examples from `showcase.py` and scripts\n3. **Read tests** - `tests/` directory shows real usage patterns\n4. **Ask questions** - Use the semantic search: `python scripts/search_codebase.py \"your question\"`\n5. **Build something** - Best way to learn is to apply the concepts\n\n"
    },
    {
      "path": "09-journey/journey-advanced.md",
      "title": "Learning Journey: Advanced",
      "section": "journey",
      "tags": [
        "journey",
        "learning-path",
        "advanced"
      ],
      "source_files": [
        "git log",
        "CLAUDE.md"
      ],
      "excerpt": "",
      "keywords": [
        "book",
        "concept",
        "foundations",
        "modules",
        "concepts",
        "time",
        "clustering",
        "first",
        "introduced",
        "mentions"
      ],
      "full_content": "# Mastery Path (Advanced)\n\n*Deep dives into sophisticated algorithms. For those ready to master the full system.*\n\n---\n\n**Concepts:** 3\n\n**Estimated Time:** ~105 minutes\n\n---\n\n## 1. Concept Clustering\n\n**First Introduced:** 2025-12-09\n\n**Mentions in History:** 2 commits\n\n**Reading Time:** ~35 min\n\n**Prerequisites:** None (foundational concept)\n\n**Where to Learn:**\n- Foundations: `book/01-foundations/`\n- Modules: `book/02-modules/`\n- Evolution: `book/07-concepts/concept-clustering.md`\n\n**Key Takeaway:**\nUnderstand the role of concept clustering in the information retrieval pipeline.\n---\n\n## 2. Semantic Relations\n\n**First Introduced:** 2025-12-09\n\n**Mentions in History:** 1 commits\n\n**Reading Time:** ~35 min\n\n**Prerequisites:**\n- Query Expansion\n\n**Where to Learn:**\n- Foundations: `book/01-foundations/`\n- Modules: `book/02-modules/`\n- Evolution: `book/07-concepts/semantic-relations.md`\n\n**Key Takeaway:**\nDive into pattern-based extraction of typed relationships.\n---\n\n## 3. Louvain\n\n**First Introduced:** 2025-12-11\n\n**Mentions in History:** 7 commits\n\n**Reading Time:** ~35 min\n\n**Prerequisites:** None (foundational concept)\n\n**Where to Learn:**\n- Foundations: `book/01-foundations/`\n- Modules: `book/02-modules/`\n- Evolution: `book/07-concepts/louvain.md`\n\n**Key Takeaway:**\nExplore community detection for automatic concept clustering.\n---\n\n"
    },
    {
      "path": "09-journey/journey-beginner.md",
      "title": "Learning Journey: Beginner",
      "section": "journey",
      "tags": [
        "journey",
        "learning-path",
        "beginner"
      ],
      "source_files": [
        "git log",
        "CLAUDE.md"
      ],
      "excerpt": "",
      "keywords": [
        "book",
        "concepts",
        "foundations",
        "modules",
        "foundational",
        "time",
        "learn",
        "first",
        "introduced",
        "mentions"
      ],
      "full_content": "# Start Here (Foundational)\n\n*Core concepts that unlock everything else. Start here if you're new to information retrieval.*\n\n---\n\n**Concepts:** 3\n\n**Estimated Time:** ~45 minutes\n\n---\n\n## 1. Tokenization\n\n**First Introduced:** 2025-12-10\n\n**Mentions in History:** 3 commits\n\n**Reading Time:** ~15 min\n\n**Prerequisites:** None (foundational concept)\n\n**Related Concepts:**\n- Bm25\n\n**Where to Learn:**\n- Foundations: `book/01-foundations/`\n- Modules: `book/02-modules/`\n- Evolution: `book/07-concepts/tokenization.md`\n\n**Key Takeaway:**\nLearn how text is broken into processable units - the foundation of all text analysis.\n---\n\n## 2. Stop Words\n\n**First Introduced:** 2025-12-13\n\n**Mentions in History:** 1 commits\n\n**Reading Time:** ~15 min\n\n**Prerequisites:** None (foundational concept)\n\n**Where to Learn:**\n- Foundations: `book/01-foundations/`\n- Modules: `book/02-modules/`\n- Evolution: `book/07-concepts/stop-words.md`\n\n**Key Takeaway:**\nSee why common words are filtered to focus on meaningful content.\n---\n\n## 3. Tf-Idf\n\n**First Introduced:** 2025-12-15\n\n**Mentions in History:** 2 commits\n\n**Reading Time:** ~15 min\n\n**Prerequisites:** None (foundational concept)\n\n**Related Concepts:**\n- Bm25\n- Query Expansion\n\n**Where to Learn:**\n- Foundations: `book/01-foundations/`\n- Modules: `book/02-modules/`\n- Evolution: `book/07-concepts/tf-idf.md`\n\n**Key Takeaway:**\nMaster the classic algorithm for term importance - weighing frequency against distinctiveness.\n---\n\n"
    },
    {
      "path": "09-journey/journey-intermediate.md",
      "title": "Learning Journey: Intermediate",
      "section": "journey",
      "tags": [
        "journey",
        "learning-path",
        "intermediate"
      ],
      "source_files": [
        "git log",
        "CLAUDE.md"
      ],
      "excerpt": "",
      "keywords": [
        "book",
        "foundations",
        "modules",
        "concepts",
        "time",
        "learn",
        "first",
        "introduced",
        "mentions",
        "history"
      ],
      "full_content": "# Going Deeper (Intermediate)\n\n*Build on the foundations. These concepts add power and flexibility to your understanding.*\n\n---\n\n**Concepts:** 6\n\n**Estimated Time:** ~150 minutes\n\n---\n\n## 1. Pagerank\n\n**First Introduced:** 2025-12-09\n\n**Mentions in History:** 4 commits\n\n**Reading Time:** ~25 min\n\n**Prerequisites:** None (foundational concept)\n\n**Where to Learn:**\n- Foundations: `book/01-foundations/`\n- Modules: `book/02-modules/`\n- Evolution: `book/07-concepts/pagerank.md`\n\n**Key Takeaway:**\nLearn how graph algorithms measure importance through connections.\n---\n\n## 2. Lateral Connections\n\n**First Introduced:** 2025-12-09\n\n**Mentions in History:** 2 commits\n\n**Reading Time:** ~25 min\n\n**Prerequisites:** None (foundational concept)\n\n**Where to Learn:**\n- Foundations: `book/01-foundations/`\n- Modules: `book/02-modules/`\n- Evolution: `book/07-concepts/lateral-connections.md`\n\n**Key Takeaway:**\nGrasp the Hebbian-inspired network of term relationships.\n---\n\n## 3. Query Expansion\n\n**First Introduced:** 2025-12-10\n\n**Mentions in History:** 4 commits\n\n**Reading Time:** ~25 min\n\n**Prerequisites:**\n- Tf-Idf\n\n**Related Concepts:**\n- Tf-Idf\n\n**Where to Learn:**\n- Foundations: `book/01-foundations/`\n- Modules: `book/02-modules/`\n- Evolution: `book/07-concepts/query-expansion.md`\n\n**Key Takeaway:**\nSee how searches become smarter by exploring related terms.\n---\n\n## 4. Incremental Indexing\n\n**First Introduced:** 2025-12-10\n\n**Mentions in History:** 1 commits\n\n**Reading Time:** ~25 min\n\n**Prerequisites:** None (foundational concept)\n\n**Where to Learn:**\n- Foundations: `book/01-foundations/`\n- Modules: `book/02-modules/`\n- Evolution: `book/07-concepts/incremental-indexing.md`\n\n**Key Takeaway:**\nUnderstand the role of incremental indexing in the information retrieval pipeline.\n---\n\n## 5. Bm25\n\n**First Introduced:** 2025-12-15\n\n**Mentions in History:** 9 commits\n\n**Reading Time:** ~25 min\n\n**Prerequisites:**\n- Tokenization\n- Tf-Idf\n\n**Related Concepts:**\n- Tf-Idf\n- Tokenization\n\n**Where to Learn:**\n- Foundations: `book/01-foundations/`\n- Modules: `book/02-modules/`\n- Evolution: `book/07-concepts/bm25.md`\n\n**Key Takeaway:**\nUnderstand the modern scoring function that improves on TF-IDF with saturation.\n---\n\n## 6. Persistence\n\n**First Introduced:** 2025-12-16\n\n**Mentions in History:** 1 commits\n\n**Reading Time:** ~25 min\n\n**Prerequisites:** None (foundational concept)\n\n**Where to Learn:**\n- Foundations: `book/01-foundations/`\n- Modules: `book/02-modules/`\n- Evolution: `book/07-concepts/persistence.md`\n\n**Key Takeaway:**\nUnderstand the role of persistence in the information retrieval pipeline.\n---\n\n"
    },
    {
      "path": "docs/CONTRIBUTING.md",
      "title": "CONTRIBUTING",
      "section": "docs",
      "tags": [],
      "source_files": [],
      "excerpt": "> **Welcome!** This guide shows you how to add new generators and extend \"The Cortical Chronicles.\"",
      "keywords": [
        "self",
        "python",
        "content",
        "generate",
        "data",
        "str",
        "def",
        "generator",
        "errors",
        "return"
      ],
      "full_content": "# Contributing to The Cortical Chronicles\n\n> **Welcome!** This guide shows you how to add new generators and extend \"The Cortical Chronicles.\"\n\n---\n\n## Table of Contents\n\n- [Quick Start](#quick-start)\n- [Generator Architecture](#generator-architecture)\n- [Step-by-Step: Adding a Generator](#step-by-step-adding-a-generator)\n- [Best Practices](#best-practices)\n- [Testing Your Generator](#testing-your-generator)\n- [CI Integration](#ci-integration)\n- [Examples](#examples)\n\n---\n\n## Quick Start\n\n### Prerequisites\n\n```bash\n# Ensure you have dependencies\npip install -e \".[dev]\"\n\n# Verify PyYAML is installed\npython -c \"import yaml; print('PyYAML OK')\"\n\n# Test existing generators\npython scripts/generate_book.py --list\npython scripts/generate_book.py --dry-run\n```\n\n### Template for New Generator\n\n```python\nfrom scripts.generate_book import ChapterGenerator\nfrom pathlib import Path\nfrom typing import Dict, Any, List\n\nclass MyChapterGenerator(ChapterGenerator):\n    \"\"\"Generate chapters from <your data source>.\"\"\"\n\n    @property\n    def name(self) -> str:\n        \"\"\"Generator name for CLI and logging.\"\"\"\n        return \"mychapter\"\n\n    @property\n    def output_dir(self) -> str:\n        \"\"\"Subdirectory in book/ for output.\"\"\"\n        return \"03-decisions\"  # Choose: 00-05 based on content type\n\n    def generate(self, dry_run: bool = False, verbose: bool = False) -> Dict[str, Any]:\n        \"\"\"\n        Generate chapter content.\n\n        Args:\n            dry_run: If True, don't write files (just log)\n            verbose: If True, print detailed progress\n\n        Returns:\n            Dict with:\n                - files: List of generated file paths\n                - stats: Generation statistics\n                - errors: Any errors encountered\n        \"\"\"\n        errors = []\n        stats = {\n            \"items_processed\": 0,\n            \"chapters_written\": 0\n        }\n\n        if verbose:\n            print(\"  Processing data...\")\n\n        # 1. Load your data source\n        try:\n            data = self._load_data()\n        except Exception as e:\n            errors.append(f\"Failed to load data: {e}\")\n            return {\"files\": [], \"stats\": stats, \"errors\": errors}\n\n        # 2. Process each item\n        for item in data:\n            content = self._generate_chapter_content(item)\n            filename = self._generate_filename(item)\n\n            if verbose:\n                print(f\"  Generating: {filename}\")\n\n            self.write_chapter(filename, content, dry_run=dry_run)\n            stats[\"chapters_written\"] += 1\n\n        stats[\"items_processed\"] = len(data)\n\n        return {\n            \"files\": [str(f) for f in self.generated_files],\n            \"stats\": stats,\n            \"errors\": errors\n        }\n\n    def _load_data(self) -> List[Any]:\n        \"\"\"Load your data source.\"\"\"\n        # Implement data loading\n        pass\n\n    def _generate_chapter_content(self, item: Any) -> str:\n        \"\"\"Generate markdown content for one chapter.\"\"\"\n        # Generate frontmatter\n        frontmatter = self.generate_frontmatter(\n            title=item['title'],\n            tags=['tag1', 'tag2'],\n            source_files=['source.py']\n        )\n\n        # Build content\n        content = frontmatter\n        content += f\"# {item['title']}\\n\\n\"\n        content += item['body']\n        content += \"\\n\\n---\\n\\n\"\n        content += \"*This chapter is part of [The Cortical Chronicles](../README.md).*\\n\"\n\n        return content\n\n    def _generate_filename(self, item: Any) -> str:\n        \"\"\"Generate filename from item.\"\"\"\n        # Slugify the title\n        slug = item['title'].lower().replace(' ', '-')\n        return f\"{slug}.md\"\n```\n\n---\n\n## Generator Architecture\n\n### Base Class: `ChapterGenerator`\n\nAll generators inherit from `ChapterGenerator` (defined in `scripts/generate_book.py`).\n\n**Required Methods:**\n\n| Method | Returns | Purpose |\n|--------|---------|---------|\n| `name` (property) | `str` | Generator identifier for CLI |\n| `output_dir` (property) | `str` | Subdirectory in book/ |\n| `generate(dry_run, verbose)` | `Dict[str, Any]` | Main generation logic |\n\n**Provided Helper Methods:**\n\n| Method | Purpose |\n|--------|---------|\n| `write_chapter(filename, content, dry_run)` | Write chapter with standard handling |\n| `generate_frontmatter(title, tags, sources)` | Generate YAML frontmatter |\n\n**Instance Variables:**\n\n- `self.book_dir`: Path to book/ directory\n- `self.generated_files`: List of written file paths (auto-tracked by `write_chapter`)\n\n### Output Directories\n\nChoose the appropriate section for your content:\n\n| Directory | Purpose | Example Generators |\n|-----------|---------|-------------------|\n| `00-preface` | Book introduction | (manual) |\n| `01-foundations` | Algorithm theory | AlgorithmChapterGenerator |\n| `02-architecture` | Module documentation | ModuleDocGenerator |\n| `03-decisions` | ADRs, design decisions | DecisionRecordGenerator |\n| `04-evolution` | Commit narratives | CommitNarrativeGenerator |\n| `05-future` | Roadmap, vision | (placeholder) |\n\n### Return Value Format\n\n```python\n{\n    \"files\": [\n        \"book/01-foundations/alg-pagerank.md\",\n        \"book/01-foundations/alg-bm25.md\"\n    ],\n    \"stats\": {\n        \"algorithms_found\": 6,\n        \"chapters_written\": 6,\n        \"custom_metric\": 42\n    },\n    \"errors\": [\n        \"Warning: Optional data not found\"\n    ]\n}\n```\n\n---\n\n## Step-by-Step: Adding a Generator\n\n### Step 1: Create the Generator Class\n\nAdd to `scripts/generate_book.py` or create a new file:\n\n```python\nclass MyChapterGenerator(ChapterGenerator):\n    \"\"\"One-line description.\"\"\"\n\n    @property\n    def name(self) -> str:\n        return \"mychapter\"\n\n    @property\n    def output_dir(self) -> str:\n        return \"03-decisions\"  # Choose appropriate section\n\n    def generate(self, dry_run: bool = False, verbose: bool = False) -> Dict[str, Any]:\n        # Implementation\n        pass\n```\n\n### Step 2: Register in BookBuilder\n\nIn `scripts/generate_book.py`, find the `main()` function and add:\n\n```python\ndef main():\n    # ...existing code...\n\n    # Register real generators\n    builder.register_generator(AlgorithmChapterGenerator(book_dir=args.output))\n    builder.register_generator(ModuleDocGenerator(book_dir=args.output))\n    builder.register_generator(CommitNarrativeGenerator(book_dir=args.output))\n    builder.register_generator(MyChapterGenerator(book_dir=args.output))  # <-- ADD THIS\n    builder.register_generator(SearchIndexGenerator(book_dir=args.output))\n\n    # ...rest of main()...\n```\n\n**Important:** Register `SearchIndexGenerator` LAST so it indexes all other chapters.\n\n### Step 3: Test the Generator\n\n```bash\n# List all generators (verify yours appears)\npython scripts/generate_book.py --list\n\n# Test with dry-run\npython scripts/generate_book.py --chapter mychapter --dry-run --verbose\n\n# Generate for real\npython scripts/generate_book.py --chapter mychapter --verbose\n\n# Verify output\nls -la book/03-decisions/\n```\n\n### Step 4: Regenerate Search Index\n\nAfter adding new chapters:\n\n```bash\npython scripts/generate_book.py --chapter search\n```\n\n### Step 5: Test Full Build\n\n```bash\n# Full regeneration\npython scripts/generate_book.py --verbose\n\n# Verify all chapters\nfind book/ -name \"*.md\" | sort\n\n# Validate JSON outputs\npython -c \"import json; json.load(open('book/index.json'))\"\npython -c \"import json; json.load(open('book/search.json'))\"\n```\n\n---\n\n## Best Practices\n\n### 1. Output File Naming\n\n**Convention:**\n\n- Algorithm docs: `alg-<name>.md` (e.g., `alg-pagerank.md`)\n- Module docs: `mod-<category>.md` (e.g., `mod-processor.md`)\n- Narrative docs: `<topic>.md` (e.g., `timeline.md`, `features.md`)\n- Decision records: `adr-<num>-<slug>.md` (e.g., `adr-001-architecture.md`)\n\n**Rules:**\n\n- Use lowercase\n- Use hyphens, not underscores\n- Be descriptive but concise\n- Avoid special characters\n\n### 2. Frontmatter Requirements\n\nAll chapters must have YAML frontmatter:\n\n```yaml\n---\ntitle: \"Chapter Title\"\ngenerated: \"2025-12-16T10:30:00Z\"\ngenerator: \"mychapter\"\nsource_files:\n  - \"path/to/source.py\"\n  - \"path/to/data.json\"\ntags:\n  - tag1\n  - tag2\n  - tag3\n---\n```\n\n**Required Fields:**\n\n- `title`: Human-readable chapter title\n- `generated`: ISO 8601 timestamp (use `datetime.utcnow().isoformat() + \"Z\"`)\n- `generator`: Your generator's `name` property\n- `source_files`: List of source files used\n- `tags`: List of tags for categorization\n\n**Use the helper:**\n\n```python\nfrontmatter = self.generate_frontmatter(\n    title=\"My Chapter\",\n    tags=['algorithms', 'foundations'],\n    source_files=['docs/VISION.md', 'cortical/analysis.py']\n)\n```\n\n### 3. Cross-Reference Patterns\n\n**Link to other chapters:**\n\n```markdown\nSee [Algorithm Documentation](alg-pagerank.md) for details.\nSee [Architecture Overview](../02-architecture/index.md).\n```\n\n**Link to source code:**\n\n```markdown\n**Implementation:** `cortical/analysis.py:compute_pagerank()`\n```\n\n**Link to ADRs:**\n\n```markdown\n**Related Decision:** [ADR-001: Architecture](../../samples/decisions/adr-001-*.md)\n```\n\n**Link to git commits:**\n\n```markdown\n**Commit:** `a1b2c3d`\n```\n\n### 4. Error Handling\n\n**Always handle exceptions gracefully:**\n\n```python\ndef generate(self, dry_run: bool = False, verbose: bool = False) -> Dict[str, Any]:\n    errors = []\n    stats = {}\n\n    try:\n        data = self._load_data()\n    except FileNotFoundError as e:\n        errors.append(f\"Source file not found: {e}\")\n        return {\"files\": [], \"stats\": stats, \"errors\": errors}\n    except Exception as e:\n        errors.append(f\"Unexpected error loading data: {e}\")\n        return {\"files\": [], \"stats\": stats, \"errors\": errors}\n\n    # Continue processing...\n```\n\n**Error message guidelines:**\n\n- Be specific (include file names, line numbers)\n- Suggest fixes when possible\n- Use warnings for non-critical issues\n- Return partial results if some items succeed\n\n**Example:**\n\n```python\n# Good\nerrors.append(\"Failed to parse VISION.md line 42: Missing '###' prefix\")\n\n# Bad\nerrors.append(\"Error in file\")\n```\n\n### 5. Verbose Output\n\nProvide helpful progress indicators:\n\n```python\ndef generate(self, dry_run: bool = False, verbose: bool = False) -> Dict[str, Any]:\n    if verbose:\n        print(\"  Loading source data...\")\n\n    data = self._load_data()\n\n    if verbose:\n        print(f\"  Found {len(data)} items to process\")\n\n    for item in data:\n        if verbose:\n            print(f\"  Generating: {item['title']}\")\n\n        # Process item...\n```\n\n**Guidelines:**\n\n- Use 2-space indentation for generator output\n- Log counts and progress\n- Don't log every detail (avoid spam)\n- Use dry-run mode for testing without writes\n\n### 6. Dry-Run Support\n\nAlways respect the `dry_run` parameter:\n\n```python\n# Good: Use write_chapter helper (handles dry_run automatically)\nself.write_chapter(filename, content, dry_run=dry_run)\n\n# If you need custom file writing:\nif dry_run:\n    print(f\"  Would write: {output_path}\")\nelse:\n    output_path.write_text(content)\n    self.generated_files.append(output_path)\n```\n\n### 7. Statistics Reporting\n\nReturn meaningful statistics:\n\n```python\nstats = {\n    \"items_found\": len(all_items),\n    \"items_processed\": len(processed_items),\n    \"chapters_written\": len(self.generated_files),\n    \"items_skipped\": len(skipped_items),\n    \"warnings\": len(warnings)\n}\n```\n\n---\n\n## Testing Your Generator\n\n### Unit Testing\n\nCreate a test file `tests/test_book_generation.py`:\n\n```python\nimport unittest\nfrom pathlib import Path\nfrom scripts.generate_book import MyChapterGenerator\n\nclass TestMyChapterGenerator(unittest.TestCase):\n    def setUp(self):\n        self.generator = MyChapterGenerator()\n\n    def test_name(self):\n        self.assertEqual(self.generator.name, \"mychapter\")\n\n    def test_output_dir(self):\n        self.assertEqual(self.generator.output_dir, \"03-decisions\")\n\n    def test_dry_run(self):\n        \"\"\"Test that dry-run doesn't write files.\"\"\"\n        result = self.generator.generate(dry_run=True, verbose=False)\n        self.assertEqual(result['files'], [])\n        self.assertGreaterEqual(result['stats']['items_processed'], 0)\n\n    def test_generate(self):\n        \"\"\"Test actual generation.\"\"\"\n        result = self.generator.generate(dry_run=False, verbose=False)\n        self.assertGreater(len(result['files']), 0)\n        self.assertEqual(result['errors'], [])\n```\n\n### Integration Testing\n\n```bash\n# 1. Full dry-run test\npython scripts/generate_book.py --dry-run --verbose\n\n# 2. Generate to temporary directory\npython scripts/generate_book.py --output /tmp/test-book --chapter mychapter\n\n# 3. Verify outputs\nls -la /tmp/test-book/03-decisions/\ncat /tmp/test-book/03-decisions/example.md\n\n# 4. Validate frontmatter\npython -c \"\nimport yaml\nfrom pathlib import Path\nfor f in Path('/tmp/test-book').glob('**/*.md'):\n    content = f.read_text()\n    if content.startswith('---'):\n        fm = content.split('---', 2)[1]\n        yaml.safe_load(fm)\n        print(f'{f.name}: OK')\n\"\n\n# 5. Clean up\nrm -rf /tmp/test-book\n```\n\n### Common Test Cases\n\n1. **Empty data source** - Should return gracefully\n2. **Malformed data** - Should log errors, continue processing\n3. **Missing dependencies** - Should fail gracefully with clear error\n4. **Dry-run mode** - Should not write any files\n5. **Verbose mode** - Should print progress\n6. **Frontmatter validation** - YAML should parse correctly\n7. **Cross-references** - Links should be valid\n\n---\n\n## CI Integration\n\n### GitHub Actions Workflow\n\nAdd to `.github/workflows/book-generation.yml`:\n\n```yaml\nname: Generate Book\n\non:\n  push:\n    branches: [main]\n    paths:\n      - 'docs/VISION.md'\n      - 'cortical/**/*.py'\n      - 'scripts/generate_book.py'\n  workflow_dispatch:\n\njobs:\n  generate:\n    runs-on: ubuntu-latest\n\n    steps:\n      - uses: actions/checkout@v3\n        with:\n          fetch-depth: 0  # Full history for commit narratives\n\n      - uses: actions/setup-python@v4\n        with:\n          python-version: '3.10'\n\n      - name: Install dependencies\n        run: |\n          pip install -e \".[dev]\"\n\n      - name: Generate book\n        run: |\n          python scripts/generate_book.py --verbose\n\n      - name: Validate outputs\n        run: |\n          python -c \"import json; json.load(open('book/index.json'))\"\n          python -c \"import json; json.load(open('book/search.json'))\"\n\n      - name: Upload artifact\n        uses: actions/upload-artifact@v3\n        with:\n          name: cortical-chronicles\n          path: book/\n```\n\n### Pre-Commit Hook\n\nAdd to `.git/hooks/pre-commit`:\n\n```bash\n#!/bin/bash\n# Regenerate book if source files changed\n\nSOURCES=\"docs/VISION.md cortical/ scripts/generate_book.py\"\nCHANGED=$(git diff --cached --name-only $SOURCES)\n\nif [ -n \"$CHANGED\" ]; then\n    echo \"\ud83d\udcda Regenerating book chapters...\"\n    python scripts/generate_book.py --verbose || exit 1\n\n    # Stage generated files\n    git add book/\nfi\n```\n\n---\n\n## Examples\n\n### Example 1: Simple Data-Driven Generator\n\nGenerate chapters from JSON files:\n\n```python\nclass DataChapterGenerator(ChapterGenerator):\n    \"\"\"Generate chapters from data/*.json files.\"\"\"\n\n    @property\n    def name(self) -> str:\n        return \"data\"\n\n    @property\n    def output_dir(self) -> str:\n        return \"05-future\"\n\n    def generate(self, dry_run: bool = False, verbose: bool = False) -> Dict[str, Any]:\n        import json\n\n        data_dir = Path(__file__).parent.parent / \"data\"\n        json_files = list(data_dir.glob(\"*.json\"))\n\n        if verbose:\n            print(f\"  Found {len(json_files)} JSON files\")\n\n        for json_file in json_files:\n            data = json.loads(json_file.read_text())\n\n            content = self.generate_frontmatter(\n                title=data['title'],\n                tags=data.get('tags', []),\n                source_files=[str(json_file)]\n            )\n            content += f\"# {data['title']}\\n\\n\"\n            content += data['body'] + \"\\n\"\n\n            filename = json_file.stem + \".md\"\n            self.write_chapter(filename, content, dry_run)\n\n        return {\n            \"files\": [str(f) for f in self.generated_files],\n            \"stats\": {\"files_processed\": len(json_files)},\n            \"errors\": []\n        }\n```\n\n### Example 2: Git History Generator\n\nGenerate from commit messages:\n\n```python\nclass GitHistoryGenerator(ChapterGenerator):\n    \"\"\"Generate timeline from git history.\"\"\"\n\n    @property\n    def name(self) -> str:\n        return \"timeline\"\n\n    @property\n    def output_dir(self) -> str:\n        return \"04-evolution\"\n\n    def _run_git(self, *args) -> str:\n        import subprocess\n        result = subprocess.run(\n            [\"git\", *args],\n            capture_output=True,\n            text=True,\n            check=True\n        )\n        return result.stdout.strip()\n\n    def generate(self, dry_run: bool = False, verbose: bool = False) -> Dict[str, Any]:\n        try:\n            log_output = self._run_git(\"log\", \"--format=%H|%aI|%s|%an\", \"-100\")\n        except Exception as e:\n            return {\n                \"files\": [],\n                \"stats\": {},\n                \"errors\": [f\"Git error: {e}\"]\n            }\n\n        commits = []\n        for line in log_output.split('\\n'):\n            if not line.strip():\n                continue\n            hash_val, timestamp, message, author = line.split('|', 3)\n            commits.append({\n                'hash': hash_val[:7],\n                'date': timestamp[:10],\n                'message': message,\n                'author': author\n            })\n\n        # Generate timeline content\n        content = self.generate_frontmatter(\n            title=\"Project Timeline\",\n            tags=['timeline', 'history'],\n            source_files=['git log']\n        )\n        content += \"# Project Timeline\\n\\n\"\n\n        for commit in commits:\n            content += f\"- **{commit['date']}** (`{commit['hash']}`): {commit['message']}\\n\"\n\n        self.write_chapter(\"timeline.md\", content, dry_run)\n\n        return {\n            \"files\": [str(f) for f in self.generated_files],\n            \"stats\": {\"commits_processed\": len(commits)},\n            \"errors\": []\n        }\n```\n\n### Example 3: Multi-File Generator\n\nGenerate multiple chapters from one data source:\n\n```python\nclass MultiChapterGenerator(ChapterGenerator):\n    \"\"\"Generate multiple chapters from single source.\"\"\"\n\n    @property\n    def name(self) -> str:\n        return \"concepts\"\n\n    @property\n    def output_dir(self) -> str:\n        return \"02-architecture\"\n\n    def generate(self, dry_run: bool = False, verbose: bool = False) -> Dict[str, Any]:\n        # Load Louvain clusters\n        clusters = self._load_clusters()\n\n        if verbose:\n            print(f\"  Found {len(clusters)} concept clusters\")\n\n        # Generate one chapter per cluster\n        for cluster_id, terms in clusters.items():\n            content = self.generate_frontmatter(\n                title=f\"Concept Cluster {cluster_id}\",\n                tags=['concepts', 'clustering', 'louvain'],\n                source_files=['corpus_dev.pkl']\n            )\n\n            content += f\"# Concept Cluster {cluster_id}\\n\\n\"\n            content += f\"**Terms:** {', '.join(terms[:20])}\\n\\n\"\n\n            filename = f\"concept-{cluster_id}.md\"\n            self.write_chapter(filename, content, dry_run)\n\n        # Generate index\n        index_content = self._generate_index(clusters)\n        self.write_chapter(\"concepts-index.md\", index_content, dry_run)\n\n        return {\n            \"files\": [str(f) for f in self.generated_files],\n            \"stats\": {\n                \"clusters_found\": len(clusters),\n                \"chapters_written\": len(clusters) + 1\n            },\n            \"errors\": []\n        }\n\n    def _load_clusters(self) -> Dict[int, List[str]]:\n        # Load from processor\n        pass\n\n    def _generate_index(self, clusters: Dict) -> str:\n        # Generate index page\n        pass\n```\n\n---\n\n## Common Pitfalls\n\n### \u274c Don't: Hardcode Paths\n\n```python\n# Bad\noutput_path = Path(\"/home/user/book/01-foundations/chapter.md\")\n\n# Good\noutput_path = self.book_dir / self.output_dir / \"chapter.md\"\n```\n\n### \u274c Don't: Ignore Dry-Run\n\n```python\n# Bad\nwith open(output_path, 'w') as f:\n    f.write(content)\n\n# Good\nself.write_chapter(filename, content, dry_run=dry_run)\n```\n\n### \u274c Don't: Swallow Errors\n\n```python\n# Bad\ntry:\n    data = self._load_data()\nexcept:\n    pass  # Silent failure!\n\n# Good\ntry:\n    data = self._load_data()\nexcept Exception as e:\n    errors.append(f\"Failed to load data: {e}\")\n    return {\"files\": [], \"stats\": {}, \"errors\": errors}\n```\n\n### \u274c Don't: Generate Unsafe Filenames\n\n```python\n# Bad\nfilename = f\"{user_input}.md\"  # Could be \"../../../etc/passwd.md\"\n\n# Good\nfilename = self._sanitize_filename(user_input) + \".md\"\n\ndef _sanitize_filename(self, text: str) -> str:\n    # Remove path separators\n    text = text.replace('/', '-').replace('\\\\', '-')\n    # Remove special chars\n    text = re.sub(r'[^\\w\\s-]', '', text)\n    # Normalize whitespace\n    text = re.sub(r'[-\\s]+', '-', text)\n    return text.lower().strip('-')\n```\n\n---\n\n## Next Steps\n\n1. **Study existing generators** in `scripts/generate_book.py`\n2. **Create your generator** following the template\n3. **Test with dry-run** before writing files\n4. **Validate frontmatter** with YAML parser\n5. **Regenerate search index** after adding chapters\n6. **Document your generator** in this guide\n\n---\n\n## Questions?\n\n- Check [TROUBLESHOOTING.md](TROUBLESHOOTING.md) for common issues\n- Review existing generators for patterns\n- Test with `--dry-run --verbose` for debugging\n\n---\n\n*This guide is part of [The Cortical Chronicles](../README.md) documentation.*\n"
    },
    {
      "path": "docs/TROUBLESHOOTING.md",
      "title": "TROUBLESHOOTING",
      "section": "docs",
      "tags": [],
      "source_files": [],
      "excerpt": "> **Quick Recovery**: For most errors, run `python scripts/generate_book.py --dry-run --verbose` to diagnose without writing files. This guide helps diagnose and fix common issues when generating...",
      "keywords": [
        "git",
        "book",
        "python",
        "check",
        "json",
        "chapter",
        "scripts",
        "bash",
        "generate_book",
        "file"
      ],
      "full_content": "# Troubleshooting Guide\n\n> **Quick Recovery**: For most errors, run `python scripts/generate_book.py --dry-run --verbose` to diagnose without writing files.\n\nThis guide helps diagnose and fix common issues when generating \"The Cortical Chronicles.\"\n\n---\n\n## Table of Contents\n\n- [Common Errors](#common-errors)\n- [Diagnostic Commands](#diagnostic-commands)\n- [Recovery Procedures](#recovery-procedures)\n- [Error Reference](#error-reference)\n- [Getting Help](#getting-help)\n\n---\n\n## Common Errors\n\n### 1. YAML Parsing Errors in .ai_meta Files\n\n**Symptoms:**\n```\nWarning: Failed to parse analysis.py.ai_meta: ...\nYAML parsing error\n```\n\n**Causes:**\n- Malformed YAML frontmatter\n- Unescaped special characters in docstrings\n- Missing comment header stripping\n- Mixed tabs and spaces\n\n**Solutions:**\n\n```bash\n# Check if .ai_meta files exist\nls -la cortical/*.ai_meta\n\n# Regenerate metadata files\npython scripts/generate_ai_metadata.py --force\n\n# Test parsing a specific file\npython -c \"import yaml; yaml.safe_load(open('cortical/analysis.py.ai_meta').read())\"\n```\n\n**Prevention:**\n- Run `generate_ai_metadata.py` after major docstring changes\n- Avoid special YAML characters (`:`, `{`, `}`, `[`, `]`) in docstrings without quoting\n\n### 2. Missing VISION.md Sections\n\n**Symptoms:**\n```\nalgorithms_found: 0\nNo algorithm sections extracted\n```\n\n**Causes:**\n- VISION.md missing \"Deep Algorithm Analysis\" section\n- Section header format changed\n- Regex pattern mismatch\n\n**Solutions:**\n\n```bash\n# Verify VISION.md structure\ngrep -n \"## Deep Algorithm Analysis\" docs/VISION.md\n\n# Check for algorithm sections\ngrep -n \"### Algorithm\" docs/VISION.md\n\n# Regenerate with verbose logging\npython scripts/generate_book.py --chapter foundations --verbose\n```\n\n**Expected Structure:**\n```markdown\n## Deep Algorithm Analysis\n\n### Algorithm 1: PageRank \u2014 Importance Discovery\n\n**Implementation:** `cortical/analysis.py:compute_pagerank()`\n...\n\n### Algorithm 2: BM25/TF-IDF \u2014 Distinctiveness Scoring\n...\n```\n\n### 3. Git History Access Issues\n\n**Symptoms:**\n```\nWarning: Failed to read git history: ...\nNo git history found\nCould not read git history\n```\n\n**Causes:**\n- Not in a git repository\n- Insufficient permissions\n- Git not installed\n- Detached HEAD state\n\n**Solutions:**\n\n```bash\n# Check git availability\nwhich git\ngit --version\n\n# Verify repository\ngit status\n\n# Test git log access\ngit log -5 --format=\"%H|%aI|%s|%an\"\n\n# Check permissions\nls -la .git/\n```\n\n**Workarounds:**\n- Skip evolution chapters: `python scripts/generate_book.py --chapter foundations`\n- Initialize git if missing: `git init && git add . && git commit -m \"Initial commit\"`\n\n### 4. Missing Dependencies\n\n**Symptoms:**\n```\nModuleNotFoundError: No module named 'yaml'\nImportError: cannot import name 'yaml'\n```\n\n**Solution:**\n\n```bash\n# Install required dependencies\npip install pyyaml\n\n# Or install all dev dependencies\npip install -e \".[dev]\"\n\n# Verify installation\npython -c \"import yaml; print('PyYAML OK')\"\n```\n\n### 5. Permission Errors Writing to book/\n\n**Symptoms:**\n```\nPermissionError: [Errno 13] Permission denied: 'book/01-foundations/...'\nOSError: [Errno 30] Read-only file system\n```\n\n**Solutions:**\n\n```bash\n# Check directory permissions\nls -ld book/\nls -la book/01-foundations/\n\n# Fix permissions\nchmod -R u+w book/\n\n# Try dry-run first\npython scripts/generate_book.py --dry-run\n\n# Check disk space\ndf -h .\n```\n\n### 6. Empty or Missing ML Data\n\n**Symptoms:**\n```\nwith_ml_data: 0\nWarning: Failed to load ML data\n```\n\n**Causes:**\n- `.git-ml/tracked/commits.jsonl` doesn't exist\n- ML collection not started\n- File is empty\n\n**Solutions:**\n\n```bash\n# Check ML data file\nls -lh .git-ml/tracked/commits.jsonl\n\n# Backfill ML data\npython scripts/ml_data_collector.py backfill -n 100\n\n# Verify data\npython scripts/ml_data_collector.py stats\n```\n\n**Note:** ML data is optional. Chapters will generate without it (just with fewer details).\n\n### 7. Search Index Generation Failures\n\n**Symptoms:**\n```\nFailed to parse index.md: ...\nJSONDecodeError\n```\n\n**Causes:**\n- Malformed frontmatter in generated chapters\n- Invalid JSON structure\n- Missing chapter files\n\n**Solutions:**\n\n```bash\n# Regenerate chapters first\npython scripts/generate_book.py --chapter foundations\npython scripts/generate_book.py --chapter architecture\n\n# Then regenerate search index\npython scripts/generate_book.py --chapter search\n\n# Validate generated JSON\npython -c \"import json; json.load(open('book/index.json'))\"\npython -c \"import json; json.load(open('book/search.json'))\"\n```\n\n---\n\n## Diagnostic Commands\n\n### Health Check\n\n```bash\n# Full diagnostic run (no writes)\npython scripts/generate_book.py --dry-run --verbose\n```\n\nExpected output:\n```\nRegistered generator: foundations\nRegistered generator: architecture\n...\nGenerating: foundations\n  Found 6 algorithms in VISION.md\n  Generating: alg-pagerank.md\n...\nTotal files: 25\n```\n\n### Check Individual Generators\n\n```bash\n# List all generators\npython scripts/generate_book.py --list\n\n# Test specific generator\npython scripts/generate_book.py --chapter foundations --dry-run --verbose\npython scripts/generate_book.py --chapter architecture --dry-run --verbose\npython scripts/generate_book.py --chapter evolution --dry-run --verbose\n```\n\n### Verify Dependencies\n\n```bash\n# Check Python version (3.8+ required)\npython --version\n\n# Check required packages\npython -c \"import yaml; print('PyYAML:', yaml.__version__)\"\npython -c \"import json; print('json: OK')\"\npython -c \"import re; print('re: OK')\"\n\n# Check git\ngit --version\ngit status\n```\n\n### Verify Source Files\n\n```bash\n# Check VISION.md\ntest -f docs/VISION.md && echo \"VISION.md exists\" || echo \"VISION.md MISSING\"\ngrep -c \"### Algorithm\" docs/VISION.md\n\n# Check .ai_meta files\nfind cortical -name \"*.ai_meta\" | wc -l\nls -lh cortical/*.ai_meta | head -5\n\n# Check git history\ngit log -5 --oneline\n```\n\n### Verify Output Structure\n\n```bash\n# Check generated files\nfind book/ -name \"*.md\" | sort\nfind book/ -name \"*.json\"\n\n# Check chapter completeness\nfor dir in book/0*-*/; do\n  echo \"$dir: $(ls \"$dir\" | wc -l) files\"\ndone\n\n# Validate JSON outputs\npython -c \"import json; json.load(open('book/index.json')); print('index.json: OK')\"\npython -c \"import json; json.load(open('book/search.json')); print('search.json: OK')\"\n```\n\n---\n\n## Recovery Procedures\n\n### Complete Rebuild\n\nWhen all else fails, regenerate from scratch:\n\n```bash\n# 1. Backup existing book (if needed)\ncp -r book/ book.backup/\n\n# 2. Clear generated files (keep docs and assets)\nrm -rf book/0*-*/\nrm -f book/index.json book/search.json\n\n# 3. Regenerate metadata (if needed)\npython scripts/generate_ai_metadata.py --force\n\n# 4. Full regeneration\npython scripts/generate_book.py --verbose\n\n# 5. Verify\nls -lR book/\npython -c \"import json; json.load(open('book/index.json'))\"\n```\n\n### Regenerate Single Chapter\n\nIf one chapter is corrupted:\n\n```bash\n# 1. Remove the chapter\nrm -rf book/01-foundations/\n\n# 2. Regenerate just that chapter\npython scripts/generate_book.py --chapter foundations --verbose\n\n# 3. Rebuild search index\npython scripts/generate_book.py --chapter search\n\n# 4. Verify\nls -la book/01-foundations/\n```\n\n### Fix Malformed Frontmatter\n\nIf chapters have malformed YAML frontmatter:\n\n```bash\n# 1. Identify the problem file\npython -c \"\nimport yaml\nfrom pathlib import Path\nfor f in Path('book').glob('**/*.md'):\n    try:\n        content = f.read_text()\n        if content.startswith('---'):\n            fm = content.split('---', 2)[1]\n            yaml.safe_load(fm)\n    except Exception as e:\n        print(f'ERROR: {f}: {e}')\n\"\n\n# 2. Remove the problematic chapter\nrm book/XX-section/problematic.md\n\n# 3. Regenerate the parent chapter\npython scripts/generate_book.py --chapter <generator-name>\n```\n\n### Restore from Git\n\nIf the book is tracked in git:\n\n```bash\n# Check what changed\ngit status book/\ngit diff book/\n\n# Restore specific file\ngit restore book/01-foundations/alg-pagerank.md\n\n# Restore entire book\ngit restore book/\n\n# Or reset to last good state\ngit log --oneline -- book/\ngit restore --source=<commit-hash> book/\n```\n\n### Partial Failure Recovery\n\nIf some generators fail but others succeed:\n\n```bash\n# 1. Check which generators failed\npython scripts/generate_book.py --verbose 2>&1 | grep -A 3 \"ERROR:\"\n\n# 2. Regenerate only failed chapters\npython scripts/generate_book.py --chapter <failed-generator> --verbose\n\n# 3. Rebuild search index\npython scripts/generate_book.py --chapter search\n```\n\n---\n\n## Error Reference\n\n### Generator-Specific Errors\n\n#### AlgorithmChapterGenerator\n\n| Error | Cause | Fix |\n|-------|-------|-----|\n| `algorithms_found: 0` | VISION.md missing section | Check docs/VISION.md structure |\n| `Source file not found` | VISION.md doesn't exist | Create docs/VISION.md |\n| `chapters_written: 0` | Regex pattern mismatch | Update `_extract_algorithms()` |\n\n#### ModuleDocGenerator\n\n| Error | Cause | Fix |\n|-------|-------|-----|\n| `No .ai_meta files found` | Metadata not generated | Run `generate_ai_metadata.py` |\n| `Failed to parse <file>.ai_meta` | Malformed YAML | Regenerate metadata with `--force` |\n| `modules_documented: 0` | All parsing failed | Check YAML structure |\n\n#### CommitNarrativeGenerator\n\n| Error | Cause | Fix |\n|-------|-------|-----|\n| `No git history found` | Not in git repo | Initialize git or skip chapter |\n| `Failed to read git history` | Git not available | Install git |\n| `with_ml_data: 0` | ML data missing | Run backfill (optional) |\n\n#### SearchIndexGenerator\n\n| Error | Cause | Fix |\n|-------|-------|-----|\n| `chapters_indexed: 0` | No chapter files | Generate chapters first |\n| `Failed to parse <file>` | Malformed frontmatter | Regenerate source chapter |\n| `JSONDecodeError` | Invalid JSON structure | Check chapter YAML |\n\n### System-Level Errors\n\n| Error | Typical Cause | Solution |\n|-------|--------------|----------|\n| `PermissionError` | Read-only filesystem | Check permissions with `ls -ld book/` |\n| `FileNotFoundError` | Missing source file | Verify file exists with `ls -la` |\n| `ModuleNotFoundError: yaml` | Missing dependency | Install with `pip install pyyaml` |\n| `JSONDecodeError` | Corrupted output | Delete and regenerate file |\n| `UnicodeDecodeError` | Binary file read as text | Check file encoding |\n| `subprocess.CalledProcessError` | Git command failed | Verify git with `git status` |\n\n---\n\n## Prevention Tips\n\n### Before Generating\n\n1. **Verify prerequisites:**\n   ```bash\n   python --version  # 3.8+\n   git --version\n   python -c \"import yaml; print('OK')\"\n   ```\n\n2. **Check source files:**\n   ```bash\n   test -f docs/VISION.md || echo \"WARNING: VISION.md missing\"\n   ls cortical/*.ai_meta | wc -l  # Should be >10\n   ```\n\n3. **Test with dry-run:**\n   ```bash\n   python scripts/generate_book.py --dry-run\n   ```\n\n### After Making Changes\n\n1. **Regenerate affected chapters:**\n   - Changed VISION.md \u2192 `--chapter foundations`\n   - Changed docstrings \u2192 regenerate metadata, then `--chapter architecture`\n   - New commits \u2192 `--chapter evolution`\n\n2. **Always rebuild search index:**\n   ```bash\n   python scripts/generate_book.py --chapter search\n   ```\n\n3. **Validate outputs:**\n   ```bash\n   python -c \"import json; json.load(open('book/index.json'))\"\n   ```\n\n---\n\n## Getting Help\n\n### Debug Checklist\n\n- [ ] Run with `--dry-run --verbose`\n- [ ] Check error message in this guide\n- [ ] Verify dependencies installed\n- [ ] Test with single chapter generation\n- [ ] Check source file structure\n- [ ] Review recent git history\n- [ ] Try complete rebuild\n\n### Logging\n\nGenerate detailed logs for debugging:\n\n```bash\n# Full verbose output to file\npython scripts/generate_book.py --verbose 2>&1 | tee generation.log\n\n# Check for errors\ngrep -i \"error\\|warning\\|failed\" generation.log\n\n# Check generator stats\ngrep \"stats\" generation.log\n```\n\n### Still Stuck?\n\n1. **Check recent changes:**\n   ```bash\n   git log --oneline -10\n   git diff HEAD~5 -- docs/ cortical/\n   ```\n\n2. **Isolate the problem:**\n   - Test each generator individually\n   - Compare with known-good state\n   - Check file permissions\n\n3. **Report issue with:**\n   - Full error message\n   - Output of `--dry-run --verbose`\n   - Output of diagnostic commands\n   - Recent changes to source files\n\n---\n\n## Appendix: File Locations\n\n### Source Files\n\n| File | Purpose | Generator |\n|------|---------|-----------|\n| `docs/VISION.md` | Algorithm descriptions | foundations |\n| `cortical/*.ai_meta` | Module metadata | architecture |\n| `.git/logs/` | Git history | evolution |\n| `.git-ml/tracked/commits.jsonl` | ML commit data | evolution |\n| `samples/decisions/adr-*.md` | ADRs | decisions |\n\n### Output Files\n\n| File | Generator | Can Delete? |\n|------|-----------|-------------|\n| `book/*/index.md` | Various | Yes (regenerates) |\n| `book/01-foundations/*.md` | foundations | Yes |\n| `book/02-architecture/*.md` | architecture | Yes |\n| `book/04-evolution/*.md` | evolution | Yes |\n| `book/index.json` | search | Yes |\n| `book/search.json` | search | Yes |\n| `book/README.md` | Manual | **No** (manual) |\n| `book/docs/` | Manual | **No** (manual) |\n| `book/assets/` | Manual | **No** (manual) |\n\n---\n\n*This troubleshooting guide is part of [The Cortical Chronicles](../README.md) documentation.*\n"
    }
  ],
  "sections": {
    "preface": {
      "count": 2,
      "description": "Introduction to the book and how it works"
    },
    "foundations": {
      "count": 6,
      "description": "Core algorithms and IR theory"
    },
    "architecture": {
      "count": 10,
      "description": "Module documentation and system design"
    },
    "decisions": {
      "count": 2,
      "description": "Architectural decision records"
    },
    "evolution": {
      "count": 5,
      "description": "Project history and development narrative"
    },
    "case-studies": {
      "count": 1,
      "description": ""
    },
    "future": {
      "count": 1,
      "description": "Future plans and roadmap"
    },
    "lessons": {
      "count": 5,
      "description": ""
    },
    "concepts": {
      "count": 15,
      "description": ""
    },
    "exercises": {
      "count": 4,
      "description": ""
    },
    "journey": {
      "count": 4,
      "description": ""
    },
    "docs": {
      "count": 2,
      "description": ""
    }
  },
  "stats": {
    "total_chapters": 57,
    "total_sections": 12
  }
}