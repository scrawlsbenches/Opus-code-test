{
  "generated": "2025-12-16T17:26:23.920823Z",
  "version": "1.0.0",
  "chapters": [
    {
      "path": "00-preface/how-this-book-works.md",
      "title": "How This Book Works",
      "section": "preface",
      "tags": [
        "meta",
        "introduction",
        "self-reference"
      ],
      "source_files": [
        "scripts/generate_book.py",
        "docs/VISION.md"
      ],
      "excerpt": "> *\"The best documentation is the kind that writes itself.\"* The Cortical Chronicles is a **self-documenting book**. It uses the Cortical Text Processor\u2014the very system it documents\u2014to generate its...",
      "keywords": [
        "book",
        "chapter",
        "cortical",
        "generate_book",
        "vision",
        "architecture",
        "search",
        "index",
        "scripts",
        "itself"
      ],
      "full_content": "# How This Book Works\n\n> *\"The best documentation is the kind that writes itself.\"*\n\n## Overview\n\nThe Cortical Chronicles is a **self-documenting book**. It uses the Cortical Text Processor\u2014the very system it documents\u2014to generate its own content. This creates a fascinating recursive property: the book understands itself through the same algorithms it explains.\n\n## The Generation Process\n\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    BOOK GENERATION                           \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                              \u2502\n\u2502  Source Files           Generators           Chapters        \u2502\n\u2502  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500           \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500           \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500        \u2502\n\u2502                                                              \u2502\n\u2502  docs/VISION.md    \u2192   AlgorithmGen    \u2192   01-foundations/  \u2502\n\u2502  cortical/*.ai_meta \u2192  ModuleDocGen    \u2192   02-architecture/ \u2502\n\u2502  samples/decisions/ \u2192  DecisionGen     \u2192   03-decisions/    \u2502\n\u2502  git log           \u2192   NarrativeGen    \u2192   04-evolution/    \u2502\n\u2502  tasks/            \u2192   RoadmapGen      \u2192   05-future/       \u2502\n\u2502                                                              \u2502\n\u2502                    \u2193                                         \u2502\n\u2502              search-index.json                               \u2502\n\u2502                    \u2193                                         \u2502\n\u2502               index.html (searchable)                        \u2502\n\u2502                                                              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n## Chapter Types\n\n### 01-foundations/\nAlgorithm deep-dives extracted from `docs/VISION.md`. Each algorithm (PageRank, BM25, Louvain, etc.) gets its own chapter with:\n- Purpose and intuition\n- Mathematical formulation\n- Implementation details\n- Why it matters for code search\n\n### 02-architecture/\nModule documentation generated from `.ai_meta` files. Includes:\n- Module purpose and dependencies\n- Key functions and classes\n- Mermaid dependency graphs\n\n### 03-decisions/\nArchitecture Decision Records from `samples/decisions/`. Documents the \"why\" behind design choices.\n\n### 04-evolution/\nA narrative of project history generated from git commits. Transforms raw commit logs into a readable story of how the project evolved.\n\n### 05-future/\nRoadmap and vision from task files and `VISION.md`. Shows where the project is heading.\n\n## The Self-Reference Loop\n\nHere's what makes this book special:\n\n1. **The processor indexes its own code** \u2192 Creates a semantic graph\n2. **The generators query that graph** \u2192 Find relevant content\n3. **The book explains those algorithms** \u2192 Reader understands the system\n4. **The system processes those explanations** \u2192 Understands itself better\n\nThis isn't just cute\u2014it's a powerful test of the system's capabilities. If the Cortical Text Processor can understand and explain itself, it can understand any codebase.\n\n## Regenerating the Book\n\nThe book regenerates automatically on every push to `main`:\n\n```bash\n# Manual regeneration\npython scripts/generate_book.py\n\n# Generate specific chapter\npython scripts/generate_book.py --chapter foundations\n\n# Preview without writing\npython scripts/generate_book.py --dry-run\n```\n\n## Searching the Book\n\nThe book includes a semantic search interface. Open `index.html` to:\n- Search by keyword or concept\n- Browse by chapter\n- Follow cross-references\n\nThe search uses the same algorithms described in the book\u2014query expansion, BM25 scoring, PageRank boosting.\n\n## See Also\n\n- [Algorithm Analysis](../01-foundations/index.md) - Deep dive into the algorithms\n- [Architecture](../02-architecture/index.md) - How the code is organized\n- [Source: generate_book.py](../../scripts/generate_book.py) - The generation script\n\n## Source Files\n\nThis chapter was written manually as the seed for the book. Future chapters are auto-generated from:\n- `scripts/generate_book.py` - The orchestrator\n- `docs/VISION.md:185-430` - Algorithm documentation source\n\n---\n\n*This chapter is part of [The Cortical Chronicles](../README.md),\na self-documenting book generated by the Cortical Text Processor.*\n"
    },
    {
      "path": "01-foundations/alg-bm25.md",
      "title": "BM25/TF-IDF \u2014 Distinctiveness Scoring",
      "section": "foundations",
      "tags": [
        "algorithms",
        "foundations",
        "ir-theory"
      ],
      "source_files": [
        "docs/VISION.md",
        "cortical/analysis/tfidf.py"
      ],
      "excerpt": "``` BM25(t, d) = IDF(t) \u00d7 (tf(t,d) \u00d7 (k1 + 1)) / (tf(t,d) + k1 \u00d7 (1 - b + b \u00d7 |d|/avgdl)) ``` Where: This dual approach allows:",
      "keywords": [
        "idf",
        "term",
        "document",
        "bm25",
        "frequency",
        "corpus",
        "cortical",
        "importance",
        "scoring",
        "specific"
      ],
      "full_content": "# BM25/TF-IDF \u2014 Distinctiveness Scoring\n\n**Purpose:** Score how well a term distinguishes a specific document from the rest of the corpus.\n\n**Implementation:** `cortical/analysis/tfidf.py`\n\n**BM25 Formula (Default):**\n```\nBM25(t, d) = IDF(t) \u00d7 (tf(t,d) \u00d7 (k1 + 1)) / (tf(t,d) + k1 \u00d7 (1 - b + b \u00d7 |d|/avgdl))\n```\n\nWhere:\n- `IDF(t) = log((N - df(t) + 0.5) / (df(t) + 0.5) + 1)` \u2014 Inverse document frequency with smoothing\n- `tf(t,d)` \u2014 Term frequency in document d\n- `k1 = 1.2` \u2014 Term frequency saturation (diminishing returns after ~12 occurrences)\n- `b = 0.75` \u2014 Length normalization factor\n\n**Why BM25 Over TF-IDF:**\n- Non-negative IDF even for terms appearing in most documents\n- Length normalization prevents long files from unfairly dominating\n- Term frequency saturation models realistic relevance (saying \"API\" 100 times doesn't make a doc 100\u00d7 more relevant than saying it once)\n\n**Dual Storage Strategy:**\n- **Global TF-IDF** (`col.tfidf`): Term importance to entire corpus\n- **Per-Document TF-IDF** (`col.tfidf_per_doc[doc_id]`): Term importance within specific document\n\nThis dual approach allows:\n- Fast corpus-wide importance filtering\n- Accurate per-document relevance scoring for search\n\n---\n\n*This chapter is part of [The Cortical Chronicles](../README.md), a self-documenting book generated by the Cortical Text Processor.*\n"
    },
    {
      "path": "01-foundations/alg-graph-boosted-search.md",
      "title": "Graph-Boosted Search (GB-BM25) \u2014 Hybrid Ranking",
      "section": "foundations",
      "tags": [
        "algorithms",
        "foundations",
        "ir-theory"
      ],
      "source_files": [
        "docs/VISION.md",
        "cortical/query/search.py:425-564"
      ],
      "excerpt": "``` final_score = (0.5 \u00d7 normalized_bm25) + (0.3 \u00d7 normalized_pagerank) + (0.2 \u00d7 normalized_proximity) \u00d7 coverage_multiplier (0.5 to 1.5) ``` 1. **BM25 Base Score (50%):** 2. **PageRank Boost...",
      "keywords": [
        "documents",
        "terms",
        "query",
        "matching",
        "graph",
        "bm25",
        "cortical",
        "term",
        "boost",
        "multiplier"
      ],
      "full_content": "# Graph-Boosted Search (GB-BM25) \u2014 Hybrid Ranking\n\n**Purpose:** Combine BM25 relevance with graph structure signals.\n\n**Implementation:** `cortical/query/search.py:425-564`\n\n**Scoring Formula:**\n```\nfinal_score = (0.5 \u00d7 normalized_bm25) + (0.3 \u00d7 normalized_pagerank) + (0.2 \u00d7 normalized_proximity)\n            \u00d7 coverage_multiplier (0.5 to 1.5)\n```\n\n**Three Signal Sources:**\n\n1. **BM25 Base Score (50%):**\n   - Standard term frequency \u00d7 inverse document frequency\n   - Per-document scoring using `col.tfidf_per_doc`\n\n2. **PageRank Boost (30%):**\n   - Sum of matched term PageRanks\n   - Rewards documents containing important terms\n\n3. **Proximity Boost (20%):**\n   - For each pair of original query terms:\n     - Check if they're connected in the co-occurrence graph\n     - If connected, boost documents containing both\n   - Rewards documents where query terms appear together\n\n**Coverage Multiplier:**\n- Documents matching 1/5 query terms: 0.7\u00d7 multiplier\n- Documents matching all 5 query terms: 1.5\u00d7 multiplier\n- Prevents documents matching one rare term from outranking documents matching many terms\n\n---\n\n*This chapter is part of [The Cortical Chronicles](../README.md), a self-documenting book generated by the Cortical Text Processor.*\n"
    },
    {
      "path": "01-foundations/alg-louvain.md",
      "title": "Louvain Community Detection \u2014 Concept Discovery",
      "section": "foundations",
      "tags": [
        "algorithms",
        "foundations",
        "ir-theory"
      ],
      "source_files": [
        "docs/VISION.md",
        "cortical/analysis/clustering.py"
      ],
      "excerpt": "``` for each node: find neighboring communities calculate modularity gain for moving to each move to best community if gain > 0 repeat until no nodes move ``` ``` collapse each community into a...",
      "keywords": [
        "concepts",
        "clusters",
        "phase",
        "resolution",
        "community",
        "concept",
        "cortical",
        "communities",
        "semantic",
        "algorithm"
      ],
      "full_content": "# Louvain Community Detection \u2014 Concept Discovery\n\n**Purpose:** Discover semantic clusters (concepts) from the term co-occurrence graph.\n\n**Implementation:** `cortical/analysis/clustering.py`\n\n**Two-Phase Algorithm:**\n\n**Phase 1 \u2014 Local Optimization:**\n```\nfor each node:\n    find neighboring communities\n    calculate modularity gain for moving to each\n    move to best community if gain > 0\nrepeat until no nodes move\n```\n\n**Phase 2 \u2014 Network Aggregation:**\n```\ncollapse each community into a single super-node\nedges between communities become edges between super-nodes\nrepeat Phase 1 on the aggregated network\n```\n\n**Modularity Formula:**\n```\nQ = (1/2m) \u00d7 \u03a3 [A_ij - (k_i \u00d7 k_j)/(2m)] \u00d7 \u03b4(c_i, c_j)\n```\n\nThe algorithm optimizes Q, which measures how much edge weight falls within communities versus what would be expected by random chance.\n\n**Resolution Parameter:**\n- `resolution = 1.0` (default): Balanced clusters, ~32 concepts\n- `resolution = 0.5`: Coarse clusters, ~38 concepts (max cluster 64% of tokens)\n- `resolution = 2.0`: Fine-grained clusters, ~79 concepts (max cluster 4.2% of tokens)\n\n**Concept Naming:**\n```python\ntop_members = sorted(cluster_members, key=lambda m: m.pagerank, reverse=True)[:3]\nconcept_name = '/'.join(top_members)  # e.g., \"neural/learning/networks\"\n```\n\n**Why This Matters:**\n- Enables concept-level search (\"find documents about authentication\")\n- Reduces dimensionality while preserving semantic structure\n- Creates Layer 2 (Concepts) that bridges raw terms and documents\n\n---\n\n*This chapter is part of [The Cortical Chronicles](../README.md), a self-documenting book generated by the Cortical Text Processor.*\n"
    },
    {
      "path": "01-foundations/alg-pagerank.md",
      "title": "PageRank \u2014 Importance Discovery",
      "section": "foundations",
      "tags": [
        "algorithms",
        "foundations",
        "ir-theory"
      ],
      "source_files": [
        "docs/VISION.md",
        "cortical/analysis/pagerank.py"
      ],
      "excerpt": "``` importance[term] = (1 - damping) / N + damping \u00d7 \u03a3 (neighbor_importance \u00d7 edge_weight / neighbor_outgoing_sum) ``` The algorithm iteratively propagates importance through the term co-occurrence...",
      "keywords": [
        "pagerank",
        "importance",
        "damping",
        "terms",
        "cortical",
        "term",
        "iterations",
        "layer",
        "propagates",
        "referenced"
      ],
      "full_content": "# PageRank \u2014 Importance Discovery\n\n**Purpose:** Identify which terms matter most in the corpus, independent of raw frequency.\n\n**Implementation:** `cortical/analysis/pagerank.py`\n\n**How It Works:**\n```\nimportance[term] = (1 - damping) / N + damping \u00d7 \u03a3 (neighbor_importance \u00d7 edge_weight / neighbor_outgoing_sum)\n```\n\nThe algorithm iteratively propagates importance through the term co-occurrence graph. Terms that are referenced by many important terms become important themselves\u2014a recursive definition that converges to stable values.\n\n**Key Parameters:**\n- `damping = 0.85`: The probability of following a link vs. jumping to a random node\n- `tolerance = 1e-6`: Convergence threshold (stops when no term changes by more than this)\n- `max_iterations = 20`: Upper bound on iterations\n\n**Three Variants:**\n1. **Standard PageRank**: Applied to Layer 0 (tokens) and Layer 1 (bigrams)\n2. **Semantic PageRank**: Adjusts edge weights by relation type (IsA connections count 1.5\u00d7 more than CoOccurs)\n3. **Hierarchical PageRank**: Propagates importance across all 4 layers with separate cross-layer damping\n\n**Why This Matters for Code Search:**\n- Common utility functions referenced everywhere get high PageRank\n- Core abstractions that everything depends on surface naturally\n- Prevents over-emphasis on boilerplate code that appears frequently but isn't semantically central\n\n**Performance:** O(iterations \u00d7 edges), typically 100-500ms for 10K tokens with early convergence usually at 5-10 iterations.\n\n---\n\n*This chapter is part of [The Cortical Chronicles](../README.md), a self-documenting book generated by the Cortical Text Processor.*\n"
    },
    {
      "path": "01-foundations/alg-query-expansion.md",
      "title": "Query Expansion \u2014 Semantic Bridging",
      "section": "foundations",
      "tags": [
        "algorithms",
        "foundations",
        "ir-theory"
      ],
      "source_files": [
        "docs/VISION.md",
        "cortical/query/expansion.py"
      ],
      "excerpt": "1. **Lateral Connection Expansion:** 2. **Concept Cluster Membership:** 3. **Code Concept Synonyms:** ``` Query: \"neural\" Hop 0: neural (1.0) Hop 1: networks (0.4), learning (0.35) Hop 2: deep...",
      "keywords": [
        "query",
        "expansion",
        "hop",
        "terms",
        "term",
        "cortical",
        "concept",
        "score",
        "per",
        "cluster"
      ],
      "full_content": "# Query Expansion \u2014 Semantic Bridging\n\n**Purpose:** Transform literal query terms into semantically enriched term sets.\n\n**Implementation:** `cortical/query/expansion.py`\n\n**Three Expansion Methods:**\n\n1. **Lateral Connection Expansion:**\n   - Follow co-occurrence edges from query terms\n   - Score: `edge_weight \u00d7 neighbor_score \u00d7 0.6`\n   - Takes top 5 neighbors per query term\n\n2. **Concept Cluster Membership:**\n   - Find concepts containing query terms\n   - Add other cluster members as expansions\n   - Score: `concept.pagerank \u00d7 member.pagerank \u00d7 0.4`\n\n3. **Code Concept Synonyms:**\n   - Programming-specific synonym groups (get/fetch/load, create/make/build)\n   - Limited to 3 synonyms per term to prevent drift\n\n**Multi-Hop Inference:**\n```\nQuery: \"neural\"\n  Hop 0: neural (1.0)\n  Hop 1: networks (0.4), learning (0.35)\n  Hop 2: deep (0.098) \u2014 via learning with decay\n```\n\nChain validity is scored by relation type pairs:\n- `(IsA, IsA)`: 1.0 \u2014 fully transitive (dog\u2192animal\u2192living_thing)\n- `(RelatedTo, RelatedTo)`: 0.6 \u2014 weaker transitivity\n- `(Antonym, Antonym)`: 0.3 \u2014 double negation, avoid\n\n---\n\n*This chapter is part of [The Cortical Chronicles](../README.md), a self-documenting book generated by the Cortical Text Processor.*\n"
    },
    {
      "path": "01-foundations/alg-semantic-extraction.md",
      "title": "Semantic Relation Extraction \u2014 Knowledge Graph Construction",
      "section": "foundations",
      "tags": [
        "algorithms",
        "foundations",
        "ir-theory"
      ],
      "source_files": [
        "docs/VISION.md",
        "cortical/semantics.py"
      ],
      "excerpt": "24 regex patterns detect 10+ relation types: ```python r'(\\w+)\\s+(?:is|are)\\s+(?:a|an)\\s+(?:type\\s+of\\s+)?(\\w+)' \u2192 IsA (0.9 confidence) r'(\\w+)\\s+(?:is|are)\\s+(?:a\\s+)?part\\s+of' \u2192 PartOf (0.95...",
      "keywords": [
        "relation",
        "semantic",
        "isa",
        "partof",
        "causes",
        "cortical",
        "confidence",
        "extraction",
        "knowledge",
        "text"
      ],
      "full_content": "# Semantic Relation Extraction \u2014 Knowledge Graph Construction\n\n**Purpose:** Extract typed relationships (IsA, PartOf, Causes) from document text.\n\n**Implementation:** `cortical/semantics.py`\n\n**Pattern-Based Extraction:**\n24 regex patterns detect 10+ relation types:\n```python\nr'(\\w+)\\s+(?:is|are)\\s+(?:a|an)\\s+(?:type\\s+of\\s+)?(\\w+)' \u2192 IsA (0.9 confidence)\nr'(\\w+)\\s+(?:is|are)\\s+(?:a\\s+)?part\\s+of' \u2192 PartOf (0.95 confidence)\nr'(\\w+)\\s+(?:causes|leads?\\s+to)' \u2192 Causes (0.9 confidence)\n```\n\n**Semantic Retrofitting:**\nBlends co-occurrence weights with semantic relation knowledge:\n```\nnew_weight = \u03b1 \u00d7 original_weight + (1-\u03b1) \u00d7 semantic_target_weight\n```\nWith \u03b1 = 0.3, semantic signals dominate (70%) while preserving some corpus statistics (30%).\n\n**Relation Weight Multipliers:**\n| Relation | Weight | Semantics |\n|----------|--------|-----------|\n| SameAs | 2.0 | Strongest synonymy |\n| IsA | 1.5 | Hypernymy |\n| PartOf | 1.3 | Meronymy |\n| RelatedTo | 0.8 | Generic |\n| Antonym | -0.5 | Opposition |\n\n---\n\n*This chapter is part of [The Cortical Chronicles](../README.md), a self-documenting book generated by the Cortical Text Processor.*\n"
    },
    {
      "path": "02-architecture/index.md",
      "title": "Architecture Overview",
      "section": "architecture",
      "tags": [
        "architecture",
        "index",
        "modules"
      ],
      "source_files": [
        "/home/user/Opus-code-test/cortical/patterns.py",
        "/home/user/Opus-code-test/cortical/code_concepts.py",
        "/home/user/Opus-code-test/cortical/persistence.py",
        "/home/user/Opus-code-test/cortical/semantics.py",
        "/home/user/Opus-code-test/cortical/constants.py",
        "/home/user/Opus-code-test/cortical/embeddings.py",
        "/home/user/Opus-code-test/cortical/chunk_index.py",
        "/home/user/Opus-code-test/cortical/results.py",
        "/home/user/Opus-code-test/cortical/tokenizer.py",
        "/home/user/Opus-code-test/cortical/observability.py",
        "/home/user/Opus-code-test/cortical/diff.py",
        "/home/user/Opus-code-test/cortical/progress.py",
        "/home/user/Opus-code-test/cortical/gaps.py",
        "/home/user/Opus-code-test/cortical/minicolumn.py",
        "/home/user/Opus-code-test/cortical/config.py",
        "/home/user/Opus-code-test/cortical/fingerprint.py",
        "/home/user/Opus-code-test/cortical/layers.py",
        "/home/user/Opus-code-test/cortical/mcp_server.py",
        "/home/user/Opus-code-test/cortical/fluent.py",
        "/home/user/Opus-code-test/cortical/cli_wrapper.py",
        "/home/user/Opus-code-test/cortical/types.py",
        "/home/user/Opus-code-test/cortical/state_storage.py",
        "/home/user/Opus-code-test/cortical/validation.py",
        "/home/user/Opus-code-test/cortical/analysis/clustering.py",
        "/home/user/Opus-code-test/cortical/analysis/quality.py",
        "/home/user/Opus-code-test/cortical/analysis/pagerank.py",
        "/home/user/Opus-code-test/cortical/analysis/__init__.py",
        "/home/user/Opus-code-test/cortical/analysis/tfidf.py",
        "/home/user/Opus-code-test/cortical/analysis/activation.py",
        "/home/user/Opus-code-test/cortical/analysis/connections.py",
        "/home/user/Opus-code-test/cortical/analysis/utils.py",
        "/home/user/Opus-code-test/cortical/query/ranking.py",
        "/home/user/Opus-code-test/cortical/query/intent.py",
        "/home/user/Opus-code-test/cortical/query/passages.py",
        "/home/user/Opus-code-test/cortical/query/__init__.py",
        "/home/user/Opus-code-test/cortical/query/search.py",
        "/home/user/Opus-code-test/cortical/query/definitions.py",
        "/home/user/Opus-code-test/cortical/query/chunking.py",
        "/home/user/Opus-code-test/cortical/query/expansion.py",
        "/home/user/Opus-code-test/cortical/processor/persistence_api.py",
        "/home/user/Opus-code-test/cortical/processor/documents.py",
        "/home/user/Opus-code-test/cortical/processor/__init__.py",
        "/home/user/Opus-code-test/cortical/processor/core.py",
        "/home/user/Opus-code-test/cortical/processor/introspection.py",
        "/home/user/Opus-code-test/cortical/processor/compute.py"
      ],
      "excerpt": "This section documents the architecture of the Cortical Text Processor through automatically extracted module metadata. 8 modules: 3 modules: 3 modules: 3 modules: 3 modules: 3 modules: 6 modules: 8...",
      "keywords": [
        "modules",
        "mod",
        "processor",
        "cortical",
        "module",
        "__init__",
        "observability",
        "persistence",
        "architecture",
        "documents"
      ],
      "full_content": "# Architecture Overview\n\nThis section documents the architecture of the Cortical Text Processor through automatically extracted module metadata.\n\n## Statistics\n\n- **Total Modules**: 45\n- **Module Groups**: 9\n- **Classes**: 50\n- **Functions**: 422\n\n## Module Groups\n\n### [Analysis](mod-analysis.md)\n\n8 modules:\n\n- `__init__.py`\n- `activation.py`\n- `clustering.py`\n- `connections.py`\n- `pagerank.py`\n- `quality.py`\n- `tfidf.py`\n- `utils.py`\n\n### [Configuration](mod-configuration.md)\n\n3 modules:\n\n- `config.py`\n- `constants.py`\n- `validation.py`\n\n### [Data Structures](mod-data-structures.md)\n\n3 modules:\n\n- `layers.py`\n- `minicolumn.py`\n- `types.py`\n\n### [Nlp](mod-nlp.md)\n\n3 modules:\n\n- `embeddings.py`\n- `semantics.py`\n- `tokenizer.py`\n\n### [Observability](mod-observability.md)\n\n3 modules:\n\n- `observability.py`\n- `progress.py`\n- `results.py`\n\n### [Persistence](mod-persistence.md)\n\n3 modules:\n\n- `chunk_index.py`\n- `persistence.py`\n- `state_storage.py`\n\n### [Processor](mod-processor.md)\n\n6 modules:\n\n- `__init__.py`\n- `compute.py`\n- `core.py`\n- `documents.py`\n- `introspection.py`\n- `persistence_api.py`\n\n### [Query](mod-query.md)\n\n8 modules:\n\n- `__init__.py`\n- `chunking.py`\n- `definitions.py`\n- `expansion.py`\n- `intent.py`\n- `passages.py`\n- `ranking.py`\n- `search.py`\n\n### [Utilities](mod-utilities.md)\n\n8 modules:\n\n- `cli_wrapper.py`\n- `code_concepts.py`\n- `diff.py`\n- `fingerprint.py`\n- `fluent.py`\n- `gaps.py`\n- `mcp_server.py`\n- `patterns.py`\n\n---\n\n*This chapter is part of [The Cortical Chronicles](../README.md), a self-documenting book generated by the Cortical Text Processor.*\n"
    },
    {
      "path": "02-architecture/mod-analysis.md",
      "title": "Graph Algorithms",
      "section": "architecture",
      "tags": [
        "architecture",
        "modules",
        "analysis"
      ],
      "source_files": [
        "/home/user/Opus-code-test/cortical/analysis/clustering.py",
        "/home/user/Opus-code-test/cortical/analysis/quality.py",
        "/home/user/Opus-code-test/cortical/analysis/pagerank.py",
        "/home/user/Opus-code-test/cortical/analysis/__init__.py",
        "/home/user/Opus-code-test/cortical/analysis/tfidf.py",
        "/home/user/Opus-code-test/cortical/analysis/activation.py",
        "/home/user/Opus-code-test/cortical/analysis/connections.py",
        "/home/user/Opus-code-test/cortical/analysis/utils.py"
      ],
      "excerpt": "Graph algorithms for computing importance, relevance, and clusters. Analysis Module =============== Graph analysis algorithms for the cortical network. Contains implementations of: Activation...",
      "keywords": [
        "float",
        "dict",
        "int",
        "str",
        "layers",
        "hierarchicallayer",
        "python",
        "corticallayer",
        "algorithms",
        "clustering"
      ],
      "full_content": "# Graph Algorithms\n\nGraph algorithms for computing importance, relevance, and clusters.\n\n## Modules\n\n- **__init__.py**: Analysis Module\n- **activation.py**: Activation propagation algorithm.\n- **clustering.py**: Clustering algorithms for community detection.\n- **connections.py**: Connection building algorithms for network layers.\n- **pagerank.py**: PageRank algorithms for importance scoring.\n- **quality.py**: Clustering quality metrics.\n- **tfidf.py**: TF-IDF and BM25 scoring algorithms.\n- **utils.py**: Utility functions and classes for analysis algorithms.\n\n\n## __init__.py\n\nAnalysis Module\n===============\n\nGraph analysis algorithms for the cortical network.\n\nContains implementations of:\n- PageRank for importance scoring\n- TF-IDF for term weighting\n- Louvain community det...\n\n\n### Dependencies\n\n**Standard Library:**\n\n- `activation.propagate_activation`\n- `clustering._louvain_core`\n- `clustering.build_concept_clusters`\n- `clustering.cluster_by_label_propagation`\n- `clustering.cluster_by_louvain`\n- ... and 22 more\n\n\n\n## activation.py\n\nActivation propagation algorithm.\n\nContains:\n- propagate_activation: Spread activation through the network layers\n\n\n### Functions\n\n#### propagate_activation\n\n```python\npropagate_activation(layers: Dict[CorticalLayer, HierarchicalLayer], iterations: int = 3, decay: float = 0.8, lateral_weight: float = 0.3) -> None\n```\n\nPropagate activation through the network.\n\n### Dependencies\n\n**Standard Library:**\n\n- `layers.CorticalLayer`\n- `layers.HierarchicalLayer`\n- `typing.Dict`\n\n\n\n## clustering.py\n\nClustering algorithms for community detection.\n\nContains:\n- cluster_by_louvain: Louvain modularity optimization (recommended)\n- cluster_by_label_propagation: Label propagation clustering (legacy)\n- bu...\n\n\n### Functions\n\n#### cluster_by_label_propagation\n\n```python\ncluster_by_label_propagation(layer: HierarchicalLayer, min_cluster_size: int = 3, max_iterations: int = 20, cluster_strictness: float = 1.0, bridge_weight: float = 0.0) -> Dict[int, List[str]]\n```\n\nCluster minicolumns using label propagation.\n\n#### cluster_by_louvain\n\n```python\ncluster_by_louvain(layer: HierarchicalLayer, min_cluster_size: int = 3, resolution: float = 1.0, max_iterations: int = 10) -> Dict[int, List[str]]\n```\n\nCluster minicolumns using Louvain community detection.\n\n#### build_concept_clusters\n\n```python\nbuild_concept_clusters(layers: Dict[CorticalLayer, HierarchicalLayer], clusters: Dict[int, List[str]], doc_vote_threshold: float = 0.1) -> None\n```\n\nBuild concept layer from token clusters.\n\n### Dependencies\n\n**Standard Library:**\n\n- `collections.defaultdict`\n- `layers.CorticalLayer`\n- `layers.HierarchicalLayer`\n- `typing.Dict`\n- `typing.List`\n- ... and 2 more\n\n\n\n## connections.py\n\nConnection building algorithms for network layers.\n\nContains:\n- compute_document_connections: Build document-to-document similarity connections\n- compute_bigram_connections: Build lateral connections ...\n\n\n### Functions\n\n#### compute_concept_connections\n\n```python\ncompute_concept_connections(layers: Dict[CorticalLayer, HierarchicalLayer], semantic_relations: List[Tuple[str, str, str, float]] = None, min_shared_docs: int = 1, min_jaccard: float = 0.1, use_member_semantics: bool = False, use_embedding_similarity: bool = False, embedding_threshold: float = 0.3, embeddings: Dict[str, List[float]] = None) -> Dict[str, Any]\n```\n\nBuild lateral connections between concepts in Layer 2.\n\n#### compute_bigram_connections\n\n```python\ncompute_bigram_connections(layers: Dict[CorticalLayer, HierarchicalLayer], min_shared_docs: int = 1, component_weight: float = 0.5, chain_weight: float = 0.7, cooccurrence_weight: float = 0.3, max_bigrams_per_term: int = 100, max_bigrams_per_doc: int = 500, max_connections_per_bigram: int = 50) -> Dict[str, Any]\n```\n\nBuild lateral connections between bigrams in Layer 1.\n\n#### compute_document_connections\n\n```python\ncompute_document_connections(layers: Dict[CorticalLayer, HierarchicalLayer], documents: Dict[str, str], min_shared_terms: int = 3) -> None\n```\n\nBuild lateral connections between documents.\n\n### Dependencies\n\n**Standard Library:**\n\n- `collections.defaultdict`\n- `layers.CorticalLayer`\n- `layers.HierarchicalLayer`\n- `minicolumn.Minicolumn`\n- `typing.Any`\n- ... and 5 more\n\n\n\n## pagerank.py\n\nPageRank algorithms for importance scoring.\n\nContains:\n- compute_pagerank: Standard PageRank for a single layer\n- compute_semantic_pagerank: PageRank with semantic relation weighting\n- compute_hierarc...\n\n\n### Functions\n\n#### compute_pagerank\n\n```python\ncompute_pagerank(layer: HierarchicalLayer, damping: float = 0.85, iterations: int = 20, tolerance: float = 1e-06) -> Dict[str, float]\n```\n\nCompute PageRank scores for minicolumns in a layer.\n\n#### compute_semantic_pagerank\n\n```python\ncompute_semantic_pagerank(layer: HierarchicalLayer, semantic_relations: List[Tuple[str, str, str, float]], relation_weights: Optional[Dict[str, float]] = None, damping: float = 0.85, iterations: int = 20, tolerance: float = 1e-06) -> Dict[str, Any]\n```\n\nCompute PageRank with semantic relation type weighting.\n\n#### compute_hierarchical_pagerank\n\n```python\ncompute_hierarchical_pagerank(layers: Dict[CorticalLayer, HierarchicalLayer], layer_iterations: int = 10, global_iterations: int = 5, damping: float = 0.85, cross_layer_damping: float = 0.7, tolerance: float = 0.0001) -> Dict[str, Any]\n```\n\nCompute PageRank with cross-layer propagation.\n\n### Dependencies\n\n**Standard Library:**\n\n- `collections.defaultdict`\n- `constants.RELATION_WEIGHTS`\n- `layers.CorticalLayer`\n- `layers.HierarchicalLayer`\n- `typing.Any`\n- ... and 4 more\n\n\n\n## quality.py\n\nClustering quality metrics.\n\nContains:\n- compute_clustering_quality: Comprehensive quality evaluation (modularity, silhouette, balance)\n- _compute_modularity: Modularity Q metric\n- _compute_silhouette...\n\n\n### Functions\n\n#### compute_clustering_quality\n\n```python\ncompute_clustering_quality(layers: Dict[CorticalLayer, HierarchicalLayer], sample_size: int = 500) -> Dict[str, Any]\n```\n\nCompute clustering quality metrics for the concept layer.\n\n### Dependencies\n\n**Standard Library:**\n\n- `collections.defaultdict`\n- `layers.CorticalLayer`\n- `layers.HierarchicalLayer`\n- `random`\n- `typing.Any`\n- ... and 3 more\n\n\n\n## tfidf.py\n\nTF-IDF and BM25 scoring algorithms.\n\nContains:\n- compute_tfidf: Traditional TF-IDF scoring\n- compute_bm25: Okapi BM25 scoring with length normalization\n- _tfidf_core: Pure TF-IDF algorithm for unit te...\n\n\n### Functions\n\n#### compute_tfidf\n\n```python\ncompute_tfidf(layers: Dict[CorticalLayer, HierarchicalLayer], documents: Dict[str, str]) -> None\n```\n\nCompute TF-IDF scores for tokens.\n\n#### compute_bm25\n\n```python\ncompute_bm25(layers: Dict[CorticalLayer, HierarchicalLayer], documents: Dict[str, str], doc_lengths: Dict[str, int], avg_doc_length: float, k1: float = 1.2, b: float = 0.75) -> None\n```\n\nCompute BM25 scores for tokens.\n\n### Dependencies\n\n**Standard Library:**\n\n- `layers.CorticalLayer`\n- `layers.HierarchicalLayer`\n- `math`\n- `typing.Dict`\n- `typing.Tuple`\n\n\n\n## utils.py\n\nUtility functions and classes for analysis algorithms.\n\nContains:\n- SparseMatrix: Zero-dependency sparse matrix for bigram connections\n- Similarity functions: cosine_similarity, _doc_similarity, _vect...\n\n\n### Classes\n\n#### SparseMatrix\n\nSimple sparse matrix implementation using dictionary of keys (DOK) format.\n\n**Methods:**\n\n- `set`\n- `get`\n- `multiply_transpose`\n- `get_nonzero`\n\n### Functions\n\n#### cosine_similarity\n\n```python\ncosine_similarity(vec1: Dict[str, float], vec2: Dict[str, float]) -> float\n```\n\nCompute cosine similarity between two sparse vectors.\n\n#### SparseMatrix.set\n\n```python\nSparseMatrix.set(self, row: int, col: int, value: float) -> None\n```\n\nSet value at (row, col).\n\n#### SparseMatrix.get\n\n```python\nSparseMatrix.get(self, row: int, col: int) -> float\n```\n\nGet value at (row, col).\n\n#### SparseMatrix.multiply_transpose\n\n```python\nSparseMatrix.multiply_transpose(self) -> 'SparseMatrix'\n```\n\nMultiply this matrix by its transpose: M * M^T\n\n#### SparseMatrix.get_nonzero\n\n```python\nSparseMatrix.get_nonzero(self) -> List[Tuple[int, int, float]]\n```\n\nGet all non-zero entries.\n\n### Dependencies\n\n**Standard Library:**\n\n- `collections.defaultdict`\n- `math`\n- `typing.Dict`\n- `typing.List`\n- `typing.Tuple`\n\n\n\n---\n\n*This chapter is part of [The Cortical Chronicles](../README.md), a self-documenting book generated by the Cortical Text Processor.*\n"
    },
    {
      "path": "02-architecture/mod-configuration.md",
      "title": "Configuration",
      "section": "architecture",
      "tags": [
        "architecture",
        "modules",
        "configuration"
      ],
      "source_files": [
        "/home/user/Opus-code-test/cortical/constants.py",
        "/home/user/Opus-code-test/cortical/config.py",
        "/home/user/Opus-code-test/cortical/validation.py"
      ],
      "excerpt": "Configuration management and validation. Configuration Module ==================== Centralized configuration for the Cortical Text Processor. This module provides a dataclass-based configuration...",
      "keywords": [
        "configuration",
        "python",
        "corticalconfig",
        "value",
        "module",
        "cortical",
        "typing",
        "none",
        "validation",
        "text"
      ],
      "full_content": "# Configuration\n\nConfiguration management and validation.\n\n## Modules\n\n- **config.py**: Configuration Module\n- **constants.py**: Centralized constants for the Cortical Text Processor.\n- **validation.py**: Validation Module\n\n\n## config.py\n\nConfiguration Module\n====================\n\nCentralized configuration for the Cortical Text Processor.\n\nThis module provides a dataclass-based configuration system that allows\nusers to customize algori...\n\n\n### Classes\n\n#### CorticalConfig\n\nConfiguration settings for the Cortical Text Processor.\n\n**Methods:**\n\n- `copy`\n- `to_dict`\n- `from_dict`\n\n### Functions\n\n#### get_default_config\n\n```python\nget_default_config() -> CorticalConfig\n```\n\nGet a new instance of the default configuration.\n\n#### CorticalConfig.copy\n\n```python\nCorticalConfig.copy(self) -> 'CorticalConfig'\n```\n\nCreate a copy of this configuration.\n\n#### CorticalConfig.to_dict\n\n```python\nCorticalConfig.to_dict(self) -> Dict\n```\n\nConvert configuration to a dictionary for serialization.\n\n#### CorticalConfig.from_dict\n\n```python\nCorticalConfig.from_dict(cls, data: Dict) -> 'CorticalConfig'\n```\n\nCreate configuration from a dictionary.\n\n### Dependencies\n\n**Standard Library:**\n\n- `dataclasses.dataclass`\n- `dataclasses.field`\n- `math`\n- `typing.Dict`\n- `typing.FrozenSet`\n- ... and 1 more\n\n\n\n## constants.py\n\nCentralized constants for the Cortical Text Processor.\n\nThis module provides a single source of truth for constants used across\nmultiple modules, preventing drift and inconsistencies.\n\nTask #96: Centr...\n\n\n### Dependencies\n\n**Standard Library:**\n\n- `typing.Dict`\n- `typing.FrozenSet`\n\n\n\n## validation.py\n\nValidation Module\n=================\n\nInput validation utilities and decorators for the Cortical Text Processor.\n\nThis module provides reusable validators and decorators to ensure\nparameters are valid ...\n\n\n### Functions\n\n#### validate_non_empty_string\n\n```python\nvalidate_non_empty_string(value: Any, param_name: str) -> None\n```\n\nValidate that a value is a non-empty string.\n\n#### validate_positive_int\n\n```python\nvalidate_positive_int(value: Any, param_name: str) -> None\n```\n\nValidate that a value is a positive integer.\n\n#### validate_non_negative_int\n\n```python\nvalidate_non_negative_int(value: Any, param_name: str) -> None\n```\n\nValidate that a value is a non-negative integer.\n\n#### validate_range\n\n```python\nvalidate_range(value: Any, param_name: str, min_val: Optional[float] = None, max_val: Optional[float] = None, inclusive: bool = True) -> None\n```\n\nValidate that a numeric value is within a specified range.\n\n#### validate_params\n\n```python\nvalidate_params(**validators: Callable[[Any], None]) -> Callable[[F], F]\n```\n\nDecorator to validate function parameters.\n\n#### marks_stale\n\n```python\nmarks_stale(*computation_types: str) -> Callable[[F], F]\n```\n\nDecorator to mark computations as stale after method execution.\n\n#### marks_fresh\n\n```python\nmarks_fresh(*computation_types: str) -> Callable[[F], F]\n```\n\nDecorator to mark computations as fresh after method execution.\n\n### Dependencies\n\n**Standard Library:**\n\n- `functools.wraps`\n- `inspect`\n- `typing.Any`\n- `typing.Callable`\n- `typing.Optional`\n- ... and 2 more\n\n\n\n---\n\n*This chapter is part of [The Cortical Chronicles](../README.md), a self-documenting book generated by the Cortical Text Processor.*\n"
    },
    {
      "path": "02-architecture/mod-data-structures.md",
      "title": "Data Structures",
      "section": "architecture",
      "tags": [
        "architecture",
        "modules",
        "data-structures"
      ],
      "source_files": [
        "/home/user/Opus-code-test/cortical/minicolumn.py",
        "/home/user/Opus-code-test/cortical/layers.py",
        "/home/user/Opus-code-test/cortical/types.py"
      ],
      "excerpt": "Fundamental data structures used throughout the system. Layers Module ============= Defines the hierarchical layer structure inspired by the visual cortex. The neocortex processes information through...",
      "keywords": [
        "minicolumn",
        "python",
        "hierarchicallayer",
        "self",
        "str",
        "layer",
        "float",
        "dict",
        "edge",
        "get"
      ],
      "full_content": "# Data Structures\n\nFundamental data structures used throughout the system.\n\n## Modules\n\n- **layers.py**: Layers Module\n- **minicolumn.py**: Minicolumn Module\n- **types.py**: Type Aliases for the Cortical Text Processor.\n\n\n## layers.py\n\nLayers Module\n=============\n\nDefines the hierarchical layer structure inspired by the visual cortex.\n\nThe neocortex processes information through a hierarchy of layers,\neach extracting progressively m...\n\n\n### Classes\n\n#### CorticalLayer\n\nEnumeration of cortical processing layers.\n\n**Methods:**\n\n- `description`\n- `analogy`\n\n#### HierarchicalLayer\n\nA layer in the cortical hierarchy containing minicolumns.\n\n**Methods:**\n\n- `get_or_create_minicolumn`\n- `get_minicolumn`\n- `get_by_id`\n- `remove_minicolumn`\n- `column_count`\n- `total_connections`\n- `average_activation`\n- `activation_range`\n- `sparsity`\n- `top_by_pagerank`\n- `top_by_tfidf`\n- `top_by_activation`\n- `to_dict`\n- `from_dict`\n\n### Functions\n\n#### CorticalLayer.description\n\n```python\nCorticalLayer.description(self) -> str\n```\n\nHuman-readable description of this layer.\n\n#### CorticalLayer.analogy\n\n```python\nCorticalLayer.analogy(self) -> str\n```\n\nVisual cortex analogy for this layer.\n\n#### HierarchicalLayer.get_or_create_minicolumn\n\n```python\nHierarchicalLayer.get_or_create_minicolumn(self, content: str) -> Minicolumn\n```\n\nGet existing minicolumn or create new one.\n\n#### HierarchicalLayer.get_minicolumn\n\n```python\nHierarchicalLayer.get_minicolumn(self, content: str) -> Optional[Minicolumn]\n```\n\nGet a minicolumn by content, or None if not found.\n\n#### HierarchicalLayer.get_by_id\n\n```python\nHierarchicalLayer.get_by_id(self, col_id: str) -> Optional[Minicolumn]\n```\n\nGet a minicolumn by its ID in O(1) time.\n\n#### HierarchicalLayer.remove_minicolumn\n\n```python\nHierarchicalLayer.remove_minicolumn(self, content: str) -> bool\n```\n\nRemove a minicolumn from this layer.\n\n#### HierarchicalLayer.column_count\n\n```python\nHierarchicalLayer.column_count(self) -> int\n```\n\nReturn the number of minicolumns in this layer.\n\n#### HierarchicalLayer.total_connections\n\n```python\nHierarchicalLayer.total_connections(self) -> int\n```\n\nReturn total number of lateral connections in this layer.\n\n#### HierarchicalLayer.average_activation\n\n```python\nHierarchicalLayer.average_activation(self) -> float\n```\n\nCalculate average activation across all minicolumns.\n\n#### HierarchicalLayer.activation_range\n\n```python\nHierarchicalLayer.activation_range(self) -> tuple\n```\n\nReturn (min, max) activation values.\n\n#### HierarchicalLayer.sparsity\n\n```python\nHierarchicalLayer.sparsity(self, threshold_fraction: float = 0.5) -> float\n```\n\nCalculate sparsity (fraction of columns with below-average activation).\n\n#### HierarchicalLayer.top_by_pagerank\n\n```python\nHierarchicalLayer.top_by_pagerank(self, n: int = 10) -> list\n```\n\nGet top minicolumns by PageRank score.\n\n#### HierarchicalLayer.top_by_tfidf\n\n```python\nHierarchicalLayer.top_by_tfidf(self, n: int = 10) -> list\n```\n\nGet top minicolumns by TF-IDF score.\n\n#### HierarchicalLayer.top_by_activation\n\n```python\nHierarchicalLayer.top_by_activation(self, n: int = 10) -> list\n```\n\nGet top minicolumns by activation level.\n\n#### HierarchicalLayer.to_dict\n\n```python\nHierarchicalLayer.to_dict(self) -> Dict\n```\n\nConvert layer to dictionary for serialization.\n\n#### HierarchicalLayer.from_dict\n\n```python\nHierarchicalLayer.from_dict(cls, data: Dict) -> 'HierarchicalLayer'\n```\n\nCreate a layer from dictionary representation.\n\n### Dependencies\n\n**Standard Library:**\n\n- `enum.IntEnum`\n- `minicolumn.Minicolumn`\n- `typing.Dict`\n- `typing.Iterator`\n- `typing.Optional`\n\n\n\n## minicolumn.py\n\nMinicolumn Module\n=================\n\nCore data structure representing a cortical minicolumn.\n\nIn the neocortex, minicolumns are vertical structures containing\n~80-100 neurons that respond to similar f...\n\n\n### Classes\n\n#### Edge\n\nTyped edge with metadata for ConceptNet-style graph representation.\n\n**Methods:**\n\n- `to_dict`\n- `from_dict`\n\n#### Minicolumn\n\nA minicolumn represents a single concept/feature at a given hierarchy level.\n\n**Methods:**\n\n- `lateral_connections`\n- `lateral_connections`\n- `add_lateral_connection`\n- `add_lateral_connections_batch`\n- `set_lateral_connection_weight`\n- `add_typed_connection`\n- `get_typed_connection`\n- `get_connections_by_type`\n- `get_connections_by_source`\n- `add_feedforward_connection`\n- `add_feedback_connection`\n- `connection_count`\n- `top_connections`\n- `to_dict`\n- `from_dict`\n\n### Functions\n\n#### Edge.to_dict\n\n```python\nEdge.to_dict(self) -> Dict\n```\n\nConvert to dictionary for serialization.\n\n#### Edge.from_dict\n\n```python\nEdge.from_dict(cls, data: Dict) -> 'Edge'\n```\n\nCreate an Edge from dictionary representation.\n\n#### Minicolumn.lateral_connections\n\n```python\nMinicolumn.lateral_connections(self, value: Dict[str, float]) -> None\n```\n\nSet lateral connections from a dictionary (for deserialization).\n\n#### Minicolumn.add_lateral_connection\n\n```python\nMinicolumn.add_lateral_connection(self, target_id: str, weight: float = 1.0) -> None\n```\n\nAdd or strengthen a lateral connection to another column.\n\n#### Minicolumn.add_lateral_connections_batch\n\n```python\nMinicolumn.add_lateral_connections_batch(self, connections: Dict[str, float]) -> None\n```\n\nAdd or strengthen multiple lateral connections at once.\n\n#### Minicolumn.set_lateral_connection_weight\n\n```python\nMinicolumn.set_lateral_connection_weight(self, target_id: str, weight: float) -> None\n```\n\nSet the weight of a lateral connection directly (not additive).\n\n#### Minicolumn.add_typed_connection\n\n```python\nMinicolumn.add_typed_connection(self, target_id: str, weight: float = 1.0, relation_type: str = 'co_occurrence', confidence: float = 1.0, source: str = 'corpus') -> None\n```\n\nAdd or update a typed connection with metadata.\n\n#### Minicolumn.get_typed_connection\n\n```python\nMinicolumn.get_typed_connection(self, target_id: str) -> Optional[Edge]\n```\n\nGet a typed connection by target ID.\n\n#### Minicolumn.get_connections_by_type\n\n```python\nMinicolumn.get_connections_by_type(self, relation_type: str) -> List[Edge]\n```\n\nGet all typed connections with a specific relation type.\n\n#### Minicolumn.get_connections_by_source\n\n```python\nMinicolumn.get_connections_by_source(self, source: str) -> List[Edge]\n```\n\nGet all typed connections from a specific source.\n\n#### Minicolumn.add_feedforward_connection\n\n```python\nMinicolumn.add_feedforward_connection(self, target_id: str, weight: float = 1.0) -> None\n```\n\nAdd or strengthen a feedforward connection to a lower layer column.\n\n#### Minicolumn.add_feedback_connection\n\n```python\nMinicolumn.add_feedback_connection(self, target_id: str, weight: float = 1.0) -> None\n```\n\nAdd or strengthen a feedback connection to a higher layer column.\n\n#### Minicolumn.connection_count\n\n```python\nMinicolumn.connection_count(self) -> int\n```\n\nReturn the number of lateral connections.\n\n#### Minicolumn.top_connections\n\n```python\nMinicolumn.top_connections(self, n: int = 5) -> list\n```\n\nGet the strongest lateral connections.\n\n#### Minicolumn.to_dict\n\n```python\nMinicolumn.to_dict(self) -> Dict\n```\n\nConvert to dictionary for serialization.\n\n#### Minicolumn.from_dict\n\n```python\nMinicolumn.from_dict(cls, data: Dict) -> 'Minicolumn'\n```\n\nCreate a minicolumn from dictionary representation.\n\n### Dependencies\n\n**Standard Library:**\n\n- `dataclasses.asdict`\n- `dataclasses.dataclass`\n- `dataclasses.field`\n- `typing.Dict`\n- `typing.List`\n- ... and 2 more\n\n\n\n## types.py\n\nType Aliases for the Cortical Text Processor.\n\nThis module provides type aliases for complex return types used throughout\nthe library, making function signatures more readable and maintainable.\n\nTask ...\n\n\n### Dependencies\n\n**Standard Library:**\n\n- `typing.Any`\n- `typing.Dict`\n- `typing.List`\n- `typing.Optional`\n- `typing.Tuple`\n\n\n\n---\n\n*This chapter is part of [The Cortical Chronicles](../README.md), a self-documenting book generated by the Cortical Text Processor.*\n"
    },
    {
      "path": "02-architecture/mod-nlp.md",
      "title": "NLP Components",
      "section": "architecture",
      "tags": [
        "architecture",
        "modules",
        "nlp"
      ],
      "source_files": [
        "/home/user/Opus-code-test/cortical/semantics.py",
        "/home/user/Opus-code-test/cortical/embeddings.py",
        "/home/user/Opus-code-test/cortical/tokenizer.py"
      ],
      "excerpt": "Natural language processing components for tokenization and semantics. Embeddings Module ================= Graph-based embeddings for the cortical network. Implements three methods for computing term...",
      "keywords": [
        "str",
        "dict",
        "float",
        "python",
        "list",
        "tokenizer",
        "int",
        "tuple",
        "embeddings",
        "term"
      ],
      "full_content": "# NLP Components\n\nNatural language processing components for tokenization and semantics.\n\n## Modules\n\n- **embeddings.py**: Embeddings Module\n- **semantics.py**: Semantics Module\n- **tokenizer.py**: Tokenizer Module\n\n\n## embeddings.py\n\nEmbeddings Module\n=================\n\nGraph-based embeddings for the cortical network.\n\nImplements three methods for computing term embeddings from the\nconnection graph structure:\n1. Adjacency: Direct ...\n\n\n### Functions\n\n#### compute_graph_embeddings\n\n```python\ncompute_graph_embeddings(layers: Dict[CorticalLayer, HierarchicalLayer], dimensions: int = 64, method: str = 'adjacency', max_terms: Optional[int] = None) -> Tuple[Dict[str, List[float]], Dict[str, Any]]\n```\n\nCompute embeddings for tokens based on graph structure.\n\n#### embedding_similarity\n\n```python\nembedding_similarity(embeddings: Dict[str, List[float]], term1: str, term2: str) -> float\n```\n\nCompute cosine similarity between two term embeddings.\n\n#### find_similar_by_embedding\n\n```python\nfind_similar_by_embedding(embeddings: Dict[str, List[float]], term: str, top_n: int = 10) -> List[Tuple[str, float]]\n```\n\nFind terms most similar to a given term by embedding.\n\n### Dependencies\n\n**Standard Library:**\n\n- `collections.defaultdict`\n- `layers.CorticalLayer`\n- `layers.HierarchicalLayer`\n- `math`\n- `random`\n- ... and 5 more\n\n\n\n## semantics.py\n\nSemantics Module\n================\n\nCorpus-derived semantic relations and retrofitting.\n\nExtracts semantic relationships from co-occurrence patterns,\nthen uses them to adjust connection weights (retrof...\n\n\n### Functions\n\n#### extract_pattern_relations\n\n```python\nextract_pattern_relations(documents: Dict[str, str], valid_terms: Set[str], min_confidence: float = 0.5) -> List[Tuple[str, str, str, float]]\n```\n\nExtract semantic relations using pattern matching on document text.\n\n#### get_pattern_statistics\n\n```python\nget_pattern_statistics(relations: List[Tuple[str, str, str, float]]) -> Dict[str, Any]\n```\n\nGet statistics about extracted pattern-based relations.\n\n#### extract_corpus_semantics\n\n```python\nextract_corpus_semantics(layers: Dict[CorticalLayer, HierarchicalLayer], documents: Dict[str, str], tokenizer, window_size: int = 5, min_cooccurrence: int = 2, use_pattern_extraction: bool = True, min_pattern_confidence: float = 0.6, max_similarity_pairs: int = 100000, min_context_keys: int = 3) -> List[Tuple[str, str, str, float]]\n```\n\nExtract semantic relations from corpus co-occurrence patterns.\n\n#### retrofit_connections\n\n```python\nretrofit_connections(layers: Dict[CorticalLayer, HierarchicalLayer], semantic_relations: List[Tuple[str, str, str, float]], iterations: int = 10, alpha: float = 0.3) -> Dict[str, Any]\n```\n\nRetrofit lateral connections using semantic relations.\n\n#### retrofit_embeddings\n\n```python\nretrofit_embeddings(embeddings: Dict[str, List[float]], semantic_relations: List[Tuple[str, str, str, float]], iterations: int = 10, alpha: float = 0.4) -> Dict[str, Any]\n```\n\nRetrofit embeddings using semantic relations.\n\n#### get_relation_type_weight\n\n```python\nget_relation_type_weight(relation_type: str) -> float\n```\n\nGet the weight for a relation type.\n\n#### build_isa_hierarchy\n\n```python\nbuild_isa_hierarchy(semantic_relations: List[Tuple[str, str, str, float]]) -> Tuple[Dict[str, Set[str]], Dict[str, Set[str]]]\n```\n\nBuild IsA parent-child hierarchy from semantic relations.\n\n#### get_ancestors\n\n```python\nget_ancestors(term: str, parents: Dict[str, Set[str]], max_depth: int = 10) -> Dict[str, int]\n```\n\nGet all ancestors of a term with their depth in the hierarchy.\n\n#### get_descendants\n\n```python\nget_descendants(term: str, children: Dict[str, Set[str]], max_depth: int = 10) -> Dict[str, int]\n```\n\nGet all descendants of a term with their depth in the hierarchy.\n\n#### inherit_properties\n\n```python\ninherit_properties(semantic_relations: List[Tuple[str, str, str, float]], decay_factor: float = 0.7, max_depth: int = 5) -> Dict[str, Dict[str, Tuple[float, str, int]]]\n```\n\nCompute inherited properties for all terms based on IsA hierarchy.\n\n#### compute_property_similarity\n\n```python\ncompute_property_similarity(term1: str, term2: str, inherited_properties: Dict[str, Dict[str, Tuple[float, str, int]]], direct_properties: Optional[Dict[str, Dict[str, float]]] = None) -> float\n```\n\nCompute similarity between terms based on shared properties (direct + inherited).\n\n#### apply_inheritance_to_connections\n\n```python\napply_inheritance_to_connections(layers: Dict[CorticalLayer, HierarchicalLayer], inherited_properties: Dict[str, Dict[str, Tuple[float, str, int]]], boost_factor: float = 0.3) -> Dict[str, Any]\n```\n\nBoost lateral connections between terms that share inherited properties.\n\n### Dependencies\n\n**Standard Library:**\n\n- `collections.defaultdict`\n- `constants.RELATION_WEIGHTS`\n- `copy`\n- `layers.CorticalLayer`\n- `layers.HierarchicalLayer`\n- ... and 10 more\n\n\n\n## tokenizer.py\n\nTokenizer Module\n================\n\nText tokenization with stemming and word variant support.\n\nLike early visual processing, the tokenizer extracts basic features\n(words) from raw input, filtering nois...\n\n\n### Classes\n\n#### Tokenizer\n\nText tokenizer with stemming and word variant support.\n\n**Methods:**\n\n- `tokenize`\n- `extract_ngrams`\n- `stem`\n- `get_word_variants`\n- `add_word_mapping`\n\n### Functions\n\n#### split_identifier\n\n```python\nsplit_identifier(identifier: str) -> List[str]\n```\n\nSplit a code identifier into component words.\n\n#### Tokenizer.tokenize\n\n```python\nTokenizer.tokenize(self, text: str, split_identifiers: Optional[bool] = None) -> List[str]\n```\n\nExtract tokens from text.\n\n#### Tokenizer.extract_ngrams\n\n```python\nTokenizer.extract_ngrams(self, tokens: List[str], n: int = 2) -> List[str]\n```\n\nExtract n-grams from token list.\n\n#### Tokenizer.stem\n\n```python\nTokenizer.stem(self, word: str) -> str\n```\n\nApply simple suffix stripping (Porter-lite stemming).\n\n#### Tokenizer.get_word_variants\n\n```python\nTokenizer.get_word_variants(self, word: str) -> List[str]\n```\n\nGet related words/variants for query expansion.\n\n#### Tokenizer.add_word_mapping\n\n```python\nTokenizer.add_word_mapping(self, word: str, variants: List[str]) -> None\n```\n\nAdd a custom word mapping for query expansion.\n\n### Dependencies\n\n**Standard Library:**\n\n- `re`\n- `typing.Dict`\n- `typing.List`\n- `typing.Optional`\n- `typing.Set`\n- ... and 1 more\n\n\n\n---\n\n*This chapter is part of [The Cortical Chronicles](../README.md), a self-documenting book generated by the Cortical Text Processor.*\n"
    },
    {
      "path": "02-architecture/mod-observability.md",
      "title": "Observability",
      "section": "architecture",
      "tags": [
        "architecture",
        "modules",
        "observability"
      ],
      "source_files": [
        "/home/user/Opus-code-test/cortical/results.py",
        "/home/user/Opus-code-test/cortical/observability.py",
        "/home/user/Opus-code-test/cortical/progress.py"
      ],
      "excerpt": "Metrics collection and progress tracking. Observability Module ==================== Provides timing hooks, metrics collection, and trace context for monitoring the Cortical Text Processor's...",
      "keywords": [
        "str",
        "python",
        "none",
        "self",
        "metricscollector",
        "update",
        "passagematch",
        "optional",
        "dict",
        "progress"
      ],
      "full_content": "# Observability\n\nMetrics collection and progress tracking.\n\n## Modules\n\n- **observability.py**: Observability Module\n- **progress.py**: Progress reporting infrastructure for long-running operations.\n- **results.py**: Result Dataclasses for Cortical Text Processor\n\n\n## observability.py\n\nObservability Module\n====================\n\nProvides timing hooks, metrics collection, and trace context for monitoring\nthe Cortical Text Processor's performance and operations.\n\nThis module follows th...\n\n\n### Classes\n\n#### MetricsCollector\n\nCollects and aggregates timing and count metrics for operations.\n\n**Methods:**\n\n- `record_timing`\n- `record_count`\n- `get_operation_stats`\n- `get_all_stats`\n- `get_trace`\n- `reset`\n- `enable`\n- `disable`\n- `trace_context`\n- `get_summary`\n\n#### TraceContext\n\nContext for request tracing across operations.\n\n**Methods:**\n\n- `elapsed_ms`\n\n### Functions\n\n#### timed\n\n```python\ntimed(operation_name: Optional[str] = None, include_args: bool = False)\n```\n\nDecorator for timing method calls and recording to metrics.\n\n#### measure_time\n\n```python\nmeasure_time(func: Callable) -> Callable\n```\n\nSimple timing decorator that logs execution time.\n\n#### get_global_metrics\n\n```python\nget_global_metrics() -> MetricsCollector\n```\n\nGet the global metrics collector instance.\n\n#### enable_global_metrics\n\n```python\nenable_global_metrics() -> None\n```\n\nEnable global metrics collection.\n\n#### disable_global_metrics\n\n```python\ndisable_global_metrics() -> None\n```\n\nDisable global metrics collection.\n\n#### reset_global_metrics\n\n```python\nreset_global_metrics() -> None\n```\n\nReset global metrics.\n\n#### MetricsCollector.record_timing\n\n```python\nMetricsCollector.record_timing(self, operation: str, duration_ms: float, trace_id: Optional[str] = None, context: Optional[Dict[str, Any]] = None) -> None\n```\n\nRecord a timing measurement for an operation.\n\n#### MetricsCollector.record_count\n\n```python\nMetricsCollector.record_count(self, metric_name: str, count: int = 1) -> None\n```\n\nRecord a simple count metric.\n\n#### MetricsCollector.get_operation_stats\n\n```python\nMetricsCollector.get_operation_stats(self, operation: str) -> Dict[str, Any]\n```\n\nGet statistics for a specific operation.\n\n#### MetricsCollector.get_all_stats\n\n```python\nMetricsCollector.get_all_stats(self) -> Dict[str, Dict[str, Any]]\n```\n\nGet statistics for all operations.\n\n#### MetricsCollector.get_trace\n\n```python\nMetricsCollector.get_trace(self, trace_id: str) -> List[tuple]\n```\n\nGet all operations recorded for a trace ID.\n\n#### MetricsCollector.reset\n\n```python\nMetricsCollector.reset(self) -> None\n```\n\nClear all collected metrics.\n\n#### MetricsCollector.enable\n\n```python\nMetricsCollector.enable(self) -> None\n```\n\nEnable metrics collection.\n\n#### MetricsCollector.disable\n\n```python\nMetricsCollector.disable(self) -> None\n```\n\nDisable metrics collection.\n\n#### MetricsCollector.trace_context\n\n```python\nMetricsCollector.trace_context(self, trace_id: str)\n```\n\nContext manager for tracing a block of operations.\n\n#### MetricsCollector.get_summary\n\n```python\nMetricsCollector.get_summary(self) -> str\n```\n\nGet a human-readable summary of all metrics.\n\n#### TraceContext.elapsed_ms\n\n```python\nTraceContext.elapsed_ms(self) -> float\n```\n\nGet elapsed time since trace started in milliseconds.\n\n### Dependencies\n\n**Standard Library:**\n\n- `collections.defaultdict`\n- `contextlib.contextmanager`\n- `functools`\n- `logging`\n- `time`\n- ... and 5 more\n\n\n\n## progress.py\n\nProgress reporting infrastructure for long-running operations.\n\nThis module provides a flexible progress reporting system that supports:\n- Console output with nice formatting\n- Custom callbacks for in...\n\n\n### Classes\n\n#### ProgressReporter\n\nProtocol for progress reporters.\n\n**Methods:**\n\n- `update`\n- `complete`\n\n#### ConsoleProgressReporter\n\nConsole-based progress reporter with nice formatting.\n\n**Methods:**\n\n- `update`\n- `complete`\n\n#### CallbackProgressReporter\n\nProgress reporter that calls a custom callback function.\n\n**Methods:**\n\n- `update`\n- `complete`\n\n#### SilentProgressReporter\n\nNo-op progress reporter for silent operation.\n\n**Methods:**\n\n- `update`\n- `complete`\n\n#### MultiPhaseProgress\n\nHelper for tracking progress across multiple sequential phases.\n\n**Methods:**\n\n- `start_phase`\n- `update`\n- `complete_phase`\n- `overall_progress`\n\n### Functions\n\n#### ProgressReporter.update\n\n```python\nProgressReporter.update(self, phase: str, percent: float, message: Optional[str] = None) -> None\n```\n\nUpdate progress for a specific phase.\n\n#### ProgressReporter.complete\n\n```python\nProgressReporter.complete(self, phase: str, message: Optional[str] = None) -> None\n```\n\nMark a phase as complete.\n\n#### ConsoleProgressReporter.update\n\n```python\nConsoleProgressReporter.update(self, phase: str, percent: float, message: Optional[str] = None) -> None\n```\n\nUpdate progress display.\n\n#### ConsoleProgressReporter.complete\n\n```python\nConsoleProgressReporter.complete(self, phase: str, message: Optional[str] = None) -> None\n```\n\nMark phase as complete and move to new line.\n\n#### CallbackProgressReporter.update\n\n```python\nCallbackProgressReporter.update(self, phase: str, percent: float, message: Optional[str] = None) -> None\n```\n\nCall callback with progress update.\n\n#### CallbackProgressReporter.complete\n\n```python\nCallbackProgressReporter.complete(self, phase: str, message: Optional[str] = None) -> None\n```\n\nCall callback with completion notification.\n\n#### SilentProgressReporter.update\n\n```python\nSilentProgressReporter.update(self, phase: str, percent: float, message: Optional[str] = None) -> None\n```\n\nDo nothing.\n\n#### SilentProgressReporter.complete\n\n```python\nSilentProgressReporter.complete(self, phase: str, message: Optional[str] = None) -> None\n```\n\nDo nothing.\n\n#### MultiPhaseProgress.start_phase\n\n```python\nMultiPhaseProgress.start_phase(self, phase: str) -> None\n```\n\nStart a new phase.\n\n#### MultiPhaseProgress.update\n\n```python\nMultiPhaseProgress.update(self, percent: float, message: Optional[str] = None) -> None\n```\n\nUpdate progress within current phase.\n\n#### MultiPhaseProgress.complete_phase\n\n```python\nMultiPhaseProgress.complete_phase(self, message: Optional[str] = None) -> None\n```\n\nMark current phase as complete.\n\n#### MultiPhaseProgress.overall_progress\n\n```python\nMultiPhaseProgress.overall_progress(self) -> float\n```\n\nGet overall progress across all phases (0-100).\n\n### Dependencies\n\n**Standard Library:**\n\n- `abc.ABC`\n- `abc.abstractmethod`\n- `sys`\n- `time`\n- `typing.Any`\n- ... and 4 more\n\n\n\n## results.py\n\nResult Dataclasses for Cortical Text Processor\n===============================================\n\nStrongly-typed result containers for query operations that provide\nIDE autocomplete and type checking su...\n\n\n### Classes\n\n#### DocumentMatch\n\nA document search result with relevance score.\n\n**Methods:**\n\n- `to_dict`\n- `to_tuple`\n- `from_tuple`\n- `from_dict`\n\n#### PassageMatch\n\nA passage retrieval result with text, location, and relevance score.\n\n**Methods:**\n\n- `to_dict`\n- `to_tuple`\n- `location`\n- `length`\n- `from_tuple`\n- `from_dict`\n\n#### QueryResult\n\nComplete query result with matches and metadata.\n\n**Methods:**\n\n- `to_dict`\n- `top_match`\n- `match_count`\n- `average_score`\n- `from_dict`\n\n### Functions\n\n#### convert_document_matches\n\n```python\nconvert_document_matches(results: List[tuple], metadata: Optional[Dict[str, Dict[str, Any]]] = None) -> List[DocumentMatch]\n```\n\nConvert list of (doc_id, score) tuples to DocumentMatch objects.\n\n#### convert_passage_matches\n\n```python\nconvert_passage_matches(results: List[tuple], metadata: Optional[Dict[str, Dict[str, Any]]] = None) -> List[PassageMatch]\n```\n\nConvert list of (doc_id, text, start, end, score) tuples to PassageMatch objects.\n\n#### DocumentMatch.to_dict\n\n```python\nDocumentMatch.to_dict(self) -> Dict[str, Any]\n```\n\nConvert to dictionary.\n\n#### DocumentMatch.to_tuple\n\n```python\nDocumentMatch.to_tuple(self) -> tuple\n```\n\nConvert to tuple format (doc_id, score).\n\n#### DocumentMatch.from_tuple\n\n```python\nDocumentMatch.from_tuple(cls, doc_id: str, score: float, metadata: Optional[Dict[str, Any]] = None) -> 'DocumentMatch'\n```\n\nCreate from tuple format (doc_id, score).\n\n#### DocumentMatch.from_dict\n\n```python\nDocumentMatch.from_dict(cls, data: Dict[str, Any]) -> 'DocumentMatch'\n```\n\nCreate from dictionary.\n\n#### PassageMatch.to_dict\n\n```python\nPassageMatch.to_dict(self) -> Dict[str, Any]\n```\n\nConvert to dictionary.\n\n#### PassageMatch.to_tuple\n\n```python\nPassageMatch.to_tuple(self) -> tuple\n```\n\nConvert to tuple format (doc_id, text, start, end, score).\n\n#### PassageMatch.location\n\n```python\nPassageMatch.location(self) -> str\n```\n\nGet citation-style location string.\n\n#### PassageMatch.length\n\n```python\nPassageMatch.length(self) -> int\n```\n\nGet passage length in characters.\n\n#### PassageMatch.from_tuple\n\n```python\nPassageMatch.from_tuple(cls, doc_id: str, text: str, start: int, end: int, score: float, metadata: Optional[Dict[str, Any]] = None) -> 'PassageMatch'\n```\n\nCreate from tuple format (doc_id, text, start, end, score).\n\n#### PassageMatch.from_dict\n\n```python\nPassageMatch.from_dict(cls, data: Dict[str, Any]) -> 'PassageMatch'\n```\n\nCreate from dictionary.\n\n#### QueryResult.to_dict\n\n```python\nQueryResult.to_dict(self) -> Dict[str, Any]\n```\n\nConvert to dictionary with nested match dicts.\n\n#### QueryResult.top_match\n\n```python\nQueryResult.top_match(self) -> Union[DocumentMatch, PassageMatch, None]\n```\n\nGet the highest-scoring match.\n\n#### QueryResult.match_count\n\n```python\nQueryResult.match_count(self) -> int\n```\n\nGet number of matches.\n\n#### QueryResult.average_score\n\n```python\nQueryResult.average_score(self) -> float\n```\n\nGet average relevance score across all matches.\n\n#### QueryResult.from_dict\n\n```python\nQueryResult.from_dict(cls, data: Dict[str, Any]) -> 'QueryResult'\n```\n\nCreate from dictionary.\n\n### Dependencies\n\n**Standard Library:**\n\n- `dataclasses.asdict`\n- `dataclasses.dataclass`\n- `dataclasses.field`\n- `typing.Any`\n- `typing.Dict`\n- ... and 3 more\n\n\n\n---\n\n*This chapter is part of [The Cortical Chronicles](../README.md), a self-documenting book generated by the Cortical Text Processor.*\n"
    },
    {
      "path": "02-architecture/mod-persistence.md",
      "title": "Persistence Layer",
      "section": "architecture",
      "tags": [
        "architecture",
        "modules",
        "persistence"
      ],
      "source_files": [
        "/home/user/Opus-code-test/cortical/persistence.py",
        "/home/user/Opus-code-test/cortical/chunk_index.py",
        "/home/user/Opus-code-test/cortical/state_storage.py"
      ],
      "excerpt": "Save and load functionality for maintaining processor state. Chunk-based indexing for git-compatible corpus storage. This module provides append-only, time-stamped JSON chunks that can be safely...",
      "keywords": [
        "str",
        "dict",
        "python",
        "self",
        "bool",
        "none",
        "chunkloader",
        "optional",
        "chunk",
        "list"
      ],
      "full_content": "# Persistence Layer\n\nSave and load functionality for maintaining processor state.\n\n## Modules\n\n- **chunk_index.py**: Chunk-based indexing for git-compatible corpus storage.\n- **persistence.py**: Persistence Module\n- **state_storage.py**: Git-friendly State Storage Module\n\n\n## chunk_index.py\n\nChunk-based indexing for git-compatible corpus storage.\n\nThis module provides append-only, time-stamped JSON chunks that can be\nsafely committed to git without merge conflicts. Each indexing session\nc...\n\n\n### Classes\n\n#### ChunkOperation\n\nA single operation in a chunk (add, modify, or delete).\n\n**Methods:**\n\n- `to_dict`\n- `from_dict`\n\n#### Chunk\n\nA chunk containing operations from a single indexing session.\n\n**Methods:**\n\n- `to_dict`\n- `from_dict`\n- `get_filename`\n\n#### ChunkWriter\n\nWrites indexing session changes to timestamped JSON chunks.\n\n**Methods:**\n\n- `add_document`\n- `modify_document`\n- `delete_document`\n- `has_operations`\n- `save`\n\n#### ChunkLoader\n\nLoads and combines chunks to rebuild document state.\n\n**Methods:**\n\n- `get_chunk_files`\n- `load_chunk`\n- `load_all`\n- `get_documents`\n- `get_mtimes`\n- `get_metadata`\n- `get_chunks`\n- `compute_hash`\n- `is_cache_valid`\n- `save_cache_hash`\n- `get_stats`\n\n#### ChunkCompactor\n\nCompacts multiple chunk files into a single file.\n\n**Methods:**\n\n- `compact`\n\n### Functions\n\n#### get_changes_from_manifest\n\n```python\nget_changes_from_manifest(current_files: Dict[str, float], manifest: Dict[str, float]) -> Tuple[List[str], List[str], List[str]]\n```\n\nCompare current files to manifest to find changes.\n\n#### ChunkOperation.to_dict\n\n```python\nChunkOperation.to_dict(self) -> Dict[str, Any]\n```\n\nConvert to dictionary for JSON serialization.\n\n#### ChunkOperation.from_dict\n\n```python\nChunkOperation.from_dict(cls, d: Dict[str, Any]) -> 'ChunkOperation'\n```\n\nCreate from dictionary.\n\n#### Chunk.to_dict\n\n```python\nChunk.to_dict(self) -> Dict[str, Any]\n```\n\nConvert to dictionary for JSON serialization.\n\n#### Chunk.from_dict\n\n```python\nChunk.from_dict(cls, d: Dict[str, Any]) -> 'Chunk'\n```\n\nCreate from dictionary.\n\n#### Chunk.get_filename\n\n```python\nChunk.get_filename(self) -> str\n```\n\nGenerate filename for this chunk.\n\n#### ChunkWriter.add_document\n\n```python\nChunkWriter.add_document(self, doc_id: str, content: str, mtime: Optional[float] = None, metadata: Optional[Dict[str, Any]] = None)\n```\n\nRecord an add operation.\n\n#### ChunkWriter.modify_document\n\n```python\nChunkWriter.modify_document(self, doc_id: str, content: str, mtime: Optional[float] = None, metadata: Optional[Dict[str, Any]] = None)\n```\n\nRecord a modify operation.\n\n#### ChunkWriter.delete_document\n\n```python\nChunkWriter.delete_document(self, doc_id: str)\n```\n\nRecord a delete operation.\n\n#### ChunkWriter.has_operations\n\n```python\nChunkWriter.has_operations(self) -> bool\n```\n\nCheck if any operations were recorded.\n\n#### ChunkWriter.save\n\n```python\nChunkWriter.save(self, warn_size_kb: int = DEFAULT_WARN_SIZE_KB) -> Optional[Path]\n```\n\nSave chunk to file.\n\n#### ChunkLoader.get_chunk_files\n\n```python\nChunkLoader.get_chunk_files(self) -> List[Path]\n```\n\nGet all chunk files sorted by timestamp.\n\n#### ChunkLoader.load_chunk\n\n```python\nChunkLoader.load_chunk(self, filepath: Path) -> Chunk\n```\n\nLoad a single chunk file.\n\n#### ChunkLoader.load_all\n\n```python\nChunkLoader.load_all(self) -> Dict[str, str]\n```\n\nLoad all chunks and replay operations to get current document state.\n\n#### ChunkLoader.get_documents\n\n```python\nChunkLoader.get_documents(self) -> Dict[str, str]\n```\n\nGet loaded documents (calls load_all if needed).\n\n#### ChunkLoader.get_mtimes\n\n```python\nChunkLoader.get_mtimes(self) -> Dict[str, float]\n```\n\nGet document modification times.\n\n#### ChunkLoader.get_metadata\n\n```python\nChunkLoader.get_metadata(self) -> Dict[str, Dict[str, Any]]\n```\n\nGet document metadata (doc_type, headings, etc.).\n\n#### ChunkLoader.get_chunks\n\n```python\nChunkLoader.get_chunks(self) -> List[Chunk]\n```\n\nGet loaded chunks.\n\n#### ChunkLoader.compute_hash\n\n```python\nChunkLoader.compute_hash(self) -> str\n```\n\nCompute hash of current document state.\n\n#### ChunkLoader.is_cache_valid\n\n```python\nChunkLoader.is_cache_valid(self, cache_path: str, cache_hash_path: Optional[str] = None) -> bool\n```\n\nCheck if pkl cache is valid for current chunk state.\n\n#### ChunkLoader.save_cache_hash\n\n```python\nChunkLoader.save_cache_hash(self, cache_path: str, cache_hash_path: Optional[str] = None)\n```\n\nSave current document hash for cache validation.\n\n#### ChunkLoader.get_stats\n\n```python\nChunkLoader.get_stats(self) -> Dict[str, Any]\n```\n\nGet statistics about loaded chunks.\n\n#### ChunkCompactor.compact\n\n```python\nChunkCompactor.compact(self, before: Optional[str] = None, keep_recent: int = 0, dry_run: bool = False) -> Dict[str, Any]\n```\n\nCompact chunks into a single chunk.\n\n### Dependencies\n\n**Standard Library:**\n\n- `dataclasses.asdict`\n- `dataclasses.dataclass`\n- `dataclasses.field`\n- `datetime.datetime`\n- `hashlib`\n- ... and 11 more\n\n\n\n## persistence.py\n\nPersistence Module\n==================\n\nSave and load functionality for the cortical processor.\n\nSupports:\n- Pickle serialization for full state\n- JSON export for graph visualization\n- Incremental upda...\n\n\n### Classes\n\n#### SignatureVerificationError\n\nRaised when HMAC signature verification fails.\n\n### Functions\n\n#### save_processor\n\n```python\nsave_processor(filepath: str, layers: Dict[CorticalLayer, HierarchicalLayer], documents: Dict[str, str], document_metadata: Optional[Dict[str, Dict[str, Any]]] = None, embeddings: Optional[Dict[str, list]] = None, semantic_relations: Optional[list] = None, metadata: Optional[Dict] = None, verbose: bool = True, format: str = 'pickle', signing_key: Optional[bytes] = None) -> None\n```\n\nSave processor state to a file.\n\n#### load_processor\n\n```python\nload_processor(filepath: str, verbose: bool = True, format: Optional[str] = None, verify_key: Optional[bytes] = None) -> tuple\n```\n\nLoad processor state from a file.\n\n#### export_graph_json\n\n```python\nexport_graph_json(filepath: str, layers: Dict[CorticalLayer, HierarchicalLayer], layer_filter: Optional[CorticalLayer] = None, min_weight: float = 0.0, max_nodes: int = 500, verbose: bool = True) -> Dict\n```\n\nExport graph structure as JSON for visualization.\n\n#### export_embeddings_json\n\n```python\nexport_embeddings_json(filepath: str, embeddings: Dict[str, list], metadata: Optional[Dict] = None) -> None\n```\n\nExport embeddings as JSON.\n\n#### load_embeddings_json\n\n```python\nload_embeddings_json(filepath: str) -> Dict[str, list]\n```\n\nLoad embeddings from JSON.\n\n#### export_semantic_relations_json\n\n```python\nexport_semantic_relations_json(filepath: str, relations: list) -> None\n```\n\nExport semantic relations as JSON.\n\n#### load_semantic_relations_json\n\n```python\nload_semantic_relations_json(filepath: str) -> list\n```\n\nLoad semantic relations from JSON.\n\n#### get_state_summary\n\n```python\nget_state_summary(layers: Dict[CorticalLayer, HierarchicalLayer], documents: Dict[str, str]) -> Dict\n```\n\nGet a summary of the current processor state.\n\n#### export_conceptnet_json\n\n```python\nexport_conceptnet_json(filepath: str, layers: Dict[CorticalLayer, HierarchicalLayer], semantic_relations: Optional[list] = None, include_cross_layer: bool = True, include_typed_edges: bool = True, min_weight: float = 0.0, min_confidence: float = 0.0, max_nodes_per_layer: int = 100, verbose: bool = True) -> Dict[str, Any]\n```\n\nExport ConceptNet-style graph for visualization.\n\n### Dependencies\n\n**Standard Library:**\n\n- `hashlib`\n- `hmac`\n- `json`\n- `layers.CorticalLayer`\n- `layers.HierarchicalLayer`\n- ... and 9 more\n\n\n\n## state_storage.py\n\nGit-friendly State Storage Module\n=================================\n\nReplaces pickle-based persistence with JSON files that:\n- Can be diff'd and reviewed in git\n- Won't cause merge conflicts\n- Support...\n\n\n### Classes\n\n#### StateManifest\n\nManifest file tracking state version and component checksums.\n\n**Methods:**\n\n- `to_dict`\n- `from_dict`\n- `update_checksum`\n\n#### StateWriter\n\nWrites processor state to git-friendly JSON files.\n\n**Methods:**\n\n- `save_layer`\n- `save_documents`\n- `save_semantic_relations`\n- `save_embeddings`\n- `save_manifest`\n- `save_all`\n\n#### StateLoader\n\nLoads processor state from git-friendly JSON files.\n\n**Methods:**\n\n- `exists`\n- `load_manifest`\n- `validate_checksum`\n- `load_layer`\n- `load_documents`\n- `load_semantic_relations`\n- `load_embeddings`\n- `load_all`\n- `get_stats`\n\n### Functions\n\n#### migrate_pkl_to_json\n\n```python\nmigrate_pkl_to_json(pkl_path: str, json_dir: str, verbose: bool = True) -> bool\n```\n\nMigrate a pickle file to git-friendly JSON format.\n\n#### StateManifest.to_dict\n\n```python\nStateManifest.to_dict(self) -> Dict[str, Any]\n```\n\nConvert to dictionary for JSON serialization.\n\n#### StateManifest.from_dict\n\n```python\nStateManifest.from_dict(cls, data: Dict[str, Any]) -> 'StateManifest'\n```\n\nCreate manifest from dictionary.\n\n#### StateManifest.update_checksum\n\n```python\nStateManifest.update_checksum(self, component: str, content: str) -> bool\n```\n\nUpdate checksum for a component.\n\n#### StateWriter.save_layer\n\n```python\nStateWriter.save_layer(self, layer: HierarchicalLayer, force: bool = False) -> bool\n```\n\nSave a single layer to its JSON file.\n\n#### StateWriter.save_documents\n\n```python\nStateWriter.save_documents(self, documents: Dict[str, str], document_metadata: Optional[Dict[str, Dict[str, Any]]] = None, force: bool = False) -> bool\n```\n\nSave documents and metadata.\n\n#### StateWriter.save_semantic_relations\n\n```python\nStateWriter.save_semantic_relations(self, relations: List[Tuple], force: bool = False) -> bool\n```\n\nSave semantic relations.\n\n#### StateWriter.save_embeddings\n\n```python\nStateWriter.save_embeddings(self, embeddings: Dict[str, List[float]], force: bool = False) -> bool\n```\n\nSave graph embeddings.\n\n#### StateWriter.save_manifest\n\n```python\nStateWriter.save_manifest(self) -> None\n```\n\nSave the manifest file.\n\n#### StateWriter.save_all\n\n```python\nStateWriter.save_all(self, layers: Dict[CorticalLayer, HierarchicalLayer], documents: Dict[str, str], document_metadata: Optional[Dict[str, Dict[str, Any]]] = None, embeddings: Optional[Dict[str, List[float]]] = None, semantic_relations: Optional[List[Tuple]] = None, stale_computations: Optional[Set[str]] = None, force: bool = False, verbose: bool = True) -> Dict[str, bool]\n```\n\nSave all processor state.\n\n#### StateLoader.exists\n\n```python\nStateLoader.exists(self) -> bool\n```\n\nCheck if state directory exists and has manifest.\n\n#### StateLoader.load_manifest\n\n```python\nStateLoader.load_manifest(self) -> StateManifest\n```\n\nLoad the manifest file.\n\n#### StateLoader.validate_checksum\n\n```python\nStateLoader.validate_checksum(self, component: str, filepath: Path) -> bool\n```\n\nValidate a component's checksum.\n\n#### StateLoader.load_layer\n\n```python\nStateLoader.load_layer(self, level: int) -> HierarchicalLayer\n```\n\nLoad a single layer.\n\n#### StateLoader.load_documents\n\n```python\nStateLoader.load_documents(self) -> Tuple[Dict[str, str], Dict[str, Dict[str, Any]]]\n```\n\nLoad documents and metadata.\n\n#### StateLoader.load_semantic_relations\n\n```python\nStateLoader.load_semantic_relations(self) -> List[Tuple]\n```\n\nLoad semantic relations.\n\n#### StateLoader.load_embeddings\n\n```python\nStateLoader.load_embeddings(self) -> Dict[str, List[float]]\n```\n\nLoad graph embeddings.\n\n#### StateLoader.load_all\n\n```python\nStateLoader.load_all(self, validate: bool = True, verbose: bool = True) -> Tuple[Dict[CorticalLayer, HierarchicalLayer], Dict[str, str], Dict[str, Dict[str, Any]], Dict[str, List[float]], List[Tuple], Dict[str, Any]]\n```\n\nLoad all processor state.\n\n#### StateLoader.get_stats\n\n```python\nStateLoader.get_stats(self) -> Dict[str, Any]\n```\n\nGet statistics about stored state without loading everything.\n\n### Dependencies\n\n**Standard Library:**\n\n- `dataclasses.asdict`\n- `dataclasses.dataclass`\n- `dataclasses.field`\n- `datetime.datetime`\n- `hashlib`\n- ... and 13 more\n\n\n\n---\n\n*This chapter is part of [The Cortical Chronicles](../README.md), a self-documenting book generated by the Cortical Text Processor.*\n"
    },
    {
      "path": "02-architecture/mod-processor.md",
      "title": "Core Processor",
      "section": "architecture",
      "tags": [
        "architecture",
        "modules",
        "processor"
      ],
      "source_files": [
        "/home/user/Opus-code-test/cortical/processor/persistence_api.py",
        "/home/user/Opus-code-test/cortical/processor/documents.py",
        "/home/user/Opus-code-test/cortical/processor/__init__.py",
        "/home/user/Opus-code-test/cortical/processor/core.py",
        "/home/user/Opus-code-test/cortical/processor/introspection.py",
        "/home/user/Opus-code-test/cortical/processor/compute.py"
      ],
      "excerpt": "The core processor orchestrates all text processing operations. Cortical Text Processor - Main processor package. This package splits the monolithic processor.py into focused modules:...",
      "keywords": [
        "str",
        "python",
        "self",
        "dict",
        "computemixin",
        "bool",
        "introspectionmixin",
        "none",
        "int",
        "true"
      ],
      "full_content": "# Core Processor\n\nThe core processor orchestrates all text processing operations.\n\n## Modules\n\n- **__init__.py**: Cortical Text Processor - Main processor package.\n- **compute.py**: Compute methods: analysis, clustering, embeddings, semantic extraction.\n- **core.py**: Core processor functionality: initialization, staleness tracking, and layer management.\n- **documents.py**: Document management: processing, adding, removing, and metadata handling.\n- **introspection.py**: Introspection: state inspection, fingerprints, gaps, and summaries.\n- **persistence_api.py**: Persistence API: save, load, export, and migration methods.\n\n\n## __init__.py\n\nCortical Text Processor - Main processor package.\n\nThis package splits the monolithic processor.py into focused modules:\n- core.py: Initialization, staleness tracking, layer management\n- documents.py:...\n\n\n### Classes\n\n#### CorticalTextProcessor\n\nNeocortex-inspired text processing system.\n\n### Dependencies\n\n**Standard Library:**\n\n- `compute.ComputeMixin`\n- `core.CoreMixin`\n- `documents.DocumentsMixin`\n- `introspection.IntrospectionMixin`\n- `persistence_api.PersistenceMixin`\n- ... and 1 more\n\n\n\n## compute.py\n\nCompute methods: analysis, clustering, embeddings, semantic extraction.\n\nThis module contains all methods that perform computational analysis on the corpus,\nincluding PageRank, TF-IDF, clustering, and...\n\n\n### Classes\n\n#### ComputeMixin\n\nMixin providing computation functionality.\n\n**Methods:**\n\n- `recompute`\n- `compute_all`\n- `resume_from_checkpoint`\n- `propagate_activation`\n- `compute_importance`\n- `compute_semantic_importance`\n- `compute_hierarchical_importance`\n- `compute_tfidf`\n- `compute_bm25`\n- `compute_document_connections`\n- `compute_bigram_connections`\n- `build_concept_clusters`\n- `compute_clustering_quality`\n- `compute_concept_connections`\n- `extract_corpus_semantics`\n- `extract_pattern_relations`\n- `retrofit_connections`\n- `compute_property_inheritance`\n- `compute_property_similarity`\n- `compute_graph_embeddings`\n- `retrofit_embeddings`\n- `embedding_similarity`\n- `find_similar_by_embedding`\n\n### Functions\n\n#### ComputeMixin.recompute\n\n```python\nComputeMixin.recompute(self, level: str = 'stale', verbose: bool = True) -> Dict[str, bool]\n```\n\nRecompute specified analysis levels.\n\n#### ComputeMixin.compute_all\n\n```python\nComputeMixin.compute_all(self, verbose: bool = True, build_concepts: bool = True, pagerank_method: str = 'standard', connection_strategy: str = 'document_overlap', cluster_strictness: float = 1.0, bridge_weight: float = 0.0, progress_callback: Optional[ProgressReporter] = None, show_progress: bool = False, checkpoint_dir: Optional[str] = None, resume: bool = False) -> Dict[str, Any]\n```\n\nRun all computation steps.\n\n#### ComputeMixin.resume_from_checkpoint\n\n```python\nComputeMixin.resume_from_checkpoint(cls, checkpoint_dir: str, config: Optional['CorticalConfig'] = None, verbose: bool = True) -> 'CorticalTextProcessor'\n```\n\nResume processing from a checkpoint directory.\n\n#### ComputeMixin.propagate_activation\n\n```python\nComputeMixin.propagate_activation(self, iterations: int = 3, decay: float = 0.8, verbose: bool = True) -> None\n```\n\nNone\n\n#### ComputeMixin.compute_importance\n\n```python\nComputeMixin.compute_importance(self, verbose: bool = True) -> None\n```\n\nNone\n\n#### ComputeMixin.compute_semantic_importance\n\n```python\nComputeMixin.compute_semantic_importance(self, relation_weights: Optional[Dict[str, float]] = None, verbose: bool = True) -> Dict[str, Any]\n```\n\nCompute PageRank with semantic relation weighting.\n\n#### ComputeMixin.compute_hierarchical_importance\n\n```python\nComputeMixin.compute_hierarchical_importance(self, layer_iterations: int = 10, global_iterations: int = 5, cross_layer_damping: Optional[float] = None, verbose: bool = True) -> Dict[str, Any]\n```\n\nCompute PageRank with cross-layer propagation.\n\n#### ComputeMixin.compute_tfidf\n\n```python\nComputeMixin.compute_tfidf(self, verbose: bool = True) -> None\n```\n\nCompute document relevance scores using the configured algorithm.\n\n#### ComputeMixin.compute_bm25\n\n```python\nComputeMixin.compute_bm25(self, k1: float = None, b: float = None, verbose: bool = True) -> None\n```\n\nCompute BM25 scores for document relevance ranking.\n\n#### ComputeMixin.compute_document_connections\n\n```python\nComputeMixin.compute_document_connections(self, min_shared_terms: int = 3, verbose: bool = True) -> None\n```\n\nNone\n\n#### ComputeMixin.compute_bigram_connections\n\n```python\nComputeMixin.compute_bigram_connections(self, min_shared_docs: int = 1, component_weight: float = 0.5, chain_weight: float = 0.7, cooccurrence_weight: float = 0.3, max_bigrams_per_term: int = 100, max_bigrams_per_doc: int = 500, max_connections_per_bigram: int = 50, verbose: bool = True) -> Dict[str, Any]\n```\n\nBuild lateral connections between bigrams based on shared components and co-occurrence.\n\n#### ComputeMixin.build_concept_clusters\n\n```python\nComputeMixin.build_concept_clusters(self, min_cluster_size: Optional[int] = None, clustering_method: str = 'louvain', cluster_strictness: Optional[float] = None, bridge_weight: float = 0.0, resolution: Optional[float] = None, verbose: bool = True) -> Dict[int, List[str]]\n```\n\nBuild concept clusters from token layer.\n\n#### ComputeMixin.compute_clustering_quality\n\n```python\nComputeMixin.compute_clustering_quality(self, sample_size: int = 500) -> Dict[str, Any]\n```\n\nCompute clustering quality metrics for the concept layer.\n\n#### ComputeMixin.compute_concept_connections\n\n```python\nComputeMixin.compute_concept_connections(self, use_semantics: bool = True, min_shared_docs: int = 1, min_jaccard: float = 0.1, use_member_semantics: bool = False, use_embedding_similarity: bool = False, embedding_threshold: float = 0.3, verbose: bool = True) -> Dict[str, Any]\n```\n\nBuild lateral connections between concepts based on document overlap and semantics.\n\n#### ComputeMixin.extract_corpus_semantics\n\n```python\nComputeMixin.extract_corpus_semantics(self, use_pattern_extraction: bool = True, min_pattern_confidence: float = 0.6, max_similarity_pairs: int = 100000, min_context_keys: int = 3, verbose: bool = True) -> int\n```\n\nExtract semantic relations from the corpus.\n\n#### ComputeMixin.extract_pattern_relations\n\n```python\nComputeMixin.extract_pattern_relations(self, min_confidence: float = 0.6, verbose: bool = True) -> List[Tuple[str, str, str, float]]\n```\n\nExtract semantic relations using pattern matching only.\n\n#### ComputeMixin.retrofit_connections\n\n```python\nComputeMixin.retrofit_connections(self, iterations: int = 10, alpha: float = 0.3, verbose: bool = True) -> Dict\n```\n\nNone\n\n#### ComputeMixin.compute_property_inheritance\n\n```python\nComputeMixin.compute_property_inheritance(self, decay_factor: float = 0.7, max_depth: int = 5, apply_to_connections: bool = True, boost_factor: float = 0.3, verbose: bool = True) -> Dict[str, Any]\n```\n\nCompute property inheritance based on IsA hierarchy.\n\n#### ComputeMixin.compute_property_similarity\n\n```python\nComputeMixin.compute_property_similarity(self, term1: str, term2: str) -> float\n```\n\nCompute similarity between terms based on shared properties.\n\n#### ComputeMixin.compute_graph_embeddings\n\n```python\nComputeMixin.compute_graph_embeddings(self, dimensions: int = 64, method: str = 'fast', max_terms: Optional[int] = None, verbose: bool = True) -> Dict\n```\n\nCompute graph embeddings for tokens.\n\n#### ComputeMixin.retrofit_embeddings\n\n```python\nComputeMixin.retrofit_embeddings(self, iterations: int = 10, alpha: float = 0.4, verbose: bool = True) -> Dict\n```\n\nNone\n\n#### ComputeMixin.embedding_similarity\n\n```python\nComputeMixin.embedding_similarity(self, term1: str, term2: str) -> float\n```\n\nNone\n\n#### ComputeMixin.find_similar_by_embedding\n\n```python\nComputeMixin.find_similar_by_embedding(self, term: str, top_n: int = 10) -> List[Tuple[str, float]]\n```\n\nNone\n\n### Dependencies\n\n**Standard Library:**\n\n- `datetime.datetime`\n- `json`\n- `layers.CorticalLayer`\n- `logging`\n- `observability.timed`\n- ... and 11 more\n\n**Local Imports:**\n\n- `.analysis`\n- `.embeddings`\n- `.semantics`\n\n\n\n## core.py\n\nCore processor functionality: initialization, staleness tracking, and layer management.\n\nThis module contains the base class definition and core infrastructure that all\nother processor mixins depend o...\n\n\n### Classes\n\n#### CoreMixin\n\nCore mixin providing initialization and staleness tracking.\n\n**Methods:**\n\n- `is_stale`\n- `get_stale_computations`\n- `get_layer`\n- `get_metrics`\n- `get_metrics_summary`\n- `reset_metrics`\n- `enable_metrics`\n- `disable_metrics`\n- `record_metric`\n\n### Functions\n\n#### CoreMixin.is_stale\n\n```python\nCoreMixin.is_stale(self, computation_type: str) -> bool\n```\n\nCheck if a specific computation is stale.\n\n#### CoreMixin.get_stale_computations\n\n```python\nCoreMixin.get_stale_computations(self) -> set\n```\n\nGet the set of computations that are currently stale.\n\n#### CoreMixin.get_layer\n\n```python\nCoreMixin.get_layer(self, layer: CorticalLayer) -> HierarchicalLayer\n```\n\nGet a specific layer by enum.\n\n#### CoreMixin.get_metrics\n\n```python\nCoreMixin.get_metrics(self) -> Dict[str, Dict[str, Any]]\n```\n\nGet all collected metrics.\n\n#### CoreMixin.get_metrics_summary\n\n```python\nCoreMixin.get_metrics_summary(self) -> str\n```\n\nGet a human-readable summary of all metrics.\n\n#### CoreMixin.reset_metrics\n\n```python\nCoreMixin.reset_metrics(self) -> None\n```\n\nClear all collected metrics.\n\n#### CoreMixin.enable_metrics\n\n```python\nCoreMixin.enable_metrics(self) -> None\n```\n\nEnable metrics collection.\n\n#### CoreMixin.disable_metrics\n\n```python\nCoreMixin.disable_metrics(self) -> None\n```\n\nDisable metrics collection.\n\n#### CoreMixin.record_metric\n\n```python\nCoreMixin.record_metric(self, metric_name: str, count: int = 1) -> None\n```\n\nRecord a custom count metric.\n\n### Dependencies\n\n**Standard Library:**\n\n- `config.CorticalConfig`\n- `layers.CorticalLayer`\n- `layers.HierarchicalLayer`\n- `logging`\n- `minicolumn.Minicolumn`\n- ... and 5 more\n\n\n\n## documents.py\n\nDocument management: processing, adding, removing, and metadata handling.\n\nThis module contains all methods related to managing documents in the corpus.\n\n\n### Classes\n\n#### DocumentsMixin\n\nMixin providing document management functionality.\n\n**Methods:**\n\n- `process_document`\n- `set_document_metadata`\n- `get_document_metadata`\n- `get_all_document_metadata`\n- `add_document_incremental`\n- `add_documents_batch`\n- `remove_document`\n- `remove_documents_batch`\n\n### Functions\n\n#### DocumentsMixin.process_document\n\n```python\nDocumentsMixin.process_document(self, doc_id: str, content: str, metadata: Optional[Dict[str, Any]] = None) -> Dict[str, int]\n```\n\nProcess a document and add it to the corpus.\n\n#### DocumentsMixin.set_document_metadata\n\n```python\nDocumentsMixin.set_document_metadata(self, doc_id: str, **kwargs) -> None\n```\n\nSet or update metadata for a document.\n\n#### DocumentsMixin.get_document_metadata\n\n```python\nDocumentsMixin.get_document_metadata(self, doc_id: str) -> Dict[str, Any]\n```\n\nGet metadata for a document.\n\n#### DocumentsMixin.get_all_document_metadata\n\n```python\nDocumentsMixin.get_all_document_metadata(self) -> Dict[str, Dict[str, Any]]\n```\n\nGet metadata for all documents.\n\n#### DocumentsMixin.add_document_incremental\n\n```python\nDocumentsMixin.add_document_incremental(self, doc_id: str, content: str, metadata: Optional[Dict[str, Any]] = None, recompute: str = 'tfidf') -> Dict[str, int]\n```\n\nAdd a document with selective recomputation for efficiency.\n\n#### DocumentsMixin.add_documents_batch\n\n```python\nDocumentsMixin.add_documents_batch(self, documents: List[Tuple[str, str, Optional[Dict[str, Any]]]], recompute: str = 'full', verbose: bool = True) -> Dict[str, Any]\n```\n\nAdd multiple documents with a single recomputation.\n\n#### DocumentsMixin.remove_document\n\n```python\nDocumentsMixin.remove_document(self, doc_id: str, verbose: bool = False) -> Dict[str, Any]\n```\n\nRemove a document from the corpus.\n\n#### DocumentsMixin.remove_documents_batch\n\n```python\nDocumentsMixin.remove_documents_batch(self, doc_ids: List[str], recompute: str = 'none', verbose: bool = True) -> Dict[str, Any]\n```\n\nRemove multiple documents efficiently with single recomputation.\n\n### Dependencies\n\n**Standard Library:**\n\n- `copy`\n- `layers.CorticalLayer`\n- `logging`\n- `observability.timed`\n- `typing.Any`\n- ... and 4 more\n\n\n\n## introspection.py\n\nIntrospection: state inspection, fingerprints, gaps, and summaries.\n\nThis module contains methods for examining the processor state and\ncomparing texts/documents.\n\n\n### Classes\n\n#### IntrospectionMixin\n\nMixin providing introspection functionality.\n\n**Methods:**\n\n- `get_document_signature`\n- `get_corpus_summary`\n- `analyze_knowledge_gaps`\n- `detect_anomalies`\n- `get_fingerprint`\n- `compare_fingerprints`\n- `explain_fingerprint`\n- `explain_similarity`\n- `find_similar_texts`\n- `compare_with`\n- `compare_documents`\n- `what_changed`\n- `summarize_document`\n- `detect_patterns`\n- `detect_patterns_in_corpus`\n- `get_pattern_summary`\n- `get_corpus_pattern_statistics`\n- `format_pattern_report`\n- `list_available_patterns`\n- `list_pattern_categories`\n\n### Functions\n\n#### IntrospectionMixin.get_document_signature\n\n```python\nIntrospectionMixin.get_document_signature(self, doc_id: str, n: int = 10) -> List[Tuple[str, float]]\n```\n\nGet the top-n TF-IDF terms for a document.\n\n#### IntrospectionMixin.get_corpus_summary\n\n```python\nIntrospectionMixin.get_corpus_summary(self) -> Dict\n```\n\nGet summary statistics about the corpus.\n\n#### IntrospectionMixin.analyze_knowledge_gaps\n\n```python\nIntrospectionMixin.analyze_knowledge_gaps(self) -> Dict\n```\n\nAnalyze the corpus for knowledge gaps.\n\n#### IntrospectionMixin.detect_anomalies\n\n```python\nIntrospectionMixin.detect_anomalies(self, threshold: float = 0.3) -> List[Dict]\n```\n\nDetect anomalous patterns in the corpus.\n\n#### IntrospectionMixin.get_fingerprint\n\n```python\nIntrospectionMixin.get_fingerprint(self, text: str, top_n: int = 20) -> Dict\n```\n\nCompute the semantic fingerprint of a text.\n\n#### IntrospectionMixin.compare_fingerprints\n\n```python\nIntrospectionMixin.compare_fingerprints(self, fp1: Dict, fp2: Dict) -> Dict\n```\n\nCompare two fingerprints and compute similarity metrics.\n\n#### IntrospectionMixin.explain_fingerprint\n\n```python\nIntrospectionMixin.explain_fingerprint(self, fp: Dict, top_n: int = 10) -> Dict\n```\n\nGenerate a human-readable explanation of a fingerprint.\n\n#### IntrospectionMixin.explain_similarity\n\n```python\nIntrospectionMixin.explain_similarity(self, fp1: Dict, fp2: Dict) -> str\n```\n\nGenerate a human-readable explanation of fingerprint similarity.\n\n#### IntrospectionMixin.find_similar_texts\n\n```python\nIntrospectionMixin.find_similar_texts(self, text: str, candidates: List[Tuple[str, str]], top_n: int = 5) -> List[Tuple[str, float, Dict]]\n```\n\nFind texts most similar to the given text.\n\n#### IntrospectionMixin.compare_with\n\n```python\nIntrospectionMixin.compare_with(self, other: 'CorticalTextProcessor', top_movers: int = 20, min_pagerank_delta: float = 0.0001) -> 'diff_module.SemanticDiff'\n```\n\nCompare this processor state with another to find semantic differences.\n\n#### IntrospectionMixin.compare_documents\n\n```python\nIntrospectionMixin.compare_documents(self, doc_id_1: str, doc_id_2: str) -> Dict\n```\n\nCompare two documents within this corpus.\n\n#### IntrospectionMixin.what_changed\n\n```python\nIntrospectionMixin.what_changed(self, old_content: str, new_content: str) -> Dict\n```\n\nCompare two text contents to show what changed semantically.\n\n#### IntrospectionMixin.summarize_document\n\n```python\nIntrospectionMixin.summarize_document(self, doc_id: str, num_sentences: int = 3) -> str\n```\n\nGenerate a summary of a document using extractive summarization.\n\n#### IntrospectionMixin.detect_patterns\n\n```python\nIntrospectionMixin.detect_patterns(self, doc_id: str, patterns: Optional[List[str]] = None) -> Dict[str, List[int]]\n```\n\nDetect programming patterns in a specific document.\n\n#### IntrospectionMixin.detect_patterns_in_corpus\n\n```python\nIntrospectionMixin.detect_patterns_in_corpus(self, patterns: Optional[List[str]] = None) -> Dict[str, Dict[str, List[int]]]\n```\n\nDetect patterns across all documents in the corpus.\n\n#### IntrospectionMixin.get_pattern_summary\n\n```python\nIntrospectionMixin.get_pattern_summary(self, doc_id: str) -> Dict[str, int]\n```\n\nGet a summary of pattern occurrences in a document.\n\n#### IntrospectionMixin.get_corpus_pattern_statistics\n\n```python\nIntrospectionMixin.get_corpus_pattern_statistics(self) -> Dict[str, Any]\n```\n\nGet pattern statistics across the entire corpus.\n\n#### IntrospectionMixin.format_pattern_report\n\n```python\nIntrospectionMixin.format_pattern_report(self, doc_id: str, show_lines: bool = False) -> str\n```\n\nFormat pattern detection results as a human-readable report.\n\n#### IntrospectionMixin.list_available_patterns\n\n```python\nIntrospectionMixin.list_available_patterns(self) -> List[str]\n```\n\nList all available pattern names that can be detected.\n\n#### IntrospectionMixin.list_pattern_categories\n\n```python\nIntrospectionMixin.list_pattern_categories(self) -> List[str]\n```\n\nList all pattern categories.\n\n### Dependencies\n\n**Standard Library:**\n\n- `layers.CorticalLayer`\n- `logging`\n- `re`\n- `typing.Any`\n- `typing.Dict`\n- ... and 4 more\n\n**Local Imports:**\n\n- `.fingerprint`\n- `.gaps`\n- `.patterns`\n- `.persistence`\n\n\n\n## persistence_api.py\n\nPersistence API: save, load, export, and migration methods.\n\nThis module contains all methods related to saving and loading processor state.\n\n\n### Classes\n\n#### PersistenceMixin\n\nMixin providing persistence functionality.\n\n**Methods:**\n\n- `save`\n- `load`\n- `save_json`\n- `load_json`\n- `migrate_to_json`\n- `export_graph`\n- `export_conceptnet_json`\n\n### Functions\n\n#### PersistenceMixin.save\n\n```python\nPersistenceMixin.save(self, filepath: str, verbose: bool = True, signing_key: Optional[bytes] = None) -> None\n```\n\nSave processor state to a file.\n\n#### PersistenceMixin.load\n\n```python\nPersistenceMixin.load(cls, filepath: str, verbose: bool = True, verify_key: Optional[bytes] = None) -> 'CorticalTextProcessor'\n```\n\nLoad processor state from a file.\n\n#### PersistenceMixin.save_json\n\n```python\nPersistenceMixin.save_json(self, state_dir: str, force: bool = False, verbose: bool = True) -> Dict[str, bool]\n```\n\nSave processor state to git-friendly JSON format.\n\n#### PersistenceMixin.load_json\n\n```python\nPersistenceMixin.load_json(cls, state_dir: str, config: Optional[CorticalConfig] = None, verbose: bool = True) -> 'CorticalTextProcessor'\n```\n\nLoad processor from git-friendly JSON format.\n\n#### PersistenceMixin.migrate_to_json\n\n```python\nPersistenceMixin.migrate_to_json(self, pkl_path: str, json_dir: str, verbose: bool = True) -> bool\n```\n\nMigrate existing pickle file to git-friendly JSON format.\n\n#### PersistenceMixin.export_graph\n\n```python\nPersistenceMixin.export_graph(self, filepath: str, layer: Optional[CorticalLayer] = None, max_nodes: int = 500) -> Dict\n```\n\nExport graph to JSON for visualization.\n\n#### PersistenceMixin.export_conceptnet_json\n\n```python\nPersistenceMixin.export_conceptnet_json(self, filepath: str, include_cross_layer: bool = True, include_typed_edges: bool = True, min_weight: float = 0.0, min_confidence: float = 0.0, max_nodes_per_layer: int = 100, verbose: bool = True) -> Dict[str, Any]\n```\n\nExport ConceptNet-style graph for visualization.\n\n### Dependencies\n\n**Standard Library:**\n\n- `config.CorticalConfig`\n- `layers.CorticalLayer`\n- `logging`\n- `observability.timed`\n- `typing.Any`\n- ... and 3 more\n\n**Local Imports:**\n\n- `.persistence`\n- `.state_storage`\n\n\n\n---\n\n*This chapter is part of [The Cortical Chronicles](../README.md), a self-documenting book generated by the Cortical Text Processor.*\n"
    },
    {
      "path": "02-architecture/mod-query.md",
      "title": "Search & Retrieval",
      "section": "architecture",
      "tags": [
        "architecture",
        "modules",
        "query"
      ],
      "source_files": [
        "/home/user/Opus-code-test/cortical/query/ranking.py",
        "/home/user/Opus-code-test/cortical/query/intent.py",
        "/home/user/Opus-code-test/cortical/query/passages.py",
        "/home/user/Opus-code-test/cortical/query/__init__.py",
        "/home/user/Opus-code-test/cortical/query/search.py",
        "/home/user/Opus-code-test/cortical/query/definitions.py",
        "/home/user/Opus-code-test/cortical/query/chunking.py",
        "/home/user/Opus-code-test/cortical/query/expansion.py"
      ],
      "excerpt": "Search and retrieval components for finding relevant documents and passages. Query Module ============ Query expansion and search functionality. This package provides methods for expanding queries...",
      "keywords": [
        "str",
        "float",
        "dict",
        "int",
        "list",
        "python",
        "bool",
        "tuple",
        "tokenizer",
        "query"
      ],
      "full_content": "# Search & Retrieval\n\nSearch and retrieval components for finding relevant documents and passages.\n\n## Modules\n\n- **__init__.py**: Query Module\n- **chunking.py**: Chunking Module\n- **definitions.py**: Definition Search Module\n- **expansion.py**: Query Expansion Module\n- **intent.py**: Intent Query Module\n- **passages.py**: Passage Retrieval Module\n- **ranking.py**: Ranking Module\n- **search.py**: Document Search Module\n\n\n## __init__.py\n\nQuery Module\n============\n\nQuery expansion and search functionality.\n\nThis package provides methods for expanding queries using lateral connections,\nconcept clusters, and word variants, then searching...\n\n\n### Dependencies\n\n**Standard Library:**\n\n- `analogy.complete_analogy`\n- `analogy.complete_analogy_simple`\n- `analogy.find_relation_between`\n- `analogy.find_terms_with_relation`\n- `chunking.CODE_BOUNDARY_PATTERN`\n- ... and 49 more\n\n\n\n## chunking.py\n\nChunking Module\n==============\n\nFunctions for splitting documents into chunks for passage retrieval.\n\nThis module provides:\n- Fixed-size text chunking with overlap\n- Code-aware chunking aligned to sem...\n\n\n### Functions\n\n#### create_chunks\n\n```python\ncreate_chunks(text: str, chunk_size: int = 512, overlap: int = 128) -> List[Tuple[str, int, int]]\n```\n\nSplit text into overlapping chunks.\n\n#### find_code_boundaries\n\n```python\nfind_code_boundaries(text: str) -> List[int]\n```\n\nFind semantic boundaries in code (class/function definitions, decorators).\n\n#### create_code_aware_chunks\n\n```python\ncreate_code_aware_chunks(text: str, target_size: int = 512, min_size: int = 100, max_size: int = 1024) -> List[Tuple[str, int, int]]\n```\n\nCreate chunks aligned to code structure boundaries.\n\n#### is_code_file\n\n```python\nis_code_file(doc_id: str) -> bool\n```\n\nDetermine if a document is a code file based on its path/extension.\n\n#### precompute_term_cols\n\n```python\nprecompute_term_cols(query_terms: Dict[str, float], layer0: HierarchicalLayer) -> Dict[str, 'Minicolumn']\n```\n\nPre-compute minicolumn lookups for query terms.\n\n#### score_chunk_fast\n\n```python\nscore_chunk_fast(chunk_tokens: List[str], query_terms: Dict[str, float], term_cols: Dict[str, 'Minicolumn'], doc_id: Optional[str] = None) -> float\n```\n\nFast chunk scoring using pre-computed minicolumn lookups.\n\n#### score_chunk\n\n```python\nscore_chunk(chunk_text: str, query_terms: Dict[str, float], layer0: HierarchicalLayer, tokenizer: Tokenizer, doc_id: Optional[str] = None) -> float\n```\n\nScore a chunk against query terms using TF-IDF.\n\n### Dependencies\n\n**Standard Library:**\n\n- `layers.HierarchicalLayer`\n- `re`\n- `tokenizer.Tokenizer`\n- `typing.Dict`\n- `typing.List`\n- ... and 3 more\n\n\n\n## definitions.py\n\nDefinition Search Module\n========================\n\nFunctions for finding and boosting code definitions (classes, functions, methods).\n\nThis module handles:\n- Detection of definition-seeking queries (\"...\n\n\n### Classes\n\n#### DefinitionQuery\n\nInfo about a definition-seeking query.\n\n### Functions\n\n#### is_definition_query\n\n```python\nis_definition_query(query_text: str) -> Tuple[bool, Optional[str], Optional[str]]\n```\n\nDetect if a query is looking for a code definition.\n\n#### find_definition_in_text\n\n```python\nfind_definition_in_text(text: str, identifier: str, def_type: str, context_chars: int = 500) -> Optional[Tuple[str, int, int]]\n```\n\nFind a definition in source text and extract surrounding context.\n\n#### find_definition_passages\n\n```python\nfind_definition_passages(query_text: str, documents: Dict[str, str], context_chars: int = 500, boost: float = DEFINITION_BOOST) -> List[Tuple[str, str, int, int, float]]\n```\n\nFind definition passages for a definition query.\n\n#### detect_definition_query\n\n```python\ndetect_definition_query(query_text: str) -> DefinitionQuery\n```\n\nDetect if a query is searching for a code definition.\n\n#### apply_definition_boost\n\n```python\napply_definition_boost(passages: List[Tuple[str, str, int, int, float]], query_text: str, boost_factor: float = 3.0) -> List[Tuple[str, str, int, int, float]]\n```\n\nBoost passages that contain actual code definitions matching the query.\n\n#### is_test_file\n\n```python\nis_test_file(doc_id: str) -> bool\n```\n\nDetect if a document ID represents a test file.\n\n#### boost_definition_documents\n\n```python\nboost_definition_documents(doc_results: List[Tuple[str, float]], query_text: str, documents: Dict[str, str], boost_factor: float = 2.0, test_with_definition_penalty: float = 0.5, test_without_definition_penalty: float = 0.7) -> List[Tuple[str, float]]\n```\n\nBoost documents that contain the actual definition being searched for.\n\n### Dependencies\n\n**Standard Library:**\n\n- `re`\n- `typing.Any`\n- `typing.Dict`\n- `typing.List`\n- `typing.Optional`\n- ... and 2 more\n\n\n\n## expansion.py\n\nQuery Expansion Module\n=====================\n\nFunctions for expanding query terms using lateral connections,\nsemantic relations, and code concept synonyms.\n\nThis module provides:\n- Basic query expansi...\n\n\n### Functions\n\n#### score_relation_path\n\n```python\nscore_relation_path(path: List[str]) -> float\n```\n\nScore a relation path by its semantic coherence.\n\n#### expand_query\n\n```python\nexpand_query(query_text: str, layers: Dict[CorticalLayer, HierarchicalLayer], tokenizer: Tokenizer, max_expansions: int = 10, use_lateral: bool = True, use_concepts: bool = True, use_variants: bool = True, use_code_concepts: bool = False, filter_code_stop_words: bool = False, tfidf_weight: float = 0.7, max_expansion_weight: float = 2.0) -> Dict[str, float]\n```\n\nExpand a query using lateral connections and concept clusters.\n\n#### expand_query_semantic\n\n```python\nexpand_query_semantic(query_text: str, layers: Dict[CorticalLayer, HierarchicalLayer], tokenizer: Tokenizer, semantic_relations: List[Tuple[str, str, str, float]], max_expansions: int = 10) -> Dict[str, float]\n```\n\nExpand query using semantic relations extracted from corpus.\n\n#### expand_query_multihop\n\n```python\nexpand_query_multihop(query_text: str, layers: Dict[CorticalLayer, HierarchicalLayer], tokenizer: Tokenizer, semantic_relations: List[Tuple[str, str, str, float]], max_hops: int = 2, max_expansions: int = 15, decay_factor: float = 0.5, min_path_score: float = 0.2) -> Dict[str, float]\n```\n\nExpand query using multi-hop semantic inference.\n\n#### get_expanded_query_terms\n\n```python\nget_expanded_query_terms(query_text: str, layers: Dict[CorticalLayer, HierarchicalLayer], tokenizer: Tokenizer, use_expansion: bool = True, semantic_relations: Optional[List[Tuple[str, str, str, float]]] = None, use_semantic: bool = True, max_expansions: int = 5, semantic_discount: float = 0.8, filter_code_stop_words: bool = False) -> Dict[str, float]\n```\n\nGet expanded query terms with optional semantic expansion.\n\n### Dependencies\n\n**Standard Library:**\n\n- `code_concepts.expand_code_concepts`\n- `collections.defaultdict`\n- `config.DEFAULT_CHAIN_VALIDITY`\n- `layers.CorticalLayer`\n- `layers.HierarchicalLayer`\n- ... and 6 more\n\n\n\n## intent.py\n\nIntent Query Module\n==================\n\nIntent-based query understanding for natural language code search.\n\nThis module handles:\n- Parsing natural language queries to extract intent (where, how, what,...\n\n\n### Classes\n\n#### ParsedIntent\n\nStructured representation of a parsed query intent.\n\n### Functions\n\n#### parse_intent_query\n\n```python\nparse_intent_query(query_text: str) -> ParsedIntent\n```\n\nParse a natural language query to extract intent and searchable terms.\n\n#### search_by_intent\n\n```python\nsearch_by_intent(query_text: str, layers: Dict[CorticalLayer, HierarchicalLayer], tokenizer: 'Tokenizer', top_n: int = 5) -> List[Tuple[str, float, ParsedIntent]]\n```\n\nSearch the corpus using intent-based query understanding.\n\n### Dependencies\n\n**Standard Library:**\n\n- `code_concepts.get_related_terms`\n- `collections.defaultdict`\n- `layers.CorticalLayer`\n- `layers.HierarchicalLayer`\n- `typing.Dict`\n- ... and 4 more\n\n\n\n## passages.py\n\nPassage Retrieval Module\n========================\n\nFunctions for retrieving relevant passages from documents.\n\nThis module provides:\n- Passage retrieval for RAG systems\n- Batch passage retrieval\n- Int...\n\n\n### Functions\n\n#### find_passages_for_query\n\n```python\nfind_passages_for_query(query_text: str, layers: Dict[CorticalLayer, HierarchicalLayer], tokenizer: Tokenizer, documents: Dict[str, str], top_n: int = 5, chunk_size: int = 512, overlap: int = 128, use_expansion: bool = True, doc_filter: Optional[List[str]] = None, semantic_relations: Optional[List[Tuple[str, str, str, float]]] = None, use_semantic: bool = True, use_definition_search: bool = True, definition_boost: float = DEFINITION_BOOST, apply_doc_boost: bool = True, doc_metadata: Optional[Dict[str, Dict[str, Any]]] = None, auto_detect_intent: bool = True, prefer_docs: bool = False, custom_boosts: Optional[Dict[str, float]] = None, use_code_aware_chunks: bool = True, filter_code_stop_words: bool = True, test_file_penalty: float = 0.8) -> List[Tuple[str, str, int, int, float]]\n```\n\nFind text passages most relevant to a query.\n\n#### find_documents_batch\n\n```python\nfind_documents_batch(queries: List[str], layers: Dict[CorticalLayer, HierarchicalLayer], tokenizer: Tokenizer, top_n: int = 5, use_expansion: bool = True, semantic_relations: Optional[List[Tuple[str, str, str, float]]] = None, use_semantic: bool = True) -> List[List[Tuple[str, float]]]\n```\n\nFind documents for multiple queries efficiently.\n\n#### find_passages_batch\n\n```python\nfind_passages_batch(queries: List[str], layers: Dict[CorticalLayer, HierarchicalLayer], tokenizer: Tokenizer, documents: Dict[str, str], top_n: int = 5, chunk_size: int = 512, overlap: int = 128, use_expansion: bool = True, doc_filter: Optional[List[str]] = None, semantic_relations: Optional[List[Tuple[str, str, str, float]]] = None, use_semantic: bool = True) -> List[List[Tuple[str, str, int, int, float]]]\n```\n\nFind passages for multiple queries efficiently.\n\n### Dependencies\n\n**Standard Library:**\n\n- `chunking.CODE_BOUNDARY_PATTERN`\n- `chunking.create_chunks`\n- `chunking.create_code_aware_chunks`\n- `chunking.find_code_boundaries`\n- `chunking.is_code_file`\n- ... and 18 more\n\n\n\n## ranking.py\n\nRanking Module\n=============\n\nMulti-stage ranking and document type boosting for search results.\n\nThis module provides:\n- Document type boosting (docs, code, tests)\n- Conceptual vs implementation quer...\n\n\n### Functions\n\n#### is_conceptual_query\n\n```python\nis_conceptual_query(query_text: str) -> bool\n```\n\nDetermine if a query is conceptual (should boost documentation).\n\n#### get_doc_type_boost\n\n```python\nget_doc_type_boost(doc_id: str, doc_metadata: Optional[Dict[str, Dict[str, Any]]] = None, custom_boosts: Optional[Dict[str, float]] = None) -> float\n```\n\nGet the boost factor for a document based on its type.\n\n#### apply_doc_type_boost\n\n```python\napply_doc_type_boost(results: List[Tuple[str, float]], doc_metadata: Optional[Dict[str, Dict[str, Any]]] = None, boost_docs: bool = True, custom_boosts: Optional[Dict[str, float]] = None) -> List[Tuple[str, float]]\n```\n\nApply document type boosting to search results.\n\n#### find_documents_with_boost\n\n```python\nfind_documents_with_boost(query_text: str, layers: Dict[CorticalLayer, HierarchicalLayer], tokenizer: Tokenizer, top_n: int = 5, doc_metadata: Optional[Dict[str, Dict[str, Any]]] = None, auto_detect_intent: bool = True, prefer_docs: bool = False, custom_boosts: Optional[Dict[str, float]] = None, use_expansion: bool = True, semantic_relations: Optional[List[Tuple[str, str, str, float]]] = None, use_semantic: bool = True) -> List[Tuple[str, float]]\n```\n\nFind documents with optional document-type boosting.\n\n#### find_relevant_concepts\n\n```python\nfind_relevant_concepts(query_terms: Dict[str, float], layers: Dict[CorticalLayer, HierarchicalLayer], top_n: int = 5) -> List[Tuple[str, float, set]]\n```\n\nStage 1: Find concepts relevant to query terms.\n\n#### multi_stage_rank\n\n```python\nmulti_stage_rank(query_text: str, layers: Dict[CorticalLayer, HierarchicalLayer], tokenizer: Tokenizer, documents: Dict[str, str], top_n: int = 5, chunk_size: int = 512, overlap: int = 128, concept_boost: float = 0.3, use_expansion: bool = True, semantic_relations: Optional[List[Tuple[str, str, str, float]]] = None, use_semantic: bool = True) -> List[Tuple[str, str, int, int, float, Dict[str, float]]]\n```\n\nMulti-stage ranking pipeline for improved RAG performance.\n\n#### multi_stage_rank_documents\n\n```python\nmulti_stage_rank_documents(query_text: str, layers: Dict[CorticalLayer, HierarchicalLayer], tokenizer: Tokenizer, top_n: int = 5, concept_boost: float = 0.3, use_expansion: bool = True, semantic_relations: Optional[List[Tuple[str, str, str, float]]] = None, use_semantic: bool = True) -> List[Tuple[str, float, Dict[str, float]]]\n```\n\nMulti-stage ranking for documents (without chunk scoring).\n\n### Dependencies\n\n**Standard Library:**\n\n- `collections.defaultdict`\n- `constants.CONCEPTUAL_KEYWORDS`\n- `constants.DOC_TYPE_BOOSTS`\n- `constants.IMPLEMENTATION_KEYWORDS`\n- `expansion.get_expanded_query_terms`\n- ... and 9 more\n\n\n\n## search.py\n\nDocument Search Module\n=====================\n\nFunctions for searching and retrieving documents from the corpus.\n\nThis module provides:\n- Basic document search using TF-IDF scoring\n- Fast document sear...\n\n\n### Functions\n\n#### find_documents_for_query\n\n```python\nfind_documents_for_query(query_text: str, layers: Dict[CorticalLayer, HierarchicalLayer], tokenizer: Tokenizer, top_n: int = 5, use_expansion: bool = True, semantic_relations: Optional[List[Tuple[str, str, str, float]]] = None, use_semantic: bool = True, doc_name_boost: float = 2.0, filter_code_stop_words: bool = True, test_file_penalty: float = 0.8) -> List[Tuple[str, float]]\n```\n\nFind documents most relevant to a query using TF-IDF and optional expansion.\n\n#### fast_find_documents\n\n```python\nfast_find_documents(query_text: str, layers: Dict[CorticalLayer, HierarchicalLayer], tokenizer: Tokenizer, top_n: int = 5, candidate_multiplier: int = 3, use_code_concepts: bool = True, doc_name_boost: float = 2.0) -> List[Tuple[str, float]]\n```\n\nFast document search using candidate filtering.\n\n#### build_document_index\n\n```python\nbuild_document_index(layers: Dict[CorticalLayer, HierarchicalLayer]) -> Dict[str, Dict[str, float]]\n```\n\nBuild an optimized inverted index for fast querying.\n\n#### search_with_index\n\n```python\nsearch_with_index(query_text: str, index: Dict[str, Dict[str, float]], tokenizer: Tokenizer, top_n: int = 5) -> List[Tuple[str, float]]\n```\n\nSearch using a pre-built inverted index.\n\n#### query_with_spreading_activation\n\n```python\nquery_with_spreading_activation(query_text: str, layers: Dict[CorticalLayer, HierarchicalLayer], tokenizer: Tokenizer, top_n: int = 10, max_expansions: int = 8) -> List[Tuple[str, float]]\n```\n\nQuery with automatic expansion using spreading activation.\n\n#### find_related_documents\n\n```python\nfind_related_documents(doc_id: str, layers: Dict[CorticalLayer, HierarchicalLayer]) -> List[Tuple[str, float]]\n```\n\nFind documents related to a given document via lateral connections.\n\n#### graph_boosted_search\n\n```python\ngraph_boosted_search(query_text: str, layers: Dict[CorticalLayer, HierarchicalLayer], tokenizer: Tokenizer, top_n: int = 5, pagerank_weight: float = 0.3, proximity_weight: float = 0.2, use_expansion: bool = True, semantic_relations: Optional[List[Tuple[str, str, str, float]]] = None) -> List[Tuple[str, float]]\n```\n\nGraph-Boosted BM25 (GB-BM25): Hybrid scoring combining BM25 with graph signals.\n\n### Dependencies\n\n**Standard Library:**\n\n- `code_concepts.get_related_terms`\n- `collections.defaultdict`\n- `expansion.expand_query`\n- `expansion.get_expanded_query_terms`\n- `layers.CorticalLayer`\n- ... and 6 more\n\n\n\n---\n\n*This chapter is part of [The Cortical Chronicles](../README.md), a self-documenting book generated by the Cortical Text Processor.*\n"
    },
    {
      "path": "02-architecture/mod-utilities.md",
      "title": "Utilities",
      "section": "architecture",
      "tags": [
        "architecture",
        "modules",
        "utilities"
      ],
      "source_files": [
        "/home/user/Opus-code-test/cortical/patterns.py",
        "/home/user/Opus-code-test/cortical/code_concepts.py",
        "/home/user/Opus-code-test/cortical/diff.py",
        "/home/user/Opus-code-test/cortical/gaps.py",
        "/home/user/Opus-code-test/cortical/fingerprint.py",
        "/home/user/Opus-code-test/cortical/mcp_server.py",
        "/home/user/Opus-code-test/cortical/fluent.py",
        "/home/user/Opus-code-test/cortical/cli_wrapper.py"
      ],
      "excerpt": "Utility modules supporting various features. CLI wrapper framework for collecting context and triggering actions. Design philosophy: QUIET BY DEFAULT, POWERFUL WHEN NEEDED. Most of the time you just...",
      "keywords": [
        "str",
        "python",
        "self",
        "list",
        "fluentprocessor",
        "dict",
        "none",
        "optional",
        "session",
        "int"
      ],
      "full_content": "# Utilities\n\nUtility modules supporting various features.\n\n## Modules\n\n- **cli_wrapper.py**: CLI wrapper framework for collecting context and triggering actions.\n- **code_concepts.py**: Code Concepts Module\n- **diff.py**: Semantic Diff Module\n- **fingerprint.py**: Fingerprint Module\n- **fluent.py**: Fluent API for CorticalTextProcessor - chainable method interface.\n- **gaps.py**: Gaps Module\n- **mcp_server.py**: MCP (Model Context Protocol) Server for Cortical Text Processor.\n- **patterns.py**: Code Pattern Detection Module\n\n\n## cli_wrapper.py\n\nCLI wrapper framework for collecting context and triggering actions.\n\nDesign philosophy: QUIET BY DEFAULT, POWERFUL WHEN NEEDED.\n\nMost of the time you just want to run a command and check if it worked...\n\n\n### Classes\n\n#### GitContext\n\nGit repository context information.\n\n**Methods:**\n\n- `collect`\n- `to_dict`\n\n#### ExecutionContext\n\nComplete context for a CLI command execution.\n\n**Methods:**\n\n- `to_dict`\n- `to_json`\n- `summary`\n\n#### HookType\n\nTypes of hooks that can be registered.\n\n#### HookRegistry\n\nRegistry for CLI execution hooks.\n\n**Methods:**\n\n- `register`\n- `register_pre`\n- `register_post`\n- `register_success`\n- `register_error`\n- `get_hooks`\n- `trigger`\n\n#### CLIWrapper\n\nWrapper for CLI command execution with context collection and hooks.\n\n**Methods:**\n\n- `run`\n- `on_success`\n- `on_error`\n- `on_complete`\n\n#### TaskCompletionManager\n\nManager for task completion triggers and context window management.\n\n**Methods:**\n\n- `on_task_complete`\n- `on_any_complete`\n- `handle_completion`\n- `get_session_summary`\n- `should_trigger_reindex`\n\n#### ContextWindowManager\n\nManages context window state based on CLI execution history.\n\n**Methods:**\n\n- `add_execution`\n- `add_file_read`\n- `get_recent_files`\n- `get_context_summary`\n- `suggest_pruning`\n\n#### Session\n\nTrack a sequence of commands as a session.\n\n**Methods:**\n\n- `run`\n- `should_reindex`\n- `summary`\n- `results`\n- `success_rate`\n- `all_passed`\n- `modified_files`\n\n#### TaskCheckpoint\n\nSave/restore context state when switching between tasks.\n\n**Methods:**\n\n- `save`\n- `load`\n- `list_tasks`\n- `delete`\n- `summarize`\n\n### Functions\n\n#### create_wrapper_with_completion_manager\n\n```python\ncreate_wrapper_with_completion_manager() -> Tuple[CLIWrapper, TaskCompletionManager]\n```\n\nCreate a CLIWrapper with an attached TaskCompletionManager.\n\n#### run_with_context\n\n```python\nrun_with_context(command: Union[str, List[str]], **kwargs) -> ExecutionContext\n```\n\nConvenience function to run a command with full context collection.\n\n#### run\n\n```python\nrun(command: Union[str, List[str]], git: bool = False, timeout: Optional[float] = None, cwd: Optional[str] = None) -> ExecutionContext\n```\n\nRun a command. That's it.\n\n#### test_then_commit\n\n```python\ntest_then_commit(test_cmd: Union[str, List[str]] = 'python -m unittest discover -s tests', message: str = 'Update', add_all: bool = True) -> Tuple[bool, List[ExecutionContext]]\n```\n\nRun tests, commit only if they pass.\n\n#### commit_and_push\n\n```python\ncommit_and_push(message: str, add_all: bool = True, branch: Optional[str] = None) -> Tuple[bool, List[ExecutionContext]]\n```\n\nAdd, commit, and push in one go.\n\n#### sync_with_main\n\n```python\nsync_with_main(main_branch: str = 'main') -> Tuple[bool, List[ExecutionContext]]\n```\n\nFetch and rebase current branch on main.\n\n#### GitContext.collect\n\n```python\nGitContext.collect(cls, cwd: Optional[str] = None) -> 'GitContext'\n```\n\nCollect git context from current directory.\n\n#### GitContext.to_dict\n\n```python\nGitContext.to_dict(self) -> Dict[str, Any]\n```\n\nConvert to dictionary.\n\n#### ExecutionContext.to_dict\n\n```python\nExecutionContext.to_dict(self) -> Dict[str, Any]\n```\n\nConvert to dictionary for serialization.\n\n#### ExecutionContext.to_json\n\n```python\nExecutionContext.to_json(self, indent: int = 2) -> str\n```\n\nConvert to JSON string.\n\n#### ExecutionContext.summary\n\n```python\nExecutionContext.summary(self) -> str\n```\n\nReturn a concise summary string.\n\n#### HookRegistry.register\n\n```python\nHookRegistry.register(self, hook_type: HookType, callback: HookCallback, pattern: Optional[str] = None) -> None\n```\n\nRegister a hook callback.\n\n#### HookRegistry.register_pre\n\n```python\nHookRegistry.register_pre(self, pattern: Optional[str], callback: HookCallback) -> None\n```\n\nConvenience method for pre-execution hooks.\n\n#### HookRegistry.register_post\n\n```python\nHookRegistry.register_post(self, pattern: Optional[str], callback: HookCallback) -> None\n```\n\nConvenience method for post-execution hooks.\n\n#### HookRegistry.register_success\n\n```python\nHookRegistry.register_success(self, pattern: Optional[str], callback: HookCallback) -> None\n```\n\nConvenience method for success hooks.\n\n#### HookRegistry.register_error\n\n```python\nHookRegistry.register_error(self, pattern: Optional[str], callback: HookCallback) -> None\n```\n\nConvenience method for error hooks.\n\n#### HookRegistry.get_hooks\n\n```python\nHookRegistry.get_hooks(self, hook_type: HookType, command: List[str]) -> List[HookCallback]\n```\n\nGet all hooks that should be triggered for a command.\n\n#### HookRegistry.trigger\n\n```python\nHookRegistry.trigger(self, hook_type: HookType, context: ExecutionContext) -> None\n```\n\nTrigger all matching hooks.\n\n#### CLIWrapper.run\n\n```python\nCLIWrapper.run(self, command: Union[str, List[str]], cwd: Optional[str] = None, timeout: Optional[float] = None, env: Optional[Dict[str, str]] = None, **kwargs) -> ExecutionContext\n```\n\nExecute a command with context collection and hooks.\n\n#### CLIWrapper.on_success\n\n```python\nCLIWrapper.on_success(self, pattern: Optional[str] = None)\n```\n\nDecorator to register a success hook.\n\n#### CLIWrapper.on_error\n\n```python\nCLIWrapper.on_error(self, pattern: Optional[str] = None)\n```\n\nDecorator to register an error hook.\n\n#### CLIWrapper.on_complete\n\n```python\nCLIWrapper.on_complete(self, pattern: Optional[str] = None)\n```\n\nDecorator to register a completion hook (success or failure).\n\n#### TaskCompletionManager.on_task_complete\n\n```python\nTaskCompletionManager.on_task_complete(self, task_type: str, callback: HookCallback) -> None\n```\n\nRegister a callback for when a specific task type completes.\n\n#### TaskCompletionManager.on_any_complete\n\n```python\nTaskCompletionManager.on_any_complete(self, callback: HookCallback) -> None\n```\n\nRegister a callback for any task completion.\n\n#### TaskCompletionManager.handle_completion\n\n```python\nTaskCompletionManager.handle_completion(self, context: ExecutionContext) -> None\n```\n\nHandle task completion and trigger appropriate callbacks.\n\n#### TaskCompletionManager.get_session_summary\n\n```python\nTaskCompletionManager.get_session_summary(self) -> Dict[str, Any]\n```\n\nGet summary of all tasks completed in this session.\n\n#### TaskCompletionManager.should_trigger_reindex\n\n```python\nTaskCompletionManager.should_trigger_reindex(self) -> bool\n```\n\nDetermine if corpus should be re-indexed based on session activity.\n\n#### ContextWindowManager.add_execution\n\n```python\nContextWindowManager.add_execution(self, context: ExecutionContext) -> None\n```\n\nAdd an execution to the context window.\n\n#### ContextWindowManager.add_file_read\n\n```python\nContextWindowManager.add_file_read(self, filepath: str) -> None\n```\n\nTrack that a file was read.\n\n#### ContextWindowManager.get_recent_files\n\n```python\nContextWindowManager.get_recent_files(self, limit: int = 10) -> List[str]\n```\n\nGet most recently accessed files.\n\n#### ContextWindowManager.get_context_summary\n\n```python\nContextWindowManager.get_context_summary(self) -> Dict[str, Any]\n```\n\nGet a summary of current context window state.\n\n#### ContextWindowManager.suggest_pruning\n\n```python\nContextWindowManager.suggest_pruning(self) -> List[str]\n```\n\nSuggest files that could be pruned from context.\n\n#### Session.run\n\n```python\nSession.run(self, command: Union[str, List[str]], **kwargs) -> ExecutionContext\n```\n\nRun a command within this session.\n\n#### Session.should_reindex\n\n```python\nSession.should_reindex(self) -> bool\n```\n\nCheck if corpus re-indexing is recommended based on session activity.\n\n#### Session.summary\n\n```python\nSession.summary(self) -> Dict[str, Any]\n```\n\nGet a summary of this session's activity.\n\n#### Session.results\n\n```python\nSession.results(self) -> List[ExecutionContext]\n```\n\nAll command results from this session.\n\n#### Session.success_rate\n\n```python\nSession.success_rate(self) -> float\n```\n\nFraction of commands that succeeded (0.0 to 1.0).\n\n#### Session.all_passed\n\n```python\nSession.all_passed(self) -> bool\n```\n\nTrue if all commands in this session succeeded.\n\n#### Session.modified_files\n\n```python\nSession.modified_files(self) -> List[str]\n```\n\nList of files modified during this session (from git context).\n\n#### TaskCheckpoint.save\n\n```python\nTaskCheckpoint.save(self, task_name: str, context: Dict[str, Any]) -> None\n```\n\nSave context for a task.\n\n#### TaskCheckpoint.load\n\n```python\nTaskCheckpoint.load(self, task_name: str) -> Optional[Dict[str, Any]]\n```\n\nLoad context for a task. Returns None if not found.\n\n#### TaskCheckpoint.list_tasks\n\n```python\nTaskCheckpoint.list_tasks(self) -> List[str]\n```\n\nList all saved task checkpoints.\n\n#### TaskCheckpoint.delete\n\n```python\nTaskCheckpoint.delete(self, task_name: str) -> bool\n```\n\nDelete a checkpoint. Returns True if deleted.\n\n#### TaskCheckpoint.summarize\n\n```python\nTaskCheckpoint.summarize(self, task_name: str) -> Optional[str]\n```\n\nGet a one-line summary of a task checkpoint.\n\n### Dependencies\n\n**Standard Library:**\n\n- `dataclasses.asdict`\n- `dataclasses.dataclass`\n- `dataclasses.field`\n- `datetime.datetime`\n- `enum.Enum`\n- ... and 15 more\n\n\n\n## code_concepts.py\n\nCode Concepts Module\n====================\n\nProgramming concept groups for semantic code search.\n\nMaps common programming synonyms and related terms to enable\nintent-based code retrieval. When a develo...\n\n\n### Functions\n\n#### get_related_terms\n\n```python\nget_related_terms(term: str, max_terms: int = 5) -> List[str]\n```\n\nGet programming terms related to the given term.\n\n#### expand_code_concepts\n\n```python\nexpand_code_concepts(terms: List[str], max_expansions_per_term: int = 3, weight: float = 0.6) -> Dict[str, float]\n```\n\nExpand a list of terms using code concept groups.\n\n#### get_concept_group\n\n```python\nget_concept_group(term: str) -> List[str]\n```\n\nGet the concept group names a term belongs to.\n\n#### list_concept_groups\n\n```python\nlist_concept_groups() -> List[str]\n```\n\nList all available concept group names.\n\n#### get_group_terms\n\n```python\nget_group_terms(group_name: str) -> List[str]\n```\n\nGet all terms in a concept group.\n\n### Dependencies\n\n**Standard Library:**\n\n- `typing.Dict`\n- `typing.FrozenSet`\n- `typing.List`\n- `typing.Set`\n\n\n\n## diff.py\n\nSemantic Diff Module\n====================\n\nProvides \"What Changed?\" functionality for comparing:\n- Two versions of a document\n- Two processor states\n- Before/after states of a corpus\n\nThis goes beyond...\n\n\n### Classes\n\n#### TermChange\n\nRepresents a change to a term/concept.\n\n**Methods:**\n\n- `pagerank_delta`\n- `tfidf_delta`\n- `documents_added`\n- `documents_removed`\n\n#### RelationChange\n\nRepresents a change to a semantic relation.\n\n#### ClusterChange\n\nRepresents a change to concept clustering.\n\n#### SemanticDiff\n\nComplete semantic diff between two states.\n\n**Methods:**\n\n- `summary`\n- `to_dict`\n\n### Functions\n\n#### compare_processors\n\n```python\ncompare_processors(old_processor: 'CorticalTextProcessor', new_processor: 'CorticalTextProcessor', top_movers: int = 20, min_pagerank_delta: float = 0.0001) -> SemanticDiff\n```\n\nCompare two processor states to find semantic differences.\n\n#### compare_documents\n\n```python\ncompare_documents(processor: 'CorticalTextProcessor', doc_id_old: str, doc_id_new: str) -> Dict[str, Any]\n```\n\nCompare two documents within the same corpus.\n\n#### what_changed\n\n```python\nwhat_changed(processor: 'CorticalTextProcessor', old_content: str, new_content: str, temp_doc_prefix: str = '_diff_temp_') -> Dict[str, Any]\n```\n\nCompare two text contents to show what changed semantically.\n\n#### TermChange.pagerank_delta\n\n```python\nTermChange.pagerank_delta(self) -> Optional[float]\n```\n\nChange in PageRank importance.\n\n#### TermChange.tfidf_delta\n\n```python\nTermChange.tfidf_delta(self) -> Optional[float]\n```\n\nChange in TF-IDF score.\n\n#### TermChange.documents_added\n\n```python\nTermChange.documents_added(self) -> Set[str]\n```\n\nDocuments where this term newly appears.\n\n#### TermChange.documents_removed\n\n```python\nTermChange.documents_removed(self) -> Set[str]\n```\n\nDocuments where this term no longer appears.\n\n#### SemanticDiff.summary\n\n```python\nSemanticDiff.summary(self) -> str\n```\n\nGenerate a human-readable summary of changes.\n\n#### SemanticDiff.to_dict\n\n```python\nSemanticDiff.to_dict(self) -> Dict[str, Any]\n```\n\nConvert to dictionary for serialization.\n\n### Dependencies\n\n**Standard Library:**\n\n- `collections.defaultdict`\n- `dataclasses.dataclass`\n- `dataclasses.field`\n- `layers.CorticalLayer`\n- `layers.HierarchicalLayer`\n- ... and 7 more\n\n\n\n## fingerprint.py\n\nFingerprint Module\n==================\n\nSemantic fingerprinting for code comparison and similarity analysis.\n\nA fingerprint is an interpretable representation of a text's semantic\ncontent, including te...\n\n\n### Classes\n\n#### SemanticFingerprint\n\nStructured representation of a text's semantic fingerprint.\n\n### Functions\n\n#### compute_fingerprint\n\n```python\ncompute_fingerprint(text: str, tokenizer: Tokenizer, layers: Optional[Dict[CorticalLayer, HierarchicalLayer]] = None, top_n: int = 20) -> SemanticFingerprint\n```\n\nCompute the semantic fingerprint of a text.\n\n#### compare_fingerprints\n\n```python\ncompare_fingerprints(fp1: SemanticFingerprint, fp2: SemanticFingerprint) -> Dict[str, Any]\n```\n\nCompare two fingerprints and compute similarity metrics.\n\n#### explain_fingerprint\n\n```python\nexplain_fingerprint(fp: SemanticFingerprint, top_n: int = 10) -> Dict[str, Any]\n```\n\nGenerate a human-readable explanation of a fingerprint.\n\n#### explain_similarity\n\n```python\nexplain_similarity(fp1: SemanticFingerprint, fp2: SemanticFingerprint, comparison: Optional[Dict[str, Any]] = None) -> str\n```\n\nGenerate a human-readable explanation of why two fingerprints are similar.\n\n### Dependencies\n\n**Standard Library:**\n\n- `code_concepts.get_concept_group`\n- `collections.defaultdict`\n- `layers.CorticalLayer`\n- `layers.HierarchicalLayer`\n- `math`\n- ... and 7 more\n\n\n\n## fluent.py\n\nFluent API for CorticalTextProcessor - chainable method interface.\n\nExample:\n    from cortical import FluentProcessor\n\n    # Simple usage\n    results = (FluentProcessor()\n        .add_document(\"doc1\",...\n\n\n### Classes\n\n#### FluentProcessor\n\nFluent/chainable API wrapper for CorticalTextProcessor.\n\n**Methods:**\n\n- `from_existing`\n- `from_files`\n- `from_directory`\n- `load`\n- `add_document`\n- `add_documents`\n- `with_config`\n- `with_tokenizer`\n- `build`\n- `save`\n- `search`\n- `fast_search`\n- `search_passages`\n- `expand`\n- `processor`\n- `is_built`\n\n### Functions\n\n#### FluentProcessor.from_existing\n\n```python\nFluentProcessor.from_existing(cls, processor: CorticalTextProcessor) -> 'FluentProcessor'\n```\n\nCreate a FluentProcessor from an existing CorticalTextProcessor.\n\n#### FluentProcessor.from_files\n\n```python\nFluentProcessor.from_files(cls, file_paths: List[Union[str, Path]], tokenizer: Optional[Tokenizer] = None, config: Optional[CorticalConfig] = None) -> 'FluentProcessor'\n```\n\nCreate a processor from a list of files.\n\n#### FluentProcessor.from_directory\n\n```python\nFluentProcessor.from_directory(cls, directory: Union[str, Path], pattern: str = '*.txt', recursive: bool = False, tokenizer: Optional[Tokenizer] = None, config: Optional[CorticalConfig] = None) -> 'FluentProcessor'\n```\n\nCreate a processor from all files in a directory.\n\n#### FluentProcessor.load\n\n```python\nFluentProcessor.load(cls, path: Union[str, Path]) -> 'FluentProcessor'\n```\n\nLoad a processor from a saved file.\n\n#### FluentProcessor.add_document\n\n```python\nFluentProcessor.add_document(self, doc_id: str, content: str, metadata: Optional[Dict[str, Any]] = None) -> 'FluentProcessor'\n```\n\nAdd a document to the processor (chainable).\n\n#### FluentProcessor.add_documents\n\n```python\nFluentProcessor.add_documents(self, documents: Union[Dict[str, str], List[Tuple[str, str]], List[Tuple[str, str, Dict]]]) -> 'FluentProcessor'\n```\n\nAdd multiple documents at once (chainable).\n\n#### FluentProcessor.with_config\n\n```python\nFluentProcessor.with_config(self, config: CorticalConfig) -> 'FluentProcessor'\n```\n\nSet configuration (chainable).\n\n#### FluentProcessor.with_tokenizer\n\n```python\nFluentProcessor.with_tokenizer(self, tokenizer: Tokenizer) -> 'FluentProcessor'\n```\n\nSet custom tokenizer (chainable).\n\n#### FluentProcessor.build\n\n```python\nFluentProcessor.build(self, verbose: bool = True, build_concepts: bool = True, pagerank_method: str = 'standard', connection_strategy: str = 'document_overlap', cluster_strictness: float = 1.0, bridge_weight: float = 0.0, show_progress: bool = False) -> 'FluentProcessor'\n```\n\nBuild the processor by computing all analysis phases (chainable).\n\n#### FluentProcessor.save\n\n```python\nFluentProcessor.save(self, path: Union[str, Path]) -> 'FluentProcessor'\n```\n\nSave the processor to disk (chainable).\n\n#### FluentProcessor.search\n\n```python\nFluentProcessor.search(self, query: str, top_n: int = 5, use_expansion: bool = True, use_semantic: bool = True) -> List[Tuple[str, float]]\n```\n\nSearch for documents matching the query.\n\n#### FluentProcessor.fast_search\n\n```python\nFluentProcessor.fast_search(self, query: str, top_n: int = 5, candidate_multiplier: int = 3, use_code_concepts: bool = True) -> List[Tuple[str, float]]\n```\n\nFast document search with pre-filtering.\n\n#### FluentProcessor.search_passages\n\n```python\nFluentProcessor.search_passages(self, query: str, top_n: int = 5, chunk_size: Optional[int] = None, overlap: Optional[int] = None, use_expansion: bool = True) -> List[Tuple[str, str, int, int, float]]\n```\n\nSearch for passage chunks matching the query.\n\n#### FluentProcessor.expand\n\n```python\nFluentProcessor.expand(self, query: str, max_expansions: Optional[int] = None, use_variants: bool = True, use_code_concepts: bool = False) -> Dict[str, float]\n```\n\nExpand a query with related terms.\n\n#### FluentProcessor.processor\n\n```python\nFluentProcessor.processor(self) -> CorticalTextProcessor\n```\n\nAccess the underlying CorticalTextProcessor instance.\n\n#### FluentProcessor.is_built\n\n```python\nFluentProcessor.is_built(self) -> bool\n```\n\nCheck if the processor has been built.\n\n### Dependencies\n\n**Standard Library:**\n\n- `config.CorticalConfig`\n- `os`\n- `pathlib.Path`\n- `processor.CorticalTextProcessor`\n- `tokenizer.Tokenizer`\n- ... and 6 more\n\n\n\n## gaps.py\n\nGaps Module\n===========\n\nKnowledge gap detection and anomaly analysis.\n\nIdentifies:\n- Isolated documents that don't connect well to the corpus\n- Weakly covered topics (few documents)\n- Bridge opportun...\n\n\n### Functions\n\n#### analyze_knowledge_gaps\n\n```python\nanalyze_knowledge_gaps(layers: Dict[CorticalLayer, HierarchicalLayer], documents: Dict[str, str]) -> Dict\n```\n\nAnalyze the corpus to identify potential knowledge gaps.\n\n#### detect_anomalies\n\n```python\ndetect_anomalies(layers: Dict[CorticalLayer, HierarchicalLayer], documents: Dict[str, str], threshold: float = 0.3) -> List[Dict]\n```\n\nDetect documents that don't fit well with the rest of the corpus.\n\n### Dependencies\n\n**Standard Library:**\n\n- `analysis.cosine_similarity`\n- `collections.defaultdict`\n- `layers.CorticalLayer`\n- `layers.HierarchicalLayer`\n- `math`\n- ... and 5 more\n\n\n\n## mcp_server.py\n\nMCP (Model Context Protocol) Server for Cortical Text Processor.\n\nProvides an MCP server interface for AI agents to integrate with the\nCortical Text Processor, enabling semantic search, query expansio...\n\n\n### Classes\n\n#### CorticalMCPServer\n\nMCP Server wrapper for CorticalTextProcessor.\n\n**Methods:**\n\n- `run`\n\n### Functions\n\n#### create_mcp_server\n\n```python\ncreate_mcp_server(corpus_path: Optional[str] = None, config: Optional[CorticalConfig] = None) -> CorticalMCPServer\n```\n\nCreate a Cortical MCP Server instance.\n\n#### main\n\n```python\nmain()\n```\n\nMain entry point for running the MCP server from command line.\n\n#### CorticalMCPServer.run\n\n```python\nCorticalMCPServer.run(self, transport: str = 'stdio')\n```\n\nRun the MCP server.\n\n### Dependencies\n\n**Standard Library:**\n\n- `config.CorticalConfig`\n- `logging`\n- `mcp.server.FastMCP`\n- `os`\n- `pathlib.Path`\n- ... and 5 more\n\n\n\n## patterns.py\n\nCode Pattern Detection Module\n==============================\n\nDetects common programming patterns in indexed code.\n\nIdentifies design patterns, idioms, and code structures including:\n- Singleton patte...\n\n\n### Functions\n\n#### detect_patterns_in_text\n\n```python\ndetect_patterns_in_text(text: str, patterns: Optional[List[str]] = None) -> Dict[str, List[int]]\n```\n\nDetect programming patterns in a text string.\n\n#### detect_patterns_in_documents\n\n```python\ndetect_patterns_in_documents(documents: Dict[str, str], patterns: Optional[List[str]] = None) -> Dict[str, Dict[str, List[int]]]\n```\n\nDetect patterns across multiple documents.\n\n#### get_pattern_summary\n\n```python\nget_pattern_summary(pattern_results: Dict[str, List[int]]) -> Dict[str, int]\n```\n\nSummarize pattern detection results by counting occurrences.\n\n#### get_patterns_by_category\n\n```python\nget_patterns_by_category(pattern_results: Dict[str, List[int]]) -> Dict[str, Dict[str, int]]\n```\n\nGroup pattern results by category.\n\n#### get_pattern_description\n\n```python\nget_pattern_description(pattern_name: str) -> Optional[str]\n```\n\nGet the description for a pattern.\n\n#### get_pattern_category\n\n```python\nget_pattern_category(pattern_name: str) -> Optional[str]\n```\n\nGet the category for a pattern.\n\n#### list_all_patterns\n\n```python\nlist_all_patterns() -> List[str]\n```\n\nList all available pattern names.\n\n#### list_patterns_by_category\n\n```python\nlist_patterns_by_category(category: str) -> List[str]\n```\n\nList all patterns in a specific category.\n\n#### list_all_categories\n\n```python\nlist_all_categories() -> List[str]\n```\n\nList all pattern categories.\n\n#### format_pattern_report\n\n```python\nformat_pattern_report(pattern_results: Dict[str, List[int]], show_lines: bool = False) -> str\n```\n\nFormat pattern detection results as a human-readable report.\n\n#### get_corpus_pattern_statistics\n\n```python\nget_corpus_pattern_statistics(doc_patterns: Dict[str, Dict[str, List[int]]]) -> Dict[str, any]\n```\n\nCompute statistics across all documents.\n\n### Dependencies\n\n**Standard Library:**\n\n- `collections.defaultdict`\n- `re`\n- `typing.Dict`\n- `typing.List`\n- `typing.Optional`\n- ... and 2 more\n\n\n\n---\n\n*This chapter is part of [The Cortical Chronicles](../README.md), a self-documenting book generated by the Cortical Text Processor.*\n"
    },
    {
      "path": "04-evolution/bugfixes.md",
      "title": "Bug Fixes and Lessons",
      "section": "evolution",
      "tags": [
        "bugs",
        "fixes",
        "lessons-learned"
      ],
      "source_files": [
        "git log --grep=fix:"
      ],
      "excerpt": "",
      "keywords": [
        "commit",
        "date",
        "git",
        "json",
        "chats",
        "chat",
        "files",
        "changed",
        "session",
        "add"
      ],
      "full_content": "# Bug Fixes and Lessons\n\n*What broke, how we fixed it, and what we learned.*\n\n---\n\n## Overview\n\n**14 bugs** have been identified and resolved. Each fix taught us something about the system.\n\n## Bug Fix History\n\n### Archive ML session after transcript processing (T-003 16f3)\n\n**Commit:** `59072c8`  \n**Date:** 2025-12-16  \n**Files Changed:** scripts/ml_data_collector.py  \n\n### Update CSV truncation test for new defaults (input=500, output=2000)\n\n**Commit:** `ca94a01`  \n**Date:** 2025-12-16  \n\n### Fix ML data collection milestone counting and add session/action capture\n\n**Commit:** `273baef`  \n**Date:** 2025-12-16  \n**Files Changed:** .git-ml/chats/2025-12-15/chat-20251216-121720-30c3c1.json, .git-ml/chats/2025-12-16/chat-20251216-121720-01077d.json, .git-ml/chats/2025-12-16/chat-20251216-121720-306450.json, .git-ml/chats/2025-12-16/chat-20251216-121720-5ef95b.json, .git-ml/chats/2025-12-16/chat-20251216-121720-8a1e7b.json  \n*(and 6 more)*  \n\n### Address critical ML data collection and prediction issues\n\n**Commit:** `fead1c1`  \n**Date:** 2025-12-16  \n**Files Changed:** .git-ml/chats/2025-12-15/chat-20251216-115057-b5bb48.json, .git-ml/chats/2025-12-16/chat-20251216-115057-3617f9.json, .git-ml/chats/2025-12-16/chat-20251216-115057-9502fd.json, .git-ml/chats/2025-12-16/chat-20251216-115057-cbbe64.json, .git-ml/chats/2025-12-16/chat-20251216-115057-f65b7a.json  \n*(and 4 more)*  \n\n### Add missing imports in validate command\n\n**Commit:** `172ad8f`  \n**Date:** 2025-12-16  \n**Files Changed:** scripts/ml_data_collector.py  \n\n### Clean up gitignore pattern for .git-ml/commits/\n\n**Commit:** `a65d54f`  \n**Date:** 2025-12-16  \n**Files Changed:** .gitignore  \n\n### Prevent infinite commit loop in ML data collection hooks\n\n**Commit:** `66ad656`  \n**Date:** 2025-12-16  \n**Files Changed:** .git-ml/chats/2025-12-16/chat-20251216-004054-78b531.json, .git-ml/tracked/commits.jsonl, scripts/ml_data_collector.py  \n\n### Correct hook format in settings.local.json\n\n**Commit:** `19ac02a`  \n**Date:** 2025-12-16  \n**Files Changed:** .claude/settings.local.json  \n\n### Use filename-based sorting for deterministic session ordering\n\n**Commit:** `61d502d`  \n**Date:** 2025-12-15  \n\n### Increase ID suffix length to prevent collisions\n\n**Commit:** `8ac4b6b`  \n**Date:** 2025-12-15  \n\n### Add import guards for optional test dependencies\n\n**Commit:** `91ffb04`  \n**Date:** 2025-12-15  \n\n### Make session file sorting stable for deterministic ordering\n\n**Commit:** `7433b36`  \n**Date:** 2025-12-15  \n\n### Cap query expansion weights to prevent term domination\n\n**Commit:** `fecd6dc`  \n**Date:** 2025-12-15  \n\n### Add YAML frontmatter to slash commands for discovery\n\n**Commit:** `5b52da2`  \n**Date:** 2025-12-15  \n\n"
    },
    {
      "path": "04-evolution/features.md",
      "title": "Feature Evolution",
      "section": "evolution",
      "tags": [
        "features",
        "capabilities",
        "growth"
      ],
      "source_files": [
        "git log --grep=feat:"
      ],
      "excerpt": "",
      "keywords": [
        "commit",
        "date",
        "add",
        "files",
        "modified",
        "capabilities",
        "wave",
        "comprehensive",
        "test",
        "coverage"
      ],
      "full_content": "# Feature Evolution\n\n*How the Cortical Text Processor gained its capabilities.*\n\n---\n\n## Overview\n\nThe system has evolved through **22 feature additions**. Below is the narrative of how each capability came to be.\n\n## Documentation Capabilities\n\n### Add CI workflow and documentation (Wave 4)\n\n**Commit:** `940fdf2`  \n**Date:** 2025-12-16  \n**Files Modified:** 5  \n\n### Add animated GIF visualizations to README\n\n**Commit:** `b4d7c82`  \n**Date:** 2025-12-15  \n\n## Search Capabilities\n\n### Add search integration and web interface (Wave 3)\n\n**Commit:** `0022466`  \n**Date:** 2025-12-16  \n**Files Modified:** 11  \n\n### Add security concept group and TF-IDF weighted query expansion\n\n**Commit:** `af3a7e0`  \n**Date:** 2025-12-15  \n\n### Add comprehensive test coverage for query and analysis modules\n\n**Commit:** `70a4b1b`  \n**Date:** 2025-12-15  \n\n## Other Capabilities\n\n### Add content generators for Cortical Chronicles (Wave 2)\n\n**Commit:** `3022110`  \n**Date:** 2025-12-16  \n**Files Modified:** 23  \n\n### Add Cortical Chronicles book infrastructure (Wave 1)\n\n**Commit:** `c730057`  \n**Date:** 2025-12-16  \n**Files Modified:** 13  \n\n### Batch task distribution implementation via Director orchestration\n\n**Commit:** `4f915c3`  \n**Date:** 2025-12-16  \n**Files Modified:** 8  \n\n### Add orchestration extraction for director sub-agent tracking\n\n**Commit:** `4eaeb37`  \n**Date:** 2025-12-15  \n\n### Add stunning animated ASCII codebase visualizer\n\n**Commit:** `e085a0b`  \n**Date:** 2025-12-15  \n\n### Add ASCII art codebase visualization script\n\n**Commit:** `43aae33`  \n**Date:** 2025-12-15  \n\n### Complete legacy task system migration\n\n**Commit:** `33dc8b2`  \n**Date:** 2025-12-15  \n\n### Add director orchestration execution tracking system\n\n**Commit:** `4976c58`  \n**Date:** 2025-12-15  \n\n## Ml Capabilities\n\n### Add file existence filter to ML predictions\n\n**Commit:** `3cab2ba`  \n**Date:** 2025-12-16  \n**Files Modified:** 1  \n\n### Add ML file prediction model\n\n**Commit:** `ac549dd`  \n**Date:** 2025-12-16  \n**Files Modified:** 2  \n\n### Add chunked storage for git-friendly ML data\n\n**Commit:** `0754540`  \n**Date:** 2025-12-16  \n**Files Modified:** 4  \n\n### Add ML stats report to CI pipeline\n\n**Commit:** `3e05a70`  \n**Date:** 2025-12-16  \n**Files Modified:** 9  \n\n## Data Capabilities\n\n### Add git-tracked JSONL storage for orchestration data\n\n**Commit:** `fb30e38`  \n**Date:** 2025-12-15  \n\n### Add lightweight commit data for ephemeral environments\n\n**Commit:** `89d6aa5`  \n**Date:** 2025-12-15  \n**Files Modified:** 475  \n\n## Testing Capabilities\n\n### Add comprehensive test coverage for Wave 4 modules (FINAL)\n\n**Commit:** `73d6da8`  \n**Date:** 2025-12-15  \n\n### Add comprehensive test coverage for Wave 3 modules\n\n**Commit:** `036f830`  \n**Date:** 2025-12-15  \n\n### Add comprehensive test coverage for Wave 2 modules\n\n**Commit:** `5a6bb26`  \n**Date:** 2025-12-15  \n\n"
    },
    {
      "path": "04-evolution/refactors.md",
      "title": "Refactorings and Architecture Evolution",
      "section": "evolution",
      "tags": [
        "refactoring",
        "architecture",
        "design"
      ],
      "source_files": [
        "git log --grep=refactor:"
      ],
      "excerpt": "",
      "keywords": [
        "files",
        "commit",
        "date",
        "refactorings",
        "codebase",
        "improved",
        "changes",
        "lines",
        "scope",
        "affected"
      ],
      "full_content": "# Refactorings and Architecture Evolution\n\n*How the codebase structure improved over time.*\n\n---\n\n## Overview\n\nThe codebase has undergone **3 refactorings**. Each improved code quality, maintainability, or performance.\n\n## Refactoring History\n\n### Remove unused protobuf serialization (T-013 f0ff)\n\n**Commit:** `d7a98ae`  \n**Date:** 2025-12-16  \n**Changes:** +100/-1460 lines  \n**Scope:** 6 files affected  \n\n### Split large files exceeding 25000 token limit\n\n**Commit:** `21ec5ea`  \n**Date:** 2025-12-15  \n\n### Consolidate ML data to single JSONL files\n\n**Commit:** `205fe34`  \n**Date:** 2025-12-15  \n**Changes:** +658/-12208 lines  \n**Scope:** 486 files affected  \n\n"
    },
    {
      "path": "04-evolution/test_timeline.md",
      "title": "Test",
      "section": "evolution",
      "tags": [],
      "source_files": [],
      "excerpt": "",
      "keywords": [
        "add",
        "timeline",
        "december",
        "week",
        "dec",
        "feat",
        "book",
        "docs",
        "vision"
      ],
      "full_content": "# Timeline\n\n---\n\n## December 2025\n\n### Week of Dec 15\n\n- **2025-12-16**: feat: Add book\n- **2025-12-16**: docs: Add vision\n\n"
    },
    {
      "path": "04-evolution/timeline.md",
      "title": "Project Timeline",
      "section": "evolution",
      "tags": [
        "timeline",
        "chronology",
        "evolution"
      ],
      "source_files": [
        "git log"
      ],
      "excerpt": "",
      "keywords": [
        "add",
        "feat",
        "wave",
        "cortical",
        "chronicles",
        "docs",
        "vision",
        "fix",
        "project",
        "timeline"
      ],
      "full_content": "# Project Timeline\n\n*A chronological journey through the Cortical Text Processor's development.*\n\n---\n\n## December 2025\n\n### Week of Dec 15\n\n- **2025-12-16**: feat: Add CI workflow and documentation (Wave 4)\n- **2025-12-16**: feat: Add search integration and web interface (Wave 3)\n- **2025-12-16**: feat: Add content generators for Cortical Chronicles (Wave 2)\n- **2025-12-16**: feat: Add Cortical Chronicles book infrastructure (Wave 1)\n- **2025-12-16**: docs: Add deep algorithm analysis and author reflections to VISION.md\n- **2025-12-16**: docs: Add product vision document for legacy feature roadmap\n- **2025-12-16**: refactor: Remove unused protobuf serialization (T-013 f0ff)\n- **2025-12-16**: fix: Archive ML session after transcript processing (T-003 16f3)\n- **2025-12-16**: fix: Update CSV truncation test for new defaults (input=500, output=2000)\n- **2025-12-16**: feat: Batch task distribution implementation via Director orchestration\n\n"
    },
    {
      "path": "docs/CONTRIBUTING.md",
      "title": "CONTRIBUTING",
      "section": "docs",
      "tags": [],
      "source_files": [],
      "excerpt": "> **Welcome!** This guide shows you how to add new generators and extend \"The Cortical Chronicles.\"",
      "keywords": [
        "self",
        "python",
        "content",
        "generate",
        "data",
        "str",
        "def",
        "generator",
        "errors",
        "return"
      ],
      "full_content": "# Contributing to The Cortical Chronicles\n\n> **Welcome!** This guide shows you how to add new generators and extend \"The Cortical Chronicles.\"\n\n---\n\n## Table of Contents\n\n- [Quick Start](#quick-start)\n- [Generator Architecture](#generator-architecture)\n- [Step-by-Step: Adding a Generator](#step-by-step-adding-a-generator)\n- [Best Practices](#best-practices)\n- [Testing Your Generator](#testing-your-generator)\n- [CI Integration](#ci-integration)\n- [Examples](#examples)\n\n---\n\n## Quick Start\n\n### Prerequisites\n\n```bash\n# Ensure you have dependencies\npip install -e \".[dev]\"\n\n# Verify PyYAML is installed\npython -c \"import yaml; print('PyYAML OK')\"\n\n# Test existing generators\npython scripts/generate_book.py --list\npython scripts/generate_book.py --dry-run\n```\n\n### Template for New Generator\n\n```python\nfrom scripts.generate_book import ChapterGenerator\nfrom pathlib import Path\nfrom typing import Dict, Any, List\n\nclass MyChapterGenerator(ChapterGenerator):\n    \"\"\"Generate chapters from <your data source>.\"\"\"\n\n    @property\n    def name(self) -> str:\n        \"\"\"Generator name for CLI and logging.\"\"\"\n        return \"mychapter\"\n\n    @property\n    def output_dir(self) -> str:\n        \"\"\"Subdirectory in book/ for output.\"\"\"\n        return \"03-decisions\"  # Choose: 00-05 based on content type\n\n    def generate(self, dry_run: bool = False, verbose: bool = False) -> Dict[str, Any]:\n        \"\"\"\n        Generate chapter content.\n\n        Args:\n            dry_run: If True, don't write files (just log)\n            verbose: If True, print detailed progress\n\n        Returns:\n            Dict with:\n                - files: List of generated file paths\n                - stats: Generation statistics\n                - errors: Any errors encountered\n        \"\"\"\n        errors = []\n        stats = {\n            \"items_processed\": 0,\n            \"chapters_written\": 0\n        }\n\n        if verbose:\n            print(\"  Processing data...\")\n\n        # 1. Load your data source\n        try:\n            data = self._load_data()\n        except Exception as e:\n            errors.append(f\"Failed to load data: {e}\")\n            return {\"files\": [], \"stats\": stats, \"errors\": errors}\n\n        # 2. Process each item\n        for item in data:\n            content = self._generate_chapter_content(item)\n            filename = self._generate_filename(item)\n\n            if verbose:\n                print(f\"  Generating: {filename}\")\n\n            self.write_chapter(filename, content, dry_run=dry_run)\n            stats[\"chapters_written\"] += 1\n\n        stats[\"items_processed\"] = len(data)\n\n        return {\n            \"files\": [str(f) for f in self.generated_files],\n            \"stats\": stats,\n            \"errors\": errors\n        }\n\n    def _load_data(self) -> List[Any]:\n        \"\"\"Load your data source.\"\"\"\n        # Implement data loading\n        pass\n\n    def _generate_chapter_content(self, item: Any) -> str:\n        \"\"\"Generate markdown content for one chapter.\"\"\"\n        # Generate frontmatter\n        frontmatter = self.generate_frontmatter(\n            title=item['title'],\n            tags=['tag1', 'tag2'],\n            source_files=['source.py']\n        )\n\n        # Build content\n        content = frontmatter\n        content += f\"# {item['title']}\\n\\n\"\n        content += item['body']\n        content += \"\\n\\n---\\n\\n\"\n        content += \"*This chapter is part of [The Cortical Chronicles](../README.md).*\\n\"\n\n        return content\n\n    def _generate_filename(self, item: Any) -> str:\n        \"\"\"Generate filename from item.\"\"\"\n        # Slugify the title\n        slug = item['title'].lower().replace(' ', '-')\n        return f\"{slug}.md\"\n```\n\n---\n\n## Generator Architecture\n\n### Base Class: `ChapterGenerator`\n\nAll generators inherit from `ChapterGenerator` (defined in `scripts/generate_book.py`).\n\n**Required Methods:**\n\n| Method | Returns | Purpose |\n|--------|---------|---------|\n| `name` (property) | `str` | Generator identifier for CLI |\n| `output_dir` (property) | `str` | Subdirectory in book/ |\n| `generate(dry_run, verbose)` | `Dict[str, Any]` | Main generation logic |\n\n**Provided Helper Methods:**\n\n| Method | Purpose |\n|--------|---------|\n| `write_chapter(filename, content, dry_run)` | Write chapter with standard handling |\n| `generate_frontmatter(title, tags, sources)` | Generate YAML frontmatter |\n\n**Instance Variables:**\n\n- `self.book_dir`: Path to book/ directory\n- `self.generated_files`: List of written file paths (auto-tracked by `write_chapter`)\n\n### Output Directories\n\nChoose the appropriate section for your content:\n\n| Directory | Purpose | Example Generators |\n|-----------|---------|-------------------|\n| `00-preface` | Book introduction | (manual) |\n| `01-foundations` | Algorithm theory | AlgorithmChapterGenerator |\n| `02-architecture` | Module documentation | ModuleDocGenerator |\n| `03-decisions` | ADRs, design decisions | DecisionRecordGenerator |\n| `04-evolution` | Commit narratives | CommitNarrativeGenerator |\n| `05-future` | Roadmap, vision | (placeholder) |\n\n### Return Value Format\n\n```python\n{\n    \"files\": [\n        \"book/01-foundations/alg-pagerank.md\",\n        \"book/01-foundations/alg-bm25.md\"\n    ],\n    \"stats\": {\n        \"algorithms_found\": 6,\n        \"chapters_written\": 6,\n        \"custom_metric\": 42\n    },\n    \"errors\": [\n        \"Warning: Optional data not found\"\n    ]\n}\n```\n\n---\n\n## Step-by-Step: Adding a Generator\n\n### Step 1: Create the Generator Class\n\nAdd to `scripts/generate_book.py` or create a new file:\n\n```python\nclass MyChapterGenerator(ChapterGenerator):\n    \"\"\"One-line description.\"\"\"\n\n    @property\n    def name(self) -> str:\n        return \"mychapter\"\n\n    @property\n    def output_dir(self) -> str:\n        return \"03-decisions\"  # Choose appropriate section\n\n    def generate(self, dry_run: bool = False, verbose: bool = False) -> Dict[str, Any]:\n        # Implementation\n        pass\n```\n\n### Step 2: Register in BookBuilder\n\nIn `scripts/generate_book.py`, find the `main()` function and add:\n\n```python\ndef main():\n    # ...existing code...\n\n    # Register real generators\n    builder.register_generator(AlgorithmChapterGenerator(book_dir=args.output))\n    builder.register_generator(ModuleDocGenerator(book_dir=args.output))\n    builder.register_generator(CommitNarrativeGenerator(book_dir=args.output))\n    builder.register_generator(MyChapterGenerator(book_dir=args.output))  # <-- ADD THIS\n    builder.register_generator(SearchIndexGenerator(book_dir=args.output))\n\n    # ...rest of main()...\n```\n\n**Important:** Register `SearchIndexGenerator` LAST so it indexes all other chapters.\n\n### Step 3: Test the Generator\n\n```bash\n# List all generators (verify yours appears)\npython scripts/generate_book.py --list\n\n# Test with dry-run\npython scripts/generate_book.py --chapter mychapter --dry-run --verbose\n\n# Generate for real\npython scripts/generate_book.py --chapter mychapter --verbose\n\n# Verify output\nls -la book/03-decisions/\n```\n\n### Step 4: Regenerate Search Index\n\nAfter adding new chapters:\n\n```bash\npython scripts/generate_book.py --chapter search\n```\n\n### Step 5: Test Full Build\n\n```bash\n# Full regeneration\npython scripts/generate_book.py --verbose\n\n# Verify all chapters\nfind book/ -name \"*.md\" | sort\n\n# Validate JSON outputs\npython -c \"import json; json.load(open('book/index.json'))\"\npython -c \"import json; json.load(open('book/search.json'))\"\n```\n\n---\n\n## Best Practices\n\n### 1. Output File Naming\n\n**Convention:**\n\n- Algorithm docs: `alg-<name>.md` (e.g., `alg-pagerank.md`)\n- Module docs: `mod-<category>.md` (e.g., `mod-processor.md`)\n- Narrative docs: `<topic>.md` (e.g., `timeline.md`, `features.md`)\n- Decision records: `adr-<num>-<slug>.md` (e.g., `adr-001-architecture.md`)\n\n**Rules:**\n\n- Use lowercase\n- Use hyphens, not underscores\n- Be descriptive but concise\n- Avoid special characters\n\n### 2. Frontmatter Requirements\n\nAll chapters must have YAML frontmatter:\n\n```yaml\n---\ntitle: \"Chapter Title\"\ngenerated: \"2025-12-16T10:30:00Z\"\ngenerator: \"mychapter\"\nsource_files:\n  - \"path/to/source.py\"\n  - \"path/to/data.json\"\ntags:\n  - tag1\n  - tag2\n  - tag3\n---\n```\n\n**Required Fields:**\n\n- `title`: Human-readable chapter title\n- `generated`: ISO 8601 timestamp (use `datetime.utcnow().isoformat() + \"Z\"`)\n- `generator`: Your generator's `name` property\n- `source_files`: List of source files used\n- `tags`: List of tags for categorization\n\n**Use the helper:**\n\n```python\nfrontmatter = self.generate_frontmatter(\n    title=\"My Chapter\",\n    tags=['algorithms', 'foundations'],\n    source_files=['docs/VISION.md', 'cortical/analysis.py']\n)\n```\n\n### 3. Cross-Reference Patterns\n\n**Link to other chapters:**\n\n```markdown\nSee [Algorithm Documentation](alg-pagerank.md) for details.\nSee [Architecture Overview](../02-architecture/index.md).\n```\n\n**Link to source code:**\n\n```markdown\n**Implementation:** `cortical/analysis.py:compute_pagerank()`\n```\n\n**Link to ADRs:**\n\n```markdown\n**Related Decision:** [ADR-001: Architecture](../../samples/decisions/adr-001-*.md)\n```\n\n**Link to git commits:**\n\n```markdown\n**Commit:** `a1b2c3d`\n```\n\n### 4. Error Handling\n\n**Always handle exceptions gracefully:**\n\n```python\ndef generate(self, dry_run: bool = False, verbose: bool = False) -> Dict[str, Any]:\n    errors = []\n    stats = {}\n\n    try:\n        data = self._load_data()\n    except FileNotFoundError as e:\n        errors.append(f\"Source file not found: {e}\")\n        return {\"files\": [], \"stats\": stats, \"errors\": errors}\n    except Exception as e:\n        errors.append(f\"Unexpected error loading data: {e}\")\n        return {\"files\": [], \"stats\": stats, \"errors\": errors}\n\n    # Continue processing...\n```\n\n**Error message guidelines:**\n\n- Be specific (include file names, line numbers)\n- Suggest fixes when possible\n- Use warnings for non-critical issues\n- Return partial results if some items succeed\n\n**Example:**\n\n```python\n# Good\nerrors.append(\"Failed to parse VISION.md line 42: Missing '###' prefix\")\n\n# Bad\nerrors.append(\"Error in file\")\n```\n\n### 5. Verbose Output\n\nProvide helpful progress indicators:\n\n```python\ndef generate(self, dry_run: bool = False, verbose: bool = False) -> Dict[str, Any]:\n    if verbose:\n        print(\"  Loading source data...\")\n\n    data = self._load_data()\n\n    if verbose:\n        print(f\"  Found {len(data)} items to process\")\n\n    for item in data:\n        if verbose:\n            print(f\"  Generating: {item['title']}\")\n\n        # Process item...\n```\n\n**Guidelines:**\n\n- Use 2-space indentation for generator output\n- Log counts and progress\n- Don't log every detail (avoid spam)\n- Use dry-run mode for testing without writes\n\n### 6. Dry-Run Support\n\nAlways respect the `dry_run` parameter:\n\n```python\n# Good: Use write_chapter helper (handles dry_run automatically)\nself.write_chapter(filename, content, dry_run=dry_run)\n\n# If you need custom file writing:\nif dry_run:\n    print(f\"  Would write: {output_path}\")\nelse:\n    output_path.write_text(content)\n    self.generated_files.append(output_path)\n```\n\n### 7. Statistics Reporting\n\nReturn meaningful statistics:\n\n```python\nstats = {\n    \"items_found\": len(all_items),\n    \"items_processed\": len(processed_items),\n    \"chapters_written\": len(self.generated_files),\n    \"items_skipped\": len(skipped_items),\n    \"warnings\": len(warnings)\n}\n```\n\n---\n\n## Testing Your Generator\n\n### Unit Testing\n\nCreate a test file `tests/test_book_generation.py`:\n\n```python\nimport unittest\nfrom pathlib import Path\nfrom scripts.generate_book import MyChapterGenerator\n\nclass TestMyChapterGenerator(unittest.TestCase):\n    def setUp(self):\n        self.generator = MyChapterGenerator()\n\n    def test_name(self):\n        self.assertEqual(self.generator.name, \"mychapter\")\n\n    def test_output_dir(self):\n        self.assertEqual(self.generator.output_dir, \"03-decisions\")\n\n    def test_dry_run(self):\n        \"\"\"Test that dry-run doesn't write files.\"\"\"\n        result = self.generator.generate(dry_run=True, verbose=False)\n        self.assertEqual(result['files'], [])\n        self.assertGreaterEqual(result['stats']['items_processed'], 0)\n\n    def test_generate(self):\n        \"\"\"Test actual generation.\"\"\"\n        result = self.generator.generate(dry_run=False, verbose=False)\n        self.assertGreater(len(result['files']), 0)\n        self.assertEqual(result['errors'], [])\n```\n\n### Integration Testing\n\n```bash\n# 1. Full dry-run test\npython scripts/generate_book.py --dry-run --verbose\n\n# 2. Generate to temporary directory\npython scripts/generate_book.py --output /tmp/test-book --chapter mychapter\n\n# 3. Verify outputs\nls -la /tmp/test-book/03-decisions/\ncat /tmp/test-book/03-decisions/example.md\n\n# 4. Validate frontmatter\npython -c \"\nimport yaml\nfrom pathlib import Path\nfor f in Path('/tmp/test-book').glob('**/*.md'):\n    content = f.read_text()\n    if content.startswith('---'):\n        fm = content.split('---', 2)[1]\n        yaml.safe_load(fm)\n        print(f'{f.name}: OK')\n\"\n\n# 5. Clean up\nrm -rf /tmp/test-book\n```\n\n### Common Test Cases\n\n1. **Empty data source** - Should return gracefully\n2. **Malformed data** - Should log errors, continue processing\n3. **Missing dependencies** - Should fail gracefully with clear error\n4. **Dry-run mode** - Should not write any files\n5. **Verbose mode** - Should print progress\n6. **Frontmatter validation** - YAML should parse correctly\n7. **Cross-references** - Links should be valid\n\n---\n\n## CI Integration\n\n### GitHub Actions Workflow\n\nAdd to `.github/workflows/book-generation.yml`:\n\n```yaml\nname: Generate Book\n\non:\n  push:\n    branches: [main]\n    paths:\n      - 'docs/VISION.md'\n      - 'cortical/**/*.py'\n      - 'scripts/generate_book.py'\n  workflow_dispatch:\n\njobs:\n  generate:\n    runs-on: ubuntu-latest\n\n    steps:\n      - uses: actions/checkout@v3\n        with:\n          fetch-depth: 0  # Full history for commit narratives\n\n      - uses: actions/setup-python@v4\n        with:\n          python-version: '3.10'\n\n      - name: Install dependencies\n        run: |\n          pip install -e \".[dev]\"\n\n      - name: Generate book\n        run: |\n          python scripts/generate_book.py --verbose\n\n      - name: Validate outputs\n        run: |\n          python -c \"import json; json.load(open('book/index.json'))\"\n          python -c \"import json; json.load(open('book/search.json'))\"\n\n      - name: Upload artifact\n        uses: actions/upload-artifact@v3\n        with:\n          name: cortical-chronicles\n          path: book/\n```\n\n### Pre-Commit Hook\n\nAdd to `.git/hooks/pre-commit`:\n\n```bash\n#!/bin/bash\n# Regenerate book if source files changed\n\nSOURCES=\"docs/VISION.md cortical/ scripts/generate_book.py\"\nCHANGED=$(git diff --cached --name-only $SOURCES)\n\nif [ -n \"$CHANGED\" ]; then\n    echo \"\ud83d\udcda Regenerating book chapters...\"\n    python scripts/generate_book.py --verbose || exit 1\n\n    # Stage generated files\n    git add book/\nfi\n```\n\n---\n\n## Examples\n\n### Example 1: Simple Data-Driven Generator\n\nGenerate chapters from JSON files:\n\n```python\nclass DataChapterGenerator(ChapterGenerator):\n    \"\"\"Generate chapters from data/*.json files.\"\"\"\n\n    @property\n    def name(self) -> str:\n        return \"data\"\n\n    @property\n    def output_dir(self) -> str:\n        return \"05-future\"\n\n    def generate(self, dry_run: bool = False, verbose: bool = False) -> Dict[str, Any]:\n        import json\n\n        data_dir = Path(__file__).parent.parent / \"data\"\n        json_files = list(data_dir.glob(\"*.json\"))\n\n        if verbose:\n            print(f\"  Found {len(json_files)} JSON files\")\n\n        for json_file in json_files:\n            data = json.loads(json_file.read_text())\n\n            content = self.generate_frontmatter(\n                title=data['title'],\n                tags=data.get('tags', []),\n                source_files=[str(json_file)]\n            )\n            content += f\"# {data['title']}\\n\\n\"\n            content += data['body'] + \"\\n\"\n\n            filename = json_file.stem + \".md\"\n            self.write_chapter(filename, content, dry_run)\n\n        return {\n            \"files\": [str(f) for f in self.generated_files],\n            \"stats\": {\"files_processed\": len(json_files)},\n            \"errors\": []\n        }\n```\n\n### Example 2: Git History Generator\n\nGenerate from commit messages:\n\n```python\nclass GitHistoryGenerator(ChapterGenerator):\n    \"\"\"Generate timeline from git history.\"\"\"\n\n    @property\n    def name(self) -> str:\n        return \"timeline\"\n\n    @property\n    def output_dir(self) -> str:\n        return \"04-evolution\"\n\n    def _run_git(self, *args) -> str:\n        import subprocess\n        result = subprocess.run(\n            [\"git\", *args],\n            capture_output=True,\n            text=True,\n            check=True\n        )\n        return result.stdout.strip()\n\n    def generate(self, dry_run: bool = False, verbose: bool = False) -> Dict[str, Any]:\n        try:\n            log_output = self._run_git(\"log\", \"--format=%H|%aI|%s|%an\", \"-100\")\n        except Exception as e:\n            return {\n                \"files\": [],\n                \"stats\": {},\n                \"errors\": [f\"Git error: {e}\"]\n            }\n\n        commits = []\n        for line in log_output.split('\\n'):\n            if not line.strip():\n                continue\n            hash_val, timestamp, message, author = line.split('|', 3)\n            commits.append({\n                'hash': hash_val[:7],\n                'date': timestamp[:10],\n                'message': message,\n                'author': author\n            })\n\n        # Generate timeline content\n        content = self.generate_frontmatter(\n            title=\"Project Timeline\",\n            tags=['timeline', 'history'],\n            source_files=['git log']\n        )\n        content += \"# Project Timeline\\n\\n\"\n\n        for commit in commits:\n            content += f\"- **{commit['date']}** (`{commit['hash']}`): {commit['message']}\\n\"\n\n        self.write_chapter(\"timeline.md\", content, dry_run)\n\n        return {\n            \"files\": [str(f) for f in self.generated_files],\n            \"stats\": {\"commits_processed\": len(commits)},\n            \"errors\": []\n        }\n```\n\n### Example 3: Multi-File Generator\n\nGenerate multiple chapters from one data source:\n\n```python\nclass MultiChapterGenerator(ChapterGenerator):\n    \"\"\"Generate multiple chapters from single source.\"\"\"\n\n    @property\n    def name(self) -> str:\n        return \"concepts\"\n\n    @property\n    def output_dir(self) -> str:\n        return \"02-architecture\"\n\n    def generate(self, dry_run: bool = False, verbose: bool = False) -> Dict[str, Any]:\n        # Load Louvain clusters\n        clusters = self._load_clusters()\n\n        if verbose:\n            print(f\"  Found {len(clusters)} concept clusters\")\n\n        # Generate one chapter per cluster\n        for cluster_id, terms in clusters.items():\n            content = self.generate_frontmatter(\n                title=f\"Concept Cluster {cluster_id}\",\n                tags=['concepts', 'clustering', 'louvain'],\n                source_files=['corpus_dev.pkl']\n            )\n\n            content += f\"# Concept Cluster {cluster_id}\\n\\n\"\n            content += f\"**Terms:** {', '.join(terms[:20])}\\n\\n\"\n\n            filename = f\"concept-{cluster_id}.md\"\n            self.write_chapter(filename, content, dry_run)\n\n        # Generate index\n        index_content = self._generate_index(clusters)\n        self.write_chapter(\"concepts-index.md\", index_content, dry_run)\n\n        return {\n            \"files\": [str(f) for f in self.generated_files],\n            \"stats\": {\n                \"clusters_found\": len(clusters),\n                \"chapters_written\": len(clusters) + 1\n            },\n            \"errors\": []\n        }\n\n    def _load_clusters(self) -> Dict[int, List[str]]:\n        # Load from processor\n        pass\n\n    def _generate_index(self, clusters: Dict) -> str:\n        # Generate index page\n        pass\n```\n\n---\n\n## Common Pitfalls\n\n### \u274c Don't: Hardcode Paths\n\n```python\n# Bad\noutput_path = Path(\"/home/user/book/01-foundations/chapter.md\")\n\n# Good\noutput_path = self.book_dir / self.output_dir / \"chapter.md\"\n```\n\n### \u274c Don't: Ignore Dry-Run\n\n```python\n# Bad\nwith open(output_path, 'w') as f:\n    f.write(content)\n\n# Good\nself.write_chapter(filename, content, dry_run=dry_run)\n```\n\n### \u274c Don't: Swallow Errors\n\n```python\n# Bad\ntry:\n    data = self._load_data()\nexcept:\n    pass  # Silent failure!\n\n# Good\ntry:\n    data = self._load_data()\nexcept Exception as e:\n    errors.append(f\"Failed to load data: {e}\")\n    return {\"files\": [], \"stats\": {}, \"errors\": errors}\n```\n\n### \u274c Don't: Generate Unsafe Filenames\n\n```python\n# Bad\nfilename = f\"{user_input}.md\"  # Could be \"../../../etc/passwd.md\"\n\n# Good\nfilename = self._sanitize_filename(user_input) + \".md\"\n\ndef _sanitize_filename(self, text: str) -> str:\n    # Remove path separators\n    text = text.replace('/', '-').replace('\\\\', '-')\n    # Remove special chars\n    text = re.sub(r'[^\\w\\s-]', '', text)\n    # Normalize whitespace\n    text = re.sub(r'[-\\s]+', '-', text)\n    return text.lower().strip('-')\n```\n\n---\n\n## Next Steps\n\n1. **Study existing generators** in `scripts/generate_book.py`\n2. **Create your generator** following the template\n3. **Test with dry-run** before writing files\n4. **Validate frontmatter** with YAML parser\n5. **Regenerate search index** after adding chapters\n6. **Document your generator** in this guide\n\n---\n\n## Questions?\n\n- Check [TROUBLESHOOTING.md](TROUBLESHOOTING.md) for common issues\n- Review existing generators for patterns\n- Test with `--dry-run --verbose` for debugging\n\n---\n\n*This guide is part of [The Cortical Chronicles](../README.md) documentation.*\n"
    },
    {
      "path": "docs/TROUBLESHOOTING.md",
      "title": "TROUBLESHOOTING",
      "section": "docs",
      "tags": [],
      "source_files": [],
      "excerpt": "> **Quick Recovery**: For most errors, run `python scripts/generate_book.py --dry-run --verbose` to diagnose without writing files. This guide helps diagnose and fix common issues when generating...",
      "keywords": [
        "git",
        "book",
        "python",
        "check",
        "json",
        "chapter",
        "scripts",
        "bash",
        "generate_book",
        "file"
      ],
      "full_content": "# Troubleshooting Guide\n\n> **Quick Recovery**: For most errors, run `python scripts/generate_book.py --dry-run --verbose` to diagnose without writing files.\n\nThis guide helps diagnose and fix common issues when generating \"The Cortical Chronicles.\"\n\n---\n\n## Table of Contents\n\n- [Common Errors](#common-errors)\n- [Diagnostic Commands](#diagnostic-commands)\n- [Recovery Procedures](#recovery-procedures)\n- [Error Reference](#error-reference)\n- [Getting Help](#getting-help)\n\n---\n\n## Common Errors\n\n### 1. YAML Parsing Errors in .ai_meta Files\n\n**Symptoms:**\n```\nWarning: Failed to parse analysis.py.ai_meta: ...\nYAML parsing error\n```\n\n**Causes:**\n- Malformed YAML frontmatter\n- Unescaped special characters in docstrings\n- Missing comment header stripping\n- Mixed tabs and spaces\n\n**Solutions:**\n\n```bash\n# Check if .ai_meta files exist\nls -la cortical/*.ai_meta\n\n# Regenerate metadata files\npython scripts/generate_ai_metadata.py --force\n\n# Test parsing a specific file\npython -c \"import yaml; yaml.safe_load(open('cortical/analysis.py.ai_meta').read())\"\n```\n\n**Prevention:**\n- Run `generate_ai_metadata.py` after major docstring changes\n- Avoid special YAML characters (`:`, `{`, `}`, `[`, `]`) in docstrings without quoting\n\n### 2. Missing VISION.md Sections\n\n**Symptoms:**\n```\nalgorithms_found: 0\nNo algorithm sections extracted\n```\n\n**Causes:**\n- VISION.md missing \"Deep Algorithm Analysis\" section\n- Section header format changed\n- Regex pattern mismatch\n\n**Solutions:**\n\n```bash\n# Verify VISION.md structure\ngrep -n \"## Deep Algorithm Analysis\" docs/VISION.md\n\n# Check for algorithm sections\ngrep -n \"### Algorithm\" docs/VISION.md\n\n# Regenerate with verbose logging\npython scripts/generate_book.py --chapter foundations --verbose\n```\n\n**Expected Structure:**\n```markdown\n## Deep Algorithm Analysis\n\n### Algorithm 1: PageRank \u2014 Importance Discovery\n\n**Implementation:** `cortical/analysis.py:compute_pagerank()`\n...\n\n### Algorithm 2: BM25/TF-IDF \u2014 Distinctiveness Scoring\n...\n```\n\n### 3. Git History Access Issues\n\n**Symptoms:**\n```\nWarning: Failed to read git history: ...\nNo git history found\nCould not read git history\n```\n\n**Causes:**\n- Not in a git repository\n- Insufficient permissions\n- Git not installed\n- Detached HEAD state\n\n**Solutions:**\n\n```bash\n# Check git availability\nwhich git\ngit --version\n\n# Verify repository\ngit status\n\n# Test git log access\ngit log -5 --format=\"%H|%aI|%s|%an\"\n\n# Check permissions\nls -la .git/\n```\n\n**Workarounds:**\n- Skip evolution chapters: `python scripts/generate_book.py --chapter foundations`\n- Initialize git if missing: `git init && git add . && git commit -m \"Initial commit\"`\n\n### 4. Missing Dependencies\n\n**Symptoms:**\n```\nModuleNotFoundError: No module named 'yaml'\nImportError: cannot import name 'yaml'\n```\n\n**Solution:**\n\n```bash\n# Install required dependencies\npip install pyyaml\n\n# Or install all dev dependencies\npip install -e \".[dev]\"\n\n# Verify installation\npython -c \"import yaml; print('PyYAML OK')\"\n```\n\n### 5. Permission Errors Writing to book/\n\n**Symptoms:**\n```\nPermissionError: [Errno 13] Permission denied: 'book/01-foundations/...'\nOSError: [Errno 30] Read-only file system\n```\n\n**Solutions:**\n\n```bash\n# Check directory permissions\nls -ld book/\nls -la book/01-foundations/\n\n# Fix permissions\nchmod -R u+w book/\n\n# Try dry-run first\npython scripts/generate_book.py --dry-run\n\n# Check disk space\ndf -h .\n```\n\n### 6. Empty or Missing ML Data\n\n**Symptoms:**\n```\nwith_ml_data: 0\nWarning: Failed to load ML data\n```\n\n**Causes:**\n- `.git-ml/tracked/commits.jsonl` doesn't exist\n- ML collection not started\n- File is empty\n\n**Solutions:**\n\n```bash\n# Check ML data file\nls -lh .git-ml/tracked/commits.jsonl\n\n# Backfill ML data\npython scripts/ml_data_collector.py backfill -n 100\n\n# Verify data\npython scripts/ml_data_collector.py stats\n```\n\n**Note:** ML data is optional. Chapters will generate without it (just with fewer details).\n\n### 7. Search Index Generation Failures\n\n**Symptoms:**\n```\nFailed to parse index.md: ...\nJSONDecodeError\n```\n\n**Causes:**\n- Malformed frontmatter in generated chapters\n- Invalid JSON structure\n- Missing chapter files\n\n**Solutions:**\n\n```bash\n# Regenerate chapters first\npython scripts/generate_book.py --chapter foundations\npython scripts/generate_book.py --chapter architecture\n\n# Then regenerate search index\npython scripts/generate_book.py --chapter search\n\n# Validate generated JSON\npython -c \"import json; json.load(open('book/index.json'))\"\npython -c \"import json; json.load(open('book/search.json'))\"\n```\n\n---\n\n## Diagnostic Commands\n\n### Health Check\n\n```bash\n# Full diagnostic run (no writes)\npython scripts/generate_book.py --dry-run --verbose\n```\n\nExpected output:\n```\nRegistered generator: foundations\nRegistered generator: architecture\n...\nGenerating: foundations\n  Found 6 algorithms in VISION.md\n  Generating: alg-pagerank.md\n...\nTotal files: 25\n```\n\n### Check Individual Generators\n\n```bash\n# List all generators\npython scripts/generate_book.py --list\n\n# Test specific generator\npython scripts/generate_book.py --chapter foundations --dry-run --verbose\npython scripts/generate_book.py --chapter architecture --dry-run --verbose\npython scripts/generate_book.py --chapter evolution --dry-run --verbose\n```\n\n### Verify Dependencies\n\n```bash\n# Check Python version (3.8+ required)\npython --version\n\n# Check required packages\npython -c \"import yaml; print('PyYAML:', yaml.__version__)\"\npython -c \"import json; print('json: OK')\"\npython -c \"import re; print('re: OK')\"\n\n# Check git\ngit --version\ngit status\n```\n\n### Verify Source Files\n\n```bash\n# Check VISION.md\ntest -f docs/VISION.md && echo \"VISION.md exists\" || echo \"VISION.md MISSING\"\ngrep -c \"### Algorithm\" docs/VISION.md\n\n# Check .ai_meta files\nfind cortical -name \"*.ai_meta\" | wc -l\nls -lh cortical/*.ai_meta | head -5\n\n# Check git history\ngit log -5 --oneline\n```\n\n### Verify Output Structure\n\n```bash\n# Check generated files\nfind book/ -name \"*.md\" | sort\nfind book/ -name \"*.json\"\n\n# Check chapter completeness\nfor dir in book/0*-*/; do\n  echo \"$dir: $(ls \"$dir\" | wc -l) files\"\ndone\n\n# Validate JSON outputs\npython -c \"import json; json.load(open('book/index.json')); print('index.json: OK')\"\npython -c \"import json; json.load(open('book/search.json')); print('search.json: OK')\"\n```\n\n---\n\n## Recovery Procedures\n\n### Complete Rebuild\n\nWhen all else fails, regenerate from scratch:\n\n```bash\n# 1. Backup existing book (if needed)\ncp -r book/ book.backup/\n\n# 2. Clear generated files (keep docs and assets)\nrm -rf book/0*-*/\nrm -f book/index.json book/search.json\n\n# 3. Regenerate metadata (if needed)\npython scripts/generate_ai_metadata.py --force\n\n# 4. Full regeneration\npython scripts/generate_book.py --verbose\n\n# 5. Verify\nls -lR book/\npython -c \"import json; json.load(open('book/index.json'))\"\n```\n\n### Regenerate Single Chapter\n\nIf one chapter is corrupted:\n\n```bash\n# 1. Remove the chapter\nrm -rf book/01-foundations/\n\n# 2. Regenerate just that chapter\npython scripts/generate_book.py --chapter foundations --verbose\n\n# 3. Rebuild search index\npython scripts/generate_book.py --chapter search\n\n# 4. Verify\nls -la book/01-foundations/\n```\n\n### Fix Malformed Frontmatter\n\nIf chapters have malformed YAML frontmatter:\n\n```bash\n# 1. Identify the problem file\npython -c \"\nimport yaml\nfrom pathlib import Path\nfor f in Path('book').glob('**/*.md'):\n    try:\n        content = f.read_text()\n        if content.startswith('---'):\n            fm = content.split('---', 2)[1]\n            yaml.safe_load(fm)\n    except Exception as e:\n        print(f'ERROR: {f}: {e}')\n\"\n\n# 2. Remove the problematic chapter\nrm book/XX-section/problematic.md\n\n# 3. Regenerate the parent chapter\npython scripts/generate_book.py --chapter <generator-name>\n```\n\n### Restore from Git\n\nIf the book is tracked in git:\n\n```bash\n# Check what changed\ngit status book/\ngit diff book/\n\n# Restore specific file\ngit restore book/01-foundations/alg-pagerank.md\n\n# Restore entire book\ngit restore book/\n\n# Or reset to last good state\ngit log --oneline -- book/\ngit restore --source=<commit-hash> book/\n```\n\n### Partial Failure Recovery\n\nIf some generators fail but others succeed:\n\n```bash\n# 1. Check which generators failed\npython scripts/generate_book.py --verbose 2>&1 | grep -A 3 \"ERROR:\"\n\n# 2. Regenerate only failed chapters\npython scripts/generate_book.py --chapter <failed-generator> --verbose\n\n# 3. Rebuild search index\npython scripts/generate_book.py --chapter search\n```\n\n---\n\n## Error Reference\n\n### Generator-Specific Errors\n\n#### AlgorithmChapterGenerator\n\n| Error | Cause | Fix |\n|-------|-------|-----|\n| `algorithms_found: 0` | VISION.md missing section | Check docs/VISION.md structure |\n| `Source file not found` | VISION.md doesn't exist | Create docs/VISION.md |\n| `chapters_written: 0` | Regex pattern mismatch | Update `_extract_algorithms()` |\n\n#### ModuleDocGenerator\n\n| Error | Cause | Fix |\n|-------|-------|-----|\n| `No .ai_meta files found` | Metadata not generated | Run `generate_ai_metadata.py` |\n| `Failed to parse <file>.ai_meta` | Malformed YAML | Regenerate metadata with `--force` |\n| `modules_documented: 0` | All parsing failed | Check YAML structure |\n\n#### CommitNarrativeGenerator\n\n| Error | Cause | Fix |\n|-------|-------|-----|\n| `No git history found` | Not in git repo | Initialize git or skip chapter |\n| `Failed to read git history` | Git not available | Install git |\n| `with_ml_data: 0` | ML data missing | Run backfill (optional) |\n\n#### SearchIndexGenerator\n\n| Error | Cause | Fix |\n|-------|-------|-----|\n| `chapters_indexed: 0` | No chapter files | Generate chapters first |\n| `Failed to parse <file>` | Malformed frontmatter | Regenerate source chapter |\n| `JSONDecodeError` | Invalid JSON structure | Check chapter YAML |\n\n### System-Level Errors\n\n| Error | Typical Cause | Solution |\n|-------|--------------|----------|\n| `PermissionError` | Read-only filesystem | Check permissions with `ls -ld book/` |\n| `FileNotFoundError` | Missing source file | Verify file exists with `ls -la` |\n| `ModuleNotFoundError: yaml` | Missing dependency | Install with `pip install pyyaml` |\n| `JSONDecodeError` | Corrupted output | Delete and regenerate file |\n| `UnicodeDecodeError` | Binary file read as text | Check file encoding |\n| `subprocess.CalledProcessError` | Git command failed | Verify git with `git status` |\n\n---\n\n## Prevention Tips\n\n### Before Generating\n\n1. **Verify prerequisites:**\n   ```bash\n   python --version  # 3.8+\n   git --version\n   python -c \"import yaml; print('OK')\"\n   ```\n\n2. **Check source files:**\n   ```bash\n   test -f docs/VISION.md || echo \"WARNING: VISION.md missing\"\n   ls cortical/*.ai_meta | wc -l  # Should be >10\n   ```\n\n3. **Test with dry-run:**\n   ```bash\n   python scripts/generate_book.py --dry-run\n   ```\n\n### After Making Changes\n\n1. **Regenerate affected chapters:**\n   - Changed VISION.md \u2192 `--chapter foundations`\n   - Changed docstrings \u2192 regenerate metadata, then `--chapter architecture`\n   - New commits \u2192 `--chapter evolution`\n\n2. **Always rebuild search index:**\n   ```bash\n   python scripts/generate_book.py --chapter search\n   ```\n\n3. **Validate outputs:**\n   ```bash\n   python -c \"import json; json.load(open('book/index.json'))\"\n   ```\n\n---\n\n## Getting Help\n\n### Debug Checklist\n\n- [ ] Run with `--dry-run --verbose`\n- [ ] Check error message in this guide\n- [ ] Verify dependencies installed\n- [ ] Test with single chapter generation\n- [ ] Check source file structure\n- [ ] Review recent git history\n- [ ] Try complete rebuild\n\n### Logging\n\nGenerate detailed logs for debugging:\n\n```bash\n# Full verbose output to file\npython scripts/generate_book.py --verbose 2>&1 | tee generation.log\n\n# Check for errors\ngrep -i \"error\\|warning\\|failed\" generation.log\n\n# Check generator stats\ngrep \"stats\" generation.log\n```\n\n### Still Stuck?\n\n1. **Check recent changes:**\n   ```bash\n   git log --oneline -10\n   git diff HEAD~5 -- docs/ cortical/\n   ```\n\n2. **Isolate the problem:**\n   - Test each generator individually\n   - Compare with known-good state\n   - Check file permissions\n\n3. **Report issue with:**\n   - Full error message\n   - Output of `--dry-run --verbose`\n   - Output of diagnostic commands\n   - Recent changes to source files\n\n---\n\n## Appendix: File Locations\n\n### Source Files\n\n| File | Purpose | Generator |\n|------|---------|-----------|\n| `docs/VISION.md` | Algorithm descriptions | foundations |\n| `cortical/*.ai_meta` | Module metadata | architecture |\n| `.git/logs/` | Git history | evolution |\n| `.git-ml/tracked/commits.jsonl` | ML commit data | evolution |\n| `samples/decisions/adr-*.md` | ADRs | decisions |\n\n### Output Files\n\n| File | Generator | Can Delete? |\n|------|-----------|-------------|\n| `book/*/index.md` | Various | Yes (regenerates) |\n| `book/01-foundations/*.md` | foundations | Yes |\n| `book/02-architecture/*.md` | architecture | Yes |\n| `book/04-evolution/*.md` | evolution | Yes |\n| `book/index.json` | search | Yes |\n| `book/search.json` | search | Yes |\n| `book/README.md` | Manual | **No** (manual) |\n| `book/docs/` | Manual | **No** (manual) |\n| `book/assets/` | Manual | **No** (manual) |\n\n---\n\n*This troubleshooting guide is part of [The Cortical Chronicles](../README.md) documentation.*\n"
    }
  ],
  "sections": {
    "preface": {
      "count": 1,
      "description": "Introduction to the book and how it works"
    },
    "foundations": {
      "count": 6,
      "description": "Core algorithms and IR theory"
    },
    "architecture": {
      "count": 10,
      "description": "Module documentation and system design"
    },
    "evolution": {
      "count": 5,
      "description": "Project history and development narrative"
    },
    "docs": {
      "count": 2,
      "description": ""
    }
  },
  "stats": {
    "total_chapters": 24,
    "total_sections": 5
  }
}