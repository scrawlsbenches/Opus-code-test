{
  "generated": "2025-12-17T00:26:24.454799Z",
  "documents": [
    {
      "id": "00-preface/how-this-book-works",
      "title": "How This Book Works",
      "content": "# How This Book Works\n\n> *\"The best documentation is the kind that writes itself.\"*\n\n## Overview\n\nThe Cortical Chronicles is a **self-documenting book**. It uses the Cortical Text Processor\u2014the very system it documents\u2014to generate its own content. This creates a fascinating recursive property: the book understands itself through the same algorithms it explains.\n\n## The Generation Process\n\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    BOOK GENERATION                           \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                              \u2502\n\u2502  Source Files           Generators           Chapters        \u2502\n\u2502  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500           \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500           \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500        \u2502\n\u2502                                                              \u2502\n\u2502  docs/VISION.md    \u2192   AlgorithmGen    \u2192   01-foundations/  \u2502\n\u2502  cortical/*.ai_meta \u2192  ModuleDocGen    \u2192   02-architecture/ \u2502\n\u2502  samples/decisions/ \u2192  DecisionGen     \u2192   03-decisions/    \u2502\n\u2502  git log           \u2192   NarrativeGen    \u2192   04-evolution/    \u2502\n\u2502  tasks/            \u2192   RoadmapGen      \u2192   05-future/       \u2502\n\u2502                                                              \u2502\n\u2502                    \u2193                                         \u2502\n\u2502              search-index.json                               \u2502\n\u2502                    \u2193                                         \u2502\n\u2502               index.html (searchable)                        \u2502\n\u2502                                                              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n## Chapter Types\n\n### 01-foundations/\nAlgorithm deep-dives extracted from `docs/VISION.md`. Each algorithm (PageRank, BM25, Louvain, etc.) gets its own chapter with:\n- Purpose and intuition\n- Mathematical formulation\n- Implementation details\n- Why it matters for code search\n\n### 02-architecture/\nModule documentation generated from `.ai_meta` files. Includes:\n- Module purpose and dependencies\n- Key functions and classes\n- Mermaid dependency graphs\n\n### 03-decisions/\nArchitecture Decision Records from `samples/decisions/`. Documents the \"why\" behind design choices.\n\n### 04-evolution/\nA narrative of project history generated from git commits. Transforms raw commit logs into a readable story of how the project evolved.\n\n### 05-future/\nRoadmap and vision from task files and `VISION.md`. Shows where the project is heading.\n\n## The Self-Reference Loop\n\nHere's what makes this book special:\n\n1. **The processor indexes its own code** \u2192 Creates a semantic graph\n2. **The generators query that graph** \u2192 Find relevant content\n3. **The book explains those algorithms** \u2192 Reader understands the system\n4. **The system processes those explanations** \u2192 Understands itself better\n\nThis isn't just cute\u2014it's a powerful test of the system's capabilities. If the Cortical Text Processor can understand and explain itself, it can understand any codebase.\n\n## Regenerating the Book\n\nThe book regenerates automatically on every push to `main`:\n\n```bash\n# Manual regeneration\npython scripts/generate_book.py\n\n# Generate specific chapter\npython scripts/generate_book.py --chapter foundations\n\n# Preview without writing\npython scripts/generate_book.py --dry-run\n```\n\n## Searching the Book\n\nThe book includes a semantic search interface. Open `index.html` to:\n- Search by keyword or concept\n- Browse by chapter\n- Follow cross-references\n\nThe search uses the same algorithms described in the book\u2014query expansion, BM25 scoring, PageRank boosting.\n\n## See Also\n\n- [Algorithm Analysis](../01-foundations/index.md) - Deep dive into the algorithms\n- [Architecture](../02-architecture/index.md) - How the code is organized\n- [Source: generate_book.py](../../scripts/generate_book.py) - The generation script\n\n## Source Files\n\nThis chapter was written manually as the seed for the book. Future chapters are auto-generated from:\n- `scripts/generate_book.py` - The orchestrator\n- `docs/VISION.md:185-430` - Algorithm documentation source\n\n---\n\n*This chapter is part of [The Cortical Chronicles](../README.md),\na self-documenting book generated by the Cortical Text Processor.*\n",
      "keywords": [
        "book",
        "chapter",
        "cortical",
        "generate_book",
        "vision",
        "architecture",
        "search",
        "index",
        "scripts",
        "itself"
      ]
    },
    {
      "id": "00-preface/project-origin",
      "title": "Project Origin: From Package to Platform",
      "content": "# Project Origin: From Package to Platform\n\n*How a zip file became a self-documenting semantic search engine.*\n\n## The Beginning\n\nOn December 9th, 2025, a package arrived. Not through a mail service, but as a GitHub upload: `cortical_package.zip`. Inside this compressed archive were the seeds of what would become a sophisticated information retrieval system\u201411,100 lines of Python implementing biological metaphors for text analysis.\n\n```\nCommit: 74d8f6b - Extract cortical_package from zip archive\nDate:   2025-12-09 18:12:22 +0000\n```\n\nThe extraction was straightforward. The journey ahead was anything but.\n\n## The First Hour: Cleanup and Discovery\n\nWithin minutes of extraction, the cleanup began. The package had arrived with traces of its previous life\u2014cached Python bytecode, temporary files, the detritus of development. These were swept away methodically:\n\n```\nf66a1ae - Clean up repository structure         (18:16:54 UTC)\n324d751 - Add Python patterns to .gitignore     (18:19:25 UTC)\n```\n\nBut cleanup revealed something more interesting: the code had bugs. Not catastrophic failures, but the kind of edge cases that emerge when sophisticated algorithms meet real-world data.\n\n## The Bug List: A Contract Emerges\n\nAt 18:31:03 UTC, barely an hour after extraction, `TASK_LIST.md` appeared:\n\n```\nCommit: d89ceee - Add TASK_LIST.md documenting required bug fixes\n```\n\nThis wasn't just a to-do list. It was an acknowledgment: *This code is powerful, but it needs care.* The list documented validation gaps, edge cases in clustering algorithms, missing error handling. Twenty minutes later, the fixes landed:\n\n```\nCommit: 75097e9 - Fix bugs and add comprehensive unit tests\nTime:   18:51:22 UTC\n```\n\nThe system could now handle empty inputs, validate parameters, and fail gracefully. But more importantly, it established a pattern: **test first, ship second**.\n\n## The Foundation: CLAUDE.md is Born\n\nTwo minutes after the bug fixes, at 18:53:11 UTC, something remarkable happened:\n\n```\nCommit: 9badbcb - Add CLAUDE.md project guide for Claude Code\n```\n\nThis wasn't just documentation. It was a *contract between human and AI developers*. CLAUDE.md laid out:\n\n- **Persona**: You are a senior computational neuroscience engineer\n- **Philosophy**: Profile before optimizing, understand before acting\n- **Architecture**: Here's how the layers work, here's where to find things\n- **Rules**: Don't use underscores in bigrams, always use `get_by_id()` for O(1) lookups\n\nIt was both a guide and a guardrail. A way to ensure that future modifications\u2014whether by human or AI\u2014would respect the system's design principles.\n\n## The Corpus Grows\n\nThe next phase focused on content. Seven sample documents arrived first, then 44 more. The system needed diversity to test its semantic clustering:\n\n```\nb69a296 - Add 7 new sample documents             (19:02:23 UTC)\n892c826 - Add 44 diverse sample documents        (23:05:22 UTC)\n```\n\nFrom machine learning papers to cooking recipes, from code documentation to philosophical essays\u2014the corpus became a microcosm of human knowledge. The system could now be tested not just with toy examples, but with real semantic complexity.\n\n## The RAG Journey: Tasks 9-30\n\nThen came the transformation. Between December 9th and 10th, 2025, the system evolved from basic text processing to full Retrieval-Augmented Generation capabilities. The commits tell the story:\n\n### Phase 1: The RAG Foundations (Dec 9, ~19:51-19:57 UTC)\n\n```\n2085418 - Add document metadata support (Task 9)\nbf75e5d - Activate Layer 2 concept clustering (Task 10)\nf27d18e - Integrate semantic relations (Task 11)\n8f862b0 - Persist full computed state (Task 12)\n```\n\nIn six minutes of commits, the system gained:\n- **Citations**: Documents could now carry metadata for proper attribution\n- **Concepts**: Layer 2 clustering activated by default\n- **Relations**: Semantic understanding woven into retrieval\n- **Persistence**: Full state saving including graph embeddings\n\n### Phase 2: Production Features (Dec 9, ~21:06-21:14 UTC)\n\n```\n38fb4f7 - Add incremental document indexing (Task 15)\nb3c29af - Add multi-stage ranking pipeline (Task 17)\n900cce1 - Add batch query API (Task 18)\n```\n\nThe system could now handle live updates, sophisticated ranking, and batch processing. It was becoming production-ready.\n\n### Phase 3: The ConceptNet Vision (Dec 9-10, ~22:37 onwards)\n\nThen the ambition escalated:\n\n```\nc6eefdc - Add ConceptNet-enhanced PageRank task list\n        (Tasks 19-30 planned at 22:37:45 UTC)\n```\n\nOver the next two hours, twelve tasks were completed:\n\n- **Cross-layer connections**: Feedforward and feedback between layers\n- **Lateral connections**: Within bigrams and concepts\n- **Typed edges**: Relation types (IsA, PartOf, UsedFor, etc.)\n- **Multi-hop inference**: Reasoning chains across the graph\n- **Pattern extraction**: Automatic relation discovery from text\n- **Graph export**: ConceptNet-style visualization\n\nBy midnight UTC on December 10th, the system had evolved from a package to a *knowledge graph platform*.\n\n## The Numbers That Tell the Story\n\n| Metric | Initial Package | After 24 Hours | Today |\n|--------|----------------|----------------|-------|\n| **Commits** | 1 (extraction) | ~40 | 699 |\n| **Code Lines** | ~8,500 | ~10,000 | ~11,100 |\n| **Tasks Completed** | 0 | 30 | 200+ |\n| **Test Coverage** | Unknown | >80% | >89% |\n| **Sample Documents** | 0 | 51 | 125+ |\n\n## The Self-Documenting Dream\n\nHundreds of commits later, on December 16th, 2025, something extraordinary happened. The system that had been built to analyze text began to analyze *itself*:\n\n```\nc730057 - Add Cortical Chronicles book infrastructure (Wave 1)\n3022110 - Add content generators (Wave 2)\n0022466 - Add search integration and web interface (Wave 3)\n940fdf2 - Add CI workflow and documentation (Wave 4)\n```\n\nThe book you're reading now was generated by the code it describes. The system indexed its own source code, its own documentation, its own commit history\u2014and synthesized this narrative.\n\n## Where We Are Now\n\nToday, the Cortical Text Processor is:\n\n- **~11,100 lines** of core library code\n- **Zero external dependencies** for runtime\n- **699 commits** of careful evolution\n- **89%+ test coverage** maintained rigorously\n- **4-layer architecture** with typed semantic relations\n- **A book that writes itself** from living code\n\nBut more than the numbers, it embodies a philosophy:\n\n> *\"Profile before optimizing, understand before acting, test before shipping.\"*\n\nThat philosophy started in CLAUDE.md on day one. It continues in every commit, every test, every design decision.\n\n## The Commits That Started It All\n\n| Commit | Date | Milestone |\n|--------|------|-----------|\n| `27a6531` | Nov 25, 2025 | Repository created |\n| `74d8f6b` | Dec 9, 18:12 | Package extraction |\n| `9badbcb` | Dec 9, 18:53 | CLAUDE.md foundation |\n| `2085418` | Dec 9, 19:51 | RAG journey begins (Task 9) |\n| `e40a80c` | Dec 10, 00:24 | ConceptNet integration complete (Task 29) |\n| `c730057` | Dec 16, 2025 | Self-documenting book begins |\n| `082aa21` | Dec 16, 2025 | Six intelligent book generators |\n\n## What This Means\n\nA project's origin story isn't just historical curiosity. It reveals:\n\n1. **Intent**: This was designed to be *understood*, not just used\n2. **Discipline**: Testing and documentation weren't afterthoughts\n3. **Ambition**: From day one, the goal was semantic understanding\n4. **Partnership**: Human and AI working together, guided by CLAUDE.md\n\nThe zip file that arrived on December 9th contained code. But the code that emerged in the days and weeks that followed became something more: a platform for knowledge, a canvas for collaboration, and ultimately, its own biographer.\n\n---\n\n*The journey from package to platform took 699 commits. The journey from platform to self-awareness took one more: the commit that added this book.*\n",
      "keywords": [
        "add",
        "system",
        "utc",
        "code",
        "task",
        "dec",
        "commit",
        "package",
        "semantic",
        "claude"
      ]
    },
    {
      "id": "00-preface/the-living-book-vision",
      "title": "The Living Book Vision",
      "content": "# The Living Book Vision\n\n> *\"Code tells you what. Comments tell you why. A living book tells you the journey.\"*\n\n## The Idea\n\nWhat if documentation wrote itself\u2014not as an afterthought, but as a natural byproduct of development?\n\nThis isn't science fiction. It's what you're reading right now.\n\nThe Cortical Chronicles is a **living book**: it grows with the codebase, captures the stories behind decisions, and transforms raw development artifacts into narrative chapters. Every debugging session becomes a case study. Every bugfix becomes a lesson. Every commit becomes a paragraph in an ongoing story.\n\n## How It Works\n\n### The Data We Already Capture\n\nDuring development, we're already collecting everything a book needs:\n\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                 THE RAW MATERIALS OF A BOOK                      \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                                  \u2502\n\u2502  Developer Questions    \u2192 \"Why isn't search finding X?\"          \u2502\n\u2502  Investigation Traces   \u2192 Files read, tools used, paths tried    \u2502\n\u2502  Breakthroughs         \u2192 \"Aha! The bottleneck is in bigrams!\"    \u2502\n\u2502  Solutions             \u2192 Commits with diffs and context          \u2502\n\u2502  Decisions             \u2192 ADRs with rationale                     \u2502\n\u2502  Outcomes              \u2192 CI results, test coverage               \u2502\n\u2502                                                                  \u2502\n\u2502  Together, these form NARRATIVE ARCS:                           \u2502\n\u2502                                                                  \u2502\n\u2502  Problem \u2192 Investigation \u2192 Discovery \u2192 Solution \u2192 Lesson         \u2502\n\u2502                                                                  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n### From Data to Story\n\nThe generators transform structured data into readable chapters:\n\n| Data Source | Generator | Chapter Type |\n|-------------|-----------|--------------|\n| ML Sessions | CaseStudyGenerator | Problem-solving narratives |\n| Bugfix Commits | LessonExtractor | Distilled wisdom |\n| ADRs + Context | DecisionStoryGenerator | The \"why\" behind choices |\n| Concept Clusters | ConceptEvolutionGenerator | How ideas grew over time |\n| Test Cases | ExerciseGenerator | Reader engagement |\n| PageRank Scores | ReaderJourneyGenerator | Progressive learning paths |\n\n## What Makes This Different\n\n### Traditional Documentation\n\n```\nprocess_document(doc_id, text)\n\nProcess a document and add it to the corpus.\n\nParameters:\n  doc_id: Unique document identifier\n  text: Document content\n\nReturns:\n  None\n```\n\n### A Living Book\n\n> **Chapter 5: Processing Your First Document**\n>\n> Before the system can search, it needs to understand. That understanding begins with `process_document()`.\n>\n> Think of it as reading a book for the first time. You don't just see words\u2014you build a mental map: which ideas connect, which concepts are central, which phrases recur.\n>\n> The processor does the same thing, but algorithmically. When you call:\n>\n> ```python\n> processor.process_document(\"readme\", open(\"README.md\").read())\n> ```\n>\n> ...a cascade of analysis begins. Tokens are extracted. Bigrams form. Lateral connections strengthen between co-occurring terms. The document joins a growing graph of semantic relationships.\n>\n> **Try It Yourself:** Process the CLAUDE.md file and examine what concepts emerge. Which terms have the highest PageRank? What does that tell you about the document's focus?\n\n## The Chapter Types\n\n### Case Studies: Learning from Real Problems\n\nEvery debugging session is a story waiting to be told:\n\n> **Case Study: The Great Performance Hunt**\n>\n> It started with a timeout. The `compute_all()` function was hanging on just 125 documents.\n>\n> The obvious suspect was Louvain clustering\u2014our most complex algorithm. But we profiled first...\n\n### Lessons: Wisdom Distilled from Experience\n\n600+ commits contain patterns worth preserving:\n\n> **Lesson #23: Profile Before Optimizing**\n>\n> **The Mistake:** Assumed Louvain was slow because it's complex.\n> **The Reality:** 99% of time was in `bigram_connections()`.\n> **The Principle:** The obvious culprit is often innocent. Data beats intuition.\n\n### Concept Evolution: Watching Ideas Grow\n\nTrack how key concepts emerged and strengthened:\n\n> **How \"Importance\" Became a First-Class Concept**\n>\n> Week 1: First mention in `analysis.py`\u2014a simple PageRank score.\n> Week 3: Connected to \"relevance\", \"ranking\", \"boost\".\n> Week 6: Cluster of 15 related terms. Central to search quality.\n\n### Exercises: Active Learning\n\nTest cases become teaching moments:\n\n> **Exercise: Query Expansion**\n>\n> Given the query \"neural networks\", write code to expand it with related terms.\n>\n> *Hint 1:* Use `processor.expand_query()`\n> *Hint 2:* Consider lateral connections\n> *Solution:* [Reveal]\n\n## The Self-Reference Loop\n\nHere's the beautiful recursion:\n\n1. **We write code** \u2192 Creates development artifacts\n2. **ML captures the process** \u2192 Structured session data\n3. **Generators synthesize narratives** \u2192 Book chapters\n4. **The book explains the system** \u2192 Readers understand\n5. **Understanding leads to better code** \u2192 Loop continues\n\nThe Cortical Text Processor documents itself using its own algorithms. If it can understand and explain itself, it can understand any codebase.\n\n## Why This Matters\n\n### For Developers\n\n- Documentation stays current automatically\n- Lessons are captured, not forgotten\n- Onboarding accelerates via structured learning paths\n\n### For Teams\n\n- Institutional knowledge persists across turnover\n- Decisions are documented with full context\n- Best practices emerge from analyzed patterns\n\n### For Publishers\n\n- Authentic problem-solving narratives\n- Genuine lessons from real development\n- A unique angle: the book that writes itself\n\n## The Vision\n\nImagine every software project generating its own living book:\n\n- New team members read the story of how the system evolved\n- Debugging sessions become teaching materials\n- Architecture decisions carry their full context\n- The gap between \"code\" and \"understanding\" closes\n\nThis isn't just documentation. It's **computational autobiography**\u2014a system telling its own story through the act of being built.\n\n---\n\n## What You'll Find in This Book\n\n| Section | Content |\n|---------|---------|\n| **Foundations** | The algorithms that power semantic search |\n| **Architecture** | How the code is organized and why |\n| **Decisions** | The choices that shaped the system |\n| **Evolution** | Timeline of how we got here |\n| **Case Studies** | Problem-solving narratives |\n| **Lessons** | Distilled wisdom from 600+ commits |\n| **Concepts** | How key ideas emerged and grew |\n| **Exercises** | Hands-on learning opportunities |\n| **Journey** | Your personalized learning path |\n\nEach section is generated from the codebase itself\u2014living documentation that grows with the system.\n\n---\n\n## See Also\n\n- [How This Book Works](./how-this-book-works.md) - Technical details of generation\n- [Full Vision Document](../../docs/BOOK-GENERATION-VISION.md) - Complete specification\n- [Product Vision](../../docs/VISION.md) - Overall product direction\n\n---\n\n*This chapter is part of [The Cortical Chronicles](../README.md),\na self-documenting book generated by the Cortical Text Processor.*\n",
      "keywords": [
        "book",
        "document",
        "living",
        "vision",
        "code",
        "data",
        "learning",
        "system",
        "documentation",
        "itself"
      ]
    },
    {
      "id": "01-foundations/alg-bm25",
      "title": "BM25/TF-IDF \u2014 Distinctiveness Scoring",
      "content": "# BM25/TF-IDF \u2014 Distinctiveness Scoring\n\n**Purpose:** Score how well a term distinguishes a specific document from the rest of the corpus.\n\n**Implementation:** `cortical/analysis/tfidf.py`\n\n**BM25 Formula (Default):**\n```\nBM25(t, d) = IDF(t) \u00d7 (tf(t,d) \u00d7 (k1 + 1)) / (tf(t,d) + k1 \u00d7 (1 - b + b \u00d7 |d|/avgdl))\n```\n\nWhere:\n- `IDF(t) = log((N - df(t) + 0.5) / (df(t) + 0.5) + 1)` \u2014 Inverse document frequency with smoothing\n- `tf(t,d)` \u2014 Term frequency in document d\n- `k1 = 1.2` \u2014 Term frequency saturation (diminishing returns after ~12 occurrences)\n- `b = 0.75` \u2014 Length normalization factor\n\n**Why BM25 Over TF-IDF:**\n- Non-negative IDF even for terms appearing in most documents\n- Length normalization prevents long files from unfairly dominating\n- Term frequency saturation models realistic relevance (saying \"API\" 100 times doesn't make a doc 100\u00d7 more relevant than saying it once)\n\n**Dual Storage Strategy:**\n- **Global TF-IDF** (`col.tfidf`): Term importance to entire corpus\n- **Per-Document TF-IDF** (`col.tfidf_per_doc[doc_id]`): Term importance within specific document\n\nThis dual approach allows:\n- Fast corpus-wide importance filtering\n- Accurate per-document relevance scoring for search\n\n---\n\n*This chapter is part of [The Cortical Chronicles](../README.md), a self-documenting book generated by the Cortical Text Processor.*\n",
      "keywords": [
        "idf",
        "term",
        "document",
        "bm25",
        "frequency",
        "corpus",
        "cortical",
        "importance",
        "scoring",
        "specific"
      ]
    },
    {
      "id": "01-foundations/alg-graph-boosted-search",
      "title": "Graph-Boosted Search (GB-BM25) \u2014 Hybrid Ranking",
      "content": "# Graph-Boosted Search (GB-BM25) \u2014 Hybrid Ranking\n\n**Purpose:** Combine BM25 relevance with graph structure signals.\n\n**Implementation:** `cortical/query/search.py:425-564`\n\n**Scoring Formula:**\n```\nfinal_score = (0.5 \u00d7 normalized_bm25) + (0.3 \u00d7 normalized_pagerank) + (0.2 \u00d7 normalized_proximity)\n            \u00d7 coverage_multiplier (0.5 to 1.5)\n```\n\n**Three Signal Sources:**\n\n1. **BM25 Base Score (50%):**\n   - Standard term frequency \u00d7 inverse document frequency\n   - Per-document scoring using `col.tfidf_per_doc`\n\n2. **PageRank Boost (30%):**\n   - Sum of matched term PageRanks\n   - Rewards documents containing important terms\n\n3. **Proximity Boost (20%):**\n   - For each pair of original query terms:\n     - Check if they're connected in the co-occurrence graph\n     - If connected, boost documents containing both\n   - Rewards documents where query terms appear together\n\n**Coverage Multiplier:**\n- Documents matching 1/5 query terms: 0.7\u00d7 multiplier\n- Documents matching all 5 query terms: 1.5\u00d7 multiplier\n- Prevents documents matching one rare term from outranking documents matching many terms\n\n---\n\n*This chapter is part of [The Cortical Chronicles](../README.md), a self-documenting book generated by the Cortical Text Processor.*\n",
      "keywords": [
        "documents",
        "terms",
        "query",
        "matching",
        "graph",
        "bm25",
        "cortical",
        "term",
        "boost",
        "multiplier"
      ]
    },
    {
      "id": "01-foundations/alg-louvain",
      "title": "Louvain Community Detection \u2014 Concept Discovery",
      "content": "# Louvain Community Detection \u2014 Concept Discovery\n\n**Purpose:** Discover semantic clusters (concepts) from the term co-occurrence graph.\n\n**Implementation:** `cortical/analysis/clustering.py`\n\n**Two-Phase Algorithm:**\n\n**Phase 1 \u2014 Local Optimization:**\n```\nfor each node:\n    find neighboring communities\n    calculate modularity gain for moving to each\n    move to best community if gain > 0\nrepeat until no nodes move\n```\n\n**Phase 2 \u2014 Network Aggregation:**\n```\ncollapse each community into a single super-node\nedges between communities become edges between super-nodes\nrepeat Phase 1 on the aggregated network\n```\n\n**Modularity Formula:**\n```\nQ = (1/2m) \u00d7 \u03a3 [A_ij - (k_i \u00d7 k_j)/(2m)] \u00d7 \u03b4(c_i, c_j)\n```\n\nThe algorithm optimizes Q, which measures how much edge weight falls within communities versus what would be expected by random chance.\n\n**Resolution Parameter:**\n- `resolution = 1.0` (default): Balanced clusters, ~32 concepts\n- `resolution = 0.5`: Coarse clusters, ~38 concepts (max cluster 64% of tokens)\n- `resolution = 2.0`: Fine-grained clusters, ~79 concepts (max cluster 4.2% of tokens)\n\n**Concept Naming:**\n```python\ntop_members = sorted(cluster_members, key=lambda m: m.pagerank, reverse=True)[:3]\nconcept_name = '/'.join(top_members)  # e.g., \"neural/learning/networks\"\n```\n\n**Why This Matters:**\n- Enables concept-level search (\"find documents about authentication\")\n- Reduces dimensionality while preserving semantic structure\n- Creates Layer 2 (Concepts) that bridges raw terms and documents\n\n---\n\n*This chapter is part of [The Cortical Chronicles](../README.md), a self-documenting book generated by the Cortical Text Processor.*\n",
      "keywords": [
        "concepts",
        "clusters",
        "phase",
        "resolution",
        "community",
        "concept",
        "cortical",
        "communities",
        "semantic",
        "algorithm"
      ]
    },
    {
      "id": "01-foundations/alg-pagerank",
      "title": "PageRank \u2014 Importance Discovery",
      "content": "# PageRank \u2014 Importance Discovery\n\n**Purpose:** Identify which terms matter most in the corpus, independent of raw frequency.\n\n**Implementation:** `cortical/analysis/pagerank.py`\n\n**How It Works:**\n```\nimportance[term] = (1 - damping) / N + damping \u00d7 \u03a3 (neighbor_importance \u00d7 edge_weight / neighbor_outgoing_sum)\n```\n\nThe algorithm iteratively propagates importance through the term co-occurrence graph. Terms that are referenced by many important terms become important themselves\u2014a recursive definition that converges to stable values.\n\n**Key Parameters:**\n- `damping = 0.85`: The probability of following a link vs. jumping to a random node\n- `tolerance = 1e-6`: Convergence threshold (stops when no term changes by more than this)\n- `max_iterations = 20`: Upper bound on iterations\n\n**Three Variants:**\n1. **Standard PageRank**: Applied to Layer 0 (tokens) and Layer 1 (bigrams)\n2. **Semantic PageRank**: Adjusts edge weights by relation type (IsA connections count 1.5\u00d7 more than CoOccurs)\n3. **Hierarchical PageRank**: Propagates importance across all 4 layers with separate cross-layer damping\n\n**Why This Matters for Code Search:**\n- Common utility functions referenced everywhere get high PageRank\n- Core abstractions that everything depends on surface naturally\n- Prevents over-emphasis on boilerplate code that appears frequently but isn't semantically central\n\n**Performance:** O(iterations \u00d7 edges), typically 100-500ms for 10K tokens with early convergence usually at 5-10 iterations.\n\n---\n\n*This chapter is part of [The Cortical Chronicles](../README.md), a self-documenting book generated by the Cortical Text Processor.*\n",
      "keywords": [
        "pagerank",
        "importance",
        "damping",
        "terms",
        "cortical",
        "term",
        "iterations",
        "layer",
        "propagates",
        "referenced"
      ]
    },
    {
      "id": "01-foundations/alg-query-expansion",
      "title": "Query Expansion \u2014 Semantic Bridging",
      "content": "# Query Expansion \u2014 Semantic Bridging\n\n**Purpose:** Transform literal query terms into semantically enriched term sets.\n\n**Implementation:** `cortical/query/expansion.py`\n\n**Three Expansion Methods:**\n\n1. **Lateral Connection Expansion:**\n   - Follow co-occurrence edges from query terms\n   - Score: `edge_weight \u00d7 neighbor_score \u00d7 0.6`\n   - Takes top 5 neighbors per query term\n\n2. **Concept Cluster Membership:**\n   - Find concepts containing query terms\n   - Add other cluster members as expansions\n   - Score: `concept.pagerank \u00d7 member.pagerank \u00d7 0.4`\n\n3. **Code Concept Synonyms:**\n   - Programming-specific synonym groups (get/fetch/load, create/make/build)\n   - Limited to 3 synonyms per term to prevent drift\n\n**Multi-Hop Inference:**\n```\nQuery: \"neural\"\n  Hop 0: neural (1.0)\n  Hop 1: networks (0.4), learning (0.35)\n  Hop 2: deep (0.098) \u2014 via learning with decay\n```\n\nChain validity is scored by relation type pairs:\n- `(IsA, IsA)`: 1.0 \u2014 fully transitive (dog\u2192animal\u2192living_thing)\n- `(RelatedTo, RelatedTo)`: 0.6 \u2014 weaker transitivity\n- `(Antonym, Antonym)`: 0.3 \u2014 double negation, avoid\n\n---\n\n*This chapter is part of [The Cortical Chronicles](../README.md), a self-documenting book generated by the Cortical Text Processor.*\n",
      "keywords": [
        "query",
        "expansion",
        "hop",
        "terms",
        "term",
        "cortical",
        "concept",
        "score",
        "per",
        "cluster"
      ]
    },
    {
      "id": "01-foundations/alg-semantic-extraction",
      "title": "Semantic Relation Extraction \u2014 Knowledge Graph Construction",
      "content": "# Semantic Relation Extraction \u2014 Knowledge Graph Construction\n\n**Purpose:** Extract typed relationships (IsA, PartOf, Causes) from document text.\n\n**Implementation:** `cortical/semantics.py`\n\n**Pattern-Based Extraction:**\n24 regex patterns detect 10+ relation types:\n```python\nr'(\\w+)\\s+(?:is|are)\\s+(?:a|an)\\s+(?:type\\s+of\\s+)?(\\w+)' \u2192 IsA (0.9 confidence)\nr'(\\w+)\\s+(?:is|are)\\s+(?:a\\s+)?part\\s+of' \u2192 PartOf (0.95 confidence)\nr'(\\w+)\\s+(?:causes|leads?\\s+to)' \u2192 Causes (0.9 confidence)\n```\n\n**Semantic Retrofitting:**\nBlends co-occurrence weights with semantic relation knowledge:\n```\nnew_weight = \u03b1 \u00d7 original_weight + (1-\u03b1) \u00d7 semantic_target_weight\n```\nWith \u03b1 = 0.3, semantic signals dominate (70%) while preserving some corpus statistics (30%).\n\n**Relation Weight Multipliers:**\n| Relation | Weight | Semantics |\n|----------|--------|-----------|\n| SameAs | 2.0 | Strongest synonymy |\n| IsA | 1.5 | Hypernymy |\n| PartOf | 1.3 | Meronymy |\n| RelatedTo | 0.8 | Generic |\n| Antonym | -0.5 | Opposition |\n\n---\n\n*This chapter is part of [The Cortical Chronicles](../README.md), a self-documenting book generated by the Cortical Text Processor.*\n",
      "keywords": [
        "relation",
        "semantic",
        "isa",
        "partof",
        "causes",
        "cortical",
        "confidence",
        "extraction",
        "knowledge",
        "text"
      ]
    },
    {
      "id": "02-architecture/index",
      "title": "Architecture Overview",
      "content": "# Architecture Overview\n\nThis section documents the architecture of the Cortical Text Processor through automatically extracted module metadata.\n\n## Statistics\n\n- **Total Modules**: 45\n- **Module Groups**: 9\n- **Classes**: 50\n- **Functions**: 422\n\n## Module Groups\n\n### [Analysis](mod-analysis.md)\n\n8 modules:\n\n- `__init__.py`\n- `activation.py`\n- `clustering.py`\n- `connections.py`\n- `pagerank.py`\n- `quality.py`\n- `tfidf.py`\n- `utils.py`\n\n### [Configuration](mod-configuration.md)\n\n3 modules:\n\n- `config.py`\n- `constants.py`\n- `validation.py`\n\n### [Data Structures](mod-data-structures.md)\n\n3 modules:\n\n- `layers.py`\n- `minicolumn.py`\n- `types.py`\n\n### [Nlp](mod-nlp.md)\n\n3 modules:\n\n- `embeddings.py`\n- `semantics.py`\n- `tokenizer.py`\n\n### [Observability](mod-observability.md)\n\n3 modules:\n\n- `observability.py`\n- `progress.py`\n- `results.py`\n\n### [Persistence](mod-persistence.md)\n\n3 modules:\n\n- `chunk_index.py`\n- `persistence.py`\n- `state_storage.py`\n\n### [Processor](mod-processor.md)\n\n6 modules:\n\n- `__init__.py`\n- `compute.py`\n- `core.py`\n- `documents.py`\n- `introspection.py`\n- `persistence_api.py`\n\n### [Query](mod-query.md)\n\n8 modules:\n\n- `__init__.py`\n- `chunking.py`\n- `definitions.py`\n- `expansion.py`\n- `intent.py`\n- `passages.py`\n- `ranking.py`\n- `search.py`\n\n### [Utilities](mod-utilities.md)\n\n8 modules:\n\n- `cli_wrapper.py`\n- `code_concepts.py`\n- `diff.py`\n- `fingerprint.py`\n- `fluent.py`\n- `gaps.py`\n- `mcp_server.py`\n- `patterns.py`\n\n---\n\n*This chapter is part of [The Cortical Chronicles](../README.md), a self-documenting book generated by the Cortical Text Processor.*\n",
      "keywords": [
        "modules",
        "mod",
        "processor",
        "cortical",
        "module",
        "__init__",
        "observability",
        "persistence",
        "architecture",
        "documents"
      ]
    },
    {
      "id": "02-architecture/mod-analysis",
      "title": "Graph Algorithms",
      "content": "# Graph Algorithms\n\nGraph algorithms for computing importance, relevance, and clusters.\n\n## Modules\n\n- **__init__.py**: Analysis Module\n- **activation.py**: Activation propagation algorithm.\n- **clustering.py**: Clustering algorithms for community detection.\n- **connections.py**: Connection building algorithms for network layers.\n- **pagerank.py**: PageRank algorithms for importance scoring.\n- **quality.py**: Clustering quality metrics.\n- **tfidf.py**: TF-IDF and BM25 scoring algorithms.\n- **utils.py**: Utility functions and classes for analysis algorithms.\n\n\n## __init__.py\n\nAnalysis Module\n===============\n\nGraph analysis algorithms for the cortical network.\n\nContains implementations of:\n- PageRank for importance scoring\n- TF-IDF for term weighting\n- Louvain community det...\n\n\n### Dependencies\n\n**Standard Library:**\n\n- `activation.propagate_activation`\n- `clustering._louvain_core`\n- `clustering.build_concept_clusters`\n- `clustering.cluster_by_label_propagation`\n- `clustering.cluster_by_louvain`\n- ... and 22 more\n\n\n\n## activation.py\n\nActivation propagation algorithm.\n\nContains:\n- propagate_activation: Spread activation through the network layers\n\n\n### Functions\n\n#### propagate_activation\n\n```python\npropagate_activation(layers: Dict[CorticalLayer, HierarchicalLayer], iterations: int = 3, decay: float = 0.8, lateral_weight: float = 0.3) -> None\n```\n\nPropagate activation through the network.\n\n### Dependencies\n\n**Standard Library:**\n\n- `layers.CorticalLayer`\n- `layers.HierarchicalLayer`\n- `typing.Dict`\n\n\n\n## clustering.py\n\nClustering algorithms for community detection.\n\nContains:\n- cluster_by_louvain: Louvain modularity optimization (recommended)\n- cluster_by_label_propagation: Label propagation clustering (legacy)\n- bu...\n\n\n### Functions\n\n#### cluster_by_label_propagation\n\n```python\ncluster_by_label_propagation(layer: HierarchicalLayer, min_cluster_size: int = 3, max_iterations: int = 20, cluster_strictness: float = 1.0, bridge_weight: float = 0.0) -> Dict[int, List[str]]\n```\n\nCluster minicolumns using label propagation.\n\n#### cluster_by_louvain\n\n```python\ncluster_by_louvain(layer: HierarchicalLayer, min_cluster_size: int = 3, resolution: float = 1.0, max_iterations: int = 10) -> Dict[int, List[str]]\n```\n\nCluster minicolumns using Louvain community detection.\n\n#### build_concept_clusters\n\n```python\nbuild_concept_clusters(layers: Dict[CorticalLayer, HierarchicalLayer], clusters: Dict[int, List[str]], doc_vote_threshold: float = 0.1) -> None\n```\n\nBuild concept layer from token clusters.\n\n### Dependencies\n\n**Standard Library:**\n\n- `collections.defaultdict`\n- `layers.CorticalLayer`\n- `layers.HierarchicalLayer`\n- `typing.Dict`\n- `typing.List`\n- ... and 2 more\n\n\n\n## connections.py\n\nConnection building algorithms for network layers.\n\nContains:\n- compute_document_connections: Build document-to-document similarity connections\n- compute_bigram_connections: Build lateral connections ...\n\n\n### Functions\n\n#### compute_concept_connections\n\n```python\ncompute_concept_connections(layers: Dict[CorticalLayer, HierarchicalLayer], semantic_relations: List[Tuple[str, str, str, float]] = None, min_shared_docs: int = 1, min_jaccard: float = 0.1, use_member_semantics: bool = False, use_embedding_similarity: bool = False, embedding_threshold: float = 0.3, embeddings: Dict[str, List[float]] = None) -> Dict[str, Any]\n```\n\nBuild lateral connections between concepts in Layer 2.\n\n#### compute_bigram_connections\n\n```python\ncompute_bigram_connections(layers: Dict[CorticalLayer, HierarchicalLayer], min_shared_docs: int = 1, component_weight: float = 0.5, chain_weight: float = 0.7, cooccurrence_weight: float = 0.3, max_bigrams_per_term: int = 100, max_bigrams_per_doc: int = 500, max_connections_per_bigram: int = 50) -> Dict[str, Any]\n```\n\nBuild lateral connections between bigrams in Layer 1.\n\n#### compute_document_connections\n\n```python\ncompute_document_connections(layers: Dict[CorticalLayer, HierarchicalLayer], documents: Dict[str, str], min_shared_terms: int = 3) -> None\n```\n\nBuild lateral connections between documents.\n\n### Dependencies\n\n**Standard Library:**\n\n- `collections.defaultdict`\n- `layers.CorticalLayer`\n- `layers.HierarchicalLayer`\n- `minicolumn.Minicolumn`\n- `typing.Any`\n- ... and 5 more\n\n\n\n## pagerank.py\n\nPageRank algorithms for importance scoring.\n\nContains:\n- compute_pagerank: Standard PageRank for a single layer\n- compute_semantic_pagerank: PageRank with semantic relation weighting\n- compute_hierarc...\n\n\n### Functions\n\n#### compute_pagerank\n\n```python\ncompute_pagerank(layer: HierarchicalLayer, damping: float = 0.85, iterations: int = 20, tolerance: float = 1e-06) -> Dict[str, float]\n```\n\nCompute PageRank scores for minicolumns in a layer.\n\n#### compute_semantic_pagerank\n\n```python\ncompute_semantic_pagerank(layer: HierarchicalLayer, semantic_relations: List[Tuple[str, str, str, float]], relation_weights: Optional[Dict[str, float]] = None, damping: float = 0.85, iterations: int = 20, tolerance: float = 1e-06) -> Dict[str, Any]\n```\n\nCompute PageRank with semantic relation type weighting.\n\n#### compute_hierarchical_pagerank\n\n```python\ncompute_hierarchical_pagerank(layers: Dict[CorticalLayer, HierarchicalLayer], layer_iterations: int = 10, global_iterations: int = 5, damping: float = 0.85, cross_layer_damping: float = 0.7, tolerance: float = 0.0001) -> Dict[str, Any]\n```\n\nCompute PageRank with cross-layer propagation.\n\n### Dependencies\n\n**Standard Library:**\n\n- `collections.defaultdict`\n- `constants.RELATION_WEIGHTS`\n- `layers.CorticalLayer`\n- `layers.HierarchicalLayer`\n- `typing.Any`\n- ... and 4 more\n\n\n\n## quality.py\n\nClustering quality metrics.\n\nContains:\n- compute_clustering_quality: Comprehensive quality evaluation (modularity, silhouette, balance)\n- _compute_modularity: Modularity Q metric\n- _compute_silhouette...\n\n\n### Functions\n\n#### compute_clustering_quality\n\n```python\ncompute_clustering_quality(layers: Dict[CorticalLayer, HierarchicalLayer], sample_size: int = 500) -> Dict[str, Any]\n```\n\nCompute clustering quality metrics for the concept layer.\n\n### Dependencies\n\n**Standard Library:**\n\n- `collections.defaultdict`\n- `layers.CorticalLayer`\n- `layers.HierarchicalLayer`\n- `random`\n- `typing.Any`\n- ... and 3 more\n\n\n\n## tfidf.py\n\nTF-IDF and BM25 scoring algorithms.\n\nContains:\n- compute_tfidf: Traditional TF-IDF scoring\n- compute_bm25: Okapi BM25 scoring with length normalization\n- _tfidf_core: Pure TF-IDF algorithm for unit te...\n\n\n### Functions\n\n#### compute_tfidf\n\n```python\ncompute_tfidf(layers: Dict[CorticalLayer, HierarchicalLayer], documents: Dict[str, str]) -> None\n```\n\nCompute TF-IDF scores for tokens.\n\n#### compute_bm25\n\n```python\ncompute_bm25(layers: Dict[CorticalLayer, HierarchicalLayer], documents: Dict[str, str], doc_lengths: Dict[str, int], avg_doc_length: float, k1: float = 1.2, b: float = 0.75) -> None\n```\n\nCompute BM25 scores for tokens.\n\n### Dependencies\n\n**Standard Library:**\n\n- `layers.CorticalLayer`\n- `layers.HierarchicalLayer`\n- `math`\n- `typing.Dict`\n- `typing.Tuple`\n\n\n\n## utils.py\n\nUtility functions and classes for analysis algorithms.\n\nContains:\n- SparseMatrix: Zero-dependency sparse matrix for bigram connections\n- Similarity functions: cosine_similarity, _doc_similarity, _vect...\n\n\n### Classes\n\n#### SparseMatrix\n\nSimple sparse matrix implementation using dictionary of keys (DOK) format.\n\n**Methods:**\n\n- `set`\n- `get`\n- `multiply_transpose`\n- `get_nonzero`\n\n### Functions\n\n#### cosine_similarity\n\n```python\ncosine_similarity(vec1: Dict[str, float], vec2: Dict[str, float]) -> float\n```\n\nCompute cosine similarity between two sparse vectors.\n\n#### SparseMatrix.set\n\n```python\nSparseMatrix.set(self, row: int, col: int, value: float) -> None\n```\n\nSet value at (row, col).\n\n#### SparseMatrix.get\n\n```python\nSparseMatrix.get(self, row: int, col: int) -> float\n```\n\nGet value at (row, col).\n\n#### SparseMatrix.multiply_transpose\n\n```python\nSparseMatrix.multiply_transpose(self) -> 'SparseMatrix'\n```\n\nMultiply this matrix by its transpose: M * M^T\n\n#### SparseMatrix.get_nonzero\n\n```python\nSparseMatrix.get_nonzero(self) -> List[Tuple[int, int, float]]\n```\n\nGet all non-zero entries.\n\n### Dependencies\n\n**Standard Library:**\n\n- `collections.defaultdict`\n- `math`\n- `typing.Dict`\n- `typing.List`\n- `typing.Tuple`\n\n\n\n---\n\n*This chapter is part of [The Cortical Chronicles](../README.md), a self-documenting book generated by the Cortical Text Processor.*\n",
      "keywords": [
        "float",
        "dict",
        "int",
        "str",
        "layers",
        "hierarchicallayer",
        "python",
        "corticallayer",
        "algorithms",
        "clustering"
      ]
    },
    {
      "id": "02-architecture/mod-configuration",
      "title": "Configuration",
      "content": "# Configuration\n\nConfiguration management and validation.\n\n## Modules\n\n- **config.py**: Configuration Module\n- **constants.py**: Centralized constants for the Cortical Text Processor.\n- **validation.py**: Validation Module\n\n\n## config.py\n\nConfiguration Module\n====================\n\nCentralized configuration for the Cortical Text Processor.\n\nThis module provides a dataclass-based configuration system that allows\nusers to customize algori...\n\n\n### Classes\n\n#### CorticalConfig\n\nConfiguration settings for the Cortical Text Processor.\n\n**Methods:**\n\n- `copy`\n- `to_dict`\n- `from_dict`\n\n### Functions\n\n#### get_default_config\n\n```python\nget_default_config() -> CorticalConfig\n```\n\nGet a new instance of the default configuration.\n\n#### CorticalConfig.copy\n\n```python\nCorticalConfig.copy(self) -> 'CorticalConfig'\n```\n\nCreate a copy of this configuration.\n\n#### CorticalConfig.to_dict\n\n```python\nCorticalConfig.to_dict(self) -> Dict\n```\n\nConvert configuration to a dictionary for serialization.\n\n#### CorticalConfig.from_dict\n\n```python\nCorticalConfig.from_dict(cls, data: Dict) -> 'CorticalConfig'\n```\n\nCreate configuration from a dictionary.\n\n### Dependencies\n\n**Standard Library:**\n\n- `dataclasses.dataclass`\n- `dataclasses.field`\n- `math`\n- `typing.Dict`\n- `typing.FrozenSet`\n- ... and 1 more\n\n\n\n## constants.py\n\nCentralized constants for the Cortical Text Processor.\n\nThis module provides a single source of truth for constants used across\nmultiple modules, preventing drift and inconsistencies.\n\nTask #96: Centr...\n\n\n### Dependencies\n\n**Standard Library:**\n\n- `typing.Dict`\n- `typing.FrozenSet`\n\n\n\n## validation.py\n\nValidation Module\n=================\n\nInput validation utilities and decorators for the Cortical Text Processor.\n\nThis module provides reusable validators and decorators to ensure\nparameters are valid ...\n\n\n### Functions\n\n#### validate_non_empty_string\n\n```python\nvalidate_non_empty_string(value: Any, param_name: str) -> None\n```\n\nValidate that a value is a non-empty string.\n\n#### validate_positive_int\n\n```python\nvalidate_positive_int(value: Any, param_name: str) -> None\n```\n\nValidate that a value is a positive integer.\n\n#### validate_non_negative_int\n\n```python\nvalidate_non_negative_int(value: Any, param_name: str) -> None\n```\n\nValidate that a value is a non-negative integer.\n\n#### validate_range\n\n```python\nvalidate_range(value: Any, param_name: str, min_val: Optional[float] = None, max_val: Optional[float] = None, inclusive: bool = True) -> None\n```\n\nValidate that a numeric value is within a specified range.\n\n#### validate_params\n\n```python\nvalidate_params(**validators: Callable[[Any], None]) -> Callable[[F], F]\n```\n\nDecorator to validate function parameters.\n\n#### marks_stale\n\n```python\nmarks_stale(*computation_types: str) -> Callable[[F], F]\n```\n\nDecorator to mark computations as stale after method execution.\n\n#### marks_fresh\n\n```python\nmarks_fresh(*computation_types: str) -> Callable[[F], F]\n```\n\nDecorator to mark computations as fresh after method execution.\n\n### Dependencies\n\n**Standard Library:**\n\n- `functools.wraps`\n- `inspect`\n- `typing.Any`\n- `typing.Callable`\n- `typing.Optional`\n- ... and 2 more\n\n\n\n---\n\n*This chapter is part of [The Cortical Chronicles](../README.md), a self-documenting book generated by the Cortical Text Processor.*\n",
      "keywords": [
        "configuration",
        "python",
        "corticalconfig",
        "value",
        "module",
        "cortical",
        "typing",
        "none",
        "validation",
        "text"
      ]
    },
    {
      "id": "02-architecture/mod-data-structures",
      "title": "Data Structures",
      "content": "# Data Structures\n\nFundamental data structures used throughout the system.\n\n## Modules\n\n- **layers.py**: Layers Module\n- **minicolumn.py**: Minicolumn Module\n- **types.py**: Type Aliases for the Cortical Text Processor.\n\n\n## layers.py\n\nLayers Module\n=============\n\nDefines the hierarchical layer structure inspired by the visual cortex.\n\nThe neocortex processes information through a hierarchy of layers,\neach extracting progressively m...\n\n\n### Classes\n\n#### CorticalLayer\n\nEnumeration of cortical processing layers.\n\n**Methods:**\n\n- `description`\n- `analogy`\n\n#### HierarchicalLayer\n\nA layer in the cortical hierarchy containing minicolumns.\n\n**Methods:**\n\n- `get_or_create_minicolumn`\n- `get_minicolumn`\n- `get_by_id`\n- `remove_minicolumn`\n- `column_count`\n- `total_connections`\n- `average_activation`\n- `activation_range`\n- `sparsity`\n- `top_by_pagerank`\n- `top_by_tfidf`\n- `top_by_activation`\n- `to_dict`\n- `from_dict`\n\n### Functions\n\n#### CorticalLayer.description\n\n```python\nCorticalLayer.description(self) -> str\n```\n\nHuman-readable description of this layer.\n\n#### CorticalLayer.analogy\n\n```python\nCorticalLayer.analogy(self) -> str\n```\n\nVisual cortex analogy for this layer.\n\n#### HierarchicalLayer.get_or_create_minicolumn\n\n```python\nHierarchicalLayer.get_or_create_minicolumn(self, content: str) -> Minicolumn\n```\n\nGet existing minicolumn or create new one.\n\n#### HierarchicalLayer.get_minicolumn\n\n```python\nHierarchicalLayer.get_minicolumn(self, content: str) -> Optional[Minicolumn]\n```\n\nGet a minicolumn by content, or None if not found.\n\n#### HierarchicalLayer.get_by_id\n\n```python\nHierarchicalLayer.get_by_id(self, col_id: str) -> Optional[Minicolumn]\n```\n\nGet a minicolumn by its ID in O(1) time.\n\n#### HierarchicalLayer.remove_minicolumn\n\n```python\nHierarchicalLayer.remove_minicolumn(self, content: str) -> bool\n```\n\nRemove a minicolumn from this layer.\n\n#### HierarchicalLayer.column_count\n\n```python\nHierarchicalLayer.column_count(self) -> int\n```\n\nReturn the number of minicolumns in this layer.\n\n#### HierarchicalLayer.total_connections\n\n```python\nHierarchicalLayer.total_connections(self) -> int\n```\n\nReturn total number of lateral connections in this layer.\n\n#### HierarchicalLayer.average_activation\n\n```python\nHierarchicalLayer.average_activation(self) -> float\n```\n\nCalculate average activation across all minicolumns.\n\n#### HierarchicalLayer.activation_range\n\n```python\nHierarchicalLayer.activation_range(self) -> tuple\n```\n\nReturn (min, max) activation values.\n\n#### HierarchicalLayer.sparsity\n\n```python\nHierarchicalLayer.sparsity(self, threshold_fraction: float = 0.5) -> float\n```\n\nCalculate sparsity (fraction of columns with below-average activation).\n\n#### HierarchicalLayer.top_by_pagerank\n\n```python\nHierarchicalLayer.top_by_pagerank(self, n: int = 10) -> list\n```\n\nGet top minicolumns by PageRank score.\n\n#### HierarchicalLayer.top_by_tfidf\n\n```python\nHierarchicalLayer.top_by_tfidf(self, n: int = 10) -> list\n```\n\nGet top minicolumns by TF-IDF score.\n\n#### HierarchicalLayer.top_by_activation\n\n```python\nHierarchicalLayer.top_by_activation(self, n: int = 10) -> list\n```\n\nGet top minicolumns by activation level.\n\n#### HierarchicalLayer.to_dict\n\n```python\nHierarchicalLayer.to_dict(self) -> Dict\n```\n\nConvert layer to dictionary for serialization.\n\n#### HierarchicalLayer.from_dict\n\n```python\nHierarchicalLayer.from_dict(cls, data: Dict) -> 'HierarchicalLayer'\n```\n\nCreate a layer from dictionary representation.\n\n### Dependencies\n\n**Standard Library:**\n\n- `enum.IntEnum`\n- `minicolumn.Minicolumn`\n- `typing.Dict`\n- `typing.Iterator`\n- `typing.Optional`\n\n\n\n## minicolumn.py\n\nMinicolumn Module\n=================\n\nCore data structure representing a cortical minicolumn.\n\nIn the neocortex, minicolumns are vertical structures containing\n~80-100 neurons that respond to similar f...\n\n\n### Classes\n\n#### Edge\n\nTyped edge with metadata for ConceptNet-style graph representation.\n\n**Methods:**\n\n- `to_dict`\n- `from_dict`\n\n#### Minicolumn\n\nA minicolumn represents a single concept/feature at a given hierarchy level.\n\n**Methods:**\n\n- `lateral_connections`\n- `lateral_connections`\n- `add_lateral_connection`\n- `add_lateral_connections_batch`\n- `set_lateral_connection_weight`\n- `add_typed_connection`\n- `get_typed_connection`\n- `get_connections_by_type`\n- `get_connections_by_source`\n- `add_feedforward_connection`\n- `add_feedback_connection`\n- `connection_count`\n- `top_connections`\n- `to_dict`\n- `from_dict`\n\n### Functions\n\n#### Edge.to_dict\n\n```python\nEdge.to_dict(self) -> Dict\n```\n\nConvert to dictionary for serialization.\n\n#### Edge.from_dict\n\n```python\nEdge.from_dict(cls, data: Dict) -> 'Edge'\n```\n\nCreate an Edge from dictionary representation.\n\n#### Minicolumn.lateral_connections\n\n```python\nMinicolumn.lateral_connections(self, value: Dict[str, float]) -> None\n```\n\nSet lateral connections from a dictionary (for deserialization).\n\n#### Minicolumn.add_lateral_connection\n\n```python\nMinicolumn.add_lateral_connection(self, target_id: str, weight: float = 1.0) -> None\n```\n\nAdd or strengthen a lateral connection to another column.\n\n#### Minicolumn.add_lateral_connections_batch\n\n```python\nMinicolumn.add_lateral_connections_batch(self, connections: Dict[str, float]) -> None\n```\n\nAdd or strengthen multiple lateral connections at once.\n\n#### Minicolumn.set_lateral_connection_weight\n\n```python\nMinicolumn.set_lateral_connection_weight(self, target_id: str, weight: float) -> None\n```\n\nSet the weight of a lateral connection directly (not additive).\n\n#### Minicolumn.add_typed_connection\n\n```python\nMinicolumn.add_typed_connection(self, target_id: str, weight: float = 1.0, relation_type: str = 'co_occurrence', confidence: float = 1.0, source: str = 'corpus') -> None\n```\n\nAdd or update a typed connection with metadata.\n\n#### Minicolumn.get_typed_connection\n\n```python\nMinicolumn.get_typed_connection(self, target_id: str) -> Optional[Edge]\n```\n\nGet a typed connection by target ID.\n\n#### Minicolumn.get_connections_by_type\n\n```python\nMinicolumn.get_connections_by_type(self, relation_type: str) -> List[Edge]\n```\n\nGet all typed connections with a specific relation type.\n\n#### Minicolumn.get_connections_by_source\n\n```python\nMinicolumn.get_connections_by_source(self, source: str) -> List[Edge]\n```\n\nGet all typed connections from a specific source.\n\n#### Minicolumn.add_feedforward_connection\n\n```python\nMinicolumn.add_feedforward_connection(self, target_id: str, weight: float = 1.0) -> None\n```\n\nAdd or strengthen a feedforward connection to a lower layer column.\n\n#### Minicolumn.add_feedback_connection\n\n```python\nMinicolumn.add_feedback_connection(self, target_id: str, weight: float = 1.0) -> None\n```\n\nAdd or strengthen a feedback connection to a higher layer column.\n\n#### Minicolumn.connection_count\n\n```python\nMinicolumn.connection_count(self) -> int\n```\n\nReturn the number of lateral connections.\n\n#### Minicolumn.top_connections\n\n```python\nMinicolumn.top_connections(self, n: int = 5) -> list\n```\n\nGet the strongest lateral connections.\n\n#### Minicolumn.to_dict\n\n```python\nMinicolumn.to_dict(self) -> Dict\n```\n\nConvert to dictionary for serialization.\n\n#### Minicolumn.from_dict\n\n```python\nMinicolumn.from_dict(cls, data: Dict) -> 'Minicolumn'\n```\n\nCreate a minicolumn from dictionary representation.\n\n### Dependencies\n\n**Standard Library:**\n\n- `dataclasses.asdict`\n- `dataclasses.dataclass`\n- `dataclasses.field`\n- `typing.Dict`\n- `typing.List`\n- ... and 2 more\n\n\n\n## types.py\n\nType Aliases for the Cortical Text Processor.\n\nThis module provides type aliases for complex return types used throughout\nthe library, making function signatures more readable and maintainable.\n\nTask ...\n\n\n### Dependencies\n\n**Standard Library:**\n\n- `typing.Any`\n- `typing.Dict`\n- `typing.List`\n- `typing.Optional`\n- `typing.Tuple`\n\n\n\n---\n\n*This chapter is part of [The Cortical Chronicles](../README.md), a self-documenting book generated by the Cortical Text Processor.*\n",
      "keywords": [
        "minicolumn",
        "python",
        "hierarchicallayer",
        "self",
        "str",
        "layer",
        "float",
        "dict",
        "edge",
        "get"
      ]
    },
    {
      "id": "02-architecture/mod-nlp",
      "title": "NLP Components",
      "content": "# NLP Components\n\nNatural language processing components for tokenization and semantics.\n\n## Modules\n\n- **embeddings.py**: Embeddings Module\n- **semantics.py**: Semantics Module\n- **tokenizer.py**: Tokenizer Module\n\n\n## embeddings.py\n\nEmbeddings Module\n=================\n\nGraph-based embeddings for the cortical network.\n\nImplements three methods for computing term embeddings from the\nconnection graph structure:\n1. Adjacency: Direct ...\n\n\n### Functions\n\n#### compute_graph_embeddings\n\n```python\ncompute_graph_embeddings(layers: Dict[CorticalLayer, HierarchicalLayer], dimensions: int = 64, method: str = 'adjacency', max_terms: Optional[int] = None) -> Tuple[Dict[str, List[float]], Dict[str, Any]]\n```\n\nCompute embeddings for tokens based on graph structure.\n\n#### embedding_similarity\n\n```python\nembedding_similarity(embeddings: Dict[str, List[float]], term1: str, term2: str) -> float\n```\n\nCompute cosine similarity between two term embeddings.\n\n#### find_similar_by_embedding\n\n```python\nfind_similar_by_embedding(embeddings: Dict[str, List[float]], term: str, top_n: int = 10) -> List[Tuple[str, float]]\n```\n\nFind terms most similar to a given term by embedding.\n\n### Dependencies\n\n**Standard Library:**\n\n- `collections.defaultdict`\n- `layers.CorticalLayer`\n- `layers.HierarchicalLayer`\n- `math`\n- `random`\n- ... and 5 more\n\n\n\n## semantics.py\n\nSemantics Module\n================\n\nCorpus-derived semantic relations and retrofitting.\n\nExtracts semantic relationships from co-occurrence patterns,\nthen uses them to adjust connection weights (retrof...\n\n\n### Functions\n\n#### extract_pattern_relations\n\n```python\nextract_pattern_relations(documents: Dict[str, str], valid_terms: Set[str], min_confidence: float = 0.5) -> List[Tuple[str, str, str, float]]\n```\n\nExtract semantic relations using pattern matching on document text.\n\n#### get_pattern_statistics\n\n```python\nget_pattern_statistics(relations: List[Tuple[str, str, str, float]]) -> Dict[str, Any]\n```\n\nGet statistics about extracted pattern-based relations.\n\n#### extract_corpus_semantics\n\n```python\nextract_corpus_semantics(layers: Dict[CorticalLayer, HierarchicalLayer], documents: Dict[str, str], tokenizer, window_size: int = 5, min_cooccurrence: int = 2, use_pattern_extraction: bool = True, min_pattern_confidence: float = 0.6, max_similarity_pairs: int = 100000, min_context_keys: int = 3) -> List[Tuple[str, str, str, float]]\n```\n\nExtract semantic relations from corpus co-occurrence patterns.\n\n#### retrofit_connections\n\n```python\nretrofit_connections(layers: Dict[CorticalLayer, HierarchicalLayer], semantic_relations: List[Tuple[str, str, str, float]], iterations: int = 10, alpha: float = 0.3) -> Dict[str, Any]\n```\n\nRetrofit lateral connections using semantic relations.\n\n#### retrofit_embeddings\n\n```python\nretrofit_embeddings(embeddings: Dict[str, List[float]], semantic_relations: List[Tuple[str, str, str, float]], iterations: int = 10, alpha: float = 0.4) -> Dict[str, Any]\n```\n\nRetrofit embeddings using semantic relations.\n\n#### get_relation_type_weight\n\n```python\nget_relation_type_weight(relation_type: str) -> float\n```\n\nGet the weight for a relation type.\n\n#### build_isa_hierarchy\n\n```python\nbuild_isa_hierarchy(semantic_relations: List[Tuple[str, str, str, float]]) -> Tuple[Dict[str, Set[str]], Dict[str, Set[str]]]\n```\n\nBuild IsA parent-child hierarchy from semantic relations.\n\n#### get_ancestors\n\n```python\nget_ancestors(term: str, parents: Dict[str, Set[str]], max_depth: int = 10) -> Dict[str, int]\n```\n\nGet all ancestors of a term with their depth in the hierarchy.\n\n#### get_descendants\n\n```python\nget_descendants(term: str, children: Dict[str, Set[str]], max_depth: int = 10) -> Dict[str, int]\n```\n\nGet all descendants of a term with their depth in the hierarchy.\n\n#### inherit_properties\n\n```python\ninherit_properties(semantic_relations: List[Tuple[str, str, str, float]], decay_factor: float = 0.7, max_depth: int = 5) -> Dict[str, Dict[str, Tuple[float, str, int]]]\n```\n\nCompute inherited properties for all terms based on IsA hierarchy.\n\n#### compute_property_similarity\n\n```python\ncompute_property_similarity(term1: str, term2: str, inherited_properties: Dict[str, Dict[str, Tuple[float, str, int]]], direct_properties: Optional[Dict[str, Dict[str, float]]] = None) -> float\n```\n\nCompute similarity between terms based on shared properties (direct + inherited).\n\n#### apply_inheritance_to_connections\n\n```python\napply_inheritance_to_connections(layers: Dict[CorticalLayer, HierarchicalLayer], inherited_properties: Dict[str, Dict[str, Tuple[float, str, int]]], boost_factor: float = 0.3) -> Dict[str, Any]\n```\n\nBoost lateral connections between terms that share inherited properties.\n\n### Dependencies\n\n**Standard Library:**\n\n- `collections.defaultdict`\n- `constants.RELATION_WEIGHTS`\n- `copy`\n- `layers.CorticalLayer`\n- `layers.HierarchicalLayer`\n- ... and 10 more\n\n\n\n## tokenizer.py\n\nTokenizer Module\n================\n\nText tokenization with stemming and word variant support.\n\nLike early visual processing, the tokenizer extracts basic features\n(words) from raw input, filtering nois...\n\n\n### Classes\n\n#### Tokenizer\n\nText tokenizer with stemming and word variant support.\n\n**Methods:**\n\n- `tokenize`\n- `extract_ngrams`\n- `stem`\n- `get_word_variants`\n- `add_word_mapping`\n\n### Functions\n\n#### split_identifier\n\n```python\nsplit_identifier(identifier: str) -> List[str]\n```\n\nSplit a code identifier into component words.\n\n#### Tokenizer.tokenize\n\n```python\nTokenizer.tokenize(self, text: str, split_identifiers: Optional[bool] = None) -> List[str]\n```\n\nExtract tokens from text.\n\n#### Tokenizer.extract_ngrams\n\n```python\nTokenizer.extract_ngrams(self, tokens: List[str], n: int = 2) -> List[str]\n```\n\nExtract n-grams from token list.\n\n#### Tokenizer.stem\n\n```python\nTokenizer.stem(self, word: str) -> str\n```\n\nApply simple suffix stripping (Porter-lite stemming).\n\n#### Tokenizer.get_word_variants\n\n```python\nTokenizer.get_word_variants(self, word: str) -> List[str]\n```\n\nGet related words/variants for query expansion.\n\n#### Tokenizer.add_word_mapping\n\n```python\nTokenizer.add_word_mapping(self, word: str, variants: List[str]) -> None\n```\n\nAdd a custom word mapping for query expansion.\n\n### Dependencies\n\n**Standard Library:**\n\n- `re`\n- `typing.Dict`\n- `typing.List`\n- `typing.Optional`\n- `typing.Set`\n- ... and 1 more\n\n\n\n---\n\n*This chapter is part of [The Cortical Chronicles](../README.md), a self-documenting book generated by the Cortical Text Processor.*\n",
      "keywords": [
        "str",
        "dict",
        "float",
        "python",
        "list",
        "tokenizer",
        "int",
        "tuple",
        "embeddings",
        "term"
      ]
    },
    {
      "id": "02-architecture/mod-observability",
      "title": "Observability",
      "content": "# Observability\n\nMetrics collection and progress tracking.\n\n## Modules\n\n- **observability.py**: Observability Module\n- **progress.py**: Progress reporting infrastructure for long-running operations.\n- **results.py**: Result Dataclasses for Cortical Text Processor\n\n\n## observability.py\n\nObservability Module\n====================\n\nProvides timing hooks, metrics collection, and trace context for monitoring\nthe Cortical Text Processor's performance and operations.\n\nThis module follows th...\n\n\n### Classes\n\n#### MetricsCollector\n\nCollects and aggregates timing and count metrics for operations.\n\n**Methods:**\n\n- `record_timing`\n- `record_count`\n- `get_operation_stats`\n- `get_all_stats`\n- `get_trace`\n- `reset`\n- `enable`\n- `disable`\n- `trace_context`\n- `get_summary`\n\n#### TraceContext\n\nContext for request tracing across operations.\n\n**Methods:**\n\n- `elapsed_ms`\n\n### Functions\n\n#### timed\n\n```python\ntimed(operation_name: Optional[str] = None, include_args: bool = False)\n```\n\nDecorator for timing method calls and recording to metrics.\n\n#### measure_time\n\n```python\nmeasure_time(func: Callable) -> Callable\n```\n\nSimple timing decorator that logs execution time.\n\n#### get_global_metrics\n\n```python\nget_global_metrics() -> MetricsCollector\n```\n\nGet the global metrics collector instance.\n\n#### enable_global_metrics\n\n```python\nenable_global_metrics() -> None\n```\n\nEnable global metrics collection.\n\n#### disable_global_metrics\n\n```python\ndisable_global_metrics() -> None\n```\n\nDisable global metrics collection.\n\n#### reset_global_metrics\n\n```python\nreset_global_metrics() -> None\n```\n\nReset global metrics.\n\n#### MetricsCollector.record_timing\n\n```python\nMetricsCollector.record_timing(self, operation: str, duration_ms: float, trace_id: Optional[str] = None, context: Optional[Dict[str, Any]] = None) -> None\n```\n\nRecord a timing measurement for an operation.\n\n#### MetricsCollector.record_count\n\n```python\nMetricsCollector.record_count(self, metric_name: str, count: int = 1) -> None\n```\n\nRecord a simple count metric.\n\n#### MetricsCollector.get_operation_stats\n\n```python\nMetricsCollector.get_operation_stats(self, operation: str) -> Dict[str, Any]\n```\n\nGet statistics for a specific operation.\n\n#### MetricsCollector.get_all_stats\n\n```python\nMetricsCollector.get_all_stats(self) -> Dict[str, Dict[str, Any]]\n```\n\nGet statistics for all operations.\n\n#### MetricsCollector.get_trace\n\n```python\nMetricsCollector.get_trace(self, trace_id: str) -> List[tuple]\n```\n\nGet all operations recorded for a trace ID.\n\n#### MetricsCollector.reset\n\n```python\nMetricsCollector.reset(self) -> None\n```\n\nClear all collected metrics.\n\n#### MetricsCollector.enable\n\n```python\nMetricsCollector.enable(self) -> None\n```\n\nEnable metrics collection.\n\n#### MetricsCollector.disable\n\n```python\nMetricsCollector.disable(self) -> None\n```\n\nDisable metrics collection.\n\n#### MetricsCollector.trace_context\n\n```python\nMetricsCollector.trace_context(self, trace_id: str)\n```\n\nContext manager for tracing a block of operations.\n\n#### MetricsCollector.get_summary\n\n```python\nMetricsCollector.get_summary(self) -> str\n```\n\nGet a human-readable summary of all metrics.\n\n#### TraceContext.elapsed_ms\n\n```python\nTraceContext.elapsed_ms(self) -> float\n```\n\nGet elapsed time since trace started in milliseconds.\n\n### Dependencies\n\n**Standard Library:**\n\n- `collections.defaultdict`\n- `contextlib.contextmanager`\n- `functools`\n- `logging`\n- `time`\n- ... and 5 more\n\n\n\n## progress.py\n\nProgress reporting infrastructure for long-running operations.\n\nThis module provides a flexible progress reporting system that supports:\n- Console output with nice formatting\n- Custom callbacks for in...\n\n\n### Classes\n\n#### ProgressReporter\n\nProtocol for progress reporters.\n\n**Methods:**\n\n- `update`\n- `complete`\n\n#### ConsoleProgressReporter\n\nConsole-based progress reporter with nice formatting.\n\n**Methods:**\n\n- `update`\n- `complete`\n\n#### CallbackProgressReporter\n\nProgress reporter that calls a custom callback function.\n\n**Methods:**\n\n- `update`\n- `complete`\n\n#### SilentProgressReporter\n\nNo-op progress reporter for silent operation.\n\n**Methods:**\n\n- `update`\n- `complete`\n\n#### MultiPhaseProgress\n\nHelper for tracking progress across multiple sequential phases.\n\n**Methods:**\n\n- `start_phase`\n- `update`\n- `complete_phase`\n- `overall_progress`\n\n### Functions\n\n#### ProgressReporter.update\n\n```python\nProgressReporter.update(self, phase: str, percent: float, message: Optional[str] = None) -> None\n```\n\nUpdate progress for a specific phase.\n\n#### ProgressReporter.complete\n\n```python\nProgressReporter.complete(self, phase: str, message: Optional[str] = None) -> None\n```\n\nMark a phase as complete.\n\n#### ConsoleProgressReporter.update\n\n```python\nConsoleProgressReporter.update(self, phase: str, percent: float, message: Optional[str] = None) -> None\n```\n\nUpdate progress display.\n\n#### ConsoleProgressReporter.complete\n\n```python\nConsoleProgressReporter.complete(self, phase: str, message: Optional[str] = None) -> None\n```\n\nMark phase as complete and move to new line.\n\n#### CallbackProgressReporter.update\n\n```python\nCallbackProgressReporter.update(self, phase: str, percent: float, message: Optional[str] = None) -> None\n```\n\nCall callback with progress update.\n\n#### CallbackProgressReporter.complete\n\n```python\nCallbackProgressReporter.complete(self, phase: str, message: Optional[str] = None) -> None\n```\n\nCall callback with completion notification.\n\n#### SilentProgressReporter.update\n\n```python\nSilentProgressReporter.update(self, phase: str, percent: float, message: Optional[str] = None) -> None\n```\n\nDo nothing.\n\n#### SilentProgressReporter.complete\n\n```python\nSilentProgressReporter.complete(self, phase: str, message: Optional[str] = None) -> None\n```\n\nDo nothing.\n\n#### MultiPhaseProgress.start_phase\n\n```python\nMultiPhaseProgress.start_phase(self, phase: str) -> None\n```\n\nStart a new phase.\n\n#### MultiPhaseProgress.update\n\n```python\nMultiPhaseProgress.update(self, percent: float, message: Optional[str] = None) -> None\n```\n\nUpdate progress within current phase.\n\n#### MultiPhaseProgress.complete_phase\n\n```python\nMultiPhaseProgress.complete_phase(self, message: Optional[str] = None) -> None\n```\n\nMark current phase as complete.\n\n#### MultiPhaseProgress.overall_progress\n\n```python\nMultiPhaseProgress.overall_progress(self) -> float\n```\n\nGet overall progress across all phases (0-100).\n\n### Dependencies\n\n**Standard Library:**\n\n- `abc.ABC`\n- `abc.abstractmethod`\n- `sys`\n- `time`\n- `typing.Any`\n- ... and 4 more\n\n\n\n## results.py\n\nResult Dataclasses for Cortical Text Processor\n===============================================\n\nStrongly-typed result containers for query operations that provide\nIDE autocomplete and type checking su...\n\n\n### Classes\n\n#### DocumentMatch\n\nA document search result with relevance score.\n\n**Methods:**\n\n- `to_dict`\n- `to_tuple`\n- `from_tuple`\n- `from_dict`\n\n#### PassageMatch\n\nA passage retrieval result with text, location, and relevance score.\n\n**Methods:**\n\n- `to_dict`\n- `to_tuple`\n- `location`\n- `length`\n- `from_tuple`\n- `from_dict`\n\n#### QueryResult\n\nComplete query result with matches and metadata.\n\n**Methods:**\n\n- `to_dict`\n- `top_match`\n- `match_count`\n- `average_score`\n- `from_dict`\n\n### Functions\n\n#### convert_document_matches\n\n```python\nconvert_document_matches(results: List[tuple], metadata: Optional[Dict[str, Dict[str, Any]]] = None) -> List[DocumentMatch]\n```\n\nConvert list of (doc_id, score) tuples to DocumentMatch objects.\n\n#### convert_passage_matches\n\n```python\nconvert_passage_matches(results: List[tuple], metadata: Optional[Dict[str, Dict[str, Any]]] = None) -> List[PassageMatch]\n```\n\nConvert list of (doc_id, text, start, end, score) tuples to PassageMatch objects.\n\n#### DocumentMatch.to_dict\n\n```python\nDocumentMatch.to_dict(self) -> Dict[str, Any]\n```\n\nConvert to dictionary.\n\n#### DocumentMatch.to_tuple\n\n```python\nDocumentMatch.to_tuple(self) -> tuple\n```\n\nConvert to tuple format (doc_id, score).\n\n#### DocumentMatch.from_tuple\n\n```python\nDocumentMatch.from_tuple(cls, doc_id: str, score: float, metadata: Optional[Dict[str, Any]] = None) -> 'DocumentMatch'\n```\n\nCreate from tuple format (doc_id, score).\n\n#### DocumentMatch.from_dict\n\n```python\nDocumentMatch.from_dict(cls, data: Dict[str, Any]) -> 'DocumentMatch'\n```\n\nCreate from dictionary.\n\n#### PassageMatch.to_dict\n\n```python\nPassageMatch.to_dict(self) -> Dict[str, Any]\n```\n\nConvert to dictionary.\n\n#### PassageMatch.to_tuple\n\n```python\nPassageMatch.to_tuple(self) -> tuple\n```\n\nConvert to tuple format (doc_id, text, start, end, score).\n\n#### PassageMatch.location\n\n```python\nPassageMatch.location(self) -> str\n```\n\nGet citation-style location string.\n\n#### PassageMatch.length\n\n```python\nPassageMatch.length(self) -> int\n```\n\nGet passage length in characters.\n\n#### PassageMatch.from_tuple\n\n```python\nPassageMatch.from_tuple(cls, doc_id: str, text: str, start: int, end: int, score: float, metadata: Optional[Dict[str, Any]] = None) -> 'PassageMatch'\n```\n\nCreate from tuple format (doc_id, text, start, end, score).\n\n#### PassageMatch.from_dict\n\n```python\nPassageMatch.from_dict(cls, data: Dict[str, Any]) -> 'PassageMatch'\n```\n\nCreate from dictionary.\n\n#### QueryResult.to_dict\n\n```python\nQueryResult.to_dict(self) -> Dict[str, Any]\n```\n\nConvert to dictionary with nested match dicts.\n\n#### QueryResult.top_match\n\n```python\nQueryResult.top_match(self) -> Union[DocumentMatch, PassageMatch, None]\n```\n\nGet the highest-scoring match.\n\n#### QueryResult.match_count\n\n```python\nQueryResult.match_count(self) -> int\n```\n\nGet number of matches.\n\n#### QueryResult.average_score\n\n```python\nQueryResult.average_score(self) -> float\n```\n\nGet average relevance score across all matches.\n\n#### QueryResult.from_dict\n\n```python\nQueryResult.from_dict(cls, data: Dict[str, Any]) -> 'QueryResult'\n```\n\nCreate from dictionary.\n\n### Dependencies\n\n**Standard Library:**\n\n- `dataclasses.asdict`\n- `dataclasses.dataclass`\n- `dataclasses.field`\n- `typing.Any`\n- `typing.Dict`\n- ... and 3 more\n\n\n\n---\n\n*This chapter is part of [The Cortical Chronicles](../README.md), a self-documenting book generated by the Cortical Text Processor.*\n",
      "keywords": [
        "str",
        "python",
        "none",
        "self",
        "metricscollector",
        "update",
        "passagematch",
        "optional",
        "dict",
        "progress"
      ]
    },
    {
      "id": "02-architecture/mod-persistence",
      "title": "Persistence Layer",
      "content": "# Persistence Layer\n\nSave and load functionality for maintaining processor state.\n\n## Modules\n\n- **chunk_index.py**: Chunk-based indexing for git-compatible corpus storage.\n- **persistence.py**: Persistence Module\n- **state_storage.py**: Git-friendly State Storage Module\n\n\n## chunk_index.py\n\nChunk-based indexing for git-compatible corpus storage.\n\nThis module provides append-only, time-stamped JSON chunks that can be\nsafely committed to git without merge conflicts. Each indexing session\nc...\n\n\n### Classes\n\n#### ChunkOperation\n\nA single operation in a chunk (add, modify, or delete).\n\n**Methods:**\n\n- `to_dict`\n- `from_dict`\n\n#### Chunk\n\nA chunk containing operations from a single indexing session.\n\n**Methods:**\n\n- `to_dict`\n- `from_dict`\n- `get_filename`\n\n#### ChunkWriter\n\nWrites indexing session changes to timestamped JSON chunks.\n\n**Methods:**\n\n- `add_document`\n- `modify_document`\n- `delete_document`\n- `has_operations`\n- `save`\n\n#### ChunkLoader\n\nLoads and combines chunks to rebuild document state.\n\n**Methods:**\n\n- `get_chunk_files`\n- `load_chunk`\n- `load_all`\n- `get_documents`\n- `get_mtimes`\n- `get_metadata`\n- `get_chunks`\n- `compute_hash`\n- `is_cache_valid`\n- `save_cache_hash`\n- `get_stats`\n\n#### ChunkCompactor\n\nCompacts multiple chunk files into a single file.\n\n**Methods:**\n\n- `compact`\n\n### Functions\n\n#### get_changes_from_manifest\n\n```python\nget_changes_from_manifest(current_files: Dict[str, float], manifest: Dict[str, float]) -> Tuple[List[str], List[str], List[str]]\n```\n\nCompare current files to manifest to find changes.\n\n#### ChunkOperation.to_dict\n\n```python\nChunkOperation.to_dict(self) -> Dict[str, Any]\n```\n\nConvert to dictionary for JSON serialization.\n\n#### ChunkOperation.from_dict\n\n```python\nChunkOperation.from_dict(cls, d: Dict[str, Any]) -> 'ChunkOperation'\n```\n\nCreate from dictionary.\n\n#### Chunk.to_dict\n\n```python\nChunk.to_dict(self) -> Dict[str, Any]\n```\n\nConvert to dictionary for JSON serialization.\n\n#### Chunk.from_dict\n\n```python\nChunk.from_dict(cls, d: Dict[str, Any]) -> 'Chunk'\n```\n\nCreate from dictionary.\n\n#### Chunk.get_filename\n\n```python\nChunk.get_filename(self) -> str\n```\n\nGenerate filename for this chunk.\n\n#### ChunkWriter.add_document\n\n```python\nChunkWriter.add_document(self, doc_id: str, content: str, mtime: Optional[float] = None, metadata: Optional[Dict[str, Any]] = None)\n```\n\nRecord an add operation.\n\n#### ChunkWriter.modify_document\n\n```python\nChunkWriter.modify_document(self, doc_id: str, content: str, mtime: Optional[float] = None, metadata: Optional[Dict[str, Any]] = None)\n```\n\nRecord a modify operation.\n\n#### ChunkWriter.delete_document\n\n```python\nChunkWriter.delete_document(self, doc_id: str)\n```\n\nRecord a delete operation.\n\n#### ChunkWriter.has_operations\n\n```python\nChunkWriter.has_operations(self) -> bool\n```\n\nCheck if any operations were recorded.\n\n#### ChunkWriter.save\n\n```python\nChunkWriter.save(self, warn_size_kb: int = DEFAULT_WARN_SIZE_KB) -> Optional[Path]\n```\n\nSave chunk to file.\n\n#### ChunkLoader.get_chunk_files\n\n```python\nChunkLoader.get_chunk_files(self) -> List[Path]\n```\n\nGet all chunk files sorted by timestamp.\n\n#### ChunkLoader.load_chunk\n\n```python\nChunkLoader.load_chunk(self, filepath: Path) -> Chunk\n```\n\nLoad a single chunk file.\n\n#### ChunkLoader.load_all\n\n```python\nChunkLoader.load_all(self) -> Dict[str, str]\n```\n\nLoad all chunks and replay operations to get current document state.\n\n#### ChunkLoader.get_documents\n\n```python\nChunkLoader.get_documents(self) -> Dict[str, str]\n```\n\nGet loaded documents (calls load_all if needed).\n\n#### ChunkLoader.get_mtimes\n\n```python\nChunkLoader.get_mtimes(self) -> Dict[str, float]\n```\n\nGet document modification times.\n\n#### ChunkLoader.get_metadata\n\n```python\nChunkLoader.get_metadata(self) -> Dict[str, Dict[str, Any]]\n```\n\nGet document metadata (doc_type, headings, etc.).\n\n#### ChunkLoader.get_chunks\n\n```python\nChunkLoader.get_chunks(self) -> List[Chunk]\n```\n\nGet loaded chunks.\n\n#### ChunkLoader.compute_hash\n\n```python\nChunkLoader.compute_hash(self) -> str\n```\n\nCompute hash of current document state.\n\n#### ChunkLoader.is_cache_valid\n\n```python\nChunkLoader.is_cache_valid(self, cache_path: str, cache_hash_path: Optional[str] = None) -> bool\n```\n\nCheck if pkl cache is valid for current chunk state.\n\n#### ChunkLoader.save_cache_hash\n\n```python\nChunkLoader.save_cache_hash(self, cache_path: str, cache_hash_path: Optional[str] = None)\n```\n\nSave current document hash for cache validation.\n\n#### ChunkLoader.get_stats\n\n```python\nChunkLoader.get_stats(self) -> Dict[str, Any]\n```\n\nGet statistics about loaded chunks.\n\n#### ChunkCompactor.compact\n\n```python\nChunkCompactor.compact(self, before: Optional[str] = None, keep_recent: int = 0, dry_run: bool = False) -> Dict[str, Any]\n```\n\nCompact chunks into a single chunk.\n\n### Dependencies\n\n**Standard Library:**\n\n- `dataclasses.asdict`\n- `dataclasses.dataclass`\n- `dataclasses.field`\n- `datetime.datetime`\n- `hashlib`\n- ... and 11 more\n\n\n\n## persistence.py\n\nPersistence Module\n==================\n\nSave and load functionality for the cortical processor.\n\nSupports:\n- Pickle serialization for full state\n- JSON export for graph visualization\n- Incremental upda...\n\n\n### Classes\n\n#### SignatureVerificationError\n\nRaised when HMAC signature verification fails.\n\n### Functions\n\n#### save_processor\n\n```python\nsave_processor(filepath: str, layers: Dict[CorticalLayer, HierarchicalLayer], documents: Dict[str, str], document_metadata: Optional[Dict[str, Dict[str, Any]]] = None, embeddings: Optional[Dict[str, list]] = None, semantic_relations: Optional[list] = None, metadata: Optional[Dict] = None, verbose: bool = True, format: str = 'pickle', signing_key: Optional[bytes] = None) -> None\n```\n\nSave processor state to a file.\n\n#### load_processor\n\n```python\nload_processor(filepath: str, verbose: bool = True, format: Optional[str] = None, verify_key: Optional[bytes] = None) -> tuple\n```\n\nLoad processor state from a file.\n\n#### export_graph_json\n\n```python\nexport_graph_json(filepath: str, layers: Dict[CorticalLayer, HierarchicalLayer], layer_filter: Optional[CorticalLayer] = None, min_weight: float = 0.0, max_nodes: int = 500, verbose: bool = True) -> Dict\n```\n\nExport graph structure as JSON for visualization.\n\n#### export_embeddings_json\n\n```python\nexport_embeddings_json(filepath: str, embeddings: Dict[str, list], metadata: Optional[Dict] = None) -> None\n```\n\nExport embeddings as JSON.\n\n#### load_embeddings_json\n\n```python\nload_embeddings_json(filepath: str) -> Dict[str, list]\n```\n\nLoad embeddings from JSON.\n\n#### export_semantic_relations_json\n\n```python\nexport_semantic_relations_json(filepath: str, relations: list) -> None\n```\n\nExport semantic relations as JSON.\n\n#### load_semantic_relations_json\n\n```python\nload_semantic_relations_json(filepath: str) -> list\n```\n\nLoad semantic relations from JSON.\n\n#### get_state_summary\n\n```python\nget_state_summary(layers: Dict[CorticalLayer, HierarchicalLayer], documents: Dict[str, str]) -> Dict\n```\n\nGet a summary of the current processor state.\n\n#### export_conceptnet_json\n\n```python\nexport_conceptnet_json(filepath: str, layers: Dict[CorticalLayer, HierarchicalLayer], semantic_relations: Optional[list] = None, include_cross_layer: bool = True, include_typed_edges: bool = True, min_weight: float = 0.0, min_confidence: float = 0.0, max_nodes_per_layer: int = 100, verbose: bool = True) -> Dict[str, Any]\n```\n\nExport ConceptNet-style graph for visualization.\n\n### Dependencies\n\n**Standard Library:**\n\n- `hashlib`\n- `hmac`\n- `json`\n- `layers.CorticalLayer`\n- `layers.HierarchicalLayer`\n- ... and 9 more\n\n\n\n## state_storage.py\n\nGit-friendly State Storage Module\n=================================\n\nReplaces pickle-based persistence with JSON files that:\n- Can be diff'd and reviewed in git\n- Won't cause merge conflicts\n- Support...\n\n\n### Classes\n\n#### StateManifest\n\nManifest file tracking state version and component checksums.\n\n**Methods:**\n\n- `to_dict`\n- `from_dict`\n- `update_checksum`\n\n#### StateWriter\n\nWrites processor state to git-friendly JSON files.\n\n**Methods:**\n\n- `save_layer`\n- `save_documents`\n- `save_semantic_relations`\n- `save_embeddings`\n- `save_manifest`\n- `save_all`\n\n#### StateLoader\n\nLoads processor state from git-friendly JSON files.\n\n**Methods:**\n\n- `exists`\n- `load_manifest`\n- `validate_checksum`\n- `load_layer`\n- `load_documents`\n- `load_semantic_relations`\n- `load_embeddings`\n- `load_all`\n- `get_stats`\n\n### Functions\n\n#### migrate_pkl_to_json\n\n```python\nmigrate_pkl_to_json(pkl_path: str, json_dir: str, verbose: bool = True) -> bool\n```\n\nMigrate a pickle file to git-friendly JSON format.\n\n#### StateManifest.to_dict\n\n```python\nStateManifest.to_dict(self) -> Dict[str, Any]\n```\n\nConvert to dictionary for JSON serialization.\n\n#### StateManifest.from_dict\n\n```python\nStateManifest.from_dict(cls, data: Dict[str, Any]) -> 'StateManifest'\n```\n\nCreate manifest from dictionary.\n\n#### StateManifest.update_checksum\n\n```python\nStateManifest.update_checksum(self, component: str, content: str) -> bool\n```\n\nUpdate checksum for a component.\n\n#### StateWriter.save_layer\n\n```python\nStateWriter.save_layer(self, layer: HierarchicalLayer, force: bool = False) -> bool\n```\n\nSave a single layer to its JSON file.\n\n#### StateWriter.save_documents\n\n```python\nStateWriter.save_documents(self, documents: Dict[str, str], document_metadata: Optional[Dict[str, Dict[str, Any]]] = None, force: bool = False) -> bool\n```\n\nSave documents and metadata.\n\n#### StateWriter.save_semantic_relations\n\n```python\nStateWriter.save_semantic_relations(self, relations: List[Tuple], force: bool = False) -> bool\n```\n\nSave semantic relations.\n\n#### StateWriter.save_embeddings\n\n```python\nStateWriter.save_embeddings(self, embeddings: Dict[str, List[float]], force: bool = False) -> bool\n```\n\nSave graph embeddings.\n\n#### StateWriter.save_manifest\n\n```python\nStateWriter.save_manifest(self) -> None\n```\n\nSave the manifest file.\n\n#### StateWriter.save_all\n\n```python\nStateWriter.save_all(self, layers: Dict[CorticalLayer, HierarchicalLayer], documents: Dict[str, str], document_metadata: Optional[Dict[str, Dict[str, Any]]] = None, embeddings: Optional[Dict[str, List[float]]] = None, semantic_relations: Optional[List[Tuple]] = None, stale_computations: Optional[Set[str]] = None, force: bool = False, verbose: bool = True) -> Dict[str, bool]\n```\n\nSave all processor state.\n\n#### StateLoader.exists\n\n```python\nStateLoader.exists(self) -> bool\n```\n\nCheck if state directory exists and has manifest.\n\n#### StateLoader.load_manifest\n\n```python\nStateLoader.load_manifest(self) -> StateManifest\n```\n\nLoad the manifest file.\n\n#### StateLoader.validate_checksum\n\n```python\nStateLoader.validate_checksum(self, component: str, filepath: Path) -> bool\n```\n\nValidate a component's checksum.\n\n#### StateLoader.load_layer\n\n```python\nStateLoader.load_layer(self, level: int) -> HierarchicalLayer\n```\n\nLoad a single layer.\n\n#### StateLoader.load_documents\n\n```python\nStateLoader.load_documents(self) -> Tuple[Dict[str, str], Dict[str, Dict[str, Any]]]\n```\n\nLoad documents and metadata.\n\n#### StateLoader.load_semantic_relations\n\n```python\nStateLoader.load_semantic_relations(self) -> List[Tuple]\n```\n\nLoad semantic relations.\n\n#### StateLoader.load_embeddings\n\n```python\nStateLoader.load_embeddings(self) -> Dict[str, List[float]]\n```\n\nLoad graph embeddings.\n\n#### StateLoader.load_all\n\n```python\nStateLoader.load_all(self, validate: bool = True, verbose: bool = True) -> Tuple[Dict[CorticalLayer, HierarchicalLayer], Dict[str, str], Dict[str, Dict[str, Any]], Dict[str, List[float]], List[Tuple], Dict[str, Any]]\n```\n\nLoad all processor state.\n\n#### StateLoader.get_stats\n\n```python\nStateLoader.get_stats(self) -> Dict[str, Any]\n```\n\nGet statistics about stored state without loading everything.\n\n### Dependencies\n\n**Standard Library:**\n\n- `dataclasses.asdict`\n- `dataclasses.dataclass`\n- `dataclasses.field`\n- `datetime.datetime`\n- `hashlib`\n- ... and 13 more\n\n\n\n---\n\n*This chapter is part of [The Cortical Chronicles](../README.md), a self-documenting book generated by the Cortical Text Processor.*\n",
      "keywords": [
        "str",
        "dict",
        "python",
        "self",
        "bool",
        "none",
        "chunkloader",
        "optional",
        "chunk",
        "list"
      ]
    },
    {
      "id": "02-architecture/mod-processor",
      "title": "Core Processor",
      "content": "# Core Processor\n\nThe core processor orchestrates all text processing operations.\n\n## Modules\n\n- **__init__.py**: Cortical Text Processor - Main processor package.\n- **compute.py**: Compute methods: analysis, clustering, embeddings, semantic extraction.\n- **core.py**: Core processor functionality: initialization, staleness tracking, and layer management.\n- **documents.py**: Document management: processing, adding, removing, and metadata handling.\n- **introspection.py**: Introspection: state inspection, fingerprints, gaps, and summaries.\n- **persistence_api.py**: Persistence API: save, load, export, and migration methods.\n\n\n## __init__.py\n\nCortical Text Processor - Main processor package.\n\nThis package splits the monolithic processor.py into focused modules:\n- core.py: Initialization, staleness tracking, layer management\n- documents.py:...\n\n\n### Classes\n\n#### CorticalTextProcessor\n\nNeocortex-inspired text processing system.\n\n### Dependencies\n\n**Standard Library:**\n\n- `compute.ComputeMixin`\n- `core.CoreMixin`\n- `documents.DocumentsMixin`\n- `introspection.IntrospectionMixin`\n- `persistence_api.PersistenceMixin`\n- ... and 1 more\n\n\n\n## compute.py\n\nCompute methods: analysis, clustering, embeddings, semantic extraction.\n\nThis module contains all methods that perform computational analysis on the corpus,\nincluding PageRank, TF-IDF, clustering, and...\n\n\n### Classes\n\n#### ComputeMixin\n\nMixin providing computation functionality.\n\n**Methods:**\n\n- `recompute`\n- `compute_all`\n- `resume_from_checkpoint`\n- `propagate_activation`\n- `compute_importance`\n- `compute_semantic_importance`\n- `compute_hierarchical_importance`\n- `compute_tfidf`\n- `compute_bm25`\n- `compute_document_connections`\n- `compute_bigram_connections`\n- `build_concept_clusters`\n- `compute_clustering_quality`\n- `compute_concept_connections`\n- `extract_corpus_semantics`\n- `extract_pattern_relations`\n- `retrofit_connections`\n- `compute_property_inheritance`\n- `compute_property_similarity`\n- `compute_graph_embeddings`\n- `retrofit_embeddings`\n- `embedding_similarity`\n- `find_similar_by_embedding`\n\n### Functions\n\n#### ComputeMixin.recompute\n\n```python\nComputeMixin.recompute(self, level: str = 'stale', verbose: bool = True) -> Dict[str, bool]\n```\n\nRecompute specified analysis levels.\n\n#### ComputeMixin.compute_all\n\n```python\nComputeMixin.compute_all(self, verbose: bool = True, build_concepts: bool = True, pagerank_method: str = 'standard', connection_strategy: str = 'document_overlap', cluster_strictness: float = 1.0, bridge_weight: float = 0.0, progress_callback: Optional[ProgressReporter] = None, show_progress: bool = False, checkpoint_dir: Optional[str] = None, resume: bool = False) -> Dict[str, Any]\n```\n\nRun all computation steps.\n\n#### ComputeMixin.resume_from_checkpoint\n\n```python\nComputeMixin.resume_from_checkpoint(cls, checkpoint_dir: str, config: Optional['CorticalConfig'] = None, verbose: bool = True) -> 'CorticalTextProcessor'\n```\n\nResume processing from a checkpoint directory.\n\n#### ComputeMixin.propagate_activation\n\n```python\nComputeMixin.propagate_activation(self, iterations: int = 3, decay: float = 0.8, verbose: bool = True) -> None\n```\n\nNone\n\n#### ComputeMixin.compute_importance\n\n```python\nComputeMixin.compute_importance(self, verbose: bool = True) -> None\n```\n\nNone\n\n#### ComputeMixin.compute_semantic_importance\n\n```python\nComputeMixin.compute_semantic_importance(self, relation_weights: Optional[Dict[str, float]] = None, verbose: bool = True) -> Dict[str, Any]\n```\n\nCompute PageRank with semantic relation weighting.\n\n#### ComputeMixin.compute_hierarchical_importance\n\n```python\nComputeMixin.compute_hierarchical_importance(self, layer_iterations: int = 10, global_iterations: int = 5, cross_layer_damping: Optional[float] = None, verbose: bool = True) -> Dict[str, Any]\n```\n\nCompute PageRank with cross-layer propagation.\n\n#### ComputeMixin.compute_tfidf\n\n```python\nComputeMixin.compute_tfidf(self, verbose: bool = True) -> None\n```\n\nCompute document relevance scores using the configured algorithm.\n\n#### ComputeMixin.compute_bm25\n\n```python\nComputeMixin.compute_bm25(self, k1: float = None, b: float = None, verbose: bool = True) -> None\n```\n\nCompute BM25 scores for document relevance ranking.\n\n#### ComputeMixin.compute_document_connections\n\n```python\nComputeMixin.compute_document_connections(self, min_shared_terms: int = 3, verbose: bool = True) -> None\n```\n\nNone\n\n#### ComputeMixin.compute_bigram_connections\n\n```python\nComputeMixin.compute_bigram_connections(self, min_shared_docs: int = 1, component_weight: float = 0.5, chain_weight: float = 0.7, cooccurrence_weight: float = 0.3, max_bigrams_per_term: int = 100, max_bigrams_per_doc: int = 500, max_connections_per_bigram: int = 50, verbose: bool = True) -> Dict[str, Any]\n```\n\nBuild lateral connections between bigrams based on shared components and co-occurrence.\n\n#### ComputeMixin.build_concept_clusters\n\n```python\nComputeMixin.build_concept_clusters(self, min_cluster_size: Optional[int] = None, clustering_method: str = 'louvain', cluster_strictness: Optional[float] = None, bridge_weight: float = 0.0, resolution: Optional[float] = None, verbose: bool = True) -> Dict[int, List[str]]\n```\n\nBuild concept clusters from token layer.\n\n#### ComputeMixin.compute_clustering_quality\n\n```python\nComputeMixin.compute_clustering_quality(self, sample_size: int = 500) -> Dict[str, Any]\n```\n\nCompute clustering quality metrics for the concept layer.\n\n#### ComputeMixin.compute_concept_connections\n\n```python\nComputeMixin.compute_concept_connections(self, use_semantics: bool = True, min_shared_docs: int = 1, min_jaccard: float = 0.1, use_member_semantics: bool = False, use_embedding_similarity: bool = False, embedding_threshold: float = 0.3, verbose: bool = True) -> Dict[str, Any]\n```\n\nBuild lateral connections between concepts based on document overlap and semantics.\n\n#### ComputeMixin.extract_corpus_semantics\n\n```python\nComputeMixin.extract_corpus_semantics(self, use_pattern_extraction: bool = True, min_pattern_confidence: float = 0.6, max_similarity_pairs: int = 100000, min_context_keys: int = 3, verbose: bool = True) -> int\n```\n\nExtract semantic relations from the corpus.\n\n#### ComputeMixin.extract_pattern_relations\n\n```python\nComputeMixin.extract_pattern_relations(self, min_confidence: float = 0.6, verbose: bool = True) -> List[Tuple[str, str, str, float]]\n```\n\nExtract semantic relations using pattern matching only.\n\n#### ComputeMixin.retrofit_connections\n\n```python\nComputeMixin.retrofit_connections(self, iterations: int = 10, alpha: float = 0.3, verbose: bool = True) -> Dict\n```\n\nNone\n\n#### ComputeMixin.compute_property_inheritance\n\n```python\nComputeMixin.compute_property_inheritance(self, decay_factor: float = 0.7, max_depth: int = 5, apply_to_connections: bool = True, boost_factor: float = 0.3, verbose: bool = True) -> Dict[str, Any]\n```\n\nCompute property inheritance based on IsA hierarchy.\n\n#### ComputeMixin.compute_property_similarity\n\n```python\nComputeMixin.compute_property_similarity(self, term1: str, term2: str) -> float\n```\n\nCompute similarity between terms based on shared properties.\n\n#### ComputeMixin.compute_graph_embeddings\n\n```python\nComputeMixin.compute_graph_embeddings(self, dimensions: int = 64, method: str = 'fast', max_terms: Optional[int] = None, verbose: bool = True) -> Dict\n```\n\nCompute graph embeddings for tokens.\n\n#### ComputeMixin.retrofit_embeddings\n\n```python\nComputeMixin.retrofit_embeddings(self, iterations: int = 10, alpha: float = 0.4, verbose: bool = True) -> Dict\n```\n\nNone\n\n#### ComputeMixin.embedding_similarity\n\n```python\nComputeMixin.embedding_similarity(self, term1: str, term2: str) -> float\n```\n\nNone\n\n#### ComputeMixin.find_similar_by_embedding\n\n```python\nComputeMixin.find_similar_by_embedding(self, term: str, top_n: int = 10) -> List[Tuple[str, float]]\n```\n\nNone\n\n### Dependencies\n\n**Standard Library:**\n\n- `datetime.datetime`\n- `json`\n- `layers.CorticalLayer`\n- `logging`\n- `observability.timed`\n- ... and 11 more\n\n**Local Imports:**\n\n- `.analysis`\n- `.embeddings`\n- `.semantics`\n\n\n\n## core.py\n\nCore processor functionality: initialization, staleness tracking, and layer management.\n\nThis module contains the base class definition and core infrastructure that all\nother processor mixins depend o...\n\n\n### Classes\n\n#### CoreMixin\n\nCore mixin providing initialization and staleness tracking.\n\n**Methods:**\n\n- `is_stale`\n- `get_stale_computations`\n- `get_layer`\n- `get_metrics`\n- `get_metrics_summary`\n- `reset_metrics`\n- `enable_metrics`\n- `disable_metrics`\n- `record_metric`\n\n### Functions\n\n#### CoreMixin.is_stale\n\n```python\nCoreMixin.is_stale(self, computation_type: str) -> bool\n```\n\nCheck if a specific computation is stale.\n\n#### CoreMixin.get_stale_computations\n\n```python\nCoreMixin.get_stale_computations(self) -> set\n```\n\nGet the set of computations that are currently stale.\n\n#### CoreMixin.get_layer\n\n```python\nCoreMixin.get_layer(self, layer: CorticalLayer) -> HierarchicalLayer\n```\n\nGet a specific layer by enum.\n\n#### CoreMixin.get_metrics\n\n```python\nCoreMixin.get_metrics(self) -> Dict[str, Dict[str, Any]]\n```\n\nGet all collected metrics.\n\n#### CoreMixin.get_metrics_summary\n\n```python\nCoreMixin.get_metrics_summary(self) -> str\n```\n\nGet a human-readable summary of all metrics.\n\n#### CoreMixin.reset_metrics\n\n```python\nCoreMixin.reset_metrics(self) -> None\n```\n\nClear all collected metrics.\n\n#### CoreMixin.enable_metrics\n\n```python\nCoreMixin.enable_metrics(self) -> None\n```\n\nEnable metrics collection.\n\n#### CoreMixin.disable_metrics\n\n```python\nCoreMixin.disable_metrics(self) -> None\n```\n\nDisable metrics collection.\n\n#### CoreMixin.record_metric\n\n```python\nCoreMixin.record_metric(self, metric_name: str, count: int = 1) -> None\n```\n\nRecord a custom count metric.\n\n### Dependencies\n\n**Standard Library:**\n\n- `config.CorticalConfig`\n- `layers.CorticalLayer`\n- `layers.HierarchicalLayer`\n- `logging`\n- `minicolumn.Minicolumn`\n- ... and 5 more\n\n\n\n## documents.py\n\nDocument management: processing, adding, removing, and metadata handling.\n\nThis module contains all methods related to managing documents in the corpus.\n\n\n### Classes\n\n#### DocumentsMixin\n\nMixin providing document management functionality.\n\n**Methods:**\n\n- `process_document`\n- `set_document_metadata`\n- `get_document_metadata`\n- `get_all_document_metadata`\n- `add_document_incremental`\n- `add_documents_batch`\n- `remove_document`\n- `remove_documents_batch`\n\n### Functions\n\n#### DocumentsMixin.process_document\n\n```python\nDocumentsMixin.process_document(self, doc_id: str, content: str, metadata: Optional[Dict[str, Any]] = None) -> Dict[str, int]\n```\n\nProcess a document and add it to the corpus.\n\n#### DocumentsMixin.set_document_metadata\n\n```python\nDocumentsMixin.set_document_metadata(self, doc_id: str, **kwargs) -> None\n```\n\nSet or update metadata for a document.\n\n#### DocumentsMixin.get_document_metadata\n\n```python\nDocumentsMixin.get_document_metadata(self, doc_id: str) -> Dict[str, Any]\n```\n\nGet metadata for a document.\n\n#### DocumentsMixin.get_all_document_metadata\n\n```python\nDocumentsMixin.get_all_document_metadata(self) -> Dict[str, Dict[str, Any]]\n```\n\nGet metadata for all documents.\n\n#### DocumentsMixin.add_document_incremental\n\n```python\nDocumentsMixin.add_document_incremental(self, doc_id: str, content: str, metadata: Optional[Dict[str, Any]] = None, recompute: str = 'tfidf') -> Dict[str, int]\n```\n\nAdd a document with selective recomputation for efficiency.\n\n#### DocumentsMixin.add_documents_batch\n\n```python\nDocumentsMixin.add_documents_batch(self, documents: List[Tuple[str, str, Optional[Dict[str, Any]]]], recompute: str = 'full', verbose: bool = True) -> Dict[str, Any]\n```\n\nAdd multiple documents with a single recomputation.\n\n#### DocumentsMixin.remove_document\n\n```python\nDocumentsMixin.remove_document(self, doc_id: str, verbose: bool = False) -> Dict[str, Any]\n```\n\nRemove a document from the corpus.\n\n#### DocumentsMixin.remove_documents_batch\n\n```python\nDocumentsMixin.remove_documents_batch(self, doc_ids: List[str], recompute: str = 'none', verbose: bool = True) -> Dict[str, Any]\n```\n\nRemove multiple documents efficiently with single recomputation.\n\n### Dependencies\n\n**Standard Library:**\n\n- `copy`\n- `layers.CorticalLayer`\n- `logging`\n- `observability.timed`\n- `typing.Any`\n- ... and 4 more\n\n\n\n## introspection.py\n\nIntrospection: state inspection, fingerprints, gaps, and summaries.\n\nThis module contains methods for examining the processor state and\ncomparing texts/documents.\n\n\n### Classes\n\n#### IntrospectionMixin\n\nMixin providing introspection functionality.\n\n**Methods:**\n\n- `get_document_signature`\n- `get_corpus_summary`\n- `analyze_knowledge_gaps`\n- `detect_anomalies`\n- `get_fingerprint`\n- `compare_fingerprints`\n- `explain_fingerprint`\n- `explain_similarity`\n- `find_similar_texts`\n- `compare_with`\n- `compare_documents`\n- `what_changed`\n- `summarize_document`\n- `detect_patterns`\n- `detect_patterns_in_corpus`\n- `get_pattern_summary`\n- `get_corpus_pattern_statistics`\n- `format_pattern_report`\n- `list_available_patterns`\n- `list_pattern_categories`\n\n### Functions\n\n#### IntrospectionMixin.get_document_signature\n\n```python\nIntrospectionMixin.get_document_signature(self, doc_id: str, n: int = 10) -> List[Tuple[str, float]]\n```\n\nGet the top-n TF-IDF terms for a document.\n\n#### IntrospectionMixin.get_corpus_summary\n\n```python\nIntrospectionMixin.get_corpus_summary(self) -> Dict\n```\n\nGet summary statistics about the corpus.\n\n#### IntrospectionMixin.analyze_knowledge_gaps\n\n```python\nIntrospectionMixin.analyze_knowledge_gaps(self) -> Dict\n```\n\nAnalyze the corpus for knowledge gaps.\n\n#### IntrospectionMixin.detect_anomalies\n\n```python\nIntrospectionMixin.detect_anomalies(self, threshold: float = 0.3) -> List[Dict]\n```\n\nDetect anomalous patterns in the corpus.\n\n#### IntrospectionMixin.get_fingerprint\n\n```python\nIntrospectionMixin.get_fingerprint(self, text: str, top_n: int = 20) -> Dict\n```\n\nCompute the semantic fingerprint of a text.\n\n#### IntrospectionMixin.compare_fingerprints\n\n```python\nIntrospectionMixin.compare_fingerprints(self, fp1: Dict, fp2: Dict) -> Dict\n```\n\nCompare two fingerprints and compute similarity metrics.\n\n#### IntrospectionMixin.explain_fingerprint\n\n```python\nIntrospectionMixin.explain_fingerprint(self, fp: Dict, top_n: int = 10) -> Dict\n```\n\nGenerate a human-readable explanation of a fingerprint.\n\n#### IntrospectionMixin.explain_similarity\n\n```python\nIntrospectionMixin.explain_similarity(self, fp1: Dict, fp2: Dict) -> str\n```\n\nGenerate a human-readable explanation of fingerprint similarity.\n\n#### IntrospectionMixin.find_similar_texts\n\n```python\nIntrospectionMixin.find_similar_texts(self, text: str, candidates: List[Tuple[str, str]], top_n: int = 5) -> List[Tuple[str, float, Dict]]\n```\n\nFind texts most similar to the given text.\n\n#### IntrospectionMixin.compare_with\n\n```python\nIntrospectionMixin.compare_with(self, other: 'CorticalTextProcessor', top_movers: int = 20, min_pagerank_delta: float = 0.0001) -> 'diff_module.SemanticDiff'\n```\n\nCompare this processor state with another to find semantic differences.\n\n#### IntrospectionMixin.compare_documents\n\n```python\nIntrospectionMixin.compare_documents(self, doc_id_1: str, doc_id_2: str) -> Dict\n```\n\nCompare two documents within this corpus.\n\n#### IntrospectionMixin.what_changed\n\n```python\nIntrospectionMixin.what_changed(self, old_content: str, new_content: str) -> Dict\n```\n\nCompare two text contents to show what changed semantically.\n\n#### IntrospectionMixin.summarize_document\n\n```python\nIntrospectionMixin.summarize_document(self, doc_id: str, num_sentences: int = 3) -> str\n```\n\nGenerate a summary of a document using extractive summarization.\n\n#### IntrospectionMixin.detect_patterns\n\n```python\nIntrospectionMixin.detect_patterns(self, doc_id: str, patterns: Optional[List[str]] = None) -> Dict[str, List[int]]\n```\n\nDetect programming patterns in a specific document.\n\n#### IntrospectionMixin.detect_patterns_in_corpus\n\n```python\nIntrospectionMixin.detect_patterns_in_corpus(self, patterns: Optional[List[str]] = None) -> Dict[str, Dict[str, List[int]]]\n```\n\nDetect patterns across all documents in the corpus.\n\n#### IntrospectionMixin.get_pattern_summary\n\n```python\nIntrospectionMixin.get_pattern_summary(self, doc_id: str) -> Dict[str, int]\n```\n\nGet a summary of pattern occurrences in a document.\n\n#### IntrospectionMixin.get_corpus_pattern_statistics\n\n```python\nIntrospectionMixin.get_corpus_pattern_statistics(self) -> Dict[str, Any]\n```\n\nGet pattern statistics across the entire corpus.\n\n#### IntrospectionMixin.format_pattern_report\n\n```python\nIntrospectionMixin.format_pattern_report(self, doc_id: str, show_lines: bool = False) -> str\n```\n\nFormat pattern detection results as a human-readable report.\n\n#### IntrospectionMixin.list_available_patterns\n\n```python\nIntrospectionMixin.list_available_patterns(self) -> List[str]\n```\n\nList all available pattern names that can be detected.\n\n#### IntrospectionMixin.list_pattern_categories\n\n```python\nIntrospectionMixin.list_pattern_categories(self) -> List[str]\n```\n\nList all pattern categories.\n\n### Dependencies\n\n**Standard Library:**\n\n- `layers.CorticalLayer`\n- `logging`\n- `re`\n- `typing.Any`\n- `typing.Dict`\n- ... and 4 more\n\n**Local Imports:**\n\n- `.fingerprint`\n- `.gaps`\n- `.patterns`\n- `.persistence`\n\n\n\n## persistence_api.py\n\nPersistence API: save, load, export, and migration methods.\n\nThis module contains all methods related to saving and loading processor state.\n\n\n### Classes\n\n#### PersistenceMixin\n\nMixin providing persistence functionality.\n\n**Methods:**\n\n- `save`\n- `load`\n- `save_json`\n- `load_json`\n- `migrate_to_json`\n- `export_graph`\n- `export_conceptnet_json`\n\n### Functions\n\n#### PersistenceMixin.save\n\n```python\nPersistenceMixin.save(self, filepath: str, verbose: bool = True, signing_key: Optional[bytes] = None) -> None\n```\n\nSave processor state to a file.\n\n#### PersistenceMixin.load\n\n```python\nPersistenceMixin.load(cls, filepath: str, verbose: bool = True, verify_key: Optional[bytes] = None) -> 'CorticalTextProcessor'\n```\n\nLoad processor state from a file.\n\n#### PersistenceMixin.save_json\n\n```python\nPersistenceMixin.save_json(self, state_dir: str, force: bool = False, verbose: bool = True) -> Dict[str, bool]\n```\n\nSave processor state to git-friendly JSON format.\n\n#### PersistenceMixin.load_json\n\n```python\nPersistenceMixin.load_json(cls, state_dir: str, config: Optional[CorticalConfig] = None, verbose: bool = True) -> 'CorticalTextProcessor'\n```\n\nLoad processor from git-friendly JSON format.\n\n#### PersistenceMixin.migrate_to_json\n\n```python\nPersistenceMixin.migrate_to_json(self, pkl_path: str, json_dir: str, verbose: bool = True) -> bool\n```\n\nMigrate existing pickle file to git-friendly JSON format.\n\n#### PersistenceMixin.export_graph\n\n```python\nPersistenceMixin.export_graph(self, filepath: str, layer: Optional[CorticalLayer] = None, max_nodes: int = 500) -> Dict\n```\n\nExport graph to JSON for visualization.\n\n#### PersistenceMixin.export_conceptnet_json\n\n```python\nPersistenceMixin.export_conceptnet_json(self, filepath: str, include_cross_layer: bool = True, include_typed_edges: bool = True, min_weight: float = 0.0, min_confidence: float = 0.0, max_nodes_per_layer: int = 100, verbose: bool = True) -> Dict[str, Any]\n```\n\nExport ConceptNet-style graph for visualization.\n\n### Dependencies\n\n**Standard Library:**\n\n- `config.CorticalConfig`\n- `layers.CorticalLayer`\n- `logging`\n- `observability.timed`\n- `typing.Any`\n- ... and 3 more\n\n**Local Imports:**\n\n- `.persistence`\n- `.state_storage`\n\n\n\n---\n\n*This chapter is part of [The Cortical Chronicles](../README.md), a self-documenting book generated by the Cortical Text Processor.*\n",
      "keywords": [
        "str",
        "python",
        "self",
        "dict",
        "computemixin",
        "bool",
        "introspectionmixin",
        "none",
        "int",
        "true"
      ]
    },
    {
      "id": "02-architecture/mod-query",
      "title": "Search & Retrieval",
      "content": "# Search & Retrieval\n\nSearch and retrieval components for finding relevant documents and passages.\n\n## Modules\n\n- **__init__.py**: Query Module\n- **chunking.py**: Chunking Module\n- **definitions.py**: Definition Search Module\n- **expansion.py**: Query Expansion Module\n- **intent.py**: Intent Query Module\n- **passages.py**: Passage Retrieval Module\n- **ranking.py**: Ranking Module\n- **search.py**: Document Search Module\n\n\n## __init__.py\n\nQuery Module\n============\n\nQuery expansion and search functionality.\n\nThis package provides methods for expanding queries using lateral connections,\nconcept clusters, and word variants, then searching...\n\n\n### Dependencies\n\n**Standard Library:**\n\n- `analogy.complete_analogy`\n- `analogy.complete_analogy_simple`\n- `analogy.find_relation_between`\n- `analogy.find_terms_with_relation`\n- `chunking.CODE_BOUNDARY_PATTERN`\n- ... and 49 more\n\n\n\n## chunking.py\n\nChunking Module\n==============\n\nFunctions for splitting documents into chunks for passage retrieval.\n\nThis module provides:\n- Fixed-size text chunking with overlap\n- Code-aware chunking aligned to sem...\n\n\n### Functions\n\n#### create_chunks\n\n```python\ncreate_chunks(text: str, chunk_size: int = 512, overlap: int = 128) -> List[Tuple[str, int, int]]\n```\n\nSplit text into overlapping chunks.\n\n#### find_code_boundaries\n\n```python\nfind_code_boundaries(text: str) -> List[int]\n```\n\nFind semantic boundaries in code (class/function definitions, decorators).\n\n#### create_code_aware_chunks\n\n```python\ncreate_code_aware_chunks(text: str, target_size: int = 512, min_size: int = 100, max_size: int = 1024) -> List[Tuple[str, int, int]]\n```\n\nCreate chunks aligned to code structure boundaries.\n\n#### is_code_file\n\n```python\nis_code_file(doc_id: str) -> bool\n```\n\nDetermine if a document is a code file based on its path/extension.\n\n#### precompute_term_cols\n\n```python\nprecompute_term_cols(query_terms: Dict[str, float], layer0: HierarchicalLayer) -> Dict[str, 'Minicolumn']\n```\n\nPre-compute minicolumn lookups for query terms.\n\n#### score_chunk_fast\n\n```python\nscore_chunk_fast(chunk_tokens: List[str], query_terms: Dict[str, float], term_cols: Dict[str, 'Minicolumn'], doc_id: Optional[str] = None) -> float\n```\n\nFast chunk scoring using pre-computed minicolumn lookups.\n\n#### score_chunk\n\n```python\nscore_chunk(chunk_text: str, query_terms: Dict[str, float], layer0: HierarchicalLayer, tokenizer: Tokenizer, doc_id: Optional[str] = None) -> float\n```\n\nScore a chunk against query terms using TF-IDF.\n\n### Dependencies\n\n**Standard Library:**\n\n- `layers.HierarchicalLayer`\n- `re`\n- `tokenizer.Tokenizer`\n- `typing.Dict`\n- `typing.List`\n- ... and 3 more\n\n\n\n## definitions.py\n\nDefinition Search Module\n========================\n\nFunctions for finding and boosting code definitions (classes, functions, methods).\n\nThis module handles:\n- Detection of definition-seeking queries (\"...\n\n\n### Classes\n\n#### DefinitionQuery\n\nInfo about a definition-seeking query.\n\n### Functions\n\n#### is_definition_query\n\n```python\nis_definition_query(query_text: str) -> Tuple[bool, Optional[str], Optional[str]]\n```\n\nDetect if a query is looking for a code definition.\n\n#### find_definition_in_text\n\n```python\nfind_definition_in_text(text: str, identifier: str, def_type: str, context_chars: int = 500) -> Optional[Tuple[str, int, int]]\n```\n\nFind a definition in source text and extract surrounding context.\n\n#### find_definition_passages\n\n```python\nfind_definition_passages(query_text: str, documents: Dict[str, str], context_chars: int = 500, boost: float = DEFINITION_BOOST) -> List[Tuple[str, str, int, int, float]]\n```\n\nFind definition passages for a definition query.\n\n#### detect_definition_query\n\n```python\ndetect_definition_query(query_text: str) -> DefinitionQuery\n```\n\nDetect if a query is searching for a code definition.\n\n#### apply_definition_boost\n\n```python\napply_definition_boost(passages: List[Tuple[str, str, int, int, float]], query_text: str, boost_factor: float = 3.0) -> List[Tuple[str, str, int, int, float]]\n```\n\nBoost passages that contain actual code definitions matching the query.\n\n#### is_test_file\n\n```python\nis_test_file(doc_id: str) -> bool\n```\n\nDetect if a document ID represents a test file.\n\n#### boost_definition_documents\n\n```python\nboost_definition_documents(doc_results: List[Tuple[str, float]], query_text: str, documents: Dict[str, str], boost_factor: float = 2.0, test_with_definition_penalty: float = 0.5, test_without_definition_penalty: float = 0.7) -> List[Tuple[str, float]]\n```\n\nBoost documents that contain the actual definition being searched for.\n\n### Dependencies\n\n**Standard Library:**\n\n- `re`\n- `typing.Any`\n- `typing.Dict`\n- `typing.List`\n- `typing.Optional`\n- ... and 2 more\n\n\n\n## expansion.py\n\nQuery Expansion Module\n=====================\n\nFunctions for expanding query terms using lateral connections,\nsemantic relations, and code concept synonyms.\n\nThis module provides:\n- Basic query expansi...\n\n\n### Functions\n\n#### score_relation_path\n\n```python\nscore_relation_path(path: List[str]) -> float\n```\n\nScore a relation path by its semantic coherence.\n\n#### expand_query\n\n```python\nexpand_query(query_text: str, layers: Dict[CorticalLayer, HierarchicalLayer], tokenizer: Tokenizer, max_expansions: int = 10, use_lateral: bool = True, use_concepts: bool = True, use_variants: bool = True, use_code_concepts: bool = False, filter_code_stop_words: bool = False, tfidf_weight: float = 0.7, max_expansion_weight: float = 2.0) -> Dict[str, float]\n```\n\nExpand a query using lateral connections and concept clusters.\n\n#### expand_query_semantic\n\n```python\nexpand_query_semantic(query_text: str, layers: Dict[CorticalLayer, HierarchicalLayer], tokenizer: Tokenizer, semantic_relations: List[Tuple[str, str, str, float]], max_expansions: int = 10) -> Dict[str, float]\n```\n\nExpand query using semantic relations extracted from corpus.\n\n#### expand_query_multihop\n\n```python\nexpand_query_multihop(query_text: str, layers: Dict[CorticalLayer, HierarchicalLayer], tokenizer: Tokenizer, semantic_relations: List[Tuple[str, str, str, float]], max_hops: int = 2, max_expansions: int = 15, decay_factor: float = 0.5, min_path_score: float = 0.2) -> Dict[str, float]\n```\n\nExpand query using multi-hop semantic inference.\n\n#### get_expanded_query_terms\n\n```python\nget_expanded_query_terms(query_text: str, layers: Dict[CorticalLayer, HierarchicalLayer], tokenizer: Tokenizer, use_expansion: bool = True, semantic_relations: Optional[List[Tuple[str, str, str, float]]] = None, use_semantic: bool = True, max_expansions: int = 5, semantic_discount: float = 0.8, filter_code_stop_words: bool = False) -> Dict[str, float]\n```\n\nGet expanded query terms with optional semantic expansion.\n\n### Dependencies\n\n**Standard Library:**\n\n- `code_concepts.expand_code_concepts`\n- `collections.defaultdict`\n- `config.DEFAULT_CHAIN_VALIDITY`\n- `layers.CorticalLayer`\n- `layers.HierarchicalLayer`\n- ... and 6 more\n\n\n\n## intent.py\n\nIntent Query Module\n==================\n\nIntent-based query understanding for natural language code search.\n\nThis module handles:\n- Parsing natural language queries to extract intent (where, how, what,...\n\n\n### Classes\n\n#### ParsedIntent\n\nStructured representation of a parsed query intent.\n\n### Functions\n\n#### parse_intent_query\n\n```python\nparse_intent_query(query_text: str) -> ParsedIntent\n```\n\nParse a natural language query to extract intent and searchable terms.\n\n#### search_by_intent\n\n```python\nsearch_by_intent(query_text: str, layers: Dict[CorticalLayer, HierarchicalLayer], tokenizer: 'Tokenizer', top_n: int = 5) -> List[Tuple[str, float, ParsedIntent]]\n```\n\nSearch the corpus using intent-based query understanding.\n\n### Dependencies\n\n**Standard Library:**\n\n- `code_concepts.get_related_terms`\n- `collections.defaultdict`\n- `layers.CorticalLayer`\n- `layers.HierarchicalLayer`\n- `typing.Dict`\n- ... and 4 more\n\n\n\n## passages.py\n\nPassage Retrieval Module\n========================\n\nFunctions for retrieving relevant passages from documents.\n\nThis module provides:\n- Passage retrieval for RAG systems\n- Batch passage retrieval\n- Int...\n\n\n### Functions\n\n#### find_passages_for_query\n\n```python\nfind_passages_for_query(query_text: str, layers: Dict[CorticalLayer, HierarchicalLayer], tokenizer: Tokenizer, documents: Dict[str, str], top_n: int = 5, chunk_size: int = 512, overlap: int = 128, use_expansion: bool = True, doc_filter: Optional[List[str]] = None, semantic_relations: Optional[List[Tuple[str, str, str, float]]] = None, use_semantic: bool = True, use_definition_search: bool = True, definition_boost: float = DEFINITION_BOOST, apply_doc_boost: bool = True, doc_metadata: Optional[Dict[str, Dict[str, Any]]] = None, auto_detect_intent: bool = True, prefer_docs: bool = False, custom_boosts: Optional[Dict[str, float]] = None, use_code_aware_chunks: bool = True, filter_code_stop_words: bool = True, test_file_penalty: float = 0.8) -> List[Tuple[str, str, int, int, float]]\n```\n\nFind text passages most relevant to a query.\n\n#### find_documents_batch\n\n```python\nfind_documents_batch(queries: List[str], layers: Dict[CorticalLayer, HierarchicalLayer], tokenizer: Tokenizer, top_n: int = 5, use_expansion: bool = True, semantic_relations: Optional[List[Tuple[str, str, str, float]]] = None, use_semantic: bool = True) -> List[List[Tuple[str, float]]]\n```\n\nFind documents for multiple queries efficiently.\n\n#### find_passages_batch\n\n```python\nfind_passages_batch(queries: List[str], layers: Dict[CorticalLayer, HierarchicalLayer], tokenizer: Tokenizer, documents: Dict[str, str], top_n: int = 5, chunk_size: int = 512, overlap: int = 128, use_expansion: bool = True, doc_filter: Optional[List[str]] = None, semantic_relations: Optional[List[Tuple[str, str, str, float]]] = None, use_semantic: bool = True) -> List[List[Tuple[str, str, int, int, float]]]\n```\n\nFind passages for multiple queries efficiently.\n\n### Dependencies\n\n**Standard Library:**\n\n- `chunking.CODE_BOUNDARY_PATTERN`\n- `chunking.create_chunks`\n- `chunking.create_code_aware_chunks`\n- `chunking.find_code_boundaries`\n- `chunking.is_code_file`\n- ... and 18 more\n\n\n\n## ranking.py\n\nRanking Module\n=============\n\nMulti-stage ranking and document type boosting for search results.\n\nThis module provides:\n- Document type boosting (docs, code, tests)\n- Conceptual vs implementation quer...\n\n\n### Functions\n\n#### is_conceptual_query\n\n```python\nis_conceptual_query(query_text: str) -> bool\n```\n\nDetermine if a query is conceptual (should boost documentation).\n\n#### get_doc_type_boost\n\n```python\nget_doc_type_boost(doc_id: str, doc_metadata: Optional[Dict[str, Dict[str, Any]]] = None, custom_boosts: Optional[Dict[str, float]] = None) -> float\n```\n\nGet the boost factor for a document based on its type.\n\n#### apply_doc_type_boost\n\n```python\napply_doc_type_boost(results: List[Tuple[str, float]], doc_metadata: Optional[Dict[str, Dict[str, Any]]] = None, boost_docs: bool = True, custom_boosts: Optional[Dict[str, float]] = None) -> List[Tuple[str, float]]\n```\n\nApply document type boosting to search results.\n\n#### find_documents_with_boost\n\n```python\nfind_documents_with_boost(query_text: str, layers: Dict[CorticalLayer, HierarchicalLayer], tokenizer: Tokenizer, top_n: int = 5, doc_metadata: Optional[Dict[str, Dict[str, Any]]] = None, auto_detect_intent: bool = True, prefer_docs: bool = False, custom_boosts: Optional[Dict[str, float]] = None, use_expansion: bool = True, semantic_relations: Optional[List[Tuple[str, str, str, float]]] = None, use_semantic: bool = True) -> List[Tuple[str, float]]\n```\n\nFind documents with optional document-type boosting.\n\n#### find_relevant_concepts\n\n```python\nfind_relevant_concepts(query_terms: Dict[str, float], layers: Dict[CorticalLayer, HierarchicalLayer], top_n: int = 5) -> List[Tuple[str, float, set]]\n```\n\nStage 1: Find concepts relevant to query terms.\n\n#### multi_stage_rank\n\n```python\nmulti_stage_rank(query_text: str, layers: Dict[CorticalLayer, HierarchicalLayer], tokenizer: Tokenizer, documents: Dict[str, str], top_n: int = 5, chunk_size: int = 512, overlap: int = 128, concept_boost: float = 0.3, use_expansion: bool = True, semantic_relations: Optional[List[Tuple[str, str, str, float]]] = None, use_semantic: bool = True) -> List[Tuple[str, str, int, int, float, Dict[str, float]]]\n```\n\nMulti-stage ranking pipeline for improved RAG performance.\n\n#### multi_stage_rank_documents\n\n```python\nmulti_stage_rank_documents(query_text: str, layers: Dict[CorticalLayer, HierarchicalLayer], tokenizer: Tokenizer, top_n: int = 5, concept_boost: float = 0.3, use_expansion: bool = True, semantic_relations: Optional[List[Tuple[str, str, str, float]]] = None, use_semantic: bool = True) -> List[Tuple[str, float, Dict[str, float]]]\n```\n\nMulti-stage ranking for documents (without chunk scoring).\n\n### Dependencies\n\n**Standard Library:**\n\n- `collections.defaultdict`\n- `constants.CONCEPTUAL_KEYWORDS`\n- `constants.DOC_TYPE_BOOSTS`\n- `constants.IMPLEMENTATION_KEYWORDS`\n- `expansion.get_expanded_query_terms`\n- ... and 9 more\n\n\n\n## search.py\n\nDocument Search Module\n=====================\n\nFunctions for searching and retrieving documents from the corpus.\n\nThis module provides:\n- Basic document search using TF-IDF scoring\n- Fast document sear...\n\n\n### Functions\n\n#### find_documents_for_query\n\n```python\nfind_documents_for_query(query_text: str, layers: Dict[CorticalLayer, HierarchicalLayer], tokenizer: Tokenizer, top_n: int = 5, use_expansion: bool = True, semantic_relations: Optional[List[Tuple[str, str, str, float]]] = None, use_semantic: bool = True, doc_name_boost: float = 2.0, filter_code_stop_words: bool = True, test_file_penalty: float = 0.8) -> List[Tuple[str, float]]\n```\n\nFind documents most relevant to a query using TF-IDF and optional expansion.\n\n#### fast_find_documents\n\n```python\nfast_find_documents(query_text: str, layers: Dict[CorticalLayer, HierarchicalLayer], tokenizer: Tokenizer, top_n: int = 5, candidate_multiplier: int = 3, use_code_concepts: bool = True, doc_name_boost: float = 2.0) -> List[Tuple[str, float]]\n```\n\nFast document search using candidate filtering.\n\n#### build_document_index\n\n```python\nbuild_document_index(layers: Dict[CorticalLayer, HierarchicalLayer]) -> Dict[str, Dict[str, float]]\n```\n\nBuild an optimized inverted index for fast querying.\n\n#### search_with_index\n\n```python\nsearch_with_index(query_text: str, index: Dict[str, Dict[str, float]], tokenizer: Tokenizer, top_n: int = 5) -> List[Tuple[str, float]]\n```\n\nSearch using a pre-built inverted index.\n\n#### query_with_spreading_activation\n\n```python\nquery_with_spreading_activation(query_text: str, layers: Dict[CorticalLayer, HierarchicalLayer], tokenizer: Tokenizer, top_n: int = 10, max_expansions: int = 8) -> List[Tuple[str, float]]\n```\n\nQuery with automatic expansion using spreading activation.\n\n#### find_related_documents\n\n```python\nfind_related_documents(doc_id: str, layers: Dict[CorticalLayer, HierarchicalLayer]) -> List[Tuple[str, float]]\n```\n\nFind documents related to a given document via lateral connections.\n\n#### graph_boosted_search\n\n```python\ngraph_boosted_search(query_text: str, layers: Dict[CorticalLayer, HierarchicalLayer], tokenizer: Tokenizer, top_n: int = 5, pagerank_weight: float = 0.3, proximity_weight: float = 0.2, use_expansion: bool = True, semantic_relations: Optional[List[Tuple[str, str, str, float]]] = None) -> List[Tuple[str, float]]\n```\n\nGraph-Boosted BM25 (GB-BM25): Hybrid scoring combining BM25 with graph signals.\n\n### Dependencies\n\n**Standard Library:**\n\n- `code_concepts.get_related_terms`\n- `collections.defaultdict`\n- `expansion.expand_query`\n- `expansion.get_expanded_query_terms`\n- `layers.CorticalLayer`\n- ... and 6 more\n\n\n\n---\n\n*This chapter is part of [The Cortical Chronicles](../README.md), a self-documenting book generated by the Cortical Text Processor.*\n",
      "keywords": [
        "str",
        "float",
        "dict",
        "int",
        "list",
        "python",
        "bool",
        "tuple",
        "tokenizer",
        "query"
      ]
    },
    {
      "id": "02-architecture/mod-utilities",
      "title": "Utilities",
      "content": "# Utilities\n\nUtility modules supporting various features.\n\n## Modules\n\n- **cli_wrapper.py**: CLI wrapper framework for collecting context and triggering actions.\n- **code_concepts.py**: Code Concepts Module\n- **diff.py**: Semantic Diff Module\n- **fingerprint.py**: Fingerprint Module\n- **fluent.py**: Fluent API for CorticalTextProcessor - chainable method interface.\n- **gaps.py**: Gaps Module\n- **mcp_server.py**: MCP (Model Context Protocol) Server for Cortical Text Processor.\n- **patterns.py**: Code Pattern Detection Module\n\n\n## cli_wrapper.py\n\nCLI wrapper framework for collecting context and triggering actions.\n\nDesign philosophy: QUIET BY DEFAULT, POWERFUL WHEN NEEDED.\n\nMost of the time you just want to run a command and check if it worked...\n\n\n### Classes\n\n#### GitContext\n\nGit repository context information.\n\n**Methods:**\n\n- `collect`\n- `to_dict`\n\n#### ExecutionContext\n\nComplete context for a CLI command execution.\n\n**Methods:**\n\n- `to_dict`\n- `to_json`\n- `summary`\n\n#### HookType\n\nTypes of hooks that can be registered.\n\n#### HookRegistry\n\nRegistry for CLI execution hooks.\n\n**Methods:**\n\n- `register`\n- `register_pre`\n- `register_post`\n- `register_success`\n- `register_error`\n- `get_hooks`\n- `trigger`\n\n#### CLIWrapper\n\nWrapper for CLI command execution with context collection and hooks.\n\n**Methods:**\n\n- `run`\n- `on_success`\n- `on_error`\n- `on_complete`\n\n#### TaskCompletionManager\n\nManager for task completion triggers and context window management.\n\n**Methods:**\n\n- `on_task_complete`\n- `on_any_complete`\n- `handle_completion`\n- `get_session_summary`\n- `should_trigger_reindex`\n\n#### ContextWindowManager\n\nManages context window state based on CLI execution history.\n\n**Methods:**\n\n- `add_execution`\n- `add_file_read`\n- `get_recent_files`\n- `get_context_summary`\n- `suggest_pruning`\n\n#### Session\n\nTrack a sequence of commands as a session.\n\n**Methods:**\n\n- `run`\n- `should_reindex`\n- `summary`\n- `results`\n- `success_rate`\n- `all_passed`\n- `modified_files`\n\n#### TaskCheckpoint\n\nSave/restore context state when switching between tasks.\n\n**Methods:**\n\n- `save`\n- `load`\n- `list_tasks`\n- `delete`\n- `summarize`\n\n### Functions\n\n#### create_wrapper_with_completion_manager\n\n```python\ncreate_wrapper_with_completion_manager() -> Tuple[CLIWrapper, TaskCompletionManager]\n```\n\nCreate a CLIWrapper with an attached TaskCompletionManager.\n\n#### run_with_context\n\n```python\nrun_with_context(command: Union[str, List[str]], **kwargs) -> ExecutionContext\n```\n\nConvenience function to run a command with full context collection.\n\n#### run\n\n```python\nrun(command: Union[str, List[str]], git: bool = False, timeout: Optional[float] = None, cwd: Optional[str] = None) -> ExecutionContext\n```\n\nRun a command. That's it.\n\n#### test_then_commit\n\n```python\ntest_then_commit(test_cmd: Union[str, List[str]] = 'python -m unittest discover -s tests', message: str = 'Update', add_all: bool = True) -> Tuple[bool, List[ExecutionContext]]\n```\n\nRun tests, commit only if they pass.\n\n#### commit_and_push\n\n```python\ncommit_and_push(message: str, add_all: bool = True, branch: Optional[str] = None) -> Tuple[bool, List[ExecutionContext]]\n```\n\nAdd, commit, and push in one go.\n\n#### sync_with_main\n\n```python\nsync_with_main(main_branch: str = 'main') -> Tuple[bool, List[ExecutionContext]]\n```\n\nFetch and rebase current branch on main.\n\n#### GitContext.collect\n\n```python\nGitContext.collect(cls, cwd: Optional[str] = None) -> 'GitContext'\n```\n\nCollect git context from current directory.\n\n#### GitContext.to_dict\n\n```python\nGitContext.to_dict(self) -> Dict[str, Any]\n```\n\nConvert to dictionary.\n\n#### ExecutionContext.to_dict\n\n```python\nExecutionContext.to_dict(self) -> Dict[str, Any]\n```\n\nConvert to dictionary for serialization.\n\n#### ExecutionContext.to_json\n\n```python\nExecutionContext.to_json(self, indent: int = 2) -> str\n```\n\nConvert to JSON string.\n\n#### ExecutionContext.summary\n\n```python\nExecutionContext.summary(self) -> str\n```\n\nReturn a concise summary string.\n\n#### HookRegistry.register\n\n```python\nHookRegistry.register(self, hook_type: HookType, callback: HookCallback, pattern: Optional[str] = None) -> None\n```\n\nRegister a hook callback.\n\n#### HookRegistry.register_pre\n\n```python\nHookRegistry.register_pre(self, pattern: Optional[str], callback: HookCallback) -> None\n```\n\nConvenience method for pre-execution hooks.\n\n#### HookRegistry.register_post\n\n```python\nHookRegistry.register_post(self, pattern: Optional[str], callback: HookCallback) -> None\n```\n\nConvenience method for post-execution hooks.\n\n#### HookRegistry.register_success\n\n```python\nHookRegistry.register_success(self, pattern: Optional[str], callback: HookCallback) -> None\n```\n\nConvenience method for success hooks.\n\n#### HookRegistry.register_error\n\n```python\nHookRegistry.register_error(self, pattern: Optional[str], callback: HookCallback) -> None\n```\n\nConvenience method for error hooks.\n\n#### HookRegistry.get_hooks\n\n```python\nHookRegistry.get_hooks(self, hook_type: HookType, command: List[str]) -> List[HookCallback]\n```\n\nGet all hooks that should be triggered for a command.\n\n#### HookRegistry.trigger\n\n```python\nHookRegistry.trigger(self, hook_type: HookType, context: ExecutionContext) -> None\n```\n\nTrigger all matching hooks.\n\n#### CLIWrapper.run\n\n```python\nCLIWrapper.run(self, command: Union[str, List[str]], cwd: Optional[str] = None, timeout: Optional[float] = None, env: Optional[Dict[str, str]] = None, **kwargs) -> ExecutionContext\n```\n\nExecute a command with context collection and hooks.\n\n#### CLIWrapper.on_success\n\n```python\nCLIWrapper.on_success(self, pattern: Optional[str] = None)\n```\n\nDecorator to register a success hook.\n\n#### CLIWrapper.on_error\n\n```python\nCLIWrapper.on_error(self, pattern: Optional[str] = None)\n```\n\nDecorator to register an error hook.\n\n#### CLIWrapper.on_complete\n\n```python\nCLIWrapper.on_complete(self, pattern: Optional[str] = None)\n```\n\nDecorator to register a completion hook (success or failure).\n\n#### TaskCompletionManager.on_task_complete\n\n```python\nTaskCompletionManager.on_task_complete(self, task_type: str, callback: HookCallback) -> None\n```\n\nRegister a callback for when a specific task type completes.\n\n#### TaskCompletionManager.on_any_complete\n\n```python\nTaskCompletionManager.on_any_complete(self, callback: HookCallback) -> None\n```\n\nRegister a callback for any task completion.\n\n#### TaskCompletionManager.handle_completion\n\n```python\nTaskCompletionManager.handle_completion(self, context: ExecutionContext) -> None\n```\n\nHandle task completion and trigger appropriate callbacks.\n\n#### TaskCompletionManager.get_session_summary\n\n```python\nTaskCompletionManager.get_session_summary(self) -> Dict[str, Any]\n```\n\nGet summary of all tasks completed in this session.\n\n#### TaskCompletionManager.should_trigger_reindex\n\n```python\nTaskCompletionManager.should_trigger_reindex(self) -> bool\n```\n\nDetermine if corpus should be re-indexed based on session activity.\n\n#### ContextWindowManager.add_execution\n\n```python\nContextWindowManager.add_execution(self, context: ExecutionContext) -> None\n```\n\nAdd an execution to the context window.\n\n#### ContextWindowManager.add_file_read\n\n```python\nContextWindowManager.add_file_read(self, filepath: str) -> None\n```\n\nTrack that a file was read.\n\n#### ContextWindowManager.get_recent_files\n\n```python\nContextWindowManager.get_recent_files(self, limit: int = 10) -> List[str]\n```\n\nGet most recently accessed files.\n\n#### ContextWindowManager.get_context_summary\n\n```python\nContextWindowManager.get_context_summary(self) -> Dict[str, Any]\n```\n\nGet a summary of current context window state.\n\n#### ContextWindowManager.suggest_pruning\n\n```python\nContextWindowManager.suggest_pruning(self) -> List[str]\n```\n\nSuggest files that could be pruned from context.\n\n#### Session.run\n\n```python\nSession.run(self, command: Union[str, List[str]], **kwargs) -> ExecutionContext\n```\n\nRun a command within this session.\n\n#### Session.should_reindex\n\n```python\nSession.should_reindex(self) -> bool\n```\n\nCheck if corpus re-indexing is recommended based on session activity.\n\n#### Session.summary\n\n```python\nSession.summary(self) -> Dict[str, Any]\n```\n\nGet a summary of this session's activity.\n\n#### Session.results\n\n```python\nSession.results(self) -> List[ExecutionContext]\n```\n\nAll command results from this session.\n\n#### Session.success_rate\n\n```python\nSession.success_rate(self) -> float\n```\n\nFraction of commands that succeeded (0.0 to 1.0).\n\n#### Session.all_passed\n\n```python\nSession.all_passed(self) -> bool\n```\n\nTrue if all commands in this session succeeded.\n\n#### Session.modified_files\n\n```python\nSession.modified_files(self) -> List[str]\n```\n\nList of files modified during this session (from git context).\n\n#### TaskCheckpoint.save\n\n```python\nTaskCheckpoint.save(self, task_name: str, context: Dict[str, Any]) -> None\n```\n\nSave context for a task.\n\n#### TaskCheckpoint.load\n\n```python\nTaskCheckpoint.load(self, task_name: str) -> Optional[Dict[str, Any]]\n```\n\nLoad context for a task. Returns None if not found.\n\n#### TaskCheckpoint.list_tasks\n\n```python\nTaskCheckpoint.list_tasks(self) -> List[str]\n```\n\nList all saved task checkpoints.\n\n#### TaskCheckpoint.delete\n\n```python\nTaskCheckpoint.delete(self, task_name: str) -> bool\n```\n\nDelete a checkpoint. Returns True if deleted.\n\n#### TaskCheckpoint.summarize\n\n```python\nTaskCheckpoint.summarize(self, task_name: str) -> Optional[str]\n```\n\nGet a one-line summary of a task checkpoint.\n\n### Dependencies\n\n**Standard Library:**\n\n- `dataclasses.asdict`\n- `dataclasses.dataclass`\n- `dataclasses.field`\n- `datetime.datetime`\n- `enum.Enum`\n- ... and 15 more\n\n\n\n## code_concepts.py\n\nCode Concepts Module\n====================\n\nProgramming concept groups for semantic code search.\n\nMaps common programming synonyms and related terms to enable\nintent-based code retrieval. When a develo...\n\n\n### Functions\n\n#### get_related_terms\n\n```python\nget_related_terms(term: str, max_terms: int = 5) -> List[str]\n```\n\nGet programming terms related to the given term.\n\n#### expand_code_concepts\n\n```python\nexpand_code_concepts(terms: List[str], max_expansions_per_term: int = 3, weight: float = 0.6) -> Dict[str, float]\n```\n\nExpand a list of terms using code concept groups.\n\n#### get_concept_group\n\n```python\nget_concept_group(term: str) -> List[str]\n```\n\nGet the concept group names a term belongs to.\n\n#### list_concept_groups\n\n```python\nlist_concept_groups() -> List[str]\n```\n\nList all available concept group names.\n\n#### get_group_terms\n\n```python\nget_group_terms(group_name: str) -> List[str]\n```\n\nGet all terms in a concept group.\n\n### Dependencies\n\n**Standard Library:**\n\n- `typing.Dict`\n- `typing.FrozenSet`\n- `typing.List`\n- `typing.Set`\n\n\n\n## diff.py\n\nSemantic Diff Module\n====================\n\nProvides \"What Changed?\" functionality for comparing:\n- Two versions of a document\n- Two processor states\n- Before/after states of a corpus\n\nThis goes beyond...\n\n\n### Classes\n\n#### TermChange\n\nRepresents a change to a term/concept.\n\n**Methods:**\n\n- `pagerank_delta`\n- `tfidf_delta`\n- `documents_added`\n- `documents_removed`\n\n#### RelationChange\n\nRepresents a change to a semantic relation.\n\n#### ClusterChange\n\nRepresents a change to concept clustering.\n\n#### SemanticDiff\n\nComplete semantic diff between two states.\n\n**Methods:**\n\n- `summary`\n- `to_dict`\n\n### Functions\n\n#### compare_processors\n\n```python\ncompare_processors(old_processor: 'CorticalTextProcessor', new_processor: 'CorticalTextProcessor', top_movers: int = 20, min_pagerank_delta: float = 0.0001) -> SemanticDiff\n```\n\nCompare two processor states to find semantic differences.\n\n#### compare_documents\n\n```python\ncompare_documents(processor: 'CorticalTextProcessor', doc_id_old: str, doc_id_new: str) -> Dict[str, Any]\n```\n\nCompare two documents within the same corpus.\n\n#### what_changed\n\n```python\nwhat_changed(processor: 'CorticalTextProcessor', old_content: str, new_content: str, temp_doc_prefix: str = '_diff_temp_') -> Dict[str, Any]\n```\n\nCompare two text contents to show what changed semantically.\n\n#### TermChange.pagerank_delta\n\n```python\nTermChange.pagerank_delta(self) -> Optional[float]\n```\n\nChange in PageRank importance.\n\n#### TermChange.tfidf_delta\n\n```python\nTermChange.tfidf_delta(self) -> Optional[float]\n```\n\nChange in TF-IDF score.\n\n#### TermChange.documents_added\n\n```python\nTermChange.documents_added(self) -> Set[str]\n```\n\nDocuments where this term newly appears.\n\n#### TermChange.documents_removed\n\n```python\nTermChange.documents_removed(self) -> Set[str]\n```\n\nDocuments where this term no longer appears.\n\n#### SemanticDiff.summary\n\n```python\nSemanticDiff.summary(self) -> str\n```\n\nGenerate a human-readable summary of changes.\n\n#### SemanticDiff.to_dict\n\n```python\nSemanticDiff.to_dict(self) -> Dict[str, Any]\n```\n\nConvert to dictionary for serialization.\n\n### Dependencies\n\n**Standard Library:**\n\n- `collections.defaultdict`\n- `dataclasses.dataclass`\n- `dataclasses.field`\n- `layers.CorticalLayer`\n- `layers.HierarchicalLayer`\n- ... and 7 more\n\n\n\n## fingerprint.py\n\nFingerprint Module\n==================\n\nSemantic fingerprinting for code comparison and similarity analysis.\n\nA fingerprint is an interpretable representation of a text's semantic\ncontent, including te...\n\n\n### Classes\n\n#### SemanticFingerprint\n\nStructured representation of a text's semantic fingerprint.\n\n### Functions\n\n#### compute_fingerprint\n\n```python\ncompute_fingerprint(text: str, tokenizer: Tokenizer, layers: Optional[Dict[CorticalLayer, HierarchicalLayer]] = None, top_n: int = 20) -> SemanticFingerprint\n```\n\nCompute the semantic fingerprint of a text.\n\n#### compare_fingerprints\n\n```python\ncompare_fingerprints(fp1: SemanticFingerprint, fp2: SemanticFingerprint) -> Dict[str, Any]\n```\n\nCompare two fingerprints and compute similarity metrics.\n\n#### explain_fingerprint\n\n```python\nexplain_fingerprint(fp: SemanticFingerprint, top_n: int = 10) -> Dict[str, Any]\n```\n\nGenerate a human-readable explanation of a fingerprint.\n\n#### explain_similarity\n\n```python\nexplain_similarity(fp1: SemanticFingerprint, fp2: SemanticFingerprint, comparison: Optional[Dict[str, Any]] = None) -> str\n```\n\nGenerate a human-readable explanation of why two fingerprints are similar.\n\n### Dependencies\n\n**Standard Library:**\n\n- `code_concepts.get_concept_group`\n- `collections.defaultdict`\n- `layers.CorticalLayer`\n- `layers.HierarchicalLayer`\n- `math`\n- ... and 7 more\n\n\n\n## fluent.py\n\nFluent API for CorticalTextProcessor - chainable method interface.\n\nExample:\n    from cortical import FluentProcessor\n\n    # Simple usage\n    results = (FluentProcessor()\n        .add_document(\"doc1\",...\n\n\n### Classes\n\n#### FluentProcessor\n\nFluent/chainable API wrapper for CorticalTextProcessor.\n\n**Methods:**\n\n- `from_existing`\n- `from_files`\n- `from_directory`\n- `load`\n- `add_document`\n- `add_documents`\n- `with_config`\n- `with_tokenizer`\n- `build`\n- `save`\n- `search`\n- `fast_search`\n- `search_passages`\n- `expand`\n- `processor`\n- `is_built`\n\n### Functions\n\n#### FluentProcessor.from_existing\n\n```python\nFluentProcessor.from_existing(cls, processor: CorticalTextProcessor) -> 'FluentProcessor'\n```\n\nCreate a FluentProcessor from an existing CorticalTextProcessor.\n\n#### FluentProcessor.from_files\n\n```python\nFluentProcessor.from_files(cls, file_paths: List[Union[str, Path]], tokenizer: Optional[Tokenizer] = None, config: Optional[CorticalConfig] = None) -> 'FluentProcessor'\n```\n\nCreate a processor from a list of files.\n\n#### FluentProcessor.from_directory\n\n```python\nFluentProcessor.from_directory(cls, directory: Union[str, Path], pattern: str = '*.txt', recursive: bool = False, tokenizer: Optional[Tokenizer] = None, config: Optional[CorticalConfig] = None) -> 'FluentProcessor'\n```\n\nCreate a processor from all files in a directory.\n\n#### FluentProcessor.load\n\n```python\nFluentProcessor.load(cls, path: Union[str, Path]) -> 'FluentProcessor'\n```\n\nLoad a processor from a saved file.\n\n#### FluentProcessor.add_document\n\n```python\nFluentProcessor.add_document(self, doc_id: str, content: str, metadata: Optional[Dict[str, Any]] = None) -> 'FluentProcessor'\n```\n\nAdd a document to the processor (chainable).\n\n#### FluentProcessor.add_documents\n\n```python\nFluentProcessor.add_documents(self, documents: Union[Dict[str, str], List[Tuple[str, str]], List[Tuple[str, str, Dict]]]) -> 'FluentProcessor'\n```\n\nAdd multiple documents at once (chainable).\n\n#### FluentProcessor.with_config\n\n```python\nFluentProcessor.with_config(self, config: CorticalConfig) -> 'FluentProcessor'\n```\n\nSet configuration (chainable).\n\n#### FluentProcessor.with_tokenizer\n\n```python\nFluentProcessor.with_tokenizer(self, tokenizer: Tokenizer) -> 'FluentProcessor'\n```\n\nSet custom tokenizer (chainable).\n\n#### FluentProcessor.build\n\n```python\nFluentProcessor.build(self, verbose: bool = True, build_concepts: bool = True, pagerank_method: str = 'standard', connection_strategy: str = 'document_overlap', cluster_strictness: float = 1.0, bridge_weight: float = 0.0, show_progress: bool = False) -> 'FluentProcessor'\n```\n\nBuild the processor by computing all analysis phases (chainable).\n\n#### FluentProcessor.save\n\n```python\nFluentProcessor.save(self, path: Union[str, Path]) -> 'FluentProcessor'\n```\n\nSave the processor to disk (chainable).\n\n#### FluentProcessor.search\n\n```python\nFluentProcessor.search(self, query: str, top_n: int = 5, use_expansion: bool = True, use_semantic: bool = True) -> List[Tuple[str, float]]\n```\n\nSearch for documents matching the query.\n\n#### FluentProcessor.fast_search\n\n```python\nFluentProcessor.fast_search(self, query: str, top_n: int = 5, candidate_multiplier: int = 3, use_code_concepts: bool = True) -> List[Tuple[str, float]]\n```\n\nFast document search with pre-filtering.\n\n#### FluentProcessor.search_passages\n\n```python\nFluentProcessor.search_passages(self, query: str, top_n: int = 5, chunk_size: Optional[int] = None, overlap: Optional[int] = None, use_expansion: bool = True) -> List[Tuple[str, str, int, int, float]]\n```\n\nSearch for passage chunks matching the query.\n\n#### FluentProcessor.expand\n\n```python\nFluentProcessor.expand(self, query: str, max_expansions: Optional[int] = None, use_variants: bool = True, use_code_concepts: bool = False) -> Dict[str, float]\n```\n\nExpand a query with related terms.\n\n#### FluentProcessor.processor\n\n```python\nFluentProcessor.processor(self) -> CorticalTextProcessor\n```\n\nAccess the underlying CorticalTextProcessor instance.\n\n#### FluentProcessor.is_built\n\n```python\nFluentProcessor.is_built(self) -> bool\n```\n\nCheck if the processor has been built.\n\n### Dependencies\n\n**Standard Library:**\n\n- `config.CorticalConfig`\n- `os`\n- `pathlib.Path`\n- `processor.CorticalTextProcessor`\n- `tokenizer.Tokenizer`\n- ... and 6 more\n\n\n\n## gaps.py\n\nGaps Module\n===========\n\nKnowledge gap detection and anomaly analysis.\n\nIdentifies:\n- Isolated documents that don't connect well to the corpus\n- Weakly covered topics (few documents)\n- Bridge opportun...\n\n\n### Functions\n\n#### analyze_knowledge_gaps\n\n```python\nanalyze_knowledge_gaps(layers: Dict[CorticalLayer, HierarchicalLayer], documents: Dict[str, str]) -> Dict\n```\n\nAnalyze the corpus to identify potential knowledge gaps.\n\n#### detect_anomalies\n\n```python\ndetect_anomalies(layers: Dict[CorticalLayer, HierarchicalLayer], documents: Dict[str, str], threshold: float = 0.3) -> List[Dict]\n```\n\nDetect documents that don't fit well with the rest of the corpus.\n\n### Dependencies\n\n**Standard Library:**\n\n- `analysis.cosine_similarity`\n- `collections.defaultdict`\n- `layers.CorticalLayer`\n- `layers.HierarchicalLayer`\n- `math`\n- ... and 5 more\n\n\n\n## mcp_server.py\n\nMCP (Model Context Protocol) Server for Cortical Text Processor.\n\nProvides an MCP server interface for AI agents to integrate with the\nCortical Text Processor, enabling semantic search, query expansio...\n\n\n### Classes\n\n#### CorticalMCPServer\n\nMCP Server wrapper for CorticalTextProcessor.\n\n**Methods:**\n\n- `run`\n\n### Functions\n\n#### create_mcp_server\n\n```python\ncreate_mcp_server(corpus_path: Optional[str] = None, config: Optional[CorticalConfig] = None) -> CorticalMCPServer\n```\n\nCreate a Cortical MCP Server instance.\n\n#### main\n\n```python\nmain()\n```\n\nMain entry point for running the MCP server from command line.\n\n#### CorticalMCPServer.run\n\n```python\nCorticalMCPServer.run(self, transport: str = 'stdio')\n```\n\nRun the MCP server.\n\n### Dependencies\n\n**Standard Library:**\n\n- `config.CorticalConfig`\n- `logging`\n- `mcp.server.FastMCP`\n- `os`\n- `pathlib.Path`\n- ... and 5 more\n\n\n\n## patterns.py\n\nCode Pattern Detection Module\n==============================\n\nDetects common programming patterns in indexed code.\n\nIdentifies design patterns, idioms, and code structures including:\n- Singleton patte...\n\n\n### Functions\n\n#### detect_patterns_in_text\n\n```python\ndetect_patterns_in_text(text: str, patterns: Optional[List[str]] = None) -> Dict[str, List[int]]\n```\n\nDetect programming patterns in a text string.\n\n#### detect_patterns_in_documents\n\n```python\ndetect_patterns_in_documents(documents: Dict[str, str], patterns: Optional[List[str]] = None) -> Dict[str, Dict[str, List[int]]]\n```\n\nDetect patterns across multiple documents.\n\n#### get_pattern_summary\n\n```python\nget_pattern_summary(pattern_results: Dict[str, List[int]]) -> Dict[str, int]\n```\n\nSummarize pattern detection results by counting occurrences.\n\n#### get_patterns_by_category\n\n```python\nget_patterns_by_category(pattern_results: Dict[str, List[int]]) -> Dict[str, Dict[str, int]]\n```\n\nGroup pattern results by category.\n\n#### get_pattern_description\n\n```python\nget_pattern_description(pattern_name: str) -> Optional[str]\n```\n\nGet the description for a pattern.\n\n#### get_pattern_category\n\n```python\nget_pattern_category(pattern_name: str) -> Optional[str]\n```\n\nGet the category for a pattern.\n\n#### list_all_patterns\n\n```python\nlist_all_patterns() -> List[str]\n```\n\nList all available pattern names.\n\n#### list_patterns_by_category\n\n```python\nlist_patterns_by_category(category: str) -> List[str]\n```\n\nList all patterns in a specific category.\n\n#### list_all_categories\n\n```python\nlist_all_categories() -> List[str]\n```\n\nList all pattern categories.\n\n#### format_pattern_report\n\n```python\nformat_pattern_report(pattern_results: Dict[str, List[int]], show_lines: bool = False) -> str\n```\n\nFormat pattern detection results as a human-readable report.\n\n#### get_corpus_pattern_statistics\n\n```python\nget_corpus_pattern_statistics(doc_patterns: Dict[str, Dict[str, List[int]]]) -> Dict[str, any]\n```\n\nCompute statistics across all documents.\n\n### Dependencies\n\n**Standard Library:**\n\n- `collections.defaultdict`\n- `re`\n- `typing.Dict`\n- `typing.List`\n- `typing.Optional`\n- ... and 2 more\n\n\n\n---\n\n*This chapter is part of [The Cortical Chronicles](../README.md), a self-documenting book generated by the Cortical Text Processor.*\n",
      "keywords": [
        "str",
        "python",
        "self",
        "list",
        "fluentprocessor",
        "dict",
        "none",
        "optional",
        "session",
        "int"
      ]
    },
    {
      "id": "03-decisions/decision-adr-microseconds-task-id",
      "title": "ADR-001: Add Microseconds to Task ID Generation",
      "content": "# ADR-001: Add Microseconds to Task ID Generation\n\n**Status:** Accepted  \n**Date:** 2025-12-14  \n**Tags:** `task-management`, `uniqueness`, `concurrency`  \n\n---\n\n## The Question\n\nTask IDs were generated with second-precision timestamps plus a 4-character hex suffix:\n\n```\nT-YYYYMMDD-HHMMSS-XXXX\nExample: T-20251214-163052-a1b2\n```\n\nDuring CI testing, the `test_unique_task_ids` test was intermittently failing:\n\n```\nAssertionError: 99 != 100\n```\n\nWhen generating 100 task IDs in a tight loop (same second), collisions occurred because:\n- Same timestamp for all IDs in that second\n- Only 4 hex chars = 65,536 possible suffixes\n- Birthday paradox: P(collision) \u2248 7% for 100 items from 65,536\n\n## The Conversation\n\n*This decision emerged from 12 recorded discussion(s).*\n\n### Discussion 1\n\n**When:** 2025-12-16  \n**Matched Keywords:** task, add  \n\n**Query:**\n\n> Please deeply think about the best way to implement these tasks in batches dispatched inteligently to sub Agents and tracked/verifyed by enabling director mode with a backup plan that covers potential failure points intelligently.\n\nTasks:\nT-002\tDocument ML milestone thresholds derivation\tLow\nT-003\tMake CSV export truncation configurable\tLow\nT-004\tRefactor session_logger.py duplication\tLow\nT-010\tImplement confidence scoring thresholds\tMedium\nT-011\tAdd semantic similarity to ML predictions\tLow\nT-0\n\n**Files Explored:** `tasks/*.json`, `scripts/task_utils.py`\n\n### Discussion 2\n\n**When:** 2025-12-16  \n**Matched Keywords:** task, add  \n\n**Query:**\n\n> Please deeply think about the best way to implement these tasks in batches dispatched inteligently to sub Agents and tracked/verifyed by enabling director mode with a backup plan that covers potential failure points intelligently.\n\nTasks:\nT-002\tDocument ML milestone thresholds derivation\tLow\nT-003\tMake CSV export truncation configurable\tLow\nT-004\tRefactor session_logger.py duplication\tLow\nT-010\tImplement confidence scoring thresholds\tMedium\nT-011\tAdd semantic similarity to ML predictions\tLow\nT-0\n\n**Files Explored:** `scripts/task_utils.py`, `tasks/*.json`, `/home/user/Opus-code-test/tests/unit/test_ml_export.py`\n\n### Discussion 3\n\n**When:** 2025-12-16  \n**Matched Keywords:** task, add  \n\n**Query:**\n\n> Please deeply think about the best way to implement these tasks in batches dispatched inteligently to sub Agents and tracked/verifyed by enabling director mode with a backup plan that covers potential failure points intelligently.\n\nTasks:\nT-002\tDocument ML milestone thresholds derivation\tLow\nT-003\tMake CSV export truncation configurable\tLow\nT-004\tRefactor session_logger.py duplication\tLow\nT-010\tImplement confidence scoring thresholds\tMedium\nT-011\tAdd semantic similarity to ML predictions\tLow\nT-0\n\n**Files Explored:** `tasks/*.json`, `/home/user/Opus-code-test/tests/unit/test_ml_export.py`, `scripts/ml_data_collector.py`, `scripts/task_utils.py`\n\n## Options Considered\n\n### Option 1: Increase Random Suffix Length\n\n```\nT-YYYYMMDD-HHMMSS-XXXXXXXX  (8 hex chars)\n```\n\n**Pros:**\n- Simple change\n- 4 billion possibilities per second\n\n**Cons:**\n- Longer IDs\n- Doesn't leverage timestamp ordering\n\n### Option 2: Add Microseconds to Timestamp\n\n```\nT-YYYYMMDD-HHMMSSffffff-XXXX\nExample: T-20251214-163052123456-a1b2\n```\n\n**Pros:**\n- Timestamps remain sortable\n- 1 million unique timestamps per second\n- Combined with 4 hex suffix = practically unlimited uniqueness\n\n**Cons:**\n- IDs are 6 characters longer\n- Existing code parsing IDs needs update\n\n### Option 3: Use UUID Only\n\n```\nT-a1b2c3d4-e5f6-7890-abcd-ef1234567890\n```\n\n**Pros:**\n- Guaranteed uniqueness\n- Standard format\n\n**Cons:**\n- Not human-readable\n- Loses temporal ordering\n- Much longer\n\n## The Decision\n\n**Chosen Option:** Option 2 - Add Microseconds to Timestamp\n\n**Rationale:**\n- Preserves temporal ordering (IDs sort chronologically)\n- Microseconds provide 1M unique slots per second\n- Combined with 4 hex chars: virtually collision-proof\n- Minimal change to existing format\n\n## Implementation\n\n```python\ndef generate_task_id(session_id: Optional[str] = None) -> str:\n    now = datetime.now()\n    date_str = now.strftime(\"%Y%m%d\")\n    time_str = now.strftime(\"%H%M%S%f\")  # Added %f for microseconds\n    suffix = session_id or generate_session_id()\n    return f\"T-{date_str}-{time_str}-{suffix}\"\n```\n\n## Consequences\n\n### Positive\n- Tests no longer flaky\n- IDs unique even under heavy concurrent generation\n- Temporal ordering preserved\n\n### Negative\n- IDs 6 characters longer\n- Tests checking ID format needed update\n\n### Neutral\n- Existing IDs continue to work (no migration needed)\n- No performance impact\n\n## In Hindsight\n\n*This decision has been referenced in 2 subsequent commit(s).*\n\n- **2025-12-14** (`53c7985`): test: Update task ID format test to expect microseconds\n- **2025-12-14** (`5970006`): fix: Add microseconds to task ID to prevent collisions\n\n---\n\n*This decision story was enriched with conversation context from 12 chat session(s). Source: [adr-microseconds-task-id.md](../../samples/decisions/adr-microseconds-task-id.md)*\n",
      "keywords": [
        "low",
        "task",
        "add",
        "ids",
        "microseconds",
        "tasks",
        "second",
        "implement",
        "thresholds",
        "hex"
      ]
    },
    {
      "id": "03-decisions/index",
      "title": "Architectural Decision Records",
      "content": "# Architectural Decision Records\n\n*Enriched with conversation context and implementation history.*\n\n---\n\n## Overview\n\n**Total Decisions:** 1  \n**Accepted:** 1  \n\n## Decision Catalog\n\n### [ADR-001: Add Microseconds to Task ID Generation](decision-adr-microseconds-task-id.md)\n\n**Status:** Accepted | **Date:** 2025-12-14\n\nTask IDs were generated with second-precision timestamps plus a 4-character hex suffix:...\n\n---\n\n*This chapter is part of [The Cortical Chronicles](../README.md), a self-documenting book generated by the Cortical Text Processor.*\n",
      "keywords": [
        "decision",
        "task",
        "accepted",
        "adr",
        "microseconds",
        "generated",
        "cortical",
        "architectural",
        "records",
        "enriched"
      ]
    },
    {
      "id": "04-evolution/bugfixes",
      "title": "Bug Fixes and Lessons",
      "content": "# Bug Fixes and Lessons\n\n*What broke, how we fixed it, and what we learned.*\n\n---\n\n## Overview\n\n**12 bugs** have been identified and resolved. Each fix taught us something about the system.\n\n## Bug Fix History\n\n### Archive ML session after transcript processing (T-003 16f3)\n\n**Commit:** `59072c8`  \n**Date:** 2025-12-16  \n**Files Changed:** scripts/ml_data_collector.py  \n\n### Update CSV truncation test for new defaults (input=500, output=2000)\n\n**Commit:** `ca94a01`  \n**Date:** 2025-12-16  \n\n### Fix ML data collection milestone counting and add session/action capture\n\n**Commit:** `273baef`  \n**Date:** 2025-12-16  \n**Files Changed:** .git-ml/chats/2025-12-15/chat-20251216-121720-30c3c1.json, .git-ml/chats/2025-12-16/chat-20251216-121720-01077d.json, .git-ml/chats/2025-12-16/chat-20251216-121720-306450.json, .git-ml/chats/2025-12-16/chat-20251216-121720-5ef95b.json, .git-ml/chats/2025-12-16/chat-20251216-121720-8a1e7b.json  \n*(and 6 more)*  \n\n### Address critical ML data collection and prediction issues\n\n**Commit:** `fead1c1`  \n**Date:** 2025-12-16  \n**Files Changed:** .git-ml/chats/2025-12-15/chat-20251216-115057-b5bb48.json, .git-ml/chats/2025-12-16/chat-20251216-115057-3617f9.json, .git-ml/chats/2025-12-16/chat-20251216-115057-9502fd.json, .git-ml/chats/2025-12-16/chat-20251216-115057-cbbe64.json, .git-ml/chats/2025-12-16/chat-20251216-115057-f65b7a.json  \n*(and 4 more)*  \n\n### Add missing imports in validate command\n\n**Commit:** `172ad8f`  \n**Date:** 2025-12-16  \n**Files Changed:** scripts/ml_data_collector.py  \n\n### Clean up gitignore pattern for .git-ml/commits/\n\n**Commit:** `a65d54f`  \n**Date:** 2025-12-16  \n**Files Changed:** .gitignore  \n\n### Prevent infinite commit loop in ML data collection hooks\n\n**Commit:** `66ad656`  \n**Date:** 2025-12-16  \n**Files Changed:** .git-ml/chats/2025-12-16/chat-20251216-004054-78b531.json, .git-ml/tracked/commits.jsonl, scripts/ml_data_collector.py  \n\n### Correct hook format in settings.local.json\n\n**Commit:** `19ac02a`  \n**Date:** 2025-12-16  \n**Files Changed:** .claude/settings.local.json  \n\n### Use filename-based sorting for deterministic session ordering\n\n**Commit:** `61d502d`  \n**Date:** 2025-12-15  \n\n### Increase ID suffix length to prevent collisions\n\n**Commit:** `8ac4b6b`  \n**Date:** 2025-12-15  \n\n### Add import guards for optional test dependencies\n\n**Commit:** `91ffb04`  \n**Date:** 2025-12-15  \n\n### Make session file sorting stable for deterministic ordering\n\n**Commit:** `7433b36`  \n**Date:** 2025-12-15  \n\n",
      "keywords": [
        "commit",
        "git",
        "json",
        "date",
        "chats",
        "chat",
        "files",
        "changed",
        "session",
        "fix"
      ]
    },
    {
      "id": "04-evolution/features",
      "title": "Feature Evolution",
      "content": "# Feature Evolution\n\n*How the Cortical Text Processor gained its capabilities.*\n\n---\n\n## Overview\n\nThe system has evolved through **22 feature additions**. Below is the narrative of how each capability came to be.\n\n## Other Capabilities\n\n### Add 6 new intelligent book generators for publisher-ready content\n\n**Commit:** `082aa21`  \n**Date:** 2025-12-17  \n**Files Modified:** 47  \n\n### Add smart caching to markdown book generation\n\n**Commit:** `afd3c5b`  \n**Date:** 2025-12-16  \n**Files Modified:** 8  \n\n### Add consolidated markdown book generation\n\n**Commit:** `f8a2ad6`  \n**Date:** 2025-12-16  \n**Files Modified:** 3  \n\n### Add content generators for Cortical Chronicles (Wave 2)\n\n**Commit:** `3022110`  \n**Date:** 2025-12-16  \n**Files Modified:** 23  \n\n### Add Cortical Chronicles book infrastructure (Wave 1)\n\n**Commit:** `c730057`  \n**Date:** 2025-12-16  \n**Files Modified:** 13  \n\n### Batch task distribution implementation via Director orchestration\n\n**Commit:** `4f915c3`  \n**Date:** 2025-12-16  \n**Files Modified:** 8  \n\n### Add orchestration extraction for director sub-agent tracking\n\n**Commit:** `4eaeb37`  \n**Date:** 2025-12-15  \n\n### Add stunning animated ASCII codebase visualizer\n\n**Commit:** `e085a0b`  \n**Date:** 2025-12-15  \n\n### Add ASCII art codebase visualization script\n\n**Commit:** `43aae33`  \n**Date:** 2025-12-15  \n\n### Complete legacy task system migration\n\n**Commit:** `33dc8b2`  \n**Date:** 2025-12-15  \n\n### Add director orchestration execution tracking system\n\n**Commit:** `4976c58`  \n**Date:** 2025-12-15  \n\n## Search Capabilities\n\n### Add chunked parallel processing for TF-IDF/BM25 (LEGACY-135)\n\n**Commit:** `5665839`  \n**Date:** 2025-12-16  \n\n### Add search integration and web interface (Wave 3)\n\n**Commit:** `0022466`  \n**Date:** 2025-12-16  \n**Files Modified:** 11  \n\n## Data Capabilities\n\n### Implement WAL + Snapshot persistence system (LEGACY-133)\n\n**Commit:** `c7e662a`  \n**Date:** 2025-12-16  \n\n### Add git-tracked JSONL storage for orchestration data\n\n**Commit:** `fb30e38`  \n**Date:** 2025-12-15  \n\n## Ml Capabilities\n\n### Implement top priorities (ML capture, state storage, legacy cleanup)\n\n**Commit:** `4820c64`  \n**Date:** 2025-12-16  \n\n### Add file existence filter to ML predictions\n\n**Commit:** `3cab2ba`  \n**Date:** 2025-12-16  \n**Files Modified:** 1  \n\n### Add ML file prediction model\n\n**Commit:** `ac549dd`  \n**Date:** 2025-12-16  \n**Files Modified:** 2  \n\n### Add chunked storage for git-friendly ML data\n\n**Commit:** `0754540`  \n**Date:** 2025-12-16  \n**Files Modified:** 4  \n\n### Add ML stats report to CI pipeline\n\n**Commit:** `3e05a70`  \n**Date:** 2025-12-16  \n**Files Modified:** 9  \n\n## Documentation Capabilities\n\n### Add CI workflow and documentation (Wave 4)\n\n**Commit:** `940fdf2`  \n**Date:** 2025-12-16  \n**Files Modified:** 5  \n\n### Add animated GIF visualizations to README\n\n**Commit:** `b4d7c82`  \n**Date:** 2025-12-15  \n\n",
      "keywords": [
        "commit",
        "date",
        "add",
        "files",
        "modified",
        "capabilities",
        "system",
        "book",
        "wave",
        "orchestration"
      ]
    },
    {
      "id": "04-evolution/refactors",
      "title": "Refactorings and Architecture Evolution",
      "content": "# Refactorings and Architecture Evolution\n\n*How the codebase structure improved over time.*\n\n---\n\n## Overview\n\nThe codebase has undergone **3 refactorings**. Each improved code quality, maintainability, or performance.\n\n## Refactoring History\n\n### Complete legacy task system cleanup\n\n**Commit:** `8dedda6`  \n**Date:** 2025-12-16  \n\n### Remove unused protobuf serialization (T-013 f0ff)\n\n**Commit:** `d7a98ae`  \n**Date:** 2025-12-16  \n**Changes:** +100/-1460 lines  \n**Scope:** 6 files affected  \n\n### Split large files exceeding 25000 token limit\n\n**Commit:** `21ec5ea`  \n**Date:** 2025-12-15  \n\n",
      "keywords": [
        "commit",
        "date",
        "refactorings",
        "codebase",
        "improved",
        "files",
        "architecture",
        "evolution",
        "structure",
        "time"
      ]
    },
    {
      "id": "04-evolution/test_timeline",
      "title": "Test",
      "content": "# Timeline\n\n---\n\n## December 2025\n\n### Week of Dec 15\n\n- **2025-12-16**: feat: Add book\n- **2025-12-16**: docs: Add vision\n\n",
      "keywords": [
        "add",
        "timeline",
        "december",
        "week",
        "dec",
        "feat",
        "book",
        "docs",
        "vision"
      ]
    },
    {
      "id": "04-evolution/timeline",
      "title": "Project Timeline",
      "content": "# Project Timeline\n\n*A chronological journey through the Cortical Text Processor's development.*\n\n---\n\n## December 2025\n\n### Week of Dec 15\n\n- **2025-12-17**: ml: Capture session data\n- **2025-12-17**: feat: Add 6 new intelligent book generators for publisher-ready content\n- **2025-12-16**: ml: Capture session data\n- **2025-12-16**: docs: Add living book generation vision\n- **2025-12-16**: ml: Capture session data\n- **2025-12-16**: feat: Add smart caching to markdown book generation\n- **2025-12-16**: ml: Capture session data\n- **2025-12-16**: feat: Add consolidated markdown book generation\n- **2025-12-16**: feat: Add chunked parallel processing for TF-IDF/BM25 (LEGACY-135)\n- **2025-12-16**: feat: Implement WAL + Snapshot persistence system (LEGACY-133)\n\n",
      "keywords": [
        "feat",
        "add",
        "capture",
        "session",
        "data",
        "book",
        "generation",
        "markdown",
        "legacy",
        "project"
      ]
    },
    {
      "id": "05-case-studies/case-performance-hunt",
      "title": "The Great Performance Hunt",
      "content": "# Case Study: The Great Performance Hunt\n\n*A tale of assumptions, profiling, and unexpected bottlenecks.*\n\n## The Problem\n\nIt started with a timeout. The `compute_all()` function was hanging on a corpus of just 125 documents\u2014far smaller than our target of 10,000+. Something was fundamentally wrong.\n\nThe system would start processing, print \"Computing PageRank...\", then silence. No errors, no warnings, just an infinite wait. After 30 seconds, the timeout would trigger and the process would die.\n\nThis wasn't a minor performance issue. This was a showstopper.\n\n## The Suspect\n\nThe obvious culprit was Louvain clustering. Think about it:\n\n- **Most complex algorithm** - O(n log n) community detection with multiple passes\n- **Graph manipulation** - Rewiring communities iteratively until modularity converges\n- **Nested loops** - Pass after pass until stability\n\nEvery instinct, every pattern-matching neuron in our brains said: \"Start there. It has to be Louvain.\"\n\nBut assumptions are dangerous.\n\n## The Investigation\n\nWe started with profiling, not guessing. The `profile_full_analysis.py` script measured every phase of `compute_all()`:\n\n```bash\npython scripts/profile_full_analysis.py\n```\n\nThe results were shocking:\n\n| Phase | Before | After | Fix |\n|-------|--------|-------|-----|\n| `bigram_connections` | **20.85s timeout** | 10.79s | `max_bigrams_per_term=100`, `max_bigrams_per_doc=500` |\n| `semantics` | **30.05s timeout** | 5.56s | `max_similarity_pairs=100000`, `min_context_keys=3` |\n| `louvain` | **2.2s** | 2.2s | **Not the bottleneck!** |\n\nRead that last line again: **Louvain was innocent.**\n\nThe algorithm everyone suspected\u2014the complex graph clustering with multiple iterative passes\u2014was responsible for just 2.2 seconds of a 50+ second hang.\n\n99% of the execution time was hidden in `bigram_connections()` and `extract_corpus_semantics()`.\n\n## The Discovery\n\nThen we saw it. Looking at the code in `cortical/analysis/connections.py`:\n\n```python\n# Build indexes for efficient lookup\nleft_index: Dict[str, List[Minicolumn]] = defaultdict(list)\nright_index: Dict[str, List[Minicolumn]] = defaultdict(list)\n\nfor bigram in bigrams:\n    parts = bigram.content.split(' ')\n    if len(parts) == 2:\n        left_index[parts[0]].append(bigram)\n        right_index[parts[1]].append(bigram)\n\n# Connect bigrams sharing components\nfor component, bigram_list in left_index.items():\n    # THIS is where it exploded\n    for i, b1 in enumerate(bigram_list):\n        for b2 in bigram_list[i+1:]:\n            # Create connection between b1 and b2\n```\n\nThe problem was hiding in plain sight. For every term that appears in bigrams, we were creating connections between **all pairs** of bigrams containing that term.\n\n**Common terms like \"self\" appeared in hundreds of bigrams.**\n\nIf \"self\" appears in 300 bigrams (self_attention, self_healing, self_referential, etc.), the nested loop creates:\n\n```\n300 \u00d7 299 / 2 = 44,850 connections\n```\n\nFor a single term.\n\nNow imagine dozens of common terms (\"return\", \"function\", \"value\", \"data\", \"process\"). Each creating tens of thousands of pairwise connections.\n\n**O(n\u00b2) complexity from common terms was creating millions of pairs.**\n\nThe complexity analysis confirmed it:\n\n```python\n# Without limits: O(n_bigrams\u00b2) worst case from common terms creating all-to-all connections\n# With limits: O(n_terms * max_bigrams_per_term\u00b2 + n_docs * max_bigrams_per_doc\u00b2)\n# Typical with defaults (100, 500): O(n_terms * 10000 + n_docs * 250000) \u2248 O(n_bigrams) linear\n```\n\nWithout limits, the algorithm had **quadratic worst-case complexity**. With limits, it became **effectively linear**.\n\n## The Solution\n\nThe fix was elegant: **skip overly common terms** to prevent the O(n\u00b2) explosion:\n\n```python\ndef compute_bigram_connections(\n    layers: Dict[CorticalLayer, HierarchicalLayer],\n    component_weight: float = 0.5,\n    chain_weight: float = 0.7,\n    cooccurrence_weight: float = 0.3,\n    max_bigrams_per_term: int = 100,      # NEW: Prevent O(n\u00b2) from common terms\n    max_bigrams_per_doc: int = 500,       # NEW: Prevent O(n\u00b2) from large docs\n    max_connections_per_bigram: int = 50  # NEW: Cap per-bigram connections\n) -> Dict[str, Any]:\n    \"\"\"\n    Compute lateral connections between bigrams.\n\n    Args:\n        max_bigrams_per_term: Skip terms appearing in more than this many bigrams\n            to avoid O(n\u00b2) explosion from common terms like \"self\", \"return\"\n        max_bigrams_per_doc: Skip documents with more than this many bigrams for\n            co-occurrence connections to avoid O(n\u00b2) explosion\n    \"\"\"\n```\n\nWith these limits in place:\n\n```python\n# Left component matches: \"neural_networks\" \u2194 \"neural_processing\"\nfor component, bigram_list in left_index.items():\n    # Skip overly common terms to avoid O(n\u00b2) explosion\n    if len(bigram_list) > max_bigrams_per_term:\n        skipped_common_terms += 1\n        continue\n\n    for i, b1 in enumerate(bigram_list):\n        for b2 in bigram_list[i+1:]:\n            # Safe now - bounded by max_bigrams_per_term\u00b2\n            create_connection(b1, b2, weight=component_weight)\n```\n\n**Results:**\n- `bigram_connections`: 20.85s timeout \u2192 **10.79s** (48% improvement)\n- `semantics`: 30.05s timeout \u2192 **5.56s** (81% improvement)\n- Total `compute_all()`: timeout \u2192 **~27s** (viable for production)\n\nThe same approach was applied to `extract_corpus_semantics()`:\n\n```python\nmax_similarity_pairs: int = 100000  # Prevent similarity explosion\nmin_context_keys: int = 3           # Require meaningful context overlap\n```\n\n## The Lesson\n\n**Profile before optimizing.** The obvious culprit is often innocent. The real bottleneck hides in unexpected places.\n\nWe suspected Louvain\u2014the complex, iterative graph algorithm with nested loops and community rewiring. The actual problem was a simple nested loop over common terms, creating millions of unnecessary connections.\n\n**Key takeaways:**\n\n1. **Measure, don't assume** - Run the profiler before making changes\n2. **Look for O(n\u00b2) patterns** - Nested loops over unbounded collections\n3. **Common items are dangerous** - High-frequency terms/documents create all-to-all explosions\n4. **Add limits early** - Prevent worst-case scenarios with sensible bounds\n5. **Track what you skip** - Return stats on skipped items for monitoring\n\n## The Aftermath\n\nThis investigation led to several improvements across the codebase:\n\n1. **Profiling became standard practice** - `profile_full_analysis.py` is now run routinely\n2. **O(n\u00b2) awareness** - Code reviews specifically check for quadratic patterns\n3. **Limit parameters everywhere** - All connection-building functions have max limits\n4. **Performance tests** - Regression tests verify compute times stay bounded\n5. **Documentation** - CLAUDE.md now includes \"Performance Lessons Learned\" section\n\nThe fix enabled the system to scale from 125 documents (timeout) to **10,000+ documents** (27 seconds).\n\n## Try It Yourself\n\nRun the profiler on your own corpus to identify bottlenecks:\n\n```bash\npython scripts/profile_full_analysis.py\n```\n\nWatch for these warning signs:\n- O(n\u00b2) patterns in loops over connections\n- Common terms/documents creating explosions\n- Phases taking >10x longer than expected\n- Nested loops without bounds\n\nLook for code like this:\n\n```python\n# DANGER: O(n\u00b2) if items list is unbounded\nfor i, item1 in enumerate(items):\n    for item2 in items[i+1:]:\n        # Creates n \u00d7 (n-1) / 2 operations\n```\n\nAnd replace with:\n\n```python\n# SAFE: Bounded by max_items_per_group\nfor group, items in grouped_items.items():\n    if len(items) > max_items_per_group:\n        continue  # Skip overly common groups\n\n    for i, item1 in enumerate(items):\n        for item2 in items[i+1:]:\n            # Now bounded by max_items_per_group\u00b2\n```\n\n---\n\n**Remember:** The algorithm you suspect is often innocent. The real bottleneck is hiding in the code you didn't think to check.\n\n**Profile first. Optimize second. Always.**\n",
      "keywords": [
        "items",
        "common",
        "terms",
        "connections",
        "python",
        "bigrams",
        "timeout",
        "bigram_list",
        "limits",
        "documents"
      ]
    },
    {
      "id": "05-case-studies/index",
      "title": "index",
      "content": "# Case Studies\n\n*Real problem-solving sessions from the development of the Cortical Text Processor*\n\nNo case studies available yet. Case studies are generated from ML session data when sessions demonstrate significant problem-solving narratives.\n\n**What makes a good case study?**\n\n- At least 5 exchanges (substantial investigation)\n- Multiple tools used (shows exploration)\n- Resulted in commits (concrete outcome)\n- Clear problem statement (queries starting with 'fix', 'why', 'how do', etc.)\n\n*These case studies are automatically generated from ML session data collected during development. They demonstrate real problem-solving workflows and serve as both documentation and learning material.*\n",
      "keywords": [
        "case",
        "studies",
        "problem",
        "solving",
        "real",
        "sessions",
        "development",
        "generated",
        "session",
        "data"
      ]
    },
    {
      "id": "05-case-studies/synthesized-2025-12-14-add-session-memory-and-knowled",
      "title": "synthesized-2025-12-14-add-session-memory-and-knowled",
      "content": "# Case Study: Add session memory and knowledge transfer\n\n*Synthesized from commit history: 2025-12-14*\n\n## The Problem\n\nA new feature was required: Add director agent orchestration prompt. This would require careful implementation and testing.\n\n## The Journey\n\nThe development progressed through several stages:\n\n1. **Add director agent orchestration prompt** - Modified 2 files (+425/-1 lines)\n2. **Add memory system CLI and improve documentation** - Modified 5 files (+477/-16 lines)\n3. **Add session memory and knowledge transfer** - Modified 3 files (+254/-16 lines)\n4. **Add session handoff, auto-memory, CI link checker, and tests** - Modified 6 files (+1375/-10 lines)\n\n\n## The Solution\n\nMerge pull request #81 from scrawlsbenches/claude/implement-director-mode-NKbiu\n\nThe solution involved changes to 47 files, adding 14229 lines and removing 173 lines.\n\n\n## The Lesson\n\n**Feature development is iterative.** Breaking work into smaller commits makes it easier to review, test, and debug.\n\n## Technical Details\n\n**Files Modified:** 49\n\n- `.claude/commands/director.md`\n- `.github/workflows/ci.yml`\n- `.markdown-link-check.json`\n- `CLAUDE.md`\n- `README.md`\n- `cortical/observability.py`\n- `cortical/patterns.py`\n- `cortical/processor/compute.py`\n- `cortical/processor/core.py`\n- `cortical/processor/documents.py`\n\n*...and 39 more files*\n\n**Code Changes:** +16760/-216 lines\n\n**Commits:** 5\n\n\n## Commits in This Story\n\n- `4ab60f2` (2025-12-14): feat: Add director agent orchestration prompt\n- `d647b53` (2025-12-14): feat: Add memory system CLI and improve documentation\n- `2160f3d` (2025-12-14): memory: Add session memory and knowledge transfer\n- `6684152` (2025-12-14): feat: Add session handoff, auto-memory, CI link checker, and tests\n- `2a11bf3` (2025-12-14): Merge pull request #81 from scrawlsbenches/claude/implement-director-mode-NKbiu\n\n---\n\n*This case study was automatically synthesized from git commit history.*\n",
      "keywords": [
        "add",
        "memory",
        "files",
        "lines",
        "director",
        "session",
        "modified",
        "cortical",
        "claude",
        "knowledge"
      ]
    },
    {
      "id": "05-case-studies/synthesized-2025-12-14-bug-fix---add-test-file-penalt",
      "title": "synthesized-2025-12-14-bug-fix---add-test-file-penalt",
      "content": "# Case Study: Bug Fix - Add test file penalty and code stop word filtering to search\n\n*Synthesized from commit history: 2025-12-14*\n\n## The Problem\n\nA new feature was required: Director mode batch execution - 6 tasks completed in parallel. This would require careful implementation and testing.\n\n## The Journey\n\nThe solution was implemented directly.\n\n\n## The Solution\n\nAdd test file penalty and code stop word filtering to search\n\nThe solution involved changes to 3 files, adding 51 lines and removing 9 lines.\n\n\n## The Lesson\n\n**Bugs often hide in unexpected places.** Thorough investigation and testing are essential for finding root causes.\n\n## Technical Details\n\n**Files Modified:** 33\n\n- `CLAUDE.md`\n- `IMPLEMENTATION_SUMMARY.md`\n- `PATTERN_DETECTION_GUIDE.md`\n- `cortical/observability.py`\n- `cortical/patterns.py`\n- `cortical/processor/compute.py`\n- `cortical/processor/core.py`\n- `cortical/processor/documents.py`\n- `cortical/processor/introspection.py`\n- `cortical/processor/persistence_api.py`\n\n*...and 23 more files*\n\n**Code Changes:** +9432/-117 lines\n\n**Commits:** 2\n\n\n## Commits in This Story\n\n- `a9478fd` (2025-12-14): feat: Director mode batch execution - 6 tasks completed in parallel\n- `1fafc8b` (2025-12-14): fix: Add test file penalty and code stop word filtering to search\n\n---\n\n*This case study was automatically synthesized from git commit history.*\n",
      "keywords": [
        "cortical",
        "processor",
        "code",
        "add",
        "test",
        "file",
        "penalty",
        "stop",
        "word",
        "filtering"
      ]
    },
    {
      "id": "05-case-studies/synthesized-2025-12-14-bug-fix---replace-external-act",
      "title": "synthesized-2025-12-14-bug-fix---replace-external-act",
      "content": "# Case Study: Bug Fix - Replace external action with native Python link checker\n\n*Synthesized from commit history: 2025-12-14*\n\n## The Problem\n\nDevelopment work began: Merge pull request #81 from scrawlsbenches/claude/implement-director-mode-NKbiu.\n\n## The Journey\n\nThe development progressed through several stages:\n\n1. **Merge pull request #81 from scrawlsbenches/claude/implement-director-mode-NKbiu** - Modified 47 files (+14229/-173 lines)\n2. **Add session handoff, auto-memory, CI link checker, and tests** - Modified 6 files (+1375/-10 lines)\n3. **Replace external action with native Python link checker** - Modified 5 files (+172/-34 lines)\n\n\n## The Solution\n\nAdjust Native Over External threshold to 20000 lines\n\nThe solution involved changes to 1 files, adding 1 lines and removing 1 lines.\n\n\n## The Lesson\n\n**Bugs often hide in unexpected places.** Thorough investigation and testing are essential for finding root causes.\n\n## Technical Details\n\n**Files Modified:** 48\n\n- `.github/workflows/ci.yml`\n- `.markdown-link-check.json`\n- `CLAUDE.md`\n- `README.md`\n- `cortical/observability.py`\n- `cortical/patterns.py`\n- `cortical/processor/compute.py`\n- `cortical/processor/core.py`\n- `cortical/processor/documents.py`\n- `cortical/processor/introspection.py`\n\n*...and 38 more files*\n\n**Code Changes:** +15777/-218 lines\n\n**Commits:** 4\n\n\n## Commits in This Story\n\n- `2a11bf3` (2025-12-14): Merge pull request #81 from scrawlsbenches/claude/implement-director-mode-NKbiu\n- `6684152` (2025-12-14): feat: Add session handoff, auto-memory, CI link checker, and tests\n- `901a181` (2025-12-14): fix: Replace external action with native Python link checker\n- `00f88d4` (2025-12-14): docs: Adjust Native Over External threshold to 20000 lines\n\n---\n\n*This case study was automatically synthesized from git commit history.*\n",
      "keywords": [
        "lines",
        "link",
        "files",
        "cortical",
        "external",
        "native",
        "checker",
        "claude",
        "modified",
        "processor"
      ]
    },
    {
      "id": "05-case-studies/synthesized-2025-12-14-bug-fix---update-skipped-tests",
      "title": "synthesized-2025-12-14-bug-fix---update-skipped-tests",
      "content": "# Case Study: Bug Fix - Update skipped tests for processor/ package refactor\n\n*Synthesized from commit history: 2025-12-14*\n\n## The Problem\n\nCode quality improvements were needed. The refactoring affected 9 files: Split processor.py into modular processor/ package (LEGACY-095).\n\n## The Journey\n\nThe solution was implemented directly.\n\n\n## The Solution\n\nUpdate skipped tests for processor/ package refactor\n\nThe solution involved changes to 2 files, adding 79 lines and removing 14 lines.\n\n\n## The Lesson\n\n**Bugs often hide in unexpected places.** Thorough investigation and testing are essential for finding root causes.\n\n## Technical Details\n\n**Files Modified:** 10\n\n- `CLAUDE.md`\n- `cortical/__init__.py`\n- `cortical/processor/__init__.py`\n- `cortical/processor/compute.py`\n- `cortical/processor/core.py`\n- `cortical/processor/documents.py`\n- `cortical/processor/introspection.py`\n- `cortical/processor/persistence_api.py`\n- `cortical/processor/query_api.py`\n- `tests/test_generate_ai_metadata.py`\n\n**Code Changes:** +2913/-18 lines\n\n**Commits:** 2\n\n\n## Commits in This Story\n\n- `090910f` (2025-12-14): refactor: Split processor.py into modular processor/ package (LEGACY-095)\n- `d6718db` (2025-12-14): fix: Update skipped tests for processor/ package refactor\n\n---\n\n*This case study was automatically synthesized from git commit history.*\n",
      "keywords": [
        "processor",
        "cortical",
        "package",
        "tests",
        "refactor",
        "update",
        "skipped",
        "files",
        "solution",
        "lines"
      ]
    },
    {
      "id": "05-case-studies/synthesized-2025-12-14-bug-fix---use-heredoc-for-pyth",
      "title": "synthesized-2025-12-14-bug-fix---use-heredoc-for-pyth",
      "content": "# Case Study: Bug Fix - Use heredoc for Python in CI to avoid YAML syntax error\n\n*Synthesized from commit history: 2025-12-14*\n\n## The Problem\n\nDevelopment work began: Make push trigger explicit for all branches.\n\n## The Journey\n\nThe development progressed through several stages:\n\n1. **Make push trigger explicit for all branches** - Modified 1 files (+2/-0 lines)\n2. **Use heredoc for Python in CI to avoid YAML syntax error** - Modified 1 files (+14/-14 lines)\n\n\n## The Solution\n\nAdd test_mcp_server.py to integration tests for coverage\n\nThe solution involved changes to 1 files, adding 1 lines and removing 0 lines.\n\n\n## The Lesson\n\n**Bugs often hide in unexpected places.** Thorough investigation and testing are essential for finding root causes.\n\n## Technical Details\n\n**Files Modified:** 1\n\n- `.github/workflows/ci.yml`\n\n**Code Changes:** +17/-14 lines\n\n**Commits:** 3\n\n\n## Commits in This Story\n\n- `dfedf5e` (2025-12-14): ci: Make push trigger explicit for all branches\n- `ef84437` (2025-12-14): fix: Use heredoc for Python in CI to avoid YAML syntax error\n- `0aec3d3` (2025-12-14): ci: Add test_mcp_server.py to integration tests for coverage\n\n---\n\n*This case study was automatically synthesized from git commit history.*\n",
      "keywords": [
        "lines",
        "files",
        "use",
        "heredoc",
        "python",
        "avoid",
        "yaml",
        "syntax",
        "error",
        "make"
      ]
    },
    {
      "id": "05-case-studies/synthesized-2025-12-14-clean-up-directory-structure-a",
      "title": "synthesized-2025-12-14-clean-up-directory-structure-a",
      "content": "# Case Study: Clean up directory structure and queue search relevance fixes\n\n*Synthesized from commit history: 2025-12-14*\n\n## The Problem\n\nA new feature was required: Director mode batch execution - 6 tasks completed in parallel. This would require careful implementation and testing.\n\n## The Journey\n\nThe development progressed through several stages:\n\n1. **Director mode batch execution - 6 tasks completed in parallel** - Modified 31 files (+9381/-108 lines)\n2. **Update task status - mark 6 tasks completed from director mode batch** - Modified 2 files (+2747/-9 lines)\n3. **Clean up directory structure and queue search relevance fixes** - Modified 5 files (+68/-293 lines)\n4. **Add test file penalty and code stop word filtering to search** - Modified 3 files (+51/-9 lines)\n\n\n## The Solution\n\nMark search relevance tasks T-002, T-003 as completed\n\nThe solution involved changes to 1 files, adding 23 lines and removing 9 lines.\n\n\n## The Lesson\n\n**Bugs often hide in unexpected places.** Thorough investigation and testing are essential for finding root causes.\n\n## Technical Details\n\n**Files Modified:** 39\n\n- `CLAUDE.md`\n- `IMPLEMENTATION_SUMMARY.md`\n- `PATTERN_DETECTION_GUIDE.md`\n- `cortical/observability.py`\n- `cortical/patterns.py`\n- `cortical/processor/compute.py`\n- `cortical/processor/core.py`\n- `cortical/processor/documents.py`\n- `cortical/processor/introspection.py`\n- `cortical/processor/persistence_api.py`\n\n*...and 29 more files*\n\n**Code Changes:** +12270/-428 lines\n\n**Commits:** 5\n\n\n## Commits in This Story\n\n- `a9478fd` (2025-12-14): feat: Director mode batch execution - 6 tasks completed in parallel\n- `afc7a2d` (2025-12-14): chore: Update task status - mark 6 tasks completed from director mode batch\n- `cd8b9f5` (2025-12-14): chore: Clean up directory structure and queue search relevance fixes\n- `1fafc8b` (2025-12-14): fix: Add test file penalty and code stop word filtering to search\n- `461cb9a` (2025-12-14): chore: Mark search relevance tasks T-002, T-003 as completed\n\n---\n\n*This case study was automatically synthesized from git commit history.*\n",
      "keywords": [
        "search",
        "tasks",
        "completed",
        "files",
        "lines",
        "cortical",
        "relevance",
        "director",
        "mode",
        "batch"
      ]
    },
    {
      "id": "05-case-studies/synthesized-2025-12-14-feature-development---add-dire",
      "title": "synthesized-2025-12-14-feature-development---add-dire",
      "content": "# Case Study: Feature Development - Add director agent orchestration prompt\n\n*Synthesized from commit history: 2025-12-14*\n\n## The Problem\n\nA new feature was required: Add future tasks for text-as-memories integration. This would require careful implementation and testing.\n\n## The Journey\n\nThe development progressed through several stages:\n\n1. **Add future tasks for text-as-memories integration** - Modified 1 files (+91/-1 lines)\n2. **Add memory-manager skill and CLAUDE.md documentation** - Modified 2 files (+241/-1 lines)\n3. **Add /knowledge-transfer slash command** - Modified 2 files (+215/-0 lines)\n4. **Add merge-safety task for memory/decision filenames** - Modified 1 files (+16/-1 lines)\n5. **Add documentation improvement tasks** - Modified 1 files (+46/-1 lines)\n6. **Add director agent orchestration prompt** - Modified 2 files (+425/-1 lines)\n7. **Add memory system CLI and improve documentation** - Modified 5 files (+477/-16 lines)\n8. **Add session memory and knowledge transfer** - Modified 3 files (+254/-16 lines)\n9. **Add session handoff, auto-memory, CI link checker, and tests** - Modified 6 files (+1375/-10 lines)\n\n\n## The Solution\n\nMerge pull request #81 from scrawlsbenches/claude/implement-director-mode-NKbiu\n\nThe solution involved changes to 47 files, adding 14229 lines and removing 173 lines.\n\n\n## The Lesson\n\n**Feature development is iterative.** Breaking work into smaller commits makes it easier to review, test, and debug.\n\n## Technical Details\n\n**Files Modified:** 52\n\n- `.claude/commands/director.md`\n- `.claude/commands/knowledge-transfer.md`\n- `.claude/skills/memory-manager/SKILL.md`\n- `.github/workflows/ci.yml`\n- `.markdown-link-check.json`\n- `CLAUDE.md`\n- `README.md`\n- `cortical/observability.py`\n- `cortical/patterns.py`\n- `cortical/processor/compute.py`\n\n*...and 42 more files*\n\n**Code Changes:** +17369/-220 lines\n\n**Commits:** 10\n\n\n## Commits in This Story\n\n- `966b992` (2025-12-14): feat: Add future tasks for text-as-memories integration\n- `6d2c934` (2025-12-14): feat: Add memory-manager skill and CLAUDE.md documentation\n- `b7453a8` (2025-12-14): feat: Add /knowledge-transfer slash command\n- `ec81905` (2025-12-14): task: Add merge-safety task for memory/decision filenames\n- `87b259c` (2025-12-14): task: Add documentation improvement tasks\n- `4ab60f2` (2025-12-14): feat: Add director agent orchestration prompt\n- `d647b53` (2025-12-14): feat: Add memory system CLI and improve documentation\n- `2160f3d` (2025-12-14): memory: Add session memory and knowledge transfer\n- `6684152` (2025-12-14): feat: Add session handoff, auto-memory, CI link checker, and tests\n- `2a11bf3` (2025-12-14): Merge pull request #81 from scrawlsbenches/claude/implement-director-mode-NKbiu\n\n---\n\n*This case study was automatically synthesized from git commit history.*\n",
      "keywords": [
        "add",
        "files",
        "lines",
        "memory",
        "modified",
        "claude",
        "director",
        "documentation",
        "feat",
        "tasks"
      ]
    },
    {
      "id": "05-case-studies/synthesized-2025-12-14-feature-development---add-sess",
      "title": "synthesized-2025-12-14-feature-development---add-sess",
      "content": "# Case Study: Feature Development - Add session handoff, auto-memory, CI link checker, and tests\n\n*Synthesized from commit history: 2025-12-14*\n\n## The Problem\n\nA new feature was required: Add memory system CLI and improve documentation. This would require careful implementation and testing.\n\n## The Journey\n\nThe development progressed through several stages:\n\n1. **Add memory system CLI and improve documentation** - Modified 5 files (+477/-16 lines)\n2. **Add session memory and knowledge transfer** - Modified 3 files (+254/-16 lines)\n3. **Add session handoff, auto-memory, CI link checker, and tests** - Modified 6 files (+1375/-10 lines)\n\n\n## The Solution\n\nMerge pull request #81 from scrawlsbenches/claude/implement-director-mode-NKbiu\n\nThe solution involved changes to 47 files, adding 14229 lines and removing 173 lines.\n\n\n## The Lesson\n\n**Feature development is iterative.** Breaking work into smaller commits makes it easier to review, test, and debug.\n\n## Technical Details\n\n**Files Modified:** 48\n\n- `.github/workflows/ci.yml`\n- `.markdown-link-check.json`\n- `CLAUDE.md`\n- `README.md`\n- `cortical/observability.py`\n- `cortical/patterns.py`\n- `cortical/processor/compute.py`\n- `cortical/processor/core.py`\n- `cortical/processor/documents.py`\n- `cortical/processor/introspection.py`\n\n*...and 38 more files*\n\n**Code Changes:** +16335/-215 lines\n\n**Commits:** 4\n\n\n## Commits in This Story\n\n- `d647b53` (2025-12-14): feat: Add memory system CLI and improve documentation\n- `2160f3d` (2025-12-14): memory: Add session memory and knowledge transfer\n- `6684152` (2025-12-14): feat: Add session handoff, auto-memory, CI link checker, and tests\n- `2a11bf3` (2025-12-14): Merge pull request #81 from scrawlsbenches/claude/implement-director-mode-NKbiu\n\n---\n\n*This case study was automatically synthesized from git commit history.*\n",
      "keywords": [
        "memory",
        "add",
        "files",
        "lines",
        "cortical",
        "session",
        "link",
        "modified",
        "processor",
        "feature"
      ]
    },
    {
      "id": "05-case-studies/synthesized-2025-12-14-refactoring---migrate-to-merge",
      "title": "synthesized-2025-12-14-refactoring---migrate-to-merge",
      "content": "# Case Study: Refactoring - Migrate to merge-friendly task system and add security tasks\n\n*Synthesized from commit history: 2025-12-14*\n\n## The Problem\n\nDevelopment work began: Rename CLAUDE.md.potential to CLAUDE.md.\n\n## The Journey\n\nThe development progressed through several stages:\n\n1. **Rename CLAUDE.md.potential to CLAUDE.md** - Modified 1 files (+0/-0 lines)\n2. **Add comprehensive security knowledge transfer document** - Modified 1 files (+478/-0 lines)\n3. **Migrate to merge-friendly task system and add security tasks** - Modified 6 files (+3217/-30 lines)\n\n\n## The Solution\n\nAdd pickle warnings, deprecation notices, and CI security scanning\n\nThe solution involved changes to 3 files, adding 109 lines and removing 0 lines.\n\n\n## The Lesson\n\n**Code quality is an ongoing process.** Regular refactoring keeps the codebase maintainable and reduces technical debt.\n\n## Technical Details\n\n**Files Modified:** 10\n\n- `.claude/skills/task-manager/SKILL.md`\n- `.github/workflows/ci.yml`\n- `CLAUDE.md`\n- `README.md`\n- `TASK_LIST.md`\n- `cortical/persistence.py`\n- `docs/security-knowledge-transfer.md`\n- `scripts/migrate_legacy_tasks.py`\n- `tasks/2025-12-14_11-15-01_41d5.json`\n- `tasks/legacy_migration.json`\n\n**Code Changes:** +3804/-30 lines\n\n**Commits:** 4\n\n\n## Commits in This Story\n\n- `77b1970` (2025-12-14): chore: Rename CLAUDE.md.potential to CLAUDE.md\n- `46a0116` (2025-12-14): docs: Add comprehensive security knowledge transfer document\n- `b41c51d` (2025-12-14): refactor: Migrate to merge-friendly task system and add security tasks\n- `90b989f` (2025-12-14): security: Add pickle warnings, deprecation notices, and CI security scanning\n\n---\n\n*This case study was automatically synthesized from git commit history.*\n",
      "keywords": [
        "security",
        "claude",
        "add",
        "lines",
        "tasks",
        "files",
        "task",
        "modified",
        "migrate",
        "merge"
      ]
    },
    {
      "id": "05-case-studies/synthesized-2025-12-14-refactoring---split-processor.",
      "title": "synthesized-2025-12-14-refactoring---split-processor.",
      "content": "# Case Study: Refactoring - Split processor.py into modular processor/ package (LEGACY-095)\n\n*Synthesized from commit history: 2025-12-14*\n\n## The Problem\n\nA new feature was required: Add HMAC signature verification for pickle files (SEC-003). This would require careful implementation and testing.\n\n## The Journey\n\nThe development progressed through several stages:\n\n1. **Add HMAC signature verification for pickle files (SEC-003)** - Modified 7 files (+1872/-16 lines)\n2. **Merge pull request #80 from scrawlsbenches/claude/resume-dog-fooding-9RPIV** - Modified 18 files (+2582/-15 lines)\n3. **Split processor.py into modular processor/ package (LEGACY-095)** - Modified 9 files (+2834/-4 lines)\n\n\n## The Solution\n\nRemove dead processor.py and add mixin boundary tests (LEGACY-095)\n\nThe solution involved changes to 2 files, adding 530 lines and removing 3234 lines.\n\n\n## The Lesson\n\n**Code quality is an ongoing process.** Regular refactoring keeps the codebase maintainable and reduces technical debt.\n\n## Technical Details\n\n**Files Modified:** 32\n\n- `.claude/commands/director.md`\n- `.claude/commands/knowledge-transfer.md`\n- `.claude/skills/memory-manager/SKILL.md`\n- `.gitignore`\n- `CLAUDE.md`\n- `cortical/__init__.py`\n- `cortical/config.py`\n- `cortical/persistence.py`\n- `cortical/processor.py`\n- `cortical/processor/__init__.py`\n\n*...and 22 more files*\n\n**Code Changes:** +7818/-3269 lines\n\n**Commits:** 4\n\n\n## Commits in This Story\n\n- `6f3a1cc` (2025-12-14): feat: Add HMAC signature verification for pickle files (SEC-003)\n- `3a2d7af` (2025-12-14): Merge pull request #80 from scrawlsbenches/claude/resume-dog-fooding-9RPIV\n- `090910f` (2025-12-14): refactor: Split processor.py into modular processor/ package (LEGACY-095)\n- `890dda8` (2025-12-14): chore: Remove dead processor.py and add mixin boundary tests (LEGACY-095)\n\n---\n\n*This case study was automatically synthesized from git commit history.*\n",
      "keywords": [
        "processor",
        "files",
        "lines",
        "claude",
        "legacy",
        "add",
        "cortical",
        "modified",
        "split",
        "modular"
      ]
    },
    {
      "id": "05-case-studies/synthesized-2025-12-14-update-task-id-format-test-to-",
      "title": "synthesized-2025-12-14-update-task-id-format-test-to-",
      "content": "# Case Study: Update task ID format test to expect microseconds\n\n*Synthesized from commit history: 2025-12-14*\n\n## The Problem\n\nA bug was discovered: Add microseconds to task ID to prevent collisions. The issue needed investigation and resolution.\n\n## The Journey\n\nThe solution was implemented directly.\n\n\n## The Solution\n\nUpdate task ID format test to expect microseconds\n\nThe solution involved changes to 1 files, adding 2 lines and removing 2 lines.\n\n\n## The Lesson\n\n**Bugs often hide in unexpected places.** Thorough investigation and testing are essential for finding root causes.\n\n## Technical Details\n\n**Files Modified:** 2\n\n- `scripts/task_utils.py`\n- `tests/unit/test_task_utils.py`\n\n**Code Changes:** +7/-6 lines\n\n**Commits:** 2\n\n\n## Commits in This Story\n\n- `5970006` (2025-12-14): fix: Add microseconds to task ID to prevent collisions\n- `53c7985` (2025-12-14): test: Update task ID format test to expect microseconds\n\n---\n\n*This case study was automatically synthesized from git commit history.*\n",
      "keywords": [
        "task",
        "microseconds",
        "test",
        "update",
        "format",
        "expect",
        "solution",
        "lines",
        "case",
        "study"
      ]
    },
    {
      "id": "05-case-studies/synthesized-2025-12-15-add-ml-commit-data-for-previou",
      "title": "synthesized-2025-12-15-add-ml-commit-data-for-previou",
      "content": "# Case Study: Add ML commit data for previous commit\n\n*Synthesized from commit history: 2025-12-15*\n\n## The Problem\n\nA new feature was required: Add CI auto-capture and GitHub PR/Issue data collection. This would require careful implementation and testing.\n\n## The Journey\n\nThe development progressed through several stages:\n\n1. **Add CI auto-capture and GitHub PR/Issue data collection** - Modified 5 files (+567624/-2 lines)\n2. **Add ML commit data for previous commit** - Modified 1 files (+568009/-0 lines)\n3. **Add ML commit data** - Modified 1 files (+568045/-0 lines)\n4. **Stop tracking ML commit data files (too large for GitHub)** - Modified 472 files (+4/-2263268 lines)\n5. **Add lightweight commit data for ephemeral environments** - Modified 475 files (+11659/-13 lines)\n6. **Add ML commit data for previous commit** - Modified 1 files (+492/-0 lines)\n7. **Add ML commit data for previous commit** - Modified 1 files (+18/-0 lines)\n8. **Add ML commit data** - Modified 1 files (+18/-0 lines)\n9. **Add ML commit data** - Modified 1 files (+18/-0 lines)\n\n\n## The Solution\n\nAdd ML commit data\n\nThe solution involved changes to 1 files, adding 18 lines and removing 0 lines.\n\n\n## The Lesson\n\n**Bugs often hide in unexpected places.** Thorough investigation and testing are essential for finding root causes.\n\n## Technical Details\n\n**Files Modified:** 953\n\n- `.git-ml/commits-lite/0039ad5b13fb_2025-12-11.json`\n- `.git-ml/commits-lite/00f88d48ab42_2025-12-14.json`\n- `.git-ml/commits-lite/051d20028ddd_2025-12-13.json`\n- `.git-ml/commits-lite/051d924cae88_2025-12-13.json`\n- `.git-ml/commits-lite/059085dfe407_2025-12-10.json`\n- `.git-ml/commits-lite/0598bade86fa_2025-12-11.json`\n- `.git-ml/commits-lite/061a157b98f1_2025-12-13.json`\n- `.git-ml/commits-lite/063c542400da_2025-12-10.json`\n- `.git-ml/commits-lite/0656744909c4_2025-12-12.json`\n- `.git-ml/commits-lite/06897859b4fa_2025-12-11.json`\n\n*...and 943 more files*\n\n**Code Changes:** +1715905/-2263283 lines\n\n**Commits:** 10\n\n\n## Commits in This Story\n\n- `59bc226` (2025-12-15): feat: Add CI auto-capture and GitHub PR/Issue data collection\n- `5849304` (2025-12-15): chore: Add ML commit data for previous commit\n- `c4d25ae` (2025-12-15): chore: Add ML commit data\n- `a6f39e0` (2025-12-15): fix: Stop tracking ML commit data files (too large for GitHub)\n- `89d6aa5` (2025-12-15): feat: Add lightweight commit data for ephemeral environments\n- `84ddf26` (2025-12-15): chore: Add ML commit data for previous commit\n- `af67a1e` (2025-12-15): chore: Add ML commit data for previous commit\n- `a3471f5` (2025-12-15): chore: Add ML commit data\n- `6368f87` (2025-12-15): chore: Add ML commit data\n- `5fbd60d` (2025-12-15): chore: Add ML commit data\n\n---\n\n*This case study was automatically synthesized from git commit history.*\n",
      "keywords": [
        "commit",
        "data",
        "add",
        "files",
        "lines",
        "commits",
        "git",
        "modified",
        "lite",
        "json"
      ]
    },
    {
      "id": "05-case-studies/synthesized-2025-12-15-add-ml-commit-data",
      "title": "synthesized-2025-12-15-add-ml-commit-data",
      "content": "# Case Study: Add ML commit data\n\n*Synthesized from commit history: 2025-12-15*\n\n## The Problem\n\nA new feature was required: Add lightweight commit data for ephemeral environments. This would require careful implementation and testing.\n\n## The Journey\n\nThe development progressed through several stages:\n\n1. **Add lightweight commit data for ephemeral environments** - Modified 475 files (+11659/-13 lines)\n2. **Add ML commit data for previous commit** - Modified 1 files (+492/-0 lines)\n3. **Add ML commit data for previous commit** - Modified 1 files (+18/-0 lines)\n4. **Add ML commit data** - Modified 1 files (+18/-0 lines)\n5. **Add ML commit data** - Modified 1 files (+18/-0 lines)\n6. **Add ML commit data** - Modified 1 files (+18/-0 lines)\n7. **Add ML commit data** - Modified 1 files (+18/-0 lines)\n8. **Add ML commit data** - Modified 1 files (+18/-0 lines)\n9. **Add ML commit data** - Modified 1 files (+18/-0 lines)\n\n\n## The Solution\n\nAdd ML commit data\n\nThe solution involved changes to 1 files, adding 18 lines and removing 0 lines.\n\n\n## The Lesson\n\n**Feature development is iterative.** Breaking work into smaller commits makes it easier to review, test, and debug.\n\n## Technical Details\n\n**Files Modified:** 484\n\n- `.git-ml/commits-lite/0039ad5b13fb_2025-12-11.json`\n- `.git-ml/commits-lite/00f88d48ab42_2025-12-14.json`\n- `.git-ml/commits-lite/051d20028ddd_2025-12-13.json`\n- `.git-ml/commits-lite/051d924cae88_2025-12-13.json`\n- `.git-ml/commits-lite/059085dfe407_2025-12-10.json`\n- `.git-ml/commits-lite/0598bade86fa_2025-12-11.json`\n- `.git-ml/commits-lite/061a157b98f1_2025-12-13.json`\n- `.git-ml/commits-lite/063c542400da_2025-12-10.json`\n- `.git-ml/commits-lite/0656744909c4_2025-12-12.json`\n- `.git-ml/commits-lite/06897859b4fa_2025-12-11.json`\n\n*...and 474 more files*\n\n**Code Changes:** +12295/-13 lines\n\n**Commits:** 10\n\n\n## Commits in This Story\n\n- `89d6aa5` (2025-12-15): feat: Add lightweight commit data for ephemeral environments\n- `84ddf26` (2025-12-15): chore: Add ML commit data for previous commit\n- `af67a1e` (2025-12-15): chore: Add ML commit data for previous commit\n- `a3471f5` (2025-12-15): chore: Add ML commit data\n- `6368f87` (2025-12-15): chore: Add ML commit data\n- `5fbd60d` (2025-12-15): chore: Add ML commit data\n- `c85a5a5` (2025-12-15): chore: Add ML commit data\n- `7ace489` (2025-12-15): chore: Add ML commit data\n- `4ba9a0b` (2025-12-15): chore: Add ML commit data\n- `eadbbc8` (2025-12-15): chore: Add ML commit data\n\n---\n\n*This case study was automatically synthesized from git commit history.*\n",
      "keywords": [
        "commit",
        "add",
        "data",
        "commits",
        "files",
        "lines",
        "git",
        "modified",
        "lite",
        "json"
      ]
    },
    {
      "id": "05-case-studies/synthesized-2025-12-15-bug-fix---harden-ml-data-colle",
      "title": "synthesized-2025-12-15-bug-fix---harden-ml-data-colle",
      "content": "# Case Study: Bug Fix - Harden ML data collector with critical fixes\n\n*Synthesized from commit history: 2025-12-15*\n\n## The Problem\n\nA new feature was required: Add ML data collection infrastructure for project-specific micro-model. This would require careful implementation and testing.\n\n## The Journey\n\nThe solution was implemented directly.\n\n\n## The Solution\n\nHarden ML data collector with critical fixes\n\nThe solution involved changes to 1 files, adding 151 lines and removing 54 lines.\n\n\n## The Lesson\n\n**Bugs often hide in unexpected places.** Thorough investigation and testing are essential for finding root causes.\n\n## Technical Details\n\n**Files Modified:** 4\n\n- `.claude/hooks/session_logger.py`\n- `.claude/skills/ml-logger/SKILL.md`\n- `.gitignore`\n- `scripts/ml_data_collector.py`\n\n**Code Changes:** +1190/-54 lines\n\n**Commits:** 2\n\n\n## Commits in This Story\n\n- `1568f3c` (2025-12-15): feat: Add ML data collection infrastructure for project-specific micro-model\n- `4438d60` (2025-12-15): fix: Harden ML data collector with critical fixes\n\n---\n\n*This case study was automatically synthesized from git commit history.*\n",
      "keywords": [
        "data",
        "harden",
        "collector",
        "critical",
        "fixes",
        "solution",
        "lines",
        "case",
        "study",
        "fix"
      ]
    },
    {
      "id": "05-case-studies/synthesized-2025-12-15-bug-fix---increase-ml-data-ret",
      "title": "synthesized-2025-12-15-bug-fix---increase-ml-data-ret",
      "content": "# Case Study: Bug Fix - Increase ML data retention to 2 years for training milestones\n\n*Synthesized from commit history: 2025-12-15*\n\n## The Problem\n\nA new feature was required: Add privacy features to ML data collection. This would require careful implementation and testing.\n\n## The Journey\n\nThe development progressed through several stages:\n\n1. **Add privacy features to ML data collection** - Modified 2 files (+628/-1 lines)\n2. **Add ML data collection section to README** - Modified 1 files (+89/-0 lines)\n3. **Increase ML data retention to 2 years for training milestones** - Modified 2 files (+7/-5 lines)\n4. **Add automatic ML data collection on session startup** - Modified 3 files (+95/-22 lines)\n\n\n## The Solution\n\nShare ML commit data and aggregated patterns in git\n\nThe solution involved changes to 474 files, adding 561272 lines and removing 4 lines.\n\n\n## The Lesson\n\n**Bugs often hide in unexpected places.** Thorough investigation and testing are essential for finding root causes.\n\n## Technical Details\n\n**Files Modified:** 478\n\n- `.claude/settings.local.json`\n- `.git-ml/commits/0039ad5b_2025-12-11_24b1b10a.json`\n- `.git-ml/commits/00f88d48_2025-12-14_8749d448.json`\n- `.git-ml/commits/051d2002_2025-12-13_7896a312.json`\n- `.git-ml/commits/051d924c_2025-12-13_bfbb2049.json`\n- `.git-ml/commits/059085df_2025-12-10_4adc6156.json`\n- `.git-ml/commits/0598bade_2025-12-11_ab9a0c3b.json`\n- `.git-ml/commits/061a157b_2025-12-13_af26e95e.json`\n- `.git-ml/commits/063c5424_2025-12-10_c2422fa6.json`\n- `.git-ml/commits/06567449_2025-12-12_95eabdde.json`\n\n*...and 468 more files*\n\n**Code Changes:** +562091/-32 lines\n\n**Commits:** 5\n\n\n## Commits in This Story\n\n- `e188508` (2025-12-15): feat: Add privacy features to ML data collection\n- `df75750` (2025-12-15): docs: Add ML data collection section to README\n- `95e9f06` (2025-12-15): fix: Increase ML data retention to 2 years for training milestones\n- `b805e13` (2025-12-15): feat: Add automatic ML data collection on session startup\n- `6570973` (2025-12-15): feat: Share ML commit data and aggregated patterns in git\n\n---\n\n*This case study was automatically synthesized from git commit history.*\n",
      "keywords": [
        "data",
        "git",
        "commits",
        "json",
        "add",
        "collection",
        "files",
        "lines",
        "modified",
        "commit"
      ]
    },
    {
      "id": "05-case-studies/synthesized-2025-12-15-bug-fix---stop-tracking-ml-com",
      "title": "synthesized-2025-12-15-bug-fix---stop-tracking-ml-com",
      "content": "# Case Study: Bug Fix - Stop tracking ML commit data files (too large for GitHub)\n\n*Synthesized from commit history: 2025-12-15*\n\n## The Problem\n\nA new feature was required: Add automatic ML data collection on session startup. This would require careful implementation and testing.\n\n## The Journey\n\nThe development progressed through several stages:\n\n1. **Add automatic ML data collection on session startup** - Modified 3 files (+95/-22 lines)\n2. **Share ML commit data and aggregated patterns in git** - Modified 474 files (+561272/-4 lines)\n3. **Add CI auto-capture and GitHub PR/Issue data collection** - Modified 5 files (+567624/-2 lines)\n4. **Add ML commit data for previous commit** - Modified 1 files (+568009/-0 lines)\n5. **Add ML commit data** - Modified 1 files (+568045/-0 lines)\n6. **Stop tracking ML commit data files (too large for GitHub)** - Modified 472 files (+4/-2263268 lines)\n7. **Add lightweight commit data for ephemeral environments** - Modified 475 files (+11659/-13 lines)\n8. **Add ML commit data for previous commit** - Modified 1 files (+492/-0 lines)\n9. **Add ML commit data for previous commit** - Modified 1 files (+18/-0 lines)\n\n\n## The Solution\n\nAdd ML commit data\n\nThe solution involved changes to 1 files, adding 18 lines and removing 0 lines.\n\n\n## The Lesson\n\n**Bugs often hide in unexpected places.** Thorough investigation and testing are essential for finding root causes.\n\n## Technical Details\n\n**Files Modified:** 957\n\n- `.claude/settings.local.json`\n- `.git-ml/commits-lite/0039ad5b13fb_2025-12-11.json`\n- `.git-ml/commits-lite/00f88d48ab42_2025-12-14.json`\n- `.git-ml/commits-lite/051d20028ddd_2025-12-13.json`\n- `.git-ml/commits-lite/051d924cae88_2025-12-13.json`\n- `.git-ml/commits-lite/059085dfe407_2025-12-10.json`\n- `.git-ml/commits-lite/0598bade86fa_2025-12-11.json`\n- `.git-ml/commits-lite/061a157b98f1_2025-12-13.json`\n- `.git-ml/commits-lite/063c542400da_2025-12-10.json`\n- `.git-ml/commits-lite/0656744909c4_2025-12-12.json`\n\n*...and 947 more files*\n\n**Code Changes:** +2277236/-2263309 lines\n\n**Commits:** 10\n\n\n## Commits in This Story\n\n- `b805e13` (2025-12-15): feat: Add automatic ML data collection on session startup\n- `6570973` (2025-12-15): feat: Share ML commit data and aggregated patterns in git\n- `59bc226` (2025-12-15): feat: Add CI auto-capture and GitHub PR/Issue data collection\n- `5849304` (2025-12-15): chore: Add ML commit data for previous commit\n- `c4d25ae` (2025-12-15): chore: Add ML commit data\n- `a6f39e0` (2025-12-15): fix: Stop tracking ML commit data files (too large for GitHub)\n- `89d6aa5` (2025-12-15): feat: Add lightweight commit data for ephemeral environments\n- `84ddf26` (2025-12-15): chore: Add ML commit data for previous commit\n- `af67a1e` (2025-12-15): chore: Add ML commit data for previous commit\n- `a3471f5` (2025-12-15): chore: Add ML commit data\n\n---\n\n*This case study was automatically synthesized from git commit history.*\n",
      "keywords": [
        "commit",
        "data",
        "add",
        "files",
        "lines",
        "git",
        "commits",
        "modified",
        "json",
        "lite"
      ]
    },
    {
      "id": "05-case-studies/synthesized-2025-12-15-bug-fix---update-tests-for-bm2",
      "title": "synthesized-2025-12-15-bug-fix---update-tests-for-bm2",
      "content": "# Case Study: Bug Fix - Update tests for BM25 default and stop word tokenization\n\n*Synthesized from commit history: 2025-12-15*\n\n## The Problem\n\nDevelopment work began: Add Stop hook config and update task tracking.\n\n## The Journey\n\nThe development progressed through several stages:\n\n1. **Add Stop hook config and update task tracking** - Modified 3 files (+148/-10 lines)\n2. **Update tests for BM25 default and stop word tokenization** - Modified 2 files (+23/-10 lines)\n\n\n## The Solution\n\nMerge remote-tracking branch 'origin/main' into claude/multi-index-design-DvifZ\n\nThe solution involved changes to 22 files, adding 3516 lines and removing 75 lines.\n\n\n## The Lesson\n\n**Bugs often hide in unexpected places.** Thorough investigation and testing are essential for finding root causes.\n\n## Technical Details\n\n**Files Modified:** 25\n\n- `.claude/settings.local.json`\n- `CLAUDE.md`\n- `benchmarks/BASELINE_SUMMARY.md`\n- `benchmarks/after_bm25.json`\n- `benchmarks/baseline_tfidf.json`\n- `benchmarks/baseline_tfidf_real.json`\n- `cortical/analysis.py`\n- `cortical/config.py`\n- `cortical/processor/compute.py`\n- `cortical/processor/core.py`\n\n*...and 15 more files*\n\n**Code Changes:** +3687/-95 lines\n\n**Commits:** 3\n\n\n## Commits in This Story\n\n- `293a467` (2025-12-15): chore: Add Stop hook config and update task tracking\n- `9dc7268` (2025-12-15): fix: Update tests for BM25 default and stop word tokenization\n- `ed36d6e` (2025-12-15): Merge remote-tracking branch 'origin/main' into claude/multi-index-design-DvifZ\n\n---\n\n*This case study was automatically synthesized from git commit history.*\n",
      "keywords": [
        "update",
        "stop",
        "tracking",
        "files",
        "lines",
        "config",
        "claude",
        "json",
        "benchmarks",
        "cortical"
      ]
    },
    {
      "id": "05-case-studies/synthesized-2025-12-15-feature-development---add-ci-s",
      "title": "synthesized-2025-12-15-feature-development---add-ci-s",
      "content": "# Case Study: Feature Development - Add CI status integration for ML outcome tracking\n\n*Synthesized from commit history: 2025-12-15*\n\n## The Problem\n\nA new feature was required: Add commit-chat session linking for ML training. This would require careful implementation and testing.\n\n## The Journey\n\nThe development progressed through several stages:\n\n1. **Add commit-chat session linking for ML training** - Modified 1 files (+234/-6 lines)\n2. **Add CI status integration for ML outcome tracking** - Modified 1 files (+180/-0 lines)\n\n\n## The Solution\n\nAddress audit findings and add documentation\n\nThe solution involved changes to 4 files, adding 201 lines and removing 15 lines.\n\n\n## The Lesson\n\n**Bugs often hide in unexpected places.** Thorough investigation and testing are essential for finding root causes.\n\n## Technical Details\n\n**Files Modified:** 4\n\n- `.claude/commands/ml-log.md`\n- `.claude/commands/ml-stats.md`\n- `CLAUDE.md`\n- `scripts/ml_data_collector.py`\n\n**Code Changes:** +615/-21 lines\n\n**Commits:** 3\n\n\n## Commits in This Story\n\n- `1b7f6d5` (2025-12-15): feat: Add commit-chat session linking for ML training\n- `ed66817` (2025-12-15): feat: Add CI status integration for ML outcome tracking\n- `36be3a1` (2025-12-15): fix: Address audit findings and add documentation\n\n---\n\n*This case study was automatically synthesized from git commit history.*\n",
      "keywords": [
        "add",
        "commit",
        "lines",
        "files",
        "status",
        "integration",
        "outcome",
        "tracking",
        "chat",
        "session"
      ]
    },
    {
      "id": "05-case-studies/synthesized-2025-12-15-feature-development---add-comp",
      "title": "synthesized-2025-12-15-feature-development---add-comp",
      "content": "# Case Study: Feature Development - Add comprehensive delegation command template\n\n*Synthesized from commit history: 2025-12-15*\n\n## The Problem\n\nA new feature was required: Add reusable pre-merge sanity check command. This would require careful implementation and testing.\n\n## The Journey\n\nThe development progressed through several stages:\n\n1. **Add reusable pre-merge sanity check command** - Modified 1 files (+79/-0 lines)\n2. **Add comprehensive delegation command template** - Modified 1 files (+165/-0 lines)\n\n\n## The Solution\n\nMerge remote-tracking branch 'origin/main' into claude/multi-index-design-DvifZ\n\nThe solution involved changes to 28 files, adding 6031 lines and removing 63 lines.\n\n\n## The Lesson\n\n**Feature development is iterative.** Breaking work into smaller commits makes it easier to review, test, and debug.\n\n## Technical Details\n\n**Files Modified:** 28\n\n- `.claude/commands/delegate.md`\n- `.claude/commands/ml-log.md`\n- `.claude/commands/ml-stats.md`\n- `.claude/commands/sanity-check.md`\n- `.claude/hooks/session_logger.py`\n- `.claude/settings.local.json`\n- `.claude/skills/ml-logger/SKILL.md`\n- `.gitignore`\n- `CLAUDE.md`\n- `README.md`\n\n*...and 18 more files*\n\n**Code Changes:** +6275/-63 lines\n\n**Commits:** 3\n\n\n## Commits in This Story\n\n- `cc0ff38` (2025-12-15): feat: Add reusable pre-merge sanity check command\n- `aac63a7` (2025-12-15): feat: Add comprehensive delegation command template\n- `f371d95` (2025-12-15): Merge remote-tracking branch 'origin/main' into claude/multi-index-design-DvifZ\n\n---\n\n*This case study was automatically synthesized from git commit history.*\n",
      "keywords": [
        "claude",
        "add",
        "command",
        "merge",
        "files",
        "lines",
        "sanity",
        "check",
        "commands",
        "feature"
      ]
    },
    {
      "id": "05-case-studies/synthesized-2025-12-15-feature-development---add-expo",
      "title": "synthesized-2025-12-15-feature-development---add-expo",
      "content": "# Case Study: Feature Development - Add export, feedback, and quality-report commands to ML collector\n\n*Synthesized from commit history: 2025-12-15*\n\n## The Problem\n\nA new feature was required: Add session handoff generator for context preservation. This would require careful implementation and testing.\n\n## The Journey\n\nThe development progressed through several stages:\n\n1. **Add session handoff generator for context preservation** - Modified 4 files (+442/-0 lines)\n2. **Add export, feedback, and quality-report commands to ML collector** - Modified 1 files (+769/-7 lines)\n\n\n## The Solution\n\nUpdate stale query.py and processor.py references\n\nThe solution involved changes to 11 files, adding 95 lines and removing 71 lines.\n\n\n## The Lesson\n\n**Feature development is iterative.** Breaking work into smaller commits makes it easier to review, test, and debug.\n\n## Technical Details\n\n**Files Modified:** 14\n\n- `.claude/skills/ml-logger/SKILL.md`\n- `CLAUDE.md`\n- `docs/algorithms.md`\n- `docs/architecture.md`\n- `docs/claude-usage.md`\n- `docs/code-of-ethics.md`\n- `docs/devex-tools.md`\n- `docs/dogfooding.md`\n- `docs/glossary.md`\n- `docs/louvain_resolution_analysis.md`\n\n*...and 4 more files*\n\n**Code Changes:** +1306/-78 lines\n\n**Commits:** 3\n\n\n## Commits in This Story\n\n- `9bd4067` (2025-12-15): feat: Add session handoff generator for context preservation\n- `a75761b` (2025-12-15): feat: Add export, feedback, and quality-report commands to ML collector\n- `86cc3bb` (2025-12-15): docs: Update stale query.py and processor.py references\n\n---\n\n*This case study was automatically synthesized from git commit history.*\n",
      "keywords": [
        "docs",
        "add",
        "files",
        "lines",
        "feature",
        "development",
        "export",
        "feedback",
        "quality",
        "report"
      ]
    },
    {
      "id": "05-case-studies/synthesized-2025-12-15-feature-development---add-ligh",
      "title": "synthesized-2025-12-15-feature-development---add-ligh",
      "content": "# Case Study: Feature Development - Add lightweight commit data for ephemeral environments\n\n*Synthesized from commit history: 2025-12-15*\n\n## The Problem\n\nA new feature was required: Share ML commit data and aggregated patterns in git. This would require careful implementation and testing.\n\n## The Journey\n\nThe development progressed through several stages:\n\n1. **Share ML commit data and aggregated patterns in git** - Modified 474 files (+561272/-4 lines)\n2. **Add CI auto-capture and GitHub PR/Issue data collection** - Modified 5 files (+567624/-2 lines)\n3. **Add ML commit data for previous commit** - Modified 1 files (+568009/-0 lines)\n4. **Add ML commit data** - Modified 1 files (+568045/-0 lines)\n5. **Stop tracking ML commit data files (too large for GitHub)** - Modified 472 files (+4/-2263268 lines)\n6. **Add lightweight commit data for ephemeral environments** - Modified 475 files (+11659/-13 lines)\n7. **Add ML commit data for previous commit** - Modified 1 files (+492/-0 lines)\n8. **Add ML commit data for previous commit** - Modified 1 files (+18/-0 lines)\n9. **Add ML commit data** - Modified 1 files (+18/-0 lines)\n\n\n## The Solution\n\nAdd ML commit data\n\nThe solution involved changes to 1 files, adding 18 lines and removing 0 lines.\n\n\n## The Lesson\n\n**Bugs often hide in unexpected places.** Thorough investigation and testing are essential for finding root causes.\n\n## Technical Details\n\n**Files Modified:** 956\n\n- `.git-ml/commits-lite/0039ad5b13fb_2025-12-11.json`\n- `.git-ml/commits-lite/00f88d48ab42_2025-12-14.json`\n- `.git-ml/commits-lite/051d20028ddd_2025-12-13.json`\n- `.git-ml/commits-lite/051d924cae88_2025-12-13.json`\n- `.git-ml/commits-lite/059085dfe407_2025-12-10.json`\n- `.git-ml/commits-lite/0598bade86fa_2025-12-11.json`\n- `.git-ml/commits-lite/061a157b98f1_2025-12-13.json`\n- `.git-ml/commits-lite/063c542400da_2025-12-10.json`\n- `.git-ml/commits-lite/0656744909c4_2025-12-12.json`\n- `.git-ml/commits-lite/06897859b4fa_2025-12-11.json`\n\n*...and 946 more files*\n\n**Code Changes:** +2277159/-2263287 lines\n\n**Commits:** 10\n\n\n## Commits in This Story\n\n- `6570973` (2025-12-15): feat: Share ML commit data and aggregated patterns in git\n- `59bc226` (2025-12-15): feat: Add CI auto-capture and GitHub PR/Issue data collection\n- `5849304` (2025-12-15): chore: Add ML commit data for previous commit\n- `c4d25ae` (2025-12-15): chore: Add ML commit data\n- `a6f39e0` (2025-12-15): fix: Stop tracking ML commit data files (too large for GitHub)\n- `89d6aa5` (2025-12-15): feat: Add lightweight commit data for ephemeral environments\n- `84ddf26` (2025-12-15): chore: Add ML commit data for previous commit\n- `af67a1e` (2025-12-15): chore: Add ML commit data for previous commit\n- `a3471f5` (2025-12-15): chore: Add ML commit data\n- `6368f87` (2025-12-15): chore: Add ML commit data\n\n---\n\n*This case study was automatically synthesized from git commit history.*\n",
      "keywords": [
        "commit",
        "data",
        "add",
        "git",
        "files",
        "lines",
        "commits",
        "modified",
        "lite",
        "json"
      ]
    },
    {
      "id": "05-case-studies/synthesized-2025-12-15-feature-development---add-sche",
      "title": "synthesized-2025-12-15-feature-development---add-sche",
      "content": "# Case Study: Feature Development - Add schema validation for ML data integrity\n\n*Synthesized from commit history: 2025-12-15*\n\n## The Problem\n\nA new feature was required: Add CI status integration for ML outcome tracking. This would require careful implementation and testing.\n\n## The Journey\n\nThe development progressed through several stages:\n\n1. **Add CI status integration for ML outcome tracking** - Modified 1 files (+180/-0 lines)\n2. **Add commit-chat session linking for ML training** - Modified 1 files (+234/-6 lines)\n3. **Add schema validation for ML data integrity** - Modified 1 files (+177/-9 lines)\n4. **Address audit findings and add documentation** - Modified 4 files (+201/-15 lines)\n\n\n## The Solution\n\nAdd Development Environment Setup section to CLAUDE.md\n\nThe solution involved changes to 1 files, adding 23 lines and removing 0 lines.\n\n\n## The Lesson\n\n**Bugs often hide in unexpected places.** Thorough investigation and testing are essential for finding root causes.\n\n## Technical Details\n\n**Files Modified:** 4\n\n- `.claude/commands/ml-log.md`\n- `.claude/commands/ml-stats.md`\n- `CLAUDE.md`\n- `scripts/ml_data_collector.py`\n\n**Code Changes:** +815/-30 lines\n\n**Commits:** 5\n\n\n## Commits in This Story\n\n- `ed66817` (2025-12-15): feat: Add CI status integration for ML outcome tracking\n- `1b7f6d5` (2025-12-15): feat: Add commit-chat session linking for ML training\n- `1d9f520` (2025-12-15): feat: Add schema validation for ML data integrity\n- `36be3a1` (2025-12-15): fix: Address audit findings and add documentation\n- `bdea0a5` (2025-12-15): Add Development Environment Setup section to CLAUDE.md\n\n---\n\n*This case study was automatically synthesized from git commit history.*\n",
      "keywords": [
        "add",
        "lines",
        "files",
        "modified",
        "claude",
        "development",
        "commit",
        "schema",
        "validation",
        "data"
      ]
    },
    {
      "id": "05-case-studies/synthesized-2025-12-15-refactoring---consolidate-ml-d",
      "title": "synthesized-2025-12-15-refactoring---consolidate-ml-d",
      "content": "# Case Study: Refactoring - Consolidate ML data to single JSONL files\n\n*Synthesized from commit history: 2025-12-15*\n\n## The Problem\n\nDevelopment work began: Add ML commit data.\n\n## The Journey\n\nThe development progressed through several stages:\n\n1. **Add ML commit data** - Modified 1 files (+18/-0 lines)\n2. **Add ML commit data** - Modified 1 files (+18/-0 lines)\n3. **Consolidate ML data to single JSONL files** - Modified 486 files (+658/-12208 lines)\n\n\n## The Solution\n\nMerge pull request #95 from scrawlsbenches/claude/check-director-data-collection-WzK3q\n\nThe solution involved changes to 8 files, adding 776 lines and removing 10 lines.\n\n\n## The Lesson\n\n**Code quality is an ongoing process.** Regular refactoring keeps the codebase maintainable and reduces technical debt.\n\n## Technical Details\n\n**Files Modified:** 492\n\n- `.claude/commands/director.md`\n- `.git-ml/commits-lite/0039ad5b13fb_2025-12-11.json`\n- `.git-ml/commits-lite/00f88d48ab42_2025-12-14.json`\n- `.git-ml/commits-lite/051d20028ddd_2025-12-13.json`\n- `.git-ml/commits-lite/051d924cae88_2025-12-13.json`\n- `.git-ml/commits-lite/059085dfe407_2025-12-10.json`\n- `.git-ml/commits-lite/0598bade86fa_2025-12-11.json`\n- `.git-ml/commits-lite/061a157b98f1_2025-12-13.json`\n- `.git-ml/commits-lite/063c542400da_2025-12-10.json`\n- `.git-ml/commits-lite/0656744909c4_2025-12-12.json`\n\n*...and 482 more files*\n\n**Code Changes:** +1470/-12218 lines\n\n**Commits:** 4\n\n\n## Commits in This Story\n\n- `eadbbc8` (2025-12-15): chore: Add ML commit data\n- `4ba9a0b` (2025-12-15): chore: Add ML commit data\n- `205fe34` (2025-12-15): refactor: Consolidate ML data to single JSONL files\n- `8e919a3` (2025-12-15): Merge pull request #95 from scrawlsbenches/claude/check-director-data-collection-WzK3q\n\n---\n\n*This case study was automatically synthesized from git commit history.*\n",
      "keywords": [
        "commits",
        "data",
        "git",
        "files",
        "lite",
        "json",
        "commit",
        "lines",
        "add",
        "modified"
      ]
    },
    {
      "id": "05-case-studies/synthesized-2025-12-16-add-unit-tests-for-cortical-ch",
      "title": "synthesized-2025-12-16-add-unit-tests-for-cortical-ch",
      "content": "# Case Study: Add unit tests for Cortical Chronicles generators\n\n*Synthesized from commit history: 2025-12-16*\n\n## The Problem\n\nA new feature was required: Add search integration and web interface (Wave 3). This would require careful implementation and testing.\n\n## The Journey\n\nThe development progressed through several stages:\n\n1. **Add search integration and web interface (Wave 3)** - Modified 11 files (+2959/-1 lines)\n2. **Add CI workflow and documentation (Wave 4)** - Modified 5 files (+1724/-1 lines)\n3. **Wave 5 integration testing verification** - Modified 25 files (+227/-88 lines)\n4. **ML data sync** - Modified 43 files (+904/-6 lines)\n5. **ML data sync** - Modified 44 files (+923/-0 lines)\n6. **Add unit tests for Cortical Chronicles generators** - Modified 2 files (+1372/-0 lines)\n7. **ML data sync** - Modified 43 files (+922/-0 lines)\n8. **ML data sync** - Modified 45 files (+941/-0 lines)\n9. **ML data sync** - Modified 46 files (+981/-0 lines)\n\n\n## The Solution\n\nAdd consolidated markdown book generation\n\nThe solution involved changes to 3 files, adding 6392 lines and removing 2 lines.\n\n\n## The Lesson\n\n**Feature development is iterative.** Breaking work into smaller commits makes it easier to review, test, and debug.\n\n## Technical Details\n\n**Files Modified:** 254\n\n- `.git-ml/actions/2025-12-16/A-20251216-172758-1540-000.json`\n- `.git-ml/actions/2025-12-16/A-20251216-172758-1540-001.json`\n- `.git-ml/actions/2025-12-16/A-20251216-172758-1540-002.json`\n- `.git-ml/actions/2025-12-16/A-20251216-172758-1540-003.json`\n- `.git-ml/actions/2025-12-16/A-20251216-172758-1540-004.json`\n- `.git-ml/actions/2025-12-16/A-20251216-172758-1540-005.json`\n- `.git-ml/actions/2025-12-16/A-20251216-172758-1540-006.json`\n- `.git-ml/actions/2025-12-16/A-20251216-172758-1540-007.json`\n- `.git-ml/actions/2025-12-16/A-20251216-172758-1540-008.json`\n- `.git-ml/actions/2025-12-16/A-20251216-172758-1540-009.json`\n\n*...and 244 more files*\n\n**Code Changes:** +17345/-98 lines\n\n**Commits:** 10\n\n\n## Commits in This Story\n\n- `0022466` (2025-12-16): feat: Add search integration and web interface (Wave 3)\n- `940fdf2` (2025-12-16): feat: Add CI workflow and documentation (Wave 4)\n- `1504899` (2025-12-16): chore: Wave 5 integration testing verification\n- `590f46f` (2025-12-16): chore: ML data sync\n- `be019d3` (2025-12-16): chore: ML data sync\n- `a09bd89` (2025-12-16): test: Add unit tests for Cortical Chronicles generators\n- `a16f142` (2025-12-16): chore: ML data sync\n- `d8ba759` (2025-12-16): chore: ML data sync\n- `18ffbcd` (2025-12-16): chore: ML data sync\n- `f8a2ad6` (2025-12-16): feat: Add consolidated markdown book generation\n\n---\n\n*This case study was automatically synthesized from git commit history.*\n",
      "keywords": [
        "files",
        "lines",
        "git",
        "add",
        "modified",
        "data",
        "sync",
        "actions",
        "json",
        "wave"
      ]
    },
    {
      "id": "05-case-studies/synthesized-2025-12-16-bug-fix---address-critical-ml-",
      "title": "synthesized-2025-12-16-bug-fix---address-critical-ml-",
      "content": "# Case Study: Bug Fix - Address critical ML data collection and prediction issues\n\n*Synthesized from commit history: 2025-12-16*\n\n## The Problem\n\nDevelopment work began: Add ML chat data from investigation sessions.\n\n## The Journey\n\nThe development progressed through several stages:\n\n1. **Add ML chat data from investigation sessions** - Modified 6 files (+98/-27 lines)\n2. **Mock file existence in ML prediction tests** - Modified 2 files (+21/-8 lines)\n3. **Address critical ML data collection and prediction issues** - Modified 9 files (+148/-17 lines)\n\n\n## The Solution\n\nUpdate ML chat data from orchestration session\n\nThe solution involved changes to 14 files, adding 146 lines and removing 6 lines.\n\n\n## The Lesson\n\n**Bugs often hide in unexpected places.** Thorough investigation and testing are essential for finding root causes.\n\n## Technical Details\n\n**Files Modified:** 19\n\n- `.git-ml/chats/2025-12-15/chat-20251216-115057-b5bb48.json`\n- `.git-ml/chats/2025-12-15/chat-20251216-120351-efac30.json`\n- `.git-ml/chats/2025-12-16/chat-20251216-115057-3617f9.json`\n- `.git-ml/chats/2025-12-16/chat-20251216-115057-9502fd.json`\n- `.git-ml/chats/2025-12-16/chat-20251216-115057-cbbe64.json`\n- `.git-ml/chats/2025-12-16/chat-20251216-115057-f65b7a.json`\n- `.git-ml/chats/2025-12-16/chat-20251216-120351-1b48d7.json`\n- `.git-ml/chats/2025-12-16/chat-20251216-120351-86411f.json`\n- `.git-ml/chats/2025-12-16/chat-20251216-120351-c08c4a.json`\n- `.git-ml/chats/2025-12-16/chat-20251216-120351-c1c3b1.json`\n\n*...and 9 more files*\n\n**Code Changes:** +413/-58 lines\n\n**Commits:** 4\n\n\n## Commits in This Story\n\n- `c08cc75` (2025-12-16): chore: Add ML chat data from investigation sessions\n- `ec8db7a` (2025-12-16): fix(tests): Mock file existence in ML prediction tests\n- `fead1c1` (2025-12-16): fix: Address critical ML data collection and prediction issues\n- `a304a1d` (2025-12-16): chore: Update ML chat data from orchestration session\n\n---\n\n*This case study was automatically synthesized from git commit history.*\n",
      "keywords": [
        "chat",
        "git",
        "chats",
        "json",
        "data",
        "files",
        "lines",
        "prediction",
        "investigation",
        "modified"
      ]
    },
    {
      "id": "05-case-studies/synthesized-2025-12-16-bug-fix---archive-ml-session-a",
      "title": "synthesized-2025-12-16-bug-fix---archive-ml-session-a",
      "content": "# Case Study: Bug Fix - Archive ML session after transcript processing (T-003 16f3)\n\n*Synthesized from commit history: 2025-12-16*\n\n## The Problem\n\nDevelopment work began: ML data from director review session.\n\n## The Journey\n\nThe solution was implemented directly.\n\n\n## The Solution\n\nArchive ML session after transcript processing (T-003 16f3)\n\nThe solution involved changes to 1 files, adding 12 lines and removing 0 lines.\n\n\n## The Lesson\n\n**Bugs often hide in unexpected places.** Thorough investigation and testing are essential for finding root causes.\n\n## Technical Details\n\n**Files Modified:** 13\n\n- `.git-ml/actions/2025-12-16/A-20251216-132552-2869-000.json`\n- `.git-ml/actions/2025-12-16/A-20251216-132552-2869-001.json`\n- `.git-ml/actions/2025-12-16/A-20251216-132552-2869-002.json`\n- `.git-ml/actions/2025-12-16/A-20251216-132827-2869-000.json`\n- `.git-ml/actions/2025-12-16/A-20251216-132827-2869-001.json`\n- `.git-ml/actions/2025-12-16/A-20251216-132827-2869-002.json`\n- `.git-ml/chats/2025-12-16/chat-20251216-132552-3c1c3c.json`\n- `.git-ml/chats/2025-12-16/chat-20251216-132552-8b8c20.json`\n- `.git-ml/chats/2025-12-16/chat-20251216-132827-0135aa.json`\n- `.git-ml/chats/2025-12-16/chat-20251216-132827-5d9c33.json`\n\n*...and 3 more files*\n\n**Code Changes:** +241/-1 lines\n\n**Commits:** 2\n\n\n## Commits in This Story\n\n- `10036d1` (2025-12-16): chore: ML data from director review session\n- `59072c8` (2025-12-16): fix: Archive ML session after transcript processing (T-003 16f3)\n\n---\n\n*This case study was automatically synthesized from git commit history.*\n",
      "keywords": [
        "git",
        "json",
        "actions",
        "session",
        "chats",
        "chat",
        "archive",
        "transcript",
        "processing",
        "solution"
      ]
    },
    {
      "id": "05-case-studies/synthesized-2025-12-16-bug-fix---fix-ml-data-collecti",
      "title": "synthesized-2025-12-16-bug-fix---fix-ml-data-collecti",
      "content": "# Case Study: Bug Fix - Fix ML data collection milestone counting and add session/action capture\n\n*Synthesized from commit history: 2025-12-16*\n\n## The Problem\n\nDevelopment work began: Update ML chat data from investigation session.\n\n## The Journey\n\nThe development progressed through several stages:\n\n1. **Update ML chat data from investigation session** - Modified 22 files (+182/-33 lines)\n2. **Update ML chat data from orchestration session** - Modified 14 files (+146/-6 lines)\n3. **Fix ML data collection milestone counting and add session/action capture** - Modified 11 files (+95/-29 lines)\n4. **ML data from session with new action/session collection** - Modified 18 files (+319/-1 lines)\n\n\n## The Solution\n\nBatch task distribution implementation via Director orchestration\n\nThe solution involved changes to 8 files, adding 3185 lines and removing 89 lines.\n\n\n## The Lesson\n\n**Bugs often hide in unexpected places.** Thorough investigation and testing are essential for finding root causes.\n\n## Technical Details\n\n**Files Modified:** 46\n\n- `.claude/hooks/session_logger.py`\n- `.git-ml/actions/2025-12-15/A-20251216-122826-0299-000.json`\n- `.git-ml/actions/2025-12-16/A-20251216-122826-0299-001.json`\n- `.git-ml/actions/2025-12-16/A-20251216-122826-0299-002.json`\n- `.git-ml/actions/2025-12-16/A-20251216-122826-0299-003.json`\n- `.git-ml/actions/2025-12-16/A-20251216-122826-0299-004.json`\n- `.git-ml/actions/2025-12-16/A-20251216-122826-0299-005.json`\n- `.git-ml/chats/2025-12-15/chat-20251216-115057-b5bb48.json`\n- `.git-ml/chats/2025-12-15/chat-20251216-120351-efac30.json`\n- `.git-ml/chats/2025-12-15/chat-20251216-121720-30c3c1.json`\n\n*...and 36 more files*\n\n**Code Changes:** +3927/-158 lines\n\n**Commits:** 5\n\n\n## Commits in This Story\n\n- `ba3a05b` (2025-12-16): chore: Update ML chat data from investigation session\n- `a304a1d` (2025-12-16): chore: Update ML chat data from orchestration session\n- `273baef` (2025-12-16): fix: Fix ML data collection milestone counting and add session/action capture\n- `de8ca40` (2025-12-16): chore: ML data from session with new action/session collection\n- `4f915c3` (2025-12-16): feat: Batch task distribution implementation via Director orchestration\n\n---\n\n*This case study was automatically synthesized from git commit history.*\n",
      "keywords": [
        "session",
        "data",
        "git",
        "json",
        "chat",
        "files",
        "lines",
        "actions",
        "fix",
        "collection"
      ]
    },
    {
      "id": "05-case-studies/synthesized-2025-12-16-bug-fix---prevent-infinite-com",
      "title": "synthesized-2025-12-16-bug-fix---prevent-infinite-com",
      "content": "# Case Study: Bug Fix - Prevent infinite commit loop in ML data collection hooks\n\n*Synthesized from commit history: 2025-12-16*\n\n## The Problem\n\nDevelopment work began: ML tracking data.\n\n## The Journey\n\nThe development progressed through several stages:\n\n1. **ML tracking data** - Modified 2 files (+2/-1 lines)\n2. **ML tracking data** - Modified 2 files (+2/-1 lines)\n3. **ML tracking data** - Modified 2 files (+2/-1 lines)\n4. **Prevent infinite commit loop in ML data collection hooks** - Modified 3 files (+9/-1 lines)\n5. **ML tracking data** - Modified 2 files (+2/-1 lines)\n\n\n## The Solution\n\nML tracking data\n\nThe solution involved changes to 4 files, adding 62 lines and removing 1 lines.\n\n\n## The Lesson\n\n**Bugs often hide in unexpected places.** Thorough investigation and testing are essential for finding root causes.\n\n## Technical Details\n\n**Files Modified:** 7\n\n- `.git-ml/chats/2025-12-16/chat-20251216-004054-78b531.json`\n- `.git-ml/chats/2025-12-16/chat-20251216-004643-110372.json`\n- `.git-ml/chats/2025-12-16/chat-20251216-004643-4911e7.json`\n- `.git-ml/chats/2025-12-16/chat-20251216-004643-99bc2f.json`\n- `.git-ml/current_session.json`\n- `.git-ml/tracked/commits.jsonl`\n- `scripts/ml_data_collector.py`\n\n**Code Changes:** +79/-6 lines\n\n**Commits:** 6\n\n\n## Commits in This Story\n\n- `9abdc28` (2025-12-16): data: ML tracking data\n- `f4c0e9f` (2025-12-16): data: ML tracking data\n- `49801a4` (2025-12-16): data: ML tracking data\n- `66ad656` (2025-12-16): fix: Prevent infinite commit loop in ML data collection hooks\n- `c8fc6b4` (2025-12-16): data: ML tracking data\n- `b66610a` (2025-12-16): data: ML tracking data\n\n---\n\n*This case study was automatically synthesized from git commit history.*\n",
      "keywords": [
        "data",
        "tracking",
        "lines",
        "files",
        "git",
        "modified",
        "commit",
        "json",
        "chats",
        "chat"
      ]
    },
    {
      "id": "05-case-studies/synthesized-2025-12-16-ml-data-sync",
      "title": "synthesized-2025-12-16-ml-data-sync",
      "content": "# Case Study: ML data sync\n\n*Synthesized from commit history: 2025-12-16*\n\n## The Problem\n\nA new feature was required: Add CI workflow and documentation (Wave 4). This would require careful implementation and testing.\n\n## The Journey\n\nThe development progressed through several stages:\n\n1. **Add CI workflow and documentation (Wave 4)** - Modified 5 files (+1724/-1 lines)\n2. **Wave 5 integration testing verification** - Modified 25 files (+227/-88 lines)\n3. **ML data sync** - Modified 43 files (+904/-6 lines)\n4. **ML data sync** - Modified 44 files (+923/-0 lines)\n5. **Add unit tests for Cortical Chronicles generators** - Modified 2 files (+1372/-0 lines)\n6. **ML data sync** - Modified 43 files (+922/-0 lines)\n7. **ML data sync** - Modified 45 files (+941/-0 lines)\n8. **ML data sync** - Modified 46 files (+981/-0 lines)\n9. **Add consolidated markdown book generation** - Modified 3 files (+6392/-2 lines)\n\n\n## The Solution\n\nCapture session data\n\nThe solution involved changes to 2 files, adding 2 lines and removing 0 lines.\n\n\n## The Lesson\n\n**Feature development is iterative.** Breaking work into smaller commits makes it easier to review, test, and debug.\n\n## Technical Details\n\n**Files Modified:** 247\n\n- `.git-ml/actions/2025-12-16/A-20251216-172758-1540-000.json`\n- `.git-ml/actions/2025-12-16/A-20251216-172758-1540-001.json`\n- `.git-ml/actions/2025-12-16/A-20251216-172758-1540-002.json`\n- `.git-ml/actions/2025-12-16/A-20251216-172758-1540-003.json`\n- `.git-ml/actions/2025-12-16/A-20251216-172758-1540-004.json`\n- `.git-ml/actions/2025-12-16/A-20251216-172758-1540-005.json`\n- `.git-ml/actions/2025-12-16/A-20251216-172758-1540-006.json`\n- `.git-ml/actions/2025-12-16/A-20251216-172758-1540-007.json`\n- `.git-ml/actions/2025-12-16/A-20251216-172758-1540-008.json`\n- `.git-ml/actions/2025-12-16/A-20251216-172758-1540-009.json`\n\n*...and 237 more files*\n\n**Code Changes:** +14388/-97 lines\n\n**Commits:** 10\n\n\n## Commits in This Story\n\n- `940fdf2` (2025-12-16): feat: Add CI workflow and documentation (Wave 4)\n- `1504899` (2025-12-16): chore: Wave 5 integration testing verification\n- `590f46f` (2025-12-16): chore: ML data sync\n- `be019d3` (2025-12-16): chore: ML data sync\n- `a09bd89` (2025-12-16): test: Add unit tests for Cortical Chronicles generators\n- `a16f142` (2025-12-16): chore: ML data sync\n- `d8ba759` (2025-12-16): chore: ML data sync\n- `18ffbcd` (2025-12-16): chore: ML data sync\n- `f8a2ad6` (2025-12-16): feat: Add consolidated markdown book generation\n- `0b34b17` (2025-12-16): ml: Capture session data\n\n---\n\n*This case study was automatically synthesized from git commit history.*\n",
      "keywords": [
        "data",
        "files",
        "lines",
        "sync",
        "git",
        "modified",
        "actions",
        "json",
        "add",
        "chore"
      ]
    },
    {
      "id": "05-case-studies/synthesized-2025-12-16-ml-tracking-data",
      "title": "synthesized-2025-12-16-ml-tracking-data",
      "content": "# Case Study: ML tracking data\n\n*Synthesized from commit history: 2025-12-16*\n\n## The Problem\n\nA new feature was required: Add chunked storage for git-friendly ML data. This would require careful implementation and testing.\n\n## The Journey\n\nThe development progressed through several stages:\n\n1. **Add chunked storage for git-friendly ML data** - Modified 4 files (+1068/-0 lines)\n2. **ML tracking data** - Modified 7 files (+95/-8 lines)\n3. **Add ML data collection knowledge transfer document** - Modified 1 files (+340/-0 lines)\n4. **ML tracking data** - Modified 7 files (+98/-2 lines)\n5. **ML tracking data** - Modified 12 files (+216/-3 lines)\n6. **Add missing imports in validate command** - Modified 1 files (+5/-0 lines)\n\n\n## The Solution\n\nAdd ML file prediction model\n\nThe solution involved changes to 2 files, adding 1098 lines and removing 0 lines.\n\n\n## The Lesson\n\n**Bugs often hide in unexpected places.** Thorough investigation and testing are essential for finding root causes.\n\n## Technical Details\n\n**Files Modified:** 30\n\n- `.git-ml/chats/2025-12-16/chat-20251216-005017-2e0bba.json`\n- `.git-ml/chats/2025-12-16/chat-20251216-005017-51f512.json`\n- `.git-ml/chats/2025-12-16/chat-20251216-005017-9b8abf.json`\n- `.git-ml/chats/2025-12-16/chat-20251216-005017-d826a3.json`\n- `.git-ml/chats/2025-12-16/chat-20251216-010357-15ba9d.json`\n- `.git-ml/chats/2025-12-16/chat-20251216-010357-279b0c.json`\n- `.git-ml/chats/2025-12-16/chat-20251216-010357-a70b9d.json`\n- `.git-ml/chats/2025-12-16/chat-20251216-010357-a99a7b.json`\n- `.git-ml/chats/2025-12-16/chat-20251216-010357-f3ffe9.json`\n- `.git-ml/chats/2025-12-16/chat-20251216-012424-3b0705.json`\n\n*...and 20 more files*\n\n**Code Changes:** +2920/-13 lines\n\n**Commits:** 7\n\n\n## Commits in This Story\n\n- `0754540` (2025-12-16): feat: Add chunked storage for git-friendly ML data\n- `cd7c692` (2025-12-16): data: ML tracking data\n- `5d3bf6d` (2025-12-16): docs: Add ML data collection knowledge transfer document\n- `5664b51` (2025-12-16): data: ML tracking data\n- `07ff40c` (2025-12-16): data: ML tracking data\n- `172ad8f` (2025-12-16): fix: Add missing imports in validate command\n- `ac549dd` (2025-12-16): feat: Add ML file prediction model\n\n---\n\n*This case study was automatically synthesized from git commit history.*\n",
      "keywords": [
        "data",
        "git",
        "chats",
        "chat",
        "json",
        "add",
        "files",
        "lines",
        "tracking",
        "modified"
      ]
    },
    {
      "id": "05-case-studies/synthesized-index",
      "title": "synthesized-index",
      "content": "# Synthesized Case Studies\n\n*Case studies automatically synthesized from git commit history*\n\nThese stories are reconstructed from related commit sequences, showing how real development work unfolds over time.\n\n---\n\n### [Bug Fix - Update skipped tests for processor/ package refactor](synthesized-2025-12-14-bug-fix---update-skipped-tests.md)\n\n**2 commits** | **10 files**\n\nSplit processor.py into modular processor/ package (LEGACY-095)\n\n---\n\n### [Bug Fix - Use heredoc for Python in CI to avoid YAML syntax error](synthesized-2025-12-14-bug-fix---use-heredoc-for-pyth.md)\n\n**3 commits** | **1 files**\n\nMake push trigger explicit for all branches\n\n---\n\n### [Update task ID format test to expect microseconds](synthesized-2025-12-14-update-task-id-format-test-to-.md)\n\n**2 commits** | **2 files**\n\nAdd microseconds to task ID to prevent collisions\n\n---\n\n### [Bug Fix - Replace external action with native Python link checker](synthesized-2025-12-14-bug-fix---replace-external-act.md)\n\n**4 commits** | **48 files**\n\nMerge pull request #81 from scrawlsbenches/claude/implement-director-mode-NKbiu\n\n---\n\n### [Bug Fix - Add test file penalty and code stop word filtering to search](synthesized-2025-12-14-bug-fix---add-test-file-penalt.md)\n\n**2 commits** | **33 files**\n\nDirector mode batch execution - 6 tasks completed in parallel\n\n---\n\n### [Bug Fix - Harden ML data collector with critical fixes](synthesized-2025-12-15-bug-fix---harden-ml-data-colle.md)\n\n**2 commits** | **4 files**\n\nAdd ML data collection infrastructure for project-specific micro-model\n\n---\n\n### [Feature Development - Add schema validation for ML data integrity](synthesized-2025-12-15-feature-development---add-sche.md)\n\n**5 commits** | **4 files**\n\nAdd CI status integration for ML outcome tracking\n\n---\n\n### [Bug Fix - Update tests for BM25 default and stop word tokenization](synthesized-2025-12-15-bug-fix---update-tests-for-bm2.md)\n\n**3 commits** | **25 files**\n\nAdd Stop hook config and update task tracking\n\n---\n\n### [Bug Fix - Increase ML data retention to 2 years for training milestones](synthesized-2025-12-15-bug-fix---increase-ml-data-ret.md)\n\n**5 commits** | **478 files**\n\nAdd privacy features to ML data collection\n\n---\n\n### [Bug Fix - Stop tracking ML commit data files (too large for GitHub)](synthesized-2025-12-15-bug-fix---stop-tracking-ml-com.md)\n\n**6 commits** | **949 files**\n\nAdd ML commit data\n\n---\n\n### [Bug Fix - Prevent infinite commit loop in ML data collection hooks](synthesized-2025-12-16-bug-fix---prevent-infinite-com.md)\n\n**6 commits** | **7 files**\n\nML tracking data\n\n---\n\n### [Bug Fix - Address critical ML data collection and prediction issues](synthesized-2025-12-16-bug-fix---address-critical-ml-.md)\n\n**4 commits** | **19 files**\n\nAdd ML chat data from investigation sessions\n\n---\n\n### [Bug Fix - Fix ML data collection milestone counting and add session/action capture](synthesized-2025-12-16-bug-fix---fix-ml-data-collecti.md)\n\n**5 commits** | **46 files**\n\nUpdate ML chat data from investigation session\n\n---\n\n### [Bug Fix - Archive ML session after transcript processing (T-003 16f3)](synthesized-2025-12-16-bug-fix---archive-ml-session-a.md)\n\n**2 commits** | **13 files**\n\nML data from director review session\n\n---\n\n### [Refactoring - Migrate to merge-friendly task system and add security tasks](synthesized-2025-12-14-refactoring---migrate-to-merge.md)\n\n**4 commits** | **10 files**\n\nRename CLAUDE.md.potential to CLAUDE.md\n\n---\n\n### [Refactoring - Split processor.py into modular processor/ package (LEGACY-095)](synthesized-2025-12-14-refactoring---split-processor..md)\n\n**4 commits** | **32 files**\n\nMerge pull request #80 from scrawlsbenches/claude/resume-dog-fooding-9RPIV\n\n---\n\n### [Refactoring - Consolidate ML data to single JSONL files](synthesized-2025-12-15-refactoring---consolidate-ml-d.md)\n\n**4 commits** | **492 files**\n\nAdd ML commit data\n\n---\n\n### [Refactoring - Split processor.py into modular processor/ package (LEGACY-095)](synthesized-2025-12-14-refactoring---split-processor..md)\n\n**4 commits** | **32 files**\n\nAdd HMAC signature verification for pickle files (SEC-003)\n\n---\n\n### [Feature Development - Add director agent orchestration prompt](synthesized-2025-12-14-feature-development---add-dire.md)\n\n**10 commits** | **52 files**\n\nAdd future tasks for text-as-memories integration\n\n---\n\n### [Add session memory and knowledge transfer](synthesized-2025-12-14-add-session-memory-and-knowled.md)\n\n**5 commits** | **49 files**\n\nAdd director agent orchestration prompt\n\n---\n\n### [Feature Development - Add session handoff, auto-memory, CI link checker, and tests](synthesized-2025-12-14-feature-development---add-sess.md)\n\n**4 commits** | **48 files**\n\nAdd memory system CLI and improve documentation\n\n---\n\n### [Clean up directory structure and queue search relevance fixes](synthesized-2025-12-14-clean-up-directory-structure-a.md)\n\n**5 commits** | **39 files**\n\nDirector mode batch execution - 6 tasks completed in parallel\n\n---\n\n### [Feature Development - Add CI status integration for ML outcome tracking](synthesized-2025-12-15-feature-development---add-ci-s.md)\n\n**4 commits** | **4 files**\n\nAdd schema validation for ML data integrity\n\n---\n\n### [Feature Development - Add CI status integration for ML outcome tracking](synthesized-2025-12-15-feature-development---add-ci-s.md)\n\n**3 commits** | **4 files**\n\nAdd commit-chat session linking for ML training\n\n---\n\n### [Feature Development - Add export, feedback, and quality-report commands to ML collector](synthesized-2025-12-15-feature-development---add-expo.md)\n\n**3 commits** | **14 files**\n\nAdd session handoff generator for context preservation\n\n---\n\n### [Feature Development - Add comprehensive delegation command template](synthesized-2025-12-15-feature-development---add-comp.md)\n\n**3 commits** | **28 files**\n\nAdd reusable pre-merge sanity check command\n\n---\n\n### [Add ML commit data for previous commit](synthesized-2025-12-15-add-ml-commit-data-for-previou.md)\n\n**10 commits** | **956 files**\n\nAdd privacy features to ML data collection\n\n---\n\n### [Bug Fix - Stop tracking ML commit data files (too large for GitHub)](synthesized-2025-12-15-bug-fix---stop-tracking-ml-com.md)\n\n**10 commits** | **957 files**\n\nAdd automatic ML data collection on session startup\n\n---\n\n### [Feature Development - Add lightweight commit data for ephemeral environments](synthesized-2025-12-15-feature-development---add-ligh.md)\n\n**10 commits** | **956 files**\n\nShare ML commit data and aggregated patterns in git\n\n---\n\n### [Add ML commit data for previous commit](synthesized-2025-12-15-add-ml-commit-data-for-previou.md)\n\n**10 commits** | **953 files**\n\nAdd CI auto-capture and GitHub PR/Issue data collection\n\n---\n\n### [Add ML commit data](synthesized-2025-12-15-add-ml-commit-data.md)\n\n**10 commits** | **484 files**\n\nAdd lightweight commit data for ephemeral environments\n\n---\n\n### [ML tracking data](synthesized-2025-12-16-ml-tracking-data.md)\n\n**10 commits** | **34 files**\n\nAdd ML stats report to CI pipeline\n\n---\n\n### [ML tracking data](synthesized-2025-12-16-ml-tracking-data.md)\n\n**7 commits** | **30 files**\n\nAdd chunked storage for git-friendly ML data\n\n---\n\n### [ML data sync](synthesized-2025-12-16-ml-data-sync.md)\n\n**10 commits** | **218 files**\n\nAdd Cortical Chronicles book infrastructure (Wave 1)\n\n---\n\n### [ML data sync](synthesized-2025-12-16-ml-data-sync.md)\n\n**10 commits** | **254 files**\n\nAdd content generators for Cortical Chronicles (Wave 2)\n\n---\n\n### [Add unit tests for Cortical Chronicles generators](synthesized-2025-12-16-add-unit-tests-for-cortical-ch.md)\n\n**10 commits** | **254 files**\n\nAdd search integration and web interface (Wave 3)\n\n---\n\n### [ML data sync](synthesized-2025-12-16-ml-data-sync.md)\n\n**10 commits** | **247 files**\n\nAdd CI workflow and documentation (Wave 4)\n\n---\n\n",
      "keywords": [
        "add",
        "data",
        "files",
        "synthesized",
        "commits",
        "fix",
        "bug",
        "commit",
        "development",
        "feature"
      ]
    },
    {
      "id": "05-future/index",
      "title": "Future Chapter",
      "content": "# Future\n\n*This chapter will be auto-generated in a future update.*\n",
      "keywords": [
        "future",
        "chapter",
        "auto",
        "generated",
        "update"
      ]
    },
    {
      "id": "06-lessons/index",
      "title": "Lessons Learned",
      "content": "# Lessons Learned\n\n*What the Cortical Text Processor taught us about building IR systems.*\n\n---\n\n## Overview\n\nThrough **51 lessons** extracted from development history, we've learned how to build better search systems. Each bug fixed, each optimization made, and each refactoring completed taught us something valuable.\n\n## Statistics\n\n- **Total Commits Analyzed**: 300\n- **Lessons Extracted**: 51\n\n### By Category\n\n- **Performance**: 1 lessons\n- **Correctness**: 24 lessons\n- **Architecture**: 10 lessons\n- **Testing**: 16 lessons\n\n## Lesson Categories\n\n### [Performance Lessons](lessons-performance.md)\n\nHow we learned to optimize search and graph algorithms\n\n**1 lessons** from development history.\n\n### [Correctness Lessons](lessons-correctness.md)\n\nBugs we fixed and edge cases we discovered\n\n**24 lessons** from development history.\n\n### [Architecture Lessons](lessons-architecture.md)\n\nHow we evolved the codebase structure\n\n**10 lessons** from development history.\n\n### [Testing Lessons](lessons-testing.md)\n\nWhat we learned about verifying correctness\n\n**16 lessons** from development history.\n\n---\n\n*This chapter is part of [The Cortical Chronicles](../README.md), a self-documenting book generated by the Cortical Text Processor.*\n",
      "keywords": [
        "lessons",
        "development",
        "history",
        "learned",
        "correctness",
        "cortical",
        "performance",
        "architecture",
        "testing",
        "text"
      ]
    },
    {
      "id": "06-lessons/lessons-architecture",
      "title": "Architecture Lessons",
      "content": "# Architecture Lessons\n\n*How we evolved the code structure over time.*\n\n---\n\n## Overview\n\nThis chapter captures **10 lessons** from architecture work. Each entry shows the problem, the solution, and the principle we extracted.\n\n### Complete legacy task system cleanup\n\n**Commit:** `8dedda6`  \n**Date:** 2025-12-16  \n**Files Changed:** 4  \n  - `docs/archive/migrate_legacy_tasks.py`\n  - `scripts/select_task.py`\n  - `scripts/task_graph.py`\n  - *(and 1 more)*\n\n**The Lesson:** Maintain clear structure. The lesson? Complete legacy task system cleanup\n\n### Feat: Add Cortical Chronicles book infrastructure (Wave 1)\n\n**Commit:** `c730057`  \n**Date:** 2025-12-16  \n**Files Changed:** 13  \n  - `.git-ml/current_session.json`\n  - `.git-ml/tracked/commits.jsonl`\n  - `book/00-preface/.gitkeep`\n  - *(and 10 more)*\n**Changes:** +434/-0 lines  \n\n**The Lesson:** Maintain clear structure. The lesson? feat: Add Cortical Chronicles book infrastructure (Wave 1)\n\n### Remove unused protobuf serialization (T-013 f0ff)\n\n**Commit:** `d7a98ae`  \n**Date:** 2025-12-16  \n**Files Changed:** 6  \n  - `cortical/persistence.py`\n  - `cortical/proto/__init__.py`\n  - `cortical/proto/schema.proto`\n  - *(and 3 more)*\n**Changes:** +100/-1460 lines  \n\n**The Lesson:** Maintain clear structure. The lesson? Remove unused protobuf serialization (T-013 f0ff)\n\n### Data: Add orchestration extraction data from quality audit session\n\n**Commit:** `c85c668`  \n**Date:** 2025-12-16  \n**Files Changed:** 1  \n  - `.git-ml/tracked/orchestration.jsonl`\n**Changes:** +1/-0 lines  \n\n**The Lesson:** Keep modules focused. The lesson? data: Add orchestration extraction data from quality audit session\n\n### Data: Add orchestration extraction data for ML training\n\n**Commit:** `bb75148`  \n**Date:** 2025-12-15  \n**Files Changed:** 1  \n  - `.git-ml/orchestration/05c8c5d9-75c6-4267-9fef-1d5573ba201b_orchestration.json`\n**Changes:** +66/-0 lines  \n\n**The Lesson:** Keep modules focused. The lesson? data: Add orchestration extraction data for ML training\n\n### Feat: Add orchestration extraction for director sub-agent tracking\n\n**Commit:** `4eaeb37`  \n**Date:** 2025-12-15  \n**Files Changed:** 4  \n  - `.gitignore`\n  - `scripts/ml_collector/__init__.py`\n  - `scripts/ml_collector/orchestration.py`\n  - *(and 1 more)*\n\n**The Lesson:** Keep modules focused. The lesson? feat: Add orchestration extraction for director sub-agent tracking\n\n### Split large files exceeding 25000 token limit\n\n**Commit:** `21ec5ea`  \n**Date:** 2025-12-15  \n**Files Changed:** 36  \n  - `.refactor-backup/BACKUP_PLAN.md`\n  - `.refactor-backup/analysis.py`\n  - `.refactor-backup/ml_data_collector.py`\n  - *(and 33 more)*\n\n**The Lesson:** Keep modules focused. The lesson? Split large files exceeding 25000 token limit\n\n### Consolidate ML data to single JSONL files\n\n**Commit:** `205fe34`  \n**Date:** 2025-12-15  \n**Files Changed:** 486  \n  - `.git-ml/commits-lite/0039ad5b13fb_2025-12-11.json`\n  - `.git-ml/commits-lite/00f88d48ab42_2025-12-14.json`\n  - `.git-ml/commits-lite/051d20028ddd_2025-12-13.json`\n  - *(and 483 more)*\n**Changes:** +658/-12208 lines  \n\n**The Lesson:** Maintain clear structure. The lesson? Consolidate ML data to single JSONL files\n\n### Feat: Add ML data collection infrastructure for project-specific micro-model\n\n**Commit:** `1568f3c`  \n**Date:** 2025-12-15  \n**Files Changed:** 4  \n  - `.claude/hooks/session_logger.py`\n  - `.claude/skills/ml-logger/SKILL.md`\n  - `.gitignore`\n  - *(and 1 more)*\n**Changes:** +1039/-0 lines  \n\n**The Lesson:** Maintain clear structure. The lesson? feat: Add ML data collection infrastructure for project-specific micro-model\n\n### Clean up directory structure and queue search relevance fixes\n\n**Commit:** `cd8b9f5`  \n**Date:** 2025-12-14  \n**Files Changed:** 5  \n  - `IMPLEMENTATION_SUMMARY.md`\n  - `docs/PATTERN_DETECTION_GUIDE.md`\n  - `examples/demo_pattern_detection.py`\n  - *(and 2 more)*\n**Changes:** +68/-293 lines  \n\n**The Lesson:** Maintain clear structure. The lesson? Clean up directory structure and queue search relevance fixes\n\n---\n\n*This chapter is part of [The Cortical Chronicles](../README.md), a self-documenting book generated by the Cortical Text Processor.*\n",
      "keywords": [
        "lesson",
        "files",
        "data",
        "commit",
        "date",
        "changed",
        "add",
        "structure",
        "orchestration",
        "cortical"
      ]
    },
    {
      "id": "06-lessons/lessons-correctness",
      "title": "Correctness Lessons",
      "content": "# Correctness Lessons\n\n*Bugs we encountered and how we fixed them.*\n\n---\n\n## Overview\n\nThis chapter captures **24 lessons** from correctness work. Each entry shows the problem, the solution, and the principle we extracted.\n\n### Archive ML session after transcript processing (T-003 16f3)\n\n**Commit:** `59072c8`  \n**Date:** 2025-12-16  \n**Files Changed:** 1  \n  - `scripts/ml_data_collector.py`\n**Changes:** +12/-0 lines  \n\n**The Lesson:** Verify assumptions with tests. The wisdom: Archive ML session after transcript processing (T-003 16f3)\n\n### Update CSV truncation test for new defaults (input=500, output=2000)\n\n**Commit:** `ca94a01`  \n**Date:** 2025-12-16  \n**Files Changed:** 4  \n  - `.git-ml/chats/2025-12-16/chat-20251216-125311-0ce6d9.json`\n  - `.git-ml/chats/2025-12-16/chat-20251216-132048-ba08bf.json`\n  - `.git-ml/tracked/commits.jsonl`\n  - *(and 1 more)*\n\n**The Lesson:** Verify assumptions with tests. The wisdom: Update CSV truncation test for new defaults (input=500, output=2000)\n\n### Fix ML data collection milestone counting and add session/action capture\n\n**Commit:** `273baef`  \n**Date:** 2025-12-16  \n**Files Changed:** 11  \n  - `.git-ml/chats/2025-12-15/chat-20251216-121720-30c3c1.json`\n  - `.git-ml/chats/2025-12-16/chat-20251216-121720-01077d.json`\n  - `.git-ml/chats/2025-12-16/chat-20251216-121720-306450.json`\n  - *(and 8 more)*\n**Changes:** +95/-29 lines  \n\n**The Lesson:** Verify assumptions with tests. The wisdom: Fix ML data collection milestone counting and add session/action capture\n\n### Address critical ML data collection and prediction issues\n\n**Commit:** `fead1c1`  \n**Date:** 2025-12-16  \n**Files Changed:** 9  \n  - `.git-ml/chats/2025-12-15/chat-20251216-115057-b5bb48.json`\n  - `.git-ml/chats/2025-12-16/chat-20251216-115057-3617f9.json`\n  - `.git-ml/chats/2025-12-16/chat-20251216-115057-9502fd.json`\n  - *(and 6 more)*\n**Changes:** +148/-17 lines  \n\n**The Lesson:** Verify assumptions with tests. The wisdom: Address critical ML data collection and prediction issues\n\n### Fix(proto): Make protobuf loading lazy to fix CI smoke test failures\n\n**Commit:** `a93518f`  \n**Date:** 2025-12-16  \n**Files Changed:** 2  \n  - `cortical/proto/__init__.py`\n  - `cortical/proto/serialization.py`\n**Changes:** +53/-19 lines  \n\n**The Lesson:** Verify assumptions with tests. The wisdom: fix(proto): Make protobuf loading lazy to fix CI smoke test failures\n\n### Add missing imports in validate command\n\n**Commit:** `172ad8f`  \n**Date:** 2025-12-16  \n**Files Changed:** 1  \n  - `scripts/ml_data_collector.py`\n**Changes:** +5/-0 lines  \n\n**The Lesson:** Verify assumptions with tests. The wisdom: Add missing imports in validate command\n\n### Clean up gitignore pattern for .git-ml/commits/\n\n**Commit:** `a65d54f`  \n**Date:** 2025-12-16  \n**Files Changed:** 1  \n  - `.gitignore`\n**Changes:** +2/-1 lines  \n\n**The Lesson:** Verify assumptions with tests. The wisdom: Clean up gitignore pattern for .git-ml/commits/\n\n### Prevent infinite commit loop in ML data collection hooks\n\n**Commit:** `66ad656`  \n**Date:** 2025-12-16  \n**Files Changed:** 3  \n  - `.git-ml/chats/2025-12-16/chat-20251216-004054-78b531.json`\n  - `.git-ml/tracked/commits.jsonl`\n  - `scripts/ml_data_collector.py`\n**Changes:** +9/-1 lines  \n\n**The Lesson:** Verify assumptions with tests. The wisdom: Prevent infinite commit loop in ML data collection hooks\n\n### Correct hook format in settings.local.json\n\n**Commit:** `19ac02a`  \n**Date:** 2025-12-16  \n**Files Changed:** 1  \n  - `.claude/settings.local.json`\n**Changes:** +14/-4 lines  \n\n**The Lesson:** Verify assumptions with tests. The wisdom: Correct hook format in settings.local.json\n\n### Use filename-based sorting for deterministic session ordering\n\n**Commit:** `61d502d`  \n**Date:** 2025-12-15  \n**Files Changed:** 2  \n  - `scripts/session_context.py`\n  - `tests/unit/test_session_context.py`\n\n**The Lesson:** Verify assumptions with tests. The wisdom: Use filename-based sorting for deterministic session ordering\n\n### Increase ID suffix length to prevent collisions\n\n**Commit:** `8ac4b6b`  \n**Date:** 2025-12-15  \n**Files Changed:** 2  \n  - `scripts/orchestration_utils.py`\n  - `tests/unit/test_orchestration_utils.py`\n\n**The Lesson:** Verify assumptions with tests. The wisdom: Increase ID suffix length to prevent collisions\n\n### Add import guards for optional test dependencies\n\n**Commit:** `91ffb04`  \n**Date:** 2025-12-15  \n**Files Changed:** 2  \n  - `tests/security/test_fuzzing.py`\n  - `tests/test_mcp_server.py`\n\n**The Lesson:** Verify assumptions with tests. The wisdom: Add import guards for optional test dependencies\n\n### Make session file sorting stable for deterministic ordering\n\n**Commit:** `7433b36`  \n**Date:** 2025-12-15  \n**Files Changed:** 1  \n  - `scripts/session_context.py`\n\n**The Lesson:** Verify assumptions with tests. The wisdom: Make session file sorting stable for deterministic ordering\n\n### Feat(LEGACY-130): Expand customer service corpus and fix xfailed tests\n\n**Commit:** `7f9664d`  \n**Date:** 2025-12-15  \n**Files Changed:** 6  \n  - `samples/customer_service/complaint_escalation_procedures.txt`\n  - `samples/customer_service/empathy_and_active_listening.txt`\n  - `samples/customer_service/refund_request_handling.txt`\n  - *(and 3 more)*\n\n**The Lesson:** Verify assumptions with tests. The wisdom: feat(LEGACY-130): Expand customer service corpus and fix xfailed tests\n\n### Cap query expansion weights to prevent term domination\n\n**Commit:** `fecd6dc`  \n**Date:** 2025-12-15  \n**Files Changed:** 3  \n  - `cortical/query/expansion.py`\n  - `tests/behavioral/test_customer_service_quality.py`\n  - `tests/unit/test_query_expansion.py`\n\n**The Lesson:** Verify assumptions with tests. The wisdom: Cap query expansion weights to prevent term domination\n\n### Add YAML frontmatter to slash commands for discovery\n\n**Commit:** `5b52da2`  \n**Date:** 2025-12-15  \n**Files Changed:** 7  \n  - `.claude/commands/delegate.md`\n  - `.claude/commands/director.md`\n  - `.claude/commands/knowledge-transfer.md`\n  - *(and 4 more)*\n\n**The Lesson:** Verify assumptions with tests. The wisdom: Add YAML frontmatter to slash commands for discovery\n\n### Stop tracking ML commit data files (too large for GitHub)\n\n**Commit:** `a6f39e0`  \n**Date:** 2025-12-15  \n**Files Changed:** 472  \n  - `.git-ml/commits/0039ad5b_2025-12-11_24b1b10a.json`\n  - `.git-ml/commits/00f88d48_2025-12-14_8749d448.json`\n  - `.git-ml/commits/051d2002_2025-12-13_7896a312.json`\n  - *(and 469 more)*\n**Changes:** +4/-2263268 lines  \n\n**The Lesson:** Verify assumptions with tests. The wisdom: Stop tracking ML commit data files (too large for GitHub)\n\n### Increase ML data retention to 2 years for training milestones\n\n**Commit:** `95e9f06`  \n**Date:** 2025-12-15  \n**Files Changed:** 2  \n  - `README.md`\n  - `scripts/ml_data_collector.py`\n**Changes:** +7/-5 lines  \n\n**The Lesson:** Verify assumptions with tests. The wisdom: Increase ML data retention to 2 years for training milestones\n\n### Update tests for BM25 default and stop word tokenization\n\n**Commit:** `9dc7268`  \n**Date:** 2025-12-15  \n**Files Changed:** 2  \n  - `tests/unit/test_processor_core.py`\n  - `tests/unit/test_query_search.py`\n**Changes:** +23/-10 lines  \n\n**The Lesson:** Verify assumptions with tests. The wisdom: Update tests for BM25 default and stop word tokenization\n\n### Address audit findings and add documentation\n\n**Commit:** `36be3a1`  \n**Date:** 2025-12-15  \n**Files Changed:** 4  \n  - `.claude/commands/ml-log.md`\n  - `.claude/commands/ml-stats.md`\n  - `CLAUDE.md`\n  - *(and 1 more)*\n**Changes:** +201/-15 lines  \n\n**The Lesson:** Verify assumptions with tests. The wisdom: Address audit findings and add documentation\n\n### Harden ML data collector with critical fixes\n\n**Commit:** `4438d60`  \n**Date:** 2025-12-15  \n**Files Changed:** 1  \n  - `scripts/ml_data_collector.py`\n**Changes:** +151/-54 lines  \n\n**The Lesson:** Verify assumptions with tests. The wisdom: Harden ML data collector with critical fixes\n\n### Correct line number assertions in pattern detection tests\n\n**Commit:** `1b9901d`  \n**Date:** 2025-12-15  \n**Files Changed:** 1  \n  - `tests/unit/test_patterns.py`\n**Changes:** +5/-5 lines  \n\n**The Lesson:** Verify assumptions with tests. The wisdom: Correct line number assertions in pattern detection tests\n\n### Add test file penalty and code stop word filtering to search\n\n**Commit:** `1fafc8b`  \n**Date:** 2025-12-14  \n**Files Changed:** 3  \n  - `cortical/processor/query_api.py`\n  - `cortical/query/passages.py`\n  - `cortical/query/search.py`\n**Changes:** +51/-9 lines  \n\n**The Lesson:** Verify assumptions with tests. The wisdom: Add test file penalty and code stop word filtering to search\n\n### Replace external action with native Python link checker\n\n**Commit:** `901a181`  \n**Date:** 2025-12-14  \n**Files Changed:** 5  \n  - `.github/workflows/ci.yml`\n  - `.markdown-link-check.json`\n  - `scripts/resolve_wiki_links.py`\n  - *(and 2 more)*\n**Changes:** +172/-34 lines  \n\n**The Lesson:** Validate inputs early. The lesson? Replace external action with native Python link checker\n\n---\n\n*This chapter is part of [The Cortical Chronicles](../README.md), a self-documenting book generated by the Cortical Text Processor.*\n",
      "keywords": [
        "tests",
        "commit",
        "files",
        "lesson",
        "date",
        "changed",
        "verify",
        "assumptions",
        "wisdom",
        "changes"
      ]
    },
    {
      "id": "06-lessons/lessons-performance",
      "title": "Performance Lessons",
      "content": "# Performance Lessons\n\n*What we learned about making the system fast and efficient.*\n\n---\n\n## Overview\n\nThis chapter captures **1 lessons** from performance work. Each entry shows the problem, the solution, and the principle we extracted.\n\n### Feat: Optimize compute_all and add Graph-Boosted search (GB-BM25)\n\n**Commit:** `fcce0c2`  \n**Date:** 2025-12-15  \n**Files Changed:** 5  \n  - `cortical/analysis.py`\n  - `cortical/processor/query_api.py`\n  - `cortical/query/__init__.py`\n  - *(and 2 more)*\n**Changes:** +244/-62 lines  \n\n**The Lesson:** Optimize based on evidence. The lesson? feat: Optimize compute_all and add Graph-Boosted search (GB-BM25)\n\n---\n\n*This chapter is part of [The Cortical Chronicles](../README.md), a self-documenting book generated by the Cortical Text Processor.*\n",
      "keywords": [
        "cortical",
        "optimize",
        "performance",
        "lessons",
        "chapter",
        "feat",
        "compute_all",
        "add",
        "graph",
        "boosted"
      ]
    },
    {
      "id": "06-lessons/lessons-testing",
      "title": "Testing Lessons",
      "content": "# Testing Lessons\n\n*Insights from writing and maintaining tests.*\n\n---\n\n## Overview\n\nThis chapter captures **16 lessons** from testing work. Each entry shows the problem, the solution, and the principle we extracted.\n\n### Add unit tests for Cortical Chronicles generators\n\n**Commit:** `a09bd89`  \n**Date:** 2025-12-16  \n**Files Changed:** 2  \n  - `.git-ml/tracked/commits.jsonl`\n  - `tests/unit/test_generate_book.py`\n**Changes:** +1372/-0 lines  \n\n**The Lesson:** Test what you build. The lesson? Add unit tests for Cortical Chronicles generators\n\n### Fix(tests): Mock file existence in ML prediction tests\n\n**Commit:** `ec8db7a`  \n**Date:** 2025-12-16  \n**Files Changed:** 2  \n  - `tests/unit/test_ml_file_prediction.py`\n  - `tests/unit/test_protobuf_serialization.py`\n**Changes:** +21/-8 lines  \n\n**The Lesson:** Mock external dependencies. The wisdom: fix(tests): Mock file existence in ML prediction tests\n\n### Fix(tests): Mock file existence in ML prediction tests\n\n**Commit:** `4f7e195`  \n**Date:** 2025-12-16  \n**Files Changed:** 1  \n  - `tests/unit/test_ml_file_prediction.py`\n**Changes:** +10/-6 lines  \n\n**The Lesson:** Mock external dependencies. The wisdom: fix(tests): Mock file existence in ML prediction tests\n\n### Add comprehensive test suite for orchestration.py (33 tests)\n\n**Commit:** `d999c84`  \n**Date:** 2025-12-15  \n**Files Changed:** 2  \n  - `CLAUDE.md`\n  - `tests/unit/test_ml_orchestration.py`\n**Changes:** +801/-1 lines  \n\n**The Lesson:** Test what you build. The lesson? Add comprehensive test suite for orchestration.py (33 tests)\n\n### Update README test count (3800+) and coverage badge (>90%)\n\n**Commit:** `4ec93d5`  \n**Date:** 2025-12-15  \n**Files Changed:** 1  \n  - `README.md`\n\n**The Lesson:** Measure coverage to find gaps. The lesson? Update README test count (3800+) and coverage badge (>90%)\n\n### Update task status for Wave 4 completed coverage tests (ALL COMPLETE)\n\n**Commit:** `3b9a071`  \n**Date:** 2025-12-15  \n**Files Changed:** 1  \n  - `tasks/2025-12-15_05-23-36_ceac.json`\n\n**The Lesson:** Measure coverage to find gaps. The lesson? Update task status for Wave 4 completed coverage tests (ALL COMPLETE)\n\n### Feat: Add comprehensive test coverage for Wave 4 modules (FINAL)\n\n**Commit:** `73d6da8`  \n**Date:** 2025-12-15  \n**Files Changed:** 4  \n  - `tests/unit/test_code_concepts_coverage.py`\n  - `tests/unit/test_diff_coverage.py`\n  - `tests/unit/test_fluent_coverage.py`\n  - *(and 1 more)*\n\n**The Lesson:** Measure coverage to find gaps. The lesson? feat: Add comprehensive test coverage for Wave 4 modules (FINAL)\n\n### Update task status for Wave 3 completed coverage tests\n\n**Commit:** `0b2eaf2`  \n**Date:** 2025-12-15  \n**Files Changed:** 1  \n  - `tasks/2025-12-15_05-23-36_ceac.json`\n\n**The Lesson:** Measure coverage to find gaps. The lesson? Update task status for Wave 3 completed coverage tests\n\n### Feat: Add comprehensive test coverage for Wave 3 modules\n\n**Commit:** `036f830`  \n**Date:** 2025-12-15  \n**Files Changed:** 4  \n  - `tests/unit/test_config_coverage.py`\n  - `tests/unit/test_fingerprint_coverage.py`\n  - `tests/unit/test_query_chunking.py`\n  - *(and 1 more)*\n\n**The Lesson:** Measure coverage to find gaps. The lesson? feat: Add comprehensive test coverage for Wave 3 modules\n\n### Update task status for Wave 2 completed coverage tests\n\n**Commit:** `66f7df2`  \n**Date:** 2025-12-15  \n**Files Changed:** 1  \n  - `tasks/2025-12-15_05-23-36_ceac.json`\n\n**The Lesson:** Measure coverage to find gaps. The lesson? Update task status for Wave 2 completed coverage tests\n\n### Feat: Add comprehensive test coverage for Wave 2 modules\n\n**Commit:** `5a6bb26`  \n**Date:** 2025-12-15  \n**Files Changed:** 4  \n  - `tests/unit/test_embeddings_coverage.py`\n  - `tests/unit/test_query_definitions.py`\n  - `tests/unit/test_query_passages.py`\n  - *(and 1 more)*\n\n**The Lesson:** Measure coverage to find gaps. The lesson? feat: Add comprehensive test coverage for Wave 2 modules\n\n### Update task status for completed coverage tests\n\n**Commit:** `cc147fd`  \n**Date:** 2025-12-15  \n**Files Changed:** 1  \n  - `tasks/2025-12-15_05-23-36_ceac.json`\n\n**The Lesson:** Measure coverage to find gaps. The lesson? Update task status for completed coverage tests\n\n### Feat: Add comprehensive test coverage for query and analysis modules\n\n**Commit:** `70a4b1b`  \n**Date:** 2025-12-15  \n**Files Changed:** 7  \n  - `requirements.txt`\n  - `tasks/2025-12-14_01-53-45_7b60.json`\n  - `tasks/2025-12-14_11-11-44_legacy-migration.json`\n  - *(and 4 more)*\n\n**The Lesson:** Measure coverage to find gaps. The lesson? feat: Add comprehensive test coverage for query and analysis modules\n\n### Add unit tests for ML collector export, feedback, quality commands\n\n**Commit:** `1899ed8`  \n**Date:** 2025-12-15  \n**Files Changed:** 3  \n  - `tests/unit/test_ml_export.py`\n  - `tests/unit/test_ml_feedback.py`\n  - `tests/unit/test_ml_quality.py`\n**Changes:** +1842/-0 lines  \n\n**The Lesson:** Test what you build. The lesson? Add unit tests for ML collector export, feedback, quality commands\n\n### Add 16 code coverage improvement tasks\n\n**Commit:** `d0732b4`  \n**Date:** 2025-12-15  \n**Files Changed:** 1  \n  - `tasks/2025-12-15_05-23-36_ceac.json`\n**Changes:** +248/-0 lines  \n\n**The Lesson:** Measure coverage to find gaps. The lesson? Add 16 code coverage improvement tasks\n\n### Merge pull request #85 from scrawlsbenches/claude/fix-coverage-module-82miT\n\n**Commit:** `d09bbce`  \n**Date:** 2025-12-15  \n**Changes:** +23/-0 lines  \n\n**The Lesson:** Measure coverage to find gaps. The lesson? Merge pull request #85 from scrawlsbenches/claude/fix-coverage-module-82miT\n\n---\n\n*This chapter is part of [The Cortical Chronicles](../README.md), a self-documenting book generated by the Cortical Text Processor.*\n",
      "keywords": [
        "tests",
        "coverage",
        "lesson",
        "unit",
        "add",
        "commit",
        "date",
        "files",
        "changed",
        "test"
      ]
    },
    {
      "id": "07-concepts/bigram",
      "title": "Concept Evolution: Bigram",
      "content": "# Concept Evolution: Bigram\n\n*Tracking the emergence and growth of 'bigram' through commit history.*\n\n---\n\n## Birth\n\n**First Appearance:** 2025-12-09\n\nThe concept of 'bigram' first emerged in commit `50450d1`:\n\n> Add bigram lateral connections (Task 21) and code review concerns\n\n*By Claude*\n\n## Growth Timeline\n\nThe concept has been mentioned in **7 commits** across **1 months** of development.\n\n### December 2025: Emergence\n\n**7 commits** mentioning this concept.\n\n- `50450d1`: Add bigram lateral connections (Task 21) and code review concerns\n- `18f45ef`: Optimize semantic extraction and bigram connections (2x speedup)\n- `0f578c3`: Add expert code review: identify critical bigram bug and verify fixes\n- *(and 4 more)*\n\n## Peak Activity\n\nThe concept saw its most intensive development during week **2025-W49** with **7 commits**.\n\n## Related Concepts\n\nThe 'bigram' concept frequently appears alongside:\n\n- **Lateral Connections** (1 co-occurrences)\n- **Semantic** (1 co-occurrences)\n\n## The Concept Today\n\nMost recent mention was on 2025-12-11:\n\n> Add tests for bigram connection parameters and improve coverage to 90%\n\nThis concept has evolved from its initial appearance to become an **emerging aspect** of the design.\n\n## Key Commits\n\nNotable commits that shaped this concept:\n\n### First: `50450d1`\n\n**Date:** 2025-12-09\n\n**Message:** Add bigram lateral connections (Task 21) and code review concerns\n\n### Midpoint: `17e8147`\n\n**Date:** 2025-12-10\n\n**Message:** Add code review findings: critical bigram separator bugs\n\n### Latest: `1b9438e`\n\n**Date:** 2025-12-11\n\n**Message:** Add tests for bigram connection parameters and improve coverage to 90%\n\n",
      "keywords": [
        "bigram",
        "concept",
        "add",
        "connections",
        "code",
        "review",
        "commits",
        "lateral",
        "first",
        "task"
      ]
    },
    {
      "id": "07-concepts/bm25",
      "title": "Concept Evolution: Bm25",
      "content": "# Concept Evolution: Bm25\n\n*Tracking the emergence and growth of 'bm25' through commit history.*\n\n---\n\n## Birth\n\n**First Appearance:** 2025-12-15\n\nThe concept of 'bm25' first emerged in commit `0a52858`:\n\n> feat: Implement BM25 scoring algorithm as default\n\n*By Claude*\n\n## Growth Timeline\n\nThe concept has been mentioned in **9 commits** across **1 months** of development.\n\n### December 2025: Emergence\n\n**9 commits** mentioning this concept.\n\n- `0a52858`: feat: Implement BM25 scoring algorithm as default\n- `fcce0c2`: feat: Optimize compute_all and add Graph-Boosted search (GB-BM25)\n- `63064c7`: docs: Add BM25/GB-BM25 documentation and tests\n- *(and 6 more)*\n\n## Peak Activity\n\nThe concept saw its most intensive development during week **2025-W50** with **9 commits**.\n\n## Related Concepts\n\nThe 'bm25' concept frequently appears alongside:\n\n- **Search** (2 co-occurrences)\n- **Graph** (1 co-occurrences)\n- **Tokenization** (1 co-occurrences)\n\n## The Concept Today\n\nMost recent mention was on 2025-12-16:\n\n> feat: Add chunked parallel processing for TF-IDF/BM25 (LEGACY-135)\n\nThis concept has evolved from its initial appearance to become an **emerging aspect** of the design.\n\n## Key Commits\n\nNotable commits that shaped this concept:\n\n### First: `0a52858`\n\n**Date:** 2025-12-15\n\n**Message:** feat: Implement BM25 scoring algorithm as default\n\n### Midpoint: `9dc7268`\n\n**Date:** 2025-12-15\n\n**Message:** fix: Update tests for BM25 default and stop word tokenization\n\n### Latest: `5665839`\n\n**Date:** 2025-12-16\n\n**Message:** feat: Add chunked parallel processing for TF-IDF/BM25 (LEGACY-135)\n\n",
      "keywords": [
        "bm25",
        "concept",
        "feat",
        "commits",
        "default",
        "add",
        "first",
        "implement",
        "scoring",
        "algorithm"
      ]
    },
    {
      "id": "07-concepts/clustering",
      "title": "Concept Evolution: Clustering",
      "content": "# Concept Evolution: Clustering\n\n*Tracking the emergence and growth of 'clustering' through commit history.*\n\n---\n\n## Birth\n\n**First Appearance:** 2025-12-09\n\nThe concept of 'clustering' first emerged in commit `bf75e5d`:\n\n> Activate Layer 2 concept clustering by default (Task 10)\n\n*By Claude*\n\n## Growth Timeline\n\nThe concept has been mentioned in **7 commits** across **1 months** of development.\n\n### December 2025: Emergence\n\n**7 commits** mentioning this concept.\n\n- `bf75e5d`: Activate Layer 2 concept clustering by default (Task 10)\n- `e7933b6`: Implement Task 4: Improve clustering to reduce topic isolation\n- `0d24482`: Fix tests to not skip - provide sufficient data for concept clustering\n- *(and 4 more)*\n\n## Peak Activity\n\nThe concept saw its most intensive development during week **2025-W49** with **7 commits**.\n\n## The Concept Today\n\nMost recent mention was on 2025-12-12:\n\n> Task #143: Investigate negative silhouette score in clustering\n\nThis concept has evolved from its initial appearance to become an **emerging aspect** of the design.\n\n## Key Commits\n\nNotable commits that shaped this concept:\n\n### First: `bf75e5d`\n\n**Date:** 2025-12-09\n\n**Message:** Activate Layer 2 concept clustering by default (Task 10)\n\n### Midpoint: `bda9504`\n\n**Date:** 2025-12-11\n\n**Message:** Add critical clustering tasks #123-125 and regression tests (Task #124)\n\n### Latest: `2753132`\n\n**Date:** 2025-12-12\n\n**Message:** Task #143: Investigate negative silhouette score in clustering\n\n",
      "keywords": [
        "concept",
        "clustering",
        "task",
        "commits",
        "first",
        "bf75e5d",
        "activate",
        "layer",
        "default",
        "date"
      ]
    },
    {
      "id": "07-concepts/context",
      "title": "Concept Evolution: Context",
      "content": "# Concept Evolution: Context\n\n*Tracking the emergence and growth of 'context' through commit history.*\n\n---\n\n## Birth\n\n**First Appearance:** 2025-12-11\n\nThe concept of 'context' first emerged in commit `a530bea`:\n\n> Add CLI wrapper framework for context collection and task triggers\n\n*By Claude*\n\n## Growth Timeline\n\nThe concept has been mentioned in **3 commits** across **1 months** of development.\n\n### December 2025: Emergence\n\n**3 commits** mentioning this concept.\n\n- `a530bea`: Add CLI wrapper framework for context collection and task triggers\n- `4e10104`: Merge pull request #37 from scrawlsbenches/claude/cli-wrapper-context-01JScUxQPSb4rGC2XhtXPSYB\n- `9bd4067`: feat: Add session handoff generator for context preservation\n\n## Peak Activity\n\nThe concept saw its most intensive development during week **2025-W49** with **2 commits**.\n\n## The Concept Today\n\nMost recent mention was on 2025-12-15:\n\n> feat: Add session handoff generator for context preservation\n\nThis concept has evolved from its initial appearance to become an **emerging aspect** of the design.\n\n## Key Commits\n\nNotable commits that shaped this concept:\n\n### First: `a530bea`\n\n**Date:** 2025-12-11\n\n**Message:** Add CLI wrapper framework for context collection and task triggers\n\n### Midpoint: `4e10104`\n\n**Date:** 2025-12-11\n\n**Message:** Merge pull request #37 from scrawlsbenches/claude/cli-wrapper-context-01JScUxQPSb4rGC2XhtXPSYB\n\n### Latest: `9bd4067`\n\n**Date:** 2025-12-15\n\n**Message:** feat: Add session handoff generator for context preservation\n\n",
      "keywords": [
        "context",
        "concept",
        "add",
        "cli",
        "wrapper",
        "commits",
        "first",
        "a530bea",
        "framework",
        "collection"
      ]
    },
    {
      "id": "07-concepts/definition",
      "title": "Concept Evolution: Definition",
      "content": "# Concept Evolution: Definition\n\n*Tracking the emergence and growth of 'definition' through commit history.*\n\n---\n\n## Birth\n\n**First Appearance:** 2025-12-11\n\nThe concept of 'definition' first emerged in commit `60c3483`:\n\n> Add direct definition pattern search for code search (Task #84)\n\n*By Claude*\n\n## Growth Timeline\n\nThe concept has been mentioned in **4 commits** across **1 months** of development.\n\n### December 2025: Emergence\n\n**4 commits** mentioning this concept.\n\n- `60c3483`: Add direct definition pattern search for code search (Task #84)\n- `66a4078`: Merge main, add task #128 for definition boost search quality issue\n- `d85cc90`: Fix definition boost to deprioritize test files over real implementations (Task #128)\n- *(and 1 more)*\n\n## Peak Activity\n\nThe concept saw its most intensive development during week **2025-W49** with **4 commits**.\n\n## Related Concepts\n\nThe 'definition' concept frequently appears alongside:\n\n- **Search** (2 co-occurrences)\n\n## The Concept Today\n\nMost recent mention was on 2025-12-11:\n\n> Mark Tasks #128, #132, #136 complete - definition boost and O(n\u00b2) fixes\n\nThis concept has evolved from its initial appearance to become an **emerging aspect** of the design.\n\n## Key Commits\n\nNotable commits that shaped this concept:\n\n### First: `60c3483`\n\n**Date:** 2025-12-11\n\n**Message:** Add direct definition pattern search for code search (Task #84)\n\n### Midpoint: `d85cc90`\n\n**Date:** 2025-12-11\n\n**Message:** Fix definition boost to deprioritize test files over real implementations (Task #128)\n\n### Latest: `0689785`\n\n**Date:** 2025-12-11\n\n**Message:** Mark Tasks #128, #132, #136 complete - definition boost and O(n\u00b2) fixes\n\n",
      "keywords": [
        "definition",
        "concept",
        "search",
        "task",
        "commits",
        "boost",
        "add",
        "first",
        "direct",
        "pattern"
      ]
    },
    {
      "id": "07-concepts/embeddings",
      "title": "Concept Evolution: Embeddings",
      "content": "# Concept Evolution: Embeddings\n\n*Tracking the emergence and growth of 'embeddings' through commit history.*\n\n---\n\n## Birth\n\n**First Appearance:** 2025-12-09\n\nThe concept of 'embeddings' first emerged in commit `8f862b0`:\n\n> Persist full computed state including embeddings (Task 12)\n\n*By Claude*\n\n## Growth Timeline\n\nThe concept has been mentioned in **5 commits** across **1 months** of development.\n\n### December 2025: Emergence\n\n**5 commits** mentioning this concept.\n\n- `8f862b0`: Persist full computed state including embeddings (Task 12)\n- `6cb35d6`: Add critical task #122: Investigate Concept Layer & Embeddings regressions\n- `919e8a7`: Fix cluster_strictness inversion and improve embeddings (Task #122)\n- *(and 2 more)*\n\n## Peak Activity\n\nThe concept saw its most intensive development during week **2025-W49** with **5 commits**.\n\n## The Concept Today\n\nMost recent mention was on 2025-12-13:\n\n> Fix test_retrofit_embeddings_invalid_alpha_zero to match new validation\n\nThis concept has evolved from its initial appearance to become an **emerging aspect** of the design.\n\n## Key Commits\n\nNotable commits that shaped this concept:\n\n### First: `8f862b0`\n\n**Date:** 2025-12-09\n\n**Message:** Persist full computed state including embeddings (Task 12)\n\n### Midpoint: `919e8a7`\n\n**Date:** 2025-12-11\n\n**Message:** Fix cluster_strictness inversion and improve embeddings (Task #122)\n\n### Latest: `69c206b`\n\n**Date:** 2025-12-13\n\n**Message:** Fix test_retrofit_embeddings_invalid_alpha_zero to match new validation\n\n",
      "keywords": [
        "concept",
        "embeddings",
        "task",
        "commits",
        "fix",
        "first",
        "persist",
        "full",
        "computed",
        "state"
      ]
    },
    {
      "id": "07-concepts/graph",
      "title": "Concept Evolution: Graph",
      "content": "# Concept Evolution: Graph\n\n*Tracking the emergence and growth of 'graph' through commit history.*\n\n---\n\n## Birth\n\n**First Appearance:** 2025-12-10\n\nThe concept of 'graph' first emerged in commit `e40a80c`:\n\n> Add ConceptNet-style graph visualization export (Task 29)\n\n*By Claude*\n\n## Growth Timeline\n\nThe concept has been mentioned in **3 commits** across **1 months** of development.\n\n### December 2025: Emergence\n\n**3 commits** mentioning this concept.\n\n- `e40a80c`: Add ConceptNet-style graph visualization export (Task 29)\n- `e5dd3d5`: Task #145: Improve graph embedding quality for common terms\n- `fcce0c2`: feat: Optimize compute_all and add Graph-Boosted search (GB-BM25)\n\n## Peak Activity\n\nThe concept saw its most intensive development during week **2025-W49** with **2 commits**.\n\n## Related Concepts\n\nThe 'graph' concept frequently appears alongside:\n\n- **Bm25** (1 co-occurrences)\n- **Search** (1 co-occurrences)\n\n## The Concept Today\n\nMost recent mention was on 2025-12-15:\n\n> feat: Optimize compute_all and add Graph-Boosted search (GB-BM25)\n\nThis concept has evolved from its initial appearance to become an **emerging aspect** of the design.\n\n## Key Commits\n\nNotable commits that shaped this concept:\n\n### First: `e40a80c`\n\n**Date:** 2025-12-10\n\n**Message:** Add ConceptNet-style graph visualization export (Task 29)\n\n### Midpoint: `e5dd3d5`\n\n**Date:** 2025-12-12\n\n**Message:** Task #145: Improve graph embedding quality for common terms\n\n### Latest: `fcce0c2`\n\n**Date:** 2025-12-15\n\n**Message:** feat: Optimize compute_all and add Graph-Boosted search (GB-BM25)\n\n",
      "keywords": [
        "graph",
        "concept",
        "add",
        "task",
        "commits",
        "search",
        "bm25",
        "first",
        "e40a80c",
        "conceptnet"
      ]
    },
    {
      "id": "07-concepts/incremental",
      "title": "Concept Evolution: Incremental",
      "content": "# Concept Evolution: Incremental\n\n*Tracking the emergence and growth of 'incremental' through commit history.*\n\n---\n\n## Birth\n\n**First Appearance:** 2025-12-09\n\nThe concept of 'incremental' first emerged in commit `38fb4f7`:\n\n> Add incremental document indexing (Task 15)\n\n*By Claude*\n\n## Growth Timeline\n\nThe concept has been mentioned in **5 commits** across **1 months** of development.\n\n### December 2025: Emergence\n\n**5 commits** mentioning this concept.\n\n- `38fb4f7`: Add incremental document indexing (Task 15)\n- `3682739`: Add incremental codebase indexing with progress tracking\n- `b360793`: Update documentation for incremental indexing features\n- *(and 2 more)*\n\n## Peak Activity\n\nThe concept saw its most intensive development during week **2025-W49** with **5 commits**.\n\n## The Concept Today\n\nMost recent mention was on 2025-12-11:\n\n> Add incremental batch mode for full analysis\n\nThis concept has evolved from its initial appearance to become an **emerging aspect** of the design.\n\n## Key Commits\n\nNotable commits that shaped this concept:\n\n### First: `38fb4f7`\n\n**Date:** 2025-12-09\n\n**Message:** Add incremental document indexing (Task 15)\n\n### Midpoint: `b360793`\n\n**Date:** 2025-12-10\n\n**Message:** Update documentation for incremental indexing features\n\n### Latest: `256e842`\n\n**Date:** 2025-12-11\n\n**Message:** Add incremental batch mode for full analysis\n\n",
      "keywords": [
        "incremental",
        "concept",
        "add",
        "indexing",
        "commits",
        "first",
        "document",
        "task",
        "date",
        "message"
      ]
    },
    {
      "id": "07-concepts/index",
      "title": "Concept Evolution Index",
      "content": "# Concept Evolution Index\n\n*A guide to how key concepts emerged and grew in the Cortical Text Processor.*\n\n---\n\n## Overview\n\nThis section tracks the evolution of **14 core concepts** through the project's commit history. Each concept chapter shows:\n\n- When the concept first appeared\n- How it grew over time\n- Related concepts and connections\n- Current state and importance\n\n## Concepts by Importance\n\n### [Search](search.md)\n\n**Mentions:** 26 commits\n\n**First seen:** 2025-12-10\n\n**Related to:** Passage, Definition, Louvain\n\n*A core component of the system architecture.*\n\n### [Semantic](semantic.md)\n\n**Mentions:** 9 commits\n\n**First seen:** 2025-12-09\n\n**Related to:** Retrieval, Bigram, Fingerprint\n\n*An emerging concept in recent development.*\n\n### [Bm25](bm25.md)\n\n**Mentions:** 9 commits\n\n**First seen:** 2025-12-15\n\n**Related to:** Search, Graph, Tokenization\n\n*An emerging concept in recent development.*\n\n### [Clustering](clustering.md)\n\n**Mentions:** 7 commits\n\n**First seen:** 2025-12-09\n\n*An emerging concept in recent development.*\n\n### [Bigram](bigram.md)\n\n**Mentions:** 7 commits\n\n**First seen:** 2025-12-09\n\n**Related to:** Lateral Connections, Semantic\n\n*An emerging concept in recent development.*\n\n### [Louvain](louvain.md)\n\n**Mentions:** 7 commits\n\n**First seen:** 2025-12-11\n\n**Related to:** Search\n\n*An emerging concept in recent development.*\n\n### [Embeddings](embeddings.md)\n\n**Mentions:** 5 commits\n\n**First seen:** 2025-12-09\n\n*An emerging concept in recent development.*\n\n### [Incremental](incremental.md)\n\n**Mentions:** 5 commits\n\n**First seen:** 2025-12-09\n\n*An emerging concept in recent development.*\n\n### [Pagerank](pagerank.md)\n\n**Mentions:** 4 commits\n\n**First seen:** 2025-12-09\n\n*An emerging concept in recent development.*\n\n### [Query Expansion](query-expansion.md)\n\n**Mentions:** 4 commits\n\n**First seen:** 2025-12-10\n\n*An emerging concept in recent development.*\n\n### [Definition](definition.md)\n\n**Mentions:** 4 commits\n\n**First seen:** 2025-12-11\n\n**Related to:** Search\n\n*An emerging concept in recent development.*\n\n### [Graph](graph.md)\n\n**Mentions:** 3 commits\n\n**First seen:** 2025-12-10\n\n**Related to:** Bm25, Search\n\n*An emerging concept in recent development.*\n\n### [Tokenization](tokenization.md)\n\n**Mentions:** 3 commits\n\n**First seen:** 2025-12-10\n\n**Related to:** Bm25\n\n*An emerging concept in recent development.*\n\n### [Context](context.md)\n\n**Mentions:** 3 commits\n\n**First seen:** 2025-12-11\n\n*An emerging concept in recent development.*\n\n---\n\n*Each concept chapter provides detailed evolution timeline and key commits.*\n",
      "keywords": [
        "concept",
        "first",
        "commits",
        "mentions",
        "seen",
        "emerging",
        "recent",
        "development",
        "related",
        "search"
      ]
    },
    {
      "id": "07-concepts/louvain",
      "title": "Concept Evolution: Louvain",
      "content": "# Concept Evolution: Louvain\n\n*Tracking the emergence and growth of 'louvain' through commit history.*\n\n---\n\n## Birth\n\n**First Appearance:** 2025-12-11\n\nThe concept of 'louvain' first emerged in commit `62c7fdf`:\n\n> Implement Louvain community detection (Task #123)\n\n*By Claude*\n\n## Growth Timeline\n\nThe concept has been mentioned in **7 commits** across **1 months** of development.\n\n### December 2025: Emergence\n\n**7 commits** mentioning this concept.\n\n- `62c7fdf`: Implement Louvain community detection (Task #123)\n- `b2b7f92`: Add Task #126: Investigate optimal Louvain resolution\n- `e85c299`: Merge pull request #34 from scrawlsbenches/claude/louvain-community-detection-01FsvWk3GKjFLpEiPwQT4sBc\n- *(and 4 more)*\n\n## Peak Activity\n\nThe concept saw its most intensive development during week **2025-W49** with **7 commits**.\n\n## Related Concepts\n\nThe 'louvain' concept frequently appears alongside:\n\n- **Search** (2 co-occurrences)\n\n## The Concept Today\n\nMost recent mention was on 2025-12-13:\n\n> Add louvain_resolution parameter to CorticalConfig\n\nThis concept has evolved from its initial appearance to become an **emerging aspect** of the design.\n\n## Key Commits\n\nNotable commits that shaped this concept:\n\n### First: `62c7fdf`\n\n**Date:** 2025-12-11\n\n**Message:** Implement Louvain community detection (Task #123)\n\n### Midpoint: `dda7d0c`\n\n**Date:** 2025-12-11\n\n**Message:** Complete Task #126: Louvain resolution parameter research\n\n### Latest: `a47bb61`\n\n**Date:** 2025-12-13\n\n**Message:** Add louvain_resolution parameter to CorticalConfig\n\n",
      "keywords": [
        "louvain",
        "concept",
        "task",
        "commits",
        "community",
        "detection",
        "first",
        "implement",
        "add",
        "parameter"
      ]
    },
    {
      "id": "07-concepts/pagerank",
      "title": "Concept Evolution: Pagerank",
      "content": "# Concept Evolution: Pagerank\n\n*Tracking the emergence and growth of 'pagerank' through commit history.*\n\n---\n\n## Birth\n\n**First Appearance:** 2025-12-09\n\nThe concept of 'pagerank' first emerged in commit `c6eefdc`:\n\n> Add ConceptNet-enhanced PageRank task list (Tasks 19-30)\n\n*By Claude*\n\n## Growth Timeline\n\nThe concept has been mentioned in **4 commits** across **1 months** of development.\n\n### December 2025: Emergence\n\n**4 commits** mentioning this concept.\n\n- `c6eefdc`: Add ConceptNet-enhanced PageRank task list (Tasks 19-30)\n- `f6b8389`: Add relation-weighted PageRank (Task 22)\n- `cc57677`: Add cross-layer PageRank propagation (Task 23)\n- *(and 1 more)*\n\n## Peak Activity\n\nThe concept saw its most intensive development during week **2025-W49** with **4 commits**.\n\n## The Concept Today\n\nMost recent mention was on 2025-12-10:\n\n> Add overlapping PageRank, ConceptNet, and Neocortex samples\n\nThis concept has evolved from its initial appearance to become an **emerging aspect** of the design.\n\n## Key Commits\n\nNotable commits that shaped this concept:\n\n### First: `c6eefdc`\n\n**Date:** 2025-12-09\n\n**Message:** Add ConceptNet-enhanced PageRank task list (Tasks 19-30)\n\n### Midpoint: `cc57677`\n\n**Date:** 2025-12-10\n\n**Message:** Add cross-layer PageRank propagation (Task 23)\n\n### Latest: `1e4d35d`\n\n**Date:** 2025-12-10\n\n**Message:** Add overlapping PageRank, ConceptNet, and Neocortex samples\n\n",
      "keywords": [
        "pagerank",
        "concept",
        "add",
        "task",
        "conceptnet",
        "commits",
        "first",
        "c6eefdc",
        "enhanced",
        "list"
      ]
    },
    {
      "id": "07-concepts/query-expansion",
      "title": "Concept Evolution: Query Expansion",
      "content": "# Concept Evolution: Query Expansion\n\n*Tracking the emergence and growth of 'query expansion' through commit history.*\n\n---\n\n## Birth\n\n**First Appearance:** 2025-12-10\n\nThe concept of 'query expansion' first emerged in commit `16c13a0`:\n\n> Document magic numbers and extract query expansion helper\n\n*By Claude*\n\n## Growth Timeline\n\nThe concept has been mentioned in **4 commits** across **1 months** of development.\n\n### December 2025: Emergence\n\n**4 commits** mentioning this concept.\n\n- `16c13a0`: Document magic numbers and extract query expansion helper\n- `a819131`: Add LRU cache for query expansion results (Task #45)\n- `af3a7e0`: feat: Add security concept group and TF-IDF weighted query expansion\n- *(and 1 more)*\n\n## Peak Activity\n\nThe concept saw its most intensive development during week **2025-W49** with **2 commits**.\n\n## The Concept Today\n\nMost recent mention was on 2025-12-15:\n\n> fix: Cap query expansion weights to prevent term domination\n\nThis concept has evolved from its initial appearance to become an **emerging aspect** of the design.\n\n## Key Commits\n\nNotable commits that shaped this concept:\n\n### First: `16c13a0`\n\n**Date:** 2025-12-10\n\n**Message:** Document magic numbers and extract query expansion helper\n\n### Midpoint: `af3a7e0`\n\n**Date:** 2025-12-15\n\n**Message:** feat: Add security concept group and TF-IDF weighted query expansion\n\n### Latest: `fecd6dc`\n\n**Date:** 2025-12-15\n\n**Message:** fix: Cap query expansion weights to prevent term domination\n\n",
      "keywords": [
        "query",
        "expansion",
        "concept",
        "commits",
        "first",
        "document",
        "magic",
        "numbers",
        "extract",
        "helper"
      ]
    },
    {
      "id": "07-concepts/search",
      "title": "Concept Evolution: Search",
      "content": "# Concept Evolution: Search\n\n*Tracking the emergence and growth of 'search' through commit history.*\n\n---\n\n## Birth\n\n**First Appearance:** 2025-12-10\n\nThe concept of 'search' first emerged in commit `dc6db89`:\n\n> Implement dog-fooding: search codebase with its own IR system (Task #47)\n\n*By Claude*\n\n## Growth Timeline\n\nThe concept has been mentioned in **26 commits** across **1 months** of development.\n\n### December 2025: Emergence\n\n**26 commits** mentioning this concept.\n\n- `dc6db89`: Implement dog-fooding: search codebase with its own IR system (Task #47)\n- `975fc91`: Add intent-based code search enhancement tasks (#48-52)\n- `2ad03ed`: Add query optimization for faster code search (Task #52)\n- *(and 23 more)*\n\n## Peak Activity\n\nThe concept saw its most intensive development during week **2025-W49** with **20 commits**.\n\n## Related Concepts\n\nThe 'search' concept frequently appears alongside:\n\n- **Passage** (2 co-occurrences)\n- **Definition** (2 co-occurrences)\n- **Louvain** (2 co-occurrences)\n- **Bm25** (2 co-occurrences)\n- **Intent** (1 co-occurrences)\n\n## The Concept Today\n\nMost recent mention was on 2025-12-16:\n\n> feat: Add search integration and web interface (Wave 3)\n\nThis concept has evolved from its initial appearance to become a **core component** of the system.\n\n## Key Commits\n\nNotable commits that shaped this concept:\n\n### First: `dc6db89`\n\n**Date:** 2025-12-10\n\n**Message:** Implement dog-fooding: search codebase with its own IR system (Task #47)\n\n### Midpoint: `0f75675`\n\n**Date:** 2025-12-11\n\n**Message:** Add Python code samples and update showcase for code search features\n\n### Latest: `0022466`\n\n**Date:** 2025-12-16\n\n**Message:** feat: Add search integration and web interface (Wave 3)\n\n",
      "keywords": [
        "search",
        "concept",
        "commits",
        "add",
        "occurrences",
        "system",
        "task",
        "code",
        "first",
        "dc6db89"
      ]
    },
    {
      "id": "07-concepts/semantic",
      "title": "Concept Evolution: Semantic",
      "content": "# Concept Evolution: Semantic\n\n*Tracking the emergence and growth of 'semantic' through commit history.*\n\n---\n\n## Birth\n\n**First Appearance:** 2025-12-09\n\nThe concept of 'semantic' first emerged in commit `f27d18e`:\n\n> Integrate semantic relations into retrieval (Task 11)\n\n*By Claude*\n\n## Growth Timeline\n\nThe concept has been mentioned in **9 commits** across **1 months** of development.\n\n### December 2025: Emergence\n\n**9 commits** mentioning this concept.\n\n- `f27d18e`: Integrate semantic relations into retrieval (Task 11)\n- `4e113e7`: Add multi-hop semantic inference (Tasks 25-26)\n- `18f45ef`: Optimize semantic extraction and bigram connections (2x speedup)\n- *(and 6 more)*\n\n## Peak Activity\n\nThe concept saw its most intensive development during week **2025-W49** with **7 commits**.\n\n## Related Concepts\n\nThe 'semantic' concept frequently appears alongside:\n\n- **Retrieval** (1 co-occurrences)\n- **Bigram** (1 co-occurrences)\n- **Fingerprint** (1 co-occurrences)\n- **Similarity** (1 co-occurrences)\n\n## The Concept Today\n\nMost recent mention was on 2025-12-14:\n\n> feat: Add \"What Changed?\" semantic diff (LEGACY-075)\n\nThis concept has evolved from its initial appearance to become an **emerging aspect** of the design.\n\n## Key Commits\n\nNotable commits that shaped this concept:\n\n### First: `f27d18e`\n\n**Date:** 2025-12-09\n\n**Message:** Integrate semantic relations into retrieval (Task 11)\n\n### Midpoint: `626c008`\n\n**Date:** 2025-12-11\n\n**Message:** Add semantic chunk boundaries for code (Task #86)\n\n### Latest: `a31d1c7`\n\n**Date:** 2025-12-14\n\n**Message:** feat: Add \"What Changed?\" semantic diff (LEGACY-075)\n\n",
      "keywords": [
        "semantic",
        "concept",
        "commits",
        "retrieval",
        "task",
        "add",
        "occurrences",
        "first",
        "f27d18e",
        "integrate"
      ]
    },
    {
      "id": "07-concepts/tokenization",
      "title": "Concept Evolution: Tokenization",
      "content": "# Concept Evolution: Tokenization\n\n*Tracking the emergence and growth of 'tokenization' through commit history.*\n\n---\n\n## Birth\n\n**First Appearance:** 2025-12-10\n\nThe concept of 'tokenization' first emerged in commit `d2b42d9`:\n\n> Fix polysemy section: clarify tokenization vs actual polysemy\n\n*By Claude*\n\n## Growth Timeline\n\nThe concept has been mentioned in **3 commits** across **1 months** of development.\n\n### December 2025: Emergence\n\n**3 commits** mentioning this concept.\n\n- `d2b42d9`: Fix polysemy section: clarify tokenization vs actual polysemy\n- `2571bb8`: Add code-aware tokenization with identifier splitting (Task #48)\n- `9dc7268`: fix: Update tests for BM25 default and stop word tokenization\n\n## Peak Activity\n\nThe concept saw its most intensive development during week **2025-W49** with **2 commits**.\n\n## Related Concepts\n\nThe 'tokenization' concept frequently appears alongside:\n\n- **Bm25** (1 co-occurrences)\n\n## The Concept Today\n\nMost recent mention was on 2025-12-15:\n\n> fix: Update tests for BM25 default and stop word tokenization\n\nThis concept has evolved from its initial appearance to become an **emerging aspect** of the design.\n\n## Key Commits\n\nNotable commits that shaped this concept:\n\n### First: `d2b42d9`\n\n**Date:** 2025-12-10\n\n**Message:** Fix polysemy section: clarify tokenization vs actual polysemy\n\n### Midpoint: `2571bb8`\n\n**Date:** 2025-12-10\n\n**Message:** Add code-aware tokenization with identifier splitting (Task #48)\n\n### Latest: `9dc7268`\n\n**Date:** 2025-12-15\n\n**Message:** fix: Update tests for BM25 default and stop word tokenization\n\n",
      "keywords": [
        "tokenization",
        "concept",
        "fix",
        "polysemy",
        "commits",
        "bm25",
        "first",
        "d2b42d9",
        "section",
        "clarify"
      ]
    },
    {
      "id": "08-exercises/ex-advanced",
      "title": "Exercises: Advanced",
      "content": "# Advanced Exercises\n\n*Hands-on coding exercises to master advanced concepts.*\n\n**Difficulty Level:** Advanced\n\n---\n\n## Introduction\n\nChallenge yourself with advanced features:\n\n- Semantic relation extraction\n- Fingerprint-based similarity\n- Graph embeddings\n- Knowledge gap detection\n\n## Exercise: Empty Documents\n\n**Concept:** Empty documents return no relations\n\n**Difficulty:** Advanced\n\n**Time:** ~20 minutes\n\n**Source:** `test_semantics.py`\n\n### Setup\n\n```python\nfrom cortical import CorticalTextProcessor\n```\n\n### Your Task\n\nEmpty documents return no relations.\n\n### Hints\n\n<details>\n<summary>Hint 1</summary>\n\nStart by considering the edge case of empty input.\n\n</details>\n\n<details>\n<summary>Hint 2</summary>\n\nCheck the expected value and comparison in the assertion\n\n</details>\n\n### Solution\n\n<details>\n<summary>Click to reveal</summary>\n\n```python\ndef test_empty_documents(self):\n        \"\"\"Empty documents return no relations.\"\"\"\n        result = extract_pattern_relations({}, {\"term1\", \"term2\"})\n        assert result == []\n```\n\n</details>\n\n---\n\n## Exercise: Empty Valid Terms\n\n**Concept:** No valid terms means no relations extracted\n\n**Difficulty:** Advanced\n\n**Time:** ~20 minutes\n\n**Source:** `test_semantics.py`\n\n### Setup\n\n```python\nfrom cortical import CorticalTextProcessor\n```\n\n### Your Task\n\nNo valid terms means no relations extracted.\n\n### Hints\n\n<details>\n<summary>Hint 1</summary>\n\nStart by considering the edge case of empty input.\n\n</details>\n\n<details>\n<summary>Hint 2</summary>\n\nCheck the expected value and comparison in the assertion\n\n</details>\n\n### Solution\n\n<details>\n<summary>Click to reveal</summary>\n\n```python\ndef test_empty_valid_terms(self):\n        \"\"\"No valid terms means no relations extracted.\"\"\"\n        docs = {\"doc1\": \"A dog is an animal.\"}\n        result = extract_pattern_relations(docs, set())\n        assert result == []\n```\n\n</details>\n\n---\n\n## Exercise: Empty Relations\n\n**Concept:** Empty relations list\n\n**Difficulty:** Advanced\n\n**Time:** ~20 minutes\n\n**Source:** `test_semantics.py`\n\n### Setup\n\n```python\nfrom cortical import CorticalTextProcessor\n```\n\n### Your Task\n\nEmpty relations list.\n\n### Hints\n\n<details>\n<summary>Hint 1</summary>\n\nStart by considering the edge case of empty input.\n\n</details>\n\n<details>\n<summary>Hint 2</summary>\n\nCheck the expected value and comparison in the assertion\n\n</details>\n\n### Solution\n\n<details>\n<summary>Click to reveal</summary>\n\n```python\ndef test_empty_relations(self):\n        \"\"\"Empty relations list.\"\"\"\n        result = get_pattern_statistics([])\n        assert result[\"total_relations\"] == 0\n        assert result[\"relation_type_counts\"] == {}\n```\n\n</details>\n\n---\n\n## Exercise: Single Relation\n\n**Concept:** Single relation statistics\n\n**Difficulty:** Advanced\n\n**Time:** ~20 minutes\n\n**Source:** `test_semantics.py`\n\n### Setup\n\n```python\nfrom cortical import CorticalTextProcessor\n```\n\n### Your Task\n\nSingle relation statistics.\n\n### Hints\n\n<details>\n<summary>Hint 1</summary>\n\nFocus on the simplest case with just one element.\n\n</details>\n\n<details>\n<summary>Hint 2</summary>\n\nCheck the expected value and comparison in the assertion\n\n</details>\n\n### Solution\n\n<details>\n<summary>Click to reveal</summary>\n\n```python\ndef test_single_relation(self):\n        \"\"\"Single relation statistics.\"\"\"\n        relations = [(\"dog\", \"IsA\", \"animal\", 0.9)]\n        result = get_pattern_statistics(relations)\n        assert result[\"total_relations\"] == 1\n        assert result[\"relation_type_counts\"][\"IsA\"] == 1\n```\n\n</details>\n\n---\n\n## Exercise: Empty Relations\n\n**Concept:** Empty relations produce empty hierarchy\n\n**Difficulty:** Advanced\n\n**Time:** ~20 minutes\n\n**Source:** `test_semantics.py`\n\n### Setup\n\n```python\nfrom cortical import CorticalTextProcessor\n```\n\n### Your Task\n\nEmpty relations produce empty hierarchy.\n\n### Hints\n\n<details>\n<summary>Hint 1</summary>\n\nStart by considering the edge case of empty input.\n\n</details>\n\n<details>\n<summary>Hint 2</summary>\n\nCheck the expected value and comparison in the assertion\n\n</details>\n\n### Solution\n\n<details>\n<summary>Click to reveal</summary>\n\n```python\ndef test_empty_relations(self):\n        \"\"\"Empty relations produce empty hierarchy.\"\"\"\n        parents, children = build_isa_hierarchy([])\n        assert parents == {}\n        assert children == {}\n```\n\n</details>\n\n---\n\n## Exercise: Single Isa\n\n**Concept:** Single IsA relation creates parent-child\n\n**Difficulty:** Advanced\n\n**Time:** ~20 minutes\n\n**Source:** `test_semantics.py`\n\n### Setup\n\n```python\nfrom cortical import CorticalTextProcessor\n```\n\n### Your Task\n\nSingle IsA relation creates parent-child.\n\n### Hints\n\n<details>\n<summary>Hint 1</summary>\n\nFocus on the simplest case with just one element.\n\n</details>\n\n### Solution\n\n<details>\n<summary>Click to reveal</summary>\n\n```python\ndef test_single_isa(self):\n        \"\"\"Single IsA relation creates parent-child.\"\"\"\n        relations = [(\"dog\", \"IsA\", \"animal\", 0.9)]\n        parents, children = build_isa_hierarchy(relations)\n        assert \"dog\" in parents\n        assert \"animal\" in parents[\"dog\"]\n        assert \"animal\" in children\n        assert \"dog\" in children[\"animal\"]\n```\n\n</details>\n\n---\n\n## Exercise: Hierarchy Chain\n\n**Concept:** Chain: poodle IsA dog IsA animal\n\n**Difficulty:** Advanced\n\n**Time:** ~20 minutes\n\n**Source:** `test_semantics.py`\n\n### Setup\n\n```python\nfrom cortical import CorticalTextProcessor\n```\n\n### Your Task\n\nChain: poodle IsA dog IsA animal.\n\n### Hints\n\n<details>\n<summary>Hint 1</summary>\n\nThink about how elements connect in sequence.\n\n</details>\n\n### Solution\n\n<details>\n<summary>Click to reveal</summary>\n\n```python\ndef test_hierarchy_chain(self):\n        \"\"\"Chain: poodle IsA dog IsA animal.\"\"\"\n        relations = [\n            (\"poodle\", \"IsA\", \"dog\", 0.9),\n            (\"dog\", \"IsA\", \"animal\", 0.9)\n        ]\n        parents, children = build_isa_hierarchy(relations)\n        assert \"poodle\" in parents\n        assert \"dog\" in parents[\"poodle\"]\n        assert \"dog\" in parents\n        assert \"animal\" in parents[\"dog\"]\n```\n\n</details>\n\n---\n\n## Exercise: Empty Hierarchy\n\n**Concept:** Empty hierarchy returns empty ancestors\n\n**Difficulty:** Advanced\n\n**Time:** ~20 minutes\n\n**Source:** `test_semantics.py`\n\n### Setup\n\n```python\nfrom cortical import CorticalTextProcessor\n```\n\n### Your Task\n\nEmpty hierarchy returns empty ancestors.\n\n### Hints\n\n<details>\n<summary>Hint 1</summary>\n\nStart by considering the edge case of empty input.\n\n</details>\n\n<details>\n<summary>Hint 2</summary>\n\nCheck the expected value and comparison in the assertion\n\n</details>\n\n### Solution\n\n<details>\n<summary>Click to reveal</summary>\n\n```python\ndef test_empty_hierarchy(self):\n        \"\"\"Empty hierarchy returns empty ancestors.\"\"\"\n        result = get_ancestors(\"dog\", {})\n        assert result == {}\n```\n\n</details>\n\n---\n\n## Exercise: Empty Hierarchy\n\n**Concept:** Empty children dict returns empty descendants\n\n**Difficulty:** Advanced\n\n**Time:** ~20 minutes\n\n**Source:** `test_semantics.py`\n\n### Setup\n\n```python\nfrom cortical import CorticalTextProcessor\n```\n\n### Your Task\n\nEmpty children dict returns empty descendants.\n\n### Hints\n\n<details>\n<summary>Hint 1</summary>\n\nStart by considering the edge case of empty input.\n\n</details>\n\n<details>\n<summary>Hint 2</summary>\n\nCheck the expected value and comparison in the assertion\n\n</details>\n\n### Solution\n\n<details>\n<summary>Click to reveal</summary>\n\n```python\ndef test_empty_hierarchy(self):\n        \"\"\"Empty children dict returns empty descendants.\"\"\"\n        result = get_descendants(\"animal\", {})\n        assert result == {}\n```\n\n</details>\n\n---\n\n## Exercise: Empty Corpus\n\n**Concept:** Empty corpus returns no relations\n\n**Difficulty:** Advanced\n\n**Time:** ~20 minutes\n\n**Source:** `test_semantics.py`\n\n### Setup\n\n```python\nfrom cortical import CorticalTextProcessor\nfrom cortical.tokenizer import Tokenizer\n```\n\n### Your Task\n\nEmpty corpus returns no relations.\n\n### Hints\n\n<details>\n<summary>Hint 1</summary>\n\nStart by considering the edge case of empty input.\n\n</details>\n\n<details>\n<summary>Hint 2</summary>\n\nCheck the expected value and comparison in the assertion\n\n</details>\n\n### Solution\n\n<details>\n<summary>Click to reveal</summary>\n\n```python\ndef test_empty_corpus(self):\n        \"\"\"Empty corpus returns no relations.\"\"\"\n        layers = {CorticalLayer.TOKENS: HierarchicalLayer(CorticalLayer.TOKENS)}\n        tokenizer = Tokenizer()\n        result = extract_corpus_semantics(layers, {}, tokenizer)\n        assert result == []\n```\n\n</details>\n\n---\n\n---\n\n*Completed 10 exercises? Check out the other topics for more challenges!*\n",
      "keywords": [
        "details",
        "summary",
        "empty",
        "relations",
        "python",
        "assert",
        "hint",
        "result",
        "dog",
        "isa"
      ]
    },
    {
      "id": "08-exercises/ex-foundations",
      "title": "Exercises: Foundations",
      "content": "# Foundations Exercises\n\n*Hands-on coding exercises to master foundations concepts.*\n\n**Difficulty Level:** Beginner\n\n---\n\n## Introduction\n\nThese exercises cover the fundamental algorithms and data structures of the Cortical Text Processor:\n\n- PageRank for term importance\n- TF-IDF for relevance scoring\n- Graph structures and connections\n- Tokenization and text processing\n\n## Exercise: Empty Graph\n\n**Concept:** Empty graph returns empty dict\n\n**Difficulty:** Beginner\n\n**Time:** ~10 minutes\n\n**Source:** `test_analysis.py`\n\n### Setup\n\n```python\nfrom cortical import CorticalTextProcessor\n```\n\n### Your Task\n\nEmpty graph returns empty dict.\n\n### Hints\n\n<details>\n<summary>Hint 1</summary>\n\nStart by considering the edge case of empty input.\n\n</details>\n\n<details>\n<summary>Hint 2</summary>\n\nPageRank is computed with `compute_pagerank()` or `compute_importance()`\n\n</details>\n\n<details>\n<summary>Hint 3</summary>\n\nCheck the expected value and comparison in the assertion\n\n</details>\n\n### Solution\n\n<details>\n<summary>Click to reveal</summary>\n\n```python\ndef test_empty_graph(self):\n        \"\"\"Empty graph returns empty dict.\"\"\"\n        result = _pagerank_core({})\n        assert result == {}\n```\n\n</details>\n\n---\n\n## Exercise: Single Node No Edges\n\n**Concept:** Single node with no edges gets base rank from damping\n\n**Difficulty:** Beginner\n\n**Time:** ~20 minutes\n\n**Source:** `test_analysis.py`\n\n### Setup\n\n```python\nfrom cortical import CorticalTextProcessor\n```\n\n### Your Task\n\nSingle node with no edges gets base rank from damping.\n\n### Hints\n\n<details>\n<summary>Hint 1</summary>\n\nFocus on the simplest case with just one element.\n\n</details>\n\n<details>\n<summary>Hint 2</summary>\n\nPageRank is computed with `compute_pagerank()` or `compute_importance()`\n\n</details>\n\n<details>\n<summary>Hint 3</summary>\n\nCheck the expected value and comparison in the assertion\n\n</details>\n\n### Solution\n\n<details>\n<summary>Click to reveal</summary>\n\n```python\ndef test_single_node_no_edges(self):\n        \"\"\"Single node with no edges gets base rank from damping.\"\"\"\n        graph = {\"a\": []}\n        result = _pagerank_core(graph, damping=0.85)\n        assert \"a\" in result\n        # With no incoming edges, rank = (1-d)/n = 0.15/1 = 0.15\n        assert result[\"a\"] == pytest.approx(0.15)\n```\n\n</details>\n\n---\n\n## Exercise: Single Node Self Loop\n\n**Concept:** Single node with self-loop still gets rank 1.0\n\n**Difficulty:** Beginner\n\n**Time:** ~20 minutes\n\n**Source:** `test_analysis.py`\n\n### Setup\n\n```python\nfrom cortical import CorticalTextProcessor\n```\n\n### Your Task\n\nSingle node with self-loop still gets rank 1.0.\n\n### Hints\n\n<details>\n<summary>Hint 1</summary>\n\nFocus on the simplest case with just one element.\n\n</details>\n\n<details>\n<summary>Hint 2</summary>\n\nPageRank is computed with `compute_pagerank()` or `compute_importance()`\n\n</details>\n\n<details>\n<summary>Hint 3</summary>\n\nCheck the expected value and comparison in the assertion\n\n</details>\n\n### Solution\n\n<details>\n<summary>Click to reveal</summary>\n\n```python\ndef test_single_node_self_loop(self):\n        \"\"\"Single node with self-loop still gets rank 1.0.\"\"\"\n        graph = {\"a\": [(\"a\", 1.0)]}\n        result = _pagerank_core(graph)\n        assert result[\"a\"] == pytest.approx(1.0)\n```\n\n</details>\n\n---\n\n## Exercise: Three Node Chain\n\n**Concept:** Chain: a -> b -> c. C should have highest rank\n\n**Difficulty:** Beginner\n\n**Time:** ~20 minutes\n\n**Source:** `test_analysis.py`\n\n### Setup\n\n```python\nfrom cortical import CorticalTextProcessor\n```\n\n### Your Task\n\nChain: a -> b -> c. C should have highest rank.\n\n### Hints\n\n<details>\n<summary>Hint 1</summary>\n\nThink about how elements connect in sequence.\n\n</details>\n\n<details>\n<summary>Hint 2</summary>\n\nPageRank is computed with `compute_pagerank()` or `compute_importance()`\n\n</details>\n\n### Solution\n\n<details>\n<summary>Click to reveal</summary>\n\n```python\ndef test_three_node_chain(self):\n        \"\"\"Chain: a -> b -> c. C should have highest rank.\"\"\"\n        graph = {\n            \"a\": [(\"b\", 1.0)],\n            \"b\": [(\"c\", 1.0)],\n            \"c\": []\n        }\n        result = _pagerank_core(graph)\n        # c receives transitively, b receives from a\n        assert result[\"c\"] >= result[\"b\"]\n        assert result[\"b\"] >= result[\"a\"]\n```\n\n</details>\n\n---\n\n## Exercise: Cycle\n\n**Concept:** Cycle: a -> b -> c -> a. All should have equal rank\n\n**Difficulty:** Beginner\n\n**Time:** ~20 minutes\n\n**Source:** `test_analysis.py`\n\n### Setup\n\n```python\nfrom cortical import CorticalTextProcessor\n```\n\n### Your Task\n\nCycle: a -> b -> c -> a. All should have equal rank.\n\n### Hints\n\n<details>\n<summary>Hint 1</summary>\n\nThink about how elements connect in sequence.\n\n</details>\n\n<details>\n<summary>Hint 2</summary>\n\nPageRank is computed with `compute_pagerank()` or `compute_importance()`\n\n</details>\n\n<details>\n<summary>Hint 3</summary>\n\nCheck the expected value and comparison in the assertion\n\n</details>\n\n### Solution\n\n<details>\n<summary>Click to reveal</summary>\n\n```python\ndef test_cycle(self):\n        \"\"\"Cycle: a -> b -> c -> a. All should have equal rank.\"\"\"\n        graph = {\n            \"a\": [(\"b\", 1.0)],\n            \"b\": [(\"c\", 1.0)],\n            \"c\": [(\"a\", 1.0)]\n        }\n        result = _pagerank_core(graph)\n        # All nodes in cycle should have equal rank\n        assert result[\"a\"] == pytest.approx(result[\"b\"], rel=0.01)\n        assert result[\"b\"] == pytest.approx(result[\"c\"], rel=0.01)\n```\n\n</details>\n\n---\n\n## Exercise: Empty Corpus\n\n**Concept:** Empty corpus returns empty dict\n\n**Difficulty:** Beginner\n\n**Time:** ~10 minutes\n\n**Source:** `test_analysis.py`\n\n### Setup\n\n```python\nfrom cortical import CorticalTextProcessor\n```\n\n### Your Task\n\nEmpty corpus returns empty dict.\n\n### Hints\n\n<details>\n<summary>Hint 1</summary>\n\nStart by considering the edge case of empty input.\n\n</details>\n\n<details>\n<summary>Hint 2</summary>\n\nCheck the expected value and comparison in the assertion\n\n</details>\n\n### Solution\n\n<details>\n<summary>Click to reveal</summary>\n\n```python\ndef test_empty_corpus(self):\n        \"\"\"Empty corpus returns empty dict.\"\"\"\n        result = _tfidf_core({}, num_docs=0)\n        assert result == {}\n```\n\n</details>\n\n---\n\n## Exercise: Single Term Single Doc\n\n**Concept:** Single term in single doc has IDF of 0\n\n**Difficulty:** Beginner\n\n**Time:** ~20 minutes\n\n**Source:** `test_analysis.py`\n\n### Setup\n\n```python\nfrom cortical import CorticalTextProcessor\n```\n\n### Your Task\n\nSingle term in single doc has IDF of 0.\n\n### Hints\n\n<details>\n<summary>Hint 1</summary>\n\nFocus on the simplest case with just one element.\n\n</details>\n\n<details>\n<summary>Hint 2</summary>\n\nCheck the expected value and comparison in the assertion\n\n</details>\n\n### Solution\n\n<details>\n<summary>Click to reveal</summary>\n\n```python\ndef test_single_term_single_doc(self):\n        \"\"\"Single term in single doc has IDF of 0.\"\"\"\n        stats = {\n            \"term\": (5, 1, {\"doc1\": 5})\n        }\n        result = _tfidf_core(stats, num_docs=1)\n        # IDF = log(1/1) = 0, so TF-IDF = 0\n        assert result[\"term\"][0] == pytest.approx(0.0)\n```\n\n</details>\n\n---\n\n## Exercise: Empty Graph\n\n**Concept:** Empty graph returns empty dict\n\n**Difficulty:** Beginner\n\n**Time:** ~10 minutes\n\n**Source:** `test_analysis.py`\n\n### Setup\n\n```python\nfrom cortical import CorticalTextProcessor\n```\n\n### Your Task\n\nEmpty graph returns empty dict.\n\n### Hints\n\n<details>\n<summary>Hint 1</summary>\n\nStart by considering the edge case of empty input.\n\n</details>\n\n<details>\n<summary>Hint 2</summary>\n\nCheck the expected value and comparison in the assertion\n\n</details>\n\n### Solution\n\n<details>\n<summary>Click to reveal</summary>\n\n```python\ndef test_empty_graph(self):\n        \"\"\"Empty graph returns empty dict.\"\"\"\n        result = _louvain_core({})\n        assert result == {}\n```\n\n</details>\n\n---\n\n## Exercise: Single Node\n\n**Concept:** Single node is its own community\n\n**Difficulty:** Beginner\n\n**Time:** ~10 minutes\n\n**Source:** `test_analysis.py`\n\n### Setup\n\n```python\nfrom cortical import CorticalTextProcessor\n```\n\n### Your Task\n\nSingle node is its own community.\n\n### Hints\n\n<details>\n<summary>Hint 1</summary>\n\nFocus on the simplest case with just one element.\n\n</details>\n\n<details>\n<summary>Hint 2</summary>\n\nCheck the expected value and comparison in the assertion\n\n</details>\n\n### Solution\n\n<details>\n<summary>Click to reveal</summary>\n\n```python\ndef test_single_node(self):\n        \"\"\"Single node is its own community.\"\"\"\n        result = _louvain_core({\"a\": {}})\n        assert \"a\" in result\n        assert result[\"a\"] == 0\n```\n\n</details>\n\n---\n\n## Exercise: Empty Graph\n\n**Concept:** Empty graph has zero modularity\n\n**Difficulty:** Beginner\n\n**Time:** ~10 minutes\n\n**Source:** `test_analysis.py`\n\n### Setup\n\n```python\nfrom cortical import CorticalTextProcessor\n```\n\n### Your Task\n\nEmpty graph has zero modularity.\n\n### Hints\n\n<details>\n<summary>Hint 1</summary>\n\nStart by considering the edge case of empty input.\n\n</details>\n\n<details>\n<summary>Hint 2</summary>\n\nCheck the expected value and comparison in the assertion\n\n</details>\n\n### Solution\n\n<details>\n<summary>Click to reveal</summary>\n\n```python\ndef test_empty_graph(self):\n        \"\"\"Empty graph has zero modularity.\"\"\"\n        result = _modularity_core({}, {})\n        assert result == 0.0\n```\n\n</details>\n\n---\n\n---\n\n*Completed 10 exercises? Check out the other topics for more challenges!*\n",
      "keywords": [
        "details",
        "summary",
        "empty",
        "result",
        "hint",
        "graph",
        "python",
        "single",
        "self",
        "assert"
      ]
    },
    {
      "id": "08-exercises/ex-search",
      "title": "Exercises: Search",
      "content": "# Search Exercises\n\n*Hands-on coding exercises to master search concepts.*\n\n**Difficulty Level:** Intermediate\n\n---\n\n## Introduction\n\nMaster the search and retrieval capabilities:\n\n- Query expansion techniques\n- Document ranking algorithms\n- Passage retrieval\n- Definition extraction\n\n## Exercise: Empty Query\n\n**Concept:** Empty query returns empty results\n\n**Difficulty:** Intermediate\n\n**Time:** ~20 minutes\n\n**Source:** `test_query_search.py`\n\n### Setup\n\n```python\nfrom cortical import CorticalTextProcessor\nfrom cortical.tokenizer import Tokenizer\n```\n\n### Your Task\n\nEmpty query returns empty results.\n\n### Hints\n\n<details>\n<summary>Hint 1</summary>\n\nStart by considering the edge case of empty input.\n\n</details>\n\n<details>\n<summary>Hint 2</summary>\n\nYou may need to create mock layers or minicolumns for testing\n\n</details>\n\n<details>\n<summary>Hint 3</summary>\n\nCheck the expected value and comparison in the assertion\n\n</details>\n\n### Solution\n\n<details>\n<summary>Click to reveal</summary>\n\n```python\ndef test_empty_query(self):\n        \"\"\"Empty query returns empty results.\"\"\"\n        layers = MockLayers.single_term(\"term\", tfidf=1.0, doc_ids=[\"doc1\"])\n        tokenizer = Tokenizer()\n\n        # Tokenizer will return empty list for empty string\n        result = find_documents_for_query(\"\", layers, tokenizer)\n        assert result == []\n```\n\n</details>\n\n---\n\n## Exercise: Single Term Single Doc\n\n**Concept:** Single term matching single document\n\n**Difficulty:** Intermediate\n\n**Time:** ~20 minutes\n\n**Source:** `test_query_search.py`\n\n### Setup\n\n```python\nfrom cortical import CorticalTextProcessor\nfrom cortical.tokenizer import Tokenizer\n```\n\n### Your Task\n\nSingle term matching single document.\n\n### Hints\n\n<details>\n<summary>Hint 1</summary>\n\nFocus on the simplest case with just one element.\n\n</details>\n\n<details>\n<summary>Hint 2</summary>\n\nYou may need to create mock layers or minicolumns for testing\n\n</details>\n\n<details>\n<summary>Hint 3</summary>\n\nCheck the expected value and comparison in the assertion\n\n</details>\n\n### Solution\n\n<details>\n<summary>Click to reveal</summary>\n\n```python\ndef test_single_term_single_doc(self):\n        \"\"\"Single term matching single document.\"\"\"\n        # Create layer with term in doc1\n        col = MockMinicolumn(\n            content=\"neural\",\n            tfidf=2.5,\n            document_ids={\"doc1\"},\n            tfidf_per_doc={\"doc1\": 2.5}\n        )\n        layers = MockLayers.empty()\n        layers[MockLayers.TOKENS] = MockHierarchicalLayer([col])\n\n        tokenizer = Tokenizer()\n        result = find_documents_for_query(\n            \"neural\", layers, tokenizer, use_expansion=False\n        )\n\n        assert len(result) == 1\n        assert result[0][0] == \"doc1\"\n        assert result[0][1] > 0\n```\n\n</details>\n\n---\n\n## Exercise: Single Term Multiple Docs\n\n**Concept:** Single term in multiple documents ranked by TF-IDF\n\n**Difficulty:** Intermediate\n\n**Time:** ~20 minutes\n\n**Source:** `test_query_search.py`\n\n### Setup\n\n```python\nfrom cortical import CorticalTextProcessor\nfrom cortical.tokenizer import Tokenizer\n```\n\n### Your Task\n\nSingle term in multiple documents ranked by TF-IDF.\n\n### Hints\n\n<details>\n<summary>Hint 1</summary>\n\nFocus on the simplest case with just one element.\n\n</details>\n\n<details>\n<summary>Hint 2</summary>\n\nYou may need to create mock layers or minicolumns for testing\n\n</details>\n\n<details>\n<summary>Hint 3</summary>\n\nCheck the expected value and comparison in the assertion\n\n</details>\n\n### Solution\n\n<details>\n<summary>Click to reveal</summary>\n\n```python\ndef test_single_term_multiple_docs(self):\n        \"\"\"Single term in multiple documents ranked by TF-IDF.\"\"\"\n        col = MockMinicolumn(\n            content=\"algorithm\",\n            tfidf=3.0,\n            document_ids={\"doc1\", \"doc2\", \"doc3\"},\n            tfidf_per_doc={\"doc1\": 5.0, \"doc2\": 3.0, \"doc3\": 1.0}\n        )\n        layers = MockLayers.empty()\n        layers[MockLayers.TOKENS] = MockHierarchicalLayer([col])\n\n        tokenizer = Tokenizer()\n        result = find_documents_for_query(\n            \"algorithm\", layers, tokenizer, use_expansion=False\n        )\n\n        assert len(result) == 3\n        # Should be sorted by TF-IDF score\n        assert result[0][0] == \"doc1\"  # Highest score\n        assert result[1][0] == \"doc2\"\n        assert result[2][0] == \"doc3\"  # Lowest score\n        assert result[0][1] > result[1][1] > result[2][1]\n```\n\n</details>\n\n---\n\n## Exercise: Query Expansion Disabled\n\n**Concept:** use_expansion=False uses only query terms\n\n**Difficulty:** Intermediate\n\n**Time:** ~20 minutes\n\n**Source:** `test_query_search.py`\n\n### Setup\n\n```python\nfrom cortical import CorticalTextProcessor\nfrom cortical.tokenizer import Tokenizer\n```\n\n### Your Task\n\nuse_expansion=False uses only query terms.\n\n### Hints\n\n<details>\n<summary>Hint 1</summary>\n\nBreak down the problem into smaller steps.\n\n</details>\n\n<details>\n<summary>Hint 2</summary>\n\nPageRank is computed with `compute_pagerank()` or `compute_importance()`\n\n</details>\n\n<details>\n<summary>Hint 3</summary>\n\nCheck the expected value and comparison in the assertion\n\n</details>\n\n### Solution\n\n<details>\n<summary>Click to reveal</summary>\n\n```python\ndef test_query_expansion_disabled(self):\n        \"\"\"use_expansion=False uses only query terms.\"\"\"\n        # Create connected terms\n        layers = (\n            LayerBuilder()\n            .with_term(\"neural\", tfidf=2.0, pagerank=0.8)\n            .with_term(\"network\", tfidf=2.0, pagerank=0.6)\n            .with_connection(\"neural\", \"network\", weight=5.0)\n            .with_document(\"doc1\", [\"neural\"])\n            .with_document(\"doc2\", [\"network\"])\n            .build()\n        )\n\n        layer0 = layers[MockLayers.TOKENS]\n        layer0.get_minicolumn(\"neural\").tfidf_per_doc = {\"doc1\": 2.0}\n        layer0.get_minicolumn(\"network\").tfidf_per_doc = {\"doc2\": 2.0}\n\n        tokenizer = Tokenizer()\n        result = find_documents_for_query(\n            \"neural\", layers, tokenizer,\n            use_expansion=False\n        )\n\n        # Should only find doc1 (contains \"neural\")\n        assert len(result) == 1\n        assert result[0][0] == \"doc1\"\n```\n\n</details>\n\n---\n\n## Exercise: Empty Corpus\n\n**Concept:** Empty corpus returns empty results\n\n**Difficulty:** Intermediate\n\n**Time:** ~20 minutes\n\n**Source:** `test_query_search.py`\n\n### Setup\n\n```python\nfrom cortical import CorticalTextProcessor\nfrom cortical.tokenizer import Tokenizer\n```\n\n### Your Task\n\nEmpty corpus returns empty results.\n\n### Hints\n\n<details>\n<summary>Hint 1</summary>\n\nStart by considering the edge case of empty input.\n\n</details>\n\n<details>\n<summary>Hint 2</summary>\n\nYou may need to create mock layers or minicolumns for testing\n\n</details>\n\n<details>\n<summary>Hint 3</summary>\n\nCheck the expected value and comparison in the assertion\n\n</details>\n\n### Solution\n\n<details>\n<summary>Click to reveal</summary>\n\n```python\ndef test_empty_corpus(self):\n        \"\"\"Empty corpus returns empty results.\"\"\"\n        layers = MockLayers.empty()\n        tokenizer = Tokenizer()\n\n        result = find_documents_for_query(\"query\", layers, tokenizer)\n\n        assert result == []\n```\n\n</details>\n\n---\n\n## Exercise: Single Term Match\n\n**Concept:** Fast search finds document with matching term\n\n**Difficulty:** Intermediate\n\n**Time:** ~20 minutes\n\n**Source:** `test_query_search.py`\n\n### Setup\n\n```python\nfrom cortical import CorticalTextProcessor\nfrom cortical.tokenizer import Tokenizer\n```\n\n### Your Task\n\nFast search finds document with matching term.\n\n### Hints\n\n<details>\n<summary>Hint 1</summary>\n\nFocus on the simplest case with just one element.\n\n</details>\n\n<details>\n<summary>Hint 2</summary>\n\nYou may need to create mock layers or minicolumns for testing\n\n</details>\n\n<details>\n<summary>Hint 3</summary>\n\nCheck the expected value and comparison in the assertion\n\n</details>\n\n### Solution\n\n<details>\n<summary>Click to reveal</summary>\n\n```python\ndef test_single_term_match(self):\n        \"\"\"Fast search finds document with matching term.\"\"\"\n        col = MockMinicolumn(\n            content=\"algorithm\",\n            document_ids={\"doc1\"},\n            tfidf_per_doc={\"doc1\": 3.0}\n        )\n        layers = MockLayers.empty()\n        layers[MockLayers.TOKENS] = MockHierarchicalLayer([col])\n\n        tokenizer = Tokenizer()\n        result = fast_find_documents(\"algorithm\", layers, tokenizer)\n\n        assert len(result) == 1\n        assert result[0][0] == \"doc1\"\n```\n\n</details>\n\n---\n\n## Exercise: Empty Query\n\n**Concept:** Empty query returns empty results\n\n**Difficulty:** Intermediate\n\n**Time:** ~20 minutes\n\n**Source:** `test_query_search.py`\n\n### Setup\n\n```python\nfrom cortical import CorticalTextProcessor\nfrom cortical.tokenizer import Tokenizer\n```\n\n### Your Task\n\nEmpty query returns empty results.\n\n### Hints\n\n<details>\n<summary>Hint 1</summary>\n\nStart by considering the edge case of empty input.\n\n</details>\n\n<details>\n<summary>Hint 2</summary>\n\nYou may need to create mock layers or minicolumns for testing\n\n</details>\n\n<details>\n<summary>Hint 3</summary>\n\nCheck the expected value and comparison in the assertion\n\n</details>\n\n### Solution\n\n<details>\n<summary>Click to reveal</summary>\n\n```python\ndef test_empty_query(self):\n        \"\"\"Empty query returns empty results.\"\"\"\n        layers = MockLayers.single_term(\"term\", doc_ids=[\"doc1\"])\n        tokenizer = Tokenizer()\n\n        result = fast_find_documents(\"\", layers, tokenizer)\n\n        assert result == []\n```\n\n</details>\n\n---\n\n## Exercise: No Candidates Returns Empty\n\n**Concept:** No matching candidates returns empty\n\n**Difficulty:** Intermediate\n\n**Time:** ~20 minutes\n\n**Source:** `test_query_search.py`\n\n### Setup\n\n```python\nfrom cortical import CorticalTextProcessor\nfrom cortical.tokenizer import Tokenizer\n```\n\n### Your Task\n\nNo matching candidates returns empty.\n\n### Hints\n\n<details>\n<summary>Hint 1</summary>\n\nStart by considering the edge case of empty input.\n\n</details>\n\n<details>\n<summary>Hint 2</summary>\n\nYou may need to create mock layers or minicolumns for testing\n\n</details>\n\n<details>\n<summary>Hint 3</summary>\n\nCheck the expected value and comparison in the assertion\n\n</details>\n\n### Solution\n\n<details>\n<summary>Click to reveal</summary>\n\n```python\ndef test_no_candidates_returns_empty(self):\n        \"\"\"No matching candidates returns empty.\"\"\"\n        layers = MockLayers.single_term(\"existing\", doc_ids=[\"doc1\"])\n        tokenizer = Tokenizer()\n\n        result = fast_find_documents(\"nonexistent\", layers, tokenizer)\n\n        assert result == []\n```\n\n</details>\n\n---\n\n## Exercise: Empty Layer\n\n**Concept:** Empty layer returns empty index\n\n**Difficulty:** Intermediate\n\n**Time:** ~10 minutes\n\n**Source:** `test_query_search.py`\n\n### Setup\n\n```python\nfrom cortical import CorticalTextProcessor\n```\n\n### Your Task\n\nEmpty layer returns empty index.\n\n### Hints\n\n<details>\n<summary>Hint 1</summary>\n\nStart by considering the edge case of empty input.\n\n</details>\n\n<details>\n<summary>Hint 2</summary>\n\nYou may need to create mock layers or minicolumns for testing\n\n</details>\n\n<details>\n<summary>Hint 3</summary>\n\nCheck the expected value and comparison in the assertion\n\n</details>\n\n### Solution\n\n<details>\n<summary>Click to reveal</summary>\n\n```python\ndef test_empty_layer(self):\n        \"\"\"Empty layer returns empty index.\"\"\"\n        layers = MockLayers.empty()\n        result = build_document_index(layers)\n        assert result == {}\n```\n\n</details>\n\n---\n\n## Exercise: Single Term Single Doc\n\n**Concept:** Single term in single document\n\n**Difficulty:** Intermediate\n\n**Time:** ~20 minutes\n\n**Source:** `test_query_search.py`\n\n### Setup\n\n```python\nfrom cortical import CorticalTextProcessor\n```\n\n### Your Task\n\nSingle term in single document.\n\n### Hints\n\n<details>\n<summary>Hint 1</summary>\n\nFocus on the simplest case with just one element.\n\n</details>\n\n<details>\n<summary>Hint 2</summary>\n\nYou may need to create mock layers or minicolumns for testing\n\n</details>\n\n<details>\n<summary>Hint 3</summary>\n\nCheck the expected value and comparison in the assertion\n\n</details>\n\n### Solution\n\n<details>\n<summary>Click to reveal</summary>\n\n```python\ndef test_single_term_single_doc(self):\n        \"\"\"Single term in single document.\"\"\"\n        col = MockMinicolumn(\n            content=\"term\",\n            document_ids={\"doc1\"},\n            tfidf_per_doc={\"doc1\": 2.5}\n        )\n        layers = MockLayers.empty()\n        layers[MockLayers.TOKENS] = MockHierarchicalLayer([col])\n\n        result = build_document_index(layers)\n\n        assert \"term\" in result\n        assert result[\"term\"] == {\"doc1\": 2.5}\n```\n\n</details>\n\n---\n\n---\n\n*Completed 10 exercises? Check out the other topics for more challenges!*\n",
      "keywords": [
        "details",
        "summary",
        "empty",
        "tokenizer",
        "layers",
        "result",
        "hint",
        "term",
        "single",
        "python"
      ]
    },
    {
      "id": "08-exercises/index",
      "title": "Exercise Index",
      "content": "# Exercises\n\n*Hands-on coding exercises derived from the test suite.*\n\n---\n\n## Overview\n\nLearn by doing! These exercises are extracted from the Cortical Text Processor's test suite and transformed into learning challenges.\n\nEach exercise includes:\n\n- **Clear task description** - What you need to implement\n- **Progressive hints** - Guidance without spoilers\n- **Complete solution** - Reference implementation from tests\n- **Verification** - How to check your answer\n\n## Exercise Topics\n\n### [Foundations](ex-foundations.md)\n\n**Difficulty:** Beginner\n\n**Exercises:** 10\n\nCore algorithms and data structures. Start here if you're new!\n\n### [Search](ex-search.md)\n\n**Difficulty:** Intermediate\n\n**Exercises:** 10\n\nSearch, ranking, and retrieval techniques. Build on foundations.\n\n### [Advanced](ex-advanced.md)\n\n**Difficulty:** Advanced\n\n**Exercises:** 10\n\nAdvanced features and complex algorithms. For experienced users.\n\n## Learning Path\n\n**Recommended progression:**\n\n1. Start with **Foundations** exercises\n2. Move to **Search** once comfortable\n3. Challenge yourself with **Advanced** topics\n\n## Tips for Success\n\n- **Read the test carefully** - The docstring explains what's being tested\n- **Use hints progressively** - Try solving first, then reveal hints as needed\n- **Run the solution** - Verify your understanding by executing the code\n- **Experiment** - Modify parameters and see how behavior changes\n\n---\n\n*Total exercises: 30*\n",
      "keywords": [
        "exercises",
        "advanced",
        "foundations",
        "search",
        "test",
        "hints",
        "difficulty",
        "suite",
        "learning",
        "exercise"
      ]
    },
    {
      "id": "09-journey/index",
      "title": "Your Learning Journey",
      "content": "# Your Learning Journey\n\n*A progressive path through the Cortical Text Processor, designed for learners at all levels.*\n\n---\n\n## Overview\n\nThis learning journey is organized into three progressive stages, each building on the previous one. The concepts are ordered based on:\n\n- **Dependencies:** What you need to know first\n- **Complexity:** From simple to sophisticated\n- **Historical emergence:** When concepts first appeared in development\n\n## Learning Paths\n\n### [Beginner Path](journey-beginner.md)\n\n**Concepts:** 3  \n**Time:** ~45 minutes  \n**Preview:** Tokenization, Stop Words, Tf-Idf\n\n### [Intermediate Path](journey-intermediate.md)\n\n**Concepts:** 6  \n**Time:** ~150 minutes  \n**Preview:** Pagerank, Lateral Connections, Query Expansion, and 3 more\n\n### [Advanced Path](journey-advanced.md)\n\n**Concepts:** 3  \n**Time:** ~105 minutes  \n**Preview:** Concept Clustering, Semantic Relations, Louvain\n\n---\n\n# Suggested Study Schedule\n\n*A practical 4-week plan to master the Cortical Text Processor.*\n\n---\n\n## Week 1: Foundations\n\n**Goal:** Understand the core building blocks\n\n**Concepts:**\n- Tokenization (~15 min)\n- Stop Words (~15 min)\n- Tf-Idf (~15 min)\n\n**Total Time:** ~45 minutes\n\n**Activities:**\n- Read foundation chapters\n- Run `showcase.py` to see concepts in action\n- Experiment with basic tokenization and TF-IDF\n\n## Week 2: Building Complexity\n\n**Goal:** Add graph-based features to your mental model\n\n**Concepts:**\n- Pagerank (~25 min)\n- Lateral Connections (~25 min)\n- Query Expansion (~25 min)\n\n**Total Time:** ~75 minutes\n\n**Activities:**\n- Study PageRank and BM25 implementations\n- Index a sample corpus with `scripts/index_codebase.py`\n- Explore query expansion behavior\n\n## Week 3: Advanced Structures\n\n**Goal:** Understand hierarchical organization and persistence\n\n**Concepts:**\n- Incremental Indexing (~25 min)\n- Bm25 (~25 min)\n- Persistence (~25 min)\n\n**Total Time:** ~75 minutes\n\n**Activities:**\n- Review minicolumn and layer architecture\n- Practice save/load operations\n- Test incremental indexing\n\n## Week 4: Mastery\n\n**Goal:** Master sophisticated algorithms and optimization\n\n**Concepts:**\n- Concept Clustering (~35 min)\n- Semantic Relations (~35 min)\n- Louvain (~35 min)\n\n**Total Time:** ~105 minutes\n\n**Activities:**\n- Study Louvain clustering implementation\n- Experiment with semantic relations extraction\n- Profile performance with `scripts/profile_full_analysis.py`\n- Implement a custom feature using the library\n\n## Tips for Success\n\n1. **Follow the order** - Prerequisites build on each other\n2. **Code along** - Run examples from `showcase.py` and scripts\n3. **Read tests** - `tests/` directory shows real usage patterns\n4. **Ask questions** - Use the semantic search: `python scripts/search_codebase.py \"your question\"`\n5. **Build something** - Best way to learn is to apply the concepts\n\n",
      "keywords": [
        "min",
        "concepts",
        "time",
        "minutes",
        "journey",
        "week",
        "path",
        "semantic",
        "goal",
        "total"
      ]
    },
    {
      "id": "09-journey/journey-advanced",
      "title": "Learning Journey: Advanced",
      "content": "# Mastery Path (Advanced)\n\n*Deep dives into sophisticated algorithms. For those ready to master the full system.*\n\n---\n\n**Concepts:** 3\n\n**Estimated Time:** ~105 minutes\n\n---\n\n## 1. Concept Clustering\n\n**First Introduced:** 2025-12-09\n\n**Mentions in History:** 2 commits\n\n**Reading Time:** ~35 min\n\n**Prerequisites:** None (foundational concept)\n\n**Where to Learn:**\n- Foundations: `book/01-foundations/`\n- Modules: `book/02-modules/`\n- Evolution: `book/07-concepts/concept-clustering.md`\n\n**Key Takeaway:**\nUnderstand the role of concept clustering in the information retrieval pipeline.\n---\n\n## 2. Semantic Relations\n\n**First Introduced:** 2025-12-09\n\n**Mentions in History:** 1 commits\n\n**Reading Time:** ~35 min\n\n**Prerequisites:**\n- Query Expansion\n\n**Where to Learn:**\n- Foundations: `book/01-foundations/`\n- Modules: `book/02-modules/`\n- Evolution: `book/07-concepts/semantic-relations.md`\n\n**Key Takeaway:**\nDive into pattern-based extraction of typed relationships.\n---\n\n## 3. Louvain\n\n**First Introduced:** 2025-12-11\n\n**Mentions in History:** 7 commits\n\n**Reading Time:** ~35 min\n\n**Prerequisites:** None (foundational concept)\n\n**Where to Learn:**\n- Foundations: `book/01-foundations/`\n- Modules: `book/02-modules/`\n- Evolution: `book/07-concepts/louvain.md`\n\n**Key Takeaway:**\nExplore community detection for automatic concept clustering.\n---\n\n",
      "keywords": [
        "book",
        "concept",
        "foundations",
        "modules",
        "concepts",
        "time",
        "clustering",
        "first",
        "introduced",
        "mentions"
      ]
    },
    {
      "id": "09-journey/journey-beginner",
      "title": "Learning Journey: Beginner",
      "content": "# Start Here (Foundational)\n\n*Core concepts that unlock everything else. Start here if you're new to information retrieval.*\n\n---\n\n**Concepts:** 3\n\n**Estimated Time:** ~45 minutes\n\n---\n\n## 1. Tokenization\n\n**First Introduced:** 2025-12-10\n\n**Mentions in History:** 3 commits\n\n**Reading Time:** ~15 min\n\n**Prerequisites:** None (foundational concept)\n\n**Related Concepts:**\n- Bm25\n\n**Where to Learn:**\n- Foundations: `book/01-foundations/`\n- Modules: `book/02-modules/`\n- Evolution: `book/07-concepts/tokenization.md`\n\n**Key Takeaway:**\nLearn how text is broken into processable units - the foundation of all text analysis.\n---\n\n## 2. Stop Words\n\n**First Introduced:** 2025-12-13\n\n**Mentions in History:** 1 commits\n\n**Reading Time:** ~15 min\n\n**Prerequisites:** None (foundational concept)\n\n**Where to Learn:**\n- Foundations: `book/01-foundations/`\n- Modules: `book/02-modules/`\n- Evolution: `book/07-concepts/stop-words.md`\n\n**Key Takeaway:**\nSee why common words are filtered to focus on meaningful content.\n---\n\n## 3. Tf-Idf\n\n**First Introduced:** 2025-12-15\n\n**Mentions in History:** 2 commits\n\n**Reading Time:** ~15 min\n\n**Prerequisites:** None (foundational concept)\n\n**Related Concepts:**\n- Bm25\n- Query Expansion\n\n**Where to Learn:**\n- Foundations: `book/01-foundations/`\n- Modules: `book/02-modules/`\n- Evolution: `book/07-concepts/tf-idf.md`\n\n**Key Takeaway:**\nMaster the classic algorithm for term importance - weighing frequency against distinctiveness.\n---\n\n",
      "keywords": [
        "book",
        "concepts",
        "foundations",
        "modules",
        "foundational",
        "time",
        "learn",
        "first",
        "introduced",
        "mentions"
      ]
    },
    {
      "id": "09-journey/journey-intermediate",
      "title": "Learning Journey: Intermediate",
      "content": "# Going Deeper (Intermediate)\n\n*Build on the foundations. These concepts add power and flexibility to your understanding.*\n\n---\n\n**Concepts:** 6\n\n**Estimated Time:** ~150 minutes\n\n---\n\n## 1. Pagerank\n\n**First Introduced:** 2025-12-09\n\n**Mentions in History:** 4 commits\n\n**Reading Time:** ~25 min\n\n**Prerequisites:** None (foundational concept)\n\n**Where to Learn:**\n- Foundations: `book/01-foundations/`\n- Modules: `book/02-modules/`\n- Evolution: `book/07-concepts/pagerank.md`\n\n**Key Takeaway:**\nLearn how graph algorithms measure importance through connections.\n---\n\n## 2. Lateral Connections\n\n**First Introduced:** 2025-12-09\n\n**Mentions in History:** 2 commits\n\n**Reading Time:** ~25 min\n\n**Prerequisites:** None (foundational concept)\n\n**Where to Learn:**\n- Foundations: `book/01-foundations/`\n- Modules: `book/02-modules/`\n- Evolution: `book/07-concepts/lateral-connections.md`\n\n**Key Takeaway:**\nGrasp the Hebbian-inspired network of term relationships.\n---\n\n## 3. Query Expansion\n\n**First Introduced:** 2025-12-10\n\n**Mentions in History:** 4 commits\n\n**Reading Time:** ~25 min\n\n**Prerequisites:**\n- Tf-Idf\n\n**Related Concepts:**\n- Tf-Idf\n\n**Where to Learn:**\n- Foundations: `book/01-foundations/`\n- Modules: `book/02-modules/`\n- Evolution: `book/07-concepts/query-expansion.md`\n\n**Key Takeaway:**\nSee how searches become smarter by exploring related terms.\n---\n\n## 4. Incremental Indexing\n\n**First Introduced:** 2025-12-10\n\n**Mentions in History:** 1 commits\n\n**Reading Time:** ~25 min\n\n**Prerequisites:** None (foundational concept)\n\n**Where to Learn:**\n- Foundations: `book/01-foundations/`\n- Modules: `book/02-modules/`\n- Evolution: `book/07-concepts/incremental-indexing.md`\n\n**Key Takeaway:**\nUnderstand the role of incremental indexing in the information retrieval pipeline.\n---\n\n## 5. Bm25\n\n**First Introduced:** 2025-12-15\n\n**Mentions in History:** 9 commits\n\n**Reading Time:** ~25 min\n\n**Prerequisites:**\n- Tokenization\n- Tf-Idf\n\n**Related Concepts:**\n- Tf-Idf\n- Tokenization\n\n**Where to Learn:**\n- Foundations: `book/01-foundations/`\n- Modules: `book/02-modules/`\n- Evolution: `book/07-concepts/bm25.md`\n\n**Key Takeaway:**\nUnderstand the modern scoring function that improves on TF-IDF with saturation.\n---\n\n## 6. Persistence\n\n**First Introduced:** 2025-12-16\n\n**Mentions in History:** 1 commits\n\n**Reading Time:** ~25 min\n\n**Prerequisites:** None (foundational concept)\n\n**Where to Learn:**\n- Foundations: `book/01-foundations/`\n- Modules: `book/02-modules/`\n- Evolution: `book/07-concepts/persistence.md`\n\n**Key Takeaway:**\nUnderstand the role of persistence in the information retrieval pipeline.\n---\n\n",
      "keywords": [
        "book",
        "foundations",
        "modules",
        "concepts",
        "time",
        "learn",
        "first",
        "introduced",
        "mentions",
        "history"
      ]
    },
    {
      "id": "docs/CONTRIBUTING",
      "title": "CONTRIBUTING",
      "content": "# Contributing to The Cortical Chronicles\n\n> **Welcome!** This guide shows you how to add new generators and extend \"The Cortical Chronicles.\"\n\n---\n\n## Table of Contents\n\n- [Quick Start](#quick-start)\n- [Generator Architecture](#generator-architecture)\n- [Step-by-Step: Adding a Generator](#step-by-step-adding-a-generator)\n- [Best Practices](#best-practices)\n- [Testing Your Generator](#testing-your-generator)\n- [CI Integration](#ci-integration)\n- [Examples](#examples)\n\n---\n\n## Quick Start\n\n### Prerequisites\n\n```bash\n# Ensure you have dependencies\npip install -e \".[dev]\"\n\n# Verify PyYAML is installed\npython -c \"import yaml; print('PyYAML OK')\"\n\n# Test existing generators\npython scripts/generate_book.py --list\npython scripts/generate_book.py --dry-run\n```\n\n### Template for New Generator\n\n```python\nfrom scripts.generate_book import ChapterGenerator\nfrom pathlib import Path\nfrom typing import Dict, Any, List\n\nclass MyChapterGenerator(ChapterGenerator):\n    \"\"\"Generate chapters from <your data source>.\"\"\"\n\n    @property\n    def name(self) -> str:\n        \"\"\"Generator name for CLI and logging.\"\"\"\n        return \"mychapter\"\n\n    @property\n    def output_dir(self) -> str:\n        \"\"\"Subdirectory in book/ for output.\"\"\"\n        return \"03-decisions\"  # Choose: 00-05 based on content type\n\n    def generate(self, dry_run: bool = False, verbose: bool = False) -> Dict[str, Any]:\n        \"\"\"\n        Generate chapter content.\n\n        Args:\n            dry_run: If True, don't write files (just log)\n            verbose: If True, print detailed progress\n\n        Returns:\n            Dict with:\n                - files: List of generated file paths\n                - stats: Generation statistics\n                - errors: Any errors encountered\n        \"\"\"\n        errors = []\n        stats = {\n            \"items_processed\": 0,\n            \"chapters_written\": 0\n        }\n\n        if verbose:\n            print(\"  Processing data...\")\n\n        # 1. Load your data source\n        try:\n            data = self._load_data()\n        except Exception as e:\n            errors.append(f\"Failed to load data: {e}\")\n            return {\"files\": [], \"stats\": stats, \"errors\": errors}\n\n        # 2. Process each item\n        for item in data:\n            content = self._generate_chapter_content(item)\n            filename = self._generate_filename(item)\n\n            if verbose:\n                print(f\"  Generating: {filename}\")\n\n            self.write_chapter(filename, content, dry_run=dry_run)\n            stats[\"chapters_written\"] += 1\n\n        stats[\"items_processed\"] = len(data)\n\n        return {\n            \"files\": [str(f) for f in self.generated_files],\n            \"stats\": stats,\n            \"errors\": errors\n        }\n\n    def _load_data(self) -> List[Any]:\n        \"\"\"Load your data source.\"\"\"\n        # Implement data loading\n        pass\n\n    def _generate_chapter_content(self, item: Any) -> str:\n        \"\"\"Generate markdown content for one chapter.\"\"\"\n        # Generate frontmatter\n        frontmatter = self.generate_frontmatter(\n            title=item['title'],\n            tags=['tag1', 'tag2'],\n            source_files=['source.py']\n        )\n\n        # Build content\n        content = frontmatter\n        content += f\"# {item['title']}\\n\\n\"\n        content += item['body']\n        content += \"\\n\\n---\\n\\n\"\n        content += \"*This chapter is part of [The Cortical Chronicles](../README.md).*\\n\"\n\n        return content\n\n    def _generate_filename(self, item: Any) -> str:\n        \"\"\"Generate filename from item.\"\"\"\n        # Slugify the title\n        slug = item['title'].lower().replace(' ', '-')\n        return f\"{slug}.md\"\n```\n\n---\n\n## Generator Architecture\n\n### Base Class: `ChapterGenerator`\n\nAll generators inherit from `ChapterGenerator` (defined in `scripts/generate_book.py`).\n\n**Required Methods:**\n\n| Method | Returns | Purpose |\n|--------|---------|---------|\n| `name` (property) | `str` | Generator identifier for CLI |\n| `output_dir` (property) | `str` | Subdirectory in book/ |\n| `generate(dry_run, verbose)` | `Dict[str, Any]` | Main generation logic |\n\n**Provided Helper Methods:**\n\n| Method | Purpose |\n|--------|---------|\n| `write_chapter(filename, content, dry_run)` | Write chapter with standard handling |\n| `generate_frontmatter(title, tags, sources)` | Generate YAML frontmatter |\n\n**Instance Variables:**\n\n- `self.book_dir`: Path to book/ directory\n- `self.generated_files`: List of written file paths (auto-tracked by `write_chapter`)\n\n### Output Directories\n\nChoose the appropriate section for your content:\n\n| Directory | Purpose | Example Generators |\n|-----------|---------|-------------------|\n| `00-preface` | Book introduction | (manual) |\n| `01-foundations` | Algorithm theory | AlgorithmChapterGenerator |\n| `02-architecture` | Module documentation | ModuleDocGenerator |\n| `03-decisions` | ADRs, design decisions | DecisionRecordGenerator |\n| `04-evolution` | Commit narratives | CommitNarrativeGenerator |\n| `05-future` | Roadmap, vision | (placeholder) |\n\n### Return Value Format\n\n```python\n{\n    \"files\": [\n        \"book/01-foundations/alg-pagerank.md\",\n        \"book/01-foundations/alg-bm25.md\"\n    ],\n    \"stats\": {\n        \"algorithms_found\": 6,\n        \"chapters_written\": 6,\n        \"custom_metric\": 42\n    },\n    \"errors\": [\n        \"Warning: Optional data not found\"\n    ]\n}\n```\n\n---\n\n## Step-by-Step: Adding a Generator\n\n### Step 1: Create the Generator Class\n\nAdd to `scripts/generate_book.py` or create a new file:\n\n```python\nclass MyChapterGenerator(ChapterGenerator):\n    \"\"\"One-line description.\"\"\"\n\n    @property\n    def name(self) -> str:\n        return \"mychapter\"\n\n    @property\n    def output_dir(self) -> str:\n        return \"03-decisions\"  # Choose appropriate section\n\n    def generate(self, dry_run: bool = False, verbose: bool = False) -> Dict[str, Any]:\n        # Implementation\n        pass\n```\n\n### Step 2: Register in BookBuilder\n\nIn `scripts/generate_book.py`, find the `main()` function and add:\n\n```python\ndef main():\n    # ...existing code...\n\n    # Register real generators\n    builder.register_generator(AlgorithmChapterGenerator(book_dir=args.output))\n    builder.register_generator(ModuleDocGenerator(book_dir=args.output))\n    builder.register_generator(CommitNarrativeGenerator(book_dir=args.output))\n    builder.register_generator(MyChapterGenerator(book_dir=args.output))  # <-- ADD THIS\n    builder.register_generator(SearchIndexGenerator(book_dir=args.output))\n\n    # ...rest of main()...\n```\n\n**Important:** Register `SearchIndexGenerator` LAST so it indexes all other chapters.\n\n### Step 3: Test the Generator\n\n```bash\n# List all generators (verify yours appears)\npython scripts/generate_book.py --list\n\n# Test with dry-run\npython scripts/generate_book.py --chapter mychapter --dry-run --verbose\n\n# Generate for real\npython scripts/generate_book.py --chapter mychapter --verbose\n\n# Verify output\nls -la book/03-decisions/\n```\n\n### Step 4: Regenerate Search Index\n\nAfter adding new chapters:\n\n```bash\npython scripts/generate_book.py --chapter search\n```\n\n### Step 5: Test Full Build\n\n```bash\n# Full regeneration\npython scripts/generate_book.py --verbose\n\n# Verify all chapters\nfind book/ -name \"*.md\" | sort\n\n# Validate JSON outputs\npython -c \"import json; json.load(open('book/index.json'))\"\npython -c \"import json; json.load(open('book/search.json'))\"\n```\n\n---\n\n## Best Practices\n\n### 1. Output File Naming\n\n**Convention:**\n\n- Algorithm docs: `alg-<name>.md` (e.g., `alg-pagerank.md`)\n- Module docs: `mod-<category>.md` (e.g., `mod-processor.md`)\n- Narrative docs: `<topic>.md` (e.g., `timeline.md`, `features.md`)\n- Decision records: `adr-<num>-<slug>.md` (e.g., `adr-001-architecture.md`)\n\n**Rules:**\n\n- Use lowercase\n- Use hyphens, not underscores\n- Be descriptive but concise\n- Avoid special characters\n\n### 2. Frontmatter Requirements\n\nAll chapters must have YAML frontmatter:\n\n```yaml\n---\ntitle: \"Chapter Title\"\ngenerated: \"2025-12-16T10:30:00Z\"\ngenerator: \"mychapter\"\nsource_files:\n  - \"path/to/source.py\"\n  - \"path/to/data.json\"\ntags:\n  - tag1\n  - tag2\n  - tag3\n---\n```\n\n**Required Fields:**\n\n- `title`: Human-readable chapter title\n- `generated`: ISO 8601 timestamp (use `datetime.utcnow().isoformat() + \"Z\"`)\n- `generator`: Your generator's `name` property\n- `source_files`: List of source files used\n- `tags`: List of tags for categorization\n\n**Use the helper:**\n\n```python\nfrontmatter = self.generate_frontmatter(\n    title=\"My Chapter\",\n    tags=['algorithms', 'foundations'],\n    source_files=['docs/VISION.md', 'cortical/analysis.py']\n)\n```\n\n### 3. Cross-Reference Patterns\n\n**Link to other chapters:**\n\n```markdown\nSee [Algorithm Documentation](alg-pagerank.md) for details.\nSee [Architecture Overview](../02-architecture/index.md).\n```\n\n**Link to source code:**\n\n```markdown\n**Implementation:** `cortical/analysis.py:compute_pagerank()`\n```\n\n**Link to ADRs:**\n\n```markdown\n**Related Decision:** [ADR-001: Architecture](../../samples/decisions/adr-001-*.md)\n```\n\n**Link to git commits:**\n\n```markdown\n**Commit:** `a1b2c3d`\n```\n\n### 4. Error Handling\n\n**Always handle exceptions gracefully:**\n\n```python\ndef generate(self, dry_run: bool = False, verbose: bool = False) -> Dict[str, Any]:\n    errors = []\n    stats = {}\n\n    try:\n        data = self._load_data()\n    except FileNotFoundError as e:\n        errors.append(f\"Source file not found: {e}\")\n        return {\"files\": [], \"stats\": stats, \"errors\": errors}\n    except Exception as e:\n        errors.append(f\"Unexpected error loading data: {e}\")\n        return {\"files\": [], \"stats\": stats, \"errors\": errors}\n\n    # Continue processing...\n```\n\n**Error message guidelines:**\n\n- Be specific (include file names, line numbers)\n- Suggest fixes when possible\n- Use warnings for non-critical issues\n- Return partial results if some items succeed\n\n**Example:**\n\n```python\n# Good\nerrors.append(\"Failed to parse VISION.md line 42: Missing '###' prefix\")\n\n# Bad\nerrors.append(\"Error in file\")\n```\n\n### 5. Verbose Output\n\nProvide helpful progress indicators:\n\n```python\ndef generate(self, dry_run: bool = False, verbose: bool = False) -> Dict[str, Any]:\n    if verbose:\n        print(\"  Loading source data...\")\n\n    data = self._load_data()\n\n    if verbose:\n        print(f\"  Found {len(data)} items to process\")\n\n    for item in data:\n        if verbose:\n            print(f\"  Generating: {item['title']}\")\n\n        # Process item...\n```\n\n**Guidelines:**\n\n- Use 2-space indentation for generator output\n- Log counts and progress\n- Don't log every detail (avoid spam)\n- Use dry-run mode for testing without writes\n\n### 6. Dry-Run Support\n\nAlways respect the `dry_run` parameter:\n\n```python\n# Good: Use write_chapter helper (handles dry_run automatically)\nself.write_chapter(filename, content, dry_run=dry_run)\n\n# If you need custom file writing:\nif dry_run:\n    print(f\"  Would write: {output_path}\")\nelse:\n    output_path.write_text(content)\n    self.generated_files.append(output_path)\n```\n\n### 7. Statistics Reporting\n\nReturn meaningful statistics:\n\n```python\nstats = {\n    \"items_found\": len(all_items),\n    \"items_processed\": len(processed_items),\n    \"chapters_written\": len(self.generated_files),\n    \"items_skipped\": len(skipped_items),\n    \"warnings\": len(warnings)\n}\n```\n\n---\n\n## Testing Your Generator\n\n### Unit Testing\n\nCreate a test file `tests/test_book_generation.py`:\n\n```python\nimport unittest\nfrom pathlib import Path\nfrom scripts.generate_book import MyChapterGenerator\n\nclass TestMyChapterGenerator(unittest.TestCase):\n    def setUp(self):\n        self.generator = MyChapterGenerator()\n\n    def test_name(self):\n        self.assertEqual(self.generator.name, \"mychapter\")\n\n    def test_output_dir(self):\n        self.assertEqual(self.generator.output_dir, \"03-decisions\")\n\n    def test_dry_run(self):\n        \"\"\"Test that dry-run doesn't write files.\"\"\"\n        result = self.generator.generate(dry_run=True, verbose=False)\n        self.assertEqual(result['files'], [])\n        self.assertGreaterEqual(result['stats']['items_processed'], 0)\n\n    def test_generate(self):\n        \"\"\"Test actual generation.\"\"\"\n        result = self.generator.generate(dry_run=False, verbose=False)\n        self.assertGreater(len(result['files']), 0)\n        self.assertEqual(result['errors'], [])\n```\n\n### Integration Testing\n\n```bash\n# 1. Full dry-run test\npython scripts/generate_book.py --dry-run --verbose\n\n# 2. Generate to temporary directory\npython scripts/generate_book.py --output /tmp/test-book --chapter mychapter\n\n# 3. Verify outputs\nls -la /tmp/test-book/03-decisions/\ncat /tmp/test-book/03-decisions/example.md\n\n# 4. Validate frontmatter\npython -c \"\nimport yaml\nfrom pathlib import Path\nfor f in Path('/tmp/test-book').glob('**/*.md'):\n    content = f.read_text()\n    if content.startswith('---'):\n        fm = content.split('---', 2)[1]\n        yaml.safe_load(fm)\n        print(f'{f.name}: OK')\n\"\n\n# 5. Clean up\nrm -rf /tmp/test-book\n```\n\n### Common Test Cases\n\n1. **Empty data source** - Should return gracefully\n2. **Malformed data** - Should log errors, continue processing\n3. **Missing dependencies** - Should fail gracefully with clear error\n4. **Dry-run mode** - Should not write any files\n5. **Verbose mode** - Should print progress\n6. **Frontmatter validation** - YAML should parse correctly\n7. **Cross-references** - Links should be valid\n\n---\n\n## CI Integration\n\n### GitHub Actions Workflow\n\nAdd to `.github/workflows/book-generation.yml`:\n\n```yaml\nname: Generate Book\n\non:\n  push:\n    branches: [main]\n    paths:\n      - 'docs/VISION.md'\n      - 'cortical/**/*.py'\n      - 'scripts/generate_book.py'\n  workflow_dispatch:\n\njobs:\n  generate:\n    runs-on: ubuntu-latest\n\n    steps:\n      - uses: actions/checkout@v3\n        with:\n          fetch-depth: 0  # Full history for commit narratives\n\n      - uses: actions/setup-python@v4\n        with:\n          python-version: '3.10'\n\n      - name: Install dependencies\n        run: |\n          pip install -e \".[dev]\"\n\n      - name: Generate book\n        run: |\n          python scripts/generate_book.py --verbose\n\n      - name: Validate outputs\n        run: |\n          python -c \"import json; json.load(open('book/index.json'))\"\n          python -c \"import json; json.load(open('book/search.json'))\"\n\n      - name: Upload artifact\n        uses: actions/upload-artifact@v3\n        with:\n          name: cortical-chronicles\n          path: book/\n```\n\n### Pre-Commit Hook\n\nAdd to `.git/hooks/pre-commit`:\n\n```bash\n#!/bin/bash\n# Regenerate book if source files changed\n\nSOURCES=\"docs/VISION.md cortical/ scripts/generate_book.py\"\nCHANGED=$(git diff --cached --name-only $SOURCES)\n\nif [ -n \"$CHANGED\" ]; then\n    echo \"\ud83d\udcda Regenerating book chapters...\"\n    python scripts/generate_book.py --verbose || exit 1\n\n    # Stage generated files\n    git add book/\nfi\n```\n\n---\n\n## Examples\n\n### Example 1: Simple Data-Driven Generator\n\nGenerate chapters from JSON files:\n\n```python\nclass DataChapterGenerator(ChapterGenerator):\n    \"\"\"Generate chapters from data/*.json files.\"\"\"\n\n    @property\n    def name(self) -> str:\n        return \"data\"\n\n    @property\n    def output_dir(self) -> str:\n        return \"05-future\"\n\n    def generate(self, dry_run: bool = False, verbose: bool = False) -> Dict[str, Any]:\n        import json\n\n        data_dir = Path(__file__).parent.parent / \"data\"\n        json_files = list(data_dir.glob(\"*.json\"))\n\n        if verbose:\n            print(f\"  Found {len(json_files)} JSON files\")\n\n        for json_file in json_files:\n            data = json.loads(json_file.read_text())\n\n            content = self.generate_frontmatter(\n                title=data['title'],\n                tags=data.get('tags', []),\n                source_files=[str(json_file)]\n            )\n            content += f\"# {data['title']}\\n\\n\"\n            content += data['body'] + \"\\n\"\n\n            filename = json_file.stem + \".md\"\n            self.write_chapter(filename, content, dry_run)\n\n        return {\n            \"files\": [str(f) for f in self.generated_files],\n            \"stats\": {\"files_processed\": len(json_files)},\n            \"errors\": []\n        }\n```\n\n### Example 2: Git History Generator\n\nGenerate from commit messages:\n\n```python\nclass GitHistoryGenerator(ChapterGenerator):\n    \"\"\"Generate timeline from git history.\"\"\"\n\n    @property\n    def name(self) -> str:\n        return \"timeline\"\n\n    @property\n    def output_dir(self) -> str:\n        return \"04-evolution\"\n\n    def _run_git(self, *args) -> str:\n        import subprocess\n        result = subprocess.run(\n            [\"git\", *args],\n            capture_output=True,\n            text=True,\n            check=True\n        )\n        return result.stdout.strip()\n\n    def generate(self, dry_run: bool = False, verbose: bool = False) -> Dict[str, Any]:\n        try:\n            log_output = self._run_git(\"log\", \"--format=%H|%aI|%s|%an\", \"-100\")\n        except Exception as e:\n            return {\n                \"files\": [],\n                \"stats\": {},\n                \"errors\": [f\"Git error: {e}\"]\n            }\n\n        commits = []\n        for line in log_output.split('\\n'):\n            if not line.strip():\n                continue\n            hash_val, timestamp, message, author = line.split('|', 3)\n            commits.append({\n                'hash': hash_val[:7],\n                'date': timestamp[:10],\n                'message': message,\n                'author': author\n            })\n\n        # Generate timeline content\n        content = self.generate_frontmatter(\n            title=\"Project Timeline\",\n            tags=['timeline', 'history'],\n            source_files=['git log']\n        )\n        content += \"# Project Timeline\\n\\n\"\n\n        for commit in commits:\n            content += f\"- **{commit['date']}** (`{commit['hash']}`): {commit['message']}\\n\"\n\n        self.write_chapter(\"timeline.md\", content, dry_run)\n\n        return {\n            \"files\": [str(f) for f in self.generated_files],\n            \"stats\": {\"commits_processed\": len(commits)},\n            \"errors\": []\n        }\n```\n\n### Example 3: Multi-File Generator\n\nGenerate multiple chapters from one data source:\n\n```python\nclass MultiChapterGenerator(ChapterGenerator):\n    \"\"\"Generate multiple chapters from single source.\"\"\"\n\n    @property\n    def name(self) -> str:\n        return \"concepts\"\n\n    @property\n    def output_dir(self) -> str:\n        return \"02-architecture\"\n\n    def generate(self, dry_run: bool = False, verbose: bool = False) -> Dict[str, Any]:\n        # Load Louvain clusters\n        clusters = self._load_clusters()\n\n        if verbose:\n            print(f\"  Found {len(clusters)} concept clusters\")\n\n        # Generate one chapter per cluster\n        for cluster_id, terms in clusters.items():\n            content = self.generate_frontmatter(\n                title=f\"Concept Cluster {cluster_id}\",\n                tags=['concepts', 'clustering', 'louvain'],\n                source_files=['corpus_dev.pkl']\n            )\n\n            content += f\"# Concept Cluster {cluster_id}\\n\\n\"\n            content += f\"**Terms:** {', '.join(terms[:20])}\\n\\n\"\n\n            filename = f\"concept-{cluster_id}.md\"\n            self.write_chapter(filename, content, dry_run)\n\n        # Generate index\n        index_content = self._generate_index(clusters)\n        self.write_chapter(\"concepts-index.md\", index_content, dry_run)\n\n        return {\n            \"files\": [str(f) for f in self.generated_files],\n            \"stats\": {\n                \"clusters_found\": len(clusters),\n                \"chapters_written\": len(clusters) + 1\n            },\n            \"errors\": []\n        }\n\n    def _load_clusters(self) -> Dict[int, List[str]]:\n        # Load from processor\n        pass\n\n    def _generate_index(self, clusters: Dict) -> str:\n        # Generate index page\n        pass\n```\n\n---\n\n## Common Pitfalls\n\n### \u274c Don't: Hardcode Paths\n\n```python\n# Bad\noutput_path = Path(\"/home/user/book/01-foundations/chapter.md\")\n\n# Good\noutput_path = self.book_dir / self.output_dir / \"chapter.md\"\n```\n\n### \u274c Don't: Ignore Dry-Run\n\n```python\n# Bad\nwith open(output_path, 'w') as f:\n    f.write(content)\n\n# Good\nself.write_chapter(filename, content, dry_run=dry_run)\n```\n\n### \u274c Don't: Swallow Errors\n\n```python\n# Bad\ntry:\n    data = self._load_data()\nexcept:\n    pass  # Silent failure!\n\n# Good\ntry:\n    data = self._load_data()\nexcept Exception as e:\n    errors.append(f\"Failed to load data: {e}\")\n    return {\"files\": [], \"stats\": {}, \"errors\": errors}\n```\n\n### \u274c Don't: Generate Unsafe Filenames\n\n```python\n# Bad\nfilename = f\"{user_input}.md\"  # Could be \"../../../etc/passwd.md\"\n\n# Good\nfilename = self._sanitize_filename(user_input) + \".md\"\n\ndef _sanitize_filename(self, text: str) -> str:\n    # Remove path separators\n    text = text.replace('/', '-').replace('\\\\', '-')\n    # Remove special chars\n    text = re.sub(r'[^\\w\\s-]', '', text)\n    # Normalize whitespace\n    text = re.sub(r'[-\\s]+', '-', text)\n    return text.lower().strip('-')\n```\n\n---\n\n## Next Steps\n\n1. **Study existing generators** in `scripts/generate_book.py`\n2. **Create your generator** following the template\n3. **Test with dry-run** before writing files\n4. **Validate frontmatter** with YAML parser\n5. **Regenerate search index** after adding chapters\n6. **Document your generator** in this guide\n\n---\n\n## Questions?\n\n- Check [TROUBLESHOOTING.md](TROUBLESHOOTING.md) for common issues\n- Review existing generators for patterns\n- Test with `--dry-run --verbose` for debugging\n\n---\n\n*This guide is part of [The Cortical Chronicles](../README.md) documentation.*\n",
      "keywords": [
        "self",
        "python",
        "content",
        "generate",
        "data",
        "str",
        "def",
        "generator",
        "errors",
        "return"
      ]
    },
    {
      "id": "docs/TROUBLESHOOTING",
      "title": "TROUBLESHOOTING",
      "content": "# Troubleshooting Guide\n\n> **Quick Recovery**: For most errors, run `python scripts/generate_book.py --dry-run --verbose` to diagnose without writing files.\n\nThis guide helps diagnose and fix common issues when generating \"The Cortical Chronicles.\"\n\n---\n\n## Table of Contents\n\n- [Common Errors](#common-errors)\n- [Diagnostic Commands](#diagnostic-commands)\n- [Recovery Procedures](#recovery-procedures)\n- [Error Reference](#error-reference)\n- [Getting Help](#getting-help)\n\n---\n\n## Common Errors\n\n### 1. YAML Parsing Errors in .ai_meta Files\n\n**Symptoms:**\n```\nWarning: Failed to parse analysis.py.ai_meta: ...\nYAML parsing error\n```\n\n**Causes:**\n- Malformed YAML frontmatter\n- Unescaped special characters in docstrings\n- Missing comment header stripping\n- Mixed tabs and spaces\n\n**Solutions:**\n\n```bash\n# Check if .ai_meta files exist\nls -la cortical/*.ai_meta\n\n# Regenerate metadata files\npython scripts/generate_ai_metadata.py --force\n\n# Test parsing a specific file\npython -c \"import yaml; yaml.safe_load(open('cortical/analysis.py.ai_meta').read())\"\n```\n\n**Prevention:**\n- Run `generate_ai_metadata.py` after major docstring changes\n- Avoid special YAML characters (`:`, `{`, `}`, `[`, `]`) in docstrings without quoting\n\n### 2. Missing VISION.md Sections\n\n**Symptoms:**\n```\nalgorithms_found: 0\nNo algorithm sections extracted\n```\n\n**Causes:**\n- VISION.md missing \"Deep Algorithm Analysis\" section\n- Section header format changed\n- Regex pattern mismatch\n\n**Solutions:**\n\n```bash\n# Verify VISION.md structure\ngrep -n \"## Deep Algorithm Analysis\" docs/VISION.md\n\n# Check for algorithm sections\ngrep -n \"### Algorithm\" docs/VISION.md\n\n# Regenerate with verbose logging\npython scripts/generate_book.py --chapter foundations --verbose\n```\n\n**Expected Structure:**\n```markdown\n## Deep Algorithm Analysis\n\n### Algorithm 1: PageRank \u2014 Importance Discovery\n\n**Implementation:** `cortical/analysis.py:compute_pagerank()`\n...\n\n### Algorithm 2: BM25/TF-IDF \u2014 Distinctiveness Scoring\n...\n```\n\n### 3. Git History Access Issues\n\n**Symptoms:**\n```\nWarning: Failed to read git history: ...\nNo git history found\nCould not read git history\n```\n\n**Causes:**\n- Not in a git repository\n- Insufficient permissions\n- Git not installed\n- Detached HEAD state\n\n**Solutions:**\n\n```bash\n# Check git availability\nwhich git\ngit --version\n\n# Verify repository\ngit status\n\n# Test git log access\ngit log -5 --format=\"%H|%aI|%s|%an\"\n\n# Check permissions\nls -la .git/\n```\n\n**Workarounds:**\n- Skip evolution chapters: `python scripts/generate_book.py --chapter foundations`\n- Initialize git if missing: `git init && git add . && git commit -m \"Initial commit\"`\n\n### 4. Missing Dependencies\n\n**Symptoms:**\n```\nModuleNotFoundError: No module named 'yaml'\nImportError: cannot import name 'yaml'\n```\n\n**Solution:**\n\n```bash\n# Install required dependencies\npip install pyyaml\n\n# Or install all dev dependencies\npip install -e \".[dev]\"\n\n# Verify installation\npython -c \"import yaml; print('PyYAML OK')\"\n```\n\n### 5. Permission Errors Writing to book/\n\n**Symptoms:**\n```\nPermissionError: [Errno 13] Permission denied: 'book/01-foundations/...'\nOSError: [Errno 30] Read-only file system\n```\n\n**Solutions:**\n\n```bash\n# Check directory permissions\nls -ld book/\nls -la book/01-foundations/\n\n# Fix permissions\nchmod -R u+w book/\n\n# Try dry-run first\npython scripts/generate_book.py --dry-run\n\n# Check disk space\ndf -h .\n```\n\n### 6. Empty or Missing ML Data\n\n**Symptoms:**\n```\nwith_ml_data: 0\nWarning: Failed to load ML data\n```\n\n**Causes:**\n- `.git-ml/tracked/commits.jsonl` doesn't exist\n- ML collection not started\n- File is empty\n\n**Solutions:**\n\n```bash\n# Check ML data file\nls -lh .git-ml/tracked/commits.jsonl\n\n# Backfill ML data\npython scripts/ml_data_collector.py backfill -n 100\n\n# Verify data\npython scripts/ml_data_collector.py stats\n```\n\n**Note:** ML data is optional. Chapters will generate without it (just with fewer details).\n\n### 7. Search Index Generation Failures\n\n**Symptoms:**\n```\nFailed to parse index.md: ...\nJSONDecodeError\n```\n\n**Causes:**\n- Malformed frontmatter in generated chapters\n- Invalid JSON structure\n- Missing chapter files\n\n**Solutions:**\n\n```bash\n# Regenerate chapters first\npython scripts/generate_book.py --chapter foundations\npython scripts/generate_book.py --chapter architecture\n\n# Then regenerate search index\npython scripts/generate_book.py --chapter search\n\n# Validate generated JSON\npython -c \"import json; json.load(open('book/index.json'))\"\npython -c \"import json; json.load(open('book/search.json'))\"\n```\n\n---\n\n## Diagnostic Commands\n\n### Health Check\n\n```bash\n# Full diagnostic run (no writes)\npython scripts/generate_book.py --dry-run --verbose\n```\n\nExpected output:\n```\nRegistered generator: foundations\nRegistered generator: architecture\n...\nGenerating: foundations\n  Found 6 algorithms in VISION.md\n  Generating: alg-pagerank.md\n...\nTotal files: 25\n```\n\n### Check Individual Generators\n\n```bash\n# List all generators\npython scripts/generate_book.py --list\n\n# Test specific generator\npython scripts/generate_book.py --chapter foundations --dry-run --verbose\npython scripts/generate_book.py --chapter architecture --dry-run --verbose\npython scripts/generate_book.py --chapter evolution --dry-run --verbose\n```\n\n### Verify Dependencies\n\n```bash\n# Check Python version (3.8+ required)\npython --version\n\n# Check required packages\npython -c \"import yaml; print('PyYAML:', yaml.__version__)\"\npython -c \"import json; print('json: OK')\"\npython -c \"import re; print('re: OK')\"\n\n# Check git\ngit --version\ngit status\n```\n\n### Verify Source Files\n\n```bash\n# Check VISION.md\ntest -f docs/VISION.md && echo \"VISION.md exists\" || echo \"VISION.md MISSING\"\ngrep -c \"### Algorithm\" docs/VISION.md\n\n# Check .ai_meta files\nfind cortical -name \"*.ai_meta\" | wc -l\nls -lh cortical/*.ai_meta | head -5\n\n# Check git history\ngit log -5 --oneline\n```\n\n### Verify Output Structure\n\n```bash\n# Check generated files\nfind book/ -name \"*.md\" | sort\nfind book/ -name \"*.json\"\n\n# Check chapter completeness\nfor dir in book/0*-*/; do\n  echo \"$dir: $(ls \"$dir\" | wc -l) files\"\ndone\n\n# Validate JSON outputs\npython -c \"import json; json.load(open('book/index.json')); print('index.json: OK')\"\npython -c \"import json; json.load(open('book/search.json')); print('search.json: OK')\"\n```\n\n---\n\n## Recovery Procedures\n\n### Complete Rebuild\n\nWhen all else fails, regenerate from scratch:\n\n```bash\n# 1. Backup existing book (if needed)\ncp -r book/ book.backup/\n\n# 2. Clear generated files (keep docs and assets)\nrm -rf book/0*-*/\nrm -f book/index.json book/search.json\n\n# 3. Regenerate metadata (if needed)\npython scripts/generate_ai_metadata.py --force\n\n# 4. Full regeneration\npython scripts/generate_book.py --verbose\n\n# 5. Verify\nls -lR book/\npython -c \"import json; json.load(open('book/index.json'))\"\n```\n\n### Regenerate Single Chapter\n\nIf one chapter is corrupted:\n\n```bash\n# 1. Remove the chapter\nrm -rf book/01-foundations/\n\n# 2. Regenerate just that chapter\npython scripts/generate_book.py --chapter foundations --verbose\n\n# 3. Rebuild search index\npython scripts/generate_book.py --chapter search\n\n# 4. Verify\nls -la book/01-foundations/\n```\n\n### Fix Malformed Frontmatter\n\nIf chapters have malformed YAML frontmatter:\n\n```bash\n# 1. Identify the problem file\npython -c \"\nimport yaml\nfrom pathlib import Path\nfor f in Path('book').glob('**/*.md'):\n    try:\n        content = f.read_text()\n        if content.startswith('---'):\n            fm = content.split('---', 2)[1]\n            yaml.safe_load(fm)\n    except Exception as e:\n        print(f'ERROR: {f}: {e}')\n\"\n\n# 2. Remove the problematic chapter\nrm book/XX-section/problematic.md\n\n# 3. Regenerate the parent chapter\npython scripts/generate_book.py --chapter <generator-name>\n```\n\n### Restore from Git\n\nIf the book is tracked in git:\n\n```bash\n# Check what changed\ngit status book/\ngit diff book/\n\n# Restore specific file\ngit restore book/01-foundations/alg-pagerank.md\n\n# Restore entire book\ngit restore book/\n\n# Or reset to last good state\ngit log --oneline -- book/\ngit restore --source=<commit-hash> book/\n```\n\n### Partial Failure Recovery\n\nIf some generators fail but others succeed:\n\n```bash\n# 1. Check which generators failed\npython scripts/generate_book.py --verbose 2>&1 | grep -A 3 \"ERROR:\"\n\n# 2. Regenerate only failed chapters\npython scripts/generate_book.py --chapter <failed-generator> --verbose\n\n# 3. Rebuild search index\npython scripts/generate_book.py --chapter search\n```\n\n---\n\n## Error Reference\n\n### Generator-Specific Errors\n\n#### AlgorithmChapterGenerator\n\n| Error | Cause | Fix |\n|-------|-------|-----|\n| `algorithms_found: 0` | VISION.md missing section | Check docs/VISION.md structure |\n| `Source file not found` | VISION.md doesn't exist | Create docs/VISION.md |\n| `chapters_written: 0` | Regex pattern mismatch | Update `_extract_algorithms()` |\n\n#### ModuleDocGenerator\n\n| Error | Cause | Fix |\n|-------|-------|-----|\n| `No .ai_meta files found` | Metadata not generated | Run `generate_ai_metadata.py` |\n| `Failed to parse <file>.ai_meta` | Malformed YAML | Regenerate metadata with `--force` |\n| `modules_documented: 0` | All parsing failed | Check YAML structure |\n\n#### CommitNarrativeGenerator\n\n| Error | Cause | Fix |\n|-------|-------|-----|\n| `No git history found` | Not in git repo | Initialize git or skip chapter |\n| `Failed to read git history` | Git not available | Install git |\n| `with_ml_data: 0` | ML data missing | Run backfill (optional) |\n\n#### SearchIndexGenerator\n\n| Error | Cause | Fix |\n|-------|-------|-----|\n| `chapters_indexed: 0` | No chapter files | Generate chapters first |\n| `Failed to parse <file>` | Malformed frontmatter | Regenerate source chapter |\n| `JSONDecodeError` | Invalid JSON structure | Check chapter YAML |\n\n### System-Level Errors\n\n| Error | Typical Cause | Solution |\n|-------|--------------|----------|\n| `PermissionError` | Read-only filesystem | Check permissions with `ls -ld book/` |\n| `FileNotFoundError` | Missing source file | Verify file exists with `ls -la` |\n| `ModuleNotFoundError: yaml` | Missing dependency | Install with `pip install pyyaml` |\n| `JSONDecodeError` | Corrupted output | Delete and regenerate file |\n| `UnicodeDecodeError` | Binary file read as text | Check file encoding |\n| `subprocess.CalledProcessError` | Git command failed | Verify git with `git status` |\n\n---\n\n## Prevention Tips\n\n### Before Generating\n\n1. **Verify prerequisites:**\n   ```bash\n   python --version  # 3.8+\n   git --version\n   python -c \"import yaml; print('OK')\"\n   ```\n\n2. **Check source files:**\n   ```bash\n   test -f docs/VISION.md || echo \"WARNING: VISION.md missing\"\n   ls cortical/*.ai_meta | wc -l  # Should be >10\n   ```\n\n3. **Test with dry-run:**\n   ```bash\n   python scripts/generate_book.py --dry-run\n   ```\n\n### After Making Changes\n\n1. **Regenerate affected chapters:**\n   - Changed VISION.md \u2192 `--chapter foundations`\n   - Changed docstrings \u2192 regenerate metadata, then `--chapter architecture`\n   - New commits \u2192 `--chapter evolution`\n\n2. **Always rebuild search index:**\n   ```bash\n   python scripts/generate_book.py --chapter search\n   ```\n\n3. **Validate outputs:**\n   ```bash\n   python -c \"import json; json.load(open('book/index.json'))\"\n   ```\n\n---\n\n## Getting Help\n\n### Debug Checklist\n\n- [ ] Run with `--dry-run --verbose`\n- [ ] Check error message in this guide\n- [ ] Verify dependencies installed\n- [ ] Test with single chapter generation\n- [ ] Check source file structure\n- [ ] Review recent git history\n- [ ] Try complete rebuild\n\n### Logging\n\nGenerate detailed logs for debugging:\n\n```bash\n# Full verbose output to file\npython scripts/generate_book.py --verbose 2>&1 | tee generation.log\n\n# Check for errors\ngrep -i \"error\\|warning\\|failed\" generation.log\n\n# Check generator stats\ngrep \"stats\" generation.log\n```\n\n### Still Stuck?\n\n1. **Check recent changes:**\n   ```bash\n   git log --oneline -10\n   git diff HEAD~5 -- docs/ cortical/\n   ```\n\n2. **Isolate the problem:**\n   - Test each generator individually\n   - Compare with known-good state\n   - Check file permissions\n\n3. **Report issue with:**\n   - Full error message\n   - Output of `--dry-run --verbose`\n   - Output of diagnostic commands\n   - Recent changes to source files\n\n---\n\n## Appendix: File Locations\n\n### Source Files\n\n| File | Purpose | Generator |\n|------|---------|-----------|\n| `docs/VISION.md` | Algorithm descriptions | foundations |\n| `cortical/*.ai_meta` | Module metadata | architecture |\n| `.git/logs/` | Git history | evolution |\n| `.git-ml/tracked/commits.jsonl` | ML commit data | evolution |\n| `samples/decisions/adr-*.md` | ADRs | decisions |\n\n### Output Files\n\n| File | Generator | Can Delete? |\n|------|-----------|-------------|\n| `book/*/index.md` | Various | Yes (regenerates) |\n| `book/01-foundations/*.md` | foundations | Yes |\n| `book/02-architecture/*.md` | architecture | Yes |\n| `book/04-evolution/*.md` | evolution | Yes |\n| `book/index.json` | search | Yes |\n| `book/search.json` | search | Yes |\n| `book/README.md` | Manual | **No** (manual) |\n| `book/docs/` | Manual | **No** (manual) |\n| `book/assets/` | Manual | **No** (manual) |\n\n---\n\n*This troubleshooting guide is part of [The Cortical Chronicles](../README.md) documentation.*\n",
      "keywords": [
        "git",
        "book",
        "python",
        "check",
        "json",
        "chapter",
        "scripts",
        "bash",
        "generate_book",
        "file"
      ]
    }
  ]
}