---
title: "BM25/TF-IDF — Distinctiveness Scoring"
generated: "2025-12-16T20:01:28.082458Z"
generator: "foundations"
source_files:
  - "docs/VISION.md"
  - "cortical/analysis/tfidf.py"
tags:
  - algorithms
  - foundations
  - ir-theory
---

# BM25/TF-IDF — Distinctiveness Scoring

**Purpose:** Score how well a term distinguishes a specific document from the rest of the corpus.

**Implementation:** `cortical/analysis/tfidf.py`

**BM25 Formula (Default):**
```
BM25(t, d) = IDF(t) × (tf(t,d) × (k1 + 1)) / (tf(t,d) + k1 × (1 - b + b × |d|/avgdl))
```

Where:
- `IDF(t) = log((N - df(t) + 0.5) / (df(t) + 0.5) + 1)` — Inverse document frequency with smoothing
- `tf(t,d)` — Term frequency in document d
- `k1 = 1.2` — Term frequency saturation (diminishing returns after ~12 occurrences)
- `b = 0.75` — Length normalization factor

**Why BM25 Over TF-IDF:**
- Non-negative IDF even for terms appearing in most documents
- Length normalization prevents long files from unfairly dominating
- Term frequency saturation models realistic relevance (saying "API" 100 times doesn't make a doc 100× more relevant than saying it once)

**Dual Storage Strategy:**
- **Global TF-IDF** (`col.tfidf`): Term importance to entire corpus
- **Per-Document TF-IDF** (`col.tfidf_per_doc[doc_id]`): Term importance within specific document

This dual approach allows:
- Fast corpus-wide importance filtering
- Accurate per-document relevance scoring for search

---

*This chapter is part of [The Cortical Chronicles](../README.md), a self-documenting book generated by the Cortical Text Processor.*
