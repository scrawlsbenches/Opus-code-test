---
title: "PageRank — Importance Discovery"
generated: "2025-12-17T00:01:46.426419Z"
generator: "foundations"
source_files:
  - "docs/VISION.md"
  - "cortical/analysis/pagerank.py"
tags:
  - algorithms
  - foundations
  - ir-theory
---

# PageRank — Importance Discovery

**Purpose:** Identify which terms matter most in the corpus, independent of raw frequency.

**Implementation:** `cortical/analysis/pagerank.py`

**How It Works:**
```
importance[term] = (1 - damping) / N + damping × Σ (neighbor_importance × edge_weight / neighbor_outgoing_sum)
```

The algorithm iteratively propagates importance through the term co-occurrence graph. Terms that are referenced by many important terms become important themselves—a recursive definition that converges to stable values.

**Key Parameters:**
- `damping = 0.85`: The probability of following a link vs. jumping to a random node
- `tolerance = 1e-6`: Convergence threshold (stops when no term changes by more than this)
- `max_iterations = 20`: Upper bound on iterations

**Three Variants:**
1. **Standard PageRank**: Applied to Layer 0 (tokens) and Layer 1 (bigrams)
2. **Semantic PageRank**: Adjusts edge weights by relation type (IsA connections count 1.5× more than CoOccurs)
3. **Hierarchical PageRank**: Propagates importance across all 4 layers with separate cross-layer damping

**Why This Matters for Code Search:**
- Common utility functions referenced everywhere get high PageRank
- Core abstractions that everything depends on surface naturally
- Prevents over-emphasis on boilerplate code that appears frequently but isn't semantically central

**Performance:** O(iterations × edges), typically 100-500ms for 10K tokens with early convergence usually at 5-10 iterations.

---

*This chapter is part of [The Cortical Chronicles](../README.md), a self-documenting book generated by the Cortical Text Processor.*
