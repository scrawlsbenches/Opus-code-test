name: CI - Test Suite

# =============================================================================
# CI TEST ARCHITECTURE - READ BEFORE MODIFYING
# =============================================================================
#
# CRITICAL: Pytest runs unittest-based tests natively!
# DO NOT run both pytest and unittest on the same test files.
# This doubles CI time and provides no benefit.
#
# ‚ùå WRONG (runs tests twice):
#    coverage run -m pytest tests/
#    coverage run --append -m unittest discover -s tests
#
# ‚úÖ CORRECT (pytest handles both):
#    coverage run -m pytest tests/
#
# PARALLEL ARCHITECTURE (optimized for speed):
#
#   validate-tasks       smoke-tests      showcase
#        (< 5s)           (< 30s)         (parallel)
#          ‚îÇ                 ‚îÇ                ‚îÇ
#          ‚îÇ                 ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
#          ‚îÇ                 ‚îÇ                ‚îÇ                 ‚îÇ
#          ‚ñº                 ‚ñº                ‚ñº                 ‚ñº
#    (continues)  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
#                 ‚îÇ unit-tests ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îÇ      ‚îÇ regression-tests ‚îÇ
#                 ‚îÇ integration    ‚îÇ (cov) ‚îÇ      ‚îÇ behavioral-tests ‚îÇ
#                 ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îÇ performance-tests‚îÇ
#                         ‚îÇ                       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
#                         ‚ñº                           (no coverage)
#                  coverage-report
#                (combines coverage
#                 from unit + integration
#                 WITHOUT re-running tests)
#
# Each stage runs specific test files to avoid duplication.
# coverage-report ONLY combines coverage data - it does NOT run tests again.
# =============================================================================

on:
  push:
    branches:
      - '**'  # Run on all branch pushes (explicit)
  pull_request:
    branches: [ main ]
  workflow_dispatch:  # Allow manual triggering

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  # ==========================================================================
  # Stage 0: Task Validation (< 5s)
  # Quick check for valid task files in tasks/ directory - runs in parallel with smoke
  # ==========================================================================
  validate-tasks:
    name: "üìã Validate Tasks"
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4

    - name: Set up Python 3.11
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'

    - name: Validate tasks/*.json files
      run: |
        echo "=== Validating Task Files ==="
        python scripts/validate_tasks.py
        echo "‚úÖ Task validation passed"

    - name: Report Pending Tasks
      if: always()
      run: |
        echo "=== Pending Tasks Report ==="
        python scripts/ci_task_report.py --github
      env:
        GITHUB_STEP_SUMMARY: ${{ github.step_summary }}

  # ==========================================================================
  # Stage 1: Smoke Tests (< 30s)
  # Quick sanity check - if this fails, something is fundamentally broken
  # ==========================================================================
  smoke-tests:
    name: "üí® Smoke Tests"
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4

    - name: Set up Python 3.11
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'

    - name: Install test dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev]"

    - name: Run smoke tests
      run: |
        echo "=== Running Smoke Tests ==="
        python -m pytest tests/smoke/ -v --tb=short
        echo "‚úÖ Smoke tests passed"

  # ==========================================================================
  # Stage 2: Unit Tests with Coverage (< 2 min)
  # Fast, isolated tests for individual components
  # Runs in PARALLEL with integration-tests after smoke-tests pass
  # ==========================================================================
  unit-tests:
    name: "üß™ Unit Tests"
    runs-on: ubuntu-latest
    needs: smoke-tests
    steps:
    - uses: actions/checkout@v4

    - name: Set up Python 3.11
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'

    - name: Install test dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev]"

    - name: Run unit tests with coverage
      run: |
        echo "=== Running Unit Tests ==="
        # Run unit tests - pytest handles both pytest AND unittest style tests
        # Using a single pytest call is faster than multiple unittest discovers
        # Note: Legacy tests removed 2025-12-13 - now covered by tests/unit/
        coverage run --source=cortical -m pytest \
          tests/unit/ \
          -v --tb=short

        coverage report --include="cortical/*"

    - name: Upload unit test coverage data
      uses: actions/upload-artifact@v4
      with:
        name: coverage-unit
        path: .coverage
        # Rename to avoid collision when combining
        include-hidden-files: true

  # ==========================================================================
  # Stage 3: Integration Tests (< 3 min)
  # Tests for component interactions
  # Runs in PARALLEL with unit-tests after smoke-tests pass
  # ==========================================================================
  integration-tests:
    name: "üîó Integration Tests"
    runs-on: ubuntu-latest
    needs: smoke-tests  # Changed: runs parallel with unit-tests
    steps:
    - uses: actions/checkout@v4

    - name: Set up Python 3.11
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'

    - name: Install test dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev]"

    - name: Run integration tests with coverage
      run: |
        echo "=== Running Integration Tests ==="
        # Run integration tests - single pytest call is faster than multiple unittest discovers
        # Includes tests/integration/ plus remaining legacy tests that provide unique coverage
        # Note: 3 legacy tests removed 2025-12-13 (test_behavioral.py, test_intent_query.py,
        #       test_query_optimization.py) - now covered by unit tests
        coverage run --source=cortical -m pytest \
          tests/integration/ \
          tests/test_incremental_indexing.py \
          tests/test_edge_cases.py \
          tests/test_coverage_gaps.py \
          tests/test_analyze_louvain_resolution.py \
          tests/test_evaluate_cluster.py \
          tests/test_cli_wrapper.py \
          tests/test_search_codebase.py \
          tests/test_ask_codebase.py \
          tests/test_generate_ai_metadata.py \
          tests/test_showcase.py \
          tests/test_mcp_server.py \
          -v --tb=short

        coverage report --include="cortical/*"

    - name: Upload integration test coverage data
      uses: actions/upload-artifact@v4
      with:
        name: coverage-integration
        path: .coverage
        include-hidden-files: true

  # ==========================================================================
  # Stage 4: Regression Tests (< 1 min)
  # Tests for specific bugs that were fixed
  # Runs in PARALLEL with unit/integration after smoke-tests pass
  # ==========================================================================
  regression-tests:
    name: "üîí Regression Tests"
    runs-on: ubuntu-latest
    needs: smoke-tests  # Changed: runs parallel, only needs smoke
    steps:
    - uses: actions/checkout@v4

    - name: Set up Python 3.11
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'

    - name: Install test dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev]"

    - name: Run regression tests
      run: |
        echo "=== Running Regression Tests ==="
        python -m pytest tests/regression/ -v --tb=short
        echo "‚úÖ All regressions still fixed"

  # ==========================================================================
  # Stage 5: Behavioral Tests (< 2 min)
  # Tests for user-facing quality and relevance
  # Runs in PARALLEL with unit/integration after smoke-tests pass
  # ==========================================================================
  behavioral-tests:
    name: "üéØ Behavioral Tests"
    runs-on: ubuntu-latest
    needs: smoke-tests  # Changed: runs parallel, only needs smoke
    steps:
    - uses: actions/checkout@v4

    - name: Set up Python 3.11
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'

    - name: Install test dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev]"

    - name: Run behavioral tests
      run: |
        echo "=== Running Behavioral Tests ==="
        # Note: tests/test_behavioral.py removed 2025-12-13 - superseded by tests/behavioral/
        python -m pytest tests/behavioral/ -v --tb=short
        echo "‚úÖ Behavioral quality verified"

  # ==========================================================================
  # Stage 6: Performance Tests (< 1 min, no coverage)
  # Timing-based tests to catch performance regressions
  # Runs in PARALLEL with unit/integration after smoke-tests pass
  # ==========================================================================
  performance-tests:
    name: "‚è±Ô∏è Performance Tests"
    runs-on: ubuntu-latest
    needs: smoke-tests  # Changed: runs parallel, only needs smoke
    steps:
    - uses: actions/checkout@v4

    - name: Set up Python 3.11
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'

    - name: Install test dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev]"

    - name: Run performance tests
      run: |
        echo "=== Running Performance Tests (no coverage) ==="
        python -m pytest tests/performance/ -v --tb=short -s
        echo "‚úÖ Performance within thresholds"

  # ==========================================================================
  # Stage 7: Full Coverage Report
  # Combines coverage from unit + integration tests WITHOUT re-running tests
  # ==========================================================================
  coverage-report:
    name: "üìä Coverage Report"
    runs-on: ubuntu-latest
    needs: [unit-tests, integration-tests]
    steps:
    - uses: actions/checkout@v4

    - name: Set up Python 3.11
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'

    - name: Install coverage
      run: |
        python -m pip install --upgrade pip
        pip install coverage

    - name: Download unit coverage
      uses: actions/download-artifact@v4
      with:
        name: coverage-unit
        path: coverage-unit

    - name: Download integration coverage
      uses: actions/download-artifact@v4
      with:
        name: coverage-integration
        path: coverage-integration

    - name: Combine coverage data and generate report
      run: |
        echo "=== Combining Coverage Data ==="
        # ‚ö†Ô∏è  IMPORTANT: This job ONLY combines coverage - NO test runs!
        #
        # Tests were already run in unit-tests and integration-tests stages.
        # Running tests again here would double CI time.
        #
        # We use 'coverage combine' to merge the .coverage files from
        # the parallel test jobs.

        # Move coverage files to current directory with unique names
        mv coverage-unit/.coverage .coverage.unit
        mv coverage-integration/.coverage .coverage.integration

        # Combine coverage data from both jobs
        coverage combine .coverage.unit .coverage.integration

        # Generate reports
        echo "=== Coverage Report ==="
        coverage report -m --include="cortical/*"
        coverage xml -o coverage.xml

        # Check threshold (fail if below 89%)
        coverage report --fail-under=89 --include="cortical/*"
        echo "‚úÖ Coverage threshold met"

    - name: Upload final coverage report
      uses: actions/upload-artifact@v4
      with:
        name: coverage-report
        path: coverage.xml

  # ==========================================================================
  # Showcase (runs in parallel with all other jobs)
  # Runs the full showcase demo - independent of test results
  # ==========================================================================
  showcase:
    name: "üé≠ Showcase Demo"
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4

    - name: Set up Python 3.11
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'

    - name: Run showcase
      run: |
        echo "=== Running Showcase Demo ==="
        python showcase.py

  # ==========================================================================
  # Security Scanning (runs in parallel with all other jobs)
  # Static analysis and dependency scanning for security vulnerabilities
  # ==========================================================================
  security-scan:
    name: "üîê Security Scan"
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4

    - name: Set up Python 3.11
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'

    - name: Install security tools
      run: |
        python -m pip install --upgrade pip
        pip install bandit pip-audit detect-secrets

    - name: Run Bandit (SAST)
      run: |
        echo "=== Running Bandit Static Analysis ==="
        # -ll = only show medium and higher severity issues
        # -r = recursive
        # Skip B101 (assert) as we use asserts for invariants in non-production code
        bandit -r cortical/ -ll -s B101 -f txt || true
        echo "‚úÖ Bandit scan complete"

    - name: Run pip-audit (Dependency Scanning)
      run: |
        echo "=== Running pip-audit Dependency Check ==="
        pip install -e ".[dev]"
        pip-audit --desc || echo "‚ö†Ô∏è Review dependency vulnerabilities above"

    - name: Run detect-secrets (Secret Scanning)
      run: |
        echo "=== Running detect-secrets ==="
        # Scan for accidentally committed secrets
        detect-secrets scan --all-files --exclude-files '\.git/.*' --exclude-files '.*\.pkl' --exclude-files '.*\.json' > .secrets-baseline.json || true
        # Check if any high-entropy secrets were found (excluding test fixtures)
        python3 << 'PYTHON_SCRIPT'
        import json
        with open('.secrets-baseline.json') as f:
            data = json.load(f)
            results = data.get('results', {})
            real_secrets = {k: v for k, v in results.items() if not k.startswith('tests/')}
            if real_secrets:
                print('‚ö†Ô∏è Potential secrets found in:')
                for file in real_secrets:
                    print(f'  - {file}')
                print('Please review and ensure no real secrets are committed.')
            else:
                print('‚úÖ No secrets detected in source files')
        PYTHON_SCRIPT

  # ==========================================================================
  # Markdown Link Checker (runs in parallel with all other jobs)
  # Uses our own Python script - NO external dependencies/actions
  # Principle: Prefer native implementations over 3rd party APIs/actions
  # ==========================================================================
  markdown-links:
    name: "üîó Markdown Links"
    runs-on: ubuntu-latest
    continue-on-error: true  # Non-blocking initially
    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'

    - name: Check wiki-links in markdown files
      run: |
        echo "Checking wiki-links in documentation..."
        python scripts/resolve_wiki_links.py --check docs/ || true
        python scripts/resolve_wiki_links.py --check samples/memories/ || true
        python scripts/resolve_wiki_links.py --check samples/decisions/ || true
        echo "Link check complete (informational only)"

  # ==========================================================================
  # ML Data Collection: CI Result Auto-Capture
  # Records test results for model training
  # ==========================================================================
  ml-ci-capture:
    name: "üìä ML CI Capture"
    runs-on: ubuntu-latest
    needs: [coverage-report]
    if: always()  # Run even if tests fail
    steps:
    - uses: actions/checkout@v4
      with:
        fetch-depth: 0  # Full history for commit lookup

    - name: Set up Python 3.11
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'

    - name: Capture CI results for ML training
      env:
        CI_RESULT: ${{ needs.coverage-report.result == 'success' && 'pass' || 'fail' }}
        CI_COVERAGE: ${{ needs.coverage-report.outputs.coverage || '' }}
        GITHUB_SHA: ${{ github.sha }}
        GITHUB_WORKFLOW: ${{ github.workflow }}
        GITHUB_JOB: ${{ github.job }}
        GITHUB_RUN_ID: ${{ github.run_id }}
        GITHUB_RUN_NUMBER: ${{ github.run_number }}
      run: |
        echo "=== Recording CI Results for ML Training ==="
        echo "Commit: $GITHUB_SHA"
        echo "Result: $CI_RESULT"

        # Ensure .git-ml directory exists
        mkdir -p .git-ml/commits

        # Try to record the result (may fail if commit not in data yet)
        python scripts/ml_data_collector.py ci-autocapture || echo "Note: Commit may not be in ML data yet"

        echo "=== ML CI Capture Complete ==="

  # ==========================================================================
  # ML Data Collection: Stats Report
  # Shows current progress toward ML training data goals
  # ==========================================================================
  ml-stats-report:
    name: "üìà ML Stats Report"
    runs-on: ubuntu-latest
    needs: [ml-ci-capture]
    if: always()  # Run even if other jobs fail
    steps:
    - uses: actions/checkout@v4
      with:
        fetch-depth: 0  # Full history for accurate stats

    - name: Set up Python 3.11
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'

    - name: Generate ML Data Collection Report
      run: |
        echo "=== ML Data Collection Progress ==="

        # Run stats report
        python scripts/ml_data_collector.py stats

        echo ""
        echo "=== Training Timeline Estimate ==="

        # Run estimate report
        python scripts/ml_data_collector.py estimate

    - name: Write GitHub Summary
      run: |
        echo "## üìä ML Data Collection Report" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY

        # Capture stats in a format suitable for markdown
        python3 << 'PYTHON_SCRIPT' >> $GITHUB_STEP_SUMMARY
        import sys
        sys.path.insert(0, 'scripts')

        try:
            from ml_collector.config import (
                COMMITS_DIR, COMMITS_LITE_FILE, SESSIONS_LITE_FILE,
                CHATS_DIR, ACTIONS_DIR, SESSIONS_DIR
            )
            from pathlib import Path

            # Count data files
            commits_full = len(list(COMMITS_DIR.glob("*.json"))) if COMMITS_DIR.exists() else 0
            commits_lite = sum(1 for _ in open(COMMITS_LITE_FILE)) if COMMITS_LITE_FILE.exists() else 0
            sessions_lite = sum(1 for _ in open(SESSIONS_LITE_FILE)) if SESSIONS_LITE_FILE.exists() else 0
            chats = sum(1 for d in CHATS_DIR.iterdir() if d.is_dir() for f in d.glob("*.json")) if CHATS_DIR.exists() else 0

            # Calculate progress
            commit_target = 5000
            chat_target = 5000
            commit_pct = min(100, (commits_lite / commit_target) * 100)
            chat_pct = min(100, (chats / chat_target) * 100)

            print("### Current Progress")
            print("")
            print("| Metric | Current | Target | Progress |")
            print("|--------|---------|--------|----------|")
            print(f"| Commits (lite) | {commits_lite:,} | {commit_target:,} | {commit_pct:.1f}% |")
            print(f"| Commits (full) | {commits_full:,} | {commit_target:,} | {(commits_full/commit_target)*100:.1f}% |")
            print(f"| Chats | {chats:,} | {chat_target:,} | {chat_pct:.1f}% |")
            print(f"| Sessions (lite) | {sessions_lite:,} | 2,000 | {(sessions_lite/2000)*100:.1f}% |")
            print("")

            # Estimate time
            if commits_lite > 0:
                # Rough estimate based on repo age and commits
                days_estimate = int((commit_target - commits_lite) / max(1, commits_lite / 30) * 30)
                print(f"### Timeline Estimate")
                print(f"")
                print(f"At current collection rate: **~{days_estimate} days** to reach commit target")
                print("")

            print("### Training Milestones")
            print("")
            file_pred_ready = commits_lite >= 500 and chats >= 200
            commit_msg_ready = commits_lite >= 2000 and chats >= 1000
            code_sugg_ready = commits_lite >= 5000 and chats >= 5000

            print(f"- {'‚úÖ' if file_pred_ready else '‚¨ú'} **File Prediction** (500 commits, 200 chats)")
            print(f"- {'‚úÖ' if commit_msg_ready else '‚¨ú'} **Commit Messages** (2,000 commits, 1,000 chats)")
            print(f"- {'‚úÖ' if code_sugg_ready else '‚¨ú'} **Code Suggestions** (5,000 commits, 5,000 chats)")

        except Exception as e:
            print(f"*Could not generate detailed stats: {e}*")
        PYTHON_SCRIPT
