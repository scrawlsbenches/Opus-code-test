{
  "version": 1,
  "session_id": "2420",
  "started_at": "2025-12-16T10:38:46.794756",
  "saved_at": "2025-12-16T10:38:46.795202",
  "tasks": [
    {
      "id": "T-20251216-103846-2420-001",
      "title": "Ensure test_evaluate_cluster.py doesn't skip in CI",
      "status": "pending",
      "priority": "low",
      "category": "ci",
      "description": "Test skips if samples/ directory missing or insufficient docs. Verify CI has proper test fixtures or mark appropriately.",
      "depends_on": [],
      "effort": "medium",
      "created_at": "2025-12-16T10:38:46.794804",
      "updated_at": null,
      "completed_at": null,
      "context": {},
      "retrospective": null
    },
    {
      "id": "T-20251216-103846-2420-002",
      "title": "Review test_semantics_coverage.py numpy dependency",
      "status": "pending",
      "priority": "low",
      "category": "enhancement",
      "description": "Two tests skip if numpy unavailable. Consider adding numpy to dev deps or marking with @pytest.mark.optional.",
      "depends_on": [],
      "effort": "medium",
      "created_at": "2025-12-16T10:38:46.794885",
      "updated_at": null,
      "completed_at": null,
      "context": {},
      "retrospective": null
    },
    {
      "id": "T-20251216-103846-2420-003",
      "title": "Document runtime skip conditions in test files",
      "status": "pending",
      "priority": "low",
      "category": "docs",
      "description": "Tests with skipTest() calls should document why they skip. Consider converting to explicit markers where possible.",
      "depends_on": [],
      "effort": "medium",
      "created_at": "2025-12-16T10:38:46.794926",
      "updated_at": null,
      "completed_at": null,
      "context": {},
      "retrospective": null
    }
  ]
}