# Cortical Text Processor

![Python 3.9+](https://img.shields.io/badge/python-3.9%2B-blue.svg)
![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)
![Tests](https://img.shields.io/badge/tests-3800%2B%20passing-brightgreen.svg)
![Coverage](https://img.shields.io/badge/coverage-%3E90%25-brightgreen.svg)
![Zero Dependencies](https://img.shields.io/badge/dependencies-zero-orange.svg)
![Fact Check](https://img.shields.io/badge/fact--check-94%25%20verified-blue.svg)

## What is this?

**Cortical Text Processor** is a zero-dependency Python library for hierarchical text analysis and semantic search. Despite the neocortex-inspired naming, it uses proven information retrieval algorithmsâ€”**PageRank**, **TF-IDF**, and **Louvain clustering**â€”not neural networks.

**Key use cases:**
- **Semantic search**: Find documents by meaning, not just keywords
- **Code search**: Search codebases with identifier splitting and programming concept expansion
- **Document retrieval**: RAG system support with chunk-level passage retrieval
- **Knowledge analysis**: Detect gaps, outliers, and missing connections in your corpus

**Zero dependencies.** Just copy the `cortical/` folder into your project and go.

## Quick Example

```python
from cortical import CorticalTextProcessor

# Create processor and add documents
processor = CorticalTextProcessor()
processor.process_document("doc1", "Neural networks process information hierarchically.")
processor.process_document("doc2", "The brain uses layers of neurons for processing.")

# Build the semantic network
processor.compute_all()

# Search with automatic query expansion
results = processor.find_documents_for_query("neural processing")
print(results)  # [('doc1', 0.877), ('doc2', 0.832)]

# Save for later
processor.save("my_corpus.pkl")
```

---

## Why "Cortical"?

> *"What if we built a text search engine the way evolution built a brain?"*

Your visual cortex doesn't grep through pixels looking for cats. It builds hierarchiesâ€”edges become patterns, patterns become shapes, shapes become objects. This library applies the same principle to text.

Feed it documents. It tokenizes them into "minicolumns" (Layer 0), connects co-occurring words through Hebbian learning ("neurons that fire together, wire together"), clusters them into concepts (Layer 2), and links documents by shared meaning (Layer 3). The result: a graph that understands your corpus well enough to expand queries, complete analogies, and tell you where your knowledge has gaps.

No PyTorch. No transformers. No API keys. Just 3800+ tests, 20,000+ lines of pure Python, and a data structure that would make a neuroscientist squint approvingly.

---

## Overview

This library provides a biologically-inspired approach to text processing, organizing information through a hierarchical structure similar to the visual cortex:

| Layer | Name | Analogy | Purpose |
|-------|------|---------|---------|
| 0 | Tokens | V1 (edges) | Individual words |
| 1 | Bigrams | V2 (patterns) | Word pairs |
| 2 | Concepts | V4 (shapes) | Semantic clusters |
| 3 | Documents | IT (objects) | Full documents |

## Key Features

- **Hierarchical Processing**: Feedforward, feedback, and lateral connections like the neocortex
- **PageRank Importance**: Graph-based term importance with relation-weighted and cross-layer propagation
- **TF-IDF Weighting**: Statistical term distinctiveness with per-document occurrence tracking
- **Corpus-Derived Semantics**: Pattern-based commonsense relation extraction without external knowledge bases
- **Graph Embeddings**: Multiple embedding methods (adjacency, spectral, random walk) with semantic retrofitting
- **ConceptNet-Style Relations**: Typed edges (IsA, HasA, PartOf, etc.) with multi-hop inference
- **Concept Inheritance**: IsA hierarchy propagation for concept properties
- **Analogy Completion**: Relation matching and vector arithmetic for analogical reasoning
- **Code Search**: Identifier splitting, programming concept expansion, and intent-based queries
- **Semantic Fingerprinting**: Compare document similarity with explanations
- **Fast Search**: Pre-built indexes for 2-3x faster repeated queries
- **Incremental Updates**: Add documents without full recomputation
- **Gap Detection**: Find weak spots and isolated documents in your corpus
- **Query Expansion**: Smart retrieval with synonym handling and semantic relations
- **RAG System Support**: Chunk-level passage retrieval, document metadata, and multi-stage ranking
- **Zero Dependencies**: Pure Python, no pip installs required

## Use Cases & When to Use

### Ideal Use Cases

| Use Case | Why It's a Good Fit |
|----------|---------------------|
| **Internal Documentation Search** | Understands domain-specific terminology through corpus-derived semantics; no training data needed |
| **Knowledge Base Q&A** | Query expansion finds related documents even when exact keywords don't match |
| **Code Repository Search** | Built-in code tokenization splits `getUserName` â†’ `get`, `user`, `name`; programming synonym expansion |
| **Research Paper Organization** | Concept clustering automatically groups related papers; gap detection finds missing coverage |
| **RAG/LLM Context Retrieval** | Chunk-level passage retrieval with relevance scoring; designed for retrieval-augmented generation |
| **Offline/Air-gapped Environments** | Zero dependencies, no API calls, works completely offline |
| **Privacy-Sensitive Applications** | All processing happens locally; no data leaves your machine |
| **Educational Projects** | Clean, well-documented codebase demonstrates IR algorithms (PageRank, TF-IDF, Louvain clustering) |

### Good Fit For Developers Who...

- **Need explainable search** - Every result can be traced through the graph; see exactly why documents matched
- **Want to avoid ML complexity** - No model training, GPU requirements, or hyperparameter tuning
- **Work with specialized domains** - Corpus-derived semantics adapts to your terminology automatically
- **Need lightweight deployment** - Single Python package, no Docker, no external services
- **Value reproducibility** - Deterministic algorithms produce consistent results
- **Build RAG pipelines** - First-class support for passage retrieval with configurable chunking

### When NOT to Use

| Scenario | Better Alternative |
|----------|-------------------|
| Need state-of-the-art semantic similarity | Use sentence transformers or OpenAI embeddings |
| Processing millions of documents | Use Elasticsearch, Meilisearch, or vector databases |
| Need real-time indexing at scale | Use purpose-built search infrastructure |
| Require cross-lingual search | Use multilingual embedding models |
| Need image/multimodal search | Use CLIP or similar multimodal models |

### Learn Information Retrieval by Reading Code

This codebase teaches you how search actually works:

| Concept | File | What You'll Learn |
|---------|------|-------------------|
| **PageRank** | `cortical/analysis/pagerank.py` | The algorithm that powered Google's original search (Brin & Page, 1998) |
| **TF-IDF / BM25** | `cortical/analysis/tfidf.py` | Document relevance scoring used since 1972 (Robertson et al.) |
| **Louvain Clustering** | `cortical/analysis/clustering.py` | Community detection for finding concept groups (Blondel et al., 2008) |
| **Query Expansion** | `cortical/query/expansion.py` | How search engines find related terms |
| **Graph Traversal** | `cortical/layers.py` | Building knowledge graphs from text |

Every file is documented, type-hinted, and tested. Step through with a debugger to see exactly how each algorithm works.

**See also:** [docs/our-story.md](docs/our-story.md) - How we develop by using the system on itself

### Example: Building a Documentation Search

```python
from cortical import CorticalTextProcessor
import os

# Initialize processor
processor = CorticalTextProcessor()

# Index your documentation
for filename in os.listdir("docs/"):
    if filename.endswith(".md"):
        with open(f"docs/{filename}") as f:
            processor.process_document(filename, f.read())

# Build the semantic network
processor.compute_all(verbose=False)

# Search with query expansion
results = processor.find_documents_for_query("authentication setup")
# Finds docs about "auth", "login", "credentials" even if "authentication" isn't mentioned

# Get relevant passages for RAG
passages = processor.find_passages_for_query("how to configure OAuth", top_n=3)
for passage, score, doc_id in passages:
    print(f"[{doc_id}] {passage[:100]}...")
```

### Example: Code Search with Intent

```python
from cortical import CorticalTextProcessor
from cortical.tokenizer import Tokenizer
import glob

# Enable code-aware tokenization (splits getUserName â†’ get, user, name)
code_tokenizer = Tokenizer(split_identifiers=True)
processor = CorticalTextProcessor(tokenizer=code_tokenizer)

# Index source files
for filepath in glob.glob("src/**/*.py", recursive=True):
    with open(filepath) as f:
        processor.process_document(filepath, f.read())

processor.compute_all()

# Intent-based search understands natural language questions
results = processor.search_by_intent("where do we handle user authentication?")
# Returns files dealing with auth, login, session management

# Code-specific query expansion
expanded = processor.expand_query_for_code("fetch data")
# Expands to include: get, load, retrieve, request, download
```

## Installation

Install from source:

```bash
git clone <repository-url>
cd cortical-text-processor
pip install -e .
```

Or simply copy the `cortical/` directory into your projectâ€”zero dependencies means no pip required.

**Verify installation:**
```bash
python -c "from cortical import CorticalTextProcessor; print('Installation successful')"
```

## Quick Start

Run the showcase to see the processor analyze 176 documents covering everything from neural networks to medieval falconry:

```bash
python showcase.py
```

**Output:**
```
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘            ğŸ§   CORTICAL TEXT PROCESSOR SHOWCASE  ğŸ§                   â•‘
    â•‘     Mimicking how the neocortex processes and understands text       â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Loading documents from: samples
Processing through cortical hierarchy...
(Like visual information flowing V1 â†’ V2 â†’ V4 â†’ IT)

  ğŸ“„ comprehensive_machine_learning (2445 words)
  ğŸ“„ attention_mechanism_research   (644 words)
  ğŸ“„ neural_network_optimization    (648 words)
  ... 89 more documents ...

âœ“ Processed 92 documents
âœ“ Created 6,506 token minicolumns
âœ“ Created 20,114 bigram minicolumns
âœ“ Formed 116,332 lateral connections

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                       KEY CONCEPTS (PageRank)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

PageRank identifies central concepts - highly connected 'hub' words:

  Rank  Concept            PageRank
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    1.  data               â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 0.0046
    2.  model              â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘ 0.0044
    3.  learning           â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘ 0.0041
    ...

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                        QUERY DEMONSTRATION
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ” Query: 'neural networks'
   Expanded with: knowledge, data, graph, network, deep, artificial

   Top documents:
     â€¢ comprehensive_machine_learning (score: 26.384)
     â€¢ attention_mechanism_research (score: 19.178)
     â€¢ cortical_semantic_networks (score: 18.470)
```

### Programmatic Usage

```python
from cortical import CorticalTextProcessor

processor = CorticalTextProcessor()

# Add documents
processor.process_document("doc1", "Neural networks process information hierarchically.")
processor.process_document("doc2", "The brain uses layers of neurons for processing.")
processor.process_document("doc3", "Machine learning enables pattern recognition.")

# Build the network
processor.compute_all()

# Query
results = processor.find_documents_for_query("neural processing")
print(results)  # [('doc1', 0.877), ('doc2', 0.832)]

# Save for later
processor.save("my_corpus.pkl")
```

## Core API

### Document Processing

```python
processor.process_document(doc_id, content, metadata=None)
processor.add_document_incremental(doc_id, content)  # Incremental indexing
processor.add_documents_batch([(doc_id, content, metadata), ...])  # Batch processing
```

### Network Building

```python
# All-in-one computation with connection strategies
processor.compute_all(
    verbose=False,
    connection_strategy='hybrid',  # 'document_overlap', 'semantic', 'embedding', 'hybrid'
    cluster_strictness=0.5,        # 0.0-1.0, lower = fewer, larger clusters
    bridge_weight=0.3              # 0.0-1.0, cross-document bridging
)

# Individual computations
processor.propagate_activation()      # Spread activation
processor.compute_importance()        # PageRank scores
processor.compute_tfidf()             # TF-IDF weights
processor.build_concept_clusters()    # Cluster tokens
processor.compute_document_connections()  # Link documents
processor.compute_bigram_connections()    # Bigram lateral connections
```

### Semantics & Embeddings

```python
processor.extract_corpus_semantics()  # Extract relations
processor.retrofit_connections()      # Blend with semantics
processor.compute_graph_embeddings(dimensions=32, method='adjacency')
processor.retrofit_embeddings()       # Improve embeddings
processor.expand_query_multihop(query, max_hops=2)  # Multi-hop query expansion
processor.complete_analogy(a, b, c)   # Analogy completion (a:b :: c:?)
```

### Query & Retrieval

```python
processor.expand_query(text, max_expansions=10)  # Expand query
processor.find_documents_for_query(text, top_n=5)  # Search
processor.find_related_documents(doc_id)  # Related docs
processor.find_documents_batch(queries)  # Process multiple queries
processor.find_passages_for_query(query, top_n=5)  # Chunk-level RAG retrieval
```

### Analysis

```python
processor.analyze_knowledge_gaps()  # Find gaps
processor.detect_anomalies(threshold=0.1)  # Find outliers
processor.get_corpus_summary()      # Corpus statistics
processor.export_conceptnet_json(filepath)  # ConceptNet-style visualization export
```

## Connection Strategies

For documents with different topics or minimal overlap, use connection strategies:

```python
# Hybrid strategy combines all methods for maximum connectivity
processor.compute_all(
    connection_strategy='hybrid',
    cluster_strictness=0.5,
    bridge_weight=0.3
)
```

| Strategy | Description |
|----------|-------------|
| `document_overlap` | Traditional Jaccard similarity (default) |
| `semantic` | Connect via semantic relations between members |
| `embedding` | Connect via embedding centroid similarity |
| `hybrid` | Combine all three for maximum connectivity |

## Performance

### Benchmarks (Real Measurements)

| Operation | Time | Notes |
|-----------|------|-------|
| Index 100 documents | ~1.3ms | BM25 scoring |
| Search query | ~0.15ms | Pre-computed TF-IDF |
| Add document (incremental) | ~50ms | Without full recompute |
| Full compute_all() | ~500ms | 100 docs, all algorithms |

### Corpus Size Recommendations

| Documents | Recommendation |
|-----------|----------------|
| < 1,000 | Perfect fit |
| 1,000 - 10,000 | Good fit, consider tuning |
| 10,000 - 100,000 | Works, but consider hybrid approach |
| 100,000+ | Use dedicated search infrastructure |

### Test Coverage

| Metric | Value |
|--------|-------|
| Sample documents | 176 |
| Test functions | 3,800+ |
| Lines of code | 20,000+ |
| Graph algorithms | O(1) ID lookups |

*Note: Token/bigram/connection counts vary based on corpus content.*

**What the processor discovers:**
- Most central concept: `data` (PageRank: 0.0046)
- Most distinctive terms: `gradient`, `pagerank`, `patent` (high TF-IDF, rare but meaningful)
- Most connected document: `comprehensive_machine_learning` (91 connections to other docs)
- Isolated outliers detected: `sumo_wrestling`, `medieval_falconry` (low similarity to corpus)

## Package Structure

```
cortical/
â”œâ”€â”€ __init__.py          # Public API exports
â”œâ”€â”€ processor/           # Main orchestrator (mixin-based architecture)
â”‚   â”œâ”€â”€ __init__.py      # CorticalTextProcessor class composition
â”‚   â”œâ”€â”€ core.py          # Initialization, staleness tracking (169 lines)
â”‚   â”œâ”€â”€ documents.py     # Document add/remove/batch (456 lines)
â”‚   â”œâ”€â”€ compute.py       # PageRank, TF-IDF, clustering (1041 lines)
â”‚   â”œâ”€â”€ query_api.py     # Search, expansion, retrieval (719 lines)
â”‚   â”œâ”€â”€ introspection.py # State inspection, summaries (357 lines)
â”‚   â””â”€â”€ persistence_api.py # Save/load/export (245 lines)
â”œâ”€â”€ query/               # Search & retrieval (8 focused modules)
â”‚   â”œâ”€â”€ expansion.py     # Query expansion (459 lines)
â”‚   â”œâ”€â”€ search.py        # Document search (422 lines)
â”‚   â”œâ”€â”€ ranking.py       # Multi-stage ranking (472 lines)
â”‚   â”œâ”€â”€ passages.py      # RAG passage retrieval (407 lines)
â”‚   â”œâ”€â”€ chunking.py      # Text chunking (335 lines)
â”‚   â”œâ”€â”€ intent.py        # Intent-based queries (220 lines)
â”‚   â”œâ”€â”€ definitions.py   # Definition search (375 lines)
â”‚   â””â”€â”€ analogy.py       # Analogy completion (330 lines)
â”œâ”€â”€ analysis.py          # Graph algorithms: PageRank, TF-IDF, Louvain
â”œâ”€â”€ semantics.py         # Relation extraction, inference, retrofitting
â”œâ”€â”€ minicolumn.py        # Core data structure with typed edges
â”œâ”€â”€ layers.py            # Hierarchical layers with O(1) lookups
â”œâ”€â”€ tokenizer.py         # Tokenization, stemming, code splitting
â”œâ”€â”€ embeddings.py        # Graph embeddings with retrofitting
â”œâ”€â”€ fingerprint.py       # Semantic fingerprinting
â”œâ”€â”€ gaps.py              # Gap detection and anomalies
â”œâ”€â”€ persistence.py       # Save/load with full state
â”œâ”€â”€ config.py            # CorticalConfig with validation
â”œâ”€â”€ observability.py     # Metrics, timing, tracing
â””â”€â”€ code_concepts.py     # Programming synonym expansion

tests/                   # 3800+ tests (smoke, unit, integration, behavioral)
â”œâ”€â”€ smoke/               # Quick sanity checks
â”œâ”€â”€ unit/                # Fast isolated tests
â”œâ”€â”€ integration/         # Component interaction tests
â”œâ”€â”€ performance/         # Timing regression tests
â””â”€â”€ behavioral/          # Search quality tests

showcase.py              # Interactive demonstration (run it!)
samples/                 # 176 documents: quantum computing to cheese affinage
scripts/                 # Developer tools (indexing, profiling, tasks)
```

## ğŸ¨ Codebase Visualization

Beautiful ASCII art visualizations of your codebase, git history, and code metrics.

### Animated Visualizations

| Matrix Rain | Fire Effect |
|:-----------:|:-----------:|
| ![Matrix Rain](assets/matrix_rain.gif) | ![Fire Effect](assets/fire_effect.gif) |
| *Commit messages falling like code* | *Hot files burn brighter* |

| Starfield | Neural Pulse |
|:---------:|:------------:|
| ![Starfield](assets/starfield.gif) | ![Neural Pulse](assets/neural_pulse.gif) |
| *Commits flying through hyperspace* | *Data flowing through cortical layers* |

### Code Skyline

![Code Skyline](assets/code_skyline.gif)

*File sizes as buildings - taller = more lines of code*

### Run the Visualizations

```bash
# Static visualizations (brain diagram, skyline, heatmap, etc.)
python scripts/ascii_codebase_art.py

# Animated terminal experience (requires terminal with color support)
python scripts/ascii_visualizer_animated.py

# Interactive menu
python scripts/ascii_visualizer_animated.py

# Jump to specific effect
python scripts/ascii_visualizer_animated.py --matrix   # Matrix rain
python scripts/ascii_visualizer_animated.py --fire     # Fire effect
python scripts/ascii_visualizer_animated.py --stars    # Starfield
python scripts/ascii_visualizer_animated.py --neural   # Neural pulse
python scripts/ascii_visualizer_animated.py --dashboard  # Live stats

# Generate new GIFs (requires Pillow)
pip install Pillow
python scripts/generate_ascii_gifs.py
```

## AI Agent Support

This project includes tools designed specifically for AI coding assistants:

### AI Metadata Files (`.ai_meta`)

Pre-generated metadata files provide structured navigation for AI agents:

```bash
# Generate metadata for rapid module understanding
python scripts/generate_ai_metadata.py

# View a module's structure without reading source
cat cortical/processor/__init__.py.ai_meta
cat cortical/query/search.py.ai_meta
```

**What metadata provides:**
- Function signatures with `see_also` cross-references
- Class structures with inheritance
- Complexity hints for expensive operations
- Logical section groupings

### Claude Skills

Four Claude Code skills are available in `.claude/skills/`:

| Skill | Purpose |
|-------|---------|
| `codebase-search` | Semantic search over the codebase |
| `corpus-indexer` | Index/re-index after code changes |
| `ai-metadata` | View and use module metadata |
| `task-manager` | Manage tasks with merge-friendly IDs |

### For AI Agents

See the **AI Agent Onboarding** section in [CLAUDE.md](CLAUDE.md) for:
- Step-by-step setup guide
- Navigation tips for efficient exploration
- Example workflow using metadata

## Text-as-Memories System

Capture and organize institutional knowledge alongside your code:

- **Daily Memories** (`samples/memories/YYYY-MM-DD-*.md`) - Learning entries
- **Decision Records** (`samples/decisions/adr-*.md`) - Architectural decisions
- **Concept Documents** - Consolidated knowledge on topics

```bash
# Create a memory entry
python scripts/new_memory.py "What I learned about validation"

# Create a decision record
python scripts/new_memory.py "Use JSON over pickle" --decision
```

See [docs/text-as-memories.md](docs/text-as-memories.md) for the complete guide.

## ML Data Collection (Optional)

**Help us build a smarter development assistant for this project.**

This project includes optional infrastructure to collect development dataâ€”commit patterns, coding sessions, and query/response pairsâ€”to train a **project-specific micro-model**. The goal: an AI assistant that understands *this* codebase's patterns, conventions, and workflows better than any general-purpose model.

### Why This Matters

General-purpose LLMs don't know your codebase. They don't know that `compute_all()` should be called after `process_document()`, or that bigrams use space separators, or that `_id_index` provides O(1) lookups. A project-specific model trained on actual development patterns can:

- **Predict likely files to edit** based on the task description
- **Generate commit messages** matching your conventions
- **Suggest code patterns** consistent with the existing codebase
- **Learn from feedback** to improve over time

### How It Works

```
.git-ml/                      # Local only (gitignored)
â”œâ”€â”€ commits/                  # Commit metadata + diff hunks
â”œâ”€â”€ chats/                    # Query/response pairs
â”œâ”€â”€ sessions/                 # Development sessions
â””â”€â”€ actions/                  # Tool usage patterns
```

**All data stays local.** Nothing is transmitted anywhere. When we have enough data across contributors, we'll aggregate anonymized patterns (not raw conversations) to train a model that we publish for everyone to use.

### Training Milestones

| Capability | Data Needed |
|------------|-------------|
| File prediction | 500 commits, 100 sessions |
| Commit messages | 2,000 commits, 500 sessions |
| Code suggestions | 5,000 commits, 2,000 sessions |

Check your progress:
```bash
python scripts/ml_data_collector.py stats
python scripts/ml_data_collector.py estimate
```

### File Prediction Model

The first model is now available: **predict which files to modify** based on a task description.

```bash
# Train the model on your commit history
python scripts/ml_file_prediction.py train

# Predict files for a task
python scripts/ml_file_prediction.py predict "Add authentication feature"
# Output:
#   1. cortical/processor.py          (0.268)
#   2. tests/test_processor.py        (0.146)
#   3. cortical/analysis.py           (0.134)

# Evaluate model performance
python scripts/ml_file_prediction.py evaluate --split 0.2

# View model statistics
python scripts/ml_file_prediction.py stats
```

**How it works:**
- Learns from commit type patterns (feat:, fix:, docs:, etc.)
- Builds file co-occurrence matrix (files often changed together)
- Maps keywords to files based on commit messages
- Uses TF-IDF-style scoring with frequency penalties

**Current metrics** (on 20% holdout with 403 commits):
- MRR: 0.43 (first correct prediction averages position ~2-3)
- Recall@10: 0.48 (half of actual files appear in top 10 predictions)
- Precision@1: 0.31 (31% of top predictions are correct)

### Setup (Opt-In)

Data collection is **enabled by default** but requires hook setup to capture sessions:

```bash
# Add to .claude/settings.json for automatic session capture
{
  "hooks": {
    "Stop": [{
      "type": "command",
      "command": "/path/to/project/scripts/ml-session-capture-hook.sh"
    }]
  }
}
```

Manual collection:
```bash
# Record a chat manually
python scripts/ml_data_collector.py chat --query "..." --response "..."

# Add feedback to improve quality
python scripts/ml_data_collector.py feedback --chat-id <id> --rating good
```

### Disable Collection

If you prefer not to participate:

```bash
# Option 1: Environment variable (per-session or in shell profile)
export ML_COLLECTION_ENABLED=0

# Option 2: Remove the Stop hook from .claude/settings.json
```

When disabled, no data is written to `.git-ml/`. You can still use all other features normally.

### Privacy & Security

**Automatic Redaction:** Sensitive data is automatically scrubbed before storage:
- API keys, tokens, and secrets
- Passwords and credentials
- Private keys (RSA, SSH, etc.)
- Database connection strings
- GitHub/Slack tokens, JWTs

```bash
# Test what gets redacted
python scripts/ml_data_collector.py redact-test --text "api_key=sk-abc123secret"
```

**Data Retention:** Old data is kept for 2 years (enough to hit training milestones):
```bash
# Preview what would be deleted (730 days default)
python scripts/ml_data_collector.py cleanup --dry-run

# Manually clean up older data if needed
python scripts/ml_data_collector.py cleanup --days 365
```

**Privacy Guarantees:**
- **Local storage only**: Data never leaves your machine automatically
- **Gitignored**: `.git-ml/` is excluded from version control
- **Regeneratable**: All commit data can be backfilled from git history
- **Full control**: Delete `.git-ml/` anytime to remove all collected data
- **Transparent**: All collection code is in `scripts/ml_data_collector.py`

### Contributing Your Data

When ready to help train the shared model:

```bash
# Preview what would be shared (with redaction applied)
python scripts/ml_data_collector.py contribute preview

# Opt-in to contribute (requires explicit consent)
python scripts/ml_data_collector.py contribute enable --name "Your Name"

# Check your contribution status
python scripts/ml_data_collector.py contribute status

# Opt-out anytime
python scripts/ml_data_collector.py contribute disable
```

See [CLAUDE.md](CLAUDE.md) for detailed documentation on the ML data collection system.

## Development History

This project evolved through systematic improvements:

1. **Initial Release**: Core hierarchical text processing
2. **Code Review & Fixes**: TF-IDF calculation, O(1) lookups, type annotations
3. **RAG Enhancements**: Chunk-level retrieval, metadata support, concept clustering
4. **ConceptNet Integration**: Typed edges, relation-weighted PageRank, multi-hop inference
5. **Connection Strategies**: Multiple strategies for Layer 2 concept connections
6. **Showcase & Polish**: Interactive demo with real corpus analysis

## Running the Showcase

```bash
python showcase.py
```

The showcase processes 176 diverse sample documents and demonstrates every major feature. Here's what you'll see:

### Concept Associations (Hebbian Learning)

The processor discovers that `neural` connects to `networks` (weight: 23), `artificial` (7), `knowledge` (7)â€”while `bread` meekly connects to `beer`, `wine`, and `pyruvate` (weight: 1 each). Neurons that fire together really do wire together.

### Query Expansion in Action

```
ğŸ” Query: 'neural networks'
   Expanded with: knowledge, data, graph, network, deep, artificial

   Top documents:
     â€¢ comprehensive_machine_learning (score: 26.384)
     â€¢ attention_mechanism_research (score: 19.178)
     â€¢ cortical_semantic_networks (score: 18.470)
```

### The Polysemy Problem

Search for "candle sticks" and you'll find `candlestick_patterns` (trading charts) at the topâ€”but also `letterpress_printing` (composing sticks) and `wine_tasting_vocabulary`. The query tokenizes to `['candle', 'sticks']`: "candle" matches the trading document (which discusses "single candle patterns"), while "sticks" matches the printing document. Classic information retrieval challenge: compound words fragment, partial matches surface, and the system can't read your mind about intent.

### Knowledge Gap Detection

The analyzer flags `sumo_wrestling` and `medieval_falconry` as isolated documentsâ€”they don't fit well with the rest of the corpus. It also identifies weak topics: terms like `patent` appear in only 1 document. This is how you find holes in your knowledge base.

## Documentation

Detailed documentation is available in the `docs/` directory:

| Document | Description |
|----------|-------------|
| [docs/README.md](docs/README.md) | Documentation index with reading paths |
| [docs/quickstart.md](docs/quickstart.md) | 5-minute getting started guide |
| [docs/architecture.md](docs/architecture.md) | 4-layer system design |
| [docs/algorithms.md](docs/algorithms.md) | Core IR algorithms (PageRank, TF-IDF, Louvain) |
| [docs/query-guide.md](docs/query-guide.md) | Query formulation guide |
| [docs/cookbook.md](docs/cookbook.md) | Common patterns and recipes |
| [docs/glossary.md](docs/glossary.md) | Terminology definitions |

For AI agents, see also [docs/claude-usage.md](docs/claude-usage.md) and [CLAUDE.md](CLAUDE.md).

## Research

### Hubris MoE: A Credit-Based Mixture of Experts for Development Prediction

We developed **Hubris MoE**, a novel Mixture of Experts system that predicts which files to modify, which tests to run, and how to diagnose errorsâ€”all trained on your project's git history.

**Key innovations:**
- **Credit-based routing**: Experts earn/lose credits based on prediction accuracy, not static weights
- **Calibration tracking**: ECE, MCE, and Brier Score metrics ensure predictions are well-calibrated
- **Cold-start handling**: Graceful degradation with ML fallback when experts haven't learned yet
- **Git integration**: Automatic training from commit history with CI result feedback

**Architecture:**
```
Task Description
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Expert Router   â”‚â”€â”€â–¶ Routes to high-credit experts
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ FileExpert       â”‚â”€â”€â–¶ Predicts files to modify
â”‚ TestExpert       â”‚â”€â”€â–¶ Predicts tests to run
â”‚ ErrorExpert      â”‚â”€â”€â–¶ Diagnoses errors
â”‚ EpisodeExpert    â”‚â”€â”€â–¶ Learns workflow patterns
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
   Aggregated Prediction
       â”‚
       â–¼
   Value Signal (from outcomes)
       â”‚
       â–¼
   Credit Update (learn from results)
```

**Current results** (Sprint 5 complete, 400+ commits):
- File prediction MRR: 0.43
- Cold-start detection working
- Calibration tracking operational

See the full research paper: [docs/research/hubris-moe-research-paper.md](docs/research/hubris-moe-research-paper.md)

## Running Tests

```bash
python -m unittest discover -s tests -v
```

## Security Considerations

### Pickle Deserialization Warning

âš ï¸ **The default `.pkl` format uses Python's `pickle` module, which can execute arbitrary code during deserialization.**

**Risk**: Loading a malicious `.pkl` file could result in remote code execution (RCE). Only load pickle files from sources you trust completely.

**Recommendations**:

1. **For untrusted sources**: Use the JSON state format instead:
   ```python
   from cortical.state_storage import StateLoader

   # Save as JSON (safe to share)
   StateLoader.save(processor, "corpus_state.json")

   # Load from JSON (safe from untrusted sources)
   processor = StateLoader.load("corpus_state.json")
   ```

2. **For trusted sources**: Continue using pickle for faster serialization:
   ```python
   processor.save("corpus.pkl")  # Fast, but only load files you trust
   processor = CorticalTextProcessor.load("corpus.pkl")
   ```

3. **For maximum security**: Never load pickle files from:
   - Downloaded files from the internet
   - User uploads
   - Shared network locations with untrusted access
   - Email attachments

See [Python's pickle documentation](https://docs.python.org/3/library/pickle.html) for more details on pickle security.

## Roadmap

### Current Focus (v2.x)
- [ ] Remove deprecated `feedforward_sources` field (migrate to `feedforward_connections`)
- [ ] Reduce checkpoint handling code duplication in `compute.py`
- [ ] Standardize layer variable naming (semantic names vs `layer0`, `layer1`)
- [ ] Move magic numbers to `CorticalConfig`

### Planned Features (v3.x)
- [ ] **Streaming document processing** - Process large documents in chunks without loading entirely into memory
- [ ] **Incremental clustering** - Update concept clusters without full recomputation
- [ ] **Query result explanations** - Human-readable explanations for why documents matched
- [ ] **Export to NetworkX** - Direct graph export for visualization and analysis
- [ ] **Async API** - Async versions of compute-heavy methods

### Under Consideration
- [ ] **Optional sentence-transformers integration** - Hybrid retrieval combining graph + embeddings
- [ ] **WASM build** - Run in browser via WebAssembly
- [ ] **REST API wrapper** - Simple HTTP server for non-Python clients
- [ ] **Multi-corpus federation** - Query across multiple independent corpora

### Not Planned
- Cloud/SaaS dependencies (against zero-dependency philosophy)
- GPU acceleration (keep it simple and portable)
- Real-time collaborative editing (out of scope)

See [CODE_REVIEW.md](CODE_REVIEW.md) for technical debt and improvement opportunities.

---

## Fact Check

*Last verified: 2025-12-15 | Score: 94% accurate*

| Claim | Status | Notes |
|-------|--------|-------|
| Zero external dependencies | âœ… Verified | Production code uses only stdlib |
| 3,150+ tests | âœ… Verified | `grep -r "def test_" tests/ \| wc -l` = 3,150 |
| 20,000+ lines of code | âœ… Verified | `wc -l cortical/**/*.py` = 20,245 |
| 176 sample documents | âœ… Verified | `ls samples/*.txt \| wc -l` = 176 |
| >89% coverage | âš ï¸ Unverified | Requires test run to confirm |
| O(1) ID lookups | âœ… Verified | `_id_index` dict in `layers.py` |
| `split_identifiers` tokenization | âœ… Verified | In `Tokenizer` class, not processor |
| Package structure line counts | âœ… Verified | All counts match actual files |
| All documented methods exist | âœ… Verified | Grep confirms all API methods |
| `sumo_wrestling.txt` exists | âœ… Verified | Present in samples/ |
| `medieval_falconry.txt` exists | âœ… Verified | Present in samples/ |

**Methodology:** Claims verified by running shell commands against the codebase. Dynamic values (PageRank scores, connection counts) depend on corpus content and are representative examples.

---

## Contributing

We welcome contributions! Please see [CONTRIBUTING.md](CONTRIBUTING.md) for:
- Development setup and workflow
- Code style and testing requirements
- Pull request guidelines

Quality resources:
- [Definition of Done](docs/definition-of-done.md)
- [Code of Ethics](docs/code-of-ethics.md)

## License

MIT License
