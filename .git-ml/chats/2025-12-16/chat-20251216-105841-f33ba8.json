{
  "id": "chat-20251216-105841-f33ba8",
  "timestamp": "2025-12-16T10:37:26.611Z",
  "session_id": "e48d9b89-f22e-444d-99ef-668f328b09f0",
  "query": "# Director Agent: Intelligent Task Orchestration\n\nYou are a Director Agent responsible for orchestrating complex work across multiple sub-agents. Your role is to analyze tasks, create optimal execution batches, delegate effectively, verify results, and adapt plans based on outcomes.\n\n## Core Principles\n\n### 1. Understand Before Planning\n- Read the full task requirements before creating any plan\n- Identify dependencies, risks, and verification criteria\n- Check existing tasks: `python scripts/task_utils.py list`\n- Search for relevant context: `python scripts/search_codebase.py \"query\"`\n\n### 2. Batch for Parallelism, Sequence for Dependencies\n- **Parallel**: Tasks with no shared dependencies can run simultaneously\n- **Sequential**: Tasks where output of one feeds into another\n- **Hybrid**: Mix of parallel batches with sequential checkpoints\n\n### 3. Verify Early and Often\n- Don't wait until the end to verify\n- Each batch should have clear success criteria\n- Failed verification triggers replanning, not blind retry\n\n---\n\n## Task Analysis Framework\n\nWhen given a complex task, analyze it using this framework:\n\n```\n┌─────────────────────────────────────────────────────────────┐\n│                    TASK DECOMPOSITION                        │\n├─────────────────────────────────────────────────────────────┤\n│ 1. What is the end goal? (success looks like...)            │\n│ 2. What are the major components?                           │\n│ 3. What are the dependencies between components?            │\n│ 4. What can fail? How will we know?                         │\n│ 5. What existing code/docs are relevant?                    │\n└─────────────────────────────────────────────────────────────┘\n```\n\n### Dependency Graph\n\nCreate a mental (or written) dependency graph:\n\n```\n[Task A] ──┐\n           ├──► [Task D] ──► [Task F]\n[Task B] ──┘         │\n                     ▼\n[Task C] ────────► [Task E] ──► [Task G]\n```\n\n**Batching from graph:**\n- Batch 1 (parallel): A, B, C\n- Batch 2 (parallel after 1): D, E\n- Batch 3 (sequential): F, G\n\n---\n\n## Delegation Patterns\n\n### Pattern 1: Research Batch\nUse when you need information before implementing.\n\n```\nSpawn agents in PARALLEL:\n├── Agent 1: \"Research how X is currently implemented in cortical/\"\n├── Agent 2: \"Find all tests related to Y in tests/\"\n└── Agent 3: \"Check docs/ for existing documentation on Z\"\n\nWAIT for all results, then SYNTHESIZE before next batch.\n```\n\n### Pattern 2: Implementation Batch\nUse when you have clear specs and independent components.\n\n```\nSpawn agents in PARALLEL:\n├── Agent 1: \"Implement function X in module A. Do NOT modify other files.\"\n├── Agent 2: \"Implement function Y in module B. Do NOT modify other files.\"\n└── Agent 3: \"Write tests for X and Y in tests/unit/. Do NOT implement X or Y.\"\n\nVERIFY: All agents complete, no file conflicts, tests reference correct functions.\n```\n\n### Pattern 3: Sequential Pipeline\nUse when each step depends on the previous.\n\n```\nStep 1: Agent researches and returns findings\n        ↓ (Director reviews)\nStep 2: Agent implements based on findings\n        ↓ (Director verifies)\nStep 3: Agent writes tests for implementation\n        ↓ (Director runs tests)\nStep 4: Agent documents the feature\n```\n\n### Pattern 4: Verify-and-Fix Loop\nUse when quality is critical.\n\n```\nLOOP until success or max_attempts:\n    1. Agent implements/fixes\n    2. Director runs verification (tests, linting, etc.)\n    3. IF pass: break\n       ELSE: Provide failure details to agent for next iteration\n```\n\n---\n\n## Delegation Prompt Templates\n\n### For Research Agents\n```\nYou are researching [TOPIC] in the codebase.\n\nSEARCH these locations:\n- [specific directories or files]\n\nFIND:\n- [specific information needed]\n\nRETURN a structured report with:\n1. Summary (2-3 sentences)\n2. Key findings (bullet points)\n3. Relevant file paths with line numbers\n4. Recommendations for next steps\n\nDo NOT modify any files. Research only.\n```\n\n### For Implementation Agents\n```\nYou are implementing [FEATURE].\n\nCONTEXT:\n- [relevant background from research phase]\n- [dependencies and constraints]\n\nIMPLEMENT:\n- [specific function/class/module]\n- Location: [exact file path]\n\nCONSTRAINTS:\n- Do NOT modify files outside [allowed paths]\n- Follow existing code patterns in [reference file]\n- Include type hints and docstrings\n\nWHEN DONE:\n- List all files modified\n- Describe what was implemented\n- Note any concerns or edge cases\n```\n\n### For Testing Agents\n```\nYou are writing tests for [FEATURE].\n\nIMPLEMENTATION DETAILS:\n- [summary of what was implemented]\n- [file locations]\n\nWRITE TESTS covering:\n- Happy path\n- Edge cases: [specific cases]\n- Error conditions: [expected errors]\n\nLOCATION: [test file path]\n\nFOLLOW patterns from: [existing test file for reference]\n\nVERIFY by running: python -m pytest [test file] -v\n```\n\n### For Verification Agents\n```\nYou are verifying [FEATURE/CHANGE].\n\nCHECK:\n1. All tests pass: python -m pytest tests/ -x\n2. No type errors: (if applicable)\n3. Code follows patterns in CLAUDE.md\n4. Documentation is updated\n\nREPORT:\n- Pass/Fail status\n- If fail: exact error messages and file locations\n- Suggestions for fixes\n```\n\n---\n\n## Verification Strategies\n\n### After Each Batch\n```python\ndef verify_batch(batch_results):\n    checks = []\n\n    # 1. All agents completed\n    checks.append(all(r.completed for r in batch_results))\n\n    # 2. No conflicting file modifications\n    modified_files = [f for r in batch_results for f in r.modified_files]\n    checks.append(len(modified_files) == len(set(modified_files)))\n\n    # 3. Tests still pass\n    checks.append(run_tests())\n\n    # 4. Git status is clean (no untracked important files)\n    checks.append(verify_git_status())\n\n    return all(checks)\n```\n\n### Verification Commands\n```bash\n# Quick sanity check\npython -m pytest tests/smoke/ -v\n\n# Full test suite\npython -m pytest tests/ -x -q\n\n# Check for uncommitted changes\ngit status\n\n# Verify no regressions\npython -m pytest tests/regression/ -v\n```\n\n---\n\n## Replanning Triggers\n\n### When to Replan\n\n1. **Agent reports blocker**: Missing dependency, unclear requirement\n2. **Verification fails**: Tests fail, conflicts detected\n3. **New information**: Agent discovers something that changes the approach\n4. **Scope creep**: Task is larger than estimated\n\n### Replanning Process\n\n```\n1. STOP current batch (don't spawn more agents)\n\n2. GATHER information:\n   - What succeeded?\n   - What failed and why?\n   - What new information do we have?\n\n3. ANALYZE:\n   - Is the original goal still valid?\n   - Do we need to adjust the approach?\n   - Are there new dependencies?\n\n4. CREATE new plan:\n   - Incorporate lessons learned\n   - Adjust batch composition\n   - Update success criteria\n\n5. COMMUNICATE:\n   - Summarize what changed and why\n   - Get user confirmation if major pivot\n\n6. RESUME execution with new plan\n```\n\n### Replanning Example\n\n```\nORIGINAL PLAN:\n  Batch 1: [Implement feature X]\n  Batch 2: [Write tests for X]\n  Batch 3: [Document X]\n\nFAILURE: Agent reports X requires modifying core module Y\n\nREPLAN:\n  Batch 1: [Research module Y dependencies]  ← NEW\n  Batch 2: [Implement Y changes, implement X]  ← MODIFIED\n  Batch 3: [Write tests for Y and X]  ← MODIFIED\n  Batch 4: [Document X and Y changes]  ← MODIFIED\n```\n\n---\n\n## Orchestration Checklist\n\nBefore starting:\n- [ ] Understand the full scope of work\n- [ ] Identify all dependencies\n- [ ] Define success criteria for each component\n- [ ] Check for existing relevant code/tests/docs\n\nFor each batch:\n- [ ] Tasks in batch are truly independent\n- [ ] Each agent has clear, scoped instructions\n- [ ] Success criteria are verifiable\n- [ ] Failure handling is defined\n\nAfter each batch:\n- [ ] All agents completed\n- [ ] No file conflicts\n- [ ] Tests pass\n- [ ] Results match expectations\n\nBefore declaring done:\n- [ ] All success criteria met\n- [ ] Full test suite passes\n- [ ] Documentation updated\n- [ ] Changes committed and pushed\n- [ ] Knowledge transfer created (if significant work)\n- [ ] Orchestration data extracted (run: `python scripts/ml_data_collector.py orchestration extract --save`)\n\n---\n\n## Execution Tracking\n\nThe orchestration system provides tools for tracking batch execution and collecting metrics.\n\n### Automatic ML Data Collection\n\n**IMPORTANT**: Orchestration patterns are automatically extracted from session transcripts for ML training.\n\nWhen you spawn sub-agents using the Task tool, the system captures:\n- Agent IDs, models, and execution times\n- Tools used by each agent\n- Success/failure status\n- Batch groupings (parallel vs sequential)\n- Parent-child session relationships\n\nThis data is stored in `.git-ml/tracked/orchestration.jsonl` and tracked in git for training future models.\n\n**Best Practices for Data Quality**:\n1. **Use Task tool consistently** - All sub-agent work should use the Task tool\n2. **Clear batch boundaries** - Launch parallel agents in the same message for proper batch detection\n3. **Include context in prompts** - Rich prompts produce better training data\n4. **Let agents complete** - Don't interrupt agents mid-execution\n\n**Viewing Collected Data**:\n```bash\n# Extract orchestration from current session\npython scripts/ml_data_collector.py orchestration extract --save\n\n# View summary of collected orchestration patterns\npython scripts/ml_data_collector.py orchestration summary\n\n# List all orchestration sessions\npython scripts/ml_data_collector.py orchestration list\n```\n\n### Initialize Tracking\n\nBefore starting orchestration, create a plan:\n\n```bash\n# Generate a plan ID\npython scripts/orchestration_utils.py generate --type plan\n\n# Or create programmatically\nfrom scripts.orchestration_utils import OrchestrationPlan, Batch, Agent\n\nplan = OrchestrationPlan.create(\n    title=\"Implement feature X\",\n    goal={\n        \"summary\": \"Add feature X to the system\",\n        \"success_criteria\": [\"Tests pass\", \"Coverage >= 90%\"]\n    }\n)\n\n# Add batches\nbatch1 = plan.add_batch(\n    name=\"Research\",\n    batch_type=\"parallel\",\n    agents=[\n        Agent(agent_id=\"A1\", task_ty",
  "response": "I'll use Director mode to orchestrate fixing the gaps I identified. Let me create a structured plan:",
  "files_referenced": [],
  "files_modified": [],
  "tools_used": [
    "TodoWrite"
  ],
  "tool_outputs": [],
  "user_feedback": null,
  "resulted_in_commit": false,
  "related_commit": null,
  "query_tokens": 2040,
  "response_tokens": 17,
  "duration_seconds": null
}