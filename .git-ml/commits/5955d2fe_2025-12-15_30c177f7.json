{
  "hash": "5955d2fe113fe90a68aae8a12df650e0e103a137",
  "message": "Merge pull request #87 from scrawlsbenches/claude/analyze-git-history-lines-etXYM",
  "author": "scrawlsbenches",
  "timestamp": "2025-12-15 08:26:49 -0500",
  "branch": "claude/multi-index-design-DvifZ",
  "files_changed": [
    ".claude/commands/delegate.md",
    ".claude/commands/ml-log.md",
    ".claude/commands/ml-stats.md",
    ".claude/commands/sanity-check.md",
    ".claude/hooks/session_logger.py",
    ".claude/settings.local.json",
    ".claude/skills/ml-logger/SKILL.md",
    ".gitignore",
    "CLAUDE.md",
    "README.md",
    "docs/algorithms.md",
    "docs/architecture.md",
    "docs/claude-usage.md",
    "docs/code-of-ethics.md",
    "docs/devex-tools.md",
    "docs/dogfooding.md",
    "docs/glossary.md",
    "docs/louvain_resolution_analysis.md",
    "docs/patterns.md",
    "docs/quickstart.md",
    "scripts/ml-session-capture-hook.sh",
    "scripts/ml_data_collector.py",
    "scripts/run_tests.py",
    "tasks/2025-12-15_11-09-14_3f69.json",
    "tests/unit/test_ml_export.py",
    "tests/unit/test_ml_feedback.py",
    "tests/unit/test_ml_quality.py",
    "tests/unit/test_ml_session_handoff.py"
  ],
  "insertions": 6031,
  "deletions": 63,
  "hunks": [
    {
      "file": ".claude/commands/delegate.md",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "# Delegated Task: $ARGUMENTS",
        "",
        "You are a delegated agent with NO prior context. Everything you need is in this prompt.",
        "",
        "---",
        "",
        "## ENVIRONMENT",
        "",
        "```",
        "Working Directory: /home/user/Opus-code-test",
        "Project: Cortical Text Processor (Python library for hierarchical text analysis)",
        "Key Docs: CLAUDE.md (read first if unsure about anything)",
        "```",
        "",
        "## SETUP (always do this first)",
        "",
        "```bash",
        "cd /home/user/Opus-code-test",
        "git fetch origin",
        "git status",
        "```",
        "",
        "If the task mentions a specific branch, checkout that branch:",
        "```bash",
        "git checkout <branch-name>",
        "```",
        "",
        "---",
        "",
        "## TASK CLASSIFICATION",
        "",
        "Based on the task description, identify the type:",
        "",
        "| Type | Keywords | Mode | Output |",
        "|------|----------|------|--------|",
        "| **sanity-check** | \"sanity\", \"verify\", \"pre-merge\" | READ-ONLY | PASS/FAIL table |",
        "| **code-review** | \"review\", \"audit\", \"check code\" | READ-ONLY | Findings list |",
        "| **investigate** | \"why\", \"debug\", \"find\", \"understand\" | READ-ONLY | Analysis report |",
        "| **test** | \"test\", \"coverage\", \"add tests\" | WRITE | Test files + results |",
        "| **fix** | \"fix\", \"bug\", \"broken\", \"error\" | WRITE | Fix + verification |",
        "| **implement** | \"add\", \"create\", \"implement\", \"build\" | WRITE | Code + tests |",
        "",
        "---",
        "",
        "## TASK-SPECIFIC GUIDANCE",
        "",
        "### For SANITY-CHECK tasks:",
        "```bash",
        "# 1. Verify clean state",
        "git status --short",
        "",
        "# 2. Check dependencies",
        "python /home/user/Opus-code-test/scripts/run_tests.py --check-deps",
        "",
        "# 3. Run smoke tests",
        "python /home/user/Opus-code-test/scripts/run_tests.py smoke -q",
        "",
        "# 4. Verify key commands work (if ML-related)",
        "python /home/user/Opus-code-test/scripts/ml_data_collector.py stats",
        "```",
        "",
        "### For CODE-REVIEW tasks:",
        "1. Identify files to review: `git diff --name-only origin/main..HEAD`",
        "2. Read each file, focusing on:",
        "   - Logic errors",
        "   - Edge cases not handled",
        "   - Security issues",
        "   - Performance concerns",
        "3. Do NOT make changes - report findings only",
        "",
        "### For INVESTIGATE tasks:",
        "1. Reproduce the issue first",
        "2. Read relevant code (use absolute paths)",
        "3. Form hypothesis",
        "4. Verify hypothesis with targeted tests",
        "5. Report root cause and recommended fix",
        "",
        "### For TEST tasks:",
        "1. Check current coverage: `python -m coverage report --include=\"cortical/*\"`",
        "2. Identify untested paths in target module",
        "3. Create test file at `tests/unit/test_<module>.py`",
        "4. Run new tests: `python -m unittest tests.unit.test_<module> -v`",
        "5. Verify coverage improved",
        "",
        "### For FIX tasks:",
        "1. Reproduce the bug first",
        "2. Write a failing test that captures the bug",
        "3. Implement the fix",
        "4. Verify test now passes",
        "5. Run smoke tests to check for regressions",
        "",
        "### For IMPLEMENT tasks:",
        "1. Read CLAUDE.md for patterns and conventions",
        "2. Check for similar implementations in codebase",
        "3. Implement following existing patterns",
        "4. Add tests for new code",
        "5. Update documentation if needed",
        "",
        "---",
        "",
        "## UNIVERSAL RULES",
        "",
        "1. **Absolute paths only** - Always use `/home/user/Opus-code-test/...`",
        "2. **Never use `cd`** after setup - it persists and causes confusion",
        "3. **Distinguish issues**:",
        "   - ENV issue: missing dependency, permissions (not your fault)",
        "   - CODE issue: bug, test failure, syntax error (report this)",
        "4. **No hallucinating** - If a file doesn't exist, say so",
        "5. **Ask if stuck** - If task is ambiguous, state assumptions clearly",
        "",
        "---",
        "",
        "## OUTPUT FORMAT",
        "",
        "Always end with a structured report:",
        "",
        "```markdown",
        "## Delegation Report",
        "",
        "**Task:** [one-line summary]",
        "**Type:** [sanity-check|code-review|investigate|test|fix|implement]",
        "**Branch:** [branch name or \"main\"]",
        "",
        "### Results",
        "",
        "| Check/Action | Status | Notes |",
        "|--------------|--------|-------|",
        "| ... | PASS/FAIL/SKIP | ... |",
        "",
        "### Findings (if any)",
        "- Finding 1: ...",
        "- Finding 2: ...",
        "",
        "### Changes Made (if WRITE mode)",
        "- File 1: [created|modified] - description",
        "- File 2: ...",
        "",
        "### Verdict",
        "**[PASS|FAIL|NEEDS-REVIEW]**: [one sentence summary]",
        "",
        "### Recommended Next Steps (if any)",
        "1. ...",
        "2. ...",
        "```",
        "",
        "---",
        "",
        "## FAILURE HANDLING",
        "",
        "| Problem | Action |",
        "|---------|--------|",
        "| pytest not installed | Use `python -m unittest discover -s tests -v` instead |",
        "| coverage not installed | Skip coverage, note as ENV limitation |",
        "| File not found | Verify path, check if on correct branch |",
        "| Permission denied | Note as ENV issue, not code problem |",
        "| Tests fail | Report which tests, include error output |",
        "| Ambiguous task | State your interpretation, proceed with assumptions noted |",
        "",
        "---",
        "",
        "## NOW EXECUTE",
        "",
        "Task: $ARGUMENTS",
        "",
        "Begin by classifying the task type, then follow the appropriate guidance above."
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": ".claude/commands/ml-log.md",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "# Log Chat Exchange for ML Training",
        "",
        "Log significant chat exchanges to train a project-specific micro-model.",
        "",
        "## When to Use",
        "",
        "After completing significant work (bug fix, feature implementation, debugging session), log the exchange:",
        "",
        "```bash",
        "python .claude/hooks/session_logger.py \\",
        "    --query \"USER_QUERY_HERE\" \\",
        "    --response \"BRIEF_SUMMARY_OF_RESPONSE\" \\",
        "    --files-read file1.py file2.py \\",
        "    --files-modified file3.py \\",
        "    --tools Read,Edit,Bash,Grep \\",
        "    --feedback positive",
        "```",
        "",
        "## Parameters",
        "",
        "| Parameter | Description |",
        "|-----------|-------------|",
        "| `--query` | The user's original question (2-3 sentences) |",
        "| `--response` | Summary of what was accomplished (3-5 sentences) |",
        "| `--files-read` | Files that were examined |",
        "| `--files-modified` | Files that were changed |",
        "| `--tools` | Tools used: Read, Edit, Write, Bash, Grep, Glob |",
        "| `--feedback` | positive, negative, or neutral |",
        "",
        "## Session Management",
        "",
        "```bash",
        "# Start session (optional - auto-starts on first log)",
        "python scripts/ml_data_collector.py session start",
        "",
        "# End session with summary",
        "python scripts/ml_data_collector.py session end --summary \"What was accomplished\"",
        "",
        "# Check session status",
        "python scripts/ml_data_collector.py session status",
        "```",
        "",
        "## Why This Matters",
        "",
        "This data trains a micro-model specific to THIS project that learns your coding patterns, common workflows, and project-specific terminology."
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": ".claude/commands/ml-stats.md",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "# ML Collection Statistics",
        "",
        "Show ML data collection progress and training viability estimates.",
        "",
        "## Instructions",
        "",
        "Run these commands to check ML data collection status:",
        "",
        "```bash",
        "# Show collection statistics",
        "python scripts/ml_data_collector.py stats",
        "",
        "# Show training estimates and timeline",
        "python scripts/ml_data_collector.py estimate",
        "```",
        "",
        "## What This Shows",
        "",
        "- **Data counts**: Commits, chats, actions, sessions collected",
        "- **Data sizes**: Storage used by each data type",
        "- **Training milestones**: Progress toward file prediction, commit messages, code suggestions",
        "- **Time estimates**: How long until training becomes viable",
        "",
        "## Disabling Collection",
        "",
        "To disable ML data collection:",
        "",
        "```bash",
        "export ML_COLLECTION_ENABLED=0",
        "```",
        "",
        "This stops collection but still allows viewing stats."
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": ".claude/commands/sanity-check.md",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "# Pre-merge sanity check for branch: $ARGUMENTS",
        "",
        "You have NO prior context. This is a fresh thread.",
        "",
        "## SETUP (do this first)",
        "",
        "```bash",
        "cd /home/user/Opus-code-test",
        "git fetch origin $ARGUMENTS",
        "git checkout $ARGUMENTS",
        "```",
        "",
        "## CHECKS (use absolute paths throughout)",
        "",
        "Run these in order, report results for each:",
        "",
        "1. **Git status** - should be clean",
        "   ```bash",
        "   git status --short",
        "   ```",
        "",
        "2. **Check test dependencies**",
        "   ```bash",
        "   python /home/user/Opus-code-test/scripts/run_tests.py --check-deps",
        "   ```",
        "",
        "3. **Run smoke tests**",
        "   ```bash",
        "   python /home/user/Opus-code-test/scripts/run_tests.py smoke -q",
        "   ```",
        "   - If pytest missing and can't install, try: `python -m unittest discover -s tests/smoke -v`",
        "",
        "4. **Verify ML collector commands work** (if applicable)",
        "   ```bash",
        "   python /home/user/Opus-code-test/scripts/ml_data_collector.py stats",
        "   python /home/user/Opus-code-test/scripts/ml_data_collector.py quality-report",
        "   ```",
        "",
        "5. **Check for syntax errors in new/modified files**",
        "   ```bash",
        "   git diff --name-only origin/main..HEAD -- '*.py' | head -10",
        "   # Then for each: python -m py_compile <file>",
        "   ```",
        "",
        "## RULES",
        "",
        "- Use ABSOLUTE paths only (never relative paths)",
        "- Do NOT use `cd` after setup - it persists and causes confusion",
        "- Distinguish between:",
        "  - **ENV issues**: missing pytest, permissions, etc. (not PR's fault)",
        "  - **CODE issues**: actual bugs, syntax errors, failing tests (PR's fault)",
        "- Do NOT make any changes - this is read-only verification",
        "",
        "## REPORT FORMAT",
        "",
        "```",
        "## Sanity Check: <branch-name>",
        "",
        "| Check | Result | Notes |",
        "|-------|--------|-------|",
        "| Git status | PASS/FAIL | |",
        "| Dependencies | PASS/FAIL | |",
        "| Smoke tests | PASS/FAIL | |",
        "| ML commands | PASS/FAIL/SKIP | |",
        "| Syntax check | PASS/FAIL | |",
        "",
        "**Verdict: PASS** (ready to merge)",
        "-- or --",
        "**Verdict: FAIL** (issues found)",
        "- Issue 1: ...",
        "- Issue 2: ...",
        "```",
        "",
        "## CLEANUP",
        "",
        "When done, return to original branch:",
        "```bash",
        "git checkout -",
        "```"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": ".claude/hooks/session_logger.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "#!/usr/bin/env python3",
        "\"\"\"",
        "Claude Session Logger Hook",
        "",
        "This script logs Claude Code chat sessions for ML training data collection.",
        "It can be called manually or integrated with Claude Code hooks.",
        "",
        "Usage:",
        "    # Log a complete exchange",
        "    python .claude/hooks/session_logger.py \\",
        "        --query \"How do I fix the timeout bug?\" \\",
        "        --response \"Let me look at the code...\" \\",
        "        --files-read cortical/processor.py \\",
        "        --files-modified cortical/processor.py \\",
        "        --tools Read,Edit,Bash",
        "",
        "    # Start a session",
        "    python .claude/hooks/session_logger.py --start-session",
        "",
        "    # End a session with summary",
        "    python .claude/hooks/session_logger.py --end-session --summary \"Fixed timeout bugs\"",
        "\"\"\"",
        "",
        "import json",
        "import os",
        "import sys",
        "import hashlib",
        "from datetime import datetime",
        "from pathlib import Path",
        "from typing import List, Optional",
        "",
        "# Add scripts to path",
        "sys.path.insert(0, str(Path(__file__).parent.parent.parent / \"scripts\"))",
        "",
        "try:",
        "    from ml_data_collector import (",
        "        log_chat, log_action, ensure_dirs,",
        "        SESSIONS_DIR, generate_session_id",
        "    )",
        "    ML_COLLECTOR_AVAILABLE = True",
        "except ImportError:",
        "    ML_COLLECTOR_AVAILABLE = False",
        "",
        "",
        "# Session state file",
        "SESSION_STATE_FILE = Path(\".git-ml/current_session.json\")",
        "",
        "",
        "def get_current_session() -> Optional[dict]:",
        "    \"\"\"Get the current active session.\"\"\"",
        "    if SESSION_STATE_FILE.exists():",
        "        with open(SESSION_STATE_FILE) as f:",
        "            return json.load(f)",
        "    return None",
        "",
        "",
        "def start_session() -> str:",
        "    \"\"\"Start a new logging session.\"\"\"",
        "    ensure_dirs()",
        "",
        "    session = {",
        "        \"id\": generate_session_id(),",
        "        \"started_at\": datetime.now().isoformat(),",
        "        \"exchanges\": 0,",
        "        \"files_touched\": [],",
        "        \"tools_used\": [],",
        "    }",
        "",
        "    SESSION_STATE_FILE.parent.mkdir(parents=True, exist_ok=True)",
        "    with open(SESSION_STATE_FILE, \"w\") as f:",
        "        json.dump(session, f, indent=2)",
        "",
        "    print(f\"Started session: {session['id']}\")",
        "    return session[\"id\"]",
        "",
        "",
        "def end_session(summary: Optional[str] = None):",
        "    \"\"\"End the current session and save summary.\"\"\"",
        "    session = get_current_session()",
        "    if not session:",
        "        print(\"No active session\")",
        "        return",
        "",
        "    session[\"ended_at\"] = datetime.now().isoformat()",
        "    session[\"summary\"] = summary",
        "",
        "    # Save to sessions directory",
        "    ensure_dirs()",
        "    SESSIONS_DIR.mkdir(parents=True, exist_ok=True)",
        "",
        "    filename = f\"{session['started_at'][:10]}_{session['id']}.json\"",
        "    filepath = SESSIONS_DIR / filename",
        "",
        "    with open(filepath, \"w\") as f:",
        "        json.dump(session, f, indent=2)",
        "",
        "    # Remove current session file",
        "    SESSION_STATE_FILE.unlink()",
        "",
        "    print(f\"Ended session: {session['id']}\")",
        "    print(f\"  Exchanges: {session['exchanges']}\")",
        "    print(f\"  Files: {len(session['files_touched'])}\")",
        "    print(f\"  Saved to: {filepath}\")",
        "",
        "",
        "def log_exchange(",
        "    query: str,",
        "    response: str,",
        "    files_read: Optional[List[str]] = None,",
        "    files_modified: Optional[List[str]] = None,",
        "    tools: Optional[List[str]] = None,",
        "    feedback: Optional[str] = None,",
        "):",
        "    \"\"\"Log a query/response exchange.\"\"\"",
        "    if not ML_COLLECTOR_AVAILABLE:",
        "        print(\"Warning: ml_data_collector not available\")",
        "        return",
        "",
        "    # Get or create session",
        "    session = get_current_session()",
        "    if not session:",
        "        session_id = start_session()",
        "        session = get_current_session()",
        "    else:",
        "        session_id = session[\"id\"]",
        "",
        "    # Log the chat",
        "    entry = log_chat(",
        "        query=query,",
        "        response=response,",
        "        session_id=session_id,",
        "        files_referenced=files_read or [],",
        "        files_modified=files_modified or [],",
        "        tools_used=tools or [],",
        "        user_feedback=feedback,",
        "    )",
        "",
        "    # Update session state",
        "    session[\"exchanges\"] += 1",
        "    session[\"files_touched\"] = list(set(",
        "        session[\"files_touched\"] +",
        "        (files_read or []) +",
        "        (files_modified or [])",
        "    ))",
        "    session[\"tools_used\"] = list(set(",
        "        session[\"tools_used\"] +",
        "        (tools or [])",
        "    ))",
        "",
        "    with open(SESSION_STATE_FILE, \"w\") as f:",
        "        json.dump(session, f, indent=2)",
        "",
        "    print(f\"Logged exchange: {entry.id}\")",
        "",
        "",
        "def log_tool_use(tool_name: str, target: str, success: bool = True):",
        "    \"\"\"Log a tool use action.\"\"\"",
        "    if not ML_COLLECTOR_AVAILABLE:",
        "        return",
        "",
        "    session = get_current_session()",
        "    session_id = session[\"id\"] if session else None",
        "",
        "    log_action(",
        "        action_type=f\"tool:{tool_name}\",",
        "        target=target,",
        "        session_id=session_id,",
        "        success=success,",
        "    )",
        "",
        "",
        "def main():",
        "    import argparse",
        "",
        "    parser = argparse.ArgumentParser(description=\"Claude Session Logger\")",
        "    parser.add_argument(\"--start-session\", action=\"store_true\",",
        "                        help=\"Start a new session\")",
        "    parser.add_argument(\"--end-session\", action=\"store_true\",",
        "                        help=\"End the current session\")",
        "    parser.add_argument(\"--summary\", help=\"Session summary (with --end-session)\")",
        "",
        "    parser.add_argument(\"--query\", help=\"User query text\")",
        "    parser.add_argument(\"--response\", help=\"Assistant response text\")",
        "    parser.add_argument(\"--files-read\", nargs=\"*\", default=[],",
        "                        help=\"Files that were read\")",
        "    parser.add_argument(\"--files-modified\", nargs=\"*\", default=[],",
        "                        help=\"Files that were modified\")",
        "    parser.add_argument(\"--tools\", help=\"Comma-separated list of tools used\")",
        "    parser.add_argument(\"--feedback\", choices=[\"positive\", \"negative\", \"neutral\"],",
        "                        help=\"User feedback on the exchange\")",
        "",
        "    parser.add_argument(\"--log-tool\", help=\"Log a tool use (format: tool:target)\")",
        "",
        "    args = parser.parse_args()",
        "",
        "    if args.start_session:",
        "        start_session()",
        "",
        "    elif args.end_session:",
        "        end_session(args.summary)",
        "",
        "    elif args.query and args.response:",
        "        tools = args.tools.split(\",\") if args.tools else []",
        "        log_exchange(",
        "            query=args.query,",
        "            response=args.response,",
        "            files_read=args.files_read,",
        "            files_modified=args.files_modified,",
        "            tools=tools,",
        "            feedback=args.feedback,",
        "        )",
        "",
        "    elif args.log_tool:",
        "        parts = args.log_tool.split(\":\", 1)",
        "        if len(parts) == 2:",
        "            log_tool_use(parts[0], parts[1])",
        "",
        "    else:",
        "        parser.print_help()",
        "",
        "",
        "if __name__ == \"__main__\":",
        "    main()"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": ".claude/settings.local.json",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "{",
        "  \"hooks\": {",
        "    \"Stop\": [",
        "      {",
        "        \"type\": \"command\",",
        "        \"command\": \"./scripts/ml-session-capture-hook.sh\"",
        "      }",
        "    ]",
        "  }",
        "}"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": ".claude/skills/ml-logger/SKILL.md",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "# ML Data Logger Skill",
        "",
        "Log chat exchanges and actions for training a project-specific micro-model.",
        "",
        "## When to Use",
        "",
        "Use this skill when you want to:",
        "- Log a significant query/response exchange for ML training",
        "- Start or end a development session",
        "- Check data collection progress",
        "",
        "## Quick Commands",
        "",
        "### Log Current Exchange",
        "",
        "After completing a significant task, log it:",
        "",
        "```bash",
        "python .claude/hooks/session_logger.py \\",
        "    --query \"USER_QUERY_HERE\" \\",
        "    --response \"SUMMARY_OF_RESPONSE\" \\",
        "    --files-read FILE1 FILE2 \\",
        "    --files-modified FILE3 \\",
        "    --tools Read,Edit,Bash",
        "```",
        "",
        "### Session Management",
        "",
        "```bash",
        "# Start session at beginning of work",
        "python .claude/hooks/session_logger.py --start-session",
        "",
        "# End session when done",
        "python .claude/hooks/session_logger.py --end-session --summary \"What was accomplished\"",
        "```",
        "",
        "### Check Progress",
        "",
        "```bash",
        "python scripts/ml_data_collector.py stats",
        "python scripts/ml_data_collector.py estimate",
        "```",
        "",
        "### Generate Session Handoff",
        "",
        "Create a markdown summary of the current session for context handoff:",
        "",
        "```bash",
        "python scripts/ml_data_collector.py handoff",
        "```",
        "",
        "This generates a document with:",
        "- Session summary (ID, duration, exchanges)",
        "- Key work done (summarized from queries)",
        "- Files touched (modified and referenced)",
        "- Related commits from the session",
        "- Suggested next steps (based on patterns)",
        "",
        "## What Gets Collected",
        "",
        "| Data Type | Contents | Use |",
        "|-----------|----------|-----|",
        "| **Query** | User's question/request | Input for generation |",
        "| **Response** | Assistant's answer summary | Target for generation |",
        "| **Files Read** | Which files were examined | Context prediction |",
        "| **Files Modified** | Which files were changed | Change prediction |",
        "| **Tools Used** | Which tools were invoked | Workflow prediction |",
        "| **Feedback** | User satisfaction | Quality filtering |",
        "",
        "## Integration Pattern",
        "",
        "After completing significant work:",
        "",
        "1. Summarize what the user asked",
        "2. Summarize what was done",
        "3. List files touched",
        "4. Log the exchange",
        "",
        "## Example",
        "",
        "```bash",
        "# After fixing a bug",
        "python .claude/hooks/session_logger.py \\",
        "    --query \"Fix the timeout issue in compute_all\" \\",
        "    --response \"Increased timeout from 10s to 30s in processor.py, added retry logic\" \\",
        "    --files-read cortical/processor.py tests/test_processor.py \\",
        "    --files-modified cortical/processor.py \\",
        "    --tools Read,Edit,Bash \\",
        "    --feedback positive",
        "```",
        "",
        "## Why This Matters",
        "",
        "This data trains a micro-model that learns:",
        "- YOUR project's patterns",
        "- YOUR coding style",
        "- YOUR common workflows",
        "- YOUR file relationships",
        "",
        "The model becomes a personalized assistant that understands THIS codebase deeply.",
        "",
        "## Disabling Collection",
        "",
        "To temporarily disable ML data collection:",
        "",
        "```bash",
        "export ML_COLLECTION_ENABLED=0",
        "```",
        "",
        "Stats and validation commands still work when disabled. Only collection (commit, chat, action) is blocked.",
        "",
        "## CI Integration",
        "",
        "Record CI results for commits:",
        "",
        "```bash",
        "# After CI run",
        "python scripts/ml_data_collector.py ci set \\",
        "    --commit $(git rev-parse HEAD) \\",
        "    --result pass \\",
        "    --coverage 89.5 \\",
        "    --tests-passed 150",
        "```",
        "",
        "This enables the model to learn which code changes pass/fail CI."
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": ".gitignore",
      "function": "CodeCoverage/",
      "start_line": 85,
      "lines_added": [
        "",
        "# ML training data (local, regeneratable with `python scripts/ml_data_collector.py backfill`)",
        ".git-ml/"
      ],
      "lines_removed": [],
      "context_before": [
        "TestResult.xml",
        "nunit-*.xml",
        "# Indexer progress files",
        ".index_progress.json",
        ".index_incremental_progress.json",
        "tasks/.current_session.json",
        "",
        "# Generated protobuf code (regenerated at runtime from schema.proto)",
        "cortical/proto/schema_pb2.py",
        "cortical/proto/__pycache__/"
      ],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "CLAUDE.md",
      "function": "python scripts/search_codebase.py \"what did we learn about validation\"",
      "start_line": 1322,
      "lines_added": [
        "## ML Data Collection: Project-Specific Micro-Model",
        "",
        "The project automatically collects enriched commit and chat data to train a micro-model that learns THIS project's patterns, coding style, and workflows.",
        "",
        "### What Gets Collected",
        "",
        "| Data Type | Location | Contents |",
        "|-----------|----------|----------|",
        "| **Commits** | `.git-ml/commits/` | Git history with diff hunks, temporal context, CI results |",
        "| **Chats** | `.git-ml/chats/` | Query/response pairs with files touched and tools used |",
        "| **Sessions** | `.git-ml/sessions/` | Development sessions linking chats to commits |",
        "| **Actions** | `.git-ml/actions/` | Individual tool uses and operations |",
        "",
        "**Note:** All ML data is stored in `.git-ml/` which is gitignored and regeneratable via backfill.",
        "",
        "### Quick Commands",
        "",
        "```bash",
        "# Check collection progress",
        "python scripts/ml_data_collector.py stats",
        "",
        "# Estimate when training becomes viable",
        "python scripts/ml_data_collector.py estimate",
        "",
        "# Validate collected data",
        "python scripts/ml_data_collector.py validate",
        "",
        "# Session management",
        "python scripts/ml_data_collector.py session status",
        "python scripts/ml_data_collector.py session start",
        "python scripts/ml_data_collector.py session end --summary \"What was accomplished\"",
        "",
        "# Generate session handoff document",
        "python scripts/ml_data_collector.py handoff",
        "",
        "# Record CI results",
        "python scripts/ml_data_collector.py ci set --commit abc123 --result pass --coverage 89.5",
        "",
        "# Backfill historical commits",
        "python scripts/ml_data_collector.py backfill -n 100",
        "```",
        "",
        "### Disabling Collection",
        "",
        "```bash",
        "# Disable for current session",
        "export ML_COLLECTION_ENABLED=0",
        "",
        "# Stats and validation still work when disabled",
        "```",
        "",
        "### Automatic Session Capture",
        "",
        "**Zero-friction capture via Claude Code Stop hook:**",
        "",
        "The ML data collector automatically captures complete session transcripts when Claude Code sessions end. This eliminates manual logging entirely.",
        "",
        "**Setup:**",
        "```bash",
        "# Add to ~/.claude/settings.json or project .claude/settings.json:",
        "{",
        "  \"hooks\": {",
        "    \"Stop\": [",
        "      {",
        "        \"type\": \"command\",",
        "        \"command\": \"/path/to/Opus-code-test/scripts/ml-session-capture-hook.sh\"",
        "      }",
        "    ]",
        "  }",
        "}",
        "```",
        "",
        "**What gets captured automatically:**",
        "- Full query/response pairs from the transcript",
        "- All tool uses (Task, Read, Edit, Bash, Grep, etc.)",
        "- Files referenced and modified",
        "- Thinking blocks (if present)",
        "- Session linkage to commits",
        "",
        "**Process transcript manually:**",
        "```bash",
        "# Process a specific transcript file",
        "python scripts/ml_data_collector.py transcript --file /path/to/transcript.jsonl",
        "",
        "# Dry run (show what would be captured without saving)",
        "python scripts/ml_data_collector.py transcript --file /path/to/transcript.jsonl --dry-run --verbose",
        "```",
        "",
        "### Integration",
        "",
        "Data collection is automatic via hooks:",
        "- **Stop hook**: Captures full session transcripts with all exchanges (recommended)",
        "- **post-commit**: Captures commit metadata with diff hunks",
        "- **pre-push**: Reports collection stats",
        "",
        "See `.claude/skills/ml-logger/SKILL.md` for detailed logging usage.",
        "",
        "---",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "",
        "When completing a task, consider creating a memory entry from the retrospective:",
        "- What was learned?",
        "- What connections were made?",
        "- What should future developers know?",
        "",
        "See `docs/text-as-memories.md` for the full guide.",
        "",
        "---",
        ""
      ],
      "context_after": [
        "## File Quick Links",
        "",
        "- **Main API**: `cortical/processor/` - `CorticalTextProcessor` class (split into mixins)",
        "- **Graph algorithms**: `cortical/analysis.py` - PageRank, TF-IDF, clustering",
        "- **Search**: `cortical/query/` - query expansion, document retrieval (split into 8 modules)",
        "- **Data structures**: `cortical/minicolumn.py` - `Minicolumn`, `Edge`",
        "- **Configuration**: `cortical/config.py` - `CorticalConfig` dataclass",
        "- **Tests**: `tests/test_processor.py` - most comprehensive test file",
        "- **Demo**: `showcase.py` - interactive demonstration",
        ""
      ],
      "change_type": "add"
    },
    {
      "file": "README.md",
      "function": null,
      "start_line": 1,
      "lines_added": [
        "## What is this?",
        "",
        "**Cortical Text Processor** is a zero-dependency Python library for hierarchical text analysis and semantic search. Despite the neocortex-inspired naming, it uses proven information retrieval algorithms—**PageRank**, **TF-IDF**, and **Louvain clustering**—not neural networks.",
        "",
        "**Key use cases:**",
        "- **Semantic search**: Find documents by meaning, not just keywords",
        "- **Code search**: Search codebases with identifier splitting and programming concept expansion",
        "- **Document retrieval**: RAG system support with chunk-level passage retrieval",
        "- **Knowledge analysis**: Detect gaps, outliers, and missing connections in your corpus",
        "",
        "**Zero dependencies.** Just copy the `cortical/` folder into your project and go.",
        "",
        "## Quick Example",
        "",
        "```python",
        "from cortical import CorticalTextProcessor",
        "",
        "# Create processor and add documents",
        "processor = CorticalTextProcessor()",
        "processor.process_document(\"doc1\", \"Neural networks process information hierarchically.\")",
        "processor.process_document(\"doc2\", \"The brain uses layers of neurons for processing.\")",
        "",
        "# Build the semantic network",
        "processor.compute_all()",
        "",
        "# Search with automatic query expansion",
        "results = processor.find_documents_for_query(\"neural processing\")",
        "print(results)  # [('doc1', 0.877), ('doc2', 0.832)]",
        "",
        "# Save for later",
        "processor.save(\"my_corpus.pkl\")",
        "```",
        "## Why \"Cortical\"?",
        ""
      ],
      "lines_removed": [
        "A neocortex-inspired text processing library with **zero external dependencies** for semantic analysis, document retrieval, and knowledge gap detection."
      ],
      "context_before": [
        "# Cortical Text Processor",
        "",
        "![Python 3.8+](https://img.shields.io/badge/python-3.8%2B-blue.svg)",
        "![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)",
        "![Tests](https://img.shields.io/badge/tests-3150%20passing-brightgreen.svg)",
        "![Coverage](https://img.shields.io/badge/coverage-%3E89%25-brightgreen.svg)",
        "![Zero Dependencies](https://img.shields.io/badge/dependencies-zero-orange.svg)",
        "![Fact Check](https://img.shields.io/badge/fact--check-94%25%20verified-blue.svg)",
        ""
      ],
      "context_after": [
        "",
        "---",
        "",
        "> *\"What if we built a text search engine the way evolution built a brain?\"*",
        "",
        "Your visual cortex doesn't grep through pixels looking for cats. It builds hierarchies—edges become patterns, patterns become shapes, shapes become objects. This library applies the same principle to text.",
        "",
        "Feed it documents. It tokenizes them into \"minicolumns\" (Layer 0), connects co-occurring words through Hebbian learning (\"neurons that fire together, wire together\"), clusters them into concepts (Layer 2), and links documents by shared meaning (Layer 3). The result: a graph that understands your corpus well enough to expand queries, complete analogies, and tell you where your knowledge has gaps.",
        "",
        "No PyTorch. No transformers. No API keys. Just 3100+ tests, 20,000+ lines of pure Python, and a data structure that would make a neuroscientist squint approvingly.",
        "",
        "---",
        ""
      ],
      "change_type": "modify"
    },
    {
      "file": "README.md",
      "function": "This library provides a biologically-inspired approach to text processing, organ",
      "start_line": 35,
      "lines_added": [
        "- **Code Search**: Identifier splitting, programming concept expansion, and intent-based queries",
        "- **Semantic Fingerprinting**: Compare document similarity with explanations",
        "- **Fast Search**: Pre-built indexes for 2-3x faster repeated queries",
        "- **Incremental Updates**: Add documents without full recomputation"
      ],
      "lines_removed": [],
      "context_before": [
        "## Key Features",
        "",
        "- **Hierarchical Processing**: Feedforward, feedback, and lateral connections like the neocortex",
        "- **PageRank Importance**: Graph-based term importance with relation-weighted and cross-layer propagation",
        "- **TF-IDF Weighting**: Statistical term distinctiveness with per-document occurrence tracking",
        "- **Corpus-Derived Semantics**: Pattern-based commonsense relation extraction without external knowledge bases",
        "- **Graph Embeddings**: Multiple embedding methods (adjacency, spectral, random walk) with semantic retrofitting",
        "- **ConceptNet-Style Relations**: Typed edges (IsA, HasA, PartOf, etc.) with multi-hop inference",
        "- **Concept Inheritance**: IsA hierarchy propagation for concept properties",
        "- **Analogy Completion**: Relation matching and vector arithmetic for analogical reasoning"
      ],
      "context_after": [
        "- **Gap Detection**: Find weak spots and isolated documents in your corpus",
        "- **Query Expansion**: Smart retrieval with synonym handling and semantic relations",
        "- **RAG System Support**: Chunk-level passage retrieval, document metadata, and multi-stage ranking",
        "- **Zero Dependencies**: Pure Python, no pip installs required",
        "",
        "## Use Cases & When to Use",
        "",
        "### Ideal Use Cases",
        "",
        "| Use Case | Why It's a Good Fit |"
      ],
      "change_type": "add"
    },
    {
      "file": "docs/algorithms.md",
      "function": "This document describes the information retrieval algorithms implemented in the",
      "start_line": 4,
      "lines_added": [
        "| Query Expansion | Semantic search | `query/expansion.py` |"
      ],
      "lines_removed": [
        "| Query Expansion | Semantic search | `query.py:55-176` |"
      ],
      "context_before": [
        "",
        "## Overview",
        "",
        "The system uses standard IR algorithms with a hierarchical, layered architecture:",
        "",
        "| Algorithm | Purpose | Primary File |",
        "|-----------|---------|--------------|",
        "| PageRank | Importance scoring | `analysis.py:22-95` |",
        "| TF-IDF | Term weighting | `analysis.py:394-433` |",
        "| Label Propagation | Concept clustering | `analysis.py:502-636` |"
      ],
      "context_after": [
        "| Relation Extraction | Knowledge building | `semantics.py:109-186` |",
        "",
        "---",
        "",
        "## PageRank - Importance Scoring",
        "",
        "PageRank measures term importance based on network structure. Terms connected to other important terms receive higher scores.",
        "",
        "### Standard PageRank",
        ""
      ],
      "change_type": "modify"
    },
    {
      "file": "docs/algorithms.md",
      "function": "After clustering, each cluster becomes a concept in Layer 2:",
      "start_line": 136,
      "lines_added": [
        "**Location:** `query/expansion.py`",
        "**Location:** `query/expansion.py`"
      ],
      "lines_removed": [
        "**Location:** `query.py:55-176`",
        "**Location:** `query.py:407-531`"
      ],
      "context_before": [
        "- Named after top 3 members by PageRank: `\"neural/networks/learning\"`",
        "- Connected bidirectionally to member tokens",
        "- Aggregates member properties (documents, activation, pagerank)",
        "",
        "---",
        "",
        "## Query Expansion",
        "",
        "### Basic Expansion",
        ""
      ],
      "context_after": [
        "",
        "Expands query terms to find semantically related words.",
        "",
        "**Three Expansion Methods:**",
        "",
        "1. **Lateral Connections** - Direct word associations from co-occurrence",
        "   - Score: `connection_weight × neighbor_pagerank × 0.6`",
        "",
        "2. **Concept Clusters** - Words from same semantic category",
        "   - Score: `concept_pagerank × member_pagerank × 0.4`",
        "",
        "3. **Code Concepts** - Programming synonyms (optional)",
        "   - Example: \"get\" → \"fetch\", \"load\", \"retrieve\"",
        "   - Score: `0.6`",
        "",
        "### Multi-Hop Expansion",
        "",
        "",
        "Finds related terms through transitive relation chains.",
        "",
        "**Example Chains:**",
        "- `\"dog\" → IsA → \"animal\" → HasProperty → \"living\"`",
        "- `\"car\" → PartOf → \"engine\" → UsedFor → \"transportation\"`",
        "",
        "**Chain Validity Scoring:**",
        "Not all relation chains are equally valid:",
        "```"
      ],
      "change_type": "modify"
    },
    {
      "file": "docs/algorithms.md",
      "function": "Not all relation chains are equally valid:",
      "start_line": 178,
      "lines_added": [
        "**Location:** `query/intent.py`"
      ],
      "lines_removed": [
        "**Location:** `query.py:179-284`"
      ],
      "context_before": [
        "(Antonym, Antonym): 0.3   - Double negation, unreliable",
        "```",
        "",
        "**Parameters:**",
        "- `max_hops`: Maximum chain depth (default 2)",
        "- `decay_factor`: Weight decay per hop (default 0.5)",
        "- `min_path_score`: Minimum chain validity (default 0.2)",
        "",
        "### Intent-Based Query Parsing",
        ""
      ],
      "context_after": [
        "",
        "Parses natural language queries to extract intent.",
        "",
        "**Intent Types:**",
        "- `\"where\"` → `location` (find file/function location)",
        "- `\"how\"` → `implementation` (find implementation details)",
        "- `\"what\"` → `definition` (find definitions)",
        "- `\"why\"` → `rationale` (find explanations/comments)",
        "- `\"when\"` → `lifecycle` (find lifecycle events)",
        ""
      ],
      "change_type": "modify"
    },
    {
      "file": "docs/architecture.md",
      "function": "This document describes both the **module architecture** (how code files interac",
      "start_line": 9,
      "lines_added": [
        "5. **Orchestration Layer** - processor/ package coordinates everything (mixin-based composition)",
        "│  │          processor/ package (Public API)                   │ │",
        "│  │  - CorticalTextProcessor class (mixin composition)         │ │",
        "│  │  - CoreMixin, DocumentsMixin, ComputeMixin, etc.           │ │"
      ],
      "lines_removed": [
        "5. **Orchestration Layer** - processor.py coordinates everything",
        "│  │              processor.py (Public API)                     │ │",
        "│  │  - CorticalTextProcessor class                             │ │"
      ],
      "context_before": [
        "This section maps the codebase structure, showing which modules depend on which, and how components interact.",
        "",
        "## Module Dependency Overview",
        "",
        "The codebase is organized into five architectural layers:",
        "",
        "1. **Foundation Layer** - Data structures and utilities (no cortical dependencies)",
        "2. **Algorithm Layer** - Domain logic for analysis, semantics, embeddings",
        "3. **Query Layer** - Modular search and retrieval functions",
        "4. **Persistence Layer** - Save/load and git-friendly chunk storage"
      ],
      "context_after": [
        "",
        "### Complete Module Dependency Graph",
        "",
        "```",
        "┌─────────────────────────────────────────────────────────────────┐",
        "│                       ORCHESTRATION LAYER                        │",
        "│                                                                  │",
        "│  ┌────────────────────────────────────────────────────────────┐ │",
        "│  │  - Coordinates all components                              │ │",
        "│  │  - Staleness tracking                                      │ │",
        "│  └───────┬──────────────────────────────────────────────────┬─┘ │",
        "└──────────┼──────────────────────────────────────────────────┼───┘",
        "           │                                                  │",
        "           ▼                                                  ▼",
        "┌──────────────────────────────────────────────┐   ┌─────────────────┐",
        "│          ALGORITHM LAYER                     │   │ PERSISTENCE     │",
        "│                                              │   │                 │",
        "│  ┌──────────────┐  ┌──────────────┐         │   │ persistence.py  │"
      ],
      "change_type": "modify"
    },
    {
      "file": "docs/architecture.md",
      "function": "The codebase is organized into five architectural layers:",
      "start_line": 103,
      "lines_added": [
        "**processor/ package** (2,817 lines total)",
        "- **Pattern**: Mixin-based composition - functionality split across focused modules",
        "- **Structure**:",
        "  - `__init__.py` (63 lines) - Re-exports CorticalTextProcessor",
        "  - `core.py` (108 lines) - CoreMixin: initialization, staleness tracking",
        "  - `documents.py` (454 lines) - DocumentsMixin: process_document, add/remove",
        "  - `compute.py` (1,033 lines) - ComputeMixin: compute_all, PageRank, TF-IDF, clustering",
        "  - `query_api.py` (699 lines) - QueryMixin: find_documents_for_query, expand_query",
        "  - `introspection.py` (217 lines) - IntrospectionMixin: fingerprints, gaps, summaries",
        "  - `persistence_api.py` (243 lines) - PersistenceMixin: save, load, export",
        "  - `process_document()` - Add documents to corpus (DocumentsMixin)",
        "  - `compute_all()` - Run all analysis phases (ComputeMixin)",
        "  - `find_documents_for_query()` - Search wrapper (QueryMixin)",
        "  - `find_passages_for_query()` - RAG wrapper (QueryMixin)",
        "  - Staleness tracking for incremental updates (CoreMixin)"
      ],
      "lines_removed": [
        "**processor.py** (2,301 lines)",
        "- **Pattern**: Facade - delegates to specialized modules",
        "  - `process_document()` - Add documents to corpus",
        "  - `compute_all()` - Run all analysis phases",
        "  - `find_documents_for_query()` - Search wrapper",
        "  - `find_passages_for_query()` - RAG wrapper",
        "  - Staleness tracking for incremental updates"
      ],
      "context_before": [
        "│  │  │ - Stop words │  │ - Expansion  │                        │ │",
        "│  │  └──────────────┘  └──────────────┘                        │ │",
        "│  └─────────────────────────────────────────────────────────────┘ │",
        "└───────────────────────────────────────────────────────────────────┘",
        "```",
        "",
        "## Component Responsibilities",
        "",
        "### Orchestration Layer",
        ""
      ],
      "context_after": [
        "- **Role**: Main orchestrator and public API",
        "- **Key Functions**:",
        "- **Imports**: All other modules (analysis, semantics, embeddings, gaps, fingerprint, query, persistence)",
        "- **Used By**: External users, scripts",
        "",
        "### Algorithm Layer",
        "",
        "**analysis.py** (1,123 lines)",
        "- **Role**: Graph algorithms",
        "- **Key Functions**:",
        "  - `compute_pagerank()` - Importance scoring",
        "  - `compute_tfidf()` - Term weighting"
      ],
      "change_type": "modify"
    },
    {
      "file": "docs/architecture.md",
      "function": "The query layer is split into focused submodules, all re-exported from `query/__",
      "start_line": 277,
      "lines_added": [
        "        │       processor/documents.py       │",
        "        │    (DocumentsMixin)                │"
      ],
      "lines_removed": [
        "        │       processor.py                 │"
      ],
      "context_before": [
        "                         ▼",
        "                  ┌─────────────┐",
        "                  │ tokenizer.py│",
        "                  │  Tokenize   │",
        "                  │  + Stem     │",
        "                  │  + Filter   │",
        "                  └──────┬──────┘",
        "                         │",
        "                         ▼",
        "        ┌────────────────────────────────────┐"
      ],
      "context_after": [
        "        │    process_document()              │",
        "        └────────────────┬───────────────────┘",
        "                         │",
        "         ┌───────────────┼───────────────┐",
        "         │               │               │",
        "         ▼               ▼               ▼",
        "    ┌────────┐     ┌─────────┐     ┌─────────┐",
        "    │Layer 0 │     │ Layer 1 │     │ Layer 3 │",
        "    │ TOKENS │────▶│ BIGRAMS │     │   DOC   │",
        "    └────────┘     └─────────┘     └─────────┘",
        "         │"
      ],
      "change_type": "modify"
    },
    {
      "file": "docs/architecture.md",
      "function": "The query layer is split into focused submodules, all re-exported from `query/__",
      "start_line": 349,
      "lines_added": [
        "### Pattern 1: Mixin-Based Composition",
        "The processor package uses mixin composition to organize functionality:",
        "# processor/__init__.py assembles the complete class",
        "class CorticalTextProcessor(",
        "    CoreMixin,",
        "    DocumentsMixin,",
        "    ComputeMixin,",
        "    QueryMixin,",
        "    IntrospectionMixin,",
        "    PersistenceMixin",
        "):",
        "    pass",
        "",
        "# processor/compute.py (ComputeMixin) delegates to analysis.py",
        "**Benefits**: Clean separation of concerns, modular testing, each file stays focused (<1100 lines)"
      ],
      "lines_removed": [
        "### Pattern 1: Orchestrator Pattern",
        "processor.py acts as a facade, delegating to specialized modules:",
        "# processor.py delegates to analysis.py",
        "**Benefits**: Clean public API, focused modules, easy testing"
      ],
      "context_before": [
        "         └───────────────┼───────────────┘",
        "                         │",
        "                    Ranked Results",
        "                         │",
        "                         ▼",
        "                   Return to User",
        "```",
        "",
        "## Interaction Patterns",
        ""
      ],
      "context_after": [
        "",
        "",
        "```python",
        "def compute_importance(self):",
        "    pagerank_scores = analysis.compute_pagerank(",
        "        self.layers[CorticalLayer.TOKENS],",
        "        damping=self.config.pagerank_damping",
        "    )",
        "    # Update minicolumns with scores",
        "```",
        "",
        "",
        "### Pattern 2: Layered Processing",
        "",
        "All algorithm modules operate on the same layer abstraction:",
        "",
        "```python",
        "# Common pattern across analysis, semantics, embeddings, gaps",
        "def some_algorithm(",
        "    layers: Dict[CorticalLayer, HierarchicalLayer],",
        "    **kwargs"
      ],
      "change_type": "modify"
    },
    {
      "file": "docs/architecture.md",
      "function": "query/__init__.py     ← Re-exports all public symbols",
      "start_line": 403,
      "lines_added": [
        "The processor package (CoreMixin + ComputeMixin) tracks which computations need recomputation:",
        "# processor/documents.py (DocumentsMixin) marks all stale when documents change",
        "# processor/compute.py (ComputeMixin) only recomputes stale components"
      ],
      "lines_removed": [
        "processor.py tracks which computations need recomputation:",
        "# Mark all stale when documents change",
        "# compute_all() only recomputes stale components"
      ],
      "context_before": [
        "├── chunking.py       ← Text splitting",
        "├── intent.py         ← Intent parsing",
        "├── definitions.py    ← Definition-specific",
        "└── analogy.py        ← Analogy completion",
        "```",
        "",
        "**Benefits**: Files stay under 400 lines, clear boundaries, easy to extend",
        "",
        "### Pattern 4: Staleness Tracking",
        ""
      ],
      "context_after": [
        "",
        "```python",
        "def process_document(self, doc_id, content):",
        "    # ... process ...",
        "    self._mark_all_stale()",
        "",
        "def compute_all(self):",
        "    if self.is_stale(self.COMP_TFIDF):",
        "        self.compute_tfidf()",
        "    if self.is_stale(self.COMP_PAGERANK):",
        "        self.compute_importance()",
        "```",
        "",
        "**Benefits**: Avoids redundant computation, supports incremental updates",
        "",
        "## Mermaid Diagrams"
      ],
      "change_type": "modify"
    },
    {
      "file": "docs/architecture.md",
      "function": "graph TD",
      "start_line": 456,
      "lines_added": [
        "    processor[processor/<br/>CorticalTextProcessor<br/>(mixin composition)]"
      ],
      "lines_removed": [
        "    processor[processor.py<br/>CorticalTextProcessor]"
      ],
      "context_before": [
        "    query_ranking[query/ranking.py<br/>Multi-stage rank]",
        "    query_chunking[query/chunking.py<br/>Text chunking]",
        "    query_intent[query/intent.py<br/>Intent parsing]",
        "    query_analogy[query/analogy.py<br/>Analogies]",
        "",
        "    %% Persistence Layer",
        "    persistence[persistence.py<br/>Save/load]",
        "    chunk_index[chunk_index.py<br/>Chunk storage]",
        "",
        "    %% Orchestration"
      ],
      "context_after": [
        "",
        "    %% Dependencies",
        "    layers --> minicolumn",
        "",
        "    analysis --> layers",
        "    analysis --> minicolumn",
        "    analysis --> constants",
        "",
        "    semantics --> layers",
        "    semantics --> minicolumn"
      ],
      "change_type": "modify"
    },
    {
      "file": "docs/architecture.md",
      "function": "minicolumn.feedback_connections: Dict[str, float]",
      "start_line": 716,
      "lines_added": [
        "**Location:** `processor/documents.py` (DocumentsMixin)"
      ],
      "lines_removed": [
        "**Location:** `processor.py:54-137`"
      ],
      "context_before": [
        "- Token → containing bigrams: `\"neural\" → [\"neural networks\", \"neural processing\"]`",
        "- Token → containing concepts: `\"neural\" → [\"neural/networks/learning\"]`",
        "- Token → containing documents: `\"neural\" → [\"doc1\", \"doc2\"]`",
        "",
        "---",
        "",
        "## Data Flow",
        "",
        "### Document Processing",
        ""
      ],
      "context_after": [
        "",
        "When a document is processed:",
        "",
        "```",
        "INPUT: \"Neural networks process data.\"",
        "",
        "1. TOKENIZATION",
        "   → [\"neural\", \"networks\", \"process\", \"data\"]",
        "   → Create Layer 0 minicolumns",
        ""
      ],
      "change_type": "modify"
    },
    {
      "file": "docs/architecture.md",
      "function": "INPUT: \"Neural networks process data.\"",
      "start_line": 748,
      "lines_added": [
        "**Location:** `processor/compute.py` (ComputeMixin - `compute_all()`)"
      ],
      "lines_removed": [
        "**Location:** `processor.py:452-596` (`compute_all()`)"
      ],
      "context_before": [
        "",
        "5. BIGRAM-TOKEN CONNECTIONS",
        "   → bigram.feedforward_connections[\"L0_neural\"] = 1.0",
        "   → token.feedback_connections[\"L1_neural networks\"] = 1.0",
        "```",
        "",
        "**Important:** Bigrams use SPACE separators: `\"neural networks\"`, not `\"neural_networks\"`.",
        "",
        "### Network Computation",
        ""
      ],
      "context_after": [
        "",
        "After processing documents, compute the full network:",
        "",
        "```",
        "1. ACTIVATION PROPAGATION",
        "   → Spread activation through connections",
        "   → Simulates information flow",
        "",
        "2. PAGERANK",
        "   → Compute importance for Layer 0 and Layer 1"
      ],
      "change_type": "modify"
    },
    {
      "file": "docs/architecture.md",
      "function": "After processing documents, compute the full network:",
      "start_line": 789,
      "lines_added": [
        "**Location:** `processor/query_api.py` (QueryMixin) + `query/` package"
      ],
      "lines_removed": [
        "**Location:** `query.py`"
      ],
      "context_before": [
        "",
        "7. CONCEPT CONNECTIONS",
        "   → Connect Layer 2 concepts by:",
        "     - Document overlap (Jaccard similarity)",
        "     - Semantic relations between members",
        "     - Embedding similarity (optional)",
        "```",
        "",
        "### Query Flow",
        ""
      ],
      "context_after": [
        "",
        "When a query is executed:",
        "",
        "```",
        "INPUT: \"neural networks\"",
        "",
        "1. TOKENIZE QUERY",
        "   → [\"neural\", \"networks\"]",
        "",
        "2. EXPAND QUERY"
      ],
      "change_type": "modify"
    },
    {
      "file": "docs/architecture.md",
      "function": "INPUT: \"neural networks\"",
      "start_line": 896,
      "lines_added": [
        "Used throughout `analysis.py` and `query/` modules.",
        "**Location:** `processor/core.py` (CoreMixin)",
        "**Location:** `processor/core.py` (CoreMixin)",
        "| process_document() | `processor/documents.py` | DocumentsMixin |",
        "| compute_all() | `processor/compute.py` | ComputeMixin |"
      ],
      "lines_removed": [
        "Used throughout `analysis.py` and `query.py`.",
        "**Location:** `processor.py:49`",
        "**Location:** `processor.py:51-52`",
        "| process_document() | `processor.py` | 54-137 |",
        "| compute_all() | `processor.py` | 452-596 |"
      ],
      "context_before": [
        "```python",
        "# WRONG - O(n):",
        "for col in layer.minicolumns.values():",
        "    if col.id == target_id:",
        "        neighbor = col",
        "",
        "# RIGHT - O(1):",
        "neighbor = layer.get_by_id(target_id)",
        "```",
        ""
      ],
      "context_after": [
        "",
        "### Staleness Tracking",
        "",
        "",
        "```python",
        "self._stale_computations: set",
        "```",
        "",
        "Tracks which computations need rerunning after corpus changes:",
        "- `COMP_TFIDF`",
        "- `COMP_PAGERANK`",
        "- `COMP_ACTIVATION`",
        "- `COMP_DOC_CONNECTIONS`",
        "- `COMP_BIGRAM_CONNECTIONS`",
        "- `COMP_CONCEPTS`",
        "",
        "### Query Caching",
        "",
        "",
        "```python",
        "self._query_expansion_cache: Dict[str, Dict[str, float]]",
        "self._query_cache_max_size: int = 100",
        "```",
        "",
        "LRU cache for query expansion results. Cleared after `compute_all()`.",
        "",
        "---",
        "",
        "## File Reference",
        "",
        "| Component | File | Lines |",
        "|-----------|------|-------|",
        "| CorticalLayer enum | `layers.py` | 21-56 |",
        "| HierarchicalLayer | `layers.py` | 59-273 |",
        "| Minicolumn | `minicolumn.py` | 56-357 |",
        "| Edge | `minicolumn.py` | 16-53 |",
        "| Tokenizer | `tokenizer.py` | Full file |",
        "",
        "---",
        "",
        "## Visual Summary",
        "",
        "```",
        "┌─────────────────────────────────────────────────────────────┐",
        "│                    Layer 3: DOCUMENTS                        │",
        "│  ┌─────────┐    ┌─────────┐                                 │"
      ],
      "change_type": "modify"
    },
    {
      "file": "docs/claude-usage.md",
      "function": "You want to see how something is actually implemented, not just read the file di",
      "start_line": 71,
      "lines_added": [
        "If you already know the file path (e.g., `cortical/processor/compute.py`), reading directly is faster than searching."
      ],
      "lines_removed": [
        "If you already know the file path (e.g., `cortical/processor.py`), reading directly is faster than searching."
      ],
      "context_before": [
        "",
        "**Cost consideration:** Search is fast (~1 second for typical queries), so it's efficient for exploratory research.",
        "",
        "---",
        "",
        "## When to Use Direct File Reading",
        "",
        "Use **direct file reading** (Read tool) when you:",
        "",
        "### 1. Know the exact file location"
      ],
      "context_after": [
        "",
        "### 2. Need the complete file context",
        "When you need to see the entire file structure, imports, and all methods in a class, reading the file is more efficient than multiple targeted searches.",
        "",
        "### 3. Are implementing a pattern you've already found",
        "After a search tells you the file location, switch to direct reading to implement your changes.",
        "",
        "### 4. Need accurate line numbers for edits",
        "While search provides file:line references, reading the file confirms the exact content at those lines.",
        ""
      ],
      "change_type": "modify"
    },
    {
      "file": "docs/claude-usage.md",
      "function": "python scripts/search_codebase.py \"PageRank\" --fast",
      "start_line": 544,
      "lines_added": [
        "git log -p cortical/query/ | grep \"function_name\""
      ],
      "lines_removed": [
        "git log -p cortical/query.py | grep \"function_name\""
      ],
      "context_before": [
        "**Workaround:** Use without `--fast` for specific passages, or read the file directly after getting the filename.",
        "",
        "### Limitation 5: Index Doesn't Cover Git History",
        "",
        "**Problem:** You can't search for how code looked before changes.",
        "",
        "**Reason:** The index is built from current files only.",
        "",
        "**Workaround:** Use git history for temporal queries:",
        "```bash"
      ],
      "context_after": [
        "```",
        "",
        "### Limitation 6: Documentation May Be Outdated",
        "",
        "**Problem:** Docs in the index reflect what was written, not necessarily what code actually does.",
        "",
        "```",
        "Query: \"how layer computation works\"",
        "Result: May find outdated documentation",
        "```"
      ],
      "change_type": "modify"
    },
    {
      "file": "docs/code-of-ethics.md",
      "function": "Testing is a discovery process. Issues found during dog-fooding are **not distra",
      "start_line": 24,
      "lines_added": [
        "  - **File**: `/home/user/Opus-code-test/cortical/query/passages.py:find_passages_for_query`"
      ],
      "lines_removed": [
        "  - **File**: `/home/user/Opus-code-test/cortical/query.py:find_passages_for_query`"
      ],
      "context_before": [
        "",
        "**Requirements:**",
        "- Add tasks to `TASK_LIST.md` immediately upon discovery",
        "- Include severity/priority assessment",
        "- Reference the test case or usage scenario that revealed it",
        "- Link to related code locations with absolute paths",
        "",
        "**Example:**",
        "```markdown",
        "- [ ] **Task #X**: Fix passage-level search doc-type boosting"
      ],
      "context_after": [
        "  - **Issue**: Document-level search applies doc-type boosting, but passage-level search does not",
        "  - **Discovered**: Dog-fooding test with code search queries",
        "  - **Priority**: Medium - reduces search quality for mixed-type corpora",
        "```",
        "",
        "### No \"it works well enough\" - if there's a limitation, document it",
        "",
        "Undocumented limitations are landmines for future developers. They waste time, create confusion, and erode trust in the codebase.",
        "",
        "**Requirements:**"
      ],
      "change_type": "modify"
    },
    {
      "file": "docs/devex-tools.md",
      "function": "Overall Health: Good (64/100)",
      "start_line": 77,
      "lines_added": [
        "python scripts/find_similar.py cortical/processor/__init__.py",
        "python scripts/find_similar.py cortical/processor/__init__.py --top 10",
        "python scripts/find_similar.py cortical/processor/__init__.py --verbose",
        "python scripts/find_similar.py cortical/processor/__init__.py --explain"
      ],
      "lines_removed": [
        "python scripts/find_similar.py cortical/processor.py",
        "python scripts/find_similar.py cortical/processor.py --top 10",
        "python scripts/find_similar.py cortical/processor.py --verbose",
        "python scripts/find_similar.py cortical/processor.py --explain"
      ],
      "context_before": [
        "---",
        "",
        "## 2. Find Similar Code",
        "",
        "**Purpose:** Locate code blocks similar to a given file or text snippet using semantic fingerprinting.",
        "",
        "### Basic Usage",
        "",
        "```bash",
        "# Find similar to a file"
      ],
      "context_after": [
        "",
        "# More results",
        "",
        "# Show full passages",
        "",
        "# Explain why they're similar",
        "",
        "# Find similar to text snippet",
        "python scripts/find_similar.py --text \"def compute_pagerank(graph, damping=0.85):\"",
        "",
        "# Adjust sensitivity",
        "python scripts/find_similar.py file.py --min-similarity 0.3",
        "```",
        "",
        "### How It Works",
        ""
      ],
      "change_type": "modify"
    },
    {
      "file": "docs/devex-tools.md",
      "function": "python scripts/find_similar.py file.py --min-similarity 0.3",
      "start_line": 139,
      "lines_added": [
        "python scripts/explain_code.py cortical/processor/compute.py",
        "python scripts/explain_code.py cortical/processor/compute.py --verbose",
        "python scripts/explain_code.py cortical/processor/compute.py --relations"
      ],
      "lines_removed": [
        "python scripts/explain_code.py cortical/processor.py",
        "python scripts/explain_code.py cortical/processor.py --verbose",
        "python scripts/explain_code.py cortical/processor.py --relations"
      ],
      "context_before": [
        "---",
        "",
        "## 3. Explain This Code",
        "",
        "**Purpose:** Analyze and explain what a code file is about using semantic analysis.",
        "",
        "### Basic Usage",
        "",
        "```bash",
        "# Analyze a file"
      ],
      "context_after": [
        "",
        "# Detailed analysis",
        "",
        "# Show semantic relations",
        "",
        "# Analyze text directly",
        "python scripts/explain_code.py --text \"your code snippet here\"",
        "```",
        "",
        "### What It Shows",
        "",
        "- **Key Terms:** Most important terms by TF-IDF weight",
        "- **Primary Concepts:** Programming concepts detected (e.g., iteration, storage, auth)",
        "- **Concept Clusters:** Which concept clusters this file contributes to"
      ],
      "change_type": "modify"
    },
    {
      "file": "docs/devex-tools.md",
      "function": "python scripts/explain_code.py --text \"your code snippet here\"",
      "start_line": 201,
      "lines_added": [
        "python scripts/suggest_related.py cortical/processor/__init__.py",
        "python scripts/suggest_related.py cortical/processor/__init__.py --top 15",
        "python scripts/suggest_related.py cortical/processor/__init__.py --verbose",
        "python scripts/suggest_related.py cortical/processor/__init__.py --imports-only"
      ],
      "lines_removed": [
        "python scripts/suggest_related.py cortical/processor.py",
        "python scripts/suggest_related.py cortical/processor.py --top 15",
        "python scripts/suggest_related.py cortical/processor.py --verbose",
        "python scripts/suggest_related.py cortical/processor.py --imports-only"
      ],
      "context_before": [
        "---",
        "",
        "## 4. Suggest Related Files",
        "",
        "**Purpose:** Find files related to a given file through imports, concepts, and semantic similarity.",
        "",
        "### Basic Usage",
        "",
        "```bash",
        "# Find related files"
      ],
      "context_after": [
        "",
        "# More suggestions",
        "",
        "# Detailed information",
        "",
        "# Only import relationships",
        "```",
        "",
        "### Relationship Types",
        "",
        "1. **Imports:** Files this file imports",
        "2. **Imported By:** Files that import this file",
        "3. **Shared Concepts:** Files that share concept clusters",
        "4. **Semantically Similar:** Files with similar semantic fingerprints",
        "",
        "### How It Works"
      ],
      "change_type": "modify"
    },
    {
      "file": "docs/devex-tools.md",
      "function": "python scripts/suggest_related.py cortical/processor.py --imports-only",
      "start_line": 233,
      "lines_added": [
        "  [CODE] cortical/query/"
      ],
      "lines_removed": [
        "  [CODE] cortical/query.py"
      ],
      "context_before": [
        "- **Concept Matching:** Finds files in the same concept clusters",
        "- **Semantic Similarity:** Compares full-file fingerprints",
        "- **Combined Ranking:** Presents results by relationship type",
        "",
        "### Example Output",
        "",
        "```",
        "📦 Imports (4 files):",
        "  [CODE] cortical/analysis.py",
        "  [CODE] cortical/semantics.py"
      ],
      "context_after": [
        "",
        "📥 Imported By (6 files):",
        "  [TEST] tests/test_processor.py",
        "  [TEST] tests/unit/test_processor_core.py",
        "",
        "💡 Shared Concepts (5 files):",
        "  [TEST] tests/test_coverage_gaps.py           (score: 8.2)",
        "  [CODE] cortical/persistence.py               (score: 6.5)",
        "",
        "🔍 Semantically Similar (5 files):"
      ],
      "change_type": "modify"
    },
    {
      "file": "docs/dogfooding.md",
      "function": "Incremental indexing is the key to efficient dog-fooding. Instead of rebuilding",
      "start_line": 151,
      "lines_added": [
        "# cortical/query/expansion.py:XX  [0.847]"
      ],
      "lines_removed": [
        "# cortical/query.py:55  [0.847]"
      ],
      "context_before": [
        "",
        "## Search Capabilities",
        "",
        "### Basic Search",
        "",
        "```bash",
        "# Find code related to a concept",
        "python scripts/search_codebase.py \"query expansion\"",
        "",
        "# Output shows file:line references"
      ],
      "context_after": [
        "#   def get_expanded_query_terms(...)",
        "```",
        "",
        "### Query Expansion",
        "",
        "The search automatically expands queries with related terms:",
        "",
        "```bash",
        "python scripts/search_codebase.py \"PageRank\" --expand",
        ""
      ],
      "change_type": "modify"
    },
    {
      "file": "docs/glossary.md",
      "function": "Community detection algorithm for clustering. Tokens adopt the most common label",
      "start_line": 124,
      "lines_added": [
        "**Location:** `query/expansion.py`"
      ],
      "lines_removed": [
        "**Location:** `query.py:55-176`"
      ],
      "context_before": [
        "**Parameters:**",
        "- `cluster_strictness`: Higher = more separate clusters",
        "- `bridge_weight`: Synthetic inter-document connections",
        "",
        "### Damping Factor",
        "PageRank parameter (default 0.85) representing probability of following a link vs. random jump. Lower damping = more randomness in importance distribution.",
        "",
        "### Query Expansion",
        "Process of adding related terms to a search query based on lateral connections, concept membership, or semantic relations.",
        ""
      ],
      "context_after": [
        "",
        "### Spreading Activation",
        "Information propagation through connections. Activation starts at query terms and spreads to connected nodes, simulating neural activation patterns.",
        "",
        "---",
        "",
        "## Semantic Relations",
        "",
        "### IsA",
        "Hypernym/hyponym relationship. \"A dog IsA animal\" means dog is a type of animal."
      ],
      "change_type": "modify"
    },
    {
      "file": "docs/glossary.md",
      "function": "System for knowing which computations need rerunning after corpus changes. Preve",
      "start_line": 234,
      "lines_added": [
        "**Location:** `query/intent.py`",
        "**Location:** `query/expansion.py`",
        "**Location:** `query/chunking.py`",
        "**Location:** `query/search.py`"
      ],
      "lines_removed": [
        "**Location:** `query.py:179-284`",
        "**Location:** `query.py:407-531`",
        "**Location:** `query.py:937-978`",
        "**Location:** `query.py` (fast search functions)"
      ],
      "context_before": [
        "",
        "**Location:** `processor.py:49`",
        "",
        "---",
        "",
        "## Search Concepts",
        "",
        "### Intent Parsing",
        "Extracting user intent from natural language queries. Maps question words to intent types (where→location, how→implementation).",
        ""
      ],
      "context_after": [
        "",
        "### Multi-hop Expansion",
        "Query expansion through chains of semantic relations. Finds terms 2+ hops away through valid relation paths.",
        "",
        "",
        "### Chunk",
        "A segment of document text for passage retrieval. Created with configurable size and overlap.",
        "",
        "",
        "### Inverted Index",
        "Pre-computed mapping from terms to containing documents. Enables fast candidate filtering.",
        "",
        "",
        "---",
        "",
        "## Code Concepts",
        "",
        "### Programming Concept Groups",
        "Collections of synonymous programming terms. \"get\", \"fetch\", \"load\", \"retrieve\" are grouped together.",
        "",
        "**Location:** `code_concepts.py`",
        ""
      ],
      "change_type": "modify"
    },
    {
      "file": "docs/glossary.md",
      "function": "Processing multiple queries or documents together to amortize overhead.",
      "start_line": 300,
      "lines_added": [
        "| Query Expansion | `query/expansion.py` |"
      ],
      "lines_removed": [
        "| Query Expansion | `query.py` |"
      ],
      "context_before": [
        "",
        "| Term | Primary File |",
        "|------|--------------|",
        "| Minicolumn | `minicolumn.py` |",
        "| Edge | `minicolumn.py` |",
        "| HierarchicalLayer | `layers.py` |",
        "| CorticalLayer | `layers.py` |",
        "| PageRank | `analysis.py` |",
        "| TF-IDF | `analysis.py` |",
        "| Label Propagation | `analysis.py` |"
      ],
      "context_after": [
        "| Relation Extraction | `semantics.py` |",
        "| Retrofitting | `semantics.py` |",
        "| Tokenization | `tokenizer.py` |",
        "| Fingerprint | `fingerprint.py` |",
        "| Code Concepts | `code_concepts.py` |"
      ],
      "change_type": "modify"
    },
    {
      "file": "docs/louvain_resolution_analysis.md",
      "function": "The current default of `resolution=1.0` is well-chosen because:",
      "start_line": 177,
      "lines_added": [
        "The existing default values in `cortical/analysis.py:cluster_by_louvain()` and `cortical/processor/compute.py:build_concept_clusters()` should remain at `resolution=1.0`."
      ],
      "lines_removed": [
        "The existing default values in `cortical/analysis.py:cluster_by_louvain()` and `cortical/processor.py:build_concept_clusters()` should remain at `resolution=1.0`."
      ],
      "context_before": [
        "",
        "### Document for Advanced Users",
        "",
        "Add documentation explaining:",
        "- Higher resolution (>1.0) → more, smaller clusters",
        "- Lower resolution (<1.0) → fewer, larger clusters",
        "- All values 0.5-3.0 produce valid community structure",
        "",
        "### No Code Changes Required",
        ""
      ],
      "context_after": [
        "",
        "---",
        "",
        "## Appendix: Reproducing This Analysis",
        "",
        "```bash",
        "# Run the analysis script",
        "python scripts/analyze_louvain_resolution.py --verbose",
        "",
        "# Test specific resolutions"
      ],
      "change_type": "modify"
    },
    {
      "file": "docs/patterns.md",
      "function": "for query in queries:",
      "start_line": 282,
      "lines_added": [
        "# Will prefer: docs/architecture.md over cortical/processor/__init__.py"
      ],
      "lines_removed": [
        "# Will prefer: docs/architecture.md over cortical/processor.py"
      ],
      "context_before": [
        "",
        "---",
        "",
        "### Pattern 10: Intent-Aware Search",
        "",
        "Search with intent understanding:",
        "",
        "```python",
        "# Conceptual query: boost documentation",
        "results = processor.search_by_intent(\"what is the 4-layer architecture?\")"
      ],
      "context_after": [
        "",
        "# Implementation query: boost code",
        "results = processor.search_by_intent(\"where is PageRank computed?\")",
        "# Will prefer: cortical/analysis.py over docs/algorithms.md",
        "```",
        "",
        "---",
        "",
        "## Document Type Boosting",
        ""
      ],
      "change_type": "modify"
    },
    {
      "file": "docs/quickstart.md",
      "function": "processor = CorticalTextProcessor.load(\"my_corpus.pkl\")",
      "start_line": 152,
      "lines_added": [
        "   cat cortical/processor/__init__.py.ai_meta  # Structured overview of main API",
        "**Note:** The processor is now a package (`cortical/processor/`) with mixin-based composition for better organization.",
        ""
      ],
      "lines_removed": [
        "   cat cortical/processor.py.ai_meta  # Structured overview"
      ],
      "context_before": [
        "- Read [cookbook.md](cookbook.md) for common recipes",
        "- See [CLAUDE.md](../CLAUDE.md) for the full developer guide",
        "- Check [architecture.md](architecture.md) for how it works",
        "",
        "### For AI Agents",
        "",
        "If you're an AI coding assistant exploring this codebase:",
        "",
        "1. **Use `.ai_meta` files** for rapid module understanding:",
        "   ```bash"
      ],
      "context_after": [
        "   ```",
        "",
        "2. **Check Claude skills** in `.claude/skills/` for:",
        "   - `codebase-search` - Semantic search",
        "   - `corpus-indexer` - Index management",
        "   - `ai-metadata` - Metadata viewer",
        "",
        "3. **See AI Agent Onboarding** in [CLAUDE.md](../CLAUDE.md#ai-agent-onboarding) for detailed guidance",
        "",
        "## Common Patterns",
        "",
        "### Batch Processing",
        "",
        "```python",
        "documents = [",
        "    (\"doc1\", \"First document content...\"),",
        "    (\"doc2\", \"Second document content...\"),",
        "    (\"doc3\", \"Third document content...\"),",
        "]"
      ],
      "change_type": "modify"
    },
    {
      "file": "scripts/ml-session-capture-hook.sh",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "#!/bin/bash",
        "#",
        "# ML Session Capture Hook for Claude Code",
        "#",
        "# This Stop hook automatically captures session transcripts for ML training.",
        "# It reads the transcript_path from stdin and processes it via ml_data_collector.py",
        "#",
        "# Installation: Add to ~/.claude/settings.json hooks.Stop array",
        "#",
        "",
        "# Read JSON input from stdin",
        "input=$(cat)",
        "",
        "# Check if stop hook is already active (recursion prevention)",
        "stop_hook_active=$(echo \"$input\" | jq -r '.stop_hook_active // \"false\"')",
        "if [[ \"$stop_hook_active\" == \"true\" ]]; then",
        "    exit 0",
        "fi",
        "",
        "# Check if ML collection is disabled",
        "if [[ \"${ML_COLLECTION_ENABLED:-1}\" == \"0\" ]]; then",
        "    exit 0",
        "fi",
        "",
        "# Extract transcript path from input",
        "transcript_path=$(echo \"$input\" | jq -r '.transcript_path // empty')",
        "session_id=$(echo \"$input\" | jq -r '.session_id // empty')",
        "cwd=$(echo \"$input\" | jq -r '.cwd // empty')",
        "",
        "# Bail if no transcript path",
        "if [[ -z \"$transcript_path\" ]] || [[ ! -f \"$transcript_path\" ]]; then",
        "    exit 0",
        "fi",
        "",
        "# Find the ml_data_collector.py script",
        "# Try current working directory first, then the cwd from input",
        "SCRIPT_DIR=\"$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\"",
        "COLLECTOR=\"\"",
        "",
        "if [[ -f \"${cwd}/scripts/ml_data_collector.py\" ]]; then",
        "    COLLECTOR=\"${cwd}/scripts/ml_data_collector.py\"",
        "elif [[ -f \"${SCRIPT_DIR}/../scripts/ml_data_collector.py\" ]]; then",
        "    COLLECTOR=\"${SCRIPT_DIR}/../scripts/ml_data_collector.py\"",
        "elif [[ -f \"./scripts/ml_data_collector.py\" ]]; then",
        "    COLLECTOR=\"./scripts/ml_data_collector.py\"",
        "fi",
        "",
        "# Only proceed if we found the collector and we're in a project that uses it",
        "if [[ -z \"$COLLECTOR\" ]]; then",
        "    exit 0",
        "fi",
        "",
        "# Check if this project has ML collection enabled (has .git-ml directory or is the right project)",
        "if [[ ! -d \"${cwd}/.git-ml\" ]] && [[ ! -f \"${cwd}/scripts/ml_data_collector.py\" ]]; then",
        "    exit 0",
        "fi",
        "",
        "# Process the transcript",
        "cd \"$cwd\" 2>/dev/null || exit 0",
        "",
        "# Call the transcript processor (suppress errors to not block session end)",
        "python3 \"$COLLECTOR\" transcript \\",
        "    --file \"$transcript_path\" \\",
        "    --session-id \"$session_id\" \\",
        "    2>/dev/null || true",
        "",
        "exit 0"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "scripts/ml_data_collector.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "#!/usr/bin/env python3",
        "\"\"\"",
        "ML Data Collector for Project-Specific Language Model Training",
        "",
        "This module collects enriched data from git commits, chat sessions, and developer",
        "actions to train a micro-model specific to this project.",
        "",
        "Usage:",
        "    # Collect commit data (call from git hook)",
        "    python scripts/ml_data_collector.py commit",
        "",
        "    # Log a chat session",
        "    python scripts/ml_data_collector.py chat --query \"...\" --response \"...\"",
        "",
        "    # Show statistics",
        "    python scripts/ml_data_collector.py stats",
        "",
        "    # Estimate when training is viable",
        "    python scripts/ml_data_collector.py estimate",
        "",
        "    # Analyze data quality",
        "    python scripts/ml_data_collector.py quality-report",
        "",
        "    # Generate session handoff document",
        "    python scripts/ml_data_collector.py handoff",
        "",
        "    # Add feedback to a chat",
        "    python scripts/ml_data_collector.py feedback --chat-id <id> --rating good [--comment \"text\"]",
        "",
        "    # List recent chats and their feedback status",
        "    python scripts/ml_data_collector.py feedback --list [--limit 20]",
        "",
        "    # Export data for training",
        "    python scripts/ml_data_collector.py export --format jsonl --output training_data.jsonl",
        "\"\"\"",
        "",
        "import json",
        "import os",
        "import subprocess",
        "import hashlib",
        "import re",
        "import shlex",
        "import tempfile",
        "import uuid",
        "import fcntl",
        "import logging",
        "from datetime import datetime",
        "from pathlib import Path",
        "from typing import Dict, List, Optional, Any",
        "from dataclasses import dataclass, asdict, field",
        "from contextlib import contextmanager",
        "",
        "# Setup logging",
        "logger = logging.getLogger(__name__)",
        "",
        "# Environment variable to disable collection",
        "ML_COLLECTION_ENABLED = os.getenv(\"ML_COLLECTION_ENABLED\", \"1\") != \"0\"",
        "",
        "",
        "class GitCommandError(Exception):",
        "    \"\"\"Raised when a git command fails.\"\"\"",
        "    pass",
        "",
        "",
        "class SchemaValidationError(Exception):",
        "    \"\"\"Raised when data fails schema validation.\"\"\"",
        "    pass",
        "",
        "",
        "# ============================================================================",
        "# CONFIGURATION",
        "# ============================================================================",
        "",
        "ML_DATA_DIR = Path(\".git-ml\")",
        "COMMITS_DIR = ML_DATA_DIR / \"commits\"",
        "SESSIONS_DIR = ML_DATA_DIR / \"sessions\"",
        "CHATS_DIR = ML_DATA_DIR / \"chats\"",
        "ACTIONS_DIR = ML_DATA_DIR / \"actions\"",
        "CURRENT_SESSION_FILE = ML_DATA_DIR / \"current_session.json\"",
        "",
        "# Training milestones",
        "MILESTONES = {",
        "    \"file_prediction\": {\"commits\": 500, \"sessions\": 100, \"chats\": 200},",
        "    \"commit_messages\": {\"commits\": 2000, \"sessions\": 500, \"chats\": 1000},",
        "    \"code_suggestions\": {\"commits\": 5000, \"sessions\": 2000, \"chats\": 5000},",
        "}",
        "",
        "# Schema validation definitions",
        "COMMIT_SCHEMA = {",
        "    \"required\": [\"hash\", \"message\", \"author\", \"timestamp\", \"branch\", \"files_changed\",",
        "                 \"insertions\", \"deletions\", \"hunks\", \"hour_of_day\", \"day_of_week\"],",
        "    \"types\": {",
        "        \"hash\": str, \"message\": str, \"author\": str, \"timestamp\": str, \"branch\": str,",
        "        \"files_changed\": list, \"insertions\": int, \"deletions\": int, \"hunks\": list,",
        "        \"hour_of_day\": int, \"day_of_week\": str, \"is_merge\": bool, \"is_initial\": bool,",
        "        \"parent_count\": int, \"session_id\": (str, type(None)), \"related_chats\": list,",
        "    }",
        "}",
        "",
        "CHAT_SCHEMA = {",
        "    \"required\": [\"id\", \"timestamp\", \"session_id\", \"query\", \"response\",",
        "                 \"files_referenced\", \"files_modified\", \"tools_used\"],",
        "    \"types\": {",
        "        \"id\": str, \"timestamp\": str, \"session_id\": str, \"query\": str, \"response\": str,",
        "        \"files_referenced\": list, \"files_modified\": list, \"tools_used\": list,",
        "        \"query_tokens\": int, \"response_tokens\": int,",
        "        \"user_feedback\": (dict, str, type(None)),  # Can be dict, legacy string, or None",
        "    }",
        "}",
        "",
        "ACTION_SCHEMA = {",
        "    \"required\": [\"id\", \"timestamp\", \"session_id\", \"action_type\", \"target\"],",
        "    \"types\": {",
        "        \"id\": str, \"timestamp\": str, \"session_id\": str,",
        "        \"action_type\": str, \"target\": str, \"success\": bool,",
        "    }",
        "}",
        "",
        "",
        "def validate_schema(data: dict, schema: dict, data_type: str) -> List[str]:",
        "    \"\"\"Validate data against a schema. Returns list of errors (empty if valid).\"\"\"",
        "    errors = []",
        "    for field in schema[\"required\"]:",
        "        if field not in data:",
        "            errors.append(f\"{data_type}: missing required field '{field}'\")",
        "    for field, expected in schema[\"types\"].items():",
        "        if field in data and data[field] is not None:",
        "            if isinstance(expected, tuple):",
        "                if not isinstance(data[field], expected):",
        "                    errors.append(f\"{data_type}: field '{field}' has wrong type\")",
        "            elif not isinstance(data[field], expected):",
        "                errors.append(f\"{data_type}: field '{field}' has type {type(data[field]).__name__}, expected {expected.__name__}\")",
        "    return errors",
        "",
        "",
        "# ============================================================================",
        "# FILE LOCKING (for concurrent access safety)",
        "# ============================================================================",
        "",
        "@contextmanager",
        "def file_lock(filepath: Path, exclusive: bool = True):",
        "    \"\"\"Context manager for file locking to prevent race conditions.",
        "",
        "    Args:",
        "        filepath: Path to lock file",
        "        exclusive: True for write lock, False for read lock",
        "    \"\"\"",
        "    filepath.parent.mkdir(parents=True, exist_ok=True)",
        "    lock_file = filepath.with_suffix(filepath.suffix + '.lock')",
        "",
        "    with open(lock_file, 'w') as f:",
        "        lock_type = fcntl.LOCK_EX if exclusive else fcntl.LOCK_SH",
        "        try:",
        "            fcntl.flock(f.fileno(), lock_type)",
        "            yield",
        "        finally:",
        "            fcntl.flock(f.fileno(), fcntl.LOCK_UN)",
        "",
        "",
        "# ============================================================================",
        "# SESSION MANAGEMENT (for commit-chat linking)",
        "# ============================================================================",
        "",
        "def get_current_session() -> Optional[Dict]:",
        "    \"\"\"Get the current active session info.",
        "",
        "    Returns dict with 'id', 'started_at', 'chat_ids' or None if no session.",
        "    \"\"\"",
        "    if CURRENT_SESSION_FILE.exists():",
        "        try:",
        "            with open(CURRENT_SESSION_FILE, 'r', encoding='utf-8') as f:",
        "                return json.load(f)",
        "        except json.JSONDecodeError as e:",
        "            logger.warning(f\"Corrupted session file: {e}\")",
        "            return None",
        "        except IOError as e:",
        "            logger.error(f\"Cannot read session file: {e}\")",
        "            return None",
        "    return None",
        "",
        "",
        "def start_session(session_id: Optional[str] = None) -> str:",
        "    \"\"\"Start a new session for commit-chat linking.",
        "",
        "    Args:",
        "        session_id: Optional session ID. Generated if not provided.",
        "",
        "    Returns:",
        "        The session ID.",
        "    \"\"\"",
        "    ensure_dirs()",
        "",
        "    session_id = session_id or generate_session_id()",
        "    session_data = {",
        "        'id': session_id,",
        "        'started_at': datetime.now().isoformat(),",
        "        'chat_ids': [],",
        "        'action_ids': [],",
        "    }",
        "",
        "    atomic_write_json(CURRENT_SESSION_FILE, session_data)",
        "    return session_id",
        "",
        "",
        "def get_or_create_session() -> str:",
        "    \"\"\"Get current session ID or create a new one.",
        "",
        "    Returns:",
        "        The current session ID.",
        "    \"\"\"",
        "    session = get_current_session()",
        "    if session:",
        "        return session['id']",
        "    return start_session()",
        "",
        "",
        "def add_chat_to_session(chat_id: str):",
        "    \"\"\"Record a chat ID in the current session for later commit linking.",
        "",
        "    Uses file locking to prevent race conditions with concurrent access.",
        "    \"\"\"",
        "    ensure_dirs()",
        "",
        "    # Use file lock to prevent race conditions",
        "    with file_lock(CURRENT_SESSION_FILE):",
        "        session = get_current_session()",
        "        if not session:",
        "            # Auto-start session if needed",
        "            session = {",
        "                'id': generate_session_id(),",
        "                'started_at': datetime.now().isoformat(),",
        "                'chat_ids': [],",
        "                'action_ids': [],",
        "            }",
        "",
        "        if chat_id not in session['chat_ids']:",
        "            session['chat_ids'].append(chat_id)",
        "            atomic_write_json(CURRENT_SESSION_FILE, session)",
        "",
        "",
        "def link_commit_to_session_chats(commit_hash: str) -> List[str]:",
        "    \"\"\"Link a commit to all chats from the current session.",
        "",
        "    Updates the chat entries to record that they resulted in this commit.",
        "    Also updates the commit's related_chats field.",
        "",
        "    Args:",
        "        commit_hash: The commit hash to link.",
        "",
        "    Returns:",
        "        List of chat IDs that were linked.",
        "    \"\"\"",
        "    session = get_current_session()",
        "    if not session or not session.get('chat_ids'):",
        "        return []",
        "",
        "    linked_chats = []",
        "",
        "    # Update each chat entry",
        "    for chat_id in session['chat_ids']:",
        "        chat_file = find_chat_file(chat_id)",
        "        if chat_file and chat_file.exists():",
        "            try:",
        "                with open(chat_file, 'r', encoding='utf-8') as f:",
        "                    chat_data = json.load(f)",
        "",
        "                # Mark chat as resulting in commit",
        "                chat_data['resulted_in_commit'] = True",
        "                chat_data['related_commit'] = commit_hash",
        "",
        "                atomic_write_json(chat_file, chat_data)",
        "                linked_chats.append(chat_id)",
        "            except (json.JSONDecodeError, IOError):",
        "                continue",
        "",
        "    return linked_chats",
        "",
        "",
        "def find_chat_file(chat_id: str) -> Optional[Path]:",
        "    \"\"\"Find the file path for a chat ID.\"\"\"",
        "    if not CHATS_DIR.exists():",
        "        return None",
        "",
        "    # Chat files are organized by date",
        "    for date_dir in CHATS_DIR.iterdir():",
        "        if date_dir.is_dir():",
        "            chat_file = date_dir / f\"{chat_id}.json\"",
        "            if chat_file.exists():",
        "                return chat_file",
        "",
        "    return None",
        "",
        "",
        "def add_chat_feedback(",
        "    chat_id: str,",
        "    rating: str,",
        "    comment: Optional[str] = None,",
        "    force: bool = False",
        ") -> bool:",
        "    \"\"\"Add or update user feedback for a chat entry.",
        "",
        "    Args:",
        "        chat_id: The chat ID to add feedback to.",
        "        rating: Rating value (good, bad, neutral).",
        "        comment: Optional feedback comment.",
        "        force: If True, overwrite existing feedback.",
        "",
        "    Returns:",
        "        True if feedback was added/updated, False if chat not found or already has feedback.",
        "    \"\"\"",
        "    # Validate rating",
        "    valid_ratings = {\"good\", \"bad\", \"neutral\"}",
        "    if rating not in valid_ratings:",
        "        raise ValueError(f\"Invalid rating '{rating}'. Must be one of: {', '.join(valid_ratings)}\")",
        "",
        "    # Find the chat file",
        "    chat_file = find_chat_file(chat_id)",
        "    if not chat_file:",
        "        return False",
        "",
        "    try:",
        "        # Load existing chat data",
        "        with open(chat_file, 'r', encoding='utf-8') as f:",
        "            chat_data = json.load(f)",
        "",
        "        # Check if feedback already exists",
        "        existing_feedback = chat_data.get('user_feedback')",
        "        if existing_feedback and not force:",
        "            # Check if it's a dict (new format) or string (legacy)",
        "            if isinstance(existing_feedback, dict):",
        "                return False",
        "            # Legacy string format - allow upgrade to dict format",
        "",
        "        # Add feedback",
        "        chat_data['user_feedback'] = {",
        "            'rating': rating,",
        "            'comment': comment,",
        "            'timestamp': datetime.now().isoformat(),",
        "        }",
        "",
        "        # Save atomically",
        "        atomic_write_json(chat_file, chat_data)",
        "        return True",
        "",
        "    except (json.JSONDecodeError, IOError) as e:",
        "        logger.error(f\"Error updating chat feedback: {e}\")",
        "        return False",
        "",
        "",
        "def list_chats_needing_feedback(limit: int = 10) -> List[Dict[str, Any]]:",
        "    \"\"\"List recent chats that don't have feedback yet.",
        "",
        "    Args:",
        "        limit: Maximum number of chats to return.",
        "",
        "    Returns:",
        "        List of chat info dicts with id, timestamp, query preview, and has_feedback status.",
        "    \"\"\"",
        "    if not CHATS_DIR.exists():",
        "        return []",
        "",
        "    chats = []",
        "",
        "    # Iterate through date directories in reverse order (most recent first)",
        "    date_dirs = sorted(CHATS_DIR.iterdir(), reverse=True)",
        "    for date_dir in date_dirs:",
        "        if not date_dir.is_dir():",
        "            continue",
        "",
        "        # Get all chat files in this date directory",
        "        chat_files = sorted(date_dir.glob(\"*.json\"), key=lambda f: f.stat().st_mtime, reverse=True)",
        "",
        "        for chat_file in chat_files:",
        "            if len(chats) >= limit:",
        "                break",
        "",
        "            try:",
        "                with open(chat_file, 'r', encoding='utf-8') as f:",
        "                    chat_data = json.load(f)",
        "",
        "                # Check if chat has feedback",
        "                feedback = chat_data.get('user_feedback')",
        "                has_feedback = False",
        "                feedback_rating = None",
        "",
        "                if feedback:",
        "                    if isinstance(feedback, dict):",
        "                        has_feedback = True",
        "                        feedback_rating = feedback.get('rating')",
        "                    elif isinstance(feedback, str):",
        "                        # Legacy string format",
        "                        has_feedback = True",
        "                        feedback_rating = feedback",
        "",
        "                chat_info = {",
        "                    'id': chat_data.get('id', 'unknown'),",
        "                    'timestamp': chat_data.get('timestamp', ''),",
        "                    'query': chat_data.get('query', '')[:100],  # First 100 chars",
        "                    'has_feedback': has_feedback,",
        "                    'feedback_rating': feedback_rating,",
        "                    'session_id': chat_data.get('session_id', ''),",
        "                }",
        "                chats.append(chat_info)",
        "",
        "            except (json.JSONDecodeError, IOError) as e:",
        "                logger.warning(f\"Error reading chat {chat_file}: {e}\")",
        "                continue",
        "",
        "        if len(chats) >= limit:",
        "            break",
        "",
        "    return chats",
        "",
        "",
        "def end_session(summary: Optional[str] = None) -> Optional[Dict]:",
        "    \"\"\"End the current session and archive it.",
        "",
        "    Args:",
        "        summary: Optional summary of what was accomplished.",
        "",
        "    Returns:",
        "        The ended session data or None if no session.",
        "    \"\"\"",
        "    session = get_current_session()",
        "    if not session:",
        "        return None",
        "",
        "    session['ended_at'] = datetime.now().isoformat()",
        "    session['summary'] = summary",
        "",
        "    # Save to sessions archive",
        "    ensure_dirs()",
        "    SESSIONS_DIR.mkdir(parents=True, exist_ok=True)",
        "",
        "    archive_file = SESSIONS_DIR / f\"{session['started_at'][:10]}_{session['id']}.json\"",
        "    atomic_write_json(archive_file, session)",
        "",
        "    # Remove current session file",
        "    if CURRENT_SESSION_FILE.exists():",
        "        CURRENT_SESSION_FILE.unlink()",
        "",
        "    return session",
        "",
        "",
        "def generate_session_handoff() -> str:",
        "    \"\"\"Generate a session handoff document with summary of work done.",
        "",
        "    Returns:",
        "        Markdown-formatted handoff document, or error message if no session.",
        "    \"\"\"",
        "    session = get_current_session()",
        "    if not session:",
        "        return \"No active session found. Use 'session start' to begin tracking.\"",
        "",
        "    # Parse session info",
        "    session_id = session['id']",
        "    started_at = session['started_at']",
        "    chat_ids = session.get('chat_ids', [])",
        "",
        "    # Calculate duration",
        "    start_time = datetime.fromisoformat(started_at)",
        "    duration = datetime.now() - start_time",
        "    hours = int(duration.total_seconds() // 3600)",
        "    minutes = int((duration.total_seconds() % 3600) // 60)",
        "    duration_str = f\"{hours}h {minutes}m\" if hours > 0 else f\"{minutes}m\"",
        "",
        "    # Load chat entries to analyze work done",
        "    chats = []",
        "    all_files_referenced = set()",
        "    all_files_modified = set()",
        "    all_tools_used = set()",
        "",
        "    for chat_id in chat_ids:",
        "        chat_file = find_chat_file(chat_id)",
        "        if chat_file and chat_file.exists():",
        "            try:",
        "                with open(chat_file, 'r', encoding='utf-8') as f:",
        "                    chat_data = json.load(f)",
        "                    chats.append(chat_data)",
        "                    all_files_referenced.update(chat_data.get('files_referenced', []))",
        "                    all_files_modified.update(chat_data.get('files_modified', []))",
        "                    all_tools_used.update(chat_data.get('tools_used', []))",
        "            except (json.JSONDecodeError, IOError):",
        "                continue",
        "",
        "    # Summarize key tasks from queries",
        "    key_tasks = []",
        "    for chat in chats[-10:]:  # Last 10 chats",
        "        query = chat.get('query', '')",
        "        # Extract first sentence or first 80 chars as task summary",
        "        task_summary = query.split('.')[0][:80]",
        "        if task_summary and task_summary not in key_tasks:",
        "            key_tasks.append(task_summary)",
        "",
        "    # Find related commits",
        "    related_commits = []",
        "    if COMMITS_DIR.exists():",
        "        for commit_file in COMMITS_DIR.glob(\"*.json\"):",
        "            try:",
        "                with open(commit_file, 'r', encoding='utf-8') as f:",
        "                    commit_data = json.load(f)",
        "                    if commit_data.get('session_id') == session_id:",
        "                        related_commits.append({",
        "                            'hash': commit_data['hash'][:8],",
        "                            'message': commit_data['message'],",
        "                            'timestamp': commit_data['timestamp']",
        "                        })",
        "            except (json.JSONDecodeError, IOError):",
        "                continue",
        "",
        "    # Sort commits by timestamp",
        "    related_commits.sort(key=lambda c: c['timestamp'])",
        "",
        "    # Generate suggested next steps based on patterns",
        "    suggestions = []",
        "",
        "    # Check for incomplete work patterns",
        "    if any('test' in tool.lower() for tool in all_tools_used):",
        "        if not any('pass' in c['message'].lower() for c in related_commits):",
        "            suggestions.append(\"Run and verify tests pass before committing\")",
        "",
        "    if all_files_modified and not related_commits:",
        "        suggestions.append(\"Review modified files and commit changes if ready\")",
        "",
        "    if 'Edit' in all_tools_used or 'Write' in all_tools_used:",
        "        suggestions.append(\"Consider running the test suite to verify changes\")",
        "",
        "    if len(chats) > 5 and not related_commits:",
        "        suggestions.append(\"Session has multiple exchanges but no commits - consider saving work\")",
        "",
        "    # Check for unresolved errors in recent responses",
        "    recent_errors = any('error' in chat.get('response', '').lower() or",
        "                       'failed' in chat.get('response', '').lower()",
        "                       for chat in chats[-3:])",
        "    if recent_errors:",
        "        suggestions.append(\"Recent responses mention errors - may need debugging or fixes\")",
        "",
        "    if not suggestions:",
        "        suggestions.append(\"Continue with planned work\")",
        "",
        "    # Build markdown document",
        "    md = []",
        "    md.append(f\"# Session Handoff: {session_id}\")",
        "    md.append(\"\")",
        "    md.append(\"## Summary\")",
        "    md.append(f\"- Started: {started_at}\")",
        "    md.append(f\"- Duration: {duration_str}\")",
        "    md.append(f\"- Exchanges: {len(chats)}\")",
        "    md.append(f\"- Tools used: {', '.join(sorted(all_tools_used)) if all_tools_used else 'none'}\")",
        "    md.append(\"\")",
        "",
        "    md.append(\"## Key Work Done\")",
        "    if key_tasks:",
        "        for task in key_tasks[:5]:  # Top 5 tasks",
        "            md.append(f\"- {task}\")",
        "    else:",
        "        md.append(\"- No significant work recorded\")",
        "    md.append(\"\")",
        "",
        "    md.append(\"## Files Touched\")",
        "    if all_files_modified:",
        "        md.append(\"### Modified:\")",
        "        for f in sorted(all_files_modified)[:10]:  # Top 10 files",
        "            md.append(f\"- {f}\")",
        "    if all_files_referenced and all_files_referenced - all_files_modified:",
        "        md.append(\"### Referenced:\")",
        "        for f in sorted(all_files_referenced - all_files_modified)[:10]:",
        "            md.append(f\"- {f}\")",
        "    if not all_files_modified and not all_files_referenced:",
        "        md.append(\"- No files modified or referenced\")",
        "    md.append(\"\")",
        "",
        "    md.append(\"## Related Commits\")",
        "    if related_commits:",
        "        for commit in related_commits:",
        "            md.append(f\"- `{commit['hash']}`: {commit['message']}\")",
        "    else:",
        "        md.append(\"- No commits made in this session\")",
        "    md.append(\"\")",
        "",
        "    md.append(\"## Suggested Next Steps\")",
        "    for suggestion in suggestions:",
        "        md.append(f\"- {suggestion}\")",
        "    md.append(\"\")",
        "",
        "    return \"\\n\".join(md)",
        "",
        "",
        "",
        "# ============================================================================",
        "# CI STATUS INTEGRATION",
        "# ============================================================================",
        "",
        "def find_commit_file(commit_hash: str) -> Optional[Path]:",
        "    \"\"\"Find the data file for a commit by its hash (full or prefix).\"\"\"",
        "    if not COMMITS_DIR.exists():",
        "        return None",
        "",
        "    # Search for files starting with the commit hash prefix",
        "    for f in COMMITS_DIR.glob(f\"{commit_hash[:8]}_*.json\"):",
        "        return f",
        "",
        "    # Try full hash search if prefix didn't match",
        "    for f in COMMITS_DIR.glob(\"*.json\"):",
        "        try:",
        "            with open(f, 'r', encoding='utf-8') as fp:",
        "                data = json.load(fp)",
        "            if data.get('hash', '').startswith(commit_hash):",
        "                return f",
        "        except (json.JSONDecodeError, IOError):",
        "            continue",
        "",
        "    return None",
        "",
        "",
        "def update_commit_ci_result(",
        "    commit_hash: str,",
        "    result: str,",
        "    details: Optional[Dict] = None",
        ") -> bool:",
        "    \"\"\"Update a commit's CI result.",
        "",
        "    Args:",
        "        commit_hash: Full or partial commit hash.",
        "        result: CI result (e.g., \"pass\", \"fail\", \"error\", \"pending\").",
        "        details: Optional dict with additional CI info (test_count, failures, etc.)",
        "",
        "    Returns:",
        "        True if commit was found and updated, False otherwise.",
        "    \"\"\"",
        "    commit_file = find_commit_file(commit_hash)",
        "    if not commit_file:",
        "        return False",
        "",
        "    try:",
        "        with open(commit_file, 'r', encoding='utf-8') as f:",
        "            data = json.load(f)",
        "",
        "        # Update CI fields",
        "        data['ci_result'] = result",
        "        if details:",
        "            data['ci_details'] = details",
        "        data['ci_updated_at'] = datetime.now().isoformat()",
        "",
        "        atomic_write_json(commit_file, data)",
        "        return True",
        "",
        "    except (json.JSONDecodeError, IOError):",
        "        return False",
        "",
        "",
        "def mark_commit_reverted(commit_hash: str, reverting_commit: Optional[str] = None) -> bool:",
        "    \"\"\"Mark a commit as reverted.",
        "",
        "    Args:",
        "        commit_hash: The commit that was reverted.",
        "        reverting_commit: The commit that performed the revert.",
        "",
        "    Returns:",
        "        True if commit was found and updated.",
        "    \"\"\"",
        "    commit_file = find_commit_file(commit_hash)",
        "    if not commit_file:",
        "        return False",
        "",
        "    try:",
        "        with open(commit_file, 'r', encoding='utf-8') as f:",
        "            data = json.load(f)",
        "",
        "        data['reverted'] = True",
        "        if reverting_commit:",
        "            data['reverted_by'] = reverting_commit",
        "        data['reverted_at'] = datetime.now().isoformat()",
        "",
        "        atomic_write_json(commit_file, data)",
        "        return True",
        "",
        "    except (json.JSONDecodeError, IOError):",
        "        return False",
        "",
        "",
        "# ============================================================================",
        "# TRANSCRIPT PARSING (for automatic session capture via Stop hook)",
        "# ============================================================================",
        "",
        "@dataclass",
        "class TranscriptExchange:",
        "    \"\"\"A single query/response exchange extracted from a transcript.\"\"\"",
        "    query: str",
        "    response: str",
        "    tools_used: List[str]",
        "    tool_inputs: List[Dict]",
        "    timestamp: str",
        "    thinking: Optional[str] = None",
        "",
        "",
        "def parse_transcript_jsonl(filepath: Path) -> List[TranscriptExchange]:",
        "    \"\"\"Parse a Claude Code transcript JSONL file into exchanges.",
        "",
        "    The JSONL format has entries with:",
        "    - type: \"user\" or \"assistant\"",
        "    - message.content: string (user) or array of content blocks (assistant)",
        "    - timestamp: ISO timestamp",
        "",
        "    Returns list of TranscriptExchange objects.",
        "    \"\"\"",
        "    if not filepath.exists():",
        "        logger.warning(f\"Transcript file not found: {filepath}\")",
        "        return []",
        "",
        "    exchanges = []",
        "    current_query = None",
        "    current_response_parts = []",
        "    current_tools = []",
        "    current_tool_inputs = []",
        "    current_thinking = None",
        "    current_timestamp = None",
        "",
        "    try:",
        "        with open(filepath, 'r', encoding='utf-8') as f:",
        "            for line in f:",
        "                line = line.strip()",
        "                if not line:",
        "                    continue",
        "",
        "                try:",
        "                    entry = json.loads(line)",
        "                except json.JSONDecodeError:",
        "                    continue",
        "",
        "                entry_type = entry.get('type')",
        "                message = entry.get('message', {})",
        "                timestamp = entry.get('timestamp', '')",
        "",
        "                if entry_type == 'user':",
        "                    # Save previous exchange if we have one",
        "                    if current_query and current_response_parts:",
        "                        exchanges.append(TranscriptExchange(",
        "                            query=current_query,",
        "                            response=' '.join(current_response_parts),",
        "                            tools_used=current_tools,",
        "                            tool_inputs=current_tool_inputs,",
        "                            timestamp=current_timestamp or timestamp,",
        "                            thinking=current_thinking,",
        "                        ))",
        "",
        "                    # Start new exchange",
        "                    content = message.get('content', '')",
        "                    if isinstance(content, str):",
        "                        current_query = content",
        "                    elif isinstance(content, list):",
        "                        # Extract text from content blocks",
        "                        current_query = ' '.join(",
        "                            c.get('text', '') for c in content",
        "                            if c.get('type') == 'text'",
        "                        )",
        "                    current_response_parts = []",
        "                    current_tools = []",
        "                    current_tool_inputs = []",
        "                    current_thinking = None",
        "                    current_timestamp = timestamp",
        "",
        "                elif entry_type == 'assistant':",
        "                    content = message.get('content', [])",
        "                    if isinstance(content, list):",
        "                        for block in content:",
        "                            block_type = block.get('type')",
        "",
        "                            if block_type == 'text':",
        "                                text = block.get('text', '')",
        "                                if text:",
        "                                    current_response_parts.append(text)",
        "",
        "                            elif block_type == 'thinking':",
        "                                current_thinking = block.get('thinking', '')",
        "",
        "                            elif block_type == 'tool_use':",
        "                                tool_name = block.get('name', '')",
        "                                tool_input = block.get('input', {})",
        "                                if tool_name and tool_name not in current_tools:",
        "                                    current_tools.append(tool_name)",
        "                                current_tool_inputs.append({",
        "                                    'tool': tool_name,",
        "                                    'input': tool_input,",
        "                                })",
        "",
        "        # Don't forget the last exchange",
        "        if current_query and current_response_parts:",
        "            exchanges.append(TranscriptExchange(",
        "                query=current_query,",
        "                response=' '.join(current_response_parts),",
        "                tools_used=current_tools,",
        "                tool_inputs=current_tool_inputs,",
        "                timestamp=current_timestamp or '',",
        "                thinking=current_thinking,",
        "            ))",
        "",
        "    except IOError as e:",
        "        logger.error(f\"Error reading transcript: {e}\")",
        "        return []",
        "",
        "    return exchanges",
        "",
        "",
        "def extract_files_from_tool_inputs(tool_inputs: List[Dict]) -> tuple:",
        "    \"\"\"Extract file references and modifications from tool inputs.",
        "",
        "    Returns (files_referenced, files_modified) tuple.",
        "    \"\"\"",
        "    files_referenced = set()",
        "    files_modified = set()",
        "",
        "    for ti in tool_inputs:",
        "        tool = ti.get('tool', '')",
        "        inp = ti.get('input', {})",
        "",
        "        if tool == 'Read':",
        "            path = inp.get('file_path', '')",
        "            if path:",
        "                files_referenced.add(path)",
        "",
        "        elif tool in ('Edit', 'Write', 'MultiEdit'):",
        "            path = inp.get('file_path', '')",
        "            if path:",
        "                files_modified.add(path)",
        "",
        "        elif tool == 'NotebookEdit':",
        "            # NotebookEdit modifies notebooks",
        "            path = inp.get('notebook_path', '')",
        "            if path:",
        "                files_modified.add(path)",
        "",
        "        elif tool == 'Bash':",
        "            # Try to extract file paths from command",
        "            cmd = inp.get('command', '')",
        "",
        "            # Common file extensions to track",
        "            FILE_EXTENSIONS = (",
        "                '.py', '.md', '.json', '.yaml', '.yml', '.toml', '.ini', '.cfg',",
        "                '.txt', '.rst', '.sh', '.bash', '.zsh',",
        "                '.js', '.ts', '.jsx', '.tsx', '.mjs', '.cjs',",
        "                '.html', '.css', '.scss', '.less',",
        "                '.c', '.cpp', '.h', '.hpp', '.cc',",
        "                '.java', '.kt', '.scala',",
        "                '.go', '.rs', '.rb', '.php', '.pl',",
        "                '.sql', '.graphql',",
        "                '.xml', '.csv', '.env',",
        "                '.dockerfile', 'Dockerfile', 'Makefile', 'Jenkinsfile'",
        "            )",
        "",
        "            # Use shlex.split() for safer parsing (handles quoted paths)",
        "            try:",
        "                words = shlex.split(cmd)",
        "            except ValueError:",
        "                # Fallback to simple split if shlex fails",
        "                words = cmd.split()",
        "",
        "            for word in words:",
        "                # Strip quotes that might remain",
        "                word = word.strip('\\'\"')",
        "",
        "                # Skip flags/options",
        "                if word.startswith('-'):",
        "                    # But check if it's a flag with a value like --cov=\"file.py\"",
        "                    if '=' in word:",
        "                        # Extract the value part after =",
        "                        _, value = word.split('=', 1)",
        "                        value = value.strip('\\'\"')",
        "                        if any(value.endswith(ext) for ext in FILE_EXTENSIONS):",
        "                            files_referenced.add(value)",
        "                    continue",
        "",
        "                # Check if it ends with a tracked extension",
        "                if any(word.endswith(ext) for ext in FILE_EXTENSIONS):",
        "                    files_referenced.add(word)",
        "                # Also catch special files without extensions (case-insensitive)",
        "                elif any(word.lower().endswith(name.lower()) for name in ('Dockerfile', 'Makefile', 'Jenkinsfile')):",
        "                    files_referenced.add(word)",
        "",
        "        elif tool == 'Glob':",
        "            path = inp.get('path', '')",
        "            if path:",
        "                files_referenced.add(path)",
        "",
        "        elif tool == 'Grep':",
        "            path = inp.get('path', '')",
        "            if path:",
        "                files_referenced.add(path)",
        "",
        "    return list(files_referenced), list(files_modified)",
        "",
        "",
        "def process_transcript(",
        "    filepath: Path,",
        "    session_id: Optional[str] = None,",
        "    save_exchanges: bool = True",
        ") -> Dict[str, Any]:",
        "    \"\"\"Process a transcript file and optionally save exchanges.",
        "",
        "    Args:",
        "        filepath: Path to the JSONL transcript",
        "        session_id: Optional session ID to use (extracted from transcript if not provided)",
        "        save_exchanges: Whether to save exchanges to .git-ml/chats/",
        "",
        "    Returns:",
        "        Summary dict with counts and extracted data.",
        "    \"\"\"",
        "    exchanges = parse_transcript_jsonl(filepath)",
        "",
        "    if not exchanges:",
        "        return {'status': 'empty', 'exchanges': 0}",
        "",
        "    # Use provided session_id or generate one",
        "    if not session_id:",
        "        session_id = generate_session_id()",
        "",
        "    saved_count = 0",
        "    total_tools = set()",
        "    all_files_ref = set()",
        "    all_files_mod = set()",
        "",
        "    for ex in exchanges:",
        "        files_ref, files_mod = extract_files_from_tool_inputs(ex.tool_inputs)",
        "        all_files_ref.update(files_ref)",
        "        all_files_mod.update(files_mod)",
        "        total_tools.update(ex.tools_used)",
        "",
        "        if save_exchanges:",
        "            try:",
        "                entry = ChatEntry(",
        "                    id=generate_chat_id(),",
        "                    timestamp=ex.timestamp or datetime.now().isoformat(),",
        "                    session_id=session_id,",
        "                    query=ex.query[:10000],  # Limit query length",
        "                    response=ex.response[:50000],  # Limit response length",
        "                    files_referenced=files_ref,",
        "                    files_modified=files_mod,",
        "                    tools_used=ex.tools_used,",
        "                    query_tokens=len(ex.query.split()),",
        "                    response_tokens=len(ex.response.split()),",
        "                )",
        "                save_chat_entry(entry, validate=True)",
        "                add_chat_to_session(entry.id)",
        "                saved_count += 1",
        "            except Exception as e:",
        "                logger.error(f\"Error saving exchange: {e}\")",
        "",
        "    return {",
        "        'status': 'success',",
        "        'exchanges': len(exchanges),",
        "        'saved': saved_count,",
        "        'session_id': session_id,",
        "        'tools_used': list(total_tools),",
        "        'files_referenced': list(all_files_ref),",
        "        'files_modified': list(all_files_mod),",
        "    }",
        "",
        "",
        "# ============================================================================",
        "# DATA SCHEMAS",
        "# ============================================================================",
        "",
        "@dataclass",
        "class DiffHunk:",
        "    \"\"\"A single diff hunk from a commit.\"\"\"",
        "    file: str",
        "    function: Optional[str]  # Function/class containing the change",
        "    change_type: str  # add, modify, delete, rename",
        "    start_line: int",
        "    lines_added: List[str]",
        "    lines_removed: List[str]",
        "    context_before: List[str]",
        "    context_after: List[str]",
        "",
        "",
        "@dataclass",
        "class CommitContext:",
        "    \"\"\"Rich context captured at commit time.\"\"\"",
        "    # Git metadata",
        "    hash: str",
        "    message: str",
        "    author: str",
        "    timestamp: str",
        "    branch: str",
        "",
        "    # Files changed",
        "    files_changed: List[str]",
        "    insertions: int",
        "    deletions: int",
        "",
        "    # Diff structure",
        "    hunks: List[Dict]",
        "",
        "    # Temporal context",
        "    hour_of_day: int",
        "    day_of_week: str",
        "    seconds_since_last_commit: Optional[int]",
        "",
        "    # Commit type detection",
        "    is_merge: bool = False",
        "    is_initial: bool = False",
        "    parent_count: int = 1",
        "",
        "    # Session context (if available)",
        "    session_id: Optional[str] = None",
        "    related_chats: List[str] = field(default_factory=list)",
        "",
        "    # Outcome tracking (filled in later)",
        "    ci_result: Optional[str] = None",
        "    reverted: bool = False",
        "    amended: bool = False",
        "",
        "",
        "@dataclass",
        "class ChatEntry:",
        "    \"\"\"A query/response pair from a chat session.\"\"\"",
        "    id: str",
        "    timestamp: str",
        "    session_id: str",
        "",
        "    # The conversation",
        "    query: str",
        "    response: str",
        "",
        "    # Context",
        "    files_referenced: List[str]",
        "    files_modified: List[str]",
        "    tools_used: List[str]",
        "",
        "    # Outcome",
        "    user_feedback: Optional[str] = None  # positive, negative, neutral",
        "    resulted_in_commit: bool = False",
        "    related_commit: Optional[str] = None",
        "",
        "    # Metadata",
        "    query_tokens: int = 0",
        "    response_tokens: int = 0",
        "    duration_seconds: Optional[float] = None",
        "",
        "",
        "@dataclass",
        "class ActionEntry:",
        "    \"\"\"A discrete action taken during development.\"\"\"",
        "    id: str",
        "    timestamp: str",
        "    session_id: str",
        "",
        "    action_type: str  # search, read, edit, test, commit, etc.",
        "    target: str  # file path, query string, etc.",
        "",
        "    # Context",
        "    context: Dict[str, Any] = field(default_factory=dict)",
        "",
        "    # Outcome",
        "    success: bool = True",
        "    result_summary: Optional[str] = None",
        "",
        "",
        "# ============================================================================",
        "# DATA COLLECTION FUNCTIONS",
        "# ============================================================================",
        "",
        "def ensure_dirs():",
        "    \"\"\"Create data directories if they don't exist.\"\"\"",
        "    for dir_path in [COMMITS_DIR, SESSIONS_DIR, CHATS_DIR, ACTIONS_DIR]:",
        "        dir_path.mkdir(parents=True, exist_ok=True)",
        "",
        "",
        "def run_git(args: List[str], check: bool = True) -> str:",
        "    \"\"\"Run a git command and return output.",
        "",
        "    Args:",
        "        args: Git command arguments (without 'git' prefix)",
        "        check: If True, raise GitCommandError on non-zero exit",
        "",
        "    Returns:",
        "        Command stdout stripped of whitespace",
        "",
        "    Raises:",
        "        GitCommandError: If check=True and command fails",
        "    \"\"\"",
        "    result = subprocess.run(",
        "        [\"git\"] + args,",
        "        capture_output=True,",
        "        text=True,",
        "        cwd=str(Path.cwd())",
        "    )",
        "    if check and result.returncode != 0:",
        "        raise GitCommandError(",
        "            f\"git {args[0]} failed (exit {result.returncode}): {result.stderr.strip()}\"",
        "        )",
        "    return result.stdout.strip()",
        "",
        "",
        "def get_last_commit_time() -> Optional[datetime]:",
        "    \"\"\"Get timestamp of the previous commit.\"\"\"",
        "    output = run_git([\"log\", \"-2\", \"--format=%ct\"])",
        "    lines = output.strip().split(\"\\n\")",
        "    if len(lines) >= 2:",
        "        return datetime.fromtimestamp(int(lines[1]))",
        "    return None",
        "",
        "",
        "def parse_diff_hunks(commit_hash: str, is_merge: bool = False) -> List[Dict]:",
        "    \"\"\"Parse diff hunks from a commit into structured data.",
        "",
        "    Args:",
        "        commit_hash: Git commit hash",
        "        is_merge: If True, use --first-parent to get meaningful diff",
        "",
        "    Returns:",
        "        List of hunk dictionaries with file, function, lines, etc.",
        "    \"\"\"",
        "    # Use -U10 for more context (better for ML training)",
        "    # For merge commits, use --first-parent to get the actual changes",
        "    args = [\"show\", \"--format=\", \"-U10\", commit_hash]",
        "    if is_merge:",
        "        args.insert(2, \"--first-parent\")",
        "",
        "    diff_output = run_git(args, check=False)  # Don't fail on empty diffs",
        "",
        "    hunks = []",
        "    current_file = None",
        "    current_hunk = None",
        "",
        "    for line in diff_output.split(\"\\n\"):",
        "        # New file",
        "        if line.startswith(\"diff --git\"):",
        "            if current_hunk:",
        "                hunks.append(current_hunk)",
        "            match = re.search(r\"b/(.+)$\", line)",
        "            current_file = match.group(1) if match else \"unknown\"",
        "            current_hunk = None",
        "",
        "        # Hunk header",
        "        elif line.startswith(\"@@\"):",
        "            if current_hunk:",
        "                hunks.append(current_hunk)",
        "",
        "            # Parse line numbers",
        "            match = re.search(r\"@@ -(\\d+)\", line)",
        "            start_line = int(match.group(1)) if match else 0",
        "",
        "            # Extract function context if present",
        "            func_match = re.search(r\"@@ .+ @@ (.+)$\", line)",
        "            function = func_match.group(1).strip() if func_match else None",
        "",
        "            current_hunk = {",
        "                \"file\": current_file,",
        "                \"function\": function,",
        "                \"start_line\": start_line,",
        "                \"lines_added\": [],",
        "                \"lines_removed\": [],",
        "                \"context_before\": [],",
        "                \"context_after\": [],",
        "            }",
        "",
        "        # Diff content",
        "        elif current_hunk is not None:",
        "            if line.startswith(\"+\") and not line.startswith(\"+++\"):",
        "                current_hunk[\"lines_added\"].append(line[1:])",
        "            elif line.startswith(\"-\") and not line.startswith(\"---\"):",
        "                current_hunk[\"lines_removed\"].append(line[1:])",
        "            elif line.startswith(\" \"):",
        "                # Context line",
        "                if not current_hunk[\"lines_added\"] and not current_hunk[\"lines_removed\"]:",
        "                    current_hunk[\"context_before\"].append(line[1:])",
        "                else:",
        "                    current_hunk[\"context_after\"].append(line[1:])",
        "",
        "    if current_hunk:",
        "        hunks.append(current_hunk)",
        "",
        "    # Determine change type for each hunk",
        "    for hunk in hunks:",
        "        if hunk[\"lines_added\"] and not hunk[\"lines_removed\"]:",
        "            hunk[\"change_type\"] = \"add\"",
        "        elif hunk[\"lines_removed\"] and not hunk[\"lines_added\"]:",
        "            hunk[\"change_type\"] = \"delete\"",
        "        else:",
        "            hunk[\"change_type\"] = \"modify\"",
        "",
        "    return hunks",
        "",
        "",
        "def collect_commit_data(commit_hash: Optional[str] = None) -> CommitContext:",
        "    \"\"\"Collect rich context for a commit.\"\"\"",
        "    if commit_hash is None:",
        "        commit_hash = run_git([\"rev-parse\", \"HEAD\"])",
        "",
        "    # Basic metadata",
        "    message = run_git([\"log\", \"-1\", \"--format=%s\", commit_hash])",
        "    author = run_git([\"log\", \"-1\", \"--format=%an\", commit_hash])",
        "    timestamp = run_git([\"log\", \"-1\", \"--format=%ci\", commit_hash])",
        "    branch = run_git([\"rev-parse\", \"--abbrev-ref\", \"HEAD\"], check=False) or \"HEAD\"",
        "",
        "    # Detect merge/initial commit by counting parents",
        "    parents_output = run_git([\"rev-list\", \"--parents\", \"-n1\", commit_hash])",
        "    parents = parents_output.split()",
        "    parent_count = len(parents) - 1  # First element is the commit itself",
        "    is_merge = parent_count > 1",
        "    is_initial = parent_count == 0",
        "",
        "    # Files and stats - for merges, use --first-parent for meaningful diff",
        "    if is_merge:",
        "        files_output = run_git(",
        "            [\"diff\", \"--name-only\", f\"{commit_hash}^1\", commit_hash],",
        "            check=False",
        "        )",
        "        stats = run_git(",
        "            [\"diff\", \"--stat\", f\"{commit_hash}^1\", commit_hash],",
        "            check=False",
        "        )",
        "    else:",
        "        files_output = run_git([\"show\", \"--name-only\", \"--format=\", commit_hash])",
        "        stats = run_git([\"show\", \"--stat\", \"--format=\", commit_hash])",
        "",
        "    files_changed = [f for f in files_output.split(\"\\n\") if f]",
        "",
        "    insertions = 0",
        "    deletions = 0",
        "    if stats:",
        "        match = re.search(r\"(\\d+) insertion\", stats)",
        "        if match:",
        "            insertions = int(match.group(1))",
        "        match = re.search(r\"(\\d+) deletion\", stats)",
        "        if match:",
        "            deletions = int(match.group(1))",
        "",
        "    # Temporal context - use commit timestamp, not current time for backfill",
        "    commit_time = run_git([\"log\", \"-1\", \"--format=%ct\", commit_hash])",
        "    try:",
        "        commit_dt = datetime.fromtimestamp(int(commit_time))",
        "    except (ValueError, OSError):",
        "        commit_dt = datetime.now()",
        "",
        "    last_commit = get_last_commit_time()",
        "    seconds_since = None",
        "    if last_commit:",
        "        seconds_since = int((commit_dt - last_commit).total_seconds())",
        "",
        "    # Parse diff hunks (pass is_merge flag for proper handling)",
        "    hunks = parse_diff_hunks(commit_hash, is_merge=is_merge)",
        "",
        "    return CommitContext(",
        "        hash=commit_hash,",
        "        message=message,",
        "        author=author,",
        "        timestamp=timestamp,",
        "        branch=branch,",
        "        files_changed=files_changed,",
        "        insertions=insertions,",
        "        deletions=deletions,",
        "        hunks=hunks,",
        "        hour_of_day=commit_dt.hour,",
        "        day_of_week=commit_dt.strftime(\"%A\"),",
        "        seconds_since_last_commit=seconds_since,",
        "        is_merge=is_merge,",
        "        is_initial=is_initial,",
        "        parent_count=parent_count,",
        "    )",
        "",
        "",
        "def atomic_write_json(filepath: Path, data: dict):",
        "    \"\"\"Write JSON atomically using temp file + rename.",
        "",
        "    This prevents data corruption if the process is interrupted.",
        "    \"\"\"",
        "    # Write to temp file in same directory (for same-filesystem rename)",
        "    temp_fd, temp_path = tempfile.mkstemp(",
        "        suffix=\".tmp\",",
        "        prefix=filepath.stem + \"_\",",
        "        dir=filepath.parent",
        "    )",
        "    try:",
        "        with os.fdopen(temp_fd, \"w\", encoding=\"utf-8\") as f:",
        "            json.dump(data, f, indent=2, ensure_ascii=False)",
        "        # Atomic rename (on POSIX systems)",
        "        os.replace(temp_path, filepath)",
        "    except Exception:",
        "        # Clean up temp file on failure",
        "        if os.path.exists(temp_path):",
        "            os.unlink(temp_path)",
        "        raise",
        "",
        "",
        "def save_commit_data(context: CommitContext, validate: bool = True, link_session: bool = True):",
        "    \"\"\"Save commit context to disk atomically with validation.",
        "",
        "    Args:",
        "        context: The commit context to save.",
        "        validate: Whether to validate against schema.",
        "        link_session: Whether to link with current session chats.",
        "    \"\"\"",
        "    ensure_dirs()",
        "",
        "    # Link to current session if available",
        "    if link_session:",
        "        session = get_current_session()",
        "        if session:",
        "            context.session_id = session['id']",
        "            context.related_chats = session.get('chat_ids', [])",
        "",
        "    data = asdict(context)",
        "",
        "    # Validate before writing",
        "    if validate:",
        "        errors = validate_schema(data, COMMIT_SCHEMA, \"commit\")",
        "        if errors:",
        "            raise SchemaValidationError(f\"Commit validation failed: {errors}\")",
        "",
        "    # Use full hash + UUID suffix to prevent collisions",
        "    unique_id = uuid.uuid4().hex[:8]",
        "    filename = f\"{context.hash[:8]}_{context.timestamp[:10]}_{unique_id}.json\"",
        "    filepath = COMMITS_DIR / filename",
        "",
        "    atomic_write_json(filepath, data)",
        "    print(f\"Saved commit data to {filepath}\")",
        "",
        "    # Update chat entries to link back to this commit",
        "    if link_session and context.related_chats:",
        "        linked = link_commit_to_session_chats(context.hash)",
        "        if linked:",
        "            print(f\"Linked {len(linked)} chat(s) to commit {context.hash[:8]}\")",
        "",
        "",
        "def generate_chat_id() -> str:",
        "    \"\"\"Generate unique chat entry ID.\"\"\"",
        "    timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")",
        "    suffix = hashlib.sha256(str(datetime.now().timestamp()).encode()).hexdigest()[:6]",
        "    return f\"chat-{timestamp}-{suffix}\"",
        "",
        "",
        "def generate_session_id() -> str:",
        "    \"\"\"Generate unique session ID.\"\"\"",
        "    return hashlib.sha256(str(datetime.now().timestamp()).encode()).hexdigest()[:8]",
        "",
        "",
        "def save_chat_entry(entry: ChatEntry, validate: bool = True):",
        "    \"\"\"Save a chat entry to disk atomically with validation.\"\"\"",
        "    ensure_dirs()",
        "",
        "    data = asdict(entry)",
        "",
        "    # Validate before writing",
        "    if validate:",
        "        errors = validate_schema(data, CHAT_SCHEMA, \"chat\")",
        "        if errors:",
        "            raise SchemaValidationError(f\"Chat validation failed: {errors}\")",
        "",
        "    # Organize by date",
        "    date_dir = CHATS_DIR / entry.timestamp[:10]",
        "    date_dir.mkdir(exist_ok=True)",
        "",
        "    filename = f\"{entry.id}.json\"",
        "    filepath = date_dir / filename",
        "",
        "    atomic_write_json(filepath, data)",
        "    print(f\"Saved chat entry to {filepath}\")",
        "",
        "",
        "def log_chat(",
        "    query: str,",
        "    response: str,",
        "    session_id: Optional[str] = None,",
        "    files_referenced: Optional[List[str]] = None,",
        "    files_modified: Optional[List[str]] = None,",
        "    tools_used: Optional[List[str]] = None,",
        "    user_feedback: Optional[str] = None,",
        ") -> ChatEntry:",
        "    \"\"\"Log a chat query/response pair.",
        "",
        "    If no session_id is provided, uses the current session (creating one if needed).",
        "    The chat is automatically registered with the session for commit linking.",
        "    \"\"\"",
        "    # Use current session or create one",
        "    if session_id is None:",
        "        session_id = get_or_create_session()",
        "",
        "    entry = ChatEntry(",
        "        id=generate_chat_id(),",
        "        timestamp=datetime.now().isoformat(),",
        "        session_id=session_id,",
        "        query=query,",
        "        response=response,",
        "        files_referenced=files_referenced or [],",
        "        files_modified=files_modified or [],",
        "        tools_used=tools_used or [],",
        "        user_feedback=user_feedback,",
        "        query_tokens=len(query.split()),  # Rough estimate",
        "        response_tokens=len(response.split()),",
        "    )",
        "",
        "    save_chat_entry(entry)",
        "",
        "    # Register with session for commit linking",
        "    add_chat_to_session(entry.id)",
        "",
        "    return entry",
        "",
        "",
        "def save_action(entry: ActionEntry, validate: bool = True):",
        "    \"\"\"Save an action entry to disk atomically with validation.\"\"\"",
        "    ensure_dirs()",
        "",
        "    data = asdict(entry)",
        "",
        "    # Validate before writing",
        "    if validate:",
        "        errors = validate_schema(data, ACTION_SCHEMA, \"action\")",
        "        if errors:",
        "            raise SchemaValidationError(f\"Action validation failed: {errors}\")",
        "",
        "    date_dir = ACTIONS_DIR / entry.timestamp[:10]",
        "    date_dir.mkdir(exist_ok=True)",
        "",
        "    filename = f\"{entry.id}.json\"",
        "    filepath = date_dir / filename",
        "",
        "    atomic_write_json(filepath, data)",
        "",
        "",
        "def log_action(",
        "    action_type: str,",
        "    target: str,",
        "    session_id: Optional[str] = None,",
        "    context: Optional[Dict] = None,",
        "    success: bool = True,",
        "    result_summary: Optional[str] = None,",
        ") -> ActionEntry:",
        "    \"\"\"Log a discrete action.\"\"\"",
        "",
        "    timestamp = datetime.now()",
        "    action_id = f\"act-{timestamp.strftime('%Y%m%d-%H%M%S')}-{hashlib.sha256(str(timestamp.timestamp()).encode()).hexdigest()[:4]}\"",
        "",
        "    entry = ActionEntry(",
        "        id=action_id,",
        "        timestamp=timestamp.isoformat(),",
        "        session_id=session_id or generate_session_id(),",
        "        action_type=action_type,",
        "        target=target,",
        "        context=context or {},",
        "        success=success,",
        "        result_summary=result_summary,",
        "    )",
        "",
        "    save_action(entry)",
        "    return entry",
        "",
        "",
        "# ============================================================================",
        "# DATA EXPORT FOR TRAINING",
        "# ============================================================================",
        "",
        "def _summarize_diff(hunks: List[Dict]) -> str:",
        "    \"\"\"Summarize diff hunks into a concise description for training.\"\"\"",
        "    if not hunks:",
        "        return \"\"",
        "",
        "    # Group by file",
        "    files = {}",
        "    for hunk in hunks:",
        "        file = hunk.get('file', 'unknown')",
        "        if file not in files:",
        "            files[file] = {'add': 0, 'delete': 0, 'modify': 0}",
        "        change_type = hunk.get('change_type', 'modify')",
        "        files[file][change_type] = files[file].get(change_type, 0) + 1",
        "",
        "    # Create summary",
        "    parts = []",
        "    for file, changes in files.items():",
        "        change_desc = []",
        "        if changes['add'] > 0:",
        "            change_desc.append(f\"+{changes['add']}\")",
        "        if changes['delete'] > 0:",
        "            change_desc.append(f\"-{changes['delete']}\")",
        "        if changes['modify'] > 0:",
        "            change_desc.append(f\"~{changes['modify']}\")",
        "        parts.append(f\"{file}: {' '.join(change_desc)}\")",
        "",
        "    return '; '.join(parts[:10])  # Limit to first 10 files",
        "",
        "",
        "def _export_jsonl(records: List[Dict], output_path: Path):",
        "    \"\"\"Export records as JSONL (one JSON per line).\"\"\"",
        "    with open(output_path, 'w', encoding='utf-8') as f:",
        "        for record in records:",
        "            f.write(json.dumps(record, ensure_ascii=False) + '\\n')",
        "",
        "",
        "def _export_csv(records: List[Dict], output_path: Path):",
        "    \"\"\"Export records as CSV.\"\"\"",
        "    import csv",
        "",
        "    with open(output_path, 'w', encoding='utf-8', newline='') as f:",
        "        writer = csv.DictWriter(f, fieldnames=[",
        "            'type', 'timestamp', 'input', 'output',",
        "            'session_id', 'files', 'tools_used'",
        "        ])",
        "        writer.writeheader()",
        "",
        "        for record in records:",
        "            context = record.get('context', {})",
        "            row = {",
        "                'type': record.get('type', ''),",
        "                'timestamp': record.get('timestamp', ''),",
        "                'input': record.get('input', '')[:1000],  # Truncate for CSV",
        "                'output': record.get('output', '')[:1000],",
        "                'session_id': context.get('session_id', ''),",
        "                'files': '; '.join(context.get('files', []))[:500],",
        "                'tools_used': '; '.join(context.get('tools_used', [])),",
        "            }",
        "            writer.writerow(row)",
        "",
        "",
        "def _export_huggingface(records: List[Dict], output_path: Path):",
        "    \"\"\"Export records in HuggingFace Dataset dict format.\"\"\"",
        "    # HuggingFace datasets format: dict of lists",
        "    dataset = {",
        "        'type': [],",
        "        'timestamp': [],",
        "        'input': [],",
        "        'output': [],",
        "        'session_id': [],",
        "        'files': [],",
        "        'tools_used': [],",
        "    }",
        "",
        "    for record in records:",
        "        context = record.get('context', {})",
        "        dataset['type'].append(record.get('type', ''))",
        "        dataset['timestamp'].append(record.get('timestamp', ''))",
        "        dataset['input'].append(record.get('input', ''))",
        "        dataset['output'].append(record.get('output', ''))",
        "        dataset['session_id'].append(context.get('session_id', ''))",
        "        dataset['files'].append(context.get('files', []))",
        "        dataset['tools_used'].append(context.get('tools_used', []))",
        "",
        "    # Save as JSON in HuggingFace format",
        "    with open(output_path, 'w', encoding='utf-8') as f:",
        "        json.dump(dataset, f, indent=2, ensure_ascii=False)",
        "",
        "",
        "def export_data(format: str, output_path: Path) -> Dict[str, Any]:",
        "    \"\"\"Export collected ML data in training-ready formats.",
        "",
        "    Args:",
        "        format: Output format (jsonl, csv, huggingface)",
        "        output_path: Path to write the exported data",
        "",
        "    Returns:",
        "        Stats dict with counts and file paths",
        "",
        "    Raises:",
        "        ValueError: If format is invalid",
        "    \"\"\"",
        "    ensure_dirs()",
        "",
        "    # Collect all data",
        "    all_records = []",
        "",
        "    # Load commits",
        "    if COMMITS_DIR.exists():",
        "        for commit_file in COMMITS_DIR.glob(\"*.json\"):",
        "            try:",
        "                with open(commit_file, 'r', encoding='utf-8') as f:",
        "                    commit_data = json.load(f)",
        "",
        "                # Transform commit to training format",
        "                record = {",
        "                    \"type\": \"commit\",",
        "                    \"timestamp\": commit_data.get('timestamp', ''),",
        "                    \"input\": commit_data.get('message', ''),",
        "                    \"output\": _summarize_diff(commit_data.get('hunks', [])),",
        "                    \"context\": {",
        "                        \"files\": commit_data.get('files_changed', []),",
        "                        \"session_id\": commit_data.get('session_id', ''),",
        "                        \"tools_used\": [],",
        "                        \"insertions\": commit_data.get('insertions', 0),",
        "                        \"deletions\": commit_data.get('deletions', 0),",
        "                        \"branch\": commit_data.get('branch', ''),",
        "                    }",
        "                }",
        "                all_records.append(record)",
        "            except (json.JSONDecodeError, IOError) as e:",
        "                logger.warning(f\"Error reading commit {commit_file}: {e}\")",
        "",
        "    # Load chats",
        "    if CHATS_DIR.exists():",
        "        for chat_file in CHATS_DIR.glob(\"**/*.json\"):",
        "            try:",
        "                with open(chat_file, 'r', encoding='utf-8') as f:",
        "                    chat_data = json.load(f)",
        "",
        "                record = {",
        "                    \"type\": \"chat\",",
        "                    \"timestamp\": chat_data.get('timestamp', ''),",
        "                    \"input\": chat_data.get('query', ''),",
        "                    \"output\": chat_data.get('response', ''),",
        "                    \"context\": {",
        "                        \"files\": chat_data.get('files_referenced', []) + chat_data.get('files_modified', []),",
        "                        \"session_id\": chat_data.get('session_id', ''),",
        "                        \"tools_used\": chat_data.get('tools_used', []),",
        "                    }",
        "                }",
        "                all_records.append(record)",
        "            except (json.JSONDecodeError, IOError) as e:",
        "                logger.warning(f\"Error reading chat {chat_file}: {e}\")",
        "",
        "    # Sort by timestamp",
        "    all_records.sort(key=lambda r: r['timestamp'])",
        "",
        "    # Export based on format",
        "    output_path = Path(output_path)",
        "    output_path.parent.mkdir(parents=True, exist_ok=True)",
        "",
        "    if format == \"jsonl\":",
        "        _export_jsonl(all_records, output_path)",
        "    elif format == \"csv\":",
        "        _export_csv(all_records, output_path)",
        "    elif format == \"huggingface\":",
        "        _export_huggingface(all_records, output_path)",
        "    else:",
        "        raise ValueError(f\"Unknown format: {format}\")",
        "",
        "    return {",
        "        \"format\": format,",
        "        \"output_path\": str(output_path),",
        "        \"records\": len(all_records),",
        "        \"commits\": sum(1 for r in all_records if r['type'] == 'commit'),",
        "        \"chats\": sum(1 for r in all_records if r['type'] == 'chat'),",
        "    }",
        "",
        "",
        "# ============================================================================",
        "# STATISTICS AND ESTIMATION",
        "# ============================================================================",
        "",
        "def count_data() -> Dict[str, int]:",
        "    \"\"\"Count collected data entries.\"\"\"",
        "    ensure_dirs()",
        "",
        "    counts = {",
        "        \"commits\": 0,",
        "        \"chats\": 0,",
        "        \"actions\": 0,",
        "        \"sessions\": 0,",
        "    }",
        "",
        "    # Count commits",
        "    if COMMITS_DIR.exists():",
        "        counts[\"commits\"] = len(list(COMMITS_DIR.glob(\"*.json\")))",
        "",
        "    # Count chats",
        "    if CHATS_DIR.exists():",
        "        counts[\"chats\"] = len(list(CHATS_DIR.glob(\"**/*.json\")))",
        "",
        "    # Count actions",
        "    if ACTIONS_DIR.exists():",
        "        counts[\"actions\"] = len(list(ACTIONS_DIR.glob(\"**/*.json\")))",
        "",
        "    # Count sessions",
        "    if SESSIONS_DIR.exists():",
        "        counts[\"sessions\"] = len(list(SESSIONS_DIR.glob(\"*.json\")))",
        "",
        "    return counts",
        "",
        "",
        "def calculate_data_size() -> Dict[str, int]:",
        "    \"\"\"Calculate total size of collected data.\"\"\"",
        "    ensure_dirs()",
        "",
        "    sizes = {}",
        "    for name, dir_path in [",
        "        (\"commits\", COMMITS_DIR),",
        "        (\"chats\", CHATS_DIR),",
        "        (\"actions\", ACTIONS_DIR),",
        "        (\"sessions\", SESSIONS_DIR),",
        "    ]:",
        "        total = 0",
        "        if dir_path.exists():",
        "            for f in dir_path.glob(\"**/*.json\"):",
        "                total += f.stat().st_size",
        "        sizes[name] = total",
        "",
        "    sizes[\"total\"] = sum(sizes.values())",
        "    return sizes",
        "",
        "",
        "def estimate_progress() -> Dict[str, Dict]:",
        "    \"\"\"Estimate progress toward training milestones.\"\"\"",
        "    counts = count_data()",
        "",
        "    progress = {}",
        "    for milestone, requirements in MILESTONES.items():",
        "        milestone_progress = {}",
        "        for data_type, required in requirements.items():",
        "            current = counts.get(data_type, 0)",
        "            milestone_progress[data_type] = {",
        "                \"current\": current,",
        "                \"required\": required,",
        "                \"percent\": min(100, int(100 * current / required)),",
        "            }",
        "",
        "        # Overall milestone progress (minimum of all types)",
        "        overall = min(p[\"percent\"] for p in milestone_progress.values())",
        "        milestone_progress[\"overall\"] = overall",
        "        progress[milestone] = milestone_progress",
        "",
        "    return progress",
        "",
        "",
        "def print_stats():",
        "    \"\"\"Print collection statistics.\"\"\"",
        "    counts = count_data()",
        "    sizes = calculate_data_size()",
        "    progress = estimate_progress()",
        "",
        "    print(\"\\n\" + \"=\" * 60)",
        "    print(\"ML DATA COLLECTION STATISTICS\")",
        "    print(\"=\" * 60)",
        "",
        "    print(\"\\n📊 Data Counts:\")",
        "    print(f\"   Commits:  {counts['commits']:,}\")",
        "    print(f\"   Chats:    {counts['chats']:,}\")",
        "    print(f\"   Actions:  {counts['actions']:,}\")",
        "    print(f\"   Sessions: {counts['sessions']:,}\")",
        "",
        "    print(\"\\n💾 Data Sizes:\")",
        "    for name, size in sizes.items():",
        "        if size > 1024 * 1024:",
        "            print(f\"   {name.capitalize():10s}: {size / 1024 / 1024:.2f} MB\")",
        "        elif size > 1024:",
        "            print(f\"   {name.capitalize():10s}: {size / 1024:.2f} KB\")",
        "        else:",
        "            print(f\"   {name.capitalize():10s}: {size} bytes\")",
        "",
        "    print(\"\\n🎯 Training Milestones:\")",
        "    for milestone, data in progress.items():",
        "        overall = data.pop(\"overall\")",
        "        bar = \"█\" * (overall // 5) + \"░\" * (20 - overall // 5)",
        "        print(f\"\\n   {milestone.replace('_', ' ').title()}: [{bar}] {overall}%\")",
        "        for data_type, info in data.items():",
        "            print(f\"      {data_type}: {info['current']}/{info['required']}\")",
        "",
        "    print(\"\\n\" + \"=\" * 60)",
        "",
        "",
        "def estimate_project_size():",
        "    \"\"\"Estimate final project size when all milestones are reached.\"\"\"",
        "    # Current averages",
        "    sizes = calculate_data_size()",
        "    counts = count_data()",
        "",
        "    # Calculate average sizes per entry type",
        "    avg_commit_size = sizes[\"commits\"] / max(1, counts[\"commits\"])",
        "    avg_chat_size = sizes[\"chats\"] / max(1, counts[\"chats\"]) if counts[\"chats\"] > 0 else 2000  # estimate 2KB per chat",
        "    avg_action_size = sizes[\"actions\"] / max(1, counts[\"actions\"]) if counts[\"actions\"] > 0 else 500  # estimate 500B per action",
        "",
        "    # Target for \"code_suggestions\" milestone (the highest)",
        "    target_commits = MILESTONES[\"code_suggestions\"][\"commits\"]",
        "    target_chats = MILESTONES[\"code_suggestions\"][\"chats\"]",
        "    target_actions = target_chats * 10  # Estimate 10 actions per chat",
        "",
        "    estimated_total = (",
        "        target_commits * max(avg_commit_size, 5000) +  # ~5KB per commit if no data",
        "        target_chats * max(avg_chat_size, 2000) +",
        "        target_actions * max(avg_action_size, 500)",
        "    )",
        "",
        "    print(\"\\n\" + \"=\" * 60)",
        "    print(\"PROJECT SIZE ESTIMATE (Full Collection)\")",
        "    print(\"=\" * 60)",
        "",
        "    print(\"\\n📈 Target Data Points:\")",
        "    print(f\"   Commits:  {target_commits:,}\")",
        "    print(f\"   Chats:    {target_chats:,}\")",
        "    print(f\"   Actions:  {target_actions:,} (estimated)\")",
        "",
        "    print(\"\\n💾 Estimated Sizes:\")",
        "    print(f\"   Commits data:  {target_commits * max(avg_commit_size, 5000) / 1024 / 1024:.1f} MB\")",
        "    print(f\"   Chats data:    {target_chats * max(avg_chat_size, 2000) / 1024 / 1024:.1f} MB\")",
        "    print(f\"   Actions data:  {target_actions * max(avg_action_size, 500) / 1024 / 1024:.1f} MB\")",
        "    print(f\"   ─────────────────────────\")",
        "    print(f\"   TOTAL:         {estimated_total / 1024 / 1024:.1f} MB\")",
        "",
        "    print(\"\\n🧠 Model Training Estimates:\")",
        "    print(f\"   Vocabulary size:     ~15,000 tokens (this project)\")",
        "    print(f\"   Training examples:   ~{target_commits + target_chats:,}\")",
        "    print(f\"   Micro-model size:    1-10 MB (1-10M parameters)\")",
        "    print(f\"   Training time:       ~1-4 hours (single GPU)\")",
        "    print(f\"   Inference:           <100ms on CPU\")",
        "",
        "    print(\"\\n⏱️  Time to Collection Complete:\")",
        "    counts = count_data()",
        "    commits_per_day = 20  # Based on current rate",
        "    chats_per_day = 15  # Estimated",
        "",
        "    days_for_commits = (target_commits - counts[\"commits\"]) / commits_per_day",
        "    days_for_chats = (target_chats - counts[\"chats\"]) / chats_per_day",
        "",
        "    days_needed = max(days_for_commits, days_for_chats)",
        "    print(f\"   At current rate:     ~{int(days_needed)} days ({int(days_needed/30)} months)\")",
        "    print(f\"   With active use:     ~{int(days_needed * 0.5)} days (more chatting)\")",
        "",
        "    print(\"\\n\" + \"=\" * 60)",
        "",
        "",
        "# ============================================================================",
        "# DATA QUALITY ANALYSIS",
        "# ============================================================================",
        "",
        "def analyze_data_quality() -> Dict[str, Any]:",
        "    \"\"\"Analyze data quality across all collected ML data.",
        "",
        "    Returns:",
        "        Dictionary with completeness, diversity, anomalies, and quality score.",
        "    \"\"\"",
        "    ensure_dirs()",
        "",
        "    # Initialize metrics containers",
        "    completeness = {",
        "        'chats_complete': 0,",
        "        'chats_total': 0,",
        "        'commits_with_ci': 0,",
        "        'commits_total': 0,",
        "        'sessions_with_commits': 0,",
        "        'sessions_total': 0,",
        "        'chats_with_feedback': 0,",
        "    }",
        "",
        "    diversity = {",
        "        'unique_files': set(),",
        "        'unique_tools': {},",
        "        'query_lengths': [],",
        "        'response_lengths': [],",
        "    }",
        "",
        "    anomalies = {",
        "        'empty_responses': 0,",
        "        'zero_file_commits': 0,",
        "        'empty_sessions': 0,",
        "        'potential_duplicates': 0,",
        "    }",
        "",
        "    # Track duplicates (timestamp + content hash)",
        "    seen_entries = set()",
        "",
        "    # Analyze commits",
        "    if COMMITS_DIR.exists():",
        "        for commit_file in COMMITS_DIR.glob(\"*.json\"):",
        "            try:",
        "                with open(commit_file, 'r', encoding='utf-8') as f:",
        "                    data = json.load(f)",
        "",
        "                completeness['commits_total'] += 1",
        "",
        "                # Check CI results",
        "                if data.get('ci_result'):",
        "                    completeness['commits_with_ci'] += 1",
        "",
        "                # Track files",
        "                diversity['unique_files'].update(data.get('files_changed', []))",
        "",
        "                # Check anomalies",
        "                if not data.get('files_changed'):",
        "                    anomalies['zero_file_commits'] += 1",
        "",
        "                # Check duplicates (timestamp + message hash)",
        "                entry_key = (data.get('timestamp', ''),",
        "                           hashlib.md5(data.get('message', '').encode()).hexdigest()[:8])",
        "                if entry_key in seen_entries:",
        "                    anomalies['potential_duplicates'] += 1",
        "                else:",
        "                    seen_entries.add(entry_key)",
        "",
        "            except (json.JSONDecodeError, IOError):",
        "                continue",
        "",
        "    # Analyze chats",
        "    if CHATS_DIR.exists():",
        "        for chat_file in CHATS_DIR.glob(\"**/*.json\"):",
        "            try:",
        "                with open(chat_file, 'r', encoding='utf-8') as f:",
        "                    data = json.load(f)",
        "",
        "                completeness['chats_total'] += 1",
        "",
        "                # Check completeness (all required fields from CHAT_SCHEMA)",
        "                errors = validate_schema(data, CHAT_SCHEMA, \"chat\")",
        "                if not errors:",
        "                    completeness['chats_complete'] += 1",
        "",
        "                # Check feedback",
        "                if data.get('user_feedback'):",
        "                    completeness['chats_with_feedback'] += 1",
        "",
        "                # Track diversity",
        "                diversity['unique_files'].update(data.get('files_referenced', []))",
        "                diversity['unique_files'].update(data.get('files_modified', []))",
        "",
        "                for tool in data.get('tools_used', []):",
        "                    diversity['unique_tools'][tool] = diversity['unique_tools'].get(tool, 0) + 1",
        "",
        "                query = data.get('query', '')",
        "                response = data.get('response', '')",
        "",
        "                diversity['query_lengths'].append(len(query))",
        "                diversity['response_lengths'].append(len(response))",
        "",
        "                # Check anomalies",
        "                if not response or len(response.strip()) == 0:",
        "                    anomalies['empty_responses'] += 1",
        "",
        "                # Check duplicates (timestamp + query hash)",
        "                entry_key = (data.get('timestamp', ''),",
        "                           hashlib.md5(query.encode()).hexdigest()[:8])",
        "                if entry_key in seen_entries:",
        "                    anomalies['potential_duplicates'] += 1",
        "                else:",
        "                    seen_entries.add(entry_key)",
        "",
        "            except (json.JSONDecodeError, IOError):",
        "                continue",
        "",
        "    # Analyze sessions",
        "    if SESSIONS_DIR.exists():",
        "        for session_file in SESSIONS_DIR.glob(\"*.json\"):",
        "            try:",
        "                with open(session_file, 'r', encoding='utf-8') as f:",
        "                    data = json.load(f)",
        "",
        "                completeness['sessions_total'] += 1",
        "",
        "                # Check if session has chats",
        "                if not data.get('chat_ids'):",
        "                    anomalies['empty_sessions'] += 1",
        "",
        "            except (json.JSONDecodeError, IOError):",
        "                continue",
        "",
        "    # Count sessions with commits by checking commits with session_id",
        "    session_ids_with_commits = set()",
        "    if COMMITS_DIR.exists():",
        "        for commit_file in COMMITS_DIR.glob(\"*.json\"):",
        "            try:",
        "                with open(commit_file, 'r', encoding='utf-8') as f:",
        "                    data = json.load(f)",
        "                    session_id = data.get('session_id')",
        "                    if session_id:",
        "                        session_ids_with_commits.add(session_id)",
        "            except (json.JSONDecodeError, IOError):",
        "                continue",
        "",
        "    completeness['sessions_with_commits'] = len(session_ids_with_commits)",
        "",
        "    # Calculate percentages for completeness",
        "    completeness_metrics = {",
        "        'chats_complete_pct': (completeness['chats_complete'] / max(1, completeness['chats_total'])) * 100,",
        "        'commits_with_ci_pct': (completeness['commits_with_ci'] / max(1, completeness['commits_total'])) * 100,",
        "        'sessions_with_commits_pct': (completeness['sessions_with_commits'] / max(1, completeness['sessions_total'])) * 100,",
        "        'chats_with_feedback_pct': (completeness['chats_with_feedback'] / max(1, completeness['chats_total'])) * 100,",
        "    }",
        "",
        "    # Calculate diversity statistics",
        "    diversity_stats = {",
        "        'unique_files': len(diversity['unique_files']),",
        "        'unique_tools': len(diversity['unique_tools']),",
        "        'tool_distribution': diversity['unique_tools'],",
        "        'query_length_min': min(diversity['query_lengths']) if diversity['query_lengths'] else 0,",
        "        'query_length_avg': sum(diversity['query_lengths']) / max(1, len(diversity['query_lengths'])) if diversity['query_lengths'] else 0,",
        "        'query_length_max': max(diversity['query_lengths']) if diversity['query_lengths'] else 0,",
        "        'response_length_min': min(diversity['response_lengths']) if diversity['response_lengths'] else 0,",
        "        'response_length_avg': sum(diversity['response_lengths']) / max(1, len(diversity['response_lengths'])) if diversity['response_lengths'] else 0,",
        "        'response_length_max': max(diversity['response_lengths']) if diversity['response_lengths'] else 0,",
        "    }",
        "",
        "    # Calculate quality score (0-100)",
        "    # Weighted components:",
        "    # - Completeness: 40%",
        "    # - Low anomalies: 30%",
        "    # - Diversity: 30%",
        "",
        "    # Completeness score (average of all completeness metrics)",
        "    completeness_score = (",
        "        completeness_metrics['chats_complete_pct'] * 0.4 +",
        "        completeness_metrics['commits_with_ci_pct'] * 0.2 +",
        "        completeness_metrics['sessions_with_commits_pct'] * 0.3 +",
        "        completeness_metrics['chats_with_feedback_pct'] * 0.1",
        "    )",
        "",
        "    # Anomaly score (penalize based on anomaly percentage)",
        "    total_entries = completeness['chats_total'] + completeness['commits_total'] + completeness['sessions_total']",
        "    total_anomalies = (anomalies['empty_responses'] + anomalies['zero_file_commits'] +",
        "                      anomalies['empty_sessions'] + anomalies['potential_duplicates'])",
        "    anomaly_rate = total_anomalies / max(1, total_entries)",
        "    anomaly_score = max(0, 100 - (anomaly_rate * 200))  # Cap at 0, scale anomalies harshly",
        "",
        "    # Diversity score (based on having diverse tools and files)",
        "    # Good diversity: >5 tools, >50 files = 100%, scale down from there",
        "    tool_score = min(100, (diversity_stats['unique_tools'] / 5.0) * 100)",
        "    file_score = min(100, (diversity_stats['unique_files'] / 50.0) * 100)",
        "    diversity_score = (tool_score + file_score) / 2",
        "",
        "    # Overall quality score",
        "    quality_score = int(",
        "        completeness_score * 0.4 +",
        "        anomaly_score * 0.3 +",
        "        diversity_score * 0.3",
        "    )",
        "",
        "    return {",
        "        'completeness': {",
        "            'chats_complete': completeness['chats_complete'],",
        "            'chats_total': completeness['chats_total'],",
        "            'chats_complete_pct': completeness_metrics['chats_complete_pct'],",
        "            'commits_with_ci': completeness['commits_with_ci'],",
        "            'commits_total': completeness['commits_total'],",
        "            'commits_with_ci_pct': completeness_metrics['commits_with_ci_pct'],",
        "            'sessions_with_commits': completeness['sessions_with_commits'],",
        "            'sessions_total': completeness['sessions_total'],",
        "            'sessions_with_commits_pct': completeness_metrics['sessions_with_commits_pct'],",
        "            'chats_with_feedback': completeness['chats_with_feedback'],",
        "            'chats_with_feedback_pct': completeness_metrics['chats_with_feedback_pct'],",
        "        },",
        "        'diversity': diversity_stats,",
        "        'anomalies': anomalies,",
        "        'quality_score': quality_score,",
        "    }",
        "",
        "",
        "def print_quality_report():",
        "    \"\"\"Print a comprehensive data quality report.\"\"\"",
        "    result = analyze_data_quality()",
        "",
        "    comp = result['completeness']",
        "    div = result['diversity']",
        "    anom = result['anomalies']",
        "    score = result['quality_score']",
        "",
        "    print(\"\\n\" + \"=\" * 60)",
        "    print(\"DATA QUALITY REPORT\")",
        "    print(\"=\" * 60)",
        "",
        "    print(\"\\n📊 Completeness:\")",
        "    print(f\"   Chats with all fields:    {comp['chats_complete_pct']:>3.0f}% ({comp['chats_complete']}/{comp['chats_total']})\")",
        "    print(f\"   Commits with CI results:  {comp['commits_with_ci_pct']:>3.0f}% ({comp['commits_with_ci']}/{comp['commits_total']})\")",
        "    print(f\"   Sessions with commits:    {comp['sessions_with_commits_pct']:>3.0f}% ({comp['sessions_with_commits']}/{comp['sessions_total']})\")",
        "    print(f\"   Chats with feedback:      {comp['chats_with_feedback_pct']:>3.0f}% ({comp['chats_with_feedback']}/{comp['chats_total']})\")",
        "",
        "    print(\"\\n📈 Diversity:\")",
        "    print(f\"   Unique files:             {div['unique_files']}\")",
        "    print(f\"   Unique tools:             {div['unique_tools']}\")",
        "    if div['tool_distribution']:",
        "        print(\"   Tool usage:\")",
        "        for tool, count in sorted(div['tool_distribution'].items(), key=lambda x: -x[1])[:8]:",
        "            print(f\"      {tool}: {count}\")",
        "        if len(div['tool_distribution']) > 8:",
        "            print(f\"      ... and {len(div['tool_distribution']) - 8} more\")",
        "    print(f\"   Query length:             min={div['query_length_min']}, avg={div['query_length_avg']:.0f}, max={div['query_length_max']} chars\")",
        "    print(f\"   Response length:          min={div['response_length_min']}, avg={div['response_length_avg']:.0f}, max={div['response_length_max']} chars\")",
        "",
        "    print(\"\\n⚠️  Anomalies:\")",
        "    print(f\"   Empty responses:          {anom['empty_responses']}\")",
        "    print(f\"   Zero-file commits:        {anom['zero_file_commits']}\")",
        "    print(f\"   Empty sessions:           {anom['empty_sessions']}\")",
        "    print(f\"   Potential duplicates:     {anom['potential_duplicates']}\")",
        "",
        "    print(f\"\\n🎯 Quality Score: {score}/100\")",
        "    print(\"=\" * 60 + \"\\n\")",
        "",
        "",
        "# ============================================================================",
        "# GIT HOOKS",
        "# ============================================================================",
        "",
        "ML_HOOK_MARKER = \"# ML-DATA-COLLECTOR-HOOK\"",
        "",
        "POST_COMMIT_SNIPPET = '''",
        "# ML-DATA-COLLECTOR-HOOK",
        "# ML Data Collection - Post-Commit Hook",
        "# Automatically collects enriched commit data for model training",
        "python scripts/ml_data_collector.py commit 2>/dev/null || true",
        "# END-ML-DATA-COLLECTOR-HOOK",
        "'''",
        "",
        "PRE_PUSH_SNIPPET = '''",
        "# ML-DATA-COLLECTOR-HOOK",
        "# ML Data Collection - Pre-Push Hook",
        "# Validates data collection is working before push",
        "if [ -d \".git-ml/commits\" ]; then",
        "    count=$(ls -1 .git-ml/commits/*.json 2>/dev/null | wc -l)",
        "    echo \"📊 ML Data: $count commits collected\"",
        "fi",
        "# END-ML-DATA-COLLECTOR-HOOK",
        "'''",
        "",
        "",
        "def install_hooks():",
        "    \"\"\"Install git hooks for data collection, merging with existing hooks.\"\"\"",
        "    hooks_dir = Path(\".git/hooks\")",
        "",
        "    for hook_name, snippet in [(\"post-commit\", POST_COMMIT_SNIPPET), (\"pre-push\", PRE_PUSH_SNIPPET)]:",
        "        hook_path = hooks_dir / hook_name",
        "",
        "        if hook_path.exists():",
        "            existing = hook_path.read_text(encoding=\"utf-8\")",
        "",
        "            # Check if our hook is already installed",
        "            if ML_HOOK_MARKER in existing:",
        "                print(f\"✓ {hook_name}: ML hook already installed\")",
        "                continue",
        "",
        "            # Append to existing hook",
        "            with open(hook_path, \"a\", encoding=\"utf-8\") as f:",
        "                f.write(snippet)",
        "            print(f\"✓ {hook_name}: Added ML hook to existing hook\")",
        "",
        "        else:",
        "            # Create new hook with shebang",
        "            with open(hook_path, \"w\", encoding=\"utf-8\") as f:",
        "                f.write(\"#!/bin/bash\\n\")",
        "                f.write(snippet)",
        "                f.write(\"\\nexit 0\\n\")",
        "            hook_path.chmod(0o755)",
        "            print(f\"✓ {hook_name}: Created new hook\")",
        "",
        "    print(\"\\nML hooks installed! Commit data will be collected automatically.\")",
        "",
        "",
        "# ============================================================================",
        "# CLI",
        "# ============================================================================",
        "",
        "def main():",
        "    import sys",
        "",
        "    if len(sys.argv) < 2:",
        "        print(__doc__)",
        "        return",
        "",
        "    command = sys.argv[1]",
        "",
        "    # Allow stats/estimate/validate/export/feedback/quality-report even when collection is disabled",
        "    read_only_commands = {\"stats\", \"estimate\", \"validate\", \"session\", \"export\", \"feedback\", \"quality-report\"}",
        "",
        "    # Check if collection is disabled (via ML_COLLECTION_ENABLED=0)",
        "    if not ML_COLLECTION_ENABLED and command not in read_only_commands:",
        "        # Silently exit for collection commands when disabled",
        "        return",
        "",
        "    if command == \"commit\":",
        "        # Collect data for current or specified commit",
        "        commit_hash = sys.argv[2] if len(sys.argv) > 2 else None",
        "        context = collect_commit_data(commit_hash)",
        "        save_commit_data(context)",
        "",
        "    elif command == \"backfill\":",
        "        # Backfill historical commits (no session linking for historical data)",
        "        import argparse",
        "        parser = argparse.ArgumentParser()",
        "        parser.add_argument(\"-n\", \"--num\", type=int, default=100,",
        "                            help=\"Number of commits to backfill\")",
        "        args = parser.parse_args(sys.argv[2:])",
        "",
        "        hashes = run_git([\"log\", f\"-{args.num}\", \"--format=%H\"]).split(\"\\n\")",
        "        hashes = [h for h in hashes if h]",
        "        print(f\"Backfilling {len(hashes)} commits...\")",
        "",
        "        for i, h in enumerate(hashes):",
        "            try:",
        "                context = collect_commit_data(h)",
        "                # Disable session linking for historical backfill",
        "                save_commit_data(context, link_session=False)",
        "                if (i + 1) % 10 == 0:",
        "                    print(f\"  Progress: {i + 1}/{len(hashes)}\")",
        "            except Exception as e:",
        "                print(f\"  Error on {h[:8]}: {e}\")",
        "",
        "        print(f\"Backfill complete: {len(hashes)} commits\")",
        "",
        "    elif command == \"chat\":",
        "        # Log a chat entry",
        "        import argparse",
        "        parser = argparse.ArgumentParser()",
        "        parser.add_argument(\"--query\", required=True)",
        "        parser.add_argument(\"--response\", required=True)",
        "        parser.add_argument(\"--session\", default=None)",
        "        parser.add_argument(\"--files-ref\", nargs=\"*\", default=[])",
        "        parser.add_argument(\"--files-mod\", nargs=\"*\", default=[])",
        "        parser.add_argument(\"--tools\", nargs=\"*\", default=[])",
        "        parser.add_argument(\"--feedback\", choices=[\"positive\", \"negative\", \"neutral\"])",
        "",
        "        args = parser.parse_args(sys.argv[2:])",
        "",
        "        entry = log_chat(",
        "            query=args.query,",
        "            response=args.response,",
        "            session_id=args.session,",
        "            files_referenced=args.files_ref,",
        "            files_modified=args.files_mod,",
        "            tools_used=args.tools,",
        "            user_feedback=args.feedback,",
        "        )",
        "        print(f\"Logged chat: {entry.id}\")",
        "",
        "    elif command == \"action\":",
        "        # Log an action",
        "        import argparse",
        "        parser = argparse.ArgumentParser()",
        "        parser.add_argument(\"--type\", required=True)",
        "        parser.add_argument(\"--target\", required=True)",
        "        parser.add_argument(\"--session\", default=None)",
        "",
        "        args = parser.parse_args(sys.argv[2:])",
        "",
        "        entry = log_action(",
        "            action_type=args.type,",
        "            target=args.target,",
        "            session_id=args.session,",
        "        )",
        "        print(f\"Logged action: {entry.id}\")",
        "",
        "    elif command == \"stats\":",
        "        print_stats()",
        "",
        "    elif command == \"estimate\":",
        "        estimate_project_size()",
        "",
        "    elif command == \"quality-report\":",
        "        print_quality_report()",
        "",
        "    elif command == \"install-hooks\":",
        "        install_hooks()",
        "",
        "    elif command == \"validate\":",
        "        # Validate existing data against schemas",
        "        import argparse",
        "        parser = argparse.ArgumentParser()",
        "        parser.add_argument(\"--fix\", action=\"store_true\",",
        "                            help=\"Attempt to fix invalid entries\")",
        "        parser.add_argument(\"--verbose\", \"-v\", action=\"store_true\",",
        "                            help=\"Show all validation errors\")",
        "        args = parser.parse_args(sys.argv[2:])",
        "",
        "        print(\"\\n\" + \"=\" * 60)",
        "        print(\"VALIDATING ML DATA AGAINST SCHEMAS\")",
        "        print(\"=\" * 60)",
        "",
        "        all_errors = []",
        "",
        "        # Validate commits",
        "        if COMMITS_DIR.exists():",
        "            print(f\"\\n📁 Validating commits...\")",
        "            commit_files = list(COMMITS_DIR.glob(\"*.json\"))",
        "            commit_errors = 0",
        "            for f in commit_files:",
        "                try:",
        "                    with open(f, \"r\", encoding=\"utf-8\") as fp:",
        "                        data = json.load(fp)",
        "                    errors = validate_schema(data, COMMIT_SCHEMA, f\"commit:{f.name}\")",
        "                    if errors:",
        "                        commit_errors += 1",
        "                        all_errors.extend(errors)",
        "                        if args.verbose:",
        "                            for err in errors:",
        "                                print(f\"   ❌ {err}\")",
        "                except json.JSONDecodeError as e:",
        "                    commit_errors += 1",
        "                    all_errors.append(f\"commit:{f.name}: invalid JSON: {e}\")",
        "                    if args.verbose:",
        "                        print(f\"   ❌ {f.name}: invalid JSON\")",
        "            print(f\"   ✓ {len(commit_files) - commit_errors}/{len(commit_files)} valid\")",
        "",
        "        # Validate chats",
        "        if CHATS_DIR.exists():",
        "            print(f\"\\n📁 Validating chats...\")",
        "            chat_files = list(CHATS_DIR.glob(\"**/*.json\"))",
        "            chat_errors = 0",
        "            for f in chat_files:",
        "                try:",
        "                    with open(f, \"r\", encoding=\"utf-8\") as fp:",
        "                        data = json.load(fp)",
        "                    errors = validate_schema(data, CHAT_SCHEMA, f\"chat:{f.name}\")",
        "                    if errors:",
        "                        chat_errors += 1",
        "                        all_errors.extend(errors)",
        "                        if args.verbose:",
        "                            for err in errors:",
        "                                print(f\"   ❌ {err}\")",
        "                except json.JSONDecodeError as e:",
        "                    chat_errors += 1",
        "                    all_errors.append(f\"chat:{f.name}: invalid JSON: {e}\")",
        "            print(f\"   ✓ {len(chat_files) - chat_errors}/{len(chat_files)} valid\")",
        "",
        "        # Validate actions",
        "        if ACTIONS_DIR.exists():",
        "            print(f\"\\n📁 Validating actions...\")",
        "            action_files = list(ACTIONS_DIR.glob(\"**/*.json\"))",
        "            action_errors = 0",
        "            for f in action_files:",
        "                try:",
        "                    with open(f, \"r\", encoding=\"utf-8\") as fp:",
        "                        data = json.load(fp)",
        "                    errors = validate_schema(data, ACTION_SCHEMA, f\"action:{f.name}\")",
        "                    if errors:",
        "                        action_errors += 1",
        "                        all_errors.extend(errors)",
        "                        if args.verbose:",
        "                            for err in errors:",
        "                                print(f\"   ❌ {err}\")",
        "                except json.JSONDecodeError as e:",
        "                    action_errors += 1",
        "                    all_errors.append(f\"action:{f.name}: invalid JSON\")",
        "            print(f\"   ✓ {len(action_files) - action_errors}/{len(action_files)} valid\")",
        "",
        "        # Summary",
        "        print(\"\\n\" + \"-\" * 60)",
        "        if all_errors:",
        "            print(f\"⚠️  Found {len(all_errors)} validation errors\")",
        "            if not args.verbose:",
        "                print(\"   Run with --verbose to see details\")",
        "        else:",
        "            print(\"✅ All data validated successfully!\")",
        "        print(\"=\" * 60 + \"\\n\")",
        "",
        "    elif command == \"handoff\":",
        "        # Generate session handoff document",
        "        handoff_doc = generate_session_handoff()",
        "        print(handoff_doc)",
        "",
        "    elif command == \"session\":",
        "        # Session management for commit-chat linking",
        "        import argparse",
        "        parser = argparse.ArgumentParser()",
        "        parser.add_argument(\"action\", choices=[\"start\", \"end\", \"status\"],",
        "                            help=\"Session action\")",
        "        parser.add_argument(\"--summary\", help=\"Summary for end action\")",
        "        args = parser.parse_args(sys.argv[2:])",
        "",
        "        if args.action == \"start\":",
        "            session_id = start_session()",
        "            print(f\"Started session: {session_id}\")",
        "            print(\"Chats will be linked to commits made in this session.\")",
        "",
        "        elif args.action == \"end\":",
        "            session = end_session(args.summary)",
        "            if session:",
        "                print(f\"Ended session: {session['id']}\")",
        "                print(f\"  Chats logged: {len(session.get('chat_ids', []))}\")",
        "                print(f\"  Duration: {session.get('started_at', '?')} → {session.get('ended_at', '?')}\")",
        "            else:",
        "                print(\"No active session to end.\")",
        "",
        "        elif args.action == \"status\":",
        "            session = get_current_session()",
        "            if session:",
        "                print(\"\\n\" + \"=\" * 50)",
        "                print(\"CURRENT SESSION\")",
        "                print(\"=\" * 50)",
        "                print(f\"  ID:         {session['id']}\")",
        "                print(f\"  Started:    {session['started_at']}\")",
        "                print(f\"  Chats:      {len(session.get('chat_ids', []))}\")",
        "                if session.get('chat_ids'):",
        "                    print(\"  Chat IDs:\")",
        "                    for cid in session['chat_ids'][:5]:  # Show first 5",
        "                        print(f\"    - {cid}\")",
        "                    if len(session['chat_ids']) > 5:",
        "                        print(f\"    ... and {len(session['chat_ids']) - 5} more\")",
        "                print(\"=\" * 50 + \"\\n\")",
        "            else:",
        "                print(\"No active session. Use 'session start' to begin.\")",
        "",
        "    elif command == \"ci\":",
        "        # CI status integration for recording test results",
        "        import argparse",
        "        parser = argparse.ArgumentParser()",
        "        parser.add_argument(\"action\", choices=[\"set\", \"get\"],",
        "                            help=\"CI action\")",
        "        parser.add_argument(\"--commit\", required=True,",
        "                            help=\"Commit hash (full or prefix)\")",
        "        parser.add_argument(\"--result\", choices=[\"pass\", \"fail\", \"error\", \"pending\"],",
        "                            help=\"CI result (for set action)\")",
        "        parser.add_argument(\"--tests-passed\", type=int,",
        "                            help=\"Number of tests passed\")",
        "        parser.add_argument(\"--tests-failed\", type=int,",
        "                            help=\"Number of tests failed\")",
        "        parser.add_argument(\"--coverage\", type=float,",
        "                            help=\"Code coverage percentage\")",
        "        parser.add_argument(\"--duration\", type=float,",
        "                            help=\"CI duration in seconds\")",
        "        parser.add_argument(\"--message\", help=\"CI message or failure details\")",
        "        args = parser.parse_args(sys.argv[2:])",
        "",
        "        if args.action == \"set\":",
        "            if not args.result:",
        "                print(\"Error: --result is required for 'set' action\")",
        "                sys.exit(1)",
        "",
        "            # Build details dict from optional args",
        "            details = {}",
        "            if args.tests_passed is not None:",
        "                details['tests_passed'] = args.tests_passed",
        "            if args.tests_failed is not None:",
        "                details['tests_failed'] = args.tests_failed",
        "            if args.coverage is not None:",
        "                details['coverage'] = args.coverage",
        "            if args.duration is not None:",
        "                details['duration_seconds'] = args.duration",
        "            if args.message:",
        "                details['message'] = args.message",
        "",
        "            success = update_commit_ci_result(",
        "                args.commit,",
        "                args.result,",
        "                details if details else None",
        "            )",
        "",
        "            if success:",
        "                print(f\"✓ Updated CI result for {args.commit[:8]}: {args.result}\")",
        "                if details:",
        "                    for k, v in details.items():",
        "                        print(f\"  {k}: {v}\")",
        "            else:",
        "                print(f\"✗ Commit not found: {args.commit}\")",
        "                sys.exit(1)",
        "",
        "        elif args.action == \"get\":",
        "            commit_file = find_commit_file(args.commit)",
        "            if commit_file:",
        "                with open(commit_file, 'r', encoding='utf-8') as f:",
        "                    data = json.load(f)",
        "                print(f\"\\nCI Status for {data['hash'][:12]}:\")",
        "                print(f\"  Result: {data.get('ci_result', 'not set')}\")",
        "                if data.get('ci_details'):",
        "                    print(\"  Details:\")",
        "                    for k, v in data['ci_details'].items():",
        "                        print(f\"    {k}: {v}\")",
        "                if data.get('ci_updated_at'):",
        "                    print(f\"  Updated: {data['ci_updated_at']}\")",
        "            else:",
        "                print(f\"✗ Commit not found: {args.commit}\")",
        "                sys.exit(1)",
        "",
        "    elif command == \"revert\":",
        "        # Mark a commit as reverted",
        "        import argparse",
        "        parser = argparse.ArgumentParser()",
        "        parser.add_argument(\"--commit\", required=True,",
        "                            help=\"Commit that was reverted\")",
        "        parser.add_argument(\"--by\",",
        "                            help=\"Commit that performed the revert\")",
        "        args = parser.parse_args(sys.argv[2:])",
        "",
        "        success = mark_commit_reverted(args.commit, args.by)",
        "        if success:",
        "            print(f\"✓ Marked {args.commit[:8]} as reverted\")",
        "        else:",
        "            print(f\"✗ Commit not found: {args.commit}\")",
        "            sys.exit(1)",
        "",
        "    elif command == \"transcript\":",
        "        # Process a Claude Code transcript file (called by Stop hook)",
        "        import argparse",
        "        parser = argparse.ArgumentParser()",
        "        parser.add_argument(\"--file\", \"-f\", required=True,",
        "                            help=\"Path to transcript JSONL file\")",
        "        parser.add_argument(\"--session-id\",",
        "                            help=\"Session ID to use (auto-generated if not provided)\")",
        "        parser.add_argument(\"--dry-run\", action=\"store_true\",",
        "                            help=\"Parse and show stats without saving\")",
        "        parser.add_argument(\"--verbose\", \"-v\", action=\"store_true\",",
        "                            help=\"Show detailed output\")",
        "        args = parser.parse_args(sys.argv[2:])",
        "",
        "        filepath = Path(args.file)",
        "        if not filepath.exists():",
        "            print(f\"✗ Transcript file not found: {filepath}\")",
        "            sys.exit(1)",
        "",
        "        if args.verbose:",
        "            print(f\"\\n{'='*60}\")",
        "            print(\"PROCESSING CLAUDE CODE TRANSCRIPT\")",
        "            print(f\"{'='*60}\")",
        "            print(f\"File: {filepath}\")",
        "            print(f\"Size: {filepath.stat().st_size / 1024:.1f} KB\")",
        "",
        "        # Process the transcript",
        "        result = process_transcript(",
        "            filepath,",
        "            session_id=args.session_id,",
        "            save_exchanges=not args.dry_run",
        "        )",
        "",
        "        if args.verbose or args.dry_run:",
        "            print(f\"\\n📊 Transcript Analysis:\")",
        "            print(f\"   Exchanges found: {result.get('exchanges', 0)}\")",
        "            if not args.dry_run:",
        "                print(f\"   Exchanges saved: {result.get('saved', 0)}\")",
        "            print(f\"   Session ID: {result.get('session_id', 'N/A')}\")",
        "            print(f\"   Tools used: {', '.join(result.get('tools_used', [])) or 'none'}\")",
        "            print(f\"   Files referenced: {len(result.get('files_referenced', []))}\")",
        "            print(f\"   Files modified: {len(result.get('files_modified', []))}\")",
        "            print(f\"{'='*60}\\n\")",
        "        else:",
        "            # Minimal output for hook usage",
        "            saved = result.get('saved', 0)",
        "            if saved > 0:",
        "                print(f\"📝 ML: Captured {saved} exchange(s) from session\")",
        "",
        "    elif command == \"export\":",
        "        # Export data for training",
        "        import argparse",
        "        parser = argparse.ArgumentParser()",
        "        parser.add_argument(\"--format\", required=True,",
        "                            choices=[\"jsonl\", \"csv\", \"huggingface\"],",
        "                            help=\"Output format\")",
        "        parser.add_argument(\"--output\", required=True,",
        "                            help=\"Output file path\")",
        "        args = parser.parse_args(sys.argv[2:])",
        "",
        "        output_path = Path(args.output)",
        "",
        "        # Validate output path",
        "        if output_path.exists():",
        "            response = input(f\"⚠️  {output_path} already exists. Overwrite? [y/N] \")",
        "            if response.lower() != 'y':",
        "                print(\"Export cancelled.\")",
        "                sys.exit(0)",
        "",
        "        # Check if we have data to export",
        "        counts = count_data()",
        "        if counts['commits'] == 0 and counts['chats'] == 0:",
        "            print(\"⚠️  No data to export. Collect some commits and chats first.\")",
        "            sys.exit(1)",
        "",
        "        print(f\"\\n{'='*60}\")",
        "        print(\"EXPORTING ML DATA\")",
        "        print(f\"{'='*60}\")",
        "        print(f\"Format: {args.format}\")",
        "        print(f\"Output: {output_path}\")",
        "        print(f\"Data: {counts['commits']} commits, {counts['chats']} chats\")",
        "        print()",
        "",
        "        try:",
        "            stats = export_data(args.format, output_path)",
        "            print(f\"✅ Export complete!\")",
        "            print(f\"   Records: {stats['records']}\")",
        "            print(f\"   Commits: {stats['commits']}\")",
        "            print(f\"   Chats: {stats['chats']}\")",
        "            print(f\"   File: {stats['output_path']}\")",
        "            print(f\"{'='*60}\\n\")",
        "        except Exception as e:",
        "            print(f\"✗ Export failed: {e}\")",
        "            sys.exit(1)",
        "",
        "    elif command == \"feedback\":",
        "        # Add or view feedback for chat entries",
        "        import argparse",
        "        parser = argparse.ArgumentParser()",
        "        parser.add_argument(\"--chat-id\",",
        "                            help=\"Chat ID to add feedback to\")",
        "        parser.add_argument(\"--rating\", choices=[\"good\", \"bad\", \"neutral\"],",
        "                            help=\"Feedback rating\")",
        "        parser.add_argument(\"--comment\",",
        "                            help=\"Optional feedback comment\")",
        "        parser.add_argument(\"--force\", action=\"store_true\",",
        "                            help=\"Overwrite existing feedback\")",
        "        parser.add_argument(\"--list\", action=\"store_true\",",
        "                            help=\"List recent chats (showing feedback status)\")",
        "        parser.add_argument(\"--limit\", type=int, default=10,",
        "                            help=\"Number of chats to show (default: 10)\")",
        "        args = parser.parse_args(sys.argv[2:])",
        "",
        "        if args.list:",
        "            # List recent chats and their feedback status",
        "            chats = list_chats_needing_feedback(limit=args.limit)",
        "",
        "            if not chats:",
        "                print(\"No chat entries found.\")",
        "                return",
        "",
        "            print(f\"\\n{'='*60}\")",
        "            print(f\"RECENT CHATS (last {args.limit})\")",
        "            print(f\"{'='*60}\\n\")",
        "",
        "            for chat in chats:",
        "                feedback_status = \"✓\" if chat['has_feedback'] else \"○\"",
        "                rating_display = f\" [{chat['feedback_rating']}]\" if chat['feedback_rating'] else \"\"",
        "",
        "                print(f\"{feedback_status} {chat['id']}\")",
        "                print(f\"   Time: {chat['timestamp']}\")",
        "                print(f\"   Query: {chat['query']}\")",
        "                if chat['has_feedback']:",
        "                    print(f\"   Feedback: {chat['feedback_rating']}\")",
        "                print()",
        "",
        "            # Show summary",
        "            with_feedback = sum(1 for c in chats if c['has_feedback'])",
        "            without_feedback = len(chats) - with_feedback",
        "            print(f\"{'='*60}\")",
        "            print(f\"Summary: {with_feedback} with feedback, {without_feedback} without\")",
        "            print(f\"{'='*60}\\n\")",
        "",
        "        else:",
        "            # Add feedback to a specific chat",
        "            if not args.chat_id:",
        "                print(\"Error: --chat-id is required when not using --list\")",
        "                sys.exit(1)",
        "",
        "            if not args.rating:",
        "                print(\"Error: --rating is required when adding feedback\")",
        "                sys.exit(1)",
        "",
        "            try:",
        "                success = add_chat_feedback(",
        "                    chat_id=args.chat_id,",
        "                    rating=args.rating,",
        "                    comment=args.comment,",
        "                    force=args.force",
        "                )",
        "",
        "                if success:",
        "                    print(f\"✓ Added feedback to chat {args.chat_id}\")",
        "                    print(f\"   Rating: {args.rating}\")",
        "                    if args.comment:",
        "                        print(f\"   Comment: {args.comment}\")",
        "                else:",
        "                    # Check if chat exists or already has feedback",
        "                    chat_file = find_chat_file(args.chat_id)",
        "                    if not chat_file:",
        "                        print(f\"✗ Chat not found: {args.chat_id}\")",
        "                        sys.exit(1)",
        "                    else:",
        "                        print(f\"✗ Chat {args.chat_id} already has feedback.\")",
        "                        print(\"   Use --force to overwrite.\")",
        "                        sys.exit(1)",
        "",
        "            except ValueError as e:",
        "                print(f\"✗ {e}\")",
        "                sys.exit(1)",
        "",
        "    else:",
        "        print(f\"Unknown command: {command}\")",
        "        print(__doc__)",
        "",
        "",
        "if __name__ == \"__main__\":",
        "    main()"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "scripts/run_tests.py",
      "function": null,
      "start_line": 1,
      "lines_added": [
        "Automatically installs pytest/coverage if missing (can be disabled).",
        "",
        "    -v, --verbose       Show verbose output",
        "    -q, --quiet         Show minimal output",
        "    --no-capture        Show print statements (pytest -s)",
        "    --failfast, -x      Stop on first failure",
        "    --no-auto-install   Do not auto-install missing dependencies",
        "    --check-deps        Only check dependencies, do not run tests",
        "",
        "Dependency Handling:",
        "    By default, this script will attempt to install pytest and coverage",
        "    if they are not available. Use --no-auto-install to disable this.",
        "    Use --check-deps to see what's installed without running tests.",
        "# Test dependencies with their pip package names",
        "TEST_DEPENDENCIES = {",
        "    'pytest': 'pytest',",
        "    'coverage': 'coverage',",
        "}",
        "",
        "# Cache for dependency check results",
        "_deps_checked = False",
        "_deps_available = {}",
        "",
        "",
        "def check_dependency(module_name):",
        "    \"\"\"Check if a Python module is importable.\"\"\"",
        "    try:",
        "        __import__(module_name)",
        "        return True",
        "    except ImportError:",
        "        return False",
        "",
        "",
        "def install_dependency(package_name, quiet=True):",
        "    \"\"\"Attempt to install a package via pip.\"\"\"",
        "    cmd = [sys.executable, '-m', 'pip', 'install', package_name]",
        "    if quiet:",
        "        cmd.append('-q')",
        "",
        "    try:",
        "        result = subprocess.run(cmd, capture_output=quiet, timeout=120)",
        "        return result.returncode == 0",
        "    except (subprocess.TimeoutExpired, Exception):",
        "        return False",
        "",
        "",
        "def ensure_test_dependencies(auto_install=True, verbose=False):",
        "    \"\"\"",
        "    Ensure test dependencies are available.",
        "",
        "    Args:",
        "        auto_install: If True, attempt to install missing dependencies",
        "        verbose: If True, print status messages",
        "",
        "    Returns:",
        "        dict: {module_name: bool} indicating availability",
        "",
        "    This function:",
        "    1. Checks if pytest/coverage are installed",
        "    2. If missing and auto_install=True, attempts to install them",
        "    3. Returns availability status for each dependency",
        "    4. Caches results to avoid repeated checks",
        "    \"\"\"",
        "    global _deps_checked, _deps_available",
        "",
        "    # Return cached results if already checked",
        "    if _deps_checked:",
        "        return _deps_available",
        "",
        "    missing = []",
        "",
        "    for module_name, package_name in TEST_DEPENDENCIES.items():",
        "        if check_dependency(module_name):",
        "            _deps_available[module_name] = True",
        "            if verbose:",
        "                print(f\"  ✓ {module_name} available\")",
        "        else:",
        "            missing.append((module_name, package_name))",
        "            _deps_available[module_name] = False",
        "",
        "    # Attempt to install missing dependencies",
        "    if missing and auto_install:",
        "        if verbose:",
        "            print(f\"\\n📦 Installing missing test dependencies...\")",
        "",
        "        for module_name, package_name in missing:",
        "            if verbose:",
        "                print(f\"  Installing {package_name}...\", end=' ', flush=True)",
        "",
        "            if install_dependency(package_name, quiet=not verbose):",
        "                _deps_available[module_name] = True",
        "                if verbose:",
        "                    print(\"✓\")",
        "            else:",
        "                if verbose:",
        "                    print(\"✗\")",
        "                # Don't print error here - let caller handle it",
        "",
        "    _deps_checked = True",
        "    return _deps_available",
        "",
        "",
        "def get_fallback_command(category):",
        "    \"\"\"",
        "    Get a fallback unittest command when pytest is unavailable.",
        "",
        "    Returns a command that will work without pytest.",
        "    \"\"\"",
        "    # Map categories to unittest-compatible test discovery",
        "    paths = []",
        "",
        "    if category in CATEGORIES:",
        "        paths = list(CATEGORIES[category].get('paths', []))",
        "        paths.extend(CATEGORIES[category].get('also_run', []))",
        "    elif category in SUITES:",
        "        for cat in SUITES[category]:",
        "            paths.extend(CATEGORIES[cat].get('paths', []))",
        "            paths.extend(CATEGORIES[cat].get('also_run', []))",
        "",
        "    return [sys.executable, '-m', 'unittest', 'discover', '-s', 'tests', '-v']",
        "",
        ""
      ],
      "lines_removed": [
        "    -v, --verbose    Show verbose output",
        "    -q, --quiet      Show minimal output",
        "    --no-capture     Show print statements (pytest -s)",
        "    --failfast       Stop on first failure"
      ],
      "context_before": [
        "#!/usr/bin/env python3",
        "\"\"\"",
        "Test Runner Script",
        "==================",
        "",
        "Convenient script for running different test categories locally.",
        ""
      ],
      "context_after": [
        "Usage:",
        "    python scripts/run_tests.py              # Run all tests",
        "    python scripts/run_tests.py smoke        # Run smoke tests only",
        "    python scripts/run_tests.py unit         # Run unit tests",
        "    python scripts/run_tests.py integration  # Run integration tests",
        "    python scripts/run_tests.py performance  # Run performance tests (no coverage)",
        "    python scripts/run_tests.py regression   # Run regression tests",
        "    python scripts/run_tests.py behavioral   # Run behavioral tests",
        "    python scripts/run_tests.py quick        # Run smoke + unit (fast feedback)",
        "    python scripts/run_tests.py precommit    # Run smoke + unit + integration",
        "    python scripts/run_tests.py coverage     # Run with coverage report",
        "",
        "Options:",
        "\"\"\"",
        "",
        "import argparse",
        "import subprocess",
        "import sys",
        "import os",
        "import time",
        "",
        "# Ensure we're running from repo root",
        "REPO_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))",
        "os.chdir(REPO_ROOT)",
        "",
        "",
        "# Test category definitions",
        "CATEGORIES = {",
        "    'smoke': {",
        "        'description': 'Quick sanity checks',",
        "        'paths': ['tests/smoke/'],"
      ],
      "change_type": "modify"
    },
    {
      "file": "scripts/run_tests.py",
      "function": "SUITES = {",
      "start_line": 100,
      "lines_added": [
        "    \"\"\"Run pytest with the given paths and options.",
        "",
        "    If pytest is not available, falls back to unittest with a warning.",
        "    \"\"\"",
        "    deps = ensure_test_dependencies(auto_install=True, verbose=verbose)",
        "",
        "    if not deps.get('pytest', False):",
        "        print(\"\\n⚠️  pytest not available and could not be installed.\")",
        "        print(\"   Falling back to unittest (some features may not work).\")",
        "        print(\"   To install manually: pip install pytest\\n\")",
        "        return subprocess.run(",
        "            [sys.executable, '-m', 'unittest', 'discover', '-s', 'tests', '-v']",
        "        ).returncode",
        ""
      ],
      "lines_removed": [
        "    \"\"\"Run pytest with the given paths and options.\"\"\""
      ],
      "context_before": [
        "def print_header(text, char='='):",
        "    \"\"\"Print a formatted header.\"\"\"",
        "    width = 70",
        "    print(f\"\\n{char * width}\")",
        "    print(f\" {text}\")",
        "    print(f\"{char * width}\")",
        "",
        "",
        "def run_pytest(paths, verbose=False, quiet=False, no_capture=False,",
        "               failfast=False, no_coverage=False):"
      ],
      "context_after": [
        "    cmd = [sys.executable, '-m', 'pytest']",
        "",
        "    # Add paths",
        "    cmd.extend(paths)",
        "",
        "    # Add options",
        "    if verbose:",
        "        cmd.append('-v')",
        "    elif quiet:",
        "        cmd.append('-q')"
      ],
      "change_type": "modify"
    },
    {
      "file": "scripts/run_tests.py",
      "function": "def run_category(category, verbose=False, quiet=False, no_capture=False,",
      "start_line": 187,
      "lines_added": [
        "    \"\"\"Run full test suite with coverage.",
        "",
        "    If coverage is not available, attempts to install it first.",
        "    Falls back to running tests without coverage if installation fails.",
        "    \"\"\"",
        "    deps = ensure_test_dependencies(auto_install=True, verbose=True)",
        "",
        "    if not deps.get('coverage', False):",
        "        print(\"\\n⚠️  coverage not available and could not be installed.\")",
        "        print(\"   Running tests without coverage.\")",
        "        print(\"   To install manually: pip install coverage\\n\")",
        "        return subprocess.run(",
        "            [sys.executable, '-m', 'unittest', 'discover', '-s', 'tests', '-v']",
        "        ).returncode",
        ""
      ],
      "lines_removed": [
        "    \"\"\"Run full test suite with coverage.\"\"\""
      ],
      "context_before": [
        "",
        "    if result == 0:",
        "        print(f\"\\n✅ {category.upper()} tests PASSED in {elapsed:.1f}s\")",
        "    else:",
        "        print(f\"\\n❌ {category.upper()} tests FAILED in {elapsed:.1f}s\")",
        "",
        "    return result",
        "",
        "",
        "def run_with_coverage():"
      ],
      "context_after": [
        "    print_header(\"Running Full Test Suite with Coverage\")",
        "",
        "    cmd = [",
        "        sys.executable, '-m', 'coverage', 'run', '--source=cortical',",
        "        '-m', 'unittest', 'discover', '-s', 'tests', '-v'",
        "    ]",
        "",
        "    result = subprocess.run(cmd).returncode",
        "",
        "    if result == 0:",
        "        print(\"\\n--- Coverage Report ---\")",
        "        subprocess.run(["
      ],
      "change_type": "modify"
    },
    {
      "file": "scripts/run_tests.py",
      "function": "def main():",
      "start_line": 230,
      "lines_added": [
        "    parser.add_argument('--no-auto-install', action='store_true',",
        "                        help='Do not auto-install missing dependencies')",
        "    parser.add_argument('--check-deps', action='store_true',",
        "                        help='Only check dependencies, do not run tests')",
        "    # Handle dependency check mode",
        "    if args.check_deps:",
        "        print(\"Checking test dependencies...\")",
        "        deps = ensure_test_dependencies(auto_install=False, verbose=True)",
        "        missing = [k for k, v in deps.items() if not v]",
        "        if missing:",
        "            print(f\"\\n❌ Missing: {', '.join(missing)}\")",
        "            print(f\"   Install with: pip install {' '.join(missing)}\")",
        "            sys.exit(1)",
        "        else:",
        "            print(\"\\n✅ All test dependencies available\")",
        "            sys.exit(0)",
        "",
        "    # Pre-check dependencies (silently auto-install unless --no-auto-install)",
        "    if not args.no_auto_install and not args.quiet:",
        "        deps = ensure_test_dependencies(auto_install=True, verbose=args.verbose)",
        "        missing = [k for k, v in deps.items() if not v]",
        "        if missing and not args.quiet:",
        "            print(f\"⚠️  Some dependencies unavailable: {', '.join(missing)}\")",
        "            print(\"   Tests will run with reduced functionality.\\n\")",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "    )",
        "",
        "    parser.add_argument('-v', '--verbose', action='store_true',",
        "                        help='Verbose output')",
        "    parser.add_argument('-q', '--quiet', action='store_true',",
        "                        help='Quiet output')",
        "    parser.add_argument('--no-capture', action='store_true',",
        "                        help='Show print statements')",
        "    parser.add_argument('--failfast', '-x', action='store_true',",
        "                        help='Stop on first failure')"
      ],
      "context_after": [
        "",
        "    args = parser.parse_args()",
        "",
        "    # Handle coverage mode",
        "    if args.category == 'coverage':",
        "        sys.exit(run_with_coverage())",
        "",
        "    # Handle suites",
        "    if args.category in SUITES:",
        "        categories = SUITES[args.category]",
        "    else:",
        "        categories = [args.category]",
        ""
      ],
      "change_type": "add"
    },
    {
      "file": "tasks/2025-12-15_11-09-14_3f69.json",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "{",
        "  \"version\": 1,",
        "  \"session_id\": \"3f69\",",
        "  \"started_at\": \"2025-12-15T11:09:14.066092\",",
        "  \"saved_at\": \"2025-12-15T11:29:29.122166\",",
        "  \"tasks\": [",
        "    {",
        "      \"id\": \"T-20251215-110918-3f69-001\",",
        "      \"title\": \"ML-P0: Build training data export command\",",
        "      \"status\": \"completed\",",
        "      \"priority\": \"high\",",
        "      \"category\": \"feature\",",
        "      \"description\": \"Add 'export' command to ml_data_collector.py with formats: jsonl, huggingface, csv\",",
        "      \"depends_on\": [],",
        "      \"effort\": \"medium\",",
        "      \"created_at\": \"2025-12-15T11:09:18.000924\",",
        "      \"updated_at\": \"2025-12-15T11:29:17.806814\",",
        "      \"completed_at\": \"2025-12-15T11:29:17.806814\",",
        "      \"context\": {},",
        "      \"retrospective\": null",
        "    },",
        "    {",
        "      \"id\": \"T-20251215-110922-3f69-002\",",
        "      \"title\": \"ML-P1: Add feedback collection command\",",
        "      \"status\": \"completed\",",
        "      \"priority\": \"medium\",",
        "      \"category\": \"feature\",",
        "      \"description\": \"Add 'feedback' command to rate chat quality (good/bad/neutral)\",",
        "      \"depends_on\": [],",
        "      \"effort\": \"medium\",",
        "      \"created_at\": \"2025-12-15T11:09:22.443713\",",
        "      \"updated_at\": \"2025-12-15T11:29:28.329042\",",
        "      \"completed_at\": \"2025-12-15T11:29:28.329042\",",
        "      \"context\": {},",
        "      \"retrospective\": null",
        "    },",
        "    {",
        "      \"id\": \"T-20251215-110927-3f69-003\",",
        "      \"title\": \"ML-P1: Add data quality metrics command\",",
        "      \"status\": \"completed\",",
        "      \"priority\": \"medium\",",
        "      \"category\": \"feature\",",
        "      \"description\": \"Add 'quality-report' command showing data completeness, diversity, anomalies\",",
        "      \"depends_on\": [],",
        "      \"effort\": \"medium\",",
        "      \"created_at\": \"2025-12-15T11:09:27.698976\",",
        "      \"updated_at\": \"2025-12-15T11:29:28.464546\",",
        "      \"completed_at\": \"2025-12-15T11:29:28.464546\",",
        "      \"context\": {},",
        "      \"retrospective\": null",
        "    },",
        "    {",
        "      \"id\": \"T-20251215-110941-3f69-004\",",
        "      \"title\": \"ML-P2: Expand Bash file extraction\",",
        "      \"status\": \"completed\",",
        "      \"priority\": \"low\",",
        "      \"category\": \"bugfix\",",
        "      \"description\": \"Add more file extensions to extract_files_from_tool_inputs() - .yaml, .yml, .sh, .js, .ts, etc.\",",
        "      \"depends_on\": [],",
        "      \"effort\": \"medium\",",
        "      \"created_at\": \"2025-12-15T11:09:41.688758\",",
        "      \"updated_at\": \"2025-12-15T11:29:28.598066\",",
        "      \"completed_at\": \"2025-12-15T11:29:28.598066\",",
        "      \"context\": {},",
        "      \"retrospective\": null",
        "    },",
        "    {",
        "      \"id\": \"T-20251215-110946-3f69-005\",",
        "      \"title\": \"ML-P2: Add NotebookEdit tool support\",",
        "      \"status\": \"completed\",",
        "      \"priority\": \"low\",",
        "      \"category\": \"bugfix\",",
        "      \"description\": \"Track notebook_path from NotebookEdit tool in file modifications\",",
        "      \"depends_on\": [],",
        "      \"effort\": \"medium\",",
        "      \"created_at\": \"2025-12-15T11:09:46.589225\",",
        "      \"updated_at\": \"2025-12-15T11:29:28.734449\",",
        "      \"completed_at\": \"2025-12-15T11:29:28.734449\",",
        "      \"context\": {},",
        "      \"retrospective\": null",
        "    },",
        "    {",
        "      \"id\": \"T-20251215-110950-3f69-006\",",
        "      \"title\": \"DOC-P1: Fix query.py references in docs\",",
        "      \"status\": \"completed\",",
        "      \"priority\": \"medium\",",
        "      \"category\": \"docs\",",
        "      \"description\": \"Update all query.py:XX-YY references to use query/module.py paths (algorithms.md, glossary.md, dogfooding.md)\",",
        "      \"depends_on\": [],",
        "      \"effort\": \"medium\",",
        "      \"created_at\": \"2025-12-15T11:09:50.743136\",",
        "      \"updated_at\": \"2025-12-15T11:29:28.869699\",",
        "      \"completed_at\": \"2025-12-15T11:29:28.869699\",",
        "      \"context\": {},",
        "      \"retrospective\": null",
        "    },",
        "    {",
        "      \"id\": \"T-20251215-110955-3f69-007\",",
        "      \"title\": \"DOC-P1: Fix processor.py references in docs\",",
        "      \"status\": \"completed\",",
        "      \"priority\": \"medium\",",
        "      \"category\": \"docs\",",
        "      \"description\": \"Update processor.py references to processor/ package structure in CLAUDE.md, docs/*.md\",",
        "      \"depends_on\": [],",
        "      \"effort\": \"medium\",",
        "      \"created_at\": \"2025-12-15T11:09:55.315663\",",
        "      \"updated_at\": \"2025-12-15T11:29:28.994181\",",
        "      \"completed_at\": \"2025-12-15T11:29:28.994181\",",
        "      \"context\": {},",
        "      \"retrospective\": null",
        "    },",
        "    {",
        "      \"id\": \"T-20251215-111007-3f69-008\",",
        "      \"title\": \"ML-TEST: Unit tests for ML data collector\",",
        "      \"status\": \"completed\",",
        "      \"priority\": \"high\",",
        "      \"category\": \"test\",",
        "      \"description\": \"Add unit tests for export, feedback, quality-report commands\",",
        "      \"depends_on\": [],",
        "      \"effort\": \"medium\",",
        "      \"created_at\": \"2025-12-15T11:10:07.816567\",",
        "      \"updated_at\": \"2025-12-15T11:29:29.121986\",",
        "      \"completed_at\": \"2025-12-15T11:29:29.121986\",",
        "      \"context\": {},",
        "      \"retrospective\": null",
        "    }",
        "  ]",
        "}"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "tests/unit/test_ml_export.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "#!/usr/bin/env python3",
        "\"\"\"",
        "Unit tests for ML data collector export functionality.",
        "",
        "Tests the export_data() function and related helpers for exporting",
        "collected ML data in training-ready formats (JSONL, CSV, HuggingFace).",
        "\"\"\"",
        "",
        "import csv",
        "import json",
        "import os",
        "import sys",
        "import tempfile",
        "import unittest",
        "from datetime import datetime",
        "from pathlib import Path",
        "",
        "# Add scripts to path",
        "sys.path.insert(0, str(Path(__file__).parent.parent.parent / \"scripts\"))",
        "",
        "import ml_data_collector as ml",
        "",
        "",
        "class TestSummarizeDiff(unittest.TestCase):",
        "    \"\"\"Tests for _summarize_diff helper function.\"\"\"",
        "",
        "    def test_empty_hunks(self):",
        "        \"\"\"Should return empty string for empty hunks list.\"\"\"",
        "        result = ml._summarize_diff([])",
        "        self.assertEqual(result, \"\")",
        "",
        "    def test_single_file_add_hunk(self):",
        "        \"\"\"Should summarize single add hunk.\"\"\"",
        "        hunks = [",
        "            {\"file\": \"test.py\", \"change_type\": \"add\"}",
        "        ]",
        "        result = ml._summarize_diff(hunks)",
        "        self.assertEqual(result, \"test.py: +1\")",
        "",
        "    def test_single_file_delete_hunk(self):",
        "        \"\"\"Should summarize single delete hunk.\"\"\"",
        "        hunks = [",
        "            {\"file\": \"test.py\", \"change_type\": \"delete\"}",
        "        ]",
        "        result = ml._summarize_diff(hunks)",
        "        self.assertEqual(result, \"test.py: -1\")",
        "",
        "    def test_single_file_modify_hunk(self):",
        "        \"\"\"Should summarize single modify hunk.\"\"\"",
        "        hunks = [",
        "            {\"file\": \"test.py\", \"change_type\": \"modify\"}",
        "        ]",
        "        result = ml._summarize_diff(hunks)",
        "        self.assertEqual(result, \"test.py: ~1\")",
        "",
        "    def test_single_file_multiple_changes(self):",
        "        \"\"\"Should summarize multiple changes to same file.\"\"\"",
        "        hunks = [",
        "            {\"file\": \"test.py\", \"change_type\": \"add\"},",
        "            {\"file\": \"test.py\", \"change_type\": \"modify\"},",
        "            {\"file\": \"test.py\", \"change_type\": \"modify\"},",
        "        ]",
        "        result = ml._summarize_diff(hunks)",
        "        self.assertEqual(result, \"test.py: +1 ~2\")",
        "",
        "    def test_multiple_files(self):",
        "        \"\"\"Should summarize changes across multiple files.\"\"\"",
        "        hunks = [",
        "            {\"file\": \"file1.py\", \"change_type\": \"add\"},",
        "            {\"file\": \"file2.py\", \"change_type\": \"delete\"},",
        "            {\"file\": \"file3.py\", \"change_type\": \"modify\"},",
        "        ]",
        "        result = ml._summarize_diff(hunks)",
        "        self.assertIn(\"file1.py: +1\", result)",
        "        self.assertIn(\"file2.py: -1\", result)",
        "        self.assertIn(\"file3.py: ~1\", result)",
        "        # Files separated by semicolons",
        "        self.assertEqual(result.count(\";\"), 2)",
        "",
        "    def test_missing_file_field(self):",
        "        \"\"\"Should handle hunks missing file field.\"\"\"",
        "        hunks = [",
        "            {\"change_type\": \"add\"},",
        "        ]",
        "        result = ml._summarize_diff(hunks)",
        "        self.assertIn(\"unknown\", result)",
        "",
        "    def test_missing_change_type_field(self):",
        "        \"\"\"Should default to modify when change_type missing.\"\"\"",
        "        hunks = [",
        "            {\"file\": \"test.py\"},",
        "        ]",
        "        result = ml._summarize_diff(hunks)",
        "        self.assertEqual(result, \"test.py: ~1\")",
        "",
        "    def test_limit_to_ten_files(self):",
        "        \"\"\"Should limit summary to first 10 files.\"\"\"",
        "        hunks = [",
        "            {\"file\": f\"file{i}.py\", \"change_type\": \"modify\"}",
        "            for i in range(20)",
        "        ]",
        "        result = ml._summarize_diff(hunks)",
        "        # Should only include first 10 files (9 semicolons)",
        "        self.assertEqual(result.count(\";\"), 9)",
        "",
        "",
        "class TestExportData(unittest.TestCase):",
        "    \"\"\"Tests for export_data() function.\"\"\"",
        "",
        "    def setUp(self):",
        "        \"\"\"Set up test environment with temporary directories.\"\"\"",
        "        self.test_dir = tempfile.TemporaryDirectory()",
        "        self.test_path = Path(self.test_dir.name)",
        "",
        "        # Temporarily override ML data directories",
        "        self.original_ml_data_dir = ml.ML_DATA_DIR",
        "        self.original_commits_dir = ml.COMMITS_DIR",
        "        self.original_chats_dir = ml.CHATS_DIR",
        "",
        "        ml.ML_DATA_DIR = self.test_path / \".git-ml\"",
        "        ml.COMMITS_DIR = ml.ML_DATA_DIR / \"commits\"",
        "        ml.CHATS_DIR = ml.ML_DATA_DIR / \"chats\"",
        "",
        "        # Create test directories",
        "        ml.ensure_dirs()",
        "",
        "    def tearDown(self):",
        "        \"\"\"Clean up test environment.\"\"\"",
        "        # Restore original directories",
        "        ml.ML_DATA_DIR = self.original_ml_data_dir",
        "        ml.COMMITS_DIR = self.original_commits_dir",
        "        ml.CHATS_DIR = self.original_chats_dir",
        "",
        "        # Clean up temp directory",
        "        self.test_dir.cleanup()",
        "",
        "    def test_export_empty_data_jsonl(self):",
        "        \"\"\"Should export empty JSONL file when no data exists.\"\"\"",
        "        output_path = self.test_path / \"export.jsonl\"",
        "        stats = ml.export_data(\"jsonl\", output_path)",
        "",
        "        # Check stats",
        "        self.assertEqual(stats[\"format\"], \"jsonl\")",
        "        self.assertEqual(stats[\"records\"], 0)",
        "        self.assertEqual(stats[\"commits\"], 0)",
        "        self.assertEqual(stats[\"chats\"], 0)",
        "        self.assertEqual(stats[\"output_path\"], str(output_path))",
        "",
        "        # Check file is empty",
        "        self.assertTrue(output_path.exists())",
        "        content = output_path.read_text()",
        "        self.assertEqual(content, \"\")",
        "",
        "    def test_export_empty_data_csv(self):",
        "        \"\"\"Should export CSV with headers only when no data exists.\"\"\"",
        "        output_path = self.test_path / \"export.csv\"",
        "        stats = ml.export_data(\"csv\", output_path)",
        "",
        "        # Check stats",
        "        self.assertEqual(stats[\"format\"], \"csv\")",
        "        self.assertEqual(stats[\"records\"], 0)",
        "",
        "        # Check file has headers only",
        "        self.assertTrue(output_path.exists())",
        "        with open(output_path, 'r', encoding='utf-8') as f:",
        "            reader = csv.DictReader(f)",
        "            headers = reader.fieldnames",
        "            self.assertIn('type', headers)",
        "            self.assertIn('timestamp', headers)",
        "            self.assertIn('input', headers)",
        "            self.assertIn('output', headers)",
        "            # Should have no data rows",
        "            rows = list(reader)",
        "            self.assertEqual(len(rows), 0)",
        "",
        "    def test_export_empty_data_huggingface(self):",
        "        \"\"\"Should export empty HuggingFace format when no data exists.\"\"\"",
        "        output_path = self.test_path / \"export.json\"",
        "        stats = ml.export_data(\"huggingface\", output_path)",
        "",
        "        # Check stats",
        "        self.assertEqual(stats[\"format\"], \"huggingface\")",
        "        self.assertEqual(stats[\"records\"], 0)",
        "",
        "        # Check file has correct structure",
        "        self.assertTrue(output_path.exists())",
        "        with open(output_path, 'r', encoding='utf-8') as f:",
        "            data = json.load(f)",
        "",
        "        # Should be dict of lists",
        "        self.assertIsInstance(data, dict)",
        "        self.assertIn('type', data)",
        "        self.assertIn('timestamp', data)",
        "        self.assertIn('input', data)",
        "        self.assertIn('output', data)",
        "        self.assertIn('session_id', data)",
        "        self.assertIn('files', data)",
        "        self.assertIn('tools_used', data)",
        "",
        "        # All lists should be empty",
        "        for key in data:",
        "            self.assertIsInstance(data[key], list)",
        "            self.assertEqual(len(data[key]), 0)",
        "",
        "    def test_export_with_commit_data_jsonl(self):",
        "        \"\"\"Should export commit data in JSONL format.\"\"\"",
        "        # Create mock commit",
        "        commit_data = {",
        "            \"hash\": \"abc123\",",
        "            \"message\": \"feat: Add feature\",",
        "            \"timestamp\": \"2025-12-15T10:00:00\",",
        "            \"files_changed\": [\"file1.py\", \"file2.py\"],",
        "            \"insertions\": 50,",
        "            \"deletions\": 10,",
        "            \"branch\": \"main\",",
        "            \"session_id\": \"sess1\",",
        "            \"hunks\": [",
        "                {\"file\": \"file1.py\", \"change_type\": \"add\"},",
        "                {\"file\": \"file2.py\", \"change_type\": \"modify\"},",
        "            ]",
        "        }",
        "",
        "        commit_file = ml.COMMITS_DIR / \"abc123_test.json\"",
        "        with open(commit_file, 'w', encoding='utf-8') as f:",
        "            json.dump(commit_data, f)",
        "",
        "        # Export",
        "        output_path = self.test_path / \"export.jsonl\"",
        "        stats = ml.export_data(\"jsonl\", output_path)",
        "",
        "        # Check stats",
        "        self.assertEqual(stats[\"records\"], 1)",
        "        self.assertEqual(stats[\"commits\"], 1)",
        "        self.assertEqual(stats[\"chats\"], 0)",
        "",
        "        # Check JSONL content",
        "        with open(output_path, 'r', encoding='utf-8') as f:",
        "            lines = f.readlines()",
        "",
        "        self.assertEqual(len(lines), 1)",
        "        record = json.loads(lines[0])",
        "",
        "        # Check record structure",
        "        self.assertEqual(record[\"type\"], \"commit\")",
        "        self.assertEqual(record[\"timestamp\"], \"2025-12-15T10:00:00\")",
        "        self.assertEqual(record[\"input\"], \"feat: Add feature\")",
        "        self.assertIn(\"file1.py\", record[\"output\"])  # Diff summary",
        "        self.assertEqual(record[\"context\"][\"session_id\"], \"sess1\")",
        "        self.assertEqual(record[\"context\"][\"files\"], [\"file1.py\", \"file2.py\"])",
        "        self.assertEqual(record[\"context\"][\"insertions\"], 50)",
        "        self.assertEqual(record[\"context\"][\"deletions\"], 10)",
        "",
        "    def test_export_with_chat_data_jsonl(self):",
        "        \"\"\"Should export chat data in JSONL format.\"\"\"",
        "        # Create mock chat",
        "        chat_data = {",
        "            \"id\": \"chat-001\",",
        "            \"timestamp\": \"2025-12-15T11:00:00\",",
        "            \"session_id\": \"sess1\",",
        "            \"query\": \"How do I fix the bug?\",",
        "            \"response\": \"You need to update line 42\",",
        "            \"files_referenced\": [\"bug.py\"],",
        "            \"files_modified\": [\"bug.py\", \"test.py\"],",
        "            \"tools_used\": [\"Read\", \"Edit\"],",
        "        }",
        "",
        "        chat_file = ml.CHATS_DIR / \"2025-12-15\" / \"chat-001.json\"",
        "        chat_file.parent.mkdir(parents=True, exist_ok=True)",
        "        with open(chat_file, 'w', encoding='utf-8') as f:",
        "            json.dump(chat_data, f)",
        "",
        "        # Export",
        "        output_path = self.test_path / \"export.jsonl\"",
        "        stats = ml.export_data(\"jsonl\", output_path)",
        "",
        "        # Check stats",
        "        self.assertEqual(stats[\"records\"], 1)",
        "        self.assertEqual(stats[\"commits\"], 0)",
        "        self.assertEqual(stats[\"chats\"], 1)",
        "",
        "        # Check JSONL content",
        "        with open(output_path, 'r', encoding='utf-8') as f:",
        "            lines = f.readlines()",
        "",
        "        self.assertEqual(len(lines), 1)",
        "        record = json.loads(lines[0])",
        "",
        "        # Check record structure",
        "        self.assertEqual(record[\"type\"], \"chat\")",
        "        self.assertEqual(record[\"timestamp\"], \"2025-12-15T11:00:00\")",
        "        self.assertEqual(record[\"input\"], \"How do I fix the bug?\")",
        "        self.assertEqual(record[\"output\"], \"You need to update line 42\")",
        "        self.assertEqual(record[\"context\"][\"session_id\"], \"sess1\")",
        "        self.assertIn(\"bug.py\", record[\"context\"][\"files\"])",
        "        self.assertIn(\"test.py\", record[\"context\"][\"files\"])",
        "        self.assertEqual(record[\"context\"][\"tools_used\"], [\"Read\", \"Edit\"])",
        "",
        "    def test_export_mixed_data_csv(self):",
        "        \"\"\"Should export both commits and chats in CSV format.\"\"\"",
        "        # Create commit",
        "        commit_data = {",
        "            \"hash\": \"abc123\",",
        "            \"message\": \"fix: Bug fix\",",
        "            \"timestamp\": \"2025-12-15T09:00:00\",",
        "            \"files_changed\": [\"fix.py\"],",
        "            \"insertions\": 5,",
        "            \"deletions\": 2,",
        "            \"branch\": \"main\",",
        "            \"session_id\": \"sess1\",",
        "            \"hunks\": [{\"file\": \"fix.py\", \"change_type\": \"modify\"}]",
        "        }",
        "        commit_file = ml.COMMITS_DIR / \"abc123_test.json\"",
        "        with open(commit_file, 'w', encoding='utf-8') as f:",
        "            json.dump(commit_data, f)",
        "",
        "        # Create chat",
        "        chat_data = {",
        "            \"timestamp\": \"2025-12-15T10:00:00\",",
        "            \"session_id\": \"sess1\",",
        "            \"query\": \"Test query\",",
        "            \"response\": \"Test response\",",
        "            \"files_referenced\": [\"test.py\"],",
        "            \"files_modified\": [],",
        "            \"tools_used\": [\"Read\"],",
        "        }",
        "        chat_file = ml.CHATS_DIR / \"chat-001.json\"",
        "        with open(chat_file, 'w', encoding='utf-8') as f:",
        "            json.dump(chat_data, f)",
        "",
        "        # Export",
        "        output_path = self.test_path / \"export.csv\"",
        "        stats = ml.export_data(\"csv\", output_path)",
        "",
        "        # Check stats",
        "        self.assertEqual(stats[\"records\"], 2)",
        "        self.assertEqual(stats[\"commits\"], 1)",
        "        self.assertEqual(stats[\"chats\"], 1)",
        "",
        "        # Check CSV content",
        "        with open(output_path, 'r', encoding='utf-8') as f:",
        "            reader = csv.DictReader(f)",
        "            rows = list(reader)",
        "",
        "        self.assertEqual(len(rows), 2)",
        "",
        "        # First row should be commit (sorted by timestamp)",
        "        self.assertEqual(rows[0][\"type\"], \"commit\")",
        "        self.assertEqual(rows[0][\"timestamp\"], \"2025-12-15T09:00:00\")",
        "        self.assertEqual(rows[0][\"input\"], \"fix: Bug fix\")",
        "        self.assertIn(\"fix.py\", rows[0][\"output\"])",
        "",
        "        # Second row should be chat",
        "        self.assertEqual(rows[1][\"type\"], \"chat\")",
        "        self.assertEqual(rows[1][\"timestamp\"], \"2025-12-15T10:00:00\")",
        "        self.assertEqual(rows[1][\"input\"], \"Test query\")",
        "        self.assertEqual(rows[1][\"output\"], \"Test response\")",
        "",
        "    def test_export_huggingface_format(self):",
        "        \"\"\"Should export in HuggingFace Dataset dict-of-lists format.\"\"\"",
        "        # Create test data",
        "        chat_data = {",
        "            \"timestamp\": \"2025-12-15T10:00:00\",",
        "            \"session_id\": \"sess1\",",
        "            \"query\": \"Query text\",",
        "            \"response\": \"Response text\",",
        "            \"files_referenced\": [\"file1.py\"],",
        "            \"files_modified\": [\"file2.py\"],",
        "            \"tools_used\": [\"Read\", \"Edit\"],",
        "        }",
        "        chat_file = ml.CHATS_DIR / \"chat-001.json\"",
        "        with open(chat_file, 'w', encoding='utf-8') as f:",
        "            json.dump(chat_data, f)",
        "",
        "        # Export",
        "        output_path = self.test_path / \"export.json\"",
        "        stats = ml.export_data(\"huggingface\", output_path)",
        "",
        "        # Check stats",
        "        self.assertEqual(stats[\"records\"], 1)",
        "",
        "        # Check HuggingFace format",
        "        with open(output_path, 'r', encoding='utf-8') as f:",
        "            data = json.load(f)",
        "",
        "        # Should be dict of lists",
        "        self.assertIsInstance(data, dict)",
        "",
        "        # Check all required fields",
        "        required_fields = ['type', 'timestamp', 'input', 'output',",
        "                          'session_id', 'files', 'tools_used']",
        "        for field in required_fields:",
        "            self.assertIn(field, data)",
        "            self.assertIsInstance(data[field], list)",
        "            self.assertEqual(len(data[field]), 1)",
        "",
        "        # Check values",
        "        self.assertEqual(data['type'][0], 'chat')",
        "        self.assertEqual(data['timestamp'][0], '2025-12-15T10:00:00')",
        "        self.assertEqual(data['input'][0], 'Query text')",
        "        self.assertEqual(data['output'][0], 'Response text')",
        "        self.assertEqual(data['session_id'][0], 'sess1')",
        "        self.assertIsInstance(data['files'][0], list)",
        "        self.assertIn('file1.py', data['files'][0])",
        "        self.assertIn('file2.py', data['files'][0])",
        "        self.assertEqual(data['tools_used'][0], ['Read', 'Edit'])",
        "",
        "    def test_export_huggingface_equal_lengths(self):",
        "        \"\"\"HuggingFace format should have equal-length lists.\"\"\"",
        "        # Create multiple records",
        "        for i in range(3):",
        "            chat_data = {",
        "                \"timestamp\": f\"2025-12-15T1{i}:00:00\",",
        "                \"session_id\": \"sess1\",",
        "                \"query\": f\"Query {i}\",",
        "                \"response\": f\"Response {i}\",",
        "                \"files_referenced\": [],",
        "                \"files_modified\": [],",
        "                \"tools_used\": [],",
        "            }",
        "            chat_file = ml.CHATS_DIR / f\"chat-00{i}.json\"",
        "            with open(chat_file, 'w', encoding='utf-8') as f:",
        "                json.dump(chat_data, f)",
        "",
        "        # Export",
        "        output_path = self.test_path / \"export.json\"",
        "        ml.export_data(\"huggingface\", output_path)",
        "",
        "        # Check all lists have same length",
        "        with open(output_path, 'r', encoding='utf-8') as f:",
        "            data = json.load(f)",
        "",
        "        lengths = {key: len(value) for key, value in data.items()}",
        "        unique_lengths = set(lengths.values())",
        "        self.assertEqual(len(unique_lengths), 1)  # All same length",
        "        self.assertEqual(list(unique_lengths)[0], 3)",
        "",
        "    def test_export_invalid_format(self):",
        "        \"\"\"Should raise ValueError for invalid format.\"\"\"",
        "        output_path = self.test_path / \"export.txt\"",
        "        with self.assertRaises(ValueError) as context:",
        "            ml.export_data(\"invalid_format\", output_path)",
        "",
        "        self.assertIn(\"Unknown format\", str(context.exception))",
        "",
        "    def test_export_creates_parent_directories(self):",
        "        \"\"\"Should create parent directories if they don't exist.\"\"\"",
        "        output_path = self.test_path / \"nested\" / \"path\" / \"export.jsonl\"",
        "        self.assertFalse(output_path.parent.exists())",
        "",
        "        ml.export_data(\"jsonl\", output_path)",
        "",
        "        self.assertTrue(output_path.parent.exists())",
        "        self.assertTrue(output_path.exists())",
        "",
        "    def test_export_csv_truncates_long_fields(self):",
        "        \"\"\"CSV export should truncate long input/output fields.\"\"\"",
        "        # Create chat with very long text",
        "        long_text = \"x\" * 2000",
        "        chat_data = {",
        "            \"timestamp\": \"2025-12-15T10:00:00\",",
        "            \"session_id\": \"sess1\",",
        "            \"query\": long_text,",
        "            \"response\": long_text,",
        "            \"files_referenced\": [],",
        "            \"files_modified\": [],",
        "            \"tools_used\": [],",
        "        }",
        "        chat_file = ml.CHATS_DIR / \"chat-001.json\"",
        "        with open(chat_file, 'w', encoding='utf-8') as f:",
        "            json.dump(chat_data, f)",
        "",
        "        # Export",
        "        output_path = self.test_path / \"export.csv\"",
        "        ml.export_data(\"csv\", output_path)",
        "",
        "        # Check truncation",
        "        with open(output_path, 'r', encoding='utf-8') as f:",
        "            reader = csv.DictReader(f)",
        "            row = next(reader)",
        "",
        "        # Should be truncated to 1000 chars",
        "        self.assertLessEqual(len(row['input']), 1000)",
        "        self.assertLessEqual(len(row['output']), 1000)",
        "",
        "    def test_export_csv_escapes_special_chars(self):",
        "        \"\"\"CSV export should properly escape special characters.\"\"\"",
        "        # Create chat with special chars",
        "        chat_data = {",
        "            \"timestamp\": \"2025-12-15T10:00:00\",",
        "            \"session_id\": \"sess1\",",
        "            \"query\": 'Text with \"quotes\" and, commas',",
        "            \"response\": \"Text with\\nnewlines\",",
        "            \"files_referenced\": [],",
        "            \"files_modified\": [],",
        "            \"tools_used\": [],",
        "        }",
        "        chat_file = ml.CHATS_DIR / \"chat-001.json\"",
        "        with open(chat_file, 'w', encoding='utf-8') as f:",
        "            json.dump(chat_data, f)",
        "",
        "        # Export",
        "        output_path = self.test_path / \"export.csv\"",
        "        ml.export_data(\"csv\", output_path)",
        "",
        "        # Should be able to read back without errors",
        "        with open(output_path, 'r', encoding='utf-8') as f:",
        "            reader = csv.DictReader(f)",
        "            row = next(reader)",
        "",
        "        self.assertIn('quotes', row['input'])",
        "        self.assertIn('newlines', row['output'])",
        "",
        "    def test_export_jsonl_valid_json_per_line(self):",
        "        \"\"\"JSONL export should have valid JSON on each line.\"\"\"",
        "        # Create multiple records",
        "        for i in range(3):",
        "            chat_data = {",
        "                \"timestamp\": f\"2025-12-15T1{i}:00:00\",",
        "                \"session_id\": \"sess1\",",
        "                \"query\": f\"Query {i}\",",
        "                \"response\": f\"Response {i}\",",
        "                \"files_referenced\": [],",
        "                \"files_modified\": [],",
        "                \"tools_used\": [],",
        "            }",
        "            chat_file = ml.CHATS_DIR / f\"chat-00{i}.json\"",
        "            with open(chat_file, 'w', encoding='utf-8') as f:",
        "                json.dump(chat_data, f)",
        "",
        "        # Export",
        "        output_path = self.test_path / \"export.jsonl\"",
        "        ml.export_data(\"jsonl\", output_path)",
        "",
        "        # Check each line is valid JSON",
        "        with open(output_path, 'r', encoding='utf-8') as f:",
        "            lines = f.readlines()",
        "",
        "        self.assertEqual(len(lines), 3)",
        "        for line in lines:",
        "            # Should not raise JSONDecodeError",
        "            record = json.loads(line)",
        "            self.assertIsInstance(record, dict)",
        "            self.assertIn('type', record)",
        "            self.assertIn('timestamp', record)",
        "",
        "    def test_export_sorts_by_timestamp(self):",
        "        \"\"\"Export should sort records by timestamp.\"\"\"",
        "        # Create records with different timestamps (out of order)",
        "        timestamps = [\"2025-12-15T12:00:00\", \"2025-12-15T09:00:00\", \"2025-12-15T15:00:00\"]",
        "",
        "        for i, ts in enumerate(timestamps):",
        "            chat_data = {",
        "                \"timestamp\": ts,",
        "                \"session_id\": \"sess1\",",
        "                \"query\": f\"Query {i}\",",
        "                \"response\": f\"Response {i}\",",
        "                \"files_referenced\": [],",
        "                \"files_modified\": [],",
        "                \"tools_used\": [],",
        "            }",
        "            chat_file = ml.CHATS_DIR / f\"chat-00{i}.json\"",
        "            with open(chat_file, 'w', encoding='utf-8') as f:",
        "                json.dump(chat_data, f)",
        "",
        "        # Export",
        "        output_path = self.test_path / \"export.jsonl\"",
        "        ml.export_data(\"jsonl\", output_path)",
        "",
        "        # Check order",
        "        with open(output_path, 'r', encoding='utf-8') as f:",
        "            records = [json.loads(line) for line in f.readlines()]",
        "",
        "        # Should be sorted",
        "        sorted_timestamps = sorted(timestamps)",
        "        for i, record in enumerate(records):",
        "            self.assertEqual(record['timestamp'], sorted_timestamps[i])",
        "",
        "",
        "if __name__ == \"__main__\":",
        "    unittest.main()"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "tests/unit/test_ml_feedback.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "#!/usr/bin/env python3",
        "\"\"\"",
        "Unit tests for ML data collector feedback functionality.",
        "",
        "Tests the add_chat_feedback() and list_chats_needing_feedback() functions,",
        "as well as schema validation for feedback data.",
        "\"\"\"",
        "",
        "import json",
        "import os",
        "import sys",
        "import tempfile",
        "import unittest",
        "from datetime import datetime",
        "from pathlib import Path",
        "from unittest.mock import patch, MagicMock",
        "",
        "# Add scripts to path",
        "sys.path.insert(0, str(Path(__file__).parent.parent.parent / \"scripts\"))",
        "",
        "import ml_data_collector as ml",
        "",
        "",
        "class TestAddChatFeedback(unittest.TestCase):",
        "    \"\"\"Test add_chat_feedback() function.\"\"\"",
        "",
        "    def setUp(self):",
        "        \"\"\"Set up test environment with temporary directories.\"\"\"",
        "        self.test_dir = tempfile.TemporaryDirectory()",
        "        self.test_path = Path(self.test_dir.name)",
        "",
        "        # Temporarily override ML data directories",
        "        self.original_ml_data_dir = ml.ML_DATA_DIR",
        "        self.original_chats_dir = ml.CHATS_DIR",
        "",
        "        ml.ML_DATA_DIR = self.test_path / \".git-ml\"",
        "        ml.CHATS_DIR = ml.ML_DATA_DIR / \"chats\"",
        "",
        "        # Create test directories",
        "        ml.ensure_dirs()",
        "",
        "    def tearDown(self):",
        "        \"\"\"Clean up test environment.\"\"\"",
        "        # Restore original directories",
        "        ml.ML_DATA_DIR = self.original_ml_data_dir",
        "        ml.CHATS_DIR = self.original_chats_dir",
        "",
        "        # Clean up temp directory",
        "        self.test_dir.cleanup()",
        "",
        "    def _create_test_chat(self, chat_id: str, user_feedback=None) -> Path:",
        "        \"\"\"Helper to create a test chat file.\"\"\"",
        "        # Create date directory",
        "        date_str = datetime.now().strftime(\"%Y-%m-%d\")",
        "        date_dir = ml.CHATS_DIR / date_str",
        "        date_dir.mkdir(parents=True, exist_ok=True)",
        "",
        "        # Create chat file",
        "        chat_data = {",
        "            \"id\": chat_id,",
        "            \"timestamp\": datetime.now().isoformat(),",
        "            \"session_id\": \"test-session\",",
        "            \"query\": \"Test query\",",
        "            \"response\": \"Test response\",",
        "            \"files_referenced\": [],",
        "            \"files_modified\": [],",
        "            \"tools_used\": [\"Read\"],",
        "        }",
        "",
        "        if user_feedback is not None:",
        "            chat_data[\"user_feedback\"] = user_feedback",
        "",
        "        chat_file = date_dir / f\"{chat_id}.json\"",
        "        with open(chat_file, 'w', encoding='utf-8') as f:",
        "            json.dump(chat_data, f)",
        "",
        "        return chat_file",
        "",
        "    def test_add_feedback_to_new_chat(self):",
        "        \"\"\"Test adding feedback to a chat without existing feedback.\"\"\"",
        "        chat_id = \"chat-test-001\"",
        "        self._create_test_chat(chat_id)",
        "",
        "        # Add feedback",
        "        result = ml.add_chat_feedback(chat_id, \"good\", \"Great response!\")",
        "",
        "        # Should succeed",
        "        self.assertTrue(result)",
        "",
        "        # Verify feedback was added",
        "        chat_file = ml.find_chat_file(chat_id)",
        "        with open(chat_file, 'r', encoding='utf-8') as f:",
        "            chat_data = json.load(f)",
        "",
        "        self.assertIn(\"user_feedback\", chat_data)",
        "        self.assertIsInstance(chat_data[\"user_feedback\"], dict)",
        "        self.assertEqual(chat_data[\"user_feedback\"][\"rating\"], \"good\")",
        "        self.assertEqual(chat_data[\"user_feedback\"][\"comment\"], \"Great response!\")",
        "        self.assertIn(\"timestamp\", chat_data[\"user_feedback\"])",
        "",
        "    def test_add_feedback_minimal(self):",
        "        \"\"\"Test adding feedback without comment.\"\"\"",
        "        chat_id = \"chat-test-002\"",
        "        self._create_test_chat(chat_id)",
        "",
        "        # Add feedback without comment",
        "        result = ml.add_chat_feedback(chat_id, \"neutral\")",
        "",
        "        # Should succeed",
        "        self.assertTrue(result)",
        "",
        "        # Verify feedback",
        "        chat_file = ml.find_chat_file(chat_id)",
        "        with open(chat_file, 'r', encoding='utf-8') as f:",
        "            chat_data = json.load(f)",
        "",
        "        self.assertEqual(chat_data[\"user_feedback\"][\"rating\"], \"neutral\")",
        "        self.assertIsNone(chat_data[\"user_feedback\"][\"comment\"])",
        "",
        "    def test_add_feedback_all_ratings(self):",
        "        \"\"\"Test all valid rating values.\"\"\"",
        "        valid_ratings = [\"good\", \"bad\", \"neutral\"]",
        "",
        "        for i, rating in enumerate(valid_ratings):",
        "            chat_id = f\"chat-test-rating-{i}\"",
        "            self._create_test_chat(chat_id)",
        "",
        "            result = ml.add_chat_feedback(chat_id, rating)",
        "            self.assertTrue(result)",
        "",
        "            # Verify rating",
        "            chat_file = ml.find_chat_file(chat_id)",
        "            with open(chat_file, 'r', encoding='utf-8') as f:",
        "                chat_data = json.load(f)",
        "",
        "            self.assertEqual(chat_data[\"user_feedback\"][\"rating\"], rating)",
        "",
        "    def test_add_feedback_invalid_chat_id(self):",
        "        \"\"\"Test adding feedback to non-existent chat.\"\"\"",
        "        result = ml.add_chat_feedback(\"nonexistent-chat-id\", \"good\")",
        "",
        "        # Should return False (chat not found)",
        "        self.assertFalse(result)",
        "",
        "    def test_add_feedback_invalid_rating(self):",
        "        \"\"\"Test adding feedback with invalid rating value.\"\"\"",
        "        chat_id = \"chat-test-003\"",
        "        self._create_test_chat(chat_id)",
        "",
        "        # Should raise ValueError",
        "        with self.assertRaises(ValueError) as ctx:",
        "            ml.add_chat_feedback(chat_id, \"excellent\")",
        "",
        "        self.assertIn(\"Invalid rating\", str(ctx.exception))",
        "        self.assertIn(\"excellent\", str(ctx.exception))",
        "",
        "    def test_add_feedback_overwrite_without_force(self):",
        "        \"\"\"Test that overwriting existing feedback without force fails.\"\"\"",
        "        chat_id = \"chat-test-004\"",
        "        existing_feedback = {",
        "            \"rating\": \"good\",",
        "            \"comment\": \"Original feedback\",",
        "            \"timestamp\": datetime.now().isoformat(),",
        "        }",
        "        self._create_test_chat(chat_id, user_feedback=existing_feedback)",
        "",
        "        # Try to update without force",
        "        result = ml.add_chat_feedback(chat_id, \"bad\", \"New feedback\")",
        "",
        "        # Should fail",
        "        self.assertFalse(result)",
        "",
        "        # Verify original feedback unchanged",
        "        chat_file = ml.find_chat_file(chat_id)",
        "        with open(chat_file, 'r', encoding='utf-8') as f:",
        "            chat_data = json.load(f)",
        "",
        "        self.assertEqual(chat_data[\"user_feedback\"][\"rating\"], \"good\")",
        "        self.assertEqual(chat_data[\"user_feedback\"][\"comment\"], \"Original feedback\")",
        "",
        "    def test_add_feedback_overwrite_with_force(self):",
        "        \"\"\"Test overwriting existing feedback with force=True.\"\"\"",
        "        chat_id = \"chat-test-005\"",
        "        existing_feedback = {",
        "            \"rating\": \"good\",",
        "            \"comment\": \"Original feedback\",",
        "            \"timestamp\": datetime.now().isoformat(),",
        "        }",
        "        self._create_test_chat(chat_id, user_feedback=existing_feedback)",
        "",
        "        # Update with force",
        "        result = ml.add_chat_feedback(chat_id, \"bad\", \"Updated feedback\", force=True)",
        "",
        "        # Should succeed",
        "        self.assertTrue(result)",
        "",
        "        # Verify feedback was updated",
        "        chat_file = ml.find_chat_file(chat_id)",
        "        with open(chat_file, 'r', encoding='utf-8') as f:",
        "            chat_data = json.load(f)",
        "",
        "        self.assertEqual(chat_data[\"user_feedback\"][\"rating\"], \"bad\")",
        "        self.assertEqual(chat_data[\"user_feedback\"][\"comment\"], \"Updated feedback\")",
        "",
        "    def test_add_feedback_upgrade_legacy_format(self):",
        "        \"\"\"Test upgrading from legacy string format to dict format.\"\"\"",
        "        chat_id = \"chat-test-006\"",
        "        # Create chat with legacy string feedback",
        "        self._create_test_chat(chat_id, user_feedback=\"good\")",
        "",
        "        # Try to add feedback (should allow upgrade from legacy format)",
        "        result = ml.add_chat_feedback(chat_id, \"neutral\", \"Upgraded feedback\")",
        "",
        "        # Should succeed (legacy format allows implicit upgrade)",
        "        self.assertTrue(result)",
        "",
        "        # Verify feedback is now in dict format",
        "        chat_file = ml.find_chat_file(chat_id)",
        "        with open(chat_file, 'r', encoding='utf-8') as f:",
        "            chat_data = json.load(f)",
        "",
        "        self.assertIsInstance(chat_data[\"user_feedback\"], dict)",
        "        self.assertEqual(chat_data[\"user_feedback\"][\"rating\"], \"neutral\")",
        "        self.assertEqual(chat_data[\"user_feedback\"][\"comment\"], \"Upgraded feedback\")",
        "",
        "",
        "class TestListChatsNeedingFeedback(unittest.TestCase):",
        "    \"\"\"Test list_chats_needing_feedback() function.\"\"\"",
        "",
        "    def setUp(self):",
        "        \"\"\"Set up test environment with temporary directories.\"\"\"",
        "        self.test_dir = tempfile.TemporaryDirectory()",
        "        self.test_path = Path(self.test_dir.name)",
        "",
        "        # Temporarily override ML data directories",
        "        self.original_ml_data_dir = ml.ML_DATA_DIR",
        "        self.original_chats_dir = ml.CHATS_DIR",
        "",
        "        ml.ML_DATA_DIR = self.test_path / \".git-ml\"",
        "        ml.CHATS_DIR = ml.ML_DATA_DIR / \"chats\"",
        "",
        "        # Create test directories",
        "        ml.ensure_dirs()",
        "",
        "    def tearDown(self):",
        "        \"\"\"Clean up test environment.\"\"\"",
        "        # Restore original directories",
        "        ml.ML_DATA_DIR = self.original_ml_data_dir",
        "        ml.CHATS_DIR = self.original_chats_dir",
        "",
        "        # Clean up temp directory",
        "        self.test_dir.cleanup()",
        "",
        "    def _create_test_chat(",
        "        self,",
        "        chat_id: str,",
        "        query: str = \"Test query\",",
        "        user_feedback=None,",
        "        date_str: str = None",
        "    ):",
        "        \"\"\"Helper to create a test chat file.\"\"\"",
        "        if date_str is None:",
        "            date_str = datetime.now().strftime(\"%Y-%m-%d\")",
        "",
        "        date_dir = ml.CHATS_DIR / date_str",
        "        date_dir.mkdir(parents=True, exist_ok=True)",
        "",
        "        chat_data = {",
        "            \"id\": chat_id,",
        "            \"timestamp\": datetime.now().isoformat(),",
        "            \"session_id\": \"test-session\",",
        "            \"query\": query,",
        "            \"response\": \"Test response\",",
        "            \"files_referenced\": [],",
        "            \"files_modified\": [],",
        "            \"tools_used\": [],",
        "        }",
        "",
        "        if user_feedback is not None:",
        "            chat_data[\"user_feedback\"] = user_feedback",
        "",
        "        chat_file = date_dir / f\"{chat_id}.json\"",
        "        with open(chat_file, 'w', encoding='utf-8') as f:",
        "            json.dump(chat_data, f)",
        "",
        "    def test_list_empty_directory(self):",
        "        \"\"\"Test listing when no chats exist.\"\"\"",
        "        result = ml.list_chats_needing_feedback()",
        "",
        "        # Should return empty list",
        "        self.assertEqual(result, [])",
        "",
        "    def test_list_chats_all_without_feedback(self):",
        "        \"\"\"Test listing when all chats lack feedback.\"\"\"",
        "        # Create 3 chats without feedback",
        "        for i in range(3):",
        "            self._create_test_chat(f\"chat-test-{i}\", f\"Query {i}\")",
        "",
        "        result = ml.list_chats_needing_feedback()",
        "",
        "        # Should return all 3 chats",
        "        self.assertEqual(len(result), 3)",
        "",
        "        # All should not have feedback",
        "        for chat in result:",
        "            self.assertFalse(chat[\"has_feedback\"])",
        "            self.assertIsNone(chat[\"feedback_rating\"])",
        "",
        "    def test_list_chats_all_with_feedback(self):",
        "        \"\"\"Test listing when all chats have feedback.\"\"\"",
        "        # Create chats with dict-format feedback",
        "        for i in range(3):",
        "            feedback = {",
        "                \"rating\": \"good\",",
        "                \"comment\": \"Test\",",
        "                \"timestamp\": datetime.now().isoformat(),",
        "            }",
        "            self._create_test_chat(f\"chat-test-{i}\", f\"Query {i}\", user_feedback=feedback)",
        "",
        "        result = ml.list_chats_needing_feedback()",
        "",
        "        # Should return all chats (function lists all chats, not just needing feedback)",
        "        self.assertEqual(len(result), 3)",
        "",
        "        # All should have feedback",
        "        for chat in result:",
        "            self.assertTrue(chat[\"has_feedback\"])",
        "            self.assertEqual(chat[\"feedback_rating\"], \"good\")",
        "",
        "    def test_list_chats_mixed_feedback(self):",
        "        \"\"\"Test listing with mix of chats with and without feedback.\"\"\"",
        "        # Create 2 chats without feedback",
        "        self._create_test_chat(\"chat-no-fb-1\", \"No feedback 1\")",
        "        self._create_test_chat(\"chat-no-fb-2\", \"No feedback 2\")",
        "",
        "        # Create 2 chats with feedback",
        "        feedback = {\"rating\": \"bad\", \"comment\": None, \"timestamp\": datetime.now().isoformat()}",
        "        self._create_test_chat(\"chat-with-fb-1\", \"Has feedback 1\", user_feedback=feedback)",
        "        self._create_test_chat(\"chat-with-fb-2\", \"Has feedback 2\", user_feedback=feedback)",
        "",
        "        result = ml.list_chats_needing_feedback()",
        "",
        "        # Should return all 4 chats",
        "        self.assertEqual(len(result), 4)",
        "",
        "        # Count feedback status",
        "        with_feedback = sum(1 for c in result if c[\"has_feedback\"])",
        "        without_feedback = sum(1 for c in result if not c[\"has_feedback\"])",
        "",
        "        self.assertEqual(with_feedback, 2)",
        "        self.assertEqual(without_feedback, 2)",
        "",
        "    def test_list_chats_limit_parameter(self):",
        "        \"\"\"Test that limit parameter works correctly.\"\"\"",
        "        # Create 10 chats",
        "        for i in range(10):",
        "            self._create_test_chat(f\"chat-test-{i:02d}\", f\"Query {i}\")",
        "",
        "        # Request only 5",
        "        result = ml.list_chats_needing_feedback(limit=5)",
        "",
        "        # Should return exactly 5",
        "        self.assertEqual(len(result), 5)",
        "",
        "        # Request 20 (more than exist)",
        "        result = ml.list_chats_needing_feedback(limit=20)",
        "",
        "        # Should return all 10",
        "        self.assertEqual(len(result), 10)",
        "",
        "    def test_list_chats_query_truncation(self):",
        "        \"\"\"Test that long queries are truncated.\"\"\"",
        "        long_query = \"A\" * 200  # 200 characters",
        "        self._create_test_chat(\"chat-test-long\", long_query)",
        "",
        "        result = ml.list_chats_needing_feedback()",
        "",
        "        # Query should be truncated to 100 chars",
        "        self.assertEqual(len(result[0][\"query\"]), 100)",
        "        self.assertEqual(result[0][\"query\"], \"A\" * 100)",
        "",
        "    def test_list_chats_legacy_string_feedback(self):",
        "        \"\"\"Test that legacy string format feedback is recognized.\"\"\"",
        "        # Create chat with legacy string feedback",
        "        self._create_test_chat(\"chat-legacy\", \"Legacy chat\", user_feedback=\"good\")",
        "",
        "        result = ml.list_chats_needing_feedback()",
        "",
        "        # Should recognize as having feedback",
        "        self.assertEqual(len(result), 1)",
        "        self.assertTrue(result[0][\"has_feedback\"])",
        "        self.assertEqual(result[0][\"feedback_rating\"], \"good\")",
        "",
        "    def test_list_chats_reverse_chronological(self):",
        "        \"\"\"Test that chats are returned in reverse chronological order (most recent first).\"\"\"",
        "        import time",
        "",
        "        # Create chats with slight time gaps",
        "        for i in range(3):",
        "            self._create_test_chat(f\"chat-test-{i}\", f\"Query {i}\")",
        "            time.sleep(0.01)  # Small delay to ensure different mtimes",
        "",
        "        result = ml.list_chats_needing_feedback()",
        "",
        "        # IDs should be in reverse order (most recent first)",
        "        # Note: This depends on file mtime, which should be newest first",
        "        self.assertEqual(len(result), 3)",
        "        # The most recently created should be first",
        "        self.assertEqual(result[0][\"id\"], \"chat-test-2\")",
        "",
        "    def test_list_chats_multiple_dates(self):",
        "        \"\"\"Test listing chats across multiple date directories.\"\"\"",
        "        # Create chats on different dates",
        "        self._create_test_chat(\"chat-2025-01\", \"Query 1\", date_str=\"2025-01-15\")",
        "        self._create_test_chat(\"chat-2025-02\", \"Query 2\", date_str=\"2025-02-15\")",
        "        self._create_test_chat(\"chat-2025-03\", \"Query 3\", date_str=\"2025-03-15\")",
        "",
        "        result = ml.list_chats_needing_feedback()",
        "",
        "        # Should find all chats across dates",
        "        self.assertEqual(len(result), 3)",
        "",
        "        # Should be sorted by date (most recent first)",
        "        chat_ids = [c[\"id\"] for c in result]",
        "        self.assertIn(\"chat-2025-01\", chat_ids)",
        "        self.assertIn(\"chat-2025-02\", chat_ids)",
        "        self.assertIn(\"chat-2025-03\", chat_ids)",
        "",
        "",
        "class TestFeedbackSchemaValidation(unittest.TestCase):",
        "    \"\"\"Test schema validation for feedback data.\"\"\"",
        "",
        "    def test_validate_chat_with_dict_feedback(self):",
        "        \"\"\"Test that chat with dict-format feedback validates.\"\"\"",
        "        chat_data = {",
        "            \"id\": \"test-chat\",",
        "            \"timestamp\": datetime.now().isoformat(),",
        "            \"session_id\": \"test-session\",",
        "            \"query\": \"Test query\",",
        "            \"response\": \"Test response\",",
        "            \"files_referenced\": [],",
        "            \"files_modified\": [],",
        "            \"tools_used\": [],",
        "            \"user_feedback\": {",
        "                \"rating\": \"good\",",
        "                \"comment\": \"Great!\",",
        "                \"timestamp\": datetime.now().isoformat(),",
        "            },",
        "        }",
        "",
        "        errors = ml.validate_schema(chat_data, ml.CHAT_SCHEMA, \"chat\")",
        "",
        "        # Should have no validation errors",
        "        self.assertEqual(errors, [])",
        "",
        "    def test_validate_chat_with_string_feedback(self):",
        "        \"\"\"Test that chat with legacy string-format feedback validates.\"\"\"",
        "        chat_data = {",
        "            \"id\": \"test-chat\",",
        "            \"timestamp\": datetime.now().isoformat(),",
        "            \"session_id\": \"test-session\",",
        "            \"query\": \"Test query\",",
        "            \"response\": \"Test response\",",
        "            \"files_referenced\": [],",
        "            \"files_modified\": [],",
        "            \"tools_used\": [],",
        "            \"user_feedback\": \"good\",  # Legacy string format",
        "        }",
        "",
        "        errors = ml.validate_schema(chat_data, ml.CHAT_SCHEMA, \"chat\")",
        "",
        "        # Should have no validation errors",
        "        self.assertEqual(errors, [])",
        "",
        "    def test_validate_chat_without_feedback(self):",
        "        \"\"\"Test that chat without feedback validates.\"\"\"",
        "        chat_data = {",
        "            \"id\": \"test-chat\",",
        "            \"timestamp\": datetime.now().isoformat(),",
        "            \"session_id\": \"test-session\",",
        "            \"query\": \"Test query\",",
        "            \"response\": \"Test response\",",
        "            \"files_referenced\": [],",
        "            \"files_modified\": [],",
        "            \"tools_used\": [],",
        "        }",
        "",
        "        errors = ml.validate_schema(chat_data, ml.CHAT_SCHEMA, \"chat\")",
        "",
        "        # Should have no validation errors",
        "        self.assertEqual(errors, [])",
        "",
        "    def test_validate_chat_with_none_feedback(self):",
        "        \"\"\"Test that chat with None feedback validates.\"\"\"",
        "        chat_data = {",
        "            \"id\": \"test-chat\",",
        "            \"timestamp\": datetime.now().isoformat(),",
        "            \"session_id\": \"test-session\",",
        "            \"query\": \"Test query\",",
        "            \"response\": \"Test response\",",
        "            \"files_referenced\": [],",
        "            \"files_modified\": [],",
        "            \"tools_used\": [],",
        "            \"user_feedback\": None,",
        "        }",
        "",
        "        errors = ml.validate_schema(chat_data, ml.CHAT_SCHEMA, \"chat\")",
        "",
        "        # Should have no validation errors",
        "        self.assertEqual(errors, [])",
        "",
        "    def test_validate_chat_with_invalid_feedback_type(self):",
        "        \"\"\"Test that chat with invalid feedback type fails validation.\"\"\"",
        "        chat_data = {",
        "            \"id\": \"test-chat\",",
        "            \"timestamp\": datetime.now().isoformat(),",
        "            \"session_id\": \"test-session\",",
        "            \"query\": \"Test query\",",
        "            \"response\": \"Test response\",",
        "            \"files_referenced\": [],",
        "            \"files_modified\": [],",
        "            \"tools_used\": [],",
        "            \"user_feedback\": 123,  # Invalid: should be dict, str, or None",
        "        }",
        "",
        "        errors = ml.validate_schema(chat_data, ml.CHAT_SCHEMA, \"chat\")",
        "",
        "        # Should have validation errors",
        "        self.assertGreater(len(errors), 0)",
        "        self.assertTrue(any(\"user_feedback\" in error for error in errors))",
        "",
        "",
        "if __name__ == \"__main__\":",
        "    unittest.main()"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "tests/unit/test_ml_quality.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "#!/usr/bin/env python3",
        "\"\"\"",
        "Tests for ML data collector quality-report functionality.",
        "\"\"\"",
        "",
        "import json",
        "import os",
        "import sys",
        "import tempfile",
        "import unittest",
        "from datetime import datetime",
        "from pathlib import Path",
        "from unittest.mock import patch, MagicMock",
        "",
        "# Add scripts to path",
        "sys.path.insert(0, str(Path(__file__).parent.parent.parent / \"scripts\"))",
        "",
        "import ml_data_collector as ml",
        "",
        "",
        "class TestDataQualityAnalysis(unittest.TestCase):",
        "    \"\"\"Test data quality analysis and reporting.\"\"\"",
        "",
        "    def setUp(self):",
        "        \"\"\"Set up test environment with temporary directories.\"\"\"",
        "        self.test_dir = tempfile.TemporaryDirectory()",
        "        self.test_path = Path(self.test_dir.name)",
        "",
        "        # Temporarily override ML data directories",
        "        self.original_ml_data_dir = ml.ML_DATA_DIR",
        "        self.original_commits_dir = ml.COMMITS_DIR",
        "        self.original_sessions_dir = ml.SESSIONS_DIR",
        "        self.original_chats_dir = ml.CHATS_DIR",
        "        self.original_actions_dir = ml.ACTIONS_DIR",
        "",
        "        ml.ML_DATA_DIR = self.test_path / \".git-ml\"",
        "        ml.COMMITS_DIR = ml.ML_DATA_DIR / \"commits\"",
        "        ml.SESSIONS_DIR = ml.ML_DATA_DIR / \"sessions\"",
        "        ml.CHATS_DIR = ml.ML_DATA_DIR / \"chats\"",
        "        ml.ACTIONS_DIR = ml.ML_DATA_DIR / \"actions\"",
        "",
        "        # Create test directories",
        "        ml.ensure_dirs()",
        "",
        "    def tearDown(self):",
        "        \"\"\"Clean up test environment.\"\"\"",
        "        # Restore original directories",
        "        ml.ML_DATA_DIR = self.original_ml_data_dir",
        "        ml.COMMITS_DIR = self.original_commits_dir",
        "        ml.SESSIONS_DIR = self.original_sessions_dir",
        "        ml.CHATS_DIR = self.original_chats_dir",
        "        ml.ACTIONS_DIR = self.original_actions_dir",
        "",
        "        # Clean up temp directory",
        "        self.test_dir.cleanup()",
        "",
        "    def test_analyze_empty_data(self):",
        "        \"\"\"Test quality analysis with no data.\"\"\"",
        "        result = ml.analyze_data_quality()",
        "",
        "        # Should return structure with zero counts",
        "        self.assertIn('completeness', result)",
        "        self.assertIn('diversity', result)",
        "        self.assertIn('anomalies', result)",
        "        self.assertIn('quality_score', result)",
        "",
        "        comp = result['completeness']",
        "        self.assertEqual(comp['chats_total'], 0)",
        "        self.assertEqual(comp['commits_total'], 0)",
        "        self.assertEqual(comp['sessions_total'], 0)",
        "",
        "        # Quality score should be low with no data",
        "        self.assertIsInstance(result['quality_score'], int)",
        "        self.assertLessEqual(result['quality_score'], 100)",
        "        self.assertGreaterEqual(result['quality_score'], 0)",
        "",
        "    def test_completeness_all_fields_present(self):",
        "        \"\"\"Test completeness calculation with all required fields.\"\"\"",
        "        # Create complete chat entry",
        "        chat_data = {",
        "            'id': 'chat-test-001',",
        "            'timestamp': datetime.now().isoformat(),",
        "            'session_id': 'session-001',",
        "            'query': 'Test query',",
        "            'response': 'Test response',",
        "            'files_referenced': ['/path/to/file.py'],",
        "            'files_modified': [],",
        "            'tools_used': ['Read'],",
        "            'query_tokens': 10,",
        "            'response_tokens': 20,",
        "            'user_feedback': None,",
        "        }",
        "",
        "        chat_file = ml.CHATS_DIR / \"chat-test-001.json\"",
        "        with open(chat_file, 'w', encoding='utf-8') as f:",
        "            json.dump(chat_data, f)",
        "",
        "        result = ml.analyze_data_quality()",
        "",
        "        comp = result['completeness']",
        "        self.assertEqual(comp['chats_total'], 1)",
        "        self.assertEqual(comp['chats_complete'], 1)",
        "        self.assertEqual(comp['chats_complete_pct'], 100.0)",
        "",
        "    def test_completeness_missing_fields(self):",
        "        \"\"\"Test completeness calculation with missing fields.\"\"\"",
        "        # Create incomplete chat entry (missing required fields)",
        "        incomplete_chat = {",
        "            'id': 'chat-test-002',",
        "            'timestamp': datetime.now().isoformat(),",
        "            'query': 'Test query',",
        "            # Missing: session_id, response, files_referenced, files_modified, tools_used",
        "        }",
        "",
        "        chat_file = ml.CHATS_DIR / \"chat-test-002.json\"",
        "        with open(chat_file, 'w', encoding='utf-8') as f:",
        "            json.dump(incomplete_chat, f)",
        "",
        "        result = ml.analyze_data_quality()",
        "",
        "        comp = result['completeness']",
        "        self.assertEqual(comp['chats_total'], 1)",
        "        self.assertEqual(comp['chats_complete'], 0)",
        "        self.assertEqual(comp['chats_complete_pct'], 0.0)",
        "",
        "    def test_commits_with_ci_results(self):",
        "        \"\"\"Test tracking commits with CI results.\"\"\"",
        "        # Commit with CI results",
        "        commit1 = {",
        "            'hash': 'abc123def456',",
        "            'message': 'Test commit',",
        "            'timestamp': datetime.now().isoformat(),",
        "            'files_changed': ['/path/to/file.py'],",
        "            'ci_result': {'status': 'pass', 'coverage': 89.5},",
        "        }",
        "",
        "        # Commit without CI results",
        "        commit2 = {",
        "            'hash': 'def456abc789',",
        "            'message': 'Another commit',",
        "            'timestamp': datetime.now().isoformat(),",
        "            'files_changed': ['/path/to/other.py'],",
        "        }",
        "",
        "        with open(ml.COMMITS_DIR / \"commit1.json\", 'w', encoding='utf-8') as f:",
        "            json.dump(commit1, f)",
        "        with open(ml.COMMITS_DIR / \"commit2.json\", 'w', encoding='utf-8') as f:",
        "            json.dump(commit2, f)",
        "",
        "        result = ml.analyze_data_quality()",
        "",
        "        comp = result['completeness']",
        "        self.assertEqual(comp['commits_total'], 2)",
        "        self.assertEqual(comp['commits_with_ci'], 1)",
        "        self.assertEqual(comp['commits_with_ci_pct'], 50.0)",
        "",
        "    def test_sessions_with_commits(self):",
        "        \"\"\"Test tracking sessions with linked commits.\"\"\"",
        "        # Create session",
        "        session_data = {",
        "            'id': 'session-001',",
        "            'start_time': datetime.now().isoformat(),",
        "            'chat_ids': ['chat-001'],",
        "        }",
        "",
        "        with open(ml.SESSIONS_DIR / \"session-001.json\", 'w', encoding='utf-8') as f:",
        "            json.dump(session_data, f)",
        "",
        "        # Create commit linked to session",
        "        commit_data = {",
        "            'hash': 'abc123',",
        "            'message': 'Test',",
        "            'timestamp': datetime.now().isoformat(),",
        "            'files_changed': ['test.py'],",
        "            'session_id': 'session-001',",
        "        }",
        "",
        "        with open(ml.COMMITS_DIR / \"commit.json\", 'w', encoding='utf-8') as f:",
        "            json.dump(commit_data, f)",
        "",
        "        result = ml.analyze_data_quality()",
        "",
        "        comp = result['completeness']",
        "        self.assertEqual(comp['sessions_total'], 1)",
        "        self.assertEqual(comp['sessions_with_commits'], 1)",
        "        self.assertEqual(comp['sessions_with_commits_pct'], 100.0)",
        "",
        "    def test_chats_with_feedback(self):",
        "        \"\"\"Test tracking chats with user feedback.\"\"\"",
        "        # Chat with feedback",
        "        chat1 = {",
        "            'id': 'chat-001',",
        "            'timestamp': datetime.now().isoformat(),",
        "            'session_id': 'session-001',",
        "            'query': 'Test',",
        "            'response': 'Response',",
        "            'files_referenced': [],",
        "            'files_modified': [],",
        "            'tools_used': [],",
        "            'user_feedback': {'rating': 5, 'comment': 'Great!'},",
        "        }",
        "",
        "        # Chat without feedback",
        "        chat2 = {",
        "            'id': 'chat-002',",
        "            'timestamp': datetime.now().isoformat(),",
        "            'session_id': 'session-001',",
        "            'query': 'Test',",
        "            'response': 'Response',",
        "            'files_referenced': [],",
        "            'files_modified': [],",
        "            'tools_used': [],",
        "        }",
        "",
        "        with open(ml.CHATS_DIR / \"chat1.json\", 'w', encoding='utf-8') as f:",
        "            json.dump(chat1, f)",
        "        with open(ml.CHATS_DIR / \"chat2.json\", 'w', encoding='utf-8') as f:",
        "            json.dump(chat2, f)",
        "",
        "        result = ml.analyze_data_quality()",
        "",
        "        comp = result['completeness']",
        "        self.assertEqual(comp['chats_total'], 2)",
        "        self.assertEqual(comp['chats_with_feedback'], 1)",
        "        self.assertEqual(comp['chats_with_feedback_pct'], 50.0)",
        "",
        "    def test_diversity_unique_files(self):",
        "        \"\"\"Test tracking unique files across chats and commits.\"\"\"",
        "        # Chat with files",
        "        chat = {",
        "            'id': 'chat-001',",
        "            'timestamp': datetime.now().isoformat(),",
        "            'session_id': 'session-001',",
        "            'query': 'Test',",
        "            'response': 'Response',",
        "            'files_referenced': ['/file1.py', '/file2.py'],",
        "            'files_modified': ['/file3.py'],",
        "            'tools_used': [],",
        "        }",
        "",
        "        # Commit with files (one overlapping)",
        "        commit = {",
        "            'hash': 'abc123',",
        "            'message': 'Test',",
        "            'timestamp': datetime.now().isoformat(),",
        "            'files_changed': ['/file3.py', '/file4.py'],",
        "        }",
        "",
        "        with open(ml.CHATS_DIR / \"chat.json\", 'w', encoding='utf-8') as f:",
        "            json.dump(chat, f)",
        "        with open(ml.COMMITS_DIR / \"commit.json\", 'w', encoding='utf-8') as f:",
        "            json.dump(commit, f)",
        "",
        "        result = ml.analyze_data_quality()",
        "",
        "        div = result['diversity']",
        "        # Should have 4 unique files: file1, file2, file3, file4",
        "        self.assertEqual(div['unique_files'], 4)",
        "",
        "    def test_diversity_unique_tools(self):",
        "        \"\"\"Test tracking unique tools and their usage counts.\"\"\"",
        "        chats = [",
        "            {",
        "                'id': f'chat-{i}',",
        "                'timestamp': datetime.now().isoformat(),",
        "                'session_id': 'session-001',",
        "                'query': 'Test',",
        "                'response': 'Response',",
        "                'files_referenced': [],",
        "                'files_modified': [],",
        "                'tools_used': ['Read', 'Edit'] if i % 2 == 0 else ['Bash', 'Read'],",
        "            }",
        "            for i in range(4)",
        "        ]",
        "",
        "        for chat in chats:",
        "            chat_file = ml.CHATS_DIR / f\"{chat['id']}.json\"",
        "            with open(chat_file, 'w', encoding='utf-8') as f:",
        "                json.dump(chat, f)",
        "",
        "        result = ml.analyze_data_quality()",
        "",
        "        div = result['diversity']",
        "        # Should have 3 unique tools: Read, Edit, Bash",
        "        self.assertEqual(div['unique_tools'], 3)",
        "",
        "        # Check tool distribution",
        "        tool_dist = div['tool_distribution']",
        "        self.assertEqual(tool_dist['Read'], 4)  # Used in all 4 chats",
        "        self.assertEqual(tool_dist['Edit'], 2)  # Used in 2 chats",
        "        self.assertEqual(tool_dist['Bash'], 2)  # Used in 2 chats",
        "",
        "    def test_diversity_query_response_lengths(self):",
        "        \"\"\"Test tracking query and response length statistics.\"\"\"",
        "        chats = [",
        "            {",
        "                'id': f'chat-{i}',",
        "                'timestamp': datetime.now().isoformat(),",
        "                'session_id': 'session-001',",
        "                'query': 'Q' * (10 * (i + 1)),  # 10, 20, 30, 40, 50",
        "                'response': 'R' * (100 * (i + 1)),  # 100, 200, 300, 400, 500",
        "                'files_referenced': [],",
        "                'files_modified': [],",
        "                'tools_used': [],",
        "            }",
        "            for i in range(5)",
        "        ]",
        "",
        "        for chat in chats:",
        "            chat_file = ml.CHATS_DIR / f\"{chat['id']}.json\"",
        "            with open(chat_file, 'w', encoding='utf-8') as f:",
        "                json.dump(chat, f)",
        "",
        "        result = ml.analyze_data_quality()",
        "",
        "        div = result['diversity']",
        "        self.assertEqual(div['query_length_min'], 10)",
        "        self.assertEqual(div['query_length_avg'], 30)  # (10+20+30+40+50)/5",
        "        self.assertEqual(div['query_length_max'], 50)",
        "        self.assertEqual(div['response_length_min'], 100)",
        "        self.assertEqual(div['response_length_avg'], 300)  # (100+200+300+400+500)/5",
        "        self.assertEqual(div['response_length_max'], 500)",
        "",
        "    def test_anomaly_empty_responses(self):",
        "        \"\"\"Test detection of empty responses.\"\"\"",
        "        chats = [",
        "            {",
        "                'id': 'chat-empty',",
        "                'timestamp': datetime.now().isoformat(),",
        "                'session_id': 'session-001',",
        "                'query': 'Test',",
        "                'response': '',  # Empty response",
        "                'files_referenced': [],",
        "                'files_modified': [],",
        "                'tools_used': [],",
        "            },",
        "            {",
        "                'id': 'chat-whitespace',",
        "                'timestamp': datetime.now().isoformat(),",
        "                'session_id': 'session-001',",
        "                'query': 'Test',",
        "                'response': '   ',  # Whitespace only",
        "                'files_referenced': [],",
        "                'files_modified': [],",
        "                'tools_used': [],",
        "            },",
        "            {",
        "                'id': 'chat-normal',",
        "                'timestamp': datetime.now().isoformat(),",
        "                'session_id': 'session-001',",
        "                'query': 'Test',",
        "                'response': 'Normal response',",
        "                'files_referenced': [],",
        "                'files_modified': [],",
        "                'tools_used': [],",
        "            },",
        "        ]",
        "",
        "        for chat in chats:",
        "            chat_file = ml.CHATS_DIR / f\"{chat['id']}.json\"",
        "            with open(chat_file, 'w', encoding='utf-8') as f:",
        "                json.dump(chat, f)",
        "",
        "        result = ml.analyze_data_quality()",
        "",
        "        anom = result['anomalies']",
        "        self.assertEqual(anom['empty_responses'], 2)",
        "",
        "    def test_anomaly_zero_file_commits(self):",
        "        \"\"\"Test detection of commits with no files changed.\"\"\"",
        "        commits = [",
        "            {",
        "                'hash': 'abc123',",
        "                'message': 'Empty commit',",
        "                'timestamp': datetime.now().isoformat(),",
        "                'files_changed': [],  # No files",
        "            },",
        "            {",
        "                'hash': 'def456',",
        "                'message': 'Normal commit',",
        "                'timestamp': datetime.now().isoformat(),",
        "                'files_changed': ['file.py'],",
        "            },",
        "        ]",
        "",
        "        for i, commit in enumerate(commits):",
        "            commit_file = ml.COMMITS_DIR / f\"commit{i}.json\"",
        "            with open(commit_file, 'w', encoding='utf-8') as f:",
        "                json.dump(commit, f)",
        "",
        "        result = ml.analyze_data_quality()",
        "",
        "        anom = result['anomalies']",
        "        self.assertEqual(anom['zero_file_commits'], 1)",
        "",
        "    def test_anomaly_empty_sessions(self):",
        "        \"\"\"Test detection of sessions with no chats.\"\"\"",
        "        sessions = [",
        "            {",
        "                'id': 'session-empty',",
        "                'start_time': datetime.now().isoformat(),",
        "                'chat_ids': [],  # No chats",
        "            },",
        "            {",
        "                'id': 'session-normal',",
        "                'start_time': datetime.now().isoformat(),",
        "                'chat_ids': ['chat-001'],",
        "            },",
        "        ]",
        "",
        "        for session in sessions:",
        "            session_file = ml.SESSIONS_DIR / f\"{session['id']}.json\"",
        "            with open(session_file, 'w', encoding='utf-8') as f:",
        "                json.dump(session, f)",
        "",
        "        result = ml.analyze_data_quality()",
        "",
        "        anom = result['anomalies']",
        "        self.assertEqual(anom['empty_sessions'], 1)",
        "",
        "    def test_anomaly_potential_duplicates(self):",
        "        \"\"\"Test detection of potential duplicate entries.\"\"\"",
        "        timestamp = datetime.now().isoformat()",
        "",
        "        # Two chats with same timestamp and query",
        "        chats = [",
        "            {",
        "                'id': 'chat-001',",
        "                'timestamp': timestamp,",
        "                'session_id': 'session-001',",
        "                'query': 'Identical query',",
        "                'response': 'Response 1',",
        "                'files_referenced': [],",
        "                'files_modified': [],",
        "                'tools_used': [],",
        "            },",
        "            {",
        "                'id': 'chat-002',",
        "                'timestamp': timestamp,",
        "                'session_id': 'session-001',",
        "                'query': 'Identical query',",
        "                'response': 'Response 2',",
        "                'files_referenced': [],",
        "                'files_modified': [],",
        "                'tools_used': [],",
        "            },",
        "        ]",
        "",
        "        for chat in chats:",
        "            chat_file = ml.CHATS_DIR / f\"{chat['id']}.json\"",
        "            with open(chat_file, 'w', encoding='utf-8') as f:",
        "                json.dump(chat, f)",
        "",
        "        result = ml.analyze_data_quality()",
        "",
        "        anom = result['anomalies']",
        "        # Second entry should be flagged as duplicate",
        "        self.assertEqual(anom['potential_duplicates'], 1)",
        "",
        "    def test_quality_score_perfect_data(self):",
        "        \"\"\"Test quality score calculation with perfect data.\"\"\"",
        "        # Create perfect data: complete chats, diverse tools/files, no anomalies",
        "",
        "        # Create 10 complete chats",
        "        for i in range(10):",
        "            chat = {",
        "                'id': f'chat-{i:03d}',",
        "                'timestamp': datetime.now().isoformat(),",
        "                'session_id': 'session-001',",
        "                'query': f'Query {i}',",
        "                'response': f'Response {i}',",
        "                'files_referenced': [f'/file{i}.py'],",
        "                'files_modified': [f'/modified{i}.py'],",
        "                'tools_used': ['Read', 'Edit', 'Grep', 'Bash', 'Write'][i % 5:i % 5 + 2],",
        "                'user_feedback': {'rating': 5},",
        "            }",
        "            chat_file = ml.CHATS_DIR / f\"{chat['id']}.json\"",
        "            with open(chat_file, 'w', encoding='utf-8') as f:",
        "                json.dump(chat, f)",
        "",
        "        # Create 5 commits with CI results",
        "        for i in range(5):",
        "            commit = {",
        "                'hash': f'commit{i:03d}',",
        "                'message': f'Commit {i}',",
        "                'timestamp': datetime.now().isoformat(),",
        "                'files_changed': [f'/commit_file{i}.py'] * (i + 1),",
        "                'ci_result': {'status': 'pass', 'coverage': 90},",
        "                'session_id': 'session-001',",
        "            }",
        "            commit_file = ml.COMMITS_DIR / f\"commit{i}.json\"",
        "            with open(commit_file, 'w', encoding='utf-8') as f:",
        "                json.dump(commit, f)",
        "",
        "        # Create session",
        "        session = {",
        "            'id': 'session-001',",
        "            'start_time': datetime.now().isoformat(),",
        "            'chat_ids': [f'chat-{i:03d}' for i in range(10)],",
        "        }",
        "        with open(ml.SESSIONS_DIR / \"session-001.json\", 'w', encoding='utf-8') as f:",
        "            json.dump(session, f)",
        "",
        "        result = ml.analyze_data_quality()",
        "",
        "        # With perfect data, quality score should be very high",
        "        self.assertGreater(result['quality_score'], 80)",
        "        self.assertLessEqual(result['quality_score'], 100)",
        "",
        "    def test_quality_score_low_with_anomalies(self):",
        "        \"\"\"Test that anomalies reduce quality score.\"\"\"",
        "        # Create data with many anomalies",
        "",
        "        # Empty responses",
        "        for i in range(10):",
        "            chat = {",
        "                'id': f'chat-{i:03d}',",
        "                'timestamp': datetime.now().isoformat(),",
        "                'session_id': 'session-001',",
        "                'query': f'Query {i}',",
        "                'response': '',  # Empty!",
        "                'files_referenced': [],",
        "                'files_modified': [],",
        "                'tools_used': [],",
        "            }",
        "            chat_file = ml.CHATS_DIR / f\"{chat['id']}.json\"",
        "            with open(chat_file, 'w', encoding='utf-8') as f:",
        "                json.dump(chat, f)",
        "",
        "        # Zero-file commits",
        "        for i in range(5):",
        "            commit = {",
        "                'hash': f'commit{i:03d}',",
        "                'message': f'Empty commit {i}',",
        "                'timestamp': datetime.now().isoformat(),",
        "                'files_changed': [],  # No files!",
        "            }",
        "            commit_file = ml.COMMITS_DIR / f\"commit{i}.json\"",
        "            with open(commit_file, 'w', encoding='utf-8') as f:",
        "                json.dump(commit, f)",
        "",
        "        # Empty sessions",
        "        for i in range(3):",
        "            session = {",
        "                'id': f'session-{i:03d}',",
        "                'start_time': datetime.now().isoformat(),",
        "                'chat_ids': [],  # No chats!",
        "            }",
        "            session_file = ml.SESSIONS_DIR / f\"session-{i:03d}.json\"",
        "            with open(session_file, 'w', encoding='utf-8') as f:",
        "                json.dump(session, f)",
        "",
        "        result = ml.analyze_data_quality()",
        "",
        "        # With many anomalies, quality score should be lower",
        "        # Not necessarily very low since some completeness metrics may still be ok",
        "        self.assertLess(result['quality_score'], 100)",
        "",
        "    def test_corrupted_json_files_skipped(self):",
        "        \"\"\"Test that corrupted JSON files are skipped gracefully.\"\"\"",
        "        # Create valid chat",
        "        valid_chat = {",
        "            'id': 'chat-valid',",
        "            'timestamp': datetime.now().isoformat(),",
        "            'session_id': 'session-001',",
        "            'query': 'Test',",
        "            'response': 'Response',",
        "            'files_referenced': [],",
        "            'files_modified': [],",
        "            'tools_used': [],",
        "        }",
        "        with open(ml.CHATS_DIR / \"valid.json\", 'w', encoding='utf-8') as f:",
        "            json.dump(valid_chat, f)",
        "",
        "        # Create corrupted JSON file",
        "        with open(ml.CHATS_DIR / \"corrupted.json\", 'w', encoding='utf-8') as f:",
        "            f.write(\"{ invalid json content }\")",
        "",
        "        # Should not raise an error",
        "        result = ml.analyze_data_quality()",
        "",
        "        # Should count only the valid chat",
        "        comp = result['completeness']",
        "        self.assertEqual(comp['chats_total'], 1)",
        "",
        "    def test_missing_directories_handled(self):",
        "        \"\"\"Test that missing directories are handled gracefully.\"\"\"",
        "        # Remove all data directories",
        "        import shutil",
        "        shutil.rmtree(ml.ML_DATA_DIR)",
        "",
        "        # Should not raise an error, just return empty metrics",
        "        result = ml.analyze_data_quality()",
        "",
        "        self.assertEqual(result['completeness']['chats_total'], 0)",
        "        self.assertEqual(result['completeness']['commits_total'], 0)",
        "        self.assertEqual(result['completeness']['sessions_total'], 0)",
        "",
        "    def test_mixed_valid_invalid_data(self):",
        "        \"\"\"Test analysis with mix of valid and invalid data.\"\"\"",
        "        # Valid complete chat",
        "        chat1 = {",
        "            'id': 'chat-001',",
        "            'timestamp': datetime.now().isoformat(),",
        "            'session_id': 'session-001',",
        "            'query': 'Test',",
        "            'response': 'Response',",
        "            'files_referenced': ['/file.py'],",
        "            'files_modified': [],",
        "            'tools_used': ['Read'],",
        "        }",
        "",
        "        # Invalid incomplete chat",
        "        chat2 = {",
        "            'id': 'chat-002',",
        "            'timestamp': datetime.now().isoformat(),",
        "            'query': 'Test',",
        "            # Missing required fields",
        "        }",
        "",
        "        # Valid commit with CI",
        "        commit1 = {",
        "            'hash': 'abc123',",
        "            'message': 'Test',",
        "            'timestamp': datetime.now().isoformat(),",
        "            'files_changed': ['file.py'],",
        "            'ci_result': {'status': 'pass'},",
        "        }",
        "",
        "        # Valid commit without CI",
        "        commit2 = {",
        "            'hash': 'def456',",
        "            'message': 'Test',",
        "            'timestamp': datetime.now().isoformat(),",
        "            'files_changed': ['other.py'],",
        "        }",
        "",
        "        with open(ml.CHATS_DIR / \"chat1.json\", 'w', encoding='utf-8') as f:",
        "            json.dump(chat1, f)",
        "        with open(ml.CHATS_DIR / \"chat2.json\", 'w', encoding='utf-8') as f:",
        "            json.dump(chat2, f)",
        "        with open(ml.COMMITS_DIR / \"commit1.json\", 'w', encoding='utf-8') as f:",
        "            json.dump(commit1, f)",
        "        with open(ml.COMMITS_DIR / \"commit2.json\", 'w', encoding='utf-8') as f:",
        "            json.dump(commit2, f)",
        "",
        "        result = ml.analyze_data_quality()",
        "",
        "        comp = result['completeness']",
        "        self.assertEqual(comp['chats_total'], 2)",
        "        self.assertEqual(comp['chats_complete'], 1)",
        "        self.assertEqual(comp['commits_total'], 2)",
        "        self.assertEqual(comp['commits_with_ci'], 1)",
        "",
        "    def test_quality_score_boundaries(self):",
        "        \"\"\"Test that quality score is always between 0 and 100.\"\"\"",
        "        # Test with various data scenarios",
        "        scenarios = [",
        "            {},  # Empty",
        "            # Will add data in loop",
        "        ]",
        "",
        "        # Scenario 1: Empty data",
        "        result = ml.analyze_data_quality()",
        "        self.assertGreaterEqual(result['quality_score'], 0)",
        "        self.assertLessEqual(result['quality_score'], 100)",
        "",
        "        # Scenario 2: Some data",
        "        chat = {",
        "            'id': 'chat-001',",
        "            'timestamp': datetime.now().isoformat(),",
        "            'session_id': 'session-001',",
        "            'query': 'Test',",
        "            'response': 'Response',",
        "            'files_referenced': [],",
        "            'files_modified': [],",
        "            'tools_used': [],",
        "        }",
        "        with open(ml.CHATS_DIR / \"chat.json\", 'w', encoding='utf-8') as f:",
        "            json.dump(chat, f)",
        "",
        "        result = ml.analyze_data_quality()",
        "        self.assertGreaterEqual(result['quality_score'], 0)",
        "        self.assertLessEqual(result['quality_score'], 100)",
        "",
        "    def test_zero_division_protection(self):",
        "        \"\"\"Test that analysis handles edge cases without division by zero.\"\"\"",
        "        # Empty data should not cause division by zero",
        "        result = ml.analyze_data_quality()",
        "",
        "        # All percentages should be valid numbers",
        "        comp = result['completeness']",
        "        for key in comp:",
        "            if '_pct' in key:",
        "                self.assertIsInstance(comp[key], (int, float))",
        "                self.assertGreaterEqual(comp[key], 0)",
        "",
        "        # Diversity stats should have valid averages",
        "        div = result['diversity']",
        "        self.assertGreaterEqual(div['query_length_avg'], 0)",
        "        self.assertGreaterEqual(div['response_length_avg'], 0)",
        "",
        "    def test_diversity_with_no_lengths(self):",
        "        \"\"\"Test diversity calculation when no query/response data exists.\"\"\"",
        "        # Create commits only (no chats)",
        "        commit = {",
        "            'hash': 'abc123',",
        "            'message': 'Test',",
        "            'timestamp': datetime.now().isoformat(),",
        "            'files_changed': ['file.py'],",
        "        }",
        "        with open(ml.COMMITS_DIR / \"commit.json\", 'w', encoding='utf-8') as f:",
        "            json.dump(commit, f)",
        "",
        "        result = ml.analyze_data_quality()",
        "",
        "        div = result['diversity']",
        "        # Should handle empty length lists gracefully",
        "        self.assertEqual(div['query_length_min'], 0)",
        "        self.assertEqual(div['query_length_avg'], 0)",
        "        self.assertEqual(div['query_length_max'], 0)",
        "        self.assertEqual(div['response_length_min'], 0)",
        "        self.assertEqual(div['response_length_avg'], 0)",
        "        self.assertEqual(div['response_length_max'], 0)",
        "",
        "",
        "if __name__ == \"__main__\":",
        "    unittest.main()"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "tests/unit/test_ml_session_handoff.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "#!/usr/bin/env python3",
        "\"\"\"",
        "Tests for ML data collector session handoff functionality.",
        "\"\"\"",
        "",
        "import json",
        "import os",
        "import sys",
        "import tempfile",
        "import unittest",
        "from datetime import datetime",
        "from pathlib import Path",
        "from unittest.mock import patch, MagicMock",
        "",
        "# Add scripts to path",
        "sys.path.insert(0, str(Path(__file__).parent.parent.parent / \"scripts\"))",
        "",
        "import ml_data_collector as ml",
        "",
        "",
        "class TestSessionHandoff(unittest.TestCase):",
        "    \"\"\"Test session handoff document generation.\"\"\"",
        "",
        "    def setUp(self):",
        "        \"\"\"Set up test environment with temporary directories.\"\"\"",
        "        self.test_dir = tempfile.TemporaryDirectory()",
        "        self.test_path = Path(self.test_dir.name)",
        "",
        "        # Temporarily override ML data directories",
        "        self.original_ml_data_dir = ml.ML_DATA_DIR",
        "        self.original_commits_dir = ml.COMMITS_DIR",
        "        self.original_sessions_dir = ml.SESSIONS_DIR",
        "        self.original_chats_dir = ml.CHATS_DIR",
        "        self.original_current_session_file = ml.CURRENT_SESSION_FILE",
        "",
        "        ml.ML_DATA_DIR = self.test_path / \".git-ml\"",
        "        ml.COMMITS_DIR = ml.ML_DATA_DIR / \"commits\"",
        "        ml.SESSIONS_DIR = ml.ML_DATA_DIR / \"sessions\"",
        "        ml.CHATS_DIR = ml.ML_DATA_DIR / \"chats\"",
        "        ml.CURRENT_SESSION_FILE = ml.ML_DATA_DIR / \"current_session.json\"",
        "",
        "        # Create test directories",
        "        ml.ensure_dirs()",
        "",
        "    def tearDown(self):",
        "        \"\"\"Clean up test environment.\"\"\"",
        "        # Restore original directories",
        "        ml.ML_DATA_DIR = self.original_ml_data_dir",
        "        ml.COMMITS_DIR = self.original_commits_dir",
        "        ml.SESSIONS_DIR = self.original_sessions_dir",
        "        ml.CHATS_DIR = self.original_chats_dir",
        "        ml.CURRENT_SESSION_FILE = self.original_current_session_file",
        "",
        "        # Clean up temp directory",
        "        self.test_dir.cleanup()",
        "",
        "    def test_handoff_no_session(self):",
        "        \"\"\"Test handoff with no active session.\"\"\"",
        "        handoff = ml.generate_session_handoff()",
        "        self.assertIn(\"No active session\", handoff)",
        "",
        "    def test_handoff_empty_session(self):",
        "        \"\"\"Test handoff with empty session (no chats).\"\"\"",
        "        # Create empty session",
        "        session_id = ml.start_session()",
        "",
        "        handoff = ml.generate_session_handoff()",
        "",
        "        # Verify structure",
        "        self.assertIn(\"Session Handoff:\", handoff)",
        "        self.assertIn(session_id, handoff)",
        "        self.assertIn(\"## Summary\", handoff)",
        "        self.assertIn(\"## Key Work Done\", handoff)",
        "        self.assertIn(\"## Files Touched\", handoff)",
        "        self.assertIn(\"## Related Commits\", handoff)",
        "        self.assertIn(\"## Suggested Next Steps\", handoff)",
        "",
        "        # Verify empty session indicators",
        "        self.assertIn(\"Exchanges: 0\", handoff)",
        "        self.assertIn(\"No significant work recorded\", handoff)",
        "        self.assertIn(\"No files modified or referenced\", handoff)",
        "        self.assertIn(\"No commits made in this session\", handoff)",
        "",
        "    def test_handoff_with_chats(self):",
        "        \"\"\"Test handoff with chat entries.\"\"\"",
        "        # Create session",
        "        session_id = ml.start_session()",
        "",
        "        # Add some chat entries",
        "        chat1 = ml.ChatEntry(",
        "            id=\"chat-test-001\",",
        "            timestamp=datetime.now().isoformat(),",
        "            session_id=session_id,",
        "            query=\"How do I implement feature X?\",",
        "            response=\"You can implement it by...\",",
        "            files_referenced=[\"/path/to/file1.py\"],",
        "            files_modified=[],",
        "            tools_used=[\"Read\", \"Grep\"],",
        "        )",
        "        ml.save_chat_entry(chat1, validate=False)",
        "        ml.add_chat_to_session(chat1.id)",
        "",
        "        chat2 = ml.ChatEntry(",
        "            id=\"chat-test-002\",",
        "            timestamp=datetime.now().isoformat(),",
        "            session_id=session_id,",
        "            query=\"Fix bug in authentication\",",
        "            response=\"The bug is in auth.py line 42\",",
        "            files_referenced=[\"/path/to/auth.py\"],",
        "            files_modified=[\"/path/to/auth.py\"],",
        "            tools_used=[\"Read\", \"Edit\"],",
        "        )",
        "        ml.save_chat_entry(chat2, validate=False)",
        "        ml.add_chat_to_session(chat2.id)",
        "",
        "        handoff = ml.generate_session_handoff()",
        "",
        "        # Verify chat data appears",
        "        self.assertIn(\"Exchanges: 2\", handoff)",
        "        self.assertIn(\"How do I implement feature X\", handoff)",
        "        self.assertIn(\"Fix bug in authentication\", handoff)",
        "        self.assertIn(\"/path/to/auth.py\", handoff)",
        "        self.assertIn(\"Edit, Grep, Read\", handoff)  # Sorted tools",
        "",
        "    def test_handoff_with_commits(self):",
        "        \"\"\"Test handoff with related commits.\"\"\"",
        "        # Create session",
        "        session_id = ml.start_session()",
        "",
        "        # Create a mock commit linked to session",
        "        commit_data = {",
        "            \"hash\": \"abc123def456\",",
        "            \"message\": \"feat: Add session handoff feature\",",
        "            \"author\": \"Test Author\",",
        "            \"timestamp\": datetime.now().isoformat(),",
        "            \"branch\": \"main\",",
        "            \"files_changed\": [\"scripts/ml_data_collector.py\"],",
        "            \"insertions\": 100,",
        "            \"deletions\": 10,",
        "            \"hunks\": [],",
        "            \"hour_of_day\": 10,",
        "            \"day_of_week\": \"Monday\",",
        "            \"seconds_since_last_commit\": None,",
        "            \"is_merge\": False,",
        "            \"is_initial\": False,",
        "            \"parent_count\": 1,",
        "            \"session_id\": session_id,",
        "            \"related_chats\": [],",
        "        }",
        "",
        "        commit_file = ml.COMMITS_DIR / f\"{commit_data['hash'][:8]}_test.json\"",
        "        with open(commit_file, 'w', encoding='utf-8') as f:",
        "            json.dump(commit_data, f)",
        "",
        "        handoff = ml.generate_session_handoff()",
        "",
        "        # Verify commit appears",
        "        self.assertIn(\"abc123de\", handoff)  # First 8 chars of hash",
        "        self.assertIn(\"feat: Add session handoff feature\", handoff)",
        "",
        "    def test_handoff_suggestions_modified_files_no_commit(self):",
        "        \"\"\"Test suggestions when files are modified but no commit.\"\"\"",
        "        session_id = ml.start_session()",
        "",
        "        # Add chat with file modifications",
        "        chat = ml.ChatEntry(",
        "            id=\"chat-test-003\",",
        "            timestamp=datetime.now().isoformat(),",
        "            session_id=session_id,",
        "            query=\"Update feature\",",
        "            response=\"Done\",",
        "            files_referenced=[],",
        "            files_modified=[\"/path/to/feature.py\"],",
        "            tools_used=[\"Edit\"],",
        "        )",
        "        ml.save_chat_entry(chat, validate=False)",
        "        ml.add_chat_to_session(chat.id)",
        "",
        "        handoff = ml.generate_session_handoff()",
        "",
        "        # Should suggest committing changes",
        "        self.assertIn(\"Review modified files\", handoff)",
        "        self.assertIn(\"test suite\", handoff)",
        "",
        "    def test_handoff_suggestions_recent_errors(self):",
        "        \"\"\"Test suggestions when recent responses mention errors.\"\"\"",
        "        session_id = ml.start_session()",
        "",
        "        # Add chats with errors in responses",
        "        for i in range(3):",
        "            chat = ml.ChatEntry(",
        "                id=f\"chat-test-00{i}\",",
        "                timestamp=datetime.now().isoformat(),",
        "                session_id=session_id,",
        "                query=f\"Task {i}\",",
        "                response=\"Error: something failed\" if i == 2 else \"OK\",",
        "                files_referenced=[],",
        "                files_modified=[],",
        "                tools_used=[],",
        "            )",
        "            ml.save_chat_entry(chat, validate=False)",
        "            ml.add_chat_to_session(chat.id)",
        "",
        "        handoff = ml.generate_session_handoff()",
        "",
        "        # Should suggest debugging",
        "        self.assertIn(\"errors\", handoff.lower())",
        "        self.assertIn(\"debugging\", handoff.lower())",
        "",
        "    def test_handoff_duration_calculation(self):",
        "        \"\"\"Test that duration is calculated correctly.\"\"\"",
        "        session_id = ml.start_session()",
        "",
        "        handoff = ml.generate_session_handoff()",
        "",
        "        # Duration should be in minutes (session just started)",
        "        self.assertIn(\"0m\", handoff)",
        "",
        "    def test_handoff_tools_list(self):",
        "        \"\"\"Test that tools are listed and sorted correctly.\"\"\"",
        "        session_id = ml.start_session()",
        "",
        "        # Add chat with multiple tools",
        "        chat = ml.ChatEntry(",
        "            id=\"chat-test-tools\",",
        "            timestamp=datetime.now().isoformat(),",
        "            session_id=session_id,",
        "            query=\"Complex task\",",
        "            response=\"Done using multiple tools\",",
        "            files_referenced=[],",
        "            files_modified=[],",
        "            tools_used=[\"Write\", \"Bash\", \"Read\", \"Edit\"],",
        "        )",
        "        ml.save_chat_entry(chat, validate=False)",
        "        ml.add_chat_to_session(chat.id)",
        "",
        "        handoff = ml.generate_session_handoff()",
        "",
        "        # Tools should be sorted alphabetically",
        "        tools_line = [line for line in handoff.split('\\n') if 'Tools used:' in line][0]",
        "        self.assertIn(\"Bash, Edit, Read, Write\", tools_line)",
        "",
        "    def test_handoff_file_sections(self):",
        "        \"\"\"Test that files are separated into modified and referenced sections.\"\"\"",
        "        session_id = ml.start_session()",
        "",
        "        # Add chat with both modified and referenced files",
        "        chat = ml.ChatEntry(",
        "            id=\"chat-test-files\",",
        "            timestamp=datetime.now().isoformat(),",
        "            session_id=session_id,",
        "            query=\"Work on files\",",
        "            response=\"Updated files\",",
        "            files_referenced=[\"/ref1.py\", \"/ref2.py\"],",
        "            files_modified=[\"/mod1.py\", \"/mod2.py\"],",
        "            tools_used=[\"Read\", \"Edit\"],",
        "        )",
        "        ml.save_chat_entry(chat, validate=False)",
        "        ml.add_chat_to_session(chat.id)",
        "",
        "        handoff = ml.generate_session_handoff()",
        "",
        "        # Check for proper sections",
        "        self.assertIn(\"### Modified:\", handoff)",
        "        self.assertIn(\"### Referenced:\", handoff)",
        "        self.assertIn(\"/mod1.py\", handoff)",
        "        self.assertIn(\"/ref1.py\", handoff)",
        "",
        "",
        "if __name__ == \"__main__\":",
        "    unittest.main()"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    }
  ],
  "hour_of_day": 13,
  "day_of_week": "Monday",
  "seconds_since_last_commit": -1079,
  "is_merge": true,
  "is_initial": false,
  "parent_count": 2,
  "session_id": null,
  "related_chats": [],
  "ci_result": null,
  "reverted": false,
  "amended": false
}