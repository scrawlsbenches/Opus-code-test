{
  "hash": "9b1f6f6d7018d525df8f0e5cf2692ef2c6777b39",
  "message": "Merge pull request #11 from scrawlsbenches/claude/run-showcase-012X3h3UopsvVFG749BKgxtM",
  "author": "scrawlsbenches",
  "timestamp": "2025-12-10 00:48:56 -0500",
  "branch": "claude/multi-index-design-DvifZ",
  "files_changed": [
    "cortical/analysis.py",
    "cortical/semantics.py",
    "showcase.py",
    "tests/test_processor.py"
  ],
  "insertions": 164,
  "deletions": 55,
  "hunks": [
    {
      "file": "cortical/analysis.py",
      "function": "def compute_bigram_connections(",
      "start_line": 981,
      "lines_added": [
        "    # Use inverted index for O(d * bÂ²) instead of O(nÂ²) where d=docs, b=bigrams per doc",
        "    doc_to_bigrams: Dict[str, List[Minicolumn]] = defaultdict(list)",
        "    for bigram in bigrams:",
        "        for doc_id in bigram.document_ids:",
        "            doc_to_bigrams[doc_id].append(bigram)",
        "",
        "    # Track pairs we've already processed to avoid duplicate work",
        "    cooccur_processed: Set[Tuple[str, str]] = set()",
        "",
        "    for doc_id, doc_bigrams in doc_to_bigrams.items():",
        "        # Only compare bigrams within the same document",
        "        for i, b1 in enumerate(doc_bigrams):",
        "            docs1 = b1.document_ids",
        "            for b2 in doc_bigrams[i+1:]:",
        "                # Skip if already processed this pair",
        "                pair_key = tuple(sorted([b1.id, b2.id]))",
        "                if pair_key in cooccur_processed:",
        "                    continue",
        "                cooccur_processed.add(pair_key)",
        "",
        "                docs2 = b2.document_ids",
        "                shared_docs = docs1 & docs2",
        "                if len(shared_docs) >= min_shared_docs:",
        "                    # Weight by Jaccard similarity of document sets",
        "                    jaccard = len(shared_docs) / len(docs1 | docs2)",
        "                    weight = cooccurrence_weight * jaccard",
        "                    add_connection(b1, b2, weight, 'cooccurrence')"
      ],
      "lines_removed": [
        "    for i, b1 in enumerate(bigrams):",
        "        docs1 = b1.document_ids",
        "        if not docs1:",
        "            continue",
        "",
        "        for b2 in bigrams[i+1:]:",
        "            docs2 = b2.document_ids",
        "            if not docs2:",
        "                continue",
        "",
        "            shared_docs = docs1 & docs2",
        "            if len(shared_docs) >= min_shared_docs:",
        "                # Weight by Jaccard similarity of document sets",
        "                jaccard = len(shared_docs) / len(docs1 | docs2)",
        "                weight = cooccurrence_weight * jaccard",
        "                add_connection(b1, b2, weight, 'cooccurrence')"
      ],
      "context_before": [
        "    # \"machine_learning\" â†” \"learning_algorithms\"",
        "    for term in left_index:",
        "        if term in right_index:",
        "            # term appears as right component in some bigrams and left in others",
        "            for b_left in right_index[term]:  # ends with term",
        "                for b_right in left_index[term]:  # starts with term",
        "                    if b_left.id != b_right.id:",
        "                        add_connection(b_left, b_right, chain_weight, 'chain')",
        "",
        "    # 3. Connect bigrams that co-occur in the same documents"
      ],
      "context_after": [
        "",
        "    return {",
        "        'connections_created': len(connected_pairs),",
        "        'bigrams': len(bigrams),",
        "        'component_connections': component_connections,",
        "        'chain_connections': chain_connections,",
        "        'cooccurrence_connections': cooccurrence_connections",
        "    }",
        "",
        ""
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/semantics.py",
      "function": "Corpus-derived semantic relations and retrofitting.",
      "start_line": 7,
      "lines_added": [
        "try:",
        "    import numpy as np",
        "    HAS_NUMPY = True",
        "except ImportError:",
        "    HAS_NUMPY = False",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "Extracts semantic relationships from co-occurrence patterns,",
        "then uses them to adjust connection weights (retrofitting).",
        "This is like building a \"poor man's ConceptNet\" from the corpus itself.",
        "\"\"\"",
        "",
        "import math",
        "import re",
        "from typing import Any, Dict, List, Tuple, Set, Optional",
        "from collections import defaultdict",
        ""
      ],
      "context_after": [
        "from .layers import CorticalLayer, HierarchicalLayer",
        "from .minicolumn import Minicolumn",
        "",
        "",
        "# Relation type weights for retrofitting",
        "RELATION_WEIGHTS = {",
        "    'IsA': 1.5,",
        "    'PartOf': 1.2,",
        "    'HasA': 1.0,",
        "    'UsedFor': 0.8,"
      ],
      "change_type": "add"
    },
    {
      "file": "cortical/semantics.py",
      "function": "def extract_corpus_semantics(",
      "start_line": 260,
      "lines_added": [
        "    # Compute total once outside the loop (was being computed per iteration!)",
        "    total = sum(cooccurrence.values())",
        "",
        "",
        "",
        "    n_terms = len(terms)",
        "",
        "    if n_terms > 1 and HAS_NUMPY:",
        "        # Fast path: use numpy vectorization",
        "        # Build vocabulary of all context keys",
        "        all_keys: Set[str] = set()",
        "        for vec in context_vectors.values():",
        "            all_keys.update(vec.keys())",
        "        vocab = sorted(all_keys)",
        "        key_to_idx = {k: i for i, k in enumerate(vocab)}",
        "        n_vocab = len(vocab)",
        "",
        "        # Convert sparse vectors to dense numpy matrix",
        "        matrix = np.zeros((n_terms, n_vocab), dtype=np.float32)",
        "        for i, term in enumerate(terms):",
        "            vec = context_vectors[term]",
        "            for k, v in vec.items():",
        "                matrix[i, key_to_idx[k]] = v",
        "",
        "        # Normalize rows for cosine similarity (dot product of normalized = cosine)",
        "        norms = np.linalg.norm(matrix, axis=1, keepdims=True)",
        "        norms[norms == 0] = 1  # Avoid division by zero",
        "        matrix_norm = matrix / norms",
        "",
        "        # Compute all pairwise cosine similarities via matrix multiplication",
        "        similarities = matrix_norm @ matrix_norm.T",
        "",
        "        # Count non-zero elements per row (for min common keys filter)",
        "        nonzero_counts = (matrix > 0).astype(np.int32)",
        "",
        "        # Extract pairs with similarity > 0.3 and at least 3 common keys",
        "        for i in range(n_terms):",
        "            row_i = nonzero_counts[i]",
        "            for j in range(i + 1, n_terms):",
        "                if similarities[i, j] > 0.3:",
        "                    common_count = np.sum(row_i & nonzero_counts[j])",
        "                    if common_count >= 3:",
        "                        relations.append((terms[i], 'SimilarTo', terms[j], float(similarities[i, j])))",
        "",
        "    elif n_terms > 1:",
        "        # Fallback: pure Python implementation",
        "        magnitudes: Dict[str, float] = {}",
        "        for term in terms:",
        "            vec = context_vectors[term]",
        "            mag = math.sqrt(sum(v * v for v in vec.values()))",
        "            magnitudes[term] = mag",
        "",
        "        key_sets: Dict[str, set] = {term: set(context_vectors[term].keys()) for term in terms}",
        "",
        "        for i, t1 in enumerate(terms):",
        "            vec1 = context_vectors[t1]",
        "            mag1 = magnitudes[t1]",
        "            if mag1 == 0:",
        "                continue",
        "            keys1 = key_sets[t1]",
        "            for t2 in terms[i+1:]:",
        "                mag2 = magnitudes[t2]",
        "                if mag2 == 0:",
        "                    continue",
        "                common = keys1 & key_sets[t2]",
        "                if len(common) >= 3:",
        "                    vec2 = context_vectors[t2]",
        "                    dot = sum(vec1[k] * vec2[k] for k in common)"
      ],
      "lines_removed": [
        "            ",
        "                total = sum(cooccurrence.values())",
        "                ",
        "    for i, t1 in enumerate(terms):",
        "        vec1 = context_vectors[t1]",
        "",
        "        for t2 in terms[i+1:]:",
        "            vec2 = context_vectors[t2]",
        "            # Cosine similarity of context vectors",
        "            common = set(vec1.keys()) & set(vec2.keys())",
        "            if len(common) >= 3:",
        "                dot = sum(vec1[k] * vec2[k] for k in common)",
        "                mag1 = math.sqrt(sum(v*v for v in vec1.values()))",
        "                mag2 = math.sqrt(sum(v*v for v in vec2.values()))",
        "                if mag1 > 0 and mag2 > 0:"
      ],
      "context_before": [
        "                    other = tokens[j]",
        "                    if token < other:  # Avoid duplicates",
        "                        cooccurrence[(token, other)] += 1",
        "                    else:",
        "                        cooccurrence[(other, token)] += 1",
        "                    ",
        "                    # Build context vector",
        "                    context_vectors[token][other] += 1",
        "    ",
        "    # Extract RelatedTo from co-occurrence"
      ],
      "context_after": [
        "    for (t1, t2), count in cooccurrence.items():",
        "        if count >= min_cooccurrence:",
        "            # Normalize by frequency",
        "            col1 = layer0.get_minicolumn(t1)",
        "            col2 = layer0.get_minicolumn(t2)",
        "            if col1 and col2:",
        "                # PMI-like score",
        "                expected = (col1.occurrence_count * col2.occurrence_count) / (total + 1)",
        "                pmi = math.log((count + 1) / (expected + 1))",
        "                if pmi > 0:",
        "                    relations.append((t1, 'CoOccurs', t2, min(pmi, 3.0)))",
        "    ",
        "    # Extract SimilarTo from context similarity",
        "    terms = list(context_vectors.keys())",
        "",
        "",
        "                    sim = dot / (mag1 * mag2)",
        "                    if sim > 0.3:",
        "                        relations.append((t1, 'SimilarTo', t2, sim))",
        "",
        "    # Extract commonsense relations from text patterns",
        "    if use_pattern_extraction:",
        "        valid_terms = set(layer0.minicolumns.keys())",
        "        pattern_relations = extract_pattern_relations(",
        "            documents,",
        "            valid_terms,"
      ],
      "change_type": "modify"
    },
    {
      "file": "showcase.py",
      "function": "class CorticalShowcase:",
      "start_line": 96,
      "lines_added": [
        "        # Run all computations with hybrid strategy for better Layer 2 connectivity",
        "        self.processor.compute_all(",
        "            verbose=False,",
        "            connection_strategy='hybrid',",
        "            cluster_strictness=0.5,",
        "            bridge_weight=0.3",
        "        )"
      ],
      "lines_removed": [
        "        # Run all computations",
        "        self.processor.compute_all(verbose=False)"
      ],
      "context_before": [
        "            filepath = os.path.join(self.samples_dir, filename)",
        "            with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:",
        "                content = f.read()",
        "            ",
        "            doc_id = filename.replace('.txt', '')",
        "            self.processor.process_document(doc_id, content)",
        "            word_count = len(content.split())",
        "            self.loaded_files.append((doc_id, word_count))",
        "            print(f\"  ðŸ“„ {doc_id:30} ({word_count:3} words)\")",
        "        "
      ],
      "context_after": [
        "        print(\"\\nComputing cortical representations...\")",
        "        ",
        "        layer0 = self.processor.get_layer(CorticalLayer.TOKENS)",
        "        layer1 = self.processor.get_layer(CorticalLayer.BIGRAMS)",
        "        ",
        "        print(f\"\\nâœ“ Processed {len(self.loaded_files)} documents\")",
        "        print(f\"âœ“ Created {layer0.column_count()} token minicolumns\")",
        "        print(f\"âœ“ Created {layer1.column_count()} bigram minicolumns\")",
        "        print(f\"âœ“ Formed {layer0.total_connections()} lateral connections\")",
        "        ",
        "        return True"
      ],
      "change_type": "modify"
    },
    {
      "file": "tests/test_processor.py",
      "function": "class TestConceptConnections(unittest.TestCase):",
      "start_line": 1164,
      "lines_added": [
        "        # Create processor with documents that have NO overlap but enough content",
        "        # to form distinct concept clusters",
        "            \"doc1\",",
        "            \"Neural networks learn patterns from data using algorithms. \"",
        "            \"Deep learning models process information through layers. \"",
        "            \"Machine learning systems train on examples to improve accuracy.\"",
        "            \"doc2\",",
        "            \"Bread baking requires yeast and flour for fermentation. \"",
        "            \"Sourdough starters contain wild yeast and bacteria cultures. \"",
        "            \"Kneading dough develops gluten structure for texture.\"",
        "        processor.compute_all(verbose=False, build_concepts=False)",
        "        processor.build_concept_clusters(min_cluster_size=2, verbose=False)",
        "        self.assertGreaterEqual(layer2.column_count(), 2, \"Need at least 2 concepts\")"
      ],
      "lines_removed": [
        "        # Create processor with documents that have NO overlap",
        "            \"doc1\", \"Neural networks learn patterns from data using algorithms.\"",
        "            \"doc2\", \"Bread baking requires yeast and flour for fermentation.\"",
        "        processor.compute_all(verbose=False, build_concepts=True)",
        "        if layer2.column_count() < 2:",
        "            self.skipTest(\"Not enough concepts formed for this test\")"
      ],
      "context_before": [
        "        # The unrelated_doc about pottery should form isolated concepts",
        "        layer2 = self.processor.get_layer(CorticalLayer.CONCEPTS)",
        "",
        "        if layer2.column_count() > 0:",
        "            # At least some concepts should be isolated if topics are different",
        "            # This is a soft test since clustering may group differently",
        "            pass  # Concept isolation depends on clustering results",
        "",
        "    def test_concept_connections_zero_thresholds(self):",
        "        \"\"\"Test that min_shared_docs=0 and min_jaccard=0 allow all connections.\"\"\""
      ],
      "context_after": [
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(",
        "        )",
        "        processor.process_document(",
        "        )",
        "",
        "        layer2 = processor.get_layer(CorticalLayer.CONCEPTS)",
        "",
        "        # Clear connections",
        "        for concept in layer2.minicolumns.values():",
        "            concept.lateral_connections.clear()",
        "",
        "        # With default thresholds, should get 0 connections (no doc overlap)",
        "        stats_default = processor.compute_concept_connections(verbose=False)",
        "",
        "        # Clear again",
        "        for concept in layer2.minicolumns.values():"
      ],
      "change_type": "modify"
    },
    {
      "file": "tests/test_processor.py",
      "function": "class TestConceptConnections(unittest.TestCase):",
      "start_line": 1207,
      "lines_added": [
        "            \"doc1\",",
        "            \"Dogs are animals that bark and run in parks. \"",
        "            \"Canines make loyal pets and companions for families. \"",
        "            \"Puppies require training and socialization early.\"",
        "            \"doc2\",",
        "            \"Cats are animals that meow and climb on furniture. \"",
        "            \"Felines are independent pets that groom themselves. \"",
        "            \"Kittens play with toys and explore their surroundings.\"",
        "        processor.process_document(",
        "            \"doc3\",",
        "            \"Quantum physics studies subatomic particle behavior. \"",
        "            \"Electrons orbit atomic nuclei in probability clouds. \"",
        "            \"Wave functions describe quantum mechanical states.\"",
        "        )",
        "        processor.compute_all(verbose=False, build_concepts=False)",
        "        processor.build_concept_clusters(min_cluster_size=2, verbose=False)",
        "        self.assertGreaterEqual(layer2.column_count(), 2, \"Need at least 2 concepts\")",
        "            \"doc1\",",
        "            \"Neural networks process information through layers. \"",
        "            \"Artificial intelligence learns patterns from training data. \"",
        "            \"Machine learning algorithms optimize model parameters.\"",
        "            \"doc2\",",
        "            \"Cooking recipes require specific ingredients and techniques. \"",
        "            \"Chefs prepare dishes using various culinary methods. \"",
        "            \"Kitchen equipment helps with food preparation tasks.\"",
        "        processor.compute_all(verbose=False, build_concepts=False)",
        "        processor.build_concept_clusters(min_cluster_size=2, verbose=False)",
        "        self.assertGreaterEqual(layer2.column_count(), 2, \"Need at least 2 concepts\")",
        "            \"doc1\",",
        "            \"Machine learning algorithms process data efficiently. \"",
        "            \"Neural networks train on large datasets to find patterns. \"",
        "            \"Supervised learning requires labeled training examples.\"",
        "            \"doc2\",",
        "            \"Ocean waves crash against rocky coastal shores. \"",
        "            \"Marine biology studies creatures living underwater. \"",
        "            \"Coral reefs provide habitat for diverse fish species.\"",
        "            \"doc3\",",
        "            \"Ancient history explores civilizations from the past. \"",
        "            \"Archaeological excavations uncover buried artifacts. \"",
        "            \"Museums preserve historical objects for education.\"",
        "        processor.compute_all(verbose=False, build_concepts=False)",
        "        processor.build_concept_clusters(min_cluster_size=2, verbose=False)",
        "        self.assertGreaterEqual(layer2.column_count(), 2, \"Need at least 2 concepts\")"
      ],
      "lines_removed": [
        "            \"doc1\", \"Dogs are animals. Dogs bark and run.\"",
        "            \"doc2\", \"Cats are animals. Cats meow and climb.\"",
        "        processor.compute_all(verbose=False, build_concepts=True)",
        "        if layer2.column_count() < 2:",
        "            self.skipTest(\"Not enough concepts formed for this test\")",
        "            \"doc1\", \"Neural networks process information through layers.\"",
        "            \"doc2\", \"Deep learning models use neural architectures.\"",
        "        processor.compute_all(verbose=False, build_concepts=True)",
        "        if layer2.column_count() < 2:",
        "            self.skipTest(\"Not enough concepts formed for this test\")",
        "            \"doc1\", \"Machine learning algorithms process data efficiently.\"",
        "            \"doc2\", \"Deep learning networks learn patterns from examples.\"",
        "            \"doc3\", \"Artificial intelligence uses machine learning methods.\"",
        "        processor.compute_all(verbose=False, build_concepts=True)",
        "        if layer2.column_count() < 2:",
        "            self.skipTest(\"Not enough concepts formed for this test\")"
      ],
      "context_before": [
        "        self.assertGreaterEqual(",
        "            stats_zero['connections_created'],",
        "            stats_default['connections_created']",
        "        )",
        "",
        "    def test_concept_connections_member_semantics(self):",
        "        \"\"\"Test that use_member_semantics creates connections via semantic relations.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        # Create documents with semantically related but non-overlapping content",
        "        processor.process_document("
      ],
      "context_after": [
        "        )",
        "        processor.process_document(",
        "        )",
        "        processor.extract_corpus_semantics(verbose=False)",
        "",
        "        layer2 = processor.get_layer(CorticalLayer.CONCEPTS)",
        "",
        "        # Clear connections",
        "        for concept in layer2.minicolumns.values():",
        "            concept.lateral_connections.clear()",
        "",
        "        # With member semantics enabled",
        "        stats = processor.compute_concept_connections(",
        "            use_member_semantics=True,",
        "            verbose=False",
        "        )",
        "",
        "        # Should have statistics for semantic connections",
        "        self.assertIn('semantic_connections', stats)",
        "        self.assertIn('doc_overlap_connections', stats)",
        "",
        "    def test_concept_connections_embedding_similarity(self):",
        "        \"\"\"Test that use_embedding_similarity creates connections via embeddings.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(",
        "        )",
        "        processor.process_document(",
        "        )",
        "        processor.compute_graph_embeddings(verbose=False)",
        "",
        "        layer2 = processor.get_layer(CorticalLayer.CONCEPTS)",
        "",
        "        # Clear connections",
        "        for concept in layer2.minicolumns.values():",
        "            concept.lateral_connections.clear()",
        "",
        "        # With embedding similarity enabled",
        "        stats = processor.compute_concept_connections(",
        "            use_embedding_similarity=True,",
        "            embedding_threshold=0.1,  # Low threshold to catch similarities",
        "            verbose=False",
        "        )",
        "",
        "        # Should have statistics for embedding connections",
        "        self.assertIn('embedding_connections', stats)",
        "",
        "    def test_concept_connections_combined_strategies(self):",
        "        \"\"\"Test combining multiple connection strategies.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(",
        "        )",
        "        processor.process_document(",
        "        )",
        "        processor.process_document(",
        "        )",
        "        processor.extract_corpus_semantics(verbose=False)",
        "        processor.compute_graph_embeddings(verbose=False)",
        "",
        "        layer2 = processor.get_layer(CorticalLayer.CONCEPTS)",
        "",
        "        # Clear connections",
        "        for concept in layer2.minicolumns.values():",
        "            concept.lateral_connections.clear()",
        "",
        "        # Enable all strategies",
        "        stats = processor.compute_concept_connections(",
        "            use_semantics=True,",
        "            use_member_semantics=True,",
        "            use_embedding_similarity=True,"
      ],
      "change_type": "modify"
    }
  ],
  "hour_of_day": 5,
  "day_of_week": "Wednesday",
  "seconds_since_last_commit": -460552,
  "is_merge": true,
  "is_initial": false,
  "parent_count": 2,
  "session_id": null,
  "related_chats": [],
  "ci_result": null,
  "reverted": false,
  "amended": false
}