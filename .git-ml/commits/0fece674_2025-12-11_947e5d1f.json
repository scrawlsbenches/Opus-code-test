{
  "hash": "0fece67406557ab657caf35d8b3c7bed489209b5",
  "message": "Merge pull request #32 from scrawlsbenches/claude/code-review-improvements-01MoEjyaoQAkkyWQcihzEY8T",
  "author": "scrawlsbenches",
  "timestamp": "2025-12-11 08:42:34 -0500",
  "branch": "claude/multi-index-design-DvifZ",
  "files_changed": [
    ".claude/skills/ai-metadata/SKILL.md",
    ".claude/skills/corpus-indexer/SKILL.md",
    ".gitignore",
    "CLAUDE.md",
    "README.md",
    "TASK_LIST.md",
    "docs/quickstart.md",
    "scripts/generate_ai_metadata.py",
    "tests/test_generate_ai_metadata.py"
  ],
  "insertions": 2025,
  "deletions": 18,
  "hunks": [
    {
      "file": ".claude/skills/ai-metadata/SKILL.md",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "---",
        "name: ai-metadata",
        "description: View AI-friendly metadata for code modules. Use when exploring unfamiliar modules to quickly understand structure, functions, and relationships without reading entire files.",
        "allowed-tools: Read, Bash, Glob",
        "---",
        "# AI Metadata Viewer Skill",
        "",
        "This skill provides **rapid module understanding** through pre-generated metadata files. When you need to understand a module's structure quickly, use the `.ai_meta` files instead of reading entire source files.",
        "",
        "## What AI Metadata Provides",
        "",
        "Each `.ai_meta` file contains:",
        "",
        "| Section | Description |",
        "|---------|-------------|",
        "| `module_doc` | Truncated docstring explaining module purpose |",
        "| `sections` | Logical groupings of functions (Persistence, Query, Analysis, etc.) |",
        "| `classes` | Class definitions with inheritance and method lists |",
        "| `functions` | All functions with signatures, docs, and `see_also` cross-references |",
        "| `imports` | Stdlib and local imports for understanding dependencies |",
        "| `complexity_hints` | Warnings about expensive operations (PageRank, clustering, etc.) |",
        "",
        "## Quick Start",
        "",
        "### 1. Generate Metadata (if not present)",
        "",
        "```bash",
        "# Generate for all cortical modules",
        "python scripts/generate_ai_metadata.py",
        "",
        "# Or regenerate after code changes",
        "python scripts/generate_ai_metadata.py --incremental",
        "```",
        "",
        "### 2. View Module Overview",
        "",
        "```bash",
        "# Read metadata for a specific module",
        "cat cortical/processor.py.ai_meta",
        "",
        "# Or use Read tool on the .ai_meta file",
        "```",
        "",
        "### 3. Find Available Metadata",
        "",
        "```bash",
        "# List all metadata files",
        "ls cortical/*.ai_meta tests/*.ai_meta",
        "```",
        "",
        "## Usage Patterns",
        "",
        "### Understanding a New Module",
        "",
        "Instead of reading the entire source file, read the `.ai_meta` first:",
        "",
        "```",
        "# EFFICIENT: Read metadata first (structured overview)",
        "Read cortical/processor.py.ai_meta",
        "",
        "# THEN: Read specific functions as needed",
        "Read cortical/processor.py (lines 127-150)",
        "```",
        "",
        "### Finding Related Functions",
        "",
        "The `see_also` field connects related functions:",
        "",
        "```yaml",
        "functions:",
        "  add_lateral_connection:",
        "    see_also:",
        "      - add_typed_connection",
        "      - add_feedforward_connection",
        "```",
        "",
        "### Identifying Expensive Operations",
        "",
        "The `complexity_hints` section warns about slow operations:",
        "",
        "```yaml",
        "complexity_hints:",
        "  - compute_pagerank: iterative_algorithm",
        "  - extract_corpus_semantics: corpus_wide_computation",
        "```",
        "",
        "### Understanding Code Sections",
        "",
        "Functions are grouped into logical sections:",
        "",
        "```yaml",
        "sections:",
        "  - name: Persistence",
        "    functions: [save, load, to_dict, from_dict]",
        "  - name: Query",
        "    functions: [find_documents, search, expand_query]",
        "```",
        "",
        "## Metadata File Locations",
        "",
        "| Source File | Metadata File |",
        "|-------------|---------------|",
        "| `cortical/processor.py` | `cortical/processor.py.ai_meta` |",
        "| `cortical/query.py` | `cortical/query.py.ai_meta` |",
        "| `cortical/analysis.py` | `cortical/analysis.py.ai_meta` |",
        "| `tests/test_processor.py` | `tests/test_processor.py.ai_meta` |",
        "",
        "## Commands Reference",
        "",
        "```bash",
        "# Generate all metadata (clean build)",
        "python scripts/generate_ai_metadata.py",
        "",
        "# Incremental update (only changed files)",
        "python scripts/generate_ai_metadata.py --incremental",
        "",
        "# Generate for a single file",
        "python scripts/generate_ai_metadata.py cortical/processor.py",
        "",
        "# Remove all metadata files",
        "python scripts/generate_ai_metadata.py --clean",
        "",
        "# Check metadata status",
        "ls -la cortical/*.ai_meta",
        "```",
        "",
        "## Integration with Corpus Indexer",
        "",
        "The corpus indexer can generate AI metadata automatically:",
        "",
        "```bash",
        "# Index codebase AND generate metadata",
        "python scripts/index_codebase.py --incremental && python scripts/generate_ai_metadata.py --incremental",
        "```",
        "",
        "## Tips for AI Agents",
        "",
        "1. **Start with metadata** - Read `.ai_meta` files before diving into source code",
        "2. **Use `see_also`** - Follow cross-references to understand related functionality",
        "3. **Check `complexity_hints`** - Be aware of expensive operations before calling them",
        "4. **Trust the sections** - Functions are grouped by purpose, not just alphabetically",
        "5. **Regenerate after changes** - Run `--incremental` after modifying code",
        "",
        "## Example: Understanding processor.py",
        "",
        "```bash",
        "# Step 1: Read the metadata overview",
        "cat cortical/processor.py.ai_meta | head -50",
        "",
        "# Step 2: Find functions related to search",
        "grep -A5 \"find_documents\" cortical/processor.py.ai_meta",
        "",
        "# Step 3: Check complexity hints",
        "grep -A10 \"complexity_hints\" cortical/processor.py.ai_meta",
        "",
        "# Step 4: Now read specific source code as needed",
        "```",
        "",
        "## Why Use This Instead of Reading Source?",
        "",
        "| Approach | Time | Context Used | Understanding |",
        "|----------|------|--------------|---------------|",
        "| Read entire module | Slow | High | Complete but overwhelming |",
        "| Read `.ai_meta` first | Fast | Low | Structured overview |",
        "| Search with grep | Fast | Low | Fragmented results |",
        "",
        "The metadata approach gives you **structured understanding with minimal context usage**."
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": ".claude/skills/corpus-indexer/SKILL.md",
      "function": null,
      "start_line": 1,
      "lines_added": [
        "This skill manages the codebase index used by the semantic search system and generates AI navigation metadata.",
        "- When AI metadata files (`.ai_meta`) are missing or stale",
        "",
        "# RECOMMENDED: Index + generate AI metadata in one command",
        "python scripts/index_codebase.py --incremental && python scripts/generate_ai_metadata.py --incremental",
        "## AI Metadata Generation",
        "",
        "Generate `.ai_meta` files that provide structured navigation for AI agents:",
        "",
        "```bash",
        "# Generate metadata for all modules",
        "python scripts/generate_ai_metadata.py",
        "",
        "# Incremental update (only changed files)",
        "python scripts/generate_ai_metadata.py --incremental",
        "",
        "# Generate for a single file",
        "python scripts/generate_ai_metadata.py cortical/processor.py",
        "",
        "# Clean and regenerate all",
        "python scripts/generate_ai_metadata.py --clean && python scripts/generate_ai_metadata.py",
        "```",
        "",
        "**What metadata provides:**",
        "- Module overview and docstring",
        "- Function signatures with `see_also` cross-references",
        "- Class structures with inheritance",
        "- Logical section groupings",
        "- Complexity hints for expensive operations",
        "",
        "**For detailed usage, see the `ai-metadata` skill.**",
        ""
      ],
      "lines_removed": [
        "This skill manages the codebase index used by the semantic search system."
      ],
      "context_before": [
        "---",
        "name: corpus-indexer",
        "description: Index or re-index the codebase for semantic search. Use after making significant code changes to keep the search corpus up-to-date.",
        "allowed-tools: Bash",
        "---",
        "# Corpus Indexer Skill",
        ""
      ],
      "context_after": [
        "",
        "## When to Use",
        "",
        "- After adding new files to the codebase",
        "- After significant code changes",
        "- When search results seem outdated",
        "- To verify indexing statistics",
        "",
        "## Quick Commands",
        "",
        "```bash",
        "# Fast incremental update (only changed files, ~2s)",
        "python scripts/index_codebase.py --incremental",
        "",
        "# Check what would be indexed without doing it",
        "python scripts/index_codebase.py --status",
        "",
        "# Full rebuild (still fast, ~2s)",
        "python scripts/index_codebase.py",
        "",
        "# Force full rebuild even if no changes detected",
        "python scripts/index_codebase.py --force",
        "```",
        "",
        "## Options",
        "",
        "| Option | Description |",
        "|--------|-------------|",
        "| `--incremental`, `-i` | Only re-index changed files (fastest) |",
        "| `--status`, `-s` | Show what would change without indexing |",
        "| `--force`, `-f` | Force full rebuild even if up-to-date |",
        "| `--verbose`, `-v` | Show per-file progress |",
        "| `--log FILE`, `-l FILE` | Write detailed log to file |",
        "| `--timeout N`, `-t N` | Timeout in seconds (default: 300) |"
      ],
      "change_type": "modify"
    },
    {
      "file": ".gitignore",
      "function": "__pycache__/",
      "start_line": 9,
      "lines_added": [
        "# AI metadata files (generated, not tracked)",
        "*.ai_meta",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "dist/",
        "build/",
        ".pytest_cache/",
        "",
        "# Generated corpus files",
        "corpus_dev.pkl",
        "*.pkl",
        "*.manifest.json",
        "*.pkl.hash",
        ""
      ],
      "context_after": [
        "# Indexer logs",
        "index.log",
        "*.log",
        "",
        "# Coverage",
        ".coverage",
        ".coverage.*",
        "coverage.xml",
        "htmlcov/",
        ""
      ],
      "change_type": "add"
    },
    {
      "file": "CLAUDE.md",
      "function": "Layer 3 (DOCUMENTS) ‚Üí Full documents          [IT analogy: objects]",
      "start_line": 26,
      "lines_added": [
        "## AI Agent Onboarding",
        "",
        "**New to this codebase?** Follow these steps to get oriented quickly:",
        "",
        "### Step 1: Generate AI Metadata (if missing)",
        "",
        "```bash",
        "# Check if metadata exists",
        "ls cortical/*.ai_meta",
        "",
        "# If not present, generate it (~1s)",
        "python scripts/generate_ai_metadata.py",
        "```",
        "",
        "### Step 2: Read Module Metadata First",
        "",
        "Instead of reading entire source files, start with `.ai_meta` files:",
        "",
        "```bash",
        "# Get structured overview of any module",
        "cat cortical/processor.py.ai_meta",
        "```",
        "",
        "**What metadata provides:**",
        "- Module docstring and purpose",
        "- Function signatures with `see_also` cross-references",
        "- Class structures with inheritance",
        "- Logical section groupings (Persistence, Query, Analysis, etc.)",
        "- Complexity hints for expensive operations",
        "",
        "### Step 3: Use the Full Toolchain",
        "",
        "```bash",
        "# Index codebase + generate metadata (recommended startup command)",
        "python scripts/index_codebase.py --incremental && python scripts/generate_ai_metadata.py --incremental",
        "",
        "# Then search semantically",
        "python scripts/search_codebase.py \"your query here\"",
        "```",
        "",
        "### AI Navigation Tips",
        "",
        "1. **Read `.ai_meta` before source code** - Get the map before exploring the territory",
        "2. **Follow `see_also` references** - Functions are cross-linked to related functions",
        "3. **Check `complexity_hints`** - Know which operations are expensive before calling them",
        "4. **Use semantic search** - The codebase is indexed for meaning-based retrieval",
        "5. **Trust the sections** - Functions are grouped by purpose in the metadata",
        "",
        "### Example Workflow",
        "",
        "```bash",
        "# I need to understand how search works",
        "cat cortical/query.py.ai_meta | head -100    # Get overview",
        "python scripts/search_codebase.py \"expand query\"  # Find specific code",
        "# Then read specific line ranges as needed",
        "```",
        "",
        "---",
        "",
        "‚îú‚îÄ‚îÄ processor.py      # Main orchestrator (2,301 lines) - START HERE",
        "‚îú‚îÄ‚îÄ query.py          # Search, retrieval, query expansion (2,719 lines)",
        "‚îú‚îÄ‚îÄ analysis.py       # Graph algorithms: PageRank, TF-IDF, clustering (1,123 lines)",
        "‚îú‚îÄ‚îÄ semantics.py      # Relation extraction, inheritance, retrofitting (915 lines)",
        "‚îú‚îÄ‚îÄ persistence.py    # Save/load with full state preservation (606 lines)",
        "‚îú‚îÄ‚îÄ chunk_index.py    # Git-friendly chunk-based storage (574 lines)",
        "‚îú‚îÄ‚îÄ tokenizer.py      # Tokenization, stemming, stop word removal (398 lines)",
        "‚îú‚îÄ‚îÄ minicolumn.py     # Core data structure with typed Edge connections (357 lines)",
        "‚îú‚îÄ‚îÄ config.py         # CorticalConfig dataclass with validation (352 lines)",
        "‚îú‚îÄ‚îÄ fingerprint.py    # Semantic fingerprinting and similarity (315 lines)",
        "‚îú‚îÄ‚îÄ layers.py         # HierarchicalLayer with O(1) ID lookups via _id_index (294 lines)",
        "‚îú‚îÄ‚îÄ code_concepts.py  # Programming concept synonyms for code search (249 lines)",
        "‚îú‚îÄ‚îÄ gaps.py           # Knowledge gap detection and anomaly analysis (245 lines)",
        "‚îî‚îÄ‚îÄ embeddings.py     # Graph embeddings (adjacency, spectral, random walk) (209 lines)",
        "**Total:** ~10,700 lines of core library code",
        "",
        "### Module Purpose Quick Reference",
        "",
        "| If you need to... | Look in... |",
        "|-------------------|------------|",
        "| Add/modify public API | `processor.py` - wrapper methods call other modules |",
        "| Implement search/retrieval | `query.py` - all search functions |",
        "| Add graph algorithms | `analysis.py` - PageRank, TF-IDF, clustering |",
        "| Add semantic relations | `semantics.py` - pattern extraction, retrofitting |",
        "| Modify data structures | `minicolumn.py` - Minicolumn, Edge classes |",
        "| Change layer behavior | `layers.py` - HierarchicalLayer class |",
        "| Adjust tokenization | `tokenizer.py` - stemming, stop words, ngrams |",
        "| Change configuration | `config.py` - CorticalConfig dataclass |",
        "| Modify persistence | `persistence.py` - save/load, export formats |",
        "| Add code search features | `code_concepts.py` - programming synonyms |",
        "| Modify embeddings | `embeddings.py` - graph embedding methods |",
        "| Change gap detection | `gaps.py` - knowledge gap analysis |",
        "| Add fingerprinting | `fingerprint.py` - semantic fingerprints |",
        "| Modify chunk storage | `chunk_index.py` - git-friendly indexing |",
        "",
        "### Test File Locations",
        "",
        "| When testing... | Add tests to... |",
        "|-----------------|-----------------|",
        "| Processor methods | `tests/test_processor.py` (most comprehensive) |",
        "| Query functions | `tests/test_query.py` |",
        "| Analysis algorithms | `tests/test_analysis.py` |",
        "| Semantic extraction | `tests/test_semantics.py` |",
        "| Persistence/save/load | `tests/test_persistence.py` |",
        "| Tokenization | `tests/test_tokenizer.py` |",
        "| Configuration | `tests/test_config.py` |",
        "| Layers | `tests/test_layers.py` |",
        "| Embeddings | `tests/test_embeddings.py` |",
        "| Gap detection | `tests/test_gaps.py` |",
        "| Fingerprinting | `tests/test_fingerprint.py` |",
        "| Code concepts | `tests/test_code_concepts.py` |",
        "| Chunk indexing | `tests/test_chunk_indexing.py` |",
        "| Incremental updates | `tests/test_incremental_indexing.py` |",
        "| Intent queries | `tests/test_intent_query.py` |",
        "",
        "Historical note: Bigram separator mismatch bugs have been **fixed**. Bigrams now correctly use space separators throughout the codebase (see `tokenizer.py:extract_ngrams` and `analysis.py:compute_bigram_connections`).",
        "1. **Bigrams use SPACE separators** (from `tokenizer.py:319-332`):",
        "### Common Mistakes to Avoid",
        "",
        "**‚ùå DON'T iterate to find by ID:**",
        "```python",
        "# WRONG - O(n) linear scan",
        "for col in layer.minicolumns.values():",
        "    if col.id == target_id:",
        "        return col",
        "",
        "# CORRECT - O(1) lookup",
        "col = layer.get_by_id(target_id)",
        "```",
        "",
        "**‚ùå DON'T use underscores in bigrams:**",
        "```python",
        "# WRONG - bigrams use spaces",
        "bigram = f\"{term1}_{term2}\"",
        "",
        "# CORRECT",
        "bigram = f\"{term1} {term2}\"",
        "```",
        "",
        "**‚ùå DON'T confuse global TF-IDF with per-document TF-IDF:**",
        "```python",
        "# WRONG - global TF-IDF (uses total corpus occurrence)",
        "score = col.tfidf",
        "",
        "# CORRECT - per-document TF-IDF",
        "score = col.tfidf_per_doc.get(doc_id, 0.0)",
        "```",
        "",
        "**‚ùå DON'T assume compute_all() is always needed:**",
        "```python",
        "# WRONG - overkill for incremental updates",
        "processor.add_document_incremental(doc_id, text)",
        "processor.compute_all()  # Recomputes EVERYTHING",
        "",
        "# CORRECT - let incremental handle it",
        "processor.add_document_incremental(doc_id, text)",
        "# TF-IDF and connections updated automatically",
        "```",
        "",
        "**‚ùå DON'T forget to check staleness before relying on computed values:**",
        "```python",
        "# WRONG - may be using stale data",
        "if processor.is_stale(processor.COMP_PAGERANK):",
        "    # PageRank values may be outdated!",
        "    pass",
        "",
        "# CORRECT - ensure freshness",
        "if processor.is_stale(processor.COMP_PAGERANK):",
        "    processor.compute_importance()",
        "```",
        ""
      ],
      "lines_removed": [
        "‚îú‚îÄ‚îÄ processor.py      # Main orchestrator (1,596 lines) - START HERE",
        "‚îú‚îÄ‚îÄ analysis.py       # Graph algorithms: PageRank, TF-IDF, clustering",
        "‚îú‚îÄ‚îÄ query.py          # Search, retrieval, query expansion, analogies",
        "‚îú‚îÄ‚îÄ semantics.py      # Relation extraction, inheritance, retrofitting",
        "‚îú‚îÄ‚îÄ minicolumn.py     # Core data structure with typed Edge connections",
        "‚îú‚îÄ‚îÄ layers.py         # HierarchicalLayer with O(1) ID lookups via _id_index",
        "‚îú‚îÄ‚îÄ embeddings.py     # Graph embeddings (adjacency, spectral, random walk)",
        "‚îú‚îÄ‚îÄ gaps.py           # Knowledge gap detection and anomaly analysis",
        "‚îú‚îÄ‚îÄ persistence.py    # Save/load with full state preservation",
        "‚îî‚îÄ‚îÄ tokenizer.py      # Tokenization, stemming, stop word removal",
        "The bigram separator mismatch bugs in `query.py:1442-1468` and `analysis.py:927` have been **fixed**. Bigrams now correctly use space separators throughout the codebase.",
        "1. **Bigrams use SPACE separators** (from `tokenizer.py:179`):"
      ],
      "context_before": [
        "",
        "**Core algorithms:**",
        "- **PageRank** for term importance (`analysis.py`)",
        "- **TF-IDF** for document relevance (`analysis.py`)",
        "- **Label propagation** for concept clustering (`analysis.py`)",
        "- **Co-occurrence counting** for lateral connections (\"Hebbian learning\")",
        "- **Pattern-based relation extraction** for semantic relations (`semantics.py`)",
        "",
        "---",
        ""
      ],
      "context_after": [
        "## Architecture Map",
        "",
        "```",
        "cortical/",
        "‚îÇ                     # CorticalTextProcessor is the public API",
        "```",
        "",
        "**Key data structures:**",
        "- `Minicolumn`: Core unit with `lateral_connections`, `typed_connections`, `feedforward_connections`, `feedback_connections`",
        "- `Edge`: Typed connection with `relation_type`, `weight`, `confidence`, `source`",
        "- `HierarchicalLayer`: Container with `minicolumns` dict and `_id_index` for O(1) lookups",
        "",
        "---",
        "",
        "## Critical Knowledge",
        "",
        "### Fixed Bugs (2025-12-10)",
        "",
        "### Important Implementation Details",
        "",
        "   ```python",
        "   ' '.join(tokens[i:i+n])  # \"neural networks\", not \"neural_networks\"",
        "   ```",
        "",
        "2. **Global `col.tfidf` is NOT per-document TF-IDF** - it uses total corpus occurrence count. Use `col.tfidf_per_doc[doc_id]` for true per-document TF-IDF.",
        "",
        "3. **O(1) ID lookups**: Always use `layer.get_by_id(col_id)` instead of iterating `layer.minicolumns`. The `_id_index` provides O(1) access.",
        "",
        "4. **Layer enum values**:",
        "   ```python",
        "   CorticalLayer.TOKENS = 0",
        "   CorticalLayer.BIGRAMS = 1",
        "   CorticalLayer.CONCEPTS = 2",
        "   CorticalLayer.DOCUMENTS = 3",
        "   ```",
        "",
        "5. **Minicolumn IDs follow pattern**: `L{layer}_{content}` (e.g., `L0_neural`, `L1_neural networks`)",
        "",
        "---",
        "",
        "## Development Workflow",
        "",
        "### Before Writing Code",
        "",
        "1. **Read the relevant module** - understand existing patterns",
        "2. **Check TASK_LIST.md** - see if work is already planned/done",
        "3. **Run tests first** to establish baseline:",
        "   ```bash"
      ],
      "change_type": "modify"
    },
    {
      "file": "CLAUDE.md",
      "function": "python scripts/index_codebase.py",
      "start_line": 364,
      "lines_added": [
        "Three skills are available in `.claude/skills/`:",
        "3. **ai-metadata**: View pre-generated module metadata for rapid understanding"
      ],
      "lines_removed": [
        "Two skills are available in `.claude/skills/`:"
      ],
      "context_before": [
        "python scripts/index_codebase.py --incremental",
        "",
        "# Search for code",
        "python scripts/search_codebase.py \"PageRank algorithm\"",
        "python scripts/search_codebase.py \"bigram separator\" --verbose",
        "python scripts/search_codebase.py --interactive",
        "```",
        "",
        "### Claude Skills",
        ""
      ],
      "context_after": [
        "",
        "1. **codebase-search**: Search the indexed codebase for code patterns and implementations",
        "2. **corpus-indexer**: Re-index the codebase after making changes",
        "",
        "### Indexer Options",
        "",
        "| Option | Description |",
        "|--------|-------------|",
        "| `--incremental`, `-i` | Only re-index changed files (fastest) |",
        "| `--status`, `-s` | Show what would change without indexing |",
        "| `--force`, `-f` | Force full rebuild |",
        "| `--log FILE` | Write detailed log to file |",
        "| `--verbose`, `-v` | Show per-file progress |"
      ],
      "change_type": "modify"
    },
    {
      "file": "README.md",
      "function": "cortical/",
      "start_line": 246,
      "lines_added": [
        "## AI Agent Support",
        "",
        "This project includes tools designed specifically for AI coding assistants:",
        "",
        "### AI Metadata Files (`.ai_meta`)",
        "",
        "Pre-generated metadata files provide structured navigation for AI agents:",
        "",
        "```bash",
        "# Generate metadata for rapid module understanding",
        "python scripts/generate_ai_metadata.py",
        "",
        "# View a module's structure without reading source",
        "cat cortical/processor.py.ai_meta",
        "```",
        "",
        "**What metadata provides:**",
        "- Function signatures with `see_also` cross-references",
        "- Class structures with inheritance",
        "- Complexity hints for expensive operations",
        "- Logical section groupings",
        "",
        "### Claude Skills",
        "",
        "Three Claude Code skills are available in `.claude/skills/`:",
        "",
        "| Skill | Purpose |",
        "|-------|---------|",
        "| `codebase-search` | Semantic search over the codebase |",
        "| `corpus-indexer` | Index/re-index after code changes |",
        "| `ai-metadata` | View and use module metadata |",
        "",
        "### For AI Agents",
        "",
        "See the **AI Agent Onboarding** section in [CLAUDE.md](CLAUDE.md) for:",
        "- Step-by-step setup guide",
        "- Navigation tips for efficient exploration",
        "- Example workflow using metadata",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "‚îî‚îÄ‚îÄ persistence.py   # Save/load with full state",
        "",
        "evaluation/",
        "‚îî‚îÄ‚îÄ evaluator.py     # Evaluation framework",
        "",
        "tests/               # 337 comprehensive tests",
        "showcase.py          # Interactive demonstration (run it!)",
        "samples/             # 92 documents: from quantum computing to cheese affinage",
        "```",
        ""
      ],
      "context_after": [
        "## Development History",
        "",
        "This project evolved through systematic improvements:",
        "",
        "1. **Initial Release**: Core hierarchical text processing",
        "2. **Code Review & Fixes**: TF-IDF calculation, O(1) lookups, type annotations",
        "3. **RAG Enhancements**: Chunk-level retrieval, metadata support, concept clustering",
        "4. **ConceptNet Integration**: Typed edges, relation-weighted PageRank, multi-hop inference",
        "5. **Connection Strategies**: Multiple strategies for Layer 2 concept connections",
        "6. **Showcase & Polish**: Interactive demo with real corpus analysis"
      ],
      "change_type": "add"
    },
    {
      "file": "TASK_LIST.md",
      "function": null,
      "start_line": 1,
      "lines_added": [
        "**Pending Tasks:** 26",
        "**Completed Tasks:** 85+ (see archive)",
        "| # | Task | Category | Depends | Effort |",
        "|---|------|----------|---------|--------|",
        "| 122 | Investigate Concept Layer & Embeddings regressions | BugFix | - | Medium |"
      ],
      "lines_removed": [
        "**Pending Tasks:** 17",
        "**Completed Tasks:** 82+ (see archive)",
        "*No critical tasks - all blockers resolved!*"
      ],
      "context_before": [
        "# Task List: Cortical Text Processor",
        "",
        "Active backlog for the Cortical Text Processor project. Completed tasks are archived in [TASK_ARCHIVE.md](TASK_ARCHIVE.md).",
        "",
        "**Last Updated:** 2025-12-11"
      ],
      "context_after": [
        "",
        "---",
        "",
        "## Active Backlog",
        "",
        "<!-- Machine-parseable format for automation -->",
        "",
        "### üî¥ Critical (Do Now)",
        "",
        "",
        "### üü† High (Do This Week)",
        "",
        "| # | Task | Category | Depends | Effort |",
        "|---|------|----------|---------|--------|",
        "| 94 | Split query.py into focused modules | Arch | - | Large |",
        "| 97 | Integrate CorticalConfig into processor | Arch | - | Medium |",
        "",
        "### üü° Medium (Do This Month)",
        ""
      ],
      "change_type": "modify"
    },
    {
      "file": "TASK_LIST.md",
      "function": "Active backlog for the Cortical Text Processor project. Completed tasks are arch",
      "start_line": 29,
      "lines_added": [
        "| 113 | Document staleness tracking system | AINav | - | Small |",
        "| 114 | Add type aliases for complex types | AINav | - | Small |",
        "| 115 | Create component interaction diagram | AINav | - | Medium |",
        "| 116 | Document return value semantics | AINav | - | Medium |",
        "| 117 | Create debugging cookbook | AINav | - | Medium |",
        "| 118 | Add function complexity annotations | AINav | - | Small |",
        "| 110 | Add section markers to large files | Superseded by #119 (AI metadata generator) |",
        "| 111 | Add \"See Also\" cross-references | Superseded by #119 (AI metadata generator) |",
        "| 112 | Add docstring examples | Superseded by #119 (AI metadata generator) |",
        "| 119 | Create AI metadata generator script | 2025-12-11 | scripts/generate_ai_metadata.py with tests |",
        "| 120 | Add AI metadata loader to Claude skills | 2025-12-11 | ai-metadata skill created |",
        "| 121 | Auto-regenerate AI metadata on changes | 2025-12-11 | Documented in CLAUDE.md, skills |"
      ],
      "lines_removed": [],
      "context_before": [
        "|---|------|----------|---------|--------|",
        "| 91 | Create docs/README.md index | Docs | - | Small |",
        "| 92 | Add badges to README.md | DevEx | - | Small |",
        "| 93 | Update README with docs references | Docs | 91 | Small |",
        "| 95 | Split processor.py into modules | Arch | 97 | Large |",
        "| 96 | Centralize duplicate constants | CodeQual | - | Small |",
        "| 98 | Replace print() with logging | CodeQual | - | Medium |",
        "| 99 | Add input validation to public methods | CodeQual | - | Medium |",
        "| 102 | Add tests for edge cases | Testing | - | Medium |",
        "| 107 | Add Quick Context to tasks | TaskMgmt | - | Medium |"
      ],
      "context_after": [
        "",
        "### üü¢ Low (Backlog)",
        "",
        "| # | Task | Category | Depends | Effort |",
        "|---|------|----------|---------|--------|",
        "| 73 | Add \"Find Similar Code\" command | DevEx | - | Medium |",
        "| 74 | Add \"Explain This Code\" command | DevEx | - | Medium |",
        "| 75 | Add \"What Changed?\" semantic diff | DevEx | - | Large |",
        "| 76 | Add \"Suggest Related Files\" feature | DevEx | - | Medium |",
        "| 78 | Add code pattern detection | DevEx | - | Large |",
        "| 79 | Add corpus health dashboard | DevEx | - | Medium |",
        "| 80 | Add \"Learning Mode\" for contributors | DevEx | - | Large |",
        "| 100 | Implement plugin/extension registry | Arch | - | Large |",
        "| 101 | Automate staleness tracking | Arch | - | Medium |",
        "| 106 | Add task dependency graph | TaskMgmt | - | Small |",
        "| 108 | Create task selection script | TaskMgmt | - | Medium |",
        "",
        "### ‚è∏Ô∏è Deferred",
        "",
        "| # | Task | Reason |",
        "|---|------|--------|",
        "| 7 | Document magic numbers in gaps.py | Low priority, functional as-is |",
        "| 42 | Add simple query language | Nice-to-have, not blocking |",
        "| 44 | Remove deprecated feedforward_sources | Cleanup, low impact |",
        "| 46 | Standardize return types with dataclasses | Nice-to-have |",
        "",
        "### üîÑ In Progress",
        "",
        "| # | Task | Started | Notes |",
        "|---|------|---------|-------|",
        "| 87 | Add Python code samples and showcase | 2025-12-11 | samples/*.py created |",
        "",
        "---",
        "",
        "## Recently Completed (Last 7 Days)",
        "",
        "| # | Task | Completed | Notes |",
        "|---|------|-----------|-------|",
        "| 88 | Create package installation files | 2025-12-11 | pyproject.toml, requirements.txt |",
        "| 89 | Create CONTRIBUTING.md | 2025-12-11 | Contribution guide |",
        "| 90 | Create docs/quickstart.md | 2025-12-11 | 5-minute tutorial |",
        "| 103 | Add Priority Backlog Summary | 2025-12-11 | TASK_LIST.md restructure |",
        "| 104 | Create TASK_ARCHIVE.md | 2025-12-11 | 75+ tasks archived |",
        "| 105 | Standardize task format | 2025-12-11 | Meta tags, effort estimates |",
        "| 109 | Add Recently Completed section | 2025-12-11 | Session context |",
        "| 86 | Add semantic chunk boundaries for code | 2025-12-11 | In query.py |",
        "| 85 | Improve test vs source ranking | 2025-12-11 | DOC_TYPE_BOOSTS |",
        ""
      ],
      "change_type": "add"
    },
    {
      "file": "TASK_LIST.md",
      "function": "def __init__(",
      "start_line": 521,
      "lines_added": [
        "### 110. Add Section Markers to Large Files",
        "",
        "**Meta:** `status:pending` `priority:high` `category:ai-nav`",
        "**Files:** `cortical/processor.py`, `cortical/query.py`",
        "**Effort:** Small",
        "",
        "**Problem:** Large files (processor.py: 2,301 lines, query.py: 2,719 lines) are hard to navigate. AI assistants must scan large portions to find relevant sections.",
        "",
        "**Solution:** Add clear section markers like:",
        "```python",
        "# =============================================================================",
        "# DOCUMENT MANAGEMENT",
        "# =============================================================================",
        "",
        "# =============================================================================",
        "# COMPUTATION METHODS",
        "# =============================================================================",
        "```",
        "",
        "**Acceptance Criteria:**",
        "- [ ] processor.py has 5-7 logical sections marked",
        "- [ ] query.py has 5-7 logical sections marked",
        "- [ ] Section names match CLAUDE.md terminology",
        "",
        "---",
        "",
        "### 111. Add \"See Also\" Cross-References to Docstrings",
        "",
        "**Meta:** `status:pending` `priority:high` `category:ai-nav`",
        "**Files:** `cortical/processor.py`, `cortical/query.py`, `cortical/analysis.py`",
        "**Effort:** Medium",
        "",
        "**Problem:** When reading a function, AI assistants don't know about related functions without searching.",
        "",
        "**Solution:** Add \"See Also\" sections to docstrings:",
        "```python",
        "def find_documents_for_query(self, query: str, top_n: int = 5):",
        "    \"\"\"",
        "    Find documents matching query.",
        "",
        "    See Also:",
        "        fast_find_documents: Faster search, document-level only",
        "        find_passages_for_query: Chunk-level retrieval for RAG",
        "        expand_query: Get expanded terms before searching",
        "    \"\"\"",
        "```",
        "",
        "**Target Functions:** Top 20 most-used public methods.",
        "",
        "---",
        "",
        "### 112. Add Docstring Examples for Complex Functions",
        "",
        "**Meta:** `status:pending` `priority:high` `category:ai-nav`",
        "**Files:** `cortical/query.py`, `cortical/analysis.py`, `cortical/processor.py`",
        "**Effort:** Medium",
        "",
        "**Problem:** Complex functions lack examples showing expected input/output.",
        "",
        "**Solution:** Add Examples section to docstrings:",
        "```python",
        "def expand_query(self, query: str, max_expansions: int = 10):",
        "    \"\"\"",
        "    Expand query with related terms.",
        "",
        "    Example:",
        "        >>> processor.expand_query(\"neural networks\")",
        "        {'neural': 1.0, 'networks': 1.0, 'network': 0.85,",
        "         'learning': 0.72, 'deep': 0.68}",
        "    \"\"\"",
        "```",
        "",
        "**Target Functions:**",
        "- `expand_query()`, `expand_query_semantic()`, `expand_query_multihop()`",
        "- `find_documents_for_query()`, `find_passages_for_query()`",
        "- `parse_intent_query()`, `search_by_intent()`",
        "- `complete_analogy()`",
        "- `compute_pagerank()`, `compute_tfidf()`",
        "",
        "---",
        "",
        "### 113. Document Staleness Tracking System",
        "",
        "**Meta:** `status:pending` `priority:medium` `category:ai-nav`",
        "**Files:** `CLAUDE.md` or `docs/staleness.md` (new)",
        "**Effort:** Small",
        "",
        "**Problem:** The staleness tracking system (`COMP_TFIDF`, `COMP_PAGERANK`, `is_stale()`, `_mark_all_stale()`) is powerful but not documented. AI assistants discover it through exploration.",
        "",
        "**Solution:** Add documentation explaining:",
        "- What staleness means and why it matters",
        "- List of all `COMP_*` constants and what they track",
        "- When staleness is automatically set (which methods call `_mark_all_stale()`)",
        "- How to check and resolve staleness",
        "- Example workflow showing stale ‚Üí recompute ‚Üí fresh",
        "",
        "---",
        "",
        "### 114. Add Type Aliases for Complex Types",
        "",
        "**Meta:** `status:pending` `priority:medium` `category:ai-nav`",
        "**Files:** `cortical/types.py` (new), update imports in other modules",
        "**Effort:** Small",
        "",
        "**Problem:** Complex return types like `List[Tuple[str, float, Dict[str, Any]]]` are hard to understand at a glance.",
        "",
        "**Solution:** Create type aliases:",
        "```python",
        "# cortical/types.py",
        "from typing import List, Tuple, Dict, Any",
        "",
        "# Query results",
        "DocumentScore = Tuple[str, float]  # (doc_id, score)",
        "DocumentResults = List[DocumentScore]",
        "",
        "PassageResult = Tuple[str, float, str]  # (doc_id, score, passage_text)",
        "PassageResults = List[PassageResult]",
        "",
        "IntentResult = Tuple[str, float, Dict[str, Any]]  # (doc_id, score, intent_info)",
        "IntentResults = List[IntentResult]",
        "",
        "# Graph types",
        "ConnectionMap = Dict[str, float]  # {target_id: weight}",
        "LayerDict = Dict[CorticalLayer, HierarchicalLayer]",
        "```",
        "",
        "---",
        "",
        "### 115. Create Component Interaction Diagram",
        "",
        "**Meta:** `status:pending` `priority:medium` `category:ai-nav`",
        "**Files:** `docs/architecture.md` or `CLAUDE.md`",
        "**Effort:** Medium",
        "",
        "**Problem:** Understanding how modules call each other requires tracing imports and function calls.",
        "",
        "**Solution:** Add ASCII or Mermaid diagram showing:",
        "```",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê",
        "‚îÇ processor.py‚îÇ ‚Üê Public API entry point",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò",
        "       ‚îÇ calls",
        "       ‚ñº",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê",
        "‚îÇ analysis.py ‚îÇ query.py ‚îÇ semantics.py ‚îÇ ...  ‚îÇ",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò",
        "       ‚îÇ operates on",
        "       ‚ñº",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê",
        "‚îÇ      layers.py  ‚Üí  minicolumn.py             ‚îÇ",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò",
        "```",
        "",
        "Include which module calls which, and data flow direction.",
        "",
        "---",
        "",
        "### 116. Document Return Value Semantics",
        "",
        "**Meta:** `status:pending` `priority:medium` `category:ai-nav`",
        "**Files:** `CLAUDE.md`",
        "**Effort:** Medium",
        "",
        "**Problem:** Inconsistent understanding of what functions return in edge cases (empty corpus, no matches, invalid input).",
        "",
        "**Solution:** Add section to CLAUDE.md documenting:",
        "",
        "| Scenario | Return | Example Functions |",
        "|----------|--------|-------------------|",
        "| Empty corpus | Empty list `[]` | `find_documents_for_query()` |",
        "| No matches | Empty list `[]` | `find_passages_for_query()` |",
        "| Invalid doc_id | `None` | `get_document_metadata()` |",
        "| Invalid layer | Raises `KeyError` | `get_layer()` |",
        "",
        "Also document:",
        "- When functions return `None` vs raise exceptions",
        "- Default values for optional parameters",
        "- Score ranges (0.0-1.0 vs unbounded)",
        "",
        "---",
        "",
        "### 117. Create Debugging Cookbook",
        "",
        "**Meta:** `status:pending` `priority:low` `category:ai-nav`",
        "**Files:** `docs/debugging.md` (new)",
        "**Effort:** Medium",
        "",
        "**Problem:** Common debugging scenarios require discovering patterns through trial and error.",
        "",
        "**Solution:** Create cookbook with scenarios:",
        "",
        "1. **\"Why is my query returning no results?\"**",
        "   - Check if corpus has documents",
        "   - Check if terms exist in corpus",
        "   - Use `--expand` to see what's being searched",
        "",
        "2. **\"Why are PageRank values all zero?\"**",
        "   - Check if `compute_all()` was called",
        "   - Check staleness with `is_stale()`",
        "",
        "3. **\"Why is search slow?\"**",
        "   - Use `fast_find_documents()` for document-level",
        "   - Pre-build index with `build_search_index()`",
        "",
        "4. **\"Why are bigrams not connecting?\"**",
        "   - Verify space separator (not underscore)",
        "   - Check `compute_bigram_connections()` was called",
        "",
        "---",
        "",
        "### 118. Add Function Complexity Annotations",
        "",
        "**Meta:** `status:pending` `priority:low` `category:ai-nav`",
        "**Files:** `cortical/processor.py`, `cortical/analysis.py`",
        "**Effort:** Small",
        "",
        "**Problem:** AI assistants don't know which functions are expensive to call.",
        "",
        "**Solution:** Add complexity notes to expensive functions:",
        "```python",
        "def compute_all(self, verbose: bool = True):",
        "    \"\"\"",
        "    Compute all network properties.",
        "",
        "    Complexity: O(n¬≤) where n = total minicolumns across all layers.",
        "    Typical time: 2-5 seconds for 10K documents.",
        "",
        "    Note: For incremental updates, prefer add_document_incremental()",
        "    which is O(m) where m = tokens in new document.",
        "    \"\"\"",
        "```",
        "",
        "**Target Functions:**",
        "- `compute_all()` - O(n¬≤)",
        "- `compute_pagerank()` - O(iterations √ó edges)",
        "- `build_concept_clusters()` - O(n √ó iterations)",
        "- `find_passages_for_query()` - O(docs √ó chunks)",
        "",
        "---",
        "",
        "### 119. Create AI Metadata Generator Script ‚úÖ",
        "",
        "**Meta:** `status:completed` `priority:high` `category:ai-nav`",
        "**Files:** `scripts/generate_ai_metadata.py`, `tests/test_generate_ai_metadata.py`",
        "**Effort:** Medium",
        "**Completed:** 2025-12-11",
        "",
        "**Problem:** AI navigation tasks (110-118) require modifying code files directly, cluttering them for human readers. We need a way to provide rich AI navigation aids without polluting the source code.",
        "",
        "**Solution:** Generate companion `.ai_meta` files (YAML) that provide:",
        "- Section markers with line ranges",
        "- Function cross-references (\"See Also\")",
        "- Complexity annotations",
        "- Return value semantics",
        "- Docstring examples (extracted or generated)",
        "- Import/dependency information",
        "",
        "**Output format:**",
        "```yaml",
        "# processor.py.ai_meta - Auto-generated",
        "file: cortical/processor.py",
        "lines: 2301",
        "generated: 2025-12-11T14:30:00",
        "",
        "sections:",
        "  - name: \"Document Management\"",
        "    lines: [54, 520]",
        "    functions: [process_document, add_document_incremental, remove_document]",
        "  - name: \"Computation Methods\"",
        "    lines: [613, 1140]",
        "    functions: [compute_all, compute_importance, compute_tfidf]",
        "",
        "functions:",
        "  find_documents_for_query:",
        "    line: 1596",
        "    signature: \"(query: str, top_n: int = 5) -> List[Tuple[str, float]]\"",
        "    see_also:",
        "      fast_find_documents: \"~2-3x faster, document-level only\"",
        "      find_passages_for_query: \"Chunk-level retrieval for RAG\"",
        "    complexity: \"O(terms √ó documents)\"",
        "    returns:",
        "      on_empty_corpus: \"[]\"",
        "      on_no_matches: \"[]\"",
        "",
        "dependencies:",
        "  imports: [analysis, query, semantics]",
        "  imported_by: [__init__]",
        "```",
        "",
        "**Acceptance Criteria:**",
        "- [ ] Script generates .ai_meta for all cortical/*.py files",
        "- [ ] *.ai_meta added to .gitignore",
        "- [ ] Output is valid YAML (AI-parseable)",
        "- [ ] Extracts sections by analyzing class/function groupings",
        "- [ ] Extracts function signatures and line numbers",
        "- [ ] Identifies related functions by naming patterns",
        "- [ ] Can be run incrementally (only changed files)",
        "",
        "---",
        "",
        "### 120. Add AI Metadata Loader to Claude Skills ‚úÖ",
        "",
        "**Meta:** `status:completed` `priority:high` `category:ai-nav`",
        "**Files:** `.claude/skills/ai-metadata/SKILL.md`, `.claude/skills/corpus-indexer/SKILL.md`",
        "**Effort:** Small",
        "**Depends:** 119",
        "**Completed:** 2025-12-11",
        "",
        "**Problem:** Generated .ai_meta files need to be used by AI assistants during code navigation.",
        "",
        "**Solution:** Created new `ai-metadata` skill that:",
        "1. Provides structured documentation for using .ai_meta files",
        "2. Explains metadata fields (sections, see_also, complexity hints)",
        "3. Shows best practices for AI agent navigation",
        "4. Updated corpus-indexer skill to include metadata generation commands",
        "",
        "---",
        "",
        "### 121. Auto-regenerate AI Metadata on File Changes ‚úÖ",
        "",
        "**Meta:** `status:completed` `priority:high` `category:ai-nav`",
        "**Files:** `CLAUDE.md`, `.claude/skills/corpus-indexer/SKILL.md`",
        "**Effort:** Medium",
        "**Depends:** 119",
        "**Completed:** 2025-12-11",
        "",
        "**Problem:** .ai_meta files become stale when source files change.",
        "",
        "**Solution implemented:**",
        "1. **Documented in CLAUDE.md** - AI Agent Onboarding section with startup command",
        "2. **Incremental mode** - `python scripts/generate_ai_metadata.py --incremental` only updates changed files",
        "3. **Combined workflow** - `python scripts/index_codebase.py --incremental && python scripts/generate_ai_metadata.py --incremental`",
        "4. **Skills documentation** - corpus-indexer skill explains metadata regeneration",
        "",
        "**Recommended workflow for new agents:**",
        "```bash",
        "# On arrival, check if metadata exists and regenerate if needed",
        "ls cortical/*.ai_meta || python scripts/generate_ai_metadata.py",
        "```",
        "",
        "---",
        "",
        "### 122. Investigate Concept Layer & Embeddings Regressions",
        "",
        "**Meta:** `status:pending` `priority:critical` `category:bugfix`",
        "**Files:** `cortical/analysis.py`, `cortical/embeddings.py`, `showcase.py`",
        "**Effort:** Medium",
        "",
        "**Problem:** Showcase output reveals potential regressions or bugs:",
        "",
        "1. **Concept Layer has only 3 clusters** for 95 documents",
        "   - Expected: 10-20+ concept clusters for diverse corpus",
        "   - Current: Only 3 minicolumns in Layer 2",
        "   - This severely limits semantic grouping capability",
        "",
        "2. **Graph embeddings show nonsensical similarities**",
        "   - \"neural\" shows 0.868 similarity to \"blockchain\", \"consensus\", \"uniformly\"",
        "   - Expected: \"neural\" should be most similar to \"networks\", \"learning\", \"artificial\"",
        "   - Suggests embedding algorithm may be broken or misconfigured",
        "",
        "**Investigation Steps:**",
        "",
        "1. **Git history analysis:**",
        "   ```bash",
        "   # Find when concept clustering changed",
        "   git log --oneline -p -- cortical/analysis.py | grep -A5 -B5 \"build_concept_clusters\\|label_propagation\"",
        "",
        "   # Find when embeddings changed",
        "   git log --oneline -p -- cortical/embeddings.py | grep -A5 -B5 \"compute_graph_embeddings\"",
        "",
        "   # Check for recent changes to showcase",
        "   git log --oneline -20 -- showcase.py",
        "   ```",
        "",
        "2. **Verify expected behavior:**",
        "   - Check if `cluster_strictness` parameter is being applied correctly",
        "   - Verify embedding method (adjacency vs spectral vs random_walk)",
        "   - Compare current output with any baseline metrics",
        "",
        "3. **Reproduce issue:**",
        "   ```python",
        "   processor.compute_all(verbose=True)",
        "   print(f\"Concept clusters: {processor.layers[CorticalLayer.CONCEPTS].column_count()}\")",
        "   ```",
        "",
        "4. **Check parameters:**",
        "   - Default `cluster_strictness` in showcase.py",
        "   - Default embedding `method` and `dimensions`",
        "   - Any recent parameter changes",
        "",
        "**Evidence from showcase.py output:**",
        "```",
        "Layer 2: Concept Layer (V4)",
        "       3 minicolumns, 6 connections",
        "       Purpose: Semantic clusters",
        "",
        "Terms similar to 'neural':",
        "  ‚Ä¢ uniformly (similarity: 0.868)",
        "  ‚Ä¢ overcomes (similarity: 0.868)",
        "  ‚Ä¢ blockchain (similarity: 0.868)",
        "```",
        "",
        "**Acceptance Criteria:**",
        "- [ ] Root cause identified via git history",
        "- [ ] Concept clusters > 10 for 95-doc corpus",
        "- [ ] Embedding similarities semantically meaningful",
        "- [ ] Regression test added to prevent recurrence",
        "",
        "---",
        "",
        "| BugFix | 1 | Bug fixes and regressions |",
        "| AINav | 6 | AI assistant navigation & usability |",
        "| Deferred | 7 | Low priority or superseded |"
      ],
      "lines_removed": [
        "| Deferred | 4 | Low priority, not blocking |"
      ],
      "context_before": [
        "### 109. Add \"Recently Completed\" Section for Context ‚úì",
        "",
        "**Meta:** `status:completed` `priority:low` `category:task-mgmt`",
        "**Files:** `TASK_LIST.md`",
        "**Completed:** 2025-12-11",
        "",
        "**Solution Applied:** Added \"Recently Completed (Last 7 Days)\" section with table format showing task, date, and notes.",
        "",
        "---",
        ""
      ],
      "context_after": [
        "## Category Index",
        "",
        "| Category | Pending | Description |",
        "|----------|---------|-------------|",
        "| DevEx | 6 | Developer experience (scripts, tools) |",
        "| Docs | 2 | Documentation improvements |",
        "| Arch | 4 | Architecture refactoring |",
        "| CodeQual | 3 | Code quality improvements |",
        "| Testing | 1 | Test coverage |",
        "| TaskMgmt | 2 | Task management system |",
        "",
        "---",
        "",
        "## Notes",
        "",
        "- **Effort estimates:** Small (<1 hour), Medium (1-4 hours), Large (1+ days)",
        "- **Dependencies:** Complete dependent tasks first",
        "- **Quick Context:** Key info to start task without searching",
        "- **Archive:** Full history in [TASK_ARCHIVE.md](TASK_ARCHIVE.md)",
        ""
      ],
      "change_type": "modify"
    },
    {
      "file": "docs/quickstart.md",
      "function": "processor.save(\"my_corpus.pkl\")",
      "start_line": 113,
      "lines_added": [
        "### For AI Agents",
        "",
        "If you're an AI coding assistant exploring this codebase:",
        "",
        "1. **Use `.ai_meta` files** for rapid module understanding:",
        "   ```bash",
        "   cat cortical/processor.py.ai_meta  # Structured overview",
        "   ```",
        "",
        "2. **Check Claude skills** in `.claude/skills/` for:",
        "   - `codebase-search` - Semantic search",
        "   - `corpus-indexer` - Index management",
        "   - `ai-metadata` - Metadata viewer",
        "",
        "3. **See AI Agent Onboarding** in [CLAUDE.md](../CLAUDE.md#ai-agent-onboarding) for detailed guidance",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "processor = CorticalTextProcessor.load(\"my_corpus.pkl\")",
        "```",
        "",
        "## Next Steps",
        "",
        "- Run `python showcase.py` to see all features in action",
        "- Read [cookbook.md](cookbook.md) for common recipes",
        "- See [CLAUDE.md](../CLAUDE.md) for the full developer guide",
        "- Check [architecture.md](architecture.md) for how it works",
        ""
      ],
      "context_after": [
        "## Common Patterns",
        "",
        "### Batch Processing",
        "",
        "```python",
        "documents = [",
        "    (\"doc1\", \"First document content...\"),",
        "    (\"doc2\", \"Second document content...\"),",
        "    (\"doc3\", \"Third document content...\"),",
        "]"
      ],
      "change_type": "add"
    },
    {
      "file": "scripts/generate_ai_metadata.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "#!/usr/bin/env python3",
        "\"\"\"",
        "AI Metadata Generator for Cortical Text Processor",
        "",
        "Generates .ai_meta companion files for Python source files, providing",
        "structured navigation aids for AI assistants without cluttering source code.",
        "",
        "Usage:",
        "    python scripts/generate_ai_metadata.py                    # Generate all",
        "    python scripts/generate_ai_metadata.py --incremental      # Only changed files",
        "    python scripts/generate_ai_metadata.py cortical/query.py  # Specific file",
        "    python scripts/generate_ai_metadata.py --clean            # Remove all .ai_meta",
        "",
        "Output:",
        "    For each .py file, creates a .py.ai_meta YAML file containing:",
        "    - File metadata (lines, generated timestamp)",
        "    - Logical sections with line ranges",
        "    - Function/method details (signatures, docstrings, line numbers)",
        "    - Cross-references (\"See Also\" suggestions)",
        "    - Complexity hints for expensive operations",
        "    - Import dependencies",
        "\"\"\"",
        "",
        "import ast",
        "import os",
        "import sys",
        "import hashlib",
        "import argparse",
        "from datetime import datetime",
        "from typing import Dict, List, Optional, Tuple, Any",
        "from pathlib import Path",
        "from collections import defaultdict",
        "",
        "",
        "# Patterns for identifying related functions",
        "RELATED_FUNCTION_PATTERNS = {",
        "    'find_documents': ['fast_find_documents', 'find_passages', 'search_by_intent', 'expand_query'],",
        "    'find_passages': ['find_documents', 'find_passages_batch', 'chunking'],",
        "    'expand_query': ['expand_query_semantic', 'expand_query_multihop', 'expand_query_cached'],",
        "    'compute_pagerank': ['compute_semantic_pagerank', 'compute_hierarchical_pagerank', 'compute_importance'],",
        "    'compute_tfidf': ['compute_pagerank', 'compute_importance'],",
        "    'process_document': ['add_document_incremental', 'add_documents_batch', 'remove_document'],",
        "    'save': ['load', 'export_graph', 'export_conceptnet_json'],",
        "    'load': ['save', 'from_dict'],",
        "    'get_fingerprint': ['compare_fingerprints', 'explain_fingerprint', 'explain_similarity'],",
        "}",
        "",
        "# Known expensive operations with complexity hints",
        "COMPLEXITY_HINTS = {",
        "    'compute_all': 'O(n¬≤) where n = total minicolumns. ~27s for 95 docs.',",
        "    'compute_pagerank': 'O(iterations √ó edges). Usually 20 iterations.',",
        "    'compute_semantic_pagerank': 'O(iterations √ó edges √ó relation_types).',",
        "    'compute_hierarchical_pagerank': 'O(iterations √ó edges √ó layers).',",
        "    'build_concept_clusters': 'O(n √ó iterations) label propagation.',",
        "    'compute_bigram_connections': 'O(bigrams¬≤) for co-occurrence. Can be slow.',",
        "    'find_passages_for_query': 'O(docs √ó chunks_per_doc). Use find_documents for speed.',",
        "    'extract_corpus_semantics': 'O(docs √ó patterns). Pattern matching on all text.',",
        "    'retrofit_connections': 'O(iterations √ó relations). Blends semantic relations.',",
        "}",
        "",
        "# Section detection keywords for grouping functions",
        "SECTION_KEYWORDS = {",
        "    'document': ['process_document', 'add_document', 'remove_document', 'get_document', 'set_document'],",
        "    'computation': ['compute_', '_mark_stale', '_mark_fresh', 'is_stale', 'recompute'],",
        "    'query': ['find_documents', 'find_passages', 'expand_query', 'search_', 'query_'],",
        "    'semantic': ['extract_', 'retrofit_', 'semantic', 'relation', 'inherit'],",
        "    'embedding': ['embedding', 'fingerprint', 'similarity'],",
        "    'persistence': ['save', 'load', 'export_', 'to_dict', 'from_dict'],",
        "    'analysis': ['pagerank', 'tfidf', 'cluster', 'propagate'],",
        "}",
        "",
        "",
        "class FunctionInfo:",
        "    \"\"\"Holds extracted information about a function/method.\"\"\"",
        "",
        "    def __init__(self, node, source_lines: List[str]):",
        "        # node can be ast.FunctionDef or ast.AsyncFunctionDef",
        "        self.name = node.name",
        "        self.line_start = node.lineno",
        "        self.line_end = node.end_lineno or node.lineno",
        "        self.is_private = node.name.startswith('_')",
        "        self.is_dunder = node.name.startswith('__') and node.name.endswith('__')",
        "        self.is_async = isinstance(node, ast.AsyncFunctionDef)",
        "",
        "        # Extract signature",
        "        self.signature = self._extract_signature(node)",
        "",
        "        # Extract docstring",
        "        self.docstring = ast.get_docstring(node) or \"\"",
        "        self.docstring_summary = self._get_docstring_summary()",
        "",
        "        # Extract decorators",
        "        self.decorators = [self._get_decorator_name(d) for d in node.decorator_list]",
        "",
        "        # Determine if it's a method (has 'self' or 'cls' first arg)",
        "        self.is_method = False",
        "        if node.args.args:",
        "            first_arg = node.args.args[0].arg",
        "            self.is_method = first_arg in ('self', 'cls')",
        "",
        "    def _extract_signature(self, node: ast.FunctionDef) -> str:",
        "        \"\"\"Extract function signature as string.\"\"\"",
        "        args = []",
        "",
        "        # Regular args",
        "        defaults_offset = len(node.args.args) - len(node.args.defaults)",
        "        for i, arg in enumerate(node.args.args):",
        "            arg_str = arg.arg",
        "            if arg.annotation:",
        "                arg_str += f\": {ast.unparse(arg.annotation)}\"",
        "",
        "            # Add default value if present",
        "            default_idx = i - defaults_offset",
        "            if default_idx >= 0 and default_idx < len(node.args.defaults):",
        "                default = node.args.defaults[default_idx]",
        "                arg_str += f\" = {ast.unparse(default)}\"",
        "",
        "            args.append(arg_str)",
        "",
        "        # *args",
        "        if node.args.vararg:",
        "            arg_str = f\"*{node.args.vararg.arg}\"",
        "            if node.args.vararg.annotation:",
        "                arg_str += f\": {ast.unparse(node.args.vararg.annotation)}\"",
        "            args.append(arg_str)",
        "",
        "        # **kwargs",
        "        if node.args.kwarg:",
        "            arg_str = f\"**{node.args.kwarg.arg}\"",
        "            if node.args.kwarg.annotation:",
        "                arg_str += f\": {ast.unparse(node.args.kwarg.annotation)}\"",
        "            args.append(arg_str)",
        "",
        "        sig = f\"({', '.join(args)})\"",
        "",
        "        # Return type",
        "        if node.returns:",
        "            sig += f\" -> {ast.unparse(node.returns)}\"",
        "",
        "        return sig",
        "",
        "    def _get_docstring_summary(self) -> str:",
        "        \"\"\"Get first line of docstring as summary.\"\"\"",
        "        if not self.docstring:",
        "            return \"\"",
        "        lines = self.docstring.strip().split('\\n')",
        "        return lines[0].strip() if lines else \"\"",
        "",
        "    def _get_decorator_name(self, decorator) -> str:",
        "        \"\"\"Extract decorator name.\"\"\"",
        "        if isinstance(decorator, ast.Name):",
        "            return decorator.id",
        "        elif isinstance(decorator, ast.Attribute):",
        "            return f\"{ast.unparse(decorator)}\"",
        "        elif isinstance(decorator, ast.Call):",
        "            return self._get_decorator_name(decorator.func)",
        "        return str(decorator)",
        "",
        "",
        "class ClassInfo:",
        "    \"\"\"Holds extracted information about a class.\"\"\"",
        "",
        "    def __init__(self, node: ast.ClassDef, source_lines: List[str]):",
        "        self.name = node.name",
        "        self.line_start = node.lineno",
        "        self.line_end = node.end_lineno or node.lineno",
        "        self.docstring = ast.get_docstring(node) or \"\"",
        "        self.docstring_summary = self._get_docstring_summary()",
        "",
        "        # Extract base classes",
        "        self.bases = [ast.unparse(base) for base in node.bases]",
        "",
        "        # Extract methods (both sync and async)",
        "        self.methods: List[FunctionInfo] = []",
        "        for item in node.body:",
        "            if isinstance(item, (ast.FunctionDef, ast.AsyncFunctionDef)):",
        "                self.methods.append(FunctionInfo(item, source_lines))",
        "",
        "    def _get_docstring_summary(self) -> str:",
        "        \"\"\"Get first line of docstring as summary.\"\"\"",
        "        if not self.docstring:",
        "            return \"\"",
        "        lines = self.docstring.strip().split('\\n')",
        "        return lines[0].strip() if lines else \"\"",
        "",
        "",
        "class ModuleAnalyzer:",
        "    \"\"\"Analyzes a Python module and extracts metadata.\"\"\"",
        "",
        "    def __init__(self, filepath: str):",
        "        self.filepath = filepath",
        "        self.filename = os.path.basename(filepath)",
        "",
        "        with open(filepath, 'r', encoding='utf-8') as f:",
        "            self.source = f.read()",
        "",
        "        self.source_lines = self.source.split('\\n')",
        "        self.line_count = len(self.source_lines)",
        "",
        "        # Parse AST",
        "        self.tree = ast.parse(self.source, filename=filepath)",
        "",
        "        # Extract components",
        "        self.module_docstring = ast.get_docstring(self.tree) or \"\"",
        "        self.imports: List[str] = []",
        "        self.classes: List[ClassInfo] = []",
        "        self.functions: List[FunctionInfo] = []  # Module-level functions",
        "",
        "        self._extract_components()",
        "",
        "    def _extract_components(self):",
        "        \"\"\"Extract all components from AST.\"\"\"",
        "        for node in ast.iter_child_nodes(self.tree):",
        "            if isinstance(node, ast.Import):",
        "                for alias in node.names:",
        "                    self.imports.append(alias.name)",
        "            elif isinstance(node, ast.ImportFrom):",
        "                module = node.module or \"\"",
        "                for alias in node.names:",
        "                    self.imports.append(f\"{module}.{alias.name}\")",
        "            elif isinstance(node, ast.ClassDef):",
        "                self.classes.append(ClassInfo(node, self.source_lines))",
        "            elif isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):",
        "                self.functions.append(FunctionInfo(node, self.source_lines))",
        "",
        "    def get_all_functions(self) -> List[Tuple[str, FunctionInfo]]:",
        "        \"\"\"Get all functions including class methods.\"\"\"",
        "        result = []",
        "",
        "        # Module-level functions",
        "        for func in self.functions:",
        "            result.append((None, func))",
        "",
        "        # Class methods",
        "        for cls in self.classes:",
        "            for method in cls.methods:",
        "                result.append((cls.name, method))",
        "",
        "        return result",
        "",
        "    def detect_sections(self) -> List[Dict[str, Any]]:",
        "        \"\"\"Detect logical sections in the file.\"\"\"",
        "        sections = []",
        "        all_funcs = self.get_all_functions()",
        "",
        "        if not all_funcs:",
        "            return sections",
        "",
        "        # Group functions by detected section type",
        "        section_groups = defaultdict(list)",
        "        uncategorized = []",
        "",
        "        for cls_name, func in all_funcs:",
        "            if func.is_dunder or func.is_private:",
        "                continue",
        "",
        "            categorized = False",
        "            for section_name, keywords in SECTION_KEYWORDS.items():",
        "                for keyword in keywords:",
        "                    if keyword in func.name.lower():",
        "                        section_groups[section_name].append((cls_name, func))",
        "                        categorized = True",
        "                        break",
        "                if categorized:",
        "                    break",
        "",
        "            if not categorized:",
        "                uncategorized.append((cls_name, func))",
        "",
        "        # Build section info",
        "        for section_name, funcs in section_groups.items():",
        "            if not funcs:",
        "                continue",
        "",
        "            lines = [f.line_start for _, f in funcs]",
        "            sections.append({",
        "                'name': section_name.title().replace('_', ' '),",
        "                'lines': [min(lines), max(lines)],",
        "                'functions': [f.name for _, f in funcs],",
        "            })",
        "",
        "        if uncategorized:",
        "            lines = [f.line_start for _, f in uncategorized]",
        "            sections.append({",
        "                'name': 'Other',",
        "                'lines': [min(lines), max(lines)],",
        "                'functions': [f.name for _, f in uncategorized],",
        "            })",
        "",
        "        # Sort by line number",
        "        sections.sort(key=lambda s: s['lines'][0])",
        "",
        "        return sections",
        "",
        "    def find_related_functions(self, func_name: str) -> List[str]:",
        "        \"\"\"Find related functions based on naming patterns.\"\"\"",
        "        related = []",
        "",
        "        # Check predefined patterns",
        "        for pattern, suggestions in RELATED_FUNCTION_PATTERNS.items():",
        "            if pattern in func_name:",
        "                related.extend(suggestions)",
        "",
        "        # Find functions with similar prefixes/suffixes",
        "        all_func_names = [f.name for _, f in self.get_all_functions() if not f.is_private]",
        "",
        "        # Same prefix (e.g., find_documents, find_passages)",
        "        parts = func_name.split('_')",
        "        if len(parts) >= 2:",
        "            prefix = parts[0]",
        "            for name in all_func_names:",
        "                if name != func_name and name.startswith(prefix + '_'):",
        "                    if name not in related:",
        "                        related.append(name)",
        "",
        "        # Limit to top 5",
        "        return related[:5]",
        "",
        "    def get_complexity_hint(self, func_name: str) -> Optional[str]:",
        "        \"\"\"Get complexity hint if known.\"\"\"",
        "        return COMPLEXITY_HINTS.get(func_name)",
        "",
        "    def generate_metadata(self) -> Dict[str, Any]:",
        "        \"\"\"Generate complete metadata dictionary.\"\"\"",
        "        metadata = {",
        "            'file': self.filepath,",
        "            'filename': self.filename,",
        "            'lines': self.line_count,",
        "            'generated': datetime.now().isoformat(),",
        "            'module_doc': self.module_docstring[:200] + '...' if len(self.module_docstring) > 200 else self.module_docstring,",
        "        }",
        "",
        "        # Sections",
        "        metadata['sections'] = self.detect_sections()",
        "",
        "        # Classes",
        "        if self.classes:",
        "            metadata['classes'] = {}",
        "            for cls in self.classes:",
        "                metadata['classes'][cls.name] = {",
        "                    'line': cls.line_start,",
        "                    'lines': [cls.line_start, cls.line_end],",
        "                    'bases': cls.bases,",
        "                    'doc': cls.docstring_summary,",
        "                    'methods': [m.name for m in cls.methods if not m.is_private],",
        "                }",
        "",
        "        # Functions (detailed)",
        "        metadata['functions'] = {}",
        "        for cls_name, func in self.get_all_functions():",
        "            if func.is_dunder:",
        "                continue",
        "",
        "            full_name = f\"{cls_name}.{func.name}\" if cls_name else func.name",
        "",
        "            func_meta = {",
        "                'line': func.line_start,",
        "                'signature': func.signature,",
        "                'doc': func.docstring_summary,",
        "                'private': func.is_private,",
        "            }",
        "",
        "            # Add async flag if async",
        "            if func.is_async:",
        "                func_meta['async'] = True",
        "",
        "            # Add related functions",
        "            related = self.find_related_functions(func.name)",
        "            if related:",
        "                func_meta['see_also'] = related",
        "",
        "            # Add complexity hint",
        "            complexity = self.get_complexity_hint(func.name)",
        "            if complexity:",
        "                func_meta['complexity'] = complexity",
        "",
        "            # Add decorators if present",
        "            if func.decorators:",
        "                func_meta['decorators'] = func.decorators",
        "",
        "            metadata['functions'][full_name] = func_meta",
        "",
        "        # Dependencies",
        "        metadata['imports'] = self._categorize_imports()",
        "",
        "        return metadata",
        "",
        "    def _categorize_imports(self) -> Dict[str, List[str]]:",
        "        \"\"\"Categorize imports by type.\"\"\"",
        "        stdlib = []",
        "        local = []",
        "",
        "        for imp in self.imports:",
        "            if imp.startswith('.') or imp.startswith('cortical'):",
        "                local.append(imp)",
        "            else:",
        "                stdlib.append(imp)",
        "",
        "        return {",
        "            'stdlib': sorted(set(stdlib)),",
        "            'local': sorted(set(local)),",
        "        }",
        "",
        "",
        "def dict_to_yaml(data: Any, indent: int = 0) -> str:",
        "    \"\"\"Convert dictionary to YAML string (simple implementation, no external deps).\"\"\"",
        "    lines = []",
        "    prefix = '  ' * indent",
        "",
        "    if isinstance(data, dict):",
        "        for key, value in data.items():",
        "            if isinstance(value, (dict, list)) and value:",
        "                lines.append(f\"{prefix}{key}:\")",
        "                lines.append(dict_to_yaml(value, indent + 1))",
        "            elif isinstance(value, list) and not value:",
        "                lines.append(f\"{prefix}{key}: []\")",
        "            elif isinstance(value, dict) and not value:",
        "                lines.append(f\"{prefix}{key}: {{}}\")",
        "            elif isinstance(value, str):",
        "                # Handle multiline strings",
        "                if '\\n' in value:",
        "                    lines.append(f\"{prefix}{key}: |\")",
        "                    for line in value.split('\\n'):",
        "                        lines.append(f\"{prefix}  {line}\")",
        "                elif ':' in value or '#' in value or value.startswith('{') or value.startswith('['):",
        "                    lines.append(f'{prefix}{key}: \"{value}\"')",
        "                else:",
        "                    lines.append(f\"{prefix}{key}: {value}\")",
        "            elif isinstance(value, bool):",
        "                lines.append(f\"{prefix}{key}: {str(value).lower()}\")",
        "            elif value is None:",
        "                lines.append(f\"{prefix}{key}: null\")",
        "            else:",
        "                lines.append(f\"{prefix}{key}: {value}\")",
        "    elif isinstance(data, list):",
        "        for item in data:",
        "            if isinstance(item, (dict, list)):",
        "                lines.append(f\"{prefix}-\")",
        "                lines.append(dict_to_yaml(item, indent + 1))",
        "            elif isinstance(item, str):",
        "                if ':' in item or '#' in item:",
        "                    lines.append(f'{prefix}- \"{item}\"')",
        "                else:",
        "                    lines.append(f\"{prefix}- {item}\")",
        "            else:",
        "                lines.append(f\"{prefix}- {item}\")",
        "",
        "    return '\\n'.join(lines)",
        "",
        "",
        "def generate_metadata_for_file(filepath: str) -> str:",
        "    \"\"\"Generate metadata for a single file and return YAML string.\"\"\"",
        "    analyzer = ModuleAnalyzer(filepath)",
        "    metadata = analyzer.generate_metadata()",
        "",
        "    header = f\"# {os.path.basename(filepath)}.ai_meta\\n\"",
        "    header += \"# Auto-generated AI navigation metadata - do not edit manually\\n\"",
        "    header += f\"# Regenerate with: python scripts/generate_ai_metadata.py {filepath}\\n\\n\"",
        "",
        "    return header + dict_to_yaml(metadata)",
        "",
        "",
        "def get_file_hash(filepath: str) -> str:",
        "    \"\"\"Get MD5 hash of file contents.\"\"\"",
        "    with open(filepath, 'rb') as f:",
        "        return hashlib.md5(f.read()).hexdigest()",
        "",
        "",
        "def should_regenerate(py_file: str, meta_file: str) -> bool:",
        "    \"\"\"Check if metadata needs regeneration.\"\"\"",
        "    if not os.path.exists(meta_file):",
        "        return True",
        "",
        "    py_mtime = os.path.getmtime(py_file)",
        "    meta_mtime = os.path.getmtime(meta_file)",
        "",
        "    return py_mtime > meta_mtime",
        "",
        "",
        "def main():",
        "    parser = argparse.ArgumentParser(",
        "        description='Generate AI metadata files for Python source files',",
        "        formatter_class=argparse.RawDescriptionHelpFormatter,",
        "        epilog=__doc__",
        "    )",
        "    parser.add_argument(",
        "        'files', nargs='*',",
        "        help='Specific files to process (default: all cortical/*.py)'",
        "    )",
        "    parser.add_argument(",
        "        '--incremental', '-i', action='store_true',",
        "        help='Only regenerate for changed files'",
        "    )",
        "    parser.add_argument(",
        "        '--clean', '-c', action='store_true',",
        "        help='Remove all .ai_meta files'",
        "    )",
        "    parser.add_argument(",
        "        '--output-dir', '-o', type=str, default=None,",
        "        help='Output directory for .ai_meta files (default: same as source)'",
        "    )",
        "    parser.add_argument(",
        "        '--verbose', '-v', action='store_true',",
        "        help='Verbose output'",
        "    )",
        "",
        "    args = parser.parse_args()",
        "",
        "    # Find project root",
        "    script_dir = os.path.dirname(os.path.abspath(__file__))",
        "    project_root = os.path.dirname(script_dir)",
        "",
        "    # Handle clean",
        "    if args.clean:",
        "        count = 0",
        "        for root, dirs, files in os.walk(project_root):",
        "            for f in files:",
        "                if f.endswith('.ai_meta'):",
        "                    path = os.path.join(root, f)",
        "                    os.remove(path)",
        "                    count += 1",
        "                    if args.verbose:",
        "                        print(f\"Removed: {path}\")",
        "        print(f\"Removed {count} .ai_meta files\")",
        "        return",
        "",
        "    # Determine files to process",
        "    if args.files:",
        "        py_files = [os.path.abspath(f) for f in args.files if f.endswith('.py')]",
        "    else:",
        "        # Default: all cortical/*.py files",
        "        cortical_dir = os.path.join(project_root, 'cortical')",
        "        py_files = [",
        "            os.path.join(cortical_dir, f)",
        "            for f in os.listdir(cortical_dir)",
        "            if f.endswith('.py') and not f.startswith('__')",
        "        ]",
        "",
        "    if not py_files:",
        "        print(\"No Python files to process\")",
        "        return",
        "",
        "    # Process each file",
        "    generated = 0",
        "    skipped = 0",
        "",
        "    for py_file in sorted(py_files):",
        "        if not os.path.exists(py_file):",
        "            print(f\"Warning: {py_file} not found, skipping\")",
        "            continue",
        "",
        "        # Determine output path",
        "        if args.output_dir:",
        "            meta_file = os.path.join(",
        "                args.output_dir,",
        "                os.path.basename(py_file) + '.ai_meta'",
        "            )",
        "        else:",
        "            meta_file = py_file + '.ai_meta'",
        "",
        "        # Check if regeneration needed",
        "        if args.incremental and not should_regenerate(py_file, meta_file):",
        "            skipped += 1",
        "            if args.verbose:",
        "                print(f\"Skipped (unchanged): {os.path.basename(py_file)}\")",
        "            continue",
        "",
        "        try:",
        "            yaml_content = generate_metadata_for_file(py_file)",
        "",
        "            # Ensure output directory exists",
        "            os.makedirs(os.path.dirname(meta_file) or '.', exist_ok=True)",
        "",
        "            with open(meta_file, 'w', encoding='utf-8') as f:",
        "                f.write(yaml_content)",
        "",
        "            generated += 1",
        "            if args.verbose:",
        "                print(f\"Generated: {os.path.basename(meta_file)}\")",
        "",
        "        except Exception as e:",
        "            print(f\"Error processing {py_file}: {e}\")",
        "            if args.verbose:",
        "                import traceback",
        "                traceback.print_exc()",
        "",
        "    print(f\"\\nGenerated {generated} .ai_meta files\")",
        "    if skipped:",
        "        print(f\"Skipped {skipped} unchanged files\")",
        "",
        "    # Reminder about .gitignore",
        "    gitignore_path = os.path.join(project_root, '.gitignore')",
        "    if os.path.exists(gitignore_path):",
        "        with open(gitignore_path, 'r') as f:",
        "            if '*.ai_meta' not in f.read():",
        "                print(\"\\n‚ö†Ô∏è  Remember to add '*.ai_meta' to .gitignore\")",
        "",
        "",
        "if __name__ == '__main__':",
        "    main()"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "tests/test_generate_ai_metadata.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "\"\"\"",
        "Tests for the AI metadata generator script.",
        "\"\"\"",
        "",
        "import ast",
        "import os",
        "import sys",
        "import tempfile",
        "import shutil",
        "import unittest",
        "",
        "# Add scripts directory to path",
        "sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'scripts'))",
        "",
        "from generate_ai_metadata import (",
        "    FunctionInfo,",
        "    ClassInfo,",
        "    ModuleAnalyzer,",
        "    dict_to_yaml,",
        "    generate_metadata_for_file,",
        "    should_regenerate,",
        "    COMPLEXITY_HINTS,",
        "    RELATED_FUNCTION_PATTERNS,",
        ")",
        "",
        "",
        "class TestFunctionInfo(unittest.TestCase):",
        "    \"\"\"Tests for FunctionInfo class.\"\"\"",
        "",
        "    def test_basic_function(self):",
        "        \"\"\"Test extraction of a basic function.\"\"\"",
        "        code = '''",
        "def my_function(x: int, y: str = \"default\") -> bool:",
        "    \"\"\"A test function.\"\"\"",
        "    return True",
        "'''",
        "        tree = ast.parse(code)",
        "        func_node = tree.body[0]",
        "        info = FunctionInfo(func_node, code.split('\\n'))",
        "",
        "        self.assertEqual(info.name, 'my_function')",
        "        self.assertEqual(info.line_start, 2)",
        "        self.assertFalse(info.is_private)",
        "        self.assertFalse(info.is_dunder)",
        "        self.assertFalse(info.is_async)",
        "        self.assertIn('int', info.signature)",
        "        self.assertIn('str', info.signature)",
        "        self.assertIn('bool', info.signature)",
        "        self.assertEqual(info.docstring_summary, 'A test function.')",
        "",
        "    def test_private_function(self):",
        "        \"\"\"Test detection of private functions.\"\"\"",
        "        code = '''",
        "def _private_func():",
        "    pass",
        "'''",
        "        tree = ast.parse(code)",
        "        func_node = tree.body[0]",
        "        info = FunctionInfo(func_node, code.split('\\n'))",
        "",
        "        self.assertTrue(info.is_private)",
        "        self.assertFalse(info.is_dunder)",
        "",
        "    def test_dunder_function(self):",
        "        \"\"\"Test detection of dunder methods.\"\"\"",
        "        code = '''",
        "def __init__(self):",
        "    pass",
        "'''",
        "        tree = ast.parse(code)",
        "        func_node = tree.body[0]",
        "        info = FunctionInfo(func_node, code.split('\\n'))",
        "",
        "        self.assertTrue(info.is_dunder)",
        "        self.assertTrue(info.is_private)  # Dunders start with _",
        "",
        "    def test_async_function(self):",
        "        \"\"\"Test extraction of async functions.\"\"\"",
        "        code = '''",
        "async def async_func(x: int) -> int:",
        "    \"\"\"An async function.\"\"\"",
        "    return x",
        "'''",
        "        tree = ast.parse(code)",
        "        func_node = tree.body[0]",
        "        info = FunctionInfo(func_node, code.split('\\n'))",
        "",
        "        self.assertEqual(info.name, 'async_func')",
        "        self.assertTrue(info.is_async)",
        "        self.assertIn('int', info.signature)",
        "",
        "    def test_complex_signature(self):",
        "        \"\"\"Test extraction of complex type signatures.\"\"\"",
        "        code = '''",
        "from typing import Dict, List, Optional",
        "",
        "def complex_func(",
        "    data: Dict[str, List[int]],",
        "    callback: Optional[callable] = None,",
        "    *args,",
        "    **kwargs",
        ") -> tuple:",
        "    pass",
        "'''",
        "        tree = ast.parse(code)",
        "        func_node = tree.body[1]  # Skip import",
        "        info = FunctionInfo(func_node, code.split('\\n'))",
        "",
        "        self.assertIn('Dict', info.signature)",
        "        self.assertIn('List', info.signature)",
        "        self.assertIn('Optional', info.signature)",
        "        self.assertIn('*args', info.signature)",
        "        self.assertIn('**kwargs', info.signature)",
        "",
        "    def test_decorated_function(self):",
        "        \"\"\"Test extraction of decorators.\"\"\"",
        "        code = '''",
        "@staticmethod",
        "@custom_decorator",
        "def decorated():",
        "    pass",
        "'''",
        "        tree = ast.parse(code)",
        "        func_node = tree.body[0]",
        "        info = FunctionInfo(func_node, code.split('\\n'))",
        "",
        "        self.assertIn('staticmethod', info.decorators)",
        "        self.assertIn('custom_decorator', info.decorators)",
        "",
        "    def test_method_detection(self):",
        "        \"\"\"Test detection of methods vs functions.\"\"\"",
        "        code = '''",
        "def regular_func():",
        "    pass",
        "",
        "class MyClass:",
        "    def method(self):",
        "        pass",
        "",
        "    @classmethod",
        "    def class_method(cls):",
        "        pass",
        "'''",
        "        tree = ast.parse(code)",
        "",
        "        # Regular function",
        "        func_info = FunctionInfo(tree.body[0], code.split('\\n'))",
        "        self.assertFalse(func_info.is_method)",
        "",
        "        # Instance method",
        "        method_node = tree.body[1].body[0]",
        "        method_info = FunctionInfo(method_node, code.split('\\n'))",
        "        self.assertTrue(method_info.is_method)",
        "",
        "        # Class method",
        "        classmethod_node = tree.body[1].body[1]",
        "        classmethod_info = FunctionInfo(classmethod_node, code.split('\\n'))",
        "        self.assertTrue(classmethod_info.is_method)",
        "",
        "",
        "class TestClassInfo(unittest.TestCase):",
        "    \"\"\"Tests for ClassInfo class.\"\"\"",
        "",
        "    def test_basic_class(self):",
        "        \"\"\"Test extraction of a basic class.\"\"\"",
        "        code = '''",
        "class MyClass:",
        "    \"\"\"A test class.\"\"\"",
        "",
        "    def method1(self):",
        "        pass",
        "",
        "    def method2(self):",
        "        pass",
        "'''",
        "        tree = ast.parse(code)",
        "        class_node = tree.body[0]",
        "        info = ClassInfo(class_node, code.split('\\n'))",
        "",
        "        self.assertEqual(info.name, 'MyClass')",
        "        self.assertEqual(info.docstring_summary, 'A test class.')",
        "        self.assertEqual(len(info.methods), 2)",
        "        self.assertEqual(info.methods[0].name, 'method1')",
        "        self.assertEqual(info.methods[1].name, 'method2')",
        "",
        "    def test_class_with_bases(self):",
        "        \"\"\"Test extraction of base classes.\"\"\"",
        "        code = '''",
        "class Child(Parent, Mixin):",
        "    pass",
        "'''",
        "        tree = ast.parse(code)",
        "        class_node = tree.body[0]",
        "        info = ClassInfo(class_node, code.split('\\n'))",
        "",
        "        self.assertIn('Parent', info.bases)",
        "        self.assertIn('Mixin', info.bases)",
        "",
        "    def test_class_with_async_methods(self):",
        "        \"\"\"Test that async methods are captured.\"\"\"",
        "        code = '''",
        "class AsyncClass:",
        "    async def async_method(self):",
        "        pass",
        "",
        "    def sync_method(self):",
        "        pass",
        "'''",
        "        tree = ast.parse(code)",
        "        class_node = tree.body[0]",
        "        info = ClassInfo(class_node, code.split('\\n'))",
        "",
        "        self.assertEqual(len(info.methods), 2)",
        "        async_method = [m for m in info.methods if m.name == 'async_method'][0]",
        "        sync_method = [m for m in info.methods if m.name == 'sync_method'][0]",
        "",
        "        self.assertTrue(async_method.is_async)",
        "        self.assertFalse(sync_method.is_async)",
        "",
        "    def test_empty_class(self):",
        "        \"\"\"Test handling of empty class.\"\"\"",
        "        code = '''",
        "class EmptyClass:",
        "    \"\"\"An empty class.\"\"\"",
        "    pass",
        "'''",
        "        tree = ast.parse(code)",
        "        class_node = tree.body[0]",
        "        info = ClassInfo(class_node, code.split('\\n'))",
        "",
        "        self.assertEqual(info.name, 'EmptyClass')",
        "        self.assertEqual(len(info.methods), 0)",
        "",
        "",
        "class TestModuleAnalyzer(unittest.TestCase):",
        "    \"\"\"Tests for ModuleAnalyzer class.\"\"\"",
        "",
        "    def setUp(self):",
        "        \"\"\"Create a temporary file for testing.\"\"\"",
        "        self.temp_dir = tempfile.mkdtemp()",
        "        self.test_file = os.path.join(self.temp_dir, 'test_module.py')",
        "",
        "    def tearDown(self):",
        "        \"\"\"Clean up temporary files.\"\"\"",
        "        shutil.rmtree(self.temp_dir)",
        "",
        "    def test_basic_module(self):",
        "        \"\"\"Test analysis of a basic module.\"\"\"",
        "        code = '''\"\"\"Test module docstring.\"\"\"",
        "",
        "import os",
        "from typing import Dict",
        "",
        "def func1():",
        "    \"\"\"First function.\"\"\"",
        "    pass",
        "",
        "def func2():",
        "    \"\"\"Second function.\"\"\"",
        "    pass",
        "",
        "class MyClass:",
        "    \"\"\"A class.\"\"\"",
        "    pass",
        "'''",
        "        with open(self.test_file, 'w') as f:",
        "            f.write(code)",
        "",
        "        analyzer = ModuleAnalyzer(self.test_file)",
        "",
        "        self.assertEqual(analyzer.module_docstring, 'Test module docstring.')",
        "        self.assertEqual(len(analyzer.functions), 2)",
        "        self.assertEqual(len(analyzer.classes), 1)",
        "        self.assertIn('os', analyzer.imports)",
        "",
        "    def test_find_related_functions(self):",
        "        \"\"\"Test finding related functions.\"\"\"",
        "        code = '''",
        "def find_documents():",
        "    pass",
        "",
        "def find_passages():",
        "    pass",
        "",
        "def fast_find_documents():",
        "    pass",
        "",
        "def unrelated_func():",
        "    pass",
        "'''",
        "        with open(self.test_file, 'w') as f:",
        "            f.write(code)",
        "",
        "        analyzer = ModuleAnalyzer(self.test_file)",
        "",
        "        related = analyzer.find_related_functions('find_documents')",
        "        self.assertIn('find_passages', related)",
        "        self.assertIn('fast_find_documents', related)",
        "",
        "    def test_complexity_hints(self):",
        "        \"\"\"Test retrieval of complexity hints.\"\"\"",
        "        code = '''",
        "def compute_all():",
        "    pass",
        "",
        "def compute_pagerank():",
        "    pass",
        "",
        "def regular_func():",
        "    pass",
        "'''",
        "        with open(self.test_file, 'w') as f:",
        "            f.write(code)",
        "",
        "        analyzer = ModuleAnalyzer(self.test_file)",
        "",
        "        self.assertIsNotNone(analyzer.get_complexity_hint('compute_all'))",
        "        self.assertIsNotNone(analyzer.get_complexity_hint('compute_pagerank'))",
        "        self.assertIsNone(analyzer.get_complexity_hint('regular_func'))",
        "",
        "    def test_section_detection(self):",
        "        \"\"\"Test detection of logical sections.\"\"\"",
        "        code = '''",
        "def process_document():",
        "    pass",
        "",
        "def add_document():",
        "    pass",
        "",
        "def compute_pagerank():",
        "    pass",
        "",
        "def compute_tfidf():",
        "    pass",
        "",
        "def find_documents():",
        "    pass",
        "'''",
        "        with open(self.test_file, 'w') as f:",
        "            f.write(code)",
        "",
        "        analyzer = ModuleAnalyzer(self.test_file)",
        "        sections = analyzer.detect_sections()",
        "",
        "        section_names = [s['name'] for s in sections]",
        "        # Should detect document, computation, and query sections",
        "        self.assertTrue(len(sections) >= 2)",
        "",
        "    def test_generate_metadata(self):",
        "        \"\"\"Test complete metadata generation.\"\"\"",
        "        code = '''\"\"\"Module docstring.\"\"\"",
        "",
        "from typing import List",
        "",
        "def my_func(x: int) -> List[str]:",
        "    \"\"\"A function.\"\"\"",
        "    pass",
        "",
        "class MyClass:",
        "    \"\"\"A class.\"\"\"",
        "",
        "    def method(self):",
        "        \"\"\"A method.\"\"\"",
        "        pass",
        "'''",
        "        with open(self.test_file, 'w') as f:",
        "            f.write(code)",
        "",
        "        analyzer = ModuleAnalyzer(self.test_file)",
        "        metadata = analyzer.generate_metadata()",
        "",
        "        self.assertIn('file', metadata)",
        "        self.assertIn('lines', metadata)",
        "        self.assertIn('functions', metadata)",
        "        self.assertIn('classes', metadata)",
        "        self.assertIn('imports', metadata)",
        "",
        "        # Check function metadata",
        "        self.assertIn('my_func', metadata['functions'])",
        "        func_meta = metadata['functions']['my_func']",
        "        self.assertIn('line', func_meta)",
        "        self.assertIn('signature', func_meta)",
        "",
        "        # Check class metadata",
        "        self.assertIn('MyClass', metadata['classes'])",
        "",
        "",
        "class TestDictToYaml(unittest.TestCase):",
        "    \"\"\"Tests for YAML conversion.\"\"\"",
        "",
        "    def test_simple_dict(self):",
        "        \"\"\"Test conversion of simple dictionary.\"\"\"",
        "        data = {'key': 'value', 'number': 42}",
        "        yaml = dict_to_yaml(data)",
        "",
        "        self.assertIn('key: value', yaml)",
        "        self.assertIn('number: 42', yaml)",
        "",
        "    def test_nested_dict(self):",
        "        \"\"\"Test conversion of nested dictionary.\"\"\"",
        "        data = {",
        "            'outer': {",
        "                'inner': 'value'",
        "            }",
        "        }",
        "        yaml = dict_to_yaml(data)",
        "",
        "        self.assertIn('outer:', yaml)",
        "        self.assertIn('inner: value', yaml)",
        "",
        "    def test_list(self):",
        "        \"\"\"Test conversion of lists.\"\"\"",
        "        data = {'items': ['a', 'b', 'c']}",
        "        yaml = dict_to_yaml(data)",
        "",
        "        self.assertIn('items:', yaml)",
        "        self.assertIn('- a', yaml)",
        "        self.assertIn('- b', yaml)",
        "        self.assertIn('- c', yaml)",
        "",
        "    def test_special_characters(self):",
        "        \"\"\"Test handling of special characters in strings.\"\"\"",
        "        data = {'key': 'value: with colon'}",
        "        yaml = dict_to_yaml(data)",
        "",
        "        # Should be quoted",
        "        self.assertIn('\"value: with colon\"', yaml)",
        "",
        "    def test_boolean(self):",
        "        \"\"\"Test boolean conversion.\"\"\"",
        "        data = {'flag': True, 'other': False}",
        "        yaml = dict_to_yaml(data)",
        "",
        "        self.assertIn('flag: true', yaml)",
        "        self.assertIn('other: false', yaml)",
        "",
        "    def test_none(self):",
        "        \"\"\"Test None conversion.\"\"\"",
        "        data = {'empty': None}",
        "        yaml = dict_to_yaml(data)",
        "",
        "        self.assertIn('empty: null', yaml)",
        "",
        "",
        "class TestShouldRegenerate(unittest.TestCase):",
        "    \"\"\"Tests for regeneration checking.\"\"\"",
        "",
        "    def setUp(self):",
        "        \"\"\"Create temporary files.\"\"\"",
        "        self.temp_dir = tempfile.mkdtemp()",
        "        self.py_file = os.path.join(self.temp_dir, 'test.py')",
        "        self.meta_file = os.path.join(self.temp_dir, 'test.py.ai_meta')",
        "",
        "    def tearDown(self):",
        "        \"\"\"Clean up temporary files.\"\"\"",
        "        shutil.rmtree(self.temp_dir)",
        "",
        "    def test_no_meta_file(self):",
        "        \"\"\"Should regenerate if no meta file exists.\"\"\"",
        "        with open(self.py_file, 'w') as f:",
        "            f.write('# test')",
        "",
        "        self.assertTrue(should_regenerate(self.py_file, self.meta_file))",
        "",
        "    def test_meta_newer_than_source(self):",
        "        \"\"\"Should not regenerate if meta is newer.\"\"\"",
        "        with open(self.py_file, 'w') as f:",
        "            f.write('# test')",
        "",
        "        # Create meta file after",
        "        import time",
        "        time.sleep(0.01)",
        "        with open(self.meta_file, 'w') as f:",
        "            f.write('# meta')",
        "",
        "        self.assertFalse(should_regenerate(self.py_file, self.meta_file))",
        "",
        "    def test_source_newer_than_meta(self):",
        "        \"\"\"Should regenerate if source is newer.\"\"\"",
        "        with open(self.meta_file, 'w') as f:",
        "            f.write('# meta')",
        "",
        "        import time",
        "        time.sleep(0.01)",
        "        with open(self.py_file, 'w') as f:",
        "            f.write('# test')",
        "",
        "        self.assertTrue(should_regenerate(self.py_file, self.meta_file))",
        "",
        "",
        "class TestGenerateMetadataForFile(unittest.TestCase):",
        "    \"\"\"Tests for file metadata generation.\"\"\"",
        "",
        "    def setUp(self):",
        "        \"\"\"Create a temporary file for testing.\"\"\"",
        "        self.temp_dir = tempfile.mkdtemp()",
        "        self.test_file = os.path.join(self.temp_dir, 'test_module.py')",
        "",
        "    def tearDown(self):",
        "        \"\"\"Clean up temporary files.\"\"\"",
        "        shutil.rmtree(self.temp_dir)",
        "",
        "    def test_generates_valid_yaml(self):",
        "        \"\"\"Test that generated output is valid YAML.\"\"\"",
        "        code = '''\"\"\"Test module.\"\"\"",
        "",
        "def test_func():",
        "    \"\"\"A test function.\"\"\"",
        "    pass",
        "'''",
        "        with open(self.test_file, 'w') as f:",
        "            f.write(code)",
        "",
        "        yaml_content = generate_metadata_for_file(self.test_file)",
        "",
        "        # Should start with header comment",
        "        self.assertTrue(yaml_content.startswith('#'))",
        "",
        "        # Should contain expected keys",
        "        self.assertIn('file:', yaml_content)",
        "        self.assertIn('lines:', yaml_content)",
        "        self.assertIn('functions:', yaml_content)",
        "",
        "    def test_handles_empty_file(self):",
        "        \"\"\"Test handling of empty file.\"\"\"",
        "        with open(self.test_file, 'w') as f:",
        "            f.write('')",
        "",
        "        yaml_content = generate_metadata_for_file(self.test_file)",
        "",
        "        # Empty file still has 1 line (empty string split gives [''])",
        "        self.assertIn('lines: 1', yaml_content)",
        "        self.assertIn('functions: {}', yaml_content)",
        "",
        "",
        "class TestIntegration(unittest.TestCase):",
        "    \"\"\"Integration tests for the metadata generator.\"\"\"",
        "",
        "    def test_cortical_processor_metadata(self):",
        "        \"\"\"Test metadata generation for actual cortical/processor.py.\"\"\"",
        "        processor_path = os.path.join(",
        "            os.path.dirname(__file__),",
        "            '..', 'cortical', 'processor.py'",
        "        )",
        "",
        "        if not os.path.exists(processor_path):",
        "            self.skipTest(\"processor.py not found\")",
        "",
        "        analyzer = ModuleAnalyzer(processor_path)",
        "        metadata = analyzer.generate_metadata()",
        "",
        "        # Should find the main class",
        "        self.assertIn('CorticalTextProcessor', metadata['classes'])",
        "",
        "        # Should find key functions",
        "        func_names = list(metadata['functions'].keys())",
        "        self.assertTrue(any('process_document' in name for name in func_names))",
        "        self.assertTrue(any('compute_all' in name for name in func_names))",
        "",
        "        # Should have complexity hints for expensive functions",
        "        compute_all_key = [k for k in func_names if 'compute_all' in k][0]",
        "        self.assertIn('complexity', metadata['functions'][compute_all_key])",
        "",
        "",
        "if __name__ == '__main__':",
        "    unittest.main()"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    }
  ],
  "hour_of_day": 13,
  "day_of_week": "Thursday",
  "seconds_since_last_commit": -345734,
  "is_merge": true,
  "is_initial": false,
  "parent_count": 2,
  "session_id": null,
  "related_chats": [],
  "ci_result": null,
  "reverted": false,
  "amended": false
}