{
  "hash": "83e6f6117bd81e672f575dd9a8dd33d5689a5dae",
  "message": "Implement Task #97: Integrate CorticalConfig into processor",
  "author": "Claude",
  "timestamp": "2025-12-12 00:20:33 +0000",
  "branch": "claude/multi-index-design-DvifZ",
  "files_changed": [
    "TASK_LIST.md",
    "cortical/processor.py",
    "tests/test_config.py"
  ],
  "insertions": 164,
  "deletions": 25,
  "hunks": [
    {
      "file": "TASK_LIST.md",
      "function": null,
      "start_line": 1,
      "lines_added": [
        "**Pending Tasks:** 34"
      ],
      "lines_removed": [
        "**Pending Tasks:** 35",
        "| 97 | Integrate CorticalConfig into processor | Arch | - | Medium |"
      ],
      "context_before": [
        "# Task List: Cortical Text Processor",
        "",
        "Active backlog for the Cortical Text Processor project. Completed tasks are archived in [TASK_ARCHIVE.md](TASK_ARCHIVE.md).",
        "",
        "**Last Updated:** 2025-12-11"
      ],
      "context_after": [
        "**Completed Tasks:** 90+ (see archive)",
        "",
        "---",
        "",
        "## Active Backlog",
        "",
        "<!-- Machine-parseable format for automation -->",
        "",
        "### ðŸ”´ Critical (Do Now)",
        "",
        "*All critical tasks completed!*",
        "",
        "### ðŸŸ  High (Do This Week)",
        "",
        "| # | Task | Category | Depends | Effort |",
        "|---|------|----------|---------|--------|",
        "| 94 | Split query.py into focused modules | Arch | - | Large |",
        "",
        "### ðŸŸ¡ Medium (Do This Month)",
        "",
        "| # | Task | Category | Depends | Effort |",
        "|---|------|----------|---------|--------|",
        "| 137 | Cap bigram connections to top-K per bigram | Perf | - | Small |",
        "| 138 | Use sparse matrix multiplication for bigram connections | Perf | - | Medium |",
        "| 139 | Batch bigram connection updates to reduce dict overhead | Perf | - | Small |",
        "| 133 | Implement WAL + snapshot persistence (fault-tolerant rebuild) | Arch | 132 | Large |",
        "| 134 | Implement protobuf serialization for corpus | Arch | 132 | Medium |"
      ],
      "change_type": "modify"
    },
    {
      "file": "TASK_LIST.md",
      "function": "Active backlog for the Cortical Text Processor project. Completed tasks are arch",
      "start_line": 86,
      "lines_added": [
        "| 97 | Integrate CorticalConfig into processor | 2025-12-11 | Config stored on processor, used in method defaults, saved/loaded |"
      ],
      "lines_removed": [],
      "context_before": [
        "| # | Task | Started | Notes |",
        "|---|------|---------|-------|",
        "| 87 | Add Python code samples and showcase | 2025-12-11 | samples/*.py created |",
        "",
        "---",
        "",
        "## Recently Completed (Last 7 Days)",
        "",
        "| # | Task | Completed | Notes |",
        "|---|------|-----------|-------|"
      ],
      "context_after": [
        "| 127 | Create cluster coverage evaluation script | 2025-12-11 | scripts/evaluate_cluster.py with 24 tests |",
        "| 125 | Add clustering quality metrics (modularity, silhouette) | 2025-12-11 | compute_clustering_quality() in analysis.py, showcase display |",
        "| 124 | Add minimum cluster count regression tests | 2025-12-11 | 4 new tests: coherence, showcase count, mega-cluster, distribution |",
        "| 128 | Fix definition boost that favors test mocks over real implementations | 2025-12-11 | Added is_test_file() and test file penalty |",
        "| 132 | Profile full-analysis bottleneck (bigram, semantics O(nÂ²)) | 2025-12-11 | Created profile_full_analysis.py, fixed bottlenecks |",
        "| 136 | Optimize semantics O(nÂ²) similarity with early termination | 2025-12-11 | Added max_similarity_pairs, min_context_keys |",
        "| 126 | Investigate optimal Louvain resolution for sample corpus | 2025-12-11 | Research confirms default 1.0 is optimal |",
        "| 123 | Replace label propagation with Louvain community detection | 2025-12-11 | Implemented Louvain algorithm, 34 clusters for 92 docs |",
        "| 122 | Investigate Concept Layer & Embeddings regressions | 2025-12-11 | Fixed inverted strictness, improved embeddings |",
        "| 119 | Create AI metadata generator script | 2025-12-11 | scripts/generate_ai_metadata.py with tests |"
      ],
      "change_type": "add"
    },
    {
      "file": "cortical/processor.py",
      "function": "Cortical Text Processor - Main processor class that orchestrates all components.",
      "start_line": 4,
      "lines_added": [
        "from .config import CorticalConfig"
      ],
      "lines_removed": [],
      "context_before": [
        "",
        "import os",
        "import re",
        "from typing import Dict, List, Tuple, Optional, Any",
        "import copy",
        "from collections import defaultdict",
        "",
        "from .tokenizer import Tokenizer",
        "from .minicolumn import Minicolumn",
        "from .layers import CorticalLayer, HierarchicalLayer"
      ],
      "context_after": [
        "from . import analysis",
        "from . import semantics",
        "from . import embeddings as emb_module",
        "from . import query as query_module",
        "from . import gaps as gaps_module",
        "from . import persistence",
        "from . import fingerprint as fp_module",
        "",
        "",
        "class CorticalTextProcessor:"
      ],
      "change_type": "add"
    },
    {
      "file": "cortical/processor.py",
      "function": "class CorticalTextProcessor:",
      "start_line": 26,
      "lines_added": [
        "    def __init__(",
        "        self,",
        "        tokenizer: Optional[Tokenizer] = None,",
        "        config: Optional[CorticalConfig] = None",
        "    ):",
        "        \"\"\"",
        "        Initialize the Cortical Text Processor.",
        "",
        "        Args:",
        "            tokenizer: Optional custom tokenizer. Defaults to standard Tokenizer.",
        "            config: Optional configuration. Defaults to CorticalConfig with defaults.",
        "        \"\"\"",
        "        self.config = config or CorticalConfig()"
      ],
      "lines_removed": [
        "    def __init__(self, tokenizer: Optional[Tokenizer] = None):"
      ],
      "context_before": [
        "    # Computation types for tracking staleness",
        "    COMP_TFIDF = 'tfidf'",
        "    COMP_PAGERANK = 'pagerank'",
        "    COMP_ACTIVATION = 'activation'",
        "    COMP_DOC_CONNECTIONS = 'doc_connections'",
        "    COMP_BIGRAM_CONNECTIONS = 'bigram_connections'",
        "    COMP_CONCEPTS = 'concepts'",
        "    COMP_EMBEDDINGS = 'embeddings'",
        "    COMP_SEMANTICS = 'semantics'",
        ""
      ],
      "context_after": [
        "        self.tokenizer = tokenizer or Tokenizer()",
        "        self.layers: Dict[CorticalLayer, HierarchicalLayer] = {",
        "            CorticalLayer.TOKENS: HierarchicalLayer(CorticalLayer.TOKENS),",
        "            CorticalLayer.BIGRAMS: HierarchicalLayer(CorticalLayer.BIGRAMS),",
        "            CorticalLayer.CONCEPTS: HierarchicalLayer(CorticalLayer.CONCEPTS),",
        "            CorticalLayer.DOCUMENTS: HierarchicalLayer(CorticalLayer.DOCUMENTS),",
        "        }",
        "        self.documents: Dict[str, str] = {}",
        "        self.document_metadata: Dict[str, Dict[str, Any]] = {}",
        "        self.embeddings: Dict[str, List[float]] = {}",
        "        self.semantic_relations: List[Tuple[str, str, str, float]] = []"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/processor.py",
      "function": "class CorticalTextProcessor:",
      "start_line": 827,
      "lines_added": [
        "        cross_layer_damping: Optional[float] = None,",
        "            cross_layer_damping: Damping factor at layer boundaries (default from config)",
        "        if cross_layer_damping is None:",
        "            cross_layer_damping = self.config.cross_layer_damping",
        ""
      ],
      "lines_removed": [
        "        cross_layer_damping: float = 0.7,",
        "            cross_layer_damping: Damping factor at layer boundaries (default 0.7)"
      ],
      "context_before": [
        "",
        "        return {",
        "            'total_edges_with_relations': total_edges,",
        "            **layer_stats",
        "        }",
        "",
        "    def compute_hierarchical_importance(",
        "        self,",
        "        layer_iterations: int = 10,",
        "        global_iterations: int = 5,"
      ],
      "context_after": [
        "        verbose: bool = True",
        "    ) -> Dict[str, Any]:",
        "        \"\"\"",
        "        Compute PageRank with cross-layer propagation.",
        "",
        "        This hierarchical PageRank allows importance to flow between layers:",
        "        - Upward: tokens â†’ bigrams â†’ concepts â†’ documents",
        "        - Downward: documents â†’ concepts â†’ bigrams â†’ tokens",
        "",
        "        Important tokens boost their containing bigrams and concepts.",
        "        Important documents boost their contained terms. This creates",
        "        a more holistic importance score that considers the full hierarchy.",
        "",
        "        Args:",
        "            layer_iterations: Max iterations for intra-layer PageRank (default 10)",
        "            global_iterations: Max iterations for cross-layer propagation (default 5)",
        "            verbose: Print progress messages",
        "",
        "        Returns:",
        "            Dict with statistics:",
        "            - iterations_run: Number of global iterations",
        "            - converged: Whether the algorithm converged",
        "            - layer_stats: Per-layer statistics (nodes, max/min/avg PageRank)",
        "",
        "        Example:",
        "            >>> stats = processor.compute_hierarchical_importance()",
        "            >>> print(f\"Converged: {stats['converged']}\")",
        "            >>> for layer, info in stats['layer_stats'].items():",
        "            ...     print(f\"{layer}: {info['nodes']} nodes, max PR={info['max_pagerank']:.4f}\")",
        "        \"\"\"",
        "        result = analysis.compute_hierarchical_pagerank(",
        "            self.layers,",
        "            layer_iterations=layer_iterations,",
        "            global_iterations=global_iterations,",
        "            cross_layer_damping=cross_layer_damping",
        "        )",
        "",
        "        if verbose:",
        "            status = \"converged\" if result['converged'] else \"did not converge\"",
        "            print(f\"Computed hierarchical PageRank ({result['iterations_run']} iterations, {status})\")"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/processor.py",
      "function": "class CorticalTextProcessor:",
      "start_line": 951,
      "lines_added": [
        "        min_cluster_size: Optional[int] = None,",
        "        cluster_strictness: Optional[float] = None,",
        "            min_cluster_size: Minimum tokens per cluster (default from config)",
        "                aggressiveness (0.0-1.0, default from config).",
        "                - 1.0: Strict clustering, topics stay separate"
      ],
      "lines_removed": [
        "        min_cluster_size: int = 3,",
        "        cluster_strictness: float = 1.0,",
        "            min_cluster_size: Minimum tokens per cluster (default 3)",
        "                aggressiveness (0.0-1.0).",
        "                - 1.0 (default): Strict clustering, topics stay separate"
      ],
      "context_before": [
        "                skip_parts.append(f\"{skipped_docs} large docs\")",
        "            skip_msg = f\", skipped {', '.join(skip_parts)}\" if skip_parts else \"\"",
        "            print(f\"Created {stats['connections_created']} bigram connections \"",
        "                  f\"(component: {stats['component_connections']}, \"",
        "                  f\"chain: {stats['chain_connections']}, \"",
        "                  f\"cooccur: {stats['cooccurrence_connections']}{skip_msg})\")",
        "        return stats",
        "",
        "    def build_concept_clusters(",
        "        self,"
      ],
      "context_after": [
        "        clustering_method: str = 'louvain',",
        "        bridge_weight: float = 0.0,",
        "        resolution: float = 1.0,",
        "        verbose: bool = True",
        "    ) -> Dict[int, List[str]]:",
        "        \"\"\"",
        "        Build concept clusters from token layer.",
        "",
        "        Args:",
        "            clustering_method: Algorithm to use for clustering.",
        "                - 'louvain' (default): Louvain community detection.",
        "                  Recommended for dense graphs. Produces meaningful clusters",
        "                  by optimizing modularity.",
        "                - 'label_propagation': Legacy label propagation algorithm.",
        "                  May produce mega-clusters on dense graphs (not recommended).",
        "            cluster_strictness: For label_propagation only. Controls clustering",
        "                - 0.5: Moderate mixing, allows some cross-topic clustering",
        "                - 0.0: Minimal clustering, most tokens group together",
        "            bridge_weight: For label_propagation only. Weight for synthetic",
        "                inter-document connections (0.0-1.0).",
        "            resolution: For louvain only. Resolution parameter for modularity.",
        "                - Higher values (>1.0): More, smaller clusters",
        "                - Lower values (<1.0): Fewer, larger clusters",
        "                - 1.0 (default): Standard modularity",
        "            verbose: Print progress messages",
        ""
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/processor.py",
      "function": "class CorticalTextProcessor:",
      "start_line": 1001,
      "lines_added": [
        "        if min_cluster_size is None:",
        "            min_cluster_size = self.config.min_cluster_size",
        "        if cluster_strictness is None:",
        "            cluster_strictness = self.config.cluster_strictness",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "            ...     clustering_method='louvain',",
        "            ...     resolution=1.5",
        "            ... )",
        "            >>>",
        "            >>> # Legacy label propagation (backward compatibility)",
        "            >>> clusters = processor.build_concept_clusters(",
        "            ...     clustering_method='label_propagation',",
        "            ...     cluster_strictness=0.5",
        "            ... )",
        "        \"\"\""
      ],
      "context_after": [
        "        if clustering_method == 'louvain':",
        "            clusters = analysis.cluster_by_louvain(",
        "                self.layers[CorticalLayer.TOKENS],",
        "                min_cluster_size=min_cluster_size,",
        "                resolution=resolution",
        "            )",
        "        elif clustering_method == 'label_propagation':",
        "            clusters = analysis.cluster_by_label_propagation(",
        "                self.layers[CorticalLayer.TOKENS],",
        "                min_cluster_size=min_cluster_size,"
      ],
      "change_type": "add"
    },
    {
      "file": "cortical/processor.py",
      "function": "class CorticalTextProcessor:",
      "start_line": 1349,
      "lines_added": [
        "        max_expansions: Optional[int] = None,",
        "            max_expansions: Maximum expansion terms to add (default from config)",
        "        if max_expansions is None:",
        "            max_expansions = self.config.max_query_expansions",
        "",
        "    def expand_query_for_code(self, query_text: str, max_expansions: Optional[int] = None) -> Dict[str, float]:",
        "            max_expansions: Maximum expansion terms to add (default from config + 5)",
        "        if max_expansions is None:",
        "            max_expansions = self.config.max_query_expansions + 5  # Code search benefits from more expansions",
        "",
        "        max_expansions: Optional[int] = None,",
        "            max_expansions: Maximum expansion terms to add (default from config)",
        "        if max_expansions is None:",
        "            max_expansions = self.config.max_query_expansions",
        ""
      ],
      "lines_removed": [
        "        max_expansions: int = 10,",
        "            max_expansions: Maximum expansion terms to add",
        "    def expand_query_for_code(self, query_text: str, max_expansions: int = 15) -> Dict[str, float]:",
        "            max_expansions: Maximum expansion terms to add",
        "        max_expansions: int = 10,",
        "            max_expansions: Maximum expansion terms to add"
      ],
      "context_before": [
        "    ",
        "    def embedding_similarity(self, term1: str, term2: str) -> float:",
        "        return emb_module.embedding_similarity(self.embeddings, term1, term2)",
        "    ",
        "    def find_similar_by_embedding(self, term: str, top_n: int = 10) -> List[Tuple[str, float]]:",
        "        return emb_module.find_similar_by_embedding(self.embeddings, term, top_n)",
        "    ",
        "    def expand_query(",
        "        self,",
        "        query_text: str,"
      ],
      "context_after": [
        "        use_variants: bool = True,",
        "        use_code_concepts: bool = False,",
        "        filter_code_stop_words: bool = False,",
        "        verbose: bool = False",
        "    ) -> Dict[str, float]:",
        "        \"\"\"",
        "        Expand a query using lateral connections and concept clusters.",
        "",
        "        Args:",
        "            query_text: Original query string",
        "            use_variants: Try word variants when direct match fails",
        "            use_code_concepts: Include programming synonym expansions",
        "            filter_code_stop_words: Filter ubiquitous code tokens (self, cls, etc.)",
        "",
        "        Returns:",
        "            Dict mapping terms to weights",
        "        \"\"\"",
        "        return query_module.expand_query(",
        "            query_text,",
        "            self.layers,",
        "            self.tokenizer,",
        "            max_expansions=max_expansions,",
        "            use_variants=use_variants,",
        "            use_code_concepts=use_code_concepts,",
        "            filter_code_stop_words=filter_code_stop_words",
        "        )",
        "",
        "        \"\"\"",
        "        Expand a query optimized for code search.",
        "",
        "        Enables code concept expansion to find programming synonyms",
        "        (e.g., \"fetch\" also matches \"get\", \"load\", \"retrieve\").",
        "        Also filters ubiquitous code tokens (self, cls, etc.) from expansion.",
        "",
        "        Args:",
        "            query_text: Original query string",
        "",
        "        Returns:",
        "            Dict mapping terms to weights",
        "        \"\"\"",
        "        return query_module.expand_query(",
        "            query_text,",
        "            self.layers,",
        "            self.tokenizer,",
        "            max_expansions=max_expansions,",
        "            use_variants=True,",
        "            use_code_concepts=True,",
        "            filter_code_stop_words=True  # Filter self, cls, etc.",
        "        )",
        "",
        "    def expand_query_cached(",
        "        self,",
        "        query_text: str,",
        "        use_variants: bool = True,",
        "        use_code_concepts: bool = False",
        "    ) -> Dict[str, float]:",
        "        \"\"\"",
        "        Expand a query with caching for faster repeated lookups.",
        "",
        "        Uses an LRU-style cache to avoid recomputing expansion for",
        "        frequently repeated queries. Useful in RAG loops where the",
        "        same queries may be issued multiple times.",
        "",
        "        Args:",
        "            query_text: Original query string",
        "            use_variants: Try word variants when direct match fails",
        "            use_code_concepts: Include programming synonym expansions",
        "",
        "        Returns:",
        "            Dict mapping terms to weights",
        "        \"\"\"",
        "        # Create cache key from parameters",
        "        cache_key = f\"{query_text}|{max_expansions}|{use_variants}|{use_code_concepts}\"",
        "",
        "        # Check cache",
        "        if cache_key in self._query_expansion_cache:",
        "            return self._query_expansion_cache[cache_key].copy()",
        "",
        "        # Compute expansion",
        "        result = query_module.expand_query(",
        "            query_text,"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/processor.py",
      "function": "class CorticalTextProcessor:",
      "start_line": 1852,
      "lines_added": [
        "        chunk_size: Optional[int] = None,",
        "        overlap: Optional[int] = None,"
      ],
      "lines_removed": [
        "        chunk_size: int = 512,",
        "        overlap: int = 128,"
      ],
      "context_before": [
        "            query_text,",
        "            index,",
        "            self.tokenizer,",
        "            top_n=top_n",
        "        )",
        "",
        "    def find_passages_for_query(",
        "        self,",
        "        query_text: str,",
        "        top_n: int = 5,"
      ],
      "context_after": [
        "        use_expansion: bool = True,",
        "        doc_filter: Optional[List[str]] = None,",
        "        use_semantic: bool = True,",
        "        use_definition_search: bool = True,",
        "        definition_boost: float = 5.0,",
        "        apply_doc_boost: bool = True,",
        "        auto_detect_intent: bool = True,",
        "        prefer_docs: bool = False,",
        "        custom_boosts: Optional[Dict[str, float]] = None,",
        "        use_code_aware_chunks: bool = True"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/processor.py",
      "function": "class CorticalTextProcessor:",
      "start_line": 1884,
      "lines_added": [
        "            chunk_size: Size of each chunk in characters (default from config)",
        "            overlap: Overlap between chunks in characters (default from config)"
      ],
      "lines_removed": [
        "            chunk_size: Size of each chunk in characters (default 512)",
        "            overlap: Overlap between chunks in characters (default 128)"
      ],
      "context_before": [
        "",
        "        For conceptual queries (e.g., \"what is PageRank\", \"explain architecture\"),",
        "        documentation passages are boosted when auto_detect_intent=True.",
        "",
        "        For code files (.py, .js, etc.), semantic chunk boundaries are used to",
        "        align chunks with class/function definitions rather than fixed positions.",
        "",
        "        Args:",
        "            query_text: Search query",
        "            top_n: Number of passages to return"
      ],
      "context_after": [
        "            use_expansion: Whether to expand query terms",
        "            doc_filter: Optional list of doc_ids to restrict search to",
        "            use_semantic: Whether to use semantic relations for expansion (if available)",
        "            use_definition_search: Whether to search for definition patterns (default True)",
        "            definition_boost: Score boost for definition matches (default 5.0)",
        "            apply_doc_boost: Whether to apply document-type boosting (default True)",
        "            auto_detect_intent: Auto-detect conceptual queries and boost docs (default True)",
        "            prefer_docs: Always boost documentation regardless of query type (default False)",
        "            custom_boosts: Optional custom boost factors for doc types",
        "            use_code_aware_chunks: Use semantic boundaries for code files (default True)"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/processor.py",
      "function": "class CorticalTextProcessor:",
      "start_line": 1907,
      "lines_added": [
        "        if chunk_size is None:",
        "            chunk_size = self.config.chunk_size",
        "        if overlap is None:",
        "            overlap = self.config.chunk_overlap"
      ],
      "lines_removed": [],
      "context_before": [
        "        Returns:",
        "            List of (passage_text, doc_id, start_char, end_char, score) tuples",
        "            ranked by relevance",
        "",
        "        Example:",
        "            >>> # For conceptual queries, docs are auto-boosted",
        "            >>> results = processor.find_passages_for_query(\"what is PageRank\")",
        "            >>> for passage, doc_id, start, end, score in results:",
        "            ...     print(f\"[{doc_id}:{start}-{end}] {passage[:50]}... (score: {score:.3f})\")",
        "        \"\"\""
      ],
      "context_after": [
        "        return query_module.find_passages_for_query(",
        "            query_text,",
        "            self.layers,",
        "            self.tokenizer,",
        "            self.documents,",
        "            top_n=top_n,",
        "            chunk_size=chunk_size,",
        "            overlap=overlap,",
        "            use_expansion=use_expansion,",
        "            doc_filter=doc_filter,"
      ],
      "change_type": "add"
    },
    {
      "file": "cortical/processor.py",
      "function": "class CorticalTextProcessor:",
      "start_line": 2278,
      "lines_added": [
        "        Saves all computed state including embeddings, semantic relations,",
        "        and configuration, so they don't need to be recomputed when loading.",
        "            'has_relations': bool(self.semantic_relations),",
        "            'config': self.config.to_dict()  # Save config in metadata",
        "        Restores all computed state including embeddings, semantic relations,",
        "        and configuration.",
        "",
        "        # Restore config if available, otherwise use defaults",
        "        config = None",
        "        if metadata and 'config' in metadata:",
        "            try:",
        "                config = CorticalConfig.from_dict(metadata['config'])",
        "            except (KeyError, TypeError):",
        "                # Fall back to default config if restoration fails",
        "                config = None",
        "",
        "        processor = cls(config=config)"
      ],
      "lines_removed": [
        "        Saves all computed state including embeddings and semantic relations,",
        "        so they don't need to be recomputed when loading.",
        "            'has_relations': bool(self.semantic_relations)",
        "        Restores all computed state including embeddings and semantic relations.",
        "        processor = cls()"
      ],
      "context_before": [
        "            results.append((candidate_id, comparison['overall_similarity'], comparison))",
        "",
        "        # Sort by similarity descending",
        "        results.sort(key=lambda x: x[1], reverse=True)",
        "        return results[:top_n]",
        "",
        "    def save(self, filepath: str, verbose: bool = True) -> None:",
        "        \"\"\"",
        "        Save processor state to a file.",
        ""
      ],
      "context_after": [
        "        \"\"\"",
        "        metadata = {",
        "            'has_embeddings': bool(self.embeddings),",
        "        }",
        "        persistence.save_processor(",
        "            filepath,",
        "            self.layers,",
        "            self.documents,",
        "            self.document_metadata,",
        "            self.embeddings,",
        "            self.semantic_relations,",
        "            metadata,",
        "            verbose",
        "        )",
        "",
        "    @classmethod",
        "    def load(cls, filepath: str, verbose: bool = True) -> 'CorticalTextProcessor':",
        "        \"\"\"",
        "        Load processor state from a file.",
        "",
        "        \"\"\"",
        "        result = persistence.load_processor(filepath, verbose)",
        "        layers, documents, document_metadata, embeddings, semantic_relations, metadata = result",
        "        processor.layers = layers",
        "        processor.documents = documents",
        "        processor.document_metadata = document_metadata",
        "        processor.embeddings = embeddings",
        "        processor.semantic_relations = semantic_relations",
        "        return processor",
        "    ",
        "    def export_graph(self, filepath: str, layer: Optional[CorticalLayer] = None, max_nodes: int = 500) -> Dict:",
        "        return persistence.export_graph_json(filepath, self.layers, layer, max_nodes=max_nodes)",
        ""
      ],
      "change_type": "modify"
    },
    {
      "file": "tests/test_config.py",
      "function": "class TestValidRelationChains(unittest.TestCase):",
      "start_line": 288,
      "lines_added": [
        "class TestProcessorConfigIntegration(unittest.TestCase):",
        "    \"\"\"Tests for CorticalConfig integration with CorticalTextProcessor.\"\"\"",
        "",
        "    def test_processor_accepts_config(self):",
        "        \"\"\"Test that processor accepts config parameter.\"\"\"",
        "        from cortical.processor import CorticalTextProcessor",
        "",
        "        config = CorticalConfig(min_cluster_size=5, chunk_size=256)",
        "        processor = CorticalTextProcessor(config=config)",
        "",
        "        self.assertEqual(processor.config.min_cluster_size, 5)",
        "        self.assertEqual(processor.config.chunk_size, 256)",
        "",
        "    def test_processor_uses_default_config(self):",
        "        \"\"\"Test that processor uses default config when none provided.\"\"\"",
        "        from cortical.processor import CorticalTextProcessor",
        "",
        "        processor = CorticalTextProcessor()",
        "",
        "        # Should have default values",
        "        self.assertEqual(processor.config.min_cluster_size, 3)",
        "        self.assertEqual(processor.config.chunk_size, 512)",
        "",
        "    def test_config_used_in_expand_query(self):",
        "        \"\"\"Test that config.max_query_expansions is used.\"\"\"",
        "        from cortical.processor import CorticalTextProcessor",
        "",
        "        config = CorticalConfig(max_query_expansions=3)",
        "        processor = CorticalTextProcessor(config=config)",
        "        processor.process_document(\"doc1\", \"neural network deep learning models\")",
        "        processor.compute_all(verbose=False)",
        "",
        "        # When no max_expansions specified, should use config value",
        "        result = processor.expand_query(\"neural\")",
        "        # Should respect the config limit (may have fewer if not enough expansions)",
        "        self.assertIsInstance(result, dict)",
        "",
        "    def test_config_preserved_on_save_load(self):",
        "        \"\"\"Test that config is preserved after save/load.\"\"\"",
        "        import tempfile",
        "        import os",
        "        from cortical.processor import CorticalTextProcessor",
        "",
        "        # Create processor with custom config",
        "        config = CorticalConfig(",
        "            min_cluster_size=5,",
        "            chunk_size=256,",
        "            max_query_expansions=15",
        "        )",
        "        processor = CorticalTextProcessor(config=config)",
        "        processor.process_document(\"doc1\", \"test content\")",
        "        processor.compute_all(verbose=False)",
        "",
        "        # Save and load",
        "        with tempfile.NamedTemporaryFile(suffix='.pkl', delete=False) as f:",
        "            temp_path = f.name",
        "",
        "        try:",
        "            processor.save(temp_path, verbose=False)",
        "            loaded = CorticalTextProcessor.load(temp_path, verbose=False)",
        "",
        "            # Config should be preserved",
        "            self.assertEqual(loaded.config.min_cluster_size, 5)",
        "            self.assertEqual(loaded.config.chunk_size, 256)",
        "            self.assertEqual(loaded.config.max_query_expansions, 15)",
        "        finally:",
        "            os.unlink(temp_path)",
        "",
        "    def test_load_without_config_uses_default(self):",
        "        \"\"\"Test that loading old files without config uses defaults.\"\"\"",
        "        import tempfile",
        "        import os",
        "        from cortical.processor import CorticalTextProcessor",
        "",
        "        # Create processor (with default config)",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(\"doc1\", \"test content\")",
        "        processor.compute_all(verbose=False)",
        "",
        "        with tempfile.NamedTemporaryFile(suffix='.pkl', delete=False) as f:",
        "            temp_path = f.name",
        "",
        "        try:",
        "            processor.save(temp_path, verbose=False)",
        "            loaded = CorticalTextProcessor.load(temp_path, verbose=False)",
        "",
        "            # Should have valid config (either restored or default)",
        "            self.assertIsNotNone(loaded.config)",
        "            self.assertIsInstance(loaded.config, CorticalConfig)",
        "        finally:",
        "            os.unlink(temp_path)",
        "",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "        \"\"\"Test that association chains have medium validity scores.\"\"\"",
        "        score = VALID_RELATION_CHAINS[('RelatedTo', 'RelatedTo')]",
        "        self.assertGreater(score, 0.3)",
        "        self.assertLess(score, 0.8)",
        "",
        "    def test_default_chain_validity(self):",
        "        \"\"\"Test DEFAULT_CHAIN_VALIDITY value.\"\"\"",
        "        self.assertEqual(DEFAULT_CHAIN_VALIDITY, 0.4)",
        "",
        ""
      ],
      "context_after": [
        "if __name__ == '__main__':",
        "    unittest.main()"
      ],
      "change_type": "add"
    }
  ],
  "hour_of_day": 0,
  "day_of_week": "Friday",
  "seconds_since_last_commit": -307455,
  "is_merge": false,
  "is_initial": false,
  "parent_count": 1,
  "session_id": null,
  "related_chats": [],
  "ci_result": null,
  "reverted": false,
  "amended": false
}