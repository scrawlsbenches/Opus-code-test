{
  "hash": "16c13a057f3c0dc9f409dd08c93e395fb17a663d",
  "message": "Document magic numbers and extract query expansion helper",
  "author": "Claude",
  "timestamp": "2025-12-10 05:58:03 +0000",
  "branch": "claude/multi-index-design-DvifZ",
  "files_changed": [
    "TASK_LIST.md",
    "cortical/gaps.py",
    "cortical/query.py"
  ],
  "insertions": 170,
  "deletions": 105,
  "hunks": [
    {
      "file": "TASK_LIST.md",
      "function": "processor.add_documents_batch(docs, recompute='full')",
      "start_line": 437,
      "lines_added": [
        "**Status:** [x] Completed",
        "**Solution Applied:**",
        "Added documented constants at module level with detailed explanations:",
        "- `ISOLATION_THRESHOLD = 0.02` - Documents below this avg cosine similarity are isolated",
        "- `WELL_CONNECTED_THRESHOLD = 0.03` - Documents above this are well-integrated",
        "- `WEAK_TOPIC_TFIDF_THRESHOLD = 0.005` - Terms above this TF-IDF are significant topics",
        "- `BRIDGE_SIMILARITY_MIN = 0.005` and `BRIDGE_SIMILARITY_MAX = 0.03` - Range for bridging candidates",
        "Each constant includes documentation of typical ranges and usage guidance."
      ],
      "lines_removed": [
        "**Status:** [ ] Deferred (carried over)",
        "**Magic Numbers:**",
        "- `avg_sim < 0.02` - isolation threshold",
        "- `tfidf > 0.005` - weak topic threshold",
        "- `0.005 < sim < 0.03` - bridge opportunity range",
        "**Implementation:** Add docstrings or make configurable parameters."
      ],
      "context_before": [
        "```",
        "",
        "---",
        "",
        "## RAG Low Priority",
        "",
        "### 16. Document Magic Numbers in Gap Detection",
        "",
        "**File:** `cortical/gaps.py`",
        "**Lines:** 62, 76, 99"
      ],
      "context_after": [
        "",
        "",
        "",
        "---",
        "",
        "### 17. Add Multi-Stage Ranking Pipeline",
        "",
        "**Files:** `cortical/query.py`, `cortical/processor.py`",
        "**Status:** [x] Completed",
        "",
        "**Problem:**",
        "Current ranking is flat (Token TF-IDF → Document Score). Better RAG performance with staged ranking."
      ],
      "change_type": "modify"
    },
    {
      "file": "TASK_LIST.md",
      "function": "Semantic bonus is capped at 50% boost (`min(avg_semantic, 0.5)`). This is a reas",
      "start_line": 1167,
      "lines_added": [
        "| Low | Document magic numbers | ✅ Completed | Documentation |",
        "**Bug Fix Completion:** 8/8 tasks (100%)",
        "## Code Quality Improvements (2025-12-10)",
        "",
        "### Query Expansion Helper Refactoring",
        "",
        "**File:** `cortical/query.py`",
        "**Status:** [x] Completed",
        "",
        "**Problem:**",
        "Query expansion logic (expand + semantic merge) was duplicated in 6 functions:",
        "- `find_documents_for_query()`",
        "- `find_passages_for_query()`",
        "- `find_documents_batch()`",
        "- `find_passages_batch()`",
        "- `multi_stage_rank()`",
        "- `multi_stage_rank_documents()`",
        "",
        "**Solution Applied:**",
        "Added `get_expanded_query_terms()` helper function (~60 lines) that consolidates:",
        "- Lateral connection expansion via `expand_query()`",
        "- Semantic relation expansion via `expand_query_semantic()`",
        "- Merging of expansion results with appropriate weighting",
        "- Configurable parameters: `max_expansions`, `semantic_discount`",
        "",
        "All 6 functions now use this helper, reducing code duplication by ~100 lines.",
        "",
        "---",
        ""
      ],
      "lines_removed": [
        "| Low | Document magic numbers | ⏳ Deferred | Documentation |",
        "**Bug Fix Completion:** 7/7 tasks (100%)"
      ],
      "context_before": [
        "| Medium | Add verbose parameter | ✅ Completed | Bug Fix |",
        "| Low | Add test coverage | ✅ Completed | Bug Fix |",
        "| **Critical** | **Implement chunk-level retrieval** | ✅ Completed | **RAG** |",
        "| **Critical** | **Add document metadata support** | ✅ Completed | **RAG** |",
        "| **High** | **Activate Layer 2 concepts** | ✅ Completed | **RAG** |",
        "| **High** | **Integrate semantic relations** | ✅ Completed | **RAG** |",
        "| **High** | **Persist full computed state** | ✅ Completed | **RAG** |",
        "| Medium | Fix type annotation (embeddings.py) | ✅ Completed | Bug Fix |",
        "| Medium | Optimize spectral embeddings | ✅ Completed | Performance |",
        "| Medium | Add incremental indexing | ✅ Completed | RAG |"
      ],
      "context_after": [
        "| Low | Multi-stage ranking pipeline | ✅ Completed | RAG |",
        "| Low | Batch query API | ✅ Completed | RAG |",
        "| **Critical** | **Build cross-layer feedforward connections** | ✅ Completed | **ConceptNet** |",
        "| **Critical** | **Add concept-level lateral connections** | ✅ Completed | **ConceptNet** |",
        "| **Critical** | **Add bigram lateral connections** | ✅ Completed | **ConceptNet** |",
        "| **High** | **Implement relation-weighted PageRank** | ✅ Completed | **ConceptNet** |",
        "| **High** | **Implement cross-layer PageRank propagation** | ✅ Completed | **ConceptNet** |",
        "| **High** | **Add typed edge storage** | ✅ Completed | **ConceptNet** |",
        "| Medium | Implement multi-hop semantic inference | ✅ Completed | ConceptNet |",
        "| Medium | Add relation path scoring | ✅ Completed | ConceptNet |",
        "| Medium | Implement concept inheritance | ✅ Completed | ConceptNet |",
        "| Low | Add commonsense relation extraction | ✅ Completed | ConceptNet |",
        "| Low | Visualize ConceptNet-style graph | ✅ Completed | ConceptNet |",
        "| Low | Add analogy completion | ✅ Completed | ConceptNet |",
        "",
        "**RAG Enhancement Completion:** 8/8 tasks (100%)",
        "**ConceptNet Enhancement Completion:** 12/12 tasks (100%)",
        "",
        "---",
        "",
        "## Test Results",
        "",
        "```",
        "Ran 321 tests in 0.280s",
        "OK",
        "```",
        "",
        "All tests passing as of 2025-12-10.",
        "",
        "---",
        "",
        "*Updated from code review on 2025-12-10*"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/gaps.py",
      "function": "Identifies:",
      "start_line": 12,
      "lines_added": [
        "# =============================================================================",
        "# Gap Detection Thresholds",
        "# =============================================================================",
        "# These thresholds were empirically determined for typical text corpora.",
        "# They may need adjustment for specialized domains or very small/large corpora.",
        "",
        "# Isolation threshold: Documents with average cosine similarity below this value",
        "# are considered \"isolated\" from the rest of the corpus. Value of 0.02 means",
        "# the document shares very little vocabulary overlap with other documents.",
        "# Typical range: 0.01 (very strict) to 0.05 (more lenient)",
        "ISOLATION_THRESHOLD = 0.02",
        "",
        "# Well-connected threshold: Documents with average similarity above this value",
        "# are considered well-integrated into the corpus.",
        "# Typical range: 0.02 to 0.05",
        "WELL_CONNECTED_THRESHOLD = 0.03",
        "",
        "# Weak topic TF-IDF threshold: Terms with TF-IDF above this value are considered",
        "# significant enough to represent a \"topic\". Lower values include more common terms.",
        "# Typical range: 0.001 (include more terms) to 0.01 (only distinctive terms)",
        "WEAK_TOPIC_TFIDF_THRESHOLD = 0.005",
        "",
        "# Bridge opportunity range: Document pairs with similarity in this range are",
        "# candidates for \"bridging\" - they share some vocabulary but aren't closely related.",
        "# Too similar (>0.03) means they're already well-connected; too dissimilar (<0.005)",
        "# means there's no common ground to bridge.",
        "BRIDGE_SIMILARITY_MIN = 0.005",
        "BRIDGE_SIMILARITY_MAX = 0.03",
        "",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "\"\"\"",
        "",
        "import math",
        "from typing import Dict, List, Tuple, Set, Optional",
        "from collections import defaultdict",
        "",
        "from .layers import CorticalLayer, HierarchicalLayer",
        "from .analysis import cosine_similarity",
        "",
        ""
      ],
      "context_after": [
        "def analyze_knowledge_gaps(",
        "    layers: Dict[CorticalLayer, HierarchicalLayer],",
        "    documents: Dict[str, str]",
        ") -> Dict:",
        "    \"\"\"",
        "    Analyze the corpus to identify potential knowledge gaps.",
        "    ",
        "    Args:",
        "        layers: Dictionary of layers",
        "        documents: Dictionary of documents"
      ],
      "change_type": "add"
    },
    {
      "file": "cortical/gaps.py",
      "function": "def analyze_knowledge_gaps(",
      "start_line": 52,
      "lines_added": [
        "        if avg_sim < ISOLATION_THRESHOLD:",
        "        if col.tfidf > WEAK_TOPIC_TFIDF_THRESHOLD and 1 <= doc_count <= 2:"
      ],
      "lines_removed": [
        "        if avg_sim < 0.02:",
        "        if col.tfidf > 0.005 and 1 <= doc_count <= 2:"
      ],
      "context_before": [
        "                other_vector = {col.content: col.tfidf_per_doc[other_id]",
        "                               for col in layer0.minicolumns.values()",
        "                               if other_id in col.tfidf_per_doc}",
        "                sim = cosine_similarity(doc_vector, other_vector)",
        "                similarities.append((other_id, sim))",
        "        ",
        "        avg_sim = sum(s for _, s in similarities) / len(similarities) if similarities else 0",
        "        max_sim = max((s for _, s in similarities), default=0)",
        "        doc_similarities[doc_id] = {'avg': avg_sim, 'max': max_sim}",
        "        "
      ],
      "context_after": [
        "            isolated_docs.append({",
        "                'doc_id': doc_id,",
        "                'avg_similarity': avg_sim,",
        "                'max_similarity': max_sim,",
        "                'most_similar': max(similarities, key=lambda x: x[1])[0] if similarities else None",
        "            })",
        "    ",
        "    isolated_docs.sort(key=lambda x: x['avg_similarity'])",
        "    ",
        "    # 2. Find weakly covered topics",
        "    weak_topics = []",
        "    for col in layer0.minicolumns.values():",
        "        doc_count = len(col.document_ids)",
        "            weak_topics.append({",
        "                'term': col.content,",
        "                'tfidf': col.tfidf,",
        "                'doc_count': doc_count,",
        "                'documents': list(col.document_ids),",
        "                'pagerank': col.pagerank",
        "            })",
        "    weak_topics.sort(key=lambda x: x['tfidf'] * x['pagerank'], reverse=True)",
        "    ",
        "    # 3. Find bridge opportunities"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/gaps.py",
      "function": "def analyze_knowledge_gaps(",
      "start_line": 89,
      "lines_added": [
        "            if BRIDGE_SIMILARITY_MIN < sim < BRIDGE_SIMILARITY_MAX:"
      ],
      "lines_removed": [
        "            if 0.005 < sim < 0.03:"
      ],
      "context_before": [
        "        vec1 = {col.content: col.tfidf_per_doc[doc1] ",
        "               for col in layer0.minicolumns.values() ",
        "               if doc1 in col.tfidf_per_doc}",
        "        ",
        "        for doc2 in doc_ids[i+1:]:",
        "            vec2 = {col.content: col.tfidf_per_doc[doc2]",
        "                   for col in layer0.minicolumns.values()",
        "                   if doc2 in col.tfidf_per_doc}",
        "            ",
        "            sim = cosine_similarity(vec1, vec2)"
      ],
      "context_after": [
        "                shared = set(vec1.keys()) & set(vec2.keys())",
        "                bridge_opportunities.append({",
        "                    'doc1': doc1,",
        "                    'doc2': doc2,",
        "                    'similarity': sim,",
        "                    'shared_terms': list(shared)[:5]",
        "                })",
        "    ",
        "    bridge_opportunities.sort(key=lambda x: x['similarity'], reverse=True)",
        "    "
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/gaps.py",
      "function": "def analyze_knowledge_gaps(",
      "start_line": 118,
      "lines_added": [
        "    isolated_count = len([d for d in doc_similarities.values() if d['avg'] < ISOLATION_THRESHOLD])",
        "    well_connected = len([d for d in doc_similarities.values() if d['avg'] >= WELL_CONNECTED_THRESHOLD])"
      ],
      "lines_removed": [
        "    isolated_count = len([d for d in doc_similarities.values() if d['avg'] < 0.02])",
        "    well_connected = len([d for d in doc_similarities.values() if d['avg'] >= 0.03])"
      ],
      "context_before": [
        "                connector_terms.append({",
        "                    'term': col.content,",
        "                    'bridges_isolated': list(in_isolated),",
        "                    'connects_to': list(in_connected)[:3],",
        "                    'pagerank': col.pagerank",
        "                })",
        "    connector_terms.sort(key=lambda x: len(x['bridges_isolated']), reverse=True)",
        "    ",
        "    # 5. Coverage metrics",
        "    total_docs = len(doc_ids)"
      ],
      "context_after": [
        "    coverage_score = well_connected / total_docs if total_docs > 0 else 0",
        "    ",
        "    all_avg_sims = [d['avg'] for d in doc_similarities.values()]",
        "    connectivity_score = sum(all_avg_sims) / len(all_avg_sims) if all_avg_sims else 0",
        "    ",
        "    return {",
        "        'isolated_documents': isolated_docs[:10],",
        "        'weak_topics': weak_topics[:10],",
        "        'bridge_opportunities': bridge_opportunities[:10],",
        "        'connector_terms': connector_terms[:10],"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/query.py",
      "function": "def expand_query_semantic(",
      "start_line": 354,
      "lines_added": [
        "def get_expanded_query_terms(",
        "    use_semantic: bool = True,",
        "    max_expansions: int = 5,",
        "    semantic_discount: float = 0.8",
        ") -> Dict[str, float]:",
        "    Get expanded query terms with optional semantic expansion.",
        "",
        "    This is a helper function that consolidates query expansion logic used",
        "    by multiple search functions. It handles:",
        "    - Lateral connection expansion via expand_query()",
        "    - Semantic relation expansion via expand_query_semantic()",
        "    - Merging of expansion results with appropriate weighting",
        "        query_text: Original query string",
        "        use_semantic: Whether to use semantic relations for expansion",
        "        max_expansions: Maximum expansion terms per method (default 5)",
        "        semantic_discount: Weight multiplier for semantic expansions (default 0.8)",
        "        Dict mapping terms to weights (original terms get weight 1.0,",
        "        expansions get lower weights based on connection strength)",
        "    Example:",
        "        >>> terms = get_expanded_query_terms(\"neural networks\", layers, tokenizer)",
        "        >>> # Returns: {'neural': 1.0, 'networks': 1.0, 'deep': 0.3, 'learning': 0.25, ...}",
        "    \"\"\"",
        "        query_terms = expand_query(query_text, layers, tokenizer, max_expansions=max_expansions)",
        "                query_text, layers, tokenizer, semantic_relations, max_expansions=max_expansions",
        "                    query_terms[term] = weight * semantic_discount",
        "                    query_terms[term] = max(query_terms[term], weight * semantic_discount)",
        "    return query_terms",
        "",
        "",
        "def find_documents_for_query(",
        "    query_text: str,",
        "    layers: Dict[CorticalLayer, HierarchicalLayer],",
        "    tokenizer: Tokenizer,",
        "    top_n: int = 5,",
        "    use_expansion: bool = True,",
        "    semantic_relations: Optional[List[Tuple[str, str, str, float]]] = None,",
        "    use_semantic: bool = True",
        ") -> List[Tuple[str, float]]:",
        "    \"\"\"",
        "    Find documents most relevant to a query using TF-IDF and optional expansion.",
        "",
        "    Args:",
        "        query_text: Search query",
        "        layers: Dictionary of layers",
        "        tokenizer: Tokenizer instance",
        "        top_n: Number of documents to return",
        "        use_expansion: Whether to expand query terms using lateral connections",
        "        semantic_relations: Optional list of semantic relations for expansion",
        "        use_semantic: Whether to use semantic relations for expansion (if available)",
        "",
        "    Returns:",
        "        List of (doc_id, score) tuples ranked by relevance",
        "    \"\"\"",
        "    layer0 = layers[CorticalLayer.TOKENS]",
        "",
        "    query_terms = get_expanded_query_terms(",
        "        query_text, layers, tokenizer,",
        "        use_expansion=use_expansion,",
        "        semantic_relations=semantic_relations,",
        "        use_semantic=use_semantic",
        "    )",
        ""
      ],
      "lines_removed": [
        "def find_documents_for_query(",
        "    top_n: int = 5,",
        "    use_semantic: bool = True",
        ") -> List[Tuple[str, float]]:",
        "    Find documents most relevant to a query using TF-IDF and optional expansion.",
        "        query_text: Search query",
        "        top_n: Number of documents to return",
        "        use_semantic: Whether to use semantic relations for expansion (if available)",
        "        List of (doc_id, score) tuples ranked by relevance",
        "    \"\"\"",
        "    layer0 = layers[CorticalLayer.TOKENS]",
        "        query_terms = expand_query(query_text, layers, tokenizer, max_expansions=5)",
        "                query_text, layers, tokenizer, semantic_relations, max_expansions=5",
        "                    query_terms[term] = weight * 0.8  # Slightly discount semantic expansions",
        "                    query_terms[term] = max(query_terms[term], weight * 0.8)"
      ],
      "context_before": [
        "                candidates[neighbor] = max(candidates[neighbor], weight * 0.7)",
        "    ",
        "    # Take top candidates",
        "    sorted_candidates = sorted(candidates.items(), key=lambda x: x[1], reverse=True)",
        "    for term, score in sorted_candidates[:max_expansions]:",
        "        expanded[term] = score",
        "    ",
        "    return expanded",
        "",
        ""
      ],
      "context_after": [
        "    query_text: str,",
        "    layers: Dict[CorticalLayer, HierarchicalLayer],",
        "    tokenizer: Tokenizer,",
        "    use_expansion: bool = True,",
        "    semantic_relations: Optional[List[Tuple[str, str, str, float]]] = None,",
        "    \"\"\"",
        "",
        "    Args:",
        "        layers: Dictionary of layers",
        "        tokenizer: Tokenizer instance",
        "        use_expansion: Whether to expand query terms using lateral connections",
        "        semantic_relations: Optional list of semantic relations for expansion",
        "",
        "    Returns:",
        "",
        "    if use_expansion:",
        "        # Start with lateral connection expansion",
        "",
        "        # Add semantic expansion if available",
        "        if use_semantic and semantic_relations:",
        "            semantic_terms = expand_query_semantic(",
        "            )",
        "            # Merge semantic expansions (don't override stronger weights)",
        "            for term, weight in semantic_terms.items():",
        "                if term not in query_terms:",
        "                else:",
        "                    # Take the max weight",
        "    else:",
        "        tokens = tokenizer.tokenize(query_text)",
        "        query_terms = {t: 1.0 for t in tokens}",
        "",
        "    # Score each document",
        "    doc_scores: Dict[str, float] = defaultdict(float)",
        "",
        "    for term, term_weight in query_terms.items():",
        "        col = layer0.get_minicolumn(term)",
        "        if col:",
        "            for doc_id in col.document_ids:",
        "                tfidf = col.tfidf_per_doc.get(doc_id, col.tfidf)",
        "                doc_scores[doc_id] += tfidf * term_weight",
        ""
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/query.py",
      "function": "def find_passages_for_query(",
      "start_line": 615,
      "lines_added": [
        "    query_terms = get_expanded_query_terms(",
        "        query_text, layers, tokenizer,",
        "        use_expansion=use_expansion,",
        "        semantic_relations=semantic_relations,",
        "        use_semantic=use_semantic",
        "    )"
      ],
      "lines_removed": [
        "    if use_expansion:",
        "        query_terms = expand_query(query_text, layers, tokenizer, max_expansions=5)",
        "        # Add semantic expansion if available",
        "        if use_semantic and semantic_relations:",
        "            semantic_terms = expand_query_semantic(",
        "                query_text, layers, tokenizer, semantic_relations, max_expansions=5",
        "            )",
        "            for term, weight in semantic_terms.items():",
        "                if term not in query_terms:",
        "                    query_terms[term] = weight * 0.8",
        "                else:",
        "                    query_terms[term] = max(query_terms[term], weight * 0.8)",
        "    else:",
        "        tokens = tokenizer.tokenize(query_text)",
        "        query_terms = {t: 1.0 for t in tokens}"
      ],
      "context_before": [
        "        semantic_relations: Optional list of semantic relations for expansion",
        "        use_semantic: Whether to use semantic relations for expansion (if available)",
        "",
        "    Returns:",
        "        List of (passage_text, doc_id, start_char, end_char, score) tuples",
        "        ranked by relevance",
        "    \"\"\"",
        "    layer0 = layers[CorticalLayer.TOKENS]",
        "",
        "    # Get expanded query terms"
      ],
      "context_after": [
        "",
        "    if not query_terms:",
        "        return []",
        "",
        "    # First, get candidate documents (more than we need, since we'll rank passages)",
        "    doc_scores = find_documents_for_query(",
        "        query_text, layers, tokenizer,",
        "        top_n=min(len(documents), top_n * 3),",
        "        use_expansion=use_expansion,",
        "        semantic_relations=semantic_relations,"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/query.py",
      "function": "def find_documents_batch(",
      "start_line": 719,
      "lines_added": [
        "        if query_text in expansion_cache:",
        "            query_terms = expansion_cache[query_text]",
        "            query_terms = get_expanded_query_terms(",
        "                query_text, layers, tokenizer,",
        "                use_expansion=use_expansion,",
        "                semantic_relations=semantic_relations,",
        "                use_semantic=use_semantic",
        "            )",
        "            expansion_cache[query_text] = query_terms"
      ],
      "lines_removed": [
        "        if use_expansion:",
        "            if query_text in expansion_cache:",
        "                query_terms = expansion_cache[query_text]",
        "            else:",
        "                query_terms = expand_query(query_text, layers, tokenizer, max_expansions=5)",
        "                if use_semantic and semantic_relations:",
        "                    semantic_terms = expand_query_semantic(",
        "                        query_text, layers, tokenizer, semantic_relations, max_expansions=5",
        "                    )",
        "                    for term, weight in semantic_terms.items():",
        "                        if term not in query_terms:",
        "                            query_terms[term] = weight * 0.8",
        "                        else:",
        "                            query_terms[term] = max(query_terms[term], weight * 0.8)",
        "                expansion_cache[query_text] = query_terms",
        "            tokens = tokenizer.tokenize(query_text)",
        "            query_terms = {t: 1.0 for t in tokens}"
      ],
      "context_before": [
        "    \"\"\"",
        "    layer0 = layers[CorticalLayer.TOKENS]",
        "",
        "    # Cache for expanded query terms to avoid redundant computation",
        "    expansion_cache: Dict[str, Dict[str, float]] = {}",
        "",
        "    all_results: List[List[Tuple[str, float]]] = []",
        "",
        "    for query_text in queries:",
        "        # Check cache first for expansion"
      ],
      "context_after": [
        "        else:",
        "",
        "        # Score documents",
        "        doc_scores: Dict[str, float] = defaultdict(float)",
        "        for term, term_weight in query_terms.items():",
        "            col = layer0.get_minicolumn(term)",
        "            if col:",
        "                for doc_id in col.document_ids:",
        "                    tfidf = col.tfidf_per_doc.get(doc_id, col.tfidf)",
        "                    doc_scores[doc_id] += tfidf * term_weight",
        ""
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/query.py",
      "function": "def find_passages_batch(",
      "start_line": 811,
      "lines_added": [
        "        if query_text in expansion_cache:",
        "            query_terms = expansion_cache[query_text]",
        "            query_terms = get_expanded_query_terms(",
        "                query_text, layers, tokenizer,",
        "                use_expansion=use_expansion,",
        "                semantic_relations=semantic_relations,",
        "                use_semantic=use_semantic",
        "            )",
        "            expansion_cache[query_text] = query_terms"
      ],
      "lines_removed": [
        "        if use_expansion:",
        "            if query_text in expansion_cache:",
        "                query_terms = expansion_cache[query_text]",
        "            else:",
        "                query_terms = expand_query(query_text, layers, tokenizer, max_expansions=5)",
        "                if use_semantic and semantic_relations:",
        "                    semantic_terms = expand_query_semantic(",
        "                        query_text, layers, tokenizer, semantic_relations, max_expansions=5",
        "                    )",
        "                    for term, weight in semantic_terms.items():",
        "                        if term not in query_terms:",
        "                            query_terms[term] = weight * 0.8",
        "                        else:",
        "                            query_terms[term] = max(query_terms[term], weight * 0.8)",
        "                expansion_cache[query_text] = query_terms",
        "            tokens = tokenizer.tokenize(query_text)",
        "            query_terms = {t: 1.0 for t in tokens}"
      ],
      "context_before": [
        "            continue",
        "        doc_chunks_cache[doc_id] = create_chunks(text, chunk_size, overlap)",
        "",
        "    # Cache for expanded query terms",
        "    expansion_cache: Dict[str, Dict[str, float]] = {}",
        "",
        "    all_results: List[List[Tuple[str, str, int, int, float]]] = []",
        "",
        "    for query_text in queries:",
        "        # Get expanded query terms (with caching)"
      ],
      "context_after": [
        "        else:",
        "",
        "        if not query_terms:",
        "            all_results.append([])",
        "            continue",
        "",
        "        # Get candidate documents",
        "        doc_scores = find_documents_for_query(",
        "            query_text, layers, tokenizer,",
        "            top_n=min(len(documents), top_n * 3),",
        "            use_expansion=use_expansion,"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/query.py",
      "function": "def multi_stage_rank(",
      "start_line": 962,
      "lines_added": [
        "    query_terms = get_expanded_query_terms(",
        "        query_text, layers, tokenizer,",
        "        use_expansion=use_expansion,",
        "        semantic_relations=semantic_relations,",
        "        use_semantic=use_semantic",
        "    )"
      ],
      "lines_removed": [
        "    if use_expansion:",
        "        query_terms = expand_query(query_text, layers, tokenizer, max_expansions=5)",
        "        if use_semantic and semantic_relations:",
        "            semantic_terms = expand_query_semantic(",
        "                query_text, layers, tokenizer, semantic_relations, max_expansions=5",
        "            )",
        "            for term, weight in semantic_terms.items():",
        "                if term not in query_terms:",
        "                    query_terms[term] = weight * 0.8",
        "                else:",
        "                    query_terms[term] = max(query_terms[term], weight * 0.8)",
        "    else:",
        "        tokens = tokenizer.tokenize(query_text)",
        "        query_terms = {t: 1.0 for t in tokens}"
      ],
      "context_before": [
        "        >>> results = multi_stage_rank(query, layers, tokenizer, documents)",
        "        >>> for passage, doc_id, start, end, score, stages in results:",
        "        ...     print(f\"[{doc_id}] Score: {score:.3f}\")",
        "        ...     print(f\"  Concept: {stages['concept_score']:.3f}\")",
        "        ...     print(f\"  Doc: {stages['doc_score']:.3f}\")",
        "        ...     print(f\"  Chunk: {stages['chunk_score']:.3f}\")",
        "    \"\"\"",
        "    layer0 = layers[CorticalLayer.TOKENS]",
        "",
        "    # Get expanded query terms"
      ],
      "context_after": [
        "",
        "    if not query_terms:",
        "        return []",
        "",
        "    # ========== STAGE 1: CONCEPTS ==========",
        "    # Find relevant concepts to identify topic areas",
        "    relevant_concepts = find_relevant_concepts(query_terms, layers, top_n=10)",
        "",
        "    # Build concept score per document",
        "    doc_concept_scores: Dict[str, float] = defaultdict(float)"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/query.py",
      "function": "def multi_stage_rank_documents(",
      "start_line": 1112,
      "lines_added": [
        "    query_terms = get_expanded_query_terms(",
        "        query_text, layers, tokenizer,",
        "        use_expansion=use_expansion,",
        "        semantic_relations=semantic_relations,",
        "        use_semantic=use_semantic",
        "    )"
      ],
      "lines_removed": [
        "    if use_expansion:",
        "        query_terms = expand_query(query_text, layers, tokenizer, max_expansions=5)",
        "        if use_semantic and semantic_relations:",
        "            semantic_terms = expand_query_semantic(",
        "                query_text, layers, tokenizer, semantic_relations, max_expansions=5",
        "            )",
        "            for term, weight in semantic_terms.items():",
        "                if term not in query_terms:",
        "                    query_terms[term] = weight * 0.8",
        "                else:",
        "                    query_terms[term] = max(query_terms[term], weight * 0.8)",
        "    else:",
        "        tokens = tokenizer.tokenize(query_text)",
        "        query_terms = {t: 1.0 for t in tokens}"
      ],
      "context_before": [
        "        semantic_relations: Optional list of semantic relations",
        "        use_semantic: Whether to use semantic relations",
        "",
        "    Returns:",
        "        List of (doc_id, final_score, stage_scores) tuples.",
        "        stage_scores dict contains: concept_score, tfidf_score, combined_score",
        "    \"\"\"",
        "    layer0 = layers[CorticalLayer.TOKENS]",
        "",
        "    # Get expanded query terms"
      ],
      "context_after": [
        "",
        "    if not query_terms:",
        "        return []",
        "",
        "    # Stage 1: Concepts",
        "    relevant_concepts = find_relevant_concepts(query_terms, layers, top_n=10)",
        "",
        "    doc_concept_scores: Dict[str, float] = defaultdict(float)",
        "    if relevant_concepts:",
        "        max_concept_score = max(score for _, score, _ in relevant_concepts) if relevant_concepts else 1.0"
      ],
      "change_type": "modify"
    }
  ],
  "hour_of_day": 5,
  "day_of_week": "Wednesday",
  "seconds_since_last_commit": -460005,
  "is_merge": false,
  "is_initial": false,
  "parent_count": 1,
  "session_id": null,
  "related_chats": [],
  "ci_result": null,
  "reverted": false,
  "amended": false
}