{
  "hash": "2d600d1bc4197d82b8e7b21175b1c93cbf119826",
  "message": "Merge pull request #44 from scrawlsbenches/claude/run-showcase-review-01AtQtjQRodvc3NGnpEzaQhb",
  "author": "scrawlsbenches",
  "timestamp": "2025-12-11 21:03:40 -0500",
  "branch": "claude/multi-index-design-DvifZ",
  "files_changed": [
    "TASK_LIST.md",
    "cortical/analysis.py",
    "cortical/embeddings.py",
    "cortical/processor.py",
    "cortical/query/search.py",
    "cortical/tokenizer.py",
    "showcase.py"
  ],
  "insertions": 659,
  "deletions": 76,
  "hunks": [
    {
      "file": "TASK_LIST.md",
      "function": null,
      "start_line": 1,
      "lines_added": [
        "**Pending Tasks:** 35",
        "**Completed Tasks:** 94+ (see archive)",
        "| # | Task | Category | Depends | Effort |",
        "|---|------|----------|---------|--------|",
        "| 146 | Create behavioral tests for core user workflows | Testing | - | Medium |"
      ],
      "lines_removed": [
        "**Pending Tasks:** 33",
        "**Completed Tasks:** 91+ (see archive)",
        "*All high-priority tasks completed!*"
      ],
      "context_before": [
        "# Task List: Cortical Text Processor",
        "",
        "Active backlog for the Cortical Text Processor project. Completed tasks are archived in [TASK_ARCHIVE.md](TASK_ARCHIVE.md).",
        "",
        "**Last Updated:** 2025-12-12"
      ],
      "context_after": [
        "",
        "---",
        "",
        "## Active Backlog",
        "",
        "<!-- Machine-parseable format for automation -->",
        "",
        "### ðŸ”´ Critical (Do Now)",
        "",
        "*All critical tasks completed!*",
        "",
        "### ðŸŸ  High (Do This Week)",
        "",
        "",
        "### ðŸŸ¡ Medium (Do This Month)",
        "",
        "| # | Task | Category | Depends | Effort |",
        "|---|------|----------|---------|--------|",
        "| 137 | Cap bigram connections to top-K per bigram | Perf | - | Small |",
        "| 138 | Use sparse matrix multiplication for bigram connections | Perf | - | Medium |",
        "| 139 | Batch bigram connection updates to reduce dict overhead | Perf | - | Small |",
        "| 133 | Implement WAL + snapshot persistence (fault-tolerant rebuild) | Arch | 132 | Large |",
        "| 134 | Implement protobuf serialization for corpus | Arch | 132 | Medium |"
      ],
      "change_type": "modify"
    },
    {
      "file": "TASK_LIST.md",
      "function": "Active backlog for the Cortical Text Processor project. Completed tasks are arch",
      "start_line": 83,
      "lines_added": [
        "| 145 | Improve graph embedding quality for common terms | 2025-12-12 | Added 'tfidf' method, IDF weighting to 'fast' method |",
        "| 143 | Investigate negative silhouette score in clustering | 2025-12-12 | Expected behavior: modularity â‰  silhouette (graph vs doc similarity) |",
        "| 142 | Investigate 74s compute_all() performance regression | 2025-12-12 | 5.2x speedup via fast embeddings + sampling (74s â†’ 14s) |",
        "| 144 | Boost exact document name matches in search | 2025-12-12 | doc_name_boost parameter in search functions |",
        "| 141 | Filter Python keywords/artifacts from analysis | 2025-12-12 | CODE_NOISE_TOKENS + filter_code_noise tokenizer option |"
      ],
      "lines_removed": [],
      "context_before": [
        "| # | Task | Started | Notes |",
        "|---|------|---------|-------|",
        "| 87 | Add Python code samples and showcase | 2025-12-11 | samples/*.py created |",
        "",
        "---",
        "",
        "## Recently Completed (Last 7 Days)",
        "",
        "| # | Task | Completed | Notes |",
        "|---|------|-----------|-------|"
      ],
      "context_after": [
        "| 94 | Split query.py into focused modules | 2025-12-12 | 8 modules: expansion, search, passages, chunking, intent, definitions, ranking, analogy |",
        "| 97 | Integrate CorticalConfig into processor | 2025-12-11 | Config stored on processor, used in method defaults, saved/loaded |",
        "| 127 | Create cluster coverage evaluation script | 2025-12-11 | scripts/evaluate_cluster.py with 24 tests |",
        "| 125 | Add clustering quality metrics (modularity, silhouette) | 2025-12-11 | compute_clustering_quality() in analysis.py, showcase display |",
        "| 124 | Add minimum cluster count regression tests | 2025-12-11 | 4 new tests: coherence, showcase count, mega-cluster, distribution |",
        "| 128 | Fix definition boost that favors test mocks over real implementations | 2025-12-11 | Added is_test_file() and test file penalty |",
        "| 132 | Profile full-analysis bottleneck (bigram, semantics O(nÂ²)) | 2025-12-11 | Created profile_full_analysis.py, fixed bottlenecks |",
        "| 136 | Optimize semantics O(nÂ²) similarity with early termination | 2025-12-11 | Added max_similarity_pairs, min_context_keys |",
        "| 126 | Investigate optimal Louvain resolution for sample corpus | 2025-12-11 | Research confirms default 1.0 is optimal |",
        "| 123 | Replace label propagation with Louvain community detection | 2025-12-11 | Implemented Louvain algorithm, 34 clusters for 92 docs |"
      ],
      "change_type": "add"
    },
    {
      "file": "TASK_LIST.md",
      "function": "queries = [",
      "start_line": 1294,
      "lines_added": [
        "### 141. Filter Python Keywords/Artifacts from Analysis âœ…",
        "",
        "**Meta:** `status:completed` `priority:high` `category:quality`",
        "**Files:** `cortical/tokenizer.py`, `showcase.py`",
        "**Effort:** Medium",
        "**Completed:** 2025-12-12",
        "",
        "**Problem (Dog-fooding 2025-12-12):** When Python code files are added to the corpus (samples/data_processor.py, etc.), Python keywords pollute the analysis:",
        "- \"self\" is #2 in PageRank (0.0038) - meaningless for content analysis",
        "- TF-IDF top terms: `assertequal`, `def`, `mockdatarecord`, `str`, `doc_id`",
        "- These artifacts drown out meaningful content terms",
        "",
        "**Solution Applied:**",
        "1. Added `CODE_NOISE_TOKENS` constant with common Python/test tokens",
        "2. Added `filter_code_noise` parameter to Tokenizer",
        "3. showcase.py now uses `filter_code_noise=True` by default",
        "",
        "**Acceptance Criteria:**",
        "- [x] \"self\", \"def\", \"str\" not in top 20 PageRank terms when code files present",
        "- [x] TF-IDF surfaces meaningful terms, not syntax",
        "- [x] Existing code search features still work (1121 tests pass)",
        "",
        "---",
        "",
        "### 142. Investigate 74s compute_all() Performance Regression âœ…",
        "",
        "**Meta:** `status:completed` `priority:high` `category:perf`",
        "**Files:** `cortical/processor.py`, `cortical/embeddings.py`",
        "**Effort:** Medium",
        "**Completed:** 2025-12-12",
        "",
        "**Problem (Dog-fooding 2025-12-12):** compute_all() took 74.28s for 109 documents.",
        "",
        "**Root Cause Found:** `compute_graph_embeddings()` took 58s+ using adjacency method with multi-hop propagation on 7268 tokens.",
        "",
        "**Solution Applied:**",
        "1. Added `fast` embedding method (direct adjacency, no multi-hop)",
        "2. Added `max_terms` parameter for sampling top-N by PageRank",
        "3. Auto-selects sampling: <2000 tokens = all, 2000-5000 = 1500, >5000 = 1000",
        "4. Changed default method from 'adjacency' to 'fast'",
        "",
        "**Results:**",
        "- compute_graph_embeddings: 58s â†’ 0.01s",
        "- compute_all: 74.28s â†’ 14.21s (5.2x speedup)",
        "",
        "**Acceptance Criteria:**",
        "- [x] Identify root cause with profiling data",
        "- [x] compute_all() under 30s for 109 documents (achieved 14.21s)",
        "- [x] Document findings",
        "",
        "---",
        "",
        "### 143. Investigate Negative Silhouette Score in Clustering âœ…",
        "",
        "**Meta:** `status:completed` `priority:high` `category:quality`",
        "**Files:** `cortical/analysis.py`, `cortical/processor.py`",
        "**Effort:** Medium",
        "**Completed:** 2025-12-12",
        "",
        "**Problem (Dog-fooding 2025-12-12):** Clustering quality metrics show:",
        "- silhouette = -0.00 (negative indicates poor cluster separation)",
        "- Only 33 clusters for 109 documents",
        "- modularity = 0.40 (acceptable but not great)",
        "",
        "**Investigation Findings:**",
        "The negative silhouette score is **expected behavior**, not a bug. Key insights:",
        "",
        "1. **Modularity and silhouette measure different things:**",
        "   - Modularity: Graph edge density within clusters (what Louvain optimizes)",
        "   - Silhouette: Document co-occurrence similarity (semantic coherence)",
        "",
        "2. **Tokens that co-occur in sentences don't necessarily share documents:**",
        "   - Louvain groups tokens that appear together in sentences",
        "   - Silhouette measures if tokens appear in the same documents",
        "   - These are fundamentally different similarity metrics",
        "",
        "3. **Higher Louvain resolution makes silhouette worse:**",
        "   - Resolution 1.0: silhouette=-0.03",
        "   - Resolution 3.0: silhouette=-0.20",
        "   - More clusters = less document overlap per cluster",
        "",
        "**Solution Applied:**",
        "1. Changed silhouette to use document co-occurrence (Jaccard on document sets)",
        "2. Updated quality assessment to explain \"typical graph clustering\" for silhouette -0.1 to 0.1",
        "3. Updated docstrings to explain metric interpretation",
        "4. Added `_doc_similarity()` helper function",
        "",
        "**Results:**",
        "```",
        "34 clusters with Good community structure (modularity 0.37),",
        "typical graph clustering (silhouette -0.06), moderately balanced sizes",
        "```",
        "",
        "**Key Lesson:** Don't assume metrics must be positive. Understand what each metric measures before setting acceptance criteria.",
        "",
        "---",
        "",
        "### 144. Boost Exact Document Name Matches in Search âœ…",
        "",
        "**Meta:** `status:completed` `priority:high` `category:quality`",
        "**Files:** `cortical/query/search.py`",
        "**Effort:** Small",
        "**Completed:** 2025-12-12",
        "",
        "**Problem (Dog-fooding 2025-12-12):** Query \"distributed systems\" returns:",
        "1. unix_evolution (score: 19.804)",
        "2. database_design_patterns (score: 16.583)",
        "3. comprehensive_machine_learning (score: 13.244)",
        "",
        "But `distributed_systems` document exists and should rank #1 for this query.",
        "",
        "**Solution Applied:**",
        "Added `doc_name_boost` parameter (default 2.0) to `find_documents_for_query()` and `fast_find_documents()`. Boost is proportional to query term overlap with document ID.",
        "",
        "**Results:**",
        "- \"distributed systems\" â†’ `distributed_systems` now #1",
        "- \"fermentation\" â†’ `fermentation_science` still #1",
        "- \"quantum computing\" â†’ `quantum_computing_basics` now #1",
        "",
        "**Acceptance Criteria:**",
        "- [x] Query \"distributed systems\" returns `distributed_systems` in top 2",
        "- [x] Query \"fermentation\" keeps `fermentation_science` at #1",
        "- [x] No regression on other queries (1121 tests pass)",
        "",
        "---",
        "",
        "### 145. Improve Graph Embedding Quality for Common Terms âœ…",
        "",
        "**Meta:** `status:completed` `priority:high` `category:quality`",
        "**Files:** `cortical/embeddings.py`, `cortical/processor.py`",
        "**Effort:** Medium",
        "**Completed:** 2025-12-12",
        "",
        "**Problem (Dog-fooding 2025-12-12):** Graph embeddings show semantically odd similarities:",
        "- 'data' â†’ cognitive (0.968), alternatives (0.967), server (0.958)",
        "- 'machine' â†’ rate, experience, gradients (not semantically related)",
        "",
        "**Root Cause Analysis:**",
        "Graph-based embeddings use connection patterns to landmarks. Sparse connections",
        "(only 8-13 out of 64 landmarks) cause misleading high similarity when vectors",
        "are normalized. Terms sharing large documents have similar patterns regardless",
        "of semantic meaning.",
        "",
        "**Solution Applied:**",
        "1. Added 'tfidf' embedding method that uses document distribution as feature space",
        "2. Added IDF weighting to 'fast' method to down-weight common terms",
        "3. Updated docstrings to recommend 'tfidf' for semantic similarity tasks",
        "",
        "**Results:**",
        "```",
        "FAST (graph):   'machine' â†’ rate (0.82), gradients (0.77), experience (0.73)",
        "TF-IDF:         'machine' â†’ representations (0.71), learning (0.71), image (0.70)",
        "",
        "FAST (graph):   'learning' â†’ training (0.70), approaches (0.64)",
        "TF-IDF:         'learning' â†’ neural (0.82), networks (0.81), training (0.77)",
        "```",
        "",
        "TF-IDF embeddings capture semantic similarity much better because terms appearing",
        "in similar documents are usually semantically related.",
        "",
        "**Acceptance Criteria:**",
        "- [x] 'data' filtered out by CODE_NOISE_TOKENS (Task #141)",
        "- [x] High-frequency terms don't dominate (TF-IDF naturally down-weights)",
        "- [x] Existing good similarities preserved and improved",
        "- [x] New 'tfidf' method available for semantic similarity tasks",
        "",
        "---",
        "",
        "### 146. Create Behavioral Tests for Core User Workflows",
        "",
        "**Meta:** `status:pending` `priority:high` `category:testing`",
        "**Files:** `tests/test_behavioral.py` (new)",
        "**Effort:** Medium",
        "",
        "**Purpose:** Create acceptance/behavioral tests that verify the system delivers expected",
        "user outcomes. Unlike unit tests (function works correctly) or integration tests",
        "(components work together), behavioral tests verify \"the system feels right to users.\"",
        "",
        "**Why This Matters:**",
        "Dog-fooding revealed issues that unit tests missed:",
        "- Search returning wrong documents for obvious queries",
        "- Python keywords polluting analysis results",
        "- Performance regressions making the system feel slow",
        "",
        "Behavioral tests would have caught these as regressions.",
        "",
        "**Test Categories to Implement:**",
        "",
        "1. **SearchBehavior** - \"Search should feel relevant\"",
        "   ```python",
        "   def test_document_name_matches_rank_highly(self):",
        "       \"\"\"Query matching document name should return that doc in top 2.\"\"\"",
        "       # \"distributed systems\" â†’ distributed_systems in top 2",
        "",
        "   def test_query_expansion_improves_recall(self):",
        "       \"\"\"Expanded queries should find more relevant docs than exact match.\"\"\"",
        "",
        "   def test_code_search_finds_implementations_not_tests(self):",
        "       \"\"\"Code queries should prefer real implementations over test mocks.\"\"\"",
        "   ```",
        "",
        "2. **PerformanceBehavior** - \"System should feel responsive\"",
        "   ```python",
        "   def test_compute_all_under_threshold(self):",
        "       \"\"\"Full analysis should complete within reasonable time.\"\"\"",
        "       # With 109 sample docs: < 20 seconds",
        "",
        "   def test_search_is_fast(self):",
        "       \"\"\"Single query should return quickly.\"\"\"",
        "       # < 100ms per query",
        "   ```",
        "",
        "3. **QualityBehavior** - \"Results should make sense\"",
        "   ```python",
        "   def test_pagerank_surfaces_meaningful_terms(self):",
        "       \"\"\"Top PageRank terms should be domain concepts, not noise.\"\"\"",
        "       # No 'self', 'def', 'assertequal' in top 20",
        "",
        "   def test_clustering_produces_coherent_groups(self):",
        "       \"\"\"Clusters should have good community structure.\"\"\"",
        "       # modularity > 0.3",
        "",
        "   def test_embeddings_capture_semantic_similarity(self):",
        "       \"\"\"Similar terms by embedding should be semantically related.\"\"\"",
        "       # 'learning' similar to 'neural', 'training', 'networks'",
        "   ```",
        "",
        "4. **RobustnessBehavior** - \"System should handle edge cases gracefully\"",
        "   ```python",
        "   def test_empty_query_returns_empty_results(self):",
        "       \"\"\"Empty queries should not crash or return garbage.\"\"\"",
        "",
        "   def test_unknown_terms_handled_gracefully(self):",
        "       \"\"\"Queries with unknown terms should still return results.\"\"\"",
        "   ```",
        "",
        "**Implementation Notes:**",
        "- Use `tests/test_behavioral.py` as a new test file",
        "- Load sample corpus once in `setUpClass` for performance",
        "- Use descriptive assertion messages explaining expected behavior",
        "- Document the \"why\" for each threshold/expectation",
        "",
        "**Acceptance Criteria:**",
        "- [ ] TestSearchBehavior class with 3+ tests",
        "- [ ] TestPerformanceBehavior class with 2+ tests",
        "- [ ] TestQualityBehavior class with 3+ tests",
        "- [ ] TestRobustnessBehavior class with 2+ tests",
        "- [ ] All tests pass on current codebase",
        "- [ ] Tests catch regressions from Tasks #141-145 if reverted",
        "",
        "---",
        "",
        "| Quality | 5 | Quality issues from dog-fooding |",
        "| Perf | 4 | Performance improvements |",
        "*Updated 2025-12-12 after dog-fooding showcase.py*",
        ""
      ],
      "lines_removed": [
        "| BugFix | 1 | Bug fixes and regressions |"
      ],
      "context_before": [
        "3. Trace connection paths in the concept graph",
        "4. Document interesting findings",
        "",
        "**Potential Applications:**",
        "- Improve query expansion with cross-domain terms",
        "- Suggest related documents from different domains",
        "- Identify knowledge gaps at domain boundaries",
        "",
        "---",
        ""
      ],
      "context_after": [
        "## Category Index",
        "",
        "| Category | Pending | Description |",
        "|----------|---------|-------------|",
        "| AINav | 6 | AI assistant navigation & usability |",
        "| DevEx | 8 | Developer experience (scripts, tools) |",
        "| Docs | 2 | Documentation improvements |",
        "| Arch | 4 | Architecture refactoring |",
        "| CodeQual | 3 | Code quality improvements |",
        "| Testing | 3 | Test coverage |",
        "| TaskMgmt | 2 | Task management system |",
        "| Research | 2 | Research and analysis tasks |",
        "| Samples | 1 | Sample document improvements |",
        "| Deferred | 7 | Low priority or superseded |",
        "",
        "---",
        "",
        "## Notes",
        "",
        "- **Effort estimates:** Small (<1 hour), Medium (1-4 hours), Large (1+ days)",
        "- **Dependencies:** Complete dependent tasks first",
        "- **Quick Context:** Key info to start task without searching",
        "- **Archive:** Full history in [TASK_ARCHIVE.md](TASK_ARCHIVE.md)",
        "",
        "---"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/analysis.py",
      "function": "def _compute_silhouette(",
      "start_line": 1624,
      "lines_added": [
        "    For our representation, distance = 1 - document_cooccurrence_similarity",
        "    where similarity is based on shared documents (Jaccard on document sets).",
        "    This produces more meaningful silhouette scores than connection-based",
        "    similarity because tokens in the same semantic cluster tend to appear",
        "    in the same documents."
      ],
      "lines_removed": [
        "    For our graph representation, distance = 1 - connection_similarity",
        "    where connection_similarity is based on shared lateral connections."
      ],
      "context_before": [
        "",
        "    For each token, silhouette measures how similar it is to its own cluster",
        "    compared to the nearest other cluster.",
        "",
        "    s(i) = (b(i) - a(i)) / max(a(i), b(i))",
        "",
        "    where:",
        "    - a(i) = mean distance to other points in same cluster",
        "    - b(i) = mean distance to points in nearest cluster",
        ""
      ],
      "context_after": [
        "",
        "    Returns:",
        "        Average silhouette score between -1 and 1",
        "        - s > 0.5: Strong cluster structure",
        "        - s > 0.25: Reasonable structure",
        "        - s < 0: Poor clustering",
        "    \"\"\"",
        "    if layer2.column_count() < 2:",
        "        return 0.0  # Need at least 2 clusters",
        ""
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/analysis.py",
      "function": "def _compute_silhouette(",
      "start_line": 1666,
      "lines_added": [
        "    # Build document sets for sampled tokens",
        "    # Document set: frozenset of document IDs for this token",
        "    token_docs: Dict[str, frozenset] = {}",
        "        if col and col.document_ids:",
        "            token_docs[token] = frozenset(col.document_ids)",
        "        if token not in token_to_cluster or token not in token_docs:",
        "        my_docs = token_docs[token]",
        "        if my_cluster not in valid_clusters or not my_docs:",
        "        same_cluster = [t for t in valid_clusters[my_cluster] if t != token and t in token_docs]",
        "            sim = _doc_similarity(my_docs, token_docs[other])",
        "            other_tokens_filtered = [t for t in other_tokens if t in token_docs]",
        "                sim = _doc_similarity(my_docs, token_docs[other])",
        "def _doc_similarity(docs1: frozenset, docs2: frozenset) -> float:",
        "    \"\"\"",
        "    Compute Jaccard similarity between two document sets.",
        "",
        "    Args:",
        "        docs1: Frozenset of document IDs for first token",
        "        docs2: Frozenset of document IDs for second token",
        "",
        "    Returns:",
        "        Jaccard similarity: |intersection| / |union|",
        "    \"\"\"",
        "    if not docs1 or not docs2:",
        "        return 0.0",
        "",
        "    intersection = len(docs1 & docs2)",
        "    union = len(docs1 | docs2)",
        "",
        "    return intersection / union if union > 0 else 0.0",
        "",
        ""
      ],
      "lines_removed": [
        "    # Build connection vectors for sampled tokens",
        "    # Connection vector: {neighbor_id: weight}",
        "    token_vectors: Dict[str, Dict[str, float]] = {}",
        "        if col:",
        "            token_vectors[token] = dict(col.lateral_connections)",
        "        if token not in token_to_cluster or token not in token_vectors:",
        "        my_vector = token_vectors[token]",
        "        if my_cluster not in valid_clusters:",
        "        same_cluster = [t for t in valid_clusters[my_cluster] if t != token and t in token_vectors]",
        "            sim = _vector_similarity(my_vector, token_vectors[other])",
        "            other_tokens_filtered = [t for t in other_tokens if t in token_vectors]",
        "                sim = _vector_similarity(my_vector, token_vectors[other])"
      ],
      "context_before": [
        "        all_tokens.extend(tokens)",
        "",
        "    if len(all_tokens) == 0:",
        "        return 0.0",
        "",
        "    # Sample if too many tokens (silhouette is O(nÂ²))",
        "    import random",
        "    if len(all_tokens) > sample_size:",
        "        all_tokens = random.sample(all_tokens, sample_size)",
        ""
      ],
      "context_after": [
        "    for token in all_tokens:",
        "        col = layer0.get_minicolumn(token)",
        "",
        "    # Compute silhouette for each token",
        "    silhouette_sum = 0.0",
        "    count = 0",
        "",
        "    for token in all_tokens:",
        "            continue",
        "",
        "        my_cluster = token_to_cluster[token]",
        "",
        "            continue",
        "",
        "        # a(i): mean distance to same-cluster tokens",
        "        if not same_cluster:",
        "            continue",
        "",
        "        a_i = 0.0",
        "        for other in same_cluster:",
        "            a_i += 1.0 - sim  # Distance = 1 - similarity",
        "        a_i /= len(same_cluster)",
        "",
        "        # b(i): mean distance to nearest other cluster",
        "        b_i = float('inf')",
        "        for other_cluster, other_tokens in valid_clusters.items():",
        "            if other_cluster == my_cluster:",
        "                continue",
        "",
        "            if not other_tokens_filtered:",
        "                continue",
        "",
        "            cluster_dist = 0.0",
        "            for other in other_tokens_filtered:",
        "                cluster_dist += 1.0 - sim",
        "            cluster_dist /= len(other_tokens_filtered)",
        "",
        "            b_i = min(b_i, cluster_dist)",
        "",
        "        if b_i == float('inf'):",
        "            continue",
        "",
        "        # Silhouette coefficient",
        "        max_ab = max(a_i, b_i)",
        "        if max_ab > 0:",
        "            s_i = (b_i - a_i) / max_ab",
        "            silhouette_sum += s_i",
        "            count += 1",
        "",
        "    return silhouette_sum / count if count > 0 else 0.0",
        "",
        "",
        "def _vector_similarity(vec1: Dict[str, float], vec2: Dict[str, float]) -> float:",
        "    \"\"\"",
        "    Compute similarity between two connection vectors.",
        "",
        "    Uses Jaccard-style similarity based on shared connections.",
        "    \"\"\"",
        "    if not vec1 or not vec2:",
        "        return 0.0",
        "",
        "    keys1 = set(vec1.keys())"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/analysis.py",
      "function": "def _compute_cluster_balance(layer2: HierarchicalLayer) -> float:",
      "start_line": 1799,
      "lines_added": [
        "",
        "    Note on metric interpretation:",
        "    - Modularity measures graph edge density within clusters (Louvain's objective)",
        "    - Silhouette measures document co-occurrence similarity (semantic coherence)",
        "    - These metrics measure different things: high modularity with low silhouette",
        "      is normal for graph-based clustering of text, as tokens that co-occur in",
        "      sentences don't necessarily appear in the same documents.",
        "    # Modularity assessment (primary metric for Louvain clustering)",
        "    # Silhouette assessment (measures document co-occurrence, not graph structure)",
        "    # Negative values are typical for graph-based clustering of diverse corpora",
        "    # because sentence co-occurrence != document co-occurrence",
        "    if silhouette >= 0.25:",
        "        parts.append(f\"strong topic coherence (silhouette {silhouette:.2f})\")",
        "    elif silhouette >= 0.1:",
        "        parts.append(f\"moderate topic coherence (silhouette {silhouette:.2f})\")",
        "    elif silhouette >= -0.1:",
        "        parts.append(f\"typical graph clustering (silhouette {silhouette:.2f})\")",
        "        parts.append(f\"diverse clusters (silhouette {silhouette:.2f})\")"
      ],
      "lines_removed": [
        "    # Modularity assessment",
        "    # Silhouette assessment",
        "    if silhouette >= 0.5:",
        "        parts.append(f\"well-separated clusters (silhouette {silhouette:.2f})\")",
        "    elif silhouette >= 0.25:",
        "        parts.append(f\"reasonably separated clusters (silhouette {silhouette:.2f})\")",
        "    elif silhouette >= 0:",
        "        parts.append(f\"overlapping clusters (silhouette {silhouette:.2f})\")",
        "        parts.append(f\"poorly separated clusters (silhouette {silhouette:.2f})\")"
      ],
      "context_before": [
        "",
        "",
        "def _generate_quality_assessment(",
        "    modularity: float,",
        "    silhouette: float,",
        "    balance: float,",
        "    num_clusters: int",
        ") -> str:",
        "    \"\"\"",
        "    Generate a human-readable assessment of clustering quality."
      ],
      "context_after": [
        "    \"\"\"",
        "    parts = []",
        "",
        "    if modularity >= 0.5:",
        "        parts.append(f\"Strong community structure (modularity {modularity:.2f})\")",
        "    elif modularity >= 0.3:",
        "        parts.append(f\"Good community structure (modularity {modularity:.2f})\")",
        "    elif modularity >= 0.1:",
        "        parts.append(f\"Weak community structure (modularity {modularity:.2f})\")",
        "    else:",
        "        parts.append(f\"No clear community structure (modularity {modularity:.2f})\")",
        "",
        "    else:",
        "",
        "    # Balance assessment",
        "    if balance <= 0.3:",
        "        parts.append(\"well-balanced sizes\")",
        "    elif balance <= 0.5:",
        "        parts.append(\"moderately balanced sizes\")",
        "    else:",
        "        parts.append(\"imbalanced sizes (some clusters dominate)\")",
        "",
        "    return f\"{num_clusters} clusters with {parts[0]}, {parts[1]}, {parts[2]}\""
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/embeddings.py",
      "function": "import math",
      "start_line": 15,
      "lines_added": [
        "    method: str = 'adjacency',",
        "    max_terms: Optional[int] = None",
        "",
        "        method: 'adjacency', 'random_walk', 'spectral', or 'fast'",
        "        max_terms: If set, only compute embeddings for top N terms by PageRank.",
        "                   This significantly speeds up computation for large corpora.",
        "                   Recommended: 1000-2000 for large corpora (5000+ tokens).",
        "",
        "",
        "    # Sample top terms if max_terms is specified",
        "    if max_terms is not None and max_terms < layer0.column_count():",
        "        sorted_cols = sorted(layer0.minicolumns.values(), key=lambda c: c.pagerank, reverse=True)",
        "        sampled_terms = {col.content for col in sorted_cols[:max_terms]}",
        "    else:",
        "        sampled_terms = None",
        "",
        "    if method == 'fast':",
        "        # Fast direct adjacency without multi-hop propagation",
        "        embeddings = _fast_adjacency_embeddings(layer0, dimensions, sampled_terms)",
        "    elif method == 'tfidf':",
        "        # TF-IDF based embeddings (best for semantic similarity)",
        "        embeddings = _tfidf_embeddings(layer0, dimensions, sampled_terms)",
        "    elif method == 'adjacency':",
        "        embeddings = _adjacency_embeddings(layer0, dimensions, sampled_terms)",
        "        embeddings = _random_walk_embeddings(layer0, dimensions, sampled_terms)",
        "        embeddings = _spectral_embeddings(layer0, dimensions, sampled_terms)",
        "",
        "        'terms_embedded': len(embeddings),",
        "        'max_terms': max_terms,",
        "        'sampled': max_terms is not None and max_terms < layer0.column_count()",
        "",
        "def _fast_adjacency_embeddings(",
        "    layer: HierarchicalLayer,",
        "    dimensions: int,",
        "    sampled_terms: Optional[set] = None,",
        "    use_idf_weighting: bool = True",
        ") -> Dict[str, List[float]]:",
        "    \"\"\"",
        "    Fast direct adjacency embeddings without multi-hop propagation.",
        "",
        "    Much faster than full adjacency but less expressive. Good for large corpora",
        "    where speed is more important than embedding quality.",
        "",
        "    Args:",
        "        layer: Layer to compute embeddings for",
        "        dimensions: Number of embedding dimensions (= number of landmarks)",
        "        sampled_terms: If set, only compute embeddings for these terms",
        "        use_idf_weighting: If True, weight connections by IDF of the target term.",
        "                          This down-weights connections to very common terms,",
        "                          improving embedding quality for diverse corpora.",
        "    \"\"\"",
        "    embeddings: Dict[str, List[float]] = {}",
        "",
        "    sorted_cols = sorted(layer.minicolumns.values(), key=lambda c: c.pagerank, reverse=True)",
        "    landmarks = sorted_cols[:dimensions]",
        "    landmark_ids = {lm.id: i for i, lm in enumerate(landmarks)}",
        "",
        "    # Compute IDF weights for landmarks if enabled",
        "    # IDF = log(N / df) where df = number of documents containing the term",
        "    total_docs = len(set(doc_id for col in layer.minicolumns.values() for doc_id in col.document_ids))",
        "    landmark_idf = {}",
        "    if use_idf_weighting and total_docs > 0:",
        "        for lm in landmarks:",
        "            doc_freq = max(1, len(lm.document_ids))",
        "            # Using smoothed IDF: log((N + 1) / (df + 1)) + 1",
        "            landmark_idf[lm.id] = math.log((total_docs + 1) / (doc_freq + 1)) + 1.0",
        "    else:",
        "        # No weighting - all landmarks have weight 1.0",
        "        for lm in landmarks:",
        "            landmark_idf[lm.id] = 1.0",
        "",
        "    cols_to_process = layer.minicolumns.values()",
        "    if sampled_terms is not None:",
        "        cols_to_process = [c for c in cols_to_process if c.content in sampled_terms]",
        "",
        "    for col in cols_to_process:",
        "        vec = [0.0] * dimensions",
        "",
        "        # Direct connections only, weighted by landmark IDF",
        "        for lm_id, lm_idx in landmark_ids.items():",
        "            if lm_id in col.lateral_connections:",
        "                raw_weight = col.lateral_connections[lm_id]",
        "                idf_weight = landmark_idf.get(lm_id, 1.0)",
        "                vec[lm_idx] = raw_weight * idf_weight",
        "",
        "        # Normalize",
        "        mag = math.sqrt(sum(v*v for v in vec)) + 1e-10",
        "        embeddings[col.content] = [v / mag for v in vec]",
        "",
        "    return embeddings",
        "",
        "",
        "def _tfidf_embeddings(",
        "    layer: HierarchicalLayer,",
        "    dimensions: int,",
        "    sampled_terms: Optional[set] = None",
        ") -> Dict[str, List[float]]:",
        "    \"\"\"",
        "    TF-IDF based embeddings using document distribution as feature space.",
        "",
        "    Each term's embedding is its TF-IDF scores across documents. This produces",
        "    embeddings where semantically similar terms (those appearing in similar",
        "    documents) have high cosine similarity.",
        "",
        "    This method is generally better for semantic similarity than graph-based",
        "    methods because:",
        "    1. Terms appearing in similar documents are likely semantically related",
        "    2. TF-IDF naturally down-weights common terms",
        "    3. Embeddings are dense (no sparse landmark issues)",
        "",
        "    Args:",
        "        layer: Layer to compute embeddings for",
        "        dimensions: Maximum number of document dimensions (uses top N docs by size)",
        "        sampled_terms: If set, only compute embeddings for these terms",
        "    \"\"\"",
        "    embeddings: Dict[str, List[float]] = {}",
        "",
        "    # Get all documents and sort by document \"importance\" (term count)",
        "    all_docs = set()",
        "    doc_term_count = defaultdict(int)",
        "    for col in layer.minicolumns.values():",
        "        for doc_id in col.document_ids:",
        "            all_docs.add(doc_id)",
        "            doc_term_count[doc_id] += 1",
        "",
        "    # Use top N documents as dimensions (by term coverage)",
        "    sorted_docs = sorted(all_docs, key=lambda d: -doc_term_count[d])",
        "    doc_dims = sorted_docs[:dimensions]",
        "    doc_to_idx = {doc: i for i, doc in enumerate(doc_dims)}",
        "",
        "    cols_to_process = layer.minicolumns.values()",
        "    if sampled_terms is not None:",
        "        cols_to_process = [c for c in cols_to_process if c.content in sampled_terms]",
        "",
        "    for col in cols_to_process:",
        "        vec = [0.0] * len(doc_dims)",
        "",
        "        # Fill in TF-IDF values for documents in our dimension space",
        "        for doc_id, tfidf_score in col.tfidf_per_doc.items():",
        "            if doc_id in doc_to_idx:",
        "                vec[doc_to_idx[doc_id]] = tfidf_score",
        "",
        "        # Normalize",
        "        mag = math.sqrt(sum(v*v for v in vec)) + 1e-10",
        "        embeddings[col.content] = [v / mag for v in vec]",
        "",
        "    return embeddings",
        "",
        "",
        "    sampled_terms: Optional[set] = None,",
        "        sampled_terms: If set, only compute embeddings for these terms",
        "    cols_to_process = layer.minicolumns.values()",
        "    if sampled_terms is not None:",
        "        cols_to_process = [c for c in cols_to_process if c.content in sampled_terms]",
        "",
        "    for col in cols_to_process:"
      ],
      "lines_removed": [
        "    method: str = 'adjacency'",
        "    ",
        "        method: 'adjacency', 'random_walk', or 'spectral'",
        "        ",
        "    ",
        "    if method == 'adjacency':",
        "        embeddings = _adjacency_embeddings(layer0, dimensions)",
        "        embeddings = _random_walk_embeddings(layer0, dimensions)",
        "        embeddings = _spectral_embeddings(layer0, dimensions)",
        "    ",
        "        'terms_embedded': len(embeddings)",
        "    ",
        "    for col in layer.minicolumns.values():"
      ],
      "context_before": [
        "import random",
        "from typing import Any, Dict, List, Tuple, Optional",
        "from collections import defaultdict",
        "",
        "from .layers import CorticalLayer, HierarchicalLayer",
        "",
        "",
        "def compute_graph_embeddings(",
        "    layers: Dict[CorticalLayer, HierarchicalLayer],",
        "    dimensions: int = 64,"
      ],
      "context_after": [
        ") -> Tuple[Dict[str, List[float]], Dict[str, Any]]:",
        "    \"\"\"",
        "    Compute embeddings for tokens based on graph structure.",
        "    Args:",
        "        layers: Dictionary of layers (needs TOKENS)",
        "        dimensions: Number of embedding dimensions",
        "    Returns:",
        "        Tuple of (embeddings dict, statistics dict)",
        "    \"\"\"",
        "    layer0 = layers[CorticalLayer.TOKENS]",
        "    elif method == 'random_walk':",
        "    elif method == 'spectral':",
        "    else:",
        "        raise ValueError(f\"Unknown embedding method: {method}\")",
        "    stats = {",
        "        'method': method,",
        "        'dimensions': dimensions,",
        "    }",
        "    return embeddings, stats",
        "",
        "",
        "def _adjacency_embeddings(",
        "    layer: HierarchicalLayer,",
        "    dimensions: int,",
        "    propagation_steps: int = 2,",
        "    damping: float = 0.5",
        ") -> Dict[str, List[float]]:",
        "    \"\"\"",
        "    Compute embeddings using multi-hop adjacency to landmark nodes.",
        "",
        "    Improves over simple direct adjacency by propagating through the graph,",
        "    which handles sparse graphs better and produces more meaningful embeddings.",
        "",
        "    Args:",
        "        layer: Layer to compute embeddings for",
        "        dimensions: Number of embedding dimensions (= number of landmarks)",
        "        propagation_steps: Number of propagation steps (default 2)",
        "        damping: Weight decay per step (default 0.5)",
        "    \"\"\"",
        "    embeddings: Dict[str, List[float]] = {}",
        "",
        "    sorted_cols = sorted(layer.minicolumns.values(), key=lambda c: c.pagerank, reverse=True)",
        "    landmarks = sorted_cols[:dimensions]",
        "    landmark_ids = {lm.id: i for i, lm in enumerate(landmarks)}",
        "",
        "    # Build adjacency lookup for efficient propagation",
        "    id_to_col = {col.id: col for col in layer.minicolumns.values()}",
        "",
        "        vec = [0.0] * dimensions",
        "",
        "        # Direct connections (weight = 1.0)",
        "        for lm_id, lm_idx in landmark_ids.items():",
        "            if lm_id in col.lateral_connections:",
        "                vec[lm_idx] += col.lateral_connections[lm_id]",
        "",
        "        # Multi-hop propagation: reach landmarks through neighbors",
        "        current_weight = damping",
        "        frontier = list(col.lateral_connections.items())"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/embeddings.py",
      "function": "def _adjacency_embeddings(",
      "start_line": 123,
      "lines_added": [
        "    sampled_terms: Optional[set] = None,",
        "",
        "    # Only walk from sampled terms if specified",
        "    cols_to_walk = layer.minicolumns.values()",
        "    if sampled_terms is not None:",
        "        cols_to_walk = [c for c in cols_to_walk if c.content in sampled_terms]",
        "",
        "    for col in cols_to_walk:",
        "",
        "",
        "    terms_to_embed = layer.minicolumns.keys() if sampled_terms is None else sampled_terms",
        "    for term in terms_to_embed:",
        "        if term in layer.minicolumns:",
        "            vec = [cooccurrence[term].get(lm, 0) for lm in landmarks]",
        "            mag = math.sqrt(sum(v*v for v in vec)) + 1e-10",
        "            embeddings[term] = [v / mag for v in vec]",
        ""
      ],
      "lines_removed": [
        "    ",
        "    for col in layer.minicolumns.values():",
        "    ",
        "    ",
        "    for term in layer.minicolumns:",
        "        vec = [cooccurrence[term].get(lm, 0) for lm in landmarks]",
        "        mag = math.sqrt(sum(v*v for v in vec)) + 1e-10",
        "        embeddings[term] = [v / mag for v in vec]",
        "    "
      ],
      "context_before": [
        "        # Normalize",
        "        mag = math.sqrt(sum(v*v for v in vec)) + 1e-10",
        "        embeddings[col.content] = [v / mag for v in vec]",
        "",
        "    return embeddings",
        "",
        "",
        "def _random_walk_embeddings(",
        "    layer: HierarchicalLayer,",
        "    dimensions: int,"
      ],
      "context_after": [
        "    walks_per_node: int = 10,",
        "    walk_length: int = 40,",
        "    window_size: int = 5",
        ") -> Dict[str, List[float]]:",
        "    \"\"\"Compute embeddings using random walks (DeepWalk-inspired).\"\"\"",
        "    embeddings: Dict[str, List[float]] = {}",
        "    id_to_term = {col.id: col.content for col in layer.minicolumns.values()}",
        "    cooccurrence: Dict[str, Dict[str, float]] = defaultdict(lambda: defaultdict(float))",
        "        for _ in range(walks_per_node):",
        "            walk = _weighted_random_walk(col, layer, walk_length, id_to_term)",
        "            for i, term in enumerate(walk):",
        "                for j in range(max(0, i - window_size), min(len(walk), i + window_size + 1)):",
        "                    if i != j:",
        "                        cooccurrence[term][walk[j]] += 1.0",
        "    sorted_cols = sorted(layer.minicolumns.values(), key=lambda c: c.pagerank, reverse=True)",
        "    landmarks = [c.content for c in sorted_cols[:dimensions]]",
        "    return embeddings",
        "",
        "",
        "def _weighted_random_walk(start_col, layer: HierarchicalLayer, length: int, id_to_term: Dict[str, str]) -> List[str]:",
        "    \"\"\"Perform a weighted random walk from a starting column.\"\"\"",
        "    walk = [start_col.content]",
        "    current = start_col",
        "    ",
        "    for _ in range(length - 1):",
        "        if not current.lateral_connections:"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/embeddings.py",
      "function": "def _weighted_random_walk(start_col, layer: HierarchicalLayer, length: int, id_t",
      "start_line": 183,
      "lines_added": [
        "def _spectral_embeddings(",
        "    layer: HierarchicalLayer,",
        "    dimensions: int,",
        "    sampled_terms: Optional[set] = None,",
        "    iterations: int = 50",
        ") -> Dict[str, List[float]]:",
        "    \"\"\"Compute embeddings using spectral methods (graph Laplacian).",
        "",
        "    Note: This is inherently O(dimensions Ã— iterations Ã— nÂ²) so it's slow for large graphs.",
        "    When sampled_terms is provided, only those terms get embeddings but the full graph",
        "    structure is still used for computation.",
        "    \"\"\"",
        "",
        "    # If sampling, use only sampled terms for the graph",
        "    if sampled_terms is not None:",
        "        terms = [t for t in layer.minicolumns.keys() if t in sampled_terms]",
        "    else:",
        "        terms = list(layer.minicolumns.keys())",
        "",
        "",
        "",
        "    for term in terms:",
        "        col = layer.minicolumns[term]",
        "",
        "",
        "",
        "",
        "",
        ""
      ],
      "lines_removed": [
        "def _spectral_embeddings(layer: HierarchicalLayer, dimensions: int, iterations: int = 100) -> Dict[str, List[float]]:",
        "    \"\"\"Compute embeddings using spectral methods (graph Laplacian).\"\"\"",
        "    terms = list(layer.minicolumns.keys())",
        "    ",
        "    ",
        "    for term, col in layer.minicolumns.items():",
        "    ",
        "    ",
        "        ",
        "            ",
        "        ",
        "    "
      ],
      "context_before": [
        "        next_term = id_to_term.get(next_id)",
        "        if next_term and next_term in layer.minicolumns:",
        "            current = layer.minicolumns[next_term]",
        "            walk.append(next_term)",
        "        else:",
        "            break",
        "    ",
        "    return walk",
        "",
        ""
      ],
      "context_after": [
        "    embeddings: Dict[str, List[float]] = {}",
        "    n = len(terms)",
        "    if n == 0:",
        "        return embeddings",
        "    term_to_idx = {t: i for i, t in enumerate(terms)}",
        "    adjacency: Dict[int, Dict[int, float]] = defaultdict(dict)",
        "    degrees = [0.0] * n",
        "        i = term_to_idx[term]",
        "        for neighbor_id, weight in col.lateral_connections.items():",
        "            neighbor = layer.get_by_id(neighbor_id)",
        "            if neighbor and neighbor.content in term_to_idx:",
        "                j = term_to_idx[neighbor.content]",
        "                adjacency[i][j] = weight",
        "                degrees[i] += weight",
        "    degrees = [d if d > 0 else 1.0 for d in degrees]",
        "    actual_dims = min(dimensions, n)",
        "    vectors = []",
        "    for d in range(actual_dims):",
        "        vec = [random.gauss(0, 1) for _ in range(n)]",
        "        for prev in vectors:",
        "            dot = sum(v * p for v, p in zip(vec, prev))",
        "            vec = [v - dot * p for v, p in zip(vec, prev)]",
        "        mag = math.sqrt(sum(v*v for v in vec)) + 1e-10",
        "        vec = [v / mag for v in vec]",
        "        for _ in range(iterations):",
        "            new_vec = [0.0] * n",
        "            for i in range(n):",
        "                for j, weight in adjacency[i].items():",
        "                    norm_weight = weight / math.sqrt(degrees[i] * degrees[j])",
        "                    new_vec[i] -= norm_weight * vec[j]",
        "                new_vec[i] += vec[i]",
        "            for prev in vectors:",
        "                dot = sum(v * p for v, p in zip(new_vec, prev))",
        "                new_vec = [v - dot * p for v, p in zip(new_vec, prev)]",
        "            mag = math.sqrt(sum(v*v for v in new_vec)) + 1e-10",
        "            vec = [v / mag for v in new_vec]",
        "        vectors.append(vec)",
        "    for term in terms:",
        "        i = term_to_idx[term]",
        "        embeddings[term] = [vectors[d][i] if d < len(vectors) else 0.0 for d in range(dimensions)]",
        "    ",
        "    return embeddings",
        "",
        "",
        "def embedding_similarity(embeddings: Dict[str, List[float]], term1: str, term2: str) -> float:",
        "    \"\"\"Compute cosine similarity between two term embeddings.\"\"\"",
        "    if term1 not in embeddings or term2 not in embeddings:"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/processor.py",
      "function": "class CorticalTextProcessor:",
      "start_line": 1055,
      "lines_added": [
        "          (this is what Louvain optimizes - expect >0.3 for good clustering)",
        "        - Silhouette: Document co-occurrence similarity within vs between clusters",
        "          (measures semantic coherence - typically -0.1 to 0.1 for graph clustering)",
        "        - Balance (Gini): Distribution of cluster sizes (0=equal, 1=all in one)",
        "",
        "        Note on metric interpretation:",
        "            Modularity and silhouette measure different things. Louvain clusters",
        "            tokens by sentence co-occurrence, while silhouette measures document",
        "            co-occurrence. These don't always align: tokens appearing together",
        "            in sentences may not appear in the same documents. High modularity",
        "            with low/negative silhouette is normal for diverse text corpora.",
        "            - silhouette: float (-1 to 1, typically -0.1 to 0.1 for graph clustering)",
        "            34 clusters with Good community structure (modularity 0.37),",
        "            typical graph clustering (silhouette -0.03), moderately balanced sizes"
      ],
      "lines_removed": [
        "        - Silhouette: How similar tokens are to their cluster vs other clusters",
        "        - Balance (Gini): Distribution of cluster sizes",
        "            - silhouette: float (-1 to 1, higher is better, >0.25 is reasonable)",
        "            37 clusters with Good community structure (modularity 0.40),",
        "            overlapping clusters (silhouette 0.15), moderately balanced sizes"
      ],
      "context_before": [
        "",
        "    def compute_clustering_quality(",
        "        self,",
        "        sample_size: int = 500",
        "    ) -> Dict[str, Any]:",
        "        \"\"\"",
        "        Compute clustering quality metrics for the concept layer.",
        "",
        "        Evaluates how well the clustering algorithm has performed by computing:",
        "        - Modularity: Density of within-cluster connections vs between-cluster"
      ],
      "context_after": [
        "",
        "        Args:",
        "            sample_size: Max tokens to sample for silhouette calculation",
        "                        (full calculation is O(nÂ²), sampling keeps it tractable)",
        "",
        "        Returns:",
        "            Dictionary with:",
        "            - modularity: float (-1 to 1, higher is better, >0.3 is good)",
        "            - balance: float (0 to 1, 0 = perfectly balanced, 1 = all in one)",
        "            - num_clusters: int",
        "            - quality_assessment: str (human-readable interpretation)",
        "",
        "        Example:",
        "            >>> processor.compute_all()",
        "            >>> quality = processor.compute_clustering_quality()",
        "            >>> print(f\"Modularity: {quality['modularity']:.3f}\")",
        "            >>> print(quality['quality_assessment'])",
        "",
        "        See Also:",
        "            build_concept_clusters: Creates the clusters being evaluated",
        "            compute_all: Runs full pipeline including clustering",
        "        \"\"\"",
        "        return analysis.compute_clustering_quality(self.layers, sample_size)",
        "",
        "    def compute_concept_connections(",
        "        self,",
        "        use_semantics: bool = True,"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/processor.py",
      "function": "class CorticalTextProcessor:",
      "start_line": 1349,
      "lines_added": [
        "    def compute_graph_embeddings(",
        "        self,",
        "        dimensions: int = 64,",
        "        method: str = 'fast',",
        "        max_terms: Optional[int] = None,",
        "        verbose: bool = True",
        "    ) -> Dict:",
        "        \"\"\"",
        "        Compute graph embeddings for tokens.",
        "",
        "        Args:",
        "            dimensions: Number of embedding dimensions (default 64)",
        "            method: Embedding method:",
        "                - 'tfidf': TF-IDF based embeddings. Best for semantic similarity.",
        "                  Uses document distribution as feature space. Recommended for",
        "                  finding semantically similar terms.",
        "                - 'fast': Fast graph adjacency. Good for large corpora when speed",
        "                  matters more than semantic quality.",
        "                - 'adjacency': Multi-hop graph adjacency. More expressive but slower.",
        "                - 'random_walk': DeepWalk-style random walks. Good graph structure.",
        "                - 'spectral': Graph Laplacian eigenvectors. Mathematical approach.",
        "            max_terms: Maximum number of terms to embed (by PageRank).",
        "                      If None, auto-selects based on corpus size:",
        "                      - <2000 tokens: embed all",
        "                      - 2000-5000 tokens: embed top 1500",
        "                      - >5000 tokens: embed top 1000",
        "            verbose: Print progress messages",
        "",
        "        Returns:",
        "            Statistics dict with method, dimensions, terms_embedded",
        "",
        "        Note:",
        "            For semantic similarity tasks (finding related terms), 'tfidf' method",
        "            produces significantly better results than graph-based methods because",
        "            terms appearing in similar documents are usually semantically related.",
        "        \"\"\"",
        "        # Auto-select max_terms based on corpus size",
        "        token_count = self.layers[CorticalLayer.TOKENS].column_count()",
        "        if max_terms is None:",
        "            if token_count < 2000:",
        "                max_terms = None  # Embed all",
        "            elif token_count < 5000:",
        "                max_terms = 1500",
        "            else:",
        "                max_terms = 1000",
        "",
        "        self.embeddings, stats = emb_module.compute_graph_embeddings(",
        "            self.layers, dimensions, method, max_terms",
        "        )",
        "        if verbose:",
        "            sampled = stats.get('sampled', False)",
        "            sample_info = f\", sampled top {max_terms}\" if sampled else \"\"",
        "            print(f\"Computed {stats['terms_embedded']} embeddings ({method}{sample_info})\")",
        ""
      ],
      "lines_removed": [
        "    def compute_graph_embeddings(self, dimensions: int = 64, method: str = 'adjacency', verbose: bool = True) -> Dict:",
        "        self.embeddings, stats = emb_module.compute_graph_embeddings(self.layers, dimensions, method)",
        "        if verbose: print(f\"Computed {stats['terms_embedded']} embeddings ({method})\")",
        "    "
      ],
      "context_before": [
        "            >>> # Both inherit \"living\" from \"animal\", so similarity > 0",
        "        \"\"\"",
        "        if not self.semantic_relations:",
        "            return 0.0",
        "",
        "        # Compute inherited properties on the fly if needed",
        "        inherited = semantics.inherit_properties(self.semantic_relations)",
        "",
        "        return semantics.compute_property_similarity(term1, term2, inherited)",
        "    "
      ],
      "context_after": [
        "        return stats",
        "    def retrofit_embeddings(self, iterations: int = 10, alpha: float = 0.4, verbose: bool = True) -> Dict:",
        "        if not self.embeddings: self.compute_graph_embeddings(verbose=False)",
        "        if not self.semantic_relations: self.extract_corpus_semantics(verbose=False)",
        "        stats = semantics.retrofit_embeddings(self.embeddings, self.semantic_relations, iterations, alpha)",
        "        if verbose: print(f\"Retrofitted embeddings (moved {stats['total_movement']:.2f} total)\")",
        "        return stats",
        "    ",
        "    def embedding_similarity(self, term1: str, term2: str) -> float:",
        "        return emb_module.embedding_similarity(self.embeddings, term1, term2)",
        "    "
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/query/search.py",
      "function": "from ..code_concepts import get_related_terms",
      "start_line": 22,
      "lines_added": [
        "    use_semantic: bool = True,",
        "    doc_name_boost: float = 2.0",
        "        doc_name_boost: Multiplier for documents whose name matches query terms (default 2.0)"
      ],
      "lines_removed": [
        "    use_semantic: bool = True"
      ],
      "context_before": [
        "from .expansion import expand_query, get_expanded_query_terms",
        "",
        "",
        "def find_documents_for_query(",
        "    query_text: str,",
        "    layers: Dict[CorticalLayer, HierarchicalLayer],",
        "    tokenizer: Tokenizer,",
        "    top_n: int = 5,",
        "    use_expansion: bool = True,",
        "    semantic_relations: Optional[List[Tuple[str, str, str, float]]] = None,"
      ],
      "context_after": [
        ") -> List[Tuple[str, float]]:",
        "    \"\"\"",
        "    Find documents most relevant to a query using TF-IDF and optional expansion.",
        "",
        "    Args:",
        "        query_text: Search query",
        "        layers: Dictionary of layers",
        "        tokenizer: Tokenizer instance",
        "        top_n: Number of documents to return",
        "        use_expansion: Whether to expand query terms using lateral connections",
        "        semantic_relations: Optional list of semantic relations for expansion",
        "        use_semantic: Whether to use semantic relations for expansion (if available)",
        "",
        "    Returns:",
        "        List of (doc_id, score) tuples ranked by relevance",
        "    \"\"\"",
        "    layer0 = layers[CorticalLayer.TOKENS]",
        "",
        "    query_terms = get_expanded_query_terms(",
        "        query_text, layers, tokenizer,",
        "        use_expansion=use_expansion,",
        "        semantic_relations=semantic_relations,"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/query/search.py",
      "function": "def find_documents_for_query(",
      "start_line": 58,
      "lines_added": [
        "    # Boost documents whose name matches query terms",
        "    if doc_name_boost > 1.0:",
        "        query_tokens = set(tokenizer.tokenize(query_text))",
        "        for doc_id in doc_scores:",
        "            # Tokenize document ID (handle underscores as separators)",
        "            doc_name_tokens = set(tokenizer.tokenize(doc_id.replace('_', ' ')))",
        "            # Count how many query tokens appear in doc name",
        "            matches = len(query_tokens & doc_name_tokens)",
        "            if matches > 0:",
        "                # Boost proportional to match ratio",
        "                match_ratio = matches / len(query_tokens) if query_tokens else 0",
        "                boost = 1 + (doc_name_boost - 1) * match_ratio",
        "                doc_scores[doc_id] *= boost",
        "",
        "    use_code_concepts: bool = True,",
        "    doc_name_boost: float = 2.0",
        "        doc_name_boost: Multiplier for documents whose name matches query terms (default 2.0)",
        "    query_tokens = set(tokens)",
        ""
      ],
      "lines_removed": [
        "    use_code_concepts: bool = True"
      ],
      "context_before": [
        "    # Score each document",
        "    doc_scores: Dict[str, float] = defaultdict(float)",
        "",
        "    for term, term_weight in query_terms.items():",
        "        col = layer0.get_minicolumn(term)",
        "        if col:",
        "            for doc_id in col.document_ids:",
        "                tfidf = col.tfidf_per_doc.get(doc_id, col.tfidf)",
        "                doc_scores[doc_id] += tfidf * term_weight",
        ""
      ],
      "context_after": [
        "    sorted_docs = sorted(doc_scores.items(), key=lambda x: -x[1])",
        "    return sorted_docs[:top_n]",
        "",
        "",
        "def fast_find_documents(",
        "    query_text: str,",
        "    layers: Dict[CorticalLayer, HierarchicalLayer],",
        "    tokenizer: Tokenizer,",
        "    top_n: int = 5,",
        "    candidate_multiplier: int = 3,",
        ") -> List[Tuple[str, float]]:",
        "    \"\"\"",
        "    Fast document search using candidate filtering.",
        "",
        "    Optimizes search by:",
        "    1. Using set intersection to find candidate documents",
        "    2. Only scoring top candidates fully",
        "    3. Using code concept expansion for better recall",
        "",
        "    This is ~2-3x faster than full search on large corpora while",
        "    maintaining similar result quality.",
        "",
        "    Args:",
        "        query_text: Search query",
        "        layers: Dictionary of layers",
        "        tokenizer: Tokenizer instance",
        "        top_n: Number of results to return",
        "        candidate_multiplier: Multiplier for candidate set size",
        "        use_code_concepts: Whether to use code concept expansion",
        "",
        "    Returns:",
        "        List of (doc_id, score) tuples ranked by relevance",
        "    \"\"\"",
        "    layer0 = layers[CorticalLayer.TOKENS]",
        "",
        "    # Tokenize query",
        "    tokens = tokenizer.tokenize(query_text)",
        "    if not tokens:",
        "        return []",
        "",
        "    # Phase 1: Find candidate documents (fast set operations)",
        "    # Get documents containing ANY query term",
        "    candidate_docs: Dict[str, int] = defaultdict(int)  # doc_id -> match count",
        "",
        "    for token in tokens:",
        "        col = layer0.get_minicolumn(token)",
        "        if col:",
        "            for doc_id in col.document_ids:",
        "                candidate_docs[doc_id] += 1",
        ""
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/query/search.py",
      "function": "def fast_find_documents(",
      "start_line": 146,
      "lines_added": [
        "        score *= (1 + 0.5 * coverage_boost)",
        "",
        "        # Boost documents whose name matches query terms",
        "        if doc_name_boost > 1.0:",
        "            doc_name_tokens = set(tokenizer.tokenize(doc_id.replace('_', ' ')))",
        "            matches = len(query_tokens & doc_name_tokens)",
        "            if matches > 0:",
        "                match_ratio = matches / len(query_tokens)",
        "                boost = 1 + (doc_name_boost - 1) * match_ratio",
        "                score *= boost",
        "",
        "        doc_scores[doc_id] = score"
      ],
      "lines_removed": [
        "        doc_scores[doc_id] = score * (1 + 0.5 * coverage_boost)"
      ],
      "context_before": [
        "    for doc_id, match_count in top_candidates:",
        "        score = 0.0",
        "        for token in tokens:",
        "            col = layer0.get_minicolumn(token)",
        "            if col and doc_id in col.document_ids:",
        "                tfidf = col.tfidf_per_doc.get(doc_id, col.tfidf)",
        "                score += tfidf",
        "",
        "        # Boost by match coverage",
        "        coverage_boost = match_count / len(tokens)"
      ],
      "context_after": [
        "",
        "    # Return top results",
        "    sorted_docs = sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)",
        "    return sorted_docs[:top_n]",
        "",
        "",
        "def build_document_index(",
        "    layers: Dict[CorticalLayer, HierarchicalLayer]",
        ") -> Dict[str, Dict[str, float]]:",
        "    \"\"\""
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/tokenizer.py",
      "function": "from typing import List, Set, Optional, Dict, Tuple",
      "start_line": 17,
      "lines_added": [
        "# Very common code tokens that should be filtered from corpus analysis",
        "# when mixed text/code documents are present. These dominate PageRank/TF-IDF",
        "# due to appearing in almost every method/function.",
        "CODE_NOISE_TOKENS = frozenset({",
        "    # Python-specific",
        "    'self', 'cls', 'args', 'kwargs',",
        "    'def', 'class', 'return', 'pass',",
        "    'none', 'true', 'false',",
        "    'str', 'int', 'float', 'bool', 'list', 'dict', 'set', 'tuple',",
        "    'len', 'range', 'print', 'type', 'isinstance', 'hasattr',",
        "    # Test framework noise",
        "    'assertequal', 'asserttrue', 'assertfalse', 'assertnone',",
        "    'assertis', 'assertisnot', 'assertin', 'assertnotin',",
        "    'assertraises', 'setup', 'teardown', 'unittest',",
        "    # Common variable names that are too generic",
        "    'result', 'value', 'item', 'obj', 'data', 'func',",
        "})",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "# These appear in almost every Python method/function, so they add noise",
        "# rather than signal when expanding queries for code search",
        "CODE_EXPANSION_STOP_WORDS = frozenset({",
        "    'self', 'cls',              # Class method parameters",
        "    'args', 'kwargs',           # Variadic parameters",
        "    'none', 'true', 'false',    # Literals (too common)",
        "    'return', 'pass',           # Control flow (too common)",
        "    'def', 'class',             # Definitions (search for these explicitly)",
        "})",
        ""
      ],
      "context_after": [
        "",
        "# Programming keywords that should be preserved even if in stop words",
        "PROGRAMMING_KEYWORDS = frozenset({",
        "    'def', 'class', 'function', 'return', 'import', 'from', 'if', 'else',",
        "    'elif', 'for', 'while', 'try', 'except', 'finally', 'with', 'as',",
        "    'yield', 'async', 'await', 'lambda', 'pass', 'break', 'continue',",
        "    'raise', 'assert', 'global', 'nonlocal', 'del', 'true', 'false',",
        "    'none', 'null', 'void', 'int', 'str', 'float', 'bool', 'list',",
        "    'dict', 'set', 'tuple', 'self', 'cls', 'init', 'main', 'args',",
        "    'kwargs', 'super', 'property', 'staticmethod', 'classmethod',"
      ],
      "change_type": "add"
    },
    {
      "file": "cortical/tokenizer.py",
      "function": "class Tokenizer:",
      "start_line": 198,
      "lines_added": [
        "        split_identifiers: bool = False,",
        "        filter_code_noise: bool = False",
        "            filter_code_noise: If True, filter out common code tokens (self, def, etc.)",
        "                              that dominate PageRank/TF-IDF in mixed text/code corpora.",
        "        base_stop_words = stop_words if stop_words is not None else self.DEFAULT_STOP_WORDS",
        "        # Add code noise tokens to stop words if filtering is enabled",
        "        if filter_code_noise:",
        "            self.stop_words = base_stop_words | CODE_NOISE_TOKENS",
        "        else:",
        "            self.stop_words = base_stop_words",
        "        self.filter_code_noise = filter_code_noise"
      ],
      "lines_removed": [
        "        split_identifiers: bool = False",
        "        self.stop_words = stop_words if stop_words is not None else self.DEFAULT_STOP_WORDS"
      ],
      "context_before": [
        "        # Common adjectives (too generic)",
        "        'new', 'old', 'good', 'bad', 'great', 'small', 'large', 'big', 'long',",
        "        'high', 'low', 'right', 'left', 'possible', 'important', 'major',",
        "        'available', 'able', 'like', 'different', 'similar'",
        "    })",
        "    ",
        "    def __init__(",
        "        self,",
        "        stop_words: Optional[Set[str]] = None,",
        "        min_word_length: int = 3,"
      ],
      "context_after": [
        "    ):",
        "        \"\"\"",
        "        Initialize tokenizer.",
        "",
        "        Args:",
        "            stop_words: Set of words to filter out. Uses defaults if None.",
        "            min_word_length: Minimum word length to keep.",
        "            split_identifiers: If True, split camelCase/underscore_style and include",
        "                               both original and component tokens.",
        "        \"\"\"",
        "        self.min_word_length = min_word_length",
        "        self.split_identifiers = split_identifiers",
        "        ",
        "        # Simple suffix rules for stemming (Porter-lite)",
        "        self._suffix_rules = [",
        "            ('ational', 'ate'), ('tional', 'tion'), ('enci', 'ence'),",
        "            ('anci', 'ance'), ('izer', 'ize'), ('isation', 'ize'),",
        "            ('ization', 'ize'), ('ation', 'ate'), ('ator', 'ate'),",
        "            ('alism', 'al'), ('iveness', 'ive'), ('fulness', 'ful'),",
        "            ('ousness', 'ous'), ('aliti', 'al'), ('iviti', 'ive'),",
        "            ('biliti', 'ble'), ('ement', ''), ('ment', ''), ('ness', ''),",
        "            ('ling', ''), ('ing', ''), ('ies', 'y'), ('ied', 'y'),"
      ],
      "change_type": "modify"
    },
    {
      "file": "showcase.py",
      "function": "def render_bar(value: float, max_value: float, width: int = 30) -> str:",
      "start_line": 58,
      "lines_added": [
        "        # Use code noise filtering to exclude common Python keywords",
        "        # that pollute PageRank/TF-IDF in mixed text/code corpora",
        "        from cortical.tokenizer import Tokenizer",
        "        tokenizer = Tokenizer(filter_code_noise=True)",
        "        self.processor = CorticalTextProcessor(tokenizer=tokenizer)"
      ],
      "lines_removed": [
        "        self.processor = CorticalTextProcessor()"
      ],
      "context_before": [
        "        return \" \" * width",
        "    filled = int((value / max_value) * width)",
        "    return \"â–ˆ\" * filled + \"â–‘\" * (width - filled)",
        "",
        "",
        "class CorticalShowcase:",
        "    \"\"\"Showcases the cortical text processor with interesting analysis.\"\"\"",
        "",
        "    def __init__(self, samples_dir: str = \"samples\"):",
        "        self.samples_dir = samples_dir"
      ],
      "context_after": [
        "        self.loaded_files = []",
        "        self.timer = Timer()",
        "",
        "    def run(self):",
        "        \"\"\"Run the complete demo.\"\"\"",
        "        self.print_intro()",
        "",
        "        if not self.ingest_corpus():",
        "            print(\"No documents found!\")",
        "            return"
      ],
      "change_type": "modify"
    }
  ],
  "hour_of_day": 2,
  "day_of_week": "Friday",
  "seconds_since_last_commit": -301268,
  "is_merge": true,
  "is_initial": false,
  "parent_count": 2,
  "session_id": null,
  "related_chats": [],
  "ci_result": null,
  "reverted": false,
  "amended": false
}