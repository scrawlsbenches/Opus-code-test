{
  "hash": "39d5cd7a0454b3959ef6592811a7862e0bacec2a",
  "message": "Merge pull request #26 from scrawlsbenches/claude/complete-task-list-01EDZ3RokUfxTTJ5saSogKxE",
  "author": "scrawlsbenches",
  "timestamp": "2025-12-10 19:24:30 -0500",
  "branch": "claude/multi-index-design-DvifZ",
  "files_changed": [
    "TASK_LIST.md",
    "cortical/chunk_index.py",
    "cortical/processor.py",
    "cortical/query.py",
    "scripts/index_codebase.py",
    "scripts/search_codebase.py",
    "tests/test_chunk_indexing.py",
    "tests/test_query.py"
  ],
  "insertions": 976,
  "deletions": 145,
  "hunks": [
    {
      "file": "TASK_LIST.md",
      "function": "python scripts/index_codebase.py --status --use-chunks",
      "start_line": 2020,
      "lines_added": [
        "**Files:** `scripts/index_codebase.py`",
        "**Status:** [x] Completed (2025-12-11) - Implemented as part of Task #60",
        "**Solution Applied:**",
        "Renamed the custom exception to `IndexingTimeoutError`:",
        "    \"\"\"Raised when indexing exceeds the timeout.\"\"\"",
        "**Files:** `scripts/index_codebase.py`",
        "**Status:** [x] Completed (2025-12-11)",
        "**Solution Applied:**",
        "Added cross-platform timeout implementation:",
        "1. **Platform detection**: Added `_IS_WINDOWS = platform.system() == 'Windows'`",
        "2. **Windows implementation**: Uses `threading.Timer` with `threading.Event`:",
        "   ```python",
        "   timer = threading.Timer(seconds, timeout_callback)",
        "   timer.daemon = True",
        "   timer.start()",
        "   # Check timed_out.is_set() after operations",
        "   ```",
        "",
        "3. **Unix implementation**: Continues using `signal.SIGALRM` (unchanged behavior)",
        "",
        "4. **Limitation documented**: Windows implementation cannot interrupt blocking I/O operations",
        "",
        "5. **Also addressed Task #59**: Renamed `TimeoutError` to `IndexingTimeoutError` to avoid shadowing the built-in"
      ],
      "lines_removed": [
        "**Files:** `cortical/chunk_index.py:248`, `scripts/index_codebase.py:248-249`",
        "**Status:** [ ] Not Started",
        "**Solution:**",
        "Rename the custom exception to `IndexingTimeoutError`:",
        "# chunk_index.py:248",
        "    \"\"\"Raised when indexing operation times out.\"\"\"",
        "**Files:** `scripts/index_codebase.py:274-282`",
        "**Status:** [ ] Not Started",
        "**Affected Code:**",
        "```python",
        "# Line 274-282 - Unix-only code",
        "signal.signal(signal.SIGALRM, handler)",
        "signal.alarm(seconds)",
        "```",
        "**Solution:**",
        "Add a Windows-compatible fallback using threading:",
        "```python",
        "import platform",
        "",
        "if platform.system() == 'Windows':",
        "    # Use threading.Timer fallback",
        "    timer = threading.Timer(seconds, timeout_callback)",
        "    timer.start()",
        "else:",
        "    # Use signal.SIGALRM (Unix)",
        "    signal.signal(signal.SIGALRM, handler)",
        "    signal.alarm(seconds)",
        "```",
        "Or document the limitation clearly for Windows users."
      ],
      "context_before": [
        "---",
        "",
        "## Code Review Findings (2025-12-10)",
        "",
        "The following tasks were identified during code review of PR #23 (Git-Compatible Chunk-Based Indexing):",
        "",
        "---",
        "",
        "### 59. Rename TimeoutError to Avoid Built-in Shadowing",
        ""
      ],
      "context_after": [
        "**Priority:** Low",
        "",
        "**Problem:**",
        "The `TimeoutError` class shadows Python's built-in `TimeoutError` (introduced in Python 3.3). This could cause confusion and unexpected behavior when catching timeout exceptions.",
        "",
        "```python",
        "class IndexingTimeoutError(Exception):",
        "    pass",
        "```",
        "",
        "---",
        "",
        "### 60. Add Windows Compatibility for Timeout Handler",
        "",
        "**Priority:** Medium",
        "",
        "**Problem:**",
        "The timeout handler uses `signal.SIGALRM` which is Unix-only. This will raise an `AttributeError` on Windows systems.",
        "",
        "",
        "",
        "",
        "---",
        "",
        "### 61. Add Chunk Size Warning for Large Chunks",
        "",
        "**Files:** `cortical/chunk_index.py`",
        "**Status:** [ ] Not Started",
        "**Priority:** Low",
        "",
        "**Problem:**"
      ],
      "change_type": "modify"
    },
    {
      "file": "TASK_LIST.md",
      "function": "Add documentation explaining:",
      "start_line": 2119,
      "lines_added": [
        "**Files:** `cortical/chunk_index.py`, `scripts/index_codebase.py`, `tests/test_chunk_indexing.py`",
        "**Status:** [x] Completed (2025-12-11)",
        "**Solution Applied:**",
        "1. Added `metadata` field to `ChunkOperation` dataclass",
        "2. Updated `ChunkWriter.add_document()` and `modify_document()` to accept metadata",
        "3. Updated `ChunkLoader` to track `_metadata` dict with `get_metadata()` method",
        "4. Updated `ChunkCompactor.compact()` to preserve metadata during compaction",
        "5. Added `_extract_file_metadata()` helper to extract:",
        "   - `doc_type`: 'code', 'test', 'docs', or 'root_docs'",
        "   - `headings`: List of markdown section headings (## and ###)",
        "   - `language`: 'python' or 'markdown'",
        "   - `function_count`, `class_count` for Python files",
        "6. Updated `index_with_chunks()` to extract and use metadata",
        "7. Added 12 new tests in `TestChunkMetadata` class",
        "**Files Modified:**",
        "- `cortical/chunk_index.py` - Added metadata support to ChunkOperation, ChunkWriter, ChunkLoader, ChunkCompactor",
        "- `scripts/index_codebase.py` - Added `extract_markdown_headings()`, `get_doc_type()`, `_extract_file_metadata()`",
        "- `tests/test_chunk_indexing.py` - Added 12 tests for metadata functionality",
        "**This enables Task #63** - with metadata, we can now boost docs in search.",
        "**Files:** `cortical/query.py`, `scripts/search_codebase.py`, `tests/test_query.py`",
        "**Status:** [x] Completed (2025-12-11)",
        "**Solution Applied:**",
        "1. **Document-type boosting** - Added `DOC_TYPE_BOOSTS` dict with boost factors:",
        "   - `docs/` folder: 1.5x",
        "   - Root-level .md: 1.3x",
        "   - Code files: 1.0x",
        "   - Test files: 0.8x",
        "2. **Query intent detection** - Added `is_conceptual_query()` function:",
        "   - Detects conceptual keywords: \"what\", \"explain\", \"describe\", \"architecture\", etc.",
        "   - Detects implementation keywords: \"where\", \"implement\", \"function\", etc.",
        "   - Auto-boosts docs for conceptual queries",
        "3. **New search function** - `find_documents_with_boost()`:",
        "   - `auto_detect_intent=True` - Automatically boost docs for conceptual queries",
        "   - `prefer_docs=True` - Always boost documentation",
        "   - `custom_boosts` - Override boost factors",
        "4. **Search script updates**:",
        "   - Added `--prefer-docs` flag to always boost documentation",
        "   - Added `--no-boost` flag to disable boosting (raw TF-IDF)",
        "   - Shows document type indicator in results: `[DOCS]`, `[CODE]`, `[TEST]`",
        "   - Shows query intent detection: \"(Query type: conceptual/implementation)\"",
        "5. **Processor methods added**:",
        "   - `processor.find_documents_with_boost()` - Search with doc-type boosting",
        "   - `processor.is_conceptual_query()` - Check if query is conceptual",
        "6. **Tests added** - 17 new tests in `TestDocTypeBoost` and `TestDocTypeBoostIntegration`",
        "",
        "**Usage:**",
        "```bash",
        "# Auto-detect intent (default)",
        "python scripts/search_codebase.py \"what is PageRank\"",
        "# (Query type: conceptual) → docs boosted",
        "# Force docs preference",
        "python scripts/search_codebase.py \"PageRank\" --prefer-docs",
        "# Disable boosting (raw TF-IDF)",
        "python scripts/search_codebase.py \"PageRank\" --no-boost",
        "```",
        "**Status:** [x] Completed (2025-12-11) - Implemented as part of Task #63",
        "**Solution Applied:**",
        "Added document type labels to search results output:",
        "- `[CODE]` - Code files (.py)",
        "- `[DOCS]` - Documentation in docs/ folder",
        "- `[DOC]` - Root-level markdown files",
        "- `[TEST]` - Test files in tests/",
        "**New Output:**"
      ],
      "lines_removed": [
        "**Files:** `scripts/index_codebase.py`",
        "**Status:** [ ] Not Started",
        "**Current Code (line 859):**",
        "```python",
        "# Metadata is None - no file_type, no headings, nothing!",
        "documents = [(doc_id, content, None) for doc_id, content in all_docs.items()]",
        "```",
        "",
        "**Additional Gaps:**",
        "1. `index_file()` only extracts metadata for Python files (lines 481-486)",
        "2. Markdown files get no special treatment (no heading extraction)",
        "3. Chunk format doesn't store metadata at all",
        "",
        "**Solution:**",
        "1. Add `doc_type` field to chunk operations:",
        "   ```json",
        "   {\"op\": \"add\", \"doc_id\": \"docs/algorithms.md\", \"doc_type\": \"docs\", \"headings\": [\"PageRank\", \"TF-IDF\"]}",
        "   ```",
        "",
        "2. Extract markdown headings during indexing:",
        "   ```python",
        "   if file_path.suffix == '.md':",
        "       headings = re.findall(r'^##+ (.+)$', content, re.MULTILINE)",
        "       metadata['headings'] = headings",
        "       metadata['doc_type'] = 'docs' if doc_id.startswith('docs/') else 'root_docs'",
        "   ```",
        "",
        "3. Update `ChunkWriter` and `ChunkLoader` to preserve metadata",
        "4. Pass metadata when building processor from chunks:",
        "   ```python",
        "   documents = [(doc_id, content, metadata) for doc_id, content, metadata in all_docs]",
        "   ```",
        "**This is a prerequisite for Task #63** - without metadata, we can't boost docs in search.",
        "**Files:** `cortical/query.py`, `scripts/search_codebase.py`",
        "**Status:** [ ] Not Started",
        "**Observed Behavior:**",
        "```bash",
        "$ python scripts/search_codebase.py \"PageRank algorithm\"",
        "# Returns: cortical/processor.py:578 (score: 1.904)",
        "# Expected: docs/algorithms.md (has \"PageRank - Importance Scoring\" section)",
        "$ python scripts/search_codebase.py \"4-layer architecture\"",
        "# Returns: cortical/processor.py:578 (score: 6.973)",
        "# Expected: docs/architecture.md (entire file about this topic)",
        "```",
        "**Root Causes:**",
        "1. Code files have higher token density → higher TF-IDF scores",
        "2. No document-type awareness in ranking",
        "3. Natural language in docs may not match stemmed query terms",
        "4. Section headings in markdown aren't weighted higher",
        "**Proposed Solutions:**",
        "1. **Document-type boosting** - Boost .md files for conceptual queries:",
        "   ```python",
        "   if doc_id.endswith('.md'):",
        "       score *= 1.5  # Boost documentation",
        "   ```",
        "2. **Heading extraction** - Index markdown headings as high-weight terms:",
        "   ```python",
        "   # Extract ## headings and boost their terms",
        "   headings = re.findall(r'^##+ (.+)$', content, re.MULTILINE)",
        "   ```",
        "3. **Query intent detection** - Use existing `parse_intent_query()` to detect conceptual vs implementation queries:",
        "   - \"what is PageRank\" → boost docs",
        "   - \"where is PageRank computed\" → boost code",
        "4. **Add `--prefer-docs` flag** to search_codebase.py for documentation-first search",
        "**Success Criteria:**",
        "- `\"PageRank algorithm\"` returns docs/algorithms.md as top result",
        "- `\"4-layer architecture\"` returns docs/architecture.md as top result",
        "- `\"compute pagerank\"` still returns code (implementation query)",
        "**Status:** [ ] Not Started",
        "**Current Output:**",
        "```",
        "[1] cortical/processor.py:578",
        "    Score: 1.904",
        "```",
        "**Proposed Output:**"
      ],
      "context_before": [
        "---",
        "",
        "## Dog-Fooding Findings (2025-12-10)",
        "",
        "The following issues were identified during a dog-fooding session reviewing the docs folder and testing search quality.",
        "",
        "---",
        "",
        "### 65. Add Document Metadata to Chunk-Based Indexing (Prerequisite)",
        ""
      ],
      "context_after": [
        "**Priority:** High",
        "**Blocks:** #63",
        "",
        "**Problem:**",
        "Chunk-based indexing loses all document metadata, making it impossible to implement document-type boosting for search.",
        "",
        "",
        "",
        "",
        "---",
        "",
        "### 63. Improve Search Ranking for Documentation Files",
        "",
        "**Priority:** High",
        "**Depends on:** #65",
        "",
        "**Problem:**",
        "When searching for conceptual terms like \"PageRank algorithm\" or \"4-layer architecture\", the search returns code implementations (processor.py) instead of documentation files (docs/algorithms.md, docs/architecture.md) that explicitly explain these concepts.",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "---",
        "",
        "### 64. Add Document Type Indicator to Search Results",
        "",
        "**Files:** `scripts/search_codebase.py`",
        "**Priority:** Low",
        "",
        "**Problem:**",
        "Search results show file paths but don't indicate document type at a glance. Users can't quickly distinguish documentation from code without reading the path.",
        "",
        "",
        "```",
        "[1] [CODE] cortical/processor.py:578",
        "    Score: 1.904",
        "",
        "[2] [DOCS] docs/algorithms.md:19",
        "    Score: 1.856",
        "```",
        "",
        "**Implementation:**",
        "```python"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/chunk_index.py",
      "function": "from typing import Dict, List, Optional, Tuple, Any",
      "start_line": 42,
      "lines_added": [
        "    metadata: Optional[Dict[str, Any]] = None  # Document metadata (doc_type, headings, etc.)",
        "        if self.metadata is not None:",
        "            d['metadata'] = self.metadata",
        "            mtime=d.get('mtime'),",
        "            metadata=d.get('metadata')"
      ],
      "lines_removed": [
        "            mtime=d.get('mtime')"
      ],
      "context_before": [
        "CHUNK_VERSION = 1",
        "",
        "",
        "@dataclass",
        "class ChunkOperation:",
        "    \"\"\"A single operation in a chunk (add, modify, or delete).\"\"\"",
        "    op: str  # 'add', 'modify', 'delete'",
        "    doc_id: str",
        "    content: Optional[str] = None  # None for delete operations",
        "    mtime: Optional[float] = None  # Modification time"
      ],
      "context_after": [
        "",
        "    def to_dict(self) -> Dict[str, Any]:",
        "        \"\"\"Convert to dictionary for JSON serialization.\"\"\"",
        "        d = {'op': self.op, 'doc_id': self.doc_id}",
        "        if self.content is not None:",
        "            d['content'] = self.content",
        "        if self.mtime is not None:",
        "            d['mtime'] = self.mtime",
        "        return d",
        "",
        "    @classmethod",
        "    def from_dict(cls, d: Dict[str, Any]) -> 'ChunkOperation':",
        "        \"\"\"Create from dictionary.\"\"\"",
        "        return cls(",
        "            op=d['op'],",
        "            doc_id=d['doc_id'],",
        "            content=d.get('content'),",
        "        )",
        "",
        "",
        "@dataclass",
        "class Chunk:",
        "    \"\"\"A chunk containing operations from a single indexing session.\"\"\"",
        "    version: int",
        "    timestamp: str",
        "    session_id: str",
        "    branch: str"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/chunk_index.py",
      "function": "class ChunkWriter:",
      "start_line": 135,
      "lines_added": [
        "    def add_document(",
        "        self,",
        "        doc_id: str,",
        "        content: str,",
        "        mtime: Optional[float] = None,",
        "        metadata: Optional[Dict[str, Any]] = None",
        "    ):",
        "            mtime=mtime,",
        "            metadata=metadata",
        "    def modify_document(",
        "        self,",
        "        doc_id: str,",
        "        content: str,",
        "        mtime: Optional[float] = None,",
        "        metadata: Optional[Dict[str, Any]] = None",
        "    ):",
        "            mtime=mtime,",
        "            metadata=metadata"
      ],
      "lines_removed": [
        "    def add_document(self, doc_id: str, content: str, mtime: Optional[float] = None):",
        "            mtime=mtime",
        "    def modify_document(self, doc_id: str, content: str, mtime: Optional[float] = None):",
        "            mtime=mtime"
      ],
      "context_before": [
        "                capture_output=True,",
        "                text=True,",
        "                timeout=5",
        "            )",
        "            if result.returncode == 0:",
        "                return result.stdout.strip()",
        "        except (subprocess.TimeoutExpired, FileNotFoundError):",
        "            pass",
        "        return 'unknown'",
        ""
      ],
      "context_after": [
        "        \"\"\"Record an add operation.\"\"\"",
        "        self.operations.append(ChunkOperation(",
        "            op='add',",
        "            doc_id=doc_id,",
        "            content=content,",
        "        ))",
        "",
        "        \"\"\"Record a modify operation.\"\"\"",
        "        self.operations.append(ChunkOperation(",
        "            op='modify',",
        "            doc_id=doc_id,",
        "            content=content,",
        "        ))",
        "",
        "    def delete_document(self, doc_id: str):",
        "        \"\"\"Record a delete operation.\"\"\"",
        "        self.operations.append(ChunkOperation(",
        "            op='delete',",
        "            doc_id=doc_id",
        "        ))",
        "",
        "    def has_operations(self) -> bool:"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/chunk_index.py",
      "function": "class ChunkWriter:",
      "start_line": 201,
      "lines_added": [
        "        metadata = loader.get_metadata()  # Returns {doc_id: metadata_dict}",
        "        self._metadata: Dict[str, Dict[str, Any]] = {}"
      ],
      "lines_removed": [],
      "context_before": [
        "        return filepath",
        "",
        "",
        "class ChunkLoader:",
        "    \"\"\"",
        "    Loads and combines chunks to rebuild document state.",
        "",
        "    Usage:",
        "        loader = ChunkLoader(chunks_dir='corpus_chunks')",
        "        documents = loader.load_all()  # Returns {doc_id: content}"
      ],
      "context_after": [
        "",
        "        # Check if cache is valid",
        "        if loader.is_cache_valid('corpus_dev.pkl'):",
        "            # Load from pkl",
        "        else:",
        "            # Rebuild from documents",
        "    \"\"\"",
        "",
        "    def __init__(self, chunks_dir: str = 'corpus_chunks'):",
        "        self.chunks_dir = Path(chunks_dir)",
        "        self._chunks: List[Chunk] = []",
        "        self._documents: Dict[str, str] = {}",
        "        self._mtimes: Dict[str, float] = {}",
        "        self._loaded = False",
        "",
        "    def get_chunk_files(self) -> List[Path]:",
        "        \"\"\"Get all chunk files sorted by timestamp.\"\"\"",
        "        if not self.chunks_dir.exists():",
        "            return []",
        "",
        "        files = list(self.chunks_dir.glob('*.json'))",
        "        # Sort by filename (which starts with timestamp)",
        "        return sorted(files, key=lambda p: p.name)"
      ],
      "change_type": "add"
    },
    {
      "file": "cortical/chunk_index.py",
      "function": "class ChunkLoader:",
      "start_line": 244,
      "lines_added": [
        "        self._metadata = {}",
        "                    if op.metadata:",
        "                        self._metadata[op.doc_id] = op.metadata",
        "                    if op.metadata:",
        "                        self._metadata[op.doc_id] = op.metadata",
        "                    self._metadata.pop(op.doc_id, None)",
        "    def get_metadata(self) -> Dict[str, Dict[str, Any]]:",
        "        \"\"\"Get document metadata (doc_type, headings, etc.).\"\"\"",
        "        if not self._loaded:",
        "            self.load_all()",
        "        return self._metadata",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "",
        "        Returns:",
        "            Dictionary mapping doc_id to content.",
        "        \"\"\"",
        "        if self._loaded:",
        "            return self._documents",
        "",
        "        self._chunks = []",
        "        self._documents = {}",
        "        self._mtimes = {}"
      ],
      "context_after": [
        "",
        "        for filepath in self.get_chunk_files():",
        "            chunk = self.load_chunk(filepath)",
        "            self._chunks.append(chunk)",
        "",
        "            # Replay operations",
        "            for op in chunk.operations:",
        "                if op.op == 'add':",
        "                    self._documents[op.doc_id] = op.content",
        "                    if op.mtime:",
        "                        self._mtimes[op.doc_id] = op.mtime",
        "                elif op.op == 'modify':",
        "                    self._documents[op.doc_id] = op.content",
        "                    if op.mtime:",
        "                        self._mtimes[op.doc_id] = op.mtime",
        "                elif op.op == 'delete':",
        "                    self._documents.pop(op.doc_id, None)",
        "                    self._mtimes.pop(op.doc_id, None)",
        "",
        "        self._loaded = True",
        "        return self._documents",
        "",
        "    def get_documents(self) -> Dict[str, str]:",
        "        \"\"\"Get loaded documents (calls load_all if needed).\"\"\"",
        "        if not self._loaded:",
        "            self.load_all()",
        "        return self._documents",
        "",
        "    def get_mtimes(self) -> Dict[str, float]:",
        "        \"\"\"Get document modification times.\"\"\"",
        "        if not self._loaded:",
        "            self.load_all()",
        "        return self._mtimes",
        "",
        "    def get_chunks(self) -> List[Chunk]:",
        "        \"\"\"Get loaded chunks.\"\"\"",
        "        if not self._loaded:",
        "            self.load_all()",
        "        return self._chunks",
        "",
        "    def compute_hash(self) -> str:",
        "        \"\"\"",
        "        Compute hash of current document state.",
        ""
      ],
      "change_type": "add"
    },
    {
      "file": "cortical/chunk_index.py",
      "function": "class ChunkCompactor:",
      "start_line": 442,
      "lines_added": [
        "        metadata = {}",
        "                    if op.metadata:",
        "                        metadata[op.doc_id] = op.metadata",
        "                    metadata.pop(op.doc_id, None)",
        "            writer.add_document(doc_id, content, mtimes.get(doc_id), metadata.get(doc_id))"
      ],
      "lines_removed": [
        "            writer.add_document(doc_id, content, mtimes.get(doc_id))"
      ],
      "context_before": [
        "            return {",
        "                'status': 'dry_run',",
        "                'would_compact': len(to_compact),",
        "                'would_keep': len(to_keep),",
        "                'files_to_compact': [str(f) for f in to_compact]",
        "            }",
        "",
        "        # Load and merge chunks to compact",
        "        documents = {}",
        "        mtimes = {}"
      ],
      "context_after": [
        "",
        "        for filepath in to_compact:",
        "            chunk = loader.load_chunk(filepath)",
        "            for op in chunk.operations:",
        "                if op.op in ('add', 'modify'):",
        "                    documents[op.doc_id] = op.content",
        "                    if op.mtime:",
        "                        mtimes[op.doc_id] = op.mtime",
        "                elif op.op == 'delete':",
        "                    documents.pop(op.doc_id, None)",
        "                    mtimes.pop(op.doc_id, None)",
        "",
        "        # Create compacted chunk with all remaining documents as 'add' operations",
        "        writer = ChunkWriter(str(self.chunks_dir))",
        "        writer.timestamp = datetime.now().isoformat(timespec='seconds')",
        "        writer.session_id = 'compacted_' + uuid.uuid4().hex[:8]",
        "",
        "        for doc_id, content in sorted(documents.items()):",
        "",
        "        # Save compacted chunk",
        "        compacted_path = None",
        "        if writer.has_operations():",
        "            compacted_path = writer.save()",
        "",
        "        # Delete old chunk files",
        "        for filepath in to_compact:",
        "            filepath.unlink()",
        ""
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/processor.py",
      "function": "class CorticalTextProcessor:",
      "start_line": 1654,
      "lines_added": [
        "    def find_documents_with_boost(",
        "        self,",
        "        query_text: str,",
        "        top_n: int = 5,",
        "        auto_detect_intent: bool = True,",
        "        prefer_docs: bool = False,",
        "        custom_boosts: Optional[Dict[str, float]] = None,",
        "        use_expansion: bool = True,",
        "        use_semantic: bool = True",
        "    ) -> List[Tuple[str, float]]:",
        "        \"\"\"",
        "        Find documents with optional document-type boosting.",
        "",
        "        This extends find_documents_for_query with doc_type boosting",
        "        for improved ranking of documentation vs code.",
        "",
        "        For conceptual queries (\"what is PageRank\", \"explain architecture\"),",
        "        documentation files are boosted. For implementation queries",
        "        (\"where is PageRank computed\"), code files rank higher.",
        "",
        "        Args:",
        "            query_text: Search query",
        "            top_n: Number of results to return",
        "            auto_detect_intent: If True, auto-boost docs for conceptual queries",
        "            prefer_docs: If True, always boost documentation",
        "            custom_boosts: Optional custom boost factors per doc_type:",
        "                {'docs': 1.5, 'root_docs': 1.3, 'code': 1.0, 'test': 0.8}",
        "            use_expansion: Whether to expand query terms",
        "            use_semantic: Whether to use semantic relations",
        "",
        "        Returns:",
        "            List of (doc_id, score) tuples ranked by relevance",
        "        \"\"\"",
        "        return query_module.find_documents_with_boost(",
        "            query_text,",
        "            self.layers,",
        "            self.tokenizer,",
        "            top_n=top_n,",
        "            doc_metadata=self.document_metadata,",
        "            auto_detect_intent=auto_detect_intent,",
        "            prefer_docs=prefer_docs,",
        "            custom_boosts=custom_boosts,",
        "            use_expansion=use_expansion,",
        "            semantic_relations=self.semantic_relations if use_semantic else None,",
        "            use_semantic=use_semantic",
        "        )",
        "",
        "    def is_conceptual_query(self, query_text: str) -> bool:",
        "        \"\"\"",
        "        Check if a query appears to be conceptual (should prefer docs).",
        "",
        "        Args:",
        "            query_text: Query to analyze",
        "",
        "        Returns:",
        "            True if query is conceptual, False if implementation-focused",
        "        \"\"\"",
        "        return query_module.is_conceptual_query(query_text)",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "        \"\"\"",
        "        return query_module.fast_find_documents(",
        "            query_text,",
        "            self.layers,",
        "            self.tokenizer,",
        "            top_n=top_n,",
        "            candidate_multiplier=candidate_multiplier,",
        "            use_code_concepts=use_code_concepts",
        "        )",
        ""
      ],
      "context_after": [
        "    def build_search_index(self) -> Dict[str, Dict[str, float]]:",
        "        \"\"\"",
        "        Build an optimized inverted index for fast querying.",
        "",
        "        Pre-compute this once, then use search_with_index() for",
        "        fastest possible search.",
        "",
        "        Returns:",
        "            Dict mapping terms to {doc_id: tfidf_score} dicts",
        "        \"\"\""
      ],
      "change_type": "add"
    },
    {
      "file": "cortical/query.py",
      "function": null,
      "start_line": 2,
      "lines_added": [
        "from typing import Dict, List, Tuple, Optional, TypedDict, Any"
      ],
      "lines_removed": [
        "from typing import Dict, List, Tuple, Optional, TypedDict"
      ],
      "context_before": [
        "Query Module",
        "============",
        "",
        "Query expansion and search functionality.",
        "",
        "Provides methods for expanding queries using lateral connections,",
        "concept clusters, and word variants, then searching the corpus",
        "using TF-IDF and graph-based scoring.",
        "\"\"\"",
        ""
      ],
      "context_after": [
        "from collections import defaultdict",
        "import re",
        "",
        "from .layers import CorticalLayer, HierarchicalLayer",
        "from .tokenizer import Tokenizer",
        "from .code_concepts import expand_code_concepts, get_related_terms",
        "",
        "",
        "# Intent types for query understanding",
        "class ParsedIntent(TypedDict):"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/query.py",
      "function": "ACTION_VERBS = frozenset([",
      "start_line": 45,
      "lines_added": [
        "# =============================================================================",
        "# Document Type Boosting for Search",
        "# =============================================================================",
        "",
        "# Default boost factors for each document type",
        "# Higher values make documents of that type rank higher",
        "DOC_TYPE_BOOSTS = {",
        "    'docs': 1.5,       # docs/ folder documentation",
        "    'root_docs': 1.3,  # Root-level markdown (CLAUDE.md, README.md)",
        "    'code': 1.0,       # Regular code files",
        "    'test': 0.8,       # Test files (often less relevant for conceptual queries)",
        "}",
        "",
        "# Keywords that suggest a conceptual query (should boost documentation)",
        "CONCEPTUAL_KEYWORDS = frozenset([",
        "    'what', 'explain', 'describe', 'overview', 'introduction', 'concept',",
        "    'architecture', 'design', 'pattern', 'algorithm', 'approach', 'method',",
        "    'how does', 'why does', 'purpose', 'goal', 'rationale', 'theory',",
        "    'understand', 'learn', 'documentation', 'guide', 'tutorial', 'example',",
        "])",
        "",
        "# Keywords that suggest an implementation query (should prefer code)",
        "IMPLEMENTATION_KEYWORDS = frozenset([",
        "    'where', 'implement', 'code', 'function', 'class', 'method', 'variable',",
        "    'line', 'file', 'bug', 'fix', 'error', 'exception', 'call', 'invoke',",
        "    'compute', 'calculate', 'return', 'parameter', 'argument',",
        "])",
        "",
        "",
        "def is_conceptual_query(query_text: str) -> bool:",
        "    \"\"\"",
        "    Determine if a query is conceptual (should boost documentation).",
        "",
        "    Conceptual queries ask about concepts, architecture, design, or",
        "    explanations rather than specific code locations.",
        "",
        "    Args:",
        "        query_text: The search query",
        "",
        "    Returns:",
        "        True if the query appears to be conceptual",
        "    \"\"\"",
        "    query_lower = query_text.lower()",
        "",
        "    # Check for conceptual keywords",
        "    conceptual_score = sum(",
        "        1 for kw in CONCEPTUAL_KEYWORDS if kw in query_lower",
        "    )",
        "",
        "    # Check for implementation keywords",
        "    implementation_score = sum(",
        "        1 for kw in IMPLEMENTATION_KEYWORDS if kw in query_lower",
        "    )",
        "",
        "    # Boost if query starts with \"what is\" or \"how does\"",
        "    if query_lower.startswith(('what is', 'what are', 'how does', 'explain')):",
        "        conceptual_score += 2",
        "",
        "    return conceptual_score > implementation_score",
        "",
        "",
        "def get_doc_type_boost(",
        "    doc_id: str,",
        "    doc_metadata: Optional[Dict[str, Dict[str, Any]]] = None,",
        "    custom_boosts: Optional[Dict[str, float]] = None",
        ") -> float:",
        "    \"\"\"",
        "    Get the boost factor for a document based on its type.",
        "",
        "    Args:",
        "        doc_id: Document ID",
        "        doc_metadata: Optional metadata dict {doc_id: {doc_type: ..., ...}}",
        "        custom_boosts: Optional custom boost factors",
        "",
        "    Returns:",
        "        Boost factor (1.0 = no boost)",
        "    \"\"\"",
        "    boosts = custom_boosts or DOC_TYPE_BOOSTS",
        "",
        "    # If we have metadata, use doc_type",
        "    if doc_metadata and doc_id in doc_metadata:",
        "        doc_type = doc_metadata[doc_id].get('doc_type', 'code')",
        "        return boosts.get(doc_type, 1.0)",
        "",
        "    # Fallback: infer from doc_id path",
        "    if doc_id.endswith('.md'):",
        "        if doc_id.startswith('docs/'):",
        "            return boosts.get('docs', 1.5)",
        "        return boosts.get('root_docs', 1.3)",
        "    elif doc_id.startswith('tests/'):",
        "        return boosts.get('test', 0.8)",
        "    return boosts.get('code', 1.0)",
        "",
        "",
        "def apply_doc_type_boost(",
        "    results: List[Tuple[str, float]],",
        "    doc_metadata: Optional[Dict[str, Dict[str, Any]]] = None,",
        "    boost_docs: bool = True,",
        "    custom_boosts: Optional[Dict[str, float]] = None",
        ") -> List[Tuple[str, float]]:",
        "    \"\"\"",
        "    Apply document type boosting to search results.",
        "",
        "    Args:",
        "        results: List of (doc_id, score) tuples",
        "        doc_metadata: Optional metadata dict {doc_id: {doc_type: ..., ...}}",
        "        boost_docs: Whether to apply boosting",
        "        custom_boosts: Optional custom boost factors",
        "",
        "    Returns:",
        "        Re-ranked list of (doc_id, score) tuples",
        "    \"\"\"",
        "    if not boost_docs:",
        "        return results",
        "",
        "    boosted = []",
        "    for doc_id, score in results:",
        "        boost = get_doc_type_boost(doc_id, doc_metadata, custom_boosts)",
        "        boosted.append((doc_id, score * boost))",
        "",
        "    # Re-sort by boosted scores",
        "    boosted.sort(key=lambda x: -x[1])",
        "    return boosted",
        "",
        "",
        "def find_documents_with_boost(",
        "    query_text: str,",
        "    layers: Dict[CorticalLayer, HierarchicalLayer],",
        "    tokenizer: Tokenizer,",
        "    top_n: int = 5,",
        "    doc_metadata: Optional[Dict[str, Dict[str, Any]]] = None,",
        "    auto_detect_intent: bool = True,",
        "    prefer_docs: bool = False,",
        "    custom_boosts: Optional[Dict[str, float]] = None,",
        "    use_expansion: bool = True,",
        "    semantic_relations: Optional[List[Tuple[str, str, str, float]]] = None,",
        "    use_semantic: bool = True",
        ") -> List[Tuple[str, float]]:",
        "    \"\"\"",
        "    Find documents with optional document-type boosting.",
        "",
        "    This extends find_documents_for_query with doc_type boosting",
        "    for improved ranking of documentation vs code.",
        "",
        "    Args:",
        "        query_text: Search query",
        "        layers: Dictionary of layers",
        "        tokenizer: Tokenizer instance",
        "        top_n: Number of results to return",
        "        doc_metadata: Optional document metadata for boosting",
        "        auto_detect_intent: If True, automatically boost docs for conceptual queries",
        "        prefer_docs: If True, always boost documentation (overrides auto_detect)",
        "        custom_boosts: Optional custom boost factors per doc_type",
        "        use_expansion: Whether to expand query terms",
        "        semantic_relations: Optional semantic relations for expansion",
        "        use_semantic: Whether to use semantic relations",
        "",
        "    Returns:",
        "        List of (doc_id, score) tuples ranked by relevance",
        "    \"\"\"",
        "    # Get base results (fetching more to allow re-ranking)",
        "    base_results = find_documents_for_query(",
        "        query_text, layers, tokenizer,",
        "        top_n=top_n * 2,  # Get more candidates for re-ranking",
        "        use_expansion=use_expansion,",
        "        semantic_relations=semantic_relations,",
        "        use_semantic=use_semantic",
        "    )",
        "",
        "    # Determine if we should boost docs",
        "    should_boost = prefer_docs or (auto_detect_intent and is_conceptual_query(query_text))",
        "",
        "    if should_boost:",
        "        boosted = apply_doc_type_boost(",
        "            base_results, doc_metadata, True, custom_boosts",
        "        )",
        "        return boosted[:top_n]",
        "",
        "    return base_results[:top_n]",
        "",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "    'load', 'save', 'store', 'validate', 'check', 'parse', 'format', 'convert',",
        "    'transform', 'render', 'display', 'show', 'hide', 'enable', 'disable',",
        "    'start', 'stop', 'init', 'initialize', 'setup', 'configure', 'connect',",
        "    'disconnect', 'send', 'receive', 'read', 'write', 'open', 'close',",
        "    'authenticate', 'authorize', 'login', 'logout', 'register', 'subscribe',",
        "    'publish', 'emit', 'listen', 'dispatch', 'trigger', 'call', 'invoke',",
        "    'execute', 'run', 'build', 'compile', 'test', 'deploy', 'implement',",
        "])",
        "",
        ""
      ],
      "context_after": [
        "def expand_query(",
        "    query_text: str,",
        "    layers: Dict[CorticalLayer, HierarchicalLayer],",
        "    tokenizer: Tokenizer,",
        "    max_expansions: int = 10,",
        "    use_lateral: bool = True,",
        "    use_concepts: bool = True,",
        "    use_variants: bool = True,",
        "    use_code_concepts: bool = False",
        ") -> Dict[str, float]:"
      ],
      "change_type": "add"
    },
    {
      "file": "scripts/index_codebase.py",
      "function": "Usage:",
      "start_line": 12,
      "lines_added": [
        "import platform",
        "import threading"
      ],
      "lines_removed": [],
      "context_before": [
        "    python scripts/index_codebase.py --incremental  # Only index changes",
        "    python scripts/index_codebase.py --status       # Show what would change",
        "    python scripts/index_codebase.py --force        # Force full rebuild",
        "    python scripts/index_codebase.py --log indexer.log  # Log to file",
        "\"\"\"",
        "",
        "import argparse",
        "import json",
        "import logging",
        "import os"
      ],
      "context_after": [
        "import signal",
        "import sys",
        "import time",
        "from contextlib import contextmanager",
        "from dataclasses import dataclass, field",
        "from datetime import datetime",
        "from pathlib import Path",
        "from typing import Dict, List, Optional, Tuple, Any",
        "",
        "# Add parent directory to path for imports",
        "sys.path.insert(0, str(Path(__file__).parent.parent))",
        ""
      ],
      "change_type": "add"
    },
    {
      "file": "scripts/index_codebase.py",
      "function": "class ProgressTracker:",
      "start_line": 238,
      "lines_added": [
        "class IndexingTimeoutError(Exception):",
        "# Check platform for timeout implementation",
        "_IS_WINDOWS = platform.system() == 'Windows'",
        "",
        "",
        "    Uses signal.SIGALRM on Unix systems and threading.Timer on Windows.",
        "    Note: Windows implementation cannot interrupt blocking I/O operations.",
        "",
        "    if _IS_WINDOWS:",
        "        # Windows implementation using threading.Timer",
        "        # Note: This cannot interrupt blocking operations like file I/O",
        "        timed_out = threading.Event()",
        "        def timeout_callback():",
        "            timed_out.set()",
        "            msg = f\"Indexing timed out after {seconds} seconds\"",
        "            if tracker:",
        "                tracker.error(msg)",
        "                tracker.error(\"Note: Windows timeout cannot interrupt blocking I/O\")",
        "        timer = threading.Timer(seconds, timeout_callback)",
        "        timer.daemon = True  # Don't prevent program exit",
        "        timer.start()",
        "",
        "        try:",
        "            yield",
        "            # Check if timeout occurred (for non-blocking operations)",
        "            if timed_out.is_set():",
        "                if tracker:",
        "                    tracker.print_summary()",
        "                raise IndexingTimeoutError(f\"Indexing timed out after {seconds} seconds\")",
        "        finally:",
        "            timer.cancel()",
        "    else:",
        "        # Unix implementation using signal.SIGALRM",
        "        def handler(signum, frame):",
        "            msg = f\"Indexing timed out after {seconds} seconds\"",
        "            if tracker:",
        "                tracker.error(msg)",
        "                tracker.print_summary()",
        "            raise IndexingTimeoutError(msg)",
        "",
        "        # Set the signal handler",
        "        old_handler = signal.signal(signal.SIGALRM, handler)",
        "        signal.alarm(seconds)",
        "",
        "        try:",
        "            yield",
        "        finally:",
        "            # Restore the old handler and cancel the alarm",
        "            signal.alarm(0)",
        "            signal.signal(signal.SIGALRM, old_handler)"
      ],
      "lines_removed": [
        "class TimeoutError(Exception):",
        "    def handler(signum, frame):",
        "        msg = f\"Indexing timed out after {seconds} seconds\"",
        "        if tracker:",
        "            tracker.error(msg)",
        "            tracker.print_summary()",
        "        raise TimeoutError(msg)",
        "    # Set the signal handler",
        "    old_handler = signal.signal(signal.SIGALRM, handler)",
        "    signal.alarm(seconds)",
        "    try:",
        "        yield",
        "    finally:",
        "        # Restore the old handler and cancel the alarm",
        "        signal.alarm(0)",
        "        signal.signal(signal.SIGALRM, old_handler)"
      ],
      "context_before": [
        "                self.log(f\"  - {e}\", \"error\")",
        "",
        "        if self.log_file:",
        "            self.log(f\"\\nFull log written to: {self.log_file}\", \"info\")",
        "",
        "",
        "# =============================================================================",
        "# Timeout Handler",
        "# =============================================================================",
        ""
      ],
      "context_after": [
        "    \"\"\"Raised when indexing exceeds the timeout.\"\"\"",
        "    pass",
        "",
        "",
        "@contextmanager",
        "def timeout_handler(seconds: int, tracker: Optional[ProgressTracker] = None):",
        "    \"\"\"",
        "    Context manager for timeout handling.",
        "",
        "    Args:",
        "        seconds: Timeout in seconds (0 = no timeout)",
        "        tracker: Optional progress tracker for logging",
        "    \"\"\"",
        "    if seconds <= 0:",
        "        yield",
        "        return",
        "",
        "",
        "",
        "",
        "",
        "# =============================================================================",
        "# Manifest Operations",
        "# =============================================================================",
        "",
        "def get_manifest_path(corpus_path: Path) -> Path:",
        "    \"\"\"Get the manifest file path based on corpus path.\"\"\"",
        "    return corpus_path.with_suffix('.manifest.json')",
        ""
      ],
      "change_type": "modify"
    },
    {
      "file": "scripts/index_codebase.py",
      "function": "def get_doc_files(base_path: Path) -> list:",
      "start_line": 442,
      "lines_added": [
        "def extract_markdown_headings(content: str) -> List[str]:",
        "    \"\"\"Extract section headings from markdown content.\"\"\"",
        "    import re",
        "    # Match ## and ### headings (skip # as it's usually the title)",
        "    headings = re.findall(r'^##+ (.+)$', content, re.MULTILINE)",
        "    return headings",
        "",
        "",
        "def get_doc_type(doc_id: str) -> str:",
        "    \"\"\"",
        "    Determine document type from document ID.",
        "",
        "    Returns:",
        "        One of: 'code', 'test', 'docs', 'root_docs'",
        "    \"\"\"",
        "    if doc_id.startswith('tests/'):",
        "        return 'test'",
        "    elif doc_id.startswith('docs/'):",
        "        return 'docs'",
        "    elif doc_id.endswith('.md'):",
        "        return 'root_docs'",
        "    else:",
        "        return 'code'",
        "",
        "",
        "def _extract_file_metadata(",
        "    doc_id: str,",
        "    file_path: Path,",
        "    content: str,",
        "    mtime: float",
        ") -> Dict[str, Any]:",
        "    \"\"\"",
        "    Extract metadata from a file for chunk storage.",
        "",
        "    Args:",
        "        doc_id: Document ID (relative path)",
        "        file_path: Full file path",
        "        content: File content",
        "        mtime: File modification time",
        "",
        "    Returns:",
        "        Metadata dictionary with doc_type, headings (for md), etc.",
        "    \"\"\"",
        "    metadata = {",
        "        'relative_path': doc_id,",
        "        'file_type': file_path.suffix,",
        "        'line_count': content.count('\\n') + 1,",
        "        'mtime': mtime,",
        "        'doc_type': get_doc_type(doc_id),",
        "    }",
        "",
        "    # For Python files, extract additional metadata",
        "    if file_path.suffix == '.py':",
        "        metadata['language'] = 'python'",
        "        metadata['function_count'] = content.count('\\ndef ')",
        "        metadata['class_count'] = content.count('\\nclass ')",
        "",
        "    # For Markdown files, extract headings",
        "    if file_path.suffix == '.md':",
        "        metadata['language'] = 'markdown'",
        "        metadata['headings'] = extract_markdown_headings(content)",
        "",
        "    return metadata",
        "",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "",
        "    return files",
        "",
        "",
        "def create_doc_id(file_path: Path, base_path: Path) -> str:",
        "    \"\"\"Create a document ID from file path.\"\"\"",
        "    rel_path = file_path.relative_to(base_path)",
        "    return str(rel_path)",
        "",
        ""
      ],
      "context_after": [
        "# =============================================================================",
        "# Indexing Operations",
        "# =============================================================================",
        "",
        "def index_file(",
        "    processor: CorticalTextProcessor,",
        "    file_path: Path,",
        "    base_path: Path,",
        "    tracker: Optional[ProgressTracker] = None",
        ") -> Optional[dict]:"
      ],
      "change_type": "add"
    },
    {
      "file": "scripts/index_codebase.py",
      "function": "def index_file(",
      "start_line": 469,
      "lines_added": [
        "        'doc_type': get_doc_type(doc_id),",
        "    # For Markdown files, extract headings",
        "    if file_path.suffix == '.md':",
        "        metadata['language'] = 'markdown'",
        "        metadata['headings'] = extract_markdown_headings(content)",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "            tracker.warn(f\"Could not read {doc_id}: {e}\")",
        "        return None",
        "",
        "    # Create metadata with file info",
        "    metadata = {",
        "        'file_path': str(file_path),",
        "        'relative_path': doc_id,",
        "        'file_type': file_path.suffix,",
        "        'line_count': content.count('\\n') + 1,",
        "        'mtime': get_file_mtime(file_path),"
      ],
      "context_after": [
        "    }",
        "",
        "    # For Python files, extract additional metadata",
        "    if file_path.suffix == '.py':",
        "        metadata['language'] = 'python'",
        "        # Count functions and classes",
        "        metadata['function_count'] = content.count('\\ndef ')",
        "        metadata['class_count'] = content.count('\\nclass ')",
        "",
        "    processor.process_document(doc_id, content, metadata=metadata)",
        "    return metadata",
        "",
        "",
        "def show_status(",
        "    added: List[Path],",
        "    modified: List[Path],",
        "    deleted: List[str],",
        "    base_path: Path,",
        "    tracker: ProgressTracker"
      ],
      "change_type": "add"
    },
    {
      "file": "scripts/index_codebase.py",
      "function": "def index_with_chunks(",
      "start_line": 800,
      "lines_added": [
        "            metadata = _extract_file_metadata(doc_id, file_path, content, mtime)",
        "            writer.add_document(doc_id, content, mtime, metadata)",
        "            metadata = _extract_file_metadata(doc_id, file_path, content, mtime)",
        "            writer.modify_document(doc_id, content, mtime, metadata)"
      ],
      "lines_removed": [
        "            writer.add_document(doc_id, content, mtime)",
        "            writer.modify_document(doc_id, content, mtime)"
      ],
      "context_before": [
        "    total_ops = len(added) + len(modified) + len(deleted)",
        "    tracker.start_phase(\"Recording chunk operations\", total_items=total_ops)",
        "    processed = 0",
        "",
        "    # Process added files",
        "    for doc_id in added:",
        "        file_path = base_path / doc_id",
        "        try:",
        "            content = file_path.read_text(encoding='utf-8')",
        "            mtime = get_file_mtime(file_path)"
      ],
      "context_after": [
        "            processed += 1",
        "            tracker.update_progress(processed, f\"Added: {doc_id}\" if args.verbose else None)",
        "        except Exception as e:",
        "            tracker.warn(f\"Error reading {doc_id}: {e}\")",
        "",
        "    # Process modified files",
        "    for doc_id in modified:",
        "        file_path = base_path / doc_id",
        "        try:",
        "            content = file_path.read_text(encoding='utf-8')",
        "            mtime = get_file_mtime(file_path)",
        "            processed += 1",
        "            tracker.update_progress(processed, f\"Modified: {doc_id}\" if args.verbose else None)",
        "        except Exception as e:",
        "            tracker.warn(f\"Error reading {doc_id}: {e}\")",
        "",
        "    # Record deletions",
        "    for doc_id in deleted:",
        "        writer.delete_document(doc_id)",
        "        processed += 1",
        "        tracker.update_progress(processed, f\"Deleted: {doc_id}\" if args.verbose else None)"
      ],
      "change_type": "modify"
    },
    {
      "file": "scripts/index_codebase.py",
      "function": "def index_with_chunks(",
      "start_line": 837,
      "lines_added": [
        "    all_metadata = loader.get_metadata()",
        "    tracker.log(f\"  Documents with metadata: {len(all_metadata)}\")",
        "        # Build processor from documents (with metadata)",
        "        documents = [",
        "            (doc_id, content, all_metadata.get(doc_id))",
        "            for doc_id, content in all_docs.items()",
        "        ]"
      ],
      "lines_removed": [
        "        # Build processor from documents",
        "        documents = [(doc_id, content, None) for doc_id, content in all_docs.items()]"
      ],
      "context_before": [
        "    tracker.start_phase(\"Saving chunk\")",
        "    chunk_path = writer.save()",
        "    if chunk_path:",
        "        tracker.log(f\"  Saved chunk: {chunk_path.name}\")",
        "    tracker.end_phase(\"Saving chunk\")",
        "",
        "    # Now rebuild processor from all chunks",
        "    tracker.start_phase(\"Loading documents from chunks\")",
        "    loader = ChunkLoader(str(chunks_dir))  # Reload to include new chunk",
        "    all_docs = loader.get_documents()"
      ],
      "context_after": [
        "    tracker.log(f\"  Total documents: {len(all_docs)}\")",
        "    tracker.end_phase(\"Loading documents from chunks\")",
        "",
        "    # Check if we can use cached pkl",
        "    cache_valid = loader.is_cache_valid(str(output_path))",
        "    if cache_valid and not (added or modified or deleted):",
        "        tracker.log(\"\\nCache is valid, loading from pkl...\")",
        "        processor = CorticalTextProcessor.load(str(output_path))",
        "    else:",
        "        tracker.start_phase(\"Building processor from chunks\")",
        "        processor = CorticalTextProcessor()",
        "        processor.add_documents_batch(documents, recompute='none', verbose=False)",
        "        tracker.log(f\"  Added {len(documents)} documents\")",
        "        tracker.end_phase(\"Building processor from chunks\")",
        "",
        "        # Compute analysis",
        "        fast_mode = not args.full_analysis",
        "        compute_analysis(processor, tracker, fast_mode=fast_mode)",
        "",
        "    # Print corpus statistics",
        "    tracker.log(\"\\nCorpus Statistics:\")"
      ],
      "change_type": "modify"
    },
    {
      "file": "scripts/index_codebase.py",
      "function": "Examples:",
      "start_line": 1010,
      "lines_added": [
        "    except IndexingTimeoutError:"
      ],
      "lines_removed": [
        "    except TimeoutError:"
      ],
      "context_before": [
        "    if args.compact:",
        "        run_compaction(args, base_path, tracker)",
        "        return",
        "",
        "    try:",
        "        with timeout_handler(args.timeout, tracker):",
        "            if args.use_chunks:",
        "                index_with_chunks(args, base_path, output_path, tracker)",
        "            else:",
        "                run_indexer(args, base_path, output_path, manifest_path, tracker)"
      ],
      "context_after": [
        "        tracker.log(\"\\nIndexing was terminated due to timeout.\", \"error\")",
        "        sys.exit(1)",
        "    except KeyboardInterrupt:",
        "        tracker.log(\"\\nIndexing was interrupted by user.\", \"warning\")",
        "        tracker.print_summary()",
        "        sys.exit(1)",
        "    except Exception as e:",
        "        tracker.error(f\"Unexpected error: {e}\")",
        "        tracker.print_summary()",
        "        raise"
      ],
      "change_type": "modify"
    },
    {
      "file": "scripts/search_codebase.py",
      "function": "def format_passage(passage: str, max_width: int = 80) -> str:",
      "start_line": 32,
      "lines_added": [
        "def get_doc_type_label(doc_id: str) -> str:",
        "    \"\"\"Get a display label for document type.\"\"\"",
        "    if doc_id.endswith('.md'):",
        "        if doc_id.startswith('docs/'):",
        "            return 'DOCS'",
        "        return 'DOC'",
        "    elif doc_id.startswith('tests/'):",
        "        return 'TEST'",
        "    return 'CODE'",
        "",
        "",
        "def search_codebase(",
        "    processor: CorticalTextProcessor,",
        "    query: str,",
        "    top_n: int = 5,",
        "    chunk_size: int = 400,",
        "    fast: bool = False,",
        "    prefer_docs: bool = False,",
        "    no_boost: bool = False",
        ") -> list:",
        "        prefer_docs: Always boost documentation files",
        "        no_boost: Disable all document type boosting",
        "        List of result dicts with 'file', 'line', 'passage', 'score', 'reference', 'doc_type'",
        "        # Fast mode with optional boosting",
        "        if no_boost:",
        "            doc_results = processor.fast_find_documents(query, top_n=top_n)",
        "        else:",
        "            doc_results = processor.find_documents_with_boost(",
        "                query,",
        "                top_n=top_n,",
        "                auto_detect_intent=not prefer_docs,",
        "                prefer_docs=prefer_docs",
        "            )",
        "                'reference': f\"{doc_id}:1\",",
        "                'doc_type': get_doc_type_label(doc_id)",
        "    # Full passage search - first get top documents with boosting",
        "    if no_boost:",
        "        doc_results = processor.find_documents_for_query(query, top_n=top_n * 2)",
        "    else:",
        "        doc_results = processor.find_documents_with_boost(",
        "            query,",
        "            top_n=top_n * 2,  # Get more candidates",
        "            auto_detect_intent=not prefer_docs,",
        "            prefer_docs=prefer_docs",
        "        )",
        "",
        "    # Then get passages from those documents",
        "    doc_ids = [doc_id for doc_id, _ in doc_results]",
        "        overlap=100,",
        "        doc_filter=doc_ids[:top_n * 2] if doc_ids else None",
        "            'reference': f\"{doc_id}:{line_num}\",",
        "            'doc_type': get_doc_type_label(doc_id)",
        "def display_results(results: list, verbose: bool = False, show_doc_type: bool = True):",
        "        doc_type = result.get('doc_type', '')",
        "        type_indicator = f\"[{doc_type}] \" if show_doc_type and doc_type else \"\"",
        "        print(f\"[{i}] {type_indicator}{result['reference']}\")"
      ],
      "lines_removed": [
        "def search_codebase(processor: CorticalTextProcessor, query: str,",
        "                    top_n: int = 5, chunk_size: int = 400, fast: bool = False) -> list:",
        "        List of result dicts with 'file', 'line', 'passage', 'score', 'reference'",
        "        # Fast mode: just find documents, return first lines",
        "        doc_results = processor.fast_find_documents(query, top_n=top_n)",
        "                'reference': f\"{doc_id}:1\"",
        "    # Full passage search",
        "        overlap=100",
        "            'reference': f\"{doc_id}:{line_num}\"",
        "def display_results(results: list, verbose: bool = False):",
        "        print(f\"[{i}] {result['reference']}\")"
      ],
      "context_before": [
        "    formatted = []",
        "    for line in lines[:10]:  # Limit to 10 lines",
        "        if len(line) > max_width:",
        "            line = line[:max_width - 3] + '...'",
        "        formatted.append(line)",
        "    if len(lines) > 10:",
        "        formatted.append(f'  ... ({len(lines) - 10} more lines)')",
        "    return '\\n'.join(formatted)",
        "",
        ""
      ],
      "context_after": [
        "    \"\"\"",
        "    Search the codebase and return results with file:line references.",
        "",
        "    Args:",
        "        processor: CorticalTextProcessor instance",
        "        query: Search query string",
        "        top_n: Number of results to return",
        "        chunk_size: Size of text chunks for passage extraction",
        "        fast: Use fast search mode (documents only, no passages)",
        "",
        "    Returns:",
        "    \"\"\"",
        "    if fast:",
        "        formatted_results = []",
        "        for doc_id, score in doc_results:",
        "            doc_content = processor.documents.get(doc_id, '')",
        "            # Get first 400 chars as passage",
        "            passage = doc_content[:400] if doc_content else ''",
        "            formatted_results.append({",
        "                'file': doc_id,",
        "                'line': 1,",
        "                'passage': passage,",
        "                'score': score,",
        "            })",
        "        return formatted_results",
        "",
        "    results = processor.find_passages_for_query(",
        "        query,",
        "        top_n=top_n,",
        "        chunk_size=chunk_size,",
        "    )",
        "",
        "    formatted_results = []",
        "    for passage, doc_id, start, end, score in results:",
        "        # Get the full document content to find line number",
        "        doc_content = processor.documents.get(doc_id, '')",
        "        line_num = find_line_number(doc_content, start)",
        "",
        "        formatted_results.append({",
        "            'file': doc_id,",
        "            'line': line_num,",
        "            'passage': passage,",
        "            'score': score,",
        "        })",
        "",
        "    return formatted_results",
        "",
        "",
        "    \"\"\"Display search results.\"\"\"",
        "    if not results:",
        "        print(\"No results found.\")",
        "        return",
        "",
        "    print(f\"\\nFound {len(results)} relevant passages:\\n\")",
        "",
        "    for i, result in enumerate(results, 1):",
        "        print(\"=\" * 60)",
        "        print(f\"    Score: {result['score']:.3f}\")",
        "        print(\"-\" * 60)",
        "",
        "        if verbose:",
        "            print(format_passage(result['passage']))",
        "        else:",
        "            # Show first 5 lines",
        "            lines = result['passage'].split('\\n')[:5]",
        "            for line in lines:",
        "                if len(line) > 76:"
      ],
      "change_type": "modify"
    },
    {
      "file": "scripts/search_codebase.py",
      "function": "def interactive_mode(processor: CorticalTextProcessor):",
      "start_line": 179,
      "lines_added": [
        "    parser = argparse.ArgumentParser(",
        "        description='Search the indexed codebase',",
        "        epilog=\"\"\"",
        "Examples:",
        "  %(prog)s \"PageRank algorithm\"           # Search with auto-boost",
        "  %(prog)s \"what is PageRank\" --prefer-docs  # Always boost docs",
        "  %(prog)s \"compute pagerank\" --no-boost  # Disable boosting",
        "  %(prog)s \"architecture\" --fast          # Fast document-level search",
        "        \"\"\"",
        "    )",
        "    parser.add_argument('--prefer-docs', '-d', action='store_true',",
        "                        help='Always boost documentation files in results')",
        "    parser.add_argument('--no-boost', action='store_true',",
        "                        help='Disable document type boosting (raw TF-IDF)')"
      ],
      "lines_removed": [
        "    parser = argparse.ArgumentParser(description='Search the indexed codebase')"
      ],
      "context_before": [
        "                for c in concepts:",
        "                    print(f\"  {c.content[:50]}\")",
        "            else:",
        "                print(f\"Unknown command: {cmd}\")",
        "        else:",
        "            results = search_codebase(processor, query, top_n=5)",
        "            display_results(results, verbose=True)",
        "",
        "",
        "def main():"
      ],
      "context_after": [
        "    parser.add_argument('query', nargs='?', help='Search query')",
        "    parser.add_argument('--corpus', '-c', default='corpus_dev.pkl',",
        "                        help='Corpus file path (default: corpus_dev.pkl)')",
        "    parser.add_argument('--top', '-n', type=int, default=5,",
        "                        help='Number of results (default: 5)')",
        "    parser.add_argument('--verbose', '-v', action='store_true',",
        "                        help='Show full passage text')",
        "    parser.add_argument('--expand', '-e', action='store_true',",
        "                        help='Show query expansion')",
        "    parser.add_argument('--interactive', '-i', action='store_true',",
        "                        help='Interactive search mode')",
        "    parser.add_argument('--fast', '-f', action='store_true',",
        "                        help='Fast search mode (document-level, ~2-3x faster)')",
        "    args = parser.parse_args()",
        "",
        "    base_path = Path(__file__).parent.parent",
        "    corpus_path = base_path / args.corpus",
        "",
        "    # Check if corpus exists",
        "    if not corpus_path.exists():",
        "        print(f\"Error: Corpus file not found: {corpus_path}\")",
        "        print(\"Run 'python scripts/index_codebase.py' first to create it.\")",
        "        sys.exit(1)"
      ],
      "change_type": "modify"
    },
    {
      "file": "scripts/search_codebase.py",
      "function": "def main():",
      "start_line": 216,
      "lines_added": [
        "        # Show query intent detection",
        "        is_conceptual = processor.is_conceptual_query(args.query)",
        "        if not args.no_boost:",
        "            intent_str = \"conceptual\" if is_conceptual else \"implementation\"",
        "            print(f\"(Query type: {intent_str})\")",
        "",
        "        results = search_codebase(",
        "            processor,",
        "            args.query,",
        "            top_n=args.top,",
        "            fast=args.fast,",
        "            prefer_docs=args.prefer_docs,",
        "            no_boost=args.no_boost",
        "        )"
      ],
      "lines_removed": [
        "        results = search_codebase(processor, args.query, top_n=args.top, fast=args.fast)"
      ],
      "context_before": [
        "    processor = CorticalTextProcessor.load(str(corpus_path))",
        "    print(f\"Loaded {len(processor.documents)} documents\\n\")",
        "",
        "    if args.interactive:",
        "        interactive_mode(processor)",
        "    elif args.query:",
        "        if args.expand:",
        "            expand_query_display(processor, args.query)",
        "            print()",
        ""
      ],
      "context_after": [
        "        if args.fast:",
        "            print(\"(Fast mode: document-level results)\")",
        "        display_results(results, verbose=args.verbose)",
        "    else:",
        "        parser.print_help()",
        "",
        "",
        "if __name__ == '__main__':",
        "    main()"
      ],
      "change_type": "modify"
    },
    {
      "file": "tests/test_chunk_indexing.py",
      "function": "class TestGetChangesFromManifest(unittest.TestCase):",
      "start_line": 553,
      "lines_added": [
        "class TestChunkMetadata(unittest.TestCase):",
        "    \"\"\"Test metadata support in chunk indexing.\"\"\"",
        "",
        "    def test_operation_with_metadata(self):",
        "        \"\"\"Test creating an operation with metadata.\"\"\"",
        "        metadata = {'doc_type': 'code', 'language': 'python'}",
        "        op = ChunkOperation(",
        "            op='add',",
        "            doc_id='doc1',",
        "            content='hello',",
        "            mtime=123.0,",
        "            metadata=metadata",
        "        )",
        "        self.assertEqual(op.metadata, metadata)",
        "",
        "    def test_operation_to_dict_with_metadata(self):",
        "        \"\"\"Test converting operation with metadata to dict.\"\"\"",
        "        metadata = {'doc_type': 'docs', 'headings': ['Section 1', 'Section 2']}",
        "        op = ChunkOperation(",
        "            op='add',",
        "            doc_id='doc1',",
        "            content='# Doc\\n\\n## Section 1\\n\\n## Section 2',",
        "            mtime=123.0,",
        "            metadata=metadata",
        "        )",
        "        d = op.to_dict()",
        "        self.assertIn('metadata', d)",
        "        self.assertEqual(d['metadata']['doc_type'], 'docs')",
        "        self.assertEqual(d['metadata']['headings'], ['Section 1', 'Section 2'])",
        "",
        "    def test_operation_to_dict_without_metadata(self):",
        "        \"\"\"Test that metadata is omitted from dict when None.\"\"\"",
        "        op = ChunkOperation(op='add', doc_id='doc1', content='hello')",
        "        d = op.to_dict()",
        "        self.assertNotIn('metadata', d)",
        "",
        "    def test_operation_from_dict_with_metadata(self):",
        "        \"\"\"Test creating operation from dict with metadata.\"\"\"",
        "        d = {",
        "            'op': 'add',",
        "            'doc_id': 'doc1',",
        "            'content': 'hello',",
        "            'mtime': 123.0,",
        "            'metadata': {'doc_type': 'test', 'function_count': 5}",
        "        }",
        "        op = ChunkOperation.from_dict(d)",
        "        self.assertIsNotNone(op.metadata)",
        "        self.assertEqual(op.metadata['doc_type'], 'test')",
        "        self.assertEqual(op.metadata['function_count'], 5)",
        "",
        "    def test_operation_from_dict_without_metadata(self):",
        "        \"\"\"Test creating operation from dict without metadata (backward compat).\"\"\"",
        "        d = {'op': 'add', 'doc_id': 'doc1', 'content': 'hello'}",
        "        op = ChunkOperation.from_dict(d)",
        "        self.assertIsNone(op.metadata)",
        "",
        "    def test_writer_add_with_metadata(self):",
        "        \"\"\"Test writer add_document with metadata.\"\"\"",
        "        with tempfile.TemporaryDirectory() as tmpdir:",
        "            writer = ChunkWriter(tmpdir)",
        "            metadata = {'doc_type': 'code', 'language': 'python'}",
        "            writer.add_document('doc1', 'content', mtime=123.0, metadata=metadata)",
        "",
        "            self.assertEqual(len(writer.operations), 1)",
        "            self.assertEqual(writer.operations[0].metadata, metadata)",
        "",
        "    def test_writer_modify_with_metadata(self):",
        "        \"\"\"Test writer modify_document with metadata.\"\"\"",
        "        with tempfile.TemporaryDirectory() as tmpdir:",
        "            writer = ChunkWriter(tmpdir)",
        "            metadata = {'doc_type': 'docs', 'headings': ['Intro']}",
        "            writer.modify_document('doc1', 'new content', mtime=456.0, metadata=metadata)",
        "",
        "            self.assertEqual(len(writer.operations), 1)",
        "            self.assertEqual(writer.operations[0].metadata, metadata)",
        "",
        "    def test_loader_get_metadata(self):",
        "        \"\"\"Test loader returns metadata for documents.\"\"\"",
        "        with tempfile.TemporaryDirectory() as tmpdir:",
        "            writer = ChunkWriter(tmpdir)",
        "            writer.add_document('doc1', 'content1', metadata={'doc_type': 'code'})",
        "            writer.add_document('doc2', 'content2', metadata={'doc_type': 'docs'})",
        "            writer.save()",
        "",
        "            loader = ChunkLoader(tmpdir)",
        "            loader.load_all()",
        "            metadata = loader.get_metadata()",
        "",
        "            self.assertEqual(len(metadata), 2)",
        "            self.assertEqual(metadata['doc1']['doc_type'], 'code')",
        "            self.assertEqual(metadata['doc2']['doc_type'], 'docs')",
        "",
        "    def test_loader_metadata_updated_on_modify(self):",
        "        \"\"\"Test metadata is updated when document is modified.\"\"\"",
        "        with tempfile.TemporaryDirectory() as tmpdir:",
        "            # First chunk: add with initial metadata",
        "            writer1 = ChunkWriter(tmpdir)",
        "            writer1.timestamp = '2025-12-10T10:00:00'",
        "            writer1.session_id = 'aaaa0000'",
        "            writer1.add_document('doc1', 'old', metadata={'version': 1})",
        "            writer1.save()",
        "",
        "            # Second chunk: modify with new metadata",
        "            writer2 = ChunkWriter(tmpdir)",
        "            writer2.timestamp = '2025-12-10T11:00:00'",
        "            writer2.session_id = 'bbbb1111'",
        "            writer2.modify_document('doc1', 'new', metadata={'version': 2})",
        "            writer2.save()",
        "",
        "            loader = ChunkLoader(tmpdir)",
        "            loader.load_all()",
        "            metadata = loader.get_metadata()",
        "",
        "            self.assertEqual(metadata['doc1']['version'], 2)",
        "",
        "    def test_loader_metadata_removed_on_delete(self):",
        "        \"\"\"Test metadata is removed when document is deleted.\"\"\"",
        "        with tempfile.TemporaryDirectory() as tmpdir:",
        "            # First chunk: add document",
        "            writer1 = ChunkWriter(tmpdir)",
        "            writer1.timestamp = '2025-12-10T10:00:00'",
        "            writer1.session_id = 'aaaa0000'",
        "            writer1.add_document('doc1', 'content', metadata={'doc_type': 'code'})",
        "            writer1.save()",
        "",
        "            # Second chunk: delete document",
        "            writer2 = ChunkWriter(tmpdir)",
        "            writer2.timestamp = '2025-12-10T11:00:00'",
        "            writer2.session_id = 'bbbb1111'",
        "            writer2.delete_document('doc1')",
        "            writer2.save()",
        "",
        "            loader = ChunkLoader(tmpdir)",
        "            loader.load_all()",
        "            metadata = loader.get_metadata()",
        "",
        "            self.assertNotIn('doc1', metadata)",
        "",
        "    def test_compactor_preserves_metadata(self):",
        "        \"\"\"Test compactor preserves metadata during compaction.\"\"\"",
        "        with tempfile.TemporaryDirectory() as tmpdir:",
        "            # Create chunk with metadata",
        "            writer = ChunkWriter(tmpdir)",
        "            writer.timestamp = '2025-01-01T10:00:00'",
        "            writer.session_id = 'aaaa0000'",
        "            writer.add_document(",
        "                'doc1',",
        "                'content1',",
        "                mtime=100.0,",
        "                metadata={'doc_type': 'code', 'language': 'python'}",
        "            )",
        "            writer.add_document(",
        "                'doc2',",
        "                'content2',",
        "                mtime=200.0,",
        "                metadata={'doc_type': 'docs', 'headings': ['H1', 'H2']}",
        "            )",
        "            writer.save()",
        "",
        "            # Compact",
        "            compactor = ChunkCompactor(tmpdir)",
        "            result = compactor.compact()",
        "",
        "            self.assertEqual(result['status'], 'compacted')",
        "",
        "            # Load compacted chunk and check metadata",
        "            loader = ChunkLoader(tmpdir)",
        "            loader.load_all()",
        "            metadata = loader.get_metadata()",
        "",
        "            self.assertEqual(len(metadata), 2)",
        "            self.assertEqual(metadata['doc1']['doc_type'], 'code')",
        "            self.assertEqual(metadata['doc1']['language'], 'python')",
        "            self.assertEqual(metadata['doc2']['doc_type'], 'docs')",
        "            self.assertEqual(metadata['doc2']['headings'], ['H1', 'H2'])",
        "",
        "    def test_chunk_serialization_roundtrip(self):",
        "        \"\"\"Test metadata survives JSON serialization roundtrip.\"\"\"",
        "        with tempfile.TemporaryDirectory() as tmpdir:",
        "            metadata = {",
        "                'doc_type': 'docs',",
        "                'headings': ['Introduction', 'Methods', 'Results'],",
        "                'line_count': 150,",
        "                'mtime': 1234567890.5",
        "            }",
        "            writer = ChunkWriter(tmpdir)",
        "            writer.add_document('doc1', 'content', metadata=metadata)",
        "            filepath = writer.save()",
        "",
        "            # Load from file and verify",
        "            with open(filepath) as f:",
        "                data = json.load(f)",
        "",
        "            op_data = data['operations'][0]",
        "            self.assertEqual(op_data['metadata'], metadata)",
        "",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "        current = {'doc1': 150.0, 'doc3': 300.0}  # doc1 modified, doc3 added",
        "        manifest = {'doc1': 100.0, 'doc2': 200.0}  # doc2 deleted",
        "",
        "        added, modified, deleted = get_changes_from_manifest(current, manifest)",
        "",
        "        self.assertEqual(added, ['doc3'])",
        "        self.assertEqual(modified, ['doc1'])",
        "        self.assertEqual(deleted, ['doc2'])",
        "",
        ""
      ],
      "context_after": [
        "if __name__ == '__main__':",
        "    unittest.main(verbosity=2)"
      ],
      "change_type": "add"
    },
    {
      "file": "tests/test_query.py",
      "function": "class TestEdgeCases(unittest.TestCase):",
      "start_line": 835,
      "lines_added": [
        "class TestDocTypeBoost(unittest.TestCase):",
        "    \"\"\"Test document type boosting for search results.\"\"\"",
        "",
        "    def test_is_conceptual_query_what(self):",
        "        \"\"\"'what is' queries should be conceptual.\"\"\"",
        "        from cortical.query import is_conceptual_query",
        "        self.assertTrue(is_conceptual_query(\"what is PageRank\"))",
        "        self.assertTrue(is_conceptual_query(\"What are the algorithms?\"))",
        "",
        "    def test_is_conceptual_query_explain(self):",
        "        \"\"\"'explain' queries should be conceptual.\"\"\"",
        "        from cortical.query import is_conceptual_query",
        "        self.assertTrue(is_conceptual_query(\"explain PageRank algorithm\"))",
        "        self.assertTrue(is_conceptual_query(\"Explain how TF-IDF works\"))",
        "",
        "    def test_is_conceptual_query_how_does(self):",
        "        \"\"\"'how does' queries should be conceptual.\"\"\"",
        "        from cortical.query import is_conceptual_query",
        "        self.assertTrue(is_conceptual_query(\"how does the system work\"))",
        "",
        "    def test_is_conceptual_query_where(self):",
        "        \"\"\"'where' queries should be implementation-focused.\"\"\"",
        "        from cortical.query import is_conceptual_query",
        "        self.assertFalse(is_conceptual_query(\"where is PageRank computed\"))",
        "        self.assertFalse(is_conceptual_query(\"where do we implement authentication\"))",
        "",
        "    def test_is_conceptual_query_implementation(self):",
        "        \"\"\"Queries with 'implementation' keywords should not be conceptual.\"\"\"",
        "        from cortical.query import is_conceptual_query",
        "        self.assertFalse(is_conceptual_query(\"find the function that calculates TF-IDF\"))",
        "        self.assertFalse(is_conceptual_query(\"line where error is raised\"))",
        "",
        "    def test_is_conceptual_query_neutral(self):",
        "        \"\"\"Neutral queries without keywords should not be conceptual.\"\"\"",
        "        from cortical.query import is_conceptual_query",
        "        self.assertFalse(is_conceptual_query(\"PageRank\"))",
        "        self.assertFalse(is_conceptual_query(\"bigram separator\"))",
        "",
        "    def test_get_doc_type_boost_docs_folder(self):",
        "        \"\"\"docs/ files should get high boost.\"\"\"",
        "        from cortical.query import get_doc_type_boost",
        "        boost = get_doc_type_boost(\"docs/algorithms.md\")",
        "        self.assertEqual(boost, 1.5)",
        "",
        "    def test_get_doc_type_boost_root_md(self):",
        "        \"\"\"Root-level .md files should get medium boost.\"\"\"",
        "        from cortical.query import get_doc_type_boost",
        "        boost = get_doc_type_boost(\"README.md\")",
        "        self.assertEqual(boost, 1.3)",
        "        boost = get_doc_type_boost(\"CLAUDE.md\")",
        "        self.assertEqual(boost, 1.3)",
        "",
        "    def test_get_doc_type_boost_code(self):",
        "        \"\"\"Code files should get normal boost (1.0).\"\"\"",
        "        from cortical.query import get_doc_type_boost",
        "        boost = get_doc_type_boost(\"cortical/processor.py\")",
        "        self.assertEqual(boost, 1.0)",
        "",
        "    def test_get_doc_type_boost_tests(self):",
        "        \"\"\"Test files should get lower boost.\"\"\"",
        "        from cortical.query import get_doc_type_boost",
        "        boost = get_doc_type_boost(\"tests/test_processor.py\")",
        "        self.assertEqual(boost, 0.8)",
        "",
        "    def test_get_doc_type_boost_with_metadata(self):",
        "        \"\"\"Should use metadata when available.\"\"\"",
        "        from cortical.query import get_doc_type_boost",
        "        metadata = {",
        "            \"myfile.py\": {\"doc_type\": \"docs\"}  # Override: code file marked as docs",
        "        }",
        "        boost = get_doc_type_boost(\"myfile.py\", doc_metadata=metadata)",
        "        self.assertEqual(boost, 1.5)",
        "",
        "    def test_apply_doc_type_boost_reranks(self):",
        "        \"\"\"apply_doc_type_boost should re-rank results.\"\"\"",
        "        from cortical.query import apply_doc_type_boost",
        "",
        "        # Setup: code file first, then docs",
        "        results = [",
        "            (\"cortical/query.py\", 1.0),",
        "            (\"docs/algorithms.md\", 0.9),",
        "        ]",
        "",
        "        boosted = apply_doc_type_boost(results)",
        "",
        "        # After boost: docs should be first (0.9 * 1.5 = 1.35 > 1.0)",
        "        self.assertEqual(boosted[0][0], \"docs/algorithms.md\")",
        "        self.assertAlmostEqual(boosted[0][1], 1.35, places=5)",
        "",
        "    def test_apply_doc_type_boost_no_boost(self):",
        "        \"\"\"apply_doc_type_boost should preserve order when disabled.\"\"\"",
        "        from cortical.query import apply_doc_type_boost",
        "",
        "        results = [",
        "            (\"cortical/query.py\", 1.0),",
        "            (\"docs/algorithms.md\", 0.9),",
        "        ]",
        "",
        "        not_boosted = apply_doc_type_boost(results, boost_docs=False)",
        "",
        "        # Order preserved",
        "        self.assertEqual(not_boosted[0][0], \"cortical/query.py\")",
        "        self.assertEqual(not_boosted[1][0], \"docs/algorithms.md\")",
        "",
        "    def test_apply_doc_type_boost_custom_boosts(self):",
        "        \"\"\"apply_doc_type_boost should support custom boost factors.\"\"\"",
        "        from cortical.query import apply_doc_type_boost",
        "",
        "        results = [",
        "            (\"cortical/query.py\", 1.0),",
        "            (\"tests/test_query.py\", 0.8),",
        "        ]",
        "",
        "        # Custom: boost tests instead of docs",
        "        custom = {'code': 1.0, 'test': 2.0, 'docs': 1.0, 'root_docs': 1.0}",
        "        boosted = apply_doc_type_boost(results, custom_boosts=custom)",
        "",
        "        # Test file should be first now (0.8 * 2.0 = 1.6 > 1.0)",
        "        self.assertEqual(boosted[0][0], \"tests/test_query.py\")",
        "",
        "",
        "class TestDocTypeBoostIntegration(unittest.TestCase):",
        "    \"\"\"Integration tests for document type boosting.\"\"\"",
        "",
        "    @classmethod",
        "    def setUpClass(cls):",
        "        \"\"\"Set up processor with different document types.\"\"\"",
        "        cls.processor = CorticalTextProcessor()",
        "",
        "        # Add a code file",
        "        cls.processor.process_document(",
        "            \"cortical/processor.py\",",
        "            \"PageRank algorithm implementation. def compute_pagerank(): pass\",",
        "            metadata={\"doc_type\": \"code\"}",
        "        )",
        "",
        "        # Add a docs file",
        "        cls.processor.process_document(",
        "            \"docs/algorithms.md\",",
        "            \"# Algorithms\\n\\n## PageRank\\n\\nPageRank is a link analysis algorithm.\",",
        "            metadata={\"doc_type\": \"docs\"}",
        "        )",
        "",
        "        # Add a test file",
        "        cls.processor.process_document(",
        "            \"tests/test_processor.py\",",
        "            \"class TestPageRank: def test_pagerank(self): pass\",",
        "            metadata={\"doc_type\": \"test\"}",
        "        )",
        "",
        "        cls.processor.compute_all(verbose=False)",
        "",
        "    def test_find_documents_with_boost_conceptual(self):",
        "        \"\"\"Conceptual queries should boost docs.\"\"\"",
        "        from cortical.query import find_documents_with_boost",
        "",
        "        results = find_documents_with_boost(",
        "            \"what is PageRank algorithm\",",
        "            self.processor.layers,",
        "            self.processor.tokenizer,",
        "            top_n=3,",
        "            doc_metadata=self.processor.document_metadata,",
        "            auto_detect_intent=True",
        "        )",
        "",
        "        # Docs file should be ranked higher",
        "        self.assertTrue(len(results) > 0)",
        "        # Check that results are returned (specific ranking depends on corpus)",
        "",
        "    def test_find_documents_with_boost_prefer_docs(self):",
        "        \"\"\"prefer_docs=True should always boost docs.\"\"\"",
        "        from cortical.query import find_documents_with_boost",
        "",
        "        results = find_documents_with_boost(",
        "            \"PageRank\",  # Neutral query",
        "            self.processor.layers,",
        "            self.processor.tokenizer,",
        "            top_n=3,",
        "            doc_metadata=self.processor.document_metadata,",
        "            auto_detect_intent=False,",
        "            prefer_docs=True",
        "        )",
        "",
        "        self.assertTrue(len(results) > 0)",
        "",
        "    def test_processor_wrapper_exists(self):",
        "        \"\"\"Processor should have find_documents_with_boost method.\"\"\"",
        "        self.assertTrue(hasattr(self.processor, 'find_documents_with_boost'))",
        "        self.assertTrue(hasattr(self.processor, 'is_conceptual_query'))",
        "",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "        \"\"\"find_relation_between should handle empty relations.\"\"\"",
        "        result = find_relation_between(\"a\", \"b\", [])",
        "        self.assertEqual(result, [])",
        "",
        "    def test_find_terms_empty_relations(self):",
        "        \"\"\"find_terms_with_relation should handle empty relations.\"\"\"",
        "        result = find_terms_with_relation(\"a\", \"IsA\", [])",
        "        self.assertEqual(result, [])",
        "",
        ""
      ],
      "context_after": [
        "if __name__ == '__main__':",
        "    unittest.main()"
      ],
      "change_type": "add"
    }
  ],
  "hour_of_day": 0,
  "day_of_week": "Thursday",
  "seconds_since_last_commit": -393618,
  "is_merge": true,
  "is_initial": false,
  "parent_count": 2,
  "session_id": null,
  "related_chats": [],
  "ci_result": null,
  "reverted": false,
  "amended": false
}