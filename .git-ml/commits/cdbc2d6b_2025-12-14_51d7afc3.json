{
  "hash": "cdbc2d6bbd11cde4da583d75f1cfb02b58999ea6",
  "message": "Merge pull request #76 from scrawlsbenches/claude/plan-next-priorities-zwRV6",
  "author": "scrawlsbenches",
  "timestamp": "2025-12-14 06:45:04 -0500",
  "branch": "claude/multi-index-design-DvifZ",
  "files_changed": [
    ".claude/skills/task-manager/SKILL.md",
    ".github/workflows/ci.yml",
    "README.md",
    "cortical/__init__.py",
    "cortical/diff.py",
    "cortical/persistence.py",
    "cortical/processor.py",
    "docs/README.md",
    "docs/mcp-security.md",
    "docs/session-2025-12-14-security-and-features.md",
    "pyproject.toml",
    "tasks/2025-12-14_11-15-01_41d5.json",
    "tasks/legacy_migration.json"
  ],
  "insertions": 1458,
  "deletions": 35,
  "hunks": [
    {
      "file": ".claude/skills/task-manager/SKILL.md",
      "function": "To view legacy task history:",
      "start_line": 104,
      "lines_added": [
        "",
        "## Security Model",
        "",
        "### Tool Permissions",
        "",
        "This skill uses `allowed-tools: Read, Bash, Write`:",
        "",
        "| Tool | Why Needed | Scope |",
        "|------|------------|-------|",
        "| **Read** | Read task JSON files, scripts | `tasks/`, `scripts/task_utils.py` |",
        "| **Bash** | Run Python task utilities | `python scripts/*.py` commands |",
        "| **Write** | Create/update task JSON files | `tasks/*.json` |",
        "",
        "### Why Write is Required",
        "",
        "The Write tool is needed because:",
        "1. **JSON creation**: Task session files are JSON - using Bash with heredocs for JSON is error-prone",
        "2. **Atomic updates**: Write ensures complete file writes without corruption",
        "3. **Safe by default**: Write tool requires reading the file first, preventing blind overwrites",
        "",
        "### Alternative Considered",
        "",
        "Using only Bash (e.g., `echo '{\"tasks\":...}' > file.json`) was considered but rejected:",
        "- JSON escaping in shell is error-prone",
        "- No atomic write guarantees",
        "- Less readable in transcripts",
        "- More susceptible to injection if task titles contain special characters",
        "",
        "### Practical Security",
        "",
        "The skill is scoped by:",
        "1. **Invocation context**: Only activated when task management is needed",
        "2. **Documented purpose**: Clear description limits expected operations",
        "3. **Model judgment**: Claude Code decides what operations to perform",
        "4. **Task directory convention**: All operations target `tasks/` directory",
        "",
        "### Recommendation",
        "",
        "If running in an environment with strict security requirements:",
        "1. Review task operations in the conversation transcript",
        "2. Use consolidation scripts directly via Bash if preferred",
        "3. Consider implementing path restrictions in a custom skill wrapper"
      ],
      "lines_removed": [],
      "context_before": [
        "```bash",
        "python3 -c \"import json; [print(f\\\"{t['id']}: {t['title']}\\\") for t in json.load(open('tasks/legacy_migration.json'))['tasks'][:20]]\"",
        "```",
        "",
        "## Tips",
        "",
        "1. **Create session at workflow start** - all tasks share session suffix",
        "2. **Save before commit** - persist tasks to disk",
        "3. **Consolidate weekly** - merge sessions, resolve duplicates",
        "4. **Use context field** - add file/method references for quick navigation"
      ],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "workflows/ci.yml b/.github/workflows/ci.yml",
      "function": "jobs:",
      "start_line": 361,
      "lines_added": [
        "",
        "  # ==========================================================================",
        "  # Security Scanning (runs in parallel with all other jobs)",
        "  # Static analysis and dependency scanning for security vulnerabilities",
        "  # ==========================================================================",
        "  security-scan:",
        "    name: \"ðŸ” Security Scan\"",
        "    runs-on: ubuntu-latest",
        "    steps:",
        "    - uses: actions/checkout@v4",
        "",
        "    - name: Set up Python 3.11",
        "      uses: actions/setup-python@v5",
        "      with:",
        "        python-version: '3.11'",
        "",
        "    - name: Install security tools",
        "      run: |",
        "        python -m pip install --upgrade pip",
        "        pip install bandit pip-audit detect-secrets",
        "",
        "    - name: Run Bandit (SAST)",
        "      run: |",
        "        echo \"=== Running Bandit Static Analysis ===\"",
        "        # -ll = only show medium and higher severity issues",
        "        # -r = recursive",
        "        # Skip B101 (assert) as we use asserts for invariants in non-production code",
        "        bandit -r cortical/ -ll -s B101 -f txt || true",
        "        echo \"âœ… Bandit scan complete\"",
        "",
        "    - name: Run pip-audit (Dependency Scanning)",
        "      run: |",
        "        echo \"=== Running pip-audit Dependency Check ===\"",
        "        pip install -e \".[dev]\"",
        "        pip-audit --desc || echo \"âš ï¸ Review dependency vulnerabilities above\"",
        "",
        "    - name: Run detect-secrets (Secret Scanning)",
        "      run: |",
        "        echo \"=== Running detect-secrets ===\"",
        "        # Scan for accidentally committed secrets",
        "        detect-secrets scan --all-files --exclude-files '\\.git/.*' --exclude-files '.*\\.pkl' --exclude-files '.*\\.json' > .secrets-baseline.json || true",
        "        # Check if any high-entropy secrets were found (excluding test fixtures)",
        "        python -c \"",
        "import json",
        "with open('.secrets-baseline.json') as f:",
        "    data = json.load(f)",
        "    results = data.get('results', {})",
        "    real_secrets = {k: v for k, v in results.items() if not k.startswith('tests/')}",
        "    if real_secrets:",
        "        print('âš ï¸ Potential secrets found in:')",
        "        for file in real_secrets:",
        "            print(f'  - {file}')",
        "        print('Please review and ensure no real secrets are committed.')",
        "    else:",
        "        print('âœ… No secrets detected in source files')",
        "\""
      ],
      "lines_removed": [],
      "context_before": [
        "",
        "    - name: Set up Python 3.11",
        "      uses: actions/setup-python@v5",
        "      with:",
        "        python-version: '3.11'",
        "",
        "    - name: Run showcase",
        "      run: |",
        "        echo \"=== Running Showcase Demo ===\"",
        "        python showcase.py"
      ],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "README.md",
      "function": "Detailed documentation is available in the `docs/` directory:",
      "start_line": 356,
      "lines_added": [
        "## Security Considerations",
        "",
        "### Pickle Deserialization Warning",
        "",
        "âš ï¸ **The default `.pkl` format uses Python's `pickle` module, which can execute arbitrary code during deserialization.**",
        "",
        "**Risk**: Loading a malicious `.pkl` file could result in remote code execution (RCE). Only load pickle files from sources you trust completely.",
        "",
        "**Recommendations**:",
        "",
        "1. **For untrusted sources**: Use the JSON state format instead:",
        "   ```python",
        "   from cortical.state_storage import StateLoader",
        "",
        "   # Save as JSON (safe to share)",
        "   StateLoader.save(processor, \"corpus_state.json\")",
        "",
        "   # Load from JSON (safe from untrusted sources)",
        "   processor = StateLoader.load(\"corpus_state.json\")",
        "   ```",
        "",
        "2. **For trusted sources**: Continue using pickle for faster serialization:",
        "   ```python",
        "   processor.save(\"corpus.pkl\")  # Fast, but only load files you trust",
        "   processor = CorticalTextProcessor.load(\"corpus.pkl\")",
        "   ```",
        "",
        "3. **For maximum security**: Never load pickle files from:",
        "   - Downloaded files from the internet",
        "   - User uploads",
        "   - Shared network locations with untrusted access",
        "   - Email attachments",
        "",
        "See [Python's pickle documentation](https://docs.python.org/3/library/pickle.html) for more details on pickle security.",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "| [docs/glossary.md](docs/glossary.md) | Terminology definitions |",
        "",
        "For AI agents, see also [docs/claude-usage.md](docs/claude-usage.md) and [CLAUDE.md](CLAUDE.md).",
        "",
        "## Running Tests",
        "",
        "```bash",
        "python -m unittest discover -s tests -v",
        "```",
        ""
      ],
      "context_after": [
        "## License",
        "",
        "MIT License"
      ],
      "change_type": "add"
    },
    {
      "file": "cortical/__init__.py",
      "function": "from .progress import (",
      "start_line": 27,
      "lines_added": [
        "from .diff import (",
        "    SemanticDiff,",
        "    TermChange,",
        "    RelationChange,",
        "    ClusterChange,",
        "    compare_processors,",
        "    compare_documents,",
        "    what_changed",
        ")"
      ],
      "lines_removed": [],
      "context_before": [
        "    SilentProgressReporter,",
        "    MultiPhaseProgress,",
        ")",
        "from .results import (",
        "    DocumentMatch,",
        "    PassageMatch,",
        "    QueryResult,",
        "    convert_document_matches,",
        "    convert_passage_matches",
        ")"
      ],
      "context_after": [
        "",
        "# MCP Server support (optional import)",
        "try:",
        "    from .mcp_server import CorticalMCPServer, create_mcp_server",
        "    _has_mcp = True",
        "except ImportError:",
        "    _has_mcp = False",
        "    CorticalMCPServer = None",
        "    create_mcp_server = None",
        ""
      ],
      "change_type": "add"
    },
    {
      "file": "cortical/__init__.py",
      "function": "__all__ = [",
      "start_line": 59,
      "lines_added": [
        "    # Semantic diff",
        "    \"SemanticDiff\",",
        "    \"TermChange\",",
        "    \"RelationChange\",",
        "    \"ClusterChange\",",
        "    \"compare_processors\",",
        "    \"compare_documents\",",
        "    \"what_changed\","
      ],
      "lines_removed": [],
      "context_before": [
        "    \"ProgressReporter\",",
        "    \"ConsoleProgressReporter\",",
        "    \"CallbackProgressReporter\",",
        "    \"SilentProgressReporter\",",
        "    \"MultiPhaseProgress\",",
        "    \"DocumentMatch\",",
        "    \"PassageMatch\",",
        "    \"QueryResult\",",
        "    \"convert_document_matches\",",
        "    \"convert_passage_matches\","
      ],
      "context_after": [
        "]",
        "",
        "# Add MCP exports if available",
        "if _has_mcp:",
        "    __all__.extend([",
        "        \"CorticalMCPServer\",",
        "        \"create_mcp_server\",",
        "    ])"
      ],
      "change_type": "add"
    },
    {
      "file": "cortical/diff.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "\"\"\"",
        "Semantic Diff Module",
        "====================",
        "",
        "Provides \"What Changed?\" functionality for comparing:",
        "- Two versions of a document",
        "- Two processor states",
        "- Before/after states of a corpus",
        "",
        "This goes beyond line-by-line diffs to show semantic changes:",
        "- New/removed concepts",
        "- Importance shifts (PageRank changes)",
        "- Relationship changes",
        "- Cluster membership changes",
        "\"\"\"",
        "",
        "from dataclasses import dataclass, field",
        "from typing import Dict, List, Optional, Tuple, Set, Any",
        "from collections import defaultdict",
        "",
        "from .layers import CorticalLayer, HierarchicalLayer",
        "from .minicolumn import Minicolumn",
        "",
        "",
        "@dataclass",
        "class TermChange:",
        "    \"\"\"Represents a change to a term/concept.\"\"\"",
        "    term: str",
        "    change_type: str  # 'added', 'removed', 'modified'",
        "    old_pagerank: Optional[float] = None",
        "    new_pagerank: Optional[float] = None",
        "    old_tfidf: Optional[float] = None",
        "    new_tfidf: Optional[float] = None",
        "    old_occurrences: Optional[int] = None",
        "    new_occurrences: Optional[int] = None",
        "    old_documents: Optional[Set[str]] = None",
        "    new_documents: Optional[Set[str]] = None",
        "",
        "    @property",
        "    def pagerank_delta(self) -> Optional[float]:",
        "        \"\"\"Change in PageRank importance.\"\"\"",
        "        if self.old_pagerank is not None and self.new_pagerank is not None:",
        "            return self.new_pagerank - self.old_pagerank",
        "        return None",
        "",
        "    @property",
        "    def tfidf_delta(self) -> Optional[float]:",
        "        \"\"\"Change in TF-IDF score.\"\"\"",
        "        if self.old_tfidf is not None and self.new_tfidf is not None:",
        "            return self.new_tfidf - self.old_tfidf",
        "        return None",
        "",
        "    @property",
        "    def documents_added(self) -> Set[str]:",
        "        \"\"\"Documents where this term newly appears.\"\"\"",
        "        if self.old_documents is not None and self.new_documents is not None:",
        "            return self.new_documents - self.old_documents",
        "        return set()",
        "",
        "    @property",
        "    def documents_removed(self) -> Set[str]:",
        "        \"\"\"Documents where this term no longer appears.\"\"\"",
        "        if self.old_documents is not None and self.new_documents is not None:",
        "            return self.old_documents - self.new_documents",
        "        return set()",
        "",
        "",
        "@dataclass",
        "class RelationChange:",
        "    \"\"\"Represents a change to a semantic relation.\"\"\"",
        "    source: str",
        "    target: str",
        "    relation_type: str",
        "    change_type: str  # 'added', 'removed', 'modified'",
        "    old_weight: Optional[float] = None",
        "    new_weight: Optional[float] = None",
        "    old_confidence: Optional[float] = None",
        "    new_confidence: Optional[float] = None",
        "",
        "",
        "@dataclass",
        "class ClusterChange:",
        "    \"\"\"Represents a change to concept clustering.\"\"\"",
        "    cluster_id: Optional[int]",
        "    change_type: str  # 'created', 'dissolved', 'modified'",
        "    old_members: Set[str] = field(default_factory=set)",
        "    new_members: Set[str] = field(default_factory=set)",
        "    members_added: Set[str] = field(default_factory=set)",
        "    members_removed: Set[str] = field(default_factory=set)",
        "",
        "",
        "@dataclass",
        "class SemanticDiff:",
        "    \"\"\"",
        "    Complete semantic diff between two states.",
        "",
        "    Captures what changed semantically:",
        "    - Terms added/removed/modified",
        "    - Importance shifts",
        "    - Relationship changes",
        "    - Cluster reorganization",
        "    \"\"\"",
        "    # Document changes",
        "    documents_added: List[str] = field(default_factory=list)",
        "    documents_removed: List[str] = field(default_factory=list)",
        "    documents_modified: List[str] = field(default_factory=list)",
        "",
        "    # Term changes",
        "    terms_added: List[TermChange] = field(default_factory=list)",
        "    terms_removed: List[TermChange] = field(default_factory=list)",
        "    terms_modified: List[TermChange] = field(default_factory=list)",
        "",
        "    # Top movers (biggest importance changes)",
        "    importance_increased: List[TermChange] = field(default_factory=list)",
        "    importance_decreased: List[TermChange] = field(default_factory=list)",
        "",
        "    # Relationship changes",
        "    relations_added: List[RelationChange] = field(default_factory=list)",
        "    relations_removed: List[RelationChange] = field(default_factory=list)",
        "    relations_modified: List[RelationChange] = field(default_factory=list)",
        "",
        "    # Cluster changes",
        "    clusters_created: List[ClusterChange] = field(default_factory=list)",
        "    clusters_dissolved: List[ClusterChange] = field(default_factory=list)",
        "    clusters_modified: List[ClusterChange] = field(default_factory=list)",
        "",
        "    # Summary statistics",
        "    total_term_changes: int = 0",
        "    total_relation_changes: int = 0",
        "    total_cluster_changes: int = 0",
        "",
        "    def summary(self) -> str:",
        "        \"\"\"Generate a human-readable summary of changes.\"\"\"",
        "        lines = [\"# Semantic Diff Summary\\n\"]",
        "",
        "        # Document changes",
        "        if self.documents_added or self.documents_removed or self.documents_modified:",
        "            lines.append(\"## Documents\\n\")",
        "            if self.documents_added:",
        "                lines.append(f\"- Added: {len(self.documents_added)} documents\")",
        "                for doc in self.documents_added[:5]:",
        "                    lines.append(f\"  + {doc}\")",
        "                if len(self.documents_added) > 5:",
        "                    lines.append(f\"  ... and {len(self.documents_added) - 5} more\")",
        "            if self.documents_removed:",
        "                lines.append(f\"- Removed: {len(self.documents_removed)} documents\")",
        "                for doc in self.documents_removed[:5]:",
        "                    lines.append(f\"  - {doc}\")",
        "            if self.documents_modified:",
        "                lines.append(f\"- Modified: {len(self.documents_modified)} documents\")",
        "            lines.append(\"\")",
        "",
        "        # Term changes",
        "        if self.terms_added or self.terms_removed:",
        "            lines.append(\"## Terms\\n\")",
        "            if self.terms_added:",
        "                lines.append(f\"- New terms: {len(self.terms_added)}\")",
        "                for tc in self.terms_added[:10]:",
        "                    lines.append(f\"  + {tc.term}\")",
        "                if len(self.terms_added) > 10:",
        "                    lines.append(f\"  ... and {len(self.terms_added) - 10} more\")",
        "            if self.terms_removed:",
        "                lines.append(f\"- Removed terms: {len(self.terms_removed)}\")",
        "                for tc in self.terms_removed[:10]:",
        "                    lines.append(f\"  - {tc.term}\")",
        "            lines.append(\"\")",
        "",
        "        # Importance shifts",
        "        if self.importance_increased or self.importance_decreased:",
        "            lines.append(\"## Importance Shifts (PageRank)\\n\")",
        "            if self.importance_increased:",
        "                lines.append(\"### Rising Terms\")",
        "                for tc in self.importance_increased[:10]:",
        "                    delta = tc.pagerank_delta",
        "                    if delta:",
        "                        lines.append(f\"  + {tc.term}: +{delta:.6f}\")",
        "            if self.importance_decreased:",
        "                lines.append(\"### Falling Terms\")",
        "                for tc in self.importance_decreased[:10]:",
        "                    delta = tc.pagerank_delta",
        "                    if delta:",
        "                        lines.append(f\"  - {tc.term}: {delta:.6f}\")",
        "            lines.append(\"\")",
        "",
        "        # Relation changes",
        "        if self.relations_added or self.relations_removed:",
        "            lines.append(\"## Relations\\n\")",
        "            if self.relations_added:",
        "                lines.append(f\"- New relations: {len(self.relations_added)}\")",
        "                for rc in self.relations_added[:5]:",
        "                    lines.append(f\"  + {rc.source} --{rc.relation_type}--> {rc.target}\")",
        "            if self.relations_removed:",
        "                lines.append(f\"- Removed relations: {len(self.relations_removed)}\")",
        "            lines.append(\"\")",
        "",
        "        # Cluster changes",
        "        if self.clusters_created or self.clusters_dissolved or self.clusters_modified:",
        "            lines.append(\"## Clusters\\n\")",
        "            if self.clusters_created:",
        "                lines.append(f\"- New clusters: {len(self.clusters_created)}\")",
        "            if self.clusters_dissolved:",
        "                lines.append(f\"- Dissolved clusters: {len(self.clusters_dissolved)}\")",
        "            if self.clusters_modified:",
        "                lines.append(f\"- Modified clusters: {len(self.clusters_modified)}\")",
        "            lines.append(\"\")",
        "",
        "        # Overall statistics",
        "        lines.append(\"## Statistics\\n\")",
        "        lines.append(f\"- Total term changes: {self.total_term_changes}\")",
        "        lines.append(f\"- Total relation changes: {self.total_relation_changes}\")",
        "        lines.append(f\"- Total cluster changes: {self.total_cluster_changes}\")",
        "",
        "        return \"\\n\".join(lines)",
        "",
        "    def to_dict(self) -> Dict[str, Any]:",
        "        \"\"\"Convert to dictionary for serialization.\"\"\"",
        "        return {",
        "            'documents_added': self.documents_added,",
        "            'documents_removed': self.documents_removed,",
        "            'documents_modified': self.documents_modified,",
        "            'terms_added': [{'term': t.term, 'pagerank': t.new_pagerank} for t in self.terms_added],",
        "            'terms_removed': [{'term': t.term, 'pagerank': t.old_pagerank} for t in self.terms_removed],",
        "            'importance_increased': [",
        "                {'term': t.term, 'delta': t.pagerank_delta} for t in self.importance_increased",
        "            ],",
        "            'importance_decreased': [",
        "                {'term': t.term, 'delta': t.pagerank_delta} for t in self.importance_decreased",
        "            ],",
        "            'relations_added': len(self.relations_added),",
        "            'relations_removed': len(self.relations_removed),",
        "            'clusters_created': len(self.clusters_created),",
        "            'clusters_dissolved': len(self.clusters_dissolved),",
        "            'total_term_changes': self.total_term_changes,",
        "            'total_relation_changes': self.total_relation_changes,",
        "            'total_cluster_changes': self.total_cluster_changes,",
        "        }",
        "",
        "",
        "def compare_processors(",
        "    old_processor: 'CorticalTextProcessor',",
        "    new_processor: 'CorticalTextProcessor',",
        "    top_movers: int = 20,",
        "    min_pagerank_delta: float = 0.0001",
        ") -> SemanticDiff:",
        "    \"\"\"",
        "    Compare two processor states to find semantic differences.",
        "",
        "    Args:",
        "        old_processor: The \"before\" state",
        "        new_processor: The \"after\" state",
        "        top_movers: Number of top importance changes to track",
        "        min_pagerank_delta: Minimum PageRank change to consider significant",
        "",
        "    Returns:",
        "        SemanticDiff with all detected changes",
        "    \"\"\"",
        "    diff = SemanticDiff()",
        "",
        "    # Compare documents",
        "    old_docs = set(old_processor.documents.keys())",
        "    new_docs = set(new_processor.documents.keys())",
        "",
        "    diff.documents_added = list(new_docs - old_docs)",
        "    diff.documents_removed = list(old_docs - new_docs)",
        "",
        "    # Check for modified documents (same ID, different content)",
        "    for doc_id in old_docs & new_docs:",
        "        if old_processor.documents[doc_id] != new_processor.documents[doc_id]:",
        "            diff.documents_modified.append(doc_id)",
        "",
        "    # Compare terms (Layer 0 - TOKENS)",
        "    old_layer0 = old_processor.layers.get(CorticalLayer.TOKENS)",
        "    new_layer0 = new_processor.layers.get(CorticalLayer.TOKENS)",
        "",
        "    if old_layer0 and new_layer0:",
        "        old_terms = set(old_layer0.minicolumns.keys())",
        "        new_terms = set(new_layer0.minicolumns.keys())",
        "",
        "        # New terms",
        "        for term in new_terms - old_terms:",
        "            col = new_layer0.get_minicolumn(term)",
        "            if col:",
        "                diff.terms_added.append(TermChange(",
        "                    term=term,",
        "                    change_type='added',",
        "                    new_pagerank=col.pagerank,",
        "                    new_tfidf=col.tfidf,",
        "                    new_occurrences=col.occurrence_count,",
        "                    new_documents=set(col.document_ids)",
        "                ))",
        "",
        "        # Removed terms",
        "        for term in old_terms - new_terms:",
        "            col = old_layer0.get_minicolumn(term)",
        "            if col:",
        "                diff.terms_removed.append(TermChange(",
        "                    term=term,",
        "                    change_type='removed',",
        "                    old_pagerank=col.pagerank,",
        "                    old_tfidf=col.tfidf,",
        "                    old_occurrences=col.occurrence_count,",
        "                    old_documents=set(col.document_ids)",
        "                ))",
        "",
        "        # Modified terms - find importance changes",
        "        movers = []",
        "        for term in old_terms & new_terms:",
        "            old_col = old_layer0.get_minicolumn(term)",
        "            new_col = new_layer0.get_minicolumn(term)",
        "            if old_col and new_col:",
        "                delta = new_col.pagerank - old_col.pagerank",
        "                if abs(delta) >= min_pagerank_delta:",
        "                    tc = TermChange(",
        "                        term=term,",
        "                        change_type='modified',",
        "                        old_pagerank=old_col.pagerank,",
        "                        new_pagerank=new_col.pagerank,",
        "                        old_tfidf=old_col.tfidf,",
        "                        new_tfidf=new_col.tfidf,",
        "                        old_occurrences=old_col.occurrence_count,",
        "                        new_occurrences=new_col.occurrence_count,",
        "                        old_documents=set(old_col.document_ids),",
        "                        new_documents=set(new_col.document_ids)",
        "                    )",
        "                    movers.append((delta, tc))",
        "                    diff.terms_modified.append(tc)",
        "",
        "        # Sort movers by absolute delta",
        "        movers.sort(key=lambda x: abs(x[0]), reverse=True)",
        "",
        "        for delta, tc in movers[:top_movers]:",
        "            if delta > 0:",
        "                diff.importance_increased.append(tc)",
        "            else:",
        "                diff.importance_decreased.append(tc)",
        "",
        "        # Sort by delta magnitude",
        "        diff.importance_increased.sort(key=lambda x: x.pagerank_delta or 0, reverse=True)",
        "        diff.importance_decreased.sort(key=lambda x: x.pagerank_delta or 0)",
        "",
        "    # Compare typed connections/relations",
        "    _compare_relations(old_layer0, new_layer0, diff)",
        "",
        "    # Compare clusters (Layer 2 - CONCEPTS)",
        "    _compare_clusters(",
        "        old_processor.layers.get(CorticalLayer.CONCEPTS),",
        "        new_processor.layers.get(CorticalLayer.CONCEPTS),",
        "        diff",
        "    )",
        "",
        "    # Update statistics",
        "    diff.total_term_changes = (",
        "        len(diff.terms_added) + len(diff.terms_removed) + len(diff.terms_modified)",
        "    )",
        "    diff.total_relation_changes = (",
        "        len(diff.relations_added) + len(diff.relations_removed) + len(diff.relations_modified)",
        "    )",
        "    diff.total_cluster_changes = (",
        "        len(diff.clusters_created) + len(diff.clusters_dissolved) + len(diff.clusters_modified)",
        "    )",
        "",
        "    return diff",
        "",
        "",
        "def _compare_relations(",
        "    old_layer: Optional[HierarchicalLayer],",
        "    new_layer: Optional[HierarchicalLayer],",
        "    diff: SemanticDiff",
        ") -> None:",
        "    \"\"\"Compare typed connections between layers.\"\"\"",
        "    if not old_layer or not new_layer:",
        "        return",
        "",
        "    # Build relation sets",
        "    old_relations: Dict[Tuple[str, str, str], Tuple[float, float]] = {}",
        "    new_relations: Dict[Tuple[str, str, str], Tuple[float, float]] = {}",
        "",
        "    for col in old_layer.minicolumns.values():",
        "        for target_id, edge in col.typed_connections.items():",
        "            key = (col.content, target_id, edge.relation_type)",
        "            old_relations[key] = (edge.weight, edge.confidence)",
        "",
        "    for col in new_layer.minicolumns.values():",
        "        for target_id, edge in col.typed_connections.items():",
        "            key = (col.content, target_id, edge.relation_type)",
        "            new_relations[key] = (edge.weight, edge.confidence)",
        "",
        "    old_keys = set(old_relations.keys())",
        "    new_keys = set(new_relations.keys())",
        "",
        "    # New relations",
        "    for key in new_keys - old_keys:",
        "        source, target, rel_type = key",
        "        weight, confidence = new_relations[key]",
        "        diff.relations_added.append(RelationChange(",
        "            source=source,",
        "            target=target,",
        "            relation_type=rel_type,",
        "            change_type='added',",
        "            new_weight=weight,",
        "            new_confidence=confidence",
        "        ))",
        "",
        "    # Removed relations",
        "    for key in old_keys - new_keys:",
        "        source, target, rel_type = key",
        "        weight, confidence = old_relations[key]",
        "        diff.relations_removed.append(RelationChange(",
        "            source=source,",
        "            target=target,",
        "            relation_type=rel_type,",
        "            change_type='removed',",
        "            old_weight=weight,",
        "            old_confidence=confidence",
        "        ))",
        "",
        "",
        "def _compare_clusters(",
        "    old_layer: Optional[HierarchicalLayer],",
        "    new_layer: Optional[HierarchicalLayer],",
        "    diff: SemanticDiff",
        ") -> None:",
        "    \"\"\"Compare concept clusters between layers.\"\"\"",
        "    if not old_layer or not new_layer:",
        "        return",
        "",
        "    # Build cluster membership maps",
        "    old_clusters: Dict[int, Set[str]] = defaultdict(set)",
        "    new_clusters: Dict[int, Set[str]] = defaultdict(set)",
        "",
        "    for col in old_layer.minicolumns.values():",
        "        if col.cluster_id is not None:",
        "            old_clusters[col.cluster_id].add(col.content)",
        "",
        "    for col in new_layer.minicolumns.values():",
        "        if col.cluster_id is not None:",
        "            new_clusters[col.cluster_id].add(col.content)",
        "",
        "    old_ids = set(old_clusters.keys())",
        "    new_ids = set(new_clusters.keys())",
        "",
        "    # New clusters",
        "    for cluster_id in new_ids - old_ids:",
        "        diff.clusters_created.append(ClusterChange(",
        "            cluster_id=cluster_id,",
        "            change_type='created',",
        "            new_members=new_clusters[cluster_id]",
        "        ))",
        "",
        "    # Dissolved clusters",
        "    for cluster_id in old_ids - new_ids:",
        "        diff.clusters_dissolved.append(ClusterChange(",
        "            cluster_id=cluster_id,",
        "            change_type='dissolved',",
        "            old_members=old_clusters[cluster_id]",
        "        ))",
        "",
        "    # Modified clusters (same ID, different members)",
        "    for cluster_id in old_ids & new_ids:",
        "        old_members = old_clusters[cluster_id]",
        "        new_members = new_clusters[cluster_id]",
        "        if old_members != new_members:",
        "            diff.clusters_modified.append(ClusterChange(",
        "                cluster_id=cluster_id,",
        "                change_type='modified',",
        "                old_members=old_members,",
        "                new_members=new_members,",
        "                members_added=new_members - old_members,",
        "                members_removed=old_members - new_members",
        "            ))",
        "",
        "",
        "def compare_documents(",
        "    processor: 'CorticalTextProcessor',",
        "    doc_id_old: str,",
        "    doc_id_new: str",
        ") -> Dict[str, Any]:",
        "    \"\"\"",
        "    Compare two documents within the same corpus.",
        "",
        "    Returns a simplified diff focused on document-level differences:",
        "    - Shared terms",
        "    - Unique terms in each document",
        "    - Shared bigrams",
        "    - Term importance differences",
        "",
        "    Args:",
        "        processor: The processor containing both documents",
        "        doc_id_old: ID of first document",
        "        doc_id_new: ID of second document",
        "",
        "    Returns:",
        "        Dictionary with comparison results",
        "    \"\"\"",
        "    layer0 = processor.layers.get(CorticalLayer.TOKENS)",
        "    layer1 = processor.layers.get(CorticalLayer.BIGRAMS)",
        "",
        "    if not layer0:",
        "        return {'error': 'Processor has no token layer'}",
        "",
        "    # Find terms in each document",
        "    terms_old: Set[str] = set()",
        "    terms_new: Set[str] = set()",
        "",
        "    for col in layer0.minicolumns.values():",
        "        if doc_id_old in col.document_ids:",
        "            terms_old.add(col.content)",
        "        if doc_id_new in col.document_ids:",
        "            terms_new.add(col.content)",
        "",
        "    shared_terms = terms_old & terms_new",
        "    unique_to_old = terms_old - terms_new",
        "    unique_to_new = terms_new - terms_old",
        "",
        "    # Find bigrams in each document",
        "    bigrams_old: Set[str] = set()",
        "    bigrams_new: Set[str] = set()",
        "",
        "    if layer1:",
        "        for col in layer1.minicolumns.values():",
        "            if doc_id_old in col.document_ids:",
        "                bigrams_old.add(col.content)",
        "            if doc_id_new in col.document_ids:",
        "                bigrams_new.add(col.content)",
        "",
        "    shared_bigrams = bigrams_old & bigrams_new",
        "",
        "    # Calculate Jaccard similarity",
        "    all_terms = terms_old | terms_new",
        "    jaccard_similarity = len(shared_terms) / len(all_terms) if all_terms else 0.0",
        "",
        "    return {",
        "        'doc_id_old': doc_id_old,",
        "        'doc_id_new': doc_id_new,",
        "        'terms_in_old': len(terms_old),",
        "        'terms_in_new': len(terms_new),",
        "        'shared_terms': len(shared_terms),",
        "        'unique_to_old': len(unique_to_old),",
        "        'unique_to_new': len(unique_to_new),",
        "        'jaccard_similarity': jaccard_similarity,",
        "        'shared_bigrams': len(shared_bigrams),",
        "        'top_shared_terms': list(shared_terms)[:20],",
        "        'top_unique_to_old': list(unique_to_old)[:20],",
        "        'top_unique_to_new': list(unique_to_new)[:20],",
        "        'top_shared_bigrams': list(shared_bigrams)[:10],",
        "    }",
        "",
        "",
        "def what_changed(",
        "    processor: 'CorticalTextProcessor',",
        "    old_content: str,",
        "    new_content: str,",
        "    temp_doc_prefix: str = \"_diff_temp_\"",
        ") -> Dict[str, Any]:",
        "    \"\"\"",
        "    Compare two text contents to show what changed semantically.",
        "",
        "    This is a convenience function that:",
        "    1. Temporarily adds both texts as documents",
        "    2. Computes their differences",
        "    3. Removes the temporary documents",
        "",
        "    Args:",
        "        processor: The processor to use for analysis",
        "        old_content: The \"before\" text",
        "        new_content: The \"after\" text",
        "        temp_doc_prefix: Prefix for temporary document IDs",
        "",
        "    Returns:",
        "        Dictionary with semantic diff results",
        "    \"\"\"",
        "    from .tokenizer import Tokenizer",
        "",
        "    tokenizer = processor.tokenizer",
        "",
        "    # Tokenize both contents",
        "    old_token_list = tokenizer.tokenize(old_content)",
        "    new_token_list = tokenizer.tokenize(new_content)",
        "",
        "    old_tokens = set(old_token_list)",
        "    new_tokens = set(new_token_list)",
        "",
        "    old_bigrams = set(tokenizer.extract_ngrams(old_token_list, n=2))",
        "    new_bigrams = set(tokenizer.extract_ngrams(new_token_list, n=2))",
        "",
        "    # Calculate differences",
        "    tokens_added = new_tokens - old_tokens",
        "    tokens_removed = old_tokens - new_tokens",
        "    tokens_unchanged = old_tokens & new_tokens",
        "",
        "    bigrams_added = new_bigrams - old_bigrams",
        "    bigrams_removed = old_bigrams - new_bigrams",
        "    bigrams_unchanged = old_bigrams & new_bigrams",
        "",
        "    # Calculate similarity metrics",
        "    token_jaccard = (",
        "        len(tokens_unchanged) / len(old_tokens | new_tokens)",
        "        if old_tokens | new_tokens else 0.0",
        "    )",
        "    bigram_jaccard = (",
        "        len(bigrams_unchanged) / len(old_bigrams | new_bigrams)",
        "        if old_bigrams | new_bigrams else 0.0",
        "    )",
        "",
        "    return {",
        "        'tokens': {",
        "            'added': sorted(tokens_added)[:50],",
        "            'removed': sorted(tokens_removed)[:50],",
        "            'unchanged_count': len(tokens_unchanged),",
        "            'total_old': len(old_tokens),",
        "            'total_new': len(new_tokens),",
        "            'similarity': token_jaccard,",
        "        },",
        "        'bigrams': {",
        "            'added': sorted(bigrams_added)[:30],",
        "            'removed': sorted(bigrams_removed)[:30],",
        "            'unchanged_count': len(bigrams_unchanged),",
        "            'similarity': bigram_jaccard,",
        "        },",
        "        'summary': {",
        "            'content_similarity': (token_jaccard + bigram_jaccard) / 2,",
        "            'is_significant_change': token_jaccard < 0.8,",
        "        }",
        "    }"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "cortical/persistence.py",
      "function": "Save and load functionality for the cortical processor.",
      "start_line": 7,
      "lines_added": [
        "import warnings"
      ],
      "lines_removed": [],
      "context_before": [
        "Supports:",
        "- Pickle serialization for full state",
        "- JSON export for graph visualization",
        "- Incremental updates",
        "\"\"\"",
        "",
        "import pickle",
        "import json",
        "import os",
        "import logging"
      ],
      "context_after": [
        "from typing import Dict, Optional, Any",
        "",
        "from .layers import CorticalLayer, HierarchicalLayer",
        "from .minicolumn import Minicolumn",
        "from .proto import PROTOBUF_AVAILABLE, serialize_state, deserialize_state",
        "",
        "logger = logging.getLogger(__name__)",
        "",
        "",
        "def save_processor("
      ],
      "change_type": "add"
    },
    {
      "file": "cortical/persistence.py",
      "function": "def save_processor(",
      "start_line": 49,
      "lines_added": [
        "        # Emit deprecation warning for pickle format due to security concerns",
        "        warnings.warn(",
        "            \"Pickle format is deprecated due to security concerns (arbitrary code execution). \"",
        "            \"Consider using format='protobuf' or the StateLoader JSON format instead. \"",
        "            \"See README.md 'Security Considerations' for details.\",",
        "            DeprecationWarning,",
        "            stacklevel=2",
        "        )"
      ],
      "lines_removed": [],
      "context_before": [
        "        format: Serialization format ('pickle' or 'protobuf'). Default: 'pickle'",
        "",
        "    Raises:",
        "        ValueError: If format is not 'pickle' or 'protobuf'",
        "        ImportError: If format='protobuf' but protobuf package is not installed",
        "    \"\"\"",
        "    if format not in ['pickle', 'protobuf']:",
        "        raise ValueError(f\"Invalid format '{format}'. Must be 'pickle' or 'protobuf'.\")",
        "",
        "    if format == 'pickle':"
      ],
      "context_after": [
        "        # Original pickle serialization",
        "        state = {",
        "            'version': '2.2',",
        "            'layers': {},",
        "            'documents': documents,",
        "            'document_metadata': document_metadata or {},",
        "            'embeddings': embeddings or {},",
        "            'semantic_relations': semantic_relations or [],",
        "            'metadata': metadata or {}",
        "        }"
      ],
      "change_type": "add"
    },
    {
      "file": "cortical/persistence.py",
      "function": "def load_processor(",
      "start_line": 144,
      "lines_added": [
        "        # Emit deprecation warning for pickle format due to security concerns",
        "        warnings.warn(",
        "            \"Pickle format is deprecated due to security concerns (arbitrary code execution). \"",
        "            \"Only load pickle files from trusted sources. \"",
        "            \"Consider migrating to format='protobuf' or the StateLoader JSON format. \"",
        "            \"See README.md 'Security Considerations' for details.\",",
        "            DeprecationWarning,",
        "            stacklevel=2",
        "        )"
      ],
      "lines_removed": [],
      "context_before": [
        "                        # Default to pickle for unknown formats",
        "                        format = 'pickle'",
        "                except UnicodeDecodeError:",
        "                    # Binary content - assume pickle",
        "                    format = 'pickle'",
        "",
        "    if format not in ['pickle', 'protobuf']:",
        "        raise ValueError(f\"Invalid format '{format}'. Must be 'pickle' or 'protobuf'.\")",
        "",
        "    if format == 'pickle':"
      ],
      "context_after": [
        "        # Original pickle deserialization",
        "        with open(filepath, 'rb') as f:",
        "            state = pickle.load(f)",
        "",
        "        # Reconstruct layers",
        "        layers = {}",
        "        for level_value, layer_data in state.get('layers', {}).items():",
        "            # Validate layer value before creating enum",
        "            level_int = int(level_value)",
        "            if level_int not in [0, 1, 2, 3]:"
      ],
      "change_type": "add"
    },
    {
      "file": "cortical/processor.py",
      "function": "class CorticalTextProcessor:",
      "start_line": 2845,
      "lines_added": [
        "    # =========================================================================",
        "    # Semantic Diff - \"What Changed?\"",
        "    # =========================================================================",
        "",
        "    def compare_with(",
        "        self,",
        "        other: 'CorticalTextProcessor',",
        "        top_movers: int = 20,",
        "        min_pagerank_delta: float = 0.0001",
        "    ) -> 'diff_module.SemanticDiff':",
        "        \"\"\"",
        "        Compare this processor state with another to find semantic differences.",
        "",
        "        This produces a \"What Changed?\" report showing:",
        "        - Documents added/removed/modified",
        "        - Terms added/removed",
        "        - Importance shifts (PageRank changes)",
        "        - Relationship changes",
        "        - Cluster reorganization",
        "",
        "        Args:",
        "            other: Another CorticalTextProcessor to compare with",
        "            top_movers: Number of top importance changes to track",
        "            min_pagerank_delta: Minimum PageRank change to consider significant",
        "",
        "        Returns:",
        "            SemanticDiff object with all detected changes",
        "",
        "        Example:",
        "            >>> old_processor = CorticalTextProcessor.load(\"corpus_v1.pkl\")",
        "            >>> new_processor = CorticalTextProcessor.load(\"corpus_v2.pkl\")",
        "            >>> diff = new_processor.compare_with(old_processor)",
        "            >>> print(diff.summary())",
        "        \"\"\"",
        "        from . import diff as diff_module",
        "        return diff_module.compare_processors(",
        "            old_processor=other,",
        "            new_processor=self,",
        "            top_movers=top_movers,",
        "            min_pagerank_delta=min_pagerank_delta",
        "        )",
        "",
        "    def compare_documents(self, doc_id_1: str, doc_id_2: str) -> Dict:",
        "        \"\"\"",
        "        Compare two documents within this corpus.",
        "",
        "        Args:",
        "            doc_id_1: ID of first document",
        "            doc_id_2: ID of second document",
        "",
        "        Returns:",
        "            Dict with comparison results including:",
        "            - shared_terms: Number of terms in common",
        "            - unique_to_old/new: Terms unique to each document",
        "            - jaccard_similarity: Overall similarity score",
        "            - top_shared_terms: List of shared terms",
        "        \"\"\"",
        "        from . import diff as diff_module",
        "        return diff_module.compare_documents(self, doc_id_1, doc_id_2)",
        "",
        "    def what_changed(self, old_content: str, new_content: str) -> Dict:",
        "        \"\"\"",
        "        Compare two text contents to show what changed semantically.",
        "",
        "        This is a convenience method for comparing arbitrary text without",
        "        adding them to the corpus. Useful for:",
        "        - Comparing code versions",
        "        - Checking document revisions",
        "        - Understanding content differences",
        "",
        "        Args:",
        "            old_content: The \"before\" text",
        "            new_content: The \"after\" text",
        "",
        "        Returns:",
        "            Dict with semantic diff results including:",
        "            - tokens: Added, removed, and unchanged token info",
        "            - bigrams: Added, removed, and unchanged bigram info",
        "            - summary: Overall similarity and significance",
        "",
        "        Example:",
        "            >>> diff = processor.what_changed(",
        "            ...     old_content=\"def hello(): print('hi')\",",
        "            ...     new_content=\"def hello(): print('hello world')\"",
        "            ... )",
        "            >>> print(f\"Similarity: {diff['summary']['content_similarity']:.2%}\")",
        "        \"\"\"",
        "        from . import diff as diff_module",
        "        return diff_module.what_changed(self, old_content, new_content)",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "",
        "        for candidate_id, candidate_text in candidates:",
        "            candidate_fp = self.get_fingerprint(candidate_text)",
        "            comparison = self.compare_fingerprints(query_fp, candidate_fp)",
        "            results.append((candidate_id, comparison['overall_similarity'], comparison))",
        "",
        "        # Sort by similarity descending",
        "        results.sort(key=lambda x: x[1], reverse=True)",
        "        return results[:top_n]",
        ""
      ],
      "context_after": [
        "    def save(self, filepath: str, verbose: bool = True) -> None:",
        "        \"\"\"",
        "        Save processor state to a file.",
        "",
        "        Saves all computed state including embeddings, semantic relations,",
        "        and configuration, so they don't need to be recomputed when loading.",
        "        \"\"\"",
        "        metadata = {",
        "            'has_embeddings': bool(self.embeddings),",
        "            'has_relations': bool(self.semantic_relations),"
      ],
      "change_type": "add"
    },
    {
      "file": "docs/README.md",
      "function": "Welcome to the Cortical Text Processor documentation. This index provides naviga",
      "start_line": 78,
      "lines_added": [
        "### Security",
        "",
        "| Document | Purpose |",
        "|----------|---------|",
        "| [security-knowledge-transfer.md](security-knowledge-transfer.md) | Full security review and findings |",
        "| [mcp-security.md](mcp-security.md) | MCP server security model and deployment |",
        "",
        "### Session Knowledge Transfer",
        "",
        "| Document | Purpose |",
        "|----------|---------|",
        "| [session-2025-12-14-security-and-features.md](session-2025-12-14-security-and-features.md) | Security hardening, semantic diff feature, CI fixes |",
        "",
        "*Last updated: 2025-12-14*"
      ],
      "lines_removed": [
        "*Last updated: 2025-12-12*"
      ],
      "context_before": [
        "| [dogfooding.md](dogfooding.md) | Using the system to test itself |",
        "| [dogfooding-checklist.md](dogfooding-checklist.md) | Systematic dog-fooding checklist |",
        "",
        "### AI Agent Resources",
        "",
        "| Document | Purpose |",
        "|----------|---------|",
        "| [claude-usage.md](claude-usage.md) | Guide for Claude agents using the system |",
        "| [cli-wrapper-guide.md](cli-wrapper-guide.md) | CLI wrapper for AI assistants |",
        ""
      ],
      "context_after": [
        "---",
        "",
        "## Additional Resources",
        "",
        "- **[CLAUDE.md](../CLAUDE.md)** - Main development guide (in repo root)",
        "- **[CONTRIBUTING.md](../CONTRIBUTING.md)** - How to contribute",
        "- **[TASK_LIST.md](../TASK_LIST.md)** - Active task backlog",
        "- **[README.md](../README.md)** - Project overview",
        "",
        "---",
        ""
      ],
      "change_type": "modify"
    },
    {
      "file": "docs/mcp-security.md",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "# MCP Server Security Model",
        "",
        "This document describes the security considerations for the Cortical Text Processor MCP (Model Context Protocol) server.",
        "",
        "## Overview",
        "",
        "The MCP server (`cortical/mcp_server.py`) provides an interface for AI agents to interact with the Cortical Text Processor. It exposes five tools for semantic search, passage retrieval, query expansion, corpus statistics, and document indexing.",
        "",
        "## Exposed Capabilities",
        "",
        "| Tool | Operation | Risk Level | Notes |",
        "|------|-----------|------------|-------|",
        "| `search` | Read-only query | Low | Returns document IDs and scores |",
        "| `passages` | Read-only query | Low | Returns text passages from corpus |",
        "| `expand_query` | Read-only query | Low | Returns expansion terms |",
        "| `corpus_stats` | Read-only stats | Low | Returns aggregate statistics |",
        "| `add_document` | Write operation | Medium | Adds documents to corpus |",
        "",
        "### Tool Details",
        "",
        "#### `search(query, top_n)`",
        "- **Input validation**: Query must be non-empty string; `top_n >= 1`",
        "- **Output**: Document IDs and relevance scores",
        "- **Risk**: Information disclosure if corpus contains sensitive data",
        "",
        "#### `passages(query, top_n, chunk_size)`",
        "- **Input validation**: Query must be non-empty string; `top_n >= 1`",
        "- **Output**: Actual text content from documents",
        "- **Risk**: Higher information disclosure risk than `search`",
        "",
        "#### `expand_query(query, max_expansions)`",
        "- **Input validation**: Query must be non-empty string; `max_expansions >= 1`",
        "- **Output**: Related terms and weights",
        "- **Risk**: Low - only exposes term relationships",
        "",
        "#### `corpus_stats()`",
        "- **Input validation**: None (no parameters)",
        "- **Output**: Document counts, layer statistics",
        "- **Risk**: Metadata disclosure only",
        "",
        "#### `add_document(doc_id, content, recompute)`",
        "- **Input validation**: `doc_id` non-empty string; `content` must be string; `recompute` in {'none', 'tfidf', 'full'}",
        "- **Output**: Processing statistics",
        "- **Risk**:",
        "  - Corpus pollution (malicious content injection)",
        "  - Resource exhaustion (large documents, frequent additions)",
        "  - Overwriting existing documents (no duplicate protection)",
        "",
        "## Trust Model",
        "",
        "### Who Can Call the Server",
        "",
        "The MCP server uses stdio transport by default, meaning:",
        "",
        "1. **Local Execution**: The server runs as a local process",
        "2. **No Network Exposure**: stdio transport doesn't listen on network ports",
        "3. **Caller is Trusted**: The calling process (typically an AI assistant) has full access",
        "",
        "### Trust Assumptions",
        "",
        "| Assumption | Implication |",
        "|------------|-------------|",
        "| Caller is authorized | No authentication mechanism |",
        "| Corpus content is not sensitive | Results returned without filtering |",
        "| Input is well-formed | Basic validation only |",
        "| Single-tenant usage | No multi-user isolation |",
        "",
        "### Security Boundary",
        "",
        "```",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”",
        "â”‚                    AI Assistant Process                  â”‚",
        "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚",
        "â”‚  â”‚   MCP Client    â”‚â”€â”€â”€â”€â”€â”€â”‚   Cortical MCP Server   â”‚   â”‚",
        "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ stdioâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚",
        "â”‚                                      â”‚                   â”‚",
        "â”‚                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚",
        "â”‚                            â”‚ CorticalTextProc  â”‚        â”‚",
        "â”‚                            â”‚    (in-memory)    â”‚        â”‚",
        "â”‚                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜",
        "```",
        "",
        "The security boundary is at the process level. Anyone who can communicate with the stdio interface has full access.",
        "",
        "## Input Validation",
        "",
        "### Current Validation",
        "",
        "All tools perform basic input validation:",
        "",
        "```python",
        "# String validation",
        "if not query or not query.strip():",
        "    return {\"error\": \"Query must be a non-empty string\", ...}",
        "",
        "# Numeric validation",
        "if top_n < 1:",
        "    return {\"error\": \"top_n must be at least 1\", ...}",
        "",
        "# Enum validation",
        "valid_recompute = {'none', 'tfidf', 'full'}",
        "if recompute not in valid_recompute:",
        "    return {\"error\": f\"recompute must be one of {valid_recompute}\", ...}",
        "```",
        "",
        "### Validation Gaps",
        "",
        "| Gap | Risk | Mitigation |",
        "|-----|------|------------|",
        "| No max length for `content` | Memory exhaustion | Add content size limit |",
        "| No max length for `query` | Performance impact | Add query length limit |",
        "| No rate limiting | Resource exhaustion | Implement rate limiting |",
        "| No `doc_id` format validation | Potential path issues | Validate `doc_id` characters |",
        "",
        "## Resource Considerations",
        "",
        "### Memory Usage",
        "",
        "The processor stores all documents in memory:",
        "",
        "- **Per document**: ~10-100KB depending on content",
        "- **Index structures**: ~5x document size",
        "- **No memory limits**: Can exhaust system memory",
        "",
        "### CPU Usage",
        "",
        "Resource-intensive operations:",
        "",
        "| Operation | CPU Impact | Notes |",
        "|-----------|------------|-------|",
        "| `add_document` with `recompute='full'` | High | Triggers full recomputation |",
        "| `search` with large corpus | Medium | Iterates all documents |",
        "| `passages` | Medium | Text chunking and scoring |",
        "",
        "## Rate Limiting Considerations",
        "",
        "The MCP server does not implement rate limiting. For production deployments, consider:",
        "",
        "### Recommended Limits",
        "",
        "| Metric | Suggested Limit | Rationale |",
        "|--------|-----------------|-----------|",
        "| Requests per minute | 60 | Prevent runaway queries |",
        "| Document additions per hour | 100 | Limit corpus growth |",
        "| Max document size | 1 MB | Prevent memory exhaustion |",
        "| Max query length | 1000 chars | Prevent complex queries |",
        "| Max concurrent requests | 5 | Limit resource contention |",
        "",
        "### Implementation Options",
        "",
        "1. **Wrapper Rate Limiter**: Add rate limiting wrapper around tools",
        "2. **External Proxy**: Use API gateway with rate limiting",
        "3. **Token Bucket**: Implement in-memory token bucket",
        "",
        "## Recommended Deployment Configurations",
        "",
        "### Development / Local Use",
        "",
        "```bash",
        "# Default configuration - suitable for local development",
        "python -m cortical.mcp_server",
        "```",
        "",
        "- Trust model: Single user, full trust",
        "- Network: stdio only (no network exposure)",
        "- Corpus: Ephemeral or local file",
        "",
        "### Production Considerations",
        "",
        "If deploying the MCP server in a production context:",
        "",
        "1. **Do not expose to untrusted networks**",
        "   - MCP over stdio is designed for local use",
        "   - No authentication or authorization",
        "",
        "2. **Corpus sensitivity**",
        "   - Assume all corpus content can be returned to callers",
        "   - Do not index sensitive/confidential documents unless callers are authorized",
        "",
        "3. **Resource isolation**",
        "   - Run in container with memory limits",
        "   - Set CPU quotas",
        "   - Monitor resource usage",
        "",
        "4. **Logging and auditing**",
        "   - Enable `CORTICAL_LOG_LEVEL=DEBUG` for request logging",
        "   - Log all `add_document` calls for audit trail",
        "",
        "### Environment Variables",
        "",
        "| Variable | Purpose | Default |",
        "|----------|---------|---------|",
        "| `CORTICAL_CORPUS_PATH` | Path to saved corpus | None (empty corpus) |",
        "| `CORTICAL_LOG_LEVEL` | Logging verbosity | INFO |",
        "",
        "## Security Checklist",
        "",
        "Before deploying the MCP server:",
        "",
        "- [ ] Verify corpus content is appropriate for callers",
        "- [ ] Set memory limits on container/process",
        "- [ ] Enable logging for audit trail",
        "- [ ] Consider document size limits for `add_document`",
        "- [ ] Review if `add_document` capability is needed",
        "- [ ] Test with representative workloads",
        "",
        "## Related Documentation",
        "",
        "- [Security Knowledge Transfer](security-knowledge-transfer.md) - Full security review",
        "- [README Security Section](../README.md#security-considerations) - Pickle warnings",
        "- [Architecture](architecture.md) - System design"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "docs/session-2025-12-14-security-and-features.md",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "# Knowledge Transfer: Security Hardening & Semantic Diff",
        "",
        "**Date:** 2025-12-14",
        "**Branch:** `claude/plan-next-priorities-zwRV6`",
        "**Focus:** Security improvements, semantic diff feature, CI fixes",
        "",
        "---",
        "",
        "## Executive Summary",
        "",
        "This session completed 8 tasks focused on:",
        "1. Security hardening (CI scanning, documentation, deprecation warnings)",
        "2. New \"What Changed?\" semantic diff feature",
        "3. Critical CI fix for test failures",
        "",
        "### Commits Made",
        "",
        "| Commit | Description |",
        "|--------|-------------|",
        "| `90b989f` | Security CI scanning and pickle deprecation warnings |",
        "| `091167a` | MCP server security documentation |",
        "| `a31d1c7` | Semantic diff feature (LEGACY-075) |",
        "| `a5abfb9` | Task-manager skill security documentation |",
        "| `0da2739` | Fix CI test failures from deprecation warnings |",
        "",
        "---",
        "",
        "## Part 1: Security Improvements",
        "",
        "### SEC-001: Pickle Security Warning in README",
        "",
        "**Location:** `README.md` â†’ \"Security Considerations\" section",
        "",
        "Added comprehensive warning about pickle deserialization risks:",
        "- Explains RCE risk from malicious pickle files",
        "- Recommends JSON StateLoader for untrusted sources",
        "- Links to Python documentation",
        "",
        "### SEC-002, SEC-004, SEC-005: CI Security Scanning",
        "",
        "**Location:** `.github/workflows/ci.yml` â†’ `security-scan` job",
        "",
        "Added new parallel CI job with three security tools:",
        "",
        "```yaml",
        "security-scan:",
        "  name: \"ðŸ” Security Scan\"",
        "  steps:",
        "    - name: Run Bandit (SAST)      # Static analysis",
        "    - name: Run pip-audit          # Dependency vulnerabilities",
        "    - name: Run detect-secrets     # Secret scanning",
        "```",
        "",
        "**Configuration Details:**",
        "- Bandit: `-ll -s B101` (medium+ severity, skip assert warnings)",
        "- pip-audit: Full dependency scan with descriptions",
        "- detect-secrets: Excludes `.pkl` and `.json` files, reports non-test findings",
        "",
        "### SEC-006: MCP Server Security Model",
        "",
        "**Location:** `docs/mcp-security.md`",
        "",
        "Comprehensive security documentation covering:",
        "- Tool capabilities and risk levels (search, passages, expand_query, corpus_stats, add_document)",
        "- Input validation performed by each endpoint",
        "- Trust model (stdio transport, single-tenant, no authentication)",
        "- Security boundary diagram",
        "- Rate limiting recommendations",
        "- Deployment checklist",
        "",
        "### SEC-007: Task-Manager Skill Permissions",
        "",
        "**Location:** `.claude/skills/task-manager/SKILL.md` â†’ \"Security Model\" section",
        "",
        "Documented why the skill needs `Read, Bash, Write` permissions:",
        "- Write needed for JSON task file creation",
        "- Alternatives considered (Bash heredocs) and rejected",
        "- Recommendations for strict environments",
        "",
        "### SEC-008: Pickle Deprecation Warnings",
        "",
        "**Location:** `cortical/persistence.py` â†’ `save_processor()` and `load_processor()`",
        "",
        "Added `DeprecationWarning` when using pickle format:",
        "```python",
        "warnings.warn(",
        "    \"Pickle format is deprecated due to security concerns (arbitrary code execution). \"",
        "    \"Consider using format='protobuf' or the StateLoader JSON format instead. \"",
        "    \"See README.md 'Security Considerations' for details.\",",
        "    DeprecationWarning,",
        "    stacklevel=2",
        ")",
        "```",
        "",
        "---",
        "",
        "## Part 2: Semantic Diff Feature (LEGACY-075)",
        "",
        "### New Module: `cortical/diff.py`",
        "",
        "Implements \"What Changed?\" functionality for comparing processor states.",
        "",
        "**Key Classes:**",
        "",
        "| Class | Purpose |",
        "|-------|---------|",
        "| `SemanticDiff` | Complete diff report with all changes |",
        "| `TermChange` | Tracks PageRank/TF-IDF changes per term |",
        "| `RelationChange` | Tracks typed connection changes |",
        "| `ClusterChange` | Tracks concept cluster reorganization |",
        "",
        "**Key Functions:**",
        "",
        "| Function | Purpose |",
        "|----------|---------|",
        "| `compare_processors(old, new)` | Full semantic diff between processor states |",
        "| `compare_documents(proc, doc1, doc2)` | Compare two documents in same corpus |",
        "| `what_changed(proc, old_text, new_text)` | Quick text comparison without corpus changes |",
        "",
        "### New CorticalTextProcessor Methods",
        "",
        "```python",
        "# Compare two processor states",
        "diff = new_processor.compare_with(old_processor)",
        "print(diff.summary())",
        "",
        "# Compare documents within corpus",
        "comparison = processor.compare_documents(\"doc1\", \"doc2\")",
        "print(f\"Similarity: {comparison['jaccard_similarity']:.2%}\")",
        "",
        "# Quick text comparison",
        "result = processor.what_changed(old_code, new_code)",
        "print(f\"Tokens added: {result['tokens']['added']}\")",
        "```",
        "",
        "### Example Output",
        "",
        "```",
        "# Semantic Diff Summary",
        "",
        "## Documents",
        "- Added: 1 documents",
        "  + doc3",
        "",
        "## Terms",
        "- New terms: 2",
        "  + subset",
        "  + deep",
        "",
        "## Importance Shifts (PageRank)",
        "### Rising Terms",
        "  + learning: +0.058332",
        "  + machine: +0.012112",
        "### Falling Terms",
        "  - recognition: -0.051109",
        "  - pattern: -0.051109",
        "",
        "## Relations",
        "- New relations: 11",
        "  + learning --co_occurrence--> L0_deep",
        "",
        "## Statistics",
        "- Total term changes: 11",
        "- Total relation changes: 11",
        "- Total cluster changes: 2",
        "```",
        "",
        "---",
        "",
        "## Part 3: CI Fix",
        "",
        "### Problem",
        "",
        "After adding pickle deprecation warnings (SEC-008), CI tests started failing because:",
        "1. `pyproject.toml` has `filterwarnings = [\"error\"]` (treat warnings as errors)",
        "2. Smoke tests call `save()` and `load()` which emit `DeprecationWarning`",
        "3. The warning became a test failure",
        "",
        "### Solution",
        "",
        "**Location:** `pyproject.toml` â†’ `[tool.pytest.ini_options]`",
        "",
        "Added filter to ignore our intentional deprecation warning:",
        "```toml",
        "filterwarnings = [",
        "    \"error\",",
        "    # Allow our intentional pickle deprecation warnings (SEC-008)",
        "    \"ignore:Pickle format is deprecated:DeprecationWarning\",",
        "]",
        "```",
        "",
        "### Lesson Learned",
        "",
        "When adding deprecation warnings to code that's exercised by tests, remember to update `filterwarnings` in `pyproject.toml` to prevent false CI failures.",
        "",
        "---",
        "",
        "## Part 4: Remaining Work",
        "",
        "### Security Tasks (Still Pending)",
        "",
        "| Task | Priority | Effort | Description |",
        "|------|----------|--------|-------------|",
        "| SEC-003 | Medium | 4h | HMAC verification for pickle files |",
        "| SEC-009 | Low | 4h | Security-focused test suite |",
        "| SEC-010 | Low | 8h | Input fuzzing with Hypothesis |",
        "",
        "### Feature Tasks (Still Pending)",
        "",
        "| Task | Priority | Description |",
        "|------|----------|-------------|",
        "| LEGACY-078 | Medium | Code pattern detection |",
        "| LEGACY-095 | Medium | Split processor.py (2,301 lines) |",
        "| LEGACY-187 | Medium | Async API support |",
        "| LEGACY-190 | Medium | REST API wrapper (FastAPI) |",
        "",
        "---",
        "",
        "## Files Changed in This Session",
        "",
        "### New Files",
        "- `cortical/diff.py` - Semantic diff module (610 lines)",
        "- `docs/mcp-security.md` - MCP security documentation",
        "",
        "### Modified Files",
        "- `.github/workflows/ci.yml` - Added security-scan job",
        "- `README.md` - Added Security Considerations section",
        "- `cortical/__init__.py` - Exported diff module classes",
        "- `cortical/persistence.py` - Added deprecation warnings",
        "- `cortical/processor.py` - Added semantic diff methods",
        "- `pyproject.toml` - Fixed pytest filterwarnings",
        "- `.claude/skills/task-manager/SKILL.md` - Added security model section",
        "- `docs/README.md` - Added Security section to docs index",
        "",
        "### Task Files Updated",
        "- `tasks/2025-12-14_11-15-01_41d5.json` - Marked SEC tasks completed",
        "- `tasks/legacy_migration.json` - Marked LEGACY-075 completed",
        "",
        "---",
        "",
        "## How to Use New Features",
        "",
        "### Security Scanning (Automatic)",
        "",
        "Security scans run automatically on every push:",
        "- Check CI for \"ðŸ” Security Scan\" job",
        "- Review any Bandit, pip-audit, or detect-secrets findings",
        "",
        "### Semantic Diff",
        "",
        "```python",
        "from cortical import CorticalTextProcessor",
        "",
        "# Load two versions of your corpus",
        "old = CorticalTextProcessor.load(\"corpus_v1.pkl\")",
        "new = CorticalTextProcessor.load(\"corpus_v2.pkl\")",
        "",
        "# See what changed",
        "diff = new.compare_with(old)",
        "print(diff.summary())",
        "",
        "# Or export as dict for programmatic use",
        "data = diff.to_dict()",
        "```",
        "",
        "---",
        "",
        "## Testing Changes",
        "",
        "To verify all changes work:",
        "",
        "```bash",
        "# Run smoke tests",
        "python -m pytest tests/smoke/ -v",
        "",
        "# Test semantic diff",
        "python -c \"",
        "from cortical import CorticalTextProcessor",
        "p1 = CorticalTextProcessor()",
        "p1.process_document('d1', 'Hello world')",
        "p1.compute_all()",
        "",
        "p2 = CorticalTextProcessor()",
        "p2.process_document('d1', 'Hello world')",
        "p2.process_document('d2', 'Goodbye world')",
        "p2.compute_all()",
        "",
        "diff = p2.compare_with(p1)",
        "print(diff.summary())",
        "\"",
        "```",
        "",
        "---",
        "",
        "*Document created: 2025-12-14*"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "pyproject.toml",
      "function": "omit = [",
      "start_line": 78,
      "lines_added": [
        "# But allow our intentional deprecation warnings",
        "filterwarnings = [",
        "    \"error\",",
        "    # Allow our intentional pickle deprecation warnings (SEC-008)",
        "    \"ignore:Pickle format is deprecated:DeprecationWarning\",",
        "]"
      ],
      "lines_removed": [
        "filterwarnings = [\"error\"]"
      ],
      "context_before": [
        "exclude_lines = [",
        "    \"pragma: no cover\",",
        "    \"def __repr__\",",
        "    \"raise NotImplementedError\",",
        "    \"if TYPE_CHECKING:\",",
        "    \"if __name__ == .__main__.:\",",
        "]",
        "",
        "[tool.pytest.ini_options]",
        "# Treat warnings as errors to catch issues early"
      ],
      "context_after": [],
      "change_type": "modify"
    },
    {
      "file": "tasks/2025-12-14_11-15-01_41d5.json",
      "function": null,
      "start_line": 1,
      "lines_added": [
        "  \"saved_at\": \"2025-12-14T11:35:23.935425\",",
        "      \"status\": \"completed\",",
        "      \"updated_at\": \"2025-12-14T11:28:39.474962\",",
        "      \"completed_at\": \"2025-12-14T11:28:39.474962\",",
        "      \"retrospective\": {",
        "        \"notes\": \"Implemented in commit 90b989f\"",
        "      }",
        "      \"status\": \"completed\",",
        "      \"updated_at\": \"2025-12-14T11:28:39.474977\",",
        "      \"completed_at\": \"2025-12-14T11:28:39.474977\",",
        "      \"retrospective\": {",
        "        \"notes\": \"Implemented in commit 90b989f\"",
        "      }"
      ],
      "lines_removed": [
        "  \"saved_at\": \"2025-12-14T11:15:01.282011\",",
        "      \"status\": \"pending\",",
        "      \"updated_at\": null,",
        "      \"completed_at\": null,",
        "      \"retrospective\": null",
        "      \"status\": \"pending\",",
        "      \"updated_at\": null,",
        "      \"completed_at\": null,",
        "      \"retrospective\": null"
      ],
      "context_before": [
        "{",
        "  \"version\": 1,",
        "  \"session_id\": \"41d5\",",
        "  \"started_at\": \"2025-12-14T11:15:01.281778\","
      ],
      "context_after": [
        "  \"tasks\": [",
        "    {",
        "      \"id\": \"T-20251214-111501-41d5-001\",",
        "      \"title\": \"SEC-001: Add pickle security warning to README\",",
        "      \"priority\": \"high\",",
        "      \"category\": \"security\",",
        "      \"description\": \"Add a security warning to README.md about pickle deserialization risks.\\n\\nLocation: cortical/persistence.py:156 uses pickle.load() which can execute arbitrary code.\\n\\nWarning should include:\\n- Risk explanation (RCE via malicious pickle files)\\n- Recommendation to use JSON format (StateLoader) for untrusted sources\\n- Link to Python pickle documentation security warning\\n\\nEffort: 30 minutes\",",
        "      \"depends_on\": [],",
        "      \"effort\": \"small\",",
        "      \"created_at\": \"2025-12-14T11:15:01.281819\",",
        "      \"context\": {",
        "        \"files\": [",
        "          \"README.md\",",
        "          \"cortical/persistence.py\"",
        "        ],",
        "        \"line\": 156",
        "      },",
        "    },",
        "    {",
        "      \"id\": \"T-20251214-111501-41d5-002\",",
        "      \"title\": \"SEC-002: Add Bandit SAST to CI pipeline\",",
        "      \"priority\": \"high\",",
        "      \"category\": \"security\",",
        "      \"description\": \"Add Bandit (Python static analysis security tool) to CI workflow.\\n\\nAdd to .github/workflows/ci.yml:\\n```yaml\\nsecurity-scan:\\n  name: \\\"Security Scan\\\"\\n  runs-on: ubuntu-latest\\n  steps:\\n  - uses: actions/checkout@v4\\n  - name: Set up Python\\n    uses: actions/setup-python@v5\\n    with:\\n      python-version: '3.11'\\n  - name: Run Bandit\\n    run: |\\n      pip install bandit\\n      bandit -r cortical/ -ll -f txt\\n```\\n\\nEffort: 1 hour\",",
        "      \"depends_on\": [],",
        "      \"effort\": \"small\",",
        "      \"created_at\": \"2025-12-14T11:15:01.281828\",",
        "      \"context\": {",
        "        \"files\": [",
        "          \".github/workflows/ci.yml\"",
        "        ]",
        "      },",
        "    },",
        "    {",
        "      \"id\": \"T-20251214-111501-41d5-003\",",
        "      \"title\": \"SEC-003: Implement pickle HMAC verification\",",
        "      \"status\": \"pending\",",
        "      \"priority\": \"medium\",",
        "      \"category\": \"security\",",
        "      \"description\": \"Add HMAC signature verification before loading pickle files.\\n\\nImplementation:\\n1. Generate HMAC when saving: hmac.new(secret_key, pickle_data, 'sha256')\\n2. Store signature alongside pickle file (.pkl.sig)\\n3. Verify signature before pickle.load()\\n4. Reject files with invalid/missing signatures\\n\\nThis prevents loading tampered pickle files even from trusted sources.\\n\\nEffort: 4 hours\",",
        "      \"depends_on\": [],",
        "      \"effort\": \"medium\","
      ],
      "change_type": "modify"
    },
    {
      "file": "tasks/2025-12-14_11-15-01_41d5.json",
      "function": null,
      "start_line": 62,
      "lines_added": [
        "      \"status\": \"completed\",",
        "      \"updated_at\": \"2025-12-14T11:28:39.474979\",",
        "      \"completed_at\": \"2025-12-14T11:28:39.474979\",",
        "      \"retrospective\": {",
        "        \"notes\": \"Implemented in commit 90b989f\"",
        "      }",
        "      \"status\": \"completed\",",
        "      \"updated_at\": \"2025-12-14T11:28:39.474982\",",
        "      \"completed_at\": \"2025-12-14T11:28:39.474982\",",
        "      \"retrospective\": {",
        "        \"notes\": \"Implemented in commit 90b989f\"",
        "      }",
        "      \"status\": \"completed\",",
        "      \"updated_at\": \"2025-12-14T11:30:12.719118\",",
        "      \"completed_at\": \"2025-12-14T11:30:12.719118\",",
        "      \"retrospective\": {",
        "        \"notes\": \"Created docs/mcp-security.md\"",
        "      }",
        "      \"status\": \"completed\",",
        "      \"updated_at\": \"2025-12-14T11:35:23.935232\",",
        "      \"completed_at\": \"2025-12-14T11:35:23.935232\",",
        "      \"retrospective\": {",
        "        \"notes\": \"Documented security model in SKILL.md, explained why Write permission is necessary\"",
        "      }",
        "      \"status\": \"completed\",",
        "      \"updated_at\": \"2025-12-14T11:28:39.474983\",",
        "      \"completed_at\": \"2025-12-14T11:28:39.474983\",",
        "      \"retrospective\": {",
        "        \"notes\": \"Implemented in commit 90b989f\"",
        "      }"
      ],
      "lines_removed": [
        "      \"status\": \"pending\",",
        "      \"updated_at\": null,",
        "      \"completed_at\": null,",
        "      \"retrospective\": null",
        "      \"status\": \"pending\",",
        "      \"updated_at\": null,",
        "      \"completed_at\": null,",
        "      \"retrospective\": null",
        "      \"status\": \"pending\",",
        "      \"updated_at\": null,",
        "      \"completed_at\": null,",
        "      \"retrospective\": null",
        "      \"status\": \"pending\",",
        "      \"updated_at\": null,",
        "      \"completed_at\": null,",
        "      \"retrospective\": null",
        "      \"status\": \"pending\",",
        "      \"updated_at\": null,",
        "      \"completed_at\": null,",
        "      \"retrospective\": null"
      ],
      "context_before": [
        "        ],",
        "        \"methods\": [",
        "          \"load_processor()\"",
        "        ]",
        "      },",
        "      \"retrospective\": null",
        "    },",
        "    {",
        "      \"id\": \"T-20251214-111501-41d5-004\",",
        "      \"title\": \"SEC-004: Add pip-audit dependency scanning to CI\","
      ],
      "context_after": [
        "      \"priority\": \"medium\",",
        "      \"category\": \"security\",",
        "      \"description\": \"Add pip-audit to check for known vulnerabilities in dependencies.\\n\\nAdd step to security-scan job:\\n```yaml\\n- name: Check Dependencies\\n  run: |\\n    pip install pip-audit\\n    pip install -e \\\".[dev]\\\"\\n    pip-audit --desc || echo \\\"Review required\\\"\\n```\\n\\nEffort: 1 hour\",",
        "      \"depends_on\": [],",
        "      \"effort\": \"small\",",
        "      \"created_at\": \"2025-12-14T11:15:01.281842\",",
        "      \"context\": {",
        "        \"files\": [",
        "          \".github/workflows/ci.yml\"",
        "        ]",
        "      },",
        "    },",
        "    {",
        "      \"id\": \"T-20251214-111501-41d5-005\",",
        "      \"title\": \"SEC-005: Add detect-secrets to CI pipeline\",",
        "      \"priority\": \"medium\",",
        "      \"category\": \"security\",",
        "      \"description\": \"Add detect-secrets to scan for accidentally committed secrets.\\n\\nAdd step to security-scan job:\\n```yaml\\n- name: Scan for Secrets\\n  run: |\\n    pip install detect-secrets\\n    detect-secrets scan --all-files --exclude-files '\\\\.git/.*'\\n```\\n\\nEffort: 1 hour\",",
        "      \"depends_on\": [],",
        "      \"effort\": \"small\",",
        "      \"created_at\": \"2025-12-14T11:15:01.281848\",",
        "      \"context\": {",
        "        \"files\": [",
        "          \".github/workflows/ci.yml\"",
        "        ]",
        "      },",
        "    },",
        "    {",
        "      \"id\": \"T-20251214-111501-41d5-006\",",
        "      \"title\": \"SEC-006: Document MCP server security model\",",
        "      \"priority\": \"medium\",",
        "      \"category\": \"security\",",
        "      \"description\": \"Create security documentation for the MCP server integration.\\n\\nDocument:\\n- What capabilities the MCP server exposes\\n- Input validation performed\\n- Trust model (who can call the server)\\n- Rate limiting considerations\\n- Recommended deployment configurations\\n\\nCreate: docs/security.md or docs/mcp-security.md\\n\\nEffort: 2 hours\",",
        "      \"depends_on\": [],",
        "      \"effort\": \"small\",",
        "      \"created_at\": \"2025-12-14T11:15:01.281854\",",
        "      \"context\": {",
        "        \"files\": [",
        "          \"cortical/mcp_server.py\",",
        "          \"docs/\"",
        "        ]",
        "      },",
        "    },",
        "    {",
        "      \"id\": \"T-20251214-111501-41d5-007\",",
        "      \"title\": \"SEC-007: Review task-manager skill permissions\",",
        "      \"priority\": \"low\",",
        "      \"category\": \"security\",",
        "      \"description\": \"Review and potentially restrict the task-manager skill's Write access.\\n\\nCurrent: allowed-tools: Read, Bash, Write\\nConcern: Write access could be used to modify arbitrary files\\n\\nOptions:\\n1. Remove Write, use Bash for file operations (more auditable)\\n2. Document what Write is used for and why it's necessary\\n3. Add path restrictions if skill system supports it\\n\\nEffort: 30 minutes\",",
        "      \"depends_on\": [],",
        "      \"effort\": \"small\",",
        "      \"created_at\": \"2025-12-14T11:15:01.281861\",",
        "      \"context\": {",
        "        \"files\": [",
        "          \".claude/skills/task-manager/SKILL.md\"",
        "        ]",
        "      },",
        "    },",
        "    {",
        "      \"id\": \"T-20251214-111501-41d5-008\",",
        "      \"title\": \"SEC-008: Add deprecation warning for pickle format\",",
        "      \"priority\": \"low\",",
        "      \"category\": \"security\",",
        "      \"description\": \"Add runtime deprecation warning when using pickle format.\\n\\nIn persistence.py save/load methods:\\n```python\\nimport warnings\\nif format == 'pickle':\\n    warnings.warn(\\n        \\\"Pickle format is deprecated due to security concerns. \\\"\\n        \\\"Use format='json' or format='protobuf' instead.\\\",\\n        DeprecationWarning,\\n        stacklevel=2\\n    )\\n```\\n\\nEffort: 2 hours\",",
        "      \"depends_on\": [],",
        "      \"effort\": \"small\",",
        "      \"created_at\": \"2025-12-14T11:15:01.281870\",",
        "      \"context\": {",
        "        \"files\": [",
        "          \"cortical/persistence.py\"",
        "        ]",
        "      },",
        "    },",
        "    {",
        "      \"id\": \"T-20251214-111501-41d5-009\",",
        "      \"title\": \"SEC-009: Add security-focused test suite\",",
        "      \"status\": \"pending\",",
        "      \"priority\": \"low\",",
        "      \"category\": \"security\",",
        "      \"description\": \"Create tests/security/ directory with security-focused tests.\\n\\nTest cases:\\n- Path traversal prevention in file operations\\n- Input validation on public API methods\\n- Pickle signature verification (if SEC-003 implemented)\\n- Large input handling (DoS prevention)\\n\\nEffort: 4 hours\",",
        "      \"depends_on\": [],",
        "      \"effort\": \"medium\","
      ],
      "change_type": "modify"
    },
    {
      "file": "tasks/legacy_migration.json",
      "function": null,
      "start_line": 1172,
      "lines_added": [
        "      \"status\": \"completed\",",
        "      \"updated_at\": \"2025-12-14T11:34:29.103266\",",
        "      \"completed_at\": \"2025-12-14T11:34:29.103248\",",
        "      \"retrospective\": {",
        "        \"notes\": \"Implemented cortical/diff.py with compare_with(), compare_documents(), what_changed() methods\"",
        "      }"
      ],
      "lines_removed": [
        "      \"status\": \"pending\",",
        "      \"updated_at\": \"2025-12-14T11:11:44.783627\",",
        "      \"completed_at\": null,",
        "      \"retrospective\": null"
      ],
      "context_before": [
        "      \"updated_at\": \"2025-12-14T11:11:44.783627\",",
        "      \"completed_at\": \"2025-12-11T00:00:00\",",
        "      \"context\": {",
        "        \"legacy_task_number\": 72",
        "      },",
        "      \"retrospective\": null",
        "    },",
        "    {",
        "      \"id\": \"LEGACY-075\",",
        "      \"title\": \"Add \\\"What Changed?\\\" semantic diff\","
      ],
      "context_after": [
        "      \"priority\": \"low\",",
        "      \"category\": \"devex\",",
        "      \"description\": \"Migrated from legacy TASK_LIST.md task #75\",",
        "      \"depends_on\": [],",
        "      \"effort\": \"unknown\",",
        "      \"created_at\": \"2025-12-10T00:00:00\",",
        "      \"context\": {",
        "        \"legacy_task_number\": 75",
        "      },",
        "    },",
        "    {",
        "      \"id\": \"LEGACY-077\",",
        "      \"title\": \"Add Interactive \\\"Ask the Codebase\\\" Mode\",",
        "      \"status\": \"completed\",",
        "      \"priority\": \"medium\",",
        "      \"category\": \"devex\",",
        "      \"description\": \"Migrated from legacy TASK_LIST.md task #77\",",
        "      \"depends_on\": [],",
        "      \"effort\": \"unknown\","
      ],
      "change_type": "modify"
    }
  ],
  "hour_of_day": 11,
  "day_of_week": "Sunday",
  "seconds_since_last_commit": -93584,
  "is_merge": true,
  "is_initial": false,
  "parent_count": 2,
  "session_id": null,
  "related_chats": [],
  "ci_result": null,
  "reverted": false,
  "amended": false
}