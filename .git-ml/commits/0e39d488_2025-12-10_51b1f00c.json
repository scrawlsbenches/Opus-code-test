{
  "hash": "0e39d48876e97cd3d26e391b702133c00f323411",
  "message": "Merge pull request #29 from scrawlsbenches/claude/run-full-index-build-01KKr2RsYhW1qv6vBjbzzNuk",
  "author": "scrawlsbenches",
  "timestamp": "2025-12-10 22:05:04 -0500",
  "branch": "claude/multi-index-design-DvifZ",
  "files_changed": [
    "TASK_LIST.md",
    "cortical/processor.py",
    "cortical/query.py",
    "cortical/tokenizer.py",
    "scripts/search_codebase.py",
    "tests/test_tokenizer.py"
  ],
  "insertions": 402,
  "deletions": 24,
  "hunks": [
    {
      "file": "TASK_LIST.md",
      "function": "Key files to understand:",
      "start_line": 2877,
      "lines_added": [
        "### 81. Fix Tokenizer Underscore-Prefixed Identifier Bug",
        "",
        "**File:** `cortical/tokenizer.py`",
        "**Line:** 265",
        "**Status:** [x] Completed",
        "**Priority:** High",
        "**Category:** Code Search",
        "",
        "**Problem:**",
        "The tokenizer regex `r'\\b[a-zA-Z][a-zA-Z0-9_]*\\b'` requires tokens to start with a letter, which causes Python dunder methods (`__init__`, `__slots__`, `__str__`) and private variables (`_id_index`, `_cache`) to be completely unsearchable in code search.",
        "",
        "**Found via dog-fooding:** Searching for `__slots__` returns zero results even though it exists in the codebase.",
        "",
        "**Root Cause:**",
        "```python",
        "# Line 265 - requires first char to be a letter",
        "raw_tokens = re.findall(r'\\b[a-zA-Z][a-zA-Z0-9_]*\\b', text)",
        "```",
        "",
        "**Solution:**",
        "1. Modify regex to capture underscore-prefixed identifiers:",
        "   ```python",
        "   raw_tokens = re.findall(r'\\b_*[a-zA-Z][a-zA-Z0-9_]*\\b', text)",
        "   ```",
        "2. Ensure dunder methods are preserved (not filtered as stop words)",
        "3. Add `'init'`, `'str'`, `'repr'` etc. to `PROGRAMMING_KEYWORDS` if not present",
        "4. Add tests for underscore-prefixed identifier tokenization",
        "",
        "**Impact:**",
        "- High for code search use case",
        "- Python-specific identifiers currently invisible to search",
        "- Affects private methods, dunder methods, internal variables",
        "",
        "**Files to Modify:**",
        "- `cortical/tokenizer.py` - Fix regex pattern",
        "- `tests/test_tokenizer.py` - Add tests for underscore identifiers",
        "",
        "---",
        "",
        "### 84. Add Direct Definition Pattern Search for Code Search",
        "",
        "**Files:** `cortical/query.py`, `scripts/search_codebase.py`",
        "**Status:** [ ] Not Started",
        "**Priority:** High",
        "**Category:** Code Search",
        "",
        "**Problem:**",
        "When searching for \"class Minicolumn\", the passage containing the actual class definition (`class Minicolumn:`) scores low because TF-IDF favors chunks with more query term matches. The definition chunk is mostly docstring text with few matching terms.",
        "",
        "**Found via dog-fooding:** Even with document-level boosting, the actual class definition often doesn't appear in top results.",
        "",
        "**Solution:**",
        "1. For definition queries, directly search source files for the definition pattern",
        "2. Create a synthetic high-scoring passage from the definition location",
        "3. Inject this passage into results before final ranking",
        "4. Consider using regex-based chunk extraction around definition sites",
        "",
        "**Example:**",
        "```python",
        "def find_definition_passage(doc_text: str, pattern: str, context_chars: int = 500):",
        "    \"\"\"Extract passage around a definition pattern match.\"\"\"",
        "    match = re.search(pattern, doc_text)",
        "    if match:",
        "        start = max(0, match.start() - 50)",
        "        end = min(len(doc_text), match.end() + context_chars)",
        "        return doc_text[start:end], start, end",
        "    return None",
        "```",
        "",
        "---",
        "",
        "### 85. Improve Test File vs Source File Ranking",
        "",
        "**Files:** `cortical/query.py`, `scripts/search_codebase.py`",
        "**Status:** [ ] Not Started",
        "**Priority:** Medium",
        "**Category:** Code Search",
        "",
        "**Problem:**",
        "Test files often rank higher than source files because they mention class/function names more frequently (in test method names, assertions, etc.). This is counterintuitive for users looking for implementations.",
        "",
        "**Found via dog-fooding:** Searching \"class Minicolumn\" returns test_layers.py results before minicolumn.py results.",
        "",
        "**Solution Options:**",
        "1. Add file path-based scoring penalty for test files when query intent is \"implementation\"",
        "2. Detect test files (tests/, *_test.py, test_*.py) and apply negative boost",
        "3. Add user preference for \"prefer source over tests\" in search",
        "4. Use query intent detection to auto-adjust (implementation queries → penalize tests)",
        "",
        "**Considerations:**",
        "- Should be configurable (sometimes users want to find tests)",
        "- Could integrate with existing `get_doc_type_boost()` function",
        "- Balance: tests are valuable for understanding usage patterns",
        "",
        "---",
        "",
        "### 86. Add Semantic Chunk Boundaries for Code",
        "",
        "**Files:** `cortical/query.py`",
        "**Status:** [ ] Not Started",
        "**Priority:** Medium",
        "**Category:** Code Search",
        "",
        "**Problem:**",
        "Current chunking uses fixed character boundaries which can split code mid-function or mid-class. This creates passages that lack context and score poorly.",
        "",
        "**Found via dog-fooding:** The chunk containing `class Minicolumn:` starts mid-way through the previous function's code.",
        "",
        "**Solution:**",
        "1. Detect code structure boundaries (class, def, blank lines)",
        "2. Adjust chunk boundaries to align with semantic units",
        "3. For Python: use indentation changes as boundary hints",
        "4. Consider AST-based chunking for supported languages",
        "",
        "**Example improvement:**",
        "```python",
        "def create_code_aware_chunks(text: str, target_size: int = 500):",
        "    \"\"\"Create chunks aligned to code structure boundaries.\"\"\"",
        "    # Find all class/def boundaries",
        "    boundaries = [m.start() for m in re.finditer(r'^(?:class |def )', text, re.M)]",
        "    # Create chunks that start at boundaries",
        "    ...",
        "```",
        "",
        "---",
        "",
        "| 81 | High | Fix tokenizer underscore-prefixed identifiers | ✓ Done | Code Search |",
        "| 82 | High | Add code stop words filter for query expansion | ✓ Done | Code Search |",
        "| 83 | Medium | Add definition-aware boosting for class/def queries | ✓ Done | Code Search |",
        "| 84 | High | Add direct definition pattern search | | Code Search |",
        "| 85 | Medium | Improve test file vs source file ranking | | Code Search |",
        "| 86 | Medium | Add semantic chunk boundaries for code | | Code Search |"
      ],
      "lines_removed": [],
      "context_before": [
        "  3. cortical/analysis.py - Graph algorithms",
        "",
        "[Press Enter to explore processor.py, or type a question]",
        "> How does PageRank work here?",
        "",
        "[Retrieves and explains relevant passages...]",
        "```",
        "",
        "---",
        ""
      ],
      "context_after": [
        "## Summary Table",
        "",
        "| # | Priority | Task | Status | Category |",
        "|---|----------|------|--------|----------|",
        "| 67 | Low | Fix O(n) lookup in showcase | ✓ Done | Showcase |",
        "| 68 | Medium | Add code-specific features to showcase | ✓ Done | Showcase |",
        "| 69 | Medium | Add passage-level search demo | ✓ Done | Showcase |",
        "| 70 | Low | Add performance timing to showcase | ✓ Done | Showcase |",
        "| 71 | High | Enable code-aware tokenization in index | ✓ Done | Code Index |",
        "| 72 | High | Use programming query expansion in search | ✓ Done | Code Index |",
        "| 73 | Medium | Add \"Find Similar Code\" command | | Code Index |",
        "| 74 | Medium | Add \"Explain This Code\" command | | Developer Experience |",
        "| 75 | Medium | Add \"What Changed?\" semantic diff | | Developer Experience |",
        "| 76 | Medium | Add \"Suggest Related Files\" feature | | Developer Experience |",
        "| 77 | High | Add interactive \"Ask the Codebase\" mode | ✓ Done | Developer Experience |",
        "| 78 | Low | Add code pattern detection | | Developer Experience |",
        "| 79 | Low | Add corpus health dashboard | | Developer Experience |",
        "| 80 | Low | Add \"Learning Mode\" for new contributors | | Developer Experience |",
        "",
        "---",
        "",
        "*Added 2025-12-11, completions updated 2025-12-11*"
      ],
      "change_type": "add"
    },
    {
      "file": "cortical/processor.py",
      "function": "class CorticalTextProcessor:",
      "start_line": 1261,
      "lines_added": [
        "        filter_code_stop_words: bool = False,",
        "            filter_code_stop_words: Filter ubiquitous code tokens (self, cls, etc.)",
        "            use_code_concepts=use_code_concepts,",
        "            filter_code_stop_words=filter_code_stop_words",
        "        Also filters ubiquitous code tokens (self, cls, etc.) from expansion.",
        "            use_code_concepts=True,",
        "            filter_code_stop_words=True  # Filter self, cls, etc."
      ],
      "lines_removed": [
        "            use_code_concepts=use_code_concepts",
        "            use_code_concepts=True"
      ],
      "context_before": [
        "    ",
        "    def find_similar_by_embedding(self, term: str, top_n: int = 10) -> List[Tuple[str, float]]:",
        "        return emb_module.find_similar_by_embedding(self.embeddings, term, top_n)",
        "    ",
        "    def expand_query(",
        "        self,",
        "        query_text: str,",
        "        max_expansions: int = 10,",
        "        use_variants: bool = True,",
        "        use_code_concepts: bool = False,"
      ],
      "context_after": [
        "        verbose: bool = False",
        "    ) -> Dict[str, float]:",
        "        \"\"\"",
        "        Expand a query using lateral connections and concept clusters.",
        "",
        "        Args:",
        "            query_text: Original query string",
        "            max_expansions: Maximum expansion terms to add",
        "            use_variants: Try word variants when direct match fails",
        "            use_code_concepts: Include programming synonym expansions",
        "",
        "        Returns:",
        "            Dict mapping terms to weights",
        "        \"\"\"",
        "        return query_module.expand_query(",
        "            query_text,",
        "            self.layers,",
        "            self.tokenizer,",
        "            max_expansions=max_expansions,",
        "            use_variants=use_variants,",
        "        )",
        "",
        "    def expand_query_for_code(self, query_text: str, max_expansions: int = 15) -> Dict[str, float]:",
        "        \"\"\"",
        "        Expand a query optimized for code search.",
        "",
        "        Enables code concept expansion to find programming synonyms",
        "        (e.g., \"fetch\" also matches \"get\", \"load\", \"retrieve\").",
        "",
        "        Args:",
        "            query_text: Original query string",
        "            max_expansions: Maximum expansion terms to add",
        "",
        "        Returns:",
        "            Dict mapping terms to weights",
        "        \"\"\"",
        "        return query_module.expand_query(",
        "            query_text,",
        "            self.layers,",
        "            self.tokenizer,",
        "            max_expansions=max_expansions,",
        "            use_variants=True,",
        "        )",
        "",
        "    def expand_query_cached(",
        "        self,",
        "        query_text: str,",
        "        max_expansions: int = 10,",
        "        use_variants: bool = True,",
        "        use_code_concepts: bool = False",
        "    ) -> Dict[str, float]:",
        "        \"\"\""
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/query.py",
      "function": "Query expansion and search functionality.",
      "start_line": 7,
      "lines_added": [
        "from .tokenizer import Tokenizer, CODE_EXPANSION_STOP_WORDS"
      ],
      "lines_removed": [
        "from .tokenizer import Tokenizer"
      ],
      "context_before": [
        "Provides methods for expanding queries using lateral connections,",
        "concept clusters, and word variants, then searching the corpus",
        "using TF-IDF and graph-based scoring.",
        "\"\"\"",
        "",
        "from typing import Dict, List, Tuple, Optional, TypedDict, Any",
        "from collections import defaultdict",
        "import re",
        "",
        "from .layers import CorticalLayer, HierarchicalLayer"
      ],
      "context_after": [
        "from .code_concepts import expand_code_concepts, get_related_terms",
        "",
        "",
        "# Intent types for query understanding",
        "class ParsedIntent(TypedDict):",
        "    \"\"\"Structured representation of a parsed query intent.\"\"\"",
        "    action: Optional[str]       # The verb/action (e.g., \"handle\", \"implement\")",
        "    subject: Optional[str]      # The main subject (e.g., \"authentication\")",
        "    intent: str                 # Query intent type (location, implementation, definition, etc.)",
        "    question_word: Optional[str]  # Original question word if present"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/query.py",
      "function": "def find_documents_with_boost(",
      "start_line": 234,
      "lines_added": [
        "    use_code_concepts: bool = False,",
        "    filter_code_stop_words: bool = False",
        "        filter_code_stop_words: Filter ubiquitous code tokens (self, cls, etc.)",
        "                                from expansion candidates. Useful for code search."
      ],
      "lines_removed": [
        "    use_code_concepts: bool = False"
      ],
      "context_before": [
        "",
        "",
        "def expand_query(",
        "    query_text: str,",
        "    layers: Dict[CorticalLayer, HierarchicalLayer],",
        "    tokenizer: Tokenizer,",
        "    max_expansions: int = 10,",
        "    use_lateral: bool = True,",
        "    use_concepts: bool = True,",
        "    use_variants: bool = True,"
      ],
      "context_after": [
        ") -> Dict[str, float]:",
        "    \"\"\"",
        "    Expand a query using lateral connections and concept clusters.",
        "",
        "    This mimics how the brain retrieves related memories when given a cue:",
        "    - Lateral connections: direct word associations (like priming)",
        "    - Concept clusters: semantic category membership",
        "    - Word variants: stemming and synonym mapping",
        "    - Code concepts: programming synonym groups (get/fetch/load)",
        "",
        "    Args:",
        "        query_text: Original query string",
        "        layers: Dictionary of layers",
        "        tokenizer: Tokenizer instance",
        "        max_expansions: Maximum number of expansion terms to add",
        "        use_lateral: Include terms from lateral connections",
        "        use_concepts: Include terms from concept clusters",
        "        use_variants: Try word variants when direct match fails",
        "        use_code_concepts: Include programming synonym expansions",
        "",
        "    Returns:",
        "        Dict mapping terms to weights (original terms get weight 1.0)",
        "    \"\"\"",
        "    tokens = tokenizer.tokenize(query_text)",
        "    layer0 = layers[CorticalLayer.TOKENS]",
        "    layer2 = layers.get(CorticalLayer.CONCEPTS)",
        "    ",
        "    # Start with original terms at full weight",
        "    expanded: Dict[str, float] = {}"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/query.py",
      "function": "def expand_query(",
      "start_line": 337,
      "lines_added": [
        "    # Filter out ubiquitous code tokens if requested",
        "    if filter_code_stop_words:",
        "        candidate_expansions = {",
        "            term: score for term, score in candidate_expansions.items()",
        "            if term not in CODE_EXPANSION_STOP_WORDS",
        "        }",
        "",
        "",
        "",
        "class DefinitionQuery(TypedDict):",
        "    \"\"\"Info about a definition-seeking query.\"\"\"",
        "    is_definition_query: bool",
        "    definition_type: Optional[str]  # 'class', 'function', 'method', 'variable'",
        "    identifier: Optional[str]       # The identifier being searched for",
        "    pattern: Optional[str]          # Regex pattern to find the definition",
        "",
        "",
        "def detect_definition_query(query_text: str) -> DefinitionQuery:",
        "    \"\"\"",
        "    Detect if a query is searching for a code definition.",
        "",
        "    Recognizes patterns like:",
        "    - \"class Minicolumn\" -> looking for class definition",
        "    - \"def compute_tfidf\" -> looking for function definition",
        "    - \"function handleClick\" -> looking for function definition",
        "",
        "    Args:",
        "        query_text: The search query",
        "",
        "    Returns:",
        "        DefinitionQuery with detection results and pattern to search for",
        "    \"\"\"",
        "    query_lower = query_text.lower().strip()",
        "",
        "    # Patterns for definition searches",
        "    patterns = [",
        "        # \"class ClassName\" or \"class ClassName definition\"",
        "        (r'\\bclass\\s+([A-Za-z_][A-Za-z0-9_]*)', 'class',",
        "         lambda name: rf'\\bclass\\s+{re.escape(name)}\\s*[:\\(]'),",
        "        # \"def function_name\" or \"function function_name\"",
        "        (r'\\b(?:def|function)\\s+([A-Za-z_][A-Za-z0-9_]*)', 'function',",
        "         lambda name: rf'\\bdef\\s+{re.escape(name)}\\s*\\('),",
        "        # \"method methodName\"",
        "        (r'\\bmethod\\s+([A-Za-z_][A-Za-z0-9_]*)', 'method',",
        "         lambda name: rf'\\bdef\\s+{re.escape(name)}\\s*\\('),",
        "    ]",
        "",
        "    for regex, def_type, pattern_fn in patterns:",
        "        match = re.search(regex, query_text, re.IGNORECASE)",
        "        if match:",
        "            identifier = match.group(1)",
        "            return DefinitionQuery(",
        "                is_definition_query=True,",
        "                definition_type=def_type,",
        "                identifier=identifier,",
        "                pattern=pattern_fn(identifier)",
        "            )",
        "",
        "    return DefinitionQuery(",
        "        is_definition_query=False,",
        "        definition_type=None,",
        "        identifier=None,",
        "        pattern=None",
        "    )",
        "",
        "",
        "def apply_definition_boost(",
        "    passages: List[Tuple[str, str, int, int, float]],",
        "    query_text: str,",
        "    boost_factor: float = 3.0",
        ") -> List[Tuple[str, str, int, int, float]]:",
        "    \"\"\"",
        "    Boost passages that contain actual code definitions matching the query.",
        "",
        "    When searching for \"class Minicolumn\", passages containing the actual",
        "    class definition (`class Minicolumn:`) get boosted over passages that",
        "    merely reference or use the class.",
        "",
        "    Args:",
        "        passages: List of (text, doc_id, start, end, score) tuples",
        "        query_text: The original search query",
        "        boost_factor: Multiplier for definition-containing passages (default 3.0)",
        "",
        "    Returns:",
        "        Re-scored passages with definition boost applied, sorted by new score",
        "    \"\"\"",
        "    definition_info = detect_definition_query(query_text)",
        "",
        "    if not definition_info['is_definition_query'] or not definition_info['pattern']:",
        "        return passages",
        "",
        "    pattern = re.compile(definition_info['pattern'], re.IGNORECASE)",
        "    boosted_passages = []",
        "",
        "    for text, doc_id, start, end, score in passages:",
        "        if pattern.search(text):",
        "            # This passage contains the actual definition",
        "            boosted_passages.append((text, doc_id, start, end, score * boost_factor))",
        "        else:",
        "            boosted_passages.append((text, doc_id, start, end, score))",
        "",
        "    # Re-sort by boosted scores",
        "    boosted_passages.sort(key=lambda x: x[4], reverse=True)",
        "    return boosted_passages",
        "",
        "",
        "def boost_definition_documents(",
        "    doc_results: List[Tuple[str, float]],",
        "    query_text: str,",
        "    documents: Dict[str, str],",
        "    boost_factor: float = 2.0",
        ") -> List[Tuple[str, float]]:",
        "    \"\"\"",
        "    Boost documents that contain the actual definition being searched for.",
        "",
        "    This helps ensure the source file containing a class/function definition",
        "    is included in the document candidates, even if test files mention the",
        "    identifier more frequently.",
        "",
        "    Args:",
        "        doc_results: List of (doc_id, score) tuples",
        "        query_text: The original search query",
        "        documents: Dict mapping doc_id to document text",
        "        boost_factor: Multiplier for definition-containing docs (default 2.0)",
        "",
        "    Returns:",
        "        Re-scored document results with definition boost applied",
        "    \"\"\"",
        "    definition_info = detect_definition_query(query_text)",
        "",
        "    if not definition_info['is_definition_query'] or not definition_info['pattern']:",
        "        return doc_results",
        "",
        "    pattern = re.compile(definition_info['pattern'], re.IGNORECASE)",
        "    boosted_docs = []",
        "",
        "    for doc_id, score in doc_results:",
        "        doc_text = documents.get(doc_id, '')",
        "        if pattern.search(doc_text):",
        "            # This document contains the actual definition",
        "            boosted_docs.append((doc_id, score * boost_factor))",
        "        else:",
        "            boosted_docs.append((doc_id, score))",
        "",
        "    # Re-sort by boosted scores",
        "    boosted_docs.sort(key=lambda x: x[1], reverse=True)",
        "    return boosted_docs",
        "",
        ""
      ],
      "lines_removed": [
        "    ",
        "    "
      ],
      "context_before": [
        "            list(expanded.keys()),",
        "            max_expansions_per_term=3,",
        "            weight=0.6",
        "        )",
        "        for term, weight in code_expansions.items():",
        "            if term not in expanded:",
        "                candidate_expansions[term] = max(",
        "                    candidate_expansions[term], weight",
        "                )",
        ""
      ],
      "context_after": [
        "    # Select top expansions",
        "    sorted_candidates = sorted(",
        "        candidate_expansions.items(),",
        "        key=lambda x: x[1],",
        "        reverse=True",
        "    )[:max_expansions]",
        "    for term, score in sorted_candidates:",
        "        expanded[term] = score",
        "    return expanded",
        "",
        "",
        "def parse_intent_query(query_text: str) -> ParsedIntent:",
        "    \"\"\"",
        "    Parse a natural language query to extract intent and searchable terms.",
        "",
        "    Analyzes queries like \"where do we handle authentication?\" to identify:",
        "    - Question word (where) -> intent type (location)",
        "    - Action verb (handle) -> search for handling code",
        "    - Subject (authentication) -> main topic with synonyms",
        "",
        "    Args:"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/query.py",
      "function": "def expand_query_semantic(",
      "start_line": 763,
      "lines_added": [
        "    semantic_discount: float = 0.8,",
        "    filter_code_stop_words: bool = False",
        "        filter_code_stop_words: Filter ubiquitous code tokens (self, cls, etc.)",
        "                                from expansion candidates. Useful for code search.",
        "        query_terms = expand_query(",
        "            query_text, layers, tokenizer,",
        "            max_expansions=max_expansions,",
        "            filter_code_stop_words=filter_code_stop_words",
        "        )"
      ],
      "lines_removed": [
        "    semantic_discount: float = 0.8",
        "        query_terms = expand_query(query_text, layers, tokenizer, max_expansions=max_expansions)"
      ],
      "context_before": [
        "",
        "",
        "def get_expanded_query_terms(",
        "    query_text: str,",
        "    layers: Dict[CorticalLayer, HierarchicalLayer],",
        "    tokenizer: Tokenizer,",
        "    use_expansion: bool = True,",
        "    semantic_relations: Optional[List[Tuple[str, str, str, float]]] = None,",
        "    use_semantic: bool = True,",
        "    max_expansions: int = 5,"
      ],
      "context_after": [
        ") -> Dict[str, float]:",
        "    \"\"\"",
        "    Get expanded query terms with optional semantic expansion.",
        "",
        "    This is a helper function that consolidates query expansion logic used",
        "    by multiple search functions. It handles:",
        "    - Lateral connection expansion via expand_query()",
        "    - Semantic relation expansion via expand_query_semantic()",
        "    - Merging of expansion results with appropriate weighting",
        "",
        "    Args:",
        "        query_text: Original query string",
        "        layers: Dictionary of layers",
        "        tokenizer: Tokenizer instance",
        "        use_expansion: Whether to expand query terms using lateral connections",
        "        semantic_relations: Optional list of semantic relations for expansion",
        "        use_semantic: Whether to use semantic relations for expansion",
        "        max_expansions: Maximum expansion terms per method (default 5)",
        "        semantic_discount: Weight multiplier for semantic expansions (default 0.8)",
        "",
        "    Returns:",
        "        Dict mapping terms to weights (original terms get weight 1.0,",
        "        expansions get lower weights based on connection strength)",
        "",
        "    Example:",
        "        >>> terms = get_expanded_query_terms(\"neural networks\", layers, tokenizer)",
        "        >>> # Returns: {'neural': 1.0, 'networks': 1.0, 'deep': 0.3, 'learning': 0.25, ...}",
        "    \"\"\"",
        "    if use_expansion:",
        "        # Start with lateral connection expansion",
        "",
        "        # Add semantic expansion if available",
        "        if use_semantic and semantic_relations:",
        "            semantic_terms = expand_query_semantic(",
        "                query_text, layers, tokenizer, semantic_relations, max_expansions=max_expansions",
        "            )",
        "            # Merge semantic expansions (don't override stronger weights)",
        "            for term, weight in semantic_terms.items():",
        "                if term not in query_terms:",
        "                    query_terms[term] = weight * semantic_discount"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/query.py",
      "function": "def find_passages_for_query(",
      "start_line": 1315,
      "lines_added": [
        "    # Get candidate documents",
        "        # Use provided filter directly as candidates (caller may have pre-boosted)",
        "        # Assign dummy scores since we'll re-score passages anyway",
        "        doc_scores = [(doc_id, 1.0) for doc_id in doc_filter if doc_id in documents]",
        "    else:",
        "        # No filter - get candidates via document search",
        "        doc_scores = find_documents_for_query(",
        "            query_text, layers, tokenizer,",
        "            top_n=min(len(documents), top_n * 3),",
        "            use_expansion=use_expansion,",
        "            semantic_relations=semantic_relations,",
        "            use_semantic=use_semantic",
        "        )"
      ],
      "lines_removed": [
        "    # First, get candidate documents (more than we need, since we'll rank passages)",
        "    doc_scores = find_documents_for_query(",
        "        query_text, layers, tokenizer,",
        "        top_n=min(len(documents), top_n * 3),",
        "        use_expansion=use_expansion,",
        "        semantic_relations=semantic_relations,",
        "        use_semantic=use_semantic",
        "    )",
        "",
        "    # Apply document filter if provided",
        "        doc_scores = [(doc_id, score) for doc_id, score in doc_scores if doc_id in doc_filter]"
      ],
      "context_before": [
        "        semantic_relations=semantic_relations,",
        "        use_semantic=use_semantic",
        "    )",
        "",
        "    if not query_terms:",
        "        return []",
        "",
        "    # Pre-compute minicolumn lookups for query terms (optimization)",
        "    term_cols = precompute_term_cols(query_terms, layer0)",
        ""
      ],
      "context_after": [
        "    if doc_filter:",
        "",
        "    # Score passages within candidate documents",
        "    passages: List[Tuple[str, str, int, int, float]] = []",
        "",
        "    for doc_id, doc_score in doc_scores:",
        "        if doc_id not in documents:",
        "            continue",
        "",
        "        text = documents[doc_id]",
        "        chunks = create_chunks(text, chunk_size, overlap)"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/tokenizer.py",
      "function": "Text tokenization with stemming and word variant support.",
      "start_line": 6,
      "lines_added": [
        "# Ubiquitous code tokens that pollute query expansion",
        "# These appear in almost every Python method/function, so they add noise",
        "# rather than signal when expanding queries for code search",
        "CODE_EXPANSION_STOP_WORDS = frozenset({",
        "    'self', 'cls',              # Class method parameters",
        "    'args', 'kwargs',           # Variadic parameters",
        "    'none', 'true', 'false',    # Literals (too common)",
        "    'return', 'pass',           # Control flow (too common)",
        "    'def', 'class',             # Definitions (search for these explicitly)",
        "})",
        "",
        "",
        "    # Dunder method components (for __init__, __slots__, etc.)",
        "    'repr', 'slots', 'name', 'doc', 'call', 'iter', 'next', 'enter',",
        "    'exit', 'getitem', 'setitem', 'delitem', 'contains', 'hash', 'eq',"
      ],
      "lines_removed": [],
      "context_before": [
        "",
        "Like early visual processing, the tokenizer extracts basic features",
        "(words) from raw input, filtering noise (stop words) and normalizing",
        "representations (lowercase, stemming).",
        "\"\"\"",
        "",
        "import re",
        "from typing import List, Set, Optional, Dict, Tuple",
        "",
        ""
      ],
      "context_after": [
        "# Programming keywords that should be preserved even if in stop words",
        "PROGRAMMING_KEYWORDS = frozenset({",
        "    'def', 'class', 'function', 'return', 'import', 'from', 'if', 'else',",
        "    'elif', 'for', 'while', 'try', 'except', 'finally', 'with', 'as',",
        "    'yield', 'async', 'await', 'lambda', 'pass', 'break', 'continue',",
        "    'raise', 'assert', 'global', 'nonlocal', 'del', 'true', 'false',",
        "    'none', 'null', 'void', 'int', 'str', 'float', 'bool', 'list',",
        "    'dict', 'set', 'tuple', 'self', 'cls', 'init', 'main', 'args',",
        "    'kwargs', 'super', 'property', 'staticmethod', 'classmethod',",
        "    'isinstance', 'hasattr', 'getattr', 'setattr', 'len', 'range',",
        "    'enumerate', 'zip', 'map', 'filter', 'print', 'open', 'read',",
        "    'write', 'close', 'append', 'extend', 'insert', 'remove', 'pop',",
        "    'const', 'let', 'var', 'public', 'private', 'protected', 'static',",
        "    'final', 'abstract', 'interface', 'implements', 'extends', 'new',",
        "    'this', 'constructor', 'module', 'export', 'require', 'package',",
        "    # Common identifier components that shouldn't be filtered",
        "    'get', 'set', 'add', 'put', 'has', 'can', 'run', 'max', 'min',",
        "})",
        "",
        "",
        "def split_identifier(identifier: str) -> List[str]:",
        "    \"\"\""
      ],
      "change_type": "add"
    },
    {
      "file": "cortical/tokenizer.py",
      "function": "class Tokenizer:",
      "start_line": 255,
      "lines_added": [
        "        # Also matches underscore-prefixed: __init__, _private, __slots__",
        "        raw_tokens = re.findall(r'\\b_*[a-zA-Z][a-zA-Z0-9_]*\\b', text)"
      ],
      "lines_removed": [
        "        raw_tokens = re.findall(r'\\b[a-zA-Z][a-zA-Z0-9_]*\\b', text)"
      ],
      "context_before": [
        "",
        "        Examples:",
        "            >>> t = Tokenizer(split_identifiers=True)",
        "            >>> t.tokenize(\"getUserCredentials fetches data\")",
        "            ['getusercredentials', 'get', 'user', 'credentials', 'fetches', 'data']",
        "        \"\"\"",
        "        should_split = split_identifiers if split_identifiers is not None else self.split_identifiers",
        "",
        "        # Extract potential identifiers (including camelCase with internal caps)",
        "        # Pattern matches: word2vec, getUserData, get_user_data, XMLParser"
      ],
      "context_after": [
        "",
        "        result = []",
        "        seen_splits = set()  # Only track splits to avoid duplicates from them",
        "",
        "        for token in raw_tokens:",
        "            token_lower = token.lower()",
        "",
        "            # Skip stop words and short words",
        "            if token_lower in self.stop_words or len(token_lower) < self.min_word_length:",
        "                continue"
      ],
      "change_type": "modify"
    },
    {
      "file": "scripts/search_codebase.py",
      "function": "Usage:",
      "start_line": 12,
      "lines_added": [
        "from cortical.query import (",
        "    apply_definition_boost,",
        "    boost_definition_documents,",
        "    detect_definition_query",
        ")"
      ],
      "lines_removed": [],
      "context_before": [
        "",
        "import argparse",
        "import os",
        "import sys",
        "from pathlib import Path",
        "",
        "# Add parent directory to path for imports",
        "sys.path.insert(0, str(Path(__file__).parent.parent))",
        "",
        "from cortical.processor import CorticalTextProcessor"
      ],
      "context_after": [
        "",
        "",
        "def find_line_number(doc_content: str, passage_start: int) -> int:",
        "    \"\"\"Find the line number for a character position.\"\"\"",
        "    return doc_content[:passage_start].count('\\n') + 1",
        "",
        "",
        "def format_passage(passage: str, max_width: int = 80) -> str:",
        "    \"\"\"Format a passage for display, truncating long lines.\"\"\"",
        "    lines = passage.split('\\n')"
      ],
      "change_type": "add"
    },
    {
      "file": "scripts/search_codebase.py",
      "function": "def search_codebase(",
      "start_line": 217,
      "lines_added": [
        "        doc_results = processor.find_documents_for_query(query, top_n=top_n * 3)",
        "            top_n=top_n * 3,  # Get more candidates for re-ranking",
        "    # Apply definition boost to documents (helps find class/def definitions)",
        "    doc_results = boost_definition_documents(",
        "        doc_results, query, processor.documents, boost_factor=2.0",
        "    )",
        "",
        "",
        "    # Check if this is a definition query - if so, fetch more candidates",
        "    definition_info = detect_definition_query(query)",
        "    candidate_multiplier = 5 if definition_info['is_definition_query'] else 2",
        "",
        "    # For definition queries, search fewer docs (boosted ones at top)",
        "    # For regular queries, search more broadly",
        "    doc_limit = top_n if definition_info['is_definition_query'] else top_n * 3",
        "",
        "        top_n=top_n * candidate_multiplier,  # More candidates for definition queries",
        "        doc_filter=doc_ids[:doc_limit] if doc_ids else None",
        "    # Apply definition boost for \"class X\" or \"def X\" queries",
        "    # This helps find actual definitions instead of just usages",
        "    results = apply_definition_boost(results, query, boost_factor=5.0)",
        "    results = results[:top_n]  # Trim after boosting",
        ""
      ],
      "lines_removed": [
        "        doc_results = processor.find_documents_for_query(query, top_n=top_n * 2)",
        "            top_n=top_n * 2,  # Get more candidates",
        "        top_n=top_n,",
        "        doc_filter=doc_ids[:top_n * 2] if doc_ids else None"
      ],
      "context_before": [
        "                'line': 1,",
        "                'passage': passage,",
        "                'score': score,",
        "                'reference': f\"{doc_id}:1\",",
        "                'doc_type': get_doc_type_label(doc_id)",
        "            })",
        "        return formatted_results",
        "",
        "    # Full passage search - first get top documents with boosting",
        "    if no_boost:"
      ],
      "context_after": [
        "    else:",
        "        doc_results = processor.find_documents_with_boost(",
        "            query,",
        "            auto_detect_intent=not prefer_docs,",
        "            prefer_docs=prefer_docs",
        "        )",
        "",
        "    # Then get passages from those documents",
        "    doc_ids = [doc_id for doc_id, _ in doc_results]",
        "    results = processor.find_passages_for_query(",
        "        query,",
        "        chunk_size=chunk_size,",
        "        overlap=100,",
        "    )",
        "",
        "    formatted_results = []",
        "    for passage, doc_id, start, end, score in results:",
        "        # Get the full document content to find line number",
        "        doc_content = processor.documents.get(doc_id, '')",
        "        line_num = find_line_number(doc_content, start)",
        "",
        "        formatted_results.append({",
        "            'file': doc_id,",
        "            'line': line_num,",
        "            'passage': passage,"
      ],
      "change_type": "modify"
    },
    {
      "file": "tests/test_tokenizer.py",
      "function": "class TestCodeAwareTokenization(unittest.TestCase):",
      "start_line": 205,
      "lines_added": [
        "    def test_dunder_methods_tokenized(self):",
        "        \"\"\"Test that Python dunder methods are tokenized correctly.\"\"\"",
        "        tokenizer = Tokenizer()",
        "        # Dunder methods should be captured",
        "        tokens = tokenizer.tokenize(\"def __init__(self): pass\")",
        "        self.assertIn(\"__init__\", tokens)",
        "        self.assertIn(\"self\", tokens)",
        "",
        "        tokens = tokenizer.tokenize(\"__slots__ = ['x', 'y']\")",
        "        self.assertIn(\"__slots__\", tokens)",
        "",
        "        tokens = tokenizer.tokenize(\"def __str__(self): return ''\")",
        "        self.assertIn(\"__str__\", tokens)",
        "",
        "    def test_private_variables_tokenized(self):",
        "        \"\"\"Test that private variables (underscore-prefixed) are tokenized.\"\"\"",
        "        tokenizer = Tokenizer()",
        "        tokens = tokenizer.tokenize(\"self._id_index = {}\")",
        "        self.assertIn(\"self\", tokens)",
        "        self.assertIn(\"_id_index\", tokens)",
        "",
        "        tokens = tokenizer.tokenize(\"_private_cache = []\")",
        "        self.assertIn(\"_private_cache\", tokens)",
        "",
        "    def test_dunder_methods_with_split(self):",
        "        \"\"\"Test dunder methods with identifier splitting.\"\"\"",
        "        tokenizer = Tokenizer(split_identifiers=True)",
        "        tokens = tokenizer.tokenize(\"__init__\")",
        "        self.assertIn(\"__init__\", tokens)",
        "        self.assertIn(\"init\", tokens)",
        "",
        "        tokens = tokenizer.tokenize(\"__slots__\")",
        "        self.assertIn(\"__slots__\", tokens)",
        "        self.assertIn(\"slots\", tokens)",
        "",
        "    def test_private_vars_with_split(self):",
        "        \"\"\"Test private variables with identifier splitting.\"\"\"",
        "        tokenizer = Tokenizer(split_identifiers=True)",
        "        tokens = tokenizer.tokenize(\"_id_index\")",
        "        self.assertIn(\"_id_index\", tokens)",
        "        self.assertIn(\"index\", tokens)",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "        self.assertIn(\"data\", tokens)",
        "",
        "    def test_min_length_applied_to_splits(self):",
        "        \"\"\"Test that min_word_length applies to split parts.\"\"\"",
        "        tokenizer = Tokenizer(split_identifiers=True, min_word_length=4)",
        "        tokens = tokenizer.tokenize(\"getUserID\")",
        "        # 'id' is too short (length 2)",
        "        self.assertNotIn(\"id\", tokens)",
        "        self.assertIn(\"user\", tokens)",
        ""
      ],
      "context_after": [
        "",
        "if __name__ == \"__main__\":",
        "    unittest.main(verbosity=2)"
      ],
      "change_type": "add"
    }
  ],
  "hour_of_day": 3,
  "day_of_week": "Thursday",
  "seconds_since_last_commit": -383984,
  "is_merge": true,
  "is_initial": false,
  "parent_count": 2,
  "session_id": null,
  "related_chats": [],
  "ci_result": null,
  "reverted": false,
  "amended": false
}