{
  "hash": "b4248094e67738be866fe5703c68efc918a6ef8a",
  "message": "Merge main: test coverage improvements",
  "author": "Claude",
  "timestamp": "2025-12-12 00:48:41 +0000",
  "branch": "claude/multi-index-design-DvifZ",
  "files_changed": [
    "tests/test_coverage_gaps.py"
  ],
  "insertions": 756,
  "deletions": 0,
  "hunks": [
    {
      "file": "tests/test_coverage_gaps.py",
      "function": "class TestLayerSerialization(unittest.TestCase):",
      "start_line": 741,
      "lines_added": [
        "class TestSemanticsRetrofitCoverage(unittest.TestCase):",
        "    \"\"\"Test retrofit functions in semantics module for coverage.\"\"\"",
        "",
        "    def test_retrofit_connections_invalid_alpha_too_high(self):",
        "        \"\"\"Test retrofit_connections raises ValueError when alpha > 1.\"\"\"",
        "        from cortical.semantics import retrofit_connections",
        "",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(\"doc1\", \"test content here\")",
        "        processor.compute_all(verbose=False)",
        "",
        "        with self.assertRaises(ValueError) as ctx:",
        "            retrofit_connections(processor.layers, [], alpha=1.5)",
        "        self.assertIn(\"between 0 and 1\", str(ctx.exception))",
        "",
        "    def test_retrofit_connections_invalid_alpha_negative(self):",
        "        \"\"\"Test retrofit_connections raises ValueError when alpha < 0.\"\"\"",
        "        from cortical.semantics import retrofit_connections",
        "",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(\"doc1\", \"test content here\")",
        "        processor.compute_all(verbose=False)",
        "",
        "        with self.assertRaises(ValueError) as ctx:",
        "            retrofit_connections(processor.layers, [], alpha=-0.1)",
        "        self.assertIn(\"between 0 and 1\", str(ctx.exception))",
        "",
        "    def test_retrofit_embeddings_invalid_alpha_zero(self):",
        "        \"\"\"Test retrofit_embeddings raises ValueError when alpha <= 0.\"\"\"",
        "        from cortical.semantics import retrofit_embeddings",
        "",
        "        embeddings = {\"word1\": [0.1, 0.2], \"word2\": [0.3, 0.4]}",
        "",
        "        with self.assertRaises(ValueError) as ctx:",
        "            retrofit_embeddings(embeddings, [], alpha=0.0)",
        "        self.assertIn(\"exclusive of 0\", str(ctx.exception))",
        "",
        "    def test_retrofit_embeddings_invalid_alpha_too_high(self):",
        "        \"\"\"Test retrofit_embeddings raises ValueError when alpha > 1.\"\"\"",
        "        from cortical.semantics import retrofit_embeddings",
        "",
        "        embeddings = {\"word1\": [0.1, 0.2], \"word2\": [0.3, 0.4]}",
        "",
        "        with self.assertRaises(ValueError) as ctx:",
        "            retrofit_embeddings(embeddings, [], alpha=1.5)",
        "        self.assertIn(\"between 0 and 1\", str(ctx.exception))",
        "",
        "    def test_retrofit_embeddings_term_with_no_neighbors(self):",
        "        \"\"\"Test retrofit_embeddings skips terms with no semantic neighbors.\"\"\"",
        "        from cortical.semantics import retrofit_embeddings",
        "",
        "        embeddings = {",
        "            \"isolated\": [0.1, 0.2],",
        "            \"connected1\": [0.3, 0.4],",
        "            \"connected2\": [0.5, 0.6]",
        "        }",
        "        # Only connected1 and connected2 are related",
        "        relations = [(\"connected1\", \"RelatedTo\", \"connected2\", 0.8)]",
        "",
        "        result = retrofit_embeddings(embeddings, relations, alpha=0.5)",
        "        # Should return stats dict",
        "        self.assertIsInstance(result, dict)",
        "",
        "    def test_property_similarity_no_properties(self):",
        "        \"\"\"Test compute_property_similarity returns 0 when no properties.\"\"\"",
        "        from cortical.semantics import compute_property_similarity",
        "",
        "        # Empty inheritance dictionaries",
        "        inherited = {}",
        "        direct_props = {\"other_term\": {\"some_prop\": 0.5}}",
        "",
        "        result = compute_property_similarity(",
        "            \"unknown1\", \"unknown2\", inherited, direct_props",
        "        )",
        "        self.assertEqual(result, 0.0)",
        "",
        "    def test_property_similarity_with_direct_properties(self):",
        "        \"\"\"Test compute_property_similarity includes direct properties.\"\"\"",
        "        from cortical.semantics import compute_property_similarity",
        "",
        "        inherited = {\"dog\": {\"living\": (0.7, \"animal\", 1)}}",
        "        direct_props = {",
        "            \"dog\": {\"furry\": 0.9},",
        "            \"cat\": {\"furry\": 0.8, \"meowing\": 0.7}",
        "        }",
        "",
        "        result = compute_property_similarity(\"dog\", \"cat\", inherited, direct_props)",
        "        # Both share \"furry\" property",
        "        self.assertGreater(result, 0.0)",
        "",
        "    def test_extract_semantics_max_pairs_limit(self):",
        "        \"\"\"Test that max_similarity_pairs limits the number of pairs checked.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        # Create many terms with shared context",
        "        for i in range(5):",
        "            processor.process_document(f\"doc{i}\", f\"\"\"",
        "                common shared vocabulary words term{i}",
        "                another common context overlap term{i}",
        "                more common terms context vectors term{i}",
        "            \"\"\")",
        "        processor.compute_all(verbose=False)",
        "",
        "        from cortical.semantics import extract_corpus_semantics",
        "        relations = extract_corpus_semantics(",
        "            processor.layers,",
        "            processor.documents,",
        "            processor.tokenizer,",
        "            max_similarity_pairs=5,  # Very low limit",
        "            use_pattern_extraction=False",
        "        )",
        "        self.assertIsInstance(relations, list)",
        "",
        "",
        "class TestProcessorVerboseCoverage(unittest.TestCase):",
        "    \"\"\"Test verbose output paths in processor.\"\"\"",
        "",
        "    def test_add_documents_batch_verbose(self):",
        "        \"\"\"Test verbose output during batch document processing.\"\"\"",
        "        import io",
        "        import sys",
        "",
        "        processor = CorticalTextProcessor()",
        "        docs = [",
        "            (\"doc1\", \"first document content\", None),",
        "            (\"doc2\", \"second document content\", None),",
        "        ]",
        "",
        "        captured = io.StringIO()",
        "        old_stdout = sys.stdout",
        "        sys.stdout = captured",
        "        try:",
        "            processor.add_documents_batch(docs, verbose=True, recompute='full')",
        "        finally:",
        "            sys.stdout = old_stdout",
        "",
        "        output = captured.getvalue()",
        "        self.assertIn(\"Adding\", output)",
        "",
        "    def test_add_documents_batch_invalid_content(self):",
        "        \"\"\"Test that batch processing validates content type.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        docs = [(\"doc1\", 123, None)]  # Invalid: content is int, not str",
        "",
        "        with self.assertRaises(ValueError) as ctx:",
        "            processor.add_documents_batch(docs)",
        "        self.assertIn(\"string\", str(ctx.exception).lower())",
        "",
        "    def test_compute_importance_verbose(self):",
        "        \"\"\"Test compute_importance with verbose output.\"\"\"",
        "        import io",
        "        import sys",
        "",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(\"doc1\", \"neural network deep learning\")",
        "        processor.process_document(\"doc2\", \"machine learning models\")",
        "        processor.propagate_activation(iterations=3, verbose=False)",
        "",
        "        captured = io.StringIO()",
        "        old_stdout = sys.stdout",
        "        sys.stdout = captured",
        "        try:",
        "            processor.compute_importance(verbose=True)",
        "        finally:",
        "            sys.stdout = old_stdout",
        "",
        "        output = captured.getvalue()",
        "        self.assertIn(\"PageRank\", output)",
        "",
        "    def test_compute_semantic_importance_verbose(self):",
        "        \"\"\"Test semantic importance with verbose output.\"\"\"",
        "        import io",
        "        import sys",
        "",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(\"doc1\", \"neural network deep learning\")",
        "        processor.propagate_activation(iterations=3, verbose=False)",
        "        processor.extract_corpus_semantics(verbose=False)",
        "",
        "        captured = io.StringIO()",
        "        old_stdout = sys.stdout",
        "        sys.stdout = captured",
        "        try:",
        "            processor.compute_semantic_importance(verbose=True)",
        "        finally:",
        "            sys.stdout = old_stdout",
        "",
        "        output = captured.getvalue()",
        "        self.assertTrue(len(output) > 0)",
        "",
        "    def test_build_concept_clusters_label_propagation(self):",
        "        \"\"\"Test label propagation clustering method.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(\"doc1\", \"neural network deep learning models\")",
        "        processor.process_document(\"doc2\", \"machine learning algorithms data\")",
        "        processor.compute_all(build_concepts=False, verbose=False)",
        "",
        "        clusters = processor.build_concept_clusters(",
        "            clustering_method='label_propagation',",
        "            verbose=False",
        "        )",
        "        # Returns a dict of cluster_id -> list of terms",
        "        self.assertIsInstance(clusters, dict)",
        "",
        "    def test_extract_corpus_semantics_verbose(self):",
        "        \"\"\"Test verbose output during semantic extraction.\"\"\"",
        "        import io",
        "        import sys",
        "",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(\"doc1\", \"neural networks learn patterns\")",
        "        processor.process_document(\"doc2\", \"deep learning neural models\")",
        "        processor.propagate_activation(iterations=3, verbose=False)",
        "",
        "        captured = io.StringIO()",
        "        old_stdout = sys.stdout",
        "        sys.stdout = captured",
        "        try:",
        "            processor.extract_corpus_semantics(verbose=True)",
        "        finally:",
        "            sys.stdout = old_stdout",
        "",
        "        output = captured.getvalue()",
        "        self.assertIn(\"Extracted\", output)",
        "",
        "",
        "class TestProcessorWrapperMethods(unittest.TestCase):",
        "    \"\"\"Test wrapper methods in processor for coverage.\"\"\"",
        "",
        "    def test_find_related_documents(self):",
        "        \"\"\"Test find_related_documents wrapper.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(\"doc1\", \"neural network deep learning\")",
        "        processor.process_document(\"doc2\", \"neural network machine learning\")",
        "        processor.process_document(\"doc3\", \"cooking recipes baking\")",
        "        processor.compute_all(verbose=False)",
        "",
        "        results = processor.find_related_documents(\"doc1\")",
        "        self.assertIsInstance(results, list)",
        "",
        "    def test_get_document_signature(self):",
        "        \"\"\"Test get_document_signature wrapper.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(\"doc1\", \"neural network deep learning models\")",
        "        processor.compute_tfidf(verbose=False)",
        "",
        "        signature = processor.get_document_signature(\"doc1\", n=5)",
        "        self.assertIsInstance(signature, list)",
        "        if signature:",
        "            self.assertIsInstance(signature[0], tuple)",
        "            self.assertEqual(len(signature[0]), 2)",
        "",
        "    def test_get_document_signature_nonexistent(self):",
        "        \"\"\"Test get_document_signature with nonexistent doc.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(\"doc1\", \"test\")",
        "",
        "        signature = processor.get_document_signature(\"nonexistent\")",
        "        self.assertEqual(signature, [])",
        "",
        "    def test_get_corpus_summary(self):",
        "        \"\"\"Test get_corpus_summary wrapper.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(\"doc1\", \"neural network\")",
        "        processor.compute_all(verbose=False)",
        "",
        "        summary = processor.get_corpus_summary()",
        "        self.assertIsInstance(summary, dict)",
        "",
        "    def test_embedding_similarity(self):",
        "        \"\"\"Test embedding_similarity wrapper.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(\"doc1\", \"neural network learning\")",
        "        processor.compute_graph_embeddings(verbose=False)",
        "",
        "        score = processor.embedding_similarity(\"neural\", \"network\")",
        "        self.assertIsInstance(score, float)",
        "",
        "    def test_find_similar_by_embedding(self):",
        "        \"\"\"Test find_similar_by_embedding wrapper.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(\"doc1\", \"neural network deep learning models\")",
        "        processor.compute_graph_embeddings(verbose=False)",
        "",
        "        results = processor.find_similar_by_embedding(\"neural\", top_n=5)",
        "        self.assertIsInstance(results, list)",
        "",
        "    def test_expand_query_semantic(self):",
        "        \"\"\"Test expand_query_semantic wrapper.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(\"doc1\", \"neural network deep learning\")",
        "        processor.extract_corpus_semantics(verbose=False)",
        "",
        "        results = processor.expand_query_semantic(\"neural\", max_expansions=5)",
        "        self.assertIsInstance(results, dict)",
        "",
        "    def test_set_query_cache_size_trim(self):",
        "        \"\"\"Test that setting smaller cache size trims existing cache.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(\"doc1\", \"neural network deep learning models\")",
        "        processor.compute_all(verbose=False)",
        "",
        "        # Fill cache with queries",
        "        processor.expand_query(\"query1\")",
        "        processor.expand_query(\"query2\")",
        "        processor.expand_query(\"query3\")",
        "",
        "        # Trim cache",
        "        processor.set_query_cache_size(1)",
        "        # Cache should be trimmed (can't directly check size but no error)",
        "",
        "    def test_export_graph_wrapper(self):",
        "        \"\"\"Test export_graph wrapper method.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(\"doc1\", \"neural network\")",
        "        processor.compute_all(verbose=False)",
        "",
        "        with tempfile.NamedTemporaryFile(suffix='.json', delete=False) as f:",
        "            temp_path = f.name",
        "",
        "        try:",
        "            stats = processor.export_graph(temp_path, max_nodes=100)",
        "            self.assertIsInstance(stats, dict)",
        "            self.assertTrue(os.path.exists(temp_path))",
        "        finally:",
        "            os.unlink(temp_path)",
        "",
        "",
        "class TestProcessorDocumentOperations(unittest.TestCase):",
        "    \"\"\"Test document operations in processor.\"\"\"",
        "",
        "    def test_set_document_metadata_new_doc(self):",
        "        \"\"\"Test setting metadata for new document.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        processor.set_document_metadata(\"new_doc\", key=\"value\", type=\"test\")",
        "",
        "        meta = processor.get_document_metadata(\"new_doc\")",
        "        self.assertEqual(meta[\"key\"], \"value\")",
        "        self.assertEqual(meta[\"type\"], \"test\")",
        "",
        "    def test_remove_document_with_bigrams(self):",
        "        \"\"\"Test that removing document updates bigram occurrence counts.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(\"doc1\", \"neural network test\")",
        "        processor.compute_bigram_connections(verbose=False)",
        "",
        "        # Check bigrams exist",
        "        layer1 = processor.layers[CorticalLayer.BIGRAMS]",
        "        initial_count = layer1.column_count()",
        "",
        "        # Remove document",
        "        processor.remove_document(\"doc1\")",
        "",
        "        # Bigrams should be affected",
        "        self.assertLessEqual(layer1.column_count(), initial_count)",
        "",
        "    def test_summarize_document(self):",
        "        \"\"\"Test document summarization.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        text = \"First sentence about neural networks. Second sentence about deep learning. Third sentence about machine learning. Fourth sentence about data science.\"",
        "        processor.process_document(\"doc1\", text)",
        "        processor.compute_tfidf(verbose=False)",
        "",
        "        summary = processor.summarize_document(\"doc1\", num_sentences=2)",
        "        self.assertIsInstance(summary, str)",
        "",
        "    def test_summarize_document_single_sentence(self):",
        "        \"\"\"Test summarization with fewer sentences than requested.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(\"doc1\", \"Only one sentence here.\")",
        "",
        "        summary = processor.summarize_document(\"doc1\", num_sentences=5)",
        "        self.assertIn(\"Only one sentence\", summary)",
        "",
        "    def test_summarize_document_nonexistent(self):",
        "        \"\"\"Test summarization of nonexistent document.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        summary = processor.summarize_document(\"nonexistent\", num_sentences=1)",
        "        self.assertEqual(summary, \"\")",
        "",
        "    def test_compute_property_similarity_no_relations(self):",
        "        \"\"\"Test property similarity with no semantic relations.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(\"doc1\", \"test content\")",
        "        processor.compute_all(verbose=False)",
        "        # Don't extract semantics",
        "",
        "        result = processor.compute_property_similarity(\"test\", \"content\")",
        "        self.assertEqual(result, 0.0)",
        "",
        "",
        "class TestChunkIndexLazyLoading(unittest.TestCase):",
        "    \"\"\"Test lazy loading paths in chunk index.\"\"\"",
        "",
        "    def test_get_documents_lazy_loading(self):",
        "        \"\"\"Test that get_documents triggers load_all when needed.\"\"\"",
        "        from cortical.chunk_index import ChunkWriter, ChunkLoader",
        "",
        "        with tempfile.TemporaryDirectory() as tmpdir:",
        "            writer = ChunkWriter(tmpdir)",
        "            writer.add_document(\"doc1\", \"test content\", mtime=12345.0)",
        "            writer.save()",
        "",
        "            loader = ChunkLoader(tmpdir)",
        "            # Call get_documents without calling load_all first",
        "            docs = loader.get_documents()",
        "            self.assertIn(\"doc1\", docs)",
        "",
        "    def test_get_metadata_lazy_loading(self):",
        "        \"\"\"Test that get_metadata triggers load_all when needed.\"\"\"",
        "        from cortical.chunk_index import ChunkWriter, ChunkLoader",
        "",
        "        with tempfile.TemporaryDirectory() as tmpdir:",
        "            writer = ChunkWriter(tmpdir)",
        "            writer.add_document(\"doc1\", \"test\", mtime=1000.0, metadata={\"type\": \"test\"})",
        "            writer.save()",
        "",
        "            loader = ChunkLoader(tmpdir)",
        "            # Call get_metadata without calling load_all first",
        "            meta = loader.get_metadata()",
        "            self.assertIn(\"doc1\", meta)",
        "",
        "    def test_get_chunks_lazy_loading(self):",
        "        \"\"\"Test that get_chunks triggers load_all when needed.\"\"\"",
        "        from cortical.chunk_index import ChunkWriter, ChunkLoader",
        "",
        "        with tempfile.TemporaryDirectory() as tmpdir:",
        "            writer = ChunkWriter(tmpdir)",
        "            writer.add_document(\"doc1\", \"test\", mtime=1000.0)",
        "            writer.save()",
        "",
        "            loader = ChunkLoader(tmpdir)",
        "            # Call get_chunks without calling load_all first",
        "            chunks = loader.get_chunks()",
        "            self.assertIsInstance(chunks, list)",
        "            self.assertGreater(len(chunks), 0)",
        "",
        "    def test_cache_validity_missing_cache(self):",
        "        \"\"\"Test is_cache_valid returns False when cache file missing.\"\"\"",
        "        from cortical.chunk_index import ChunkLoader",
        "",
        "        with tempfile.TemporaryDirectory() as tmpdir:",
        "            loader = ChunkLoader(tmpdir)",
        "            result = loader.is_cache_valid(\"/nonexistent/path/cache.pkl\")",
        "            self.assertFalse(result)",
        "",
        "    def test_cache_validity_missing_hash(self):",
        "        \"\"\"Test is_cache_valid returns False when hash file missing.\"\"\"",
        "        from cortical.chunk_index import ChunkLoader",
        "",
        "        with tempfile.TemporaryDirectory() as tmpdir:",
        "            with tempfile.NamedTemporaryFile(suffix='.pkl', delete=False) as f:",
        "                cache_path = f.name",
        "                f.write(b\"test data\")",
        "",
        "            try:",
        "                loader = ChunkLoader(tmpdir)",
        "                # Cache exists but hash file doesn't",
        "                result = loader.is_cache_valid(cache_path)",
        "                self.assertFalse(result)",
        "            finally:",
        "                os.unlink(cache_path)",
        "",
        "    def test_load_all_caching(self):",
        "        \"\"\"Test that load_all caches results on second call.\"\"\"",
        "        from cortical.chunk_index import ChunkWriter, ChunkLoader",
        "",
        "        with tempfile.TemporaryDirectory() as tmpdir:",
        "            writer = ChunkWriter(tmpdir)",
        "            writer.add_document(\"doc1\", \"test content\", mtime=1000.0)",
        "            writer.save()",
        "",
        "            loader = ChunkLoader(tmpdir)",
        "            docs1 = loader.load_all()",
        "            docs2 = loader.load_all()  # Should use cache",
        "",
        "            self.assertEqual(docs1, docs2)",
        "",
        "",
        "class TestChunkIndexCompaction(unittest.TestCase):",
        "    \"\"\"Test compaction in chunk index.\"\"\"",
        "",
        "    def test_compact_multiple_chunks(self):",
        "        \"\"\"Test compacting multiple chunks into one.\"\"\"",
        "        from cortical.chunk_index import ChunkWriter, ChunkLoader, ChunkCompactor",
        "",
        "        with tempfile.TemporaryDirectory() as tmpdir:",
        "            # First chunk: add 2 documents",
        "            writer1 = ChunkWriter(tmpdir)",
        "            writer1.add_document(\"doc1\", \"content1\", mtime=1000.0)",
        "            writer1.add_document(\"doc2\", \"content2\", mtime=1001.0)",
        "            writer1.save()",
        "",
        "            # Second chunk: add 1 more document",
        "            writer2 = ChunkWriter(tmpdir)",
        "            writer2.add_document(\"doc3\", \"content3\", mtime=1002.0)",
        "            writer2.save()",
        "",
        "            # Verify we have 2 chunk files before compaction",
        "            loader = ChunkLoader(tmpdir)",
        "            chunk_files_before = loader.get_chunk_files()",
        "            self.assertGreaterEqual(len(chunk_files_before), 2)",
        "",
        "            # Compact all chunks",
        "            compactor = ChunkCompactor(tmpdir)",
        "            result = compactor.compact()",
        "            self.assertIn('status', result)",
        "",
        "            # Load and verify all docs are preserved",
        "            loader2 = ChunkLoader(tmpdir)",
        "            docs = loader2.load_all()",
        "            self.assertEqual(len(docs), 3)",
        "            self.assertIn(\"doc1\", docs)",
        "            self.assertIn(\"doc2\", docs)",
        "            self.assertIn(\"doc3\", docs)",
        "",
        "",
        "class TestPersistenceTypedConnections(unittest.TestCase):",
        "    \"\"\"Test typed connections in persistence export.\"\"\"",
        "",
        "    def test_export_conceptnet_with_typed_edges(self):",
        "        \"\"\"Test export includes typed edges when requested.\"\"\"",
        "        from cortical.persistence import export_conceptnet_json",
        "",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(\"doc1\", \"neural network processing\")",
        "        processor.compute_all(verbose=False)",
        "",
        "        # Add typed connection",
        "        layer0 = processor.layers[CorticalLayer.TOKENS]",
        "        col1 = layer0.get_minicolumn(\"neural\")",
        "        col2 = layer0.get_minicolumn(\"network\")",
        "        if col1 and col2:",
        "            col1.add_typed_connection(",
        "                col2.id,",
        "                weight=0.9,",
        "                relation_type='RelatedTo',",
        "                confidence=0.95,",
        "                source='semantic'",
        "            )",
        "",
        "        with tempfile.NamedTemporaryFile(suffix='.json', delete=False) as f:",
        "            temp_path = f.name",
        "",
        "        try:",
        "            result = export_conceptnet_json(",
        "                temp_path,",
        "                processor.layers,",
        "                include_typed_edges=True,",
        "                min_weight=0.5,",
        "                min_confidence=0.8,",
        "                verbose=False",
        "            )",
        "            # Should have edges",
        "            self.assertIn('edges', result)",
        "        finally:",
        "            os.unlink(temp_path)",
        "",
        "    def test_export_conceptnet_filters_by_weight(self):",
        "        \"\"\"Test typed edges below min_weight are excluded.\"\"\"",
        "        from cortical.persistence import export_conceptnet_json",
        "",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(\"doc1\", \"neural network\")",
        "        processor.compute_all(verbose=False)",
        "",
        "        layer0 = processor.layers[CorticalLayer.TOKENS]",
        "        col1 = layer0.get_minicolumn(\"neural\")",
        "        col2 = layer0.get_minicolumn(\"network\")",
        "        if col1 and col2:",
        "            # Low weight connection",
        "            col1.add_typed_connection(col2.id, weight=0.3, relation_type='IsA', confidence=0.9)",
        "",
        "        with tempfile.NamedTemporaryFile(suffix='.json', delete=False) as f:",
        "            temp_path = f.name",
        "",
        "        try:",
        "            result = export_conceptnet_json(",
        "                temp_path,",
        "                processor.layers,",
        "                include_typed_edges=True,",
        "                min_weight=0.5,  # Higher than 0.3",
        "                verbose=False",
        "            )",
        "            # Low weight edge should be filtered out",
        "            typed_edges = [e for e in result.get('edges', []) if e.get('relation_type') == 'IsA']",
        "            self.assertEqual(len(typed_edges), 0)",
        "        finally:",
        "            os.unlink(temp_path)",
        "",
        "    def test_export_conceptnet_verbose(self):",
        "        \"\"\"Test verbose output in export.\"\"\"",
        "        import io",
        "        import sys",
        "        from cortical.persistence import export_conceptnet_json",
        "",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(\"doc1\", \"neural network\")",
        "        processor.compute_all(verbose=False)",
        "",
        "        with tempfile.NamedTemporaryFile(suffix='.json', delete=False) as f:",
        "            temp_path = f.name",
        "",
        "        captured = io.StringIO()",
        "        old_stdout = sys.stdout",
        "        sys.stdout = captured",
        "        try:",
        "            export_conceptnet_json(temp_path, processor.layers, verbose=True)",
        "        finally:",
        "            sys.stdout = old_stdout",
        "            os.unlink(temp_path)",
        "",
        "        output = captured.getvalue()",
        "        self.assertIn(\"exported\", output.lower())",
        "",
        "",
        "class TestProcessorConfigRestoration(unittest.TestCase):",
        "    \"\"\"Test config restoration during load.\"\"\"",
        "",
        "    def test_save_and_load_preserves_config(self):",
        "        \"\"\"Test that config is preserved through save/load.\"\"\"",
        "        from cortical.config import CorticalConfig",
        "",
        "        config = CorticalConfig(pagerank_damping=0.75, min_cluster_size=5)",
        "        processor = CorticalTextProcessor(config=config)",
        "        processor.process_document(\"doc1\", \"test content\")",
        "        processor.compute_all(verbose=False)",
        "",
        "        with tempfile.NamedTemporaryFile(suffix='.pkl', delete=False) as f:",
        "            temp_path = f.name",
        "",
        "        try:",
        "            processor.save(temp_path, verbose=False)",
        "            loaded = CorticalTextProcessor.load(temp_path, verbose=False)",
        "",
        "            self.assertEqual(loaded.config.pagerank_damping, 0.75)",
        "            self.assertEqual(loaded.config.min_cluster_size, 5)",
        "        finally:",
        "            os.unlink(temp_path)",
        "",
        "",
        "class TestProcessorRetrofitting(unittest.TestCase):",
        "    \"\"\"Test retrofitting methods in processor.\"\"\"",
        "",
        "    def test_retrofit_embeddings_auto_compute(self):",
        "        \"\"\"Test retrofit_embeddings auto-computes required data.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(\"doc1\", \"neural network deep learning\")",
        "        processor.process_document(\"doc2\", \"machine learning models\")",
        "        processor.compute_all(verbose=False)",
        "",
        "        # retrofit_embeddings should work even without pre-computed embeddings",
        "        stats = processor.retrofit_embeddings(verbose=False)",
        "        self.assertIsInstance(stats, dict)",
        "",
        "    def test_compute_property_inheritance_with_connections(self):",
        "        \"\"\"Test property inheritance with connection boosting.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(\"doc1\", \"dog is animal mammal living\")",
        "        processor.process_document(\"doc2\", \"cat is animal mammal living\")",
        "        processor.compute_all(verbose=False)",
        "        processor.extract_corpus_semantics(verbose=False)",
        "",
        "        result = processor.compute_property_inheritance(",
        "            apply_to_connections=True,",
        "            verbose=False",
        "        )",
        "        self.assertIn('terms_with_inheritance', result)",
        "",
        "",
        "class TestAnalysisCoverage(unittest.TestCase):",
        "    \"\"\"Additional analysis tests for coverage.\"\"\"",
        "",
        "    def test_compute_bigram_connections_verbose_limits(self):",
        "        \"\"\"Test verbose output when bigram limits are hit.\"\"\"",
        "        import io",
        "        import sys",
        "",
        "        processor = CorticalTextProcessor()",
        "        # Create document with common terms that will hit limits",
        "        processor.process_document(\"doc1\", \"the the the test test test word word word\")",
        "        processor.propagate_activation(iterations=1, verbose=False)",
        "",
        "        captured = io.StringIO()",
        "        old_stdout = sys.stdout",
        "        sys.stdout = captured",
        "        try:",
        "            processor.compute_bigram_connections(",
        "                verbose=True,",
        "                max_bigrams_per_term=2",
        "            )",
        "        finally:",
        "            sys.stdout = old_stdout",
        "",
        "        # Should have some output",
        "        output = captured.getvalue()",
        "        self.assertTrue(len(output) > 0)",
        "",
        "",
        "class TestSemanticInheritancePaths(unittest.TestCase):",
        "    \"\"\"Test inheritance paths in semantics module.\"\"\"",
        "",
        "    def test_apply_inheritance_missing_term(self):",
        "        \"\"\"Test apply_inheritance_to_connections skips missing terms.\"\"\"",
        "        from cortical.semantics import apply_inheritance_to_connections",
        "",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(\"doc1\", \"test content\")",
        "        processor.compute_all(verbose=False)",
        "",
        "        # Create inheritance with non-existent term",
        "        inherited = {\"orphan_term\": {\"prop\": (0.5, \"ancestor\", 1)}}",
        "",
        "        stats = apply_inheritance_to_connections(processor.layers, inherited)",
        "        self.assertEqual(stats['connections_boosted'], 0)",
        "",
        "    def test_get_ancestors_deep_hierarchy(self):",
        "        \"\"\"Test get_ancestors with deep hierarchy.\"\"\"",
        "        from cortical.semantics import get_ancestors",
        "",
        "        # Create linear hierarchy: a→b→c→d (parent relationships)",
        "        parents = {",
        "            \"a\": {\"b\"},",
        "            \"b\": {\"c\"},",
        "            \"c\": {\"d\"}",
        "        }",
        "",
        "        ancestors = get_ancestors(\"a\", parents, max_depth=10)",
        "        # get_ancestors returns {ancestor: depth} dict",
        "        self.assertIn(\"b\", ancestors)",
        "        self.assertIn(\"c\", ancestors)",
        "        self.assertIn(\"d\", ancestors)",
        "        # Verify depths",
        "        self.assertEqual(ancestors[\"b\"], 1)",
        "        self.assertEqual(ancestors[\"c\"], 2)",
        "        self.assertEqual(ancestors[\"d\"], 3)",
        "",
        "    def test_get_descendants_with_multiple_paths(self):",
        "        \"\"\"Test get_descendants handles multiple paths.\"\"\"",
        "        from cortical.semantics import get_descendants",
        "",
        "        # Create hierarchy with multiple children",
        "        children = {",
        "            \"animal\": {\"mammal\", \"bird\"},",
        "            \"mammal\": {\"dog\", \"cat\"},",
        "            \"bird\": {\"sparrow\"}",
        "        }",
        "",
        "        descendants = get_descendants(\"animal\", children, max_depth=2)",
        "        # get_descendants returns {descendant: depth} dict",
        "        self.assertIn(\"mammal\", descendants)",
        "        self.assertIn(\"bird\", descendants)",
        "        self.assertIn(\"dog\", descendants)",
        "        self.assertIn(\"cat\", descendants)",
        "        self.assertIn(\"sparrow\", descendants)",
        "",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "        col1 = layer.get_or_create_minicolumn(\"term1\")",
        "        col2 = layer.get_or_create_minicolumn(\"term2\")",
        "        col1.add_lateral_connection(col2.id, 0.5)",
        "",
        "        d = layer.to_dict()",
        "        self.assertEqual(d['level'], CorticalLayer.TOKENS.value)",
        "        self.assertIn('minicolumns', d)",
        "        self.assertEqual(len(d['minicolumns']), 2)",
        "",
        ""
      ],
      "context_after": [
        "if __name__ == \"__main__\":",
        "    unittest.main()"
      ],
      "change_type": "add"
    }
  ],
  "hour_of_day": 0,
  "day_of_week": "Friday",
  "seconds_since_last_commit": -305767,
  "is_merge": true,
  "is_initial": false,
  "parent_count": 2,
  "session_id": null,
  "related_chats": [],
  "ci_result": null,
  "reverted": false,
  "amended": false
}