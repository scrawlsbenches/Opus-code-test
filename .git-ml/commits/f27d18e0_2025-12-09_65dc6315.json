{
  "hash": "f27d18e0a2a4e586961e0177d85f46385edb97eb",
  "message": "Integrate semantic relations into retrieval (Task 11)",
  "author": "Claude",
  "timestamp": "2025-12-09 19:54:13 +0000",
  "branch": "claude/multi-index-design-DvifZ",
  "files_changed": [
    "cortical/processor.py",
    "cortical/query.py"
  ],
  "insertions": 78,
  "deletions": 14,
  "hunks": [
    {
      "file": "cortical/processor.py",
      "function": "class CorticalTextProcessor:",
      "start_line": 219,
      "lines_added": [
        "    def find_documents_for_query(",
        "        self,",
        "        query_text: str,",
        "        top_n: int = 5,",
        "        use_expansion: bool = True,",
        "        use_semantic: bool = True",
        "    ) -> List[Tuple[str, float]]:",
        "        \"\"\"",
        "        Find documents most relevant to a query.",
        "",
        "        Args:",
        "            query_text: Search query",
        "            top_n: Number of documents to return",
        "            use_expansion: Whether to expand query terms using lateral connections",
        "            use_semantic: Whether to use semantic relations for expansion (if available)",
        "",
        "        Returns:",
        "            List of (doc_id, score) tuples ranked by relevance",
        "        \"\"\"",
        "        return query_module.find_documents_for_query(",
        "            query_text,",
        "            self.layers,",
        "            self.tokenizer,",
        "            top_n=top_n,",
        "            use_expansion=use_expansion,",
        "            semantic_relations=self.semantic_relations if use_semantic else None,",
        "            use_semantic=use_semantic",
        "        )",
        "        doc_filter: Optional[List[str]] = None,",
        "        use_semantic: bool = True",
        "            use_semantic: Whether to use semantic relations for expansion (if available)",
        "            doc_filter=doc_filter,",
        "            semantic_relations=self.semantic_relations if use_semantic else None,",
        "            use_semantic=use_semantic"
      ],
      "lines_removed": [
        "    def find_documents_for_query(self, query_text: str, top_n: int = 5, use_expansion: bool = True) -> List[Tuple[str, float]]:",
        "        return query_module.find_documents_for_query(query_text, self.layers, self.tokenizer, top_n, use_expansion)",
        "        doc_filter: Optional[List[str]] = None",
        "            doc_filter=doc_filter"
      ],
      "context_before": [
        "    ",
        "    def find_similar_by_embedding(self, term: str, top_n: int = 10) -> List[Tuple[str, float]]:",
        "        return emb_module.find_similar_by_embedding(self.embeddings, term, top_n)",
        "    ",
        "    def expand_query(self, query_text: str, max_expansions: int = 10, use_variants: bool = True, verbose: bool = False) -> Dict[str, float]:",
        "        return query_module.expand_query(query_text, self.layers, self.tokenizer, max_expansions=max_expansions, use_variants=use_variants)",
        "    ",
        "    def expand_query_semantic(self, query_text: str, max_expansions: int = 10) -> Dict[str, float]:",
        "        return query_module.expand_query_semantic(query_text, self.layers, self.tokenizer, self.semantic_relations, max_expansions)",
        "    "
      ],
      "context_after": [
        "",
        "    def find_passages_for_query(",
        "        self,",
        "        query_text: str,",
        "        top_n: int = 5,",
        "        chunk_size: int = 512,",
        "        overlap: int = 128,",
        "        use_expansion: bool = True,",
        "    ) -> List[Tuple[str, str, int, int, float]]:",
        "        \"\"\"",
        "        Find text passages most relevant to a query (for RAG systems).",
        "",
        "        Instead of returning just document IDs, this returns actual text passages",
        "        with position information suitable for context windows and citations.",
        "",
        "        Args:",
        "            query_text: Search query",
        "            top_n: Number of passages to return",
        "            chunk_size: Size of each chunk in characters (default 512)",
        "            overlap: Overlap between chunks in characters (default 128)",
        "            use_expansion: Whether to expand query terms",
        "            doc_filter: Optional list of doc_ids to restrict search to",
        "",
        "        Returns:",
        "            List of (passage_text, doc_id, start_char, end_char, score) tuples",
        "            ranked by relevance",
        "",
        "        Example:",
        "            >>> results = processor.find_passages_for_query(\"neural networks\")",
        "            >>> for passage, doc_id, start, end, score in results:",
        "            ...     print(f\"[{doc_id}:{start}-{end}] {passage[:50]}... (score: {score:.3f})\")",
        "        \"\"\"",
        "        return query_module.find_passages_for_query(",
        "            query_text,",
        "            self.layers,",
        "            self.tokenizer,",
        "            self.documents,",
        "            top_n=top_n,",
        "            chunk_size=chunk_size,",
        "            overlap=overlap,",
        "            use_expansion=use_expansion,",
        "        )",
        "    ",
        "    def query_expanded(self, query_text: str, top_n: int = 10, max_expansions: int = 8) -> List[Tuple[str, float]]:",
        "        return query_module.query_with_spreading_activation(query_text, self.layers, self.tokenizer, top_n, max_expansions)",
        "    ",
        "    def find_related_documents(self, doc_id: str) -> List[Tuple[str, float]]:",
        "        return query_module.find_related_documents(doc_id, self.layers)",
        "    ",
        "    def analyze_knowledge_gaps(self) -> Dict:",
        "        return gaps_module.analyze_knowledge_gaps(self.layers, self.documents)"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/query.py",
      "function": "def expand_query_semantic(",
      "start_line": 172,
      "lines_added": [
        "    use_expansion: bool = True,",
        "    semantic_relations: Optional[List[Tuple[str, str, str, float]]] = None,",
        "    use_semantic: bool = True",
        "",
        "        use_expansion: Whether to expand query terms using lateral connections",
        "        semantic_relations: Optional list of semantic relations for expansion",
        "        use_semantic: Whether to use semantic relations for expansion (if available)",
        "",
        "",
        "        # Start with lateral connection expansion",
        "",
        "        # Add semantic expansion if available",
        "        if use_semantic and semantic_relations:",
        "            semantic_terms = expand_query_semantic(",
        "                query_text, layers, tokenizer, semantic_relations, max_expansions=5",
        "            )",
        "            # Merge semantic expansions (don't override stronger weights)",
        "            for term, weight in semantic_terms.items():",
        "                if term not in query_terms:",
        "                    query_terms[term] = weight * 0.8  # Slightly discount semantic expansions",
        "                else:",
        "                    # Take the max weight",
        "                    query_terms[term] = max(query_terms[term], weight * 0.8)",
        "",
        "",
        ""
      ],
      "lines_removed": [
        "    use_expansion: bool = True",
        "    ",
        "        use_expansion: Whether to expand query terms",
        "        ",
        "    ",
        "    ",
        "    ",
        "    "
      ],
      "context_before": [
        "        expanded[term] = score",
        "    ",
        "    return expanded",
        "",
        "",
        "def find_documents_for_query(",
        "    query_text: str,",
        "    layers: Dict[CorticalLayer, HierarchicalLayer],",
        "    tokenizer: Tokenizer,",
        "    top_n: int = 5,"
      ],
      "context_after": [
        ") -> List[Tuple[str, float]]:",
        "    \"\"\"",
        "    Find documents most relevant to a query using TF-IDF and optional expansion.",
        "    Args:",
        "        query_text: Search query",
        "        layers: Dictionary of layers",
        "        tokenizer: Tokenizer instance",
        "        top_n: Number of documents to return",
        "    Returns:",
        "        List of (doc_id, score) tuples ranked by relevance",
        "    \"\"\"",
        "    layer0 = layers[CorticalLayer.TOKENS]",
        "    if use_expansion:",
        "        query_terms = expand_query(query_text, layers, tokenizer, max_expansions=5)",
        "    else:",
        "        tokens = tokenizer.tokenize(query_text)",
        "        query_terms = {t: 1.0 for t in tokens}",
        "    # Score each document",
        "    doc_scores: Dict[str, float] = defaultdict(float)",
        "    for term, term_weight in query_terms.items():",
        "        col = layer0.get_minicolumn(term)",
        "        if col:",
        "            for doc_id in col.document_ids:",
        "                tfidf = col.tfidf_per_doc.get(doc_id, col.tfidf)",
        "                doc_scores[doc_id] += tfidf * term_weight",
        "    sorted_docs = sorted(doc_scores.items(), key=lambda x: -x[1])",
        "    return sorted_docs[:top_n]",
        "",
        "",
        "def query_with_spreading_activation(",
        "    query_text: str,",
        "    layers: Dict[CorticalLayer, HierarchicalLayer],",
        "    tokenizer: Tokenizer,",
        "    top_n: int = 10,",
        "    max_expansions: int = 8"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/query.py",
      "function": "def score_chunk(",
      "start_line": 380,
      "lines_added": [
        "    doc_filter: Optional[List[str]] = None,",
        "    semantic_relations: Optional[List[Tuple[str, str, str, float]]] = None,",
        "    use_semantic: bool = True",
        "        semantic_relations: Optional list of semantic relations for expansion",
        "        use_semantic: Whether to use semantic relations for expansion (if available)",
        "        # Add semantic expansion if available",
        "        if use_semantic and semantic_relations:",
        "            semantic_terms = expand_query_semantic(",
        "                query_text, layers, tokenizer, semantic_relations, max_expansions=5",
        "            )",
        "            for term, weight in semantic_terms.items():",
        "                if term not in query_terms:",
        "                    query_terms[term] = weight * 0.8",
        "                else:",
        "                    query_terms[term] = max(query_terms[term], weight * 0.8)",
        "        use_expansion=use_expansion,",
        "        semantic_relations=semantic_relations,",
        "        use_semantic=use_semantic"
      ],
      "lines_removed": [
        "    doc_filter: Optional[List[str]] = None",
        "        use_expansion=use_expansion"
      ],
      "context_before": [
        "",
        "def find_passages_for_query(",
        "    query_text: str,",
        "    layers: Dict[CorticalLayer, HierarchicalLayer],",
        "    tokenizer: Tokenizer,",
        "    documents: Dict[str, str],",
        "    top_n: int = 5,",
        "    chunk_size: int = 512,",
        "    overlap: int = 128,",
        "    use_expansion: bool = True,"
      ],
      "context_after": [
        ") -> List[Tuple[str, str, int, int, float]]:",
        "    \"\"\"",
        "    Find text passages most relevant to a query.",
        "",
        "    This is the key function for RAG systems - instead of returning document IDs,",
        "    it returns actual text passages with position information for citations.",
        "",
        "    Args:",
        "        query_text: Search query",
        "        layers: Dictionary of layers",
        "        tokenizer: Tokenizer instance",
        "        documents: Dict mapping doc_id to document text",
        "        top_n: Number of passages to return",
        "        chunk_size: Size of each chunk in characters (default 512)",
        "        overlap: Overlap between chunks in characters (default 128)",
        "        use_expansion: Whether to expand query terms",
        "        doc_filter: Optional list of doc_ids to restrict search to",
        "",
        "    Returns:",
        "        List of (passage_text, doc_id, start_char, end_char, score) tuples",
        "        ranked by relevance",
        "    \"\"\"",
        "    layer0 = layers[CorticalLayer.TOKENS]",
        "",
        "    # Get expanded query terms",
        "    if use_expansion:",
        "        query_terms = expand_query(query_text, layers, tokenizer, max_expansions=5)",
        "    else:",
        "        tokens = tokenizer.tokenize(query_text)",
        "        query_terms = {t: 1.0 for t in tokens}",
        "",
        "    if not query_terms:",
        "        return []",
        "",
        "    # First, get candidate documents (more than we need, since we'll rank passages)",
        "    doc_scores = find_documents_for_query(",
        "        query_text, layers, tokenizer,",
        "        top_n=min(len(documents), top_n * 3),",
        "    )",
        "",
        "    # Apply document filter if provided",
        "    if doc_filter:",
        "        doc_scores = [(doc_id, score) for doc_id, score in doc_scores if doc_id in doc_filter]",
        "",
        "    # Score passages within candidate documents",
        "    passages: List[Tuple[str, str, int, int, float]] = []",
        "",
        "    for doc_id, doc_score in doc_scores:"
      ],
      "change_type": "modify"
    }
  ],
  "hour_of_day": 19,
  "day_of_week": "Tuesday",
  "seconds_since_last_commit": -496235,
  "is_merge": false,
  "is_initial": false,
  "parent_count": 1,
  "session_id": null,
  "related_chats": [],
  "ci_result": null,
  "reverted": false,
  "amended": false
}