{
  "hash": "924ae0203bea61574800db0be0636f8cb6dff559",
  "message": "feat: Add comprehensive scoring algorithm benchmark suite",
  "author": "Claude",
  "timestamp": "2025-12-15 04:19:25 +0000",
  "branch": "claude/multi-index-design-DvifZ",
  "files_changed": [
    "benchmarks/BASELINE_SUMMARY.md",
    "benchmarks/baseline_tfidf.json",
    "benchmarks/baseline_tfidf_real.json",
    "scripts/benchmark_scoring.py"
  ],
  "insertions": 1338,
  "deletions": 0,
  "hunks": [
    {
      "file": "benchmarks/BASELINE_SUMMARY.md",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "# TF-IDF Baseline Performance Summary",
        "",
        "**Date:** 2025-12-15",
        "**Algorithm:** TF-IDF (current implementation)",
        "**Purpose:** Baseline measurements before BM25 migration",
        "",
        "## Executive Summary",
        "",
        "| Metric | Synthetic (100 docs) | Real Corpus (150 docs) |",
        "|--------|---------------------|------------------------|",
        "| Score Computation | 0.72ms | 16.30ms |",
        "| Search Latency (mean) | 0.15ms | 0.37ms |",
        "| Search Throughput | 6,507 QPS | ~2,700 QPS |",
        "| Scaling Complexity | O(n) | O(n) |",
        "",
        "## Detailed Benchmarks",
        "",
        "### 1. Score Computation Time",
        "",
        "How long it takes to compute TF-IDF scores for all terms.",
        "",
        "| Corpus Size | Vocabulary | Time (ms) | Per-Doc (ms) | Per-Term (us) |",
        "|-------------|------------|-----------|--------------|---------------|",
        "| 25 docs | 64 terms | 0.20 | 0.008 | 3.13 |",
        "| 50 docs | 64 terms | 0.38 | 0.008 | 5.99 |",
        "| 100 docs | 64 terms | 0.72 | 0.007 | 11.17 |",
        "| 200 docs | 64 terms | 1.42 | 0.007 | 22.17 |",
        "| **150 docs (real)** | **11,862 terms** | **16.30** | **0.109** | **1.37** |",
        "",
        "**Observations:**",
        "- Computation scales linearly with corpus size (O(n))",
        "- Per-document cost is stable (~0.007ms for synthetic, ~0.1ms for real)",
        "- Real corpus has 185x more vocabulary, hence higher absolute time",
        "",
        "### 2. Search Query Latency",
        "",
        "Time to execute a search query and return ranked results.",
        "",
        "**Synthetic Corpus (100 docs, 64 terms):**",
        "| Metric | Value |",
        "|--------|-------|",
        "| Mean Latency | 0.15ms |",
        "| Median Latency | 0.15ms |",
        "| P95 Latency | 0.18ms |",
        "| Max Latency | 0.18ms |",
        "| Throughput | 6,507 QPS |",
        "",
        "**Real Corpus (150 docs, 11,862 terms):**",
        "| Query | Latency (ms) | Results |",
        "|-------|--------------|---------|",
        "| pagerank algorithm | 0.38 | 5 |",
        "| tfidf computation | 0.30 | 5 |",
        "| lateral connections | 0.31 | 5 |",
        "| query expansion | 0.46 | 5 |",
        "| document search | 0.52 | 5 |",
        "| minicolumn layer | 0.40 | 5 |",
        "| semantic relations | 0.37 | 5 |",
        "| louvain clustering | 0.20 | 5 |",
        "| **Mean** | **0.37** | - |",
        "",
        "### 3. Search Relevance Quality",
        "",
        "Using domain-based relevance (documents from same domain should rank higher).",
        "",
        "| Query | Domain | P@1 | P@3 | MRR | Term Recall |",
        "|-------|--------|-----|-----|-----|-------------|",
        "| neural network training | ml | 0.00 | 0.00 | 0.11 | 0.80 |",
        "| database query optimization | db | 1.00 | 1.00 | 1.00 | 0.80 |",
        "| process memory management | sys | 1.00 | 1.00 | 1.00 | 0.80 |",
        "| api authentication | web | 1.00 | 1.00 | 1.00 | 0.80 |",
        "| **Mean** | - | **0.75** | **0.75** | **0.78** | **0.80** |",
        "",
        "**Known Issue:** \"neural network training\" query performs poorly - first relevant result at rank 9.",
        "",
        "### 4. Memory Footprint",
        "",
        "Memory used by TF-IDF score storage.",
        "",
        "| Corpus Size | TF-IDF Entries | Memory (KB) | Bytes/Entry |",
        "|-------------|----------------|-------------|-------------|",
        "| 25 docs | 995 | 51.8 | 53.4 |",
        "| 50 docs | 1,988 | 99.1 | 51.0 |",
        "| 100 docs | 4,010 | 193.7 | 49.5 |",
        "| 200 docs | 8,043 | 398.6 | 50.8 |",
        "",
        "**Memory scales linearly** at ~50 bytes per (term, document) entry.",
        "",
        "### 5. Scaling Behavior",
        "",
        "Log-log regression to estimate computational complexity.",
        "",
        "| Docs | Time (ms) |",
        "|------|-----------|",
        "| 10 | 0.08 |",
        "| 25 | 0.16 |",
        "| 50 | 0.33 |",
        "| 100 | 0.62 |",
        "| 150 | 0.99 |",
        "| 200 | 1.30 |",
        "",
        "**Scaling Exponent:** 0.94 (close to 1.0)",
        "**Estimated Complexity:** O(n) - linear scaling confirmed",
        "",
        "## Targets for BM25",
        "",
        "Based on these baselines, BM25 should achieve:",
        "",
        "| Metric | Baseline | Target | Notes |",
        "|--------|----------|--------|-------|",
        "| Compute Time | 16.3ms | < 20ms | Allow 20% overhead for length normalization |",
        "| Search Latency | 0.37ms | < 0.5ms | Same or better |",
        "| Mean P@3 | 0.75 | > 0.80 | Improved relevance expected |",
        "| Mean MRR | 0.78 | > 0.85 | Better first-result ranking |",
        "| Memory | 50 bytes/entry | < 60 bytes | Small overhead for doc lengths OK |",
        "| Complexity | O(n) | O(n) | Must maintain linear scaling |",
        "",
        "## Files",
        "",
        "- `baseline_tfidf.json` - Synthetic corpus benchmarks",
        "- `baseline_tfidf_real.json` - Real corpus benchmarks",
        "",
        "## How to Compare After BM25 Implementation",
        "",
        "```bash",
        "# Run benchmarks with BM25",
        "python scripts/benchmark_scoring.py --output benchmarks/after_bm25.json",
        "",
        "# Compare results",
        "python scripts/benchmark_scoring.py --compare benchmarks/baseline_tfidf.json benchmarks/after_bm25.json",
        "```",
        "",
        "## Notes",
        "",
        "1. **Synthetic corpus** has fixed 64-term vocabulary due to deterministic generation",
        "2. **Real corpus** (150 files, 11,862 terms) is more representative of actual usage",
        "3. **Query expansion** uses PageRank + lateral connections, not TF-IDF directly",
        "4. **Term recall** measures how many expected terms appear in query expansion"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "benchmarks/baseline_tfidf.json",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "{",
        "  \"version\": \"1.0\",",
        "  \"algorithm\": \"tfidf\",",
        "  \"timestamp\": \"2025-12-15T04:17:28.406343\",",
        "  \"system_info\": {",
        "    \"python_version\": \"3.11.14\",",
        "    \"platform\": \"Linux-4.4.0-x86_64-with-glibc2.39\",",
        "    \"processor\": \"x86_64\"",
        "  },",
        "  \"results\": [",
        "    {",
        "      \"name\": \"compute_scores\",",
        "      \"algorithm\": \"tfidf\",",
        "      \"timestamp\": \"2025-12-15T04:17:28.472214\",",
        "      \"corpus_size\": 25,",
        "      \"vocabulary_size\": 64,",
        "      \"metrics\": {",
        "        \"corpus_size\": 25,",
        "        \"vocabulary_size\": 64,",
        "        \"mean_time_ms\": 0.20012960000030944,",
        "        \"std_time_ms\": 0.018232865339288584,",
        "        \"min_time_ms\": 0.18339399997557848,",
        "        \"max_time_ms\": 0.22034799997072696,",
        "        \"time_per_doc_ms\": 0.008005184000012378,",
        "        \"time_per_term_us\": 3.127025000004835",
        "      }",
        "    },",
        "    {",
        "      \"name\": \"compute_scores\",",
        "      \"algorithm\": \"tfidf\",",
        "      \"timestamp\": \"2025-12-15T04:17:28.565169\",",
        "      \"corpus_size\": 50,",
        "      \"vocabulary_size\": 64,",
        "      \"metrics\": {",
        "        \"corpus_size\": 50,",
        "        \"vocabulary_size\": 64,",
        "        \"mean_time_ms\": 0.38337980001870164,",
        "        \"std_time_ms\": 0.010113204743546902,",
        "        \"min_time_ms\": 0.3726560000245627,",
        "        \"max_time_ms\": 0.39955299996563554,",
        "        \"time_per_doc_ms\": 0.007667596000374033,",
        "        \"time_per_term_us\": 5.990309375292213",
        "      }",
        "    },",
        "    {",
        "      \"name\": \"compute_scores\",",
        "      \"algorithm\": \"tfidf\",",
        "      \"timestamp\": \"2025-12-15T04:17:28.736963\",",
        "      \"corpus_size\": 100,",
        "      \"vocabulary_size\": 64,",
        "      \"metrics\": {",
        "        \"corpus_size\": 100,",
        "        \"vocabulary_size\": 64,",
        "        \"mean_time_ms\": 0.7150193999791554,",
        "        \"std_time_ms\": 0.01533470549806313,",
        "        \"min_time_ms\": 0.6907419999606645,",
        "        \"max_time_ms\": 0.7290339999599382,",
        "        \"time_per_doc_ms\": 0.007150193999791554,",
        "        \"time_per_term_us\": 11.172178124674303",
        "      }",
        "    },",
        "    {",
        "      \"name\": \"compute_scores\",",
        "      \"algorithm\": \"tfidf\",",
        "      \"timestamp\": \"2025-12-15T04:17:29.081526\",",
        "      \"corpus_size\": 200,",
        "      \"vocabulary_size\": 64,",
        "      \"metrics\": {",
        "        \"corpus_size\": 200,",
        "        \"vocabulary_size\": 64,",
        "        \"mean_time_ms\": 1.4186690000087765,",
        "        \"std_time_ms\": 0.05244864629302831,",
        "        \"min_time_ms\": 1.360104000013962,",
        "        \"max_time_ms\": 1.486779000003935,",
        "        \"time_per_doc_ms\": 0.0070933450000438825,",
        "        \"time_per_term_us\": 22.166703125137133",
        "      }",
        "    },",
        "    {",
        "      \"name\": \"search_latency\",",
        "      \"algorithm\": \"tfidf\",",
        "      \"timestamp\": \"2025-12-15T04:17:30.094282\",",
        "      \"corpus_size\": 100,",
        "      \"vocabulary_size\": 64,",
        "      \"metrics\": {",
        "        \"corpus_size\": 100,",
        "        \"vocabulary_size\": 64,",
        "        \"num_queries\": 8,",
        "        \"mean_latency_ms\": 0.15369055000178378,",
        "        \"median_latency_ms\": 0.14959700000360954,",
        "        \"p95_latency_ms\": 0.17851069999892388,",
        "        \"max_latency_ms\": 0.17851069999892388,",
        "        \"min_latency_ms\": 0.14334240000266618,",
        "        \"throughput_qps\": 6506.580918530083",
        "      }",
        "    },",
        "    {",
        "      \"name\": \"search_relevance\",",
        "      \"algorithm\": \"tfidf\",",
        "      \"timestamp\": \"2025-12-15T04:17:31.080923\",",
        "      \"corpus_size\": 100,",
        "      \"vocabulary_size\": 64,",
        "      \"metrics\": {",
        "        \"corpus_size\": 100,",
        "        \"num_queries\": 4,",
        "        \"mean_p@1\": 0.75,",
        "        \"mean_p@3\": 0.75,",
        "        \"mean_p@5\": 0.75,",
        "        \"mean_mrr\": 0.7777777777777778,",
        "        \"mean_term_recall\": 0.8,",
        "        \"per_query\": [",
        "          {",
        "            \"query\": \"neural network training\",",
        "            \"domain\": \"ml\",",
        "            \"p@1\": 0.0,",
        "            \"p@3\": 0.0,",
        "            \"p@5\": 0.0,",
        "            \"mrr\": 0.1111111111111111,",
        "            \"term_recall\": 0.8",
        "          },",
        "          {",
        "            \"query\": \"database query optimization\",",
        "            \"domain\": \"db\",",
        "            \"p@1\": 1.0,",
        "            \"p@3\": 1.0,",
        "            \"p@5\": 1.0,",
        "            \"mrr\": 1.0,",
        "            \"term_recall\": 0.8",
        "          },",
        "          {",
        "            \"query\": \"process memory management\",",
        "            \"domain\": \"sys\",",
        "            \"p@1\": 1.0,",
        "            \"p@3\": 1.0,",
        "            \"p@5\": 1.0,",
        "            \"mrr\": 1.0,",
        "            \"term_recall\": 0.8",
        "          },",
        "          {",
        "            \"query\": \"api authentication\",",
        "            \"domain\": \"web\",",
        "            \"p@1\": 1.0,",
        "            \"p@3\": 1.0,",
        "            \"p@5\": 1.0,",
        "            \"mrr\": 1.0,",
        "            \"term_recall\": 0.8",
        "          }",
        "        ]",
        "      }",
        "    },",
        "    {",
        "      \"name\": \"memory_footprint\",",
        "      \"algorithm\": \"tfidf\",",
        "      \"timestamp\": \"2025-12-15T04:17:31.291935\",",
        "      \"corpus_size\": 25,",
        "      \"vocabulary_size\": 64,",
        "      \"metrics\": {",
        "        \"corpus_size\": 25,",
        "        \"vocabulary_size\": 64,",
        "        \"score_memory_bytes\": 53088,",
        "        \"score_memory_kb\": 51.84375,",
        "        \"total_tfidf_entries\": 995,",
        "        \"bytes_per_entry\": 53.35477386934674,",
        "        \"bytes_per_term\": 829.5",
        "      }",
        "    },",
        "    {",
        "      \"name\": \"memory_footprint\",",
        "      \"algorithm\": \"tfidf\",",
        "      \"timestamp\": \"2025-12-15T04:17:31.688317\",",
        "      \"corpus_size\": 50,",
        "      \"vocabulary_size\": 64,",
        "      \"metrics\": {",
        "        \"corpus_size\": 50,",
        "        \"vocabulary_size\": 64,",
        "        \"score_memory_bytes\": 101472,",
        "        \"score_memory_kb\": 99.09375,",
        "        \"total_tfidf_entries\": 1988,",
        "        \"bytes_per_entry\": 51.04225352112676,",
        "        \"bytes_per_term\": 1585.5",
        "      }",
        "    },",
        "    {",
        "      \"name\": \"memory_footprint\",",
        "      \"algorithm\": \"tfidf\",",
        "      \"timestamp\": \"2025-12-15T04:17:32.496817\",",
        "      \"corpus_size\": 100,",
        "      \"vocabulary_size\": 64,",
        "      \"metrics\": {",
        "        \"corpus_size\": 100,",
        "        \"vocabulary_size\": 64,",
        "        \"score_memory_bytes\": 198336,",
        "        \"score_memory_kb\": 193.6875,",
        "        \"total_tfidf_entries\": 4010,",
        "        \"bytes_per_entry\": 49.46034912718204,",
        "        \"bytes_per_term\": 3099.0",
        "      }",
        "    },",
        "    {",
        "      \"name\": \"memory_footprint\",",
        "      \"algorithm\": \"tfidf\",",
        "      \"timestamp\": \"2025-12-15T04:17:34.129549\",",
        "      \"corpus_size\": 200,",
        "      \"vocabulary_size\": 64,",
        "      \"metrics\": {",
        "        \"corpus_size\": 200,",
        "        \"vocabulary_size\": 64,",
        "        \"score_memory_bytes\": 408216,",
        "        \"score_memory_kb\": 398.6484375,",
        "        \"total_tfidf_entries\": 8043,",
        "        \"bytes_per_entry\": 50.75419619544946,",
        "        \"bytes_per_term\": 6378.375",
        "      }",
        "    },",
        "    {",
        "      \"name\": \"scaling_behavior\",",
        "      \"algorithm\": \"tfidf\",",
        "      \"timestamp\": \"2025-12-15T04:17:34.914271\",",
        "      \"corpus_size\": 200,",
        "      \"vocabulary_size\": 64,",
        "      \"metrics\": {",
        "        \"data_points\": [",
        "          {",
        "            \"n_docs\": 10,",
        "            \"vocab_size\": 64,",
        "            \"time_ms\": 0.0802899999901759",
        "          },",
        "          {",
        "            \"n_docs\": 25,",
        "            \"vocab_size\": 64,",
        "            \"time_ms\": 0.16422099997726036",
        "          },",
        "          {",
        "            \"n_docs\": 50,",
        "            \"vocab_size\": 64,",
        "            \"time_ms\": 0.3338203333100864",
        "          },",
        "          {",
        "            \"n_docs\": 100,",
        "            \"vocab_size\": 64,",
        "            \"time_ms\": 0.6225586666725272",
        "          },",
        "          {",
        "            \"n_docs\": 150,",
        "            \"vocab_size\": 64,",
        "            \"time_ms\": 0.986785666649818",
        "          },",
        "          {",
        "            \"n_docs\": 200,",
        "            \"vocab_size\": 64,",
        "            \"time_ms\": 1.2952903333219485",
        "          }",
        "        ],",
        "        \"scaling_exponent\": 0.9389157086150623,",
        "        \"estimated_complexity\": \"O(n)\"",
        "      }",
        "    }",
        "  ]",
        "}"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "benchmarks/baseline_tfidf_real.json",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "{",
        "  \"version\": \"1.0\",",
        "  \"algorithm\": \"tfidf\",",
        "  \"timestamp\": \"2025-12-15T04:18:12.435168\",",
        "  \"system_info\": {",
        "    \"python_version\": \"3.11.14\",",
        "    \"platform\": \"Linux-4.4.0-x86_64-with-glibc2.39\",",
        "    \"processor\": \"x86_64\"",
        "  },",
        "  \"results\": [",
        "    {",
        "      \"name\": \"real_corpus\",",
        "      \"algorithm\": \"tfidf\",",
        "      \"timestamp\": \"2025-12-15T04:18:16.392542\",",
        "      \"corpus_size\": 150,",
        "      \"vocabulary_size\": 11862,",
        "      \"metrics\": {",
        "        \"corpus_size\": 150,",
        "        \"vocabulary_size\": 11862,",
        "        \"compute_time_ms\": 16.29648966665324,",
        "        \"mean_query_time_ms\": 0.36824840000662107,",
        "        \"max_query_time_ms\": 0.5212204000144993,",
        "        \"queries_tested\": 8",
        "      }",
        "    }",
        "  ]",
        "}"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "scripts/benchmark_scoring.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "#!/usr/bin/env python3",
        "\"\"\"",
        "Benchmark Scoring Algorithms (TF-IDF vs BM25)",
        "=============================================",
        "",
        "Comprehensive benchmark suite for comparing document scoring algorithms.",
        "Run this BEFORE and AFTER implementing BM25 to measure impact.",
        "",
        "Usage:",
        "    # Run full benchmark suite",
        "    python scripts/benchmark_scoring.py",
        "",
        "    # Run specific benchmark",
        "    python scripts/benchmark_scoring.py --benchmark compute",
        "    python scripts/benchmark_scoring.py --benchmark search",
        "    python scripts/benchmark_scoring.py --benchmark relevance",
        "",
        "    # Save results for comparison",
        "    python scripts/benchmark_scoring.py --output baseline_tfidf.json",
        "",
        "    # Compare two benchmark runs",
        "    python scripts/benchmark_scoring.py --compare baseline_tfidf.json after_bm25.json",
        "",
        "Benchmarks:",
        "    1. COMPUTE: Time to compute scores for varying corpus sizes",
        "    2. SEARCH: Query latency and throughput",
        "    3. RELEVANCE: Search quality using known-relevant queries",
        "    4. MEMORY: Memory footprint of score storage",
        "    5. SCALING: How performance changes with corpus growth",
        "\"\"\"",
        "",
        "import argparse",
        "import json",
        "import sys",
        "import time",
        "import gc",
        "import statistics",
        "from pathlib import Path",
        "from typing import Dict, List, Any, Tuple, Optional",
        "from dataclasses import dataclass, asdict",
        "from datetime import datetime",
        "",
        "# Add parent directory to path",
        "sys.path.insert(0, str(Path(__file__).parent.parent))",
        "",
        "from cortical.processor import CorticalTextProcessor",
        "from cortical.tokenizer import Tokenizer",
        "from cortical.layers import CorticalLayer",
        "",
        "",
        "# ============================================================================",
        "# BENCHMARK DATA STRUCTURES",
        "# ============================================================================",
        "",
        "@dataclass",
        "class BenchmarkResult:",
        "    \"\"\"Result from a single benchmark run.\"\"\"",
        "    name: str",
        "    algorithm: str  # 'tfidf' or 'bm25'",
        "    timestamp: str",
        "    corpus_size: int",
        "    vocabulary_size: int",
        "    metrics: Dict[str, Any]",
        "",
        "    def to_dict(self) -> dict:",
        "        return asdict(self)",
        "",
        "",
        "@dataclass",
        "class BenchmarkSuite:",
        "    \"\"\"Collection of benchmark results.\"\"\"",
        "    version: str = \"1.0\"",
        "    algorithm: str = \"tfidf\"  # Current algorithm being benchmarked",
        "    timestamp: str = \"\"",
        "    system_info: Dict[str, Any] = None",
        "    results: List[Dict] = None",
        "",
        "    def __post_init__(self):",
        "        if self.timestamp == \"\":",
        "            self.timestamp = datetime.now().isoformat()",
        "        if self.system_info is None:",
        "            self.system_info = get_system_info()",
        "        if self.results is None:",
        "            self.results = []",
        "",
        "    def add_result(self, result: BenchmarkResult):",
        "        self.results.append(result.to_dict())",
        "",
        "    def to_dict(self) -> dict:",
        "        return {",
        "            'version': self.version,",
        "            'algorithm': self.algorithm,",
        "            'timestamp': self.timestamp,",
        "            'system_info': self.system_info,",
        "            'results': self.results",
        "        }",
        "",
        "    def save(self, path: str):",
        "        with open(path, 'w') as f:",
        "            json.dump(self.to_dict(), f, indent=2)",
        "        print(f\"Results saved to {path}\")",
        "",
        "    @classmethod",
        "    def load(cls, path: str) -> 'BenchmarkSuite':",
        "        with open(path) as f:",
        "            data = json.load(f)",
        "        suite = cls(",
        "            version=data['version'],",
        "            algorithm=data['algorithm'],",
        "            timestamp=data['timestamp'],",
        "            system_info=data['system_info'],",
        "            results=data['results']",
        "        )",
        "        return suite",
        "",
        "",
        "def get_system_info() -> Dict[str, Any]:",
        "    \"\"\"Collect system information for benchmark context.\"\"\"",
        "    import platform",
        "    return {",
        "        'python_version': platform.python_version(),",
        "        'platform': platform.platform(),",
        "        'processor': platform.processor(),",
        "    }",
        "",
        "",
        "# ============================================================================",
        "# TEST CORPUS GENERATORS",
        "# ============================================================================",
        "",
        "def generate_synthetic_corpus(n_docs: int, avg_length: int = 200) -> Dict[str, str]:",
        "    \"\"\"",
        "    Generate synthetic documents for benchmarking.",
        "",
        "    Uses deterministic content for reproducible benchmarks.",
        "    \"\"\"",
        "    import hashlib",
        "",
        "    # Domain vocabulary for realistic term distributions",
        "    domains = {",
        "        'ml': ['neural', 'network', 'learning', 'model', 'training', 'gradient',",
        "               'loss', 'optimization', 'batch', 'epoch', 'layer', 'activation',",
        "               'weights', 'bias', 'backpropagation', 'forward', 'inference'],",
        "        'db': ['database', 'query', 'index', 'table', 'row', 'column', 'join',",
        "               'transaction', 'commit', 'rollback', 'lock', 'cache', 'buffer',",
        "               'storage', 'retrieval', 'schema', 'normalization'],",
        "        'sys': ['process', 'thread', 'memory', 'allocation', 'kernel', 'system',",
        "                'file', 'socket', 'network', 'protocol', 'buffer', 'stream',",
        "                'handler', 'callback', 'event', 'scheduler', 'queue'],",
        "        'web': ['request', 'response', 'server', 'client', 'http', 'api',",
        "                'endpoint', 'route', 'middleware', 'session', 'cookie',",
        "                'authentication', 'authorization', 'token', 'header'],",
        "    }",
        "",
        "    all_terms = []",
        "    for terms in domains.values():",
        "        all_terms.extend(terms)",
        "",
        "    docs = {}",
        "    for i in range(n_docs):",
        "        # Deterministic seed based on doc index",
        "        seed = int(hashlib.md5(f\"doc_{i}\".encode()).hexdigest()[:8], 16)",
        "",
        "        # Select primary domain for this doc",
        "        domain_idx = seed % len(domains)",
        "        domain = list(domains.keys())[domain_idx]",
        "        domain_terms = domains[domain]",
        "",
        "        # Generate document content",
        "        words = []",
        "        word_count = avg_length + (seed % 100) - 50  # Vary length",
        "",
        "        for j in range(word_count):",
        "            term_seed = (seed + j * 31) % 1000",
        "            if term_seed < 600:  # 60% domain terms",
        "                term_idx = (seed + j) % len(domain_terms)",
        "                words.append(domain_terms[term_idx])",
        "            else:  # 40% cross-domain terms",
        "                term_idx = (seed + j * 17) % len(all_terms)",
        "                words.append(all_terms[term_idx])",
        "",
        "        doc_id = f\"synthetic/doc_{i:04d}.txt\"",
        "        docs[doc_id] = ' '.join(words)",
        "",
        "    return docs",
        "",
        "",
        "def load_real_corpus() -> Optional[Dict[str, str]]:",
        "    \"\"\"Load real corpus if available.\"\"\"",
        "    corpus_path = Path(\"corpus_dev.pkl\")",
        "    if corpus_path.exists():",
        "        try:",
        "            processor = CorticalTextProcessor.load(str(corpus_path))",
        "            return processor.documents",
        "        except Exception as e:",
        "            print(f\"Warning: Could not load corpus: {e}\")",
        "    return None",
        "",
        "",
        "# ============================================================================",
        "# RELEVANCE TEST QUERIES",
        "# ============================================================================",
        "",
        "# Queries with expected relevant documents (for synthetic corpus)",
        "RELEVANCE_QUERIES = [",
        "    {",
        "        'query': 'neural network training',",
        "        'domain': 'ml',",
        "        'expected_terms': ['neural', 'network', 'training', 'learning', 'model'],",
        "    },",
        "    {",
        "        'query': 'database query optimization',",
        "        'domain': 'db',",
        "        'expected_terms': ['database', 'query', 'index', 'cache', 'retrieval'],",
        "    },",
        "    {",
        "        'query': 'process memory management',",
        "        'domain': 'sys',",
        "        'expected_terms': ['process', 'memory', 'allocation', 'buffer', 'kernel'],",
        "    },",
        "    {",
        "        'query': 'api authentication',",
        "        'domain': 'web',",
        "        'expected_terms': ['api', 'authentication', 'token', 'request', 'session'],",
        "    },",
        "]",
        "",
        "",
        "# ============================================================================",
        "# BENCHMARK IMPLEMENTATIONS",
        "# ============================================================================",
        "",
        "def benchmark_compute(suite: BenchmarkSuite, corpus_sizes: List[int] = None):",
        "    \"\"\"",
        "    Benchmark: Score computation time.",
        "",
        "    Measures time to compute TF-IDF/BM25 scores for varying corpus sizes.",
        "    \"\"\"",
        "    print(\"\\n\" + \"=\" * 60)",
        "    print(\"BENCHMARK: Score Computation Time\")",
        "    print(\"=\" * 60)",
        "",
        "    if corpus_sizes is None:",
        "        corpus_sizes = [25, 50, 100, 200]",
        "",
        "    tokenizer = Tokenizer(filter_code_noise=True)",
        "",
        "    for n_docs in corpus_sizes:",
        "        print(f\"\\nCorpus size: {n_docs} documents\")",
        "",
        "        # Generate corpus",
        "        docs = generate_synthetic_corpus(n_docs)",
        "",
        "        # Create processor and load documents",
        "        processor = CorticalTextProcessor(tokenizer=tokenizer)",
        "        for doc_id, content in docs.items():",
        "            processor.process_document(doc_id, content)",
        "",
        "        layer0 = processor.layers.get(CorticalLayer.TOKENS)",
        "        vocab_size = layer0.column_count() if layer0 else 0",
        "",
        "        # Warm up",
        "        processor.compute_tfidf(verbose=False)",
        "",
        "        # Benchmark TF-IDF computation (multiple runs for stability)",
        "        times = []",
        "        for _ in range(5):",
        "            # Reset TF-IDF scores",
        "            for col in layer0.minicolumns.values():",
        "                col.tfidf = 0.0",
        "                col.tfidf_per_doc = {}",
        "",
        "            gc.collect()",
        "            start = time.perf_counter()",
        "            processor.compute_tfidf(verbose=False)",
        "            elapsed = time.perf_counter() - start",
        "            times.append(elapsed)",
        "",
        "        metrics = {",
        "            'corpus_size': n_docs,",
        "            'vocabulary_size': vocab_size,",
        "            'mean_time_ms': statistics.mean(times) * 1000,",
        "            'std_time_ms': statistics.stdev(times) * 1000 if len(times) > 1 else 0,",
        "            'min_time_ms': min(times) * 1000,",
        "            'max_time_ms': max(times) * 1000,",
        "            'time_per_doc_ms': (statistics.mean(times) * 1000) / n_docs,",
        "            'time_per_term_us': (statistics.mean(times) * 1_000_000) / vocab_size if vocab_size else 0,",
        "        }",
        "",
        "        print(f\"  Vocabulary: {vocab_size} terms\")",
        "        print(f\"  Mean time: {metrics['mean_time_ms']:.2f}ms (+/- {metrics['std_time_ms']:.2f}ms)\")",
        "        print(f\"  Per-doc: {metrics['time_per_doc_ms']:.3f}ms\")",
        "        print(f\"  Per-term: {metrics['time_per_term_us']:.2f}us\")",
        "",
        "        result = BenchmarkResult(",
        "            name='compute_scores',",
        "            algorithm=suite.algorithm,",
        "            timestamp=datetime.now().isoformat(),",
        "            corpus_size=n_docs,",
        "            vocabulary_size=vocab_size,",
        "            metrics=metrics",
        "        )",
        "        suite.add_result(result)",
        "",
        "        del processor",
        "        gc.collect()",
        "",
        "",
        "def benchmark_search(suite: BenchmarkSuite, n_docs: int = 100):",
        "    \"\"\"",
        "    Benchmark: Search query latency and throughput.",
        "",
        "    Measures time to execute search queries using computed scores.",
        "    \"\"\"",
        "    print(\"\\n\" + \"=\" * 60)",
        "    print(\"BENCHMARK: Search Query Performance\")",
        "    print(\"=\" * 60)",
        "",
        "    # Setup corpus",
        "    tokenizer = Tokenizer(filter_code_noise=True)",
        "    processor = CorticalTextProcessor(tokenizer=tokenizer)",
        "",
        "    docs = generate_synthetic_corpus(n_docs)",
        "    for doc_id, content in docs.items():",
        "        processor.process_document(doc_id, content)",
        "    processor.compute_all(verbose=False)",
        "",
        "    layer0 = processor.layers.get(CorticalLayer.TOKENS)",
        "    vocab_size = layer0.column_count() if layer0 else 0",
        "",
        "    # Test queries",
        "    queries = [q['query'] for q in RELEVANCE_QUERIES]",
        "    queries.extend([",
        "        'learning optimization',",
        "        'cache buffer',",
        "        'request handler',",
        "        'network protocol',",
        "    ])",
        "",
        "    # Warm up",
        "    for q in queries[:2]:",
        "        processor.find_documents_for_query(q, top_n=5)",
        "",
        "    # Benchmark queries",
        "    print(f\"\\nCorpus: {n_docs} docs, {vocab_size} terms\")",
        "    print(f\"Running {len(queries)} queries...\")",
        "",
        "    query_times = []",
        "    for query in queries:",
        "        times = []",
        "        for _ in range(10):",
        "            start = time.perf_counter()",
        "            results = processor.find_documents_for_query(query, top_n=5)",
        "            elapsed = time.perf_counter() - start",
        "            times.append(elapsed)",
        "",
        "        avg_time = statistics.mean(times)",
        "        query_times.append(avg_time)",
        "",
        "    metrics = {",
        "        'corpus_size': n_docs,",
        "        'vocabulary_size': vocab_size,",
        "        'num_queries': len(queries),",
        "        'mean_latency_ms': statistics.mean(query_times) * 1000,",
        "        'median_latency_ms': statistics.median(query_times) * 1000,",
        "        'p95_latency_ms': sorted(query_times)[int(len(query_times) * 0.95)] * 1000,",
        "        'max_latency_ms': max(query_times) * 1000,",
        "        'min_latency_ms': min(query_times) * 1000,",
        "        'throughput_qps': 1.0 / statistics.mean(query_times) if query_times else 0,",
        "    }",
        "",
        "    print(f\"  Mean latency: {metrics['mean_latency_ms']:.2f}ms\")",
        "    print(f\"  P95 latency: {metrics['p95_latency_ms']:.2f}ms\")",
        "    print(f\"  Throughput: {metrics['throughput_qps']:.1f} queries/sec\")",
        "",
        "    result = BenchmarkResult(",
        "        name='search_latency',",
        "        algorithm=suite.algorithm,",
        "        timestamp=datetime.now().isoformat(),",
        "        corpus_size=n_docs,",
        "        vocabulary_size=vocab_size,",
        "        metrics=metrics",
        "    )",
        "    suite.add_result(result)",
        "",
        "",
        "def benchmark_relevance(suite: BenchmarkSuite, n_docs: int = 100):",
        "    \"\"\"",
        "    Benchmark: Search result relevance quality.",
        "",
        "    Measures how well the scoring algorithm ranks relevant documents.",
        "    Uses domain-based relevance (docs from same domain should rank higher).",
        "    \"\"\"",
        "    print(\"\\n\" + \"=\" * 60)",
        "    print(\"BENCHMARK: Search Relevance Quality\")",
        "    print(\"=\" * 60)",
        "",
        "    # Setup corpus with domain tracking",
        "    tokenizer = Tokenizer(filter_code_noise=True)",
        "    processor = CorticalTextProcessor(tokenizer=tokenizer)",
        "",
        "    docs = generate_synthetic_corpus(n_docs)",
        "    for doc_id, content in docs.items():",
        "        processor.process_document(doc_id, content)",
        "    processor.compute_all(verbose=False)",
        "",
        "    # Determine document domains (based on generation algorithm)",
        "    import hashlib",
        "    domains = ['ml', 'db', 'sys', 'web']",
        "    doc_domains = {}",
        "    for i in range(n_docs):",
        "        seed = int(hashlib.md5(f\"doc_{i}\".encode()).hexdigest()[:8], 16)",
        "        domain_idx = seed % len(domains)",
        "        doc_id = f\"synthetic/doc_{i:04d}.txt\"",
        "        doc_domains[doc_id] = domains[domain_idx]",
        "",
        "    # Run relevance tests",
        "    print(f\"\\nCorpus: {n_docs} docs\")",
        "",
        "    relevance_scores = []",
        "",
        "    for test in RELEVANCE_QUERIES:",
        "        query = test['query']",
        "        expected_domain = test['domain']",
        "        expected_terms = test['expected_terms']",
        "",
        "        # Get search results",
        "        results = processor.find_documents_for_query(query, top_n=10)",
        "",
        "        # Calculate precision@k (docs from expected domain in top k)",
        "        precisions = {}",
        "        for k in [1, 3, 5, 10]:",
        "            top_k = results[:k]",
        "            relevant = sum(1 for doc_id, _ in top_k",
        "                         if doc_domains.get(doc_id) == expected_domain)",
        "            precisions[f'p@{k}'] = relevant / k if k <= len(results) else 0",
        "",
        "        # Calculate MRR (mean reciprocal rank of first relevant doc)",
        "        mrr = 0.0",
        "        for rank, (doc_id, _) in enumerate(results, 1):",
        "            if doc_domains.get(doc_id) == expected_domain:",
        "                mrr = 1.0 / rank",
        "                break",
        "",
        "        # Check if expected terms appear in expanded query",
        "        expanded = processor.expand_query(query, max_expansions=10)",
        "        term_recall = sum(1 for t in expected_terms if t in expanded) / len(expected_terms)",
        "",
        "        relevance_scores.append({",
        "            'query': query,",
        "            'domain': expected_domain,",
        "            'p@1': precisions['p@1'],",
        "            'p@3': precisions['p@3'],",
        "            'p@5': precisions['p@5'],",
        "            'mrr': mrr,",
        "            'term_recall': term_recall,",
        "        })",
        "",
        "        print(f\"  '{query}': P@3={precisions['p@3']:.2f}, MRR={mrr:.2f}, TermRecall={term_recall:.2f}\")",
        "",
        "    # Aggregate metrics",
        "    metrics = {",
        "        'corpus_size': n_docs,",
        "        'num_queries': len(relevance_scores),",
        "        'mean_p@1': statistics.mean(r['p@1'] for r in relevance_scores),",
        "        'mean_p@3': statistics.mean(r['p@3'] for r in relevance_scores),",
        "        'mean_p@5': statistics.mean(r['p@5'] for r in relevance_scores),",
        "        'mean_mrr': statistics.mean(r['mrr'] for r in relevance_scores),",
        "        'mean_term_recall': statistics.mean(r['term_recall'] for r in relevance_scores),",
        "        'per_query': relevance_scores,",
        "    }",
        "",
        "    print(f\"\\n  AGGREGATE:\")",
        "    print(f\"    Mean P@3: {metrics['mean_p@3']:.3f}\")",
        "    print(f\"    Mean MRR: {metrics['mean_mrr']:.3f}\")",
        "    print(f\"    Mean Term Recall: {metrics['mean_term_recall']:.3f}\")",
        "",
        "    result = BenchmarkResult(",
        "        name='search_relevance',",
        "        algorithm=suite.algorithm,",
        "        timestamp=datetime.now().isoformat(),",
        "        corpus_size=n_docs,",
        "        vocabulary_size=processor.layers[CorticalLayer.TOKENS].column_count(),",
        "        metrics=metrics",
        "    )",
        "    suite.add_result(result)",
        "",
        "",
        "def benchmark_memory(suite: BenchmarkSuite, corpus_sizes: List[int] = None):",
        "    \"\"\"",
        "    Benchmark: Memory footprint of score storage.",
        "",
        "    Measures memory used by TF-IDF/BM25 data structures.",
        "    \"\"\"",
        "    print(\"\\n\" + \"=\" * 60)",
        "    print(\"BENCHMARK: Memory Footprint\")",
        "    print(\"=\" * 60)",
        "",
        "    if corpus_sizes is None:",
        "        corpus_sizes = [25, 50, 100, 200]",
        "",
        "    import tracemalloc",
        "",
        "    tokenizer = Tokenizer(filter_code_noise=True)",
        "",
        "    for n_docs in corpus_sizes:",
        "        print(f\"\\nCorpus size: {n_docs} documents\")",
        "",
        "        # Generate corpus",
        "        docs = generate_synthetic_corpus(n_docs)",
        "",
        "        gc.collect()",
        "        tracemalloc.start()",
        "",
        "        # Create processor and load documents",
        "        processor = CorticalTextProcessor(tokenizer=tokenizer)",
        "        for doc_id, content in docs.items():",
        "            processor.process_document(doc_id, content)",
        "",
        "        # Snapshot before computing scores",
        "        before_compute = tracemalloc.take_snapshot()",
        "",
        "        # Compute scores",
        "        processor.compute_tfidf(verbose=False)",
        "",
        "        # Snapshot after computing scores",
        "        after_compute = tracemalloc.take_snapshot()",
        "",
        "        tracemalloc.stop()",
        "",
        "        # Calculate memory difference",
        "        diff = after_compute.compare_to(before_compute, 'lineno')",
        "        score_memory = sum(stat.size_diff for stat in diff if stat.size_diff > 0)",
        "",
        "        layer0 = processor.layers.get(CorticalLayer.TOKENS)",
        "        vocab_size = layer0.column_count() if layer0 else 0",
        "",
        "        # Estimate per-doc TF-IDF dict size",
        "        total_tfidf_entries = sum(",
        "            len(col.tfidf_per_doc)",
        "            for col in layer0.minicolumns.values()",
        "        )",
        "",
        "        metrics = {",
        "            'corpus_size': n_docs,",
        "            'vocabulary_size': vocab_size,",
        "            'score_memory_bytes': score_memory,",
        "            'score_memory_kb': score_memory / 1024,",
        "            'total_tfidf_entries': total_tfidf_entries,",
        "            'bytes_per_entry': score_memory / total_tfidf_entries if total_tfidf_entries else 0,",
        "            'bytes_per_term': score_memory / vocab_size if vocab_size else 0,",
        "        }",
        "",
        "        print(f\"  Vocabulary: {vocab_size} terms\")",
        "        print(f\"  TF-IDF entries: {total_tfidf_entries}\")",
        "        print(f\"  Score memory: {metrics['score_memory_kb']:.1f} KB\")",
        "        print(f\"  Per entry: {metrics['bytes_per_entry']:.1f} bytes\")",
        "",
        "        result = BenchmarkResult(",
        "            name='memory_footprint',",
        "            algorithm=suite.algorithm,",
        "            timestamp=datetime.now().isoformat(),",
        "            corpus_size=n_docs,",
        "            vocabulary_size=vocab_size,",
        "            metrics=metrics",
        "        )",
        "        suite.add_result(result)",
        "",
        "        del processor",
        "        gc.collect()",
        "",
        "",
        "def benchmark_scaling(suite: BenchmarkSuite):",
        "    \"\"\"",
        "    Benchmark: Scaling behavior analysis.",
        "",
        "    Measures how compute time scales with corpus size to detect O(n^2) issues.",
        "    \"\"\"",
        "    print(\"\\n\" + \"=\" * 60)",
        "    print(\"BENCHMARK: Scaling Behavior\")",
        "    print(\"=\" * 60)",
        "",
        "    corpus_sizes = [10, 25, 50, 100, 150, 200]",
        "    tokenizer = Tokenizer(filter_code_noise=True)",
        "",
        "    timings = []",
        "",
        "    for n_docs in corpus_sizes:",
        "        docs = generate_synthetic_corpus(n_docs)",
        "",
        "        processor = CorticalTextProcessor(tokenizer=tokenizer)",
        "        for doc_id, content in docs.items():",
        "            processor.process_document(doc_id, content)",
        "",
        "        layer0 = processor.layers.get(CorticalLayer.TOKENS)",
        "        vocab_size = layer0.column_count() if layer0 else 0",
        "",
        "        # Time score computation",
        "        gc.collect()",
        "        times = []",
        "        for _ in range(3):",
        "            # Reset scores",
        "            for col in layer0.minicolumns.values():",
        "                col.tfidf = 0.0",
        "                col.tfidf_per_doc = {}",
        "",
        "            start = time.perf_counter()",
        "            processor.compute_tfidf(verbose=False)",
        "            elapsed = time.perf_counter() - start",
        "            times.append(elapsed)",
        "",
        "        avg_time = statistics.mean(times)",
        "        timings.append({",
        "            'n_docs': n_docs,",
        "            'vocab_size': vocab_size,",
        "            'time_ms': avg_time * 1000,",
        "        })",
        "",
        "        print(f\"  {n_docs} docs, {vocab_size} terms: {avg_time*1000:.2f}ms\")",
        "",
        "        del processor",
        "        gc.collect()",
        "",
        "    # Analyze scaling behavior",
        "    # Linear: time ~ n, Quadratic: time ~ n^2",
        "    # Fit log-log slope to estimate complexity",
        "    import math",
        "",
        "    if len(timings) >= 3:",
        "        log_n = [math.log(t['n_docs']) for t in timings]",
        "        log_t = [math.log(t['time_ms']) for t in timings]",
        "",
        "        # Simple linear regression on log-log plot",
        "        n = len(log_n)",
        "        sum_x = sum(log_n)",
        "        sum_y = sum(log_t)",
        "        sum_xy = sum(x*y for x, y in zip(log_n, log_t))",
        "        sum_xx = sum(x*x for x in log_n)",
        "",
        "        slope = (n * sum_xy - sum_x * sum_y) / (n * sum_xx - sum_x * sum_x)",
        "",
        "        complexity = \"O(n)\" if slope < 1.3 else \"O(n log n)\" if slope < 1.7 else \"O(n^2)\"",
        "",
        "        print(f\"\\n  Scaling exponent: {slope:.2f}\")",
        "        print(f\"  Estimated complexity: {complexity}\")",
        "    else:",
        "        slope = 0",
        "        complexity = \"Unknown (need more data points)\"",
        "",
        "    metrics = {",
        "        'data_points': timings,",
        "        'scaling_exponent': slope,",
        "        'estimated_complexity': complexity,",
        "    }",
        "",
        "    result = BenchmarkResult(",
        "        name='scaling_behavior',",
        "        algorithm=suite.algorithm,",
        "        timestamp=datetime.now().isoformat(),",
        "        corpus_size=max(t['n_docs'] for t in timings),",
        "        vocabulary_size=max(t['vocab_size'] for t in timings),",
        "        metrics=metrics",
        "    )",
        "    suite.add_result(result)",
        "",
        "",
        "def benchmark_real_corpus(suite: BenchmarkSuite):",
        "    \"\"\"",
        "    Benchmark: Performance on real corpus (if available).",
        "",
        "    Uses the actual indexed codebase for realistic measurements.",
        "    \"\"\"",
        "    print(\"\\n\" + \"=\" * 60)",
        "    print(\"BENCHMARK: Real Corpus Performance\")",
        "    print(\"=\" * 60)",
        "",
        "    # Try to load real corpus",
        "    corpus_path = Path(\"corpus_dev.pkl\")",
        "    if not corpus_path.exists():",
        "        print(\"  Skipping: corpus_dev.pkl not found\")",
        "        print(\"  Run: python scripts/index_codebase.py first\")",
        "        return",
        "",
        "    try:",
        "        processor = CorticalTextProcessor.load(str(corpus_path))",
        "    except Exception as e:",
        "        print(f\"  Skipping: Could not load corpus: {e}\")",
        "        return",
        "",
        "    n_docs = len(processor.documents)",
        "    layer0 = processor.layers.get(CorticalLayer.TOKENS)",
        "    vocab_size = layer0.column_count() if layer0 else 0",
        "",
        "    print(f\"  Corpus: {n_docs} documents, {vocab_size} terms\")",
        "",
        "    # Test queries relevant to this codebase",
        "    codebase_queries = [",
        "        \"pagerank algorithm\",",
        "        \"tfidf computation\",",
        "        \"lateral connections\",",
        "        \"query expansion\",",
        "        \"document search\",",
        "        \"minicolumn layer\",",
        "        \"semantic relations\",",
        "        \"louvain clustering\",",
        "    ]",
        "",
        "    # Benchmark TF-IDF computation",
        "    print(\"\\n  TF-IDF Computation:\")",
        "    times = []",
        "    for _ in range(3):",
        "        # Reset scores",
        "        for col in layer0.minicolumns.values():",
        "            col.tfidf = 0.0",
        "            col.tfidf_per_doc = {}",
        "",
        "        gc.collect()",
        "        start = time.perf_counter()",
        "        processor.compute_tfidf(verbose=False)",
        "        elapsed = time.perf_counter() - start",
        "        times.append(elapsed)",
        "",
        "    compute_time = statistics.mean(times)",
        "    print(f\"    Time: {compute_time*1000:.2f}ms\")",
        "",
        "    # Benchmark search queries",
        "    print(\"\\n  Search Queries:\")",
        "    query_times = []",
        "    for query in codebase_queries:",
        "        times = []",
        "        for _ in range(5):",
        "            start = time.perf_counter()",
        "            results = processor.find_documents_for_query(query, top_n=5)",
        "            elapsed = time.perf_counter() - start",
        "            times.append(elapsed)",
        "",
        "        avg_time = statistics.mean(times)",
        "        query_times.append(avg_time)",
        "        print(f\"    '{query}': {avg_time*1000:.2f}ms, {len(results)} results\")",
        "",
        "    metrics = {",
        "        'corpus_size': n_docs,",
        "        'vocabulary_size': vocab_size,",
        "        'compute_time_ms': compute_time * 1000,",
        "        'mean_query_time_ms': statistics.mean(query_times) * 1000,",
        "        'max_query_time_ms': max(query_times) * 1000,",
        "        'queries_tested': len(codebase_queries),",
        "    }",
        "",
        "    result = BenchmarkResult(",
        "        name='real_corpus',",
        "        algorithm=suite.algorithm,",
        "        timestamp=datetime.now().isoformat(),",
        "        corpus_size=n_docs,",
        "        vocabulary_size=vocab_size,",
        "        metrics=metrics",
        "    )",
        "    suite.add_result(result)",
        "",
        "",
        "# ============================================================================",
        "# COMPARISON TOOLS",
        "# ============================================================================",
        "",
        "def compare_results(before_path: str, after_path: str):",
        "    \"\"\"Compare two benchmark runs and report improvements/regressions.\"\"\"",
        "    print(\"\\n\" + \"=\" * 60)",
        "    print(\"BENCHMARK COMPARISON\")",
        "    print(\"=\" * 60)",
        "",
        "    before = BenchmarkSuite.load(before_path)",
        "    after = BenchmarkSuite.load(after_path)",
        "",
        "    print(f\"\\nBefore: {before.algorithm} ({before.timestamp})\")",
        "    print(f\"After:  {after.algorithm} ({after.timestamp})\")",
        "",
        "    # Group results by benchmark name",
        "    before_by_name = {}",
        "    for r in before.results:",
        "        key = (r['name'], r['corpus_size'])",
        "        before_by_name[key] = r",
        "",
        "    after_by_name = {}",
        "    for r in after.results:",
        "        key = (r['name'], r['corpus_size'])",
        "        after_by_name[key] = r",
        "",
        "    print(\"\\n\" + \"-\" * 60)",
        "    print(f\"{'Benchmark':<30} {'Before':>12} {'After':>12} {'Change':>12}\")",
        "    print(\"-\" * 60)",
        "",
        "    # Compare common benchmarks",
        "    for key in sorted(before_by_name.keys()):",
        "        if key not in after_by_name:",
        "            continue",
        "",
        "        b = before_by_name[key]['metrics']",
        "        a = after_by_name[key]['metrics']",
        "        name = f\"{key[0]} (n={key[1]})\"",
        "",
        "        # Compare key metrics",
        "        if 'mean_time_ms' in b:",
        "            b_val = b['mean_time_ms']",
        "            a_val = a['mean_time_ms']",
        "            change = ((a_val - b_val) / b_val) * 100 if b_val else 0",
        "            indicator = \"faster\" if change < 0 else \"SLOWER\"",
        "            print(f\"{name:<30} {b_val:>10.2f}ms {a_val:>10.2f}ms {change:>+10.1f}% {indicator}\")",
        "",
        "        if 'mean_latency_ms' in b:",
        "            b_val = b['mean_latency_ms']",
        "            a_val = a['mean_latency_ms']",
        "            change = ((a_val - b_val) / b_val) * 100 if b_val else 0",
        "            indicator = \"faster\" if change < 0 else \"SLOWER\"",
        "            print(f\"{name:<30} {b_val:>10.2f}ms {a_val:>10.2f}ms {change:>+10.1f}% {indicator}\")",
        "",
        "        if 'mean_p@3' in b:",
        "            b_val = b['mean_p@3']",
        "            a_val = a['mean_p@3']",
        "            change = ((a_val - b_val) / b_val) * 100 if b_val else 0",
        "            indicator = \"BETTER\" if change > 0 else \"worse\"",
        "            print(f\"{name:<30} {b_val:>10.3f}   {a_val:>10.3f}   {change:>+10.1f}% {indicator}\")",
        "",
        "    print(\"-\" * 60)",
        "",
        "",
        "# ============================================================================",
        "# MAIN",
        "# ============================================================================",
        "",
        "def run_all_benchmarks(output_path: Optional[str] = None):",
        "    \"\"\"Run all benchmarks and optionally save results.\"\"\"",
        "    suite = BenchmarkSuite(algorithm='tfidf')",
        "",
        "    print(\"\\n\" + \"=\" * 60)",
        "    print(\"SCORING ALGORITHM BENCHMARK SUITE\")",
        "    print(\"=\" * 60)",
        "    print(f\"Algorithm: {suite.algorithm}\")",
        "    print(f\"Timestamp: {suite.timestamp}\")",
        "    print(f\"Python: {suite.system_info['python_version']}\")",
        "",
        "    # Run benchmarks",
        "    benchmark_compute(suite)",
        "    benchmark_search(suite)",
        "    benchmark_relevance(suite)",
        "    benchmark_memory(suite)",
        "    benchmark_scaling(suite)",
        "    benchmark_real_corpus(suite)",
        "",
        "    # Summary",
        "    print(\"\\n\" + \"=\" * 60)",
        "    print(\"BENCHMARK SUMMARY\")",
        "    print(\"=\" * 60)",
        "",
        "    for r in suite.results:",
        "        print(f\"\\n{r['name']} (n={r['corpus_size']}):\")",
        "        for key, value in r['metrics'].items():",
        "            if key == 'per_query' or key == 'data_points':",
        "                continue",
        "            if isinstance(value, float):",
        "                print(f\"  {key}: {value:.3f}\")",
        "            else:",
        "                print(f\"  {key}: {value}\")",
        "",
        "    # Save if requested",
        "    if output_path:",
        "        suite.save(output_path)",
        "",
        "    return suite",
        "",
        "",
        "def main():",
        "    parser = argparse.ArgumentParser(",
        "        description=\"Benchmark scoring algorithms (TF-IDF vs BM25)\",",
        "        formatter_class=argparse.RawDescriptionHelpFormatter,",
        "        epilog=__doc__",
        "    )",
        "    parser.add_argument('--benchmark', choices=['all', 'compute', 'search', 'relevance',",
        "                                                  'memory', 'scaling', 'real'],",
        "                       default='all', help='Benchmark to run')",
        "    parser.add_argument('--output', '-o', help='Save results to JSON file')",
        "    parser.add_argument('--compare', nargs=2, metavar=('BEFORE', 'AFTER'),",
        "                       help='Compare two benchmark result files')",
        "    parser.add_argument('--corpus-sizes', type=int, nargs='+', default=[25, 50, 100, 200],",
        "                       help='Corpus sizes to test')",
        "",
        "    args = parser.parse_args()",
        "",
        "    if args.compare:",
        "        compare_results(args.compare[0], args.compare[1])",
        "        return",
        "",
        "    if args.benchmark == 'all':",
        "        run_all_benchmarks(args.output)",
        "    else:",
        "        suite = BenchmarkSuite(algorithm='tfidf')",
        "",
        "        if args.benchmark == 'compute':",
        "            benchmark_compute(suite, args.corpus_sizes)",
        "        elif args.benchmark == 'search':",
        "            benchmark_search(suite)",
        "        elif args.benchmark == 'relevance':",
        "            benchmark_relevance(suite)",
        "        elif args.benchmark == 'memory':",
        "            benchmark_memory(suite)",
        "        elif args.benchmark == 'scaling':",
        "            benchmark_scaling(suite)",
        "        elif args.benchmark == 'real':",
        "            benchmark_real_corpus(suite)",
        "",
        "        if args.output:",
        "            suite.save(args.output)",
        "",
        "",
        "if __name__ == '__main__':",
        "    main()"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    }
  ],
  "hour_of_day": 4,
  "day_of_week": "Monday",
  "seconds_since_last_commit": -33923,
  "is_merge": false,
  "is_initial": false,
  "parent_count": 1,
  "session_id": null,
  "related_chats": [],
  "ci_result": null,
  "reverted": false,
  "amended": false
}