{
  "hash": "d4eda23d06807c6d3957cf331f1cb441a1a60a54",
  "message": "Merge pull request #5 from scrawlsbenches/claude/plan-next-steps-01E4sb6z5gMowAraWXiHPGgc",
  "author": "scrawlsbenches",
  "timestamp": "2025-12-09 16:01:30 -0500",
  "branch": "claude/multi-index-design-DvifZ",
  "files_changed": [
    "CLAUDE.md",
    "TASK_LIST.md",
    "cortical/embeddings.py",
    "cortical/persistence.py",
    "cortical/processor.py",
    "cortical/query.py",
    "showcase.py",
    "tests/test_persistence.py",
    "tests/test_processor.py"
  ],
  "insertions": 1075,
  "deletions": 95,
  "hunks": [
    {
      "file": "CLAUDE.md",
      "function": "tests/",
      "start_line": 43,
      "lines_added": [
        "All 129 tests should pass.",
        "## Running the Showcase",
        "python showcase.py"
      ],
      "lines_removed": [
        "All 109 tests should pass.",
        "## Running the Demo",
        "python demo.py"
      ],
      "context_before": [
        "├── test_gaps.py",
        "└── test_persistence.py",
        "```",
        "",
        "## Running Tests",
        "",
        "```bash",
        "python -m unittest discover -s tests -v",
        "```",
        ""
      ],
      "context_after": [
        "",
        "",
        "```bash",
        "```",
        "",
        "## Key Classes",
        "",
        "### CorticalTextProcessor",
        "Main entry point. Coordinates document processing, computations, and queries.",
        "",
        "### HierarchicalLayer",
        "Manages minicolumns at a given hierarchy level. Has `get_by_id()` for O(1) lookups.",
        ""
      ],
      "change_type": "modify"
    },
    {
      "file": "TASK_LIST.md",
      "function": null,
      "start_line": 1,
      "lines_added": [
        "# Task List: Bug Fixes & RAG Enhancements",
        "This document tracks bug fixes and feature enhancements for the Cortical Text Processor.",
        "**Status:** Bug fixes complete | RAG enhancements planned"
      ],
      "lines_removed": [
        "# Task List: Required Bug Fixes",
        "This document tracks required bug fixes identified during the code review of the Cortical Text Processor.",
        "**Status:** All critical and high-priority tasks completed"
      ],
      "context_before": [],
      "context_after": [
        "",
        "",
        "**Last Updated:** 2025-12-09",
        "",
        "---",
        "",
        "## Critical Priority",
        "",
        "### 1. Fix Per-Document TF-IDF Calculation Bug",
        "",
        "**File:** `cortical/analysis.py`",
        "**Line:** 131",
        "**Status:** [x] Completed"
      ],
      "change_type": "modify"
    },
    {
      "file": "TASK_LIST.md",
      "function": "Removed `Counter` from the import statement.",
      "start_line": 187,
      "lines_added": [
        "---",
        "",
        "# RAG System Enhancements",
        "",
        "The following tasks are required to transform the Cortical Text Processor into a production-ready RAG (Retrieval-Augmented Generation) system.",
        "",
        "---",
        "",
        "## RAG Critical Priority",
        "",
        "### 8. Implement Chunk-Level Retrieval",
        "",
        "**Files:** `cortical/processor.py`, `cortical/query.py`",
        "**Status:** [x] Completed",
        "",
        "**Problem:**",
        "Current retrieval returns only document IDs and scores. RAG systems need actual text passages with position information for context windows and citations.",
        "",
        "**Current Behavior:**",
        "```python",
        "results = processor.find_documents_for_query(\"neural networks\")",
        "# Returns: [(\"doc1\", 3.47), (\"doc2\", 2.15)]  # Just IDs!",
        "```",
        "",
        "**Required Behavior:**",
        "```python",
        "results = processor.find_passages_for_query(\"neural networks\")",
        "# Returns: [",
        "#   (\"Neural networks process information...\", \"doc1\", 1500, 2000, 3.47),",
        "#   (text, doc_id, start_char, end_char, score)",
        "# ]",
        "```",
        "",
        "**Implementation Steps:**",
        "1. Add `find_passages_for_query()` method to `processor.py`",
        "2. Add `_create_chunks()` helper for splitting documents with overlap",
        "3. Add `_score_tokens()` helper for chunk-level scoring",
        "4. Add corresponding function to `query.py` for standalone use",
        "5. Support configurable `chunk_size` (default 512) and `overlap` (default 128)",
        "",
        "**Files to Modify:**",
        "- `cortical/processor.py` - Add new methods (~50 lines)",
        "- `cortical/query.py` - Add standalone function (~40 lines)",
        "- `tests/test_processor.py` - Add tests for chunk retrieval",
        "- `tests/test_query.py` - Add tests for passage finding",
        "",
        "---",
        "",
        "### 9. Add Document Metadata Support",
        "",
        "**Files:** `cortical/processor.py`, `cortical/persistence.py`",
        "**Status:** [x] Completed",
        "",
        "**Problem:**",
        "No way to store or retrieve document metadata (source URL, timestamp, author, etc.). RAG systems need this for proper citations and filtering.",
        "",
        "**Current Data Model:**",
        "```python",
        "self.documents: Dict[str, str] = {}  # Only doc_id → text",
        "```",
        "",
        "**Required Data Model:**",
        "```python",
        "self.documents: Dict[str, str] = {}",
        "self.document_metadata: Dict[str, Dict[str, Any]] = {}",
        "# Stores: source, timestamp, author, category, custom fields",
        "```",
        "",
        "**Implementation Steps:**",
        "1. Add `document_metadata` dict to `CorticalTextProcessor.__init__()`",
        "2. Modify `process_document()` to accept optional `metadata` parameter",
        "3. Add `set_document_metadata()` and `get_document_metadata()` methods",
        "4. Update `persistence.py` to save/load metadata",
        "5. Increment state version to `2.1`",
        "",
        "**Files to Modify:**",
        "- `cortical/processor.py` - Add metadata storage and methods",
        "- `cortical/persistence.py` - Update save/load functions",
        "- `tests/test_persistence.py` - Add metadata persistence tests",
        "",
        "---",
        "",
        "## RAG High Priority",
        "",
        "### 10. Activate Layer 2 (Concept Clustering) by Default",
        "",
        "**Files:** `cortical/processor.py`, `cortical/query.py`",
        "**Status:** [x] Completed",
        "",
        "**Problem:**",
        "Layer 2 (Concepts) has clustering code but is never populated automatically. This layer could enable topic-based filtering and hierarchical search.",
        "",
        "**Current Behavior:**",
        "- `compute_all()` does NOT call `build_concept_clusters()`",
        "- Layer 2 remains empty with 0 minicolumns",
        "- Query expansion code checks Layer 2 but finds nothing",
        "",
        "**Implementation Steps:**",
        "1. Add `build_concepts: bool = True` parameter to `compute_all()`",
        "2. Call `build_concept_clusters()` when enabled",
        "3. Update query expansion to use concepts when available",
        "4. Add concept-based document filtering option",
        "",
        "**Files to Modify:**",
        "- `cortical/processor.py` - Update `compute_all()` (~10 lines)",
        "- `cortical/query.py` - Enhance expansion logic (~20 lines)",
        "",
        "---",
        "",
        "### 11. Integrate Semantic Relations into Retrieval",
        "",
        "**Files:** `cortical/query.py`",
        "**Status:** [x] Completed",
        "",
        "**Problem:**",
        "`semantics.py` extracts relations (IsA, PartOf, RelatedTo, etc.) but they're only used for retrofitting embeddings, not for query expansion or retrieval.",
        "",
        "**Current State:**",
        "- `expand_query_semantic()` exists in `query.py` (lines 127-174)",
        "- This function is NEVER called by `find_documents_for_query()`",
        "- Semantic relations are computed but ignored during search",
        "",
        "**Implementation Steps:**",
        "1. Add `use_semantic: bool = True` parameter to `find_documents_for_query()`",
        "2. Call `expand_query_semantic()` when semantic relations exist",
        "3. Combine lateral connection expansion with semantic expansion",
        "4. Weight semantic expansions appropriately",
        "",
        "**Files to Modify:**",
        "- `cortical/query.py` - Integrate semantic expansion (~15 lines)",
        "",
        "---",
        "",
        "### 12. Persist Full Computed State",
        "",
        "**Files:** `cortical/persistence.py`, `cortical/processor.py`",
        "**Status:** [x] Completed",
        "",
        "**Problem:**",
        "Embeddings, semantic relations, and concept clusters are not saved. Loading a model requires expensive recomputation.",
        "",
        "**Currently Saved:**",
        "- Layers (tokens, bigrams, documents)",
        "- Document text",
        "- Generic metadata",
        "",
        "**NOT Saved (must recompute):**",
        "- `semantic_relations` - extracted IsA, PartOf, etc.",
        "- `embeddings` - graph embeddings for all terms",
        "- Concept clusters in Layer 2",
        "",
        "**Implementation Steps:**",
        "1. Add `semantic_relations` to save state",
        "2. Add `embeddings` to save state",
        "3. Update `load_processor()` to restore these fields",
        "4. Increment state version to `2.1`",
        "5. Handle backward compatibility with v2.0 files",
        "",
        "**Files to Modify:**",
        "- `cortical/persistence.py` - Update save/load (~30 lines)",
        "- `cortical/processor.py` - Update save/load methods",
        "",
        "---",
        "",
        "## RAG Medium Priority",
        "",
        "### 13. Fix Remaining Type Annotation",
        "",
        "**File:** `cortical/embeddings.py`",
        "**Line:** 26",
        "**Status:** [x] Completed",
        "",
        "**Problem:**",
        "```python",
        "# Current (incorrect):",
        ") -> Tuple[Dict[str, List[float]], Dict[str, any]]:",
        "",
        "# Should be:",
        ") -> Tuple[Dict[str, List[float]], Dict[str, Any]]:",
        "```",
        "",
        "**Implementation:** Single line fix, add `Any` to imports.",
        "",
        "---",
        "",
        "### 14. Optimize Spectral Embeddings Lookup",
        "",
        "**File:** `cortical/embeddings.py`",
        "**Lines:** 151-156",
        "**Status:** [x] Completed",
        "",
        "**Problem:**",
        "Spectral embeddings use O(n) linear search instead of O(1) `get_by_id()`:",
        "```python",
        "# Current (slow):",
        "for t, c in layer.minicolumns.items():",
        "    if c.id == neighbor_id:",
        "        ...",
        "",
        "# Should use:",
        "neighbor = layer.get_by_id(neighbor_id)",
        "```",
        "",
        "---",
        "",
        "### 15. Add Incremental Document Indexing",
        "",
        "**File:** `cortical/processor.py`",
        "**Status:** [ ] Not Started",
        "",
        "**Problem:**",
        "Adding a document requires calling `compute_all()` which recomputes everything. For RAG systems with frequent updates, this is inefficient.",
        "",
        "**Implementation Steps:**",
        "1. Add `add_document_incremental()` method",
        "2. Support selective recomputation (TF-IDF only, or full)",
        "3. Track which computations are stale",
        "4. Allow batch updates with single recomputation",
        "",
        "**Files to Modify:**",
        "- `cortical/processor.py` - Add incremental method (~40 lines)",
        "",
        "---",
        "",
        "## RAG Low Priority",
        "",
        "### 16. Document Magic Numbers in Gap Detection",
        "",
        "**File:** `cortical/gaps.py`",
        "**Lines:** 62, 76, 99",
        "**Status:** [ ] Deferred (carried over)",
        "",
        "**Magic Numbers:**",
        "- `avg_sim < 0.02` - isolation threshold",
        "- `tfidf > 0.005` - weak topic threshold",
        "- `0.005 < sim < 0.03` - bridge opportunity range",
        "",
        "**Implementation:** Add docstrings or make configurable parameters.",
        "",
        "---",
        "",
        "### 17. Add Multi-Stage Ranking Pipeline",
        "**Files:** `cortical/query.py`",
        "**Status:** [ ] Future Enhancement",
        "",
        "**Problem:**",
        "Current ranking is flat (Token TF-IDF → Document Score). Better RAG performance with staged ranking:",
        "",
        "1. **Stage 1 (Concepts):** Filter by topic relevance",
        "2. **Stage 2 (Documents):** Rank documents in topic",
        "3. **Stage 3 (Chunks):** Rank passages in documents",
        "4. **Stage 4 (Rerank):** Final relevance scoring",
        "",
        "---",
        "",
        "### 18. Add Batch Query API",
        "",
        "**Files:** `cortical/query.py`, `cortical/processor.py`",
        "**Status:** [ ] Future Enhancement",
        "",
        "**Problem:**",
        "No efficient way to run multiple queries. Each query repeats tokenization and expansion.",
        "",
        "**Implementation:**",
        "```python",
        "def find_documents_batch(self, queries: List[str], top_n: int = 5):",
        "    \"\"\"Process multiple queries efficiently.\"\"\"",
        "    # Batch tokenization",
        "    # Shared expansion cache",
        "    # Parallel scoring",
        "```",
        "",
        "---",
        "",
        "## Summary",
        "| Priority | Task | Status | Category |",
        "|----------|------|--------|----------|",
        "| Critical | Fix TF-IDF per-doc calculation | ✅ Completed | Bug Fix |",
        "| High | Add ID lookup optimization | ✅ Completed | Bug Fix |",
        "| Medium | Fix type annotations (semantics.py) | ✅ Completed | Bug Fix |",
        "| Medium | Remove unused import | ✅ Completed | Bug Fix |",
        "| Medium | Add verbose parameter | ✅ Completed | Bug Fix |",
        "| Low | Add test coverage | ✅ Completed | Bug Fix |",
        "| **Critical** | **Implement chunk-level retrieval** | ✅ Completed | **RAG** |",
        "| **Critical** | **Add document metadata support** | ✅ Completed | **RAG** |",
        "| **High** | **Activate Layer 2 concepts** | ✅ Completed | **RAG** |",
        "| **High** | **Integrate semantic relations** | ✅ Completed | **RAG** |",
        "| **High** | **Persist full computed state** | ✅ Completed | **RAG** |",
        "| Medium | Fix type annotation (embeddings.py) | ✅ Completed | Bug Fix |",
        "| Medium | Optimize spectral embeddings | ✅ Completed | Performance |",
        "| Medium | Add incremental indexing | ⬜ Not Started | RAG |",
        "| Low | Document magic numbers | ⏳ Deferred | Documentation |",
        "| Low | Multi-stage ranking pipeline | ⬜ Future | RAG |",
        "| Low | Batch query API | ⬜ Future | RAG |",
        "",
        "**Bug Fix Completion:** 7/7 tasks (100%)",
        "**RAG Enhancement Completion:** 5/8 tasks (63%)",
        "Ran 129 tests in 0.152s"
      ],
      "lines_removed": [
        "## Summary",
        "| Priority | Task | Status |",
        "|----------|------|--------|",
        "| Critical | Fix TF-IDF per-doc calculation | ✅ Completed |",
        "| High | Add ID lookup optimization | ✅ Completed |",
        "| Medium | Fix type annotations | ✅ Completed |",
        "| Medium | Remove unused import | ✅ Completed |",
        "| Medium | Add verbose parameter | ✅ Completed |",
        "| Low | Add test coverage | ✅ Completed |",
        "| Low | Document magic numbers | ⏳ Deferred |",
        "**Completion Rate:** 6/7 tasks (86%)",
        "Ran 109 tests in 0.131s"
      ],
      "context_before": [
        "### 7. Document Magic Numbers",
        "",
        "**File:** `cortical/gaps.py`",
        "**Lines:** 62, 76, 99",
        "**Status:** [ ] Deferred",
        "",
        "**Note:** This task remains as a future enhancement. The magic numbers are functional but could benefit from documentation or configuration options.",
        "",
        "---",
        ""
      ],
      "context_after": [
        "",
        "",
        "",
        "---",
        "",
        "## Test Results",
        "",
        "```",
        "OK",
        "```",
        "",
        "All tests passing as of 2025-12-09.",
        "",
        "---",
        "",
        "*Updated from code review on 2025-12-09*"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/embeddings.py",
      "function": "Graph-based embeddings for the cortical network.",
      "start_line": 6,
      "lines_added": [
        "from typing import Any, Dict, List, Tuple, Optional",
        ") -> Tuple[Dict[str, List[float]], Dict[str, Any]]:"
      ],
      "lines_removed": [
        "from typing import Dict, List, Tuple, Optional",
        ") -> Tuple[Dict[str, List[float]], Dict[str, any]]:"
      ],
      "context_before": [
        "",
        "Implements three methods for computing term embeddings from the",
        "connection graph structure:",
        "1. Adjacency: Direct connection weights to landmark nodes",
        "2. Random Walk: DeepWalk-inspired walk co-occurrence",
        "3. Spectral: Graph Laplacian eigenvector approximation",
        "\"\"\"",
        "",
        "import math",
        "import random"
      ],
      "context_after": [
        "from collections import defaultdict",
        "",
        "from .layers import CorticalLayer, HierarchicalLayer",
        "",
        "",
        "def compute_graph_embeddings(",
        "    layers: Dict[CorticalLayer, HierarchicalLayer],",
        "    dimensions: int = 64,",
        "    method: str = 'adjacency'",
        "    \"\"\"",
        "    Compute embeddings for tokens based on graph structure.",
        "    ",
        "    Args:",
        "        layers: Dictionary of layers (needs TOKENS)",
        "        dimensions: Number of embedding dimensions",
        "        method: 'adjacency', 'random_walk', or 'spectral'",
        "        ",
        "    Returns:",
        "        Tuple of (embeddings dict, statistics dict)"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/embeddings.py",
      "function": "def _spectral_embeddings(layer: HierarchicalLayer, dimensions: int, iterations:",
      "start_line": 141,
      "lines_added": [
        "            neighbor = layer.get_by_id(neighbor_id)",
        "            if neighbor and neighbor.content in term_to_idx:",
        "                j = term_to_idx[neighbor.content]",
        "                adjacency[i][j] = weight",
        "                degrees[i] += weight"
      ],
      "lines_removed": [
        "            for t, c in layer.minicolumns.items():",
        "                if c.id == neighbor_id:",
        "                    j = term_to_idx[t]",
        "                    adjacency[i][j] = weight",
        "                    degrees[i] += weight",
        "                    break"
      ],
      "context_before": [
        "    if n == 0:",
        "        return embeddings",
        "    ",
        "    term_to_idx = {t: i for i, t in enumerate(terms)}",
        "    adjacency: Dict[int, Dict[int, float]] = defaultdict(dict)",
        "    degrees = [0.0] * n",
        "    ",
        "    for term, col in layer.minicolumns.items():",
        "        i = term_to_idx[term]",
        "        for neighbor_id, weight in col.lateral_connections.items():"
      ],
      "context_after": [
        "    ",
        "    degrees = [d if d > 0 else 1.0 for d in degrees]",
        "    actual_dims = min(dimensions, n)",
        "    vectors = []",
        "    ",
        "    for d in range(actual_dims):",
        "        vec = [random.gauss(0, 1) for _ in range(n)]",
        "        for prev in vectors:",
        "            dot = sum(v * p for v, p in zip(vec, prev))",
        "            vec = [v - dot * p for v, p in zip(vec, prev)]"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/persistence.py",
      "function": "import os",
      "start_line": 16,
      "lines_added": [
        "    document_metadata: Optional[Dict[str, Dict[str, Any]]] = None,",
        "    embeddings: Optional[Dict[str, list]] = None,",
        "    semantic_relations: Optional[list] = None,",
        "",
        "        document_metadata: Per-document metadata (source, timestamp, etc.)",
        "        embeddings: Graph embeddings for terms (optional)",
        "        semantic_relations: Extracted semantic relations (optional)",
        "        metadata: Optional processor metadata (version, settings, etc.)",
        "        'version': '2.2',",
        "        'document_metadata': document_metadata or {},",
        "        'embeddings': embeddings or {},",
        "        'semantic_relations': semantic_relations or [],",
        "",
        "",
        "",
        "        if embeddings:",
        "            print(f\"  - {len(embeddings)} embeddings\")",
        "        if semantic_relations:",
        "            print(f\"  - {len(semantic_relations)} semantic relations\")",
        "",
        "",
        "        Tuple of (layers, documents, document_metadata, embeddings, semantic_relations, metadata)",
        "",
        "",
        "    document_metadata = state.get('document_metadata', {})",
        "    embeddings = state.get('embeddings', {})",
        "    semantic_relations = state.get('semantic_relations', [])",
        "",
        "        if embeddings:",
        "            print(f\"  - {len(embeddings)} embeddings\")",
        "        if semantic_relations:",
        "            print(f\"  - {len(semantic_relations)} semantic relations\")",
        "",
        "    return layers, documents, document_metadata, embeddings, semantic_relations, metadata"
      ],
      "lines_removed": [
        "    ",
        "        metadata: Optional metadata (version, settings, etc.)",
        "        'version': '2.0',",
        "    ",
        "    ",
        "    ",
        "    ",
        "        ",
        "        Tuple of (layers, documents, metadata)",
        "    ",
        "    ",
        "    ",
        "    ",
        "    return layers, documents, metadata"
      ],
      "context_before": [
        "from typing import Dict, Optional, Any",
        "",
        "from .layers import CorticalLayer, HierarchicalLayer",
        "from .minicolumn import Minicolumn",
        "",
        "",
        "def save_processor(",
        "    filepath: str,",
        "    layers: Dict[CorticalLayer, HierarchicalLayer],",
        "    documents: Dict[str, str],"
      ],
      "context_after": [
        "    metadata: Optional[Dict] = None,",
        "    verbose: bool = True",
        ") -> None:",
        "    \"\"\"",
        "    Save processor state to a file.",
        "    Args:",
        "        filepath: Path to save file",
        "        layers: Dictionary of all layers",
        "        documents: Document collection",
        "        verbose: Print progress",
        "    \"\"\"",
        "    state = {",
        "        'layers': {},",
        "        'documents': documents,",
        "        'metadata': metadata or {}",
        "    }",
        "    # Serialize layers",
        "    for layer_enum, layer in layers.items():",
        "        state['layers'][layer_enum.value] = layer.to_dict()",
        "    with open(filepath, 'wb') as f:",
        "        pickle.dump(state, f, protocol=pickle.HIGHEST_PROTOCOL)",
        "    if verbose:",
        "        total_cols = sum(len(layer.minicolumns) for layer in layers.values())",
        "        total_conns = sum(layer.total_connections() for layer in layers.values())",
        "        print(f\"✓ Saved processor to {filepath}\")",
        "        print(f\"  - {len(documents)} documents\")",
        "        print(f\"  - {total_cols} minicolumns\")",
        "        print(f\"  - {total_conns} connections\")",
        "",
        "",
        "def load_processor(",
        "    filepath: str,",
        "    verbose: bool = True",
        ") -> tuple:",
        "    \"\"\"",
        "    Load processor state from a file.",
        "    Args:",
        "        filepath: Path to saved file",
        "        verbose: Print progress",
        "    Returns:",
        "    \"\"\"",
        "    with open(filepath, 'rb') as f:",
        "        state = pickle.load(f)",
        "    # Reconstruct layers",
        "    layers = {}",
        "    for level_value, layer_data in state.get('layers', {}).items():",
        "        layer = HierarchicalLayer.from_dict(layer_data)",
        "        layers[CorticalLayer(int(level_value))] = layer",
        "    documents = state.get('documents', {})",
        "    metadata = state.get('metadata', {})",
        "    if verbose:",
        "        total_cols = sum(len(layer.minicolumns) for layer in layers.values())",
        "        total_conns = sum(layer.total_connections() for layer in layers.values())",
        "        print(f\"✓ Loaded processor from {filepath}\")",
        "        print(f\"  - {len(documents)} documents\")",
        "        print(f\"  - {total_cols} minicolumns\")",
        "        print(f\"  - {total_conns} connections\")",
        "",
        "",
        "def export_graph_json(",
        "    filepath: str,",
        "    layers: Dict[CorticalLayer, HierarchicalLayer],",
        "    layer_filter: Optional[CorticalLayer] = None,",
        "    min_weight: float = 0.0,",
        "    max_nodes: int = 500,",
        "    verbose: bool = True",
        ") -> Dict:"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/processor.py",
      "function": "from .layers import CorticalLayer, HierarchicalLayer",
      "start_line": 13,
      "lines_added": [
        "",
        "        self.document_metadata: Dict[str, Dict[str, Any]] = {}",
        "",
        "    def process_document(",
        "        self,",
        "        doc_id: str,",
        "        content: str,",
        "        metadata: Optional[Dict[str, Any]] = None",
        "    ) -> Dict[str, int]:",
        "        \"\"\"",
        "        Process a document and add it to the corpus.",
        "",
        "        Args:",
        "            doc_id: Unique identifier for the document",
        "            content: Document text content",
        "            metadata: Optional metadata dict (source, timestamp, author, etc.)",
        "",
        "        Returns:",
        "            Dict with processing statistics (tokens, bigrams, unique_tokens)",
        "        \"\"\"",
        "",
        "        # Store metadata if provided",
        "        if metadata:",
        "            self.document_metadata[doc_id] = metadata.copy()",
        "        elif doc_id not in self.document_metadata:",
        "            self.document_metadata[doc_id] = {}",
        ""
      ],
      "lines_removed": [
        "    ",
        "    ",
        "    def process_document(self, doc_id: str, content: str) -> Dict[str, int]:",
        "        \"\"\"Process a document and add it to the corpus.\"\"\""
      ],
      "context_before": [
        "from . import analysis",
        "from . import semantics",
        "from . import embeddings as emb_module",
        "from . import query as query_module",
        "from . import gaps as gaps_module",
        "from . import persistence",
        "",
        "",
        "class CorticalTextProcessor:",
        "    \"\"\"Neocortex-inspired text processing system.\"\"\""
      ],
      "context_after": [
        "    def __init__(self, tokenizer: Optional[Tokenizer] = None):",
        "        self.tokenizer = tokenizer or Tokenizer()",
        "        self.layers: Dict[CorticalLayer, HierarchicalLayer] = {",
        "            CorticalLayer.TOKENS: HierarchicalLayer(CorticalLayer.TOKENS),",
        "            CorticalLayer.BIGRAMS: HierarchicalLayer(CorticalLayer.BIGRAMS),",
        "            CorticalLayer.CONCEPTS: HierarchicalLayer(CorticalLayer.CONCEPTS),",
        "            CorticalLayer.DOCUMENTS: HierarchicalLayer(CorticalLayer.DOCUMENTS),",
        "        }",
        "        self.documents: Dict[str, str] = {}",
        "        self.embeddings: Dict[str, List[float]] = {}",
        "        self.semantic_relations: List[Tuple[str, str, str, float]] = []",
        "        self.documents[doc_id] = content",
        "        tokens = self.tokenizer.tokenize(content)",
        "        bigrams = self.tokenizer.extract_ngrams(tokens, n=2)",
        "        ",
        "        layer0 = self.layers[CorticalLayer.TOKENS]",
        "        layer1 = self.layers[CorticalLayer.BIGRAMS]",
        "        layer3 = self.layers[CorticalLayer.DOCUMENTS]",
        "        ",
        "        doc_col = layer3.get_or_create_minicolumn(doc_id)",
        "        doc_col.occurrence_count += 1",
        "        "
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/processor.py",
      "function": "class CorticalTextProcessor:",
      "start_line": 68,
      "lines_added": [
        "",
        "    def set_document_metadata(self, doc_id: str, **kwargs) -> None:",
        "        \"\"\"",
        "        Set or update metadata for a document.",
        "",
        "        Args:",
        "            doc_id: Document identifier",
        "            **kwargs: Metadata key-value pairs to set",
        "",
        "        Example:",
        "            >>> processor.set_document_metadata(\"doc1\",",
        "            ...     source=\"https://example.com\",",
        "            ...     author=\"John Doe\",",
        "            ...     timestamp=\"2025-12-09\"",
        "            ... )",
        "        \"\"\"",
        "        if doc_id not in self.document_metadata:",
        "            self.document_metadata[doc_id] = {}",
        "        self.document_metadata[doc_id].update(kwargs)",
        "",
        "    def get_document_metadata(self, doc_id: str) -> Dict[str, Any]:",
        "        \"\"\"",
        "        Get metadata for a document.",
        "",
        "        Args:",
        "            doc_id: Document identifier",
        "",
        "        Returns:",
        "            Metadata dict (empty dict if no metadata set)",
        "        \"\"\"",
        "        return self.document_metadata.get(doc_id, {})",
        "",
        "    def get_all_document_metadata(self) -> Dict[str, Dict[str, Any]]:",
        "        \"\"\"",
        "        Get metadata for all documents.",
        "",
        "        Returns:",
        "            Dict mapping doc_id to metadata dict (deep copy)",
        "        \"\"\"",
        "        import copy",
        "        return copy.deepcopy(self.document_metadata)",
        "",
        "    def compute_all(self, verbose: bool = True, build_concepts: bool = True) -> None:",
        "        \"\"\"",
        "        Run all computation steps.",
        "",
        "        Args:",
        "            verbose: Print progress messages",
        "            build_concepts: Build concept clusters in Layer 2 (default True)",
        "                           This enables topic-based filtering and hierarchical search.",
        "        \"\"\"",
        "        if verbose:",
        "            print(\"Computing activation propagation...\")",
        "        if verbose:",
        "            print(\"Computing importance (PageRank)...\")",
        "        if verbose:",
        "            print(\"Computing TF-IDF...\")",
        "        if verbose:",
        "            print(\"Computing document connections...\")",
        "        if build_concepts:",
        "            if verbose:",
        "                print(\"Building concept clusters...\")",
        "            self.build_concept_clusters(verbose=False)",
        "        if verbose:",
        "            print(\"Done.\")"
      ],
      "lines_removed": [
        "    ",
        "    def compute_all(self, verbose: bool = True) -> None:",
        "        \"\"\"Run all computation steps.\"\"\"",
        "        if verbose: print(\"Computing activation propagation...\")",
        "        if verbose: print(\"Computing importance (PageRank)...\")",
        "        if verbose: print(\"Computing TF-IDF...\")",
        "        if verbose: print(\"Computing document connections...\")",
        "        if verbose: print(\"Done.\")"
      ],
      "context_before": [
        "            col = layer1.get_or_create_minicolumn(bigram)",
        "            col.occurrence_count += 1",
        "            col.document_ids.add(doc_id)",
        "            col.activation += 1.0",
        "            for part in bigram.split():",
        "                token_col = layer0.get_minicolumn(part)",
        "                if token_col:",
        "                    col.feedforward_sources.add(token_col.id)",
        "        ",
        "        return {'tokens': len(tokens), 'bigrams': len(bigrams), 'unique_tokens': len(set(tokens))}"
      ],
      "context_after": [
        "        self.propagate_activation(verbose=False)",
        "        self.compute_importance(verbose=False)",
        "        self.compute_tfidf(verbose=False)",
        "        self.compute_document_connections(verbose=False)",
        "    ",
        "    def propagate_activation(self, iterations: int = 3, decay: float = 0.8, verbose: bool = True) -> None:",
        "        analysis.propagate_activation(self.layers, iterations, decay)",
        "        if verbose: print(f\"Propagated activation ({iterations} iterations)\")",
        "    ",
        "    def compute_importance(self, verbose: bool = True) -> None:",
        "        for layer_enum in [CorticalLayer.TOKENS, CorticalLayer.BIGRAMS]:",
        "            analysis.compute_pagerank(self.layers[layer_enum])",
        "        if verbose: print(\"Computed PageRank importance\")",
        "    "
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/processor.py",
      "function": "class CorticalTextProcessor:",
      "start_line": 139,
      "lines_added": [
        "    def find_documents_for_query(",
        "        self,",
        "        query_text: str,",
        "        top_n: int = 5,",
        "        use_expansion: bool = True,",
        "        use_semantic: bool = True",
        "    ) -> List[Tuple[str, float]]:",
        "        \"\"\"",
        "        Find documents most relevant to a query.",
        "",
        "        Args:",
        "            query_text: Search query",
        "            top_n: Number of documents to return",
        "            use_expansion: Whether to expand query terms using lateral connections",
        "            use_semantic: Whether to use semantic relations for expansion (if available)",
        "",
        "        Returns:",
        "            List of (doc_id, score) tuples ranked by relevance",
        "        \"\"\"",
        "        return query_module.find_documents_for_query(",
        "            query_text,",
        "            self.layers,",
        "            self.tokenizer,",
        "            top_n=top_n,",
        "            use_expansion=use_expansion,",
        "            semantic_relations=self.semantic_relations if use_semantic else None,",
        "            use_semantic=use_semantic",
        "        )",
        "",
        "    def find_passages_for_query(",
        "        self,",
        "        query_text: str,",
        "        top_n: int = 5,",
        "        chunk_size: int = 512,",
        "        overlap: int = 128,",
        "        use_expansion: bool = True,",
        "        doc_filter: Optional[List[str]] = None,",
        "        use_semantic: bool = True",
        "    ) -> List[Tuple[str, str, int, int, float]]:",
        "        \"\"\"",
        "        Find text passages most relevant to a query (for RAG systems).",
        "",
        "        Instead of returning just document IDs, this returns actual text passages",
        "        with position information suitable for context windows and citations.",
        "",
        "        Args:",
        "            query_text: Search query",
        "            top_n: Number of passages to return",
        "            chunk_size: Size of each chunk in characters (default 512)",
        "            overlap: Overlap between chunks in characters (default 128)",
        "            use_expansion: Whether to expand query terms",
        "            doc_filter: Optional list of doc_ids to restrict search to",
        "            use_semantic: Whether to use semantic relations for expansion (if available)",
        "",
        "        Returns:",
        "            List of (passage_text, doc_id, start_char, end_char, score) tuples",
        "            ranked by relevance",
        "",
        "        Example:",
        "            >>> results = processor.find_passages_for_query(\"neural networks\")",
        "            >>> for passage, doc_id, start, end, score in results:",
        "            ...     print(f\"[{doc_id}:{start}-{end}] {passage[:50]}... (score: {score:.3f})\")",
        "        \"\"\"",
        "        return query_module.find_passages_for_query(",
        "            query_text,",
        "            self.layers,",
        "            self.tokenizer,",
        "            self.documents,",
        "            top_n=top_n,",
        "            chunk_size=chunk_size,",
        "            overlap=overlap,",
        "            use_expansion=use_expansion,",
        "            doc_filter=doc_filter,",
        "            semantic_relations=self.semantic_relations if use_semantic else None,",
        "            use_semantic=use_semantic",
        "        )"
      ],
      "lines_removed": [
        "    def find_documents_for_query(self, query_text: str, top_n: int = 5, use_expansion: bool = True) -> List[Tuple[str, float]]:",
        "        return query_module.find_documents_for_query(query_text, self.layers, self.tokenizer, top_n, use_expansion)"
      ],
      "context_before": [
        "    ",
        "    def find_similar_by_embedding(self, term: str, top_n: int = 10) -> List[Tuple[str, float]]:",
        "        return emb_module.find_similar_by_embedding(self.embeddings, term, top_n)",
        "    ",
        "    def expand_query(self, query_text: str, max_expansions: int = 10, use_variants: bool = True, verbose: bool = False) -> Dict[str, float]:",
        "        return query_module.expand_query(query_text, self.layers, self.tokenizer, max_expansions=max_expansions, use_variants=use_variants)",
        "    ",
        "    def expand_query_semantic(self, query_text: str, max_expansions: int = 10) -> Dict[str, float]:",
        "        return query_module.expand_query_semantic(query_text, self.layers, self.tokenizer, self.semantic_relations, max_expansions)",
        "    "
      ],
      "context_after": [
        "    ",
        "    def query_expanded(self, query_text: str, top_n: int = 10, max_expansions: int = 8) -> List[Tuple[str, float]]:",
        "        return query_module.query_with_spreading_activation(query_text, self.layers, self.tokenizer, top_n, max_expansions)",
        "    ",
        "    def find_related_documents(self, doc_id: str) -> List[Tuple[str, float]]:",
        "        return query_module.find_related_documents(doc_id, self.layers)",
        "    ",
        "    def analyze_knowledge_gaps(self) -> Dict:",
        "        return gaps_module.analyze_knowledge_gaps(self.layers, self.documents)",
        "    "
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/processor.py",
      "function": "class CorticalTextProcessor:",
      "start_line": 167,
      "lines_added": [
        "        \"\"\"",
        "        Save processor state to a file.",
        "",
        "        Saves all computed state including embeddings and semantic relations,",
        "        so they don't need to be recomputed when loading.",
        "        \"\"\"",
        "        metadata = {",
        "            'has_embeddings': bool(self.embeddings),",
        "            'has_relations': bool(self.semantic_relations)",
        "        }",
        "        persistence.save_processor(",
        "            filepath,",
        "            self.layers,",
        "            self.documents,",
        "            self.document_metadata,",
        "            self.embeddings,",
        "            self.semantic_relations,",
        "            metadata,",
        "            verbose",
        "        )",
        "",
        "        \"\"\"",
        "        Load processor state from a file.",
        "",
        "        Restores all computed state including embeddings and semantic relations.",
        "        \"\"\"",
        "        result = persistence.load_processor(filepath, verbose)",
        "        layers, documents, document_metadata, embeddings, semantic_relations, metadata = result",
        "        processor.document_metadata = document_metadata",
        "        processor.embeddings = embeddings",
        "        processor.semantic_relations = semantic_relations"
      ],
      "lines_removed": [
        "        metadata = {'has_embeddings': bool(self.embeddings), 'has_relations': bool(self.semantic_relations)}",
        "        persistence.save_processor(filepath, self.layers, self.documents, metadata, verbose)",
        "    ",
        "        layers, documents, metadata = persistence.load_processor(filepath, verbose)"
      ],
      "context_before": [
        "    def get_document_signature(self, doc_id: str, n: int = 10) -> List[Tuple[str, float]]:",
        "        layer0 = self.layers[CorticalLayer.TOKENS]",
        "        terms = [(col.content, col.tfidf_per_doc.get(doc_id, col.tfidf)) ",
        "                 for col in layer0.minicolumns.values() if doc_id in col.document_ids]",
        "        return sorted(terms, key=lambda x: x[1], reverse=True)[:n]",
        "    ",
        "    def get_corpus_summary(self) -> Dict:",
        "        return persistence.get_state_summary(self.layers, self.documents)",
        "    ",
        "    def save(self, filepath: str, verbose: bool = True) -> None:"
      ],
      "context_after": [
        "    @classmethod",
        "    def load(cls, filepath: str, verbose: bool = True) -> 'CorticalTextProcessor':",
        "        processor = cls()",
        "        processor.layers = layers",
        "        processor.documents = documents",
        "        return processor",
        "    ",
        "    def export_graph(self, filepath: str, layer: Optional[CorticalLayer] = None, max_nodes: int = 500) -> Dict:",
        "        return persistence.export_graph_json(filepath, self.layers, layer, max_nodes=max_nodes)",
        "    ",
        "    def summarize_document(self, doc_id: str, num_sentences: int = 3) -> str:",
        "        if doc_id not in self.documents: return \"\"",
        "        content = self.documents[doc_id]",
        "        sentences = re.split(r'(?<=[.!?])\\s+', content)",
        "        if len(sentences) <= num_sentences: return content"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/query.py",
      "function": "def expand_query_semantic(",
      "start_line": 172,
      "lines_added": [
        "    use_expansion: bool = True,",
        "    semantic_relations: Optional[List[Tuple[str, str, str, float]]] = None,",
        "    use_semantic: bool = True",
        "",
        "        use_expansion: Whether to expand query terms using lateral connections",
        "        semantic_relations: Optional list of semantic relations for expansion",
        "        use_semantic: Whether to use semantic relations for expansion (if available)",
        "",
        "",
        "        # Start with lateral connection expansion",
        "",
        "        # Add semantic expansion if available",
        "        if use_semantic and semantic_relations:",
        "            semantic_terms = expand_query_semantic(",
        "                query_text, layers, tokenizer, semantic_relations, max_expansions=5",
        "            )",
        "            # Merge semantic expansions (don't override stronger weights)",
        "            for term, weight in semantic_terms.items():",
        "                if term not in query_terms:",
        "                    query_terms[term] = weight * 0.8  # Slightly discount semantic expansions",
        "                else:",
        "                    # Take the max weight",
        "                    query_terms[term] = max(query_terms[term], weight * 0.8)",
        "",
        "",
        ""
      ],
      "lines_removed": [
        "    use_expansion: bool = True",
        "    ",
        "        use_expansion: Whether to expand query terms",
        "        ",
        "    ",
        "    ",
        "    ",
        "    "
      ],
      "context_before": [
        "        expanded[term] = score",
        "    ",
        "    return expanded",
        "",
        "",
        "def find_documents_for_query(",
        "    query_text: str,",
        "    layers: Dict[CorticalLayer, HierarchicalLayer],",
        "    tokenizer: Tokenizer,",
        "    top_n: int = 5,"
      ],
      "context_after": [
        ") -> List[Tuple[str, float]]:",
        "    \"\"\"",
        "    Find documents most relevant to a query using TF-IDF and optional expansion.",
        "    Args:",
        "        query_text: Search query",
        "        layers: Dictionary of layers",
        "        tokenizer: Tokenizer instance",
        "        top_n: Number of documents to return",
        "    Returns:",
        "        List of (doc_id, score) tuples ranked by relevance",
        "    \"\"\"",
        "    layer0 = layers[CorticalLayer.TOKENS]",
        "    if use_expansion:",
        "        query_terms = expand_query(query_text, layers, tokenizer, max_expansions=5)",
        "    else:",
        "        tokens = tokenizer.tokenize(query_text)",
        "        query_terms = {t: 1.0 for t in tokens}",
        "    # Score each document",
        "    doc_scores: Dict[str, float] = defaultdict(float)",
        "    for term, term_weight in query_terms.items():",
        "        col = layer0.get_minicolumn(term)",
        "        if col:",
        "            for doc_id in col.document_ids:",
        "                tfidf = col.tfidf_per_doc.get(doc_id, col.tfidf)",
        "                doc_scores[doc_id] += tfidf * term_weight",
        "    sorted_docs = sorted(doc_scores.items(), key=lambda x: -x[1])",
        "    return sorted_docs[:top_n]",
        "",
        "",
        "def query_with_spreading_activation(",
        "    query_text: str,",
        "    layers: Dict[CorticalLayer, HierarchicalLayer],",
        "    tokenizer: Tokenizer,",
        "    top_n: int = 10,",
        "    max_expansions: int = 8"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/query.py",
      "function": "def query_with_spreading_activation(",
      "start_line": 268,
      "lines_added": [
        "",
        "",
        "",
        "",
        "",
        "",
        "def create_chunks(",
        "    text: str,",
        "    chunk_size: int = 512,",
        "    overlap: int = 128",
        ") -> List[Tuple[str, int, int]]:",
        "    \"\"\"",
        "    Split text into overlapping chunks.",
        "",
        "    Args:",
        "        text: Document text to chunk",
        "        chunk_size: Target size of each chunk in characters",
        "        overlap: Number of overlapping characters between chunks",
        "",
        "    Returns:",
        "        List of (chunk_text, start_char, end_char) tuples",
        "    \"\"\"",
        "    if not text:",
        "        return []",
        "",
        "    chunks = []",
        "    stride = max(1, chunk_size - overlap)",
        "    text_len = len(text)",
        "",
        "    for start in range(0, text_len, stride):",
        "        end = min(start + chunk_size, text_len)",
        "        chunk = text[start:end]",
        "        chunks.append((chunk, start, end))",
        "",
        "        if end >= text_len:",
        "            break",
        "",
        "    return chunks",
        "",
        "",
        "def score_chunk(",
        "    chunk_text: str,",
        "    query_terms: Dict[str, float],",
        "    layer0: HierarchicalLayer,",
        "    tokenizer: Tokenizer,",
        "    doc_id: Optional[str] = None",
        ") -> float:",
        "    \"\"\"",
        "    Score a chunk against query terms using TF-IDF.",
        "",
        "    Args:",
        "        chunk_text: Text of the chunk",
        "        query_terms: Dict mapping query terms to weights",
        "        layer0: Token layer for TF-IDF lookups",
        "        tokenizer: Tokenizer instance",
        "        doc_id: Optional document ID for per-document TF-IDF",
        "",
        "    Returns:",
        "        Relevance score for the chunk",
        "    \"\"\"",
        "    chunk_tokens = tokenizer.tokenize(chunk_text)",
        "    if not chunk_tokens:",
        "        return 0.0",
        "",
        "    # Count token occurrences in chunk",
        "    token_counts: Dict[str, int] = {}",
        "    for token in chunk_tokens:",
        "        token_counts[token] = token_counts.get(token, 0) + 1",
        "",
        "    score = 0.0",
        "    for term, term_weight in query_terms.items():",
        "        if term in token_counts:",
        "            col = layer0.get_minicolumn(term)",
        "            if col:",
        "                # Use per-document TF-IDF if available, otherwise global",
        "                tfidf = col.tfidf_per_doc.get(doc_id, col.tfidf) if doc_id else col.tfidf",
        "                # Weight by occurrence in chunk and query weight",
        "                score += tfidf * token_counts[term] * term_weight",
        "",
        "    # Normalize by chunk length to avoid bias toward longer chunks",
        "    return score / len(chunk_tokens) if chunk_tokens else 0.0",
        "",
        "",
        "def find_passages_for_query(",
        "    query_text: str,",
        "    layers: Dict[CorticalLayer, HierarchicalLayer],",
        "    tokenizer: Tokenizer,",
        "    documents: Dict[str, str],",
        "    top_n: int = 5,",
        "    chunk_size: int = 512,",
        "    overlap: int = 128,",
        "    use_expansion: bool = True,",
        "    doc_filter: Optional[List[str]] = None,",
        "    semantic_relations: Optional[List[Tuple[str, str, str, float]]] = None,",
        "    use_semantic: bool = True",
        ") -> List[Tuple[str, str, int, int, float]]:",
        "    \"\"\"",
        "    Find text passages most relevant to a query.",
        "",
        "    This is the key function for RAG systems - instead of returning document IDs,",
        "    it returns actual text passages with position information for citations.",
        "",
        "    Args:",
        "        query_text: Search query",
        "        layers: Dictionary of layers",
        "        tokenizer: Tokenizer instance",
        "        documents: Dict mapping doc_id to document text",
        "        top_n: Number of passages to return",
        "        chunk_size: Size of each chunk in characters (default 512)",
        "        overlap: Overlap between chunks in characters (default 128)",
        "        use_expansion: Whether to expand query terms",
        "        doc_filter: Optional list of doc_ids to restrict search to",
        "        semantic_relations: Optional list of semantic relations for expansion",
        "        use_semantic: Whether to use semantic relations for expansion (if available)",
        "",
        "    Returns:",
        "        List of (passage_text, doc_id, start_char, end_char, score) tuples",
        "        ranked by relevance",
        "    \"\"\"",
        "    layer0 = layers[CorticalLayer.TOKENS]",
        "",
        "    # Get expanded query terms",
        "    if use_expansion:",
        "        query_terms = expand_query(query_text, layers, tokenizer, max_expansions=5)",
        "        # Add semantic expansion if available",
        "        if use_semantic and semantic_relations:",
        "            semantic_terms = expand_query_semantic(",
        "                query_text, layers, tokenizer, semantic_relations, max_expansions=5",
        "            )",
        "            for term, weight in semantic_terms.items():",
        "                if term not in query_terms:",
        "                    query_terms[term] = weight * 0.8",
        "                else:",
        "                    query_terms[term] = max(query_terms[term], weight * 0.8)",
        "    else:",
        "        tokens = tokenizer.tokenize(query_text)",
        "        query_terms = {t: 1.0 for t in tokens}",
        "",
        "    if not query_terms:",
        "        return []",
        "",
        "    # First, get candidate documents (more than we need, since we'll rank passages)",
        "    doc_scores = find_documents_for_query(",
        "        query_text, layers, tokenizer,",
        "        top_n=min(len(documents), top_n * 3),",
        "        use_expansion=use_expansion,",
        "        semantic_relations=semantic_relations,",
        "        use_semantic=use_semantic",
        "    )",
        "",
        "    # Apply document filter if provided",
        "    if doc_filter:",
        "        doc_scores = [(doc_id, score) for doc_id, score in doc_scores if doc_id in doc_filter]",
        "",
        "    # Score passages within candidate documents",
        "    passages: List[Tuple[str, str, int, int, float]] = []",
        "",
        "    for doc_id, doc_score in doc_scores:",
        "        if doc_id not in documents:",
        "            continue",
        "",
        "        text = documents[doc_id]",
        "        chunks = create_chunks(text, chunk_size, overlap)",
        "",
        "        for chunk_text, start_char, end_char in chunks:",
        "            chunk_score = score_chunk(",
        "                chunk_text, query_terms, layer0, tokenizer, doc_id",
        "            )",
        "            # Combine chunk score with document score for final ranking",
        "            combined_score = chunk_score * (1 + doc_score * 0.1)",
        "",
        "            passages.append((",
        "                chunk_text,",
        "                doc_id,",
        "                start_char,",
        "                end_char,",
        "                combined_score",
        "            ))",
        "",
        "    # Sort by score and return top passages",
        "    passages.sort(key=lambda x: x[4], reverse=True)",
        "    return passages[:top_n]"
      ],
      "lines_removed": [
        "    ",
        "        ",
        "    ",
        "    "
      ],
      "context_before": [
        "    sorted_concepts = sorted(activated.items(), key=lambda x: -x[1])",
        "    return sorted_concepts[:top_n]",
        "",
        "",
        "def find_related_documents(",
        "    doc_id: str,",
        "    layers: Dict[CorticalLayer, HierarchicalLayer]",
        ") -> List[Tuple[str, float]]:",
        "    \"\"\"",
        "    Find documents related to a given document via lateral connections."
      ],
      "context_after": [
        "    Args:",
        "        doc_id: Source document ID",
        "        layers: Dictionary of layers",
        "    Returns:",
        "        List of (doc_id, weight) tuples for related documents",
        "    \"\"\"",
        "    layer3 = layers.get(CorticalLayer.DOCUMENTS)",
        "    if not layer3:",
        "        return []",
        "    col = layer3.get_minicolumn(doc_id)",
        "    if not col:",
        "        return []",
        "    related = []",
        "    for neighbor_id, weight in col.lateral_connections.items():",
        "        # Use O(1) ID lookup instead of linear search",
        "        neighbor = layer3.get_by_id(neighbor_id)",
        "        if neighbor:",
        "            related.append((neighbor.content, weight))",
        "",
        "    return sorted(related, key=lambda x: -x[1])"
      ],
      "change_type": "modify"
    },
    {
      "file": "showcase.py",
      "function": null,
      "start_line": 1,
      "lines_added": [
        "Cortical Text Processor Showcase",
        "================================",
        "This showcase processes a corpus of documents, demonstrating the",
        "hierarchical analysis of relationships between concepts, documents,",
        "and ideas across diverse topics."
      ],
      "lines_removed": [
        "Cortical Text Processor Demo",
        "============================",
        "This demo processes a corpus of documents, analyzing relationships",
        "between concepts, documents, and ideas across diverse topics."
      ],
      "context_before": [
        "\"\"\""
      ],
      "context_after": [
        "",
        "\"\"\"",
        "",
        "import os",
        "import sys",
        "from typing import Dict, List, Tuple",
        "",
        "from cortical import CorticalTextProcessor, CorticalLayer",
        "",
        "",
        "def print_header(title: str, char: str = \"=\"):"
      ],
      "change_type": "modify"
    },
    {
      "file": "showcase.py",
      "function": "def print_subheader(title: str):",
      "start_line": 28,
      "lines_added": [
        "class CorticalShowcase:",
        "    \"\"\"Showcases the cortical text processor with interesting analysis.\"\"\"",
        "",
        "",
        "        self.demonstrate_polysemy()",
        "    ║            🧠  CORTICAL TEXT PROCESSOR SHOWCASE  🧠                  ║"
      ],
      "lines_removed": [
        "class CorticalDemo:",
        "    \"\"\"Demonstrates the cortical text processor with interesting analysis.\"\"\"",
        "        ",
        "        ",
        "    ║              🧠  CORTICAL TEXT PROCESSOR DEMO  🧠                    ║"
      ],
      "context_before": [
        "",
        "",
        "def render_bar(value: float, max_value: float, width: int = 30) -> str:",
        "    \"\"\"Render a text-based progress bar.\"\"\"",
        "    if max_value == 0:",
        "        return \" \" * width",
        "    filled = int((value / max_value) * width)",
        "    return \"█\" * filled + \"░\" * (width - filled)",
        "",
        ""
      ],
      "context_after": [
        "    ",
        "    def __init__(self, samples_dir: str = \"samples\"):",
        "        self.samples_dir = samples_dir",
        "        self.processor = CorticalTextProcessor()",
        "        self.loaded_files = []",
        "    ",
        "    def run(self):",
        "        \"\"\"Run the complete demo.\"\"\"",
        "        self.print_intro()",
        "        if not self.ingest_corpus():",
        "            print(\"No documents found!\")",
        "            return",
        "        self.analyze_hierarchy()",
        "        self.discover_key_concepts()",
        "        self.analyze_tfidf()",
        "        self.find_concept_associations()",
        "        self.analyze_document_relationships()",
        "        self.demonstrate_queries()",
        "        self.demonstrate_gap_analysis()",
        "        self.demonstrate_embeddings()",
        "        self.print_insights()",
        "    ",
        "    def print_intro(self):",
        "        \"\"\"Print introduction.\"\"\"",
        "        print(\"\"\"",
        "    ╔══════════════════════════════════════════════════════════════════════╗",
        "    ║                                                                      ║",
        "    ║                                                                      ║",
        "    ║     Mimicking how the neocortex processes and understands text       ║",
        "    ║                                                                      ║",
        "    ╚══════════════════════════════════════════════════════════════════════╝",
        "        \"\"\")",
        "    ",
        "    def ingest_corpus(self) -> bool:",
        "        \"\"\"Ingest the document corpus from disk.\"\"\"",
        "        print_header(\"DOCUMENT INGESTION\", \"═\")",
        "        "
      ],
      "change_type": "modify"
    },
    {
      "file": "showcase.py",
      "function": "class CorticalDemo:",
      "start_line": 258,
      "lines_added": [
        "    def demonstrate_polysemy(self):",
        "        \"\"\"Demonstrate polysemy - same word, different meanings.\"\"\"",
        "        print_header(\"POLYSEMY DEMONSTRATION\", \"═\")",
        "",
        "        print(\"Polysemy occurs when the same word has multiple meanings.\")",
        "        print(\"This affects retrieval when query terms are ambiguous.\\n\")",
        "",
        "        # Query for \"candle sticks\"",
        "        query = \"candle sticks\"",
        "        print_subheader(f\"🔍 Query: '{query}'\")",
        "",
        "        results = self.processor.find_documents_for_query(query, top_n=6)",
        "        print(\"\\n    Results:\")",
        "        for doc_id, score in results:",
        "            print(f\"      • {doc_id} (score: {score:.3f})\")",
        "",
        "        # Explain the polysemy",
        "        print(\"\\n    📝 Analysis:\")",
        "        print(\"    The query tokenizes to: ['candle', 'sticks']\")",
        "        print()",
        "        print(\"    'sticks' appears in multiple contexts:\")",
        "        print(\"      • candlestick_patterns - trading chart patterns\")",
        "        print(\"      • letterpress_printing - 'composing sticks' (typesetting tools)\")",
        "        print()",
        "        print(\"    This is a classic word sense disambiguation challenge.\")",
        "        print(\"    The system correctly finds both but cannot distinguish intent.\")",
        "",
        "        # Show the actual text snippets",
        "        layer0 = self.processor.get_layer(CorticalLayer.TOKENS)",
        "        if 'sticks' in layer0.minicolumns:",
        "            col = layer0.minicolumns['sticks']",
        "            print(f\"\\n    'sticks' appears in {len(col.document_ids)} documents:\")",
        "            for doc_id in col.document_ids:",
        "                print(f\"      • {doc_id}\")",
        "",
        "        print(\"\\n    💡 Potential improvements:\")",
        "        print(\"      • Weight adjacent term matches higher (bigram boost)\")",
        "        print(\"      • Use document context for disambiguation\")",
        "        print(\"      • Implement word sense disambiguation\")",
        "        print()",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "            if new_terms:",
        "                print(f\"    Expanded with: {', '.join(new_terms[:6])}\")",
        "            ",
        "            # Find documents",
        "            results = self.processor.find_documents_for_query(query, top_n=3)",
        "            print(f\"\\n    Top documents:\")",
        "            for doc_id, score in results:",
        "                print(f\"      • {doc_id} (score: {score:.3f})\")",
        "            print()",
        "    "
      ],
      "context_after": [
        "    def demonstrate_gap_analysis(self):",
        "        \"\"\"Show knowledge gap detection.\"\"\"",
        "        print_header(\"KNOWLEDGE GAP ANALYSIS\", \"═\")",
        "        ",
        "        print(\"Detecting gaps and anomalies in the corpus:\\n\")",
        "        ",
        "        gaps = self.processor.analyze_knowledge_gaps()",
        "        ",
        "        print(f\"  Coverage Score: {gaps['coverage_score']:.1%}\")",
        "        print(f\"  Connectivity Score: {gaps['connectivity_score']:.4f}\")"
      ],
      "change_type": "add"
    },
    {
      "file": "showcase.py",
      "function": "class CorticalDemo:",
      "start_line": 344,
      "lines_added": [
        "    showcase = CorticalShowcase(samples_dir=\"samples\")",
        "    showcase.run()"
      ],
      "lines_removed": [
        "    demo = CorticalDemo(samples_dir=\"samples\")",
        "    demo.run()"
      ],
      "context_before": [
        "        print(\"  ✓ Computed TF-IDF for discriminative analysis\")",
        "        print(\"  ✓ Found associations through lateral connections\")",
        "        print(\"  ✓ Identified document relationships\")",
        "        print(\"  ✓ Detected knowledge gaps and anomalies\")",
        "        print(\"  ✓ Computed graph embeddings\")",
        "        print(\"  ✓ Enabled semantic queries with expansion\")",
        "        print(\"═\" * 70 + \"\\n\")",
        "",
        "",
        "if __name__ == \"__main__\":"
      ],
      "context_after": [],
      "change_type": "modify"
    },
    {
      "file": "tests/test_persistence.py",
      "function": "class TestSaveLoad(unittest.TestCase):",
      "start_line": 23,
      "lines_added": [
        "            save_processor(",
        "                filepath, processor.layers, processor.documents,",
        "                processor.document_metadata, processor.embeddings,",
        "                processor.semantic_relations, verbose=False",
        "            )",
        "            result = load_processor(filepath, verbose=False)",
        "            layers, documents, document_metadata, embeddings, semantic_relations, metadata = result",
        "            save_processor(",
        "                filepath, processor.layers, processor.documents,",
        "                processor.document_metadata, processor.embeddings,",
        "                processor.semantic_relations, verbose=False",
        "            )",
        "            result = load_processor(filepath, verbose=False)",
        "            layers, documents, document_metadata, embeddings, semantic_relations, metadata = result",
        "            save_processor(",
        "                filepath, processor.layers, processor.documents,",
        "                processor.document_metadata, processor.embeddings,",
        "                processor.semantic_relations, verbose=False",
        "            )",
        "            result = load_processor(filepath, verbose=False)",
        "            layers, documents, document_metadata, embeddings, semantic_relations, metadata = result",
        "            save_processor(",
        "                filepath, processor.layers, processor.documents,",
        "                processor.document_metadata, processor.embeddings,",
        "                processor.semantic_relations, verbose=False",
        "            )",
        "            result = load_processor(filepath, verbose=False)",
        "            layers, documents, document_metadata, embeddings, semantic_relations, metadata = result",
        "    def test_save_load_preserves_document_metadata(self):",
        "        \"\"\"Test that save/load preserves document metadata.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(",
        "            \"doc1\", \"Neural networks process information.\",",
        "            metadata={\"source\": \"https://example.com\", \"author\": \"Test\"}",
        "        )",
        "        processor.compute_all(verbose=False)",
        "",
        "        with tempfile.TemporaryDirectory() as tmpdir:",
        "            filepath = os.path.join(tmpdir, \"test.pkl\")",
        "            save_processor(",
        "                filepath, processor.layers, processor.documents,",
        "                processor.document_metadata, processor.embeddings,",
        "                processor.semantic_relations, verbose=False",
        "            )",
        "",
        "            result = load_processor(filepath, verbose=False)",
        "            layers, documents, document_metadata, embeddings, semantic_relations, metadata = result",
        "",
        "            self.assertEqual(document_metadata[\"doc1\"][\"source\"], \"https://example.com\")",
        "            self.assertEqual(document_metadata[\"doc1\"][\"author\"], \"Test\")",
        "",
        "    def test_save_load_preserves_embeddings(self):",
        "        \"\"\"Test that save/load preserves graph embeddings.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(\"doc1\", \"Neural networks process information.\")",
        "        processor.compute_all(verbose=False)",
        "        processor.compute_graph_embeddings(dimensions=16, verbose=False)",
        "",
        "        with tempfile.TemporaryDirectory() as tmpdir:",
        "            filepath = os.path.join(tmpdir, \"test.pkl\")",
        "            processor.save(filepath, verbose=False)",
        "",
        "            loaded = CorticalTextProcessor.load(filepath, verbose=False)",
        "",
        "            self.assertEqual(len(loaded.embeddings), len(processor.embeddings))",
        "            # Check a specific embedding is preserved",
        "            for term in processor.embeddings:",
        "                self.assertIn(term, loaded.embeddings)",
        "                self.assertEqual(processor.embeddings[term], loaded.embeddings[term])",
        "",
        "    def test_save_load_preserves_semantic_relations(self):",
        "        \"\"\"Test that save/load preserves semantic relations.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(\"doc1\", \"Neural networks are computational models.\")",
        "        processor.process_document(\"doc2\", \"Deep learning uses neural networks.\")",
        "        processor.compute_all(verbose=False)",
        "        processor.extract_corpus_semantics(verbose=False)",
        "",
        "        with tempfile.TemporaryDirectory() as tmpdir:",
        "            filepath = os.path.join(tmpdir, \"test.pkl\")",
        "            processor.save(filepath, verbose=False)",
        "",
        "            loaded = CorticalTextProcessor.load(filepath, verbose=False)",
        "",
        "            self.assertEqual(len(loaded.semantic_relations), len(processor.semantic_relations))",
        ""
      ],
      "lines_removed": [
        "            save_processor(filepath, processor.layers, processor.documents, verbose=False)",
        "            layers, documents, metadata = load_processor(filepath, verbose=False)",
        "            save_processor(filepath, processor.layers, processor.documents, verbose=False)",
        "            layers, documents, _ = load_processor(filepath, verbose=False)",
        "            save_processor(filepath, processor.layers, processor.documents, verbose=False)",
        "            layers, documents, _ = load_processor(filepath, verbose=False)",
        "            save_processor(filepath, processor.layers, processor.documents, verbose=False)",
        "            layers, documents, metadata = load_processor(filepath, verbose=False)"
      ],
      "context_before": [
        "",
        "    def test_save_and_load(self):",
        "        \"\"\"Test saving and loading processor state.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(\"doc1\", \"Neural networks process information.\")",
        "        processor.process_document(\"doc2\", \"Machine learning algorithms learn.\")",
        "        processor.compute_all(verbose=False)",
        "",
        "        with tempfile.TemporaryDirectory() as tmpdir:",
        "            filepath = os.path.join(tmpdir, \"test.pkl\")"
      ],
      "context_after": [
        "",
        "",
        "            self.assertEqual(len(documents), 2)",
        "            self.assertIn(\"doc1\", documents)",
        "            self.assertIn(\"doc2\", documents)",
        "",
        "            # Check layers were restored",
        "            layer0 = layers[CorticalLayer.TOKENS]",
        "            self.assertGreater(len(layer0.minicolumns), 0)",
        "",
        "    def test_save_load_preserves_id_index(self):",
        "        \"\"\"Test that save/load preserves the ID index.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(\"doc1\", \"neural networks deep learning\")",
        "        processor.compute_all(verbose=False)",
        "",
        "        with tempfile.TemporaryDirectory() as tmpdir:",
        "            filepath = os.path.join(tmpdir, \"test.pkl\")",
        "",
        "",
        "            layer0 = layers[CorticalLayer.TOKENS]",
        "            neural = layer0.get_minicolumn(\"neural\")",
        "",
        "            # get_by_id should work after load",
        "            retrieved = layer0.get_by_id(neural.id)",
        "            self.assertEqual(retrieved.content, \"neural\")",
        "",
        "    def test_save_load_preserves_doc_occurrence_counts(self):",
        "        \"\"\"Test that save/load preserves doc_occurrence_counts.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(\"doc1\", \"neural neural neural\")  # 3 times",
        "        processor.process_document(\"doc2\", \"neural\")  # 1 time",
        "        processor.compute_all(verbose=False)",
        "",
        "        with tempfile.TemporaryDirectory() as tmpdir:",
        "            filepath = os.path.join(tmpdir, \"test.pkl\")",
        "",
        "",
        "            layer0 = layers[CorticalLayer.TOKENS]",
        "            neural = layer0.get_minicolumn(\"neural\")",
        "",
        "            self.assertEqual(neural.doc_occurrence_counts.get(\"doc1\"), 3)",
        "            self.assertEqual(neural.doc_occurrence_counts.get(\"doc2\"), 1)",
        "",
        "    def test_save_load_empty_processor(self):",
        "        \"\"\"Test saving and loading empty processor.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "",
        "        with tempfile.TemporaryDirectory() as tmpdir:",
        "            filepath = os.path.join(tmpdir, \"test.pkl\")",
        "",
        "",
        "            self.assertEqual(len(documents), 0)",
        "",
        "",
        "class TestExportGraphJSON(unittest.TestCase):",
        "    \"\"\"Test graph JSON export.\"\"\"",
        "",
        "    def test_export_graph_json(self):",
        "        \"\"\"Test exporting graph to JSON.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(\"doc1\", \"neural networks learning\")",
        "        processor.process_document(\"doc2\", \"machine learning algorithms\")",
        "        processor.compute_all(verbose=False)"
      ],
      "change_type": "modify"
    },
    {
      "file": "tests/test_processor.py",
      "function": "class TestProcessorQuery(unittest.TestCase):",
      "start_line": 128,
      "lines_added": [
        "class TestProcessorMetadata(unittest.TestCase):",
        "    \"\"\"Test document metadata functionality.\"\"\"",
        "",
        "    def setUp(self):",
        "        self.processor = CorticalTextProcessor()",
        "",
        "    def test_process_document_with_metadata(self):",
        "        \"\"\"Test processing document with metadata.\"\"\"",
        "        metadata = {\"source\": \"https://example.com\", \"author\": \"Test Author\"}",
        "        self.processor.process_document(\"doc1\", \"Test content.\", metadata=metadata)",
        "        retrieved = self.processor.get_document_metadata(\"doc1\")",
        "        self.assertEqual(retrieved[\"source\"], \"https://example.com\")",
        "        self.assertEqual(retrieved[\"author\"], \"Test Author\")",
        "",
        "    def test_set_document_metadata(self):",
        "        \"\"\"Test setting metadata after processing.\"\"\"",
        "        self.processor.process_document(\"doc1\", \"Test content.\")",
        "        self.processor.set_document_metadata(\"doc1\", source=\"https://test.com\", timestamp=\"2025-12-09\")",
        "        metadata = self.processor.get_document_metadata(\"doc1\")",
        "        self.assertEqual(metadata[\"source\"], \"https://test.com\")",
        "        self.assertEqual(metadata[\"timestamp\"], \"2025-12-09\")",
        "",
        "    def test_update_document_metadata(self):",
        "        \"\"\"Test updating existing metadata.\"\"\"",
        "        self.processor.process_document(\"doc1\", \"Test content.\", metadata={\"author\": \"Original\"})",
        "        self.processor.set_document_metadata(\"doc1\", author=\"Updated\", category=\"AI\")",
        "        metadata = self.processor.get_document_metadata(\"doc1\")",
        "        self.assertEqual(metadata[\"author\"], \"Updated\")",
        "        self.assertEqual(metadata[\"category\"], \"AI\")",
        "",
        "    def test_get_document_metadata_missing(self):",
        "        \"\"\"Test getting metadata for nonexistent document.\"\"\"",
        "        metadata = self.processor.get_document_metadata(\"nonexistent\")",
        "        self.assertEqual(metadata, {})",
        "",
        "    def test_get_all_document_metadata(self):",
        "        \"\"\"Test getting all document metadata.\"\"\"",
        "        self.processor.process_document(\"doc1\", \"Content 1\", metadata={\"type\": \"article\"})",
        "        self.processor.process_document(\"doc2\", \"Content 2\", metadata={\"type\": \"paper\"})",
        "        all_metadata = self.processor.get_all_document_metadata()",
        "        self.assertEqual(len(all_metadata), 2)",
        "        self.assertEqual(all_metadata[\"doc1\"][\"type\"], \"article\")",
        "        self.assertEqual(all_metadata[\"doc2\"][\"type\"], \"paper\")",
        "",
        "    def test_metadata_not_modified_externally(self):",
        "        \"\"\"Test that get_all_document_metadata returns a copy.\"\"\"",
        "        self.processor.process_document(\"doc1\", \"Content\", metadata={\"key\": \"value\"})",
        "        all_metadata = self.processor.get_all_document_metadata()",
        "        all_metadata[\"doc1\"][\"key\"] = \"modified\"",
        "        # Original should be unchanged",
        "        original = self.processor.get_document_metadata(\"doc1\")",
        "        self.assertEqual(original[\"key\"], \"value\")",
        "",
        "",
        "",
        "",
        "",
        "    def test_save_and_load_with_metadata(self):",
        "        \"\"\"Test that document metadata is preserved through save/load.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(",
        "            \"doc1\",",
        "            \"Test document content.\",",
        "            metadata={\"source\": \"https://example.com\", \"author\": \"Test Author\"}",
        "        )",
        "        processor.set_document_metadata(\"doc1\", category=\"test\")",
        "        processor.compute_all(verbose=False)",
        "",
        "        with tempfile.TemporaryDirectory() as tmpdir:",
        "            filepath = os.path.join(tmpdir, \"test.pkl\")",
        "            processor.save(filepath, verbose=False)",
        "",
        "            loaded = CorticalTextProcessor.load(filepath, verbose=False)",
        "            metadata = loaded.get_document_metadata(\"doc1\")",
        "            self.assertEqual(metadata[\"source\"], \"https://example.com\")",
        "            self.assertEqual(metadata[\"author\"], \"Test Author\")",
        "            self.assertEqual(metadata[\"category\"], \"test\")",
        "",
        "",
        "class TestProcessorPassageRetrieval(unittest.TestCase):",
        "    \"\"\"Test chunk-level passage retrieval for RAG systems.\"\"\"",
        "",
        "    @classmethod",
        "    def setUpClass(cls):",
        "        cls.processor = CorticalTextProcessor()",
        "        # Create documents with distinct content for testing passage retrieval",
        "        cls.processor.process_document(\"neural_doc\", \"\"\"",
        "            Neural networks are computational models inspired by biological neurons.",
        "            They process information through interconnected layers of nodes.",
        "            Deep learning uses many layers to learn hierarchical representations.",
        "            Backpropagation is the key algorithm for training neural networks.",
        "            Convolutional neural networks excel at image recognition tasks.",
        "        \"\"\")",
        "        cls.processor.process_document(\"ml_doc\", \"\"\"",
        "            Machine learning algorithms learn patterns from data automatically.",
        "            Supervised learning requires labeled training examples.",
        "            Unsupervised learning discovers hidden structure in unlabeled data.",
        "            Reinforcement learning trains agents through rewards and penalties.",
        "            Model evaluation uses metrics like accuracy and precision.",
        "        \"\"\")",
        "        cls.processor.process_document(\"data_doc\", \"\"\"",
        "            Data preprocessing is essential for machine learning pipelines.",
        "            Feature engineering creates meaningful input representations.",
        "            Data normalization scales features to similar ranges.",
        "            Missing value imputation handles incomplete datasets.",
        "            Cross-validation ensures robust model performance estimates.",
        "        \"\"\")",
        "        cls.processor.compute_all(verbose=False)",
        "",
        "    def test_find_passages_returns_list(self):",
        "        \"\"\"Test that find_passages_for_query returns a list.\"\"\"",
        "        results = self.processor.find_passages_for_query(\"neural networks\")",
        "        self.assertIsInstance(results, list)",
        "",
        "    def test_find_passages_returns_tuples(self):",
        "        \"\"\"Test that results are tuples with correct structure.\"\"\"",
        "        results = self.processor.find_passages_for_query(\"neural networks\", top_n=1)",
        "        self.assertGreater(len(results), 0)",
        "        passage, doc_id, start, end, score = results[0]",
        "        self.assertIsInstance(passage, str)",
        "        self.assertIsInstance(doc_id, str)",
        "        self.assertIsInstance(start, int)",
        "        self.assertIsInstance(end, int)",
        "        self.assertIsInstance(score, float)",
        "",
        "    def test_find_passages_contains_text(self):",
        "        \"\"\"Test that passages contain actual text.\"\"\"",
        "        results = self.processor.find_passages_for_query(\"neural\", top_n=3)",
        "        self.assertGreater(len(results), 0)",
        "        passage, _, _, _, _ = results[0]",
        "        self.assertGreater(len(passage), 0)",
        "",
        "    def test_find_passages_position_valid(self):",
        "        \"\"\"Test that start/end positions are valid.\"\"\"",
        "        results = self.processor.find_passages_for_query(\"learning\", top_n=3)",
        "        for passage, doc_id, start, end, score in results:",
        "            self.assertGreaterEqual(start, 0)",
        "            self.assertGreater(end, start)",
        "            self.assertEqual(len(passage), end - start)",
        "",
        "    def test_find_passages_top_n_limit(self):",
        "        \"\"\"Test that top_n parameter limits results.\"\"\"",
        "        results = self.processor.find_passages_for_query(\"learning\", top_n=2)",
        "        self.assertLessEqual(len(results), 2)",
        "",
        "    def test_find_passages_chunk_size(self):",
        "        \"\"\"Test that chunk_size parameter is respected.\"\"\"",
        "        results = self.processor.find_passages_for_query(",
        "            \"neural\", top_n=5, chunk_size=100, overlap=20",
        "        )",
        "        for passage, _, _, _, _ in results:",
        "            self.assertLessEqual(len(passage), 100)",
        "",
        "    def test_find_passages_doc_filter(self):",
        "        \"\"\"Test that doc_filter restricts search.\"\"\"",
        "        results = self.processor.find_passages_for_query(",
        "            \"learning\", top_n=10, doc_filter=[\"neural_doc\"]",
        "        )",
        "        for _, doc_id, _, _, _ in results:",
        "            self.assertEqual(doc_id, \"neural_doc\")",
        "",
        "    def test_find_passages_scores_descending(self):",
        "        \"\"\"Test that results are sorted by score descending.\"\"\"",
        "        results = self.processor.find_passages_for_query(\"neural networks\", top_n=5)",
        "        if len(results) > 1:",
        "            scores = [score for _, _, _, _, score in results]",
        "            self.assertEqual(scores, sorted(scores, reverse=True))",
        "",
        "    def test_find_passages_no_expansion(self):",
        "        \"\"\"Test passage retrieval without query expansion.\"\"\"",
        "        results = self.processor.find_passages_for_query(",
        "            \"neural\", top_n=3, use_expansion=False",
        "        )",
        "        self.assertIsInstance(results, list)",
        "",
        "    def test_find_passages_empty_query(self):",
        "        \"\"\"Test handling of queries with no matching terms.\"\"\"",
        "        results = self.processor.find_passages_for_query(\"xyznonexistent123\")",
        "        self.assertEqual(len(results), 0)",
        "",
        "",
        "",
        ""
      ],
      "lines_removed": [
        "    ",
        "        ",
        "            ",
        "    ",
        "    ",
        "    "
      ],
      "context_before": [
        "        results = self.processor.find_documents_for_query(\"neural networks\", top_n=2)",
        "        self.assertGreater(len(results), 0)",
        "        self.assertEqual(results[0][0], \"neural_doc\")",
        "    ",
        "    def test_query_expanded(self):",
        "        \"\"\"Test expanded query.\"\"\"",
        "        results = self.processor.query_expanded(\"learning\", top_n=5)",
        "        self.assertIsInstance(results, list)",
        "",
        ""
      ],
      "context_after": [
        "class TestProcessorPersistence(unittest.TestCase):",
        "    \"\"\"Test processor save/load functionality.\"\"\"",
        "    def test_save_and_load(self):",
        "        \"\"\"Test saving and loading processor.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(\"doc1\", \"Test document content.\")",
        "        processor.compute_all(verbose=False)",
        "        with tempfile.TemporaryDirectory() as tmpdir:",
        "            filepath = os.path.join(tmpdir, \"test.pkl\")",
        "            processor.save(filepath, verbose=False)",
        "            loaded = CorticalTextProcessor.load(filepath, verbose=False)",
        "            self.assertEqual(len(loaded.documents), 1)",
        "            self.assertIn(\"doc1\", loaded.documents)",
        "",
        "",
        "class TestProcessorGaps(unittest.TestCase):",
        "    \"\"\"Test gap detection functionality.\"\"\"",
        "    @classmethod",
        "    def setUpClass(cls):",
        "        cls.processor = CorticalTextProcessor()",
        "        for i in range(3):",
        "            cls.processor.process_document(f\"tech_{i}\", \"\"\"",
        "                Machine learning neural networks deep learning.",
        "                Training models data processing algorithms.",
        "            \"\"\")",
        "        cls.processor.process_document(\"outlier\", \"\"\"",
        "            Medieval falconry birds hunting prey.",
        "            Falcons hawks eagles training.",
        "        \"\"\")",
        "        cls.processor.compute_all(verbose=False)",
        "    def test_analyze_knowledge_gaps(self):",
        "        \"\"\"Test gap analysis returns expected structure.\"\"\"",
        "        gaps = self.processor.analyze_knowledge_gaps()",
        "        self.assertIn('isolated_documents', gaps)",
        "        self.assertIn('weak_topics', gaps)",
        "        self.assertIn('coverage_score', gaps)",
        "    def test_detect_anomalies(self):",
        "        \"\"\"Test anomaly detection.\"\"\"",
        "        anomalies = self.processor.detect_anomalies(threshold=0.1)",
        "        self.assertIsInstance(anomalies, list)",
        "",
        "",
        "if __name__ == \"__main__\":",
        "    unittest.main(verbosity=2)"
      ],
      "change_type": "modify"
    }
  ],
  "hour_of_day": 21,
  "day_of_week": "Tuesday",
  "seconds_since_last_commit": -492198,
  "is_merge": true,
  "is_initial": false,
  "parent_count": 2,
  "session_id": null,
  "related_chats": [],
  "ci_result": null,
  "reverted": false,
  "amended": false
}