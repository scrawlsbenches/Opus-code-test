{
  "hash": "649e7731451ca9a3022a39ddcb8d88f214d8eb35",
  "message": "Merge pull request #67 from scrawlsbenches/claude/task-management-continuation-01A7rL7dCY7Pf1z7BdP5SwDf",
  "author": "scrawlsbenches",
  "timestamp": "2025-12-13 19:16:25 -0500",
  "branch": "claude/multi-index-design-DvifZ",
  "files_changed": [
    ".github/workflows/ci.yml",
    "docs/task-management-dogfooding.md",
    "docs/workflow-templates.md",
    "scripts/ci_task_create.py",
    "scripts/ci_task_report.py",
    "tasks/2025-12-13_22-32-34_e233.json",
    "tasks/2025-12-13_22-33-34_2d89.json",
    "tasks/2025-12-13_22-42-20_6ac7.json",
    "tasks/2025-12-13_22-50-18_cdd1.json",
    "tasks/2025-12-13_23-54-58_1a1d.json",
    "tests/integration/test_workflow_integration.py",
    "tests/unit/test_workflow.py"
  ],
  "insertions": 2363,
  "deletions": 25,
  "hunks": [
    {
      "file": "workflows/ci.yml b/.github/workflows/ci.yml",
      "function": "jobs:",
      "start_line": 63,
      "lines_added": [
        "    - name: Report Pending Tasks",
        "      if: always()",
        "      run: |",
        "        echo \"=== Pending Tasks Report ===\"",
        "        python scripts/ci_task_report.py --github",
        "      env:",
        "        GITHUB_STEP_SUMMARY: ${{ github.step_summary }}",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "      uses: actions/setup-python@v5",
        "      with:",
        "        python-version: '3.11'",
        "",
        "    - name: Validate TASK_LIST.md",
        "      run: |",
        "        echo \"=== Validating Task List ===\"",
        "        python scripts/validate_task_list.py",
        "        echo \"‚úÖ Task list validation passed\"",
        ""
      ],
      "context_after": [
        "  # ==========================================================================",
        "  # Stage 1: Smoke Tests (< 30s)",
        "  # Quick sanity check - if this fails, something is fundamentally broken",
        "  # ==========================================================================",
        "  smoke-tests:",
        "    name: \"üí® Smoke Tests\"",
        "    runs-on: ubuntu-latest",
        "    steps:",
        "    - uses: actions/checkout@v4",
        ""
      ],
      "change_type": "add"
    },
    {
      "file": "docs/task-management-dogfooding.md",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "# Task Management Dog-Fooding Guide",
        "",
        "This guide explains how to use the task management system to track its own development - eating our own dog food.",
        "",
        "## Overview",
        "",
        "The task management system is designed for parallel Claude agent workflows. We use it to:",
        "1. Track our own development tasks",
        "2. Coordinate between multiple agent sessions",
        "3. Ensure nothing falls through the cracks",
        "",
        "## Quick Reference",
        "",
        "```bash",
        "# List all tasks",
        "python scripts/new_task.py --list",
        "",
        "# Create a task",
        "python scripts/new_task.py \"Fix bug X\" --priority high --category bugfix",
        "",
        "# Complete a task",
        "python scripts/new_task.py --complete T-XXXXX",
        "",
        "# View task summary",
        "python scripts/new_task.py --summary",
        "",
        "# Use workflow templates",
        "python scripts/workflow.py run bugfix --bug_title \"Description\"",
        "",
        "# CI-friendly report",
        "python scripts/ci_task_report.py --github",
        "```",
        "",
        "## Dog-Fooding Workflow",
        "",
        "### 1. Start of Session",
        "",
        "At the beginning of each development session:",
        "",
        "```bash",
        "# Check pending tasks",
        "python scripts/new_task.py --list",
        "",
        "# Identify high-priority items",
        "python scripts/ci_task_report.py --quiet",
        "# Output: Tasks: 5 pending (üî¥2 üü°3 üü¢0)",
        "```",
        "",
        "**Key questions:**",
        "- Are there high-priority tasks I should address first?",
        "- Are there tasks from previous sessions that are stale?",
        "- What was I working on last time?",
        "",
        "### 2. Creating Tasks",
        "",
        "When you discover work to do:",
        "",
        "```bash",
        "# Quick task",
        "python scripts/new_task.py \"Investigate slow search\" --priority high",
        "",
        "# With workflow template (creates linked tasks)",
        "python scripts/workflow.py run bugfix --bug_title \"Search returns wrong results\"",
        "",
        "# Dry run first to preview",
        "python scripts/workflow.py run feature --feature_name \"New feature\" --dry-run",
        "```",
        "",
        "**Best practices:**",
        "- Create tasks immediately when you discover them",
        "- Use workflow templates for standard patterns",
        "- Set priority based on impact, not urgency",
        "- Include category for filtering",
        "",
        "### 3. Working on Tasks",
        "",
        "When starting work:",
        "",
        "```bash",
        "# Check what's pending",
        "python scripts/new_task.py --list --status pending",
        "",
        "# Pick a task and start working",
        "# (The system doesn't track \"in_progress\" automatically - you manage it)",
        "```",
        "",
        "### 4. Completing Tasks",
        "",
        "After finishing work:",
        "",
        "```bash",
        "# Mark task as done",
        "python scripts/new_task.py --complete T-20251213-143052-a1b2-001",
        "",
        "# Verify completion",
        "python scripts/new_task.py --list --status completed",
        "```",
        "",
        "**When to mark complete:**",
        "- Code is written and tested",
        "- Changes are committed",
        "- Related documentation updated",
        "- No blocking issues remain",
        "",
        "### 5. End of Session",
        "",
        "Before ending your session:",
        "",
        "```bash",
        "# Verify no orphaned tasks",
        "python scripts/new_task.py --list",
        "",
        "# Create continuation tasks for unfinished work",
        "python scripts/new_task.py \"Continue: Feature X implementation\" --description \"Left off at...\"",
        "",
        "# Commit task files",
        "git add tasks/*.json",
        "git commit -m \"Update task status\"",
        "git push",
        "```",
        "",
        "## Session Continuity",
        "",
        "The system maintains session state across CLI invocations:",
        "",
        "```",
        "tasks/",
        "‚îú‚îÄ‚îÄ .current_session.json       # Current session metadata",
        "‚îú‚îÄ‚îÄ 2025-12-13_14-30-52_a1b2.json  # Session 1 tasks",
        "‚îî‚îÄ‚îÄ 2025-12-13_16-00-00_b2c3.json  # Session 2 tasks",
        "```",
        "",
        "### Starting a New Session",
        "",
        "```bash",
        "# Start fresh session (clears .current_session.json)",
        "python scripts/new_task.py --new-session",
        "",
        "# Tasks in new session get new IDs",
        "python scripts/new_task.py \"First task in new session\"",
        "# Creates: T-20251213-160000-c3d4-001",
        "```",
        "",
        "### Resuming Previous Session",
        "",
        "Sessions persist until you explicitly start a new one. Task IDs continue incrementing.",
        "",
        "## Parallel Agent Workflows",
        "",
        "### Multiple Agents Working Simultaneously",
        "",
        "Each agent creates tasks with unique session IDs:",
        "",
        "```",
        "Agent A: T-20251213-143052-a1b2-001, T-20251213-143052-a1b2-002",
        "Agent B: T-20251213-143055-c3d4-001, T-20251213-143055-c3d4-002",
        "```",
        "",
        "**No merge conflicts:** Different session IDs mean different filenames.",
        "",
        "### Consolidating Work",
        "",
        "After parallel work completes:",
        "",
        "```bash",
        "# View all tasks from all sessions",
        "python scripts/new_task.py --list",
        "",
        "# Generate consolidated report",
        "python scripts/consolidate_tasks.py --output CONSOLIDATED.md",
        "```",
        "",
        "## CI Integration",
        "",
        "The CI pipeline automatically shows pending tasks:",
        "",
        "```yaml",
        "# In .github/workflows/ci.yml",
        "- name: Report Pending Tasks",
        "  run: python scripts/ci_task_report.py --github",
        "```",
        "",
        "### CI Output Formats",
        "",
        "```bash",
        "# GitHub Actions (markdown tables)",
        "python scripts/ci_task_report.py --github",
        "",
        "# Console (readable)",
        "python scripts/ci_task_report.py",
        "",
        "# Minimal (one-liner)",
        "python scripts/ci_task_report.py --quiet",
        "",
        "# Fail if high-priority tasks exist",
        "python scripts/ci_task_report.py --fail-on-high",
        "```",
        "",
        "## Workflow Templates",
        "",
        "Pre-defined task chains for common patterns:",
        "",
        "| Workflow | Tasks | Use When |",
        "|----------|-------|----------|",
        "| `bugfix` | investigate ‚Üí fix ‚Üí test ‚Üí document | Fixing bugs |",
        "| `feature` | design ‚Üí implement ‚Üí unit_tests ‚Üí integration_tests ‚Üí docs | Adding features |",
        "| `refactor` | analyze ‚Üí plan ‚Üí execute ‚Üí verify | Restructuring code |",
        "",
        "### Creating Custom Workflows",
        "",
        "```yaml",
        "# .claude/workflows/my_workflow.yaml",
        "name: \"My Workflow\"",
        "description: \"Custom task chain\"",
        "variables:",
        "  - name: target",
        "    required: true",
        "tasks:",
        "  - id: step1",
        "    title: \"First: {target}\"",
        "  - id: step2",
        "    title: \"Second: {target}\"",
        "    depends_on: [step1]",
        "```",
        "",
        "## Testing the Task System",
        "",
        "When making changes to the task system itself:",
        "",
        "### 1. Run Unit Tests",
        "",
        "```bash",
        "python -m pytest tests/unit/test_task_utils.py tests/unit/test_workflow.py -v",
        "```",
        "",
        "### 2. Run Integration Tests",
        "",
        "```bash",
        "python -m pytest tests/integration/test_task_integration.py tests/integration/test_workflow_integration.py -v",
        "```",
        "",
        "### 3. Manual Dog-Fooding",
        "",
        "```bash",
        "# Create a test task",
        "python scripts/new_task.py \"Test task\" --priority low",
        "",
        "# Verify it appears",
        "python scripts/new_task.py --list",
        "",
        "# Complete it",
        "python scripts/new_task.py --complete T-XXXXX",
        "",
        "# Verify completion",
        "python scripts/new_task.py --list --status completed",
        "",
        "# Clean up (optional)",
        "# Delete the session file from tasks/",
        "```",
        "",
        "## Common Patterns",
        "",
        "### Bug Discovery During Feature Work",
        "",
        "```bash",
        "# You're working on feature X, discover bug Y",
        "python scripts/new_task.py \"Bug: Y found during X\" --priority high --category bugfix",
        "# Continue with feature X, bug Y is tracked",
        "```",
        "",
        "### Splitting Large Tasks",
        "",
        "```bash",
        "# Original task too big",
        "python scripts/workflow.py run feature --feature_name \"Large Feature\"",
        "# Creates 5 linked tasks automatically",
        "",
        "# Or manually create subtasks",
        "python scripts/new_task.py \"Part 1: Setup\" --priority high",
        "python scripts/new_task.py \"Part 2: Core logic\" --priority high",
        "python scripts/new_task.py \"Part 3: Tests\" --priority high",
        "```",
        "",
        "### Handling Blocked Tasks",
        "",
        "If a task is blocked:",
        "1. Don't mark it complete",
        "2. Create a new task for the blocker",
        "3. Add description noting the block",
        "",
        "```bash",
        "python scripts/new_task.py \"Blocked: Need API access for X\" --priority high",
        "```",
        "",
        "## Metrics and Reporting",
        "",
        "Track progress over time:",
        "",
        "```bash",
        "# Summary counts",
        "python scripts/new_task.py --summary",
        "",
        "# Output:",
        "# pending: 5",
        "# in_progress: 0",
        "# completed: 12",
        "# deferred: 1",
        "",
        "# CI report with priority breakdown",
        "python scripts/ci_task_report.py",
        "```",
        "",
        "## Files and Locations",
        "",
        "| Path | Purpose |",
        "|------|---------|",
        "| `tasks/*.json` | Task session files |",
        "| `tasks/.current_session.json` | Active session state |",
        "| `.claude/workflows/*.yaml` | Workflow templates |",
        "| `.claude/skills/task-manager/` | Claude Code skill definition |",
        "| `scripts/task_utils.py` | Core task utilities |",
        "| `scripts/new_task.py` | CLI for task management |",
        "| `scripts/workflow.py` | Workflow template engine |",
        "| `scripts/ci_task_report.py` | CI-friendly task reporter |",
        "| `scripts/consolidate_tasks.py` | Task consolidation |",
        "",
        "## Troubleshooting",
        "",
        "### Tasks Not Showing Up",
        "",
        "```bash",
        "# Check tasks directory exists",
        "ls -la tasks/",
        "",
        "# Check for valid JSON",
        "cat tasks/*.json | python -m json.tool",
        "```",
        "",
        "### Duplicate Task IDs",
        "",
        "This shouldn't happen due to timestamp + session + counter format. If it does:",
        "",
        "```bash",
        "# Check session file",
        "cat tasks/.current_session.json",
        "",
        "# Start new session",
        "python scripts/new_task.py --new-session",
        "```",
        "",
        "### CI Report Empty",
        "",
        "```bash",
        "# Ensure tasks directory exists",
        "mkdir -p tasks",
        "",
        "# Check if any tasks exist",
        "python scripts/task_utils.py list --dir tasks",
        "```",
        "",
        "---",
        "",
        "*Remember: We build this system for ourselves. If something is painful, fix it and add a task for improving it.*"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "docs/workflow-templates.md",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "# Workflow Templates",
        "",
        "Workflow templates enable creating multiple linked tasks from a single command. This is useful for standardizing common development patterns like bug fixes, features, and refactors.",
        "",
        "## Quick Start",
        "",
        "```bash",
        "# List available workflows",
        "python scripts/workflow.py list",
        "",
        "# Run the bugfix workflow",
        "python scripts/workflow.py run bugfix --bug_title \"Login fails with special characters\"",
        "",
        "# Dry run (see tasks without creating)",
        "python scripts/workflow.py run feature --feature_name \"Dark mode\" --dry-run",
        "```",
        "",
        "## Available Workflows",
        "",
        "### Bug Fix (`bugfix`)",
        "",
        "Creates 4 tasks with dependencies:",
        "",
        "```",
        "investigate ‚Üí fix ‚Üí test",
        "                 ‚Üí document",
        "```",
        "",
        "**Variables:**",
        "| Variable | Required | Default | Description |",
        "|----------|----------|---------|-------------|",
        "| `bug_title` | Yes | - | Brief description of the bug |",
        "| `priority` | No | high | Bug priority (high/medium/low) |",
        "| `affected_file` | No | - | Primary file affected |",
        "",
        "**Example:**",
        "```bash",
        "python scripts/workflow.py run bugfix \\",
        "    --bug_title \"Search returns stale results\" \\",
        "    --priority high",
        "```",
        "",
        "### Feature (`feature`)",
        "",
        "Creates 5 tasks with dependencies:",
        "",
        "```",
        "design ‚Üí implement ‚Üí unit_tests ‚Üí documentation",
        "                  ‚Üí integration_tests ‚Üó",
        "```",
        "",
        "**Variables:**",
        "| Variable | Required | Default | Description |",
        "|----------|----------|---------|-------------|",
        "| `feature_name` | Yes | - | Name of the feature |",
        "| `priority` | No | medium | Feature priority |",
        "| `effort` | No | large | Overall effort (small/medium/large) |",
        "",
        "**Example:**",
        "```bash",
        "python scripts/workflow.py run feature \\",
        "    --feature_name \"Semantic search\" \\",
        "    --priority high \\",
        "    --effort large",
        "```",
        "",
        "### Refactor (`refactor`)",
        "",
        "Creates 4 tasks with dependencies:",
        "",
        "```",
        "analyze ‚Üí plan ‚Üí execute ‚Üí verify",
        "```",
        "",
        "**Variables:**",
        "| Variable | Required | Default | Description |",
        "|----------|----------|---------|-------------|",
        "| `refactor_target` | Yes | - | What to refactor |",
        "| `priority` | No | medium | Refactor priority |",
        "| `scope` | No | module | Scope (function/module/system) |",
        "",
        "**Example:**",
        "```bash",
        "python scripts/workflow.py run refactor \\",
        "    --refactor_target \"Query expansion logic\" \\",
        "    --scope module",
        "```",
        "",
        "## Creating Custom Workflows",
        "",
        "Workflows are YAML files in `.claude/workflows/`. Here's the structure:",
        "",
        "```yaml",
        "# .claude/workflows/my_workflow.yaml",
        "name: \"My Workflow\"",
        "description: \"Brief description of what this workflow does\"",
        "category: \"general\"",
        "",
        "variables:",
        "  - name: task_name",
        "    description: \"The name of the task\"",
        "    required: true",
        "  - name: priority",
        "    description: \"Task priority\"",
        "    default: \"medium\"",
        "    choices: [\"high\", \"medium\", \"low\"]",
        "",
        "tasks:",
        "  - id: first_task",
        "    title: \"First: {task_name}\"",
        "    category: \"planning\"",
        "    priority: \"{priority}\"",
        "    effort: \"small\"",
        "    description: |",
        "      Description with {task_name} substitution.",
        "",
        "  - id: second_task",
        "    title: \"Second: {task_name}\"",
        "    depends_on: [first_task]",
        "    description: |",
        "      This task depends on first_task completing.",
        "```",
        "",
        "### Variable Types",
        "",
        "| Field | Required | Description |",
        "|-------|----------|-------------|",
        "| `name` | Yes | Variable name (used in `{name}` placeholders) |",
        "| `description` | No | Help text shown to users |",
        "| `required` | No | If true, must be provided (default: true) |",
        "| `default` | No | Default value if not provided |",
        "| `choices` | No | List of valid values |",
        "",
        "### Task Fields",
        "",
        "| Field | Required | Default | Description |",
        "|-------|----------|---------|-------------|",
        "| `id` | Yes | - | Unique ID within workflow (used for depends_on) |",
        "| `title` | Yes | - | Task title (supports `{variable}` substitution) |",
        "| `category` | No | general | Task category |",
        "| `priority` | No | medium | Task priority (supports substitution) |",
        "| `effort` | No | medium | Effort estimate (supports substitution) |",
        "| `description` | No | \"\" | Detailed description |",
        "| `depends_on` | No | [] | List of task IDs this depends on |",
        "",
        "## API Usage",
        "",
        "You can also use workflows programmatically:",
        "",
        "```python",
        "from scripts.workflow import Workflow, run_workflow",
        "",
        "# Load a workflow",
        "workflow = Workflow.load(Path(\".claude/workflows/bugfix.yaml\"))",
        "",
        "# Run with variables",
        "variables = {\"bug_title\": \"Login bug\", \"priority\": \"high\"}",
        "tasks = run_workflow(workflow, variables, tasks_dir=\"tasks\", dry_run=False)",
        "",
        "# Access created tasks",
        "for task in tasks:",
        "    print(f\"{task.id}: {task.title}\")",
        "```",
        "",
        "## Task Output",
        "",
        "Tasks are saved as JSON files in the `tasks/` directory:",
        "",
        "```",
        "tasks/",
        "‚îú‚îÄ‚îÄ 2025-12-13_14-30-52_a1b2.json  # Session file with tasks",
        "‚îî‚îÄ‚îÄ 2025-12-13_15-00-00_c3d4.json  # Another session",
        "```",
        "",
        "Each session file contains:",
        "```json",
        "{",
        "  \"version\": 1,",
        "  \"session_id\": \"a1b2\",",
        "  \"started_at\": \"2025-12-13T14:30:52\",",
        "  \"saved_at\": \"2025-12-13T14:30:53\",",
        "  \"tasks\": [",
        "    {",
        "      \"id\": \"T-20251213-143052-a1b2-001\",",
        "      \"title\": \"Investigate: Login bug\",",
        "      \"status\": \"pending\",",
        "      \"priority\": \"high\",",
        "      \"depends_on\": []",
        "    },",
        "    {",
        "      \"id\": \"T-20251213-143052-a1b2-002\",",
        "      \"title\": \"Fix: Login bug\",",
        "      \"depends_on\": [\"T-20251213-143052-a1b2-001\"]",
        "    }",
        "  ]",
        "}",
        "```",
        "",
        "## Best Practices",
        "",
        "1. **Use dry-run first**: Always preview with `--dry-run` before creating tasks",
        "2. **Keep workflows focused**: Each workflow should handle one type of work",
        "3. **Use dependencies**: Link tasks to show the correct order",
        "4. **Descriptive titles**: Include the variable in titles for context",
        "5. **Add descriptions**: Include checklists and acceptance criteria",
        "",
        "## Integration with CI",
        "",
        "The CI pipeline shows pending tasks using `scripts/ci_task_report.py`:",
        "",
        "```bash",
        "# GitHub Actions format",
        "python scripts/ci_task_report.py --github",
        "",
        "# Console format",
        "python scripts/ci_task_report.py",
        "",
        "# Fail if high-priority tasks exist",
        "python scripts/ci_task_report.py --fail-on-high",
        "```"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "scripts/ci_task_create.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "#!/usr/bin/env python3",
        "\"\"\"",
        "Auto-create tasks from CI test failures.",
        "",
        "This script parses test output and creates tasks for failures automatically.",
        "Designed to be run in CI after test failures to ensure nothing is forgotten.",
        "",
        "Usage:",
        "    # From pytest output file",
        "    python scripts/ci_task_create.py --pytest output.txt",
        "",
        "    # From pytest-json-report",
        "    python scripts/ci_task_create.py --pytest-json report.json",
        "",
        "    # Pipe from pytest directly",
        "    pytest tests/ 2>&1 | python scripts/ci_task_create.py --pytest -",
        "",
        "    # Dry run (show tasks without creating)",
        "    pytest tests/ 2>&1 | python scripts/ci_task_create.py --pytest - --dry-run",
        "",
        "Examples:",
        "    # In CI workflow",
        "    - name: Run tests",
        "      run: pytest tests/ -v 2>&1 | tee test_output.txt || true",
        "",
        "    - name: Create tasks for failures",
        "      if: failure()",
        "      run: python scripts/ci_task_create.py --pytest test_output.txt",
        "\"\"\"",
        "",
        "import argparse",
        "import json",
        "import re",
        "import sys",
        "from pathlib import Path",
        "from typing import List, Tuple",
        "",
        "# Add scripts to path",
        "sys.path.insert(0, str(Path(__file__).parent))",
        "",
        "from task_utils import TaskSession, DEFAULT_TASKS_DIR",
        "",
        "",
        "def parse_pytest_output(content: str) -> List[Tuple[str, str, str]]:",
        "    \"\"\"",
        "    Parse pytest output to extract test failures.",
        "",
        "    Returns:",
        "        List of (test_name, file_path, error_message) tuples",
        "    \"\"\"",
        "    failures = []",
        "",
        "    # Pattern for FAILED lines: FAILED tests/test_foo.py::TestClass::test_name",
        "    failed_pattern = re.compile(r'FAILED\\s+([^\\s:]+)::(\\S+)')",
        "",
        "    # Pattern for error details in short format",
        "    error_pattern = re.compile(",
        "        r'([^\\s]+\\.py):(\\d+):\\s+(\\w+(?:Error|Exception|Failed|Failure).*?)(?=\\n[^\\s]|\\Z)',",
        "        re.MULTILINE | re.DOTALL",
        "    )",
        "",
        "    # Pattern for assertion errors",
        "    assert_pattern = re.compile(",
        "        r'>\\s+assert\\s+(.+?)\\nE\\s+(.+)',",
        "        re.MULTILINE",
        "    )",
        "",
        "    # Find all FAILED lines",
        "    for match in failed_pattern.finditer(content):",
        "        file_path = match.group(1)",
        "        test_name = match.group(2)",
        "",
        "        # Try to extract error message",
        "        error_msg = \"Test failed\"",
        "",
        "        # Look for AssertionError nearby",
        "        test_section_start = match.start()",
        "        test_section_end = content.find('FAILED', test_section_start + 1)",
        "        if test_section_end == -1:",
        "            test_section_end = len(content)",
        "",
        "        test_section = content[test_section_start:test_section_end]",
        "",
        "        # Look for assertion details",
        "        assert_match = assert_pattern.search(test_section)",
        "        if assert_match:",
        "            error_msg = f\"Assertion: {assert_match.group(2).strip()}\"",
        "        else:",
        "            # Look for any error",
        "            error_match = error_pattern.search(test_section)",
        "            if error_match:",
        "                error_msg = error_match.group(3).strip()[:100]",
        "",
        "        failures.append((test_name, file_path, error_msg))",
        "",
        "    return failures",
        "",
        "",
        "def parse_pytest_json(content: str) -> List[Tuple[str, str, str]]:",
        "    \"\"\"",
        "    Parse pytest-json-report output.",
        "",
        "    Returns:",
        "        List of (test_name, file_path, error_message) tuples",
        "    \"\"\"",
        "    failures = []",
        "",
        "    try:",
        "        data = json.loads(content)",
        "    except json.JSONDecodeError:",
        "        return failures",
        "",
        "    tests = data.get('tests', [])",
        "    for test in tests:",
        "        if test.get('outcome') == 'failed':",
        "            nodeid = test.get('nodeid', '')",
        "            parts = nodeid.split('::')",
        "",
        "            file_path = parts[0] if parts else 'unknown'",
        "            test_name = parts[-1] if parts else nodeid",
        "",
        "            # Extract error message from call phase",
        "            call = test.get('call', {})",
        "            longrepr = call.get('longrepr', '')",
        "            if isinstance(longrepr, str):",
        "                # Get first meaningful line",
        "                error_msg = longrepr.split('\\n')[0][:100]",
        "            else:",
        "                error_msg = \"Test failed\"",
        "",
        "            failures.append((test_name, file_path, error_msg))",
        "",
        "    return failures",
        "",
        "",
        "def create_tasks_for_failures(",
        "    failures: List[Tuple[str, str, str]],",
        "    tasks_dir: str = DEFAULT_TASKS_DIR,",
        "    dry_run: bool = False,",
        "    ci_run_id: str = None",
        ") -> List[str]:",
        "    \"\"\"",
        "    Create tasks for test failures.",
        "",
        "    Args:",
        "        failures: List of (test_name, file_path, error_message) tuples",
        "        tasks_dir: Directory to save tasks",
        "        dry_run: If True, print tasks without creating",
        "        ci_run_id: Optional CI run identifier for context",
        "",
        "    Returns:",
        "        List of created task IDs",
        "    \"\"\"",
        "    if not failures:",
        "        print(\"No test failures found.\")",
        "        return []",
        "",
        "    session = TaskSession()",
        "    created_ids = []",
        "",
        "    for test_name, file_path, error_msg in failures:",
        "        # Create descriptive title",
        "        title = f\"Fix failing test: {test_name}\"",
        "",
        "        # Create detailed description",
        "        description = f\"\"\"Test failure detected in CI.",
        "",
        "**Test:** {test_name}",
        "**File:** {file_path}",
        "**Error:** {error_msg}",
        "",
        "**Steps to fix:**",
        "1. Run the test locally to reproduce",
        "2. Investigate the failure",
        "3. Implement the fix",
        "4. Verify the test passes",
        "5. Ensure no regressions",
        "\"\"\"",
        "",
        "        if ci_run_id:",
        "            description += f\"\\n**CI Run:** {ci_run_id}\\n\"",
        "",
        "        context = {",
        "            \"source\": \"ci_auto_create\",",
        "            \"test_file\": file_path,",
        "            \"test_name\": test_name,",
        "            \"error\": error_msg[:200]",
        "        }",
        "",
        "        if ci_run_id:",
        "            context[\"ci_run_id\"] = ci_run_id",
        "",
        "        task = session.create_task(",
        "            title=title,",
        "            priority=\"high\",",
        "            category=\"test\",",
        "            description=description,",
        "            effort=\"small\",",
        "            context=context",
        "        )",
        "        created_ids.append(task.id)",
        "",
        "        if dry_run:",
        "            print(f\"[DRY RUN] Would create: {task.id}\")",
        "            print(f\"  Title: {title}\")",
        "            print(f\"  File: {file_path}\")",
        "            print(f\"  Error: {error_msg[:60]}...\")",
        "            print()",
        "",
        "    if not dry_run and created_ids:",
        "        filepath = session.save(tasks_dir)",
        "        print(f\"\\n‚úÖ Created {len(created_ids)} tasks for test failures\")",
        "        print(f\"Saved to: {filepath}\")",
        "        print(\"\\nCreated tasks:\")",
        "        for task_id in created_ids:",
        "            print(f\"  {task_id}\")",
        "",
        "    return created_ids",
        "",
        "",
        "def main():",
        "    parser = argparse.ArgumentParser(",
        "        description=\"Auto-create tasks from CI test failures\",",
        "        formatter_class=argparse.RawDescriptionHelpFormatter,",
        "        epilog=__doc__",
        "    )",
        "",
        "    parser.add_argument(",
        "        \"--pytest\", metavar=\"FILE\",",
        "        help=\"Parse pytest output file (use '-' for stdin)\"",
        "    )",
        "    parser.add_argument(",
        "        \"--pytest-json\", metavar=\"FILE\",",
        "        help=\"Parse pytest-json-report output file\"",
        "    )",
        "    parser.add_argument(",
        "        \"--dry-run\", action=\"store_true\",",
        "        help=\"Show tasks without creating\"",
        "    )",
        "    parser.add_argument(",
        "        \"--tasks-dir\", default=DEFAULT_TASKS_DIR,",
        "        help=f\"Tasks directory (default: {DEFAULT_TASKS_DIR})\"",
        "    )",
        "    parser.add_argument(",
        "        \"--ci-run-id\",",
        "        help=\"CI run identifier for context\"",
        "    )",
        "",
        "    args = parser.parse_args()",
        "",
        "    failures = []",
        "",
        "    if args.pytest:",
        "        if args.pytest == '-':",
        "            content = sys.stdin.read()",
        "        else:",
        "            with open(args.pytest) as f:",
        "                content = f.read()",
        "        failures = parse_pytest_output(content)",
        "",
        "    elif args.pytest_json:",
        "        with open(args.pytest_json) as f:",
        "            content = f.read()",
        "        failures = parse_pytest_json(content)",
        "",
        "    else:",
        "        parser.print_help()",
        "        print(\"\\nError: Must specify --pytest or --pytest-json\")",
        "        sys.exit(1)",
        "",
        "    if failures:",
        "        print(f\"Found {len(failures)} test failure(s)\\n\")",
        "",
        "    create_tasks_for_failures(",
        "        failures,",
        "        tasks_dir=args.tasks_dir,",
        "        dry_run=args.dry_run,",
        "        ci_run_id=args.ci_run_id",
        "    )",
        "",
        "",
        "if __name__ == \"__main__\":",
        "    main()"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "scripts/ci_task_report.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "#!/usr/bin/env python3",
        "\"\"\"",
        "CI Task Reporter - Intelligent pending task output for CI pipelines.",
        "",
        "This script outputs pending tasks in a CI-friendly format, suitable for:",
        "- GitHub Actions job summaries",
        "- Console output during CI runs",
        "- Slack/Discord notifications",
        "",
        "Features:",
        "- Groups by priority (high items first)",
        "- Shows estimated effort",
        "- Provides actionable summary",
        "- Exits with non-zero code if high-priority tasks exist (optional)",
        "",
        "Usage:",
        "    # Standard output",
        "    python scripts/ci_task_report.py",
        "",
        "    # GitHub Actions markdown format (writes to $GITHUB_STEP_SUMMARY)",
        "    python scripts/ci_task_report.py --github",
        "",
        "    # Fail CI if high-priority tasks pending",
        "    python scripts/ci_task_report.py --fail-on-high",
        "",
        "    # Quiet mode (summary only)",
        "    python scripts/ci_task_report.py --quiet",
        "\"\"\"",
        "",
        "import argparse",
        "import os",
        "import sys",
        "from pathlib import Path",
        "from typing import Dict, List",
        "",
        "# Add scripts to path",
        "sys.path.insert(0, str(Path(__file__).parent))",
        "",
        "from task_utils import load_all_tasks, Task, DEFAULT_TASKS_DIR",
        "",
        "",
        "def get_pending_tasks(tasks_dir: str = DEFAULT_TASKS_DIR) -> List[Task]:",
        "    \"\"\"Load only pending and in_progress tasks.\"\"\"",
        "    all_tasks = load_all_tasks(tasks_dir)",
        "    return [t for t in all_tasks if t.status in (\"pending\", \"in_progress\")]",
        "",
        "",
        "def group_by_priority(tasks: List[Task]) -> Dict[str, List[Task]]:",
        "    \"\"\"Group tasks by priority.\"\"\"",
        "    grouped = {\"high\": [], \"medium\": [], \"low\": []}",
        "    for task in tasks:",
        "        priority = task.priority if task.priority in grouped else \"medium\"",
        "        grouped[priority].append(task)",
        "    return grouped",
        "",
        "",
        "def format_console_report(tasks: List[Task]) -> str:",
        "    \"\"\"Format tasks for console output.\"\"\"",
        "    if not tasks:",
        "        return \"‚úÖ No pending tasks!\\n\"",
        "",
        "    grouped = group_by_priority(tasks)",
        "    lines = []",
        "",
        "    # Summary header",
        "    total = len(tasks)",
        "    high_count = len(grouped[\"high\"])",
        "    in_progress = sum(1 for t in tasks if t.status == \"in_progress\")",
        "",
        "    lines.append(\"=\" * 60)",
        "    lines.append(f\"üìã PENDING TASKS: {total} total ({high_count} high priority)\")",
        "    if in_progress:",
        "        lines.append(f\"   üîÑ {in_progress} currently in progress\")",
        "    lines.append(\"=\" * 60)",
        "",
        "    # Priority sections",
        "    priority_config = [",
        "        (\"high\", \"üî¥ HIGH PRIORITY\", \"These need attention!\"),",
        "        (\"medium\", \"üü° MEDIUM PRIORITY\", \"\"),",
        "        (\"low\", \"üü¢ LOW PRIORITY\", \"\"),",
        "    ]",
        "",
        "    for priority, header, note in priority_config:",
        "        if not grouped[priority]:",
        "            continue",
        "        lines.append(\"\")",
        "        lines.append(f\"{header}\" + (f\" - {note}\" if note else \"\"))",
        "        lines.append(\"-\" * 40)",
        "",
        "        for task in grouped[priority]:",
        "            status_marker = \"üîÑ\" if task.status == \"in_progress\" else \"  \"",
        "            effort_marker = {\"small\": \"S\", \"medium\": \"M\", \"large\": \"L\"}.get(task.effort, \"?\")",
        "            lines.append(f\"  {status_marker} [{effort_marker}] {task.id}\")",
        "            lines.append(f\"       {task.title}\")",
        "",
        "    lines.append(\"\")",
        "    lines.append(\"=\" * 60)",
        "",
        "    # Actionable summary",
        "    if high_count > 0:",
        "        lines.append(\"‚ö†Ô∏è  HIGH PRIORITY TASKS REQUIRE ATTENTION\")",
        "",
        "    return \"\\n\".join(lines)",
        "",
        "",
        "def format_github_markdown(tasks: List[Task]) -> str:",
        "    \"\"\"Format tasks as GitHub-flavored markdown for job summary.\"\"\"",
        "    if not tasks:",
        "        return \"## ‚úÖ No Pending Tasks\\n\\nAll tasks have been completed!\\n\"",
        "",
        "    grouped = group_by_priority(tasks)",
        "    lines = []",
        "",
        "    # Summary header",
        "    total = len(tasks)",
        "    high_count = len(grouped[\"high\"])",
        "    in_progress = sum(1 for t in tasks if t.status == \"in_progress\")",
        "",
        "    lines.append(\"## üìã Pending Tasks Summary\")",
        "    lines.append(\"\")",
        "    lines.append(f\"| Metric | Count |\")",
        "    lines.append(\"|--------|-------|\")",
        "    lines.append(f\"| Total Pending | **{total}** |\")",
        "    lines.append(f\"| üî¥ High Priority | {high_count} |\")",
        "    lines.append(f\"| üü° Medium Priority | {len(grouped['medium'])} |\")",
        "    lines.append(f\"| üü¢ Low Priority | {len(grouped['low'])} |\")",
        "    lines.append(f\"| üîÑ In Progress | {in_progress} |\")",
        "    lines.append(\"\")",
        "",
        "    # High priority callout",
        "    if high_count > 0:",
        "        lines.append(\"> ‚ö†Ô∏è **Attention:** There are high-priority tasks that need attention!\")",
        "        lines.append(\"\")",
        "",
        "    # Task tables by priority",
        "    priority_config = [",
        "        (\"high\", \"### üî¥ High Priority\"),",
        "        (\"medium\", \"### üü° Medium Priority\"),",
        "        (\"low\", \"### üü¢ Low Priority\"),",
        "    ]",
        "",
        "    for priority, header in priority_config:",
        "        if not grouped[priority]:",
        "            continue",
        "",
        "        lines.append(header)",
        "        lines.append(\"\")",
        "        lines.append(\"| Status | ID | Title | Effort | Category |\")",
        "        lines.append(\"|--------|----|----|--------|----------|\")",
        "",
        "        for task in grouped[priority]:",
        "            status = \"üîÑ\" if task.status == \"in_progress\" else \"üìã\"",
        "            effort = {\"small\": \"S\", \"medium\": \"M\", \"large\": \"L\"}.get(task.effort, \"?\")",
        "            # Escape pipe characters in title",
        "            title = task.title.replace(\"|\", \"\\\\|\")",
        "            lines.append(f\"| {status} | `{task.id}` | {title} | {effort} | {task.category} |\")",
        "",
        "        lines.append(\"\")",
        "",
        "    # Quick commands",
        "    lines.append(\"<details>\")",
        "    lines.append(\"<summary>üìå Quick Commands</summary>\")",
        "    lines.append(\"\")",
        "    lines.append(\"```bash\")",
        "    lines.append(\"# List all tasks\")",
        "    lines.append(\"python scripts/new_task.py --list\")",
        "    lines.append(\"\")",
        "    lines.append(\"# Complete a task\")",
        "    lines.append(\"python scripts/new_task.py --complete T-XXXXX\")",
        "    lines.append(\"\")",
        "    lines.append(\"# Create new task\")",
        "    lines.append('python scripts/new_task.py \"Task title\" --priority high')",
        "    lines.append(\"```\")",
        "    lines.append(\"</details>\")",
        "",
        "    return \"\\n\".join(lines)",
        "",
        "",
        "def format_quiet_report(tasks: List[Task]) -> str:",
        "    \"\"\"Minimal one-line summary.\"\"\"",
        "    if not tasks:",
        "        return \"Tasks: 0 pending\"",
        "",
        "    grouped = group_by_priority(tasks)",
        "    return (",
        "        f\"Tasks: {len(tasks)} pending \"",
        "        f\"(üî¥{len(grouped['high'])} üü°{len(grouped['medium'])} üü¢{len(grouped['low'])})\"",
        "    )",
        "",
        "",
        "def main():",
        "    parser = argparse.ArgumentParser(",
        "        description=\"CI Task Reporter - Output pending tasks for CI pipelines\",",
        "        formatter_class=argparse.RawDescriptionHelpFormatter,",
        "        epilog=__doc__",
        "    )",
        "",
        "    parser.add_argument(",
        "        \"--github\", action=\"store_true\",",
        "        help=\"Output GitHub-flavored markdown (writes to GITHUB_STEP_SUMMARY if available)\"",
        "    )",
        "    parser.add_argument(",
        "        \"--fail-on-high\", action=\"store_true\",",
        "        help=\"Exit with code 1 if high-priority tasks exist\"",
        "    )",
        "    parser.add_argument(",
        "        \"--quiet\", \"-q\", action=\"store_true\",",
        "        help=\"Minimal output (summary line only)\"",
        "    )",
        "    parser.add_argument(",
        "        \"--dir\", default=DEFAULT_TASKS_DIR,",
        "        help=f\"Tasks directory (default: {DEFAULT_TASKS_DIR})\"",
        "    )",
        "    parser.add_argument(",
        "        \"--output\", \"-o\",",
        "        help=\"Write report to file instead of stdout\"",
        "    )",
        "",
        "    args = parser.parse_args()",
        "",
        "    # Load pending tasks",
        "    tasks = get_pending_tasks(args.dir)",
        "",
        "    # Format report",
        "    if args.quiet:",
        "        report = format_quiet_report(tasks)",
        "    elif args.github:",
        "        report = format_github_markdown(tasks)",
        "    else:",
        "        report = format_console_report(tasks)",
        "",
        "    # Output report",
        "    if args.output:",
        "        with open(args.output, \"w\") as f:",
        "            f.write(report)",
        "        print(f\"Report written to: {args.output}\")",
        "    else:",
        "        print(report)",
        "",
        "    # GitHub Actions: Write to step summary if available",
        "    if args.github and \"GITHUB_STEP_SUMMARY\" in os.environ:",
        "        summary_file = os.environ[\"GITHUB_STEP_SUMMARY\"]",
        "        with open(summary_file, \"a\") as f:",
        "            f.write(report + \"\\n\")",
        "",
        "    # Exit code logic",
        "    if args.fail_on_high:",
        "        grouped = group_by_priority(tasks)",
        "        if grouped[\"high\"]:",
        "            print(f\"\\n‚ùå Failing: {len(grouped['high'])} high-priority tasks pending\")",
        "            sys.exit(1)",
        "",
        "",
        "if __name__ == \"__main__\":",
        "    main()"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "tasks/2025-12-13_22-32-34_e233.json",
      "function": null,
      "start_line": 1,
      "lines_added": [
        "  \"saved_at\": \"2025-12-13T23:48:47.611980\",",
        "      \"status\": \"completed\",",
        "      \"updated_at\": \"2025-12-13T23:48:47.611790\",",
        "      \"completed_at\": \"2025-12-13T23:48:47.611790\","
      ],
      "lines_removed": [
        "  \"saved_at\": \"2025-12-13T22:33:26.107227\",",
        "      \"status\": \"pending\",",
        "      \"updated_at\": null,",
        "      \"completed_at\": null,"
      ],
      "context_before": [
        "{",
        "  \"version\": 1,",
        "  \"session_id\": \"e233\",",
        "  \"started_at\": \"2025-12-13T22:32:34.072421\","
      ],
      "context_after": [
        "  \"tasks\": [",
        "    {",
        "      \"id\": \"T-20251213-223234-e233-01\",",
        "      \"title\": \"Document dog-fooding workflow\",",
        "      \"priority\": \"high\",",
        "      \"category\": \"docs\",",
        "      \"description\": \"Create a practical guide for using the task system in daily work\",",
        "      \"depends_on\": [],",
        "      \"effort\": \"small\",",
        "      \"created_at\": \"2025-12-13T22:32:34.072535\",",
        "      \"context\": {",
        "        \"files\": [",
        "          \"docs/merge-friendly-tasks.md\"",
        "        ]",
        "      }",
        "    },",
        "    {",
        "      \"id\": \"T-20251213-223234-e233-02\",",
        "      \"title\": \"Add convenience script for quick task creation\",",
        "      \"status\": \"completed\","
      ],
      "change_type": "modify"
    },
    {
      "file": "tasks/2025-12-13_22-33-34_2d89.json",
      "function": null,
      "start_line": 1,
      "lines_added": [
        "  \"saved_at\": \"2025-12-13T23:54:54.911673\",",
        "      \"status\": \"completed\",",
        "      \"updated_at\": \"2025-12-13T23:54:54.911470\",",
        "      \"completed_at\": \"2025-12-13T23:54:54.911470\",",
        "      \"status\": \"completed\",",
        "      \"updated_at\": \"2025-12-13T23:34:11.766071\",",
        "      \"completed_at\": \"2025-12-13T23:34:11.766071\","
      ],
      "lines_removed": [
        "  \"saved_at\": \"2025-12-13T23:25:22.253602\",",
        "      \"status\": \"pending\",",
        "      \"updated_at\": null,",
        "      \"completed_at\": null,",
        "      \"status\": \"pending\",",
        "      \"updated_at\": null,",
        "      \"completed_at\": null,"
      ],
      "context_before": [
        "{",
        "  \"version\": 1,",
        "  \"session_id\": \"2d89\",",
        "  \"started_at\": \"2025-12-13T22:33:34.431014\","
      ],
      "context_after": [
        "  \"tasks\": [",
        "    {",
        "      \"id\": \"T-20251213-223334-2d89-01\",",
        "      \"title\": \"Investigate performance bottleneck in search\",",
        "      \"priority\": \"high\",",
        "      \"category\": \"perf\",",
        "      \"description\": \"\",",
        "      \"depends_on\": [],",
        "      \"effort\": \"medium\",",
        "      \"created_at\": \"2025-12-13T22:33:34.431621\",",
        "      \"context\": {}",
        "    },",
        "    {",
        "      \"id\": \"T-20251213-232522-2d89-002\",",
        "      \"title\": \"Update CI to output pending tasks intelligently\",",
        "      \"priority\": \"medium\",",
        "      \"category\": \"automation\",",
        "      \"description\": \"Add a CI step that shows pending tasks in a smart way: grouped by priority, showing blockers first, with context about what's ready to work on next. Could integrate with the workflow system to show task chains and dependencies.\",",
        "      \"depends_on\": [],",
        "      \"effort\": \"medium\",",
        "      \"created_at\": \"2025-12-13T23:25:22.253447\",",
        "      \"context\": {}",
        "    }",
        "  ]",
        "}"
      ],
      "change_type": "modify"
    },
    {
      "file": "tasks/2025-12-13_22-42-20_6ac7.json",
      "function": null,
      "start_line": 1,
      "lines_added": [
        "  \"saved_at\": \"2025-12-13T23:48:47.726752\","
      ],
      "lines_removed": [
        "  \"saved_at\": \"2025-12-13T22:50:36.977690\","
      ],
      "context_before": [
        "{",
        "  \"version\": 1,",
        "  \"session_id\": \"6ac7\",",
        "  \"started_at\": \"2025-12-13T22:42:20.986896\","
      ],
      "context_after": [
        "  \"tasks\": [",
        "    {",
        "      \"id\": \"T-20251213-224220-6ac7-01\",",
        "      \"title\": \"Fix non-atomic file writes (data loss risk)\",",
        "      \"status\": \"completed\",",
        "      \"priority\": \"high\",",
        "      \"category\": \"bugfix\",",
        "      \"description\": \"TaskSession.save() should write to .tmp then atomic rename\",",
        "      \"depends_on\": [],",
        "      \"effort\": \"small\","
      ],
      "change_type": "modify"
    },
    {
      "file": "tasks/2025-12-13_22-42-20_6ac7.json",
      "function": null,
      "start_line": 75,
      "lines_added": [
        "      \"status\": \"completed\",",
        "      \"updated_at\": \"2025-12-13T23:48:47.726631\",",
        "      \"completed_at\": \"2025-12-13T23:48:47.726631\","
      ],
      "lines_removed": [
        "      \"status\": \"pending\",",
        "      \"updated_at\": null,",
        "      \"completed_at\": null,"
      ],
      "context_before": [
        "      ],",
        "      \"effort\": \"medium\",",
        "      \"created_at\": \"2025-12-13T22:42:20.986963\",",
        "      \"updated_at\": \"2025-12-13T22:50:36.977578\",",
        "      \"completed_at\": \"2025-12-13T22:50:36.977578\",",
        "      \"context\": {}",
        "    },",
        "    {",
        "      \"id\": \"T-20251213-224220-6ac7-05\",",
        "      \"title\": \"Add auto-task creation from CI test failures\","
      ],
      "context_after": [
        "      \"priority\": \"medium\",",
        "      \"category\": \"automation\",",
        "      \"description\": \"Parse pytest failures, create bugfix tasks automatically\",",
        "      \"depends_on\": [],",
        "      \"effort\": \"medium\",",
        "      \"created_at\": \"2025-12-13T22:42:20.986976\",",
        "      \"context\": {",
        "        \"files\": [",
        "          \".github/workflows/ci.yml\",",
        "          \"scripts/create_tasks_from_ci.py\"",
        "        ]",
        "      }",
        "    },",
        "    {",
        "      \"id\": \"T-20251213-224220-6ac7-06\",",
        "      \"title\": \"Add task retrospective metadata capture\","
      ],
      "change_type": "modify"
    },
    {
      "file": "tasks/2025-12-13_22-50-18_cdd1.json",
      "function": null,
      "start_line": 1,
      "lines_added": [
        "  \"saved_at\": \"2025-12-13T23:46:07.936790\","
      ],
      "lines_removed": [
        "  \"saved_at\": \"2025-12-13T22:50:37.198957\","
      ],
      "context_before": [
        "{",
        "  \"version\": 1,",
        "  \"session_id\": \"cdd1\",",
        "  \"started_at\": \"2025-12-13T22:50:18.707702\","
      ],
      "context_after": [
        "  \"tasks\": [",
        "    {",
        "      \"id\": \"T-20251213-225018-cdd1-001\",",
        "      \"title\": \"Design: Workflow templates\",",
        "      \"status\": \"completed\",",
        "      \"priority\": \"medium\",",
        "      \"category\": \"arch\",",
        "      \"description\": \"Design the API and architecture for: Workflow templates\\n\\nDeliverables:\\n- API design (function signatures, data structures)\\n- Integration points with existing code\\n- Edge cases to handle\\n\",",
        "      \"depends_on\": [],",
        "      \"effort\": \"medium\","
      ],
      "change_type": "modify"
    },
    {
      "file": "tasks/2025-12-13_22-50-18_cdd1.json",
      "function": null,
      "start_line": 30,
      "lines_added": [
        "      \"status\": \"completed\",",
        "      \"updated_at\": \"2025-12-13T23:46:07.707667\",",
        "      \"completed_at\": \"2025-12-13T23:46:07.707667\",",
        "      \"status\": \"completed\",",
        "      \"updated_at\": \"2025-12-13T23:46:07.832328\",",
        "      \"completed_at\": \"2025-12-13T23:46:07.832328\",",
        "      \"status\": \"completed\",",
        "      \"updated_at\": \"2025-12-13T23:46:07.936682\",",
        "      \"completed_at\": \"2025-12-13T23:46:07.936682\","
      ],
      "lines_removed": [
        "      \"status\": \"pending\",",
        "      \"updated_at\": null,",
        "      \"completed_at\": null,",
        "      \"status\": \"pending\",",
        "      \"updated_at\": null,",
        "      \"completed_at\": null,",
        "      \"status\": \"pending\",",
        "      \"updated_at\": null,",
        "      \"completed_at\": null,"
      ],
      "context_before": [
        "      ],",
        "      \"effort\": \"medium\",",
        "      \"created_at\": \"2025-12-13T22:50:18.707774\",",
        "      \"updated_at\": \"2025-12-13T22:50:37.198821\",",
        "      \"completed_at\": \"2025-12-13T22:50:37.198821\",",
        "      \"context\": {}",
        "    },",
        "    {",
        "      \"id\": \"T-20251213-225018-cdd1-003\",",
        "      \"title\": \"Unit tests for: Workflow templates\","
      ],
      "context_after": [
        "      \"priority\": \"high\",",
        "      \"category\": \"test\",",
        "      \"description\": \"Write comprehensive unit tests.\\n\\nCoverage requirements:\\n- Happy path scenarios\\n- Edge cases\\n- Error conditions\\n- Target: 90%+ coverage for new code\\n\",",
        "      \"depends_on\": [",
        "        \"T-20251213-225018-cdd1-002\"",
        "      ],",
        "      \"effort\": \"medium\",",
        "      \"created_at\": \"2025-12-13T22:50:18.707790\",",
        "      \"context\": {}",
        "    },",
        "    {",
        "      \"id\": \"T-20251213-225018-cdd1-004\",",
        "      \"title\": \"Integration tests for: Workflow templates\",",
        "      \"priority\": \"high\",",
        "      \"category\": \"test\",",
        "      \"description\": \"Write integration tests verifying feature works with existing system.\\n\\nTest scenarios:\\n- End-to-end workflows\\n- Interaction with other components\\n- Performance characteristics\\n\",",
        "      \"depends_on\": [",
        "        \"T-20251213-225018-cdd1-002\"",
        "      ],",
        "      \"effort\": \"medium\",",
        "      \"created_at\": \"2025-12-13T22:50:18.707805\",",
        "      \"context\": {}",
        "    },",
        "    {",
        "      \"id\": \"T-20251213-225018-cdd1-005\",",
        "      \"title\": \"Documentation for: Workflow templates\",",
        "      \"priority\": \"medium\",",
        "      \"category\": \"docs\",",
        "      \"description\": \"Document the new feature.\\n\\nInclude:\\n- Usage examples\\n- API reference updates\\n- CLAUDE.md updates if applicable\\n\",",
        "      \"depends_on\": [",
        "        \"T-20251213-225018-cdd1-003\",",
        "        \"T-20251213-225018-cdd1-004\"",
        "      ],",
        "      \"effort\": \"small\",",
        "      \"created_at\": \"2025-12-13T22:50:18.707823\",",
        "      \"context\": {}",
        "    }",
        "  ]",
        "}"
      ],
      "change_type": "modify"
    },
    {
      "file": "tasks/2025-12-13_23-54-58_1a1d.json",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "{",
        "  \"version\": 1,",
        "  \"session_id\": \"1a1d\",",
        "  \"started_at\": \"2025-12-13T23:54:58.530536\",",
        "  \"saved_at\": \"2025-12-13T23:54:58.531057\",",
        "  \"tasks\": [",
        "    {",
        "      \"id\": \"T-20251213-235458-1a1d-001\",",
        "      \"title\": \"Optimize doc_name_boost: cache tokenized document names\",",
        "      \"status\": \"pending\",",
        "      \"priority\": \"medium\",",
        "      \"category\": \"perf\",",
        "      \"description\": \"Performance bottleneck identified: doc_name_boost re-tokenizes all document names on every search.\\n\\nRoot cause: Lines 70-103 in query/search.py tokenize every doc_id on every query.\\nImpact: 70% of search time on 2000-doc corpus (3.95ms overhead).\\n\\nRecommendation: Cache tokenized doc names in Minicolumn during process_document().\\nExpected benefit: 3-4x faster searches on large corpora.\\nEffort estimate: 2-4 hours.\",",
        "      \"depends_on\": [],",
        "      \"effort\": \"medium\",",
        "      \"created_at\": \"2025-12-13T23:54:58.530954\",",
        "      \"updated_at\": null,",
        "      \"completed_at\": null,",
        "      \"context\": {}",
        "    }",
        "  ]",
        "}"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "tests/integration/test_workflow_integration.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "#!/usr/bin/env python3",
        "\"\"\"",
        "Integration tests for workflow template engine.",
        "",
        "These tests verify that the workflow system loads real templates,",
        "executes them correctly, and creates valid task files with proper",
        "dependency resolution.",
        "",
        "Run with: pytest tests/integration/test_workflow_integration.py -v",
        "\"\"\"",
        "",
        "import json",
        "import shutil",
        "import sys",
        "import tempfile",
        "import unittest",
        "from pathlib import Path",
        "",
        "# Add scripts to path",
        "sys.path.insert(0, str(Path(__file__).parent.parent.parent / \"scripts\"))",
        "",
        "from workflow import (",
        "    Workflow,",
        "    WorkflowVariable,",
        "    WorkflowTask,",
        "    list_workflows,",
        "    run_workflow,",
        "    substitute_variables,",
        "    WORKFLOWS_DIR,",
        ")",
        "from task_utils import Task, TaskSession, load_all_tasks",
        "",
        "",
        "class TestWorkflowLoading(unittest.TestCase):",
        "    \"\"\"Test loading real workflow templates from .claude/workflows/.\"\"\"",
        "",
        "    def test_load_bugfix_workflow(self):",
        "        \"\"\"Verify bugfix.yaml loads correctly.\"\"\"",
        "        workflow_path = WORKFLOWS_DIR / \"bugfix.yaml\"",
        "        self.assertTrue(workflow_path.exists(), \"bugfix.yaml should exist\")",
        "",
        "        workflow = Workflow.load(workflow_path)",
        "",
        "        self.assertEqual(workflow.name, \"Bug Fix\")",
        "        self.assertEqual(workflow.category, \"bugfix\")",
        "        self.assertEqual(len(workflow.tasks), 4, \"Bugfix workflow should have 4 tasks\")",
        "        self.assertGreater(len(workflow.variables), 0, \"Should have variables\")",
        "",
        "        # Verify task IDs",
        "        task_ids = [t.id for t in workflow.tasks]",
        "        self.assertIn(\"investigate\", task_ids)",
        "        self.assertIn(\"fix\", task_ids)",
        "        self.assertIn(\"test\", task_ids)",
        "        self.assertIn(\"document\", task_ids)",
        "",
        "        # Verify dependencies",
        "        fix_task = next(t for t in workflow.tasks if t.id == \"fix\")",
        "        self.assertIn(\"investigate\", fix_task.depends_on)",
        "",
        "    def test_load_feature_workflow(self):",
        "        \"\"\"Verify feature.yaml loads correctly.\"\"\"",
        "        workflow_path = WORKFLOWS_DIR / \"feature.yaml\"",
        "        self.assertTrue(workflow_path.exists(), \"feature.yaml should exist\")",
        "",
        "        workflow = Workflow.load(workflow_path)",
        "",
        "        self.assertEqual(workflow.name, \"Feature\")",
        "        self.assertEqual(workflow.category, \"feature\")",
        "        self.assertEqual(len(workflow.tasks), 5, \"Feature workflow should have 5 tasks\")",
        "",
        "        # Verify task IDs",
        "        task_ids = [t.id for t in workflow.tasks]",
        "        self.assertIn(\"design\", task_ids)",
        "        self.assertIn(\"implement\", task_ids)",
        "        self.assertIn(\"unit_tests\", task_ids)",
        "        self.assertIn(\"integration_tests\", task_ids)",
        "        self.assertIn(\"documentation\", task_ids)",
        "",
        "        # Verify complex dependencies",
        "        doc_task = next(t for t in workflow.tasks if t.id == \"documentation\")",
        "        self.assertIn(\"unit_tests\", doc_task.depends_on)",
        "        self.assertIn(\"integration_tests\", doc_task.depends_on)",
        "",
        "    def test_load_refactor_workflow(self):",
        "        \"\"\"Verify refactor.yaml loads correctly.\"\"\"",
        "        workflow_path = WORKFLOWS_DIR / \"refactor.yaml\"",
        "        self.assertTrue(workflow_path.exists(), \"refactor.yaml should exist\")",
        "",
        "        workflow = Workflow.load(workflow_path)",
        "",
        "        self.assertEqual(workflow.name, \"Refactor\")",
        "        self.assertEqual(workflow.category, \"refactor\")",
        "        self.assertEqual(len(workflow.tasks), 4, \"Refactor workflow should have 4 tasks\")",
        "",
        "        # Verify task IDs",
        "        task_ids = [t.id for t in workflow.tasks]",
        "        self.assertIn(\"analyze\", task_ids)",
        "        self.assertIn(\"refactor\", task_ids)",
        "        self.assertIn(\"verify\", task_ids)",
        "        self.assertIn(\"cleanup\", task_ids)",
        "",
        "    def test_list_workflows_returns_all(self):",
        "        \"\"\"Verify list_workflows finds all three templates.\"\"\"",
        "        workflows = list_workflows()",
        "",
        "        self.assertGreaterEqual(len(workflows), 3, \"Should find at least 3 workflows\")",
        "",
        "        workflow_names = [w.name for w in workflows]",
        "        self.assertIn(\"Bug Fix\", workflow_names)",
        "        self.assertIn(\"Feature\", workflow_names)",
        "        self.assertIn(\"Refactor\", workflow_names)",
        "",
        "    def test_workflow_variables_have_correct_types(self):",
        "        \"\"\"Verify workflow variables are parsed correctly.\"\"\"",
        "        workflow = Workflow.load(WORKFLOWS_DIR / \"bugfix.yaml\")",
        "",
        "        # Find bug_title variable",
        "        bug_title_var = next(v for v in workflow.variables if v.name == \"bug_title\")",
        "        self.assertTrue(bug_title_var.required)",
        "        self.assertIsNone(bug_title_var.default)",
        "",
        "        # Find priority variable with choices",
        "        priority_var = next(v for v in workflow.variables if v.name == \"priority\")",
        "        self.assertIsNotNone(priority_var.choices)",
        "        self.assertIn(\"high\", priority_var.choices)",
        "        self.assertIn(\"medium\", priority_var.choices)",
        "        self.assertIn(\"low\", priority_var.choices)",
        "",
        "",
        "class TestWorkflowExecution(unittest.TestCase):",
        "    \"\"\"Test end-to-end workflow execution with real file creation.\"\"\"",
        "",
        "    def setUp(self):",
        "        \"\"\"Create temporary directory for tasks.\"\"\"",
        "        self.temp_dir = tempfile.mkdtemp()",
        "",
        "    def tearDown(self):",
        "        \"\"\"Clean up temporary directory.\"\"\"",
        "        shutil.rmtree(self.temp_dir)",
        "",
        "    def test_bugfix_workflow_creates_four_tasks(self):",
        "        \"\"\"Verify bugfix workflow creates exactly 4 tasks.\"\"\"",
        "        workflow = Workflow.load(WORKFLOWS_DIR / \"bugfix.yaml\")",
        "        variables = {",
        "            \"bug_title\": \"Login crashes on special chars\",",
        "            \"priority\": \"high\"",
        "        }",
        "",
        "        tasks = run_workflow(workflow, variables, tasks_dir=self.temp_dir, dry_run=True)",
        "",
        "        self.assertEqual(len(tasks), 4, \"Bugfix workflow should create 4 tasks\")",
        "        self.assertIsInstance(tasks[0], Task)",
        "",
        "        # Verify titles contain substituted bug_title",
        "        for task in tasks:",
        "            self.assertIn(\"Login crashes on special chars\", task.title)",
        "",
        "    def test_bugfix_workflow_saves_to_json(self):",
        "        \"\"\"Verify workflow creates valid JSON file.\"\"\"",
        "        workflow = Workflow.load(WORKFLOWS_DIR / \"bugfix.yaml\")",
        "        variables = {",
        "            \"bug_title\": \"Null pointer in auth module\",",
        "            \"priority\": \"high\"",
        "        }",
        "",
        "        tasks = run_workflow(workflow, variables, tasks_dir=self.temp_dir, dry_run=False)",
        "",
        "        # Check that JSON file was created",
        "        json_files = list(Path(self.temp_dir).glob(\"*.json\"))",
        "        self.assertEqual(len(json_files), 1, \"Should create exactly one JSON file\")",
        "",
        "        # Verify JSON is valid",
        "        with open(json_files[0]) as f:",
        "            data = json.load(f)",
        "",
        "        self.assertEqual(data[\"version\"], 1)",
        "        self.assertIn(\"tasks\", data)",
        "        self.assertEqual(len(data[\"tasks\"]), 4)",
        "",
        "        # Verify tasks have correct structure",
        "        for task_data in data[\"tasks\"]:",
        "            self.assertIn(\"id\", task_data)",
        "            self.assertIn(\"title\", task_data)",
        "            self.assertIn(\"status\", task_data)",
        "            self.assertIn(\"depends_on\", task_data)",
        "",
        "    def test_feature_workflow_creates_five_tasks(self):",
        "        \"\"\"Verify feature workflow creates exactly 5 tasks.\"\"\"",
        "        workflow = Workflow.load(WORKFLOWS_DIR / \"feature.yaml\")",
        "        variables = {",
        "            \"feature_name\": \"Dark mode toggle\",",
        "            \"priority\": \"medium\",",
        "            \"effort\": \"large\"",
        "        }",
        "",
        "        tasks = run_workflow(workflow, variables, tasks_dir=self.temp_dir, dry_run=True)",
        "",
        "        self.assertEqual(len(tasks), 5, \"Feature workflow should create 5 tasks\")",
        "",
        "        # Verify task categories",
        "        categories = [t.category for t in tasks]",
        "        self.assertIn(\"arch\", categories)  # design task",
        "        self.assertIn(\"feature\", categories)  # implement task",
        "        self.assertIn(\"test\", categories)  # test tasks",
        "        self.assertIn(\"docs\", categories)  # documentation task",
        "",
        "    def test_refactor_workflow_creates_four_tasks(self):",
        "        \"\"\"Verify refactor workflow creates exactly 4 tasks.\"\"\"",
        "        workflow = Workflow.load(WORKFLOWS_DIR / \"refactor.yaml\")",
        "        variables = {",
        "            \"refactor_target\": \"query module split\",",
        "            \"priority\": \"medium\"",
        "        }",
        "",
        "        tasks = run_workflow(workflow, variables, tasks_dir=self.temp_dir, dry_run=True)",
        "",
        "        self.assertEqual(len(tasks), 4, \"Refactor workflow should create 4 tasks\")",
        "",
        "        # Verify titles",
        "        titles = [t.title for t in tasks]",
        "        self.assertTrue(any(\"Analyze\" in t for t in titles))",
        "        self.assertTrue(any(\"Refactor\" in t for t in titles))",
        "        self.assertTrue(any(\"Verify\" in t for t in titles))",
        "        self.assertTrue(any(\"Cleanup\" in t for t in titles))",
        "",
        "",
        "class TestDependencyResolution(unittest.TestCase):",
        "    \"\"\"Test that workflow dependencies are resolved to actual task IDs.\"\"\"",
        "",
        "    def setUp(self):",
        "        \"\"\"Create temporary directory for tasks.\"\"\"",
        "        self.temp_dir = tempfile.mkdtemp()",
        "",
        "    def tearDown(self):",
        "        \"\"\"Clean up temporary directory.\"\"\"",
        "        shutil.rmtree(self.temp_dir)",
        "",
        "    def test_bugfix_dependency_chain(self):",
        "        \"\"\"Verify bugfix workflow dependency chain is correct.\"\"\"",
        "        workflow = Workflow.load(WORKFLOWS_DIR / \"bugfix.yaml\")",
        "        variables = {\"bug_title\": \"Test bug\", \"priority\": \"high\"}",
        "",
        "        tasks = run_workflow(workflow, variables, tasks_dir=self.temp_dir, dry_run=True)",
        "",
        "        # Build task map by title pattern",
        "        investigate = next(t for t in tasks if \"Investigate\" in t.title)",
        "        fix = next(t for t in tasks if \"Fix:\" in t.title)",
        "        test = next(t for t in tasks if \"regression test\" in t.title)",
        "        document = next(t for t in tasks if \"Document\" in t.title)",
        "",
        "        # Verify dependencies use actual task IDs, not template IDs",
        "        self.assertEqual(len(investigate.depends_on), 0, \"Investigate has no dependencies\")",
        "        self.assertIn(investigate.id, fix.depends_on, \"Fix depends on investigate\")",
        "        self.assertIn(fix.id, test.depends_on, \"Test depends on fix\")",
        "        self.assertIn(fix.id, document.depends_on, \"Document depends on fix\")",
        "",
        "        # Verify IDs are not template IDs",
        "        self.assertNotIn(\"investigate\", fix.depends_on)",
        "        self.assertNotIn(\"fix\", test.depends_on)",
        "",
        "    def test_feature_complex_dependencies(self):",
        "        \"\"\"Verify feature workflow has correct dependency graph.\"\"\"",
        "        workflow = Workflow.load(WORKFLOWS_DIR / \"feature.yaml\")",
        "        variables = {",
        "            \"feature_name\": \"Test feature\",",
        "            \"priority\": \"medium\",",
        "            \"effort\": \"large\"",
        "        }",
        "",
        "        tasks = run_workflow(workflow, variables, tasks_dir=self.temp_dir, dry_run=True)",
        "",
        "        # Map tasks by title pattern",
        "        design = next(t for t in tasks if \"Design:\" in t.title)",
        "        implement = next(t for t in tasks if \"Implement:\" in t.title)",
        "        unit_tests = next(t for t in tasks if \"Unit tests\" in t.title)",
        "        integration_tests = next(t for t in tasks if \"Integration tests\" in t.title)",
        "        documentation = next(t for t in tasks if \"Documentation\" in t.title)",
        "",
        "        # Verify dependency chain",
        "        self.assertEqual(len(design.depends_on), 0, \"Design has no dependencies\")",
        "        self.assertIn(design.id, implement.depends_on, \"Implement depends on design\")",
        "        self.assertIn(implement.id, unit_tests.depends_on, \"Unit tests depend on implement\")",
        "        self.assertIn(implement.id, integration_tests.depends_on, \"Integration tests depend on implement\")",
        "",
        "        # Documentation depends on BOTH test tasks",
        "        self.assertIn(unit_tests.id, documentation.depends_on)",
        "        self.assertIn(integration_tests.id, documentation.depends_on)",
        "",
        "        # Verify no template IDs leak through",
        "        for task in tasks:",
        "            for dep_id in task.depends_on:",
        "                self.assertTrue(dep_id.startswith(\"T-\"), f\"Dependency {dep_id} should be a real task ID\")",
        "",
        "    def test_dependencies_point_to_existing_tasks(self):",
        "        \"\"\"Verify all dependency IDs reference actual tasks in the session.\"\"\"",
        "        workflow = Workflow.load(WORKFLOWS_DIR / \"feature.yaml\")",
        "        variables = {",
        "            \"feature_name\": \"Test\",",
        "            \"priority\": \"high\",",
        "            \"effort\": \"medium\"",
        "        }",
        "",
        "        tasks = run_workflow(workflow, variables, tasks_dir=self.temp_dir, dry_run=False)",
        "",
        "        # Load tasks from file",
        "        loaded_tasks = load_all_tasks(self.temp_dir)",
        "        task_ids = {t.id for t in loaded_tasks}",
        "",
        "        # Verify every dependency points to an existing task",
        "        for task in loaded_tasks:",
        "            for dep_id in task.depends_on:",
        "                self.assertIn(dep_id, task_ids, f\"Dependency {dep_id} should exist in task set\")",
        "",
        "",
        "class TestVariableSubstitution(unittest.TestCase):",
        "    \"\"\"Test variable substitution in task titles and descriptions.\"\"\"",
        "",
        "    def setUp(self):",
        "        \"\"\"Create temporary directory for tasks.\"\"\"",
        "        self.temp_dir = tempfile.mkdtemp()",
        "",
        "    def tearDown(self):",
        "        \"\"\"Clean up temporary directory.\"\"\"",
        "        shutil.rmtree(self.temp_dir)",
        "",
        "    def test_bug_title_appears_in_all_tasks(self):",
        "        \"\"\"Verify bug_title variable is substituted in all task titles.\"\"\"",
        "        workflow = Workflow.load(WORKFLOWS_DIR / \"bugfix.yaml\")",
        "        bug_title = \"Timeout in database connection\"",
        "        variables = {\"bug_title\": bug_title, \"priority\": \"high\"}",
        "",
        "        tasks = run_workflow(workflow, variables, tasks_dir=self.temp_dir, dry_run=True)",
        "",
        "        # All tasks should contain the bug title",
        "        for task in tasks:",
        "            self.assertIn(bug_title, task.title,",
        "                         f\"Task title '{task.title}' should contain '{bug_title}'\")",
        "",
        "    def test_description_substitution(self):",
        "        \"\"\"Verify variable substitution works in task descriptions.\"\"\"",
        "        workflow = Workflow.load(WORKFLOWS_DIR / \"bugfix.yaml\")",
        "        bug_title = \"Memory leak in cache\"",
        "        variables = {\"bug_title\": bug_title, \"priority\": \"high\"}",
        "",
        "        tasks = run_workflow(workflow, variables, tasks_dir=self.temp_dir, dry_run=True)",
        "",
        "        # At least one description should contain the bug title",
        "        descriptions = [t.description for t in tasks if t.description]",
        "        self.assertTrue(any(bug_title in desc for desc in descriptions),",
        "                       \"At least one description should contain bug_title\")",
        "",
        "    def test_priority_substitution(self):",
        "        \"\"\"Verify priority variable is substituted correctly.\"\"\"",
        "        workflow = Workflow.load(WORKFLOWS_DIR / \"bugfix.yaml\")",
        "        variables = {\"bug_title\": \"Test\", \"priority\": \"low\"}",
        "",
        "        tasks = run_workflow(workflow, variables, tasks_dir=self.temp_dir, dry_run=True)",
        "",
        "        # First task (investigate) should have priority from variable",
        "        investigate_task = tasks[0]",
        "        self.assertEqual(investigate_task.priority, \"low\")",
        "",
        "    def test_effort_substitution_in_feature_workflow(self):",
        "        \"\"\"Verify effort variable is substituted in feature workflow.\"\"\"",
        "        workflow = Workflow.load(WORKFLOWS_DIR / \"feature.yaml\")",
        "        variables = {",
        "            \"feature_name\": \"Test\",",
        "            \"priority\": \"high\",",
        "            \"effort\": \"small\"  # Override default",
        "        }",
        "",
        "        tasks = run_workflow(workflow, variables, tasks_dir=self.temp_dir, dry_run=True)",
        "",
        "        # Implementation task should have small effort",
        "        implement_task = next(t for t in tasks if \"Implement:\" in t.title)",
        "        self.assertEqual(implement_task.effort, \"small\")",
        "",
        "    def test_multiple_variable_substitution(self):",
        "        \"\"\"Verify multiple variables work together.\"\"\"",
        "        workflow = Workflow.load(WORKFLOWS_DIR / \"feature.yaml\")",
        "        variables = {",
        "            \"feature_name\": \"Semantic search\",",
        "            \"priority\": \"high\",",
        "            \"effort\": \"large\"",
        "        }",
        "",
        "        tasks = run_workflow(workflow, variables, tasks_dir=self.temp_dir, dry_run=True)",
        "",
        "        # Check design task has both substitutions",
        "        design_task = tasks[0]",
        "        self.assertIn(\"Semantic search\", design_task.title)",
        "        self.assertEqual(design_task.priority, \"high\")",
        "        self.assertEqual(design_task.effort, \"medium\")  # Design has hardcoded medium",
        "",
        "        # Check implementation task",
        "        impl_task = next(t for t in tasks if \"Implement:\" in t.title)",
        "        self.assertIn(\"Semantic search\", impl_task.title)",
        "        self.assertEqual(impl_task.priority, \"high\")",
        "        self.assertEqual(impl_task.effort, \"large\")",
        "",
        "    def test_substitute_variables_function(self):",
        "        \"\"\"Test the substitute_variables helper function directly.\"\"\"",
        "        text = \"Fix: {bug_title} (Priority: {priority})\"",
        "        variables = {\"bug_title\": \"Auth error\", \"priority\": \"high\"}",
        "",
        "        result = substitute_variables(text, variables)",
        "",
        "        self.assertEqual(result, \"Fix: Auth error (Priority: high)\")",
        "        self.assertNotIn(\"{\", result)",
        "        self.assertNotIn(\"}\", result)",
        "",
        "",
        "class TestWorkflowValidation(unittest.TestCase):",
        "    \"\"\"Test workflow variable validation and error handling.\"\"\"",
        "",
        "    def setUp(self):",
        "        \"\"\"Create temporary directory for tasks.\"\"\"",
        "        self.temp_dir = tempfile.mkdtemp()",
        "",
        "    def tearDown(self):",
        "        \"\"\"Clean up temporary directory.\"\"\"",
        "        shutil.rmtree(self.temp_dir)",
        "",
        "    def test_missing_required_variable_raises_error(self):",
        "        \"\"\"Verify missing required variable raises ValueError.\"\"\"",
        "        workflow = Workflow.load(WORKFLOWS_DIR / \"bugfix.yaml\")",
        "        variables = {\"priority\": \"high\"}  # Missing bug_title",
        "",
        "        with self.assertRaises(ValueError) as ctx:",
        "            run_workflow(workflow, variables, tasks_dir=self.temp_dir, dry_run=True)",
        "",
        "        self.assertIn(\"bug_title\", str(ctx.exception))",
        "",
        "    def test_invalid_choice_raises_error(self):",
        "        \"\"\"Verify invalid choice value raises ValueError.\"\"\"",
        "        workflow = Workflow.load(WORKFLOWS_DIR / \"bugfix.yaml\")",
        "        variables = {",
        "            \"bug_title\": \"Test\",",
        "            \"priority\": \"urgent\"  # Invalid - not in choices",
        "        }",
        "",
        "        with self.assertRaises(ValueError) as ctx:",
        "            run_workflow(workflow, variables, tasks_dir=self.temp_dir, dry_run=True)",
        "",
        "        self.assertIn(\"priority\", str(ctx.exception))",
        "        self.assertIn(\"urgent\", str(ctx.exception))",
        "",
        "    def test_default_value_used_when_missing(self):",
        "        \"\"\"Verify default values are used for missing optional variables.\"\"\"",
        "        workflow = Workflow.load(WORKFLOWS_DIR / \"bugfix.yaml\")",
        "        variables = {\"bug_title\": \"Test\"}  # Missing priority, should use default",
        "",
        "        tasks = run_workflow(workflow, variables, tasks_dir=self.temp_dir, dry_run=True)",
        "",
        "        # Should use default priority \"high\"",
        "        self.assertTrue(any(t.priority == \"high\" for t in tasks))",
        "",
        "",
        "class TestDryRunMode(unittest.TestCase):",
        "    \"\"\"Test dry run mode doesn't create files.\"\"\"",
        "",
        "    def setUp(self):",
        "        \"\"\"Create temporary directory for tasks.\"\"\"",
        "        self.temp_dir = tempfile.mkdtemp()",
        "",
        "    def tearDown(self):",
        "        \"\"\"Clean up temporary directory.\"\"\"",
        "        shutil.rmtree(self.temp_dir)",
        "",
        "    def test_dry_run_does_not_create_files(self):",
        "        \"\"\"Verify dry_run=True doesn't create JSON files.\"\"\"",
        "        workflow = Workflow.load(WORKFLOWS_DIR / \"bugfix.yaml\")",
        "        variables = {\"bug_title\": \"Test\", \"priority\": \"high\"}",
        "",
        "        tasks = run_workflow(workflow, variables, tasks_dir=self.temp_dir, dry_run=True)",
        "",
        "        # Should return tasks",
        "        self.assertEqual(len(tasks), 4)",
        "",
        "        # But should not create files",
        "        json_files = list(Path(self.temp_dir).glob(\"*.json\"))",
        "        self.assertEqual(len(json_files), 0, \"Dry run should not create files\")",
        "",
        "    def test_normal_run_creates_files(self):",
        "        \"\"\"Verify dry_run=False creates files.\"\"\"",
        "        workflow = Workflow.load(WORKFLOWS_DIR / \"bugfix.yaml\")",
        "        variables = {\"bug_title\": \"Test\", \"priority\": \"high\"}",
        "",
        "        tasks = run_workflow(workflow, variables, tasks_dir=self.temp_dir, dry_run=False)",
        "",
        "        # Should create exactly one JSON file",
        "        json_files = list(Path(self.temp_dir).glob(\"*.json\"))",
        "        self.assertEqual(len(json_files), 1, \"Should create one JSON file\")",
        "",
        "",
        "if __name__ == \"__main__\":",
        "    unittest.main()"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "tests/unit/test_workflow.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "#!/usr/bin/env python3",
        "\"\"\"Unit tests for workflow template engine.\"\"\"",
        "",
        "import sys",
        "import tempfile",
        "import unittest",
        "from pathlib import Path",
        "from unittest.mock import MagicMock, patch, call",
        "",
        "# Add scripts to path",
        "sys.path.insert(0, str(Path(__file__).parent.parent.parent / \"scripts\"))",
        "",
        "from workflow import (",
        "    WorkflowVariable,",
        "    WorkflowTask,",
        "    Workflow,",
        "    substitute_variables,",
        "    run_workflow,",
        ")",
        "",
        "",
        "class TestWorkflowVariable(unittest.TestCase):",
        "    \"\"\"Tests for WorkflowVariable dataclass.\"\"\"",
        "",
        "    def test_variable_with_defaults(self):",
        "        \"\"\"WorkflowVariable should use default values.\"\"\"",
        "        var = WorkflowVariable(",
        "            name=\"test_var\",",
        "            description=\"Test variable\"",
        "        )",
        "        self.assertEqual(var.name, \"test_var\")",
        "        self.assertEqual(var.description, \"Test variable\")",
        "        self.assertTrue(var.required)",
        "        self.assertIsNone(var.default)",
        "        self.assertIsNone(var.choices)",
        "",
        "    def test_variable_with_all_fields(self):",
        "        \"\"\"WorkflowVariable should accept all optional fields.\"\"\"",
        "        var = WorkflowVariable(",
        "            name=\"priority\",",
        "            description=\"Task priority\",",
        "            required=False,",
        "            default=\"medium\",",
        "            choices=[\"low\", \"medium\", \"high\"]",
        "        )",
        "        self.assertEqual(var.name, \"priority\")",
        "        self.assertEqual(var.description, \"Task priority\")",
        "        self.assertFalse(var.required)",
        "        self.assertEqual(var.default, \"medium\")",
        "        self.assertEqual(var.choices, [\"low\", \"medium\", \"high\"])",
        "",
        "    def test_variable_required_false(self):",
        "        \"\"\"WorkflowVariable should allow required=False.\"\"\"",
        "        var = WorkflowVariable(",
        "            name=\"optional\",",
        "            description=\"Optional field\",",
        "            required=False",
        "        )",
        "        self.assertFalse(var.required)",
        "",
        "    def test_variable_with_default_no_choices(self):",
        "        \"\"\"WorkflowVariable should allow default without choices.\"\"\"",
        "        var = WorkflowVariable(",
        "            name=\"effort\",",
        "            description=\"Effort estimate\",",
        "            default=\"medium\"",
        "        )",
        "        self.assertEqual(var.default, \"medium\")",
        "        self.assertIsNone(var.choices)",
        "",
        "",
        "class TestWorkflowTask(unittest.TestCase):",
        "    \"\"\"Tests for WorkflowTask dataclass.\"\"\"",
        "",
        "    def test_task_post_init_none_depends_on(self):",
        "        \"\"\"__post_init__ should initialize depends_on to empty list if None.\"\"\"",
        "        task = WorkflowTask(",
        "            id=\"task1\",",
        "            title=\"Test Task\",",
        "            depends_on=None",
        "        )",
        "        self.assertEqual(task.depends_on, [])",
        "",
        "    def test_task_post_init_preserves_depends_on(self):",
        "        \"\"\"__post_init__ should preserve non-None depends_on.\"\"\"",
        "        task = WorkflowTask(",
        "            id=\"task1\",",
        "            title=\"Test Task\",",
        "            depends_on=[\"task0\"]",
        "        )",
        "        self.assertEqual(task.depends_on, [\"task0\"])",
        "",
        "    def test_task_with_defaults(self):",
        "        \"\"\"WorkflowTask should use default values.\"\"\"",
        "        task = WorkflowTask(",
        "            id=\"task1\",",
        "            title=\"Test Task\"",
        "        )",
        "        self.assertEqual(task.id, \"task1\")",
        "        self.assertEqual(task.title, \"Test Task\")",
        "        self.assertEqual(task.category, \"general\")",
        "        self.assertEqual(task.priority, \"medium\")",
        "        self.assertEqual(task.effort, \"medium\")",
        "        self.assertEqual(task.description, \"\")",
        "        self.assertEqual(task.depends_on, [])",
        "",
        "    def test_task_with_all_fields(self):",
        "        \"\"\"WorkflowTask should accept all fields.\"\"\"",
        "        task = WorkflowTask(",
        "            id=\"task1\",",
        "            title=\"Test Task\",",
        "            category=\"bugfix\",",
        "            priority=\"high\",",
        "            effort=\"large\",",
        "            description=\"Detailed description\",",
        "            depends_on=[\"task0\", \"task2\"]",
        "        )",
        "        self.assertEqual(task.id, \"task1\")",
        "        self.assertEqual(task.title, \"Test Task\")",
        "        self.assertEqual(task.category, \"bugfix\")",
        "        self.assertEqual(task.priority, \"high\")",
        "        self.assertEqual(task.effort, \"large\")",
        "        self.assertEqual(task.description, \"Detailed description\")",
        "        self.assertEqual(task.depends_on, [\"task0\", \"task2\"])",
        "",
        "",
        "class TestWorkflowFromDict(unittest.TestCase):",
        "    \"\"\"Tests for Workflow.from_dict() parsing.\"\"\"",
        "",
        "    def test_minimal_workflow(self):",
        "        \"\"\"from_dict should parse workflow with minimal fields.\"\"\"",
        "        data = {",
        "            \"name\": \"Test Workflow\",",
        "            \"description\": \"A test workflow\",",
        "            \"category\": \"test\"",
        "        }",
        "        workflow = Workflow.from_dict(data)",
        "        self.assertEqual(workflow.name, \"Test Workflow\")",
        "        self.assertEqual(workflow.description, \"A test workflow\")",
        "        self.assertEqual(workflow.category, \"test\")",
        "        self.assertEqual(workflow.variables, [])",
        "        self.assertEqual(workflow.tasks, [])",
        "",
        "    def test_workflow_with_empty_variables(self):",
        "        \"\"\"from_dict should handle empty variables list.\"\"\"",
        "        data = {",
        "            \"name\": \"Test\",",
        "            \"variables\": []",
        "        }",
        "        workflow = Workflow.from_dict(data)",
        "        self.assertEqual(workflow.variables, [])",
        "",
        "    def test_workflow_with_empty_tasks(self):",
        "        \"\"\"from_dict should handle empty tasks list.\"\"\"",
        "        data = {",
        "            \"name\": \"Test\",",
        "            \"tasks\": []",
        "        }",
        "        workflow = Workflow.from_dict(data)",
        "        self.assertEqual(workflow.tasks, [])",
        "",
        "    def test_workflow_with_variables(self):",
        "        \"\"\"from_dict should parse variables with all optional fields.\"\"\"",
        "        data = {",
        "            \"name\": \"Test\",",
        "            \"variables\": [",
        "                {",
        "                    \"name\": \"bug_title\",",
        "                    \"description\": \"Bug title\",",
        "                    \"required\": True",
        "                },",
        "                {",
        "                    \"name\": \"priority\",",
        "                    \"description\": \"Priority level\",",
        "                    \"required\": False,",
        "                    \"default\": \"medium\",",
        "                    \"choices\": [\"low\", \"medium\", \"high\"]",
        "                }",
        "            ]",
        "        }",
        "        workflow = Workflow.from_dict(data)",
        "        self.assertEqual(len(workflow.variables), 2)",
        "",
        "        var1 = workflow.variables[0]",
        "        self.assertEqual(var1.name, \"bug_title\")",
        "        self.assertEqual(var1.description, \"Bug title\")",
        "        self.assertTrue(var1.required)",
        "        self.assertIsNone(var1.default)",
        "        self.assertIsNone(var1.choices)",
        "",
        "        var2 = workflow.variables[1]",
        "        self.assertEqual(var2.name, \"priority\")",
        "        self.assertEqual(var2.description, \"Priority level\")",
        "        self.assertFalse(var2.required)",
        "        self.assertEqual(var2.default, \"medium\")",
        "        self.assertEqual(var2.choices, [\"low\", \"medium\", \"high\"])",
        "",
        "    def test_workflow_with_tasks(self):",
        "        \"\"\"from_dict should parse tasks with all fields.\"\"\"",
        "        data = {",
        "            \"name\": \"Test\",",
        "            \"tasks\": [",
        "                {",
        "                    \"id\": \"task1\",",
        "                    \"title\": \"Fix {bug_title}\",",
        "                    \"category\": \"bugfix\",",
        "                    \"priority\": \"high\",",
        "                    \"effort\": \"small\",",
        "                    \"description\": \"Fix the bug\",",
        "                    \"depends_on\": []",
        "                },",
        "                {",
        "                    \"id\": \"task2\",",
        "                    \"title\": \"Test fix\",",
        "                    \"depends_on\": [\"task1\"]",
        "                }",
        "            ]",
        "        }",
        "        workflow = Workflow.from_dict(data)",
        "        self.assertEqual(len(workflow.tasks), 2)",
        "",
        "        task1 = workflow.tasks[0]",
        "        self.assertEqual(task1.id, \"task1\")",
        "        self.assertEqual(task1.title, \"Fix {bug_title}\")",
        "        self.assertEqual(task1.category, \"bugfix\")",
        "        self.assertEqual(task1.priority, \"high\")",
        "        self.assertEqual(task1.effort, \"small\")",
        "        self.assertEqual(task1.description, \"Fix the bug\")",
        "        self.assertEqual(task1.depends_on, [])",
        "",
        "        task2 = workflow.tasks[1]",
        "        self.assertEqual(task2.id, \"task2\")",
        "        self.assertEqual(task2.title, \"Test fix\")",
        "        self.assertEqual(task2.category, \"general\")  # default",
        "        self.assertEqual(task2.priority, \"medium\")  # default",
        "        self.assertEqual(task2.depends_on, [\"task1\"])",
        "",
        "    def test_workflow_default_category(self):",
        "        \"\"\"from_dict should use 'general' as default category.\"\"\"",
        "        data = {",
        "            \"name\": \"Test\"",
        "        }",
        "        workflow = Workflow.from_dict(data)",
        "        self.assertEqual(workflow.category, \"general\")",
        "",
        "    def test_workflow_default_description(self):",
        "        \"\"\"from_dict should use empty string as default description.\"\"\"",
        "        data = {",
        "            \"name\": \"Test\"",
        "        }",
        "        workflow = Workflow.from_dict(data)",
        "        self.assertEqual(workflow.description, \"\")",
        "",
        "",
        "class TestSubstituteVariables(unittest.TestCase):",
        "    \"\"\"Tests for substitute_variables() function.\"\"\"",
        "",
        "    def test_single_variable(self):",
        "        \"\"\"Should substitute single variable placeholder.\"\"\"",
        "        text = \"Fix {bug_title}\"",
        "        variables = {\"bug_title\": \"Login crash\"}",
        "        result = substitute_variables(text, variables)",
        "        self.assertEqual(result, \"Fix Login crash\")",
        "",
        "    def test_multiple_variables(self):",
        "        \"\"\"Should substitute multiple variable placeholders.\"\"\"",
        "        text = \"Fix {bug_title} with {priority} priority\"",
        "        variables = {",
        "            \"bug_title\": \"Login crash\",",
        "            \"priority\": \"high\"",
        "        }",
        "        result = substitute_variables(text, variables)",
        "        self.assertEqual(result, \"Fix Login crash with high priority\")",
        "",
        "    def test_variable_not_in_dict(self):",
        "        \"\"\"Should leave placeholder unchanged if variable not in dict.\"\"\"",
        "        text = \"Fix {bug_title} with {priority}\"",
        "        variables = {\"bug_title\": \"Login crash\"}",
        "        result = substitute_variables(text, variables)",
        "        self.assertEqual(result, \"Fix Login crash with {priority}\")",
        "",
        "    def test_empty_variables_dict(self):",
        "        \"\"\"Should return original text if variables dict is empty.\"\"\"",
        "        text = \"Fix {bug_title}\"",
        "        variables = {}",
        "        result = substitute_variables(text, variables)",
        "        self.assertEqual(result, \"Fix {bug_title}\")",
        "",
        "    def test_no_placeholders(self):",
        "        \"\"\"Should return original text if no placeholders.\"\"\"",
        "        text = \"Simple text without placeholders\"",
        "        variables = {\"bug_title\": \"Something\"}",
        "        result = substitute_variables(text, variables)",
        "        self.assertEqual(result, \"Simple text without placeholders\")",
        "",
        "    def test_multiple_occurrences(self):",
        "        \"\"\"Should substitute all occurrences of same variable.\"\"\"",
        "        text = \"{priority} bug: {bug_title} needs {priority} attention\"",
        "        variables = {",
        "            \"bug_title\": \"Login crash\",",
        "            \"priority\": \"high\"",
        "        }",
        "        result = substitute_variables(text, variables)",
        "        self.assertEqual(result, \"high bug: Login crash needs high attention\")",
        "",
        "    def test_nested_braces(self):",
        "        \"\"\"Should handle text with non-variable braces.\"\"\"",
        "        text = \"Code: function() { return {value}; }\"",
        "        variables = {\"value\": \"42\"}",
        "        result = substitute_variables(text, variables)",
        "        self.assertEqual(result, \"Code: function() { return 42; }\")",
        "",
        "",
        "class TestRunWorkflow(unittest.TestCase):",
        "    \"\"\"Tests for run_workflow() function.\"\"\"",
        "",
        "    def test_required_variable_missing_raises_error(self):",
        "        \"\"\"Should raise ValueError if required variable is missing.\"\"\"",
        "        workflow = Workflow(",
        "            name=\"Test\",",
        "            description=\"Test workflow\",",
        "            category=\"test\",",
        "            variables=[",
        "                WorkflowVariable(",
        "                    name=\"bug_title\",",
        "                    description=\"Bug title\",",
        "                    required=True",
        "                )",
        "            ],",
        "            tasks=[]",
        "        )",
        "        variables = {}",
        "",
        "        with self.assertRaises(ValueError) as cm:",
        "            run_workflow(workflow, variables, dry_run=True)",
        "",
        "        self.assertIn(\"Missing required variable: bug_title\", str(cm.exception))",
        "",
        "    def test_required_variable_with_default_uses_default(self):",
        "        \"\"\"Should use default value if required variable is missing but has default.\"\"\"",
        "        workflow = Workflow(",
        "            name=\"Test\",",
        "            description=\"Test workflow\",",
        "            category=\"test\",",
        "            variables=[",
        "                WorkflowVariable(",
        "                    name=\"priority\",",
        "                    description=\"Priority\",",
        "                    required=True,",
        "                    default=\"medium\"",
        "                )",
        "            ],",
        "            tasks=[",
        "                WorkflowTask(",
        "                    id=\"task1\",",
        "                    title=\"Task with {priority} priority\",",
        "                    priority=\"{priority}\"",
        "                )",
        "            ]",
        "        )",
        "        variables = {}",
        "",
        "        tasks = run_workflow(workflow, variables, dry_run=True)",
        "",
        "        self.assertEqual(len(tasks), 1)",
        "        self.assertEqual(tasks[0].title, \"Task with medium priority\")",
        "        self.assertEqual(tasks[0].priority, \"medium\")",
        "",
        "    def test_choice_validation_valid(self):",
        "        \"\"\"Should accept valid choice value.\"\"\"",
        "        workflow = Workflow(",
        "            name=\"Test\",",
        "            description=\"Test workflow\",",
        "            category=\"test\",",
        "            variables=[",
        "                WorkflowVariable(",
        "                    name=\"priority\",",
        "                    description=\"Priority\",",
        "                    choices=[\"low\", \"medium\", \"high\"]",
        "                )",
        "            ],",
        "            tasks=[",
        "                WorkflowTask(",
        "                    id=\"task1\",",
        "                    title=\"Task\"",
        "                )",
        "            ]",
        "        )",
        "        variables = {\"priority\": \"high\"}",
        "",
        "        # Should not raise",
        "        tasks = run_workflow(workflow, variables, dry_run=True)",
        "        self.assertEqual(len(tasks), 1)",
        "",
        "    def test_choice_validation_invalid_raises_error(self):",
        "        \"\"\"Should raise ValueError if choice value is invalid.\"\"\"",
        "        workflow = Workflow(",
        "            name=\"Test\",",
        "            description=\"Test workflow\",",
        "            category=\"test\",",
        "            variables=[",
        "                WorkflowVariable(",
        "                    name=\"priority\",",
        "                    description=\"Priority\",",
        "                    choices=[\"low\", \"medium\", \"high\"]",
        "                )",
        "            ],",
        "            tasks=[]",
        "        )",
        "        variables = {\"priority\": \"invalid\"}",
        "",
        "        with self.assertRaises(ValueError) as cm:",
        "            run_workflow(workflow, variables, dry_run=True)",
        "",
        "        self.assertIn(\"Invalid value for priority: invalid\", str(cm.exception))",
        "        self.assertIn(\"Must be one of:\", str(cm.exception))",
        "",
        "    @patch('workflow.TaskSession')",
        "    def test_dry_run_mode_no_save(self, mock_session_class):",
        "        \"\"\"Dry run should create tasks but not save them.\"\"\"",
        "        mock_session = MagicMock()",
        "        mock_session_class.return_value = mock_session",
        "",
        "        # Mock the task creation",
        "        mock_task = MagicMock()",
        "        mock_task.id = \"T-test-001\"",
        "        mock_task.title = \"Test Task\"",
        "        mock_task.priority = \"medium\"",
        "        mock_task.description = \"\"",
        "        mock_task.depends_on = []",
        "        mock_session.create_task.return_value = mock_task",
        "",
        "        workflow = Workflow(",
        "            name=\"Test\",",
        "            description=\"Test workflow\",",
        "            category=\"test\",",
        "            variables=[],",
        "            tasks=[",
        "                WorkflowTask(",
        "                    id=\"task1\",",
        "                    title=\"Test Task\"",
        "                )",
        "            ]",
        "        )",
        "        variables = {}",
        "",
        "        tasks = run_workflow(workflow, variables, dry_run=True)",
        "",
        "        self.assertEqual(len(tasks), 1)",
        "        # save() should NOT be called in dry run mode",
        "        mock_session.save.assert_not_called()",
        "",
        "    @patch('workflow.TaskSession')",
        "    def test_normal_mode_saves(self, mock_session_class):",
        "        \"\"\"Normal mode should create and save tasks.\"\"\"",
        "        mock_session = MagicMock()",
        "        mock_session_class.return_value = mock_session",
        "",
        "        # Mock the task creation",
        "        mock_task = MagicMock()",
        "        mock_task.id = \"T-test-001\"",
        "        mock_task.title = \"Test Task\"",
        "        mock_task.priority = \"medium\"",
        "        mock_task.description = \"\"",
        "        mock_task.depends_on = []",
        "        mock_session.create_task.return_value = mock_task",
        "        mock_session.save.return_value = Path(\"tasks/test.json\")",
        "",
        "        workflow = Workflow(",
        "            name=\"Test\",",
        "            description=\"Test workflow\",",
        "            category=\"test\",",
        "            variables=[],",
        "            tasks=[",
        "                WorkflowTask(",
        "                    id=\"task1\",",
        "                    title=\"Test Task\"",
        "                )",
        "            ]",
        "        )",
        "        variables = {}",
        "",
        "        tasks = run_workflow(workflow, variables, dry_run=False)",
        "",
        "        self.assertEqual(len(tasks), 1)",
        "        # save() SHOULD be called in normal mode",
        "        mock_session.save.assert_called_once_with(\"tasks\")",
        "",
        "    @patch('workflow.TaskSession')",
        "    def test_task_dependency_resolution(self, mock_session_class):",
        "        \"\"\"Should resolve workflow task IDs to actual task IDs.\"\"\"",
        "        mock_session = MagicMock()",
        "        mock_session_class.return_value = mock_session",
        "",
        "        # Mock task creation to return different IDs",
        "        mock_task1 = MagicMock()",
        "        mock_task1.id = \"T-actual-001\"",
        "        mock_task1.title = \"Task 1\"",
        "        mock_task1.priority = \"medium\"",
        "        mock_task1.description = \"\"",
        "        mock_task1.depends_on = []",
        "",
        "        mock_task2 = MagicMock()",
        "        mock_task2.id = \"T-actual-002\"",
        "        mock_task2.title = \"Task 2\"",
        "        mock_task2.priority = \"medium\"",
        "        mock_task2.description = \"\"",
        "        mock_task2.depends_on = [\"T-actual-001\"]",
        "",
        "        mock_session.create_task.side_effect = [mock_task1, mock_task2]",
        "",
        "        workflow = Workflow(",
        "            name=\"Test\",",
        "            description=\"Test workflow\",",
        "            category=\"test\",",
        "            variables=[],",
        "            tasks=[",
        "                WorkflowTask(",
        "                    id=\"wf_task1\",",
        "                    title=\"Task 1\"",
        "                ),",
        "                WorkflowTask(",
        "                    id=\"wf_task2\",",
        "                    title=\"Task 2\",",
        "                    depends_on=[\"wf_task1\"]",
        "                )",
        "            ]",
        "        )",
        "        variables = {}",
        "",
        "        tasks = run_workflow(workflow, variables, dry_run=True)",
        "",
        "        self.assertEqual(len(tasks), 2)",
        "        # Second task should have actual ID of first task in depends_on",
        "        second_call = mock_session.create_task.call_args_list[1]",
        "        self.assertEqual(second_call[1]['depends_on'], [\"T-actual-001\"])",
        "",
        "    @patch('workflow.TaskSession')",
        "    def test_variable_substitution_in_all_fields(self, mock_session_class):",
        "        \"\"\"Should substitute variables in title, description, priority, effort.\"\"\"",
        "        mock_session = MagicMock()",
        "        mock_session_class.return_value = mock_session",
        "",
        "        mock_task = MagicMock()",
        "        mock_task.id = \"T-test-001\"",
        "        mock_task.title = \"Fix Login crash\"",
        "        mock_task.priority = \"high\"",
        "        mock_task.effort = \"large\"",
        "        mock_task.description = \"Critical bug: Login crash\"",
        "        mock_task.depends_on = []",
        "        mock_session.create_task.return_value = mock_task",
        "",
        "        workflow = Workflow(",
        "            name=\"Test\",",
        "            description=\"Test workflow\",",
        "            category=\"test\",",
        "            variables=[",
        "                WorkflowVariable(name=\"bug_title\", description=\"Bug title\"),",
        "                WorkflowVariable(name=\"priority\", description=\"Priority\"),",
        "                WorkflowVariable(name=\"effort\", description=\"Effort\")",
        "            ],",
        "            tasks=[",
        "                WorkflowTask(",
        "                    id=\"task1\",",
        "                    title=\"Fix {bug_title}\",",
        "                    priority=\"{priority}\",",
        "                    effort=\"{effort}\",",
        "                    description=\"Critical bug: {bug_title}\"",
        "                )",
        "            ]",
        "        )",
        "        variables = {",
        "            \"bug_title\": \"Login crash\",",
        "            \"priority\": \"high\",",
        "            \"effort\": \"large\"",
        "        }",
        "",
        "        tasks = run_workflow(workflow, variables, dry_run=True)",
        "",
        "        # Verify substitution happened in create_task call",
        "        call_args = mock_session.create_task.call_args",
        "        self.assertEqual(call_args[1]['title'], \"Fix Login crash\")",
        "        self.assertEqual(call_args[1]['priority'], \"high\")",
        "        self.assertEqual(call_args[1]['effort'], \"large\")",
        "        self.assertEqual(call_args[1]['description'], \"Critical bug: Login crash\")",
        "",
        "    @patch('workflow.TaskSession')",
        "    def test_multiple_tasks_created(self, mock_session_class):",
        "        \"\"\"Should create all tasks in workflow.\"\"\"",
        "        mock_session = MagicMock()",
        "        mock_session_class.return_value = mock_session",
        "",
        "        mock_tasks = []",
        "        for i in range(3):",
        "            mock_task = MagicMock()",
        "            mock_task.id = f\"T-test-{i:03d}\"",
        "            mock_task.title = f\"Task {i+1}\"",
        "            mock_task.priority = \"medium\"",
        "            mock_task.description = \"\"",
        "            mock_task.depends_on = []",
        "            mock_tasks.append(mock_task)",
        "",
        "        mock_session.create_task.side_effect = mock_tasks",
        "",
        "        workflow = Workflow(",
        "            name=\"Test\",",
        "            description=\"Test workflow\",",
        "            category=\"test\",",
        "            variables=[],",
        "            tasks=[",
        "                WorkflowTask(id=\"task1\", title=\"Task 1\"),",
        "                WorkflowTask(id=\"task2\", title=\"Task 2\"),",
        "                WorkflowTask(id=\"task3\", title=\"Task 3\")",
        "            ]",
        "        )",
        "        variables = {}",
        "",
        "        tasks = run_workflow(workflow, variables, dry_run=True)",
        "",
        "        self.assertEqual(len(tasks), 3)",
        "        self.assertEqual(mock_session.create_task.call_count, 3)",
        "",
        "    @patch('workflow.TaskSession')",
        "    def test_custom_tasks_dir(self, mock_session_class):",
        "        \"\"\"Should use custom tasks directory when provided.\"\"\"",
        "        mock_session = MagicMock()",
        "        mock_session_class.return_value = mock_session",
        "",
        "        mock_task = MagicMock()",
        "        mock_task.id = \"T-test-001\"",
        "        mock_task.title = \"Test Task\"",
        "        mock_task.priority = \"medium\"",
        "        mock_task.description = \"\"",
        "        mock_task.depends_on = []",
        "        mock_session.create_task.return_value = mock_task",
        "        mock_session.save.return_value = Path(\"custom_dir/test.json\")",
        "",
        "        workflow = Workflow(",
        "            name=\"Test\",",
        "            description=\"Test workflow\",",
        "            category=\"test\",",
        "            variables=[],",
        "            tasks=[",
        "                WorkflowTask(id=\"task1\", title=\"Test Task\")",
        "            ]",
        "        )",
        "        variables = {}",
        "",
        "        tasks = run_workflow(workflow, variables, tasks_dir=\"custom_dir\", dry_run=False)",
        "",
        "        mock_session.save.assert_called_once_with(\"custom_dir\")",
        "",
        "    @patch('workflow.TaskSession')",
        "    def test_missing_dependency_ignored(self, mock_session_class):",
        "        \"\"\"Should ignore dependencies that haven't been created yet.\"\"\"",
        "        mock_session = MagicMock()",
        "        mock_session_class.return_value = mock_session",
        "",
        "        mock_task = MagicMock()",
        "        mock_task.id = \"T-test-001\"",
        "        mock_task.title = \"Task\"",
        "        mock_task.priority = \"medium\"",
        "        mock_task.description = \"\"",
        "        mock_task.depends_on = []",
        "        mock_session.create_task.return_value = mock_task",
        "",
        "        workflow = Workflow(",
        "            name=\"Test\",",
        "            description=\"Test workflow\",",
        "            category=\"test\",",
        "            variables=[],",
        "            tasks=[",
        "                WorkflowTask(",
        "                    id=\"task1\",",
        "                    title=\"Task\",",
        "                    depends_on=[\"nonexistent_task\"]",
        "                )",
        "            ]",
        "        )",
        "        variables = {}",
        "",
        "        tasks = run_workflow(workflow, variables, dry_run=True)",
        "",
        "        # Should create task with empty depends_on (missing dependency ignored)",
        "        call_args = mock_session.create_task.call_args",
        "        self.assertEqual(call_args[1]['depends_on'], [])",
        "",
        "",
        "if __name__ == \"__main__\":",
        "    unittest.main()"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    }
  ],
  "hour_of_day": 0,
  "day_of_week": "Sunday",
  "seconds_since_last_commit": -134903,
  "is_merge": true,
  "is_initial": false,
  "parent_count": 2,
  "session_id": null,
  "related_chats": [],
  "ci_result": null,
  "reverted": false,
  "amended": false
}