{
  "hash": "900cce1a0e3fe7a136e0e8c202d3ed247f477958",
  "message": "Add batch query API (Task 18)",
  "author": "Claude",
  "timestamp": "2025-12-09 21:10:07 +0000",
  "branch": "claude/multi-index-design-DvifZ",
  "files_changed": [
    "TASK_LIST.md",
    "cortical/processor.py",
    "cortical/query.py",
    "tests/test_processor.py"
  ],
  "insertions": 447,
  "deletions": 11,
  "hunks": [
    {
      "file": "TASK_LIST.md",
      "function": "Current ranking is flat (Token TF-IDF → Document Score). Better RAG performanc",
      "start_line": 466,
      "lines_added": [
        "**Status:** [x] Completed",
        "**Solution Applied:**",
        "1. Added `find_documents_batch()` function to `query.py` with expansion caching",
        "2. Added `find_passages_batch()` function to `query.py` with chunk pre-computation",
        "3. Added corresponding methods to `CorticalTextProcessor`",
        "4. Both functions share tokenization and expansion caches across queries",
        "",
        "**Files Modified:**",
        "- `cortical/query.py` - Added batch query functions (~180 lines)",
        "- `cortical/processor.py` - Added processor wrapper methods (~90 lines)",
        "- `tests/test_processor.py` - Added 14 tests for batch query functionality",
        "",
        "**Usage Examples:**",
        "# Batch document search",
        "queries = [\"neural networks\", \"machine learning\", \"data processing\"]",
        "results = processor.find_documents_batch(queries, top_n=3)",
        "for query, docs in zip(queries, results):",
        "    print(f\"{query}: {[doc_id for doc_id, _ in docs]}\")",
        "",
        "# Batch passage search (for RAG)",
        "results = processor.find_passages_batch(queries, top_n=5, chunk_size=512)",
        "for query, passages in zip(queries, results):",
        "    print(f\"{query}: {len(passages)} passages found\")"
      ],
      "lines_removed": [
        "**Status:** [ ] Future Enhancement",
        "**Implementation:**",
        "def find_documents_batch(self, queries: List[str], top_n: int = 5):",
        "    \"\"\"Process multiple queries efficiently.\"\"\"",
        "    # Batch tokenization",
        "    # Shared expansion cache",
        "    # Parallel scoring"
      ],
      "context_before": [
        "1. **Stage 1 (Concepts):** Filter by topic relevance",
        "2. **Stage 2 (Documents):** Rank documents in topic",
        "3. **Stage 3 (Chunks):** Rank passages in documents",
        "4. **Stage 4 (Rerank):** Final relevance scoring",
        "",
        "---",
        "",
        "### 18. Add Batch Query API",
        "",
        "**Files:** `cortical/query.py`, `cortical/processor.py`"
      ],
      "context_after": [
        "",
        "**Problem:**",
        "No efficient way to run multiple queries. Each query repeats tokenization and expansion.",
        "",
        "```python",
        "```",
        "",
        "---",
        "",
        "## Summary",
        "",
        "| Priority | Task | Status | Category |",
        "|----------|------|--------|----------|",
        "| Critical | Fix TF-IDF per-doc calculation | ✅ Completed | Bug Fix |",
        "| High | Add ID lookup optimization | ✅ Completed | Bug Fix |"
      ],
      "change_type": "modify"
    },
    {
      "file": "TASK_LIST.md",
      "function": "def find_documents_batch(self, queries: List[str], top_n: int = 5):",
      "start_line": 502,
      "lines_added": [
        "| Low | Batch query API | ✅ Completed | RAG |",
        "**RAG Enhancement Completion:** 7/8 tasks (88%)",
        "Ran 158 tests in 0.153s"
      ],
      "lines_removed": [
        "| Low | Batch query API | ⬜ Future | RAG |",
        "**RAG Enhancement Completion:** 6/8 tasks (75%)",
        "Ran 144 tests in 0.141s"
      ],
      "context_before": [
        "| **Critical** | **Implement chunk-level retrieval** | ✅ Completed | **RAG** |",
        "| **Critical** | **Add document metadata support** | ✅ Completed | **RAG** |",
        "| **High** | **Activate Layer 2 concepts** | ✅ Completed | **RAG** |",
        "| **High** | **Integrate semantic relations** | ✅ Completed | **RAG** |",
        "| **High** | **Persist full computed state** | ✅ Completed | **RAG** |",
        "| Medium | Fix type annotation (embeddings.py) | ✅ Completed | Bug Fix |",
        "| Medium | Optimize spectral embeddings | ✅ Completed | Performance |",
        "| Medium | Add incremental indexing | ✅ Completed | RAG |",
        "| Low | Document magic numbers | ⏳ Deferred | Documentation |",
        "| Low | Multi-stage ranking pipeline | ⬜ Future | RAG |"
      ],
      "context_after": [
        "",
        "**Bug Fix Completion:** 7/7 tasks (100%)",
        "",
        "---",
        "",
        "## Test Results",
        "",
        "```",
        "OK",
        "```",
        "",
        "All tests passing as of 2025-12-09.",
        "",
        "---",
        "",
        "*Updated from code review on 2025-12-09*"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/processor.py",
      "function": "class CorticalTextProcessor:",
      "start_line": 560,
      "lines_added": [
        "",
        "    def find_documents_batch(",
        "        self,",
        "        queries: List[str],",
        "        top_n: int = 5,",
        "        use_expansion: bool = True,",
        "        use_semantic: bool = True",
        "    ) -> List[List[Tuple[str, float]]]:",
        "        \"\"\"",
        "        Find documents for multiple queries efficiently.",
        "",
        "        More efficient than calling find_documents_for_query() multiple times",
        "        because it shares tokenization and expansion caching across queries.",
        "",
        "        Args:",
        "            queries: List of search query strings",
        "            top_n: Number of documents to return per query",
        "            use_expansion: Whether to expand query terms using lateral connections",
        "            use_semantic: Whether to use semantic relations for expansion",
        "",
        "        Returns:",
        "            List of results, one per query. Each result is a list of (doc_id, score) tuples.",
        "",
        "        Example:",
        "            >>> queries = [\"neural networks\", \"machine learning\", \"data processing\"]",
        "            >>> results = processor.find_documents_batch(queries, top_n=3)",
        "            >>> for query, docs in zip(queries, results):",
        "            ...     print(f\"{query}: {[doc_id for doc_id, _ in docs]}\")",
        "        \"\"\"",
        "        return query_module.find_documents_batch(",
        "            queries,",
        "            self.layers,",
        "            self.tokenizer,",
        "            top_n=top_n,",
        "            use_expansion=use_expansion,",
        "            semantic_relations=self.semantic_relations if use_semantic else None,",
        "            use_semantic=use_semantic",
        "        )",
        "",
        "    def find_passages_batch(",
        "        self,",
        "        queries: List[str],",
        "        top_n: int = 5,",
        "        chunk_size: int = 512,",
        "        overlap: int = 128,",
        "        use_expansion: bool = True,",
        "        doc_filter: Optional[List[str]] = None,",
        "        use_semantic: bool = True",
        "    ) -> List[List[Tuple[str, str, int, int, float]]]:",
        "        \"\"\"",
        "        Find passages for multiple queries efficiently.",
        "",
        "        More efficient than calling find_passages_for_query() multiple times",
        "        because it shares chunk computation and expansion caching across queries.",
        "",
        "        Args:",
        "            queries: List of search query strings",
        "            top_n: Number of passages to return per query",
        "            chunk_size: Size of each chunk in characters (default 512)",
        "            overlap: Overlap between chunks in characters (default 128)",
        "            use_expansion: Whether to expand query terms",
        "            doc_filter: Optional list of doc_ids to restrict search to",
        "            use_semantic: Whether to use semantic relations for expansion",
        "",
        "        Returns:",
        "            List of results, one per query. Each result is a list of",
        "            (passage_text, doc_id, start_char, end_char, score) tuples.",
        "",
        "        Example:",
        "            >>> queries = [\"neural networks\", \"deep learning\"]",
        "            >>> results = processor.find_passages_batch(queries)",
        "            >>> for query, passages in zip(queries, results):",
        "            ...     print(f\"{query}: {len(passages)} passages found\")",
        "        \"\"\"",
        "        return query_module.find_passages_batch(",
        "            queries,",
        "            self.layers,",
        "            self.tokenizer,",
        "            self.documents,",
        "            top_n=top_n,",
        "            chunk_size=chunk_size,",
        "            overlap=overlap,",
        "            use_expansion=use_expansion,",
        "            doc_filter=doc_filter,",
        "            semantic_relations=self.semantic_relations if use_semantic else None,",
        "            use_semantic=use_semantic",
        "        )",
        ""
      ],
      "lines_removed": [
        "    "
      ],
      "context_before": [
        "            self.tokenizer,",
        "            self.documents,",
        "            top_n=top_n,",
        "            chunk_size=chunk_size,",
        "            overlap=overlap,",
        "            use_expansion=use_expansion,",
        "            doc_filter=doc_filter,",
        "            semantic_relations=self.semantic_relations if use_semantic else None,",
        "            use_semantic=use_semantic",
        "        )"
      ],
      "context_after": [
        "    def query_expanded(self, query_text: str, top_n: int = 10, max_expansions: int = 8) -> List[Tuple[str, float]]:",
        "        return query_module.query_with_spreading_activation(query_text, self.layers, self.tokenizer, top_n, max_expansions)",
        "    ",
        "    def find_related_documents(self, doc_id: str) -> List[Tuple[str, float]]:",
        "        return query_module.find_related_documents(doc_id, self.layers)",
        "    ",
        "    def analyze_knowledge_gaps(self) -> Dict:",
        "        return gaps_module.analyze_knowledge_gaps(self.layers, self.documents)",
        "    ",
        "    def detect_anomalies(self, threshold: float = 0.3) -> List[Dict]:"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/query.py",
      "function": "def find_passages_for_query(",
      "start_line": 488,
      "lines_added": [
        "",
        "",
        "def find_documents_batch(",
        "    queries: List[str],",
        "    layers: Dict[CorticalLayer, HierarchicalLayer],",
        "    tokenizer: Tokenizer,",
        "    top_n: int = 5,",
        "    use_expansion: bool = True,",
        "    semantic_relations: Optional[List[Tuple[str, str, str, float]]] = None,",
        "    use_semantic: bool = True",
        ") -> List[List[Tuple[str, float]]]:",
        "    \"\"\"",
        "    Find documents for multiple queries efficiently.",
        "",
        "    More efficient than calling find_documents_for_query() multiple times",
        "    because it shares tokenization and expansion caching across queries.",
        "",
        "    Args:",
        "        queries: List of search query strings",
        "        layers: Dictionary of layers",
        "        tokenizer: Tokenizer instance",
        "        top_n: Number of documents to return per query",
        "        use_expansion: Whether to expand query terms",
        "        semantic_relations: Optional list of semantic relations for expansion",
        "        use_semantic: Whether to use semantic relations for expansion",
        "",
        "    Returns:",
        "        List of results, one per query. Each result is a list of (doc_id, score) tuples.",
        "",
        "    Example:",
        "        >>> queries = [\"neural networks\", \"machine learning\", \"data processing\"]",
        "        >>> results = find_documents_batch(queries, layers, tokenizer, top_n=3)",
        "        >>> for query, docs in zip(queries, results):",
        "        ...     print(f\"{query}: {[doc_id for doc_id, _ in docs]}\")",
        "    \"\"\"",
        "    layer0 = layers[CorticalLayer.TOKENS]",
        "",
        "    # Cache for expanded query terms to avoid redundant computation",
        "    expansion_cache: Dict[str, Dict[str, float]] = {}",
        "",
        "    all_results: List[List[Tuple[str, float]]] = []",
        "",
        "    for query_text in queries:",
        "        # Check cache first for expansion",
        "        if use_expansion:",
        "            if query_text in expansion_cache:",
        "                query_terms = expansion_cache[query_text]",
        "            else:",
        "                query_terms = expand_query(query_text, layers, tokenizer, max_expansions=5)",
        "                if use_semantic and semantic_relations:",
        "                    semantic_terms = expand_query_semantic(",
        "                        query_text, layers, tokenizer, semantic_relations, max_expansions=5",
        "                    )",
        "                    for term, weight in semantic_terms.items():",
        "                        if term not in query_terms:",
        "                            query_terms[term] = weight * 0.8",
        "                        else:",
        "                            query_terms[term] = max(query_terms[term], weight * 0.8)",
        "                expansion_cache[query_text] = query_terms",
        "        else:",
        "            tokens = tokenizer.tokenize(query_text)",
        "            query_terms = {t: 1.0 for t in tokens}",
        "",
        "        # Score documents",
        "        doc_scores: Dict[str, float] = defaultdict(float)",
        "        for term, term_weight in query_terms.items():",
        "            col = layer0.get_minicolumn(term)",
        "            if col:",
        "                for doc_id in col.document_ids:",
        "                    tfidf = col.tfidf_per_doc.get(doc_id, col.tfidf)",
        "                    doc_scores[doc_id] += tfidf * term_weight",
        "",
        "        sorted_docs = sorted(doc_scores.items(), key=lambda x: -x[1])",
        "        all_results.append(sorted_docs[:top_n])",
        "",
        "    return all_results",
        "",
        "",
        "def find_passages_batch(",
        "    queries: List[str],",
        "    layers: Dict[CorticalLayer, HierarchicalLayer],",
        "    tokenizer: Tokenizer,",
        "    documents: Dict[str, str],",
        "    top_n: int = 5,",
        "    chunk_size: int = 512,",
        "    overlap: int = 128,",
        "    use_expansion: bool = True,",
        "    doc_filter: Optional[List[str]] = None,",
        "    semantic_relations: Optional[List[Tuple[str, str, str, float]]] = None,",
        "    use_semantic: bool = True",
        ") -> List[List[Tuple[str, str, int, int, float]]]:",
        "    \"\"\"",
        "    Find passages for multiple queries efficiently.",
        "",
        "    More efficient than calling find_passages_for_query() multiple times",
        "    because it shares chunk computation and expansion caching across queries.",
        "",
        "    Args:",
        "        queries: List of search query strings",
        "        layers: Dictionary of layers",
        "        tokenizer: Tokenizer instance",
        "        documents: Dict mapping doc_id to document text",
        "        top_n: Number of passages to return per query",
        "        chunk_size: Size of each chunk in characters",
        "        overlap: Overlap between chunks in characters",
        "        use_expansion: Whether to expand query terms",
        "        doc_filter: Optional list of doc_ids to restrict search to",
        "        semantic_relations: Optional list of semantic relations for expansion",
        "        use_semantic: Whether to use semantic relations for expansion",
        "",
        "    Returns:",
        "        List of results, one per query. Each result is a list of",
        "        (passage_text, doc_id, start_char, end_char, score) tuples.",
        "",
        "    Example:",
        "        >>> queries = [\"neural networks\", \"deep learning\"]",
        "        >>> results = find_passages_batch(queries, layers, tokenizer, documents)",
        "        >>> for query, passages in zip(queries, results):",
        "        ...     print(f\"{query}: {len(passages)} passages found\")",
        "    \"\"\"",
        "    layer0 = layers[CorticalLayer.TOKENS]",
        "",
        "    # Pre-compute chunks for all documents to avoid redundant chunking",
        "    doc_chunks_cache: Dict[str, List[Tuple[str, int, int]]] = {}",
        "    for doc_id, text in documents.items():",
        "        if doc_filter and doc_id not in doc_filter:",
        "            continue",
        "        doc_chunks_cache[doc_id] = create_chunks(text, chunk_size, overlap)",
        "",
        "    # Cache for expanded query terms",
        "    expansion_cache: Dict[str, Dict[str, float]] = {}",
        "",
        "    all_results: List[List[Tuple[str, str, int, int, float]]] = []",
        "",
        "    for query_text in queries:",
        "        # Get expanded query terms (with caching)",
        "        if use_expansion:",
        "            if query_text in expansion_cache:",
        "                query_terms = expansion_cache[query_text]",
        "            else:",
        "                query_terms = expand_query(query_text, layers, tokenizer, max_expansions=5)",
        "                if use_semantic and semantic_relations:",
        "                    semantic_terms = expand_query_semantic(",
        "                        query_text, layers, tokenizer, semantic_relations, max_expansions=5",
        "                    )",
        "                    for term, weight in semantic_terms.items():",
        "                        if term not in query_terms:",
        "                            query_terms[term] = weight * 0.8",
        "                        else:",
        "                            query_terms[term] = max(query_terms[term], weight * 0.8)",
        "                expansion_cache[query_text] = query_terms",
        "        else:",
        "            tokens = tokenizer.tokenize(query_text)",
        "            query_terms = {t: 1.0 for t in tokens}",
        "",
        "        if not query_terms:",
        "            all_results.append([])",
        "            continue",
        "",
        "        # Get candidate documents",
        "        doc_scores = find_documents_for_query(",
        "            query_text, layers, tokenizer,",
        "            top_n=min(len(documents), top_n * 3),",
        "            use_expansion=use_expansion,",
        "            semantic_relations=semantic_relations,",
        "            use_semantic=use_semantic",
        "        )",
        "",
        "        # Apply document filter",
        "        if doc_filter:",
        "            doc_scores = [(doc_id, score) for doc_id, score in doc_scores if doc_id in doc_filter]",
        "",
        "        # Score passages using cached chunks",
        "        passages: List[Tuple[str, str, int, int, float]] = []",
        "",
        "        for doc_id, doc_score in doc_scores:",
        "            if doc_id not in doc_chunks_cache:",
        "                continue",
        "",
        "            for chunk_text, start_char, end_char in doc_chunks_cache[doc_id]:",
        "                chunk_score = score_chunk(",
        "                    chunk_text, query_terms, layer0, tokenizer, doc_id",
        "                )",
        "                combined_score = chunk_score * (1 + doc_score * 0.1)",
        "                passages.append((chunk_text, doc_id, start_char, end_char, combined_score))",
        "",
        "        passages.sort(key=lambda x: x[4], reverse=True)",
        "        all_results.append(passages[:top_n])",
        "",
        "    return all_results"
      ],
      "lines_removed": [],
      "context_before": [
        "                chunk_text,",
        "                doc_id,",
        "                start_char,",
        "                end_char,",
        "                combined_score",
        "            ))",
        "",
        "    # Sort by score and return top passages",
        "    passages.sort(key=lambda x: x[4], reverse=True)",
        "    return passages[:top_n]"
      ],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "tests/test_processor.py",
      "function": "class TestProcessorGaps(unittest.TestCase):",
      "start_line": 353,
      "lines_added": [
        "class TestProcessorBatchQuery(unittest.TestCase):",
        "    \"\"\"Test batch query functionality for efficient multi-query search.\"\"\"",
        "",
        "    @classmethod",
        "    def setUpClass(cls):",
        "        cls.processor = CorticalTextProcessor()",
        "        cls.processor.process_document(\"neural_doc\", \"\"\"",
        "            Neural networks are computational models inspired by biological neurons.",
        "            Deep learning uses many layers to learn hierarchical representations.",
        "            Backpropagation is the key algorithm for training neural networks.",
        "        \"\"\")",
        "        cls.processor.process_document(\"ml_doc\", \"\"\"",
        "            Machine learning algorithms learn patterns from data automatically.",
        "            Supervised learning requires labeled training examples.",
        "            Model evaluation uses metrics like accuracy and precision.",
        "        \"\"\")",
        "        cls.processor.process_document(\"data_doc\", \"\"\"",
        "            Data preprocessing is essential for machine learning pipelines.",
        "            Feature engineering creates meaningful input representations.",
        "            Data normalization scales features to similar ranges.",
        "        \"\"\")",
        "        cls.processor.compute_all(verbose=False)",
        "",
        "    def test_find_documents_batch_returns_list(self):",
        "        \"\"\"Test that find_documents_batch returns a list of results.\"\"\"",
        "        queries = [\"neural networks\", \"machine learning\"]",
        "        results = self.processor.find_documents_batch(queries, top_n=2)",
        "        self.assertIsInstance(results, list)",
        "        self.assertEqual(len(results), 2)",
        "",
        "    def test_find_documents_batch_result_structure(self):",
        "        \"\"\"Test that each result has correct structure.\"\"\"",
        "        queries = [\"neural\", \"data\"]",
        "        results = self.processor.find_documents_batch(queries, top_n=3)",
        "        for result in results:",
        "            self.assertIsInstance(result, list)",
        "            for doc_id, score in result:",
        "                self.assertIsInstance(doc_id, str)",
        "                self.assertIsInstance(score, float)",
        "",
        "    def test_find_documents_batch_returns_relevant_docs(self):",
        "        \"\"\"Test that batch queries return relevant documents.\"\"\"",
        "        queries = [\"neural networks\", \"data preprocessing\"]",
        "        results = self.processor.find_documents_batch(queries, top_n=1)",
        "        # First query should find neural_doc",
        "        self.assertGreater(len(results[0]), 0)",
        "        self.assertEqual(results[0][0][0], \"neural_doc\")",
        "        # Second query should find data_doc",
        "        self.assertGreater(len(results[1]), 0)",
        "        self.assertEqual(results[1][0][0], \"data_doc\")",
        "",
        "    def test_find_documents_batch_top_n(self):",
        "        \"\"\"Test that top_n limits results per query.\"\"\"",
        "        queries = [\"learning\", \"neural\"]",
        "        results = self.processor.find_documents_batch(queries, top_n=2)",
        "        for result in results:",
        "            self.assertLessEqual(len(result), 2)",
        "",
        "    def test_find_documents_batch_empty_query_list(self):",
        "        \"\"\"Test batch with empty query list.\"\"\"",
        "        results = self.processor.find_documents_batch([], top_n=3)",
        "        self.assertEqual(results, [])",
        "",
        "    def test_find_documents_batch_no_expansion(self):",
        "        \"\"\"Test batch query without expansion.\"\"\"",
        "        queries = [\"neural\", \"data\"]",
        "        results = self.processor.find_documents_batch(",
        "            queries, top_n=2, use_expansion=False",
        "        )",
        "        self.assertEqual(len(results), 2)",
        "",
        "    def test_find_passages_batch_returns_list(self):",
        "        \"\"\"Test that find_passages_batch returns a list of results.\"\"\"",
        "        queries = [\"neural networks\", \"machine learning\"]",
        "        results = self.processor.find_passages_batch(queries, top_n=2)",
        "        self.assertIsInstance(results, list)",
        "        self.assertEqual(len(results), 2)",
        "",
        "    def test_find_passages_batch_result_structure(self):",
        "        \"\"\"Test that each passage result has correct structure.\"\"\"",
        "        queries = [\"neural\"]",
        "        results = self.processor.find_passages_batch(queries, top_n=3)",
        "        self.assertEqual(len(results), 1)",
        "        for passage, doc_id, start, end, score in results[0]:",
        "            self.assertIsInstance(passage, str)",
        "            self.assertIsInstance(doc_id, str)",
        "            self.assertIsInstance(start, int)",
        "            self.assertIsInstance(end, int)",
        "            self.assertIsInstance(score, float)",
        "",
        "    def test_find_passages_batch_top_n(self):",
        "        \"\"\"Test that top_n limits passages per query.\"\"\"",
        "        queries = [\"learning\", \"neural\"]",
        "        results = self.processor.find_passages_batch(queries, top_n=2)",
        "        for result in results:",
        "            self.assertLessEqual(len(result), 2)",
        "",
        "    def test_find_passages_batch_chunk_size(self):",
        "        \"\"\"Test that chunk_size is respected.\"\"\"",
        "        queries = [\"neural\"]",
        "        results = self.processor.find_passages_batch(",
        "            queries, top_n=5, chunk_size=100, overlap=20",
        "        )",
        "        for passage, _, _, _, _ in results[0]:",
        "            self.assertLessEqual(len(passage), 100)",
        "",
        "    def test_find_passages_batch_doc_filter(self):",
        "        \"\"\"Test that doc_filter restricts results.\"\"\"",
        "        queries = [\"learning\", \"neural\"]",
        "        results = self.processor.find_passages_batch(",
        "            queries, top_n=10, doc_filter=[\"neural_doc\"]",
        "        )",
        "        for result in results:",
        "            for _, doc_id, _, _, _ in result:",
        "                self.assertEqual(doc_id, \"neural_doc\")",
        "",
        "    def test_find_passages_batch_empty_query_list(self):",
        "        \"\"\"Test batch with empty query list.\"\"\"",
        "        results = self.processor.find_passages_batch([], top_n=3)",
        "        self.assertEqual(results, [])",
        "",
        "    def test_batch_query_consistency(self):",
        "        \"\"\"Test that batch results match individual queries.\"\"\"",
        "        queries = [\"neural networks\", \"data processing\"]",
        "        batch_results = self.processor.find_documents_batch(queries, top_n=3)",
        "",
        "        # Compare with individual queries",
        "        for i, query in enumerate(queries):",
        "            individual_result = self.processor.find_documents_for_query(query, top_n=3)",
        "            # Results should be the same (or very close)",
        "            self.assertEqual(len(batch_results[i]), len(individual_result))",
        "            for j, (doc_id, score) in enumerate(batch_results[i]):",
        "                self.assertEqual(doc_id, individual_result[j][0])",
        "",
        "    def test_batch_handles_nonexistent_terms(self):",
        "        \"\"\"Test that batch handles queries with no matches.\"\"\"",
        "        queries = [\"xyznonexistent123\", \"neural networks\"]",
        "        results = self.processor.find_documents_batch(queries, top_n=3)",
        "        self.assertEqual(len(results), 2)",
        "        self.assertEqual(len(results[0]), 0)  # No matches for nonexistent",
        "        self.assertGreater(len(results[1]), 0)  # Matches for neural networks",
        "",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "        self.assertIn('isolated_documents', gaps)",
        "        self.assertIn('weak_topics', gaps)",
        "        self.assertIn('coverage_score', gaps)",
        "",
        "    def test_detect_anomalies(self):",
        "        \"\"\"Test anomaly detection.\"\"\"",
        "        anomalies = self.processor.detect_anomalies(threshold=0.1)",
        "        self.assertIsInstance(anomalies, list)",
        "",
        ""
      ],
      "context_after": [
        "class TestProcessorIncrementalIndexing(unittest.TestCase):",
        "    \"\"\"Test incremental document indexing functionality.\"\"\"",
        "",
        "    def setUp(self):",
        "        self.processor = CorticalTextProcessor()",
        "",
        "    def test_add_document_incremental_returns_stats(self):",
        "        \"\"\"Test that add_document_incremental returns processing stats.\"\"\"",
        "        stats = self.processor.add_document_incremental(",
        "            \"doc1\", \"Neural networks process information.\", recompute='tfidf'"
      ],
      "change_type": "add"
    }
  ],
  "hour_of_day": 21,
  "day_of_week": "Tuesday",
  "seconds_since_last_commit": -491681,
  "is_merge": false,
  "is_initial": false,
  "parent_count": 1,
  "session_id": null,
  "related_chats": [],
  "ci_result": null,
  "reverted": false,
  "amended": false
}