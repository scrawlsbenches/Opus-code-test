{
  "hash": "2571bb89e35f0ac52b2ee7597f11c44bb9c8ce69",
  "message": "Add code-aware tokenization with identifier splitting (Task #48)",
  "author": "Claude",
  "timestamp": "2025-12-10 14:04:11 +0000",
  "branch": "claude/multi-index-design-DvifZ",
  "files_changed": [
    "TASK_LIST.md",
    "cortical/tokenizer.py",
    "tests/test_tokenizer.py"
  ],
  "insertions": 288,
  "deletions": 23,
  "hunks": [
    {
      "file": "TASK_LIST.md",
      "function": "Currently these are always 0 due to the bug.",
      "start_line": 1665,
      "lines_added": [
        "**Files:** `cortical/tokenizer.py`, `tests/test_tokenizer.py`",
        "**Status:** [x] Completed",
        "**Solution Applied:**",
        "1. Added `split_identifier()` function to break camelCase, PascalCase, snake_case, and SCREAMING_SNAKE_CASE",
        "2. Added `PROGRAMMING_KEYWORDS` constant for common code terms (function, class, def, get, set, etc.)",
        "3. Added `split_identifiers` parameter to `Tokenizer.__init__()` and `tokenize()` method",
        "4. Tokens include both original identifier and split components when enabled",
        "5. Split parts don't duplicate already-seen tokens, preserving proper bigram extraction",
        "tokenizer = Tokenizer(split_identifiers=True)",
        "tokens = tokenizer.tokenize(\"getUserCredentials\")",
        "# ['getusercredentials', 'get', 'user', 'credentials']",
        "**Tests Added:**",
        "- 8 tests for `split_identifier()` function (camelCase, PascalCase, snake_case, acronyms)",
        "- 8 tests for code-aware tokenization (splitting, stop word filtering, min length, deduplication)",
        ""
      ],
      "lines_removed": [
        "**Files:** `cortical/tokenizer.py`",
        "**Status:** [ ] Not Started",
        "**Solution:**",
        "1. Add `split_identifier()` function to break camelCase and snake_case",
        "2. Expand tokens to include both original and split forms",
        "3. Add programming keyword recognition (function, class, def, return, etc.)",
        "4. Preserve meaningful punctuation context (parentheses for calls, brackets for indexing)",
        "# Input: \"getUserCredentials\"",
        "# Output tokens: [\"getUserCredentials\", \"get\", \"user\", \"credentials\"]"
      ],
      "context_before": [
        "---",
        "",
        "## Intent-Based Code Search Enhancements",
        "",
        "The following tasks enhance the system's ability to understand developer intent and retrieve code by meaning rather than exact keyword matching.",
        "",
        "---",
        "",
        "### 48. Add Code-Aware Tokenization",
        ""
      ],
      "context_after": [
        "**Priority:** High",
        "",
        "**Problem:**",
        "Current tokenizer treats code like prose. It doesn't understand that `getUserCredentials`, `get_user_credentials`, and `fetch user credentials` are semantically equivalent.",
        "",
        "",
        "**Example:**",
        "```python",
        "```",
        "",
        "---",
        "",
        "### 49. Add Synonym/Concept Mapping for Code Patterns",
        "",
        "**Files:** `cortical/semantics.py`, new `cortical/code_concepts.py`",
        "**Status:** [ ] Not Started",
        "**Priority:** High",
        "",
        "**Problem:**",
        "The system doesn't know that \"fetch\", \"get\", \"retrieve\", \"load\" are often interchangeable in code contexts, or that \"auth\", \"authentication\", \"credentials\", \"login\" form a concept cluster."
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/tokenizer.py",
      "function": "Tokenizer Module",
      "start_line": 3,
      "lines_added": [
        "from typing import List, Set, Optional, Dict, Tuple",
        "",
        "",
        "# Programming keywords that should be preserved even if in stop words",
        "PROGRAMMING_KEYWORDS = frozenset({",
        "    'def', 'class', 'function', 'return', 'import', 'from', 'if', 'else',",
        "    'elif', 'for', 'while', 'try', 'except', 'finally', 'with', 'as',",
        "    'yield', 'async', 'await', 'lambda', 'pass', 'break', 'continue',",
        "    'raise', 'assert', 'global', 'nonlocal', 'del', 'true', 'false',",
        "    'none', 'null', 'void', 'int', 'str', 'float', 'bool', 'list',",
        "    'dict', 'set', 'tuple', 'self', 'cls', 'init', 'main', 'args',",
        "    'kwargs', 'super', 'property', 'staticmethod', 'classmethod',",
        "    'isinstance', 'hasattr', 'getattr', 'setattr', 'len', 'range',",
        "    'enumerate', 'zip', 'map', 'filter', 'print', 'open', 'read',",
        "    'write', 'close', 'append', 'extend', 'insert', 'remove', 'pop',",
        "    'const', 'let', 'var', 'public', 'private', 'protected', 'static',",
        "    'final', 'abstract', 'interface', 'implements', 'extends', 'new',",
        "    'this', 'constructor', 'module', 'export', 'require', 'package',",
        "    # Common identifier components that shouldn't be filtered",
        "    'get', 'set', 'add', 'put', 'has', 'can', 'run', 'max', 'min',",
        "})",
        "",
        "",
        "def split_identifier(identifier: str) -> List[str]:",
        "    \"\"\"",
        "    Split a code identifier into component words.",
        "",
        "    Handles camelCase, PascalCase, snake_case, and SCREAMING_SNAKE_CASE.",
        "",
        "    Args:",
        "        identifier: A code identifier like \"getUserCredentials\" or \"get_user_data\"",
        "",
        "    Returns:",
        "        List of component words in lowercase",
        "",
        "    Examples:",
        "        >>> split_identifier(\"getUserCredentials\")",
        "        ['get', 'user', 'credentials']",
        "        >>> split_identifier(\"get_user_data\")",
        "        ['get', 'user', 'data']",
        "        >>> split_identifier(\"XMLParser\")",
        "        ['xml', 'parser']",
        "        >>> split_identifier(\"parseHTTPResponse\")",
        "        ['parse', 'http', 'response']",
        "    \"\"\"",
        "    if not identifier:",
        "        return []",
        "",
        "    # Handle snake_case and SCREAMING_SNAKE_CASE",
        "    if '_' in identifier:",
        "        parts = [p for p in identifier.split('_') if p]",
        "        # Recursively split any camelCase parts",
        "        result = []",
        "        for part in parts:",
        "            if any(c.isupper() for c in part):  # Has any capitals - could be camelCase",
        "                result.extend(split_identifier(part))",
        "            else:",
        "                result.append(part.lower())",
        "        return [p for p in result if p]",
        "",
        "    # Handle camelCase and PascalCase",
        "    # Insert space before uppercase letters, handling acronyms",
        "    # \"parseHTTPResponse\" -> \"parse HTTP Response\" -> [\"parse\", \"http\", \"response\"]",
        "    result = []",
        "    current = []",
        "",
        "    for i, char in enumerate(identifier):",
        "        if char.isupper():",
        "            # Check if this starts a new word",
        "            if current:",
        "                # If previous was lowercase, this starts a new word",
        "                if current[-1].islower():",
        "                    result.append(''.join(current).lower())",
        "                    current = [char]",
        "                # If next char is lowercase, this uppercase starts a new word (end of acronym)",
        "                elif i + 1 < len(identifier) and identifier[i + 1].islower():",
        "                    result.append(''.join(current).lower())",
        "                    current = [char]",
        "                else:",
        "                    # Continue building acronym",
        "                    current.append(char)",
        "            else:",
        "                current.append(char)",
        "        else:",
        "            current.append(char)",
        "",
        "    if current:",
        "        result.append(''.join(current).lower())",
        "",
        "    return [p for p in result if p]"
      ],
      "lines_removed": [
        "from typing import List, Set, Optional, Dict"
      ],
      "context_before": [
        "================",
        "",
        "Text tokenization with stemming and word variant support.",
        "",
        "Like early visual processing, the tokenizer extracts basic features",
        "(words) from raw input, filtering noise (stop words) and normalizing",
        "representations (lowercase, stemming).",
        "\"\"\"",
        "",
        "import re"
      ],
      "context_after": [
        "",
        "",
        "class Tokenizer:",
        "    \"\"\"",
        "    Text tokenizer with stemming and word variant support.",
        "    ",
        "    Extracts tokens from text, filters stop words, and provides",
        "    word variant expansion for query normalization.",
        "    ",
        "    Attributes:"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/tokenizer.py",
      "function": "class Tokenizer:",
      "start_line": 90,
      "lines_added": [
        "    def __init__(",
        "        self,",
        "        stop_words: Optional[Set[str]] = None,",
        "        min_word_length: int = 3,",
        "        split_identifiers: bool = False",
        "    ):",
        "",
        "            split_identifiers: If True, split camelCase/snake_case and include",
        "                               both original and component tokens.",
        "        self.split_identifiers = split_identifiers"
      ],
      "lines_removed": [
        "    def __init__(self, stop_words: Optional[Set[str]] = None, min_word_length: int = 3):",
        "        "
      ],
      "context_before": [
        "        # Common nouns (too generic)",
        "        'way', 'ways', 'thing', 'things', 'time', 'times', 'year', 'years',",
        "        'day', 'days', 'place', 'part', 'parts', 'case', 'cases', 'point',",
        "        'fact', 'kind', 'type', 'form', 'forms', 'level', 'area', 'areas',",
        "        # Common adjectives (too generic)",
        "        'new', 'old', 'good', 'bad', 'great', 'small', 'large', 'big', 'long',",
        "        'high', 'low', 'right', 'left', 'possible', 'important', 'major',",
        "        'available', 'able', 'like', 'different', 'similar'",
        "    })",
        "    "
      ],
      "context_after": [
        "        \"\"\"",
        "        Initialize tokenizer.",
        "        Args:",
        "            stop_words: Set of words to filter out. Uses defaults if None.",
        "            min_word_length: Minimum word length to keep.",
        "        \"\"\"",
        "        self.stop_words = stop_words if stop_words is not None else self.DEFAULT_STOP_WORDS",
        "        self.min_word_length = min_word_length",
        "        ",
        "        # Simple suffix rules for stemming (Porter-lite)",
        "        self._suffix_rules = [",
        "            ('ational', 'ate'), ('tional', 'tion'), ('enci', 'ence'),",
        "            ('anci', 'ance'), ('izer', 'ize'), ('isation', 'ize'),",
        "            ('ization', 'ize'), ('ation', 'ate'), ('ator', 'ate'),",
        "            ('alism', 'al'), ('iveness', 'ive'), ('fulness', 'ful'),",
        "            ('ousness', 'ous'), ('aliti', 'al'), ('iviti', 'ive'),",
        "            ('biliti', 'ble'), ('ement', ''), ('ment', ''), ('ness', ''),",
        "            ('ling', ''), ('ing', ''), ('ies', 'y'), ('ied', 'y'),"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/tokenizer.py",
      "function": "class Tokenizer:",
      "start_line": 137,
      "lines_added": [
        "    def tokenize(self, text: str, split_identifiers: Optional[bool] = None) -> List[str]:",
        "",
        "            split_identifiers: Override instance setting. If True, split",
        "                              camelCase/snake_case identifiers into components.",
        "",
        "",
        "        Examples:",
        "            >>> t = Tokenizer(split_identifiers=True)",
        "            >>> t.tokenize(\"getUserCredentials fetches data\")",
        "            ['getusercredentials', 'get', 'user', 'credentials', 'fetches', 'data']",
        "        should_split = split_identifiers if split_identifiers is not None else self.split_identifiers",
        "",
        "        # Extract potential identifiers (including camelCase with internal caps)",
        "        # Pattern matches: word2vec, getUserData, get_user_data, XMLParser",
        "        raw_tokens = re.findall(r'\\b[a-zA-Z][a-zA-Z0-9_]*\\b', text)",
        "",
        "        result = []",
        "        seen_splits = set()  # Only track splits to avoid duplicates from them",
        "",
        "        for token in raw_tokens:",
        "            token_lower = token.lower()",
        "",
        "            # Skip stop words and short words",
        "            if token_lower in self.stop_words or len(token_lower) < self.min_word_length:",
        "                continue",
        "",
        "            # Add the original token (allow duplicates for proper bigram extraction)",
        "            result.append(token_lower)",
        "            # Track this token to prevent splits from duplicating it",
        "            seen_splits.add(token_lower)",
        "",
        "            # Split identifier if enabled and token looks like an identifier",
        "            if should_split and (",
        "                '_' in token or",
        "                any(c.isupper() for c in token[1:])  # Has internal capitals",
        "            ):",
        "                parts = split_identifier(token)",
        "                for part in parts:",
        "                    # Allow programming keywords even if in stop words",
        "                    is_programming_keyword = part in PROGRAMMING_KEYWORDS",
        "                    # Only add split parts once per token to avoid bloating",
        "                    if (",
        "                        part not in seen_splits and",
        "                        part != token_lower and  # Don't duplicate the original",
        "                        (is_programming_keyword or part not in self.stop_words) and",
        "                        len(part) >= self.min_word_length",
        "                    ):",
        "                        result.append(part)",
        "                        seen_splits.add(part)",
        "",
        "        return result"
      ],
      "lines_removed": [
        "    def tokenize(self, text: str) -> List[str]:",
        "        ",
        "            ",
        "        # Convert to lowercase and extract words (including alphanumeric like word2vec)",
        "        words = re.findall(r'\\b[a-z][a-z0-9]*\\b', text.lower())",
        "        ",
        "        # Filter stop words and short words",
        "        return [",
        "            w for w in words ",
        "            if w not in self.stop_words and len(w) >= self.min_word_length",
        "        ]"
      ],
      "context_before": [
        "            # Common abbreviations",
        "            'nlp': ['natural', 'language', 'processing', 'text'],",
        "            'api': ['interface', 'endpoint', 'service'],",
        "            # Synonyms",
        "            'fast': ['quick', 'rapid', 'speed'],",
        "            'slow': ['latency', 'delay'],",
        "            'big': ['large', 'scale', 'massive'],",
        "            'small': ['tiny', 'minimal', 'compact'],",
        "        }",
        "    "
      ],
      "context_after": [
        "        \"\"\"",
        "        Extract tokens from text.",
        "        Args:",
        "            text: Input text to tokenize.",
        "        Returns:",
        "            List of filtered, lowercase tokens.",
        "        \"\"\"",
        "    ",
        "    def extract_ngrams(self, tokens: List[str], n: int = 2) -> List[str]:",
        "        \"\"\"",
        "        Extract n-grams from token list.",
        "        ",
        "        Args:",
        "            tokens: List of tokens.",
        "            n: Size of n-grams to extract.",
        "            ",
        "        Returns:"
      ],
      "change_type": "modify"
    },
    {
      "file": "tests/test_tokenizer.py",
      "function": "class TestTokenizerStemming(unittest.TestCase):",
      "start_line": 84,
      "lines_added": [
        "class TestSplitIdentifier(unittest.TestCase):",
        "    \"\"\"Test the split_identifier function.\"\"\"",
        "",
        "    def test_camel_case(self):",
        "        \"\"\"Test splitting camelCase identifiers.\"\"\"",
        "        from cortical.tokenizer import split_identifier",
        "        self.assertEqual(split_identifier(\"getUserCredentials\"), [\"get\", \"user\", \"credentials\"])",
        "        self.assertEqual(split_identifier(\"processData\"), [\"process\", \"data\"])",
        "",
        "    def test_pascal_case(self):",
        "        \"\"\"Test splitting PascalCase identifiers.\"\"\"",
        "        from cortical.tokenizer import split_identifier",
        "        self.assertEqual(split_identifier(\"UserCredentials\"), [\"user\", \"credentials\"])",
        "        self.assertEqual(split_identifier(\"DataProcessor\"), [\"data\", \"processor\"])",
        "",
        "    def test_snake_case(self):",
        "        \"\"\"Test splitting snake_case identifiers.\"\"\"",
        "        from cortical.tokenizer import split_identifier",
        "        self.assertEqual(split_identifier(\"get_user_data\"), [\"get\", \"user\", \"data\"])",
        "        self.assertEqual(split_identifier(\"process_http_request\"), [\"process\", \"http\", \"request\"])",
        "",
        "    def test_screaming_snake_case(self):",
        "        \"\"\"Test splitting SCREAMING_SNAKE_CASE identifiers.\"\"\"",
        "        from cortical.tokenizer import split_identifier",
        "        self.assertEqual(split_identifier(\"MAX_RETRY_COUNT\"), [\"max\", \"retry\", \"count\"])",
        "",
        "    def test_acronyms(self):",
        "        \"\"\"Test handling of acronyms in identifiers.\"\"\"",
        "        from cortical.tokenizer import split_identifier",
        "        self.assertEqual(split_identifier(\"XMLParser\"), [\"xml\", \"parser\"])",
        "        self.assertEqual(split_identifier(\"parseHTTPResponse\"), [\"parse\", \"http\", \"response\"])",
        "        self.assertEqual(split_identifier(\"getURLString\"), [\"get\", \"url\", \"string\"])",
        "",
        "    def test_mixed_case_with_underscore(self):",
        "        \"\"\"Test mixed camelCase and snake_case.\"\"\"",
        "        from cortical.tokenizer import split_identifier",
        "        result = split_identifier(\"get_UserData\")",
        "        self.assertIn(\"get\", result)",
        "        self.assertIn(\"user\", result)",
        "        self.assertIn(\"data\", result)",
        "",
        "    def test_single_word(self):",
        "        \"\"\"Test single word identifiers.\"\"\"",
        "        from cortical.tokenizer import split_identifier",
        "        self.assertEqual(split_identifier(\"process\"), [\"process\"])",
        "        self.assertEqual(split_identifier(\"data\"), [\"data\"])",
        "",
        "    def test_empty_string(self):",
        "        \"\"\"Test empty string input.\"\"\"",
        "        from cortical.tokenizer import split_identifier",
        "        self.assertEqual(split_identifier(\"\"), [])",
        "",
        "",
        "class TestCodeAwareTokenization(unittest.TestCase):",
        "    \"\"\"Test code-aware tokenization with identifier splitting.\"\"\"",
        "",
        "    def test_split_identifiers_disabled_by_default(self):",
        "        \"\"\"Test that identifier splitting is disabled by default.\"\"\"",
        "        tokenizer = Tokenizer()",
        "        tokens = tokenizer.tokenize(\"getUserCredentials\")",
        "        self.assertEqual(tokens, [\"getusercredentials\"])",
        "",
        "    def test_split_identifiers_enabled(self):",
        "        \"\"\"Test tokenization with identifier splitting enabled.\"\"\"",
        "        tokenizer = Tokenizer(split_identifiers=True)",
        "        tokens = tokenizer.tokenize(\"getUserCredentials\")",
        "        self.assertIn(\"getusercredentials\", tokens)",
        "        self.assertIn(\"get\", tokens)",
        "        self.assertIn(\"user\", tokens)",
        "        self.assertIn(\"credentials\", tokens)",
        "",
        "    def test_split_identifiers_snake_case(self):",
        "        \"\"\"Test splitting snake_case in tokenization.\"\"\"",
        "        tokenizer = Tokenizer(split_identifiers=True)",
        "        tokens = tokenizer.tokenize(\"process_user_data\")",
        "        self.assertIn(\"process_user_data\", tokens)",
        "        self.assertIn(\"process\", tokens)",
        "        self.assertIn(\"user\", tokens)",
        "        self.assertIn(\"data\", tokens)",
        "",
        "    def test_split_identifiers_preserves_context(self):",
        "        \"\"\"Test that split tokens appear alongside regular tokens.\"\"\"",
        "        tokenizer = Tokenizer(split_identifiers=True)",
        "        tokens = tokenizer.tokenize(\"The getUserCredentials function returns data\")",
        "        self.assertIn(\"getusercredentials\", tokens)",
        "        self.assertIn(\"credentials\", tokens)",
        "        self.assertIn(\"function\", tokens)",
        "        self.assertIn(\"returns\", tokens)",
        "        self.assertIn(\"data\", tokens)",
        "",
        "    def test_split_identifiers_override(self):",
        "        \"\"\"Test overriding split_identifiers at call time.\"\"\"",
        "        tokenizer = Tokenizer(split_identifiers=False)",
        "        # Override to True",
        "        tokens = tokenizer.tokenize(\"getUserData\", split_identifiers=True)",
        "        self.assertIn(\"get\", tokens)",
        "        self.assertIn(\"user\", tokens)",
        "",
        "    def test_no_duplicate_tokens(self):",
        "        \"\"\"Test that split tokens don't create duplicates.\"\"\"",
        "        tokenizer = Tokenizer(split_identifiers=True)",
        "        tokens = tokenizer.tokenize(\"data process_data getData\")",
        "        # 'data' should appear only once",
        "        self.assertEqual(tokens.count(\"data\"), 1)",
        "",
        "    def test_stop_words_filtered_from_splits(self):",
        "        \"\"\"Test that stop words in split parts are filtered.\"\"\"",
        "        tokenizer = Tokenizer(split_identifiers=True)",
        "        # 'the' is a stop word",
        "        tokens = tokenizer.tokenize(\"getTheData\")",
        "        self.assertNotIn(\"the\", tokens)",
        "        self.assertIn(\"data\", tokens)",
        "",
        "    def test_min_length_applied_to_splits(self):",
        "        \"\"\"Test that min_word_length applies to split parts.\"\"\"",
        "        tokenizer = Tokenizer(split_identifiers=True, min_word_length=4)",
        "        tokens = tokenizer.tokenize(\"getUserID\")",
        "        # 'id' is too short (length 2)",
        "        self.assertNotIn(\"id\", tokens)",
        "        self.assertIn(\"user\", tokens)",
        "",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "        self.assertIn(\"network\", variants)",
        "        self.assertIn(\"networks\", variants)",
        "    ",
        "    def test_word_mappings_brain(self):",
        "        \"\"\"Test brain-related word mappings.\"\"\"",
        "        variants = self.tokenizer.get_word_variants(\"brain\")",
        "        self.assertIn(\"neural\", variants)",
        "        self.assertIn(\"cortical\", variants)",
        "",
        ""
      ],
      "context_after": [
        "if __name__ == \"__main__\":",
        "    unittest.main(verbosity=2)"
      ],
      "change_type": "add"
    }
  ],
  "hour_of_day": 14,
  "day_of_week": "Wednesday",
  "seconds_since_last_commit": -430837,
  "is_merge": false,
  "is_initial": false,
  "parent_count": 1,
  "session_id": null,
  "related_chats": [],
  "ci_result": null,
  "reverted": false,
  "amended": false
}