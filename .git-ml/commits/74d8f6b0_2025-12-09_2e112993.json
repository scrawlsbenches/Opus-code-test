{
  "hash": "74d8f6b0e9de2c219b70d45fbda56439fe754edd",
  "message": "Extract cortical_package from zip archive",
  "author": "Claude",
  "timestamp": "2025-12-09 18:12:22 +0000",
  "branch": "claude/multi-index-design-DvifZ",
  "files_changed": [
    "cortical_package/README.md",
    "cortical_package/cortical/__init__.py",
    "cortical_package/cortical/__pycache__/__init__.cpython-311.pyc",
    "cortical_package/cortical/__pycache__/__init__.cpython-312.pyc",
    "cortical_package/cortical/__pycache__/analysis.cpython-311.pyc",
    "cortical_package/cortical/__pycache__/analysis.cpython-312.pyc",
    "cortical_package/cortical/__pycache__/embeddings.cpython-311.pyc",
    "cortical_package/cortical/__pycache__/embeddings.cpython-312.pyc",
    "cortical_package/cortical/__pycache__/gaps.cpython-311.pyc",
    "cortical_package/cortical/__pycache__/gaps.cpython-312.pyc",
    "cortical_package/cortical/__pycache__/layers.cpython-311.pyc",
    "cortical_package/cortical/__pycache__/layers.cpython-312.pyc",
    "cortical_package/cortical/__pycache__/minicolumn.cpython-311.pyc",
    "cortical_package/cortical/__pycache__/minicolumn.cpython-312.pyc",
    "cortical_package/cortical/__pycache__/persistence.cpython-311.pyc",
    "cortical_package/cortical/__pycache__/persistence.cpython-312.pyc",
    "cortical_package/cortical/__pycache__/processor.cpython-311.pyc",
    "cortical_package/cortical/__pycache__/processor.cpython-312.pyc",
    "cortical_package/cortical/__pycache__/query.cpython-311.pyc",
    "cortical_package/cortical/__pycache__/query.cpython-312.pyc",
    "cortical_package/cortical/__pycache__/semantics.cpython-311.pyc",
    "cortical_package/cortical/__pycache__/semantics.cpython-312.pyc",
    "cortical_package/cortical/__pycache__/tokenizer.cpython-311.pyc",
    "cortical_package/cortical/__pycache__/tokenizer.cpython-312.pyc",
    "cortical_package/cortical/analysis.py",
    "cortical_package/cortical/embeddings.py",
    "cortical_package/cortical/gaps.py",
    "cortical_package/cortical/layers.py",
    "cortical_package/cortical/minicolumn.py",
    "cortical_package/cortical/persistence.py",
    "cortical_package/cortical/processor.py",
    "cortical_package/cortical/query.py",
    "cortical_package/cortical/semantics.py",
    "cortical_package/cortical/tokenizer.py",
    "cortical_package/demo.py",
    "cortical_package/evaluation/__init__.py",
    "cortical_package/evaluation/__pycache__/__init__.cpython-312.pyc",
    "cortical_package/evaluation/__pycache__/evaluator.cpython-312.pyc",
    "cortical_package/evaluation/evaluator.py",
    "cortical_package/pyproject.toml",
    "cortical_package/samples/accordion_repair.txt",
    "cortical_package/samples/algorithmic_trading.txt",
    "cortical_package/samples/astronomical_spectroscopy.txt",
    "cortical_package/samples/blockchain_consensus.txt",
    "cortical_package/samples/bonsai_cultivation.txt",
    "cortical_package/samples/brain_inspired_computing.txt",
    "cortical_package/samples/cheese_affinage.txt",
    "cortical_package/samples/commonsense_reasoning.txt",
    "cortical_package/samples/conceptnet_overview.txt",
    "cortical_package/samples/container_orchestration.txt",
    "cortical_package/samples/cortical_columns.txt",
    "cortical_package/samples/cryptocurrency_mining.txt",
    "cortical_package/samples/deep_learning_revolution.txt",
    "cortical_package/samples/distributed_systems.txt",
    "cortical_package/samples/dotnet_enterprise.txt",
    "cortical_package/samples/graph_neural_networks.txt",
    "cortical_package/samples/homelab_infrastructure.txt",
    "cortical_package/samples/kintsugi_pottery.txt",
    "cortical_package/samples/knowledge_enhanced_nlp.txt",
    "cortical_package/samples/knowledge_graphs.txt",
    "cortical_package/samples/knowledge_transfer.txt",
    "cortical_package/samples/letterpress_printing.txt",
    "cortical_package/samples/medieval_falconry.txt",
    "cortical_package/samples/memory_consolidation.txt",
    "cortical_package/samples/orchid_cultivation.txt",
    "cortical_package/samples/perfume_composition.txt",
    "cortical_package/samples/predictive_processing.txt",
    "cortical_package/samples/semantic_similarity.txt",
    "cortical_package/samples/solar_energy_systems.txt",
    "cortical_package/samples/sourdough_breadmaking.txt",
    "cortical_package/samples/sparse_coding.txt",
    "cortical_package/samples/speedcubing.txt",
    "cortical_package/samples/sumo_wrestling.txt",
    "cortical_package/samples/transformer_architecture.txt",
    "cortical_package/samples/virtualization_hypervisors.txt",
    "cortical_package/samples/word_embeddings.txt",
    "cortical_package/samples/wordnet_lexical.txt",
    "cortical_package/tests/__init__.py",
    "cortical_package/tests/__pycache__/test_layers.cpython-311.pyc",
    "cortical_package/tests/__pycache__/test_layers.cpython-312.pyc",
    "cortical_package/tests/__pycache__/test_processor.cpython-311.pyc",
    "cortical_package/tests/__pycache__/test_processor.cpython-312.pyc",
    "cortical_package/tests/__pycache__/test_tokenizer.cpython-311.pyc",
    "cortical_package/tests/__pycache__/test_tokenizer.cpython-312.pyc",
    "cortical_package/tests/test_layers.py",
    "cortical_package/tests/test_processor.py",
    "cortical_package/tests/test_tokenizer.py"
  ],
  "insertions": 4521,
  "deletions": 0,
  "hunks": [
    {
      "file": "cortical_package/README.md",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "# Cortical Text Processor",
        "",
        "A neocortex-inspired text processing library with **zero external dependencies**.",
        "",
        "## Overview",
        "",
        "This library provides a biologically-inspired approach to text processing, organizing information through a hierarchical structure similar to the visual cortex:",
        "",
        "| Layer | Name | Analogy | Purpose |",
        "|-------|------|---------|---------|",
        "| 0 | Tokens | V1 (edges) | Individual words |",
        "| 1 | Bigrams | V2 (patterns) | Word pairs |",
        "| 2 | Concepts | V4 (shapes) | Semantic clusters |",
        "| 3 | Documents | IT (objects) | Full documents |",
        "",
        "## Key Features",
        "",
        "- **Hierarchical Processing**: Feedforward and lateral connections like the neocortex",
        "- **PageRank Importance**: Graph-based term importance scoring",
        "- **TF-IDF Weighting**: Statistical term distinctiveness",
        "- **Corpus-Derived Semantics**: No external knowledge bases needed",
        "- **Graph Embeddings**: Multiple embedding methods with retrofitting",
        "- **Gap Detection**: Find weak spots in your corpus",
        "- **Query Expansion**: Smart retrieval with synonym handling",
        "- **Zero Dependencies**: Pure Python, no pip installs required",
        "",
        "## Installation",
        "",
        "```bash",
        "pip install cortical-text-processor",
        "```",
        "",
        "Or install from source:",
        "",
        "```bash",
        "git clone https://github.com/example/cortical-text-processor.git",
        "cd cortical-text-processor",
        "pip install -e .",
        "```",
        "",
        "## Quick Start",
        "",
        "```python",
        "from cortical import CorticalTextProcessor",
        "",
        "# Create processor",
        "processor = CorticalTextProcessor()",
        "",
        "# Add documents",
        "processor.process_document(\"doc1\", \"Neural networks process information hierarchically.\")",
        "processor.process_document(\"doc2\", \"The brain uses layers of neurons for processing.\")",
        "processor.process_document(\"doc3\", \"Machine learning enables pattern recognition.\")",
        "",
        "# Build the network",
        "processor.propagate_activation()",
        "processor.compute_importance()",
        "processor.compute_tfidf()",
        "",
        "# Query",
        "results = processor.find_documents_for_query(\"neural processing\")",
        "print(results)  # [('doc1', 0.85), ('doc2', 0.72), ...]",
        "",
        "# Analyze corpus health",
        "health = processor.compute_corpus_health()",
        "print(f\"Corpus health: {health['overall_score']:.0%}\")",
        "",
        "# Save for later",
        "processor.save(\"my_corpus.pkl\")",
        "```",
        "",
        "## Core API",
        "",
        "### Document Processing",
        "",
        "```python",
        "processor.process_document(doc_id, content)",
        "processor.process_documents_from_directory(path)",
        "```",
        "",
        "### Network Building",
        "",
        "```python",
        "processor.propagate_activation()      # Spread activation",
        "processor.compute_importance()        # PageRank scores",
        "processor.compute_tfidf()            # TF-IDF weights",
        "processor.build_concept_clusters()   # Cluster tokens",
        "processor.compute_document_connections()  # Link documents",
        "```",
        "",
        "### Semantics & Embeddings",
        "",
        "```python",
        "processor.extract_corpus_semantics()  # Extract relations",
        "processor.retrofit_connections()      # Blend with semantics",
        "processor.compute_graph_embeddings()  # Term embeddings",
        "processor.retrofit_embeddings()       # Improve embeddings",
        "```",
        "",
        "### Query & Retrieval",
        "",
        "```python",
        "processor.expand_query(text)              # Expand query",
        "processor.find_documents_for_query(text)  # Search",
        "processor.find_related_documents(doc_id)  # Related docs",
        "processor.summarize_document(doc_id)      # Summarize",
        "```",
        "",
        "### Analysis",
        "",
        "```python",
        "processor.analyze_knowledge_gaps()  # Find gaps",
        "processor.detect_anomalies()        # Find outliers",
        "processor.compute_corpus_health()   # Health score",
        "```",
        "",
        "## Performance",
        "",
        "Evaluation on a 37-document corpus:",
        "",
        "| Category | Score |",
        "|----------|-------|",
        "| **Overall** | **90.1%** |",
        "| Factual Retrieval | 91.7% |",
        "| Cross-Document Synthesis | 93.3% |",
        "| Gap Detection | 94.4% |",
        "| Query Expansion | 93.3% |",
        "",
        "## Package Structure",
        "",
        "```",
        "cortical/",
        "├── __init__.py      # Public API",
        "├── processor.py     # Main class",
        "├── tokenizer.py     # Tokenization + stemming",
        "├── minicolumn.py    # Core data structure",
        "├── layers.py        # Hierarchical layers",
        "├── analysis.py      # PageRank, TF-IDF",
        "├── semantics.py     # Semantic extraction",
        "├── embeddings.py    # Graph embeddings",
        "├── query.py         # Search and retrieval",
        "├── gaps.py          # Gap detection",
        "└── persistence.py   # Save/load",
        "```",
        "",
        "## License",
        "",
        "MIT License"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "cortical_package/cortical/__init__.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "\"\"\"",
        "Cortical Text Processing Package",
        "================================",
        "",
        "A neocortex-inspired text processing system for semantic analysis,",
        "document retrieval, and knowledge gap detection.",
        "",
        "Example:",
        "    from cortical import CorticalTextProcessor",
        "    ",
        "    processor = CorticalTextProcessor()",
        "    processor.process_document(\"doc1\", \"Neural networks process information...\")",
        "    processor.compute_all()",
        "    results = processor.find_documents_for_query(\"neural processing\")",
        "\"\"\"",
        "",
        "from .tokenizer import Tokenizer",
        "from .minicolumn import Minicolumn",
        "from .layers import CorticalLayer, HierarchicalLayer",
        "from .processor import CorticalTextProcessor",
        "",
        "__version__ = \"2.0.0\"",
        "__all__ = [",
        "    \"CorticalTextProcessor\",",
        "    \"CorticalLayer\",",
        "    \"HierarchicalLayer\",",
        "    \"Minicolumn\",",
        "    \"Tokenizer\",",
        "]"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "cortical_package/cortical/analysis.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "\"\"\"",
        "Analysis Module",
        "===============",
        "",
        "Graph analysis algorithms for the cortical network.",
        "",
        "Contains implementations of:",
        "- PageRank for importance scoring",
        "- TF-IDF for term weighting",
        "- Label propagation for clustering",
        "- Activation propagation for information flow",
        "\"\"\"",
        "",
        "import math",
        "from typing import Dict, List, Tuple, Set, Optional",
        "from collections import defaultdict, Counter",
        "",
        "from .layers import CorticalLayer, HierarchicalLayer",
        "from .minicolumn import Minicolumn",
        "",
        "",
        "def compute_pagerank(",
        "    layer: HierarchicalLayer,",
        "    damping: float = 0.85,",
        "    iterations: int = 20,",
        "    tolerance: float = 1e-6",
        ") -> Dict[str, float]:",
        "    \"\"\"",
        "    Compute PageRank scores for minicolumns in a layer.",
        "    ",
        "    PageRank measures importance based on connection structure.",
        "    Highly connected columns that are connected to other important",
        "    columns receive higher scores.",
        "    ",
        "    Args:",
        "        layer: The layer to compute PageRank for",
        "        damping: Damping factor (probability of following links)",
        "        iterations: Maximum number of iterations",
        "        tolerance: Convergence threshold",
        "        ",
        "    Returns:",
        "        Dictionary mapping column IDs to PageRank scores",
        "    \"\"\"",
        "    n = len(layer.minicolumns)",
        "    if n == 0:",
        "        return {}",
        "    ",
        "    # Initialize PageRank uniformly",
        "    pagerank = {col.id: 1.0 / n for col in layer.minicolumns.values()}",
        "    ",
        "    # Build incoming links map",
        "    incoming: Dict[str, List[Tuple[str, float]]] = defaultdict(list)",
        "    outgoing_sum: Dict[str, float] = defaultdict(float)",
        "    ",
        "    for col in layer.minicolumns.values():",
        "        for target_id, weight in col.lateral_connections.items():",
        "            if target_id in layer.minicolumns or any(",
        "                c.id == target_id for c in layer.minicolumns.values()",
        "            ):",
        "                incoming[target_id].append((col.id, weight))",
        "                outgoing_sum[col.id] += weight",
        "    ",
        "    # Iterate until convergence",
        "    for iteration in range(iterations):",
        "        new_pagerank = {}",
        "        max_diff = 0.0",
        "        ",
        "        for col in layer.minicolumns.values():",
        "            # Sum of weighted incoming PageRank",
        "            incoming_sum = 0.0",
        "            for source_id, weight in incoming[col.id]:",
        "                if source_id in pagerank and outgoing_sum[source_id] > 0:",
        "                    incoming_sum += pagerank[source_id] * weight / outgoing_sum[source_id]",
        "            ",
        "            # Apply damping",
        "            new_rank = (1 - damping) / n + damping * incoming_sum",
        "            new_pagerank[col.id] = new_rank",
        "            ",
        "            max_diff = max(max_diff, abs(new_rank - pagerank.get(col.id, 0)))",
        "        ",
        "        pagerank = new_pagerank",
        "        ",
        "        if max_diff < tolerance:",
        "            break",
        "    ",
        "    # Update minicolumn pagerank values",
        "    for col in layer.minicolumns.values():",
        "        col.pagerank = pagerank.get(col.id, 1.0 / n)",
        "    ",
        "    return pagerank",
        "",
        "",
        "def compute_tfidf(",
        "    layers: Dict[CorticalLayer, HierarchicalLayer],",
        "    documents: Dict[str, str]",
        ") -> None:",
        "    \"\"\"",
        "    Compute TF-IDF scores for tokens.",
        "    ",
        "    TF-IDF (Term Frequency - Inverse Document Frequency) measures",
        "    how distinctive a term is to the corpus. High TF-IDF terms are",
        "    both frequent in their documents and rare across the corpus.",
        "    ",
        "    Args:",
        "        layers: Dictionary of layers (needs TOKENS layer)",
        "        documents: Dictionary mapping doc_id to content",
        "    \"\"\"",
        "    layer0 = layers[CorticalLayer.TOKENS]",
        "    num_docs = len(documents)",
        "    ",
        "    if num_docs == 0:",
        "        return",
        "    ",
        "    for col in layer0.minicolumns.values():",
        "        # Document frequency",
        "        df = len(col.document_ids)",
        "        ",
        "        if df > 0:",
        "            # Inverse document frequency",
        "            idf = math.log(num_docs / df)",
        "            ",
        "            # Term frequency (normalized by occurrence count)",
        "            tf = math.log1p(col.occurrence_count)",
        "            ",
        "            # TF-IDF",
        "            col.tfidf = tf * idf",
        "            ",
        "            # Per-document TF-IDF",
        "            for doc_id in col.document_ids:",
        "                # Count occurrences in this document",
        "                doc_tf = sum(1 for d in [doc_id] if d in col.document_ids)",
        "                col.tfidf_per_doc[doc_id] = math.log1p(doc_tf) * idf",
        "",
        "",
        "def propagate_activation(",
        "    layers: Dict[CorticalLayer, HierarchicalLayer],",
        "    iterations: int = 3,",
        "    decay: float = 0.8,",
        "    lateral_weight: float = 0.3",
        ") -> None:",
        "    \"\"\"",
        "    Propagate activation through the network.",
        "    ",
        "    This simulates how information flows through cortical layers:",
        "    - Activation spreads to connected columns (lateral)",
        "    - Activation flows up the hierarchy (feedforward)",
        "    - Activation decays over time",
        "    ",
        "    Args:",
        "        layers: Dictionary of all layers",
        "        iterations: Number of propagation iterations",
        "        decay: How much activation decays per iteration",
        "        lateral_weight: Weight for lateral spreading",
        "    \"\"\"",
        "    for _ in range(iterations):",
        "        # Store new activations",
        "        new_activations: Dict[str, float] = {}",
        "        ",
        "        # Process each layer",
        "        for layer_enum in CorticalLayer:",
        "            if layer_enum not in layers:",
        "                continue",
        "            layer = layers[layer_enum]",
        "            ",
        "            for col in layer.minicolumns.values():",
        "                # Start with decayed current activation",
        "                new_act = col.activation * decay",
        "                ",
        "                # Add lateral input",
        "                for neighbor_id, weight in col.lateral_connections.items():",
        "                    if neighbor_id in layer.minicolumns:",
        "                        neighbor = layer.minicolumns[neighbor_id]",
        "                        new_act += neighbor.activation * weight * lateral_weight",
        "                    else:",
        "                        # Look up by ID",
        "                        for c in layer.minicolumns.values():",
        "                            if c.id == neighbor_id:",
        "                                new_act += c.activation * weight * lateral_weight",
        "                                break",
        "                ",
        "                # Add feedforward input",
        "                for source_id in col.feedforward_sources:",
        "                    # Find source in lower layers",
        "                    for lower_enum in CorticalLayer:",
        "                        if lower_enum >= layer_enum:",
        "                            break",
        "                        if lower_enum not in layers:",
        "                            continue",
        "                        lower_layer = layers[lower_enum]",
        "                        for source in lower_layer.minicolumns.values():",
        "                            if source.id == source_id:",
        "                                new_act += source.activation * 0.5",
        "                                break",
        "                ",
        "                new_activations[col.id] = new_act",
        "        ",
        "        # Apply new activations",
        "        for layer_enum in CorticalLayer:",
        "            if layer_enum not in layers:",
        "                continue",
        "            layer = layers[layer_enum]",
        "            for col in layer.minicolumns.values():",
        "                if col.id in new_activations:",
        "                    col.activation = new_activations[col.id]",
        "",
        "",
        "def cluster_by_label_propagation(",
        "    layer: HierarchicalLayer,",
        "    min_cluster_size: int = 3,",
        "    max_iterations: int = 20",
        ") -> Dict[int, List[str]]:",
        "    \"\"\"",
        "    Cluster minicolumns using label propagation.",
        "    ",
        "    Label propagation is a semi-supervised community detection",
        "    algorithm. Each node adopts the most common label among its",
        "    neighbors, causing labels to propagate through densely",
        "    connected regions.",
        "    ",
        "    Args:",
        "        layer: Layer to cluster",
        "        min_cluster_size: Minimum nodes per cluster",
        "        max_iterations: Maximum iterations",
        "        ",
        "    Returns:",
        "        Dictionary mapping cluster_id to list of column contents",
        "    \"\"\"",
        "    # Initialize each node with unique label",
        "    labels = {col.content: i for i, col in enumerate(layer.minicolumns.values())}",
        "    ",
        "    # Get column list for shuffling",
        "    columns = list(layer.minicolumns.keys())",
        "    ",
        "    for iteration in range(max_iterations):",
        "        changed = False",
        "        ",
        "        # Process in order (could shuffle for better results)",
        "        for content in columns:",
        "            col = layer.minicolumns[content]",
        "            ",
        "            # Count neighbor labels weighted by connection strength",
        "            label_weights: Dict[int, float] = defaultdict(float)",
        "            ",
        "            for neighbor_id, weight in col.lateral_connections.items():",
        "                # Find neighbor content",
        "                neighbor_content = None",
        "                for c in layer.minicolumns.values():",
        "                    if c.id == neighbor_id:",
        "                        neighbor_content = c.content",
        "                        break",
        "                ",
        "                if neighbor_content and neighbor_content in labels:",
        "                    label_weights[labels[neighbor_content]] += weight",
        "            ",
        "            # Adopt most common label",
        "            if label_weights:",
        "                best_label = max(label_weights.items(), key=lambda x: x[1])[0]",
        "                if labels[content] != best_label:",
        "                    labels[content] = best_label",
        "                    changed = True",
        "        ",
        "        if not changed:",
        "            break",
        "    ",
        "    # Build clusters",
        "    clusters: Dict[int, List[str]] = defaultdict(list)",
        "    for content, label in labels.items():",
        "        clusters[label].append(content)",
        "    ",
        "    # Filter by minimum size",
        "    filtered = {",
        "        label: members ",
        "        for label, members in clusters.items() ",
        "        if len(members) >= min_cluster_size",
        "    }",
        "    ",
        "    # Update cluster_id on minicolumns",
        "    for label, members in filtered.items():",
        "        for content in members:",
        "            if content in layer.minicolumns:",
        "                layer.minicolumns[content].cluster_id = label",
        "    ",
        "    return filtered",
        "",
        "",
        "def build_concept_clusters(",
        "    layers: Dict[CorticalLayer, HierarchicalLayer],",
        "    clusters: Dict[int, List[str]]",
        ") -> None:",
        "    \"\"\"",
        "    Build concept layer from token clusters.",
        "    ",
        "    Creates Layer 2 (Concepts) minicolumns from clustered tokens.",
        "    Each concept is named after its most important members.",
        "    ",
        "    Args:",
        "        layers: Dictionary of all layers",
        "        clusters: Cluster dictionary from label propagation",
        "    \"\"\"",
        "    layer0 = layers[CorticalLayer.TOKENS]",
        "    layer2 = layers[CorticalLayer.CONCEPTS]",
        "    ",
        "    for cluster_id, members in clusters.items():",
        "        if len(members) < 2:",
        "            continue",
        "        ",
        "        # Get member columns and sort by PageRank",
        "        member_cols = []",
        "        for m in members:",
        "            col = layer0.get_minicolumn(m)",
        "            if col:",
        "                member_cols.append(col)",
        "        ",
        "        if not member_cols:",
        "            continue",
        "        ",
        "        member_cols.sort(key=lambda c: c.pagerank, reverse=True)",
        "        ",
        "        # Name concept after top members",
        "        top_names = [c.content for c in member_cols[:3]]",
        "        concept_name = '/'.join(top_names)",
        "        ",
        "        # Create concept minicolumn",
        "        concept = layer2.get_or_create_minicolumn(concept_name)",
        "        concept.cluster_id = cluster_id",
        "        ",
        "        # Aggregate properties from members",
        "        for col in member_cols:",
        "            concept.feedforward_sources.add(col.id)",
        "            concept.document_ids.update(col.document_ids)",
        "            concept.activation += col.activation * 0.5",
        "            concept.occurrence_count += col.occurrence_count",
        "        ",
        "        # Set PageRank as average of members",
        "        concept.pagerank = sum(c.pagerank for c in member_cols) / len(member_cols)",
        "",
        "",
        "def compute_document_connections(",
        "    layers: Dict[CorticalLayer, HierarchicalLayer],",
        "    documents: Dict[str, str],",
        "    min_shared_terms: int = 3",
        ") -> None:",
        "    \"\"\"",
        "    Build lateral connections between documents.",
        "    ",
        "    Documents are connected based on shared vocabulary,",
        "    weighted by TF-IDF scores of shared terms.",
        "    ",
        "    Args:",
        "        layers: Dictionary of all layers",
        "        documents: Dictionary of documents",
        "        min_shared_terms: Minimum shared terms for connection",
        "    \"\"\"",
        "    layer0 = layers[CorticalLayer.TOKENS]",
        "    layer3 = layers[CorticalLayer.DOCUMENTS]",
        "    ",
        "    doc_ids = list(documents.keys())",
        "    ",
        "    for i, doc1 in enumerate(doc_ids):",
        "        col1 = layer3.get_minicolumn(doc1)",
        "        if not col1:",
        "            col1 = layer3.get_or_create_minicolumn(doc1)",
        "        ",
        "        for doc2 in doc_ids[i+1:]:",
        "            col2 = layer3.get_minicolumn(doc2)",
        "            if not col2:",
        "                col2 = layer3.get_or_create_minicolumn(doc2)",
        "            ",
        "            # Find shared terms",
        "            shared_weight = 0.0",
        "            shared_count = 0",
        "            ",
        "            for token_col in layer0.minicolumns.values():",
        "                if doc1 in token_col.document_ids and doc2 in token_col.document_ids:",
        "                    # Weight by TF-IDF",
        "                    weight = token_col.tfidf",
        "                    shared_weight += weight",
        "                    shared_count += 1",
        "            ",
        "            if shared_count >= min_shared_terms:",
        "                col1.add_lateral_connection(col2.id, shared_weight)",
        "                col2.add_lateral_connection(col1.id, shared_weight)",
        "",
        "",
        "def cosine_similarity(vec1: Dict[str, float], vec2: Dict[str, float]) -> float:",
        "    \"\"\"",
        "    Compute cosine similarity between two sparse vectors.",
        "    ",
        "    Args:",
        "        vec1: First vector as dict of term -> weight",
        "        vec2: Second vector as dict of term -> weight",
        "        ",
        "    Returns:",
        "        Cosine similarity between 0 and 1",
        "    \"\"\"",
        "    # Find common keys",
        "    common = set(vec1.keys()) & set(vec2.keys())",
        "    ",
        "    if not common:",
        "        return 0.0",
        "    ",
        "    # Compute dot product",
        "    dot = sum(vec1[k] * vec2[k] for k in common)",
        "    ",
        "    # Compute magnitudes",
        "    mag1 = math.sqrt(sum(v * v for v in vec1.values()))",
        "    mag2 = math.sqrt(sum(v * v for v in vec2.values()))",
        "    ",
        "    if mag1 == 0 or mag2 == 0:",
        "        return 0.0",
        "    ",
        "    return dot / (mag1 * mag2)"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "cortical_package/cortical/embeddings.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "\"\"\"",
        "Embeddings Module",
        "=================",
        "",
        "Graph-based embeddings for the cortical network.",
        "",
        "Implements three methods for computing term embeddings from the",
        "connection graph structure:",
        "1. Adjacency: Direct connection weights to landmark nodes",
        "2. Random Walk: DeepWalk-inspired walk co-occurrence",
        "3. Spectral: Graph Laplacian eigenvector approximation",
        "\"\"\"",
        "",
        "import math",
        "import random",
        "from typing import Dict, List, Tuple, Optional",
        "from collections import defaultdict",
        "",
        "from .layers import CorticalLayer, HierarchicalLayer",
        "",
        "",
        "def compute_graph_embeddings(",
        "    layers: Dict[CorticalLayer, HierarchicalLayer],",
        "    dimensions: int = 64,",
        "    method: str = 'adjacency'",
        ") -> Tuple[Dict[str, List[float]], Dict[str, any]]:",
        "    \"\"\"",
        "    Compute embeddings for tokens based on graph structure.",
        "    ",
        "    Args:",
        "        layers: Dictionary of layers (needs TOKENS)",
        "        dimensions: Number of embedding dimensions",
        "        method: 'adjacency', 'random_walk', or 'spectral'",
        "        ",
        "    Returns:",
        "        Tuple of (embeddings dict, statistics dict)",
        "    \"\"\"",
        "    layer0 = layers[CorticalLayer.TOKENS]",
        "    ",
        "    if method == 'adjacency':",
        "        embeddings = _adjacency_embeddings(layer0, dimensions)",
        "    elif method == 'random_walk':",
        "        embeddings = _random_walk_embeddings(layer0, dimensions)",
        "    elif method == 'spectral':",
        "        embeddings = _spectral_embeddings(layer0, dimensions)",
        "    else:",
        "        raise ValueError(f\"Unknown embedding method: {method}\")",
        "    ",
        "    stats = {",
        "        'method': method,",
        "        'dimensions': dimensions,",
        "        'terms_embedded': len(embeddings)",
        "    }",
        "    ",
        "    return embeddings, stats",
        "",
        "",
        "def _adjacency_embeddings(layer: HierarchicalLayer, dimensions: int) -> Dict[str, List[float]]:",
        "    \"\"\"Compute embeddings using adjacency to landmark nodes.\"\"\"",
        "    embeddings: Dict[str, List[float]] = {}",
        "    ",
        "    sorted_cols = sorted(layer.minicolumns.values(), key=lambda c: c.pagerank, reverse=True)",
        "    landmarks = sorted_cols[:dimensions]",
        "    ",
        "    for col in layer.minicolumns.values():",
        "        vec = [col.lateral_connections.get(lm.id, 0) for lm in landmarks]",
        "        mag = math.sqrt(sum(v*v for v in vec)) + 1e-10",
        "        embeddings[col.content] = [v / mag for v in vec]",
        "    ",
        "    return embeddings",
        "",
        "",
        "def _random_walk_embeddings(",
        "    layer: HierarchicalLayer,",
        "    dimensions: int,",
        "    walks_per_node: int = 10,",
        "    walk_length: int = 40,",
        "    window_size: int = 5",
        ") -> Dict[str, List[float]]:",
        "    \"\"\"Compute embeddings using random walks (DeepWalk-inspired).\"\"\"",
        "    embeddings: Dict[str, List[float]] = {}",
        "    id_to_term = {col.id: col.content for col in layer.minicolumns.values()}",
        "    cooccurrence: Dict[str, Dict[str, float]] = defaultdict(lambda: defaultdict(float))",
        "    ",
        "    for col in layer.minicolumns.values():",
        "        for _ in range(walks_per_node):",
        "            walk = _weighted_random_walk(col, layer, walk_length, id_to_term)",
        "            for i, term in enumerate(walk):",
        "                for j in range(max(0, i - window_size), min(len(walk), i + window_size + 1)):",
        "                    if i != j:",
        "                        cooccurrence[term][walk[j]] += 1.0",
        "    ",
        "    sorted_cols = sorted(layer.minicolumns.values(), key=lambda c: c.pagerank, reverse=True)",
        "    landmarks = [c.content for c in sorted_cols[:dimensions]]",
        "    ",
        "    for term in layer.minicolumns:",
        "        vec = [cooccurrence[term].get(lm, 0) for lm in landmarks]",
        "        mag = math.sqrt(sum(v*v for v in vec)) + 1e-10",
        "        embeddings[term] = [v / mag for v in vec]",
        "    ",
        "    return embeddings",
        "",
        "",
        "def _weighted_random_walk(start_col, layer: HierarchicalLayer, length: int, id_to_term: Dict[str, str]) -> List[str]:",
        "    \"\"\"Perform a weighted random walk from a starting column.\"\"\"",
        "    walk = [start_col.content]",
        "    current = start_col",
        "    ",
        "    for _ in range(length - 1):",
        "        if not current.lateral_connections:",
        "            break",
        "        neighbors = list(current.lateral_connections.items())",
        "        total_weight = sum(w for _, w in neighbors)",
        "        if total_weight == 0:",
        "            break",
        "        ",
        "        r = random.random() * total_weight",
        "        cumsum = 0.0",
        "        next_id = neighbors[0][0]",
        "        for neighbor_id, weight in neighbors:",
        "            cumsum += weight",
        "            if cumsum >= r:",
        "                next_id = neighbor_id",
        "                break",
        "        ",
        "        next_term = id_to_term.get(next_id)",
        "        if next_term and next_term in layer.minicolumns:",
        "            current = layer.minicolumns[next_term]",
        "            walk.append(next_term)",
        "        else:",
        "            break",
        "    ",
        "    return walk",
        "",
        "",
        "def _spectral_embeddings(layer: HierarchicalLayer, dimensions: int, iterations: int = 100) -> Dict[str, List[float]]:",
        "    \"\"\"Compute embeddings using spectral methods (graph Laplacian).\"\"\"",
        "    embeddings: Dict[str, List[float]] = {}",
        "    terms = list(layer.minicolumns.keys())",
        "    n = len(terms)",
        "    if n == 0:",
        "        return embeddings",
        "    ",
        "    term_to_idx = {t: i for i, t in enumerate(terms)}",
        "    adjacency: Dict[int, Dict[int, float]] = defaultdict(dict)",
        "    degrees = [0.0] * n",
        "    ",
        "    for term, col in layer.minicolumns.items():",
        "        i = term_to_idx[term]",
        "        for neighbor_id, weight in col.lateral_connections.items():",
        "            for t, c in layer.minicolumns.items():",
        "                if c.id == neighbor_id:",
        "                    j = term_to_idx[t]",
        "                    adjacency[i][j] = weight",
        "                    degrees[i] += weight",
        "                    break",
        "    ",
        "    degrees = [d if d > 0 else 1.0 for d in degrees]",
        "    actual_dims = min(dimensions, n)",
        "    vectors = []",
        "    ",
        "    for d in range(actual_dims):",
        "        vec = [random.gauss(0, 1) for _ in range(n)]",
        "        for prev in vectors:",
        "            dot = sum(v * p for v, p in zip(vec, prev))",
        "            vec = [v - dot * p for v, p in zip(vec, prev)]",
        "        mag = math.sqrt(sum(v*v for v in vec)) + 1e-10",
        "        vec = [v / mag for v in vec]",
        "        ",
        "        for _ in range(iterations):",
        "            new_vec = [0.0] * n",
        "            for i in range(n):",
        "                for j, weight in adjacency[i].items():",
        "                    norm_weight = weight / math.sqrt(degrees[i] * degrees[j])",
        "                    new_vec[i] -= norm_weight * vec[j]",
        "                new_vec[i] += vec[i]",
        "            ",
        "            for prev in vectors:",
        "                dot = sum(v * p for v, p in zip(new_vec, prev))",
        "                new_vec = [v - dot * p for v, p in zip(new_vec, prev)]",
        "            mag = math.sqrt(sum(v*v for v in new_vec)) + 1e-10",
        "            vec = [v / mag for v in new_vec]",
        "        ",
        "        vectors.append(vec)",
        "    ",
        "    for term in terms:",
        "        i = term_to_idx[term]",
        "        embeddings[term] = [vectors[d][i] if d < len(vectors) else 0.0 for d in range(dimensions)]",
        "    ",
        "    return embeddings",
        "",
        "",
        "def embedding_similarity(embeddings: Dict[str, List[float]], term1: str, term2: str) -> float:",
        "    \"\"\"Compute cosine similarity between two term embeddings.\"\"\"",
        "    if term1 not in embeddings or term2 not in embeddings:",
        "        return 0.0",
        "    vec1, vec2 = embeddings[term1], embeddings[term2]",
        "    dot = sum(a * b for a, b in zip(vec1, vec2))",
        "    mag1 = math.sqrt(sum(a * a for a in vec1))",
        "    mag2 = math.sqrt(sum(b * b for b in vec2))",
        "    return dot / (mag1 * mag2) if mag1 > 0 and mag2 > 0 else 0.0",
        "",
        "",
        "def find_similar_by_embedding(embeddings: Dict[str, List[float]], term: str, top_n: int = 10) -> List[Tuple[str, float]]:",
        "    \"\"\"Find terms most similar to a given term by embedding.\"\"\"",
        "    if term not in embeddings:",
        "        return []",
        "    similarities = [(t, embedding_similarity(embeddings, term, t)) for t in embeddings if t != term]",
        "    similarities.sort(key=lambda x: x[1], reverse=True)",
        "    return similarities[:top_n]"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "cortical_package/cortical/gaps.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "\"\"\"",
        "Gaps Module",
        "===========",
        "",
        "Knowledge gap detection and anomaly analysis.",
        "",
        "Identifies:",
        "- Isolated documents that don't connect well to the corpus",
        "- Weakly covered topics (few documents)",
        "- Bridge opportunities between document clusters",
        "- Anomalous documents that may be miscategorized",
        "\"\"\"",
        "",
        "import math",
        "from typing import Dict, List, Tuple, Set, Optional",
        "from collections import defaultdict",
        "",
        "from .layers import CorticalLayer, HierarchicalLayer",
        "from .analysis import cosine_similarity",
        "",
        "",
        "def analyze_knowledge_gaps(",
        "    layers: Dict[CorticalLayer, HierarchicalLayer],",
        "    documents: Dict[str, str]",
        ") -> Dict:",
        "    \"\"\"",
        "    Analyze the corpus to identify potential knowledge gaps.",
        "    ",
        "    Args:",
        "        layers: Dictionary of layers",
        "        documents: Dictionary of documents",
        "        ",
        "    Returns:",
        "        Dict with gap analysis results including isolated_documents,",
        "        weak_topics, bridge_opportunities, coverage_score, etc.",
        "    \"\"\"",
        "    layer0 = layers[CorticalLayer.TOKENS]",
        "    doc_ids = list(documents.keys())",
        "    ",
        "    # 1. Find isolated documents",
        "    isolated_docs = []",
        "    doc_similarities: Dict[str, Dict[str, float]] = {}",
        "    ",
        "    for doc_id in doc_ids:",
        "        doc_vector = {col.content: col.tfidf_per_doc[doc_id] ",
        "                     for col in layer0.minicolumns.values() ",
        "                     if doc_id in col.tfidf_per_doc}",
        "        ",
        "        similarities = []",
        "        for other_id in doc_ids:",
        "            if other_id != doc_id:",
        "                other_vector = {col.content: col.tfidf_per_doc[other_id]",
        "                               for col in layer0.minicolumns.values()",
        "                               if other_id in col.tfidf_per_doc}",
        "                sim = cosine_similarity(doc_vector, other_vector)",
        "                similarities.append((other_id, sim))",
        "        ",
        "        avg_sim = sum(s for _, s in similarities) / len(similarities) if similarities else 0",
        "        max_sim = max((s for _, s in similarities), default=0)",
        "        doc_similarities[doc_id] = {'avg': avg_sim, 'max': max_sim}",
        "        ",
        "        if avg_sim < 0.02:",
        "            isolated_docs.append({",
        "                'doc_id': doc_id,",
        "                'avg_similarity': avg_sim,",
        "                'max_similarity': max_sim,",
        "                'most_similar': max(similarities, key=lambda x: x[1])[0] if similarities else None",
        "            })",
        "    ",
        "    isolated_docs.sort(key=lambda x: x['avg_similarity'])",
        "    ",
        "    # 2. Find weakly covered topics",
        "    weak_topics = []",
        "    for col in layer0.minicolumns.values():",
        "        doc_count = len(col.document_ids)",
        "        if col.tfidf > 0.005 and 1 <= doc_count <= 2:",
        "            weak_topics.append({",
        "                'term': col.content,",
        "                'tfidf': col.tfidf,",
        "                'doc_count': doc_count,",
        "                'documents': list(col.document_ids),",
        "                'pagerank': col.pagerank",
        "            })",
        "    weak_topics.sort(key=lambda x: x['tfidf'] * x['pagerank'], reverse=True)",
        "    ",
        "    # 3. Find bridge opportunities",
        "    bridge_opportunities = []",
        "    for i, doc1 in enumerate(doc_ids):",
        "        vec1 = {col.content: col.tfidf_per_doc[doc1] ",
        "               for col in layer0.minicolumns.values() ",
        "               if doc1 in col.tfidf_per_doc}",
        "        ",
        "        for doc2 in doc_ids[i+1:]:",
        "            vec2 = {col.content: col.tfidf_per_doc[doc2]",
        "                   for col in layer0.minicolumns.values()",
        "                   if doc2 in col.tfidf_per_doc}",
        "            ",
        "            sim = cosine_similarity(vec1, vec2)",
        "            if 0.005 < sim < 0.03:",
        "                shared = set(vec1.keys()) & set(vec2.keys())",
        "                bridge_opportunities.append({",
        "                    'doc1': doc1,",
        "                    'doc2': doc2,",
        "                    'similarity': sim,",
        "                    'shared_terms': list(shared)[:5]",
        "                })",
        "    ",
        "    bridge_opportunities.sort(key=lambda x: x['similarity'], reverse=True)",
        "    ",
        "    # 4. Connector terms",
        "    connector_terms = []",
        "    isolated_doc_ids = {d['doc_id'] for d in isolated_docs[:5]}",
        "    if isolated_doc_ids:",
        "        for col in layer0.minicolumns.values():",
        "            in_isolated = col.document_ids & isolated_doc_ids",
        "            in_connected = col.document_ids - isolated_doc_ids",
        "            if in_isolated and in_connected:",
        "                connector_terms.append({",
        "                    'term': col.content,",
        "                    'bridges_isolated': list(in_isolated),",
        "                    'connects_to': list(in_connected)[:3],",
        "                    'pagerank': col.pagerank",
        "                })",
        "    connector_terms.sort(key=lambda x: len(x['bridges_isolated']), reverse=True)",
        "    ",
        "    # 5. Coverage metrics",
        "    total_docs = len(doc_ids)",
        "    isolated_count = len([d for d in doc_similarities.values() if d['avg'] < 0.02])",
        "    well_connected = len([d for d in doc_similarities.values() if d['avg'] >= 0.03])",
        "    coverage_score = well_connected / total_docs if total_docs > 0 else 0",
        "    ",
        "    all_avg_sims = [d['avg'] for d in doc_similarities.values()]",
        "    connectivity_score = sum(all_avg_sims) / len(all_avg_sims) if all_avg_sims else 0",
        "    ",
        "    return {",
        "        'isolated_documents': isolated_docs[:10],",
        "        'weak_topics': weak_topics[:10],",
        "        'bridge_opportunities': bridge_opportunities[:10],",
        "        'connector_terms': connector_terms[:10],",
        "        'coverage_score': coverage_score,",
        "        'connectivity_score': connectivity_score,",
        "        'summary': {",
        "            'total_documents': total_docs,",
        "            'isolated_count': isolated_count,",
        "            'well_connected_count': well_connected,",
        "            'weak_topic_count': len(weak_topics)",
        "        }",
        "    }",
        "",
        "",
        "def detect_anomalies(",
        "    layers: Dict[CorticalLayer, HierarchicalLayer],",
        "    documents: Dict[str, str],",
        "    threshold: float = 0.3",
        ") -> List[Dict]:",
        "    \"\"\"",
        "    Detect documents that don't fit well with the rest of the corpus.",
        "    ",
        "    Args:",
        "        layers: Dictionary of layers",
        "        documents: Dictionary of documents",
        "        threshold: Similarity threshold below which docs are anomalies",
        "        ",
        "    Returns:",
        "        List of anomaly reports with explanations",
        "    \"\"\"",
        "    layer0 = layers[CorticalLayer.TOKENS]",
        "    layer3 = layers.get(CorticalLayer.DOCUMENTS)",
        "    anomalies = []",
        "    ",
        "    for doc_id in documents:",
        "        doc_col = layer3.get_minicolumn(doc_id) if layer3 else None",
        "        connection_count = doc_col.connection_count() if doc_col else 0",
        "        ",
        "        doc_vector = {col.content: col.tfidf_per_doc[doc_id]",
        "                     for col in layer0.minicolumns.values()",
        "                     if doc_id in col.tfidf_per_doc}",
        "        ",
        "        similarities = []",
        "        for other_id in documents:",
        "            if other_id != doc_id:",
        "                other_vector = {col.content: col.tfidf_per_doc[other_id]",
        "                               for col in layer0.minicolumns.values()",
        "                               if other_id in col.tfidf_per_doc}",
        "                similarities.append(cosine_similarity(doc_vector, other_vector))",
        "        ",
        "        avg_similarity = sum(similarities) / len(similarities) if similarities else 0",
        "        max_similarity = max(similarities) if similarities else 0",
        "        ",
        "        is_anomaly = False",
        "        reasons = []",
        "        ",
        "        if avg_similarity < threshold:",
        "            is_anomaly = True",
        "            reasons.append(f\"Low average similarity ({avg_similarity:.1%})\")",
        "        if connection_count <= 1:",
        "            is_anomaly = True",
        "            reasons.append(f\"Few document connections ({connection_count})\")",
        "        if max_similarity < threshold * 1.5:",
        "            is_anomaly = True",
        "            reasons.append(\"No closely related documents\")",
        "        ",
        "        if is_anomaly:",
        "            sig_terms = sorted(doc_vector.items(), key=lambda x: x[1], reverse=True)[:5]",
        "            anomalies.append({",
        "                'doc_id': doc_id,",
        "                'avg_similarity': avg_similarity,",
        "                'max_similarity': max_similarity,",
        "                'connections': connection_count,",
        "                'reasons': reasons,",
        "                'distinctive_terms': [t for t, _ in sig_terms]",
        "            })",
        "    ",
        "    anomalies.sort(key=lambda x: x['avg_similarity'])",
        "    return anomalies"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "cortical_package/cortical/layers.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "\"\"\"",
        "Layers Module",
        "=============",
        "",
        "Defines the hierarchical layer structure inspired by the visual cortex.",
        "",
        "The neocortex processes information through a hierarchy of layers,",
        "each extracting progressively more abstract features:",
        "- V1: Edge detection (→ tokens)",
        "- V2: Simple patterns (→ bigrams)",
        "- V4: Complex shapes (→ concepts)",
        "- IT: Object recognition (→ documents)",
        "\"\"\"",
        "",
        "from enum import IntEnum",
        "from typing import Dict, Optional, Iterator",
        "",
        "from .minicolumn import Minicolumn",
        "",
        "",
        "class CorticalLayer(IntEnum):",
        "    \"\"\"",
        "    Enumeration of cortical processing layers.",
        "    ",
        "    Maps visual cortex layers to text processing hierarchy:",
        "        TOKENS (0): Like V1 - basic feature extraction (words)",
        "        BIGRAMS (1): Like V2 - simple patterns (word pairs)",
        "        CONCEPTS (2): Like V4 - higher-level features (clusters)",
        "        DOCUMENTS (3): Like IT - holistic recognition (full docs)",
        "    \"\"\"",
        "    TOKENS = 0      # Individual words (V1-like)",
        "    BIGRAMS = 1     # Word pairs (V2-like)",
        "    CONCEPTS = 2    # Concept clusters (V4-like)",
        "    DOCUMENTS = 3   # Full documents (IT-like)",
        "    ",
        "    @property",
        "    def description(self) -> str:",
        "        \"\"\"Human-readable description of this layer.\"\"\"",
        "        descriptions = {",
        "            0: \"Token layer - individual words (V1-like)\",",
        "            1: \"Bigram layer - word pairs (V2-like)\",",
        "            2: \"Concept layer - semantic clusters (V4-like)\",",
        "            3: \"Document layer - full documents (IT-like)\"",
        "        }",
        "        return descriptions[self.value]",
        "    ",
        "    @property",
        "    def analogy(self) -> str:",
        "        \"\"\"Visual cortex analogy for this layer.\"\"\"",
        "        analogies = {",
        "            0: \"V1-like: Edge/token detection\",",
        "            1: \"V2-like: Feature/pattern detection\",",
        "            2: \"V4-like: Shape/concept detection\",",
        "            3: \"IT-like: Object/document recognition\"",
        "        }",
        "        return analogies[self.value]",
        "",
        "",
        "class HierarchicalLayer:",
        "    \"\"\"",
        "    A layer in the cortical hierarchy containing minicolumns.",
        "    ",
        "    Each layer contains a collection of minicolumns and provides",
        "    methods for managing them. Layers are organized hierarchically,",
        "    with feedforward connections from lower to higher layers and",
        "    lateral connections within each layer.",
        "    ",
        "    Attributes:",
        "        level: The layer number (0-3)",
        "        minicolumns: Dictionary mapping IDs to Minicolumn objects",
        "        ",
        "    Example:",
        "        layer = HierarchicalLayer(CorticalLayer.TOKENS)",
        "        col = layer.get_or_create_minicolumn(\"neural\")",
        "        col.occurrence_count += 1",
        "    \"\"\"",
        "    ",
        "    def __init__(self, level: CorticalLayer):",
        "        \"\"\"",
        "        Initialize a hierarchical layer.",
        "        ",
        "        Args:",
        "            level: The CorticalLayer enum value for this layer",
        "        \"\"\"",
        "        self.level = level",
        "        self.minicolumns: Dict[str, Minicolumn] = {}",
        "    ",
        "    def get_or_create_minicolumn(self, content: str) -> Minicolumn:",
        "        \"\"\"",
        "        Get existing minicolumn or create new one.",
        "        ",
        "        This is the primary way to add content to a layer. If a",
        "        minicolumn for this content already exists, return it.",
        "        Otherwise, create a new one.",
        "        ",
        "        Args:",
        "            content: The content for this minicolumn",
        "            ",
        "        Returns:",
        "            The existing or newly created Minicolumn",
        "        \"\"\"",
        "        if content not in self.minicolumns:",
        "            col_id = f\"L{self.level}_{content}\"",
        "            self.minicolumns[content] = Minicolumn(col_id, content, self.level)",
        "        return self.minicolumns[content]",
        "    ",
        "    def get_minicolumn(self, content: str) -> Optional[Minicolumn]:",
        "        \"\"\"",
        "        Get a minicolumn by content, or None if not found.",
        "        ",
        "        Args:",
        "            content: The content to look up",
        "            ",
        "        Returns:",
        "            The Minicolumn if found, None otherwise",
        "        \"\"\"",
        "        return self.minicolumns.get(content)",
        "    ",
        "    def column_count(self) -> int:",
        "        \"\"\"Return the number of minicolumns in this layer.\"\"\"",
        "        return len(self.minicolumns)",
        "    ",
        "    def total_connections(self) -> int:",
        "        \"\"\"Return total number of lateral connections in this layer.\"\"\"",
        "        return sum(col.connection_count() for col in self.minicolumns.values())",
        "    ",
        "    def average_activation(self) -> float:",
        "        \"\"\"Calculate average activation across all minicolumns.\"\"\"",
        "        if not self.minicolumns:",
        "            return 0.0",
        "        return sum(col.activation for col in self.minicolumns.values()) / len(self.minicolumns)",
        "    ",
        "    def activation_range(self) -> tuple:",
        "        \"\"\"Return (min, max) activation values.\"\"\"",
        "        if not self.minicolumns:",
        "            return (0.0, 0.0)",
        "        activations = [col.activation for col in self.minicolumns.values()]",
        "        return (min(activations), max(activations))",
        "    ",
        "    def sparsity(self) -> float:",
        "        \"\"\"",
        "        Calculate sparsity (fraction of columns with low activation).",
        "        ",
        "        In biological neural networks, sparse representations are",
        "        more efficient and allow for more distinct patterns.",
        "        ",
        "        Returns:",
        "            Fraction of columns with activation below threshold",
        "        \"\"\"",
        "        if not self.minicolumns:",
        "            return 0.0",
        "        threshold = 1.0  # Activation threshold",
        "        low_activation = sum(1 for col in self.minicolumns.values() ",
        "                            if col.activation < threshold)",
        "        return low_activation / len(self.minicolumns)",
        "    ",
        "    def top_by_pagerank(self, n: int = 10) -> list:",
        "        \"\"\"",
        "        Get top minicolumns by PageRank score.",
        "        ",
        "        Args:",
        "            n: Number of results to return",
        "            ",
        "        Returns:",
        "            List of (content, pagerank) tuples",
        "        \"\"\"",
        "        sorted_cols = sorted(",
        "            self.minicolumns.values(),",
        "            key=lambda c: c.pagerank,",
        "            reverse=True",
        "        )",
        "        return [(col.content, col.pagerank) for col in sorted_cols[:n]]",
        "    ",
        "    def top_by_tfidf(self, n: int = 10) -> list:",
        "        \"\"\"",
        "        Get top minicolumns by TF-IDF score.",
        "        ",
        "        Args:",
        "            n: Number of results to return",
        "            ",
        "        Returns:",
        "            List of (content, tfidf) tuples",
        "        \"\"\"",
        "        sorted_cols = sorted(",
        "            self.minicolumns.values(),",
        "            key=lambda c: c.tfidf,",
        "            reverse=True",
        "        )",
        "        return [(col.content, col.tfidf) for col in sorted_cols[:n]]",
        "    ",
        "    def top_by_activation(self, n: int = 10) -> list:",
        "        \"\"\"",
        "        Get top minicolumns by activation level.",
        "        ",
        "        Args:",
        "            n: Number of results to return",
        "            ",
        "        Returns:",
        "            List of (content, activation) tuples",
        "        \"\"\"",
        "        sorted_cols = sorted(",
        "            self.minicolumns.values(),",
        "            key=lambda c: c.activation,",
        "            reverse=True",
        "        )",
        "        return [(col.content, col.activation) for col in sorted_cols[:n]]",
        "    ",
        "    def __iter__(self) -> Iterator[Minicolumn]:",
        "        \"\"\"Iterate over minicolumns in this layer.\"\"\"",
        "        return iter(self.minicolumns.values())",
        "    ",
        "    def __len__(self) -> int:",
        "        \"\"\"Return number of minicolumns.\"\"\"",
        "        return len(self.minicolumns)",
        "    ",
        "    def __contains__(self, content: str) -> bool:",
        "        \"\"\"Check if content exists in this layer.\"\"\"",
        "        return content in self.minicolumns",
        "    ",
        "    def to_dict(self) -> Dict:",
        "        \"\"\"",
        "        Convert layer to dictionary for serialization.",
        "        ",
        "        Returns:",
        "            Dictionary representation of this layer",
        "        \"\"\"",
        "        return {",
        "            'level': self.level,",
        "            'minicolumns': {",
        "                content: col.to_dict() ",
        "                for content, col in self.minicolumns.items()",
        "            }",
        "        }",
        "    ",
        "    @classmethod",
        "    def from_dict(cls, data: Dict) -> 'HierarchicalLayer':",
        "        \"\"\"",
        "        Create a layer from dictionary representation.",
        "        ",
        "        Args:",
        "            data: Dictionary with layer data",
        "            ",
        "        Returns:",
        "            New HierarchicalLayer instance",
        "        \"\"\"",
        "        layer = cls(CorticalLayer(data['level']))",
        "        for content, col_data in data.get('minicolumns', {}).items():",
        "            layer.minicolumns[content] = Minicolumn.from_dict(col_data)",
        "        return layer",
        "    ",
        "    def __repr__(self) -> str:",
        "        return f\"HierarchicalLayer(level={self.level.name}, columns={len(self.minicolumns)})\""
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "cortical_package/cortical/minicolumn.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "\"\"\"",
        "Minicolumn Module",
        "=================",
        "",
        "Core data structure representing a cortical minicolumn.",
        "",
        "In the neocortex, minicolumns are vertical structures containing",
        "~80-100 neurons that respond to similar features. This class models",
        "that concept for text processing.",
        "\"\"\"",
        "",
        "from typing import Set, Dict, Optional",
        "from collections import defaultdict",
        "",
        "",
        "class Minicolumn:",
        "    \"\"\"",
        "    A minicolumn represents a single concept/feature at a given hierarchy level.",
        "    ",
        "    In the biological neocortex, minicolumns are the fundamental processing",
        "    units. Here, each minicolumn represents:",
        "    - Layer 0: A single token/word",
        "    - Layer 1: A bigram pattern",
        "    - Layer 2: A concept cluster",
        "    - Layer 3: A document",
        "    ",
        "    Attributes:",
        "        id: Unique identifier (e.g., \"L0_neural\")",
        "        content: The actual content (word, bigram, doc_id)",
        "        layer: Which layer this column belongs to",
        "        activation: Current activation level (like neural firing rate)",
        "        occurrence_count: How many times this has been observed",
        "        document_ids: Which documents contain this content",
        "        lateral_connections: Connections to other columns at same layer",
        "        feedforward_sources: IDs of columns that feed into this one",
        "        tfidf: TF-IDF weight for this term",
        "        tfidf_per_doc: Document-specific TF-IDF scores",
        "        pagerank: Importance score from PageRank algorithm",
        "        cluster_id: Which cluster this belongs to (for Layer 0)",
        "        ",
        "    Example:",
        "        col = Minicolumn(\"L0_neural\", \"neural\", 0)",
        "        col.occurrence_count = 15",
        "        col.add_lateral_connection(\"L0_network\", 0.8)",
        "    \"\"\"",
        "    ",
        "    __slots__ = [",
        "        'id', 'content', 'layer', 'activation', 'occurrence_count',",
        "        'document_ids', 'lateral_connections', 'feedforward_sources',",
        "        'tfidf', 'tfidf_per_doc', 'pagerank', 'cluster_id'",
        "    ]",
        "    ",
        "    def __init__(self, id: str, content: str, layer: int):",
        "        \"\"\"",
        "        Initialize a minicolumn.",
        "        ",
        "        Args:",
        "            id: Unique identifier for this column",
        "            content: The content this column represents",
        "            layer: Layer number (0-3)",
        "        \"\"\"",
        "        self.id = id",
        "        self.content = content",
        "        self.layer = layer",
        "        self.activation = 0.0",
        "        self.occurrence_count = 0",
        "        self.document_ids: Set[str] = set()",
        "        self.lateral_connections: Dict[str, float] = {}",
        "        self.feedforward_sources: Set[str] = set()",
        "        self.tfidf = 0.0",
        "        self.tfidf_per_doc: Dict[str, float] = {}",
        "        self.pagerank = 1.0",
        "        self.cluster_id: Optional[int] = None",
        "    ",
        "    def add_lateral_connection(self, target_id: str, weight: float = 1.0) -> None:",
        "        \"\"\"",
        "        Add or strengthen a lateral connection to another column.",
        "        ",
        "        Lateral connections represent associations learned through",
        "        co-occurrence (like Hebbian learning: \"neurons that fire together",
        "        wire together\").",
        "        ",
        "        Args:",
        "            target_id: ID of the target minicolumn",
        "            weight: Connection strength to add",
        "        \"\"\"",
        "        self.lateral_connections[target_id] = (",
        "            self.lateral_connections.get(target_id, 0) + weight",
        "        )",
        "    ",
        "    def connection_count(self) -> int:",
        "        \"\"\"Return the number of lateral connections.\"\"\"",
        "        return len(self.lateral_connections)",
        "    ",
        "    def top_connections(self, n: int = 5) -> list:",
        "        \"\"\"",
        "        Get the strongest lateral connections.",
        "        ",
        "        Args:",
        "            n: Number of connections to return",
        "            ",
        "        Returns:",
        "            List of (target_id, weight) tuples, sorted by weight",
        "        \"\"\"",
        "        sorted_conns = sorted(",
        "            self.lateral_connections.items(),",
        "            key=lambda x: x[1],",
        "            reverse=True",
        "        )",
        "        return sorted_conns[:n]",
        "    ",
        "    def to_dict(self) -> Dict:",
        "        \"\"\"",
        "        Convert to dictionary for serialization.",
        "        ",
        "        Returns:",
        "            Dictionary representation of this minicolumn",
        "        \"\"\"",
        "        return {",
        "            'id': self.id,",
        "            'content': self.content,",
        "            'layer': self.layer,",
        "            'activation': self.activation,",
        "            'occurrence_count': self.occurrence_count,",
        "            'document_ids': list(self.document_ids),",
        "            'lateral_connections': self.lateral_connections,",
        "            'feedforward_sources': list(self.feedforward_sources),",
        "            'tfidf': self.tfidf,",
        "            'tfidf_per_doc': self.tfidf_per_doc,",
        "            'pagerank': self.pagerank,",
        "            'cluster_id': self.cluster_id",
        "        }",
        "    ",
        "    @classmethod",
        "    def from_dict(cls, data: Dict) -> 'Minicolumn':",
        "        \"\"\"",
        "        Create a minicolumn from dictionary representation.",
        "        ",
        "        Args:",
        "            data: Dictionary with minicolumn data",
        "            ",
        "        Returns:",
        "            New Minicolumn instance",
        "        \"\"\"",
        "        col = cls(data['id'], data['content'], data['layer'])",
        "        col.activation = data.get('activation', 0.0)",
        "        col.occurrence_count = data.get('occurrence_count', 0)",
        "        col.document_ids = set(data.get('document_ids', []))",
        "        col.lateral_connections = data.get('lateral_connections', {})",
        "        col.feedforward_sources = set(data.get('feedforward_sources', []))",
        "        col.tfidf = data.get('tfidf', 0.0)",
        "        col.tfidf_per_doc = data.get('tfidf_per_doc', {})",
        "        col.pagerank = data.get('pagerank', 1.0)",
        "        col.cluster_id = data.get('cluster_id')",
        "        return col",
        "    ",
        "    def __repr__(self) -> str:",
        "        return f\"Minicolumn(id={self.id}, content={self.content}, layer={self.layer})\""
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "cortical_package/cortical/persistence.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "\"\"\"",
        "Persistence Module",
        "==================",
        "",
        "Save and load functionality for the cortical processor.",
        "",
        "Supports:",
        "- Pickle serialization for full state",
        "- JSON export for graph visualization",
        "- Incremental updates",
        "\"\"\"",
        "",
        "import pickle",
        "import json",
        "import os",
        "from typing import Dict, Optional, Any",
        "",
        "from .layers import CorticalLayer, HierarchicalLayer",
        "from .minicolumn import Minicolumn",
        "",
        "",
        "def save_processor(",
        "    filepath: str,",
        "    layers: Dict[CorticalLayer, HierarchicalLayer],",
        "    documents: Dict[str, str],",
        "    metadata: Optional[Dict] = None,",
        "    verbose: bool = True",
        ") -> None:",
        "    \"\"\"",
        "    Save processor state to a file.",
        "    ",
        "    Args:",
        "        filepath: Path to save file",
        "        layers: Dictionary of all layers",
        "        documents: Document collection",
        "        metadata: Optional metadata (version, settings, etc.)",
        "        verbose: Print progress",
        "    \"\"\"",
        "    state = {",
        "        'version': '2.0',",
        "        'layers': {},",
        "        'documents': documents,",
        "        'metadata': metadata or {}",
        "    }",
        "    ",
        "    # Serialize layers",
        "    for layer_enum, layer in layers.items():",
        "        state['layers'][layer_enum.value] = layer.to_dict()",
        "    ",
        "    with open(filepath, 'wb') as f:",
        "        pickle.dump(state, f, protocol=pickle.HIGHEST_PROTOCOL)",
        "    ",
        "    if verbose:",
        "        total_cols = sum(len(layer.minicolumns) for layer in layers.values())",
        "        total_conns = sum(layer.total_connections() for layer in layers.values())",
        "        print(f\"✓ Saved processor to {filepath}\")",
        "        print(f\"  - {len(documents)} documents\")",
        "        print(f\"  - {total_cols} minicolumns\")",
        "        print(f\"  - {total_conns} connections\")",
        "",
        "",
        "def load_processor(",
        "    filepath: str,",
        "    verbose: bool = True",
        ") -> tuple:",
        "    \"\"\"",
        "    Load processor state from a file.",
        "    ",
        "    Args:",
        "        filepath: Path to saved file",
        "        verbose: Print progress",
        "        ",
        "    Returns:",
        "        Tuple of (layers, documents, metadata)",
        "    \"\"\"",
        "    with open(filepath, 'rb') as f:",
        "        state = pickle.load(f)",
        "    ",
        "    # Reconstruct layers",
        "    layers = {}",
        "    for level_value, layer_data in state.get('layers', {}).items():",
        "        layer = HierarchicalLayer.from_dict(layer_data)",
        "        layers[CorticalLayer(int(level_value))] = layer",
        "    ",
        "    documents = state.get('documents', {})",
        "    metadata = state.get('metadata', {})",
        "    ",
        "    if verbose:",
        "        total_cols = sum(len(layer.minicolumns) for layer in layers.values())",
        "        total_conns = sum(layer.total_connections() for layer in layers.values())",
        "        print(f\"✓ Loaded processor from {filepath}\")",
        "        print(f\"  - {len(documents)} documents\")",
        "        print(f\"  - {total_cols} minicolumns\")",
        "        print(f\"  - {total_conns} connections\")",
        "    ",
        "    return layers, documents, metadata",
        "",
        "",
        "def export_graph_json(",
        "    filepath: str,",
        "    layers: Dict[CorticalLayer, HierarchicalLayer],",
        "    layer_filter: Optional[CorticalLayer] = None,",
        "    min_weight: float = 0.0,",
        "    max_nodes: int = 500",
        ") -> Dict:",
        "    \"\"\"",
        "    Export graph structure as JSON for visualization.",
        "    ",
        "    Creates a format compatible with D3.js, vis.js, etc.",
        "    ",
        "    Args:",
        "        filepath: Output file path",
        "        layers: Dictionary of layers",
        "        layer_filter: Only export specific layer (None = all)",
        "        min_weight: Minimum edge weight to include",
        "        max_nodes: Maximum nodes to export",
        "        ",
        "    Returns:",
        "        The exported graph data",
        "    \"\"\"",
        "    nodes = []",
        "    edges = []",
        "    node_ids = set()",
        "    ",
        "    # Determine which layers to export",
        "    if layer_filter is not None:",
        "        layer_list = [layers.get(layer_filter)]",
        "    else:",
        "        layer_list = list(layers.values())",
        "    ",
        "    # Collect nodes (sorted by PageRank)",
        "    all_columns = []",
        "    for layer in layer_list:",
        "        if layer:",
        "            all_columns.extend(layer.minicolumns.values())",
        "    ",
        "    all_columns.sort(key=lambda c: c.pagerank, reverse=True)",
        "    ",
        "    # Take top nodes",
        "    for col in all_columns[:max_nodes]:",
        "        nodes.append({",
        "            'id': col.id,",
        "            'label': col.content,",
        "            'layer': col.layer,",
        "            'pagerank': col.pagerank,",
        "            'tfidf': col.tfidf,",
        "            'activation': col.activation,",
        "            'documents': len(col.document_ids)",
        "        })",
        "        node_ids.add(col.id)",
        "    ",
        "    # Collect edges",
        "    for col in all_columns[:max_nodes]:",
        "        for target_id, weight in col.lateral_connections.items():",
        "            if weight >= min_weight and target_id in node_ids:",
        "                edges.append({",
        "                    'source': col.id,",
        "                    'target': target_id,",
        "                    'weight': weight",
        "                })",
        "    ",
        "    graph = {",
        "        'nodes': nodes,",
        "        'edges': edges,",
        "        'metadata': {",
        "            'node_count': len(nodes),",
        "            'edge_count': len(edges),",
        "            'layers': [l.value for l in layers.keys() if l is not None]",
        "        }",
        "    }",
        "    ",
        "    with open(filepath, 'w') as f:",
        "        json.dump(graph, f, indent=2)",
        "    ",
        "    print(f\"Graph exported to {filepath}\")",
        "    print(f\"  - {len(nodes)} nodes, {len(edges)} edges\")",
        "    ",
        "    return graph",
        "",
        "",
        "def export_embeddings_json(",
        "    filepath: str,",
        "    embeddings: Dict[str, list],",
        "    metadata: Optional[Dict] = None",
        ") -> None:",
        "    \"\"\"",
        "    Export embeddings as JSON.",
        "    ",
        "    Args:",
        "        filepath: Output file path",
        "        embeddings: Dictionary of term -> embedding vector",
        "        metadata: Optional metadata",
        "    \"\"\"",
        "    data = {",
        "        'embeddings': embeddings,",
        "        'dimensions': len(next(iter(embeddings.values()))) if embeddings else 0,",
        "        'terms': len(embeddings),",
        "        'metadata': metadata or {}",
        "    }",
        "    ",
        "    with open(filepath, 'w') as f:",
        "        json.dump(data, f)",
        "    ",
        "    print(f\"Embeddings exported to {filepath}\")",
        "    print(f\"  - {len(embeddings)} terms, {data['dimensions']} dimensions\")",
        "",
        "",
        "def load_embeddings_json(filepath: str) -> Dict[str, list]:",
        "    \"\"\"",
        "    Load embeddings from JSON.",
        "    ",
        "    Args:",
        "        filepath: Input file path",
        "        ",
        "    Returns:",
        "        Dictionary of term -> embedding vector",
        "    \"\"\"",
        "    with open(filepath, 'r') as f:",
        "        data = json.load(f)",
        "    ",
        "    return data.get('embeddings', {})",
        "",
        "",
        "def export_semantic_relations_json(",
        "    filepath: str,",
        "    relations: list",
        ") -> None:",
        "    \"\"\"",
        "    Export semantic relations as JSON.",
        "    ",
        "    Args:",
        "        filepath: Output file path",
        "        relations: List of relation dictionaries",
        "    \"\"\"",
        "    with open(filepath, 'w') as f:",
        "        json.dump({",
        "            'relations': relations,",
        "            'count': len(relations)",
        "        }, f, indent=2)",
        "    ",
        "    print(f\"Relations exported to {filepath}\")",
        "    print(f\"  - {len(relations)} relations\")",
        "",
        "",
        "def load_semantic_relations_json(filepath: str) -> list:",
        "    \"\"\"",
        "    Load semantic relations from JSON.",
        "    ",
        "    Args:",
        "        filepath: Input file path",
        "        ",
        "    Returns:",
        "        List of relation dictionaries",
        "    \"\"\"",
        "    with open(filepath, 'r') as f:",
        "        data = json.load(f)",
        "    ",
        "    return data.get('relations', [])",
        "",
        "",
        "def get_state_summary(",
        "    layers: Dict[CorticalLayer, HierarchicalLayer],",
        "    documents: Dict[str, str]",
        ") -> Dict:",
        "    \"\"\"",
        "    Get a summary of the current processor state.",
        "    ",
        "    Args:",
        "        layers: Dictionary of layers",
        "        documents: Document collection",
        "        ",
        "    Returns:",
        "        Summary statistics",
        "    \"\"\"",
        "    summary = {",
        "        'documents': len(documents),",
        "        'layers': {}",
        "    }",
        "    ",
        "    for layer_enum, layer in layers.items():",
        "        summary['layers'][layer_enum.name] = {",
        "            'columns': len(layer.minicolumns),",
        "            'connections': layer.total_connections(),",
        "            'avg_activation': layer.average_activation(),",
        "            'sparsity': layer.sparsity()",
        "        }",
        "    ",
        "    summary['total_columns'] = sum(",
        "        len(layer.minicolumns) for layer in layers.values()",
        "    )",
        "    summary['total_connections'] = sum(",
        "        layer.total_connections() for layer in layers.values()",
        "    )",
        "    ",
        "    return summary"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "cortical_package/cortical/processor.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "\"\"\"",
        "Cortical Text Processor - Main processor class that orchestrates all components.",
        "\"\"\"",
        "",
        "import os",
        "import re",
        "from typing import Dict, List, Tuple, Optional, Any",
        "from collections import defaultdict",
        "",
        "from .tokenizer import Tokenizer",
        "from .minicolumn import Minicolumn",
        "from .layers import CorticalLayer, HierarchicalLayer",
        "from . import analysis",
        "from . import semantics",
        "from . import embeddings as emb_module",
        "from . import query as query_module",
        "from . import gaps as gaps_module",
        "from . import persistence",
        "",
        "",
        "class CorticalTextProcessor:",
        "    \"\"\"Neocortex-inspired text processing system.\"\"\"",
        "    ",
        "    def __init__(self, tokenizer: Optional[Tokenizer] = None):",
        "        self.tokenizer = tokenizer or Tokenizer()",
        "        self.layers: Dict[CorticalLayer, HierarchicalLayer] = {",
        "            CorticalLayer.TOKENS: HierarchicalLayer(CorticalLayer.TOKENS),",
        "            CorticalLayer.BIGRAMS: HierarchicalLayer(CorticalLayer.BIGRAMS),",
        "            CorticalLayer.CONCEPTS: HierarchicalLayer(CorticalLayer.CONCEPTS),",
        "            CorticalLayer.DOCUMENTS: HierarchicalLayer(CorticalLayer.DOCUMENTS),",
        "        }",
        "        self.documents: Dict[str, str] = {}",
        "        self.embeddings: Dict[str, List[float]] = {}",
        "        self.semantic_relations: List[Tuple[str, str, str, float]] = []",
        "    ",
        "    def process_document(self, doc_id: str, content: str) -> Dict[str, int]:",
        "        \"\"\"Process a document and add it to the corpus.\"\"\"",
        "        self.documents[doc_id] = content",
        "        tokens = self.tokenizer.tokenize(content)",
        "        bigrams = self.tokenizer.extract_ngrams(tokens, n=2)",
        "        ",
        "        layer0 = self.layers[CorticalLayer.TOKENS]",
        "        layer1 = self.layers[CorticalLayer.BIGRAMS]",
        "        layer3 = self.layers[CorticalLayer.DOCUMENTS]",
        "        ",
        "        doc_col = layer3.get_or_create_minicolumn(doc_id)",
        "        doc_col.occurrence_count += 1",
        "        ",
        "        for token in tokens:",
        "            col = layer0.get_or_create_minicolumn(token)",
        "            col.occurrence_count += 1",
        "            col.document_ids.add(doc_id)",
        "            col.activation += 1.0",
        "            doc_col.feedforward_sources.add(col.id)",
        "        ",
        "        for i, token in enumerate(tokens):",
        "            col = layer0.get_minicolumn(token)",
        "            if col:",
        "                for j in range(max(0, i-3), min(len(tokens), i+4)):",
        "                    if i != j:",
        "                        other = layer0.get_minicolumn(tokens[j])",
        "                        if other:",
        "                            col.add_lateral_connection(other.id, 1.0)",
        "        ",
        "        for bigram in bigrams:",
        "            col = layer1.get_or_create_minicolumn(bigram)",
        "            col.occurrence_count += 1",
        "            col.document_ids.add(doc_id)",
        "            col.activation += 1.0",
        "            for part in bigram.split():",
        "                token_col = layer0.get_minicolumn(part)",
        "                if token_col:",
        "                    col.feedforward_sources.add(token_col.id)",
        "        ",
        "        return {'tokens': len(tokens), 'bigrams': len(bigrams), 'unique_tokens': len(set(tokens))}",
        "    ",
        "    def compute_all(self, verbose: bool = True) -> None:",
        "        \"\"\"Run all computation steps.\"\"\"",
        "        if verbose: print(\"Computing activation propagation...\")",
        "        self.propagate_activation(verbose=False)",
        "        if verbose: print(\"Computing importance (PageRank)...\")",
        "        self.compute_importance(verbose=False)",
        "        if verbose: print(\"Computing TF-IDF...\")",
        "        self.compute_tfidf(verbose=False)",
        "        if verbose: print(\"Computing document connections...\")",
        "        self.compute_document_connections(verbose=False)",
        "        if verbose: print(\"Done.\")",
        "    ",
        "    def propagate_activation(self, iterations: int = 3, decay: float = 0.8, verbose: bool = True) -> None:",
        "        analysis.propagate_activation(self.layers, iterations, decay)",
        "        if verbose: print(f\"Propagated activation ({iterations} iterations)\")",
        "    ",
        "    def compute_importance(self, verbose: bool = True) -> None:",
        "        for layer_enum in [CorticalLayer.TOKENS, CorticalLayer.BIGRAMS]:",
        "            analysis.compute_pagerank(self.layers[layer_enum])",
        "        if verbose: print(\"Computed PageRank importance\")",
        "    ",
        "    def compute_tfidf(self, verbose: bool = True) -> None:",
        "        analysis.compute_tfidf(self.layers, self.documents)",
        "        if verbose: print(\"Computed TF-IDF scores\")",
        "    ",
        "    def compute_document_connections(self, min_shared_terms: int = 3, verbose: bool = True) -> None:",
        "        analysis.compute_document_connections(self.layers, self.documents, min_shared_terms)",
        "        if verbose: print(\"Computed document connections\")",
        "    ",
        "    def build_concept_clusters(self, verbose: bool = True) -> Dict[int, List[str]]:",
        "        clusters = analysis.cluster_by_label_propagation(self.layers[CorticalLayer.TOKENS])",
        "        analysis.build_concept_clusters(self.layers, clusters)",
        "        if verbose: print(f\"Built {len(clusters)} concept clusters\")",
        "        return clusters",
        "    ",
        "    def extract_corpus_semantics(self, verbose: bool = True) -> int:",
        "        self.semantic_relations = semantics.extract_corpus_semantics(self.layers, self.documents, self.tokenizer)",
        "        if verbose: print(f\"Extracted {len(self.semantic_relations)} semantic relations\")",
        "        return len(self.semantic_relations)",
        "    ",
        "    def retrofit_connections(self, iterations: int = 10, alpha: float = 0.3, verbose: bool = True) -> Dict:",
        "        if not self.semantic_relations: self.extract_corpus_semantics(verbose=False)",
        "        stats = semantics.retrofit_connections(self.layers, self.semantic_relations, iterations, alpha)",
        "        if verbose: print(f\"Retrofitted {stats['tokens_affected']} tokens\")",
        "        return stats",
        "    ",
        "    def compute_graph_embeddings(self, dimensions: int = 64, method: str = 'adjacency', verbose: bool = True) -> Dict:",
        "        self.embeddings, stats = emb_module.compute_graph_embeddings(self.layers, dimensions, method)",
        "        if verbose: print(f\"Computed {stats['terms_embedded']} embeddings ({method})\")",
        "        return stats",
        "    ",
        "    def retrofit_embeddings(self, iterations: int = 10, alpha: float = 0.4, verbose: bool = True) -> Dict:",
        "        if not self.embeddings: self.compute_graph_embeddings(verbose=False)",
        "        if not self.semantic_relations: self.extract_corpus_semantics(verbose=False)",
        "        stats = semantics.retrofit_embeddings(self.embeddings, self.semantic_relations, iterations, alpha)",
        "        if verbose: print(f\"Retrofitted embeddings (moved {stats['total_movement']:.2f} total)\")",
        "        return stats",
        "    ",
        "    def embedding_similarity(self, term1: str, term2: str) -> float:",
        "        return emb_module.embedding_similarity(self.embeddings, term1, term2)",
        "    ",
        "    def find_similar_by_embedding(self, term: str, top_n: int = 10) -> List[Tuple[str, float]]:",
        "        return emb_module.find_similar_by_embedding(self.embeddings, term, top_n)",
        "    ",
        "    def expand_query(self, query_text: str, max_expansions: int = 10, use_variants: bool = True, verbose: bool = False) -> Dict[str, float]:",
        "        return query_module.expand_query(query_text, self.layers, self.tokenizer, max_expansions=max_expansions, use_variants=use_variants)",
        "    ",
        "    def expand_query_semantic(self, query_text: str, max_expansions: int = 10) -> Dict[str, float]:",
        "        return query_module.expand_query_semantic(query_text, self.layers, self.tokenizer, self.semantic_relations, max_expansions)",
        "    ",
        "    def find_documents_for_query(self, query_text: str, top_n: int = 5, use_expansion: bool = True) -> List[Tuple[str, float]]:",
        "        return query_module.find_documents_for_query(query_text, self.layers, self.tokenizer, top_n, use_expansion)",
        "    ",
        "    def query_expanded(self, query_text: str, top_n: int = 10, max_expansions: int = 8) -> List[Tuple[str, float]]:",
        "        return query_module.query_with_spreading_activation(query_text, self.layers, self.tokenizer, top_n, max_expansions)",
        "    ",
        "    def find_related_documents(self, doc_id: str) -> List[Tuple[str, float]]:",
        "        return query_module.find_related_documents(doc_id, self.layers)",
        "    ",
        "    def analyze_knowledge_gaps(self) -> Dict:",
        "        return gaps_module.analyze_knowledge_gaps(self.layers, self.documents)",
        "    ",
        "    def detect_anomalies(self, threshold: float = 0.3) -> List[Dict]:",
        "        return gaps_module.detect_anomalies(self.layers, self.documents, threshold)",
        "    ",
        "    def get_layer(self, layer: CorticalLayer) -> HierarchicalLayer:",
        "        return self.layers[layer]",
        "    ",
        "    def get_document_signature(self, doc_id: str, n: int = 10) -> List[Tuple[str, float]]:",
        "        layer0 = self.layers[CorticalLayer.TOKENS]",
        "        terms = [(col.content, col.tfidf_per_doc.get(doc_id, col.tfidf)) ",
        "                 for col in layer0.minicolumns.values() if doc_id in col.document_ids]",
        "        return sorted(terms, key=lambda x: x[1], reverse=True)[:n]",
        "    ",
        "    def get_corpus_summary(self) -> Dict:",
        "        return persistence.get_state_summary(self.layers, self.documents)",
        "    ",
        "    def save(self, filepath: str, verbose: bool = True) -> None:",
        "        metadata = {'has_embeddings': bool(self.embeddings), 'has_relations': bool(self.semantic_relations)}",
        "        persistence.save_processor(filepath, self.layers, self.documents, metadata, verbose)",
        "    ",
        "    @classmethod",
        "    def load(cls, filepath: str, verbose: bool = True) -> 'CorticalTextProcessor':",
        "        layers, documents, metadata = persistence.load_processor(filepath, verbose)",
        "        processor = cls()",
        "        processor.layers = layers",
        "        processor.documents = documents",
        "        return processor",
        "    ",
        "    def export_graph(self, filepath: str, layer: Optional[CorticalLayer] = None, max_nodes: int = 500) -> Dict:",
        "        return persistence.export_graph_json(filepath, self.layers, layer, max_nodes=max_nodes)",
        "    ",
        "    def summarize_document(self, doc_id: str, num_sentences: int = 3) -> str:",
        "        if doc_id not in self.documents: return \"\"",
        "        content = self.documents[doc_id]",
        "        sentences = re.split(r'(?<=[.!?])\\s+', content)",
        "        if len(sentences) <= num_sentences: return content",
        "        ",
        "        layer0 = self.layers[CorticalLayer.TOKENS]",
        "        scored = []",
        "        for sent in sentences:",
        "            tokens = self.tokenizer.tokenize(sent)",
        "            score = sum(layer0.get_minicolumn(t).tfidf if layer0.get_minicolumn(t) else 0 for t in tokens)",
        "            scored.append((sent, score))",
        "        scored.sort(key=lambda x: x[1], reverse=True)",
        "        top = [s for s, _ in scored[:num_sentences]]",
        "        return ' '.join([s for s in sentences if s in top])",
        "    ",
        "    def __repr__(self) -> str:",
        "        stats = self.get_corpus_summary()",
        "        return f\"CorticalTextProcessor(documents={stats['documents']}, columns={stats['total_columns']})\""
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "cortical_package/cortical/query.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "\"\"\"",
        "Query Module",
        "============",
        "",
        "Query expansion and search functionality.",
        "",
        "Provides methods for expanding queries using lateral connections,",
        "concept clusters, and word variants, then searching the corpus",
        "using TF-IDF and graph-based scoring.",
        "\"\"\"",
        "",
        "from typing import Dict, List, Tuple, Optional",
        "from collections import defaultdict",
        "",
        "from .layers import CorticalLayer, HierarchicalLayer",
        "from .tokenizer import Tokenizer",
        "",
        "",
        "def expand_query(",
        "    query_text: str,",
        "    layers: Dict[CorticalLayer, HierarchicalLayer],",
        "    tokenizer: Tokenizer,",
        "    max_expansions: int = 10,",
        "    use_lateral: bool = True,",
        "    use_concepts: bool = True,",
        "    use_variants: bool = True",
        ") -> Dict[str, float]:",
        "    \"\"\"",
        "    Expand a query using lateral connections and concept clusters.",
        "    ",
        "    This mimics how the brain retrieves related memories when given a cue:",
        "    - Lateral connections: direct word associations (like priming)",
        "    - Concept clusters: semantic category membership",
        "    - Word variants: stemming and synonym mapping",
        "    ",
        "    Args:",
        "        query_text: Original query string",
        "        layers: Dictionary of layers",
        "        tokenizer: Tokenizer instance",
        "        max_expansions: Maximum number of expansion terms to add",
        "        use_lateral: Include terms from lateral connections",
        "        use_concepts: Include terms from concept clusters",
        "        use_variants: Try word variants when direct match fails",
        "        ",
        "    Returns:",
        "        Dict mapping terms to weights (original terms get weight 1.0)",
        "    \"\"\"",
        "    tokens = tokenizer.tokenize(query_text)",
        "    layer0 = layers[CorticalLayer.TOKENS]",
        "    layer2 = layers.get(CorticalLayer.CONCEPTS)",
        "    ",
        "    # Start with original terms at full weight",
        "    expanded: Dict[str, float] = {}",
        "    unmatched_tokens = []",
        "    ",
        "    for token in tokens:",
        "        col = layer0.get_minicolumn(token)",
        "        if col:",
        "            expanded[token] = 1.0",
        "        else:",
        "            unmatched_tokens.append(token)",
        "    ",
        "    # Try to match unmatched tokens using variants",
        "    if use_variants and unmatched_tokens:",
        "        for token in unmatched_tokens:",
        "            variants = tokenizer.get_word_variants(token)",
        "            for variant in variants:",
        "                col = layer0.get_minicolumn(variant)",
        "                if col and variant not in expanded:",
        "                    expanded[variant] = 0.8",
        "                    break",
        "    ",
        "    if not expanded:",
        "        return expanded",
        "    ",
        "    candidate_expansions: Dict[str, float] = defaultdict(float)",
        "    ",
        "    # Method 1: Lateral connections (direct associations)",
        "    if use_lateral:",
        "        for token in list(expanded.keys()):",
        "            col = layer0.get_minicolumn(token)",
        "            if col:",
        "                sorted_neighbors = sorted(",
        "                    col.lateral_connections.items(),",
        "                    key=lambda x: x[1],",
        "                    reverse=True",
        "                )[:5]",
        "                ",
        "                for neighbor_id, weight in sorted_neighbors:",
        "                    if neighbor_id in layer0.minicolumns:",
        "                        neighbor = layer0.minicolumns[neighbor_id]",
        "                        if neighbor.content not in expanded:",
        "                            score = weight * neighbor.pagerank * 0.6",
        "                            candidate_expansions[neighbor.content] = max(",
        "                                candidate_expansions[neighbor.content], score",
        "                            )",
        "                    else:",
        "                        # Look up by ID",
        "                        for c in layer0.minicolumns.values():",
        "                            if c.id == neighbor_id and c.content not in expanded:",
        "                                score = weight * c.pagerank * 0.6",
        "                                candidate_expansions[c.content] = max(",
        "                                    candidate_expansions[c.content], score",
        "                                )",
        "                                break",
        "    ",
        "    # Method 2: Concept cluster membership",
        "    if use_concepts and layer2 and layer2.column_count() > 0:",
        "        for token in list(expanded.keys()):",
        "            col = layer0.get_minicolumn(token)",
        "            if col:",
        "                for concept in layer2.minicolumns.values():",
        "                    if col.id in concept.feedforward_sources:",
        "                        for member_id in concept.feedforward_sources:",
        "                            if member_id in layer0.minicolumns:",
        "                                member = layer0.minicolumns[member_id]",
        "                                if member.content not in expanded:",
        "                                    score = concept.pagerank * member.pagerank * 0.4",
        "                                    candidate_expansions[member.content] = max(",
        "                                        candidate_expansions[member.content], score",
        "                                    )",
        "    ",
        "    # Select top expansions",
        "    sorted_candidates = sorted(",
        "        candidate_expansions.items(),",
        "        key=lambda x: x[1],",
        "        reverse=True",
        "    )[:max_expansions]",
        "    ",
        "    for term, score in sorted_candidates:",
        "        expanded[term] = score",
        "    ",
        "    return expanded",
        "",
        "",
        "def expand_query_semantic(",
        "    query_text: str,",
        "    layers: Dict[CorticalLayer, HierarchicalLayer],",
        "    tokenizer: Tokenizer,",
        "    semantic_relations: List[Tuple[str, str, str, float]],",
        "    max_expansions: int = 10",
        ") -> Dict[str, float]:",
        "    \"\"\"",
        "    Expand query using semantic relations extracted from corpus.",
        "    ",
        "    Args:",
        "        query_text: Original query",
        "        layers: Dictionary of layers",
        "        tokenizer: Tokenizer instance",
        "        semantic_relations: List of (term1, relation, term2, weight) tuples",
        "        max_expansions: Maximum expansions",
        "        ",
        "    Returns:",
        "        Dict mapping terms to weights",
        "    \"\"\"",
        "    tokens = tokenizer.tokenize(query_text)",
        "    layer0 = layers[CorticalLayer.TOKENS]",
        "    ",
        "    # Build semantic neighbor lookup",
        "    neighbors: Dict[str, List[Tuple[str, float]]] = defaultdict(list)",
        "    for t1, relation, t2, weight in semantic_relations:",
        "        neighbors[t1].append((t2, weight))",
        "        neighbors[t2].append((t1, weight))",
        "    ",
        "    # Start with original terms",
        "    expanded = {t: 1.0 for t in tokens if layer0.get_minicolumn(t)}",
        "    ",
        "    if not expanded:",
        "        return expanded",
        "    ",
        "    # Add semantic neighbors",
        "    candidates: Dict[str, float] = defaultdict(float)",
        "    for token in list(expanded.keys()):",
        "        for neighbor, weight in neighbors.get(token, []):",
        "            if neighbor not in expanded and layer0.get_minicolumn(neighbor):",
        "                candidates[neighbor] = max(candidates[neighbor], weight * 0.7)",
        "    ",
        "    # Take top candidates",
        "    sorted_candidates = sorted(candidates.items(), key=lambda x: x[1], reverse=True)",
        "    for term, score in sorted_candidates[:max_expansions]:",
        "        expanded[term] = score",
        "    ",
        "    return expanded",
        "",
        "",
        "def find_documents_for_query(",
        "    query_text: str,",
        "    layers: Dict[CorticalLayer, HierarchicalLayer],",
        "    tokenizer: Tokenizer,",
        "    top_n: int = 5,",
        "    use_expansion: bool = True",
        ") -> List[Tuple[str, float]]:",
        "    \"\"\"",
        "    Find documents most relevant to a query using TF-IDF and optional expansion.",
        "    ",
        "    Args:",
        "        query_text: Search query",
        "        layers: Dictionary of layers",
        "        tokenizer: Tokenizer instance",
        "        top_n: Number of documents to return",
        "        use_expansion: Whether to expand query terms",
        "        ",
        "    Returns:",
        "        List of (doc_id, score) tuples ranked by relevance",
        "    \"\"\"",
        "    layer0 = layers[CorticalLayer.TOKENS]",
        "    ",
        "    if use_expansion:",
        "        query_terms = expand_query(query_text, layers, tokenizer, max_expansions=5)",
        "    else:",
        "        tokens = tokenizer.tokenize(query_text)",
        "        query_terms = {t: 1.0 for t in tokens}",
        "    ",
        "    # Score each document",
        "    doc_scores: Dict[str, float] = defaultdict(float)",
        "    ",
        "    for term, term_weight in query_terms.items():",
        "        col = layer0.get_minicolumn(term)",
        "        if col:",
        "            for doc_id in col.document_ids:",
        "                tfidf = col.tfidf_per_doc.get(doc_id, col.tfidf)",
        "                doc_scores[doc_id] += tfidf * term_weight",
        "    ",
        "    sorted_docs = sorted(doc_scores.items(), key=lambda x: -x[1])",
        "    return sorted_docs[:top_n]",
        "",
        "",
        "def query_with_spreading_activation(",
        "    query_text: str,",
        "    layers: Dict[CorticalLayer, HierarchicalLayer],",
        "    tokenizer: Tokenizer,",
        "    top_n: int = 10,",
        "    max_expansions: int = 8",
        ") -> List[Tuple[str, float]]:",
        "    \"\"\"",
        "    Query with automatic expansion using spreading activation.",
        "    ",
        "    This is like the brain's spreading activation during memory retrieval:",
        "    a cue activates not just direct matches but semantically related concepts.",
        "    ",
        "    Args:",
        "        query_text: Search query",
        "        layers: Dictionary of layers",
        "        tokenizer: Tokenizer instance",
        "        top_n: Number of results to return",
        "        max_expansions: How many expansion terms to add",
        "        ",
        "    Returns:",
        "        List of (concept, score) tuples ranked by relevance",
        "    \"\"\"",
        "    expanded_terms = expand_query(",
        "        query_text, layers, tokenizer,",
        "        max_expansions=max_expansions",
        "    )",
        "    ",
        "    if not expanded_terms:",
        "        return []",
        "    ",
        "    layer0 = layers[CorticalLayer.TOKENS]",
        "    activated: Dict[str, float] = {}",
        "    ",
        "    # Activate based on expanded query",
        "    for term, term_weight in expanded_terms.items():",
        "        col = layer0.get_minicolumn(term)",
        "        if col:",
        "            # Direct activation",
        "            score = col.pagerank * col.activation * term_weight",
        "            activated[col.content] = activated.get(col.content, 0) + score",
        "            ",
        "            # Spread to neighbors",
        "            for neighbor_id, conn_weight in col.lateral_connections.items():",
        "                if neighbor_id in layer0.minicolumns:",
        "                    neighbor = layer0.minicolumns[neighbor_id]",
        "                    spread_score = neighbor.pagerank * conn_weight * term_weight * 0.3",
        "                    activated[neighbor.content] = activated.get(neighbor.content, 0) + spread_score",
        "                else:",
        "                    for c in layer0.minicolumns.values():",
        "                        if c.id == neighbor_id:",
        "                            spread_score = c.pagerank * conn_weight * term_weight * 0.3",
        "                            activated[c.content] = activated.get(c.content, 0) + spread_score",
        "                            break",
        "    ",
        "    sorted_concepts = sorted(activated.items(), key=lambda x: -x[1])",
        "    return sorted_concepts[:top_n]",
        "",
        "",
        "def find_related_documents(",
        "    doc_id: str,",
        "    layers: Dict[CorticalLayer, HierarchicalLayer]",
        ") -> List[Tuple[str, float]]:",
        "    \"\"\"",
        "    Find documents related to a given document via lateral connections.",
        "    ",
        "    Args:",
        "        doc_id: Source document ID",
        "        layers: Dictionary of layers",
        "        ",
        "    Returns:",
        "        List of (doc_id, weight) tuples for related documents",
        "    \"\"\"",
        "    layer3 = layers.get(CorticalLayer.DOCUMENTS)",
        "    if not layer3:",
        "        return []",
        "    ",
        "    col = layer3.get_minicolumn(doc_id)",
        "    if not col:",
        "        return []",
        "    ",
        "    related = []",
        "    for neighbor_id, weight in col.lateral_connections.items():",
        "        if neighbor_id in layer3.minicolumns:",
        "            neighbor = layer3.minicolumns[neighbor_id]",
        "            related.append((neighbor.content, weight))",
        "        else:",
        "            for c in layer3.minicolumns.values():",
        "                if c.id == neighbor_id:",
        "                    related.append((c.content, weight))",
        "                    break",
        "    ",
        "    return sorted(related, key=lambda x: -x[1])"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "cortical_package/cortical/semantics.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "\"\"\"",
        "Semantics Module",
        "================",
        "",
        "Corpus-derived semantic relations and retrofitting.",
        "",
        "Extracts semantic relationships from co-occurrence patterns,",
        "then uses them to adjust connection weights (retrofitting).",
        "This is like building a \"poor man's ConceptNet\" from the corpus itself.",
        "\"\"\"",
        "",
        "import math",
        "import re",
        "from typing import Dict, List, Tuple, Set, Optional",
        "from collections import defaultdict",
        "",
        "from .layers import CorticalLayer, HierarchicalLayer",
        "from .minicolumn import Minicolumn",
        "",
        "",
        "# Relation type weights for retrofitting",
        "RELATION_WEIGHTS = {",
        "    'IsA': 1.5,",
        "    'PartOf': 1.2,",
        "    'HasA': 1.0,",
        "    'UsedFor': 0.8,",
        "    'CapableOf': 0.7,",
        "    'AtLocation': 0.6,",
        "    'Causes': 0.9,",
        "    'HasProperty': 0.8,",
        "    'SameAs': 2.0,",
        "    'RelatedTo': 0.5,",
        "    'Antonym': -0.5,",
        "    'DerivedFrom': 1.0,",
        "    'SimilarTo': 1.5,",
        "    'CoOccurs': 0.6,",
        "    'DefinedBy': 1.0,",
        "}",
        "",
        "",
        "def extract_corpus_semantics(",
        "    layers: Dict[CorticalLayer, HierarchicalLayer],",
        "    documents: Dict[str, str],",
        "    tokenizer,",
        "    window_size: int = 5,",
        "    min_cooccurrence: int = 2",
        ") -> List[Tuple[str, str, str, float]]:",
        "    \"\"\"",
        "    Extract semantic relations from corpus co-occurrence patterns.",
        "    ",
        "    Analyzes word co-occurrences to infer semantic relationships:",
        "    - Words appearing together frequently → RelatedTo",
        "    - Words appearing in similar contexts → SimilarTo",
        "    - Words in definitional patterns → IsA, DefinedBy",
        "    ",
        "    Args:",
        "        layers: Dictionary of layers (needs TOKENS)",
        "        documents: Dictionary of documents",
        "        tokenizer: Tokenizer instance for processing text",
        "        window_size: Co-occurrence window size",
        "        min_cooccurrence: Minimum co-occurrences to form relation",
        "        ",
        "    Returns:",
        "        List of (term1, relation, term2, weight) tuples",
        "    \"\"\"",
        "    layer0 = layers[CorticalLayer.TOKENS]",
        "    relations: List[Tuple[str, str, str, float]] = []",
        "    ",
        "    # Track co-occurrences within window",
        "    cooccurrence: Dict[Tuple[str, str], int] = defaultdict(int)",
        "    ",
        "    # Track context vectors for similarity",
        "    context_vectors: Dict[str, Dict[str, int]] = defaultdict(lambda: defaultdict(int))",
        "    ",
        "    for doc_id, content in documents.items():",
        "        tokens = tokenizer.tokenize(content)",
        "        ",
        "        # Window-based co-occurrence",
        "        for i, token in enumerate(tokens):",
        "            window_start = max(0, i - window_size)",
        "            window_end = min(len(tokens), i + window_size + 1)",
        "            ",
        "            for j in range(window_start, window_end):",
        "                if i != j:",
        "                    other = tokens[j]",
        "                    if token < other:  # Avoid duplicates",
        "                        cooccurrence[(token, other)] += 1",
        "                    else:",
        "                        cooccurrence[(other, token)] += 1",
        "                    ",
        "                    # Build context vector",
        "                    context_vectors[token][other] += 1",
        "    ",
        "    # Extract RelatedTo from co-occurrence",
        "    for (t1, t2), count in cooccurrence.items():",
        "        if count >= min_cooccurrence:",
        "            # Normalize by frequency",
        "            col1 = layer0.get_minicolumn(t1)",
        "            col2 = layer0.get_minicolumn(t2)",
        "            ",
        "            if col1 and col2:",
        "                # PMI-like score",
        "                total = sum(cooccurrence.values())",
        "                expected = (col1.occurrence_count * col2.occurrence_count) / (total + 1)",
        "                pmi = math.log((count + 1) / (expected + 1))",
        "                ",
        "                if pmi > 0:",
        "                    relations.append((t1, 'CoOccurs', t2, min(pmi, 3.0)))",
        "    ",
        "    # Extract SimilarTo from context similarity",
        "    terms = list(context_vectors.keys())",
        "    for i, t1 in enumerate(terms):",
        "        vec1 = context_vectors[t1]",
        "        ",
        "        for t2 in terms[i+1:]:",
        "            vec2 = context_vectors[t2]",
        "            ",
        "            # Cosine similarity of context vectors",
        "            common = set(vec1.keys()) & set(vec2.keys())",
        "            if len(common) >= 3:",
        "                dot = sum(vec1[k] * vec2[k] for k in common)",
        "                mag1 = math.sqrt(sum(v*v for v in vec1.values()))",
        "                mag2 = math.sqrt(sum(v*v for v in vec2.values()))",
        "                ",
        "                if mag1 > 0 and mag2 > 0:",
        "                    sim = dot / (mag1 * mag2)",
        "                    if sim > 0.3:",
        "                        relations.append((t1, 'SimilarTo', t2, sim))",
        "    ",
        "    # Extract IsA from definitional patterns",
        "    isa_patterns = [",
        "        r'(\\w+)\\s+(?:is|are)\\s+(?:a|an)\\s+(?:type\\s+of\\s+)?(\\w+)',",
        "        r'(\\w+),?\\s+(?:a|an)\\s+(?:kind|type)\\s+of\\s+(\\w+)',",
        "        r'(\\w+)\\s+(?:such\\s+as|like)\\s+(\\w+)',",
        "    ]",
        "    ",
        "    for doc_id, content in documents.items():",
        "        content_lower = content.lower()",
        "        for pattern in isa_patterns:",
        "            for match in re.finditer(pattern, content_lower):",
        "                t1, t2 = match.groups()",
        "                if t1 in layer0.minicolumns and t2 in layer0.minicolumns:",
        "                    relations.append((t1, 'IsA', t2, 1.0))",
        "    ",
        "    return relations",
        "",
        "",
        "def retrofit_connections(",
        "    layers: Dict[CorticalLayer, HierarchicalLayer],",
        "    semantic_relations: List[Tuple[str, str, str, float]],",
        "    iterations: int = 10,",
        "    alpha: float = 0.3",
        ") -> Dict[str, any]:",
        "    \"\"\"",
        "    Retrofit lateral connections using semantic relations.",
        "    ",
        "    Adjusts connection weights by blending co-occurrence patterns",
        "    with semantic relations. This is inspired by Faruqui et al.'s",
        "    retrofitting algorithm for word vectors.",
        "    ",
        "    Args:",
        "        layers: Dictionary of layers",
        "        semantic_relations: List of (term1, relation, term2, weight) tuples",
        "        iterations: Number of retrofitting iterations",
        "        alpha: Blend factor (0=all semantic, 1=all original)",
        "        ",
        "    Returns:",
        "        Dictionary with retrofitting statistics",
        "    \"\"\"",
        "    layer0 = layers[CorticalLayer.TOKENS]",
        "    ",
        "    # Store original weights",
        "    original_weights: Dict[str, Dict[str, float]] = {}",
        "    for col in layer0.minicolumns.values():",
        "        original_weights[col.content] = dict(col.lateral_connections)",
        "    ",
        "    # Build semantic neighbor lookup",
        "    semantic_neighbors: Dict[str, List[Tuple[str, float]]] = defaultdict(list)",
        "    ",
        "    for t1, relation, t2, weight in semantic_relations:",
        "        relation_weight = RELATION_WEIGHTS.get(relation, 0.5)",
        "        combined_weight = weight * relation_weight",
        "        ",
        "        # Bidirectional",
        "        semantic_neighbors[t1].append((t2, combined_weight))",
        "        semantic_neighbors[t2].append((t1, combined_weight))",
        "    ",
        "    # Iterative retrofitting",
        "    tokens_affected = set()",
        "    total_adjustment = 0.0",
        "    ",
        "    for iteration in range(iterations):",
        "        iteration_adjustment = 0.0",
        "        ",
        "        for col in layer0.minicolumns.values():",
        "            term = col.content",
        "            ",
        "            if term not in semantic_neighbors:",
        "                continue",
        "            ",
        "            tokens_affected.add(term)",
        "            ",
        "            # Get semantic target weights",
        "            semantic_targets: Dict[str, float] = {}",
        "            for neighbor, weight in semantic_neighbors[term]:",
        "                neighbor_col = layer0.get_minicolumn(neighbor)",
        "                if neighbor_col:",
        "                    semantic_targets[neighbor_col.id] = weight",
        "            ",
        "            if not semantic_targets:",
        "                continue",
        "            ",
        "            # Adjust each connection",
        "            for target_id in list(col.lateral_connections.keys()):",
        "                original = original_weights[term].get(target_id, 0)",
        "                semantic = semantic_targets.get(target_id, 0)",
        "                ",
        "                # Blend original and semantic",
        "                new_weight = alpha * original + (1 - alpha) * semantic",
        "                ",
        "                if new_weight > 0:",
        "                    adjustment = abs(col.lateral_connections[target_id] - new_weight)",
        "                    iteration_adjustment += adjustment",
        "                    col.lateral_connections[target_id] = new_weight",
        "            ",
        "            # Add new semantic connections",
        "            for target_id, semantic_weight in semantic_targets.items():",
        "                if target_id not in col.lateral_connections:",
        "                    col.lateral_connections[target_id] = (1 - alpha) * semantic_weight",
        "                    iteration_adjustment += (1 - alpha) * semantic_weight",
        "        ",
        "        total_adjustment += iteration_adjustment",
        "    ",
        "    return {",
        "        'iterations': iterations,",
        "        'alpha': alpha,",
        "        'tokens_affected': len(tokens_affected),",
        "        'total_adjustment': total_adjustment,",
        "        'relations_used': len(semantic_relations)",
        "    }",
        "",
        "",
        "def retrofit_embeddings(",
        "    embeddings: Dict[str, List[float]],",
        "    semantic_relations: List[Tuple[str, str, str, float]],",
        "    iterations: int = 10,",
        "    alpha: float = 0.4",
        ") -> Dict[str, any]:",
        "    \"\"\"",
        "    Retrofit embeddings using semantic relations.",
        "    ",
        "    Like Faruqui et al.'s retrofitting, but for graph embeddings.",
        "    Pulls semantically related terms closer in embedding space.",
        "    ",
        "    Args:",
        "        embeddings: Dictionary mapping terms to embedding vectors",
        "        semantic_relations: List of (term1, relation, term2, weight) tuples",
        "        iterations: Number of iterations",
        "        alpha: Blend factor (higher = more original embedding)",
        "        ",
        "    Returns:",
        "        Dictionary with retrofitting statistics",
        "    \"\"\"",
        "    import copy",
        "    ",
        "    # Store original embeddings",
        "    original = copy.deepcopy(embeddings)",
        "    ",
        "    # Build neighbor lookup",
        "    neighbors: Dict[str, List[Tuple[str, float]]] = defaultdict(list)",
        "    ",
        "    for t1, relation, t2, weight in semantic_relations:",
        "        if t1 in embeddings and t2 in embeddings:",
        "            relation_weight = RELATION_WEIGHTS.get(relation, 0.5)",
        "            combined = weight * relation_weight",
        "            neighbors[t1].append((t2, combined))",
        "            neighbors[t2].append((t1, combined))",
        "    ",
        "    # Iterative retrofitting",
        "    total_movement = 0.0",
        "    terms_moved = set()",
        "    ",
        "    for iteration in range(iterations):",
        "        for term in list(embeddings.keys()):",
        "            if term not in neighbors or not neighbors[term]:",
        "                continue",
        "            ",
        "            terms_moved.add(term)",
        "            vec = embeddings[term]",
        "            orig = original[term]",
        "            ",
        "            # Compute semantic center (weighted average of neighbors)",
        "            semantic_center = [0.0] * len(vec)",
        "            total_weight = 0.0",
        "            ",
        "            for neighbor, weight in neighbors[term]:",
        "                if neighbor in embeddings:",
        "                    neighbor_vec = embeddings[neighbor]",
        "                    for i in range(len(vec)):",
        "                        semantic_center[i] += neighbor_vec[i] * weight",
        "                    total_weight += weight",
        "            ",
        "            if total_weight > 0:",
        "                for i in range(len(semantic_center)):",
        "                    semantic_center[i] /= total_weight",
        "                ",
        "                # Blend original with semantic center",
        "                new_vec = []",
        "                movement = 0.0",
        "                ",
        "                for i in range(len(vec)):",
        "                    new_val = alpha * orig[i] + (1 - alpha) * semantic_center[i]",
        "                    movement += abs(new_val - vec[i])",
        "                    new_vec.append(new_val)",
        "                ",
        "                embeddings[term] = new_vec",
        "                total_movement += movement",
        "    ",
        "    return {",
        "        'iterations': iterations,",
        "        'alpha': alpha,",
        "        'terms_retrofitted': len(terms_moved),",
        "        'total_movement': total_movement",
        "    }",
        "",
        "",
        "def get_relation_type_weight(relation_type: str) -> float:",
        "    \"\"\"",
        "    Get the weight for a relation type.",
        "    ",
        "    Args:",
        "        relation_type: Type of semantic relation",
        "        ",
        "    Returns:",
        "        Weight multiplier for this relation type",
        "    \"\"\"",
        "    return RELATION_WEIGHTS.get(relation_type, 0.5)"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "cortical_package/cortical/tokenizer.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "\"\"\"",
        "Tokenizer Module",
        "================",
        "",
        "Text tokenization with stemming and word variant support.",
        "",
        "Like early visual processing, the tokenizer extracts basic features",
        "(words) from raw input, filtering noise (stop words) and normalizing",
        "representations (lowercase, stemming).",
        "\"\"\"",
        "",
        "import re",
        "from typing import List, Set, Optional, Dict",
        "",
        "",
        "class Tokenizer:",
        "    \"\"\"",
        "    Text tokenizer with stemming and word variant support.",
        "    ",
        "    Extracts tokens from text, filters stop words, and provides",
        "    word variant expansion for query normalization.",
        "    ",
        "    Attributes:",
        "        stop_words: Set of words to filter out",
        "        min_word_length: Minimum word length to keep",
        "        ",
        "    Example:",
        "        tokenizer = Tokenizer()",
        "        tokens = tokenizer.tokenize(\"Neural networks process information\")",
        "        # ['neural', 'networks', 'process', 'information']",
        "        ",
        "        variants = tokenizer.get_word_variants(\"bread\")",
        "        # ['bread', 'sourdough', 'dough', 'flour', 'baking', 'breads']",
        "    \"\"\"",
        "    ",
        "    DEFAULT_STOP_WORDS = frozenset({",
        "        # Articles and conjunctions",
        "        'the', 'a', 'an', 'and', 'or', 'but', 'nor', 'yet', 'so',",
        "        # Prepositions",
        "        'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'from', 'as',",
        "        'into', 'through', 'during', 'before', 'after', 'above', 'below',",
        "        'between', 'under', 'over', 'again', 'against', 'about', 'within',",
        "        'without', 'toward', 'towards', 'upon', 'across', 'along', 'around',",
        "        'behind', 'beside', 'beyond', 'down', 'inside', 'outside', 'throughout',",
        "        # Verbs (auxiliary and common)",
        "        'is', 'was', 'are', 'were', 'been', 'be', 'being',",
        "        'have', 'has', 'had', 'having',",
        "        'do', 'does', 'did', 'doing', 'done',",
        "        'will', 'would', 'could', 'should', 'may', 'might', 'must', 'shall', 'can',",
        "        'need', 'needs', 'needed',",
        "        'get', 'gets', 'got', 'getting',",
        "        'make', 'makes', 'made', 'making',",
        "        'take', 'takes', 'took', 'taking', 'taken',",
        "        'come', 'comes', 'came', 'coming',",
        "        'give', 'gives', 'gave', 'giving', 'given',",
        "        'use', 'uses', 'used', 'using',",
        "        # Pronouns",
        "        'that', 'this', 'these', 'those', 'it', 'its', 'itself',",
        "        'they', 'them', 'their', 'theirs', 'themselves',",
        "        'he', 'she', 'him', 'her', 'his', 'hers', 'himself', 'herself',",
        "        'we', 'us', 'our', 'ours', 'ourselves',",
        "        'you', 'your', 'yours', 'yourself', 'yourselves',",
        "        'i', 'me', 'my', 'mine', 'myself',",
        "        'who', 'whom', 'whose', 'what', 'which', 'when', 'where', 'why', 'how',",
        "        # Adverbs and modifiers",
        "        'not', 'no', 'yes', 'so', 'if', 'then', 'than', 'too', 'very', 'just',",
        "        'also', 'only', 'even', 'still', 'already', 'always', 'never', 'ever',",
        "        'often', 'sometimes', 'usually', 'now', 'here', 'there', 'well', 'much',",
        "        'more', 'most', 'less', 'least', 'rather', 'quite', 'almost', 'nearly',",
        "        'really', 'actually', 'especially', 'particularly', 'generally',",
        "        # Common transitional words",
        "        'while', 'although', 'though', 'however', 'therefore', 'thus', 'hence',",
        "        'moreover', 'furthermore', 'nevertheless', 'nonetheless', 'meanwhile',",
        "        'otherwise', 'instead', 'besides', 'whereas', 'whether', 'unless',",
        "        # Common verbs",
        "        'include', 'includes', 'including', 'included',",
        "        'provide', 'provides', 'provided', 'providing',",
        "        'require', 'requires', 'required', 'requiring',",
        "        'enable', 'enables', 'enabled', 'enabling',",
        "        'allow', 'allows', 'allowed', 'allowing',",
        "        'create', 'creates', 'created', 'creating',",
        "        'become', 'becomes', 'became', 'becoming',",
        "        'remain', 'remains', 'remained', 'remaining',",
        "        'offer', 'offers', 'offered', 'offering',",
        "        'support', 'supports', 'supported', 'supporting',",
        "        # Quantifiers and determiners",
        "        'each', 'every', 'any', 'some', 'all', 'both', 'few', 'many', 'several',",
        "        'such', 'other', 'another', 'same', 'different', 'own', 'certain',",
        "        'one', 'two', 'three', 'first', 'second', 'third', 'last', 'next',",
        "        # Common nouns (too generic)",
        "        'way', 'ways', 'thing', 'things', 'time', 'times', 'year', 'years',",
        "        'day', 'days', 'place', 'part', 'parts', 'case', 'cases', 'point',",
        "        'fact', 'kind', 'type', 'form', 'forms', 'level', 'area', 'areas',",
        "        # Common adjectives (too generic)",
        "        'new', 'old', 'good', 'bad', 'great', 'small', 'large', 'big', 'long',",
        "        'high', 'low', 'right', 'left', 'possible', 'important', 'major',",
        "        'available', 'able', 'like', 'different', 'similar'",
        "    })",
        "    ",
        "    def __init__(self, stop_words: Optional[Set[str]] = None, min_word_length: int = 3):",
        "        \"\"\"",
        "        Initialize tokenizer.",
        "        ",
        "        Args:",
        "            stop_words: Set of words to filter out. Uses defaults if None.",
        "            min_word_length: Minimum word length to keep.",
        "        \"\"\"",
        "        self.stop_words = stop_words if stop_words is not None else self.DEFAULT_STOP_WORDS",
        "        self.min_word_length = min_word_length",
        "        ",
        "        # Simple suffix rules for stemming (Porter-lite)",
        "        self._suffix_rules = [",
        "            ('ational', 'ate'), ('tional', 'tion'), ('enci', 'ence'),",
        "            ('anci', 'ance'), ('izer', 'ize'), ('isation', 'ize'),",
        "            ('ization', 'ize'), ('ation', 'ate'), ('ator', 'ate'),",
        "            ('alism', 'al'), ('iveness', 'ive'), ('fulness', 'ful'),",
        "            ('ousness', 'ous'), ('aliti', 'al'), ('iviti', 'ive'),",
        "            ('biliti', 'ble'), ('ement', ''), ('ment', ''), ('ness', ''),",
        "            ('ling', ''), ('ing', ''), ('ies', 'y'), ('ied', 'y'),",
        "            ('es', ''), ('ed', ''), ('ly', ''), ('er', ''), ('est', ''),",
        "            ('ful', ''), ('less', ''), ('able', ''), ('ible', ''),",
        "            ('ness', ''), ('ment', ''), ('ity', ''),",
        "        ]",
        "        ",
        "        # Common word mappings for query normalization",
        "        self._word_mappings: Dict[str, List[str]] = {",
        "            # Bread/baking related",
        "            'bread': ['sourdough', 'dough', 'flour', 'baking', 'loaf'],",
        "            'baking': ['sourdough', 'bread', 'dough', 'flour'],",
        "            # Neural/brain related",
        "            'brain': ['neural', 'cortical', 'neurons', 'cognitive'],",
        "            'ai': ['neural', 'learning', 'artificial', 'intelligence'],",
        "            'ml': ['learning', 'machine', 'neural', 'training'],",
        "            # Database/storage",
        "            'database': ['storage', 'data', 'query', 'index'],",
        "            'db': ['database', 'storage', 'data'],",
        "            # Common abbreviations",
        "            'nlp': ['natural', 'language', 'processing', 'text'],",
        "            'api': ['interface', 'endpoint', 'service'],",
        "            # Synonyms",
        "            'fast': ['quick', 'rapid', 'speed'],",
        "            'slow': ['latency', 'delay'],",
        "            'big': ['large', 'scale', 'massive'],",
        "            'small': ['tiny', 'minimal', 'compact'],",
        "        }",
        "    ",
        "    def tokenize(self, text: str) -> List[str]:",
        "        \"\"\"",
        "        Extract tokens from text.",
        "        ",
        "        Args:",
        "            text: Input text to tokenize.",
        "            ",
        "        Returns:",
        "            List of filtered, lowercase tokens.",
        "        \"\"\"",
        "        # Convert to lowercase and extract words (including alphanumeric like word2vec)",
        "        words = re.findall(r'\\b[a-z][a-z0-9]*\\b', text.lower())",
        "        ",
        "        # Filter stop words and short words",
        "        return [",
        "            w for w in words ",
        "            if w not in self.stop_words and len(w) >= self.min_word_length",
        "        ]",
        "    ",
        "    def extract_ngrams(self, tokens: List[str], n: int = 2) -> List[str]:",
        "        \"\"\"",
        "        Extract n-grams from token list.",
        "        ",
        "        Args:",
        "            tokens: List of tokens.",
        "            n: Size of n-grams to extract.",
        "            ",
        "        Returns:",
        "            List of n-gram strings (tokens joined by space).",
        "        \"\"\"",
        "        if len(tokens) < n:",
        "            return []",
        "        return [' '.join(tokens[i:i+n]) for i in range(len(tokens) - n + 1)]",
        "    ",
        "    def stem(self, word: str) -> str:",
        "        \"\"\"",
        "        Apply simple suffix stripping (Porter-lite stemming).",
        "        ",
        "        Args:",
        "            word: Word to stem",
        "            ",
        "        Returns:",
        "            Stemmed word",
        "        \"\"\"",
        "        if len(word) <= 4:",
        "            return word",
        "        ",
        "        for suffix, replacement in self._suffix_rules:",
        "            if word.endswith(suffix):",
        "                stemmed = word[:-len(suffix)] + replacement",
        "                if len(stemmed) >= 3:",
        "                    return stemmed",
        "        ",
        "        return word",
        "    ",
        "    def get_word_variants(self, word: str) -> List[str]:",
        "        \"\"\"",
        "        Get related words/variants for query expansion.",
        "        ",
        "        Args:",
        "            word: Input word",
        "            ",
        "        Returns:",
        "            List of related words including the original",
        "        \"\"\"",
        "        word_lower = word.lower()",
        "        variants = [word_lower]",
        "        ",
        "        # Add mapped variants",
        "        if word_lower in self._word_mappings:",
        "            variants.extend(self._word_mappings[word_lower])",
        "        ",
        "        # Add stemmed version",
        "        stemmed = self.stem(word_lower)",
        "        if stemmed != word_lower:",
        "            variants.append(stemmed)",
        "        ",
        "        # Add common variations",
        "        if word_lower.endswith('s') and len(word_lower) > 3:",
        "            variants.append(word_lower[:-1])  # Remove plural",
        "        elif not word_lower.endswith('s'):",
        "            variants.append(word_lower + 's')  # Add plural",
        "        ",
        "        return list(set(variants))",
        "    ",
        "    def add_word_mapping(self, word: str, variants: List[str]) -> None:",
        "        \"\"\"",
        "        Add a custom word mapping for query expansion.",
        "        ",
        "        Args:",
        "            word: The source word",
        "            variants: List of variant words to map to",
        "        \"\"\"",
        "        word_lower = word.lower()",
        "        if word_lower in self._word_mappings:",
        "            self._word_mappings[word_lower].extend(variants)",
        "            self._word_mappings[word_lower] = list(set(self._word_mappings[word_lower]))",
        "        else:",
        "            self._word_mappings[word_lower] = variants"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "cortical_package/demo.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "\"\"\"",
        "Cortical Text Processor Demo",
        "============================",
        "",
        "This demo processes a corpus of documents, analyzing relationships",
        "between concepts, documents, and ideas across diverse topics.",
        "\"\"\"",
        "",
        "import os",
        "import sys",
        "from typing import Dict, List, Tuple",
        "",
        "from cortical import CorticalTextProcessor, CorticalLayer",
        "",
        "",
        "def print_header(title: str, char: str = \"=\"):",
        "    \"\"\"Print a formatted section header.\"\"\"",
        "    width = 70",
        "    print(f\"\\n{char * width}\")",
        "    print(f\"{title:^{width}}\")",
        "    print(f\"{char * width}\\n\")",
        "",
        "",
        "def print_subheader(title: str):",
        "    \"\"\"Print a formatted subsection header.\"\"\"",
        "    print(f\"\\n{title}\")",
        "    print(\"-\" * len(title))",
        "",
        "",
        "def render_bar(value: float, max_value: float, width: int = 30) -> str:",
        "    \"\"\"Render a text-based progress bar.\"\"\"",
        "    if max_value == 0:",
        "        return \" \" * width",
        "    filled = int((value / max_value) * width)",
        "    return \"█\" * filled + \"░\" * (width - filled)",
        "",
        "",
        "class CorticalDemo:",
        "    \"\"\"Demonstrates the cortical text processor with interesting analysis.\"\"\"",
        "    ",
        "    def __init__(self, samples_dir: str = \"samples\"):",
        "        self.samples_dir = samples_dir",
        "        self.processor = CorticalTextProcessor()",
        "        self.loaded_files = []",
        "    ",
        "    def run(self):",
        "        \"\"\"Run the complete demo.\"\"\"",
        "        self.print_intro()",
        "        ",
        "        if not self.ingest_corpus():",
        "            print(\"No documents found!\")",
        "            return",
        "        ",
        "        self.analyze_hierarchy()",
        "        self.discover_key_concepts()",
        "        self.analyze_tfidf()",
        "        self.find_concept_associations()",
        "        self.analyze_document_relationships()",
        "        self.demonstrate_queries()",
        "        self.demonstrate_gap_analysis()",
        "        self.demonstrate_embeddings()",
        "        self.print_insights()",
        "    ",
        "    def print_intro(self):",
        "        \"\"\"Print introduction.\"\"\"",
        "        print(\"\"\"",
        "    ╔══════════════════════════════════════════════════════════════════════╗",
        "    ║                                                                      ║",
        "    ║              🧠  CORTICAL TEXT PROCESSOR DEMO  🧠                    ║",
        "    ║                                                                      ║",
        "    ║     Mimicking how the neocortex processes and understands text       ║",
        "    ║                                                                      ║",
        "    ╚══════════════════════════════════════════════════════════════════════╝",
        "        \"\"\")",
        "    ",
        "    def ingest_corpus(self) -> bool:",
        "        \"\"\"Ingest the document corpus from disk.\"\"\"",
        "        print_header(\"DOCUMENT INGESTION\", \"═\")",
        "        ",
        "        print(f\"Loading documents from: {self.samples_dir}\")",
        "        print(\"Processing through cortical hierarchy...\")",
        "        print(\"(Like visual information flowing V1 → V2 → V4 → IT)\\n\")",
        "        ",
        "        if not os.path.exists(self.samples_dir):",
        "            print(f\"  ❌ Directory not found: {self.samples_dir}\")",
        "            return False",
        "        ",
        "        txt_files = sorted([f for f in os.listdir(self.samples_dir) if f.endswith('.txt')])",
        "        ",
        "        if not txt_files:",
        "            return False",
        "        ",
        "        for filename in txt_files:",
        "            filepath = os.path.join(self.samples_dir, filename)",
        "            with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:",
        "                content = f.read()",
        "            ",
        "            doc_id = filename.replace('.txt', '')",
        "            self.processor.process_document(doc_id, content)",
        "            word_count = len(content.split())",
        "            self.loaded_files.append((doc_id, word_count))",
        "            print(f\"  📄 {doc_id:30} ({word_count:3} words)\")",
        "        ",
        "        # Run all computations",
        "        print(\"\\nComputing cortical representations...\")",
        "        self.processor.compute_all(verbose=False)",
        "        ",
        "        layer0 = self.processor.get_layer(CorticalLayer.TOKENS)",
        "        layer1 = self.processor.get_layer(CorticalLayer.BIGRAMS)",
        "        ",
        "        print(f\"\\n✓ Processed {len(self.loaded_files)} documents\")",
        "        print(f\"✓ Created {layer0.column_count()} token minicolumns\")",
        "        print(f\"✓ Created {layer1.column_count()} bigram minicolumns\")",
        "        print(f\"✓ Formed {layer0.total_connections()} lateral connections\")",
        "        ",
        "        return True",
        "    ",
        "    def analyze_hierarchy(self):",
        "        \"\"\"Show the hierarchical structure.\"\"\"",
        "        print_header(\"HIERARCHICAL STRUCTURE\", \"═\")",
        "        ",
        "        print(\"The cortical model has 4 layers (like visual cortex V1→IT):\\n\")",
        "        ",
        "        layers = [",
        "            (CorticalLayer.TOKENS, \"Token Layer (V1)\", \"Individual words\"),",
        "            (CorticalLayer.BIGRAMS, \"Bigram Layer (V2)\", \"Word pairs\"),",
        "            (CorticalLayer.CONCEPTS, \"Concept Layer (V4)\", \"Semantic clusters\"),",
        "            (CorticalLayer.DOCUMENTS, \"Document Layer (IT)\", \"Full documents\"),",
        "        ]",
        "        ",
        "        for layer_enum, name, desc in layers:",
        "            layer = self.processor.get_layer(layer_enum)",
        "            count = layer.column_count()",
        "            conns = layer.total_connections()",
        "            print(f\"  Layer {layer_enum.value}: {name}\")",
        "            print(f\"         {count:,} minicolumns, {conns:,} connections\")",
        "            print(f\"         Purpose: {desc}\\n\")",
        "    ",
        "    def discover_key_concepts(self):",
        "        \"\"\"Show most important concepts via PageRank.\"\"\"",
        "        print_header(\"KEY CONCEPTS (PageRank)\", \"═\")",
        "        ",
        "        print(\"PageRank identifies central concepts - highly connected 'hub' words:\")",
        "        print(\"(Like identifying influential neurons in a network)\\n\")",
        "        ",
        "        layer0 = self.processor.get_layer(CorticalLayer.TOKENS)",
        "        ",
        "        # Get top tokens by pagerank",
        "        top_tokens = sorted(layer0.minicolumns.values(), ",
        "                           key=lambda c: c.pagerank, reverse=True)[:15]",
        "        ",
        "        if top_tokens:",
        "            max_pr = top_tokens[0].pagerank",
        "            print(\"  Rank  Concept            PageRank\")",
        "            print(\"  \" + \"─\" * 45)",
        "            ",
        "            for i, col in enumerate(top_tokens, 1):",
        "                bar = render_bar(col.pagerank, max_pr, 20)",
        "                print(f\"  {i:>3}.  {col.content:<18} {bar} {col.pagerank:.4f}\")",
        "    ",
        "    def analyze_tfidf(self):",
        "        \"\"\"Show TF-IDF analysis.\"\"\"",
        "        print_header(\"TF-IDF ANALYSIS\", \"═\")",
        "        ",
        "        print(\"TF-IDF identifies distinctive terms - rare but meaningful:\")",
        "        print(\"(High TF-IDF = important in some docs but rare across corpus)\\n\")",
        "        ",
        "        layer0 = self.processor.get_layer(CorticalLayer.TOKENS)",
        "        ",
        "        top_tfidf = sorted(layer0.minicolumns.values(),",
        "                          key=lambda c: c.tfidf, reverse=True)[:15]",
        "        ",
        "        if top_tfidf:",
        "            max_tfidf = top_tfidf[0].tfidf",
        "            print(\"  Rank  Term               TF-IDF   Documents\")",
        "            print(\"  \" + \"─\" * 50)",
        "            ",
        "            for i, col in enumerate(top_tfidf, 1):",
        "                bar = render_bar(col.tfidf, max_tfidf, 15)",
        "                doc_count = len(col.document_ids)",
        "                print(f\"  {i:>3}.  {col.content:<18} {bar} {col.tfidf:.4f}  ({doc_count} docs)\")",
        "    ",
        "    def find_concept_associations(self):",
        "        \"\"\"Show lateral connections between concepts.\"\"\"",
        "        print_header(\"CONCEPT ASSOCIATIONS\", \"═\")",
        "        ",
        "        print(\"Lateral connections form from co-occurrence (like Hebbian learning):\")",
        "        print(\"'Neurons that fire together, wire together'\\n\")",
        "        ",
        "        layer0 = self.processor.get_layer(CorticalLayer.TOKENS)",
        "        ",
        "        # Find interesting concepts and their connections",
        "        test_concepts = [\"neural\", \"learning\", \"bread\", \"systems\"]",
        "        ",
        "        for concept in test_concepts:",
        "            col = layer0.get_minicolumn(concept)",
        "            if col and col.lateral_connections:",
        "                print_subheader(f\"🔗 '{concept}' connects to:\")",
        "                ",
        "                # Get top connections",
        "                sorted_conns = sorted(col.lateral_connections.items(), ",
        "                                     key=lambda x: x[1], reverse=True)[:6]",
        "                ",
        "                for neighbor_id, weight in sorted_conns:",
        "                    # Find neighbor content",
        "                    for c in layer0.minicolumns.values():",
        "                        if c.id == neighbor_id:",
        "                            bar_len = int(min(weight, 10) * 3)",
        "                            bar = \"─\" * bar_len + \">\"",
        "                            print(f\"    {bar} {c.content} (weight: {weight:.2f})\")",
        "                            break",
        "                print()",
        "    ",
        "    def analyze_document_relationships(self):",
        "        \"\"\"Show document-level relationships.\"\"\"",
        "        print_header(\"DOCUMENT RELATIONSHIPS\", \"═\")",
        "        ",
        "        print(\"Documents connect based on shared concepts and term overlap:\\n\")",
        "        ",
        "        layer3 = self.processor.get_layer(CorticalLayer.DOCUMENTS)",
        "        ",
        "        # Find most connected documents",
        "        sorted_docs = sorted(layer3.minicolumns.values(),",
        "                            key=lambda c: c.connection_count(), reverse=True)[:5]",
        "        ",
        "        print(\"  Most connected documents (bridge topics):\")",
        "        print(\"  \" + \"─\" * 50)",
        "        ",
        "        for col in sorted_docs:",
        "            conns = col.connection_count()",
        "            print(f\"  📄 {col.content:<30} ({conns} connections)\")",
        "        ",
        "        # Show a document's relationships",
        "        if sorted_docs:",
        "            doc = sorted_docs[0]",
        "            print(f\"\\n  '{doc.content}' connects to:\")",
        "            ",
        "            related = self.processor.find_related_documents(doc.content)[:5]",
        "            for related_doc, weight in related:",
        "                print(f\"    → {related_doc} (similarity: {weight:.3f})\")",
        "    ",
        "    def demonstrate_queries(self):",
        "        \"\"\"Demonstrate query capability with expansion.\"\"\"",
        "        print_header(\"QUERY DEMONSTRATION\", \"═\")",
        "        ",
        "        print(\"Query expansion adds semantically related terms for better recall:\\n\")",
        "        ",
        "        test_queries = [\"neural networks\", \"fermentation\", \"distributed systems\"]",
        "        ",
        "        for query in test_queries:",
        "            print_subheader(f\"🔍 Query: '{query}'\")",
        "            ",
        "            # Show expansion",
        "            expanded = self.processor.expand_query(query, max_expansions=6)",
        "            original = set(self.processor.tokenizer.tokenize(query))",
        "            new_terms = [t for t in expanded.keys() if t not in original]",
        "            ",
        "            if new_terms:",
        "                print(f\"    Expanded with: {', '.join(new_terms[:6])}\")",
        "            ",
        "            # Find documents",
        "            results = self.processor.find_documents_for_query(query, top_n=3)",
        "            print(f\"\\n    Top documents:\")",
        "            for doc_id, score in results:",
        "                print(f\"      • {doc_id} (score: {score:.3f})\")",
        "            print()",
        "    ",
        "    def demonstrate_gap_analysis(self):",
        "        \"\"\"Show knowledge gap detection.\"\"\"",
        "        print_header(\"KNOWLEDGE GAP ANALYSIS\", \"═\")",
        "        ",
        "        print(\"Detecting gaps and anomalies in the corpus:\\n\")",
        "        ",
        "        gaps = self.processor.analyze_knowledge_gaps()",
        "        ",
        "        print(f\"  Coverage Score: {gaps['coverage_score']:.1%}\")",
        "        print(f\"  Connectivity Score: {gaps['connectivity_score']:.4f}\")",
        "        ",
        "        summary = gaps['summary']",
        "        print(f\"\\n  Total documents: {summary['total_documents']}\")",
        "        print(f\"  Isolated documents: {summary['isolated_count']}\")",
        "        print(f\"  Well-connected: {summary['well_connected_count']}\")",
        "        print(f\"  Weak topics found: {summary['weak_topic_count']}\")",
        "        ",
        "        if gaps['isolated_documents']:",
        "            print(\"\\n  📍 Isolated documents (don't fit well):\")",
        "            for doc in gaps['isolated_documents'][:3]:",
        "                print(f\"    • {doc['doc_id']} (avg sim: {doc['avg_similarity']:.3f})\")",
        "        ",
        "        if gaps['weak_topics']:",
        "            print(\"\\n  📍 Weak topics (thin coverage):\")",
        "            for topic in gaps['weak_topics'][:5]:",
        "                print(f\"    • '{topic['term']}' - only {topic['doc_count']} doc(s)\")",
        "    ",
        "    def demonstrate_embeddings(self):",
        "        \"\"\"Show embedding-based similarity.\"\"\"",
        "        print_header(\"GRAPH EMBEDDINGS\", \"═\")",
        "        ",
        "        print(\"Computing embeddings from graph structure...\\n\")",
        "        ",
        "        stats = self.processor.compute_graph_embeddings(",
        "            dimensions=32, method='adjacency', verbose=False",
        "        )",
        "        print(f\"  Created {stats['terms_embedded']} term embeddings\")",
        "        ",
        "        # Find similar terms",
        "        test_terms = [\"neural\", \"learning\", \"data\"]",
        "        ",
        "        for term in test_terms:",
        "            similar = self.processor.find_similar_by_embedding(term, top_n=5)",
        "            if similar:",
        "                print(f\"\\n  Terms similar to '{term}':\")",
        "                for other, sim in similar:",
        "                    print(f\"    • {other} (similarity: {sim:.3f})\")",
        "    ",
        "    def print_insights(self):",
        "        \"\"\"Print final insights and summary.\"\"\"",
        "        print_header(\"INSIGHTS & SUMMARY\", \"═\")",
        "        ",
        "        layer0 = self.processor.get_layer(CorticalLayer.TOKENS)",
        "        layer1 = self.processor.get_layer(CorticalLayer.BIGRAMS)",
        "        layer3 = self.processor.get_layer(CorticalLayer.DOCUMENTS)",
        "        ",
        "        print(\"📊 CORPUS ANALYSIS SUMMARY\\n\")",
        "        ",
        "        print(f\"  Documents processed:     {len(self.loaded_files)}\")",
        "        print(f\"  Unique tokens:           {layer0.column_count()}\")",
        "        print(f\"  Unique bigrams:          {layer1.column_count()}\")",
        "        print(f\"  Total connections:       {layer0.total_connections():,}\")",
        "        ",
        "        # Find most central token",
        "        top_token = max(layer0.minicolumns.values(), key=lambda c: c.pagerank)",
        "        print(f\"\\n  Most central concept: '{top_token.content}'\")",
        "        ",
        "        # Find most connected document",
        "        if layer3.column_count() > 0:",
        "            top_doc = max(layer3.minicolumns.values(), key=lambda c: c.connection_count())",
        "            print(f\"  Most connected document: '{top_doc.content}'\")",
        "        ",
        "        print(\"\\n\" + \"═\" * 70)",
        "        print(\"Demo complete! The cortical text processor successfully:\")",
        "        print(\"  ✓ Built hierarchical representations (Layers 0-3)\")",
        "        print(\"  ✓ Discovered key concepts via PageRank\")",
        "        print(\"  ✓ Computed TF-IDF for discriminative analysis\")",
        "        print(\"  ✓ Found associations through lateral connections\")",
        "        print(\"  ✓ Identified document relationships\")",
        "        print(\"  ✓ Detected knowledge gaps and anomalies\")",
        "        print(\"  ✓ Computed graph embeddings\")",
        "        print(\"  ✓ Enabled semantic queries with expansion\")",
        "        print(\"═\" * 70 + \"\\n\")",
        "",
        "",
        "if __name__ == \"__main__\":",
        "    demo = CorticalDemo(samples_dir=\"samples\")",
        "    demo.run()"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "cortical_package/evaluation/__init__.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "\"\"\"Evaluation framework for cortical text processor.\"\"\"",
        "from .evaluator import CorticalEvaluator"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "cortical_package/evaluation/evaluator.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "\"\"\"",
        "Evaluation Framework",
        "====================",
        "",
        "Comprehensive evaluation of cortical text processing capabilities.",
        "\"\"\"",
        "",
        "import os",
        "import re",
        "from typing import Dict, List, Tuple, Any, Optional",
        "import sys",
        "sys.path.insert(0, '..')",
        "",
        "from cortical import CorticalTextProcessor",
        "",
        "",
        "class CorticalEvaluator:",
        "    \"\"\"Evaluates cortical text processor against test cases.\"\"\"",
        "    ",
        "    def __init__(self, processor: CorticalTextProcessor = None, samples_dir: str = None):",
        "        self.processor = processor or CorticalTextProcessor()",
        "        self.samples_dir = samples_dir",
        "        self.results: List[Dict] = []",
        "        ",
        "    def load_samples(self, samples_dir: str = None) -> int:",
        "        \"\"\"Load sample documents from directory.\"\"\"",
        "        samples_dir = samples_dir or self.samples_dir",
        "        if not samples_dir or not os.path.exists(samples_dir):",
        "            return 0",
        "            ",
        "        count = 0",
        "        for filename in os.listdir(samples_dir):",
        "            if filename.endswith('.txt'):",
        "                filepath = os.path.join(samples_dir, filename)",
        "                with open(filepath, 'r') as f:",
        "                    content = f.read()",
        "                doc_id = filename.replace('.txt', '')",
        "                self.processor.process_document(doc_id, content)",
        "                count += 1",
        "        ",
        "        return count",
        "    ",
        "    def prepare(self, verbose: bool = True):",
        "        \"\"\"Run all computations to prepare for evaluation.\"\"\"",
        "        self.processor.compute_all(verbose=verbose)",
        "        self.processor.extract_corpus_semantics(verbose=verbose)",
        "        self.processor.retrofit_connections(verbose=verbose)",
        "        self.processor.compute_graph_embeddings(verbose=verbose)",
        "        self.processor.retrofit_embeddings(verbose=verbose)",
        "    ",
        "    def evaluate_factual_retrieval(self) -> Dict:",
        "        \"\"\"Test factual retrieval capability.\"\"\"",
        "        tests = [",
        "            (\"neural networks\", [\"graph_neural_networks\", \"brain_inspired_computing\"]),",
        "            (\"machine learning\", [\"deep_learning_revolution\", \"knowledge_enhanced_nlp\"]),",
        "            (\"deep learning\", [\"deep_learning_revolution\", \"graph_neural_networks\"]),",
        "        ]",
        "        ",
        "        passed = 0",
        "        total = len(tests)",
        "        ",
        "        for query, expected in tests:",
        "            results = self.processor.find_documents_for_query(query, top_n=5)",
        "            found_docs = [doc_id for doc_id, _ in results]",
        "            if any(exp in found_docs for exp in expected):",
        "                passed += 1",
        "        ",
        "        return {",
        "            'category': 'Factual Retrieval',",
        "            'passed': passed,",
        "            'total': total,",
        "            'score': passed / total if total > 0 else 0",
        "        }",
        "    ",
        "    def evaluate_query_expansion(self) -> Dict:",
        "        \"\"\"Test query expansion capability.\"\"\"",
        "        tests = [",
        "            (\"neural\", 3),  # Should expand to at least 3 terms",
        "            (\"learning\", 3),",
        "            (\"bread\", 2),",
        "        ]",
        "        ",
        "        passed = 0",
        "        total = len(tests)",
        "        ",
        "        for query, min_expansions in tests:",
        "            expanded = self.processor.expand_query(query, max_expansions=10)",
        "            if len(expanded) >= min_expansions:",
        "                passed += 1",
        "        ",
        "        return {",
        "            'category': 'Query Expansion',",
        "            'passed': passed,",
        "            'total': total,",
        "            'score': passed / total if total > 0 else 0",
        "        }",
        "    ",
        "    def evaluate_gap_detection(self) -> Dict:",
        "        \"\"\"Test gap detection capability.\"\"\"",
        "        gaps = self.processor.analyze_knowledge_gaps()",
        "        ",
        "        tests = [",
        "            ('coverage_score' in gaps, \"Has coverage score\"),",
        "            ('isolated_documents' in gaps, \"Has isolated documents\"),",
        "            ('weak_topics' in gaps, \"Has weak topics\"),",
        "        ]",
        "        ",
        "        passed = sum(1 for result, _ in tests if result)",
        "        total = len(tests)",
        "        ",
        "        return {",
        "            'category': 'Gap Detection',",
        "            'passed': passed,",
        "            'total': total,",
        "            'score': passed / total if total > 0 else 0",
        "        }",
        "    ",
        "    def run_all(self, verbose: bool = True) -> Dict:",
        "        \"\"\"Run all evaluations and return summary.\"\"\"",
        "        evaluations = [",
        "            self.evaluate_factual_retrieval,",
        "            self.evaluate_query_expansion,",
        "            self.evaluate_gap_detection,",
        "        ]",
        "        ",
        "        results = []",
        "        total_passed = 0",
        "        total_tests = 0",
        "        ",
        "        for eval_func in evaluations:",
        "            result = eval_func()",
        "            results.append(result)",
        "            total_passed += result['passed']",
        "            total_tests += result['total']",
        "            ",
        "            if verbose:",
        "                print(f\"  {result['category']}: {result['passed']}/{result['total']} ({result['score']:.1%})\")",
        "        ",
        "        overall_score = total_passed / total_tests if total_tests > 0 else 0",
        "        ",
        "        return {",
        "            'results': results,",
        "            'total_passed': total_passed,",
        "            'total_tests': total_tests,",
        "            'overall_score': overall_score",
        "        }"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "cortical_package/pyproject.toml",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "[build-system]",
        "requires = [\"setuptools>=61.0\", \"wheel\"]",
        "build-backend = \"setuptools.build_meta\"",
        "",
        "[project]",
        "name = \"cortical-text-processor\"",
        "version = \"2.0.0\"",
        "description = \"Neocortex-inspired text processing for semantic analysis and document retrieval\"",
        "readme = \"README.md\"",
        "license = {text = \"MIT\"}",
        "requires-python = \">=3.8\"",
        "authors = [",
        "    {name = \"Claude\", email = \"claude@anthropic.com\"}",
        "]",
        "keywords = [",
        "    \"nlp\",",
        "    \"text-processing\",",
        "    \"semantic-analysis\",",
        "    \"document-retrieval\",",
        "    \"knowledge-graph\",",
        "    \"neural-inspired\"",
        "]",
        "classifiers = [",
        "    \"Development Status :: 4 - Beta\",",
        "    \"Intended Audience :: Developers\",",
        "    \"Intended Audience :: Science/Research\",",
        "    \"License :: OSI Approved :: MIT License\",",
        "    \"Programming Language :: Python :: 3\",",
        "    \"Programming Language :: Python :: 3.8\",",
        "    \"Programming Language :: Python :: 3.9\",",
        "    \"Programming Language :: Python :: 3.10\",",
        "    \"Programming Language :: Python :: 3.11\",",
        "    \"Programming Language :: Python :: 3.12\",",
        "    \"Topic :: Scientific/Engineering :: Artificial Intelligence\",",
        "    \"Topic :: Text Processing :: Linguistic\",",
        "]",
        "",
        "[project.urls]",
        "Homepage = \"https://github.com/anthropic/cortical-text-processor\"",
        "Documentation = \"https://github.com/anthropic/cortical-text-processor#readme\"",
        "Repository = \"https://github.com/anthropic/cortical-text-processor\"",
        "",
        "[tool.setuptools.packages.find]",
        "where = [\".\"]",
        "include = [\"cortical*\"]",
        "",
        "[tool.pytest.ini_options]",
        "testpaths = [\"tests\"]",
        "python_files = [\"test_*.py\"]",
        "python_classes = [\"Test*\"]",
        "python_functions = [\"test_*\"]",
        "",
        "[tool.black]",
        "line-length = 100",
        "target-version = ['py38', 'py39', 'py310', 'py311']",
        "",
        "[tool.isort]",
        "profile = \"black\"",
        "line_length = 100"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "cortical_package/samples/accordion_repair.txt",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "Accordion repair demands understanding of bellows mechanics, reed voicing,",
        "and keyboard action. The bellows assembly connects treble and bass sections,",
        "with pleated cardboard, cloth tape, and leather corners requiring periodic",
        "resealing. Air leakage compromises responsiveness and tone, necessitating",
        "systematic leak detection with the instrument pressurized.",
        "",
        "Reed blocks house individual tongue assemblies mounted on aluminum plates.",
        "Each reed tongue vibrates at specific frequency determined by length, width,",
        "and thickness. Voicing adjustments tune response characteristics by filing",
        "metal or adjusting tip clearance above the vent slot. Wax seals reed plates",
        "to wooden blocks, softening with age and permitting air bypass.",
        "",
        "Keyboard mechanisms employ pallets, springs, and pushrod linkages connecting",
        "keys to tone chamber valves. Sticky keys indicate worn felt bushings, broken",
        "springs, or warped wooden components. Bass machine assemblies multiply",
        "mechanical complexity with chord button matrices triggering multiple reeds",
        "simultaneously. Register switches redirect airflow through different reed",
        "ranks, enabling timbral variation from mellow musette to bright piccolo voicing."
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "cortical_package/samples/algorithmic_trading.txt",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "Algorithmic Trading Systems",
        "",
        "Algorithmic trading executes orders using automated pre-programmed instructions accounting for variables like timing, price, and volume. High-frequency trading systems process market data through low-latency pipelines, making microsecond decisions on order placement. Mean reversion strategies identify price deviations from historical averages, betting on eventual return to equilibrium.",
        "",
        "Order matching engines form the exchange backbone, maintaining order books with bid-ask spreads. FIFO queues prioritize earlier orders at each price level. Market makers provide liquidity by continuously posting both buy and sell orders, profiting from the spread while accepting inventory risk.",
        "",
        "Risk management modules calculate position exposure, value-at-risk metrics, and drawdown limits in real-time. Circuit breakers halt trading when volatility exceeds thresholds. Backtesting frameworks replay historical tick data to validate strategies before deployment.",
        "",
        "Technical indicators like moving averages, Bollinger bands, and RSI generate trading signals. Pattern recognition algorithms identify chart formations—head-and-shoulders, double bottoms, support-resistance levels. Machine learning models increasingly replace hand-crafted rules, training on features extracted from price, volume, and order flow data.",
        "",
        "Execution algorithms minimize market impact through strategies like TWAP (time-weighted average price) and VWAP (volume-weighted average price). Smart order routing fragments large orders across multiple venues seeking best execution. Slippage monitoring tracks deviation between expected and realized prices."
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "cortical_package/samples/astronomical_spectroscopy.txt",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "Astronomical spectroscopy reveals stellar composition, temperature, and",
        "velocity by analyzing electromagnetic radiation absorption and emission patterns.",
        "Stellar atmospheres imprint characteristic absorption lines as photons",
        "traverse cooler outer layers. Fraunhofer lines in solar spectrum identify",
        "hydrogen, calcium, sodium, and iron among dozens of detectable elements.",
        "",
        "Spectral classification categorizes stars by surface temperature from hottest",
        "O-type blue giants through B, A, F, G, K to coolest M-type red dwarfs.",
        "Luminosity classes distinguish main sequence dwarfs, subgiants, giants, and",
        "supergiants sharing similar temperatures but vastly different radii. Hertzsprung-",
        "Russell diagrams plot these relationships, revealing stellar evolution pathways.",
        "",
        "Doppler shifts measure radial velocities: blueshifted lines indicate approach,",
        "redshifted lines recession. Exoplanet detection exploits stellar wobble",
        "inducing periodic velocity variations. Echelle spectrographs achieve",
        "resolutions exceeding R=100,000, resolving rotational broadening and",
        "distinguishing isotope ratios. Integral field units capture spatially resolved",
        "spectra across extended objects simultaneously, mapping galaxy kinematics",
        "and nebular ionization structures."
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "cortical_package/samples/blockchain_consensus.txt",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "Blockchain Consensus Mechanisms",
        "",
        "Blockchain networks achieve distributed consensus without central authority through cryptographic protocols. Proof-of-work requires miners to solve computational puzzles, expending energy to earn block rewards. Hash rate measures network security—higher difficulty prevents fifty-one percent attacks.",
        "",
        "Proof-of-stake validators lock collateral tokens, selected proportionally to stake for block proposal rights. Slashing penalties punish malicious behavior like double-signing or extended downtime. Delegated variants allow token holders to vote for validator nodes.",
        "",
        "Byzantine fault tolerance algorithms handle arbitrary node failures. PBFT requires two-thirds honest nodes for safety guarantees. Tendermint combines BFT consensus with proof-of-stake economics. Finality occurs within seconds rather than probabilistically over multiple confirmations.",
        "",
        "Merkle trees efficiently prove transaction inclusion without downloading entire blocks. Light clients verify headers and request specific proofs. State synchronization protocols bootstrap new nodes from trusted checkpoints.",
        "",
        "Smart contracts execute deterministically across all nodes. Gas metering prevents infinite loops and denial-of-service. Virtual machines isolate contract execution from underlying infrastructure. Ethereum's EVM established bytecode standards; newer chains explore WebAssembly alternatives.",
        "",
        "Layer-two scaling solutions process transactions off-chain. Payment channels batch multiple transfers into single settlement transactions. Rollups post compressed transaction data on-chain while executing off-chain, inheriting base layer security."
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "cortical_package/samples/bonsai_cultivation.txt",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "Bonsai cultivation miniaturizes trees through container restriction, pruning,",
        "and wiring techniques developed over centuries in Japan and China. Species",
        "selection matches climate conditions—junipers and pines tolerate outdoor",
        "temperate winters while tropical ficuses require indoor protection. Deciduous",
        "maples display seasonal foliage changes prized for autumn color.",
        "",
        "Styling follows classical forms: formal upright chokkan, informal upright",
        "moyogi, slanting shakan, cascade kengai, and windswept fukinagashi among",
        "dozens of recognized shapes. Nebari root flare establishes visual stability",
        "at soil surface. Trunk taper and movement convey age while branch ramification",
        "creates fine twiggy structure mimicking mature trees.",
        "",
        "Wiring wraps anodized aluminum or copper around branches, bending them",
        "into position over months before removal prevents scarring. Pinching new",
        "candles on pines controls internode length. Defoliation on healthy deciduous",
        "specimens triggers smaller secondary leaves. Repotting rejuvenates root",
        "systems every two to five years depending on species vigor. Akadama clay",
        "provides optimal drainage and aeration, often mixed with pumice and lava rock."
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "cortical_package/samples/brain_inspired_computing.txt",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "Brain-inspired computing seeks to build artificial systems that capture the",
        "computational principles of biological neural networks. The human brain",
        "processes information using billions of neurons connected by trillions of",
        "synapses, achieving remarkable efficiency through sparse distributed",
        "representations. Unlike conventional computers that separate memory and",
        "processing, the brain integrates computation and storage in the same",
        "physical substrate. Neuromorphic chips attempt to replicate this architecture",
        "using analog circuits that mimic synaptic plasticity. These systems excel at",
        "pattern recognition tasks while consuming far less power than traditional",
        "processors. The challenge remains bridging the gap between biological",
        "complexity and engineering feasibility."
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "cortical_package/samples/cheese_affinage.txt",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "Cheese affinage transforms fresh curds into complex aged varieties through",
        "controlled temperature, humidity, and microbial action in caves or cellars.",
        "Affineurs develop rinds, monitor moisture loss, and nurture characteristic",
        "flavors over weeks to years. Young cheeses exhibit mild, milky notes while",
        "extended aging concentrates flavors and develops crystalline tyrosine deposits.",
        "",
        "Washed-rind varieties like Epoisses develop pungent orange surfaces through",
        "repeated brine and brandy applications encouraging Brevibacterium linens growth.",
        "Bloomy rinds on Brie and Camembert result from Penicillium camemberti spores",
        "creating white fuzzy coats that break down interior paste to creamy ripeness.",
        "Blue cheeses host Penicillium roqueforti threading veins through pierced wheels.",
        "",
        "Alpine-style production influences flavor through copper kettles, raw milk",
        "microflora, and seasonal pasture variations. Thermophilic cultures tolerate",
        "cooking temperatures reaching 55 degrees Celsius. Cheddar cheddaring stacks",
        "and turns curd slabs, expelling whey and developing characteristic texture.",
        "Propionic bacteria generate carbon dioxide creating Swiss-style eyes. Proper",
        "affinage requires constant attention, turning wheels and adjusting conditions",
        "as rinds develop and interiors transform."
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "cortical_package/samples/commonsense_reasoning.txt",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "Common sense reasoning remains one of the greatest challenges in artificial intelligence. While machines excel at narrow tasks like chess or image classification, they struggle with the broad implicit knowledge humans use effortlessly. Understanding that dropped objects fall, that people eat when hungry, or that rain makes streets wet requires knowledge rarely stated explicitly.",
        "",
        "The frame problem illustrates why common sense is difficult for machines. When an action occurs, most things in the world remain unchanged, but specifying what does not change requires enormous explicit representation. Humans handle this implicitly through expectations and default reasoning that machines must learn to approximate.",
        "",
        "Early AI systems like CYC attempted to encode common sense through manual knowledge engineering. Researchers spent decades entering millions of logical assertions about everyday concepts. While comprehensive, this approach struggled to capture the contextual flexibility of human reasoning. Knowing that birds fly does not mean penguins fly.",
        "",
        "Statistical approaches learn common sense patterns from large text corpora. Language models trained on billions of words absorb implicit knowledge about how concepts relate. However, they can confidently produce nonsensical statements because correlation in text does not guarantee real-world validity. A model might learn that elephants are associated with pink from idioms rather than reality.",
        "",
        "Hybrid systems combine structured knowledge bases with neural networks. ConceptNet provides explicit relations that constrain and guide statistical learning. The knowledge graph offers a scaffold while neural components handle flexibility and generalization. This combination shows promise for more robust reasoning.",
        "",
        "Benchmarks like CommonsenseQA and SWAG evaluate common sense capabilities. These tests present scenarios requiring implicit world knowledge to answer correctly. Progress on such benchmarks indicates whether systems truly understand concepts or merely exploit surface patterns in language."
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "cortical_package/samples/conceptnet_overview.txt",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "ConceptNet is a freely available semantic network designed to help computers understand the meanings of words that people use. Unlike traditional dictionaries that define words in terms of other words, ConceptNet represents knowledge as a graph of interconnected concepts and relations.",
        "",
        "The project originated from the Open Mind Common Sense project at MIT Media Lab, which crowdsourced common sense knowledge from volunteers on the internet. This knowledge was structured into assertions like \"a dog is an animal\" or \"coffee is used for staying awake.\" Over time, ConceptNet incorporated data from other sources including WordNet, Wiktionary, and OpenCyc.",
        "",
        "ConceptNet uses a set of core relations to connect concepts. The IsA relation indicates category membership, such as \"cat IsA mammal.\" The UsedFor relation describes purpose, like \"hammer UsedFor driving nails.\" HasA indicates possession or parts, RelatedTo captures general associations, and AtLocation describes where things are typically found. Additional relations include Causes, CapableOf, PartOf, and Desires.",
        "",
        "Each edge in ConceptNet carries a weight representing confidence in that assertion. Higher weights indicate stronger evidence from multiple sources or higher agreement among contributors. This allows applications to prioritize more reliable knowledge when reasoning about concepts.",
        "",
        "The knowledge in ConceptNet spans multiple languages through a shared concept space. English concepts link to equivalent concepts in French, German, Chinese, and dozens of other languages. This multilingual structure enables cross-lingual reasoning and translation assistance.",
        "",
        "ConceptNet has found applications in natural language processing, sentiment analysis, question answering, and commonsense reasoning systems. Its structured knowledge helps fill gaps in machine learning models that struggle with implicit knowledge humans take for granted."
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "cortical_package/samples/container_orchestration.txt",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "Container Orchestration and Kubernetes",
        "",
        "Container orchestration automates deployment, scaling, and management of containerized applications. Docker packages applications with dependencies into portable images. Layers cache intermediate build steps, reducing image size through multi-stage builds.",
        "",
        "Kubernetes clusters comprise control plane nodes and worker nodes. The API server processes all requests; etcd stores cluster state. Schedulers place pods on nodes considering resource requests, affinity rules, and taints. Kubelet agents on each node manage container lifecycle.",
        "",
        "Pods group containers sharing network namespace and storage volumes. Deployments manage replica sets, enabling rolling updates and rollbacks. StatefulSets handle persistent workloads requiring stable network identities and ordered deployment.",
        "",
        "Services abstract pod networking through stable DNS names and load balancing. Ingress controllers route external traffic based on hostname and path rules. Network policies implement microsegmentation restricting pod-to-pod communication.",
        "",
        "Horizontal pod autoscaling adjusts replica counts based on CPU utilization or custom metrics. Cluster autoscaler provisions additional nodes when pods remain pending. Vertical pod autoscaler recommends resource requests based on historical usage.",
        "",
        "Helm charts package Kubernetes manifests with templating and dependency management. Operators encode operational knowledge for complex applications, automating backup, scaling, and recovery procedures. GitOps workflows synchronize cluster state from version-controlled repositories."
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "cortical_package/samples/cortical_columns.txt",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "The neocortex is organized into cortical columns, vertical structures that",
        "span all six layers of cortical tissue. Each column contains approximately",
        "ten thousand neurons and functions as a basic computational unit. Within",
        "columns, minicolumns of about one hundred neurons fire together in response",
        "to specific features. This columnar organization appears throughout sensory",
        "and motor cortex, suggesting a universal algorithm for cortical computation.",
        "Lateral connections between columns enable contextual processing and pattern",
        "completion. When you see part of an object, these connections help activate",
        "the full representation based on learned associations. Understanding cortical",
        "columns could unlock principles for building more intelligent machines."
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "cortical_package/samples/cryptocurrency_mining.txt",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "Cryptocurrency Mining Operations",
        "",
        "Cryptocurrency mining validates transactions and secures blockchain networks through computational work. ASIC miners dominate Bitcoin with SHA-256 optimization; GPUs remain viable for memory-hard algorithms like Ethash and Equihash.",
        "",
        "Mining profitability depends on hash rate, power consumption, electricity costs, and coin prices. Difficulty adjustments maintain target block intervals despite fluctuating network hash power. Pool mining aggregates hash rate, distributing rewards proportionally through PPLNS or PPS schemes.",
        "",
        "Hardware optimization maximizes efficiency measured in hashes per watt. Firmware modifications unlock higher clock speeds and lower voltages. Thermal management through adequate airflow, heatsinks, and immersion cooling extends equipment lifespan.",
        "",
        "Power infrastructure sizing accounts for peak draw plus safety margins. Dedicated circuits prevent breaker trips; PDUs distribute load evenly. Electricity arbitrage exploits rate differentials—time-of-use plans, industrial rates, or renewable generation.",
        "",
        "Mining software connects workers to pools via stratum protocol. Monitoring dashboards track hash rates, temperatures, and rejection rates. Watchdog processes restart crashed miners; auto-switching algorithms chase most profitable coins.",
        "",
        "Solar integration offsets electricity costs during peak generation hours. Battery storage enables mining during grid outages or peak pricing periods. Net metering arrangements may credit excess generation against consumption.",
        "",
        "Revenue optimization considers transaction fees during network congestion, merge-mining compatible chains, and staking rewards for proof-of-stake transitions."
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "cortical_package/samples/deep_learning_revolution.txt",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "Deep learning has revolutionized artificial intelligence by enabling machines",
        "to learn hierarchical representations directly from raw data. Convolutional",
        "neural networks excel at image recognition by detecting edges in early layers",
        "and combining them into increasingly complex features in deeper layers. This",
        "hierarchical processing mirrors the visual cortex, where simple cells detect",
        "edges and complex cells respond to more abstract patterns. The breakthrough",
        "came when researchers realized that deep networks with many layers could",
        "automatically discover the feature hierarchies that scientists had previously",
        "designed by hand. Backpropagation allows these networks to learn by adjusting",
        "millions of parameters to minimize prediction errors. Today, deep learning",
        "powers everything from voice assistants to autonomous vehicles."
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "cortical_package/samples/distributed_systems.txt",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "Distributed Systems Architecture Patterns",
        "",
        "Distributed systems coordinate multiple machines to achieve scalability, fault tolerance, and geographic distribution. CAP theorem constraints force tradeoffs between consistency, availability, and partition tolerance. Most systems choose eventual consistency, converging to agreement after network healing.",
        "",
        "Consensus protocols like Raft elect leaders managing replicated state machines. Log replication ensures followers apply entries in identical order. Heartbeat timeouts trigger leader election when failures occur. Paxos variants handle Byzantine faults but incur complexity overhead.",
        "",
        "Service mesh architectures inject sidecar proxies handling cross-cutting concerns. Envoy proxies manage service discovery, load balancing, retries, and circuit breaking. Mutual TLS authenticates service-to-service communication without application changes.",
        "",
        "Message queues decouple producers from consumers enabling asynchronous processing. Kafka partitions provide ordered, durable streams with consumer group semantics. Dead letter queues capture failed messages for investigation and reprocessing.",
        "",
        "Database sharding horizontally partitions data across nodes. Consistent hashing minimizes redistribution when cluster membership changes. Cross-shard transactions require coordination protocols adding latency.",
        "",
        "Observability pillars—logs, metrics, traces—illuminate distributed system behavior. Structured logging enables efficient querying. Prometheus scrapes metrics endpoints; Grafana visualizes dashboards. Distributed tracing correlates requests spanning multiple services through propagated context.",
        "",
        "Chaos engineering deliberately injects failures verifying system resilience. Network partitions, latency injection, and node termination reveal weaknesses before production incidents expose them."
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "cortical_package/samples/dotnet_enterprise.txt",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "Enterprise .NET Development Patterns",
        "",
        "Modern .NET applications leverage dependency injection, asynchronous programming, and clean architecture principles. ASP.NET Core middleware pipelines process requests through composable components. Minimal APIs reduce ceremony for microservice endpoints.",
        "",
        "Domain-driven design structures complex business logic around bounded contexts and aggregates. Entity Framework Core maps domain objects to relational databases with migrations tracking schema evolution. Repository patterns abstract data access behind interfaces enabling testing and flexibility.",
        "",
        "Message-driven architectures decouple components through event publishing. MassTransit and NServiceBus provide saga orchestration, retry policies, and dead letter handling. Outbox patterns ensure exactly-once delivery coordinating database transactions with message publication.",
        "",
        "Multi-tenant systems isolate customer data through database-per-tenant, schema-per-tenant, or discriminator column approaches. Tenant resolution middleware identifies context from subdomain, header, or claim. Resource quotas prevent noisy neighbor problems.",
        "",
        "Health checks expose endpoint status for load balancer integration. Readiness probes gate traffic until dependencies initialize. Liveness probes trigger restarts when applications become unresponsive.",
        "",
        "Structured logging with Serilog captures contextual properties. Correlation identifiers trace requests across service boundaries. Application Insights and OpenTelemetry provide distributed tracing and metric aggregation.",
        "",
        "Configuration providers load settings from JSON files, environment variables, and secret managers. Options patterns bind configuration sections to strongly-typed classes with validation. Feature flags enable progressive rollouts and experimentation.",
        "",
        "Testing strategies span unit tests with mocking frameworks, integration tests against test containers, and contract tests verifying API compatibility."
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "cortical_package/samples/graph_neural_networks.txt",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "Graph neural networks extend deep learning to non-Euclidean data structures.",
        "While convolutional networks assume grid-like topology, graph networks",
        "operate on arbitrary connection patterns. Message passing allows nodes to",
        "aggregate information from neighbors, building representations that capture",
        "graph structure. This paradigm applies to social networks, molecular graphs,",
        "and knowledge bases. PageRank can be viewed as an early form of graph-based",
        "importance propagation, where node centrality emerges from network topology.",
        "Modern graph networks learn to propagate information adaptively, discovering",
        "which connections matter for each task. The brain itself is a graph of",
        "neurons, making these architectures particularly relevant for neuroscience",
        "applications."
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "cortical_package/samples/homelab_infrastructure.txt",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "Home Lab Server Infrastructure",
        "",
        "Home lab environments provide hands-on experience with enterprise hardware and software. Dell PowerEdge servers like R710s offer dual-socket Xeon processors, hot-swap drive bays, and iDRAC remote management. VRTX chassis house M630 blade servers with shared storage and networking in compact form factors.",
        "",
        "Server procurement from secondary markets yields significant cost savings. Evaluate processor generation, memory capacity, and storage controller compatibility. DDR3 registered ECC memory remains economical; DDR4 systems command premiums but offer improved density and efficiency.",
        "",
        "Storage architectures range from direct-attached RAID arrays to networked solutions. ZFS on TrueNAS provides software RAID with snapshots, replication, and data integrity verification. SSD caching accelerates spinning disk pools for mixed workloads.",
        "",
        "Virtualization hypervisors—Proxmox, ESXi, Hyper-V—consolidate multiple workloads onto physical hosts. Resource pools allocate CPU and memory quotas. High availability clustering enables automatic VM failover between nodes.",
        "",
        "Network segmentation through VLANs isolates management, storage, and application traffic. PfSense or OPNsense firewalls provide routing, VPN termination, and intrusion detection. Ten-gigabit interconnects support storage traffic demands.",
        "",
        "Power consumption monitoring reveals efficiency opportunities. IPMI sensors report real-time wattage; smart PDUs enable per-outlet metering. Undervolting experiments reduce thermal output without sacrificing stability. UPS systems protect against outages and power quality issues."
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "cortical_package/samples/kintsugi_pottery.txt",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "Kintsugi transforms ceramic breakage into artistic statement by joining",
        "fragments with urushi lacquer mixed with powdered gold, silver, or platinum.",
        "This Japanese philosophy celebrates imperfection and transience, honoring",
        "object history rather than concealing damage. Wabi-sabi aesthetics embrace",
        "irregularity, asymmetry, and the beauty of wear accumulated through use.",
        "",
        "Traditional kintsugi requires patience as urushi cures slowly in humid",
        "conditions over weeks. Artisans apply multiple thin layers, sanding between",
        "coats to build durable joints stronger than original ceramic. Maki-e technique",
        "sprinkles metal powder onto tacky lacquer before final sealing. Modern",
        "alternatives substitute epoxy and metallic pigments for faster results",
        "lacking urushi's characteristic depth.",
        "",
        "Three approaches distinguish kintsugi styles: crack method fills fracture",
        "lines only, piece method replaces missing fragments with pure lacquer, and",
        "joint-call method emphasizes seams through contrasting gold thickness.",
        "Practitioners develop sensitivity to ceramic types—porcelain, stoneware,",
        "earthenware—adjusting techniques accordingly. Completed pieces command",
        "premium prices, their golden scars evidence of resilience and renewed purpose."
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "cortical_package/samples/knowledge_enhanced_nlp.txt",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "Knowledge-enhanced natural language processing combines structured knowledge bases with neural language models. Pure neural approaches learn patterns from text but lack explicit world knowledge. Knowledge graphs provide factual grounding that constrains and improves model predictions.",
        "",
        "Entity linking connects textual mentions to knowledge graph nodes. When text refers to Apple, the system must determine whether it means the fruit or the technology company. Context and entity properties from the knowledge graph disambiguate these references. Linked entities enable reasoning with structured knowledge about mentioned concepts.",
        "",
        "Knowledge graph embeddings represent entities and relations as vectors. TransE models relations as translations between entity vectors. More complex models like RotatE and ConvE capture richer relational patterns. These embeddings integrate seamlessly with neural architectures while preserving graph structure.",
        "",
        "Retrofitting adjusts pretrained word embeddings using knowledge graph relations. The process pulls synonyms closer together and pushes antonyms apart in vector space. ConceptNet Numberbatch applies this technique using diverse semantic relations. The resulting embeddings outperform purely distributional vectors on semantic benchmarks.",
        "",
        "Graph neural networks process knowledge graphs through message passing. Each node aggregates information from neighbors to update its representation. Multiple layers propagate information across longer paths. This enables reasoning over graph structure within differentiable neural frameworks.",
        "",
        "Knowledge distillation transfers structured knowledge into language model parameters. Training objectives encourage models to predict knowledge graph relations from text. The resulting models implicitly encode factual knowledge accessible through natural language queries. This bridges explicit knowledge bases with implicit neural representations.",
        "",
        "Neuro-symbolic systems maintain explicit symbolic reasoning alongside neural components. The neural subsystem handles perception and pattern matching while symbolic reasoning ensures logical consistency. This hybrid architecture combines the flexibility of learning with the reliability of structured knowledge."
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "cortical_package/samples/knowledge_graphs.txt",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "Knowledge graphs represent information as networks of entities connected by typed relationships. Unlike relational databases that store data in rigid tables, knowledge graphs embrace the flexible, interconnected nature of real-world knowledge. Each node represents an entity or concept, while edges encode specific relationships between them.",
        "",
        "The structure of knowledge graphs mirrors how humans organize knowledge through associations. When we think of Paris, we naturally connect it to France, the Eiffel Tower, romance, and croissants. Knowledge graphs capture these multidimensional relationships explicitly, enabling machines to traverse semantic connections.",
        "",
        "Graph databases like Neo4j and ArangoDB provide native storage for knowledge graphs, optimizing for relationship traversal rather than table joins. Query languages such as SPARQL and Cypher allow complex pattern matching across the graph structure. These enable questions like \"find all scientists who studied at universities in Germany and later worked at American companies.\"",
        "",
        "Major technology companies have built massive knowledge graphs to power their products. Google's Knowledge Graph enhances search results with structured information panels. Facebook's social graph connects billions of users through relationships. Amazon's product graph links items through categories, attributes, and purchase patterns.",
        "",
        "Building knowledge graphs requires both structured data extraction and natural language processing. Named entity recognition identifies concepts in text. Relation extraction discovers connections between entities. Entity resolution merges duplicate references to the same real-world object. These techniques transform unstructured text into structured knowledge.",
        "",
        "The semantic web vision proposed by Tim Berners-Lee imagined a web of machine-readable data. Standards like RDF and OWL provide formal languages for knowledge representation. While the full semantic web vision remains unrealized, knowledge graphs have become essential infrastructure for artificial intelligence applications."
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "cortical_package/samples/knowledge_transfer.txt",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "# Cortical Text Processor: Knowledge Transfer Document",
        "",
        "## Executive Summary",
        "",
        "The Cortical Text Processor is a neocortex-inspired text analysis system that models hierarchical processing, lateral connections, and importance propagation to discover relationships between concepts and documents. It combines neuroscience principles with PageRank algorithms to extract meaningful insights from text corpora.",
        "",
        "## Architecture Overview",
        "",
        "### Biological Inspiration",
        "",
        "The system models four key properties of the mammalian neocortex:",
        "",
        "1. **Hierarchical Processing**: Information flows through increasingly abstract layers, mirroring the visual pathway from V1 (edge detection) through V4 (shape recognition) to IT cortex (object identification).",
        "",
        "2. **Minicolumns**: Basic computational units containing approximately 80-120 neurons that fire together. Each minicolumn represents a concept, token, or document.",
        "",
        "3. **Lateral Connections**: Horizontal connections between minicolumns enable contextual processing and pattern completion, modeled through co-occurrence graphs.",
        "",
        "4. **Sparse Distributed Representations**: Only small percentages of neurons activate simultaneously, enabling efficient storage and noise robustness.",
        "",
        "### System Layers",
        "",
        "| Layer | Biological Analog | Function | Content |",
        "|-------|------------------|----------|---------|",
        "| 0 (Tokens) | V1 simple cells | Extract atomic features | Individual words |",
        "| 1 (Bigrams) | V2 complex cells | Detect feature combinations | Word pairs |",
        "| 2 (Concepts) | V4 hypercolumns | Cluster semantics | Topic groups |",
        "| 3 (Documents) | IT cortex | Holistic recognition | Full documents |",
        "",
        "## Core Components",
        "",
        "### Tokenizer Class",
        "",
        "Handles text preprocessing with configurable stop word filtering:",
        "",
        "- Removes punctuation and normalizes case",
        "- Filters common English words (articles, prepositions, auxiliaries)",
        "- Extracts n-grams for higher-order patterns",
        "- Minimum word length filtering (default: 3 characters)",
        "",
        "The expanded stop word list now includes transitional words like \"through\", \"while\", \"across\" that previously created spurious document connections.",
        "",
        "### Minicolumn Class",
        "",
        "Represents individual computational units with:",
        "",
        "- **activation**: Log-scaled firing rate (log1p of occurrence count)",
        "- **pagerank**: Importance score from lateral connection structure",
        "- **lateral_connections**: Dictionary mapping neighbor IDs to weights",
        "- **feedforward_connections**: Links to higher processing layers",
        "- **feedback_connections**: Top-down modulation from higher layers",
        "",
        "### CorticalLayer Class",
        "",
        "Manages collections of minicolumns at each hierarchical level:",
        "",
        "- Creates and retrieves minicolumns by content",
        "- Computes PageRank across lateral connection graph",
        "- Provides activation statistics (min, max, mean, sparsity)",
        "- Returns top concepts ranked by importance",
        "",
        "### CorticalTextProcessor Class",
        "",
        "Orchestrates the complete processing pipeline:",
        "",
        "1. **Document Processing**: Tokenizes content, builds co-occurrence graphs",
        "2. **Connection Building**: Creates lateral links based on vocabulary overlap",
        "3. **Activation Propagation**: Spreads activation through connections",
        "4. **Importance Computation**: Runs PageRank on each layer",
        "5. **Query Processing**: Activates matching concepts and retrieves related terms",
        "",
        "## PageRank Integration",
        "",
        "### Mathematical Foundation",
        "",
        "PageRank computes the principal eigenvector of the web's link matrix:",
        "",
        "```",
        "PR(p) = (1-d)/N + d × Σ(PR(i) / L(i))",
        "```",
        "",
        "Where:",
        "- d = damping factor (0.85 default)",
        "- N = total pages/concepts",
        "- L(i) = outbound links from page i",
        "",
        "### Application to Text",
        "",
        "In this system, PageRank identifies central concepts based on co-occurrence patterns rather than hyperlinks:",
        "",
        "- Concepts appearing frequently with other important concepts rank higher",
        "- Hub concepts bridging multiple topics accumulate importance",
        "- Isolated concepts with few connections rank lower",
        "",
        "The damping factor models the probability that a \"random surfer\" following connections continues versus jumping to a random concept.",
        "",
        "## Processing Pipeline",
        "",
        "### Document Ingestion",
        "",
        "```python",
        "processor = CorticalTextProcessor()",
        "processor.process_document(doc_id, content)",
        "```",
        "",
        "For each document:",
        "1. Layer 0 receives tokenized words with occurrence counts",
        "2. Layer 1 receives bigrams with feedforward links to constituent tokens",
        "3. Layer 3 receives document node linking to all unique tokens",
        "",
        "### Connection Building",
        "",
        "```python",
        "processor.build_document_connections(min_shared_tokens=3)",
        "```",
        "",
        "Documents connect based on Jaccard similarity of tokenized content. The threshold parameter controls connection sparsity.",
        "",
        "### Analysis Execution",
        "",
        "```python",
        "processor.propagate_activation(iterations=5)",
        "processor.compute_importance()",
        "```",
        "",
        "Activation propagation models spreading neural activity:",
        "- Feedforward: Higher layers aggregate from lower (0.7 existing + 0.3 sources)",
        "- Lateral: Spreads through connections (0.7 self + 0.2 neighbors + 0.1 boost)",
        "",
        "## Query System",
        "",
        "Queries activate matching minicolumns and spread to connected concepts:",
        "",
        "```python",
        "results = processor.query(\"hierarchical processing\", top_n=10)",
        "```",
        "",
        "The system:",
        "1. Tokenizes query terms",
        "2. Activates matching Layer 0 minicolumns",
        "3. Spreads activation to lateral neighbors (0.5 weight)",
        "4. Returns top concepts sorted by combined activation",
        "",
        "## Demo Application",
        "",
        "The demo.py script provides comprehensive analysis:",
        "",
        "- **Document Ingestion**: Loads .txt files from specified directory",
        "- **Hierarchical Structure**: Displays layer statistics and sparsity",
        "- **Key Concepts**: Shows top tokens and bigrams by PageRank",
        "- **Concept Associations**: Reveals lateral connection patterns",
        "- **Document Relationships**: Identifies thematic clusters",
        "- **Topic Analysis**: Shows unique vocabulary and overlap matrices",
        "- **Query Demonstration**: Activates concepts and shows spreading",
        "",
        "## Performance Characteristics",
        "",
        "### Scalability",
        "",
        "- Token layer: O(V) minicolumns where V = vocabulary size",
        "- Bigram layer: O(V²) potential but sparse in practice",
        "- Connections: O(E) where E = co-occurrence edges",
        "- PageRank: O(iterations × E) per layer",
        "",
        "### Memory Usage",
        "",
        "For a 12-document corpus with ~150 words each:",
        "- ~1,200 token minicolumns",
        "- ~1,500 bigram minicolumns",
        "- ~18,000 lateral connections",
        "- ~2MB total memory",
        "",
        "## Future Enhancements",
        "",
        "### Layer 2 Implementation",
        "",
        "The concept layer (Layer 2) remains unimplemented. Potential approaches:",
        "",
        "1. **PageRank Communities**: Cluster tokens by connection structure",
        "2. **Embedding Similarity**: Group semantically similar tokens",
        "3. **LDA Topics**: Discover latent topic distributions",
        "",
        "### Biological Fidelity",
        "",
        "Current sparsity (~60%) falls short of biological targets (95-98%). Improvements could include:",
        "",
        "- Winner-take-all competition within layers",
        "- Adaptive threshold mechanisms",
        "- Inhibitory interneuron modeling",
        "",
        "### Integration Possibilities",
        "",
        "- **RAG Systems**: Use as semantic index for retrieval",
        "- **Knowledge Graphs**: Export connections for graph databases",
        "- **Visualization**: Interactive network exploration tools",
        "",
        "## Conclusion",
        "",
        "The Cortical Text Processor demonstrates how neuroscience-inspired architectures can extract meaningful structure from text. By combining hierarchical processing with PageRank importance propagation, the system discovers central concepts, reveals document relationships, and enables semantic queries through a biologically-plausible computational framework."
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "cortical_package/samples/letterpress_printing.txt",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "Letterpress printing transfers ink from raised metal or wood type surfaces",
        "directly onto paper under impression pressure. Compositors arrange individual",
        "sorts from typecase compartments into composing sticks, building lines of",
        "justified text character by character. Leading strips separate lines while",
        "em-quads and en-spaces create word gaps and indentation.",
        "",
        "Typefaces display distinctive anatomy: serifs terminate strokes on Garamond",
        "and Caslon while sans-serif Futura and Helvetica present clean unadorned",
        "letterforms. X-height proportions, stroke contrast, and counter openness",
        "distinguish typeface personalities. Point size measures type body height;",
        "pica measurements organize page layouts with six picas per inch.",
        "",
        "Platen presses clamp paper against inked forme in vertical motion while",
        "cylinder presses roll sheets across horizontal typebeds. Makeready adjusts",
        "impression evenness through tissue underlays compensating for uneven type",
        "heights. Polymer photoplates now complement traditional foundry type, enabling",
        "photographic reproduction at letterpress quality. Printers ink rollers,",
        "carefully mixing pantone colors and adjusting tack for optimal transfer onto",
        "cotton rag papers prized for dimensional impression."
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "cortical_package/samples/medieval_falconry.txt",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "Medieval falconry distinguished nobility through mastery of raptors trained",
        "for hunting quarry. Austringers worked with goshawks and sparrowhawks while",
        "falconers flew peregrines, sakers, and gyrfalcons at herons and waterfowl.",
        "The mews housed birds between hunts, with perches, baths, and weathering yards",
        "for conditioning.",
        "",
        "Manning wild-caught birds required patience as the austringer gradually",
        "habituated raptors to human presence. Bating, the panicked flapping when",
        "startled, decreased as trust developed. Jesses attached to leather anklets",
        "tethered birds to the gloved fist, while bells and bewits aided tracking.",
        "The creance line allowed controlled free-flight practice before releasing",
        "birds unrestrained.",
        "",
        "Falconers assessed birds' condition by feeling the keel bone prominence,",
        "adjusting rations to maintain flying weight without weakness. Casting pellets",
        "of undigested fur and bone indicated healthy digestion. Imping replaced",
        "damaged flight feathers by splicing donor feathers with needles. Hooding",
        "calmed excitable birds, while lures trained them to return when stooping",
        "failed to secure prey."
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "cortical_package/samples/memory_consolidation.txt",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "Memory consolidation transforms fragile short-term memories into stable",
        "long-term representations. The hippocampus initially encodes new experiences,",
        "then gradually transfers them to neocortex during sleep. Replay of neural",
        "activity patterns during slow-wave sleep strengthens synaptic connections",
        "that encode important memories. This two-stage process prevents catastrophic",
        "forgetting, where new learning destroys old knowledge. Artificial neural",
        "networks suffer from exactly this problem when trained sequentially on",
        "different tasks. Techniques like elastic weight consolidation and memory",
        "replay buffers attempt to mimic biological consolidation. Understanding",
        "how the brain protects memories while remaining plastic could revolutionize",
        "continual learning in machines."
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "cortical_package/samples/orchid_cultivation.txt",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "Orchid cultivation spans epiphytic, lithophytic, and terrestrial species",
        "with diverse care requirements. Phalaenopsis thrive in household conditions",
        "while Masdevallias demand cool temperatures and Vandas require tropical",
        "humidity. Sympodial orchids like Cattleyas produce sequential pseudobulbs",
        "storing water and nutrients, whereas monopodial Phalaenopsis grow upward",
        "from single stems.",
        "",
        "Potting media varies by genus: sphagnum moss retains moisture for Paphiopedilums,",
        "bark chunks drain rapidly for Cattleyas, and mounted cultivation suits",
        "Bulbophyllums. Root systems distinguish healthy plants by plump velamen",
        "coating that absorbs moisture and nutrients from air. Repotting coincides",
        "with new root emergence, typically following bloom.",
        "",
        "Keiki plantlets develop on Dendrobium canes or Phalaenopsis spikes,",
        "propagating genetically identical offspring. Division separates sympodial",
        "clumps maintaining minimum three-pseudobulb segments. Meristem tissue culture",
        "produces virus-free mericlones for commercial multiplication. Fertilizing",
        "follows the \"weekly, weakly\" principle with diluted balanced formulas.",
        "Blooming triggers vary by genus: temperature drops initiate Phalaenopsis",
        "spikes while Cymbidiums require extended cool nights."
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "cortical_package/samples/perfume_composition.txt",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "Perfumery blends aromatic compounds into harmonious compositions structured",
        "around volatility-based pyramid architecture. Top notes—citrus bergamot,",
        "sparkling aldehydes, green galbanum—create initial impressions evaporating",
        "within minutes. Heart notes—floral jasmine, spicy cardamom, rose absolute—emerge",
        "as primary character during hours of wear.",
        "",
        "Base notes anchor compositions with persistent materials: earthy vetiver,",
        "animalic musk, resinous benzoin, and precious oud persist for days on skin.",
        "Synthetic molecules expand palettes beyond natural limitations—Iso E Super",
        "imparts velvety woodiness, Ambroxan delivers ambery warmth, Hedione adds",
        "transparent jasmine radiance. Accords combine multiple materials into unified",
        "olfactory impressions greater than individual components.",
        "",
        "Natural extraction methods yield absolutes through solvent extraction,",
        "essential oils through steam distillation, and concretes through enfleurage.",
        "Headspace technology captures volatile profiles from living flowers.",
        "Perfumers train noses distinguishing thousands of raw materials, developing",
        "signature styles across chypre, fougere, oriental, and floral families.",
        "IFRA regulations restrict allergens and sensitizers, constantly reshaping",
        "permissible formulations while perfumers reformulate classics to maintain",
        "compliance."
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "cortical_package/samples/predictive_processing.txt",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "Predictive processing theory suggests the brain is fundamentally a prediction",
        "machine. Rather than passively receiving sensory input, the brain constantly",
        "generates predictions about what it will perceive next. When predictions",
        "match reality, little processing is needed. When they fail, prediction errors",
        "propagate up the hierarchy to update internal models. This framework explains",
        "many perceptual phenomena, from optical illusions to attention. In artificial",
        "intelligence, predictive coding has inspired architectures where each layer",
        "tries to predict the activity of the layer below. These models achieve",
        "impressive results in video prediction and anomaly detection. The brain",
        "may minimize surprise as its fundamental objective, constantly refining",
        "its world model through experience."
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "cortical_package/samples/semantic_similarity.txt",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "Semantic similarity measures how alike two concepts or texts are in meaning. Unlike lexical similarity that compares surface forms, semantic similarity captures deeper relationships. Car and automobile are lexically different but semantically identical. Dog and wolf are semantically similar despite distinct spellings.",
        "",
        "Path-based measures use knowledge graph structure to compute similarity. The shortest path between concepts in WordNet or ConceptNet indicates relatedness. The Wu-Palmer measure additionally considers the depth of the lowest common ancestor. Deeper shared ancestors suggest more specific shared properties.",
        "",
        "Information content methods measure similarity through probability distributions. Rare concepts convey more information than common ones. The Resnik measure uses the information content of the most specific common ancestor. Lin's measure normalizes this by the information content of the compared concepts. These approaches outperform pure path length on many benchmarks.",
        "",
        "Vector space models represent similarity as geometric proximity. Word embeddings place similar words close together in high-dimensional space. Cosine similarity between vectors provides a standard similarity measure. Sentence embeddings extend this to longer texts by pooling word vectors or using dedicated encoder models.",
        "",
        "Distributional hypothesis underlies many similarity measures. Words appearing in similar contexts tend to have similar meanings. Co-occurrence statistics from large corpora capture these contextual patterns. Both explicit count-based methods and neural embeddings exploit distributional signals.",
        "",
        "Evaluation of similarity measures uses human judgments as ground truth. Datasets like SimLex-999 and WordSim-353 contain word pairs rated by annotators. System predictions correlate with these ratings to measure quality. Different datasets emphasize different aspects of similarity versus relatedness.",
        "",
        "Applications of semantic similarity span many domains. Search engines expand queries with similar terms. Plagiarism detection finds semantically equivalent paraphrases. Question answering matches queries to relevant passages. Machine translation ensures meaning preservation across languages. Text classification groups documents by semantic content rather than surface keywords."
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "cortical_package/samples/solar_energy_systems.txt",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "Solar Energy Systems Design",
        "",
        "Photovoltaic systems convert sunlight directly into electricity through semiconductor junctions. Monocrystalline panels achieve highest efficiency; polycrystalline offers cost advantages. Bifacial modules capture reflected light from ground surfaces.",
        "",
        "System sizing balances energy production against consumption patterns. Insolation data from PVWatts or local measurements inform annual yield estimates. Roof orientation and shading analysis identify optimal panel placement. Tilt angles optimize for latitude and seasonal variation.",
        "",
        "String inverters centralize DC-to-AC conversion; microinverters optimize per-panel. Power optimizers provide panel-level MPPT while feeding string inverters. Hybrid inverters integrate battery charging and grid-tie functionality.",
        "",
        "Battery storage enables self-consumption beyond daylight hours. Lithium iron phosphate chemistry offers longevity and safety; lithium nickel manganese cobalt maximizes energy density. Battery management systems monitor cell voltages, temperatures, and state of charge.",
        "",
        "Grid-tie systems export excess generation; net metering credits offset consumption. Time-of-use arbitrage stores cheap energy for peak-rate discharge. Backup configurations island critical loads during outages.",
        "",
        "Monitoring platforms track production, consumption, and system health. Current transformers measure circuit loads; smart meters provide net flow data. Anomaly detection identifies underperforming panels, inverter faults, or shading changes.",
        "",
        "Permitting requirements vary by jurisdiction—structural engineering, electrical plans, interconnection agreements. Utility inspection verifies code compliance before permission to operate. Incentive programs—tax credits, SRECs, rebates—improve financial returns."
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "cortical_package/samples/sourdough_breadmaking.txt",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "Sourdough breadmaking relies on wild yeast fermentation and lactobacillus",
        "bacteria cultures maintained in a starter. The levain develops complex flavors",
        "through extended fermentation as microorganisms consume flour sugars and produce",
        "carbon dioxide, ethanol, and organic acids. Bakers refresh their starters daily,",
        "discarding portions and feeding fresh flour and water to maintain vigor.",
        "",
        "Autolyse allows gluten development before mixing by hydrating flour without",
        "salt. Stretch-and-fold techniques build dough strength during bulk fermentation,",
        "which typically lasts four to six hours at room temperature. The windowpane test",
        "confirms adequate gluten development when dough stretches thin without tearing.",
        "",
        "Proofing in bannetons shapes loaves while the final rise develops. Scoring the",
        "dough surface with a lame controls oven spring expansion. Steam injection during",
        "initial baking gelatinizes surface starches, creating glossy, crackly crust.",
        "Internal crumb structure reveals open, irregular alveoli when fermentation and",
        "shaping succeed. Tartine-style loaves achieve ear formation through proper",
        "scoring angles and aggressive oven heat."
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "cortical_package/samples/sparse_coding.txt",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "Sparse coding is a fundamental principle of neural information processing.",
        "At any moment, only a small percentage of neurons are active, yet this",
        "sparse pattern can represent vast amounts of information through combinatorics.",
        "The visual cortex uses sparse distributed representations where each neuron",
        "responds to specific features and each object activates a unique sparse",
        "pattern. This encoding scheme offers several advantages: energy efficiency,",
        "noise robustness, and the ability to store many overlapping memories without",
        "interference. Artificial neural networks are beginning to incorporate",
        "sparsity constraints, leading to more efficient and interpretable models.",
        "Sparse autoencoders, for instance, learn compressed representations by",
        "penalizing excessive activation."
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "cortical_package/samples/speedcubing.txt",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "Speedcubing competitions measure puzzle-solving speed with Stackmat timers",
        "recording times to hundredths of seconds. World Cube Association sanctions",
        "events across seventeen puzzle categories including standard 3x3, blindfolded,",
        "one-handed, and feet solving. Competitors complete five timed solves, with",
        "fastest and slowest discarded and remaining three averaged.",
        "",
        "CFOP method dominates advanced speedcubing: Cross establishes foundation,",
        "F2L inserts first-two-layer pairs intuitively, OLL orients last-layer",
        "pieces, and PLL permutes them to completion. Algorithm memorization accelerates",
        "execution—full OLL requires fifty-seven algorithms, full PLL twenty-one.",
        "Lookahead skill maintains solving flow by planning subsequent moves during",
        "current execution.",
        "",
        "Cube hardware evolved dramatically with magnetic positioning, tensioning",
        "systems, and specialized lubricants. Corner-cutting tolerance permits",
        "misaligned moves without locking. Cubers develop personalized preferences",
        "through spring tensions and magnet strength adjustments. Color recognition",
        "and finger-tricking techniques optimize execution efficiency. Elite competitors",
        "average sub-seven seconds, with current world record at 3.13 seconds",
        "demonstrating remarkable algorithmic fluency and dexterity."
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "cortical_package/samples/sumo_wrestling.txt",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "Sumo wrestlers compete in the dohyo, a clay ring topped with sand where",
        "rikishi clash in brief explosive bouts. Yokozuna hold the highest rank,",
        "performing ritual ring-entering ceremonies with prescribed movements. Ozeki,",
        "sekiwake, and komusubi comprise sanyaku ranks below the grand champions,",
        "while maegashira populate the top makuuchi division.",
        "",
        "Tachiai initiates each bout as wrestlers explode from crouch simultaneously.",
        "Kimarite catalog eighty-two official winning techniques including yorikiri",
        "force-outs, oshidashi push-outs, and uwatenage overarm throws. Mawashi",
        "loincloth grips enable powerful belt techniques like yotsu-sumo grappling.",
        "Henka sidesteps at tachiai draw crowd disapproval despite tactical legitimacy.",
        "",
        "Chankonabe stew fuels weight gain exceeding 150 kilograms for top competitors.",
        "Heya stables house wrestlers who train communally under oyakata stablemaster",
        "supervision. Morning keiko practice sessions build stamina through shiko",
        "leg-stomping exercises, teppo pillar-striking, and butsukari-geiko collision",
        "drills. Basho tournaments occur six times yearly, with fifteen consecutive",
        "bout performances determining promotion or demotion."
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "cortical_package/samples/transformer_architecture.txt",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "The transformer architecture has become the foundation of modern natural",
        "language processing. Unlike recurrent networks that process sequences",
        "step by step, transformers use self-attention mechanisms to relate all",
        "positions in a sequence simultaneously. This parallel processing enables",
        "efficient training on massive datasets. The attention mechanism computes",
        "weighted combinations of input representations, allowing the model to",
        "focus on relevant context regardless of distance. Large language models",
        "built on transformers demonstrate emergent capabilities including reasoning,",
        "translation, and code generation. Some researchers see parallels between",
        "attention mechanisms and how the brain selectively processes information,",
        "though the biological analogy remains debated."
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "cortical_package/samples/virtualization_hypervisors.txt",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "Virtualization and Hypervisor Technologies",
        "",
        "Hypervisors abstract physical hardware enabling multiple operating systems on single machines. Type-1 hypervisors like ESXi and Hyper-V run directly on hardware; Type-2 hypervisors like VirtualBox run atop host operating systems.",
        "",
        "Apple's Virtualization framework enables lightweight VMs on Apple Silicon Macs. ARM64 guest operating systems—Linux distributions, macOS—run with near-native performance. Rosetta 2 translation allows x86_64 Linux binaries within ARM guests.",
        "",
        "KVM transforms Linux kernels into hypervisors with QEMU providing device emulation. VirtIO paravirtualized drivers optimize disk and network performance. Libvirt provides unified management API across hypervisor backends.",
        "",
        "Memory management techniques maximize density. Transparent huge pages reduce TLB pressure. Memory ballooning reclaims unused guest memory. Kernel same-page merging deduplicates identical pages across VMs.",
        "",
        "CPU scheduling affects latency-sensitive workloads. Pinning vCPUs to physical cores ensures consistent performance. NUMA-aware placement keeps memory close to processing cores. CPU governors balance power consumption against responsiveness.",
        "",
        "Storage virtualization presents block devices from various backends. Thin provisioning allocates space on demand. Copy-on-write snapshots enable instant clones. Live migration relocates running VMs between hosts with minimal downtime.",
        "",
        "Nested virtualization enables hypervisors within VMs—useful for testing and development. Hardware-assisted extensions like VT-x and AMD-V provide efficient instruction interception. Performance overhead compounds with nesting depth."
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "cortical_package/samples/word_embeddings.txt",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "Word embeddings transform words into dense numerical vectors that capture semantic meaning. Unlike sparse one-hot encodings where each word gets a unique dimension, embeddings compress meaning into typically 100 to 300 dimensions. Words with similar meanings cluster together in this vector space.",
        "",
        "Word2Vec pioneered modern embedding techniques using shallow neural networks. The skip-gram model predicts context words from a target word, while continuous bag of words predicts targets from context. Training on large text corpora produces vectors where semantic relationships emerge as geometric properties. The famous example shows king minus man plus woman approximately equals queen.",
        "",
        "GloVe embeddings take a different approach by factorizing word co-occurrence matrices. The algorithm optimizes vectors so their dot products match logarithmic co-occurrence probabilities. This combines the benefits of global statistical methods with local context learning. Stanford released pretrained GloVe vectors trained on Wikipedia and web crawl data.",
        "",
        "FastText extended Word2Vec by incorporating subword information. Instead of treating words as atomic units, FastText represents words as bags of character n-grams. This enables embeddings for out-of-vocabulary words and captures morphological patterns. Languages with rich morphology particularly benefit from this approach.",
        "",
        "ConceptNet Numberbatch combines multiple embedding sources with knowledge graph information. It retrofits distributional embeddings using ConceptNet relations, improving performance on semantic similarity benchmarks. The resulting vectors inherit both statistical patterns from text and structured knowledge from the graph.",
        "",
        "Contextualized embeddings from models like BERT and ELMo generate different vectors for the same word in different contexts. The word bank has different meanings in river bank versus savings bank. These models capture polysemy that static embeddings conflate into single vectors. However, they require more computation than simple lookup of pretrained vectors."
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "cortical_package/samples/wordnet_lexical.txt",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "WordNet is a lexical database of English that groups words into sets of cognitive synonyms called synsets. Each synset represents a distinct concept and contains words that can substitute for each other in certain contexts. This organization mirrors how the mental lexicon might structure word knowledge in the human brain.",
        "",
        "Princeton University developed WordNet starting in 1985 under the direction of George Miller. The database has grown to include over 150,000 words organized into approximately 117,000 synsets. Nouns, verbs, adjectives, and adverbs each have separate hierarchies reflecting their different semantic properties.",
        "",
        "The primary relation in WordNet is hypernymy, the is-a relationship that creates taxonomic hierarchies. Dog is a hyponym of canine, which is a hyponym of carnivore, which is a hyponym of mammal. These chains extend up to highly abstract root concepts like entity and abstraction. This tree structure enables inheritance-based reasoning about word properties.",
        "",
        "Other relations capture different semantic connections. Meronymy relates parts to wholes, such as finger to hand. Antonymy links opposites like hot and cold. Entailment connects verbs where one implies another, as snoring entails sleeping. These relations create a rich web of linguistic knowledge.",
        "",
        "WordNet has profoundly influenced natural language processing research. Word sense disambiguation uses WordNet synsets as the inventory of possible meanings. Semantic similarity measures traverse WordNet paths to compare word meanings. Information extraction systems use the taxonomy to generalize patterns.",
        "",
        "Despite its influence, WordNet has limitations that motivated projects like ConceptNet. The hierarchical structure cannot represent associative knowledge like coffee being related to morning. The focus on definitional relationships excludes common sense facts about how concepts interact in the world. Modern systems often combine WordNet's lexical precision with broader commonsense knowledge bases."
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "cortical_package/tests/__init__.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "\"\"\"Test suite for cortical text processing package.\"\"\""
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "cortical_package/tests/test_layers.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "\"\"\"Tests for Minicolumn and Layer classes.\"\"\"",
        "",
        "import unittest",
        "import sys",
        "sys.path.insert(0, '..')",
        "",
        "from cortical import Minicolumn, CorticalLayer, HierarchicalLayer",
        "",
        "",
        "class TestMinicolumn(unittest.TestCase):",
        "    \"\"\"Test the Minicolumn class.\"\"\"",
        "    ",
        "    def test_creation(self):",
        "        \"\"\"Test basic minicolumn creation.\"\"\"",
        "        col = Minicolumn(\"L0_test\", \"test\", 0)",
        "        self.assertEqual(col.id, \"L0_test\")",
        "        self.assertEqual(col.content, \"test\")",
        "        self.assertEqual(col.layer, 0)",
        "        self.assertEqual(col.activation, 0.0)",
        "    ",
        "    def test_lateral_connections(self):",
        "        \"\"\"Test adding lateral connections.\"\"\"",
        "        col = Minicolumn(\"L0_test\", \"test\", 0)",
        "        col.add_lateral_connection(\"L0_other\", 0.5)",
        "        self.assertIn(\"L0_other\", col.lateral_connections)",
        "        self.assertEqual(col.lateral_connections[\"L0_other\"], 0.5)",
        "    ",
        "    def test_connection_strengthening(self):",
        "        \"\"\"Test that repeated connections strengthen.\"\"\"",
        "        col = Minicolumn(\"L0_test\", \"test\", 0)",
        "        col.add_lateral_connection(\"L0_other\", 0.5)",
        "        col.add_lateral_connection(\"L0_other\", 0.3)",
        "        self.assertEqual(col.lateral_connections[\"L0_other\"], 0.8)",
        "    ",
        "    def test_connection_count(self):",
        "        \"\"\"Test connection count.\"\"\"",
        "        col = Minicolumn(\"L0_test\", \"test\", 0)",
        "        col.add_lateral_connection(\"L0_a\", 1.0)",
        "        col.add_lateral_connection(\"L0_b\", 1.0)",
        "        self.assertEqual(col.connection_count(), 2)",
        "    ",
        "    def test_document_ids(self):",
        "        \"\"\"Test document ID tracking.\"\"\"",
        "        col = Minicolumn(\"L0_test\", \"test\", 0)",
        "        col.document_ids.add(\"doc1\")",
        "        col.document_ids.add(\"doc2\")",
        "        self.assertEqual(len(col.document_ids), 2)",
        "    ",
        "    def test_serialization(self):",
        "        \"\"\"Test to_dict and from_dict.\"\"\"",
        "        col = Minicolumn(\"L0_test\", \"test\", 0)",
        "        col.activation = 5.0",
        "        col.occurrence_count = 10",
        "        col.document_ids.add(\"doc1\")",
        "        col.add_lateral_connection(\"L0_other\", 2.0)",
        "        ",
        "        data = col.to_dict()",
        "        restored = Minicolumn.from_dict(data)",
        "        ",
        "        self.assertEqual(restored.id, col.id)",
        "        self.assertEqual(restored.content, col.content)",
        "        self.assertEqual(restored.activation, col.activation)",
        "        self.assertEqual(restored.occurrence_count, col.occurrence_count)",
        "",
        "",
        "class TestHierarchicalLayer(unittest.TestCase):",
        "    \"\"\"Test the HierarchicalLayer class.\"\"\"",
        "    ",
        "    def test_creation(self):",
        "        \"\"\"Test layer creation.\"\"\"",
        "        layer = HierarchicalLayer(CorticalLayer.TOKENS)",
        "        self.assertEqual(layer.level, CorticalLayer.TOKENS)",
        "        self.assertEqual(len(layer.minicolumns), 0)",
        "    ",
        "    def test_get_or_create(self):",
        "        \"\"\"Test get_or_create_minicolumn.\"\"\"",
        "        layer = HierarchicalLayer(CorticalLayer.TOKENS)",
        "        col = layer.get_or_create_minicolumn(\"test\")",
        "        self.assertEqual(col.content, \"test\")",
        "        ",
        "        # Should return same column",
        "        col2 = layer.get_or_create_minicolumn(\"test\")",
        "        self.assertIs(col, col2)",
        "    ",
        "    def test_get_minicolumn(self):",
        "        \"\"\"Test get_minicolumn returns None for missing.\"\"\"",
        "        layer = HierarchicalLayer(CorticalLayer.TOKENS)",
        "        self.assertIsNone(layer.get_minicolumn(\"missing\"))",
        "        ",
        "        layer.get_or_create_minicolumn(\"exists\")",
        "        self.assertIsNotNone(layer.get_minicolumn(\"exists\"))",
        "    ",
        "    def test_column_count(self):",
        "        \"\"\"Test column counting.\"\"\"",
        "        layer = HierarchicalLayer(CorticalLayer.TOKENS)",
        "        layer.get_or_create_minicolumn(\"a\")",
        "        layer.get_or_create_minicolumn(\"b\")",
        "        layer.get_or_create_minicolumn(\"c\")",
        "        self.assertEqual(layer.column_count(), 3)",
        "    ",
        "    def test_iteration(self):",
        "        \"\"\"Test iterating over layer.\"\"\"",
        "        layer = HierarchicalLayer(CorticalLayer.TOKENS)",
        "        layer.get_or_create_minicolumn(\"a\")",
        "        layer.get_or_create_minicolumn(\"b\")",
        "        ",
        "        contents = [col.content for col in layer]",
        "        self.assertEqual(set(contents), {\"a\", \"b\"})",
        "",
        "",
        "class TestCorticalLayerEnum(unittest.TestCase):",
        "    \"\"\"Test the CorticalLayer enum.\"\"\"",
        "    ",
        "    def test_values(self):",
        "        \"\"\"Test layer values.\"\"\"",
        "        self.assertEqual(CorticalLayer.TOKENS.value, 0)",
        "        self.assertEqual(CorticalLayer.BIGRAMS.value, 1)",
        "        self.assertEqual(CorticalLayer.CONCEPTS.value, 2)",
        "        self.assertEqual(CorticalLayer.DOCUMENTS.value, 3)",
        "    ",
        "    def test_description(self):",
        "        \"\"\"Test layer descriptions.\"\"\"",
        "        self.assertIn(\"Token\", CorticalLayer.TOKENS.description)",
        "        self.assertIn(\"Document\", CorticalLayer.DOCUMENTS.description)",
        "",
        "",
        "if __name__ == \"__main__\":",
        "    unittest.main(verbosity=2)"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "cortical_package/tests/test_processor.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "\"\"\"Tests for the CorticalTextProcessor class.\"\"\"",
        "",
        "import unittest",
        "import tempfile",
        "import os",
        "import sys",
        "sys.path.insert(0, '..')",
        "",
        "from cortical import CorticalTextProcessor, CorticalLayer",
        "",
        "",
        "class TestProcessorBasic(unittest.TestCase):",
        "    \"\"\"Test basic processor functionality.\"\"\"",
        "    ",
        "    def setUp(self):",
        "        self.processor = CorticalTextProcessor()",
        "    ",
        "    def test_process_document(self):",
        "        \"\"\"Test document processing.\"\"\"",
        "        stats = self.processor.process_document(\"doc1\", \"Neural networks process information.\")",
        "        self.assertGreater(stats['tokens'], 0)",
        "        self.assertIn(\"doc1\", self.processor.documents)",
        "    ",
        "    def test_multiple_documents(self):",
        "        \"\"\"Test processing multiple documents.\"\"\"",
        "        self.processor.process_document(\"doc1\", \"Neural networks learn.\")",
        "        self.processor.process_document(\"doc2\", \"Deep learning models.\")",
        "        self.assertEqual(len(self.processor.documents), 2)",
        "    ",
        "    def test_token_layer_populated(self):",
        "        \"\"\"Test that token layer is populated.\"\"\"",
        "        self.processor.process_document(\"doc1\", \"Neural networks process information.\")",
        "        layer0 = self.processor.get_layer(CorticalLayer.TOKENS)",
        "        self.assertGreater(layer0.column_count(), 0)",
        "    ",
        "    def test_lateral_connections(self):",
        "        \"\"\"Test that lateral connections are formed.\"\"\"",
        "        self.processor.process_document(\"doc1\", \"Neural networks process information.\")",
        "        layer0 = self.processor.get_layer(CorticalLayer.TOKENS)",
        "        ",
        "        neural = layer0.get_minicolumn(\"neural\")",
        "        self.assertIsNotNone(neural)",
        "        self.assertGreater(neural.connection_count(), 0)",
        "",
        "",
        "class TestProcessorComputation(unittest.TestCase):",
        "    \"\"\"Test processor computation methods.\"\"\"",
        "    ",
        "    @classmethod",
        "    def setUpClass(cls):",
        "        cls.processor = CorticalTextProcessor()",
        "        cls.processor.process_document(\"doc1\", \"\"\"",
        "            Neural networks process information through layers.",
        "            Deep learning enables pattern recognition.",
        "        \"\"\")",
        "        cls.processor.process_document(\"doc2\", \"\"\"",
        "            Machine learning models learn from data.",
        "            Training neural networks requires optimization.",
        "        \"\"\")",
        "    ",
        "    def test_propagate_activation(self):",
        "        \"\"\"Test activation propagation.\"\"\"",
        "        self.processor.propagate_activation(iterations=3, verbose=False)",
        "        layer0 = self.processor.get_layer(CorticalLayer.TOKENS)",
        "        ",
        "        # Check some columns have activation",
        "        activations = [col.activation for col in layer0]",
        "        self.assertTrue(any(a > 0 for a in activations))",
        "    ",
        "    def test_compute_importance(self):",
        "        \"\"\"Test PageRank computation.\"\"\"",
        "        self.processor.propagate_activation(iterations=3, verbose=False)",
        "        self.processor.compute_importance(verbose=False)",
        "        ",
        "        layer0 = self.processor.get_layer(CorticalLayer.TOKENS)",
        "        pageranks = [col.pagerank for col in layer0]",
        "        self.assertTrue(all(p > 0 for p in pageranks))",
        "    ",
        "    def test_compute_tfidf(self):",
        "        \"\"\"Test TF-IDF computation.\"\"\"",
        "        # Create fresh processor for this test",
        "        # Use 3 docs where 'neural' only appears in 2, so IDF > 0",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(\"doc1\", \"Neural networks process information.\")",
        "        processor.process_document(\"doc2\", \"Machine learning neural models.\")",
        "        processor.process_document(\"doc3\", \"Database systems store data efficiently.\")",
        "        processor.compute_tfidf(verbose=False)",
        "        ",
        "        layer0 = processor.get_layer(CorticalLayer.TOKENS)",
        "        neural = layer0.get_minicolumn(\"neural\")",
        "        self.assertIsNotNone(neural)",
        "        # Now IDF = log(3/2) > 0",
        "        self.assertGreater(neural.tfidf, 0)",
        "    ",
        "    def test_compute_all(self):",
        "        \"\"\"Test compute_all runs without error.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(\"test\", \"Test document content.\")",
        "        processor.compute_all(verbose=False)",
        "",
        "",
        "class TestProcessorQuery(unittest.TestCase):",
        "    \"\"\"Test processor query functionality.\"\"\"",
        "    ",
        "    @classmethod",
        "    def setUpClass(cls):",
        "        cls.processor = CorticalTextProcessor()",
        "        cls.processor.process_document(\"neural_doc\", \"\"\"",
        "            Neural networks process information through multiple layers.",
        "            Deep learning enables complex pattern recognition.",
        "            Backpropagation trains neural network weights.",
        "        \"\"\")",
        "        cls.processor.process_document(\"ml_doc\", \"\"\"",
        "            Machine learning algorithms learn from data.",
        "            Supervised learning uses labeled examples.",
        "            Model training optimizes parameters.",
        "        \"\"\")",
        "        cls.processor.compute_all(verbose=False)",
        "    ",
        "    def test_expand_query(self):",
        "        \"\"\"Test query expansion.\"\"\"",
        "        expanded = self.processor.expand_query(\"neural\", max_expansions=5)",
        "        self.assertIn(\"neural\", expanded)",
        "        self.assertGreater(len(expanded), 1)",
        "    ",
        "    def test_find_documents(self):",
        "        \"\"\"Test document finding.\"\"\"",
        "        results = self.processor.find_documents_for_query(\"neural networks\", top_n=2)",
        "        self.assertGreater(len(results), 0)",
        "        self.assertEqual(results[0][0], \"neural_doc\")",
        "    ",
        "    def test_query_expanded(self):",
        "        \"\"\"Test expanded query.\"\"\"",
        "        results = self.processor.query_expanded(\"learning\", top_n=5)",
        "        self.assertIsInstance(results, list)",
        "",
        "",
        "class TestProcessorPersistence(unittest.TestCase):",
        "    \"\"\"Test processor save/load functionality.\"\"\"",
        "    ",
        "    def test_save_and_load(self):",
        "        \"\"\"Test saving and loading processor.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(\"doc1\", \"Test document content.\")",
        "        processor.compute_all(verbose=False)",
        "        ",
        "        with tempfile.TemporaryDirectory() as tmpdir:",
        "            filepath = os.path.join(tmpdir, \"test.pkl\")",
        "            processor.save(filepath, verbose=False)",
        "            ",
        "            loaded = CorticalTextProcessor.load(filepath, verbose=False)",
        "            self.assertEqual(len(loaded.documents), 1)",
        "            self.assertIn(\"doc1\", loaded.documents)",
        "",
        "",
        "class TestProcessorGaps(unittest.TestCase):",
        "    \"\"\"Test gap detection functionality.\"\"\"",
        "    ",
        "    @classmethod",
        "    def setUpClass(cls):",
        "        cls.processor = CorticalTextProcessor()",
        "        for i in range(3):",
        "            cls.processor.process_document(f\"tech_{i}\", \"\"\"",
        "                Machine learning neural networks deep learning.",
        "                Training models data processing algorithms.",
        "            \"\"\")",
        "        cls.processor.process_document(\"outlier\", \"\"\"",
        "            Medieval falconry birds hunting prey.",
        "            Falcons hawks eagles training.",
        "        \"\"\")",
        "        cls.processor.compute_all(verbose=False)",
        "    ",
        "    def test_analyze_knowledge_gaps(self):",
        "        \"\"\"Test gap analysis returns expected structure.\"\"\"",
        "        gaps = self.processor.analyze_knowledge_gaps()",
        "        self.assertIn('isolated_documents', gaps)",
        "        self.assertIn('weak_topics', gaps)",
        "        self.assertIn('coverage_score', gaps)",
        "    ",
        "    def test_detect_anomalies(self):",
        "        \"\"\"Test anomaly detection.\"\"\"",
        "        anomalies = self.processor.detect_anomalies(threshold=0.1)",
        "        self.assertIsInstance(anomalies, list)",
        "",
        "",
        "if __name__ == \"__main__\":",
        "    unittest.main(verbosity=2)"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "cortical_package/tests/test_tokenizer.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "\"\"\"Tests for the Tokenizer class.\"\"\"",
        "",
        "import unittest",
        "import sys",
        "sys.path.insert(0, '..')",
        "",
        "from cortical import Tokenizer",
        "",
        "",
        "class TestTokenizer(unittest.TestCase):",
        "    \"\"\"Test the Tokenizer class.\"\"\"",
        "    ",
        "    def setUp(self):",
        "        self.tokenizer = Tokenizer()",
        "    ",
        "    def test_basic_tokenization(self):",
        "        \"\"\"Test basic word extraction.\"\"\"",
        "        tokens = self.tokenizer.tokenize(\"Hello world\")",
        "        self.assertEqual(tokens, [\"hello\", \"world\"])",
        "    ",
        "    def test_stop_word_removal(self):",
        "        \"\"\"Test that stop words are removed.\"\"\"",
        "        tokens = self.tokenizer.tokenize(\"The quick brown fox\")",
        "        self.assertNotIn(\"the\", tokens)",
        "        self.assertIn(\"quick\", tokens)",
        "        self.assertIn(\"brown\", tokens)",
        "        self.assertIn(\"fox\", tokens)",
        "    ",
        "    def test_minimum_length(self):",
        "        \"\"\"Test minimum word length filtering.\"\"\"",
        "        tokens = self.tokenizer.tokenize(\"I am a test of tokenization\")",
        "        for token in tokens:",
        "            self.assertGreaterEqual(len(token), 3)",
        "    ",
        "    def test_lowercase(self):",
        "        \"\"\"Test that tokens are lowercased.\"\"\"",
        "        tokens = self.tokenizer.tokenize(\"NEURAL Networks PROCESSING\")",
        "        self.assertEqual(tokens, [\"neural\", \"networks\", \"processing\"])",
        "    ",
        "    def test_alphanumeric(self):",
        "        \"\"\"Test handling of alphanumeric tokens.\"\"\"",
        "        tokens = self.tokenizer.tokenize(\"word2vec and bert3 models\")",
        "        self.assertIn(\"word2vec\", tokens)",
        "        self.assertIn(\"bert3\", tokens)",
        "    ",
        "    def test_extract_bigrams(self):",
        "        \"\"\"Test bigram extraction.\"\"\"",
        "        tokens = [\"neural\", \"network\", \"processing\"]",
        "        bigrams = self.tokenizer.extract_ngrams(tokens, n=2)",
        "        self.assertEqual(bigrams, [\"neural network\", \"network processing\"])",
        "    ",
        "    def test_extract_trigrams(self):",
        "        \"\"\"Test trigram extraction.\"\"\"",
        "        tokens = [\"neural\", \"network\", \"information\", \"processing\"]",
        "        trigrams = self.tokenizer.extract_ngrams(tokens, n=3)",
        "        self.assertEqual(len(trigrams), 2)",
        "",
        "",
        "class TestTokenizerStemming(unittest.TestCase):",
        "    \"\"\"Test tokenizer stemming and word variants.\"\"\"",
        "    ",
        "    def setUp(self):",
        "        self.tokenizer = Tokenizer()",
        "    ",
        "    def test_stem_basic(self):",
        "        \"\"\"Test basic stemming.\"\"\"",
        "        self.assertEqual(self.tokenizer.stem(\"running\"), \"runn\")",
        "        self.assertEqual(self.tokenizer.stem(\"processing\"), \"process\")",
        "    ",
        "    def test_stem_preserves_short_words(self):",
        "        \"\"\"Test that short words are not stemmed.\"\"\"",
        "        self.assertEqual(self.tokenizer.stem(\"run\"), \"run\")",
        "        self.assertEqual(self.tokenizer.stem(\"the\"), \"the\")",
        "    ",
        "    def test_get_word_variants_basic(self):",
        "        \"\"\"Test basic word variant generation.\"\"\"",
        "        variants = self.tokenizer.get_word_variants(\"bread\")",
        "        self.assertIn(\"bread\", variants)",
        "        self.assertIn(\"sourdough\", variants)",
        "    ",
        "    def test_get_word_variants_includes_plural(self):",
        "        \"\"\"Test that variants include plural forms.\"\"\"",
        "        variants = self.tokenizer.get_word_variants(\"network\")",
        "        self.assertIn(\"network\", variants)",
        "        self.assertIn(\"networks\", variants)",
        "    ",
        "    def test_word_mappings_brain(self):",
        "        \"\"\"Test brain-related word mappings.\"\"\"",
        "        variants = self.tokenizer.get_word_variants(\"brain\")",
        "        self.assertIn(\"neural\", variants)",
        "        self.assertIn(\"cortical\", variants)",
        "",
        "",
        "if __name__ == \"__main__\":",
        "    unittest.main(verbosity=2)"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    }
  ],
  "hour_of_day": 18,
  "day_of_week": "Tuesday",
  "seconds_since_last_commit": -502346,
  "is_merge": false,
  "is_initial": false,
  "parent_count": 1,
  "session_id": null,
  "related_chats": [],
  "ci_result": null,
  "reverted": false,
  "amended": false
}