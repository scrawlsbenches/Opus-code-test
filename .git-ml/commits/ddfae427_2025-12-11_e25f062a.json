{
  "hash": "ddfae427c38cceaeb8b145ad079a14b786f78f12",
  "message": "Implement tasks #69, #70, #77: RAG demo, timing, ask_codebase",
  "author": "Claude",
  "timestamp": "2025-12-11 01:55:49 +0000",
  "branch": "claude/multi-index-design-DvifZ",
  "files_changed": [
    "TASK_LIST.md",
    "scripts/ask_codebase.py",
    "showcase.py"
  ],
  "insertions": 415,
  "deletions": 26,
  "hunks": [
    {
      "file": "TASK_LIST.md",
      "function": "Add new demonstration sections:",
      "start_line": 2463,
      "lines_added": [
        "**Status:** [x] Completed",
        "**Status:** [x] Completed"
      ],
      "lines_removed": [
        "**Status:** [ ] Not Started",
        "**Status:** [ ] Not Started"
      ],
      "context_before": [
        "1. **Code Query Expansion** - show how \"fetch data\" expands to include \"get\", \"load\", \"retrieve\"",
        "2. **Intent-Based Search** - demonstrate \"where do we handle errors?\" style queries",
        "3. **Code Fingerprinting** - compare two similar functions and explain their similarity",
        "4. **Query Intent Detection** - show how system distinguishes \"what is PageRank\" vs \"compute pagerank\"",
        "",
        "---",
        "",
        "### 69. Add Passage-Level Search Demo to Showcase",
        "",
        "**File:** `showcase.py`"
      ],
      "context_after": [
        "**Priority:** Medium",
        "",
        "**Problem:**",
        "`find_passages_for_query()` is the key RAG capability for retrieving relevant code snippets, but it's not demonstrated. This is arguably the most useful feature for LLM integration.",
        "",
        "**Solution:**",
        "Add \"RAG DEMONSTRATION\" section showing:",
        "1. Query ‚Üí relevant passages with file:line references",
        "2. How passage chunking works",
        "3. Overlap handling for context preservation",
        "4. Use case: feeding context to an LLM",
        "",
        "---",
        "",
        "### 70. Add Performance Timing to Showcase",
        "",
        "**File:** `showcase.py`",
        "**Priority:** Low",
        "",
        "**Problem:**",
        "No timing information shown. Users can't gauge performance characteristics.",
        "",
        "**Solution:**",
        "Add timing for key operations:",
        "- Document processing time",
        "- `compute_all()` time",
        "- Query expansion time"
      ],
      "change_type": "modify"
    },
    {
      "file": "TASK_LIST.md",
      "function": "python scripts/related_files.py cortical/query.py",
      "start_line": 2695,
      "lines_added": [
        "**Status:** [x] Completed"
      ],
      "lines_removed": [
        "**Status:** [ ] Not Started"
      ],
      "context_before": [
        "2. Find test files that test target",
        "3. Find docs that mention target functions",
        "4. Find files with similar fingerprints",
        "5. Rank by relevance",
        "",
        "---",
        "",
        "### 77. Add Interactive \"Ask the Codebase\" Mode",
        "",
        "**Files:** `scripts/ask_codebase.py` (new)"
      ],
      "context_after": [
        "**Priority:** High",
        "",
        "**Problem:**",
        "Current search returns passages but doesn't synthesize answers. You have to read multiple results.",
        "",
        "**Solution:**",
        "Create conversational interface that uses RAG to answer questions:",
        "```bash",
        "python scripts/ask_codebase.py",
        ""
      ],
      "change_type": "modify"
    },
    {
      "file": "TASK_LIST.md",
      "function": "Key files to understand:",
      "start_line": 2871,
      "lines_added": [
        "| 69 | Medium | Add passage-level search demo | ‚úì Done | Showcase |",
        "| 70 | Low | Add performance timing to showcase | ‚úì Done | Showcase |",
        "| 77 | High | Add interactive \"Ask the Codebase\" mode | ‚úì Done | Developer Experience |"
      ],
      "lines_removed": [
        "| 69 | Medium | Add passage-level search demo | | Showcase |",
        "| 70 | Low | Add performance timing to showcase | | Showcase |",
        "| 77 | High | Add interactive \"Ask the Codebase\" mode | | Developer Experience |"
      ],
      "context_before": [
        "```",
        "",
        "---",
        "",
        "## Summary Table",
        "",
        "| # | Priority | Task | Status | Category |",
        "|---|----------|------|--------|----------|",
        "| 67 | Low | Fix O(n) lookup in showcase | ‚úì Done | Showcase |",
        "| 68 | Medium | Add code-specific features to showcase | ‚úì Done | Showcase |"
      ],
      "context_after": [
        "| 71 | High | Enable code-aware tokenization in index | ‚úì Done | Code Index |",
        "| 72 | High | Use programming query expansion in search | ‚úì Done | Code Index |",
        "| 73 | Medium | Add \"Find Similar Code\" command | | Code Index |",
        "| 74 | Medium | Add \"Explain This Code\" command | | Developer Experience |",
        "| 75 | Medium | Add \"What Changed?\" semantic diff | | Developer Experience |",
        "| 76 | Medium | Add \"Suggest Related Files\" feature | | Developer Experience |",
        "| 78 | Low | Add code pattern detection | | Developer Experience |",
        "| 79 | Low | Add corpus health dashboard | | Developer Experience |",
        "| 80 | Low | Add \"Learning Mode\" for new contributors | | Developer Experience |",
        "",
        "---",
        "",
        "*Added 2025-12-11, completions updated 2025-12-11*"
      ],
      "change_type": "modify"
    },
    {
      "file": "scripts/ask_codebase.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "#!/usr/bin/env python3",
        "\"\"\"",
        "Ask the Codebase - Interactive Q&A using RAG.",
        "",
        "This script provides a conversational interface for asking questions",
        "about the codebase, using the Cortical Text Processor's passage retrieval",
        "to find relevant context.",
        "",
        "Usage:",
        "    python scripts/ask_codebase.py                    # Interactive mode",
        "    python scripts/ask_codebase.py \"How does X work?\" # Single question",
        "    python scripts/ask_codebase.py --sources          # Show source references",
        "\"\"\"",
        "",
        "import argparse",
        "import sys",
        "from pathlib import Path",
        "from typing import List, Tuple, Dict, Optional",
        "",
        "# Add parent directory to path for imports",
        "sys.path.insert(0, str(Path(__file__).parent.parent))",
        "",
        "from cortical.processor import CorticalTextProcessor",
        "",
        "",
        "def find_line_number(doc_content: str, char_position: int) -> int:",
        "    \"\"\"Find the line number for a character position.\"\"\"",
        "    return doc_content[:char_position].count('\\n') + 1",
        "",
        "",
        "def format_reference(doc_id: str, line_num: int) -> str:",
        "    \"\"\"Format a file:line reference.\"\"\"",
        "    return f\"{doc_id}:{line_num}\"",
        "",
        "",
        "def get_doc_type_emoji(doc_id: str) -> str:",
        "    \"\"\"Get emoji indicator for document type.\"\"\"",
        "    if doc_id.endswith('.md'):",
        "        return \"üìñ\"",
        "    elif doc_id.startswith('tests/'):",
        "        return \"üß™\"",
        "    else:",
        "        return \"üíª\"",
        "",
        "",
        "class CodebaseQA:",
        "    \"\"\"Interactive Q&A system for the codebase.\"\"\"",
        "",
        "    def __init__(self, processor: CorticalTextProcessor):",
        "        self.processor = processor",
        "",
        "    def find_relevant_passages(",
        "        self,",
        "        question: str,",
        "        top_n: int = 5,",
        "        chunk_size: int = 400",
        "    ) -> List[Tuple[str, str, int, float]]:",
        "        \"\"\"",
        "        Find passages relevant to the question.",
        "",
        "        Returns:",
        "            List of (passage_text, reference, line_num, score) tuples",
        "        \"\"\"",
        "        # Detect if this is a conceptual or implementation question",
        "        is_conceptual = self.processor.is_conceptual_query(question)",
        "",
        "        # Get relevant documents with boosting",
        "        doc_results = self.processor.find_documents_with_boost(",
        "            question,",
        "            top_n=top_n * 2,",
        "            auto_detect_intent=True",
        "        )",
        "",
        "        # Get passages from those documents",
        "        doc_ids = [doc_id for doc_id, _ in doc_results]",
        "        passages = self.processor.find_passages_for_query(",
        "            question,",
        "            top_n=top_n,",
        "            chunk_size=chunk_size,",
        "            overlap=100,",
        "            doc_filter=doc_ids if doc_ids else None",
        "        )",
        "",
        "        results = []",
        "        for passage_text, doc_id, start, end, score in passages:",
        "            doc_content = self.processor.documents.get(doc_id, \"\")",
        "            line_num = find_line_number(doc_content, start)",
        "            reference = format_reference(doc_id, line_num)",
        "            results.append((passage_text, reference, line_num, score))",
        "",
        "        return results",
        "",
        "    def format_answer(",
        "        self,",
        "        question: str,",
        "        passages: List[Tuple[str, str, int, float]],",
        "        show_sources: bool = True,",
        "        verbose: bool = False",
        "    ) -> str:",
        "        \"\"\"",
        "        Format the answer with context from passages.",
        "",
        "        This creates a structured response showing:",
        "        1. Question type (conceptual vs implementation)",
        "        2. Relevant passages with references",
        "        3. Source list for verification",
        "        \"\"\"",
        "        lines = []",
        "",
        "        # Detect question type",
        "        is_conceptual = self.processor.is_conceptual_query(question)",
        "        intent = \"conceptual\" if is_conceptual else \"implementation\"",
        "",
        "        lines.append(f\"\\n{'‚îÄ' * 60}\")",
        "        lines.append(f\"Question: {question}\")",
        "        lines.append(f\"Type: {intent}\")",
        "        lines.append(f\"{'‚îÄ' * 60}\\n\")",
        "",
        "        if not passages:",
        "            lines.append(\"No relevant passages found for this question.\")",
        "            lines.append(\"Try rephrasing or using different keywords.\")",
        "            return '\\n'.join(lines)",
        "",
        "        lines.append(\"üìö Relevant Context:\\n\")",
        "",
        "        for i, (passage_text, reference, line_num, score) in enumerate(passages, 1):",
        "            doc_id = reference.split(':')[0]",
        "            emoji = get_doc_type_emoji(doc_id)",
        "",
        "            lines.append(f\"[{i}] {emoji} {reference} (relevance: {score:.2f})\")",
        "            lines.append(\"‚îÄ\" * 50)",
        "",
        "            # Show passage content",
        "            if verbose:",
        "                # Full passage",
        "                for line in passage_text.strip().split('\\n'):",
        "                    lines.append(f\"  {line}\")",
        "            else:",
        "                # Truncated passage (first 5 lines)",
        "                passage_lines = passage_text.strip().split('\\n')[:5]",
        "                for line in passage_lines:",
        "                    if len(line) > 70:",
        "                        line = line[:67] + \"...\"",
        "                    lines.append(f\"  {line}\")",
        "                if len(passage_text.strip().split('\\n')) > 5:",
        "                    lines.append(f\"  ... (more content in source)\")",
        "",
        "            lines.append(\"\")",
        "",
        "        if show_sources:",
        "            lines.append(\"\\nüìå Sources:\")",
        "            seen_docs = set()",
        "            for passage_text, reference, line_num, score in passages:",
        "                doc_id = reference.split(':')[0]",
        "                if doc_id not in seen_docs:",
        "                    emoji = get_doc_type_emoji(doc_id)",
        "                    lines.append(f\"  {emoji} {doc_id}\")",
        "                    seen_docs.add(doc_id)",
        "",
        "        lines.append(\"\")",
        "        return '\\n'.join(lines)",
        "",
        "    def answer(",
        "        self,",
        "        question: str,",
        "        top_n: int = 3,",
        "        show_sources: bool = True,",
        "        verbose: bool = False",
        "    ) -> str:",
        "        \"\"\"Answer a question about the codebase.\"\"\"",
        "        passages = self.find_relevant_passages(question, top_n=top_n)",
        "        return self.format_answer(question, passages, show_sources, verbose)",
        "",
        "",
        "def interactive_mode(qa: CodebaseQA, verbose: bool = False):",
        "    \"\"\"Run interactive Q&A mode.\"\"\"",
        "    print(\"\\n\" + \"=\" * 60)",
        "    print(\"       üß† ASK THE CODEBASE - Interactive Q&A\")",
        "    print(\"=\" * 60)",
        "    print(\"\\nAsk questions about the codebase in natural language.\")",
        "    print(\"The system will find relevant passages to help answer.\\n\")",
        "    print(\"Commands:\")",
        "    print(\"  /verbose    - Toggle verbose mode (show full passages)\")",
        "    print(\"  /top N      - Set number of results (default: 3)\")",
        "    print(\"  /help       - Show this help\")",
        "    print(\"  /quit       - Exit\")",
        "    print()",
        "",
        "    top_n = 3",
        "    show_verbose = verbose",
        "",
        "    while True:",
        "        try:",
        "            question = input(\"Ask> \").strip()",
        "        except (EOFError, KeyboardInterrupt):",
        "            print(\"\\nGoodbye!\")",
        "            break",
        "",
        "        if not question:",
        "            continue",
        "",
        "        if question.startswith('/'):",
        "            cmd_parts = question.split(maxsplit=1)",
        "            cmd = cmd_parts[0].lower()",
        "",
        "            if cmd in ('/quit', '/exit', '/q'):",
        "                print(\"Goodbye!\")",
        "                break",
        "            elif cmd == '/help':",
        "                print(\"\\nCommands: /verbose, /top N, /quit\")",
        "                print(\"Or just type your question!\\n\")",
        "            elif cmd == '/verbose':",
        "                show_verbose = not show_verbose",
        "                status = \"ON\" if show_verbose else \"OFF\"",
        "                print(f\"Verbose mode: {status}\")",
        "            elif cmd == '/top' and len(cmd_parts) > 1:",
        "                try:",
        "                    top_n = int(cmd_parts[1])",
        "                    print(f\"Now showing top {top_n} results\")",
        "                except ValueError:",
        "                    print(\"Usage: /top N (where N is a number)\")",
        "            else:",
        "                print(f\"Unknown command: {cmd}\")",
        "        else:",
        "            answer = qa.answer(question, top_n=top_n, verbose=show_verbose)",
        "            print(answer)",
        "",
        "",
        "def main():",
        "    parser = argparse.ArgumentParser(",
        "        description='Ask questions about the codebase',",
        "        formatter_class=argparse.RawDescriptionHelpFormatter,",
        "        epilog=\"\"\"",
        "Examples:",
        "  %(prog)s                              # Interactive mode",
        "  %(prog)s \"How does PageRank work?\"    # Single question",
        "  %(prog)s \"where is TF-IDF computed\" --verbose",
        "  %(prog)s \"authentication\" --top 5     # More results",
        "        \"\"\"",
        "    )",
        "    parser.add_argument('question', nargs='?', help='Question to ask')",
        "    parser.add_argument('--corpus', '-c', default='corpus_dev.pkl',",
        "                        help='Corpus file path (default: corpus_dev.pkl)')",
        "    parser.add_argument('--top', '-n', type=int, default=3,",
        "                        help='Number of passages to retrieve (default: 3)')",
        "    parser.add_argument('--verbose', '-v', action='store_true',",
        "                        help='Show full passage content')",
        "    parser.add_argument('--no-sources', action='store_true',",
        "                        help='Hide source list')",
        "    args = parser.parse_args()",
        "",
        "    base_path = Path(__file__).parent.parent",
        "    corpus_path = base_path / args.corpus",
        "",
        "    # Check if corpus exists",
        "    if not corpus_path.exists():",
        "        print(f\"Error: Corpus file not found: {corpus_path}\")",
        "        print(\"Run 'python scripts/index_codebase.py' first to create it.\")",
        "        sys.exit(1)",
        "",
        "    # Load corpus",
        "    print(f\"Loading corpus from {corpus_path}...\")",
        "    processor = CorticalTextProcessor.load(str(corpus_path))",
        "    print(f\"Loaded {len(processor.documents)} documents\")",
        "",
        "    qa = CodebaseQA(processor)",
        "",
        "    if args.question:",
        "        # Single question mode",
        "        answer = qa.answer(",
        "            args.question,",
        "            top_n=args.top,",
        "            show_sources=not args.no_sources,",
        "            verbose=args.verbose",
        "        )",
        "        print(answer)",
        "    else:",
        "        # Interactive mode",
        "        interactive_mode(qa, verbose=args.verbose)",
        "",
        "",
        "if __name__ == '__main__':",
        "    main()"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "showcase.py",
      "function": null,
      "start_line": 2,
      "lines_added": [
        "import time",
        "class Timer:",
        "    \"\"\"Simple timer for measuring operation durations.\"\"\"",
        "",
        "    def __init__(self):",
        "        self.times: Dict[str, float] = {}",
        "        self._start: float = 0",
        "",
        "    def start(self, name: str):",
        "        \"\"\"Start timing an operation.\"\"\"",
        "        self._start = time.perf_counter()",
        "        self._current = name",
        "",
        "    def stop(self) -> float:",
        "        \"\"\"Stop timing and record the duration.\"\"\"",
        "        elapsed = time.perf_counter() - self._start",
        "        self.times[self._current] = elapsed",
        "        return elapsed",
        "",
        "    def get(self, name: str) -> float:",
        "        \"\"\"Get recorded time for an operation.\"\"\"",
        "        return self.times.get(name, 0)",
        "",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "Cortical Text Processor Showcase",
        "================================",
        "",
        "This showcase processes a corpus of documents, demonstrating the",
        "hierarchical analysis of relationships between concepts, documents,",
        "and ideas across diverse topics.",
        "\"\"\"",
        "",
        "import os",
        "import sys"
      ],
      "context_after": [
        "from typing import Dict, List, Tuple",
        "",
        "from cortical import CorticalTextProcessor, CorticalLayer",
        "",
        "",
        "def print_header(title: str, char: str = \"=\"):",
        "    \"\"\"Print a formatted section header.\"\"\"",
        "    width = 70",
        "    print(f\"\\n{char * width}\")",
        "    print(f\"{title:^{width}}\")",
        "    print(f\"{char * width}\\n\")",
        "",
        "",
        "def print_subheader(title: str):",
        "    \"\"\"Print a formatted subsection header.\"\"\""
      ],
      "change_type": "add"
    },
    {
      "file": "showcase.py",
      "function": "def print_subheader(title: str):",
      "start_line": 31,
      "lines_added": [
        "",
        "        self.timer = Timer()",
        "",
        "        self.demonstrate_passage_search()"
      ],
      "lines_removed": [
        "    ",
        "    "
      ],
      "context_before": [
        "def render_bar(value: float, max_value: float, width: int = 30) -> str:",
        "    \"\"\"Render a text-based progress bar.\"\"\"",
        "    if max_value == 0:",
        "        return \" \" * width",
        "    filled = int((value / max_value) * width)",
        "    return \"‚ñà\" * filled + \"‚ñë\" * (width - filled)",
        "",
        "",
        "class CorticalShowcase:",
        "    \"\"\"Showcases the cortical text processor with interesting analysis.\"\"\""
      ],
      "context_after": [
        "    def __init__(self, samples_dir: str = \"samples\"):",
        "        self.samples_dir = samples_dir",
        "        self.processor = CorticalTextProcessor()",
        "        self.loaded_files = []",
        "    def run(self):",
        "        \"\"\"Run the complete demo.\"\"\"",
        "        self.print_intro()",
        "",
        "        if not self.ingest_corpus():",
        "            print(\"No documents found!\")",
        "            return",
        "",
        "        self.analyze_hierarchy()",
        "        self.discover_key_concepts()",
        "        self.analyze_tfidf()",
        "        self.find_concept_associations()",
        "        self.analyze_document_relationships()",
        "        self.demonstrate_queries()",
        "        self.demonstrate_polysemy()",
        "        self.demonstrate_code_features()",
        "        self.demonstrate_gap_analysis()",
        "        self.demonstrate_embeddings()",
        "        self.print_insights()",
        "    ",
        "    def print_intro(self):",
        "        \"\"\"Print introduction.\"\"\"",
        "        print(\"\"\"",
        "    ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó"
      ],
      "change_type": "modify"
    },
    {
      "file": "showcase.py",
      "function": "class CorticalShowcase:",
      "start_line": 72,
      "lines_added": [
        "",
        "",
        "",
        "",
        "",
        "        # Time document loading",
        "        self.timer.start('document_loading')",
        "",
        "        load_time = self.timer.stop()",
        "",
        "        self.timer.start('compute_all')",
        "        compute_time = self.timer.stop()",
        "",
        "",
        "        print(f\"\\n‚è±  Document loading: {load_time:.2f}s\")",
        "        print(f\"‚è±  Compute all:      {compute_time:.2f}s\")",
        ""
      ],
      "lines_removed": [
        "        ",
        "        ",
        "        ",
        "        ",
        "        ",
        "            ",
        "        ",
        "        ",
        "        ",
        "        "
      ],
      "context_before": [
        "    ‚ïë            üß†  CORTICAL TEXT PROCESSOR SHOWCASE  üß†                  ‚ïë",
        "    ‚ïë                                                                      ‚ïë",
        "    ‚ïë     Mimicking how the neocortex processes and understands text       ‚ïë",
        "    ‚ïë                                                                      ‚ïë",
        "    ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù",
        "        \"\"\")",
        "    ",
        "    def ingest_corpus(self) -> bool:",
        "        \"\"\"Ingest the document corpus from disk.\"\"\"",
        "        print_header(\"DOCUMENT INGESTION\", \"‚ïê\")"
      ],
      "context_after": [
        "        print(f\"Loading documents from: {self.samples_dir}\")",
        "        print(\"Processing through cortical hierarchy...\")",
        "        print(\"(Like visual information flowing V1 ‚Üí V2 ‚Üí V4 ‚Üí IT)\\n\")",
        "        if not os.path.exists(self.samples_dir):",
        "            print(f\"  ‚ùå Directory not found: {self.samples_dir}\")",
        "            return False",
        "        txt_files = sorted([f for f in os.listdir(self.samples_dir) if f.endswith('.txt')])",
        "        if not txt_files:",
        "            return False",
        "        for filename in txt_files:",
        "            filepath = os.path.join(self.samples_dir, filename)",
        "            with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:",
        "                content = f.read()",
        "            doc_id = filename.replace('.txt', '')",
        "            self.processor.process_document(doc_id, content)",
        "            word_count = len(content.split())",
        "            self.loaded_files.append((doc_id, word_count))",
        "            print(f\"  üìÑ {doc_id:30} ({word_count:3} words)\")",
        "        # Run all computations with hybrid strategy for better Layer 2 connectivity",
        "        print(\"\\nComputing cortical representations...\")",
        "        self.processor.compute_all(",
        "            verbose=False,",
        "            connection_strategy='hybrid',",
        "            cluster_strictness=0.5,",
        "            bridge_weight=0.3",
        "        )",
        "        layer0 = self.processor.get_layer(CorticalLayer.TOKENS)",
        "        layer1 = self.processor.get_layer(CorticalLayer.BIGRAMS)",
        "        print(f\"\\n‚úì Processed {len(self.loaded_files)} documents\")",
        "        print(f\"‚úì Created {layer0.column_count()} token minicolumns\")",
        "        print(f\"‚úì Created {layer1.column_count()} bigram minicolumns\")",
        "        print(f\"‚úì Formed {layer0.total_connections()} lateral connections\")",
        "        return True",
        "    ",
        "    def analyze_hierarchy(self):",
        "        \"\"\"Show the hierarchical structure.\"\"\"",
        "        print_header(\"HIERARCHICAL STRUCTURE\", \"‚ïê\")",
        "        ",
        "        print(\"The cortical model has 4 layers (like visual cortex V1‚ÜíIT):\\n\")",
        "        ",
        "        layers = [",
        "            (CorticalLayer.TOKENS, \"Token Layer (V1)\", \"Individual words\"),"
      ],
      "change_type": "modify"
    },
    {
      "file": "showcase.py",
      "function": "class CorticalShowcase:",
      "start_line": 242,
      "lines_added": [
        "",
        "",
        "        total_query_time = 0",
        "",
        "",
        "            # Time expansion + search",
        "            start = time.perf_counter()",
        "",
        "",
        "",
        "            elapsed = time.perf_counter() - start",
        "            total_query_time += elapsed",
        "",
        "            print(f\"    ‚è±  {elapsed*1000:.1f}ms\")",
        "",
        "        self.timer.times['queries'] = total_query_time",
        "        print(f\"Average query time: {total_query_time/len(test_queries)*1000:.1f}ms\")",
        "",
        "    def demonstrate_passage_search(self):",
        "        \"\"\"Demonstrate passage-level retrieval for RAG applications.\"\"\"",
        "        print_header(\"PASSAGE RETRIEVAL (RAG)\", \"‚ïê\")",
        "",
        "        print(\"Passage search retrieves specific text chunks, ideal for RAG:\")",
        "        print(\"(Retrieval-Augmented Generation - feeding context to LLMs)\\n\")",
        "",
        "        # Demonstrate with a specific query",
        "        query = \"PageRank algorithm convergence\"",
        "        print_subheader(f\"üîç Query: '{query}'\")",
        "",
        "        # Time passage retrieval",
        "        self.timer.start('passage_search')",
        "",
        "        # Get passages",
        "        passages = self.processor.find_passages_for_query(",
        "            query,",
        "            top_n=3,",
        "            chunk_size=300,",
        "            overlap=50",
        "        )",
        "        passage_time = self.timer.stop()",
        "",
        "        if passages:",
        "            print(f\"\\n    Found {len(passages)} relevant passages:\\n\")",
        "",
        "            for i, (passage_text, doc_id, start, end, score) in enumerate(passages, 1):",
        "                # Calculate line number from character position",
        "                doc_content = self.processor.documents.get(doc_id, \"\")",
        "                line_num = doc_content[:start].count('\\n') + 1",
        "",
        "                print(f\"    [{i}] {doc_id}:{line_num} (score: {score:.3f})\")",
        "                print(\"    \" + \"‚îÄ\" * 50)",
        "",
        "                # Show truncated passage",
        "                lines = passage_text.strip().split('\\n')[:4]",
        "                for line in lines:",
        "                    if len(line) > 60:",
        "                        line = line[:57] + \"...\"",
        "                    print(f\"      {line}\")",
        "                if len(passage_text.strip().split('\\n')) > 4:",
        "                    print(f\"      ...\")",
        "                print()",
        "",
        "        print(f\"    ‚è±  Passage retrieval: {passage_time*1000:.1f}ms\")",
        "        print(\"\\n    üí° Use case: Feed these passages to an LLM as context\")",
        "        print(\"                 for answering questions about your corpus.\")",
        "        print()",
        ""
      ],
      "lines_removed": [
        "        ",
        "        ",
        "        ",
        "            ",
        "            ",
        "            ",
        "    "
      ],
      "context_before": [
        "            doc = sorted_docs[0]",
        "            print(f\"\\n  '{doc.content}' connects to:\")",
        "            ",
        "            related = self.processor.find_related_documents(doc.content)[:5]",
        "            for related_doc, weight in related:",
        "                print(f\"    ‚Üí {related_doc} (similarity: {weight:.3f})\")",
        "    ",
        "    def demonstrate_queries(self):",
        "        \"\"\"Demonstrate query capability with expansion.\"\"\"",
        "        print_header(\"QUERY DEMONSTRATION\", \"‚ïê\")"
      ],
      "context_after": [
        "        print(\"Query expansion adds semantically related terms for better recall:\\n\")",
        "        test_queries = [\"neural networks\", \"fermentation\", \"distributed systems\"]",
        "        for query in test_queries:",
        "            print_subheader(f\"üîç Query: '{query}'\")",
        "            # Show expansion",
        "            expanded = self.processor.expand_query(query, max_expansions=6)",
        "            original = set(self.processor.tokenizer.tokenize(query))",
        "            new_terms = [t for t in expanded.keys() if t not in original]",
        "            if new_terms:",
        "                print(f\"    Expanded with: {', '.join(new_terms[:6])}\")",
        "            # Find documents",
        "            results = self.processor.find_documents_for_query(query, top_n=3)",
        "            print(f\"\\n    Top documents:\")",
        "            for doc_id, score in results:",
        "                print(f\"      ‚Ä¢ {doc_id} (score: {score:.3f})\")",
        "            print()",
        "    def demonstrate_polysemy(self):",
        "        \"\"\"Demonstrate polysemy - same word, different meanings.\"\"\"",
        "        print_header(\"POLYSEMY DEMONSTRATION\", \"‚ïê\")",
        "",
        "        print(\"Polysemy occurs when the same word has multiple meanings.\")",
        "        print(\"This affects retrieval when query terms are ambiguous.\\n\")",
        "",
        "        # Query for \"candle sticks\"",
        "        query = \"candle sticks\"",
        "        print_subheader(f\"üîç Query: '{query}'\")"
      ],
      "change_type": "modify"
    },
    {
      "file": "showcase.py",
      "function": "class CorticalShowcase:",
      "start_line": 458,
      "lines_added": [
        "",
        "        # Performance summary",
        "        print(\"\\n‚è±  PERFORMANCE SUMMARY\\n\")",
        "        if 'document_loading' in self.timer.times:",
        "            print(f\"  Document loading:    {self.timer.get('document_loading'):.2f}s\")",
        "        if 'compute_all' in self.timer.times:",
        "            print(f\"  Compute all:         {self.timer.get('compute_all'):.2f}s\")",
        "        if 'queries' in self.timer.times:",
        "            avg_query = self.timer.get('queries') / 3 * 1000  # 3 queries",
        "            print(f\"  Avg query time:      {avg_query:.1f}ms\")",
        "        if 'passage_search' in self.timer.times:",
        "            print(f\"  Passage retrieval:   {self.timer.get('passage_search')*1000:.1f}ms\")",
        "",
        "        print(\"  ‚úì Retrieved passages for RAG applications\")"
      ],
      "lines_removed": [
        "        "
      ],
      "context_before": [
        "        print(f\"  Total connections:       {layer0.total_connections():,}\")",
        "        ",
        "        # Find most central token",
        "        top_token = max(layer0.minicolumns.values(), key=lambda c: c.pagerank)",
        "        print(f\"\\n  Most central concept: '{top_token.content}'\")",
        "        ",
        "        # Find most connected document",
        "        if layer3.column_count() > 0:",
        "            top_doc = max(layer3.minicolumns.values(), key=lambda c: c.connection_count())",
        "            print(f\"  Most connected document: '{top_doc.content}'\")"
      ],
      "context_after": [
        "        print(\"\\n\" + \"‚ïê\" * 70)",
        "        print(\"Demo complete! The cortical text processor successfully:\")",
        "        print(\"  ‚úì Built hierarchical representations (Layers 0-3)\")",
        "        print(\"  ‚úì Discovered key concepts via PageRank\")",
        "        print(\"  ‚úì Computed TF-IDF for discriminative analysis\")",
        "        print(\"  ‚úì Found associations through lateral connections\")",
        "        print(\"  ‚úì Identified document relationships\")",
        "        print(\"  ‚úì Demonstrated code search features\")",
        "        print(\"  ‚úì Detected knowledge gaps and anomalies\")",
        "        print(\"  ‚úì Computed graph embeddings\")",
        "        print(\"  ‚úì Enabled semantic queries with expansion\")",
        "        print(\"‚ïê\" * 70 + \"\\n\")",
        "",
        "",
        "if __name__ == \"__main__\":",
        "    showcase = CorticalShowcase(samples_dir=\"samples\")",
        "    showcase.run()"
      ],
      "change_type": "modify"
    }
  ],
  "hour_of_day": 1,
  "day_of_week": "Thursday",
  "seconds_since_last_commit": -388139,
  "is_merge": false,
  "is_initial": false,
  "parent_count": 1,
  "session_id": null,
  "related_chats": [],
  "ci_result": null,
  "reverted": false,
  "amended": false
}