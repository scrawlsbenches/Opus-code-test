{
  "hash": "1568f3c37d3ee88503bbf5f53b67ec0c55e5e0d3",
  "message": "feat: Add ML data collection infrastructure for project-specific micro-model",
  "author": "Claude",
  "timestamp": "2025-12-15 01:59:41 +0000",
  "branch": "claude/multi-index-design-DvifZ",
  "files_changed": [
    ".claude/hooks/session_logger.py",
    ".claude/skills/ml-logger/SKILL.md",
    ".gitignore",
    "scripts/ml_data_collector.py"
  ],
  "insertions": 1039,
  "deletions": 0,
  "hunks": [
    {
      "file": ".claude/hooks/session_logger.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "#!/usr/bin/env python3",
        "\"\"\"",
        "Claude Session Logger Hook",
        "",
        "This script logs Claude Code chat sessions for ML training data collection.",
        "It can be called manually or integrated with Claude Code hooks.",
        "",
        "Usage:",
        "    # Log a complete exchange",
        "    python .claude/hooks/session_logger.py \\",
        "        --query \"How do I fix the timeout bug?\" \\",
        "        --response \"Let me look at the code...\" \\",
        "        --files-read cortical/processor.py \\",
        "        --files-modified cortical/processor.py \\",
        "        --tools Read,Edit,Bash",
        "",
        "    # Start a session",
        "    python .claude/hooks/session_logger.py --start-session",
        "",
        "    # End a session with summary",
        "    python .claude/hooks/session_logger.py --end-session --summary \"Fixed timeout bugs\"",
        "\"\"\"",
        "",
        "import json",
        "import os",
        "import sys",
        "import hashlib",
        "from datetime import datetime",
        "from pathlib import Path",
        "from typing import List, Optional",
        "",
        "# Add scripts to path",
        "sys.path.insert(0, str(Path(__file__).parent.parent.parent / \"scripts\"))",
        "",
        "try:",
        "    from ml_data_collector import (",
        "        log_chat, log_action, ensure_dirs,",
        "        SESSIONS_DIR, generate_session_id",
        "    )",
        "    ML_COLLECTOR_AVAILABLE = True",
        "except ImportError:",
        "    ML_COLLECTOR_AVAILABLE = False",
        "",
        "",
        "# Session state file",
        "SESSION_STATE_FILE = Path(\".git-ml/current_session.json\")",
        "",
        "",
        "def get_current_session() -> Optional[dict]:",
        "    \"\"\"Get the current active session.\"\"\"",
        "    if SESSION_STATE_FILE.exists():",
        "        with open(SESSION_STATE_FILE) as f:",
        "            return json.load(f)",
        "    return None",
        "",
        "",
        "def start_session() -> str:",
        "    \"\"\"Start a new logging session.\"\"\"",
        "    ensure_dirs()",
        "",
        "    session = {",
        "        \"id\": generate_session_id(),",
        "        \"started_at\": datetime.now().isoformat(),",
        "        \"exchanges\": 0,",
        "        \"files_touched\": [],",
        "        \"tools_used\": [],",
        "    }",
        "",
        "    SESSION_STATE_FILE.parent.mkdir(parents=True, exist_ok=True)",
        "    with open(SESSION_STATE_FILE, \"w\") as f:",
        "        json.dump(session, f, indent=2)",
        "",
        "    print(f\"Started session: {session['id']}\")",
        "    return session[\"id\"]",
        "",
        "",
        "def end_session(summary: Optional[str] = None):",
        "    \"\"\"End the current session and save summary.\"\"\"",
        "    session = get_current_session()",
        "    if not session:",
        "        print(\"No active session\")",
        "        return",
        "",
        "    session[\"ended_at\"] = datetime.now().isoformat()",
        "    session[\"summary\"] = summary",
        "",
        "    # Save to sessions directory",
        "    ensure_dirs()",
        "    SESSIONS_DIR.mkdir(parents=True, exist_ok=True)",
        "",
        "    filename = f\"{session['started_at'][:10]}_{session['id']}.json\"",
        "    filepath = SESSIONS_DIR / filename",
        "",
        "    with open(filepath, \"w\") as f:",
        "        json.dump(session, f, indent=2)",
        "",
        "    # Remove current session file",
        "    SESSION_STATE_FILE.unlink()",
        "",
        "    print(f\"Ended session: {session['id']}\")",
        "    print(f\"  Exchanges: {session['exchanges']}\")",
        "    print(f\"  Files: {len(session['files_touched'])}\")",
        "    print(f\"  Saved to: {filepath}\")",
        "",
        "",
        "def log_exchange(",
        "    query: str,",
        "    response: str,",
        "    files_read: Optional[List[str]] = None,",
        "    files_modified: Optional[List[str]] = None,",
        "    tools: Optional[List[str]] = None,",
        "    feedback: Optional[str] = None,",
        "):",
        "    \"\"\"Log a query/response exchange.\"\"\"",
        "    if not ML_COLLECTOR_AVAILABLE:",
        "        print(\"Warning: ml_data_collector not available\")",
        "        return",
        "",
        "    # Get or create session",
        "    session = get_current_session()",
        "    if not session:",
        "        session_id = start_session()",
        "        session = get_current_session()",
        "    else:",
        "        session_id = session[\"id\"]",
        "",
        "    # Log the chat",
        "    entry = log_chat(",
        "        query=query,",
        "        response=response,",
        "        session_id=session_id,",
        "        files_referenced=files_read or [],",
        "        files_modified=files_modified or [],",
        "        tools_used=tools or [],",
        "        user_feedback=feedback,",
        "    )",
        "",
        "    # Update session state",
        "    session[\"exchanges\"] += 1",
        "    session[\"files_touched\"] = list(set(",
        "        session[\"files_touched\"] +",
        "        (files_read or []) +",
        "        (files_modified or [])",
        "    ))",
        "    session[\"tools_used\"] = list(set(",
        "        session[\"tools_used\"] +",
        "        (tools or [])",
        "    ))",
        "",
        "    with open(SESSION_STATE_FILE, \"w\") as f:",
        "        json.dump(session, f, indent=2)",
        "",
        "    print(f\"Logged exchange: {entry.id}\")",
        "",
        "",
        "def log_tool_use(tool_name: str, target: str, success: bool = True):",
        "    \"\"\"Log a tool use action.\"\"\"",
        "    if not ML_COLLECTOR_AVAILABLE:",
        "        return",
        "",
        "    session = get_current_session()",
        "    session_id = session[\"id\"] if session else None",
        "",
        "    log_action(",
        "        action_type=f\"tool:{tool_name}\",",
        "        target=target,",
        "        session_id=session_id,",
        "        success=success,",
        "    )",
        "",
        "",
        "def main():",
        "    import argparse",
        "",
        "    parser = argparse.ArgumentParser(description=\"Claude Session Logger\")",
        "    parser.add_argument(\"--start-session\", action=\"store_true\",",
        "                        help=\"Start a new session\")",
        "    parser.add_argument(\"--end-session\", action=\"store_true\",",
        "                        help=\"End the current session\")",
        "    parser.add_argument(\"--summary\", help=\"Session summary (with --end-session)\")",
        "",
        "    parser.add_argument(\"--query\", help=\"User query text\")",
        "    parser.add_argument(\"--response\", help=\"Assistant response text\")",
        "    parser.add_argument(\"--files-read\", nargs=\"*\", default=[],",
        "                        help=\"Files that were read\")",
        "    parser.add_argument(\"--files-modified\", nargs=\"*\", default=[],",
        "                        help=\"Files that were modified\")",
        "    parser.add_argument(\"--tools\", help=\"Comma-separated list of tools used\")",
        "    parser.add_argument(\"--feedback\", choices=[\"positive\", \"negative\", \"neutral\"],",
        "                        help=\"User feedback on the exchange\")",
        "",
        "    parser.add_argument(\"--log-tool\", help=\"Log a tool use (format: tool:target)\")",
        "",
        "    args = parser.parse_args()",
        "",
        "    if args.start_session:",
        "        start_session()",
        "",
        "    elif args.end_session:",
        "        end_session(args.summary)",
        "",
        "    elif args.query and args.response:",
        "        tools = args.tools.split(\",\") if args.tools else []",
        "        log_exchange(",
        "            query=args.query,",
        "            response=args.response,",
        "            files_read=args.files_read,",
        "            files_modified=args.files_modified,",
        "            tools=tools,",
        "            feedback=args.feedback,",
        "        )",
        "",
        "    elif args.log_tool:",
        "        parts = args.log_tool.split(\":\", 1)",
        "        if len(parts) == 2:",
        "            log_tool_use(parts[0], parts[1])",
        "",
        "    else:",
        "        parser.print_help()",
        "",
        "",
        "if __name__ == \"__main__\":",
        "    main()"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": ".claude/skills/ml-logger/SKILL.md",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "# ML Data Logger Skill",
        "",
        "Log chat exchanges and actions for training a project-specific micro-model.",
        "",
        "## When to Use",
        "",
        "Use this skill when you want to:",
        "- Log a significant query/response exchange for ML training",
        "- Start or end a development session",
        "- Check data collection progress",
        "",
        "## Quick Commands",
        "",
        "### Log Current Exchange",
        "",
        "After completing a significant task, log it:",
        "",
        "```bash",
        "python .claude/hooks/session_logger.py \\",
        "    --query \"USER_QUERY_HERE\" \\",
        "    --response \"SUMMARY_OF_RESPONSE\" \\",
        "    --files-read FILE1 FILE2 \\",
        "    --files-modified FILE3 \\",
        "    --tools Read,Edit,Bash",
        "```",
        "",
        "### Session Management",
        "",
        "```bash",
        "# Start session at beginning of work",
        "python .claude/hooks/session_logger.py --start-session",
        "",
        "# End session when done",
        "python .claude/hooks/session_logger.py --end-session --summary \"What was accomplished\"",
        "```",
        "",
        "### Check Progress",
        "",
        "```bash",
        "python scripts/ml_data_collector.py stats",
        "python scripts/ml_data_collector.py estimate",
        "```",
        "",
        "## What Gets Collected",
        "",
        "| Data Type | Contents | Use |",
        "|-----------|----------|-----|",
        "| **Query** | User's question/request | Input for generation |",
        "| **Response** | Assistant's answer summary | Target for generation |",
        "| **Files Read** | Which files were examined | Context prediction |",
        "| **Files Modified** | Which files were changed | Change prediction |",
        "| **Tools Used** | Which tools were invoked | Workflow prediction |",
        "| **Feedback** | User satisfaction | Quality filtering |",
        "",
        "## Integration Pattern",
        "",
        "After completing significant work:",
        "",
        "1. Summarize what the user asked",
        "2. Summarize what was done",
        "3. List files touched",
        "4. Log the exchange",
        "",
        "## Example",
        "",
        "```bash",
        "# After fixing a bug",
        "python .claude/hooks/session_logger.py \\",
        "    --query \"Fix the timeout issue in compute_all\" \\",
        "    --response \"Increased timeout from 10s to 30s in processor.py, added retry logic\" \\",
        "    --files-read cortical/processor.py tests/test_processor.py \\",
        "    --files-modified cortical/processor.py \\",
        "    --tools Read,Edit,Bash \\",
        "    --feedback positive",
        "```",
        "",
        "## Why This Matters",
        "",
        "This data trains a micro-model that learns:",
        "- YOUR project's patterns",
        "- YOUR coding style",
        "- YOUR common workflows",
        "- YOUR file relationships",
        "",
        "The model becomes a personalized assistant that understands THIS codebase deeply."
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": ".gitignore",
      "function": "CodeCoverage/",
      "start_line": 85,
      "lines_added": [
        "",
        "# ML training data (local, regeneratable with `python scripts/ml_data_collector.py backfill`)",
        ".git-ml/"
      ],
      "lines_removed": [],
      "context_before": [
        "TestResult.xml",
        "nunit-*.xml",
        "# Indexer progress files",
        ".index_progress.json",
        ".index_incremental_progress.json",
        "tasks/.current_session.json",
        "",
        "# Generated protobuf code (regenerated at runtime from schema.proto)",
        "cortical/proto/schema_pb2.py",
        "cortical/proto/__pycache__/"
      ],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "scripts/ml_data_collector.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "#!/usr/bin/env python3",
        "\"\"\"",
        "ML Data Collector for Project-Specific Language Model Training",
        "",
        "This module collects enriched data from git commits, chat sessions, and developer",
        "actions to train a micro-model specific to this project.",
        "",
        "Usage:",
        "    # Collect commit data (call from git hook)",
        "    python scripts/ml_data_collector.py commit",
        "",
        "    # Log a chat session",
        "    python scripts/ml_data_collector.py chat --query \"...\" --response \"...\"",
        "",
        "    # Show statistics",
        "    python scripts/ml_data_collector.py stats",
        "",
        "    # Estimate when training is viable",
        "    python scripts/ml_data_collector.py estimate",
        "\"\"\"",
        "",
        "import json",
        "import os",
        "import subprocess",
        "import hashlib",
        "import re",
        "from datetime import datetime",
        "from pathlib import Path",
        "from typing import Dict, List, Optional, Any",
        "from dataclasses import dataclass, asdict, field",
        "",
        "# ============================================================================",
        "# CONFIGURATION",
        "# ============================================================================",
        "",
        "ML_DATA_DIR = Path(\".git-ml\")",
        "COMMITS_DIR = ML_DATA_DIR / \"commits\"",
        "SESSIONS_DIR = ML_DATA_DIR / \"sessions\"",
        "CHATS_DIR = ML_DATA_DIR / \"chats\"",
        "ACTIONS_DIR = ML_DATA_DIR / \"actions\"",
        "",
        "# Training milestones",
        "MILESTONES = {",
        "    \"file_prediction\": {\"commits\": 500, \"sessions\": 100, \"chats\": 200},",
        "    \"commit_messages\": {\"commits\": 2000, \"sessions\": 500, \"chats\": 1000},",
        "    \"code_suggestions\": {\"commits\": 5000, \"sessions\": 2000, \"chats\": 5000},",
        "}",
        "",
        "# ============================================================================",
        "# DATA SCHEMAS",
        "# ============================================================================",
        "",
        "@dataclass",
        "class DiffHunk:",
        "    \"\"\"A single diff hunk from a commit.\"\"\"",
        "    file: str",
        "    function: Optional[str]  # Function/class containing the change",
        "    change_type: str  # add, modify, delete, rename",
        "    start_line: int",
        "    lines_added: List[str]",
        "    lines_removed: List[str]",
        "    context_before: List[str]",
        "    context_after: List[str]",
        "",
        "",
        "@dataclass",
        "class CommitContext:",
        "    \"\"\"Rich context captured at commit time.\"\"\"",
        "    # Git metadata",
        "    hash: str",
        "    message: str",
        "    author: str",
        "    timestamp: str",
        "    branch: str",
        "",
        "    # Files changed",
        "    files_changed: List[str]",
        "    insertions: int",
        "    deletions: int",
        "",
        "    # Diff structure",
        "    hunks: List[Dict]",
        "",
        "    # Temporal context",
        "    hour_of_day: int",
        "    day_of_week: str",
        "    seconds_since_last_commit: Optional[int]",
        "",
        "    # Session context (if available)",
        "    session_id: Optional[str] = None",
        "    related_chats: List[str] = field(default_factory=list)",
        "",
        "    # Outcome tracking (filled in later)",
        "    ci_result: Optional[str] = None",
        "    reverted: bool = False",
        "    amended: bool = False",
        "",
        "",
        "@dataclass",
        "class ChatEntry:",
        "    \"\"\"A query/response pair from a chat session.\"\"\"",
        "    id: str",
        "    timestamp: str",
        "    session_id: str",
        "",
        "    # The conversation",
        "    query: str",
        "    response: str",
        "",
        "    # Context",
        "    files_referenced: List[str]",
        "    files_modified: List[str]",
        "    tools_used: List[str]",
        "",
        "    # Outcome",
        "    user_feedback: Optional[str] = None  # positive, negative, neutral",
        "    resulted_in_commit: bool = False",
        "    related_commit: Optional[str] = None",
        "",
        "    # Metadata",
        "    query_tokens: int = 0",
        "    response_tokens: int = 0",
        "    duration_seconds: Optional[float] = None",
        "",
        "",
        "@dataclass",
        "class ActionEntry:",
        "    \"\"\"A discrete action taken during development.\"\"\"",
        "    id: str",
        "    timestamp: str",
        "    session_id: str",
        "",
        "    action_type: str  # search, read, edit, test, commit, etc.",
        "    target: str  # file path, query string, etc.",
        "",
        "    # Context",
        "    context: Dict[str, Any] = field(default_factory=dict)",
        "",
        "    # Outcome",
        "    success: bool = True",
        "    result_summary: Optional[str] = None",
        "",
        "",
        "# ============================================================================",
        "# DATA COLLECTION FUNCTIONS",
        "# ============================================================================",
        "",
        "def ensure_dirs():",
        "    \"\"\"Create data directories if they don't exist.\"\"\"",
        "    for dir_path in [COMMITS_DIR, SESSIONS_DIR, CHATS_DIR, ACTIONS_DIR]:",
        "        dir_path.mkdir(parents=True, exist_ok=True)",
        "",
        "",
        "def run_git(args: List[str]) -> str:",
        "    \"\"\"Run a git command and return output.\"\"\"",
        "    result = subprocess.run(",
        "        [\"git\"] + args,",
        "        capture_output=True,",
        "        text=True,",
        "        cwd=str(Path.cwd())",
        "    )",
        "    return result.stdout.strip()",
        "",
        "",
        "def get_last_commit_time() -> Optional[datetime]:",
        "    \"\"\"Get timestamp of the previous commit.\"\"\"",
        "    output = run_git([\"log\", \"-2\", \"--format=%ct\"])",
        "    lines = output.strip().split(\"\\n\")",
        "    if len(lines) >= 2:",
        "        return datetime.fromtimestamp(int(lines[1]))",
        "    return None",
        "",
        "",
        "def parse_diff_hunks(commit_hash: str) -> List[Dict]:",
        "    \"\"\"Parse diff hunks from a commit into structured data.\"\"\"",
        "    diff_output = run_git([\"show\", \"--format=\", \"-U3\", commit_hash])",
        "",
        "    hunks = []",
        "    current_file = None",
        "    current_hunk = None",
        "",
        "    for line in diff_output.split(\"\\n\"):",
        "        # New file",
        "        if line.startswith(\"diff --git\"):",
        "            if current_hunk:",
        "                hunks.append(current_hunk)",
        "            match = re.search(r\"b/(.+)$\", line)",
        "            current_file = match.group(1) if match else \"unknown\"",
        "            current_hunk = None",
        "",
        "        # Hunk header",
        "        elif line.startswith(\"@@\"):",
        "            if current_hunk:",
        "                hunks.append(current_hunk)",
        "",
        "            # Parse line numbers",
        "            match = re.search(r\"@@ -(\\d+)\", line)",
        "            start_line = int(match.group(1)) if match else 0",
        "",
        "            # Extract function context if present",
        "            func_match = re.search(r\"@@ .+ @@ (.+)$\", line)",
        "            function = func_match.group(1).strip() if func_match else None",
        "",
        "            current_hunk = {",
        "                \"file\": current_file,",
        "                \"function\": function,",
        "                \"start_line\": start_line,",
        "                \"lines_added\": [],",
        "                \"lines_removed\": [],",
        "                \"context_before\": [],",
        "                \"context_after\": [],",
        "            }",
        "",
        "        # Diff content",
        "        elif current_hunk is not None:",
        "            if line.startswith(\"+\") and not line.startswith(\"+++\"):",
        "                current_hunk[\"lines_added\"].append(line[1:])",
        "            elif line.startswith(\"-\") and not line.startswith(\"---\"):",
        "                current_hunk[\"lines_removed\"].append(line[1:])",
        "            elif line.startswith(\" \"):",
        "                # Context line",
        "                if not current_hunk[\"lines_added\"] and not current_hunk[\"lines_removed\"]:",
        "                    current_hunk[\"context_before\"].append(line[1:])",
        "                else:",
        "                    current_hunk[\"context_after\"].append(line[1:])",
        "",
        "    if current_hunk:",
        "        hunks.append(current_hunk)",
        "",
        "    # Determine change type for each hunk",
        "    for hunk in hunks:",
        "        if hunk[\"lines_added\"] and not hunk[\"lines_removed\"]:",
        "            hunk[\"change_type\"] = \"add\"",
        "        elif hunk[\"lines_removed\"] and not hunk[\"lines_added\"]:",
        "            hunk[\"change_type\"] = \"delete\"",
        "        else:",
        "            hunk[\"change_type\"] = \"modify\"",
        "",
        "    return hunks",
        "",
        "",
        "def collect_commit_data(commit_hash: Optional[str] = None) -> CommitContext:",
        "    \"\"\"Collect rich context for a commit.\"\"\"",
        "    if commit_hash is None:",
        "        commit_hash = run_git([\"rev-parse\", \"HEAD\"])",
        "",
        "    # Basic metadata",
        "    message = run_git([\"log\", \"-1\", \"--format=%s\", commit_hash])",
        "    author = run_git([\"log\", \"-1\", \"--format=%an\", commit_hash])",
        "    timestamp = run_git([\"log\", \"-1\", \"--format=%ci\", commit_hash])",
        "    branch = run_git([\"rev-parse\", \"--abbrev-ref\", \"HEAD\"])",
        "",
        "    # Files and stats",
        "    files_output = run_git([\"show\", \"--name-only\", \"--format=\", commit_hash])",
        "    files_changed = [f for f in files_output.split(\"\\n\") if f]",
        "",
        "    stats = run_git([\"show\", \"--stat\", \"--format=\", commit_hash])",
        "    insertions = 0",
        "    deletions = 0",
        "    if stats:",
        "        match = re.search(r\"(\\d+) insertion\", stats)",
        "        if match:",
        "            insertions = int(match.group(1))",
        "        match = re.search(r\"(\\d+) deletion\", stats)",
        "        if match:",
        "            deletions = int(match.group(1))",
        "",
        "    # Temporal context",
        "    now = datetime.now()",
        "    last_commit = get_last_commit_time()",
        "    seconds_since = None",
        "    if last_commit:",
        "        seconds_since = int((now - last_commit).total_seconds())",
        "",
        "    # Parse diff hunks",
        "    hunks = parse_diff_hunks(commit_hash)",
        "",
        "    return CommitContext(",
        "        hash=commit_hash,",
        "        message=message,",
        "        author=author,",
        "        timestamp=timestamp,",
        "        branch=branch,",
        "        files_changed=files_changed,",
        "        insertions=insertions,",
        "        deletions=deletions,",
        "        hunks=hunks,",
        "        hour_of_day=now.hour,",
        "        day_of_week=now.strftime(\"%A\"),",
        "        seconds_since_last_commit=seconds_since,",
        "    )",
        "",
        "",
        "def save_commit_data(context: CommitContext):",
        "    \"\"\"Save commit context to disk.\"\"\"",
        "    ensure_dirs()",
        "",
        "    filename = f\"{context.hash[:8]}_{context.timestamp[:10]}.json\"",
        "    filepath = COMMITS_DIR / filename",
        "",
        "    with open(filepath, \"w\") as f:",
        "        json.dump(asdict(context), f, indent=2)",
        "",
        "    print(f\"Saved commit data to {filepath}\")",
        "",
        "",
        "def generate_chat_id() -> str:",
        "    \"\"\"Generate unique chat entry ID.\"\"\"",
        "    timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")",
        "    suffix = hashlib.sha256(str(datetime.now().timestamp()).encode()).hexdigest()[:6]",
        "    return f\"chat-{timestamp}-{suffix}\"",
        "",
        "",
        "def generate_session_id() -> str:",
        "    \"\"\"Generate unique session ID.\"\"\"",
        "    return hashlib.sha256(str(datetime.now().timestamp()).encode()).hexdigest()[:8]",
        "",
        "",
        "def save_chat_entry(entry: ChatEntry):",
        "    \"\"\"Save a chat entry to disk.\"\"\"",
        "    ensure_dirs()",
        "",
        "    # Organize by date",
        "    date_dir = CHATS_DIR / entry.timestamp[:10]",
        "    date_dir.mkdir(exist_ok=True)",
        "",
        "    filename = f\"{entry.id}.json\"",
        "    filepath = date_dir / filename",
        "",
        "    with open(filepath, \"w\") as f:",
        "        json.dump(asdict(entry), f, indent=2)",
        "",
        "    print(f\"Saved chat entry to {filepath}\")",
        "",
        "",
        "def log_chat(",
        "    query: str,",
        "    response: str,",
        "    session_id: Optional[str] = None,",
        "    files_referenced: Optional[List[str]] = None,",
        "    files_modified: Optional[List[str]] = None,",
        "    tools_used: Optional[List[str]] = None,",
        "    user_feedback: Optional[str] = None,",
        ") -> ChatEntry:",
        "    \"\"\"Log a chat query/response pair.\"\"\"",
        "",
        "    entry = ChatEntry(",
        "        id=generate_chat_id(),",
        "        timestamp=datetime.now().isoformat(),",
        "        session_id=session_id or generate_session_id(),",
        "        query=query,",
        "        response=response,",
        "        files_referenced=files_referenced or [],",
        "        files_modified=files_modified or [],",
        "        tools_used=tools_used or [],",
        "        user_feedback=user_feedback,",
        "        query_tokens=len(query.split()),  # Rough estimate",
        "        response_tokens=len(response.split()),",
        "    )",
        "",
        "    save_chat_entry(entry)",
        "    return entry",
        "",
        "",
        "def save_action(entry: ActionEntry):",
        "    \"\"\"Save an action entry to disk.\"\"\"",
        "    ensure_dirs()",
        "",
        "    date_dir = ACTIONS_DIR / entry.timestamp[:10]",
        "    date_dir.mkdir(exist_ok=True)",
        "",
        "    filename = f\"{entry.id}.json\"",
        "    filepath = date_dir / filename",
        "",
        "    with open(filepath, \"w\") as f:",
        "        json.dump(asdict(entry), f, indent=2)",
        "",
        "",
        "def log_action(",
        "    action_type: str,",
        "    target: str,",
        "    session_id: Optional[str] = None,",
        "    context: Optional[Dict] = None,",
        "    success: bool = True,",
        "    result_summary: Optional[str] = None,",
        ") -> ActionEntry:",
        "    \"\"\"Log a discrete action.\"\"\"",
        "",
        "    timestamp = datetime.now()",
        "    action_id = f\"act-{timestamp.strftime('%Y%m%d-%H%M%S')}-{hashlib.sha256(str(timestamp.timestamp()).encode()).hexdigest()[:4]}\"",
        "",
        "    entry = ActionEntry(",
        "        id=action_id,",
        "        timestamp=timestamp.isoformat(),",
        "        session_id=session_id or generate_session_id(),",
        "        action_type=action_type,",
        "        target=target,",
        "        context=context or {},",
        "        success=success,",
        "        result_summary=result_summary,",
        "    )",
        "",
        "    save_action(entry)",
        "    return entry",
        "",
        "",
        "# ============================================================================",
        "# STATISTICS AND ESTIMATION",
        "# ============================================================================",
        "",
        "def count_data() -> Dict[str, int]:",
        "    \"\"\"Count collected data entries.\"\"\"",
        "    ensure_dirs()",
        "",
        "    counts = {",
        "        \"commits\": 0,",
        "        \"chats\": 0,",
        "        \"actions\": 0,",
        "        \"sessions\": 0,",
        "    }",
        "",
        "    # Count commits",
        "    if COMMITS_DIR.exists():",
        "        counts[\"commits\"] = len(list(COMMITS_DIR.glob(\"*.json\")))",
        "",
        "    # Count chats",
        "    if CHATS_DIR.exists():",
        "        counts[\"chats\"] = len(list(CHATS_DIR.glob(\"**/*.json\")))",
        "",
        "    # Count actions",
        "    if ACTIONS_DIR.exists():",
        "        counts[\"actions\"] = len(list(ACTIONS_DIR.glob(\"**/*.json\")))",
        "",
        "    # Count sessions",
        "    if SESSIONS_DIR.exists():",
        "        counts[\"sessions\"] = len(list(SESSIONS_DIR.glob(\"*.json\")))",
        "",
        "    return counts",
        "",
        "",
        "def calculate_data_size() -> Dict[str, int]:",
        "    \"\"\"Calculate total size of collected data.\"\"\"",
        "    ensure_dirs()",
        "",
        "    sizes = {}",
        "    for name, dir_path in [",
        "        (\"commits\", COMMITS_DIR),",
        "        (\"chats\", CHATS_DIR),",
        "        (\"actions\", ACTIONS_DIR),",
        "        (\"sessions\", SESSIONS_DIR),",
        "    ]:",
        "        total = 0",
        "        if dir_path.exists():",
        "            for f in dir_path.glob(\"**/*.json\"):",
        "                total += f.stat().st_size",
        "        sizes[name] = total",
        "",
        "    sizes[\"total\"] = sum(sizes.values())",
        "    return sizes",
        "",
        "",
        "def estimate_progress() -> Dict[str, Dict]:",
        "    \"\"\"Estimate progress toward training milestones.\"\"\"",
        "    counts = count_data()",
        "",
        "    progress = {}",
        "    for milestone, requirements in MILESTONES.items():",
        "        milestone_progress = {}",
        "        for data_type, required in requirements.items():",
        "            current = counts.get(data_type, 0)",
        "            milestone_progress[data_type] = {",
        "                \"current\": current,",
        "                \"required\": required,",
        "                \"percent\": min(100, int(100 * current / required)),",
        "            }",
        "",
        "        # Overall milestone progress (minimum of all types)",
        "        overall = min(p[\"percent\"] for p in milestone_progress.values())",
        "        milestone_progress[\"overall\"] = overall",
        "        progress[milestone] = milestone_progress",
        "",
        "    return progress",
        "",
        "",
        "def print_stats():",
        "    \"\"\"Print collection statistics.\"\"\"",
        "    counts = count_data()",
        "    sizes = calculate_data_size()",
        "    progress = estimate_progress()",
        "",
        "    print(\"\\n\" + \"=\" * 60)",
        "    print(\"ML DATA COLLECTION STATISTICS\")",
        "    print(\"=\" * 60)",
        "",
        "    print(\"\\nðŸ“Š Data Counts:\")",
        "    print(f\"   Commits:  {counts['commits']:,}\")",
        "    print(f\"   Chats:    {counts['chats']:,}\")",
        "    print(f\"   Actions:  {counts['actions']:,}\")",
        "    print(f\"   Sessions: {counts['sessions']:,}\")",
        "",
        "    print(\"\\nðŸ’¾ Data Sizes:\")",
        "    for name, size in sizes.items():",
        "        if size > 1024 * 1024:",
        "            print(f\"   {name.capitalize():10s}: {size / 1024 / 1024:.2f} MB\")",
        "        elif size > 1024:",
        "            print(f\"   {name.capitalize():10s}: {size / 1024:.2f} KB\")",
        "        else:",
        "            print(f\"   {name.capitalize():10s}: {size} bytes\")",
        "",
        "    print(\"\\nðŸŽ¯ Training Milestones:\")",
        "    for milestone, data in progress.items():",
        "        overall = data.pop(\"overall\")",
        "        bar = \"â–ˆ\" * (overall // 5) + \"â–‘\" * (20 - overall // 5)",
        "        print(f\"\\n   {milestone.replace('_', ' ').title()}: [{bar}] {overall}%\")",
        "        for data_type, info in data.items():",
        "            print(f\"      {data_type}: {info['current']}/{info['required']}\")",
        "",
        "    print(\"\\n\" + \"=\" * 60)",
        "",
        "",
        "def estimate_project_size():",
        "    \"\"\"Estimate final project size when all milestones are reached.\"\"\"",
        "    # Current averages",
        "    sizes = calculate_data_size()",
        "    counts = count_data()",
        "",
        "    # Calculate average sizes per entry type",
        "    avg_commit_size = sizes[\"commits\"] / max(1, counts[\"commits\"])",
        "    avg_chat_size = sizes[\"chats\"] / max(1, counts[\"chats\"]) if counts[\"chats\"] > 0 else 2000  # estimate 2KB per chat",
        "    avg_action_size = sizes[\"actions\"] / max(1, counts[\"actions\"]) if counts[\"actions\"] > 0 else 500  # estimate 500B per action",
        "",
        "    # Target for \"code_suggestions\" milestone (the highest)",
        "    target_commits = MILESTONES[\"code_suggestions\"][\"commits\"]",
        "    target_chats = MILESTONES[\"code_suggestions\"][\"chats\"]",
        "    target_actions = target_chats * 10  # Estimate 10 actions per chat",
        "",
        "    estimated_total = (",
        "        target_commits * max(avg_commit_size, 5000) +  # ~5KB per commit if no data",
        "        target_chats * max(avg_chat_size, 2000) +",
        "        target_actions * max(avg_action_size, 500)",
        "    )",
        "",
        "    print(\"\\n\" + \"=\" * 60)",
        "    print(\"PROJECT SIZE ESTIMATE (Full Collection)\")",
        "    print(\"=\" * 60)",
        "",
        "    print(\"\\nðŸ“ˆ Target Data Points:\")",
        "    print(f\"   Commits:  {target_commits:,}\")",
        "    print(f\"   Chats:    {target_chats:,}\")",
        "    print(f\"   Actions:  {target_actions:,} (estimated)\")",
        "",
        "    print(\"\\nðŸ’¾ Estimated Sizes:\")",
        "    print(f\"   Commits data:  {target_commits * max(avg_commit_size, 5000) / 1024 / 1024:.1f} MB\")",
        "    print(f\"   Chats data:    {target_chats * max(avg_chat_size, 2000) / 1024 / 1024:.1f} MB\")",
        "    print(f\"   Actions data:  {target_actions * max(avg_action_size, 500) / 1024 / 1024:.1f} MB\")",
        "    print(f\"   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\")",
        "    print(f\"   TOTAL:         {estimated_total / 1024 / 1024:.1f} MB\")",
        "",
        "    print(\"\\nðŸ§  Model Training Estimates:\")",
        "    print(f\"   Vocabulary size:     ~15,000 tokens (this project)\")",
        "    print(f\"   Training examples:   ~{target_commits + target_chats:,}\")",
        "    print(f\"   Micro-model size:    1-10 MB (1-10M parameters)\")",
        "    print(f\"   Training time:       ~1-4 hours (single GPU)\")",
        "    print(f\"   Inference:           <100ms on CPU\")",
        "",
        "    print(\"\\nâ±ï¸  Time to Collection Complete:\")",
        "    counts = count_data()",
        "    commits_per_day = 20  # Based on current rate",
        "    chats_per_day = 15  # Estimated",
        "",
        "    days_for_commits = (target_commits - counts[\"commits\"]) / commits_per_day",
        "    days_for_chats = (target_chats - counts[\"chats\"]) / chats_per_day",
        "",
        "    days_needed = max(days_for_commits, days_for_chats)",
        "    print(f\"   At current rate:     ~{int(days_needed)} days ({int(days_needed/30)} months)\")",
        "    print(f\"   With active use:     ~{int(days_needed * 0.5)} days (more chatting)\")",
        "",
        "    print(\"\\n\" + \"=\" * 60)",
        "",
        "",
        "# ============================================================================",
        "# GIT HOOKS",
        "# ============================================================================",
        "",
        "POST_COMMIT_HOOK = '''#!/bin/bash",
        "# ML Data Collection - Post-Commit Hook",
        "# Automatically collects enriched commit data for model training",
        "",
        "python scripts/ml_data_collector.py commit",
        "",
        "# Don't block the commit if collection fails",
        "exit 0",
        "'''",
        "",
        "PRE_PUSH_HOOK = '''#!/bin/bash",
        "# ML Data Collection - Pre-Push Hook",
        "# Validates data collection is working before push",
        "",
        "if [ -d \".git-ml/commits\" ]; then",
        "    count=$(ls -1 .git-ml/commits/*.json 2>/dev/null | wc -l)",
        "    echo \"ðŸ“Š ML Data: $count commits collected\"",
        "fi",
        "",
        "exit 0",
        "'''",
        "",
        "",
        "def install_hooks():",
        "    \"\"\"Install git hooks for data collection.\"\"\"",
        "    hooks_dir = Path(\".git/hooks\")",
        "",
        "    # Post-commit hook",
        "    post_commit = hooks_dir / \"post-commit\"",
        "    with open(post_commit, \"w\") as f:",
        "        f.write(POST_COMMIT_HOOK)",
        "    post_commit.chmod(0o755)",
        "    print(f\"âœ“ Installed {post_commit}\")",
        "",
        "    # Pre-push hook",
        "    pre_push = hooks_dir / \"pre-push\"",
        "    with open(pre_push, \"w\") as f:",
        "        f.write(PRE_PUSH_HOOK)",
        "    pre_push.chmod(0o755)",
        "    print(f\"âœ“ Installed {pre_push}\")",
        "",
        "    print(\"\\nGit hooks installed! Commit data will be collected automatically.\")",
        "",
        "",
        "# ============================================================================",
        "# CLI",
        "# ============================================================================",
        "",
        "def main():",
        "    import sys",
        "",
        "    if len(sys.argv) < 2:",
        "        print(__doc__)",
        "        return",
        "",
        "    command = sys.argv[1]",
        "",
        "    if command == \"commit\":",
        "        # Collect data for current or specified commit",
        "        commit_hash = sys.argv[2] if len(sys.argv) > 2 else None",
        "        context = collect_commit_data(commit_hash)",
        "        save_commit_data(context)",
        "",
        "    elif command == \"backfill\":",
        "        # Backfill historical commits",
        "        import argparse",
        "        parser = argparse.ArgumentParser()",
        "        parser.add_argument(\"-n\", \"--num\", type=int, default=100,",
        "                            help=\"Number of commits to backfill\")",
        "        args = parser.parse_args(sys.argv[2:])",
        "",
        "        hashes = run_git([\"log\", f\"-{args.num}\", \"--format=%H\"]).split(\"\\n\")",
        "        hashes = [h for h in hashes if h]",
        "        print(f\"Backfilling {len(hashes)} commits...\")",
        "",
        "        for i, h in enumerate(hashes):",
        "            try:",
        "                context = collect_commit_data(h)",
        "                save_commit_data(context)",
        "                if (i + 1) % 10 == 0:",
        "                    print(f\"  Progress: {i + 1}/{len(hashes)}\")",
        "            except Exception as e:",
        "                print(f\"  Error on {h[:8]}: {e}\")",
        "",
        "        print(f\"Backfill complete: {len(hashes)} commits\")",
        "",
        "    elif command == \"chat\":",
        "        # Log a chat entry",
        "        import argparse",
        "        parser = argparse.ArgumentParser()",
        "        parser.add_argument(\"--query\", required=True)",
        "        parser.add_argument(\"--response\", required=True)",
        "        parser.add_argument(\"--session\", default=None)",
        "        parser.add_argument(\"--files-ref\", nargs=\"*\", default=[])",
        "        parser.add_argument(\"--files-mod\", nargs=\"*\", default=[])",
        "        parser.add_argument(\"--tools\", nargs=\"*\", default=[])",
        "        parser.add_argument(\"--feedback\", choices=[\"positive\", \"negative\", \"neutral\"])",
        "",
        "        args = parser.parse_args(sys.argv[2:])",
        "",
        "        entry = log_chat(",
        "            query=args.query,",
        "            response=args.response,",
        "            session_id=args.session,",
        "            files_referenced=args.files_ref,",
        "            files_modified=args.files_mod,",
        "            tools_used=args.tools,",
        "            user_feedback=args.feedback,",
        "        )",
        "        print(f\"Logged chat: {entry.id}\")",
        "",
        "    elif command == \"action\":",
        "        # Log an action",
        "        import argparse",
        "        parser = argparse.ArgumentParser()",
        "        parser.add_argument(\"--type\", required=True)",
        "        parser.add_argument(\"--target\", required=True)",
        "        parser.add_argument(\"--session\", default=None)",
        "",
        "        args = parser.parse_args(sys.argv[2:])",
        "",
        "        entry = log_action(",
        "            action_type=args.type,",
        "            target=args.target,",
        "            session_id=args.session,",
        "        )",
        "        print(f\"Logged action: {entry.id}\")",
        "",
        "    elif command == \"stats\":",
        "        print_stats()",
        "",
        "    elif command == \"estimate\":",
        "        estimate_project_size()",
        "",
        "    elif command == \"install-hooks\":",
        "        install_hooks()",
        "",
        "    else:",
        "        print(f\"Unknown command: {command}\")",
        "        print(__doc__)",
        "",
        "",
        "if __name__ == \"__main__\":",
        "    main()"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    }
  ],
  "hour_of_day": 1,
  "day_of_week": "Monday",
  "seconds_since_last_commit": -42307,
  "is_merge": false,
  "is_initial": false,
  "parent_count": 1,
  "session_id": null,
  "related_chats": [],
  "ci_result": null,
  "reverted": false,
  "amended": false
}