{
  "hash": "507ad83ade5e7c93c89d1706c61baad9ddc52d2a",
  "message": "Merge main and renumber code review tasks to avoid collision",
  "author": "Claude",
  "timestamp": "2025-12-13 05:54:37 +0000",
  "branch": "claude/multi-index-design-DvifZ",
  "files_changed": [
    ".github/workflows/ci.yml",
    "TASK_ARCHIVE.md",
    "TASK_LIST.md",
    "cortical/__init__.py",
    "cortical/fluent.py",
    "cortical/processor.py",
    "cortical/progress.py",
    "cortical/query/definitions.py",
    "cortical/query/ranking.py",
    "cortical/query/search.py",
    "cortical/results.py",
    "docs/PROGRESS_USAGE.md",
    "examples/demo_progress.py",
    "examples/examples_results_usage.py",
    "pyproject.toml",
    "tests/test_fluent.py",
    "tests/test_progress.py",
    "tests/test_results.py",
    "tests/unit/test_fluent.py",
    "tests/unit/test_progress.py",
    "tests/unit/test_query_definitions.py",
    "tests/unit/test_query_passages.py",
    "tests/unit/test_query_search.py",
    "tests/unit/test_results.py"
  ],
  "insertions": 5118,
  "deletions": 133,
  "hunks": [
    {
      "file": "workflows/ci.yml b/.github/workflows/ci.yml",
      "function": "jobs:",
      "start_line": 234,
      "lines_added": [
        "        # Run pytest-based tests (tests/unit/, tests/smoke/, etc.)",
        "        coverage run --source=cortical -m pytest tests/ -v --ignore=tests/performance/",
        "",
        "        # Append unittest-based legacy tests",
        "        coverage run --append --source=cortical -m unittest discover -s tests -p \"test_*.py\" -v",
        ""
      ],
      "lines_removed": [
        "        # Run full test suite for comprehensive coverage",
        "        coverage run --source=cortical -m unittest discover -s tests -v"
      ],
      "context_before": [
        "",
        "    - name: Download integration coverage",
        "      uses: actions/download-artifact@v4",
        "      with:",
        "        name: coverage-integration",
        "        path: .",
        "",
        "    - name: Generate combined coverage report",
        "      run: |",
        "        echo \"=== Combined Coverage Report ===\""
      ],
      "context_after": [
        "        coverage report -m --include=\"cortical/*\"",
        "        coverage xml -o coverage.xml",
        "",
        "        # Check threshold",
        "        coverage report --fail-under=89 --include=\"cortical/*\"",
        "",
        "    - name: Upload final coverage report",
        "      uses: actions/upload-artifact@v4",
        "      with:",
        "        name: coverage-report"
      ],
      "change_type": "modify"
    },
    {
      "file": "TASK_ARCHIVE.md",
      "function": "Completed tasks moved from TASK_LIST.md. Search here for historical context and",
      "start_line": 76,
      "lines_added": [
        "| 87 | Add Python Code Samples and Showcase | Showcase | 2025-12-13 |",
        "| 88 | Create Package Installation Files | DevEx | 2025-12-11 |",
        "| 89 | Create CONTRIBUTING.md | DevEx | 2025-12-11 |",
        "| 90 | Create docs/quickstart.md | Docs | 2025-12-11 |",
        "| 91 | Create docs/README.md Index | Docs | 2025-12-12 |",
        "| 92 | Add Badges to README.md | DevEx | 2025-12-12 |",
        "| 93 | Update README with Docs References | Docs | 2025-12-12 |",
        "| 94 | Split query.py into Focused Modules | Architecture | 2025-12-12 |",
        "| 96 | Centralize Duplicate Constants | Code Quality | 2025-12-12 |",
        "| 97 | Integrate CorticalConfig into Processor | Architecture | 2025-12-11 |",
        "| 98 | Replace print() with Logging | Code Quality | 2025-12-12 |",
        "| 102 | Add Tests for Edge Cases | Testing | 2025-12-12 |",
        "| 103 | Add Priority Backlog Summary | Task Mgmt | 2025-12-11 |",
        "| 104 | Create TASK_ARCHIVE.md | Task Mgmt | 2025-12-11 |",
        "| 105 | Standardize Task Format | Task Mgmt | 2025-12-11 |",
        "| 109 | Add Recently Completed Section | Task Mgmt | 2025-12-11 |",
        "| 113 | Document Staleness Tracking System | Docs | 2025-12-12 |",
        "| 114 | Add Type Aliases for Complex Types | Code Quality | 2025-12-12 |",
        "| 115 | Create Component Interaction Diagram | Docs | 2025-12-12 |",
        "| 116 | Document Return Value Semantics | Docs | 2025-12-12 |",
        "| 119 | Create AI Metadata Generator Script | DevEx | 2025-12-11 |",
        "| 120 | Add AI Metadata Loader to Claude Skills | DevEx | 2025-12-11 |",
        "| 121 | Auto-regenerate AI Metadata on Changes | DevEx | 2025-12-11 |",
        "| 122 | Investigate Concept Layer Regressions | Research | 2025-12-11 |",
        "| 123 | Replace Label Propagation with Louvain | Algorithm | 2025-12-11 |",
        "| 124 | Add Minimum Cluster Count Tests | Testing | 2025-12-11 |",
        "| 125 | Add Clustering Quality Metrics | Algorithm | 2025-12-11 |",
        "| 126 | Investigate Louvain Resolution | Research | 2025-12-11 |",
        "| 127 | Create Cluster Coverage Evaluation Script | DevEx | 2025-12-11 |",
        "| 128 | Fix Definition Boost Test File Penalty | Bug Fix | 2025-12-11 |",
        "| 132 | Profile Full-Analysis Bottleneck | Performance | 2025-12-11 |",
        "| 136 | Optimize Semantics O(n¬≤) Similarity | Performance | 2025-12-11 |",
        "| 137 | Cap Bigram Connections to Top-K | Performance | 2025-12-12 |",
        "| 138 | Sparse Matrix for Bigram Connections | Performance | 2025-12-12 |",
        "| 139 | Batch Bigram Connection Updates | Performance | 2025-12-12 |",
        "| 141 | Filter Python Keywords from Analysis | Code Quality | 2025-12-12 |",
        "| 142 | Investigate compute_all() Regression | Performance | 2025-12-12 |",
        "| 143 | Investigate Negative Silhouette Score | Research | 2025-12-12 |",
        "| 144 | Boost Exact Document Name Matches | Search | 2025-12-12 |",
        "| 145 | Improve Graph Embedding Quality | Algorithm | 2025-12-12 |",
        "| 146 | Create Behavioral Tests | Testing | 2025-12-12 |",
        "| 147 | Fix Misleading Hardcoded Values | Bug Fix | 2025-12-12 |",
        "| 150 | Create Unit Test Fixtures and Mocks | Testing | 2025-12-12 |",
        "| 153 | Refactor query/* for Unit Testability | Testing | 2025-12-12 |",
        "| 154 | Add Unit Tests for query/* (partial) | Testing | 2025-12-12 |",
        "| 157 | Add Unit Tests for semantics.py | Testing | 2025-12-12 |",
        "| 158 | Add Unit Tests for persistence.py | Testing | 2025-12-12 |",
        "| 159 | Unit Tests for tokenizer.py | Unit Test | 2025-12-13 |",
        "| 160 | Unit Tests for embeddings.py | Unit Test | 2025-12-13 |",
        "| 161 | Unit Tests for layers.py | Unit Test | 2025-12-13 |",
        "| 162 | Unit Tests for minicolumn.py | Unit Test | 2025-12-13 |",
        "| 163 | Unit Tests for fingerprint.py | Unit Test | 2025-12-13 |",
        "| 164 | Unit Tests for gaps.py | Unit Test | 2025-12-13 |",
        "| 165-166 | Unit Tests for processor.py | Unit Test | 2025-12-13 |",
        "| 167 | Unit Tests for chunk_index.py | Unit Test | 2025-12-13 |",
        "| 168 | Unit Tests for config.py | Unit Test | 2025-12-13 |",
        "| 169 | Unit Tests for code_concepts.py | Unit Test | 2025-12-13 |",
        "| 170 | Unit Tests for query/expansion.py | Unit Test | 2025-12-13 |",
        "| 171 | Unit Tests for query/search.py | Unit Test | 2025-12-13 |",
        "| 172 | Unit Tests for query/passages.py | Unit Test | 2025-12-13 |",
        "| 173 | Unit Tests for query/definitions.py | Unit Test | 2025-12-13 |",
        "| 174 | Unit Tests for query/analogy.py | Unit Test | 2025-12-13 |",
        "| 175 | Unit Tests for query/ranking.py | Unit Test | 2025-12-13 |",
        "| 176 | Unit Tests for analysis.py | Unit Test | 2025-12-13 |",
        "| 177 | Unit Tests for semantics.py | Unit Test | 2025-12-13 |",
        "| 178 | Unit Tests for persistence.py | Unit Test | 2025-12-13 |"
      ],
      "lines_removed": [],
      "context_before": [
        "| 70 | Add Performance Timing to Showcase | Showcase | 2025-12-11 |",
        "| 71 | Enable Code-Aware Tokenization in Index | Code Index | 2025-12-11 |",
        "| 72 | Use Programming Query Expansion | Code Index | 2025-12-11 |",
        "| 77 | Add Interactive \"Ask the Codebase\" Mode | DevEx | 2025-12-11 |",
        "| 81 | Fix Tokenizer Underscore Identifiers | Code Search | 2025-12-11 |",
        "| 82 | Add Code Stop Words Filter | Code Search | 2025-12-11 |",
        "| 83 | Add Definition-Aware Boosting | Code Search | 2025-12-11 |",
        "| 84 | Add Direct Definition Pattern Search | Code Search | 2025-12-11 |",
        "| 85 | Improve Test vs Source File Ranking | Code Search | 2025-12-11 |",
        "| 86 | Add Semantic Chunk Boundaries for Code | Code Search | 2025-12-11 |"
      ],
      "context_after": [
        "",
        "---",
        "",
        "## Detailed Task History",
        "",
        "### Task 1: Fix Per-Document TF-IDF Calculation Bug ‚úì",
        "",
        "**File:** `cortical/analysis.py:131`",
        "**Completed:** 2025-12-10",
        ""
      ],
      "change_type": "add"
    },
    {
      "file": "TASK_LIST.md",
      "function": null,
      "start_line": 1,
      "lines_added": [
        "**Pending Tasks:** 45",
        "**Completed Tasks:** 184 (see archive)",
        "| 184 | Implement MCP Server for Claude Desktop integration | Integration | - | Large |",
        "| 192 | Deduplicate lateral_connections and typed_connections storage | Memory | - | Medium |",
        "| 186 | Add simplified facade methods (quick_search, rag_retrieve) | API | - | Small |",
        "| 193 | Unify alpha parameter validation in semantics.py | CodeQual | - | Small |",
        "| 194 | Add validation for invalid layer values in persistence.py load | CodeQual | - | Small |"
      ],
      "lines_removed": [
        "**Pending Tasks:** 47",
        "**Completed Tasks:** 170+ (see archive and Recently Completed)",
        "### üî¥ Critical (Do Now)",
        "",
        "*All critical tasks completed!*",
        "",
        "| 179 | Deduplicate lateral_connections and typed_connections storage | Memory | - | Medium |",
        "| 180 | Unify alpha parameter validation in semantics.py | CodeQual | - | Small |",
        "| 181 | Add validation for invalid layer values in persistence.py load | CodeQual | - | Small |"
      ],
      "context_before": [
        "# Task List: Cortical Text Processor",
        "",
        "Active backlog for the Cortical Text Processor project. Completed tasks are archived in [TASK_ARCHIVE.md](TASK_ARCHIVE.md).",
        "",
        "**Last Updated:** 2025-12-13"
      ],
      "context_after": [
        "",
        "**Unit Test Initiative:** ‚úÖ COMPLETE - 85% coverage from unit tests (1,729 tests)",
        "- 19 modules at 90%+ coverage",
        "- See [Coverage Baseline](#unit-test-coverage-baseline) for per-module status",
        "",
        "---",
        "",
        "## Active Backlog",
        "",
        "<!-- Machine-parseable format for automation -->",
        "",
        "### üü† High (Do This Week)",
        "",
        "| # | Task | Category | Depends | Effort |",
        "|---|------|----------|---------|--------|",
        "| 148 | Investigate test_search_is_fast taking 137s | Testing | - | Medium |",
        "| 149 | Fix test_compute_all_under_threshold failing (135s > 30s) | Testing | - | Medium |",
        "",
        "### üü° Medium (Do This Month)",
        "",
        "| # | Task | Category | Depends | Effort |",
        "|---|------|----------|---------|--------|",
        "| 133 | Implement WAL + snapshot persistence (fault-tolerant rebuild) | Arch | 132 | Large |",
        "| 134 | Implement protobuf serialization for corpus | Arch | 132 | Medium |",
        "| 135 | Implement chunked parallel processing for full-analysis | Arch | 132 | Large |",
        "| 95 | Split processor.py into modules | Arch | 97 | Large |",
        "| 99 | Add input validation to public methods | CodeQual | - | Medium |",
        "| 107 | Add Quick Context to tasks | TaskMgmt | - | Medium |",
        "",
        "### üü¢ Low (Backlog)",
        "",
        "| # | Task | Category | Depends | Effort |",
        "|---|------|----------|---------|--------|",
        "| 73 | Add \"Find Similar Code\" command | DevEx | - | Medium |",
        "| 74 | Add \"Explain This Code\" command | DevEx | - | Medium |",
        "| 75 | Add \"What Changed?\" semantic diff | DevEx | - | Large |",
        "| 76 | Add \"Suggest Related Files\" feature | DevEx | - | Medium |",
        "| 78 | Add code pattern detection | DevEx | - | Large |"
      ],
      "change_type": "modify"
    },
    {
      "file": "TASK_LIST.md",
      "function": "Active backlog for the Cortical Text Processor project. Completed tasks are arch",
      "start_line": 55,
      "lines_added": [
        "| 195 | Import stopwords from tokenizer.py in semantics.py | CodeQual | - | Small |",
        "| 196 | Add runtime warning for spectral embeddings on large graphs | DevEx | - | Small |",
        "| 46 | Standardize return types with dataclasses | Superseded by #185 |",
        "",
        "### üîÆ Future (Async/Advanced)",
        "",
        "| # | Task | Category | Notes |",
        "|---|------|----------|-------|",
        "| 187 | Add async API support (AsyncCorticalTextProcessor) | Architecture | Enables FastAPI, async frameworks |",
        "| 188 | Add streaming query results | Architecture | Depends on #187 |",
        "| 189 | Add observability hooks (timing, traces, metrics) | DevEx | OpenTelemetry integration |",
        "| 190 | Create REST API wrapper (FastAPI) | Integration | Depends on #187 |",
        "| 191 | Add Interactive REPL mode | DevEx | `python -m cortical --interactive` |",
        "*No tasks currently in progress*",
        "",
        "---",
        "",
        "## Recently Completed",
        "",
        "All completed tasks are now archived in [TASK_ARCHIVE.md](TASK_ARCHIVE.md).",
        "",
        "**Latest completions (2025-12-13):**",
        "- #182 Fluent API - FluentProcessor with method chaining (44 tests)",
        "- #183 Progress Feedback - ConsoleProgressReporter, callbacks (30 tests)",
        "- #185 Result Dataclasses - DocumentMatch, PassageMatch, QueryResult (56 tests)",
        "- #179 Fix definition search - line boundary fix in `find_definition_in_text()`",
        "- #180 Fix doc-type boosting - filename pattern + empty metadata fallback",
        "- #181 Fix query ranking - hybrid boost strategy for exact name matches",
        "- Unit Test Coverage Initiative: 1,729 tests, 85% coverage, 19 modules at 90%+",
        "- Tasks #159-178 (unit tests for all modules)",
        "### 184. Implement MCP Server for Claude Desktop Integration",
        "**Meta:** `status:pending` `priority:high` `category:integration`",
        "**Files:** `cortical/mcp_server.py` (new), `mcp_config.json` (new)",
        "**Effort:** Large",
        "**Problem:** AI agents must call subprocess scripts instead of native integration. Claude Desktop users can't access the processor directly.",
        "**Solution:** Create MCP (Model Context Protocol) server with tools:",
        "- `search(query, top_n)` ‚Üí document results",
        "- `passages(query, top_n)` ‚Üí RAG passages",
        "- `expand_query(query)` ‚Üí expansion terms",
        "- `corpus_stats()` ‚Üí statistics",
        "- `add_document(doc_id, content)` ‚Üí index document",
        "**Acceptance:**",
        "- [ ] Works in Claude Desktop",
        "- [ ] 5+ core tools implemented",
        "- [ ] Documentation for installation",
        "- [ ] Example MCP config file",
        "",
        "---",
        "",
        "### 186. Add Simplified Facade Methods",
        "",
        "**Meta:** `status:pending` `priority:medium` `category:api`",
        "**Files:** `cortical/processor.py`",
        "**Effort:** Small",
        "",
        "**Problem:** 80+ public methods; users don't know which to call for common tasks.",
        "",
        "**Solution:** Add purpose-focused facades:",
        "```python",
        "processor.quick_search(query)          # One-call document search",
        "processor.rag_retrieve(query, top_n=3) # Pre-configured for RAG",
        "processor.explore(query)               # With expansion visibility",
        "```",
        "**Acceptance:**",
        "- [ ] 3-4 facade methods added",
        "- [ ] Sensible defaults for each use case",
        "- [ ] Examples in quickstart.md",
        "### 192. Deduplicate lateral_connections and typed_connections storage",
        "### 193. Unify alpha parameter validation in semantics.py",
        "### 194. Add validation for invalid layer values in persistence.py load",
        "### 195. Import stopwords from tokenizer.py in semantics.py",
        "### 196. Add runtime warning for spectral embeddings on large graphs"
      ],
      "lines_removed": [
        "| 182 | Import stopwords from tokenizer.py in semantics.py | CodeQual | - | Small |",
        "| 183 | Add runtime warning for spectral embeddings on large graphs | DevEx | - | Small |",
        "| 46 | Standardize return types with dataclasses | Nice-to-have |",
        "| # | Task | Started | Notes |",
        "|---|------|---------|-------|",
        "| 87 | Add Python code samples and showcase | 2025-12-11 | samples/*.py created |",
        "",
        "",
        "## Recently Completed (Last 7 Days)",
        "",
        "| # | Task | Completed | Notes |",
        "|---|------|-----------|-------|",
        "| 159-178 | **Unit Test Coverage Initiative** | 2025-12-13 | 1,729 tests, 85% overall coverage, 19 modules at 90%+ |",
        "| 159 | Unit tests for tokenizer.py | 2025-12-13 | 77 tests, 99% coverage |",
        "| 160 | Unit tests for embeddings.py | 2025-12-13 | 72 tests, 98% coverage |",
        "| 161 | Unit tests for layers.py | 2025-12-13 | 70 tests, 99% coverage |",
        "| 162 | Unit tests for minicolumn.py | 2025-12-13 | 54 tests, 100% coverage |",
        "| 163 | Unit tests for fingerprint.py | 2025-12-13 | 61 tests, 99% coverage |",
        "| 164 | Unit tests for gaps.py | 2025-12-13 | 49 tests, 98% coverage |",
        "| 165-166 | Unit tests for processor.py | 2025-12-13 | 290 tests, 85% coverage |",
        "| 167 | Unit tests for chunk_index.py | 2025-12-13 | 82 tests, 98% coverage |",
        "| 168 | Unit tests for config.py | 2025-12-13 | 84 tests, 100% coverage |",
        "| 169 | Unit tests for code_concepts.py | 2025-12-13 | 78 tests, 98% coverage |",
        "| 170 | Unit tests for query/expansion.py | 2025-12-13 | 61 tests, 94% coverage |",
        "| 171 | Unit tests for query/search.py | 2025-12-13 | 52 tests, 95% coverage |",
        "| 172 | Unit tests for query/passages.py | 2025-12-13 | 94 tests, 92% coverage |",
        "| 173 | Unit tests for query/definitions.py | 2025-12-13 | 72 tests, 100% coverage |",
        "| 174 | Unit tests for query/analogy.py | 2025-12-13 | 55 tests, 90% coverage |",
        "| 175 | Unit tests for query/ranking.py | 2025-12-13 | 65 tests, 99% coverage |",
        "| 176 | Unit tests for analysis.py | 2025-12-13 | 140 tests, 94% coverage |",
        "| 177 | Unit tests for semantics.py | 2025-12-13 | 99 tests, 91% coverage |",
        "| 178 | Unit tests for persistence.py | 2025-12-13 | 76 tests, 94% coverage |",
        "| 157 | Add unit tests for semantics.py | 2025-12-12 | 48 tests, 24% coverage - superseded by #177 for 90% target |",
        "| 158 | Add unit tests for persistence.py | 2025-12-12 | 36 tests, 18% coverage - superseded by #178 for 90% target |",
        "| 153 | Refactor query/* for unit testability (partial) | 2025-12-12 | intent.py 72%, chunking.py 60% - remaining modules in #170-175 |",
        "| 154 | Add unit tests for query/* (partial) | 2025-12-12 | 67 tests for intent/chunking - remaining modules in #170-175 |",
        "| 150 | Create unit test fixtures and mocks for core data structures | 2025-12-12 | MockMinicolumn, MockHierarchicalLayer, MockLayers factory, LayerBuilder fluent API |",
        "| 138 | Use sparse matrix multiplication for bigram connections | 2025-12-12 | Zero-dep SparseMatrix class in analysis.py for O(n¬≤) ‚Üí O(n*k) improvement |",
        "| 98 | Replace print() with logging | 2025-12-12 | 52+ print statements ‚Üí logging.info(), all modules use getLogger(__name__) |",
        "| 102 | Add tests for edge cases | 2025-12-12 | 53 new tests in test_edge_cases.py: Unicode, large docs, malformed inputs |",
        "| 115 | Create component interaction diagram | 2025-12-12 | docs/architecture.md with ASCII + Mermaid diagrams, module dependencies |",
        "| 147 | Fix misleading hardcoded values | 2025-12-12 | 5 fixes: backwards param names, sparsity threshold, config constant, tolerance param, confidence semantics |",
        "| 139 | Batch bigram connection updates to reduce dict overhead | 2025-12-12 | add_lateral_connections_batch() method in minicolumn.py |",
        "| 137 | Cap bigram connections to top-K per bigram | 2025-12-12 | max_connections_per_bigram parameter (default 50) in analysis.py |",
        "| 116 | Document return value semantics | 2025-12-12 | Edge cases, score ranges, None vs exceptions, default parameters |",
        "| 114 | Add type aliases for complex types | 2025-12-12 | cortical/types.py with 20+ aliases: DocumentScore, PassageResult, SemanticRelation, etc. |",
        "| 113 | Document staleness tracking system | 2025-12-12 | Comprehensive docs in CLAUDE.md: computation types, API, incremental updates |",
        "| 96 | Centralize duplicate constants | 2025-12-12 | cortical/constants.py with RELATION_WEIGHTS, DOC_TYPE_BOOSTS, query keywords |",
        "| 91 | Create docs/README.md index | 2025-12-12 | Navigation by audience, reading paths, categorized docs |",
        "| 92 | Add badges to README.md | 2025-12-12 | Python, License, Tests, Coverage, Zero Dependencies badges |",
        "| 93 | Update README with docs references | 2025-12-12 | Documentation section with table linking to docs/*.md |",
        "| 146 | Create behavioral tests for core user workflows | 2025-12-12 | 11 tests across 4 categories: Search, Performance, Quality, Robustness |",
        "| 145 | Improve graph embedding quality for common terms | 2025-12-12 | Added 'tfidf' method, IDF weighting to 'fast' method |",
        "| 143 | Investigate negative silhouette score in clustering | 2025-12-12 | Expected behavior: modularity ‚â† silhouette (graph vs doc similarity) |",
        "| 142 | Investigate 74s compute_all() performance regression | 2025-12-12 | 5.2x speedup via fast embeddings + sampling (74s ‚Üí 14s) |",
        "| 144 | Boost exact document name matches in search | 2025-12-12 | doc_name_boost parameter in search functions |",
        "| 141 | Filter Python keywords/artifacts from analysis | 2025-12-12 | CODE_NOISE_TOKENS + filter_code_noise tokenizer option |",
        "| 94 | Split query.py into focused modules | 2025-12-12 | 8 modules: expansion, search, passages, chunking, intent, definitions, ranking, analogy |",
        "| 97 | Integrate CorticalConfig into processor | 2025-12-11 | Config stored on processor, used in method defaults, saved/loaded |",
        "| 127 | Create cluster coverage evaluation script | 2025-12-11 | scripts/evaluate_cluster.py with 24 tests |",
        "| 125 | Add clustering quality metrics (modularity, silhouette) | 2025-12-11 | compute_clustering_quality() in analysis.py, showcase display |",
        "| 124 | Add minimum cluster count regression tests | 2025-12-11 | 4 new tests: coherence, showcase count, mega-cluster, distribution |",
        "| 128 | Fix definition boost that favors test mocks over real implementations | 2025-12-11 | Added is_test_file() and test file penalty |",
        "| 132 | Profile full-analysis bottleneck (bigram, semantics O(n¬≤)) | 2025-12-11 | Created profile_full_analysis.py, fixed bottlenecks |",
        "| 136 | Optimize semantics O(n¬≤) similarity with early termination | 2025-12-11 | Added max_similarity_pairs, min_context_keys |",
        "| 126 | Investigate optimal Louvain resolution for sample corpus | 2025-12-11 | Research confirms default 1.0 is optimal |",
        "| 123 | Replace label propagation with Louvain community detection | 2025-12-11 | Implemented Louvain algorithm, 34 clusters for 92 docs |",
        "| 122 | Investigate Concept Layer & Embeddings regressions | 2025-12-11 | Fixed inverted strictness, improved embeddings |",
        "| 119 | Create AI metadata generator script | 2025-12-11 | scripts/generate_ai_metadata.py with tests |",
        "| 120 | Add AI metadata loader to Claude skills | 2025-12-11 | ai-metadata skill created |",
        "| 121 | Auto-regenerate AI metadata on changes | 2025-12-11 | Documented in CLAUDE.md, skills |",
        "| 88 | Create package installation files | 2025-12-11 | pyproject.toml, requirements.txt |",
        "| 89 | Create CONTRIBUTING.md | 2025-12-11 | Contribution guide |",
        "| 90 | Create docs/quickstart.md | 2025-12-11 | 5-minute tutorial |",
        "| 103 | Add Priority Backlog Summary | 2025-12-11 | TASK_LIST.md restructure |",
        "| 104 | Create TASK_ARCHIVE.md | 2025-12-11 | 75+ tasks archived |",
        "| 105 | Standardize task format | 2025-12-11 | Meta tags, effort estimates |",
        "| 109 | Add Recently Completed section | 2025-12-11 | Session context |",
        "| 86 | Add semantic chunk boundaries for code | 2025-12-11 | In query.py |",
        "| 85 | Improve test vs source ranking | 2025-12-11 | DOC_TYPE_BOOSTS |",
        "",
        "*Full details in [TASK_ARCHIVE.md](TASK_ARCHIVE.md)*",
        "### ~~150. Create Unit Test Fixtures and Mocks for Core Data Structures~~ ‚úì COMPLETED",
        "**Meta:** `status:completed` `priority:medium` `category:unit-test`",
        "**Files:** `tests/unit/mocks.py`, `tests/unit/test_mocks.py`",
        "**Completed:** 2025-12-12",
        "**Implementation Summary:**",
        "Created comprehensive test doubles (600+ lines) enabling isolated unit testing:",
        "1. **MockMinicolumn** - Full test double with all controllable attributes",
        "2. **MockHierarchicalLayer** - Supports get_minicolumn(), get_by_id(), column_count(), iteration",
        "3. **MockLayers Factory** - 10 factory methods:",
        "   - `empty()`, `single_term()`, `two_connected_terms()`, `connected_chain()`",
        "   - `complete_graph()`, `disconnected_terms()`, `document_with_terms()`",
        "   - `multi_document_corpus()`, `clustered_terms()`, `with_bigrams()`",
        "4. **LayerBuilder** - Fluent API: `with_term()`, `with_connection()`, `with_document()`, `with_cluster()`",
        "5. **Graph helpers** - `layers_to_graph()`, `layers_to_adjacency()` for algorithm testing",
        "**Verification:** 39 unit tests in test_mocks.py, all passing.",
        "### 179. Deduplicate lateral_connections and typed_connections storage",
        "### 180. Unify alpha parameter validation in semantics.py",
        "### 181. Add validation for invalid layer values in persistence.py load",
        "### 182. Import stopwords from tokenizer.py in semantics.py",
        "### 183. Add runtime warning for spectral embeddings on large graphs"
      ],
      "context_before": [
        "| 100 | Implement plugin/extension registry | Arch | - | Large |",
        "| 101 | Automate staleness tracking | Arch | - | Medium |",
        "| 106 | Add task dependency graph | TaskMgmt | - | Small |",
        "| 108 | Create task selection script | TaskMgmt | - | Medium |",
        "| 117 | Create debugging cookbook | AINav | - | Medium |",
        "| 118 | Add function complexity annotations | AINav | - | Small |",
        "| 140 | Analyze customer service cluster quality | Research | 127 | Small |",
        "| 129 | Test customer service retrieval quality | Testing | - | Small |",
        "| 130 | Expand customer service sample cluster | Samples | - | Medium |",
        "| 131 | Investigate cross-domain semantic bridges | Research | - | Medium |"
      ],
      "context_after": [
        "",
        "### ‚è∏Ô∏è Deferred",
        "",
        "| # | Task | Reason |",
        "|---|------|--------|",
        "| 110 | Add section markers to large files | Superseded by #119 (AI metadata generator) |",
        "| 111 | Add \"See Also\" cross-references | Superseded by #119 (AI metadata generator) |",
        "| 112 | Add docstring examples | Superseded by #119 (AI metadata generator) |",
        "| 7 | Document magic numbers in gaps.py | Low priority, functional as-is |",
        "| 42 | Add simple query language | Nice-to-have, not blocking |",
        "| 44 | Remove deprecated feedforward_sources | Cleanup, low impact |",
        "",
        "### üîÑ In Progress",
        "",
        "",
        "---",
        "",
        "## Pending Task Details",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "---",
        "",
        "",
        "**Meta:** `status:pending` `priority:high` `category:memory`",
        "**Files:** `cortical/minicolumn.py`",
        "",
        "**Problem:**",
        "Every typed connection is duplicated in `lateral_connections` for backward compatibility (`minicolumn.py:209-212`). For large graphs, this doubles memory for edge weights.",
        "",
        "**Options:**",
        "1. Deprecate `lateral_connections` in favor of `typed_connections`",
        "2. Make `lateral_connections` a property that derives from `typed_connections`",
        "3. Keep both but document the trade-off",
        "",
        "**Context from code review (2025-12-13):**",
        "- Found in comprehensive code review of core classes",
        "- Memory concern for large corpora with millions of edges",
        "",
        "---",
        "",
        "",
        "**Meta:** `status:pending` `priority:medium` `category:codequal`",
        "**Files:** `cortical/semantics.py`",
        "",
        "**Problem:**",
        "`retrofit_connections()` allows `alpha=0` but `retrofit_embeddings()` excludes it:",
        "- Line 405-408: `if not (0 <= alpha <= 1):`",
        "- Line 507-508: `if not (0 < alpha <= 1):`",
        "",
        "**Fix:** Either document why the difference exists or unify the validation.",
        "",
        "---",
        "",
        "",
        "**Meta:** `status:pending` `priority:medium` `category:codequal`",
        "**Files:** `cortical/persistence.py`",
        "",
        "**Problem:**",
        "In `load_processor()` (line 97-99), if `level_value` isn't a valid `CorticalLayer` enum value (0-3), an unclear `ValueError` is raised.",
        "",
        "**Fix:** Add explicit validation with helpful error message:",
        "```python",
        "try:",
        "    layer_enum = CorticalLayer(int(level_value))",
        "except ValueError:",
        "    raise ValueError(f\"Invalid layer level in saved state: {level_value}\")",
        "```",
        "",
        "---",
        "",
        "",
        "**Meta:** `status:pending` `priority:low` `category:codequal`",
        "**Files:** `cortical/semantics.py`, `cortical/tokenizer.py`",
        "",
        "**Problem:**",
        "`semantics.py` (lines 144-151) has its own hardcoded stopword set that duplicates `Tokenizer.DEFAULT_STOP_WORDS`. Changes to one won't affect the other.",
        "",
        "**Fix:** Import from `Tokenizer.DEFAULT_STOP_WORDS` or move to `constants.py`.",
        "",
        "---",
        "",
        "",
        "**Meta:** `status:pending` `priority:low` `category:devex`",
        "**Files:** `cortical/embeddings.py`",
        "",
        "**Problem:**",
        "Spectral embeddings are O(n¬≤) but there's no runtime warning when called with large graphs. Users may wait unexpectedly.",
        "",
        "**Fix:** Add warning for large graphs:",
        "```python",
        "if n > 5000:"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/__init__.py",
      "function": "Example:",
      "start_line": 12,
      "lines_added": [
        "from .fluent import FluentProcessor",
        "from .progress import (",
        "    ProgressReporter,",
        "    ConsoleProgressReporter,",
        "    CallbackProgressReporter,",
        "    SilentProgressReporter,",
        "    MultiPhaseProgress,",
        ")",
        "from .results import (",
        "    DocumentMatch,",
        "    PassageMatch,",
        "    QueryResult,",
        "    convert_document_matches,",
        "    convert_passage_matches",
        ")",
        "    \"FluentProcessor\",",
        "    \"ProgressReporter\",",
        "    \"ConsoleProgressReporter\",",
        "    \"CallbackProgressReporter\",",
        "    \"SilentProgressReporter\",",
        "    \"MultiPhaseProgress\",",
        "    \"DocumentMatch\",",
        "    \"PassageMatch\",",
        "    \"QueryResult\",",
        "    \"convert_document_matches\",",
        "    \"convert_passage_matches\","
      ],
      "lines_removed": [],
      "context_before": [
        "    processor.process_document(\"doc1\", \"Neural networks process information...\")",
        "    processor.compute_all()",
        "    results = processor.find_documents_for_query(\"neural processing\")",
        "\"\"\"",
        "",
        "from .tokenizer import Tokenizer",
        "from .minicolumn import Minicolumn, Edge",
        "from .layers import CorticalLayer, HierarchicalLayer",
        "from .processor import CorticalTextProcessor",
        "from .config import CorticalConfig, get_default_config, VALID_RELATION_CHAINS"
      ],
      "context_after": [
        "",
        "__version__ = \"2.0.0\"",
        "__all__ = [",
        "    \"CorticalTextProcessor\",",
        "    \"CorticalConfig\",",
        "    \"CorticalLayer\",",
        "    \"HierarchicalLayer\",",
        "    \"Minicolumn\",",
        "    \"Edge\",",
        "    \"Tokenizer\",",
        "    \"get_default_config\",",
        "    \"VALID_RELATION_CHAINS\",",
        "]"
      ],
      "change_type": "add"
    },
    {
      "file": "cortical/fluent.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "\"\"\"",
        "Fluent API for CorticalTextProcessor - chainable method interface.",
        "",
        "Example:",
        "    from cortical import FluentProcessor",
        "",
        "    # Simple usage",
        "    results = (FluentProcessor()",
        "        .add_document(\"doc1\", \"Neural networks process information\")",
        "        .add_document(\"doc2\", \"Deep learning uses neural architectures\")",
        "        .build()",
        "        .search(\"neural processing\", top_n=5))",
        "",
        "    # From files",
        "    results = (FluentProcessor",
        "        .from_files([\"file1.txt\", \"file2.txt\"])",
        "        .build()",
        "        .search(\"query\"))",
        "",
        "    # Advanced configuration",
        "    processor = (FluentProcessor()",
        "        .add_documents({",
        "            \"doc1\": \"content 1\",",
        "            \"doc2\": \"content 2\"",
        "        })",
        "        .build(verbose=True, build_concepts=True)",
        "        .save(\"corpus.pkl\"))",
        "\"\"\"",
        "",
        "import os",
        "from typing import Dict, List, Tuple, Optional, Any, Union",
        "from pathlib import Path",
        "",
        "from .processor import CorticalTextProcessor",
        "from .tokenizer import Tokenizer",
        "from .config import CorticalConfig",
        "",
        "",
        "class FluentProcessor:",
        "    \"\"\"",
        "    Fluent/chainable API wrapper for CorticalTextProcessor.",
        "",
        "    Provides a builder pattern interface for constructing and querying",
        "    text processors with method chaining.",
        "",
        "    Example:",
        "        >>> processor = (FluentProcessor()",
        "        ...     .add_document(\"doc1\", \"text\")",
        "        ...     .build()",
        "        ...     .search(\"query\"))",
        "    \"\"\"",
        "",
        "    def __init__(",
        "        self,",
        "        tokenizer: Optional[Tokenizer] = None,",
        "        config: Optional[CorticalConfig] = None",
        "    ):",
        "        \"\"\"",
        "        Initialize a fluent processor.",
        "",
        "        Args:",
        "            tokenizer: Optional custom tokenizer",
        "            config: Optional configuration object",
        "        \"\"\"",
        "        self._processor = CorticalTextProcessor(tokenizer=tokenizer, config=config)",
        "        self._is_built = False",
        "",
        "    @classmethod",
        "    def from_existing(cls, processor: CorticalTextProcessor) -> 'FluentProcessor':",
        "        \"\"\"",
        "        Create a FluentProcessor from an existing CorticalTextProcessor.",
        "",
        "        Args:",
        "            processor: Existing CorticalTextProcessor instance",
        "",
        "        Returns:",
        "            FluentProcessor wrapping the existing processor",
        "",
        "        Example:",
        "            >>> proc = CorticalTextProcessor()",
        "            >>> fluent = FluentProcessor.from_existing(proc)",
        "        \"\"\"",
        "        instance = cls.__new__(cls)",
        "        instance._processor = processor",
        "        instance._is_built = False",
        "        return instance",
        "",
        "    @classmethod",
        "    def from_files(",
        "        cls,",
        "        file_paths: List[Union[str, Path]],",
        "        tokenizer: Optional[Tokenizer] = None,",
        "        config: Optional[CorticalConfig] = None",
        "    ) -> 'FluentProcessor':",
        "        \"\"\"",
        "        Create a processor from a list of files.",
        "",
        "        Args:",
        "            file_paths: List of file paths to process",
        "            tokenizer: Optional custom tokenizer",
        "            config: Optional configuration object",
        "",
        "        Returns:",
        "            FluentProcessor with documents added from files",
        "",
        "        Example:",
        "            >>> processor = FluentProcessor.from_files([\"doc1.txt\", \"doc2.txt\"])",
        "        \"\"\"",
        "        instance = cls(tokenizer=tokenizer, config=config)",
        "        for path in file_paths:",
        "            path_obj = Path(path)",
        "            if not path_obj.exists():",
        "                raise FileNotFoundError(f\"File not found: {path}\")",
        "            if not path_obj.is_file():",
        "                raise ValueError(f\"Not a file: {path}\")",
        "",
        "            doc_id = path_obj.stem  # Use filename without extension as doc_id",
        "            with open(path_obj, 'r', encoding='utf-8') as f:",
        "                content = f.read()",
        "",
        "            instance._processor.process_document(doc_id, content, metadata={'source': str(path)})",
        "",
        "        return instance",
        "",
        "    @classmethod",
        "    def from_directory(",
        "        cls,",
        "        directory: Union[str, Path],",
        "        pattern: str = \"*.txt\",",
        "        recursive: bool = False,",
        "        tokenizer: Optional[Tokenizer] = None,",
        "        config: Optional[CorticalConfig] = None",
        "    ) -> 'FluentProcessor':",
        "        \"\"\"",
        "        Create a processor from all files in a directory.",
        "",
        "        Args:",
        "            directory: Directory path to scan",
        "            pattern: Glob pattern for file matching (default: \"*.txt\")",
        "            recursive: Whether to search subdirectories",
        "            tokenizer: Optional custom tokenizer",
        "            config: Optional configuration object",
        "",
        "        Returns:",
        "            FluentProcessor with documents added from directory",
        "",
        "        Example:",
        "            >>> processor = FluentProcessor.from_directory(\"./docs\", pattern=\"*.md\")",
        "        \"\"\"",
        "        dir_path = Path(directory)",
        "        if not dir_path.exists():",
        "            raise FileNotFoundError(f\"Directory not found: {directory}\")",
        "        if not dir_path.is_dir():",
        "            raise ValueError(f\"Not a directory: {directory}\")",
        "",
        "        # Find files matching pattern",
        "        if recursive:",
        "            files = list(dir_path.rglob(pattern))",
        "        else:",
        "            files = list(dir_path.glob(pattern))",
        "",
        "        if not files:",
        "            raise ValueError(f\"No files matching pattern '{pattern}' found in {directory}\")",
        "",
        "        return cls.from_files(files, tokenizer=tokenizer, config=config)",
        "",
        "    @classmethod",
        "    def load(cls, path: Union[str, Path]) -> 'FluentProcessor':",
        "        \"\"\"",
        "        Load a processor from a saved file.",
        "",
        "        Args:",
        "            path: Path to saved processor file",
        "",
        "        Returns:",
        "            FluentProcessor loaded from file",
        "",
        "        Example:",
        "            >>> processor = FluentProcessor.load(\"corpus.pkl\")",
        "        \"\"\"",
        "        loaded = CorticalTextProcessor.load(str(path))",
        "        instance = cls.from_existing(loaded)",
        "        instance._is_built = True  # Loaded processors are already built",
        "        return instance",
        "",
        "    def add_document(",
        "        self,",
        "        doc_id: str,",
        "        content: str,",
        "        metadata: Optional[Dict[str, Any]] = None",
        "    ) -> 'FluentProcessor':",
        "        \"\"\"",
        "        Add a document to the processor (chainable).",
        "",
        "        Args:",
        "            doc_id: Unique document identifier",
        "            content: Document text content",
        "            metadata: Optional metadata dictionary",
        "",
        "        Returns:",
        "            Self for method chaining",
        "",
        "        Example:",
        "            >>> processor = (FluentProcessor()",
        "            ...     .add_document(\"doc1\", \"content\")",
        "            ...     .add_document(\"doc2\", \"more content\"))",
        "        \"\"\"",
        "        self._processor.process_document(doc_id, content, metadata)",
        "        self._is_built = False  # Mark as needing rebuild",
        "        return self",
        "",
        "    def add_documents(",
        "        self,",
        "        documents: Union[Dict[str, str], List[Tuple[str, str]], List[Tuple[str, str, Dict]]]",
        "    ) -> 'FluentProcessor':",
        "        \"\"\"",
        "        Add multiple documents at once (chainable).",
        "",
        "        Args:",
        "            documents: Can be:",
        "                - Dict mapping doc_id -> content",
        "                - List of (doc_id, content) tuples",
        "                - List of (doc_id, content, metadata) tuples",
        "",
        "        Returns:",
        "            Self for method chaining",
        "",
        "        Example:",
        "            >>> # From dict",
        "            >>> processor = FluentProcessor().add_documents({",
        "            ...     \"doc1\": \"content 1\",",
        "            ...     \"doc2\": \"content 2\"",
        "            ... })",
        "",
        "            >>> # From list of tuples",
        "            >>> processor = FluentProcessor().add_documents([",
        "            ...     (\"doc1\", \"content 1\"),",
        "            ...     (\"doc2\", \"content 2\", {\"author\": \"Alice\"})",
        "            ... ])",
        "        \"\"\"",
        "        if isinstance(documents, dict):",
        "            for doc_id, content in documents.items():",
        "                self._processor.process_document(doc_id, content)",
        "        elif isinstance(documents, list):",
        "            for item in documents:",
        "                if len(item) == 2:",
        "                    doc_id, content = item",
        "                    self._processor.process_document(doc_id, content)",
        "                elif len(item) == 3:",
        "                    doc_id, content, metadata = item",
        "                    self._processor.process_document(doc_id, content, metadata)",
        "                else:",
        "                    raise ValueError(f\"Invalid document tuple: {item}. Expected (doc_id, content) or (doc_id, content, metadata)\")",
        "        else:",
        "            raise TypeError(\"documents must be a dict or list of tuples\")",
        "",
        "        self._is_built = False",
        "        return self",
        "",
        "    def with_config(self, config: CorticalConfig) -> 'FluentProcessor':",
        "        \"\"\"",
        "        Set configuration (chainable).",
        "",
        "        Args:",
        "            config: CorticalConfig object",
        "",
        "        Returns:",
        "            Self for method chaining",
        "",
        "        Example:",
        "            >>> from cortical import CorticalConfig",
        "            >>> config = CorticalConfig(min_token_length=2)",
        "            >>> processor = FluentProcessor().with_config(config)",
        "        \"\"\"",
        "        self._processor.config = config",
        "        return self",
        "",
        "    def with_tokenizer(self, tokenizer: Tokenizer) -> 'FluentProcessor':",
        "        \"\"\"",
        "        Set custom tokenizer (chainable).",
        "",
        "        Args:",
        "            tokenizer: Custom Tokenizer instance",
        "",
        "        Returns:",
        "            Self for method chaining",
        "",
        "        Example:",
        "            >>> from cortical import Tokenizer",
        "            >>> tokenizer = Tokenizer(split_identifiers=True)",
        "            >>> processor = FluentProcessor().with_tokenizer(tokenizer)",
        "        \"\"\"",
        "        self._processor.tokenizer = tokenizer",
        "        return self",
        "",
        "    def build(",
        "        self,",
        "        verbose: bool = True,",
        "        build_concepts: bool = True,",
        "        pagerank_method: str = 'standard',",
        "        connection_strategy: str = 'document_overlap',",
        "        cluster_strictness: float = 1.0,",
        "        bridge_weight: float = 0.0,",
        "        show_progress: bool = False",
        "    ) -> 'FluentProcessor':",
        "        \"\"\"",
        "        Build the processor by computing all analysis phases (chainable).",
        "",
        "        This calls compute_all() on the underlying processor to perform:",
        "        - TF-IDF computation",
        "        - PageRank importance",
        "        - Bigram connections",
        "        - Document connections",
        "        - Concept clustering (optional)",
        "",
        "        Args:",
        "            verbose: Print progress messages (deprecated, use show_progress)",
        "            build_concepts: Build concept clusters (Layer 2)",
        "            pagerank_method: 'standard', 'semantic', or 'hierarchical'",
        "            connection_strategy: 'document_overlap', 'semantic', 'embedding', or 'hybrid'",
        "            cluster_strictness: Controls clustering aggressiveness (0.0-1.0)",
        "            bridge_weight: Weight for inter-document token bridging (0.0-1.0)",
        "            show_progress: Show progress bar on console",
        "",
        "        Returns:",
        "            Self for method chaining",
        "",
        "        Example:",
        "            >>> processor = (FluentProcessor()",
        "            ...     .add_document(\"doc1\", \"content\")",
        "            ...     .build(verbose=False))",
        "        \"\"\"",
        "        self._processor.compute_all(",
        "            verbose=verbose,",
        "            build_concepts=build_concepts,",
        "            pagerank_method=pagerank_method,",
        "            connection_strategy=connection_strategy,",
        "            cluster_strictness=cluster_strictness,",
        "            bridge_weight=bridge_weight,",
        "            show_progress=show_progress",
        "        )",
        "        self._is_built = True",
        "        return self",
        "",
        "    def save(self, path: Union[str, Path]) -> 'FluentProcessor':",
        "        \"\"\"",
        "        Save the processor to disk (chainable).",
        "",
        "        Args:",
        "            path: File path to save to",
        "",
        "        Returns:",
        "            Self for method chaining",
        "",
        "        Example:",
        "            >>> processor = (FluentProcessor()",
        "            ...     .add_document(\"doc1\", \"content\")",
        "            ...     .build()",
        "            ...     .save(\"corpus.pkl\"))",
        "        \"\"\"",
        "        self._processor.save(str(path))",
        "        return self",
        "",
        "    # ========== Terminal operations (return results, not self) ==========",
        "",
        "    def search(",
        "        self,",
        "        query: str,",
        "        top_n: int = 5,",
        "        use_expansion: bool = True,",
        "        use_semantic: bool = True",
        "    ) -> List[Tuple[str, float]]:",
        "        \"\"\"",
        "        Search for documents matching the query.",
        "",
        "        Args:",
        "            query: Search query string",
        "            top_n: Number of results to return",
        "            use_expansion: Use query expansion",
        "            use_semantic: Use semantic expansion",
        "",
        "        Returns:",
        "            List of (doc_id, score) tuples sorted by relevance",
        "",
        "        Example:",
        "            >>> results = (FluentProcessor()",
        "            ...     .add_document(\"doc1\", \"neural networks\")",
        "            ...     .build()",
        "            ...     .search(\"neural\", top_n=10))",
        "        \"\"\"",
        "        return self._processor.find_documents_for_query(",
        "            query, top_n=top_n, use_expansion=use_expansion, use_semantic=use_semantic",
        "        )",
        "",
        "    def fast_search(",
        "        self,",
        "        query: str,",
        "        top_n: int = 5,",
        "        candidate_multiplier: int = 3,",
        "        use_code_concepts: bool = True",
        "    ) -> List[Tuple[str, float]]:",
        "        \"\"\"",
        "        Fast document search with pre-filtering.",
        "",
        "        Args:",
        "            query: Search query string",
        "            top_n: Number of results to return",
        "            candidate_multiplier: Candidate pool size multiplier",
        "            use_code_concepts: Use code concept expansion",
        "",
        "        Returns:",
        "            List of (doc_id, score) tuples sorted by relevance",
        "",
        "        Example:",
        "            >>> results = processor.build().fast_search(\"authentication\", top_n=5)",
        "        \"\"\"",
        "        return self._processor.fast_find_documents(",
        "            query, top_n=top_n, candidate_multiplier=candidate_multiplier,",
        "            use_code_concepts=use_code_concepts",
        "        )",
        "",
        "    def search_passages(",
        "        self,",
        "        query: str,",
        "        top_n: int = 5,",
        "        chunk_size: Optional[int] = None,",
        "        overlap: Optional[int] = None,",
        "        use_expansion: bool = True",
        "    ) -> List[Tuple[str, str, int, int, float]]:",
        "        \"\"\"",
        "        Search for passage chunks matching the query.",
        "",
        "        Args:",
        "            query: Search query string",
        "            top_n: Number of passage results",
        "            chunk_size: Token count per chunk (default from config)",
        "            overlap: Token overlap between chunks (default from config)",
        "            use_expansion: Use query expansion",
        "",
        "        Returns:",
        "            List of (doc_id, passage_text, start_pos, end_pos, score) tuples",
        "",
        "        Example:",
        "            >>> passages = processor.build().search_passages(\"neural networks\", top_n=3)",
        "        \"\"\"",
        "        return self._processor.find_passages_for_query(",
        "            query, top_n=top_n, chunk_size=chunk_size,",
        "            overlap=overlap, use_expansion=use_expansion",
        "        )",
        "",
        "    def expand(",
        "        self,",
        "        query: str,",
        "        max_expansions: Optional[int] = None,",
        "        use_variants: bool = True,",
        "        use_code_concepts: bool = False",
        "    ) -> Dict[str, float]:",
        "        \"\"\"",
        "        Expand a query with related terms.",
        "",
        "        Args:",
        "            query: Query string to expand",
        "            max_expansions: Maximum number of expansion terms",
        "            use_variants: Include term variants",
        "            use_code_concepts: Use code concept synonyms",
        "",
        "        Returns:",
        "            Dict mapping terms to expansion weights",
        "",
        "        Example:",
        "            >>> expansions = processor.build().expand(\"neural networks\")",
        "            >>> # {'neural': 1.0, 'networks': 1.0, 'network': 0.8, ...}",
        "        \"\"\"",
        "        return self._processor.expand_query(",
        "            query, max_expansions=max_expansions,",
        "            use_variants=use_variants, use_code_concepts=use_code_concepts",
        "        )",
        "",
        "    # ========== Property access to underlying processor ==========",
        "",
        "    @property",
        "    def processor(self) -> CorticalTextProcessor:",
        "        \"\"\"",
        "        Access the underlying CorticalTextProcessor instance.",
        "",
        "        Returns:",
        "            The wrapped CorticalTextProcessor",
        "",
        "        Example:",
        "            >>> fluent = FluentProcessor().add_document(\"doc1\", \"text\")",
        "            >>> raw_processor = fluent.processor",
        "            >>> raw_processor.compute_importance()",
        "        \"\"\"",
        "        return self._processor",
        "",
        "    @property",
        "    def is_built(self) -> bool:",
        "        \"\"\"",
        "        Check if the processor has been built.",
        "",
        "        Returns:",
        "            True if build() has been called",
        "        \"\"\"",
        "        return self._is_built",
        "",
        "    def __repr__(self) -> str:",
        "        \"\"\"String representation.\"\"\"",
        "        doc_count = len(self._processor.documents)",
        "        status = \"built\" if self._is_built else \"not built\"",
        "        return f\"FluentProcessor(documents={doc_count}, status={status})\""
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "cortical/processor.py",
      "function": "from .tokenizer import Tokenizer",
      "start_line": 13,
      "lines_added": [
        "from .progress import (",
        "    ProgressReporter,",
        "    ConsoleProgressReporter,",
        "    CallbackProgressReporter,",
        "    SilentProgressReporter,",
        "    MultiPhaseProgress",
        ")"
      ],
      "lines_removed": [],
      "context_before": [
        "from .minicolumn import Minicolumn",
        "from .layers import CorticalLayer, HierarchicalLayer",
        "from .config import CorticalConfig",
        "from . import analysis",
        "from . import semantics",
        "from . import embeddings as emb_module",
        "from . import query as query_module",
        "from . import gaps as gaps_module",
        "from . import persistence",
        "from . import fingerprint as fp_module"
      ],
      "context_after": [
        "",
        "logger = logging.getLogger(__name__)",
        "",
        "",
        "class CorticalTextProcessor:",
        "    \"\"\"Neocortex-inspired text processing system.\"\"\"",
        "",
        "    # Computation types for tracking staleness",
        "    COMP_TFIDF = 'tfidf'",
        "    COMP_PAGERANK = 'pagerank'"
      ],
      "change_type": "add"
    },
    {
      "file": "cortical/processor.py",
      "function": "class CorticalTextProcessor:",
      "start_line": 626,
      "lines_added": [
        "        bridge_weight: float = 0.0,",
        "        progress_callback: Optional[ProgressReporter] = None,",
        "        show_progress: bool = False",
        "            verbose: Print progress messages (deprecated, use show_progress)",
        "            progress_callback: Optional ProgressReporter for custom progress tracking",
        "            show_progress: Show progress bar on console (uses stderr)",
        "            >>> # Default behavior (silent)",
        "            >>> # With console progress bar",
        "            >>> processor.compute_all(show_progress=True)",
        "            >>>",
        "            >>> # With custom callback",
        "            >>> processor.compute_all(",
        "            ...     progress_callback=CallbackProgressReporter(",
        "            ...         lambda phase, pct, msg: print(f\"{phase}: {pct}%\")",
        "            ...     )",
        "            ... )",
        "            >>>",
        "        # Set up progress reporter",
        "        if progress_callback:",
        "            reporter = progress_callback",
        "        elif show_progress:",
        "            reporter = ConsoleProgressReporter()",
        "        else:",
        "            reporter = SilentProgressReporter()",
        "",
        "        # Define phase weights based on typical execution times",
        "        # These are estimates and may vary based on corpus size",
        "        phase_weights = {",
        "            \"Activation propagation\": 5,",
        "            \"PageRank computation\": 10,",
        "            \"TF-IDF computation\": 15,",
        "            \"Document connections\": 10,",
        "            \"Bigram connections\": 30,",
        "        }",
        "",
        "        # Add concept-related phases if building concepts",
        "        if build_concepts:",
        "            phase_weights[\"Concept clustering\"] = 15",
        "            if connection_strategy in ('semantic', 'hybrid'):",
        "                phase_weights[\"Semantic extraction\"] = 10",
        "            if connection_strategy in ('embedding', 'hybrid'):",
        "                phase_weights[\"Graph embeddings\"] = 10",
        "            phase_weights[\"Concept connections\"] = 15",
        "",
        "        # Create multi-phase progress tracker",
        "        progress = MultiPhaseProgress(reporter, phase_weights)",
        "",
        "        # Phase 1: Activation propagation",
        "        progress.start_phase(\"Activation propagation\")",
        "        progress.update(100)",
        "        progress.complete_phase()",
        "        # Phase 2: PageRank (varies by method)",
        "        progress.start_phase(\"PageRank computation\")",
        "                progress.update(30, \"Extracting semantic relations\")",
        "            progress.update(70, \"Computing semantic PageRank\")",
        "            progress.update(50, \"Computing hierarchical PageRank\")",
        "            progress.update(50, \"Computing PageRank\")",
        "        progress.update(100)",
        "        progress.complete_phase()",
        "",
        "        # Phase 3: TF-IDF",
        "        progress.start_phase(\"TF-IDF computation\")",
        "        progress.update(100)",
        "        progress.complete_phase()",
        "",
        "        # Phase 4: Document connections",
        "        progress.start_phase(\"Document connections\")",
        "        progress.update(100)",
        "        progress.complete_phase()",
        "",
        "        # Phase 5: Bigram connections",
        "        progress.start_phase(\"Bigram connections\")",
        "        progress.update(100)",
        "        progress.complete_phase()",
        "            # Phase 6: Concept clustering",
        "            progress.start_phase(\"Concept clustering\")",
        "            progress.update(100)",
        "            progress.complete_phase()",
        "            # Phase 7: Semantic extraction (if needed)",
        "                progress.start_phase(\"Semantic extraction\")",
        "                progress.update(100)",
        "                progress.complete_phase()",
        "            # Phase 8: Graph embeddings (if needed)",
        "                progress.start_phase(\"Graph embeddings\")",
        "                progress.update(100)",
        "                progress.complete_phase()",
        "            # Phase 9: Concept connections",
        "            progress.start_phase(\"Concept connections\")",
        "            progress.update(100)",
        "            progress.complete_phase()"
      ],
      "lines_removed": [
        "        bridge_weight: float = 0.0",
        "            verbose: Print progress messages",
        "            >>> # Default behavior",
        "            # For semantic/embedding strategies, extract/compute prerequisites"
      ],
      "context_before": [
        "",
        "        return recomputed",
        "",
        "    def compute_all(",
        "        self,",
        "        verbose: bool = True,",
        "        build_concepts: bool = True,",
        "        pagerank_method: str = 'standard',",
        "        connection_strategy: str = 'document_overlap',",
        "        cluster_strictness: float = 1.0,"
      ],
      "context_after": [
        "    ) -> Dict[str, Any]:",
        "        \"\"\"",
        "        Run all computation steps.",
        "",
        "        Args:",
        "            build_concepts: Build concept clusters in Layer 2 (default True)",
        "                           This enables topic-based filtering and hierarchical search.",
        "            pagerank_method: PageRank algorithm to use:",
        "                - 'standard': Traditional PageRank using connection weights",
        "                - 'semantic': ConceptNet-style PageRank with relation type weighting.",
        "                              Requires semantic relations (extracts automatically if needed).",
        "                - 'hierarchical': Cross-layer PageRank with importance propagation",
        "                                  between layers (tokens ‚Üî bigrams ‚Üî concepts ‚Üî documents).",
        "            connection_strategy: Strategy for connecting Layer 2 concepts:",
        "                - 'document_overlap': Traditional Jaccard similarity (default)",
        "                - 'semantic': Connect via semantic relations between members",
        "                - 'embedding': Connect via embedding centroid similarity",
        "                - 'hybrid': Combine all three strategies for maximum connectivity",
        "            cluster_strictness: Controls clustering aggressiveness (0.0-1.0).",
        "                Lower values create fewer, larger clusters with more connections.",
        "            bridge_weight: Weight for inter-document token bridging (0.0-1.0).",
        "                Higher values help bridge topic-isolated clusters.",
        "",
        "        Returns:",
        "            Dict with computation statistics (concept_stats, etc.)",
        "",
        "        Example:",
        "            >>> processor.compute_all()",
        "            >>>",
        "            >>> # Maximum connectivity for diverse documents",
        "            >>> processor.compute_all(",
        "            ...     connection_strategy='hybrid',",
        "            ...     cluster_strictness=0.5,",
        "            ...     bridge_weight=0.3",
        "            ... )",
        "        \"\"\"",
        "        stats: Dict[str, Any] = {}",
        "",
        "        if verbose:",
        "            logger.info(\"Computing activation propagation...\")",
        "        self.propagate_activation(verbose=False)",
        "",
        "        if pagerank_method == 'semantic':",
        "            # Extract semantic relations if not already done",
        "            if not self.semantic_relations:",
        "                if verbose:",
        "                    logger.info(\"Extracting semantic relations...\")",
        "                self.extract_corpus_semantics(verbose=False)",
        "            if verbose:",
        "                logger.info(\"Computing importance (Semantic PageRank)...\")",
        "            self.compute_semantic_importance(verbose=False)",
        "        elif pagerank_method == 'hierarchical':",
        "            if verbose:",
        "                logger.info(\"Computing importance (Hierarchical PageRank)...\")",
        "            self.compute_hierarchical_importance(verbose=False)",
        "        else:",
        "            if verbose:",
        "                logger.info(\"Computing importance (PageRank)...\")",
        "            self.compute_importance(verbose=False)",
        "        if verbose:",
        "            logger.info(\"Computing TF-IDF...\")",
        "        self.compute_tfidf(verbose=False)",
        "        if verbose:",
        "            logger.info(\"Computing document connections...\")",
        "        self.compute_document_connections(verbose=False)",
        "        if verbose:",
        "            logger.info(\"Computing bigram connections...\")",
        "        self.compute_bigram_connections(verbose=False)",
        "",
        "        if build_concepts:",
        "            if verbose:",
        "                logger.info(\"Building concept clusters...\")",
        "            clusters = self.build_concept_clusters(",
        "                cluster_strictness=cluster_strictness,",
        "                bridge_weight=bridge_weight,",
        "                verbose=False",
        "            )",
        "            stats['clusters_created'] = len(clusters)",
        "",
        "            # Determine connection parameters based on strategy",
        "            use_member_semantics = connection_strategy in ('semantic', 'hybrid')",
        "            use_embedding_similarity = connection_strategy in ('embedding', 'hybrid')",
        "",
        "            if use_member_semantics and not self.semantic_relations:",
        "                if verbose:",
        "                    logger.info(\"Extracting semantic relations...\")",
        "                self.extract_corpus_semantics(verbose=False)",
        "",
        "            if use_embedding_similarity and not self.embeddings:",
        "                if verbose:",
        "                    logger.info(\"Computing graph embeddings...\")",
        "                self.compute_graph_embeddings(verbose=False)",
        "",
        "            # Set thresholds based on strategy",
        "            if connection_strategy == 'hybrid':",
        "                min_shared_docs = 0",
        "                min_jaccard = 0.0",
        "            elif connection_strategy in ('semantic', 'embedding'):",
        "                min_shared_docs = 0",
        "                min_jaccard = 0.0",
        "            else:  # document_overlap",
        "                min_shared_docs = 1",
        "                min_jaccard = 0.1",
        "",
        "            if verbose:",
        "                logger.info(f\"Computing concept connections ({connection_strategy})...\")",
        "            concept_stats = self.compute_concept_connections(",
        "                use_member_semantics=use_member_semantics,",
        "                use_embedding_similarity=use_embedding_similarity,",
        "                min_shared_docs=min_shared_docs,",
        "                min_jaccard=min_jaccard,",
        "                verbose=False",
        "            )",
        "            stats['concept_connections'] = concept_stats",
        "",
        "        # Mark core computations as fresh",
        "        fresh_comps = [",
        "            self.COMP_ACTIVATION,",
        "            self.COMP_PAGERANK,",
        "            self.COMP_TFIDF,",
        "            self.COMP_DOC_CONNECTIONS,",
        "            self.COMP_BIGRAM_CONNECTIONS,",
        "        ]",
        "        if build_concepts:"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/progress.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "\"\"\"",
        "Progress reporting infrastructure for long-running operations.",
        "",
        "This module provides a flexible progress reporting system that supports:",
        "- Console output with nice formatting",
        "- Custom callbacks for integration with UIs",
        "- Optional ETA estimation",
        "- Phase-based progress tracking",
        "\"\"\"",
        "",
        "import sys",
        "import time",
        "from typing import Protocol, Optional, Callable, Dict, Any",
        "from abc import ABC, abstractmethod",
        "",
        "",
        "class ProgressReporter(Protocol):",
        "    \"\"\"Protocol for progress reporters.",
        "",
        "    Implementations must provide update() and complete() methods.",
        "    \"\"\"",
        "",
        "    def update(self, phase: str, percent: float, message: Optional[str] = None) -> None:",
        "        \"\"\"",
        "        Update progress for a specific phase.",
        "",
        "        Args:",
        "            phase: Name of the current phase (e.g., \"Computing TF-IDF\")",
        "            percent: Progress percentage (0.0 to 100.0)",
        "            message: Optional additional message to display",
        "        \"\"\"",
        "        ...",
        "",
        "    def complete(self, phase: str, message: Optional[str] = None) -> None:",
        "        \"\"\"",
        "        Mark a phase as complete.",
        "",
        "        Args:",
        "            phase: Name of the completed phase",
        "            message: Optional completion message",
        "        \"\"\"",
        "        ...",
        "",
        "",
        "class ConsoleProgressReporter:",
        "    \"\"\"",
        "    Console-based progress reporter with nice formatting.",
        "",
        "    Displays progress with in-place updates using carriage returns.",
        "",
        "    Example output:",
        "        Computing TF-IDF... [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà----] 75% (ETA: 5s)",
        "    \"\"\"",
        "",
        "    def __init__(",
        "        self,",
        "        file=None,",
        "        width: int = 40,",
        "        show_eta: bool = True,",
        "        use_unicode: bool = True",
        "    ):",
        "        \"\"\"",
        "        Initialize console progress reporter.",
        "",
        "        Args:",
        "            file: Output file (default: sys.stderr)",
        "            width: Width of progress bar in characters",
        "            show_eta: Whether to show estimated time remaining",
        "            use_unicode: Use Unicode block characters for progress bar",
        "        \"\"\"",
        "        self.file = file or sys.stderr",
        "        self.width = width",
        "        self.show_eta = show_eta",
        "        self.use_unicode = use_unicode",
        "",
        "        # Tracking for ETA calculation",
        "        self._phase_start_times: Dict[str, float] = {}",
        "        self._last_phase: Optional[str] = None",
        "",
        "        # Characters for progress bar",
        "        if use_unicode:",
        "            self.fill_char = '‚ñà'",
        "            self.empty_char = '‚ñë'",
        "        else:",
        "            self.fill_char = '#'",
        "            self.empty_char = '-'",
        "",
        "    def update(self, phase: str, percent: float, message: Optional[str] = None) -> None:",
        "        \"\"\"",
        "        Update progress display.",
        "",
        "        Args:",
        "            phase: Name of the current phase",
        "            percent: Progress percentage (0.0 to 100.0)",
        "            message: Optional additional message",
        "        \"\"\"",
        "        # Track phase start time for ETA",
        "        if phase != self._last_phase:",
        "            self._phase_start_times[phase] = time.time()",
        "            self._last_phase = phase",
        "",
        "        # Clamp percentage",
        "        percent = max(0.0, min(100.0, percent))",
        "",
        "        # Build progress bar",
        "        filled = int(self.width * percent / 100.0)",
        "        bar = self.fill_char * filled + self.empty_char * (self.width - filled)",
        "",
        "        # Build status line",
        "        status = f\"\\r{phase}... [{bar}] {percent:.0f}%\"",
        "",
        "        # Add ETA if enabled",
        "        if self.show_eta and percent > 0 and percent < 100:",
        "            eta = self._estimate_eta(phase, percent)",
        "            if eta is not None:",
        "                status += f\" (ETA: {eta:.0f}s)\"",
        "",
        "        # Add custom message if provided",
        "        if message:",
        "            status += f\" - {message}\"",
        "",
        "        # Write with carriage return for in-place update",
        "        self.file.write(status)",
        "        self.file.flush()",
        "",
        "    def complete(self, phase: str, message: Optional[str] = None) -> None:",
        "        \"\"\"",
        "        Mark phase as complete and move to new line.",
        "",
        "        Args:",
        "            phase: Name of the completed phase",
        "            message: Optional completion message",
        "        \"\"\"",
        "        # Show 100% complete",
        "        bar = self.fill_char * self.width",
        "        status = f\"\\r{phase}... [{bar}] 100%\"",
        "",
        "        # Add elapsed time",
        "        if phase in self._phase_start_times:",
        "            elapsed = time.time() - self._phase_start_times[phase]",
        "            status += f\" ({elapsed:.1f}s)\"",
        "",
        "        # Add custom message if provided",
        "        if message:",
        "            status += f\" - {message}\"",
        "",
        "        # Write final status and newline",
        "        self.file.write(status + \"\\n\")",
        "        self.file.flush()",
        "",
        "        # Clean up tracking",
        "        self._phase_start_times.pop(phase, None)",
        "",
        "    def _estimate_eta(self, phase: str, percent: float) -> Optional[float]:",
        "        \"\"\"",
        "        Estimate time remaining for current phase.",
        "",
        "        Args:",
        "            phase: Current phase name",
        "            percent: Current progress percentage",
        "",
        "        Returns:",
        "            Estimated seconds remaining, or None if not calculable",
        "        \"\"\"",
        "        if phase not in self._phase_start_times or percent <= 0:",
        "            return None",
        "",
        "        elapsed = time.time() - self._phase_start_times[phase]",
        "        if elapsed < 1.0:  # Wait at least 1 second for reasonable estimate",
        "            return None",
        "",
        "        # Linear extrapolation",
        "        total_estimated = elapsed / (percent / 100.0)",
        "        remaining = total_estimated - elapsed",
        "",
        "        return max(0.0, remaining)",
        "",
        "",
        "class CallbackProgressReporter:",
        "    \"\"\"",
        "    Progress reporter that calls a custom callback function.",
        "",
        "    Useful for integrating with UI frameworks, logging systems, etc.",
        "",
        "    Example:",
        "        >>> def my_callback(phase, percent, message):",
        "        ...     print(f\"{phase}: {percent}% - {message}\")",
        "        >>> reporter = CallbackProgressReporter(my_callback)",
        "        >>> reporter.update(\"Processing\", 50.0, \"halfway done\")",
        "        Processing: 50.0% - halfway done",
        "    \"\"\"",
        "",
        "    def __init__(self, callback: Callable[[str, float, Optional[str]], None]):",
        "        \"\"\"",
        "        Initialize callback-based progress reporter.",
        "",
        "        Args:",
        "            callback: Function to call with (phase, percent, message) arguments",
        "        \"\"\"",
        "        self.callback = callback",
        "",
        "    def update(self, phase: str, percent: float, message: Optional[str] = None) -> None:",
        "        \"\"\"",
        "        Call callback with progress update.",
        "",
        "        Args:",
        "            phase: Name of the current phase",
        "            percent: Progress percentage (0.0 to 100.0)",
        "            message: Optional additional message",
        "        \"\"\"",
        "        self.callback(phase, percent, message)",
        "",
        "    def complete(self, phase: str, message: Optional[str] = None) -> None:",
        "        \"\"\"",
        "        Call callback with completion notification.",
        "",
        "        Args:",
        "            phase: Name of the completed phase",
        "            message: Optional completion message",
        "        \"\"\"",
        "        self.callback(phase, 100.0, message or \"Complete\")",
        "",
        "",
        "class SilentProgressReporter:",
        "    \"\"\"",
        "    No-op progress reporter for silent operation.",
        "",
        "    Used as default when no progress reporting is needed.",
        "    \"\"\"",
        "",
        "    def update(self, phase: str, percent: float, message: Optional[str] = None) -> None:",
        "        \"\"\"Do nothing.\"\"\"",
        "        pass",
        "",
        "    def complete(self, phase: str, message: Optional[str] = None) -> None:",
        "        \"\"\"Do nothing.\"\"\"",
        "        pass",
        "",
        "",
        "class MultiPhaseProgress:",
        "    \"\"\"",
        "    Helper for tracking progress across multiple sequential phases.",
        "",
        "    Automatically calculates overall percentage based on phase weights.",
        "",
        "    Example:",
        "        >>> phases = {",
        "        ...     \"Phase 1\": 30,  # 30% of total time",
        "        ...     \"Phase 2\": 50,  # 50% of total time",
        "        ...     \"Phase 3\": 20   # 20% of total time",
        "        ... }",
        "        >>> progress = MultiPhaseProgress(reporter, phases)",
        "        >>> progress.start_phase(\"Phase 1\")",
        "        >>> progress.update(50)  # 50% of Phase 1 = 15% overall",
        "        >>> progress.complete_phase()",
        "        >>> progress.start_phase(\"Phase 2\")",
        "        >>> progress.update(100)  # 100% of Phase 2 = 80% overall",
        "    \"\"\"",
        "",
        "    def __init__(",
        "        self,",
        "        reporter: ProgressReporter,",
        "        phases: Dict[str, float],",
        "        normalize: bool = True",
        "    ):",
        "        \"\"\"",
        "        Initialize multi-phase progress tracker.",
        "",
        "        Args:",
        "            reporter: Progress reporter to use",
        "            phases: Dict mapping phase names to relative weights",
        "            normalize: Whether to normalize weights to sum to 100",
        "        \"\"\"",
        "        self.reporter = reporter",
        "        self.phases = phases.copy()",
        "",
        "        # Normalize weights if requested",
        "        if normalize:",
        "            total = sum(phases.values())",
        "            if total > 0:",
        "                self.phases = {k: v / total * 100 for k, v in phases.items()}",
        "",
        "        # Calculate cumulative offsets for each phase",
        "        self._phase_offsets: Dict[str, float] = {}",
        "        cumulative = 0.0",
        "        for phase, weight in self.phases.items():",
        "            self._phase_offsets[phase] = cumulative",
        "            cumulative += weight",
        "",
        "        self._current_phase: Optional[str] = None",
        "        self._overall_progress: float = 0.0",
        "",
        "    def start_phase(self, phase: str) -> None:",
        "        \"\"\"",
        "        Start a new phase.",
        "",
        "        Args:",
        "            phase: Name of the phase to start",
        "",
        "        Raises:",
        "            ValueError: If phase name is not in the configured phases",
        "        \"\"\"",
        "        if phase not in self.phases:",
        "            raise ValueError(f\"Unknown phase: {phase}\")",
        "",
        "        self._current_phase = phase",
        "        self._overall_progress = self._phase_offsets[phase]",
        "        self.reporter.update(phase, 0.0)",
        "",
        "    def update(self, percent: float, message: Optional[str] = None) -> None:",
        "        \"\"\"",
        "        Update progress within current phase.",
        "",
        "        Args:",
        "            percent: Progress percentage within current phase (0-100)",
        "            message: Optional status message",
        "        \"\"\"",
        "        if self._current_phase is None:",
        "            return",
        "",
        "        phase_weight = self.phases[self._current_phase]",
        "        phase_offset = self._phase_offsets[self._current_phase]",
        "",
        "        # Calculate overall progress",
        "        self._overall_progress = phase_offset + (percent / 100.0 * phase_weight)",
        "",
        "        self.reporter.update(",
        "            self._current_phase,",
        "            percent,",
        "            message",
        "        )",
        "",
        "    def complete_phase(self, message: Optional[str] = None) -> None:",
        "        \"\"\"",
        "        Mark current phase as complete.",
        "",
        "        Args:",
        "            message: Optional completion message",
        "        \"\"\"",
        "        if self._current_phase is None:",
        "            return",
        "",
        "        self.reporter.complete(self._current_phase, message)",
        "        self._current_phase = None",
        "",
        "    @property",
        "    def overall_progress(self) -> float:",
        "        \"\"\"Get overall progress across all phases (0-100).\"\"\"",
        "        return self._overall_progress"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "cortical/query/definitions.py",
      "function": "def find_definition_in_text(",
      "start_line": 110,
      "lines_added": [
        "            # Find the start of the line containing the definition",
        "            # This ensures the passage starts with the actual definition line",
        "            line_start = text.rfind('\\n', 0, match.start())",
        "            if line_start == -1:",
        "                # Match is on the first line of the text",
        "                start = 0",
        "            else:",
        "                # Start from the character after the newline",
        "                start = line_start + 1",
        "",
        "            # Extract context after the definition"
      ],
      "lines_removed": [
        "            # Extract context around the definition",
        "            start = max(0, match.start() - 50)  # Small lead-in for context"
      ],
      "context_before": [
        "            DEFINITION_SOURCE_PATTERNS['python_method'],",
        "            DEFINITION_SOURCE_PATTERNS['javascript_function'],",
        "            DEFINITION_SOURCE_PATTERNS['javascript_const_fn'],",
        "        ]",
        "",
        "    # Try each pattern",
        "    for pattern_template in patterns_to_try:",
        "        pattern = pattern_template.format(name=re.escape(identifier))",
        "        match = re.search(pattern, text, re.MULTILINE | re.IGNORECASE)",
        "        if match:"
      ],
      "context_after": [
        "            end = min(len(text), match.end() + context_chars)",
        "",
        "            # Try to extend to next blank line or class/function boundary",
        "            remaining = text[match.end():end]",
        "            # Look for a good boundary (blank line followed by non-indented text)",
        "            boundary_match = re.search(r'\\n\\n(?=[^\\s])', remaining)",
        "            if boundary_match:",
        "                end = match.end() + boundary_match.end()",
        "",
        "            passage = text[start:end]"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/query/ranking.py",
      "function": "def get_doc_type_boost(",
      "start_line": 67,
      "lines_added": [
        "    # If we have metadata with doc_type, use it",
        "    if doc_metadata and doc_id in doc_metadata and 'doc_type' in doc_metadata[doc_id]:",
        "        doc_type = doc_metadata[doc_id]['doc_type']",
        "    elif doc_id.startswith('tests/') or 'test' in doc_id.lower():",
        "        # Check both tests/ directory and files with 'test' in name"
      ],
      "lines_removed": [
        "    # If we have metadata, use doc_type",
        "    if doc_metadata and doc_id in doc_metadata:",
        "        doc_type = doc_metadata[doc_id].get('doc_type', 'code')",
        "    elif doc_id.startswith('tests/'):"
      ],
      "context_before": [
        "    Args:",
        "        doc_id: Document ID",
        "        doc_metadata: Optional metadata dict {doc_id: {doc_type: ..., ...}}",
        "        custom_boosts: Optional custom boost factors",
        "",
        "    Returns:",
        "        Boost factor (1.0 = no boost)",
        "    \"\"\"",
        "    boosts = custom_boosts or DOC_TYPE_BOOSTS",
        ""
      ],
      "context_after": [
        "        return boosts.get(doc_type, 1.0)",
        "",
        "    # Fallback: infer from doc_id path",
        "    if doc_id.endswith('.md'):",
        "        if doc_id.startswith('docs/'):",
        "            return boosts.get('docs', 1.5)",
        "        return boosts.get('root_docs', 1.3)",
        "        return boosts.get('test', 0.8)",
        "    return boosts.get('code', 1.0)",
        "",
        "",
        "def apply_doc_type_boost(",
        "    results: List[Tuple[str, float]],",
        "    doc_metadata: Optional[Dict[str, Dict[str, Any]]] = None,",
        "    boost_docs: bool = True,",
        "    custom_boosts: Optional[Dict[str, float]] = None",
        ") -> List[Tuple[str, float]]:"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/query/search.py",
      "function": "def find_documents_for_query(",
      "start_line": 61,
      "lines_added": [
        "    if doc_name_boost > 1.0 and doc_scores:",
        "        max_score = max(doc_scores.values()) if doc_scores else 0.0",
        "",
        "        # First pass: identify exact and partial matches",
        "        exact_matches = []",
        "        partial_matches = []",
        "",
        "",
        "                if match_ratio == 1.0:",
        "                    exact_matches.append(doc_id)",
        "                else:",
        "                    partial_matches.append((doc_id, match_ratio))",
        "",
        "        # Apply boosts:",
        "        # - Exact matches: ensure they rank above all non-exact matches",
        "        # - Partial matches: proportional boost",
        "        for doc_id in exact_matches:",
        "            # For exact matches, add max_score to ensure they rank first",
        "            # This guarantees exact match beats all other documents",
        "            doc_scores[doc_id] += max_score * doc_name_boost",
        "",
        "        for doc_id, match_ratio in partial_matches:",
        "            # Partial matches use proportional boost",
        "            boost = 1 + (doc_name_boost - 1) * match_ratio",
        "            doc_scores[doc_id] *= boost"
      ],
      "lines_removed": [
        "    if doc_name_boost > 1.0:",
        "                # Boost proportional to match ratio",
        "                boost = 1 + (doc_name_boost - 1) * match_ratio",
        "                doc_scores[doc_id] *= boost"
      ],
      "context_before": [
        "    doc_scores: Dict[str, float] = defaultdict(float)",
        "",
        "    for term, term_weight in query_terms.items():",
        "        col = layer0.get_minicolumn(term)",
        "        if col:",
        "            for doc_id in col.document_ids:",
        "                tfidf = col.tfidf_per_doc.get(doc_id, col.tfidf)",
        "                doc_scores[doc_id] += tfidf * term_weight",
        "",
        "    # Boost documents whose name matches query terms"
      ],
      "context_after": [
        "        query_tokens = set(tokenizer.tokenize(query_text))",
        "        for doc_id in doc_scores:",
        "            # Tokenize document ID (handle underscores as separators)",
        "            doc_name_tokens = set(tokenizer.tokenize(doc_id.replace('_', ' ')))",
        "            # Count how many query tokens appear in doc name",
        "            matches = len(query_tokens & doc_name_tokens)",
        "            if matches > 0:",
        "                match_ratio = matches / len(query_tokens) if query_tokens else 0",
        "",
        "    sorted_docs = sorted(doc_scores.items(), key=lambda x: -x[1])",
        "    return sorted_docs[:top_n]",
        "",
        "",
        "def fast_find_documents(",
        "    query_text: str,",
        "    layers: Dict[CorticalLayer, HierarchicalLayer],",
        "    tokenizer: Tokenizer,",
        "    top_n: int = 5,"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/query/search.py",
      "function": "def fast_find_documents(",
      "start_line": 139,
      "lines_added": [
        "    # Add documents whose names match query terms to candidates",
        "    # This ensures exact name matches are considered even if content doesn't match",
        "    if doc_name_boost > 1.0:",
        "        layer3 = layers.get(CorticalLayer.DOCUMENTS)",
        "        if layer3:",
        "            for doc_col in layer3.minicolumns.values():",
        "                doc_id = doc_col.content",
        "                doc_name_tokens = set(tokenizer.tokenize(doc_id.replace('_', ' ')))",
        "                matches = len(query_tokens & doc_name_tokens)",
        "                if matches > 0:",
        "                    # Ensure name-matching docs are in candidates",
        "                    # High initial score to prioritize them",
        "                    if doc_id not in candidate_docs:",
        "                        candidate_docs[doc_id] = matches * 2",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "    # If no candidates, try code concept expansion for recall",
        "    if not candidate_docs and use_code_concepts:",
        "        for token in tokens:",
        "            related = get_related_terms(token, max_terms=3)",
        "            for related_term in related:",
        "                col = layer0.get_minicolumn(related_term)",
        "                if col:",
        "                    for doc_id in col.document_ids:",
        "                        candidate_docs[doc_id] += 0.5  # Lower weight for expansion",
        ""
      ],
      "context_after": [
        "    if not candidate_docs:",
        "        return []",
        "",
        "    # Rank candidates by match count first (fast pre-filter)",
        "    sorted_candidates = sorted(",
        "        candidate_docs.items(),",
        "        key=lambda x: x[1],",
        "        reverse=True",
        "    )",
        ""
      ],
      "change_type": "add"
    },
    {
      "file": "cortical/query/search.py",
      "function": "def fast_find_documents(",
      "start_line": 168,
      "lines_added": [
        "        doc_scores[doc_id] = score",
        "",
        "    # Apply document name boost after all scores calculated",
        "    if doc_name_boost > 1.0 and doc_scores:",
        "        max_score = max(doc_scores.values())",
        "        exact_matches = []",
        "        partial_matches = []",
        "",
        "        for doc_id in doc_scores:",
        "                if match_ratio == 1.0:",
        "                    exact_matches.append(doc_id)",
        "                else:",
        "                    partial_matches.append((doc_id, match_ratio))",
        "",
        "        # Exact matches get additive boost to ensure top ranking",
        "        for doc_id in exact_matches:",
        "            doc_scores[doc_id] += max_score * doc_name_boost",
        "",
        "        # Partial matches get multiplicative boost",
        "        for doc_id, match_ratio in partial_matches:",
        "            boost = 1 + (doc_name_boost - 1) * match_ratio",
        "            doc_scores[doc_id] *= boost"
      ],
      "lines_removed": [
        "        # Boost documents whose name matches query terms",
        "        if doc_name_boost > 1.0:",
        "                boost = 1 + (doc_name_boost - 1) * match_ratio",
        "                score *= boost",
        "        doc_scores[doc_id] = score"
      ],
      "context_before": [
        "        for token in tokens:",
        "            col = layer0.get_minicolumn(token)",
        "            if col and doc_id in col.document_ids:",
        "                tfidf = col.tfidf_per_doc.get(doc_id, col.tfidf)",
        "                score += tfidf",
        "",
        "        # Boost by match coverage",
        "        coverage_boost = match_count / len(tokens)",
        "        score *= (1 + 0.5 * coverage_boost)",
        ""
      ],
      "context_after": [
        "            doc_name_tokens = set(tokenizer.tokenize(doc_id.replace('_', ' ')))",
        "            matches = len(query_tokens & doc_name_tokens)",
        "            if matches > 0:",
        "                match_ratio = matches / len(query_tokens)",
        "",
        "",
        "    # Return top results",
        "    sorted_docs = sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)",
        "    return sorted_docs[:top_n]",
        "",
        "",
        "def build_document_index(",
        "    layers: Dict[CorticalLayer, HierarchicalLayer]",
        ") -> Dict[str, Dict[str, float]]:",
        "    \"\"\""
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/results.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "\"\"\"",
        "Result Dataclasses for Cortical Text Processor",
        "===============================================",
        "",
        "Strongly-typed result containers for query operations that provide",
        "IDE autocomplete and type checking support.",
        "",
        "Example:",
        "    # Document search results",
        "    matches = processor.find_documents_for_query(\"neural networks\")",
        "    document_matches = [DocumentMatch.from_tuple(doc_id, score)",
        "                        for doc_id, score in matches]",
        "    for match in document_matches:",
        "        print(f\"{match.doc_id}: {match.score:.3f}\")",
        "",
        "    # Passage retrieval results",
        "    passages = processor.find_passages_for_query(\"PageRank algorithm\")",
        "    passage_matches = [PassageMatch.from_tuple(*p) for p in passages]",
        "    for match in passage_matches:",
        "        print(f\"[{match.doc_id}:{match.start}-{match.end}] {match.text[:50]}...\")",
        "\"\"\"",
        "",
        "from dataclasses import dataclass, field, asdict",
        "from typing import Dict, List, Any, Optional, Union",
        "",
        "",
        "@dataclass(frozen=True)",
        "class DocumentMatch:",
        "    \"\"\"",
        "    A document search result with relevance score.",
        "",
        "    Attributes:",
        "        doc_id: Document identifier",
        "        score: Relevance score (higher is more relevant)",
        "        metadata: Optional metadata dict for additional information",
        "",
        "    Example:",
        "        >>> match = DocumentMatch(\"doc1.txt\", 0.95)",
        "        >>> print(match.doc_id)",
        "        'doc1.txt'",
        "        >>> print(f\"Score: {match.score:.2f}\")",
        "        'Score: 0.95'",
        "        >>> match_dict = match.to_dict()",
        "    \"\"\"",
        "    doc_id: str",
        "    score: float",
        "    metadata: Optional[Dict[str, Any]] = None",
        "",
        "    def __repr__(self) -> str:",
        "        \"\"\"Pretty string representation.\"\"\"",
        "        if self.metadata:",
        "            return f\"DocumentMatch(doc_id='{self.doc_id}', score={self.score:.4f}, metadata={self.metadata})\"",
        "        return f\"DocumentMatch(doc_id='{self.doc_id}', score={self.score:.4f})\"",
        "",
        "    def to_dict(self) -> Dict[str, Any]:",
        "        \"\"\"",
        "        Convert to dictionary.",
        "",
        "        Returns:",
        "            Dictionary with doc_id, score, and metadata fields",
        "",
        "        Example:",
        "            >>> match = DocumentMatch(\"doc1\", 0.8)",
        "            >>> match.to_dict()",
        "            {'doc_id': 'doc1', 'score': 0.8, 'metadata': None}",
        "        \"\"\"",
        "        return asdict(self)",
        "",
        "    def to_tuple(self) -> tuple:",
        "        \"\"\"",
        "        Convert to tuple format (doc_id, score).",
        "",
        "        Returns:",
        "            Tuple of (doc_id, score) for compatibility with legacy code",
        "",
        "        Example:",
        "            >>> match = DocumentMatch(\"doc1\", 0.8)",
        "            >>> match.to_tuple()",
        "            ('doc1', 0.8)",
        "        \"\"\"",
        "        return (self.doc_id, self.score)",
        "",
        "    @classmethod",
        "    def from_tuple(cls, doc_id: str, score: float, metadata: Optional[Dict[str, Any]] = None) -> 'DocumentMatch':",
        "        \"\"\"",
        "        Create from tuple format (doc_id, score).",
        "",
        "        Args:",
        "            doc_id: Document identifier",
        "            score: Relevance score",
        "            metadata: Optional metadata dict",
        "",
        "        Returns:",
        "            DocumentMatch instance",
        "",
        "        Example:",
        "            >>> match = DocumentMatch.from_tuple(\"doc1\", 0.8)",
        "            >>> match.doc_id",
        "            'doc1'",
        "        \"\"\"",
        "        return cls(doc_id=doc_id, score=score, metadata=metadata)",
        "",
        "    @classmethod",
        "    def from_dict(cls, data: Dict[str, Any]) -> 'DocumentMatch':",
        "        \"\"\"",
        "        Create from dictionary.",
        "",
        "        Args:",
        "            data: Dictionary with doc_id, score, and optional metadata fields",
        "",
        "        Returns:",
        "            DocumentMatch instance",
        "",
        "        Example:",
        "            >>> data = {'doc_id': 'doc1', 'score': 0.8}",
        "            >>> match = DocumentMatch.from_dict(data)",
        "            >>> match.score",
        "            0.8",
        "        \"\"\"",
        "        return cls(",
        "            doc_id=data['doc_id'],",
        "            score=data['score'],",
        "            metadata=data.get('metadata')",
        "        )",
        "",
        "",
        "@dataclass(frozen=True)",
        "class PassageMatch:",
        "    \"\"\"",
        "    A passage retrieval result with text, location, and relevance score.",
        "",
        "    Suitable for RAG (Retrieval-Augmented Generation) systems where you need",
        "    actual text passages with position information for citations.",
        "",
        "    Attributes:",
        "        doc_id: Document identifier",
        "        text: Passage text content",
        "        score: Relevance score (higher is more relevant)",
        "        start: Start character position in document",
        "        end: End character position in document",
        "        metadata: Optional metadata dict for additional information",
        "",
        "    Example:",
        "        >>> match = PassageMatch(",
        "        ...     doc_id=\"doc1.py\",",
        "        ...     text=\"def compute_pagerank():\\\\n    ...\",",
        "        ...     score=0.92,",
        "        ...     start=100,",
        "        ...     end=150",
        "        ... )",
        "        >>> print(f\"[{match.doc_id}:{match.start}-{match.end}]\")",
        "        '[doc1.py:100-150]'",
        "        >>> print(match.text[:30])",
        "        'def compute_pagerank():\\n    ...'",
        "    \"\"\"",
        "    doc_id: str",
        "    text: str",
        "    score: float",
        "    start: int",
        "    end: int",
        "    metadata: Optional[Dict[str, Any]] = None",
        "",
        "    def __repr__(self) -> str:",
        "        \"\"\"Pretty string representation with truncated text.\"\"\"",
        "        text_preview = self.text[:50] + \"...\" if len(self.text) > 50 else self.text",
        "        text_preview = text_preview.replace('\\n', '\\\\n')",
        "        if self.metadata:",
        "            return (f\"PassageMatch(doc_id='{self.doc_id}', text='{text_preview}', \"",
        "                   f\"score={self.score:.4f}, start={self.start}, end={self.end}, \"",
        "                   f\"metadata={self.metadata})\")",
        "        return (f\"PassageMatch(doc_id='{self.doc_id}', text='{text_preview}', \"",
        "               f\"score={self.score:.4f}, start={self.start}, end={self.end})\")",
        "",
        "    def to_dict(self) -> Dict[str, Any]:",
        "        \"\"\"",
        "        Convert to dictionary.",
        "",
        "        Returns:",
        "            Dictionary with all fields",
        "",
        "        Example:",
        "            >>> match = PassageMatch(\"doc1\", \"text here\", 0.9, 0, 9)",
        "            >>> match.to_dict()",
        "            {'doc_id': 'doc1', 'text': 'text here', 'score': 0.9, 'start': 0, 'end': 9, 'metadata': None}",
        "        \"\"\"",
        "        return asdict(self)",
        "",
        "    def to_tuple(self) -> tuple:",
        "        \"\"\"",
        "        Convert to tuple format (doc_id, text, start, end, score).",
        "",
        "        Returns:",
        "            Tuple for compatibility with legacy code",
        "",
        "        Example:",
        "            >>> match = PassageMatch(\"doc1\", \"text\", 0.8, 0, 4)",
        "            >>> match.to_tuple()",
        "            ('doc1', 'text', 0, 4, 0.8)",
        "        \"\"\"",
        "        return (self.doc_id, self.text, self.start, self.end, self.score)",
        "",
        "    @property",
        "    def location(self) -> str:",
        "        \"\"\"",
        "        Get citation-style location string.",
        "",
        "        Returns:",
        "            Location in format \"doc_id:start-end\"",
        "",
        "        Example:",
        "            >>> match = PassageMatch(\"doc1.py\", \"text\", 0.8, 100, 150)",
        "            >>> match.location",
        "            'doc1.py:100-150'",
        "        \"\"\"",
        "        return f\"{self.doc_id}:{self.start}-{self.end}\"",
        "",
        "    @property",
        "    def length(self) -> int:",
        "        \"\"\"",
        "        Get passage length in characters.",
        "",
        "        Returns:",
        "            Number of characters in passage",
        "",
        "        Example:",
        "            >>> match = PassageMatch(\"doc1\", \"hello\", 0.8, 0, 5)",
        "            >>> match.length",
        "            5",
        "        \"\"\"",
        "        return self.end - self.start",
        "",
        "    @classmethod",
        "    def from_tuple(",
        "        cls,",
        "        doc_id: str,",
        "        text: str,",
        "        start: int,",
        "        end: int,",
        "        score: float,",
        "        metadata: Optional[Dict[str, Any]] = None",
        "    ) -> 'PassageMatch':",
        "        \"\"\"",
        "        Create from tuple format (doc_id, text, start, end, score).",
        "",
        "        Args:",
        "            doc_id: Document identifier",
        "            text: Passage text",
        "            start: Start character position",
        "            end: End character position",
        "            score: Relevance score",
        "            metadata: Optional metadata dict",
        "",
        "        Returns:",
        "            PassageMatch instance",
        "",
        "        Example:",
        "            >>> match = PassageMatch.from_tuple(\"doc1\", \"hello\", 0, 5, 0.9)",
        "            >>> match.text",
        "            'hello'",
        "        \"\"\"",
        "        return cls(",
        "            doc_id=doc_id,",
        "            text=text,",
        "            score=score,",
        "            start=start,",
        "            end=end,",
        "            metadata=metadata",
        "        )",
        "",
        "    @classmethod",
        "    def from_dict(cls, data: Dict[str, Any]) -> 'PassageMatch':",
        "        \"\"\"",
        "        Create from dictionary.",
        "",
        "        Args:",
        "            data: Dictionary with required fields",
        "",
        "        Returns:",
        "            PassageMatch instance",
        "",
        "        Example:",
        "            >>> data = {'doc_id': 'doc1', 'text': 'hi', 'score': 0.8, 'start': 0, 'end': 2}",
        "            >>> match = PassageMatch.from_dict(data)",
        "            >>> match.length",
        "            2",
        "        \"\"\"",
        "        return cls(",
        "            doc_id=data['doc_id'],",
        "            text=data['text'],",
        "            score=data['score'],",
        "            start=data['start'],",
        "            end=data['end'],",
        "            metadata=data.get('metadata')",
        "        )",
        "",
        "",
        "@dataclass(frozen=True)",
        "class QueryResult:",
        "    \"\"\"",
        "    Complete query result with matches and metadata.",
        "",
        "    Wraps search results with additional context like query expansion terms",
        "    and timing information. Useful for analyzing search quality and debugging.",
        "",
        "    Attributes:",
        "        query: Original query text",
        "        matches: List of DocumentMatch or PassageMatch results",
        "        expansion_terms: Optional dict of expanded terms and their weights",
        "        timing_ms: Optional query execution time in milliseconds",
        "        metadata: Optional metadata dict for additional information",
        "",
        "    Example:",
        "        >>> doc_matches = [DocumentMatch(\"doc1\", 0.9), DocumentMatch(\"doc2\", 0.7)]",
        "        >>> result = QueryResult(",
        "        ...     query=\"neural networks\",",
        "        ...     matches=doc_matches,",
        "        ...     expansion_terms={\"neural\": 1.0, \"network\": 0.8, \"deep\": 0.5},",
        "        ...     timing_ms=15.3",
        "        ... )",
        "        >>> print(f\"Found {len(result.matches)} matches in {result.timing_ms}ms\")",
        "        'Found 2 matches in 15.3ms'",
        "        >>> result.top_match",
        "        DocumentMatch(doc_id='doc1', score=0.9000)",
        "    \"\"\"",
        "    query: str",
        "    matches: Union[List[DocumentMatch], List[PassageMatch]]",
        "    expansion_terms: Optional[Dict[str, float]] = None",
        "    timing_ms: Optional[float] = None",
        "    metadata: Optional[Dict[str, Any]] = None",
        "",
        "    def __repr__(self) -> str:",
        "        \"\"\"Pretty string representation.\"\"\"",
        "        match_type = \"DocumentMatch\" if self.matches and isinstance(self.matches[0], DocumentMatch) else \"PassageMatch\"",
        "        return (f\"QueryResult(query='{self.query}', \"",
        "               f\"matches={len(self.matches)} x {match_type}, \"",
        "               f\"expansion_terms={len(self.expansion_terms) if self.expansion_terms else 0}, \"",
        "               f\"timing_ms={self.timing_ms})\")",
        "",
        "    def to_dict(self) -> Dict[str, Any]:",
        "        \"\"\"",
        "        Convert to dictionary with nested match dicts.",
        "",
        "        Returns:",
        "            Dictionary representation",
        "",
        "        Example:",
        "            >>> result = QueryResult(\"test\", [DocumentMatch(\"doc1\", 0.9)])",
        "            >>> d = result.to_dict()",
        "            >>> d['query']",
        "            'test'",
        "        \"\"\"",
        "        return {",
        "            'query': self.query,",
        "            'matches': [m.to_dict() for m in self.matches],",
        "            'expansion_terms': self.expansion_terms,",
        "            'timing_ms': self.timing_ms,",
        "            'metadata': self.metadata",
        "        }",
        "",
        "    @property",
        "    def top_match(self) -> Union[DocumentMatch, PassageMatch, None]:",
        "        \"\"\"",
        "        Get the highest-scoring match.",
        "",
        "        Returns:",
        "            Top match or None if no matches",
        "",
        "        Example:",
        "            >>> result = QueryResult(\"test\", [DocumentMatch(\"doc1\", 0.5), DocumentMatch(\"doc2\", 0.9)])",
        "            >>> result.top_match.doc_id",
        "            'doc2'",
        "        \"\"\"",
        "        if not self.matches:",
        "            return None",
        "        return max(self.matches, key=lambda m: m.score)",
        "",
        "    @property",
        "    def match_count(self) -> int:",
        "        \"\"\"",
        "        Get number of matches.",
        "",
        "        Returns:",
        "            Count of matches",
        "",
        "        Example:",
        "            >>> result = QueryResult(\"test\", [DocumentMatch(\"doc1\", 0.9)])",
        "            >>> result.match_count",
        "            1",
        "        \"\"\"",
        "        return len(self.matches)",
        "",
        "    @property",
        "    def average_score(self) -> float:",
        "        \"\"\"",
        "        Get average relevance score across all matches.",
        "",
        "        Returns:",
        "            Average score or 0.0 if no matches",
        "",
        "        Example:",
        "            >>> result = QueryResult(\"test\", [DocumentMatch(\"doc1\", 0.8), DocumentMatch(\"doc2\", 0.6)])",
        "            >>> result.average_score",
        "            0.7",
        "        \"\"\"",
        "        if not self.matches:",
        "            return 0.0",
        "        return sum(m.score for m in self.matches) / len(self.matches)",
        "",
        "    @classmethod",
        "    def from_dict(cls, data: Dict[str, Any]) -> 'QueryResult':",
        "        \"\"\"",
        "        Create from dictionary.",
        "",
        "        Args:",
        "            data: Dictionary with query, matches, and optional fields",
        "",
        "        Returns:",
        "            QueryResult instance",
        "",
        "        Example:",
        "            >>> data = {",
        "            ...     'query': 'test',",
        "            ...     'matches': [{'doc_id': 'doc1', 'score': 0.9, 'metadata': None}],",
        "            ...     'expansion_terms': {'test': 1.0},",
        "            ...     'timing_ms': 10.0",
        "            ... }",
        "            >>> result = QueryResult.from_dict(data)",
        "            >>> result.query",
        "            'test'",
        "        \"\"\"",
        "        # Determine match type from first match",
        "        matches = []",
        "        if data['matches']:",
        "            first_match = data['matches'][0]",
        "            if 'text' in first_match:",
        "                matches = [PassageMatch.from_dict(m) for m in data['matches']]",
        "            else:",
        "                matches = [DocumentMatch.from_dict(m) for m in data['matches']]",
        "",
        "        return cls(",
        "            query=data['query'],",
        "            matches=matches,",
        "            expansion_terms=data.get('expansion_terms'),",
        "            timing_ms=data.get('timing_ms'),",
        "            metadata=data.get('metadata')",
        "        )",
        "",
        "",
        "# Helper functions for batch conversions",
        "def convert_document_matches(",
        "    results: List[tuple],",
        "    metadata: Optional[Dict[str, Dict[str, Any]]] = None",
        ") -> List[DocumentMatch]:",
        "    \"\"\"",
        "    Convert list of (doc_id, score) tuples to DocumentMatch objects.",
        "",
        "    Args:",
        "        results: List of (doc_id, score) tuples",
        "        metadata: Optional dict mapping doc_id to metadata dict",
        "",
        "    Returns:",
        "        List of DocumentMatch objects",
        "",
        "    Example:",
        "        >>> results = [(\"doc1\", 0.9), (\"doc2\", 0.7)]",
        "        >>> matches = convert_document_matches(results)",
        "        >>> matches[0].doc_id",
        "        'doc1'",
        "    \"\"\"",
        "    if metadata:",
        "        return [DocumentMatch(doc_id, score, metadata.get(doc_id))",
        "                for doc_id, score in results]",
        "    return [DocumentMatch(doc_id, score) for doc_id, score in results]",
        "",
        "",
        "def convert_passage_matches(",
        "    results: List[tuple],",
        "    metadata: Optional[Dict[str, Dict[str, Any]]] = None",
        ") -> List[PassageMatch]:",
        "    \"\"\"",
        "    Convert list of (doc_id, text, start, end, score) tuples to PassageMatch objects.",
        "",
        "    Args:",
        "        results: List of (doc_id, text, start, end, score) tuples",
        "        metadata: Optional dict mapping doc_id to metadata dict",
        "",
        "    Returns:",
        "        List of PassageMatch objects",
        "",
        "    Example:",
        "        >>> results = [(\"doc1\", \"text here\", 0, 9, 0.9)]",
        "        >>> matches = convert_passage_matches(results)",
        "        >>> matches[0].text",
        "        'text here'",
        "    \"\"\"",
        "    if metadata:",
        "        return [PassageMatch(doc_id, text, score, start, end, metadata.get(doc_id))",
        "                for doc_id, text, start, end, score in results]",
        "    return [PassageMatch(doc_id, text, score, start, end)",
        "            for doc_id, text, start, end, score in results]"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "docs/PROGRESS_USAGE.md",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "# Progress Reporting Usage Guide",
        "",
        "## Overview",
        "",
        "The Cortical Text Processor now supports progress reporting during long-running `compute_all()` operations. This provides users with feedback during the 148-second computation process, preventing confusion about whether the process has crashed.",
        "",
        "## Quick Start",
        "",
        "### Silent Mode (Default)",
        "",
        "By default, `compute_all()` runs silently with no progress output:",
        "",
        "```python",
        "from cortical import CorticalTextProcessor",
        "",
        "processor = CorticalTextProcessor()",
        "processor.process_document(\"doc1\", \"Neural networks process information.\")",
        "processor.compute_all()  # Silent, backward compatible",
        "```",
        "",
        "### Console Progress Bar",
        "",
        "Enable a nice console progress bar with the `show_progress` parameter:",
        "",
        "```python",
        "processor.compute_all(show_progress=True)",
        "```",
        "",
        "**Output:**",
        "```",
        "Activation propagation... [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100% (0.2s)",
        "PageRank computation... [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100% (1.5s)",
        "TF-IDF computation... [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100% (2.1s)",
        "Document connections... [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100% (0.8s)",
        "Bigram connections... [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100% (45.3s)",
        "Concept clustering... [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100% (12.4s)",
        "Concept connections... [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100% (3.2s)",
        "```",
        "",
        "### Custom Callback",
        "",
        "For integration with UIs or logging systems, use a custom callback:",
        "",
        "```python",
        "from cortical import CallbackProgressReporter",
        "",
        "def my_progress_callback(phase, percent, message):",
        "    print(f\"[{phase}] {percent:.1f}% - {message or 'in progress'}\")",
        "",
        "reporter = CallbackProgressReporter(my_progress_callback)",
        "processor.compute_all(progress_callback=reporter)",
        "```",
        "",
        "## API Reference",
        "",
        "### `compute_all()` Parameters",
        "",
        "```python",
        "processor.compute_all(",
        "    progress_callback=None,  # Optional ProgressReporter instance",
        "    show_progress=False,     # Show console progress bar",
        "    verbose=True,            # Legacy logging parameter",
        "    build_concepts=True,     # Build concept clusters",
        "    pagerank_method='standard',",
        "    connection_strategy='document_overlap',",
        "    cluster_strictness=1.0,",
        "    bridge_weight=0.0",
        ")",
        "```",
        "",
        "### Progress Reporters",
        "",
        "#### `ConsoleProgressReporter`",
        "",
        "Displays progress bars on the console with Unicode block characters.",
        "",
        "```python",
        "from cortical import ConsoleProgressReporter",
        "",
        "reporter = ConsoleProgressReporter(",
        "    file=sys.stderr,     # Output file (default: stderr)",
        "    width=40,            # Progress bar width in characters",
        "    show_eta=True,       # Show estimated time remaining",
        "    use_unicode=True     # Use Unicode block chars (‚ñà) vs ASCII (#)",
        ")",
        "",
        "processor.compute_all(progress_callback=reporter)",
        "```",
        "",
        "**Features:**",
        "- In-place updates using carriage returns",
        "- Elapsed time display",
        "- ETA estimation (after 1 second of progress)",
        "- Unicode or ASCII mode",
        "",
        "#### `CallbackProgressReporter`",
        "",
        "Calls a custom function for each progress update.",
        "",
        "```python",
        "from cortical import CallbackProgressReporter",
        "",
        "def my_callback(phase: str, percent: float, message: str):",
        "    \"\"\"",
        "    Args:",
        "        phase: Phase name (e.g., \"TF-IDF computation\")",
        "        percent: Progress percentage (0.0 to 100.0)",
        "        message: Optional status message",
        "    \"\"\"",
        "    # Your custom logic here",
        "    logger.info(f\"{phase}: {percent:.1f}%\")",
        "",
        "reporter = CallbackProgressReporter(my_callback)",
        "processor.compute_all(progress_callback=reporter)",
        "```",
        "",
        "**Use cases:**",
        "- Integration with GUI progress bars",
        "- Logging to files or external systems",
        "- Real-time monitoring dashboards",
        "- Notification systems",
        "",
        "#### `SilentProgressReporter`",
        "",
        "No-op reporter (default behavior).",
        "",
        "```python",
        "from cortical import SilentProgressReporter",
        "",
        "reporter = SilentProgressReporter()",
        "processor.compute_all(progress_callback=reporter)  # No output",
        "```",
        "",
        "### `MultiPhaseProgress`",
        "",
        "Helper for managing progress across multiple sequential phases.",
        "",
        "```python",
        "from cortical import MultiPhaseProgress, ConsoleProgressReporter",
        "",
        "reporter = ConsoleProgressReporter()",
        "phases = {",
        "    \"Phase 1\": 30,  # 30% of total time",
        "    \"Phase 2\": 50,  # 50% of total time",
        "    \"Phase 3\": 20   # 20% of total time",
        "}",
        "",
        "progress = MultiPhaseProgress(reporter, phases)",
        "",
        "# Phase 1",
        "progress.start_phase(\"Phase 1\")",
        "progress.update(50.0, \"Processing...\")  # 50% of Phase 1 = 15% overall",
        "progress.complete_phase()",
        "",
        "# Phase 2",
        "progress.start_phase(\"Phase 2\")",
        "progress.update(100.0)",
        "progress.complete_phase()",
        "",
        "# Overall progress: 80% (Phase 1 + Phase 2 complete)",
        "print(f\"Overall: {progress.overall_progress:.1f}%\")",
        "```",
        "",
        "## Computation Phases",
        "",
        "The following phases are reported during `compute_all()`:",
        "",
        "| Phase | Typical Duration | Description |",
        "|-------|------------------|-------------|",
        "| Activation propagation | ~5% | Spreads activation through lateral connections |",
        "| PageRank computation | ~10% | Computes term importance scores |",
        "| TF-IDF computation | ~15% | Calculates term frequency-inverse document frequency |",
        "| Document connections | ~10% | Builds document-to-document similarity graph |",
        "| Bigram connections | ~30% | Connects bigrams via shared components (slowest) |",
        "| Concept clustering | ~15% | Clusters terms into semantic concepts (if enabled) |",
        "| Semantic extraction | ~10% | Extracts semantic relations (if needed) |",
        "| Graph embeddings | ~10% | Computes graph embeddings (if needed) |",
        "| Concept connections | ~15% | Connects concepts based on strategy (if enabled) |",
        "",
        "**Note:** Phase durations are estimates and vary based on corpus size and configuration.",
        "",
        "## Advanced Usage",
        "",
        "### Combining Progress with Verbose Logging",
        "",
        "```python",
        "# Show both progress bars and logger messages",
        "processor.compute_all(show_progress=True, verbose=True)",
        "```",
        "",
        "### Custom Progress Tracking for Specific Phases",
        "",
        "```python",
        "class PhaseLogger:",
        "    def __init__(self):",
        "        self.phases = []",
        "",
        "    def __call__(self, phase, percent, message):",
        "        if percent == 100.0:",
        "            self.phases.append(phase)",
        "            print(f\"‚úì Completed: {phase}\")",
        "",
        "reporter = CallbackProgressReporter(PhaseLogger())",
        "processor.compute_all(progress_callback=reporter)",
        "```",
        "",
        "### Integration with tqdm",
        "",
        "```python",
        "from tqdm import tqdm",
        "",
        "class TqdmProgressReporter:",
        "    def __init__(self):",
        "        self.pbar = None",
        "        self.current_phase = None",
        "",
        "    def update(self, phase, percent, message):",
        "        if phase != self.current_phase:",
        "            if self.pbar:",
        "                self.pbar.close()",
        "            self.pbar = tqdm(total=100, desc=phase)",
        "            self.current_phase = phase",
        "",
        "        if self.pbar:",
        "            self.pbar.n = int(percent)",
        "            self.pbar.refresh()",
        "",
        "    def complete(self, phase, message):",
        "        if self.pbar:",
        "            self.pbar.n = 100",
        "            self.pbar.refresh()",
        "            self.pbar.close()",
        "            self.pbar = None",
        "",
        "reporter = TqdmProgressReporter()",
        "processor.compute_all(progress_callback=reporter)",
        "```",
        "",
        "### Jupyter Notebook Integration",
        "",
        "```python",
        "from IPython.display import clear_output, display, HTML",
        "",
        "class JupyterProgressReporter:",
        "    def update(self, phase, percent, message):",
        "        clear_output(wait=True)",
        "        html = f\"\"\"",
        "        <div style=\"border: 1px solid #ccc; padding: 10px;\">",
        "            <strong>{phase}</strong><br>",
        "            <div style=\"background: #eee; height: 20px; margin-top: 5px;\">",
        "                <div style=\"background: #4CAF50; height: 20px; width: {percent}%;\"></div>",
        "            </div>",
        "            <small>{percent:.1f}% - {message or ''}</small>",
        "        </div>",
        "        \"\"\"",
        "        display(HTML(html))",
        "",
        "    def complete(self, phase, message):",
        "        self.update(phase, 100.0, message or \"Complete\")",
        "",
        "reporter = JupyterProgressReporter()",
        "processor.compute_all(progress_callback=reporter)",
        "```",
        "",
        "## Testing",
        "",
        "The progress reporting system includes comprehensive unit tests:",
        "",
        "```bash",
        "# Run progress tests",
        "python -m pytest tests/unit/test_progress.py -v",
        "",
        "# Run demo script",
        "python demo_progress.py",
        "```",
        "",
        "## Backward Compatibility",
        "",
        "The progress reporting system is fully backward compatible:",
        "",
        "- **Default behavior unchanged:** `compute_all()` is silent by default",
        "- **No breaking changes:** All existing code continues to work",
        "- **Opt-in only:** Progress reporting must be explicitly enabled",
        "",
        "```python",
        "# Old code still works exactly the same",
        "processor.compute_all()  # Silent",
        "processor.compute_all(verbose=True)  # Logger output only",
        "```",
        "",
        "## Implementation Details",
        "",
        "### Progress Protocol",
        "",
        "The `ProgressReporter` protocol defines the interface:",
        "",
        "```python",
        "from typing import Protocol",
        "",
        "class ProgressReporter(Protocol):",
        "    def update(self, phase: str, percent: float, message: Optional[str] = None) -> None:",
        "        \"\"\"Update progress for a phase.\"\"\"",
        "        ...",
        "",
        "    def complete(self, phase: str, message: Optional[str] = None) -> None:",
        "        \"\"\"Mark a phase as complete.\"\"\"",
        "        ...",
        "```",
        "",
        "Any object implementing these two methods can be used as a progress reporter.",
        "",
        "### ETA Calculation",
        "",
        "The console reporter estimates time remaining using linear extrapolation:",
        "",
        "```",
        "total_time = elapsed_time / (percent / 100)",
        "eta = total_time - elapsed_time",
        "```",
        "",
        "ETAs are shown only after at least 1 second has elapsed to ensure reasonable estimates.",
        "",
        "## Troubleshooting",
        "",
        "### Progress Bar Not Showing",
        "",
        "**Issue:** No progress output when using `show_progress=True`",
        "",
        "**Solutions:**",
        "- Progress is written to `stderr`, not `stdout`. Check your terminal's stderr handling.",
        "- In Jupyter notebooks, use a custom `JupyterProgressReporter` instead.",
        "- Ensure you're not redirecting stderr elsewhere.",
        "",
        "### Unicode Characters Not Displaying",
        "",
        "**Issue:** Progress bar shows `?` or incorrect characters",
        "",
        "**Solutions:**",
        "```python",
        "# Use ASCII mode instead of Unicode",
        "from cortical import ConsoleProgressReporter",
        "",
        "reporter = ConsoleProgressReporter(use_unicode=False)",
        "processor.compute_all(progress_callback=reporter)",
        "```",
        "",
        "### Progress Updates Too Fast/Slow",
        "",
        "**Issue:** Progress jumps or appears sluggish",
        "",
        "**Explanation:** Phase weights are estimates. Actual duration varies by corpus size.",
        "",
        "**Workaround:** For precise progress tracking, implement a custom reporter that tracks actual wall-clock time instead of phase percentages.",
        "",
        "## Performance Impact",
        "",
        "Progress reporting has negligible performance overhead:",
        "",
        "- **SilentProgressReporter:** Zero overhead (no-op methods)",
        "- **CallbackProgressReporter:** ~0.01% overhead (function call per update)",
        "- **ConsoleProgressReporter:** ~0.1% overhead (string formatting + I/O)",
        "",
        "For a 148-second computation, progress reporting adds less than 0.15 seconds.",
        "",
        "## Examples",
        "",
        "See `demo_progress.py` for complete working examples of all progress reporting modes."
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "examples/demo_progress.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "#!/usr/bin/env python",
        "\"\"\"",
        "Demonstration of progress reporting capabilities.",
        "",
        "This script shows the different ways to use progress reporting with",
        "the Cortical Text Processor's compute_all() method.",
        "\"\"\"",
        "",
        "from cortical import CorticalTextProcessor, CallbackProgressReporter",
        "",
        "",
        "def demo_silent():",
        "    \"\"\"Default behavior - no progress output.\"\"\"",
        "    print(\"=\" * 60)",
        "    print(\"Demo 1: Silent Mode (default)\")",
        "    print(\"=\" * 60)",
        "",
        "    processor = CorticalTextProcessor()",
        "    processor.process_document(\"doc1\", \"Neural networks process information efficiently.\")",
        "    processor.process_document(\"doc2\", \"Machine learning algorithms analyze large datasets.\")",
        "    processor.process_document(\"doc3\", \"Deep learning models require substantial training data.\")",
        "",
        "    print(\"Running compute_all() with default settings (silent)...\")",
        "    processor.compute_all(verbose=False)",
        "    print(\"Complete! (no progress output)\\n\")",
        "",
        "",
        "def demo_console_progress():",
        "    \"\"\"Console progress bar with nice formatting.\"\"\"",
        "    print(\"=\" * 60)",
        "    print(\"Demo 2: Console Progress Bar\")",
        "    print(\"=\" * 60)",
        "",
        "    processor = CorticalTextProcessor()",
        "    processor.process_document(\"doc1\", \"Neural networks process information efficiently.\")",
        "    processor.process_document(\"doc2\", \"Machine learning algorithms analyze large datasets.\")",
        "    processor.process_document(\"doc3\", \"Deep learning models require substantial training data.\")",
        "    processor.process_document(\"doc4\", \"Artificial intelligence systems learn from experience.\")",
        "    processor.process_document(\"doc5\", \"Data science combines statistics and programming.\")",
        "",
        "    print(\"Running compute_all() with show_progress=True:\")",
        "    processor.compute_all(show_progress=True, verbose=False)",
        "    print(\"\\nComplete!\\n\")",
        "",
        "",
        "def demo_callback():",
        "    \"\"\"Custom callback for integration with other systems.\"\"\"",
        "    print(\"=\" * 60)",
        "    print(\"Demo 3: Custom Callback\")",
        "    print(\"=\" * 60)",
        "",
        "    processor = CorticalTextProcessor()",
        "    processor.process_document(\"doc1\", \"Neural networks process information efficiently.\")",
        "    processor.process_document(\"doc2\", \"Machine learning algorithms analyze large datasets.\")",
        "    processor.process_document(\"doc3\", \"Deep learning models require substantial training data.\")",
        "",
        "    # Track progress with custom callback",
        "    progress_log = []",
        "",
        "    def custom_callback(phase, percent, message):",
        "        \"\"\"Custom callback that logs progress.\"\"\"",
        "        progress_log.append({",
        "            'phase': phase,",
        "            'percent': percent,",
        "            'message': message",
        "        })",
        "        # Print in custom format",
        "        msg_str = f\" - {message}\" if message else \"\"",
        "        print(f\"  [{phase}] {percent:5.1f}%{msg_str}\")",
        "",
        "    reporter = CallbackProgressReporter(custom_callback)",
        "",
        "    print(\"Running compute_all() with custom callback:\")",
        "    processor.compute_all(progress_callback=reporter, verbose=False)",
        "",
        "    print(f\"\\nLogged {len(progress_log)} progress updates\")",
        "    print(f\"Phases completed: {len([p for p in progress_log if p['percent'] == 100.0])}\\n\")",
        "",
        "",
        "def demo_verbose_with_progress():",
        "    \"\"\"Combining verbose logging with progress bar.\"\"\"",
        "    print(\"=\" * 60)",
        "    print(\"Demo 4: Verbose Logging + Progress Bar\")",
        "    print(\"=\" * 60)",
        "",
        "    processor = CorticalTextProcessor()",
        "    processor.process_document(\"doc1\", \"Neural networks process information efficiently.\")",
        "    processor.process_document(\"doc2\", \"Machine learning algorithms analyze large datasets.\")",
        "",
        "    print(\"Running compute_all() with both verbose=True and show_progress=True:\")",
        "    print(\"(Shows both logger messages and progress bars)\\n\")",
        "    processor.compute_all(show_progress=True, verbose=True)",
        "    print(\"\\nComplete!\\n\")",
        "",
        "",
        "def main():",
        "    \"\"\"Run all demonstrations.\"\"\"",
        "    print(\"\\n\" + \"=\" * 60)",
        "    print(\"CORTICAL TEXT PROCESSOR - PROGRESS REPORTING DEMO\")",
        "    print(\"=\" * 60 + \"\\n\")",
        "",
        "    # Demo 1: Silent (default)",
        "    demo_silent()",
        "",
        "    # Demo 2: Console progress bar",
        "    demo_console_progress()",
        "",
        "    # Demo 3: Custom callback",
        "    demo_callback()",
        "",
        "    # Demo 4: Verbose + progress",
        "    demo_verbose_with_progress()",
        "",
        "    print(\"=\" * 60)",
        "    print(\"All demos complete!\")",
        "    print(\"=\" * 60)",
        "",
        "",
        "if __name__ == '__main__':",
        "    main()"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "examples/examples_results_usage.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "#!/usr/bin/env python",
        "\"\"\"",
        "Example Usage: Result Dataclasses",
        "==================================",
        "",
        "Demonstrates the strongly-typed result containers for query operations",
        "that provide IDE autocomplete and type checking support.",
        "",
        "Task #185: Create result dataclasses for the Cortical Text Processor.",
        "\"\"\"",
        "",
        "from cortical import (",
        "    CorticalTextProcessor,",
        "    DocumentMatch,",
        "    PassageMatch,",
        "    QueryResult,",
        "    convert_document_matches,",
        "    convert_passage_matches",
        ")",
        "",
        "",
        "def example_basic_document_match():",
        "    \"\"\"Example 1: Basic DocumentMatch usage.\"\"\"",
        "    print(\"\\n\" + \"=\"*60)",
        "    print(\"EXAMPLE 1: Basic DocumentMatch Usage\")",
        "    print(\"=\"*60)",
        "",
        "    # Create matches manually",
        "    match1 = DocumentMatch(\"neural_networks.md\", 0.95)",
        "    match2 = DocumentMatch(\"deep_learning.py\", 0.87, metadata={\"type\": \"code\"})",
        "",
        "    print(f\"\\nMatch 1: {match1.doc_id} - Score: {match1.score:.2f}\")",
        "    print(f\"Match 2: {match2.doc_id} - Score: {match2.score:.2f}\")",
        "    print(f\"         Metadata: {match2.metadata}\")",
        "",
        "    # Convert to/from tuple (for compatibility with legacy code)",
        "    tuple_form = match1.to_tuple()",
        "    print(f\"\\nTuple form: {tuple_form}\")",
        "    restored = DocumentMatch.from_tuple(*tuple_form)",
        "    print(f\"Restored: {restored}\")",
        "",
        "    # Convert to/from dict (for JSON serialization)",
        "    dict_form = match1.to_dict()",
        "    print(f\"\\nDict form: {dict_form}\")",
        "",
        "",
        "def example_basic_passage_match():",
        "    \"\"\"Example 2: Basic PassageMatch usage.\"\"\"",
        "    print(\"\\n\" + \"=\"*60)",
        "    print(\"EXAMPLE 2: Basic PassageMatch Usage\")",
        "    print(\"=\"*60)",
        "",
        "    # Create passage match",
        "    passage = PassageMatch(",
        "        doc_id=\"cortical/processor.py\",",
        "        text=\"def compute_pagerank(self):\\n    \\\"\\\"\\\"Compute PageRank importance scores.\\\"\\\"\\\"\",",
        "        score=0.92,",
        "        start=1500,",
        "        end=1580",
        "    )",
        "",
        "    print(f\"\\nDocument: {passage.doc_id}\")",
        "    print(f\"Location: {passage.location}\")",
        "    print(f\"Length: {passage.length} characters\")",
        "    print(f\"Score: {passage.score:.2f}\")",
        "    print(f\"Text:\\n{passage.text}\")",
        "",
        "    # Useful properties",
        "    print(f\"\\nCitation: [{passage.location}]\")",
        "",
        "",
        "def example_with_processor():",
        "    \"\"\"Example 3: Using dataclasses with CorticalTextProcessor.\"\"\"",
        "    print(\"\\n\" + \"=\"*60)",
        "    print(\"EXAMPLE 3: Integration with CorticalTextProcessor\")",
        "    print(\"=\"*60)",
        "",
        "    # Set up processor with sample documents",
        "    processor = CorticalTextProcessor()",
        "    processor.process_document(",
        "        \"neural_networks.md\",",
        "        \"Neural networks are computational models inspired by biological neurons. \"",
        "        \"They consist of interconnected nodes that process information.\"",
        "    )",
        "    processor.process_document(",
        "        \"deep_learning.py\",",
        "        \"class DeepNetwork:\\n    def __init__(self):\\n        self.layers = []\\n    \"",
        "        \"def forward(self, input):\\n        # Process through layers\\n        pass\"",
        "    )",
        "    processor.process_document(",
        "        \"ai_overview.md\",",
        "        \"Artificial intelligence encompasses machine learning, neural networks, \"",
        "        \"and deep learning approaches to solving complex problems.\"",
        "    )",
        "    processor.compute_all()",
        "",
        "    # Perform document search",
        "    print(\"\\n--- Document Search ---\")",
        "    raw_results = processor.find_documents_for_query(\"neural networks\", top_n=3)",
        "    matches = convert_document_matches(raw_results)",
        "",
        "    for i, match in enumerate(matches, 1):",
        "        print(f\"{i}. {match.doc_id}: {match.score:.3f}\")",
        "",
        "    # Perform passage search",
        "    print(\"\\n--- Passage Search ---\")",
        "    raw_passages = processor.find_passages_for_query(\"neural\", top_n=2)",
        "    passages = convert_passage_matches(raw_passages)",
        "",
        "    for i, passage in enumerate(passages, 1):",
        "        text_preview = passage.text[:60] + \"...\" if len(passage.text) > 60 else passage.text",
        "        print(f\"{i}. [{passage.location}] {text_preview}\")",
        "        print(f\"   Score: {passage.score:.3f}\")",
        "",
        "",
        "def example_query_result_wrapper():",
        "    \"\"\"Example 4: Using QueryResult wrapper with metadata.\"\"\"",
        "    print(\"\\n\" + \"=\"*60)",
        "    print(\"EXAMPLE 4: QueryResult Wrapper with Metadata\")",
        "    print(\"=\"*60)",
        "",
        "    # Simulate search results",
        "    matches = [",
        "        DocumentMatch(\"neural_networks.md\", 0.95),",
        "        DocumentMatch(\"deep_learning.py\", 0.87),",
        "        DocumentMatch(\"ai_overview.md\", 0.72)",
        "    ]",
        "",
        "    # Wrap in QueryResult with metadata",
        "    result = QueryResult(",
        "        query=\"neural networks\",",
        "        matches=matches,",
        "        expansion_terms={",
        "            \"neural\": 1.0,",
        "            \"network\": 0.95,",
        "            \"neuron\": 0.7,",
        "            \"artificial\": 0.5",
        "        },",
        "        timing_ms=15.3",
        "    )",
        "",
        "    print(f\"\\nQuery: '{result.query}'\")",
        "    print(f\"Match count: {result.match_count}\")",
        "    print(f\"Average score: {result.average_score:.3f}\")",
        "    print(f\"Query time: {result.timing_ms}ms\")",
        "",
        "    print(f\"\\nTop match: {result.top_match.doc_id} ({result.top_match.score:.3f})\")",
        "",
        "    print(\"\\nExpansion terms:\")",
        "    for term, weight in sorted(result.expansion_terms.items(), key=lambda x: -x[1]):",
        "        print(f\"  {term}: {weight:.2f}\")",
        "",
        "    # Serialization",
        "    print(\"\\n--- Serialization Example ---\")",
        "    result_dict = result.to_dict()",
        "    print(f\"Serialized keys: {list(result_dict.keys())}\")",
        "",
        "    restored = QueryResult.from_dict(result_dict)",
        "    print(f\"Restored query: '{restored.query}'\")",
        "    print(f\"Restored matches: {restored.match_count}\")",
        "",
        "",
        "def example_batch_conversion():",
        "    \"\"\"Example 5: Batch conversion with metadata.\"\"\"",
        "    print(\"\\n\" + \"=\"*60)",
        "    print(\"EXAMPLE 5: Batch Conversion with Metadata\")",
        "    print(\"=\"*60)",
        "",
        "    # Simulate raw results from search",
        "    raw_results = [",
        "        (\"neural_networks.md\", 0.95),",
        "        (\"deep_learning.py\", 0.87),",
        "        (\"ai_overview.md\", 0.72)",
        "    ]",
        "",
        "    # Metadata for each document",
        "    metadata = {",
        "        \"neural_networks.md\": {\"type\": \"documentation\", \"size\": 2048},",
        "        \"deep_learning.py\": {\"type\": \"code\", \"language\": \"python\"},",
        "        \"ai_overview.md\": {\"type\": \"documentation\", \"size\": 1024}",
        "    }",
        "",
        "    # Convert with metadata",
        "    matches = convert_document_matches(raw_results, metadata)",
        "",
        "    print(\"\\nConverted matches with metadata:\")",
        "    for match in matches:",
        "        print(f\"  {match.doc_id}: {match.score:.2f}\")",
        "        if match.metadata:",
        "            print(f\"    {match.metadata}\")",
        "",
        "",
        "def example_type_safety():",
        "    \"\"\"Example 6: Type safety and IDE support.\"\"\"",
        "    print(\"\\n\" + \"=\"*60)",
        "    print(\"EXAMPLE 6: Type Safety and IDE Support\")",
        "    print(\"=\"*60)",
        "",
        "    # Create a match",
        "    match = DocumentMatch(\"test.txt\", 0.8)",
        "",
        "    # IDE autocomplete works because match has known attributes",
        "    print(f\"\\nAttributes available with autocomplete:\")",
        "    print(f\"  match.doc_id = {match.doc_id}\")",
        "    print(f\"  match.score = {match.score}\")",
        "    print(f\"  match.metadata = {match.metadata}\")",
        "",
        "    # Type checking catches errors at development time",
        "    print(\"\\nDataclasses are immutable (frozen):\")",
        "    try:",
        "        match.score = 0.9  # This will raise an error",
        "    except AttributeError as e:",
        "        print(f\"  ‚úì Cannot modify: {e}\")",
        "",
        "    # PassageMatch has additional useful properties",
        "    passage = PassageMatch(\"doc.py\", \"code here\", 0.9, 100, 110)",
        "    print(f\"\\nPassageMatch properties:\")",
        "    print(f\"  passage.location = {passage.location}\")",
        "    print(f\"  passage.length = {passage.length}\")",
        "",
        "",
        "def main():",
        "    \"\"\"Run all examples.\"\"\"",
        "    print(\"\\n\" + \"=\"*60)",
        "    print(\"CORTICAL TEXT PROCESSOR - Result Dataclasses Examples\")",
        "    print(\"Task #185: Strongly-typed query result containers\")",
        "    print(\"=\"*60)",
        "",
        "    example_basic_document_match()",
        "    example_basic_passage_match()",
        "    example_with_processor()",
        "    example_query_result_wrapper()",
        "    example_batch_conversion()",
        "    example_type_safety()",
        "",
        "    print(\"\\n\" + \"=\"*60)",
        "    print(\"All examples completed successfully!\")",
        "    print(\"=\"*60 + \"\\n\")",
        "",
        "",
        "if __name__ == \"__main__\":",
        "    main()"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "pyproject.toml",
      "function": "include = [\"cortical*\"]",
      "start_line": 60,
      "lines_added": [
        "    \"cortical/types.py\",",
        "    \"cortical/cli_wrapper.py\","
      ],
      "lines_removed": [],
      "context_before": [
        "",
        "[tool.setuptools.package-data]",
        "cortical = [\"py.typed\"]",
        "",
        "[tool.coverage.run]",
        "source = [\"cortical\"]",
        "branch = true",
        "omit = [",
        "    \"*/tests/*\",",
        "    \"*/__pycache__/*\","
      ],
      "context_after": [
        "]",
        "",
        "[tool.coverage.report]",
        "exclude_lines = [",
        "    \"pragma: no cover\",",
        "    \"def __repr__\",",
        "    \"raise NotImplementedError\",",
        "    \"if TYPE_CHECKING:\",",
        "    \"if __name__ == .__main__.:\",",
        "]"
      ],
      "change_type": "add"
    },
    {
      "file": "tests/test_fluent.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "\"\"\"",
        "Tests for the FluentProcessor API.",
        "",
        "Ensures the fluent/chainable interface works correctly.",
        "\"\"\"",
        "",
        "import os",
        "import tempfile",
        "import unittest",
        "",
        "from cortical import CorticalTextProcessor, CorticalConfig",
        "from cortical.fluent import FluentProcessor",
        "",
        "",
        "class TestFluentProcessorInit(unittest.TestCase):",
        "    \"\"\"Test FluentProcessor initialization.\"\"\"",
        "",
        "    def test_default_init(self):",
        "        \"\"\"Test default initialization.\"\"\"",
        "        fp = FluentProcessor()",
        "        self.assertIsNotNone(fp)",
        "        self.assertIsInstance(fp.processor, CorticalTextProcessor)",
        "        self.assertFalse(fp.is_built)",
        "",
        "    def test_init_with_config(self):",
        "        \"\"\"Test initialization with config.\"\"\"",
        "        config = CorticalConfig(pagerank_damping=0.9)",
        "        fp = FluentProcessor(config=config)",
        "        self.assertEqual(fp.processor.config.pagerank_damping, 0.9)",
        "",
        "    def test_from_existing(self):",
        "        \"\"\"Test wrapping existing processor.\"\"\"",
        "        proc = CorticalTextProcessor()",
        "        proc.process_document(\"doc1\", \"Test content\")",
        "        fp = FluentProcessor.from_existing(proc)",
        "        self.assertEqual(fp.processor, proc)",
        "",
        "    def test_repr(self):",
        "        \"\"\"Test string representation.\"\"\"",
        "        fp = FluentProcessor()",
        "        self.assertIn(\"FluentProcessor\", repr(fp))",
        "",
        "",
        "class TestFluentProcessorChaining(unittest.TestCase):",
        "    \"\"\"Test method chaining.\"\"\"",
        "",
        "    def test_add_document_returns_self(self):",
        "        \"\"\"add_document returns self for chaining.\"\"\"",
        "        fp = FluentProcessor()",
        "        result = fp.add_document(\"doc1\", \"Content\")",
        "        self.assertIs(result, fp)",
        "",
        "    def test_add_documents_returns_self(self):",
        "        \"\"\"add_documents returns self for chaining.\"\"\"",
        "        fp = FluentProcessor()",
        "        result = fp.add_documents({\"doc1\": \"Content\"})",
        "        self.assertIs(result, fp)",
        "",
        "    def test_build_returns_self(self):",
        "        \"\"\"build returns self for chaining.\"\"\"",
        "        fp = FluentProcessor()",
        "        fp.add_document(\"doc1\", \"Content\")",
        "        result = fp.build(verbose=False)",
        "        self.assertIs(result, fp)",
        "",
        "    def test_full_chain(self):",
        "        \"\"\"Test complete method chain.\"\"\"",
        "        results = (FluentProcessor()",
        "            .add_document(\"doc1\", \"Neural networks process data\")",
        "            .add_document(\"doc2\", \"Machine learning is powerful\")",
        "            .build(verbose=False)",
        "            .search(\"neural\", top_n=2))",
        "        self.assertIsInstance(results, list)",
        "",
        "",
        "class TestFluentProcessorDocuments(unittest.TestCase):",
        "    \"\"\"Test document handling.\"\"\"",
        "",
        "    def test_add_single_document(self):",
        "        \"\"\"Test adding a single document.\"\"\"",
        "        fp = FluentProcessor()",
        "        fp.add_document(\"doc1\", \"Test content here\")",
        "        self.assertIn(\"doc1\", fp.processor.documents)",
        "",
        "    def test_add_document_with_metadata(self):",
        "        \"\"\"Test adding document with metadata.\"\"\"",
        "        fp = FluentProcessor()",
        "        fp.add_document(\"doc1\", \"Content\", metadata={\"author\": \"test\"})",
        "        self.assertIn(\"doc1\", fp.processor.documents)",
        "",
        "    def test_add_documents_dict(self):",
        "        \"\"\"Test adding multiple documents from dict.\"\"\"",
        "        fp = FluentProcessor()",
        "        fp.add_documents({",
        "            \"doc1\": \"Content one\",",
        "            \"doc2\": \"Content two\"",
        "        })",
        "        self.assertIn(\"doc1\", fp.processor.documents)",
        "        self.assertIn(\"doc2\", fp.processor.documents)",
        "",
        "    def test_add_documents_tuples(self):",
        "        \"\"\"Test adding documents from list of tuples.\"\"\"",
        "        fp = FluentProcessor()",
        "        fp.add_documents([",
        "            (\"doc1\", \"Content one\"),",
        "            (\"doc2\", \"Content two\")",
        "        ])",
        "        self.assertIn(\"doc1\", fp.processor.documents)",
        "        self.assertIn(\"doc2\", fp.processor.documents)",
        "",
        "    def test_add_documents_invalid_type(self):",
        "        \"\"\"Test error on invalid input type.\"\"\"",
        "        fp = FluentProcessor()",
        "        with self.assertRaises(TypeError):",
        "            fp.add_documents(\"invalid\")",
        "",
        "",
        "class TestFluentProcessorBuild(unittest.TestCase):",
        "    \"\"\"Test build functionality.\"\"\"",
        "",
        "    def test_build_marks_built(self):",
        "        \"\"\"Test that build marks processor as built.\"\"\"",
        "        fp = FluentProcessor()",
        "        fp.add_document(\"doc1\", \"Content\")",
        "        self.assertFalse(fp.is_built)",
        "        fp.build(verbose=False)",
        "        self.assertTrue(fp.is_built)",
        "",
        "    def test_add_after_build_marks_unbuilt(self):",
        "        \"\"\"Test adding document after build marks as unbuilt.\"\"\"",
        "        fp = FluentProcessor()",
        "        fp.add_document(\"doc1\", \"Content\")",
        "        fp.build(verbose=False)",
        "        self.assertTrue(fp.is_built)",
        "        fp.add_document(\"doc2\", \"More content\")",
        "        self.assertFalse(fp.is_built)",
        "",
        "",
        "class TestFluentProcessorSearch(unittest.TestCase):",
        "    \"\"\"Test search functionality.\"\"\"",
        "",
        "    def setUp(self):",
        "        \"\"\"Set up a built processor.\"\"\"",
        "        self.fp = (FluentProcessor()",
        "            .add_document(\"neural\", \"Neural networks are computational models\")",
        "            .add_document(\"ml\", \"Machine learning algorithms learn from data\")",
        "            .add_document(\"deep\", \"Deep learning uses neural network layers\")",
        "            .build(verbose=False))",
        "",
        "    def test_search_returns_results(self):",
        "        \"\"\"Test basic search returns results.\"\"\"",
        "        results = self.fp.search(\"neural\", top_n=3)",
        "        self.assertIsInstance(results, list)",
        "        self.assertGreater(len(results), 0)",
        "",
        "    def test_search_result_structure(self):",
        "        \"\"\"Test search result tuple structure.\"\"\"",
        "        results = self.fp.search(\"neural\", top_n=1)",
        "        self.assertEqual(len(results[0]), 2)  # (doc_id, score)",
        "",
        "    def test_fast_search(self):",
        "        \"\"\"Test fast search method.\"\"\"",
        "        results = self.fp.fast_search(\"neural\", top_n=3)",
        "        self.assertIsInstance(results, list)",
        "",
        "    def test_search_passages(self):",
        "        \"\"\"Test passage search.\"\"\"",
        "        results = self.fp.search_passages(\"neural\", top_n=2)",
        "        self.assertIsInstance(results, list)",
        "",
        "    def test_expand_query(self):",
        "        \"\"\"Test query expansion.\"\"\"",
        "        expanded = self.fp.expand(\"neural\", max_expansions=5)",
        "        self.assertIsInstance(expanded, dict)",
        "",
        "",
        "class TestFluentProcessorPersistence(unittest.TestCase):",
        "    \"\"\"Test save/load functionality.\"\"\"",
        "",
        "    def test_save_and_load(self):",
        "        \"\"\"Test saving and loading processor.\"\"\"",
        "        with tempfile.NamedTemporaryFile(suffix='.pkl', delete=False) as f:",
        "            path = f.name",
        "",
        "        try:",
        "            # Save",
        "            (FluentProcessor()",
        "                .add_document(\"doc1\", \"Test content\")",
        "                .build(verbose=False)",
        "                .save(path))",
        "",
        "            # Load",
        "            fp = FluentProcessor.load(path)",
        "            self.assertTrue(fp.is_built)",
        "            self.assertIn(\"doc1\", fp.processor.documents)",
        "        finally:",
        "            if os.path.exists(path):",
        "                os.unlink(path)",
        "",
        "",
        "class TestFluentProcessorFiles(unittest.TestCase):",
        "    \"\"\"Test file loading functionality.\"\"\"",
        "",
        "    def test_from_files(self):",
        "        \"\"\"Test loading from files.\"\"\"",
        "        with tempfile.TemporaryDirectory() as tmpdir:",
        "            # Create test files",
        "            file1 = os.path.join(tmpdir, \"test1.txt\")",
        "            file2 = os.path.join(tmpdir, \"test2.txt\")",
        "            with open(file1, 'w') as f:",
        "                f.write(\"Content of file one\")",
        "            with open(file2, 'w') as f:",
        "                f.write(\"Content of file two\")",
        "",
        "            fp = FluentProcessor.from_files([file1, file2])",
        "            self.assertEqual(len(fp.processor.documents), 2)",
        "",
        "    def test_from_directory(self):",
        "        \"\"\"Test loading from directory.\"\"\"",
        "        with tempfile.TemporaryDirectory() as tmpdir:",
        "            # Create test files",
        "            for i in range(3):",
        "                path = os.path.join(tmpdir, f\"test{i}.txt\")",
        "                with open(path, 'w') as f:",
        "                    f.write(f\"Content of file {i}\")",
        "",
        "            fp = FluentProcessor.from_directory(tmpdir, pattern=\"*.txt\")",
        "            self.assertEqual(len(fp.processor.documents), 3)",
        "",
        "",
        "if __name__ == '__main__':",
        "    unittest.main()"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "tests/test_progress.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "\"\"\"",
        "Tests for the progress reporting system.",
        "",
        "Ensures progress feedback during long operations works correctly.",
        "\"\"\"",
        "",
        "import io",
        "import sys",
        "import unittest",
        "",
        "from cortical import CorticalTextProcessor",
        "from cortical.progress import (",
        "    ConsoleProgressReporter,",
        "    CallbackProgressReporter,",
        "    SilentProgressReporter,",
        "    MultiPhaseProgress,",
        ")",
        "",
        "",
        "class TestConsoleProgressReporter(unittest.TestCase):",
        "    \"\"\"Test ConsoleProgressReporter.\"\"\"",
        "",
        "    def test_update_formats_correctly(self):",
        "        \"\"\"Test update produces correct format.\"\"\"",
        "        output = io.StringIO()",
        "        reporter = ConsoleProgressReporter(file=output, width=20)",
        "        reporter.update(\"Testing\", 50)",
        "        output_str = output.getvalue()",
        "        self.assertIn(\"Testing\", output_str)",
        "        self.assertIn(\"50%\", output_str)",
        "",
        "    def test_complete_shows_100_percent(self):",
        "        \"\"\"Test complete shows 100%.\"\"\"",
        "        output = io.StringIO()",
        "        reporter = ConsoleProgressReporter(file=output)",
        "        reporter.complete(\"Testing\")",
        "        output_str = output.getvalue()",
        "        self.assertIn(\"100%\", output_str)",
        "",
        "    def test_complete_shows_elapsed_time(self):",
        "        \"\"\"Test complete shows elapsed time.\"\"\"",
        "        output = io.StringIO()",
        "        reporter = ConsoleProgressReporter(file=output)",
        "        reporter.update(\"Testing\", 50)  # Start tracking",
        "        reporter.complete(\"Testing\")",
        "        output_str = output.getvalue()",
        "        # Should have time indicator",
        "        self.assertIn(\"s\", output_str)",
        "",
        "    def test_update_with_message(self):",
        "        \"\"\"Test update with custom message.\"\"\"",
        "        output = io.StringIO()",
        "        reporter = ConsoleProgressReporter(file=output)",
        "        reporter.update(\"Testing\", 50, \"Processing items\")",
        "        output_str = output.getvalue()",
        "        self.assertIn(\"50%\", output_str)",
        "",
        "    def test_progress_bar_width(self):",
        "        \"\"\"Test progress bar respects width.\"\"\"",
        "        output = io.StringIO()",
        "        reporter = ConsoleProgressReporter(file=output, width=10)",
        "        reporter.update(\"Testing\", 50)",
        "        output_str = output.getvalue()",
        "        self.assertIn(\"Testing\", output_str)",
        "",
        "    def test_unicode_vs_ascii(self):",
        "        \"\"\"Test Unicode vs ASCII mode.\"\"\"",
        "        output1 = io.StringIO()",
        "        output2 = io.StringIO()",
        "",
        "        reporter1 = ConsoleProgressReporter(file=output1, use_unicode=True)",
        "        reporter2 = ConsoleProgressReporter(file=output2, use_unicode=False)",
        "",
        "        reporter1.update(\"Test\", 50)",
        "        reporter2.update(\"Test\", 50)",
        "",
        "        # Both should produce output",
        "        self.assertTrue(len(output1.getvalue()) > 0)",
        "        self.assertTrue(len(output2.getvalue()) > 0)",
        "",
        "    def test_percentage_clamping(self):",
        "        \"\"\"Test percentage is handled correctly.\"\"\"",
        "        output = io.StringIO()",
        "        reporter = ConsoleProgressReporter(file=output)",
        "",
        "        # Should not crash with various values",
        "        reporter.update(\"Test\", 0)",
        "        reporter.update(\"Test\", 100)",
        "        self.assertIn(\"%\", output.getvalue())",
        "",
        "",
        "class TestCallbackProgressReporter(unittest.TestCase):",
        "    \"\"\"Test CallbackProgressReporter.\"\"\"",
        "",
        "    def test_callback_invoked_on_update(self):",
        "        \"\"\"Test callback is called on update.\"\"\"",
        "        calls = []",
        "        def callback(phase, pct, msg):",
        "            calls.append((phase, pct, msg))",
        "",
        "        reporter = CallbackProgressReporter(callback)",
        "        reporter.update(\"Testing\", 50, \"message\")",
        "",
        "        self.assertEqual(len(calls), 1)",
        "        self.assertEqual(calls[0][0], \"Testing\")",
        "        self.assertEqual(calls[0][1], 50)",
        "",
        "    def test_callback_invoked_on_complete(self):",
        "        \"\"\"Test callback is called on complete.\"\"\"",
        "        calls = []",
        "        def callback(phase, pct, msg):",
        "            calls.append((phase, pct, msg))",
        "",
        "        reporter = CallbackProgressReporter(callback)",
        "        reporter.complete(\"Testing\")",
        "",
        "        self.assertEqual(len(calls), 1)",
        "        self.assertEqual(calls[0][1], 100)",
        "",
        "    def test_multiple_updates(self):",
        "        \"\"\"Test multiple update calls.\"\"\"",
        "        calls = []",
        "        def callback(phase, pct, msg):",
        "            calls.append(pct)",
        "",
        "        reporter = CallbackProgressReporter(callback)",
        "        reporter.update(\"Test\", 25)",
        "        reporter.update(\"Test\", 50)",
        "        reporter.update(\"Test\", 75)",
        "        reporter.complete(\"Test\")",
        "",
        "        self.assertEqual(calls, [25, 50, 75, 100])",
        "",
        "",
        "class TestSilentProgressReporter(unittest.TestCase):",
        "    \"\"\"Test SilentProgressReporter.\"\"\"",
        "",
        "    def test_update_does_nothing(self):",
        "        \"\"\"Test update doesn't crash.\"\"\"",
        "        reporter = SilentProgressReporter()",
        "        reporter.update(\"Test\", 50)  # Should not raise",
        "        reporter.update(\"Test\", 100, \"message\")  # Should not raise",
        "",
        "    def test_complete_does_nothing(self):",
        "        \"\"\"Test complete doesn't crash.\"\"\"",
        "        reporter = SilentProgressReporter()",
        "        reporter.complete(\"Test\")  # Should not raise",
        "        reporter.complete(\"Test\", \"message\")  # Should not raise",
        "",
        "",
        "class TestMultiPhaseProgress(unittest.TestCase):",
        "    \"\"\"Test MultiPhaseProgress helper.\"\"\"",
        "",
        "    def test_initialization(self):",
        "        \"\"\"Test initialization with phases.\"\"\"",
        "        phases = {\"phase1\": 30, \"phase2\": 70}",
        "        reporter = SilentProgressReporter()",
        "        progress = MultiPhaseProgress(reporter, phases)",
        "        self.assertEqual(len(progress.phases), 2)",
        "",
        "    def test_phase_normalization(self):",
        "        \"\"\"Test phase weights are normalized.\"\"\"",
        "        phases = {\"phase1\": 50, \"phase2\": 50}",
        "        reporter = SilentProgressReporter()",
        "        progress = MultiPhaseProgress(reporter, phases)",
        "        # Weights should sum to 100",
        "        total = sum(progress.phases.values())",
        "        self.assertAlmostEqual(total, 100.0, places=5)",
        "",
        "    def test_start_phase(self):",
        "        \"\"\"Test starting a phase.\"\"\"",
        "        calls = []",
        "        def callback(phase, pct, msg):",
        "            calls.append((phase, pct))",
        "",
        "        phases = {\"phase1\": 50, \"phase2\": 50}",
        "        reporter = CallbackProgressReporter(callback)",
        "        progress = MultiPhaseProgress(reporter, phases)",
        "        progress.start_phase(\"phase1\")",
        "",
        "        self.assertGreater(len(calls), 0)",
        "",
        "    def test_update_within_phase(self):",
        "        \"\"\"Test updating progress within a phase.\"\"\"",
        "        calls = []",
        "        def callback(phase, pct, msg):",
        "            calls.append(pct)",
        "",
        "        phases = {\"phase1\": 100}",
        "        reporter = CallbackProgressReporter(callback)",
        "        progress = MultiPhaseProgress(reporter, phases)",
        "        progress.start_phase(\"phase1\")",
        "        progress.update(50)",
        "",
        "        # Should have some progress reported",
        "        self.assertGreater(len(calls), 0)",
        "",
        "    def test_complete_phase(self):",
        "        \"\"\"Test completing a phase.\"\"\"",
        "        calls = []",
        "        def callback(phase, pct, msg):",
        "            calls.append(pct)",
        "",
        "        phases = {\"phase1\": 50, \"phase2\": 50}",
        "        reporter = CallbackProgressReporter(callback)",
        "        progress = MultiPhaseProgress(reporter, phases)",
        "        progress.start_phase(\"phase1\")",
        "        progress.complete_phase()",
        "",
        "        # Should have completed first phase",
        "        self.assertGreater(len(calls), 0)",
        "",
        "    def test_sequential_phases(self):",
        "        \"\"\"Test running phases sequentially.\"\"\"",
        "        calls = []",
        "        def callback(phase, pct, msg):",
        "            calls.append(pct)",
        "",
        "        phases = {\"phase1\": 50, \"phase2\": 50}",
        "        reporter = CallbackProgressReporter(callback)",
        "        progress = MultiPhaseProgress(reporter, phases)",
        "",
        "        progress.start_phase(\"phase1\")",
        "        progress.update(50)",
        "        progress.complete_phase()",
        "",
        "        progress.start_phase(\"phase2\")",
        "        progress.update(50)",
        "        progress.complete_phase()",
        "",
        "        # Should have multiple updates",
        "        self.assertGreater(len(calls), 2)",
        "",
        "    def test_unknown_phase_raises(self):",
        "        \"\"\"Test starting unknown phase raises error.\"\"\"",
        "        phases = {\"phase1\": 100}",
        "        reporter = SilentProgressReporter()",
        "        progress = MultiPhaseProgress(reporter, phases)",
        "",
        "        with self.assertRaises(ValueError):",
        "            progress.start_phase(\"unknown\")",
        "",
        "",
        "class TestProcessorIntegration(unittest.TestCase):",
        "    \"\"\"Test integration with CorticalTextProcessor.\"\"\"",
        "",
        "    def test_compute_all_silent_default(self):",
        "        \"\"\"Test compute_all is silent by default.\"\"\"",
        "        proc = CorticalTextProcessor()",
        "        proc.process_document(\"doc1\", \"Test content\")",
        "",
        "        # Should not raise",
        "        proc.compute_all()",
        "",
        "    def test_compute_all_with_callback(self):",
        "        \"\"\"Test compute_all with progress callback.\"\"\"",
        "        proc = CorticalTextProcessor()",
        "        proc.process_document(\"doc1\", \"Test content\")",
        "",
        "        phases = []",
        "        def callback(phase, pct, msg):",
        "            if phase not in phases:",
        "                phases.append(phase)",
        "",
        "        reporter = CallbackProgressReporter(callback)",
        "        proc.compute_all(progress_callback=reporter)",
        "",
        "        # Should have reported some phases",
        "        self.assertGreater(len(phases), 0)",
        "",
        "    def test_compute_all_with_show_progress(self):",
        "        \"\"\"Test compute_all with show_progress flag.\"\"\"",
        "        proc = CorticalTextProcessor()",
        "        proc.process_document(\"doc1\", \"Test content\")",
        "",
        "        # Redirect stderr to capture progress",
        "        old_stderr = sys.stderr",
        "        sys.stderr = io.StringIO()",
        "",
        "        try:",
        "            proc.compute_all(show_progress=True)",
        "            output = sys.stderr.getvalue()",
        "        finally:",
        "            sys.stderr = old_stderr",
        "",
        "        # Should have some progress output",
        "        self.assertGreater(len(output), 0)",
        "",
        "    def test_backward_compatibility(self):",
        "        \"\"\"Test that old code still works.\"\"\"",
        "        proc = CorticalTextProcessor()",
        "        proc.process_document(\"doc1\", \"Test content here\")",
        "",
        "        # Old-style call without any progress arguments",
        "        proc.compute_all()",
        "",
        "        # Should complete successfully",
        "        self.assertFalse(proc.is_stale(proc.COMP_TFIDF))",
        "",
        "",
        "if __name__ == '__main__':",
        "    unittest.main()"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "tests/test_results.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "\"\"\"",
        "Tests for the result dataclasses.",
        "",
        "Ensures DocumentMatch, PassageMatch, and QueryResult work correctly.",
        "\"\"\"",
        "",
        "import unittest",
        "",
        "from cortical.results import (",
        "    DocumentMatch,",
        "    PassageMatch,",
        "    QueryResult,",
        "    convert_document_matches,",
        "    convert_passage_matches,",
        ")",
        "",
        "",
        "class TestDocumentMatch(unittest.TestCase):",
        "    \"\"\"Test DocumentMatch dataclass.\"\"\"",
        "",
        "    def test_creation_minimal(self):",
        "        \"\"\"Test creating with required fields only.\"\"\"",
        "        match = DocumentMatch(doc_id=\"doc1\", score=0.95)",
        "        self.assertEqual(match.doc_id, \"doc1\")",
        "        self.assertEqual(match.score, 0.95)",
        "        self.assertIsNone(match.metadata)",
        "",
        "    def test_creation_with_metadata(self):",
        "        \"\"\"Test creating with metadata.\"\"\"",
        "        match = DocumentMatch(",
        "            doc_id=\"doc1\",",
        "            score=0.95,",
        "            metadata={\"author\": \"test\"}",
        "        )",
        "        self.assertEqual(match.metadata[\"author\"], \"test\")",
        "",
        "    def test_immutable(self):",
        "        \"\"\"Test that dataclass is frozen.\"\"\"",
        "        match = DocumentMatch(doc_id=\"doc1\", score=0.95)",
        "        with self.assertRaises(AttributeError):",
        "            match.score = 0.5",
        "",
        "    def test_repr(self):",
        "        \"\"\"Test string representation.\"\"\"",
        "        match = DocumentMatch(doc_id=\"doc1\", score=0.95)",
        "        repr_str = repr(match)",
        "        self.assertIn(\"doc1\", repr_str)",
        "        self.assertIn(\"0.95\", repr_str)",
        "",
        "    def test_to_dict(self):",
        "        \"\"\"Test conversion to dictionary.\"\"\"",
        "        match = DocumentMatch(doc_id=\"doc1\", score=0.95)",
        "        d = match.to_dict()",
        "        self.assertEqual(d[\"doc_id\"], \"doc1\")",
        "        self.assertEqual(d[\"score\"], 0.95)",
        "",
        "    def test_to_tuple(self):",
        "        \"\"\"Test conversion to tuple.\"\"\"",
        "        match = DocumentMatch(doc_id=\"doc1\", score=0.95)",
        "        t = match.to_tuple()",
        "        self.assertEqual(t, (\"doc1\", 0.95))",
        "",
        "    def test_from_tuple(self):",
        "        \"\"\"Test creation from tuple arguments.\"\"\"",
        "        match = DocumentMatch.from_tuple(\"doc1\", 0.95)",
        "        self.assertEqual(match.doc_id, \"doc1\")",
        "        self.assertEqual(match.score, 0.95)",
        "",
        "    def test_from_tuple_with_metadata(self):",
        "        \"\"\"Test creation from tuple with metadata.\"\"\"",
        "        match = DocumentMatch.from_tuple(\"doc1\", 0.95, {\"key\": \"value\"})",
        "        self.assertEqual(match.metadata[\"key\"], \"value\")",
        "",
        "    def test_from_dict(self):",
        "        \"\"\"Test creation from dictionary.\"\"\"",
        "        match = DocumentMatch.from_dict({\"doc_id\": \"doc1\", \"score\": 0.95})",
        "        self.assertEqual(match.doc_id, \"doc1\")",
        "        self.assertEqual(match.score, 0.95)",
        "",
        "    def test_roundtrip_dict(self):",
        "        \"\"\"Test dict roundtrip preserves data.\"\"\"",
        "        original = DocumentMatch(doc_id=\"doc1\", score=0.95, metadata={\"key\": \"value\"})",
        "        restored = DocumentMatch.from_dict(original.to_dict())",
        "        self.assertEqual(original.doc_id, restored.doc_id)",
        "        self.assertEqual(original.score, restored.score)",
        "        self.assertEqual(original.metadata, restored.metadata)",
        "",
        "",
        "class TestPassageMatch(unittest.TestCase):",
        "    \"\"\"Test PassageMatch dataclass.\"\"\"",
        "",
        "    def test_creation_minimal(self):",
        "        \"\"\"Test creating with required fields.\"\"\"",
        "        match = PassageMatch(",
        "            doc_id=\"doc1\",",
        "            text=\"Sample text\",",
        "            score=0.85,",
        "            start=0,",
        "            end=11",
        "        )",
        "        self.assertEqual(match.doc_id, \"doc1\")",
        "        self.assertEqual(match.text, \"Sample text\")",
        "        self.assertEqual(match.score, 0.85)",
        "        self.assertEqual(match.start, 0)",
        "        self.assertEqual(match.end, 11)",
        "",
        "    def test_creation_with_metadata(self):",
        "        \"\"\"Test creating with metadata.\"\"\"",
        "        match = PassageMatch(",
        "            doc_id=\"doc1\",",
        "            text=\"Sample text\",",
        "            score=0.85,",
        "            start=0,",
        "            end=11,",
        "            metadata={\"highlight\": True}",
        "        )",
        "        self.assertTrue(match.metadata[\"highlight\"])",
        "",
        "    def test_immutable(self):",
        "        \"\"\"Test that dataclass is frozen.\"\"\"",
        "        match = PassageMatch(doc_id=\"doc1\", text=\"Text\", score=0.5, start=0, end=4)",
        "        with self.assertRaises(AttributeError):",
        "            match.text = \"Changed\"",
        "",
        "    def test_location_property(self):",
        "        \"\"\"Test location property for citations.\"\"\"",
        "        match = PassageMatch(",
        "            doc_id=\"file.py\",",
        "            text=\"Sample\",",
        "            score=0.5,",
        "            start=100,",
        "            end=150",
        "        )",
        "        self.assertEqual(match.location, \"file.py:100-150\")",
        "",
        "    def test_length_property(self):",
        "        \"\"\"Test length property.\"\"\"",
        "        match = PassageMatch(doc_id=\"doc1\", text=\"Test\", score=0.5, start=10, end=60)",
        "        self.assertEqual(match.length, 50)",
        "",
        "    def test_to_dict(self):",
        "        \"\"\"Test conversion to dictionary.\"\"\"",
        "        match = PassageMatch(doc_id=\"doc1\", text=\"Text\", score=0.5, start=0, end=4)",
        "        d = match.to_dict()",
        "        self.assertEqual(d[\"doc_id\"], \"doc1\")",
        "        self.assertEqual(d[\"text\"], \"Text\")",
        "        self.assertEqual(d[\"score\"], 0.5)",
        "        self.assertEqual(d[\"start\"], 0)",
        "        self.assertEqual(d[\"end\"], 4)",
        "",
        "    def test_to_tuple(self):",
        "        \"\"\"Test conversion to tuple (doc_id, text, start, end, score).\"\"\"",
        "        match = PassageMatch(doc_id=\"doc1\", text=\"Text\", score=0.5, start=0, end=4)",
        "        t = match.to_tuple()",
        "        # Order is: doc_id, text, start, end, score",
        "        self.assertEqual(t, (\"doc1\", \"Text\", 0, 4, 0.5))",
        "",
        "    def test_from_tuple(self):",
        "        \"\"\"Test creation from tuple arguments.\"\"\"",
        "        # Order is: doc_id, text, start, end, score",
        "        match = PassageMatch.from_tuple(\"doc1\", \"Text\", 0, 4, 0.5)",
        "        self.assertEqual(match.doc_id, \"doc1\")",
        "        self.assertEqual(match.text, \"Text\")",
        "        self.assertEqual(match.start, 0)",
        "        self.assertEqual(match.end, 4)",
        "        self.assertEqual(match.score, 0.5)",
        "",
        "    def test_from_dict(self):",
        "        \"\"\"Test creation from dictionary.\"\"\"",
        "        match = PassageMatch.from_dict({",
        "            \"doc_id\": \"doc1\",",
        "            \"text\": \"Text\",",
        "            \"score\": 0.5,",
        "            \"start\": 0,",
        "            \"end\": 4",
        "        })",
        "        self.assertEqual(match.doc_id, \"doc1\")",
        "",
        "    def test_repr_truncates_long_text(self):",
        "        \"\"\"Test repr truncates long text.\"\"\"",
        "        long_text = \"A\" * 100",
        "        match = PassageMatch(doc_id=\"doc1\", text=long_text, score=0.5, start=0, end=100)",
        "        repr_str = repr(match)",
        "        self.assertLess(len(repr_str), len(long_text) + 100)",
        "",
        "",
        "class TestQueryResult(unittest.TestCase):",
        "    \"\"\"Test QueryResult wrapper.\"\"\"",
        "",
        "    def test_creation_with_document_matches(self):",
        "        \"\"\"Test creating with document matches.\"\"\"",
        "        matches = [",
        "            DocumentMatch(doc_id=\"doc1\", score=0.9),",
        "            DocumentMatch(doc_id=\"doc2\", score=0.7)",
        "        ]",
        "        result = QueryResult(query=\"test\", matches=matches)",
        "        self.assertEqual(result.query, \"test\")",
        "        self.assertEqual(len(result.matches), 2)",
        "",
        "    def test_creation_with_passage_matches(self):",
        "        \"\"\"Test creating with passage matches.\"\"\"",
        "        matches = [",
        "            PassageMatch(doc_id=\"doc1\", text=\"Text\", score=0.9, start=0, end=4)",
        "        ]",
        "        result = QueryResult(query=\"test\", matches=matches)",
        "        self.assertEqual(len(result.matches), 1)",
        "",
        "    def test_creation_with_all_fields(self):",
        "        \"\"\"Test creating with all optional fields.\"\"\"",
        "        result = QueryResult(",
        "            query=\"test\",",
        "            matches=[DocumentMatch(doc_id=\"doc1\", score=0.9)],",
        "            expansion_terms={\"test\": 1.0, \"testing\": 0.8},",
        "            timing_ms=15.5,",
        "            metadata={\"source\": \"api\"}",
        "        )",
        "        self.assertEqual(result.expansion_terms[\"testing\"], 0.8)",
        "        self.assertEqual(result.timing_ms, 15.5)",
        "",
        "    def test_top_match_property(self):",
        "        \"\"\"Test top_match returns highest scoring.\"\"\"",
        "        matches = [",
        "            DocumentMatch(doc_id=\"doc1\", score=0.5),",
        "            DocumentMatch(doc_id=\"doc2\", score=0.9),",
        "            DocumentMatch(doc_id=\"doc3\", score=0.7)",
        "        ]",
        "        result = QueryResult(query=\"test\", matches=matches)",
        "        self.assertEqual(result.top_match.doc_id, \"doc2\")",
        "",
        "    def test_top_match_empty(self):",
        "        \"\"\"Test top_match with no matches.\"\"\"",
        "        result = QueryResult(query=\"test\", matches=[])",
        "        self.assertIsNone(result.top_match)",
        "",
        "    def test_match_count_property(self):",
        "        \"\"\"Test match_count property.\"\"\"",
        "        matches = [",
        "            DocumentMatch(doc_id=\"doc1\", score=0.5),",
        "            DocumentMatch(doc_id=\"doc2\", score=0.9)",
        "        ]",
        "        result = QueryResult(query=\"test\", matches=matches)",
        "        self.assertEqual(result.match_count, 2)",
        "",
        "    def test_average_score_property(self):",
        "        \"\"\"Test average_score calculation.\"\"\"",
        "        matches = [",
        "            DocumentMatch(doc_id=\"doc1\", score=0.4),",
        "            DocumentMatch(doc_id=\"doc2\", score=0.6)",
        "        ]",
        "        result = QueryResult(query=\"test\", matches=matches)",
        "        self.assertEqual(result.average_score, 0.5)",
        "",
        "    def test_average_score_empty(self):",
        "        \"\"\"Test average_score with no matches.\"\"\"",
        "        result = QueryResult(query=\"test\", matches=[])",
        "        self.assertEqual(result.average_score, 0.0)",
        "",
        "    def test_to_dict(self):",
        "        \"\"\"Test conversion to dictionary.\"\"\"",
        "        result = QueryResult(",
        "            query=\"test\",",
        "            matches=[DocumentMatch(doc_id=\"doc1\", score=0.9)]",
        "        )",
        "        d = result.to_dict()",
        "        self.assertEqual(d[\"query\"], \"test\")",
        "        self.assertEqual(len(d[\"matches\"]), 1)",
        "",
        "    def test_from_dict_document_matches(self):",
        "        \"\"\"Test creation from dict with document matches.\"\"\"",
        "        d = {",
        "            \"query\": \"test\",",
        "            \"matches\": [{\"doc_id\": \"doc1\", \"score\": 0.9, \"metadata\": None}]",
        "        }",
        "        result = QueryResult.from_dict(d)",
        "        self.assertEqual(result.query, \"test\")",
        "        self.assertIsInstance(result.matches[0], DocumentMatch)",
        "",
        "    def test_from_dict_passage_matches(self):",
        "        \"\"\"Test creation from dict with passage matches.\"\"\"",
        "        d = {",
        "            \"query\": \"test\",",
        "            \"matches\": [{\"doc_id\": \"doc1\", \"text\": \"Text\", \"score\": 0.9, \"start\": 0, \"end\": 4, \"metadata\": None}]",
        "        }",
        "        result = QueryResult.from_dict(d)",
        "        self.assertIsInstance(result.matches[0], PassageMatch)",
        "",
        "",
        "class TestHelperFunctions(unittest.TestCase):",
        "    \"\"\"Test conversion helper functions.\"\"\"",
        "",
        "    def test_convert_document_matches_basic(self):",
        "        \"\"\"Test converting list of tuples.\"\"\"",
        "        raw = [(\"doc1\", 0.9), (\"doc2\", 0.7)]",
        "        matches = convert_document_matches(raw)",
        "        self.assertEqual(len(matches), 2)",
        "        self.assertIsInstance(matches[0], DocumentMatch)",
        "        self.assertEqual(matches[0].doc_id, \"doc1\")",
        "",
        "    def test_convert_document_matches_empty(self):",
        "        \"\"\"Test converting empty list.\"\"\"",
        "        matches = convert_document_matches([])",
        "        self.assertEqual(matches, [])",
        "",
        "    def test_convert_document_matches_with_metadata(self):",
        "        \"\"\"Test converting with metadata dict.\"\"\"",
        "        raw = [(\"doc1\", 0.9), (\"doc2\", 0.7)]",
        "        metadata = {\"doc1\": {\"author\": \"test\"}}",
        "        matches = convert_document_matches(raw, metadata)",
        "        self.assertEqual(matches[0].metadata[\"author\"], \"test\")",
        "        self.assertIsNone(matches[1].metadata)",
        "",
        "    def test_convert_passage_matches_basic(self):",
        "        \"\"\"Test converting passage tuples (doc_id, text, start, end, score).\"\"\"",
        "        raw = [(\"doc1\", \"Sample\", 0, 6, 0.9)]",
        "        matches = convert_passage_matches(raw)",
        "        self.assertEqual(len(matches), 1)",
        "        self.assertIsInstance(matches[0], PassageMatch)",
        "        self.assertEqual(matches[0].text, \"Sample\")",
        "",
        "    def test_convert_passage_matches_empty(self):",
        "        \"\"\"Test converting empty list.\"\"\"",
        "        matches = convert_passage_matches([])",
        "        self.assertEqual(matches, [])",
        "",
        "",
        "class TestIntegration(unittest.TestCase):",
        "    \"\"\"Test integration with CorticalTextProcessor.\"\"\"",
        "",
        "    def test_document_search_workflow(self):",
        "        \"\"\"Test converting actual search results.\"\"\"",
        "        from cortical import CorticalTextProcessor",
        "",
        "        proc = CorticalTextProcessor()",
        "        proc.process_document(\"doc1\", \"Neural networks process data\")",
        "        proc.process_document(\"doc2\", \"Machine learning is powerful\")",
        "        proc.compute_all()",
        "",
        "        raw_results = proc.find_documents_for_query(\"neural\", top_n=2)",
        "        matches = convert_document_matches(raw_results)",
        "",
        "        self.assertGreater(len(matches), 0)",
        "        self.assertIsInstance(matches[0], DocumentMatch)",
        "        self.assertIsInstance(matches[0].score, float)",
        "",
        "",
        "if __name__ == '__main__':",
        "    unittest.main()"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "tests/unit/test_fluent.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "\"\"\"",
        "Unit tests for the FluentProcessor API.",
        "",
        "Tests the fluent/chainable interface for CorticalTextProcessor.",
        "\"\"\"",
        "",
        "import pytest",
        "import tempfile",
        "from pathlib import Path",
        "",
        "from cortical import CorticalTextProcessor, CorticalConfig, Tokenizer",
        "from cortical.fluent import FluentProcessor",
        "",
        "",
        "class TestFluentProcessorBasics:",
        "    \"\"\"Test basic FluentProcessor initialization and properties.\"\"\"",
        "",
        "    def test_init_default(self):",
        "        \"\"\"Test default initialization.\"\"\"",
        "        processor = FluentProcessor()",
        "        assert processor is not None",
        "        assert isinstance(processor.processor, CorticalTextProcessor)",
        "        assert not processor.is_built",
        "",
        "    def test_init_with_config(self):",
        "        \"\"\"Test initialization with custom config.\"\"\"",
        "        config = CorticalConfig(pagerank_damping=0.9)",
        "        processor = FluentProcessor(config=config)",
        "        assert processor.processor.config.pagerank_damping == 0.9",
        "",
        "    def test_init_with_tokenizer(self):",
        "        \"\"\"Test initialization with custom tokenizer.\"\"\"",
        "        tokenizer = Tokenizer(split_identifiers=True)",
        "        processor = FluentProcessor(tokenizer=tokenizer)",
        "        assert processor.processor.tokenizer.split_identifiers",
        "",
        "    def test_from_existing(self):",
        "        \"\"\"Test creating FluentProcessor from existing processor.\"\"\"",
        "        raw = CorticalTextProcessor()",
        "        raw.process_document(\"doc1\", \"test content\")",
        "",
        "        fluent = FluentProcessor.from_existing(raw)",
        "        assert fluent.processor is raw",
        "        assert len(fluent.processor.documents) == 1",
        "",
        "    def test_repr(self):",
        "        \"\"\"Test string representation.\"\"\"",
        "        processor = FluentProcessor()",
        "        assert \"documents=0\" in repr(processor)",
        "        assert \"not built\" in repr(processor)",
        "",
        "        processor.add_document(\"doc1\", \"test\")",
        "        assert \"documents=1\" in repr(processor)",
        "",
        "",
        "class TestFluentProcessorChaining:",
        "    \"\"\"Test method chaining functionality.\"\"\"",
        "",
        "    def test_add_document_returns_self(self):",
        "        \"\"\"Test that add_document returns self for chaining.\"\"\"",
        "        processor = FluentProcessor()",
        "        result = processor.add_document(\"doc1\", \"content\")",
        "        assert result is processor",
        "",
        "    def test_add_documents_dict_returns_self(self):",
        "        \"\"\"Test that add_documents returns self for chaining.\"\"\"",
        "        processor = FluentProcessor()",
        "        result = processor.add_documents({\"doc1\": \"content1\", \"doc2\": \"content2\"})",
        "        assert result is processor",
        "",
        "    def test_build_returns_self(self):",
        "        \"\"\"Test that build returns self for chaining.\"\"\"",
        "        processor = FluentProcessor()",
        "        processor.add_document(\"doc1\", \"neural networks process information\")",
        "        result = processor.build(verbose=False)",
        "        assert result is processor",
        "        assert processor.is_built",
        "",
        "    def test_save_returns_self(self):",
        "        \"\"\"Test that save returns self for chaining.\"\"\"",
        "        with tempfile.NamedTemporaryFile(suffix='.pkl', delete=False) as f:",
        "            temp_path = f.name",
        "",
        "        try:",
        "            processor = FluentProcessor()",
        "            processor.add_document(\"doc1\", \"test content\")",
        "            processor.build(verbose=False)",
        "            result = processor.save(temp_path)",
        "            assert result is processor",
        "        finally:",
        "            Path(temp_path).unlink(missing_ok=True)",
        "",
        "    def test_with_config_returns_self(self):",
        "        \"\"\"Test that with_config returns self for chaining.\"\"\"",
        "        processor = FluentProcessor()",
        "        config = CorticalConfig(pagerank_iterations=30)",
        "        result = processor.with_config(config)",
        "        assert result is processor",
        "",
        "    def test_with_tokenizer_returns_self(self):",
        "        \"\"\"Test that with_tokenizer returns self for chaining.\"\"\"",
        "        processor = FluentProcessor()",
        "        tokenizer = Tokenizer(split_identifiers=True)",
        "        result = processor.with_tokenizer(tokenizer)",
        "        assert result is processor",
        "",
        "    def test_full_chain(self):",
        "        \"\"\"Test a complete method chain.\"\"\"",
        "        result = (FluentProcessor()",
        "            .add_document(\"doc1\", \"neural networks process information efficiently\")",
        "            .add_document(\"doc2\", \"deep learning uses neural network architectures\")",
        "            .build(verbose=False)",
        "            .search(\"neural processing\"))",
        "",
        "        assert isinstance(result, list)",
        "        assert len(result) > 0",
        "",
        "",
        "class TestFluentProcessorDocuments:",
        "    \"\"\"Test document addition methods.\"\"\"",
        "",
        "    def test_add_document_single(self):",
        "        \"\"\"Test adding a single document.\"\"\"",
        "        processor = FluentProcessor()",
        "        processor.add_document(\"doc1\", \"test content\")",
        "        assert len(processor.processor.documents) == 1",
        "        assert processor.processor.documents[\"doc1\"] == \"test content\"",
        "        assert not processor.is_built",
        "",
        "    def test_add_document_with_metadata(self):",
        "        \"\"\"Test adding document with metadata.\"\"\"",
        "        processor = FluentProcessor()",
        "        metadata = {\"author\": \"Alice\", \"date\": \"2025-01-01\"}",
        "        processor.add_document(\"doc1\", \"test content\", metadata=metadata)",
        "        assert processor.processor.document_metadata[\"doc1\"][\"author\"] == \"Alice\"",
        "",
        "    def test_add_documents_from_dict(self):",
        "        \"\"\"Test adding multiple documents from dict.\"\"\"",
        "        processor = FluentProcessor()",
        "        docs = {",
        "            \"doc1\": \"content 1\",",
        "            \"doc2\": \"content 2\",",
        "            \"doc3\": \"content 3\"",
        "        }",
        "        processor.add_documents(docs)",
        "        assert len(processor.processor.documents) == 3",
        "        assert processor.processor.documents[\"doc2\"] == \"content 2\"",
        "",
        "    def test_add_documents_from_tuples(self):",
        "        \"\"\"Test adding documents from list of tuples.\"\"\"",
        "        processor = FluentProcessor()",
        "        docs = [",
        "            (\"doc1\", \"content 1\"),",
        "            (\"doc2\", \"content 2\")",
        "        ]",
        "        processor.add_documents(docs)",
        "        assert len(processor.processor.documents) == 2",
        "",
        "    def test_add_documents_from_tuples_with_metadata(self):",
        "        \"\"\"Test adding documents from tuples with metadata.\"\"\"",
        "        processor = FluentProcessor()",
        "        docs = [",
        "            (\"doc1\", \"content 1\", {\"author\": \"Alice\"}),",
        "            (\"doc2\", \"content 2\", {\"author\": \"Bob\"})",
        "        ]",
        "        processor.add_documents(docs)",
        "        assert processor.processor.document_metadata[\"doc1\"][\"author\"] == \"Alice\"",
        "        assert processor.processor.document_metadata[\"doc2\"][\"author\"] == \"Bob\"",
        "",
        "    def test_add_documents_invalid_type(self):",
        "        \"\"\"Test that invalid document type raises error.\"\"\"",
        "        processor = FluentProcessor()",
        "        with pytest.raises(TypeError):",
        "            processor.add_documents(\"invalid\")",
        "",
        "    def test_add_documents_invalid_tuple_length(self):",
        "        \"\"\"Test that invalid tuple length raises error.\"\"\"",
        "        processor = FluentProcessor()",
        "        with pytest.raises(ValueError, match=\"Invalid document tuple\"):",
        "            processor.add_documents([(\"doc1\",)])  # Too short",
        "",
        "",
        "class TestFluentProcessorFiles:",
        "    \"\"\"Test file and directory loading methods.\"\"\"",
        "",
        "    def test_from_files(self):",
        "        \"\"\"Test loading from file list.\"\"\"",
        "        with tempfile.TemporaryDirectory() as tmpdir:",
        "            # Create test files",
        "            file1 = Path(tmpdir) / \"doc1.txt\"",
        "            file2 = Path(tmpdir) / \"doc2.txt\"",
        "            file1.write_text(\"content 1\")",
        "            file2.write_text(\"content 2\")",
        "",
        "            processor = FluentProcessor.from_files([file1, file2])",
        "            assert len(processor.processor.documents) == 2",
        "            assert \"doc1\" in processor.processor.documents",
        "            assert \"doc2\" in processor.processor.documents",
        "            assert processor.processor.documents[\"doc1\"] == \"content 1\"",
        "",
        "    def test_from_files_missing_file(self):",
        "        \"\"\"Test that missing file raises error.\"\"\"",
        "        with pytest.raises(FileNotFoundError):",
        "            FluentProcessor.from_files([\"/nonexistent/file.txt\"])",
        "",
        "    def test_from_files_not_a_file(self):",
        "        \"\"\"Test that directory path raises error.\"\"\"",
        "        with tempfile.TemporaryDirectory() as tmpdir:",
        "            with pytest.raises(ValueError, match=\"Not a file\"):",
        "                FluentProcessor.from_files([tmpdir])",
        "",
        "    def test_from_directory_default_pattern(self):",
        "        \"\"\"Test loading from directory with default pattern.\"\"\"",
        "        with tempfile.TemporaryDirectory() as tmpdir:",
        "            # Create test files",
        "            (Path(tmpdir) / \"doc1.txt\").write_text(\"content 1\")",
        "            (Path(tmpdir) / \"doc2.txt\").write_text(\"content 2\")",
        "            (Path(tmpdir) / \"readme.md\").write_text(\"readme content\")",
        "",
        "            processor = FluentProcessor.from_directory(tmpdir)",
        "            assert len(processor.processor.documents) == 2  # Only .txt files",
        "            assert \"doc1\" in processor.processor.documents",
        "",
        "    def test_from_directory_custom_pattern(self):",
        "        \"\"\"Test loading from directory with custom pattern.\"\"\"",
        "        with tempfile.TemporaryDirectory() as tmpdir:",
        "            (Path(tmpdir) / \"doc1.txt\").write_text(\"content 1\")",
        "            (Path(tmpdir) / \"readme.md\").write_text(\"readme content\")",
        "",
        "            processor = FluentProcessor.from_directory(tmpdir, pattern=\"*.md\")",
        "            assert len(processor.processor.documents) == 1",
        "            assert \"readme\" in processor.processor.documents",
        "",
        "    def test_from_directory_recursive(self):",
        "        \"\"\"Test recursive directory loading.\"\"\"",
        "        with tempfile.TemporaryDirectory() as tmpdir:",
        "            tmppath = Path(tmpdir)",
        "            (tmppath / \"doc1.txt\").write_text(\"content 1\")",
        "            subdir = tmppath / \"subdir\"",
        "            subdir.mkdir()",
        "            (subdir / \"doc2.txt\").write_text(\"content 2\")",
        "",
        "            processor = FluentProcessor.from_directory(tmpdir, recursive=True)",
        "            assert len(processor.processor.documents) == 2",
        "",
        "    def test_from_directory_not_recursive(self):",
        "        \"\"\"Test non-recursive directory loading.\"\"\"",
        "        with tempfile.TemporaryDirectory() as tmpdir:",
        "            tmppath = Path(tmpdir)",
        "            (tmppath / \"doc1.txt\").write_text(\"content 1\")",
        "            subdir = tmppath / \"subdir\"",
        "            subdir.mkdir()",
        "            (subdir / \"doc2.txt\").write_text(\"content 2\")",
        "",
        "            processor = FluentProcessor.from_directory(tmpdir, recursive=False)",
        "            assert len(processor.processor.documents) == 1  # Only top-level",
        "",
        "    def test_from_directory_missing(self):",
        "        \"\"\"Test that missing directory raises error.\"\"\"",
        "        with pytest.raises(FileNotFoundError):",
        "            FluentProcessor.from_directory(\"/nonexistent/directory\")",
        "",
        "    def test_from_directory_not_a_directory(self):",
        "        \"\"\"Test that file path raises error.\"\"\"",
        "        with tempfile.NamedTemporaryFile() as f:",
        "            with pytest.raises(ValueError, match=\"Not a directory\"):",
        "                FluentProcessor.from_directory(f.name)",
        "",
        "    def test_from_directory_no_matches(self):",
        "        \"\"\"Test that no matching files raises error.\"\"\"",
        "        with tempfile.TemporaryDirectory() as tmpdir:",
        "            with pytest.raises(ValueError, match=\"No files matching pattern\"):",
        "                FluentProcessor.from_directory(tmpdir, pattern=\"*.xyz\")",
        "",
        "",
        "class TestFluentProcessorBuild:",
        "    \"\"\"Test build functionality.\"\"\"",
        "",
        "    def test_build_marks_as_built(self):",
        "        \"\"\"Test that build marks processor as built.\"\"\"",
        "        processor = FluentProcessor()",
        "        processor.add_document(\"doc1\", \"neural networks process information\")",
        "        assert not processor.is_built",
        "",
        "        processor.build(verbose=False)",
        "        assert processor.is_built",
        "",
        "    def test_build_with_options(self):",
        "        \"\"\"Test build with various options.\"\"\"",
        "        processor = FluentProcessor()",
        "        processor.add_document(\"doc1\", \"neural networks and deep learning\")",
        "        processor.add_document(\"doc2\", \"machine learning algorithms\")",
        "",
        "        processor.build(",
        "            verbose=False,",
        "            build_concepts=True,",
        "            cluster_strictness=0.8,",
        "            bridge_weight=0.1",
        "        )",
        "        assert processor.is_built",
        "",
        "    def test_add_document_after_build_marks_stale(self):",
        "        \"\"\"Test that adding document after build marks as not built.\"\"\"",
        "        processor = FluentProcessor()",
        "        processor.add_document(\"doc1\", \"content\")",
        "        processor.build(verbose=False)",
        "        assert processor.is_built",
        "",
        "        processor.add_document(\"doc2\", \"more content\")",
        "        assert not processor.is_built",
        "",
        "",
        "class TestFluentProcessorSearch:",
        "    \"\"\"Test search methods.\"\"\"",
        "",
        "    def test_search_basic(self):",
        "        \"\"\"Test basic search.\"\"\"",
        "        processor = (FluentProcessor()",
        "            .add_document(\"doc1\", \"neural networks process information efficiently\")",
        "            .add_document(\"doc2\", \"deep learning uses neural architectures\")",
        "            .build(verbose=False))",
        "",
        "        results = processor.search(\"neural\")",
        "        assert isinstance(results, list)",
        "        assert len(results) > 0",
        "        assert all(isinstance(r, tuple) and len(r) == 2 for r in results)",
        "",
        "    def test_search_with_options(self):",
        "        \"\"\"Test search with custom options.\"\"\"",
        "        processor = (FluentProcessor()",
        "            .add_document(\"doc1\", \"neural networks\")",
        "            .add_document(\"doc2\", \"machine learning\")",
        "            .build(verbose=False))",
        "",
        "        results = processor.search(\"neural\", top_n=1, use_expansion=False)",
        "        assert len(results) <= 1",
        "",
        "    def test_fast_search(self):",
        "        \"\"\"Test fast search.\"\"\"",
        "        processor = (FluentProcessor()",
        "            .add_document(\"doc1\", \"authentication and authorization systems\")",
        "            .add_document(\"doc2\", \"database query optimization\")",
        "            .build(verbose=False))",
        "",
        "        results = processor.fast_search(\"authentication\", top_n=5)",
        "        assert isinstance(results, list)",
        "",
        "    def test_search_passages(self):",
        "        \"\"\"Test passage search.\"\"\"",
        "        processor = (FluentProcessor()",
        "            .add_document(\"doc1\", \"Neural networks are computational models. They process information efficiently. Deep learning uses these architectures.\")",
        "            .build(verbose=False))",
        "",
        "        results = processor.search_passages(\"neural networks\", top_n=2)",
        "        assert isinstance(results, list)",
        "        if results:  # May be empty for short documents",
        "            assert all(isinstance(r, tuple) and len(r) == 5 for r in results)",
        "            # Verify structure: (doc_id, passage_text, start_pos, end_pos, score)",
        "            for doc_id, passage_text, start_pos, end_pos, score in results:",
        "                assert isinstance(doc_id, str)",
        "                assert isinstance(passage_text, str)",
        "                assert isinstance(start_pos, int)",
        "                assert isinstance(end_pos, int)",
        "                assert isinstance(score, float)",
        "",
        "    def test_expand_query(self):",
        "        \"\"\"Test query expansion.\"\"\"",
        "        processor = (FluentProcessor()",
        "            .add_document(\"doc1\", \"neural networks and deep learning systems\")",
        "            .add_document(\"doc2\", \"machine learning algorithms and models\")",
        "            .build(verbose=False))",
        "",
        "        expansions = processor.expand(\"neural\", max_expansions=5)",
        "        assert isinstance(expansions, dict)",
        "        assert \"neural\" in expansions",
        "",
        "",
        "class TestFluentProcessorPersistence:",
        "    \"\"\"Test save and load functionality.\"\"\"",
        "",
        "    def test_save_and_load(self):",
        "        \"\"\"Test saving and loading processor.\"\"\"",
        "        with tempfile.NamedTemporaryFile(suffix='.pkl', delete=False) as f:",
        "            temp_path = f.name",
        "",
        "        try:",
        "            # Create and save",
        "            (FluentProcessor()",
        "                .add_document(\"doc1\", \"test content here\")",
        "                .build(verbose=False)",
        "                .save(temp_path))",
        "",
        "            # Load",
        "            loaded = FluentProcessor.load(temp_path)",
        "            assert loaded.is_built",
        "            assert len(loaded.processor.documents) == 1",
        "            assert \"doc1\" in loaded.processor.documents",
        "",
        "            # Can search immediately",
        "            results = loaded.search(\"test\")",
        "            assert isinstance(results, list)",
        "        finally:",
        "            Path(temp_path).unlink(missing_ok=True)",
        "",
        "    def test_load_marks_as_built(self):",
        "        \"\"\"Test that loading marks processor as built.\"\"\"",
        "        with tempfile.NamedTemporaryFile(suffix='.pkl', delete=False) as f:",
        "            temp_path = f.name",
        "",
        "        try:",
        "            FluentProcessor().add_document(\"doc1\", \"content\").build(verbose=False).save(temp_path)",
        "            loaded = FluentProcessor.load(temp_path)",
        "            assert loaded.is_built",
        "        finally:",
        "            Path(temp_path).unlink(missing_ok=True)",
        "",
        "",
        "class TestFluentProcessorConfiguration:",
        "    \"\"\"Test configuration methods.\"\"\"",
        "",
        "    def test_with_config(self):",
        "        \"\"\"Test setting configuration.\"\"\"",
        "        config = CorticalConfig(pagerank_damping=0.9, pagerank_iterations=30)",
        "        processor = (FluentProcessor()",
        "            .with_config(config)",
        "            .add_document(\"doc1\", \"test content\"))",
        "",
        "        assert processor.processor.config.pagerank_damping == 0.9",
        "        assert processor.processor.config.pagerank_iterations == 30",
        "",
        "    def test_with_tokenizer(self):",
        "        \"\"\"Test setting custom tokenizer.\"\"\"",
        "        tokenizer = Tokenizer(split_identifiers=True)",
        "        processor = (FluentProcessor()",
        "            .with_tokenizer(tokenizer)",
        "            .add_document(\"doc1\", \"getUserCredentials\"))",
        "",
        "        assert processor.processor.tokenizer.split_identifiers",
        "",
        "",
        "class TestFluentProcessorExamples:",
        "    \"\"\"Test example usage patterns from documentation.\"\"\"",
        "",
        "    def test_readme_example(self):",
        "        \"\"\"Test the example from README.\"\"\"",
        "        results = (FluentProcessor()",
        "            .add_document(\"doc1\", \"Neural networks process information\")",
        "            .add_document(\"doc2\", \"Deep learning uses neural architectures\")",
        "            .build(verbose=False)",
        "            .search(\"neural processing\", top_n=5))",
        "",
        "        assert isinstance(results, list)",
        "        assert len(results) > 0",
        "",
        "    def test_chained_operations(self):",
        "        \"\"\"Test complex chained operations.\"\"\"",
        "        with tempfile.NamedTemporaryFile(suffix='.pkl', delete=False) as f:",
        "            temp_path = f.name",
        "",
        "        try:",
        "            processor = (FluentProcessor()",
        "                .add_documents({",
        "                    \"doc1\": \"neural networks and deep learning\",",
        "                    \"doc2\": \"machine learning algorithms\",",
        "                    \"doc3\": \"artificial intelligence systems\"",
        "                })",
        "                .build(verbose=False)",
        "                .save(temp_path))",
        "",
        "            # Search on built processor",
        "            results = processor.search(\"neural\", top_n=2)",
        "            assert len(results) <= 2",
        "",
        "            # Expand query",
        "            expanded = processor.expand(\"learning\")",
        "            assert isinstance(expanded, dict)",
        "        finally:",
        "            Path(temp_path).unlink(missing_ok=True)",
        "",
        "    def test_from_files_workflow(self):",
        "        \"\"\"Test complete workflow with file loading.\"\"\"",
        "        with tempfile.TemporaryDirectory() as tmpdir:",
        "            # Create test files",
        "            (Path(tmpdir) / \"doc1.txt\").write_text(\"Neural networks are powerful\")",
        "            (Path(tmpdir) / \"doc2.txt\").write_text(\"Deep learning is effective\")",
        "",
        "            results = (FluentProcessor",
        "                .from_directory(tmpdir)",
        "                .build(verbose=False)",
        "                .search(\"neural\"))",
        "",
        "            assert isinstance(results, list)",
        "            assert len(results) > 0"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "tests/unit/test_progress.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "\"\"\"",
        "Unit tests for progress reporting infrastructure.",
        "",
        "Tests the progress reporting system including:",
        "- ConsoleProgressReporter formatting",
        "- CallbackProgressReporter callback invocation",
        "- SilentProgressReporter no-op behavior",
        "- MultiPhaseProgress phase tracking",
        "- Integration with CorticalTextProcessor.compute_all()",
        "\"\"\"",
        "",
        "import unittest",
        "import io",
        "import time",
        "from unittest.mock import Mock, call",
        "",
        "from cortical.progress import (",
        "    ConsoleProgressReporter,",
        "    CallbackProgressReporter,",
        "    SilentProgressReporter,",
        "    MultiPhaseProgress,",
        ")",
        "from cortical import CorticalTextProcessor",
        "",
        "",
        "class TestConsoleProgressReporter(unittest.TestCase):",
        "    \"\"\"Test console-based progress reporting.\"\"\"",
        "",
        "    def test_update_formats_correctly(self):",
        "        \"\"\"Test that update() formats output correctly.\"\"\"",
        "        buffer = io.StringIO()",
        "        reporter = ConsoleProgressReporter(file=buffer, width=20, show_eta=False)",
        "",
        "        reporter.update(\"Test phase\", 50.0)",
        "",
        "        output = buffer.getvalue()",
        "        self.assertIn(\"Test phase\", output)",
        "        self.assertIn(\"50%\", output)",
        "        self.assertIn(\"[\", output)",
        "        self.assertIn(\"]\", output)",
        "",
        "    def test_update_with_message(self):",
        "        \"\"\"Test that custom messages are included.\"\"\"",
        "        buffer = io.StringIO()",
        "        reporter = ConsoleProgressReporter(file=buffer, width=20, show_eta=False)",
        "",
        "        reporter.update(\"Test phase\", 75.0, \"custom message\")",
        "",
        "        output = buffer.getvalue()",
        "        self.assertIn(\"custom message\", output)",
        "",
        "    def test_complete_shows_100_percent(self):",
        "        \"\"\"Test that complete() shows 100% and newline.\"\"\"",
        "        buffer = io.StringIO()",
        "        reporter = ConsoleProgressReporter(file=buffer, width=20, show_eta=False)",
        "",
        "        reporter.update(\"Test phase\", 50.0)",
        "        reporter.complete(\"Test phase\")",
        "",
        "        output = buffer.getvalue()",
        "        self.assertIn(\"100%\", output)",
        "        self.assertTrue(output.endswith(\"\\n\"))",
        "",
        "    def test_complete_shows_elapsed_time(self):",
        "        \"\"\"Test that complete() shows elapsed time.\"\"\"",
        "        buffer = io.StringIO()",
        "        reporter = ConsoleProgressReporter(file=buffer, width=20, show_eta=False)",
        "",
        "        reporter.update(\"Test phase\", 50.0)",
        "        time.sleep(0.1)",
        "        reporter.complete(\"Test phase\")",
        "",
        "        output = buffer.getvalue()",
        "        # Should contain time in seconds",
        "        self.assertRegex(output, r\"\\(\\d+\\.\\d+s\\)\")",
        "",
        "    def test_complete_with_message(self):",
        "        \"\"\"Test that completion messages are included.\"\"\"",
        "        buffer = io.StringIO()",
        "        reporter = ConsoleProgressReporter(file=buffer, width=20, show_eta=False)",
        "",
        "        reporter.complete(\"Test phase\", \"All done!\")",
        "",
        "        output = buffer.getvalue()",
        "        self.assertIn(\"All done!\", output)",
        "",
        "    def test_progress_bar_width(self):",
        "        \"\"\"Test that progress bar respects width parameter.\"\"\"",
        "        buffer = io.StringIO()",
        "        reporter = ConsoleProgressReporter(file=buffer, width=10, show_eta=False)",
        "",
        "        reporter.update(\"Test\", 50.0)",
        "",
        "        output = buffer.getvalue()",
        "        # Count filled and empty characters (should be 10 total)",
        "        filled = output.count(reporter.fill_char)",
        "        empty = output.count(reporter.empty_char)",
        "        self.assertEqual(filled + empty, 10)",
        "",
        "    def test_unicode_vs_ascii_mode(self):",
        "        \"\"\"Test that Unicode and ASCII modes use different characters.\"\"\"",
        "        buffer_unicode = io.StringIO()",
        "        buffer_ascii = io.StringIO()",
        "",
        "        unicode_reporter = ConsoleProgressReporter(",
        "            file=buffer_unicode, width=10, show_eta=False, use_unicode=True",
        "        )",
        "        ascii_reporter = ConsoleProgressReporter(",
        "            file=buffer_ascii, width=10, show_eta=False, use_unicode=False",
        "        )",
        "",
        "        unicode_reporter.update(\"Test\", 50.0)",
        "        ascii_reporter.update(\"Test\", 50.0)",
        "",
        "        unicode_output = buffer_unicode.getvalue()",
        "        ascii_output = buffer_ascii.getvalue()",
        "",
        "        # Unicode uses ‚ñà and ‚ñë, ASCII uses # and -",
        "        self.assertIn('‚ñà', unicode_output)",
        "        self.assertIn('#', ascii_output)",
        "        self.assertNotIn('‚ñà', ascii_output)",
        "        self.assertNotIn('#', unicode_output)",
        "",
        "    def test_percentage_clamping(self):",
        "        \"\"\"Test that percentages are clamped to 0-100 range.\"\"\"",
        "        buffer = io.StringIO()",
        "        reporter = ConsoleProgressReporter(file=buffer, width=20, show_eta=False)",
        "",
        "        # Test negative percentage",
        "        reporter.update(\"Test\", -10.0)",
        "        output = buffer.getvalue()",
        "        self.assertIn(\"0%\", output)",
        "",
        "        # Test over 100%",
        "        buffer = io.StringIO()",
        "        reporter = ConsoleProgressReporter(file=buffer, width=20, show_eta=False)",
        "        reporter.update(\"Test\", 150.0)",
        "        output = buffer.getvalue()",
        "        self.assertIn(\"100%\", output)",
        "",
        "    def test_eta_estimation(self):",
        "        \"\"\"Test that ETA is calculated and displayed.\"\"\"",
        "        buffer = io.StringIO()",
        "        reporter = ConsoleProgressReporter(file=buffer, width=20, show_eta=True)",
        "",
        "        reporter.update(\"Test\", 10.0)",
        "        time.sleep(0.2)",
        "        reporter.update(\"Test\", 20.0)",
        "",
        "        output = buffer.getvalue()",
        "        # Should contain ETA after sufficient progress",
        "        # Note: ETA may not appear on first update",
        "        if \"ETA:\" in output:",
        "            self.assertRegex(output, r\"ETA:\\s*\\d+s\")",
        "",
        "",
        "class TestCallbackProgressReporter(unittest.TestCase):",
        "    \"\"\"Test callback-based progress reporting.\"\"\"",
        "",
        "    def test_callback_invoked_on_update(self):",
        "        \"\"\"Test that callback is called with correct arguments on update.\"\"\"",
        "        callback = Mock()",
        "        reporter = CallbackProgressReporter(callback)",
        "",
        "        reporter.update(\"Test phase\", 50.0, \"message\")",
        "",
        "        callback.assert_called_once_with(\"Test phase\", 50.0, \"message\")",
        "",
        "    def test_callback_invoked_on_complete(self):",
        "        \"\"\"Test that callback is called on completion.\"\"\"",
        "        callback = Mock()",
        "        reporter = CallbackProgressReporter(callback)",
        "",
        "        reporter.complete(\"Test phase\", \"Done\")",
        "",
        "        callback.assert_called_once_with(\"Test phase\", 100.0, \"Done\")",
        "",
        "    def test_callback_with_none_message(self):",
        "        \"\"\"Test that None message is handled correctly.\"\"\"",
        "        callback = Mock()",
        "        reporter = CallbackProgressReporter(callback)",
        "",
        "        reporter.update(\"Test phase\", 50.0, None)",
        "        reporter.complete(\"Test phase\", None)",
        "",
        "        self.assertEqual(callback.call_count, 2)",
        "        # Update call",
        "        callback.assert_any_call(\"Test phase\", 50.0, None)",
        "        # Complete call with default message",
        "        callback.assert_any_call(\"Test phase\", 100.0, \"Complete\")",
        "",
        "    def test_multiple_updates(self):",
        "        \"\"\"Test that callback is invoked for multiple updates.\"\"\"",
        "        callback = Mock()",
        "        reporter = CallbackProgressReporter(callback)",
        "",
        "        reporter.update(\"Phase 1\", 25.0)",
        "        reporter.update(\"Phase 1\", 50.0)",
        "        reporter.update(\"Phase 1\", 75.0)",
        "        reporter.complete(\"Phase 1\")",
        "",
        "        self.assertEqual(callback.call_count, 4)",
        "",
        "",
        "class TestSilentProgressReporter(unittest.TestCase):",
        "    \"\"\"Test silent (no-op) progress reporter.\"\"\"",
        "",
        "    def test_update_does_nothing(self):",
        "        \"\"\"Test that update() is a no-op.\"\"\"",
        "        reporter = SilentProgressReporter()",
        "",
        "        # Should not raise any exceptions",
        "        reporter.update(\"Test\", 50.0)",
        "        reporter.update(\"Test\", 100.0, \"message\")",
        "",
        "    def test_complete_does_nothing(self):",
        "        \"\"\"Test that complete() is a no-op.\"\"\"",
        "        reporter = SilentProgressReporter()",
        "",
        "        # Should not raise any exceptions",
        "        reporter.complete(\"Test\")",
        "        reporter.complete(\"Test\", \"message\")",
        "",
        "",
        "class TestMultiPhaseProgress(unittest.TestCase):",
        "    \"\"\"Test multi-phase progress tracking.\"\"\"",
        "",
        "    def test_initialization(self):",
        "        \"\"\"Test that MultiPhaseProgress initializes correctly.\"\"\"",
        "        callback = Mock()",
        "        reporter = CallbackProgressReporter(callback)",
        "        phases = {\"Phase 1\": 30, \"Phase 2\": 70}",
        "",
        "        progress = MultiPhaseProgress(reporter, phases)",
        "",
        "        self.assertEqual(progress.overall_progress, 0.0)",
        "",
        "    def test_phase_normalization(self):",
        "        \"\"\"Test that phase weights are normalized.\"\"\"",
        "        callback = Mock()",
        "        reporter = CallbackProgressReporter(callback)",
        "        phases = {\"Phase 1\": 1, \"Phase 2\": 2, \"Phase 3\": 1}",
        "",
        "        progress = MultiPhaseProgress(reporter, phases, normalize=True)",
        "",
        "        # Should normalize to 25%, 50%, 25%",
        "        self.assertAlmostEqual(progress.phases[\"Phase 1\"], 25.0)",
        "        self.assertAlmostEqual(progress.phases[\"Phase 2\"], 50.0)",
        "        self.assertAlmostEqual(progress.phases[\"Phase 3\"], 25.0)",
        "",
        "    def test_phase_no_normalization(self):",
        "        \"\"\"Test that normalization can be disabled.\"\"\"",
        "        callback = Mock()",
        "        reporter = CallbackProgressReporter(callback)",
        "        phases = {\"Phase 1\": 10, \"Phase 2\": 20}",
        "",
        "        progress = MultiPhaseProgress(reporter, phases, normalize=False)",
        "",
        "        # Should keep original values",
        "        self.assertEqual(progress.phases[\"Phase 1\"], 10)",
        "        self.assertEqual(progress.phases[\"Phase 2\"], 20)",
        "",
        "    def test_start_phase(self):",
        "        \"\"\"Test starting a new phase.\"\"\"",
        "        callback = Mock()",
        "        reporter = CallbackProgressReporter(callback)",
        "        phases = {\"Phase 1\": 30, \"Phase 2\": 70}",
        "",
        "        progress = MultiPhaseProgress(reporter, phases)",
        "        progress.start_phase(\"Phase 1\")",
        "",
        "        # Should call reporter.update with 0%",
        "        callback.assert_called_with(\"Phase 1\", 0.0, None)",
        "",
        "    def test_start_unknown_phase_raises(self):",
        "        \"\"\"Test that starting an unknown phase raises ValueError.\"\"\"",
        "        callback = Mock()",
        "        reporter = CallbackProgressReporter(callback)",
        "        phases = {\"Phase 1\": 30, \"Phase 2\": 70}",
        "",
        "        progress = MultiPhaseProgress(reporter, phases)",
        "",
        "        with self.assertRaises(ValueError):",
        "            progress.start_phase(\"Unknown Phase\")",
        "",
        "    def test_update_within_phase(self):",
        "        \"\"\"Test updating progress within a phase.\"\"\"",
        "        callback = Mock()",
        "        reporter = CallbackProgressReporter(callback)",
        "        phases = {\"Phase 1\": 30, \"Phase 2\": 70}",
        "",
        "        progress = MultiPhaseProgress(reporter, phases)",
        "        progress.start_phase(\"Phase 1\")",
        "        progress.update(50.0)",
        "",
        "        # 50% of Phase 1 (30% weight) = 15% overall",
        "        self.assertAlmostEqual(progress.overall_progress, 15.0)",
        "",
        "    def test_complete_phase(self):",
        "        \"\"\"Test completing a phase.\"\"\"",
        "        callback = Mock()",
        "        reporter = CallbackProgressReporter(callback)",
        "        phases = {\"Phase 1\": 30, \"Phase 2\": 70}",
        "",
        "        progress = MultiPhaseProgress(reporter, phases)",
        "        progress.start_phase(\"Phase 1\")",
        "        progress.update(100.0)",
        "        progress.complete_phase(\"Done\")",
        "",
        "        # Should call callback with 100.0 and completion message",
        "        # Last call should be the completion",
        "        last_call = callback.call_args_list[-1]",
        "        self.assertEqual(last_call[0][0], \"Phase 1\")",
        "        self.assertEqual(last_call[0][1], 100.0)",
        "        self.assertEqual(last_call[0][2], \"Done\")",
        "",
        "    def test_sequential_phases(self):",
        "        \"\"\"Test progressing through multiple phases.\"\"\"",
        "        callback = Mock()",
        "        reporter = CallbackProgressReporter(callback)",
        "        phases = {\"Phase 1\": 25, \"Phase 2\": 50, \"Phase 3\": 25}",
        "",
        "        progress = MultiPhaseProgress(reporter, phases)",
        "",
        "        # Phase 1: 0% to 25%",
        "        progress.start_phase(\"Phase 1\")",
        "        self.assertAlmostEqual(progress.overall_progress, 0.0)",
        "        progress.update(100.0)",
        "        self.assertAlmostEqual(progress.overall_progress, 25.0)",
        "        progress.complete_phase()",
        "",
        "        # Phase 2: 25% to 75%",
        "        progress.start_phase(\"Phase 2\")",
        "        progress.update(50.0)",
        "        self.assertAlmostEqual(progress.overall_progress, 50.0)",
        "        progress.update(100.0)",
        "        self.assertAlmostEqual(progress.overall_progress, 75.0)",
        "        progress.complete_phase()",
        "",
        "        # Phase 3: 75% to 100%",
        "        progress.start_phase(\"Phase 3\")",
        "        progress.update(100.0)",
        "        self.assertAlmostEqual(progress.overall_progress, 100.0)",
        "        progress.complete_phase()",
        "",
        "    def test_update_with_message(self):",
        "        \"\"\"Test that messages are passed through to reporter.\"\"\"",
        "        callback = Mock()",
        "        reporter = CallbackProgressReporter(callback)",
        "        phases = {\"Phase 1\": 100}",
        "",
        "        progress = MultiPhaseProgress(reporter, phases)",
        "        progress.start_phase(\"Phase 1\")",
        "        progress.update(50.0, \"Processing...\")",
        "",
        "        # Should call reporter.update with message",
        "        callback.assert_called_with(\"Phase 1\", 50.0, \"Processing...\")",
        "",
        "",
        "class TestProcessorIntegration(unittest.TestCase):",
        "    \"\"\"Test integration with CorticalTextProcessor.\"\"\"",
        "",
        "    def test_compute_all_with_callback(self):",
        "        \"\"\"Test compute_all() with custom callback.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(\"doc1\", \"Neural networks process information.\")",
        "        processor.process_document(\"doc2\", \"Machine learning algorithms analyze data.\")",
        "",
        "        callback = Mock()",
        "        reporter = CallbackProgressReporter(callback)",
        "",
        "        processor.compute_all(progress_callback=reporter, verbose=False)",
        "",
        "        # Callback should have been invoked multiple times",
        "        self.assertGreater(callback.call_count, 0)",
        "",
        "        # Check that phases were reported",
        "        phase_names = [call[0][0] for call in callback.call_args_list]",
        "        self.assertIn(\"TF-IDF computation\", phase_names)",
        "        self.assertIn(\"PageRank computation\", phase_names)",
        "",
        "    def test_compute_all_with_show_progress(self):",
        "        \"\"\"Test compute_all() with show_progress flag.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(\"doc1\", \"Neural networks process information.\")",
        "",
        "        # Should not raise exceptions",
        "        processor.compute_all(show_progress=True, verbose=False)",
        "",
        "    def test_compute_all_silent_by_default(self):",
        "        \"\"\"Test that compute_all() is silent by default.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(\"doc1\", \"Neural networks process information.\")",
        "",
        "        # Capture stderr to ensure nothing is written",
        "        import sys",
        "        old_stderr = sys.stderr",
        "        sys.stderr = io.StringIO()",
        "",
        "        try:",
        "            processor.compute_all(verbose=False)",
        "            output = sys.stderr.getvalue()",
        "            # Should be empty (no progress output)",
        "            self.assertEqual(output, \"\")",
        "        finally:",
        "            sys.stderr = old_stderr",
        "",
        "    def test_compute_all_phases_reported(self):",
        "        \"\"\"Test that all expected phases are reported.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(\"doc1\", \"Neural networks process information.\")",
        "        processor.process_document(\"doc2\", \"Machine learning algorithms analyze data.\")",
        "",
        "        callback = Mock()",
        "        reporter = CallbackProgressReporter(callback)",
        "",
        "        processor.compute_all(",
        "            progress_callback=reporter,",
        "            verbose=False,",
        "            build_concepts=True",
        "        )",
        "",
        "        # Extract phase names from callback calls",
        "        phase_names = set()",
        "        for call_args in callback.call_args_list:",
        "            if len(call_args[0]) > 0:",
        "                phase_names.add(call_args[0][0])",
        "",
        "        # Check expected phases",
        "        expected_phases = {",
        "            \"Activation propagation\",",
        "            \"PageRank computation\",",
        "            \"TF-IDF computation\",",
        "            \"Document connections\",",
        "            \"Bigram connections\",",
        "            \"Concept clustering\",",
        "            \"Concept connections\",",
        "        }",
        "",
        "        for phase in expected_phases:",
        "            self.assertIn(phase, phase_names, f\"Missing phase: {phase}\")",
        "",
        "    def test_compute_all_completion_calls(self):",
        "        \"\"\"Test that completion is called for each phase.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(\"doc1\", \"Neural networks process information.\")",
        "",
        "        phases_completed = []",
        "",
        "        def track_completion(phase, percent, message):",
        "            if percent == 100.0:",
        "                phases_completed.append(phase)",
        "",
        "        reporter = CallbackProgressReporter(track_completion)",
        "",
        "        processor.compute_all(",
        "            progress_callback=reporter,",
        "            verbose=False,",
        "            build_concepts=True",
        "        )",
        "",
        "        # Should have completed multiple phases",
        "        self.assertGreater(len(phases_completed), 0)",
        "        self.assertIn(\"TF-IDF computation\", phases_completed)",
        "",
        "    def test_backward_compatibility(self):",
        "        \"\"\"Test that existing code without progress parameters still works.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(\"doc1\", \"Neural networks process information.\")",
        "",
        "        # Old-style call should still work",
        "        stats = processor.compute_all(verbose=False)",
        "",
        "        # Should return stats",
        "        self.assertIsInstance(stats, dict)",
        "",
        "",
        "if __name__ == '__main__':",
        "    unittest.main()"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "tests/unit/test_query_definitions.py",
      "function": "def complex_function(",
      "start_line": 289,
      "lines_added": [
        "    def test_passage_starts_with_definition_line(self):",
        "        \"\"\"",
        "        Regression test for Task #179: Passage must start with definition line.",
        "",
        "        Bug: Previously, find_definition_in_text used `start = match.start() - 50`,",
        "        which could place start in the middle of an earlier line. When showcase.py",
        "        extracted the first line, it showed truncated/wrong content.",
        "",
        "        Fix: Now finds the start of the line containing the match, ensuring the",
        "        passage always starts with the actual definition line.",
        "        \"\"\"",
        "        # Simulate a realistic file structure with content before the definition",
        "        text = \"\"\"",
        "from typing import Dict, List",
        "from dataclasses import dataclass",
        "",
        "",
        "@dataclass",
        "class DataRecord:",
        "    id: str",
        "    content: str",
        "",
        "    def __post_init__(self):",
        "        if self.metadata is None:",
        "            self.metadata = {}",
        "",
        "",
        "class DataProcessor:",
        "    '''Main processor for handling data records.'''",
        "",
        "    def __init__(self):",
        "        self._records = {}",
        "",
        "    def clear(self):",
        "        '''Remove all records.'''",
        "        self._records.clear()",
        "",
        "",
        "def calculate_statistics(records: List[DataRecord]) -> Dict:",
        "    '''Calculate statistics for records.'''",
        "    if not records:",
        "        return {}",
        "    return {'count': len(records)}",
        "\"\"\"",
        "",
        "        # Test class definition",
        "        result = find_definition_in_text(text, \"DataProcessor\", \"class\")",
        "        assert result is not None",
        "        passage, start, end = result",
        "",
        "        # The passage should start with the actual definition line",
        "        first_line = passage.strip().split('\\n')[0]",
        "        assert first_line.startswith(\"class DataProcessor\"), (",
        "            f\"Expected first line to start with 'class DataProcessor', \"",
        "            f\"but got: {first_line!r}\"",
        "        )",
        "        # Should NOT start with truncated content like \"etadata is None\"",
        "        assert \"metadata\" not in first_line.lower() or \"dataprocessor\" in first_line.lower()",
        "",
        "        # Test function definition",
        "        result = find_definition_in_text(text, \"calculate_statistics\", \"function\")",
        "        assert result is not None",
        "        passage, start, end = result",
        "",
        "        # The passage should start with the function definition",
        "        first_line = passage.strip().split('\\n')[0]",
        "        assert first_line.startswith(\"def calculate_statistics\"), (",
        "            f\"Expected first line to start with 'def calculate_statistics', \"",
        "            f\"but got: {first_line!r}\"",
        "        )",
        "        # Should NOT start with truncated content like \"records.clear()\"",
        "        assert \"calculate_statistics\" in first_line",
        "",
        "    def test_definition_at_file_start(self):",
        "        \"\"\"Definition at the very start of file works correctly.\"\"\"",
        "        text = \"class FirstClass:\\n    pass\"",
        "        result = find_definition_in_text(text, \"FirstClass\", \"class\")",
        "        assert result is not None",
        "        passage, start, end = result",
        "",
        "        # Start should be 0 (beginning of file)",
        "        assert start == 0",
        "        # First line should be the definition",
        "        first_line = passage.strip().split('\\n')[0]",
        "        assert first_line.startswith(\"class FirstClass\")",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "        assert result is not None",
        "        passage, _, _ = result",
        "        assert \"__init__\" in passage",
        "",
        "    def test_invalid_def_type(self):",
        "        \"\"\"Invalid def_type returns None.\"\"\"",
        "        text = \"class Foo:\\n    pass\"",
        "        result = find_definition_in_text(text, \"Foo\", \"invalid_type\")",
        "        assert result is None",
        ""
      ],
      "context_after": [
        "",
        "# =============================================================================",
        "# DEFINITION PASSAGES SEARCH TESTS",
        "# =============================================================================",
        "",
        "",
        "class TestFindDefinitionPassages:",
        "    \"\"\"Tests for find_definition_passages() - main search function.\"\"\"",
        "",
        "    def test_non_definition_query(self):"
      ],
      "change_type": "add"
    },
    {
      "file": "tests/unit/test_query_passages.py",
      "function": "class MyClass:",
      "start_line": 1198,
      "lines_added": [
        "",
        "    def test_doc_type_boosting_changes_scores(self):",
        "        \"\"\"Doc-type boosting changes scores for test files.",
        "",
        "        Task #180: Verify that apply_doc_boost parameter actually affects scores.",
        "        Test files (with 'test' in name) should get lower scores when boosted.",
        "        \"\"\"",
        "        # Create two documents: regular code and test file",
        "        col = MockMinicolumn(",
        "            content=\"filter\",",
        "            tfidf=2.0,",
        "            document_ids={\"data_processor.py\", \"test_data_processor.py\"}",
        "        )",
        "        layers = MockLayers.empty()",
        "        layers[0] = MockHierarchicalLayer([col])",
        "        tokenizer = Tokenizer()",
        "        documents = {",
        "            \"data_processor.py\": \"filter data records efficiently\",",
        "            \"test_data_processor.py\": \"filter data records in tests\",",
        "        }",
        "",
        "        # Without boosting",
        "        results_no_boost = find_passages_for_query(",
        "            \"filter data\", layers, tokenizer, documents,",
        "            apply_doc_boost=False, use_expansion=False, use_definition_search=False",
        "        )",
        "",
        "        # With boosting (prefer_docs=True to enable boosting)",
        "        results_with_boost = find_passages_for_query(",
        "            \"filter data\", layers, tokenizer, documents,",
        "            apply_doc_boost=True, prefer_docs=True, use_expansion=False, use_definition_search=False",
        "        )",
        "",
        "        # Both should return results",
        "        assert len(results_no_boost) > 0",
        "        assert len(results_with_boost) > 0",
        "",
        "        # Extract scores for each document",
        "        def get_scores_by_doc(results):",
        "            scores = {}",
        "            for _, doc_id, _, _, score in results:",
        "                if doc_id not in scores or score > scores[doc_id]:",
        "                    scores[doc_id] = score",
        "            return scores",
        "",
        "        scores_no_boost = get_scores_by_doc(results_no_boost)",
        "        scores_with_boost = get_scores_by_doc(results_with_boost)",
        "",
        "        # Without boosting, both files should have similar scores",
        "        # (may differ slightly due to document length normalization)",
        "",
        "        # With boosting, test file should have lower score than regular file",
        "        if \"data_processor.py\" in scores_with_boost and \"test_data_processor.py\" in scores_with_boost:",
        "            # Test file should be penalized (0.8x boost vs 1.0x)",
        "            assert scores_with_boost[\"test_data_processor.py\"] < scores_with_boost[\"data_processor.py\"], \\",
        "                f\"Test file should have lower score with boosting. \" \\",
        "                f\"Got test={scores_with_boost['test_data_processor.py']:.3f}, \" \\",
        "                f\"regular={scores_with_boost['data_processor.py']:.3f}\"",
        "",
        "        # Verify scores actually changed between boosted and non-boosted",
        "        # At least one document's score should be different",
        "        changed = False",
        "        for doc_id in set(scores_no_boost.keys()) & set(scores_with_boost.keys()):",
        "            if abs(scores_no_boost[doc_id] - scores_with_boost[doc_id]) > 0.001:",
        "                changed = True",
        "                break",
        "",
        "        assert changed, \"Boosting should change at least one document's score\""
      ],
      "lines_removed": [],
      "context_before": [
        "        )",
        "",
        "        # Batch call",
        "        batch = find_passages_batch(",
        "            [\"test\"], layers, tokenizer, documents, use_expansion=False",
        "        )",
        "",
        "        # Results should match",
        "        assert len(batch) == 1",
        "        assert len(batch[0]) == len(single)"
      ],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "tests/unit/test_query_search.py",
      "function": "class TestFindDocumentsForQuery:",
      "start_line": 239,
      "lines_added": [
        "    def test_exact_doc_name_match_beats_high_tfidf(self):",
        "        \"\"\"",
        "        Task #181: Exact document name match ranks first even with lower TF-IDF.",
        "",
        "        Bug: Documents with high content scores could outrank exact name matches.",
        "        Fix: Exact matches get additive boost to ensure top ranking.",
        "        \"\"\"",
        "        layers = (",
        "            LayerBuilder()",
        "            .with_term(\"distributed\", tfidf=5.0)",
        "            .with_term(\"systems\", tfidf=5.0)",
        "            # distributed_systems doc has exact name match but low content",
        "            .with_document(\"distributed_systems\", [\"distributed\"])",
        "            # other_doc has high content score",
        "            .with_document(\"other_doc\", [\"distributed\", \"systems\"])",
        "            .build()",
        "        )",
        "",
        "        layer0 = layers[MockLayers.TOKENS]",
        "        # other_doc has MUCH higher TF-IDF scores",
        "        layer0.get_minicolumn(\"distributed\").tfidf_per_doc = {",
        "            \"distributed_systems\": 0.5,  # Low score",
        "            \"other_doc\": 10.0  # Very high score",
        "        }",
        "        layer0.get_minicolumn(\"systems\").tfidf_per_doc = {",
        "            \"other_doc\": 10.0  # Very high score",
        "        }",
        "",
        "        tokenizer = Tokenizer()",
        "        result = find_documents_for_query(",
        "            \"distributed systems\", layers, tokenizer,",
        "            use_expansion=False,",
        "            doc_name_boost=2.0",
        "        )",
        "",
        "        # distributed_systems should rank first due to exact name match",
        "        # despite having much lower TF-IDF score",
        "        assert result[0][0] == \"distributed_systems\"",
        "        assert result[0][1] > result[1][1]  # Score should be higher",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "        tokenizer = Tokenizer()",
        "        result = find_documents_for_query(",
        "            \"term\", layers, tokenizer,",
        "            use_expansion=False,",
        "            doc_name_boost=1.0  # No boost",
        "        )",
        "",
        "        # doc1 should win on TF-IDF alone",
        "        assert result[0][0] == \"doc1\"",
        ""
      ],
      "context_after": [
        "    def test_query_expansion_disabled(self):",
        "        \"\"\"use_expansion=False uses only query terms.\"\"\"",
        "        # Create connected terms",
        "        layers = (",
        "            LayerBuilder()",
        "            .with_term(\"neural\", tfidf=2.0, pagerank=0.8)",
        "            .with_term(\"network\", tfidf=2.0, pagerank=0.6)",
        "            .with_connection(\"neural\", \"network\", weight=5.0)",
        "            .with_document(\"doc1\", [\"neural\"])",
        "            .with_document(\"doc2\", [\"network\"])"
      ],
      "change_type": "add"
    },
    {
      "file": "tests/unit/test_query_search.py",
      "function": "class TestFastFindDocuments:",
      "start_line": 536,
      "lines_added": [
        "    def test_exact_name_match_added_to_candidates(self):",
        "        \"\"\"",
        "        Task #181: Exact name matches included in candidates even without content.",
        "",
        "        Bug: fast_find_documents excluded docs whose name matched but content didn't.",
        "        Fix: Add name-matching docs to candidate set.",
        "        \"\"\"",
        "        # Create doc that has exact name match but no matching content",
        "        layers = (",
        "            LayerBuilder()",
        "            .with_term(\"other\", tfidf=5.0)",
        "            .with_document(\"distributed_systems\", [\"other\"])  # No 'distributed' or 'systems' in content",
        "            .with_document(\"high_content_doc\", [\"other\"])",
        "            .build()",
        "        )",
        "",
        "        # Add layer3 (DOCUMENTS) for name matching",
        "        doc1 = MockMinicolumn(",
        "            content=\"distributed_systems\",",
        "            document_ids={\"distributed_systems\"}",
        "        )",
        "        doc2 = MockMinicolumn(",
        "            content=\"high_content_doc\",",
        "            document_ids={\"high_content_doc\"}",
        "        )",
        "",
        "        layers[MockLayers.DOCUMENTS] = MockHierarchicalLayer([doc1, doc2])",
        "",
        "        tokenizer = Tokenizer()",
        "        result = fast_find_documents(",
        "            \"distributed systems\", layers, tokenizer, doc_name_boost=2.0",
        "        )",
        "",
        "        # distributed_systems should be in results despite not having content match",
        "        doc_ids = [doc_id for doc_id, _ in result]",
        "        assert \"distributed_systems\" in doc_ids",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "        layers[MockLayers.TOKENS] = MockHierarchicalLayer([col])",
        "",
        "        tokenizer = Tokenizer()",
        "        result = fast_find_documents(",
        "            \"term\", layers, tokenizer, doc_name_boost=1.0",
        "        )",
        "",
        "        # high_score_doc should win on TF-IDF alone",
        "        assert result[0][0] == \"high_score_doc\"",
        ""
      ],
      "context_after": [
        "",
        "# =============================================================================",
        "# BUILD_DOCUMENT_INDEX TESTS",
        "# =============================================================================",
        "",
        "",
        "class TestBuildDocumentIndex:",
        "    \"\"\"Tests for build_document_index inverted index creation.\"\"\"",
        "",
        "    def test_empty_layer(self):"
      ],
      "change_type": "add"
    },
    {
      "file": "tests/unit/test_results.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "\"\"\"",
        "Unit Tests for Results Module",
        "==============================",
        "",
        "Task #185: Create result dataclasses for query results.",
        "",
        "Tests the DocumentMatch, PassageMatch, and QueryResult dataclasses that",
        "provide strongly-typed containers for search results with IDE support.",
        "",
        "Coverage goal: 95%",
        "Test count goal: 40+",
        "\"\"\"",
        "",
        "import pytest",
        "",
        "from cortical.results import (",
        "    DocumentMatch,",
        "    PassageMatch,",
        "    QueryResult,",
        "    convert_document_matches,",
        "    convert_passage_matches",
        ")",
        "",
        "",
        "# =============================================================================",
        "# DOCUMENTMATCH CLASS TESTS",
        "# =============================================================================",
        "",
        "",
        "class TestDocumentMatchClass:",
        "    \"\"\"Tests for the DocumentMatch dataclass.\"\"\"",
        "",
        "    def test_creation_minimal(self):",
        "        \"\"\"DocumentMatch created with minimal parameters.\"\"\"",
        "        match = DocumentMatch(\"doc1\", 0.85)",
        "        assert match.doc_id == \"doc1\"",
        "        assert match.score == 0.85",
        "        assert match.metadata is None",
        "",
        "    def test_creation_with_metadata(self):",
        "        \"\"\"DocumentMatch created with metadata.\"\"\"",
        "        metadata = {\"doc_type\": \"markdown\", \"size\": 1024}",
        "        match = DocumentMatch(\"doc1.md\", 0.92, metadata)",
        "        assert match.doc_id == \"doc1.md\"",
        "        assert match.score == 0.92",
        "        assert match.metadata == metadata",
        "        assert match.metadata[\"doc_type\"] == \"markdown\"",
        "",
        "    def test_immutable(self):",
        "        \"\"\"DocumentMatch is immutable (frozen).\"\"\"",
        "        match = DocumentMatch(\"doc1\", 0.8)",
        "        with pytest.raises(AttributeError):",
        "            match.score = 0.9",
        "",
        "    def test_repr_without_metadata(self):",
        "        \"\"\"String representation without metadata.\"\"\"",
        "        match = DocumentMatch(\"doc1\", 0.8521)",
        "        repr_str = repr(match)",
        "        assert \"DocumentMatch\" in repr_str",
        "        assert \"doc1\" in repr_str",
        "        assert \"0.8521\" in repr_str",
        "",
        "    def test_repr_with_metadata(self):",
        "        \"\"\"String representation with metadata.\"\"\"",
        "        match = DocumentMatch(\"doc1\", 0.8, {\"type\": \"test\"})",
        "        repr_str = repr(match)",
        "        assert \"metadata=\" in repr_str",
        "",
        "    def test_to_dict(self):",
        "        \"\"\"Convert to dictionary.\"\"\"",
        "        match = DocumentMatch(\"doc1\", 0.85)",
        "        d = match.to_dict()",
        "        assert d == {\"doc_id\": \"doc1\", \"score\": 0.85, \"metadata\": None}",
        "",
        "    def test_to_dict_with_metadata(self):",
        "        \"\"\"Convert to dictionary with metadata.\"\"\"",
        "        metadata = {\"key\": \"value\"}",
        "        match = DocumentMatch(\"doc1\", 0.85, metadata)",
        "        d = match.to_dict()",
        "        assert d[\"metadata\"] == metadata",
        "",
        "    def test_to_tuple(self):",
        "        \"\"\"Convert to tuple format.\"\"\"",
        "        match = DocumentMatch(\"doc1\", 0.85)",
        "        t = match.to_tuple()",
        "        assert t == (\"doc1\", 0.85)",
        "",
        "    def test_from_tuple_minimal(self):",
        "        \"\"\"Create from tuple with minimal args.\"\"\"",
        "        match = DocumentMatch.from_tuple(\"doc1\", 0.85)",
        "        assert match.doc_id == \"doc1\"",
        "        assert match.score == 0.85",
        "        assert match.metadata is None",
        "",
        "    def test_from_tuple_with_metadata(self):",
        "        \"\"\"Create from tuple with metadata.\"\"\"",
        "        metadata = {\"type\": \"test\"}",
        "        match = DocumentMatch.from_tuple(\"doc1\", 0.85, metadata)",
        "        assert match.metadata == metadata",
        "",
        "    def test_from_dict_minimal(self):",
        "        \"\"\"Create from dictionary with minimal fields.\"\"\"",
        "        data = {\"doc_id\": \"doc1\", \"score\": 0.85}",
        "        match = DocumentMatch.from_dict(data)",
        "        assert match.doc_id == \"doc1\"",
        "        assert match.score == 0.85",
        "        assert match.metadata is None",
        "",
        "    def test_from_dict_with_metadata(self):",
        "        \"\"\"Create from dictionary with metadata.\"\"\"",
        "        data = {\"doc_id\": \"doc1\", \"score\": 0.85, \"metadata\": {\"type\": \"test\"}}",
        "        match = DocumentMatch.from_dict(data)",
        "        assert match.metadata == {\"type\": \"test\"}",
        "",
        "    def test_roundtrip_dict(self):",
        "        \"\"\"Roundtrip through dictionary preserves data.\"\"\"",
        "        original = DocumentMatch(\"doc1\", 0.85, {\"key\": \"value\"})",
        "        d = original.to_dict()",
        "        restored = DocumentMatch.from_dict(d)",
        "        assert restored.doc_id == original.doc_id",
        "        assert restored.score == original.score",
        "        assert restored.metadata == original.metadata",
        "",
        "    def test_roundtrip_tuple(self):",
        "        \"\"\"Roundtrip through tuple preserves data (without metadata).\"\"\"",
        "        original = DocumentMatch(\"doc1\", 0.85)",
        "        t = original.to_tuple()",
        "        restored = DocumentMatch.from_tuple(*t)",
        "        assert restored.doc_id == original.doc_id",
        "        assert restored.score == original.score",
        "",
        "",
        "# =============================================================================",
        "# PASSAGEMATCH CLASS TESTS",
        "# =============================================================================",
        "",
        "",
        "class TestPassageMatchClass:",
        "    \"\"\"Tests for the PassageMatch dataclass.\"\"\"",
        "",
        "    def test_creation_minimal(self):",
        "        \"\"\"PassageMatch created with minimal parameters.\"\"\"",
        "        match = PassageMatch(\"doc1.py\", \"def foo():\\n    pass\", 0.9, 100, 120)",
        "        assert match.doc_id == \"doc1.py\"",
        "        assert match.text == \"def foo():\\n    pass\"",
        "        assert match.score == 0.9",
        "        assert match.start == 100",
        "        assert match.end == 120",
        "        assert match.metadata is None",
        "",
        "    def test_creation_with_metadata(self):",
        "        \"\"\"PassageMatch created with metadata.\"\"\"",
        "        metadata = {\"function\": \"foo\", \"line\": 10}",
        "        match = PassageMatch(\"doc1.py\", \"code here\", 0.85, 0, 9, metadata)",
        "        assert match.metadata == metadata",
        "",
        "    def test_immutable(self):",
        "        \"\"\"PassageMatch is immutable (frozen).\"\"\"",
        "        match = PassageMatch(\"doc1\", \"text\", 0.8, 0, 4)",
        "        with pytest.raises(AttributeError):",
        "            match.score = 0.9",
        "",
        "    def test_repr_truncates_long_text(self):",
        "        \"\"\"String representation truncates long text.\"\"\"",
        "        long_text = \"a\" * 100",
        "        match = PassageMatch(\"doc1\", long_text, 0.8, 0, 100)",
        "        repr_str = repr(match)",
        "        assert \"...\" in repr_str",
        "        assert len(repr_str) < 200  # Should be shorter than full text",
        "",
        "    def test_repr_escapes_newlines(self):",
        "        \"\"\"String representation escapes newlines.\"\"\"",
        "        match = PassageMatch(\"doc1\", \"line1\\nline2\", 0.8, 0, 11)",
        "        repr_str = repr(match)",
        "        assert \"\\\\n\" in repr_str",
        "",
        "    def test_to_dict(self):",
        "        \"\"\"Convert to dictionary.\"\"\"",
        "        match = PassageMatch(\"doc1\", \"text\", 0.85, 0, 4)",
        "        d = match.to_dict()",
        "        assert d[\"doc_id\"] == \"doc1\"",
        "        assert d[\"text\"] == \"text\"",
        "        assert d[\"score\"] == 0.85",
        "        assert d[\"start\"] == 0",
        "        assert d[\"end\"] == 4",
        "        assert d[\"metadata\"] is None",
        "",
        "    def test_to_tuple(self):",
        "        \"\"\"Convert to tuple format.\"\"\"",
        "        match = PassageMatch(\"doc1\", \"text\", 0.85, 10, 14)",
        "        t = match.to_tuple()",
        "        assert t == (\"doc1\", \"text\", 10, 14, 0.85)",
        "",
        "    def test_location_property(self):",
        "        \"\"\"Location property returns citation-style string.\"\"\"",
        "        match = PassageMatch(\"doc1.py\", \"text\", 0.8, 100, 150)",
        "        assert match.location == \"doc1.py:100-150\"",
        "",
        "    def test_length_property(self):",
        "        \"\"\"Length property returns character count.\"\"\"",
        "        match = PassageMatch(\"doc1\", \"hello\", 0.8, 0, 5)",
        "        assert match.length == 5",
        "",
        "    def test_length_property_larger_range(self):",
        "        \"\"\"Length property for larger range.\"\"\"",
        "        match = PassageMatch(\"doc1\", \"text\", 0.8, 100, 250)",
        "        assert match.length == 150",
        "",
        "    def test_from_tuple_minimal(self):",
        "        \"\"\"Create from tuple with minimal args.\"\"\"",
        "        match = PassageMatch.from_tuple(\"doc1\", \"text\", 0, 4, 0.9)",
        "        assert match.doc_id == \"doc1\"",
        "        assert match.text == \"text\"",
        "        assert match.start == 0",
        "        assert match.end == 4",
        "        assert match.score == 0.9",
        "        assert match.metadata is None",
        "",
        "    def test_from_tuple_with_metadata(self):",
        "        \"\"\"Create from tuple with metadata.\"\"\"",
        "        metadata = {\"line\": 5}",
        "        match = PassageMatch.from_tuple(\"doc1\", \"text\", 0, 4, 0.9, metadata)",
        "        assert match.metadata == metadata",
        "",
        "    def test_from_dict_minimal(self):",
        "        \"\"\"Create from dictionary with minimal fields.\"\"\"",
        "        data = {",
        "            \"doc_id\": \"doc1\",",
        "            \"text\": \"hello\",",
        "            \"score\": 0.8,",
        "            \"start\": 0,",
        "            \"end\": 5",
        "        }",
        "        match = PassageMatch.from_dict(data)",
        "        assert match.text == \"hello\"",
        "        assert match.length == 5",
        "",
        "    def test_from_dict_with_metadata(self):",
        "        \"\"\"Create from dictionary with metadata.\"\"\"",
        "        data = {",
        "            \"doc_id\": \"doc1\",",
        "            \"text\": \"hello\",",
        "            \"score\": 0.8,",
        "            \"start\": 0,",
        "            \"end\": 5,",
        "            \"metadata\": {\"type\": \"definition\"}",
        "        }",
        "        match = PassageMatch.from_dict(data)",
        "        assert match.metadata == {\"type\": \"definition\"}",
        "",
        "    def test_roundtrip_dict(self):",
        "        \"\"\"Roundtrip through dictionary preserves data.\"\"\"",
        "        original = PassageMatch(\"doc1\", \"text\", 0.8, 10, 14, {\"key\": \"value\"})",
        "        d = original.to_dict()",
        "        restored = PassageMatch.from_dict(d)",
        "        assert restored.doc_id == original.doc_id",
        "        assert restored.text == original.text",
        "        assert restored.score == original.score",
        "        assert restored.start == original.start",
        "        assert restored.end == original.end",
        "        assert restored.metadata == original.metadata",
        "",
        "    def test_roundtrip_tuple(self):",
        "        \"\"\"Roundtrip through tuple preserves data (without metadata).\"\"\"",
        "        original = PassageMatch(\"doc1\", \"text\", 0.8, 10, 14)",
        "        t = original.to_tuple()",
        "        restored = PassageMatch.from_tuple(*t)",
        "        assert restored.doc_id == original.doc_id",
        "        assert restored.text == original.text",
        "        assert restored.score == original.score",
        "",
        "",
        "# =============================================================================",
        "# QUERYRESULT CLASS TESTS",
        "# =============================================================================",
        "",
        "",
        "class TestQueryResultClass:",
        "    \"\"\"Tests for the QueryResult wrapper class.\"\"\"",
        "",
        "    def test_creation_with_document_matches(self):",
        "        \"\"\"QueryResult created with DocumentMatch list.\"\"\"",
        "        matches = [DocumentMatch(\"doc1\", 0.9), DocumentMatch(\"doc2\", 0.7)]",
        "        result = QueryResult(\"neural networks\", matches)",
        "        assert result.query == \"neural networks\"",
        "        assert len(result.matches) == 2",
        "        assert result.expansion_terms is None",
        "        assert result.timing_ms is None",
        "        assert result.metadata is None",
        "",
        "    def test_creation_with_passage_matches(self):",
        "        \"\"\"QueryResult created with PassageMatch list.\"\"\"",
        "        matches = [PassageMatch(\"doc1\", \"text1\", 0.9, 0, 5)]",
        "        result = QueryResult(\"test query\", matches)",
        "        assert len(result.matches) == 1",
        "        assert isinstance(result.matches[0], PassageMatch)",
        "",
        "    def test_creation_with_all_fields(self):",
        "        \"\"\"QueryResult created with all optional fields.\"\"\"",
        "        matches = [DocumentMatch(\"doc1\", 0.9)]",
        "        expansion = {\"neural\": 1.0, \"network\": 0.8}",
        "        metadata = {\"source\": \"test\"}",
        "        result = QueryResult(",
        "            \"neural\",",
        "            matches,",
        "            expansion_terms=expansion,",
        "            timing_ms=15.3,",
        "            metadata=metadata",
        "        )",
        "        assert result.expansion_terms == expansion",
        "        assert result.timing_ms == 15.3",
        "        assert result.metadata == metadata",
        "",
        "    def test_immutable(self):",
        "        \"\"\"QueryResult is immutable (frozen).\"\"\"",
        "        matches = [DocumentMatch(\"doc1\", 0.9)]",
        "        result = QueryResult(\"test\", matches)",
        "        with pytest.raises(AttributeError):",
        "            result.query = \"new query\"",
        "",
        "    def test_repr(self):",
        "        \"\"\"String representation shows key info.\"\"\"",
        "        matches = [DocumentMatch(\"doc1\", 0.9), DocumentMatch(\"doc2\", 0.7)]",
        "        result = QueryResult(\"test\", matches, expansion_terms={\"a\": 1.0})",
        "        repr_str = repr(result)",
        "        assert \"QueryResult\" in repr_str",
        "        assert \"test\" in repr_str",
        "        assert \"2 x DocumentMatch\" in repr_str",
        "",
        "    def test_to_dict(self):",
        "        \"\"\"Convert to dictionary with nested match dicts.\"\"\"",
        "        matches = [DocumentMatch(\"doc1\", 0.9)]",
        "        result = QueryResult(\"test\", matches, timing_ms=10.0)",
        "        d = result.to_dict()",
        "        assert d[\"query\"] == \"test\"",
        "        assert len(d[\"matches\"]) == 1",
        "        assert d[\"matches\"][0][\"doc_id\"] == \"doc1\"",
        "        assert d[\"timing_ms\"] == 10.0",
        "",
        "    def test_top_match_property(self):",
        "        \"\"\"Top match property returns highest scoring match.\"\"\"",
        "        matches = [",
        "            DocumentMatch(\"doc1\", 0.5),",
        "            DocumentMatch(\"doc2\", 0.9),",
        "            DocumentMatch(\"doc3\", 0.7)",
        "        ]",
        "        result = QueryResult(\"test\", matches)",
        "        assert result.top_match.doc_id == \"doc2\"",
        "        assert result.top_match.score == 0.9",
        "",
        "    def test_top_match_empty_matches(self):",
        "        \"\"\"Top match returns None when no matches.\"\"\"",
        "        result = QueryResult(\"test\", [])",
        "        assert result.top_match is None",
        "",
        "    def test_match_count_property(self):",
        "        \"\"\"Match count property returns number of matches.\"\"\"",
        "        matches = [DocumentMatch(\"doc1\", 0.9), DocumentMatch(\"doc2\", 0.7)]",
        "        result = QueryResult(\"test\", matches)",
        "        assert result.match_count == 2",
        "",
        "    def test_match_count_empty(self):",
        "        \"\"\"Match count returns 0 for empty matches.\"\"\"",
        "        result = QueryResult(\"test\", [])",
        "        assert result.match_count == 0",
        "",
        "    def test_average_score_property(self):",
        "        \"\"\"Average score property calculates correctly.\"\"\"",
        "        matches = [",
        "            DocumentMatch(\"doc1\", 0.8),",
        "            DocumentMatch(\"doc2\", 0.6)",
        "        ]",
        "        result = QueryResult(\"test\", matches)",
        "        assert result.average_score == 0.7",
        "",
        "    def test_average_score_empty_matches(self):",
        "        \"\"\"Average score returns 0.0 for empty matches.\"\"\"",
        "        result = QueryResult(\"test\", [])",
        "        assert result.average_score == 0.0",
        "",
        "    def test_from_dict_document_matches(self):",
        "        \"\"\"Create from dictionary with DocumentMatch results.\"\"\"",
        "        data = {",
        "            \"query\": \"test\",",
        "            \"matches\": [",
        "                {\"doc_id\": \"doc1\", \"score\": 0.9, \"metadata\": None},",
        "                {\"doc_id\": \"doc2\", \"score\": 0.7, \"metadata\": None}",
        "            ],",
        "            \"expansion_terms\": {\"test\": 1.0},",
        "            \"timing_ms\": 10.0",
        "        }",
        "        result = QueryResult.from_dict(data)",
        "        assert result.query == \"test\"",
        "        assert len(result.matches) == 2",
        "        assert isinstance(result.matches[0], DocumentMatch)",
        "        assert result.expansion_terms == {\"test\": 1.0}",
        "",
        "    def test_from_dict_passage_matches(self):",
        "        \"\"\"Create from dictionary with PassageMatch results.\"\"\"",
        "        data = {",
        "            \"query\": \"test\",",
        "            \"matches\": [",
        "                {",
        "                    \"doc_id\": \"doc1\",",
        "                    \"text\": \"hello\",",
        "                    \"score\": 0.9,",
        "                    \"start\": 0,",
        "                    \"end\": 5,",
        "                    \"metadata\": None",
        "                }",
        "            ]",
        "        }",
        "        result = QueryResult.from_dict(data)",
        "        assert len(result.matches) == 1",
        "        assert isinstance(result.matches[0], PassageMatch)",
        "        assert result.matches[0].text == \"hello\"",
        "",
        "    def test_from_dict_empty_matches(self):",
        "        \"\"\"Create from dictionary with empty matches.\"\"\"",
        "        data = {\"query\": \"test\", \"matches\": []}",
        "        result = QueryResult.from_dict(data)",
        "        assert result.match_count == 0",
        "",
        "    def test_roundtrip_dict(self):",
        "        \"\"\"Roundtrip through dictionary preserves data.\"\"\"",
        "        matches = [DocumentMatch(\"doc1\", 0.9, {\"type\": \"test\"})]",
        "        original = QueryResult(",
        "            \"test query\",",
        "            matches,",
        "            expansion_terms={\"test\": 1.0},",
        "            timing_ms=15.0",
        "        )",
        "        d = original.to_dict()",
        "        restored = QueryResult.from_dict(d)",
        "        assert restored.query == original.query",
        "        assert restored.match_count == original.match_count",
        "        assert restored.expansion_terms == original.expansion_terms",
        "        assert restored.timing_ms == original.timing_ms",
        "",
        "",
        "# =============================================================================",
        "# HELPER FUNCTION TESTS",
        "# =============================================================================",
        "",
        "",
        "class TestHelperFunctions:",
        "    \"\"\"Tests for batch conversion helper functions.\"\"\"",
        "",
        "    def test_convert_document_matches_basic(self):",
        "        \"\"\"Convert list of tuples to DocumentMatch objects.\"\"\"",
        "        results = [(\"doc1\", 0.9), (\"doc2\", 0.7), (\"doc3\", 0.5)]",
        "        matches = convert_document_matches(results)",
        "        assert len(matches) == 3",
        "        assert all(isinstance(m, DocumentMatch) for m in matches)",
        "        assert matches[0].doc_id == \"doc1\"",
        "        assert matches[0].score == 0.9",
        "",
        "    def test_convert_document_matches_with_metadata(self):",
        "        \"\"\"Convert with per-document metadata.\"\"\"",
        "        results = [(\"doc1\", 0.9), (\"doc2\", 0.7)]",
        "        metadata = {",
        "            \"doc1\": {\"type\": \"markdown\"},",
        "            \"doc2\": {\"type\": \"python\"}",
        "        }",
        "        matches = convert_document_matches(results, metadata)",
        "        assert matches[0].metadata == {\"type\": \"markdown\"}",
        "        assert matches[1].metadata == {\"type\": \"python\"}",
        "",
        "    def test_convert_document_matches_partial_metadata(self):",
        "        \"\"\"Convert with metadata for some documents.\"\"\"",
        "        results = [(\"doc1\", 0.9), (\"doc2\", 0.7)]",
        "        metadata = {\"doc1\": {\"type\": \"markdown\"}}",
        "        matches = convert_document_matches(results, metadata)",
        "        assert matches[0].metadata == {\"type\": \"markdown\"}",
        "        assert matches[1].metadata is None",
        "",
        "    def test_convert_document_matches_empty(self):",
        "        \"\"\"Convert empty list.\"\"\"",
        "        matches = convert_document_matches([])",
        "        assert matches == []",
        "",
        "    def test_convert_passage_matches_basic(self):",
        "        \"\"\"Convert list of tuples to PassageMatch objects.\"\"\"",
        "        results = [",
        "            (\"doc1\", \"text1\", 0, 5, 0.9),",
        "            (\"doc2\", \"text2\", 10, 15, 0.7)",
        "        ]",
        "        matches = convert_passage_matches(results)",
        "        assert len(matches) == 2",
        "        assert all(isinstance(m, PassageMatch) for m in matches)",
        "        assert matches[0].text == \"text1\"",
        "        assert matches[0].start == 0",
        "        assert matches[0].end == 5",
        "",
        "    def test_convert_passage_matches_with_metadata(self):",
        "        \"\"\"Convert with per-document metadata.\"\"\"",
        "        results = [(\"doc1\", \"text1\", 0, 5, 0.9)]",
        "        metadata = {\"doc1\": {\"line\": 1}}",
        "        matches = convert_passage_matches(results, metadata)",
        "        assert matches[0].metadata == {\"line\": 1}",
        "",
        "    def test_convert_passage_matches_empty(self):",
        "        \"\"\"Convert empty list.\"\"\"",
        "        matches = convert_passage_matches([])",
        "        assert matches == []",
        "",
        "",
        "# =============================================================================",
        "# INTEGRATION TESTS",
        "# =============================================================================",
        "",
        "",
        "class TestIntegration:",
        "    \"\"\"Integration tests for realistic usage patterns.\"\"\"",
        "",
        "    def test_workflow_document_search(self):",
        "        \"\"\"Realistic workflow for document search results.\"\"\"",
        "        # Simulate search results",
        "        raw_results = [",
        "            (\"neural_networks.md\", 0.95),",
        "            (\"deep_learning.py\", 0.87),",
        "            (\"ai_overview.md\", 0.72)",
        "        ]",
        "",
        "        # Convert to dataclasses",
        "        matches = convert_document_matches(raw_results)",
        "",
        "        # Access with IDE autocomplete",
        "        for match in matches:",
        "            assert hasattr(match, 'doc_id')",
        "            assert hasattr(match, 'score')",
        "",
        "        # Get top result",
        "        top = matches[0]",
        "        assert top.doc_id == \"neural_networks.md\"",
        "",
        "        # Convert back to tuple for legacy code",
        "        tuples = [m.to_tuple() for m in matches]",
        "        assert tuples[0] == (\"neural_networks.md\", 0.95)",
        "",
        "    def test_workflow_passage_retrieval(self):",
        "        \"\"\"Realistic workflow for passage retrieval.\"\"\"",
        "        # Simulate passage results",
        "        raw_results = [",
        "            (\"processor.py\", \"def compute_pagerank():\\n    ...\", 100, 150, 0.92),",
        "            (\"README.md\", \"PageRank is an algorithm...\", 500, 600, 0.85)",
        "        ]",
        "",
        "        # Convert to dataclasses",
        "        matches = convert_passage_matches(raw_results)",
        "",
        "        # Access properties",
        "        for match in matches:",
        "            location = match.location",
        "            length = match.length",
        "            assert isinstance(location, str)",
        "            assert isinstance(length, int)",
        "",
        "        # Get citation info",
        "        citation = f\"[{matches[0].location}]\"",
        "        assert citation == \"[processor.py:100-150]\"",
        "",
        "    def test_workflow_with_query_result(self):",
        "        \"\"\"Complete workflow with QueryResult wrapper.\"\"\"",
        "        # Search results",
        "        matches = [",
        "            DocumentMatch(\"doc1\", 0.9),",
        "            DocumentMatch(\"doc2\", 0.7)",
        "        ]",
        "",
        "        # Wrap in QueryResult",
        "        result = QueryResult(",
        "            query=\"neural networks\",",
        "            matches=matches,",
        "            expansion_terms={\"neural\": 1.0, \"network\": 0.8, \"deep\": 0.5},",
        "            timing_ms=12.5",
        "        )",
        "",
        "        # Analyze results",
        "        assert result.match_count == 2",
        "        assert result.top_match.score == 0.9",
        "        assert result.average_score == 0.8",
        "",
        "        # Export for logging/storage",
        "        result_dict = result.to_dict()",
        "        assert \"query\" in result_dict",
        "        assert \"matches\" in result_dict",
        "",
        "        # Restore from storage",
        "        restored = QueryResult.from_dict(result_dict)",
        "        assert restored.query == result.query",
        "        assert restored.match_count == result.match_count"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    }
  ],
  "hour_of_day": 5,
  "day_of_week": "Saturday",
  "seconds_since_last_commit": -201011,
  "is_merge": true,
  "is_initial": false,
  "parent_count": 2,
  "session_id": null,
  "related_chats": [],
  "ci_result": null,
  "reverted": false,
  "amended": false
}