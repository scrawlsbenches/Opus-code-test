{
  "hash": "9e80a6739af909ed8f5dc57b8fcc60d5e13651d9",
  "message": "Implement chunk-level passage retrieval for RAG systems (Task 8)",
  "author": "Claude",
  "timestamp": "2025-12-09 19:48:24 +0000",
  "branch": "claude/multi-index-design-DvifZ",
  "files_changed": [
    "cortical/processor.py",
    "cortical/query.py",
    "tests/test_processor.py"
  ],
  "insertions": 315,
  "deletions": 7,
  "hunks": [
    {
      "file": "cortical/processor.py",
      "function": "class CorticalTextProcessor:",
      "start_line": 141,
      "lines_added": [
        "",
        "    def find_passages_for_query(",
        "        self,",
        "        query_text: str,",
        "        top_n: int = 5,",
        "        chunk_size: int = 512,",
        "        overlap: int = 128,",
        "        use_expansion: bool = True,",
        "        doc_filter: Optional[List[str]] = None",
        "    ) -> List[Tuple[str, str, int, int, float]]:",
        "        \"\"\"",
        "        Find text passages most relevant to a query (for RAG systems).",
        "",
        "        Instead of returning just document IDs, this returns actual text passages",
        "        with position information suitable for context windows and citations.",
        "",
        "        Args:",
        "            query_text: Search query",
        "            top_n: Number of passages to return",
        "            chunk_size: Size of each chunk in characters (default 512)",
        "            overlap: Overlap between chunks in characters (default 128)",
        "            use_expansion: Whether to expand query terms",
        "            doc_filter: Optional list of doc_ids to restrict search to",
        "",
        "        Returns:",
        "            List of (passage_text, doc_id, start_char, end_char, score) tuples",
        "            ranked by relevance",
        "",
        "        Example:",
        "            >>> results = processor.find_passages_for_query(\"neural networks\")",
        "            >>> for passage, doc_id, start, end, score in results:",
        "            ...     print(f\"[{doc_id}:{start}-{end}] {passage[:50]}... (score: {score:.3f})\")",
        "        \"\"\"",
        "        return query_module.find_passages_for_query(",
        "            query_text,",
        "            self.layers,",
        "            self.tokenizer,",
        "            self.documents,",
        "            top_n=top_n,",
        "            chunk_size=chunk_size,",
        "            overlap=overlap,",
        "            use_expansion=use_expansion,",
        "            doc_filter=doc_filter",
        "        )"
      ],
      "lines_removed": [],
      "context_before": [
        "        return emb_module.find_similar_by_embedding(self.embeddings, term, top_n)",
        "    ",
        "    def expand_query(self, query_text: str, max_expansions: int = 10, use_variants: bool = True, verbose: bool = False) -> Dict[str, float]:",
        "        return query_module.expand_query(query_text, self.layers, self.tokenizer, max_expansions=max_expansions, use_variants=use_variants)",
        "    ",
        "    def expand_query_semantic(self, query_text: str, max_expansions: int = 10) -> Dict[str, float]:",
        "        return query_module.expand_query_semantic(query_text, self.layers, self.tokenizer, self.semantic_relations, max_expansions)",
        "    ",
        "    def find_documents_for_query(self, query_text: str, top_n: int = 5, use_expansion: bool = True) -> List[Tuple[str, float]]:",
        "        return query_module.find_documents_for_query(query_text, self.layers, self.tokenizer, top_n, use_expansion)"
      ],
      "context_after": [
        "    ",
        "    def query_expanded(self, query_text: str, top_n: int = 10, max_expansions: int = 8) -> List[Tuple[str, float]]:",
        "        return query_module.query_with_spreading_activation(query_text, self.layers, self.tokenizer, top_n, max_expansions)",
        "    ",
        "    def find_related_documents(self, doc_id: str) -> List[Tuple[str, float]]:",
        "        return query_module.find_related_documents(doc_id, self.layers)",
        "    ",
        "    def analyze_knowledge_gaps(self) -> Dict:",
        "        return gaps_module.analyze_knowledge_gaps(self.layers, self.documents)",
        "    "
      ],
      "change_type": "add"
    },
    {
      "file": "cortical/query.py",
      "function": "def query_with_spreading_activation(",
      "start_line": 268,
      "lines_added": [
        "",
        "",
        "",
        "",
        "",
        "",
        "def create_chunks(",
        "    text: str,",
        "    chunk_size: int = 512,",
        "    overlap: int = 128",
        ") -> List[Tuple[str, int, int]]:",
        "    \"\"\"",
        "    Split text into overlapping chunks.",
        "",
        "    Args:",
        "        text: Document text to chunk",
        "        chunk_size: Target size of each chunk in characters",
        "        overlap: Number of overlapping characters between chunks",
        "",
        "    Returns:",
        "        List of (chunk_text, start_char, end_char) tuples",
        "    \"\"\"",
        "    if not text:",
        "        return []",
        "",
        "    chunks = []",
        "    stride = max(1, chunk_size - overlap)",
        "    text_len = len(text)",
        "",
        "    for start in range(0, text_len, stride):",
        "        end = min(start + chunk_size, text_len)",
        "        chunk = text[start:end]",
        "        chunks.append((chunk, start, end))",
        "",
        "        if end >= text_len:",
        "            break",
        "",
        "    return chunks",
        "",
        "",
        "def score_chunk(",
        "    chunk_text: str,",
        "    query_terms: Dict[str, float],",
        "    layer0: HierarchicalLayer,",
        "    tokenizer: Tokenizer,",
        "    doc_id: Optional[str] = None",
        ") -> float:",
        "    \"\"\"",
        "    Score a chunk against query terms using TF-IDF.",
        "",
        "    Args:",
        "        chunk_text: Text of the chunk",
        "        query_terms: Dict mapping query terms to weights",
        "        layer0: Token layer for TF-IDF lookups",
        "        tokenizer: Tokenizer instance",
        "        doc_id: Optional document ID for per-document TF-IDF",
        "",
        "    Returns:",
        "        Relevance score for the chunk",
        "    \"\"\"",
        "    chunk_tokens = tokenizer.tokenize(chunk_text)",
        "    if not chunk_tokens:",
        "        return 0.0",
        "",
        "    # Count token occurrences in chunk",
        "    token_counts: Dict[str, int] = {}",
        "    for token in chunk_tokens:",
        "        token_counts[token] = token_counts.get(token, 0) + 1",
        "",
        "    score = 0.0",
        "    for term, term_weight in query_terms.items():",
        "        if term in token_counts:",
        "            col = layer0.get_minicolumn(term)",
        "            if col:",
        "                # Use per-document TF-IDF if available, otherwise global",
        "                tfidf = col.tfidf_per_doc.get(doc_id, col.tfidf) if doc_id else col.tfidf",
        "                # Weight by occurrence in chunk and query weight",
        "                score += tfidf * token_counts[term] * term_weight",
        "",
        "    # Normalize by chunk length to avoid bias toward longer chunks",
        "    return score / len(chunk_tokens) if chunk_tokens else 0.0",
        "",
        "",
        "def find_passages_for_query(",
        "    query_text: str,",
        "    layers: Dict[CorticalLayer, HierarchicalLayer],",
        "    tokenizer: Tokenizer,",
        "    documents: Dict[str, str],",
        "    top_n: int = 5,",
        "    chunk_size: int = 512,",
        "    overlap: int = 128,",
        "    use_expansion: bool = True,",
        "    doc_filter: Optional[List[str]] = None",
        ") -> List[Tuple[str, str, int, int, float]]:",
        "    \"\"\"",
        "    Find text passages most relevant to a query.",
        "",
        "    This is the key function for RAG systems - instead of returning document IDs,",
        "    it returns actual text passages with position information for citations.",
        "",
        "    Args:",
        "        query_text: Search query",
        "        layers: Dictionary of layers",
        "        tokenizer: Tokenizer instance",
        "        documents: Dict mapping doc_id to document text",
        "        top_n: Number of passages to return",
        "        chunk_size: Size of each chunk in characters (default 512)",
        "        overlap: Overlap between chunks in characters (default 128)",
        "        use_expansion: Whether to expand query terms",
        "        doc_filter: Optional list of doc_ids to restrict search to",
        "",
        "    Returns:",
        "        List of (passage_text, doc_id, start_char, end_char, score) tuples",
        "        ranked by relevance",
        "    \"\"\"",
        "    layer0 = layers[CorticalLayer.TOKENS]",
        "",
        "    # Get expanded query terms",
        "    if use_expansion:",
        "        query_terms = expand_query(query_text, layers, tokenizer, max_expansions=5)",
        "    else:",
        "        tokens = tokenizer.tokenize(query_text)",
        "        query_terms = {t: 1.0 for t in tokens}",
        "",
        "    if not query_terms:",
        "        return []",
        "",
        "    # First, get candidate documents (more than we need, since we'll rank passages)",
        "    doc_scores = find_documents_for_query(",
        "        query_text, layers, tokenizer,",
        "        top_n=min(len(documents), top_n * 3),",
        "        use_expansion=use_expansion",
        "    )",
        "",
        "    # Apply document filter if provided",
        "    if doc_filter:",
        "        doc_scores = [(doc_id, score) for doc_id, score in doc_scores if doc_id in doc_filter]",
        "",
        "    # Score passages within candidate documents",
        "    passages: List[Tuple[str, str, int, int, float]] = []",
        "",
        "    for doc_id, doc_score in doc_scores:",
        "        if doc_id not in documents:",
        "            continue",
        "",
        "        text = documents[doc_id]",
        "        chunks = create_chunks(text, chunk_size, overlap)",
        "",
        "        for chunk_text, start_char, end_char in chunks:",
        "            chunk_score = score_chunk(",
        "                chunk_text, query_terms, layer0, tokenizer, doc_id",
        "            )",
        "            # Combine chunk score with document score for final ranking",
        "            combined_score = chunk_score * (1 + doc_score * 0.1)",
        "",
        "            passages.append((",
        "                chunk_text,",
        "                doc_id,",
        "                start_char,",
        "                end_char,",
        "                combined_score",
        "            ))",
        "",
        "    # Sort by score and return top passages",
        "    passages.sort(key=lambda x: x[4], reverse=True)",
        "    return passages[:top_n]"
      ],
      "lines_removed": [
        "    ",
        "        ",
        "    ",
        "    "
      ],
      "context_before": [
        "    sorted_concepts = sorted(activated.items(), key=lambda x: -x[1])",
        "    return sorted_concepts[:top_n]",
        "",
        "",
        "def find_related_documents(",
        "    doc_id: str,",
        "    layers: Dict[CorticalLayer, HierarchicalLayer]",
        ") -> List[Tuple[str, float]]:",
        "    \"\"\"",
        "    Find documents related to a given document via lateral connections."
      ],
      "context_after": [
        "    Args:",
        "        doc_id: Source document ID",
        "        layers: Dictionary of layers",
        "    Returns:",
        "        List of (doc_id, weight) tuples for related documents",
        "    \"\"\"",
        "    layer3 = layers.get(CorticalLayer.DOCUMENTS)",
        "    if not layer3:",
        "        return []",
        "    col = layer3.get_minicolumn(doc_id)",
        "    if not col:",
        "        return []",
        "    related = []",
        "    for neighbor_id, weight in col.lateral_connections.items():",
        "        # Use O(1) ID lookup instead of linear search",
        "        neighbor = layer3.get_by_id(neighbor_id)",
        "        if neighbor:",
        "            related.append((neighbor.content, weight))",
        "",
        "    return sorted(related, key=lambda x: -x[1])"
      ],
      "change_type": "modify"
    },
    {
      "file": "tests/test_processor.py",
      "function": "class TestProcessorPersistence(unittest.TestCase):",
      "start_line": 146,
      "lines_added": [
        "class TestProcessorPassageRetrieval(unittest.TestCase):",
        "    \"\"\"Test chunk-level passage retrieval for RAG systems.\"\"\"",
        "",
        "    @classmethod",
        "    def setUpClass(cls):",
        "        cls.processor = CorticalTextProcessor()",
        "        # Create documents with distinct content for testing passage retrieval",
        "        cls.processor.process_document(\"neural_doc\", \"\"\"",
        "            Neural networks are computational models inspired by biological neurons.",
        "            They process information through interconnected layers of nodes.",
        "            Deep learning uses many layers to learn hierarchical representations.",
        "            Backpropagation is the key algorithm for training neural networks.",
        "            Convolutional neural networks excel at image recognition tasks.",
        "        \"\"\")",
        "        cls.processor.process_document(\"ml_doc\", \"\"\"",
        "            Machine learning algorithms learn patterns from data automatically.",
        "            Supervised learning requires labeled training examples.",
        "            Unsupervised learning discovers hidden structure in unlabeled data.",
        "            Reinforcement learning trains agents through rewards and penalties.",
        "            Model evaluation uses metrics like accuracy and precision.",
        "        \"\"\")",
        "        cls.processor.process_document(\"data_doc\", \"\"\"",
        "            Data preprocessing is essential for machine learning pipelines.",
        "            Feature engineering creates meaningful input representations.",
        "            Data normalization scales features to similar ranges.",
        "            Missing value imputation handles incomplete datasets.",
        "            Cross-validation ensures robust model performance estimates.",
        "        \"\"\")",
        "        cls.processor.compute_all(verbose=False)",
        "",
        "    def test_find_passages_returns_list(self):",
        "        \"\"\"Test that find_passages_for_query returns a list.\"\"\"",
        "        results = self.processor.find_passages_for_query(\"neural networks\")",
        "        self.assertIsInstance(results, list)",
        "",
        "    def test_find_passages_returns_tuples(self):",
        "        \"\"\"Test that results are tuples with correct structure.\"\"\"",
        "        results = self.processor.find_passages_for_query(\"neural networks\", top_n=1)",
        "        self.assertGreater(len(results), 0)",
        "        passage, doc_id, start, end, score = results[0]",
        "        self.assertIsInstance(passage, str)",
        "        self.assertIsInstance(doc_id, str)",
        "        self.assertIsInstance(start, int)",
        "        self.assertIsInstance(end, int)",
        "        self.assertIsInstance(score, float)",
        "",
        "    def test_find_passages_contains_text(self):",
        "        \"\"\"Test that passages contain actual text.\"\"\"",
        "        results = self.processor.find_passages_for_query(\"neural\", top_n=3)",
        "        self.assertGreater(len(results), 0)",
        "        passage, _, _, _, _ = results[0]",
        "        self.assertGreater(len(passage), 0)",
        "",
        "    def test_find_passages_position_valid(self):",
        "        \"\"\"Test that start/end positions are valid.\"\"\"",
        "        results = self.processor.find_passages_for_query(\"learning\", top_n=3)",
        "        for passage, doc_id, start, end, score in results:",
        "            self.assertGreaterEqual(start, 0)",
        "            self.assertGreater(end, start)",
        "            self.assertEqual(len(passage), end - start)",
        "",
        "    def test_find_passages_top_n_limit(self):",
        "        \"\"\"Test that top_n parameter limits results.\"\"\"",
        "        results = self.processor.find_passages_for_query(\"learning\", top_n=2)",
        "        self.assertLessEqual(len(results), 2)",
        "",
        "    def test_find_passages_chunk_size(self):",
        "        \"\"\"Test that chunk_size parameter is respected.\"\"\"",
        "        results = self.processor.find_passages_for_query(",
        "            \"neural\", top_n=5, chunk_size=100, overlap=20",
        "        )",
        "        for passage, _, _, _, _ in results:",
        "            self.assertLessEqual(len(passage), 100)",
        "",
        "    def test_find_passages_doc_filter(self):",
        "        \"\"\"Test that doc_filter restricts search.\"\"\"",
        "        results = self.processor.find_passages_for_query(",
        "            \"learning\", top_n=10, doc_filter=[\"neural_doc\"]",
        "        )",
        "        for _, doc_id, _, _, _ in results:",
        "            self.assertEqual(doc_id, \"neural_doc\")",
        "",
        "    def test_find_passages_scores_descending(self):",
        "        \"\"\"Test that results are sorted by score descending.\"\"\"",
        "        results = self.processor.find_passages_for_query(\"neural networks\", top_n=5)",
        "        if len(results) > 1:",
        "            scores = [score for _, _, _, _, score in results]",
        "            self.assertEqual(scores, sorted(scores, reverse=True))",
        "",
        "    def test_find_passages_no_expansion(self):",
        "        \"\"\"Test passage retrieval without query expansion.\"\"\"",
        "        results = self.processor.find_passages_for_query(",
        "            \"neural\", top_n=3, use_expansion=False",
        "        )",
        "        self.assertIsInstance(results, list)",
        "",
        "    def test_find_passages_empty_query(self):",
        "        \"\"\"Test handling of queries with no matching terms.\"\"\"",
        "        results = self.processor.find_passages_for_query(\"xyznonexistent123\")",
        "        self.assertEqual(len(results), 0)",
        "",
        "",
        "",
        "",
        ""
      ],
      "lines_removed": [
        "    ",
        "    ",
        "    "
      ],
      "context_before": [
        "        ",
        "        with tempfile.TemporaryDirectory() as tmpdir:",
        "            filepath = os.path.join(tmpdir, \"test.pkl\")",
        "            processor.save(filepath, verbose=False)",
        "            ",
        "            loaded = CorticalTextProcessor.load(filepath, verbose=False)",
        "            self.assertEqual(len(loaded.documents), 1)",
        "            self.assertIn(\"doc1\", loaded.documents)",
        "",
        ""
      ],
      "context_after": [
        "class TestProcessorGaps(unittest.TestCase):",
        "    \"\"\"Test gap detection functionality.\"\"\"",
        "    @classmethod",
        "    def setUpClass(cls):",
        "        cls.processor = CorticalTextProcessor()",
        "        for i in range(3):",
        "            cls.processor.process_document(f\"tech_{i}\", \"\"\"",
        "                Machine learning neural networks deep learning.",
        "                Training models data processing algorithms.",
        "            \"\"\")",
        "        cls.processor.process_document(\"outlier\", \"\"\"",
        "            Medieval falconry birds hunting prey.",
        "            Falcons hawks eagles training.",
        "        \"\"\")",
        "        cls.processor.compute_all(verbose=False)",
        "    def test_analyze_knowledge_gaps(self):",
        "        \"\"\"Test gap analysis returns expected structure.\"\"\"",
        "        gaps = self.processor.analyze_knowledge_gaps()",
        "        self.assertIn('isolated_documents', gaps)",
        "        self.assertIn('weak_topics', gaps)",
        "        self.assertIn('coverage_score', gaps)",
        "    def test_detect_anomalies(self):",
        "        \"\"\"Test anomaly detection.\"\"\"",
        "        anomalies = self.processor.detect_anomalies(threshold=0.1)",
        "        self.assertIsInstance(anomalies, list)",
        "",
        "",
        "if __name__ == \"__main__\":",
        "    unittest.main(verbosity=2)"
      ],
      "change_type": "modify"
    }
  ],
  "hour_of_day": 19,
  "day_of_week": "Tuesday",
  "seconds_since_last_commit": -496584,
  "is_merge": false,
  "is_initial": false,
  "parent_count": 1,
  "session_id": null,
  "related_chats": [],
  "ci_result": null,
  "reverted": false,
  "amended": false
}