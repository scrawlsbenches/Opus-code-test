{
  "hash": "864756a6b31f38dd34842b998fb19986302d61ec",
  "message": "Optimize semantics O(n²) similarity with early termination (Task #136)",
  "author": "Claude",
  "timestamp": "2025-12-11 22:30:05 +0000",
  "branch": "claude/multi-index-design-DvifZ",
  "files_changed": [
    "cortical/processor.py",
    "cortical/semantics.py"
  ],
  "insertions": 42,
  "deletions": 11,
  "hunks": [
    {
      "file": "cortical/processor.py",
      "function": "class CorticalTextProcessor:",
      "start_line": 1103,
      "lines_added": [
        "        max_similarity_pairs: int = 100000,",
        "        min_context_keys: int = 3,",
        "            max_similarity_pairs: Maximum pairs to check for SimilarTo relations.",
        "                Set to 0 for unlimited (may be slow for large corpora). Default 100000.",
        "            min_context_keys: Minimum context keys for a term to be considered for",
        "                SimilarTo relations. Terms with fewer keys are skipped. Default 3.",
        "            min_pattern_confidence=min_pattern_confidence,",
        "            max_similarity_pairs=max_similarity_pairs,",
        "            min_context_keys=min_context_keys"
      ],
      "lines_removed": [
        "            min_pattern_confidence=min_pattern_confidence"
      ],
      "context_before": [
        "                parts.append(f\"semantic: {stats['semantic_connections']}\")",
        "            if stats.get('embedding_connections', 0) > 0:",
        "                parts.append(f\"embedding: {stats['embedding_connections']}\")",
        "            print(\", \".join(parts) if len(parts) > 1 else parts[0])",
        "        return stats",
        "",
        "    def extract_corpus_semantics(",
        "        self,",
        "        use_pattern_extraction: bool = True,",
        "        min_pattern_confidence: float = 0.6,"
      ],
      "context_after": [
        "        verbose: bool = True",
        "    ) -> int:",
        "        \"\"\"",
        "        Extract semantic relations from the corpus.",
        "",
        "        Combines co-occurrence analysis with pattern-based extraction to discover",
        "        semantic relationships like IsA, HasA, UsedFor, Causes, etc.",
        "",
        "        Args:",
        "            use_pattern_extraction: Extract relations from text patterns (e.g., \"X is a Y\")",
        "            min_pattern_confidence: Minimum confidence for pattern-based relations",
        "            verbose: Print progress messages",
        "",
        "        Returns:",
        "            Number of relations extracted",
        "",
        "        Example:",
        "            >>> count = processor.extract_corpus_semantics(verbose=False)",
        "            >>> print(f\"Found {count} semantic relations\")",
        "        \"\"\"",
        "        self.semantic_relations = semantics.extract_corpus_semantics(",
        "            self.layers,",
        "            self.documents,",
        "            self.tokenizer,",
        "            use_pattern_extraction=use_pattern_extraction,",
        "        )",
        "        if verbose:",
        "            print(f\"Extracted {len(self.semantic_relations)} semantic relations\")",
        "        return len(self.semantic_relations)",
        "",
        "    def extract_pattern_relations(",
        "        self,",
        "        min_confidence: float = 0.6,",
        "        verbose: bool = True",
        "    ) -> List[Tuple[str, str, str, float]]:"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/semantics.py",
      "function": "def get_pattern_statistics(relations: List[Tuple[str, str, str, float]]) -> Dict",
      "start_line": 216,
      "lines_added": [
        "    min_pattern_confidence: float = 0.6,",
        "    max_similarity_pairs: int = 100000,",
        "    min_context_keys: int = 3",
        "        max_similarity_pairs: Maximum pairs to check for SimilarTo relations.",
        "            Set to 0 for unlimited (may be slow for large corpora). Default 100000.",
        "        min_context_keys: Minimum context keys for a term to be considered for",
        "            SimilarTo relations. Terms with fewer keys are skipped. Default 3."
      ],
      "lines_removed": [
        "    min_pattern_confidence: float = 0.6"
      ],
      "context_before": [
        "    }",
        "",
        "",
        "def extract_corpus_semantics(",
        "    layers: Dict[CorticalLayer, HierarchicalLayer],",
        "    documents: Dict[str, str],",
        "    tokenizer,",
        "    window_size: int = 5,",
        "    min_cooccurrence: int = 2,",
        "    use_pattern_extraction: bool = True,"
      ],
      "context_after": [
        ") -> List[Tuple[str, str, str, float]]:",
        "    \"\"\"",
        "    Extract semantic relations from corpus co-occurrence patterns.",
        "",
        "    Analyzes word co-occurrences to infer semantic relationships:",
        "    - Words appearing together frequently → CoOccurs",
        "    - Words appearing in similar contexts → SimilarTo",
        "    - Pattern-based extraction → IsA, HasA, UsedFor, Causes, etc.",
        "",
        "    Args:",
        "        layers: Dictionary of layers (needs TOKENS)",
        "        documents: Dictionary of documents",
        "        tokenizer: Tokenizer instance for processing text",
        "        window_size: Co-occurrence window size",
        "        min_cooccurrence: Minimum co-occurrences to form relation",
        "        use_pattern_extraction: Whether to extract relations from text patterns",
        "        min_pattern_confidence: Minimum confidence for pattern-based extraction",
        "",
        "    Returns:",
        "        List of (term1, relation, term2, weight) tuples",
        "    \"\"\"",
        "    layer0 = layers[CorticalLayer.TOKENS]",
        "    relations: List[Tuple[str, str, str, float]] = []",
        "    ",
        "    # Track co-occurrences within window",
        "    cooccurrence: Dict[Tuple[str, str], int] = defaultdict(int)",
        "    "
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/semantics.py",
      "function": "def extract_corpus_semantics(",
      "start_line": 326,
      "lines_added": [
        "        # Fallback: pure Python implementation with optimizations",
        "        # Pre-filter terms by minimum context keys",
        "        key_sets: Dict[str, set] = {}",
        "",
        "            keys = set(vec.keys())",
        "            # Skip terms with too few context keys (can't meet min_context_keys threshold)",
        "            if len(keys) < min_context_keys:",
        "                continue",
        "            key_sets[term] = keys",
        "        # Get filtered terms with enough context",
        "        filtered_terms = [t for t in terms if t in key_sets and magnitudes.get(t, 0) > 0]",
        "",
        "        # Track pairs checked for early termination",
        "        pairs_checked = 0",
        "        for i, t1 in enumerate(filtered_terms):",
        "            for t2 in filtered_terms[i+1:]:",
        "                # Check pair limit",
        "                if max_similarity_pairs > 0 and pairs_checked >= max_similarity_pairs:",
        "                    break",
        "",
        "                pairs_checked += 1",
        "                if len(common) >= min_context_keys:",
        "            # Also check outer loop for pair limit",
        "            if max_similarity_pairs > 0 and pairs_checked >= max_similarity_pairs:",
        "                break",
        ""
      ],
      "lines_removed": [
        "        # Fallback: pure Python implementation",
        "        key_sets: Dict[str, set] = {term: set(context_vectors[term].keys()) for term in terms}",
        "        for i, t1 in enumerate(terms):",
        "            if mag1 == 0:",
        "                continue",
        "            for t2 in terms[i+1:]:",
        "                if mag2 == 0:",
        "                    continue",
        "                if len(common) >= 3:"
      ],
      "context_before": [
        "        # Extract pairs with similarity > 0.3 and at least 3 common keys",
        "        for i in range(n_terms):",
        "            row_i = nonzero_counts[i]",
        "            for j in range(i + 1, n_terms):",
        "                if similarities[i, j] > 0.3:",
        "                    common_count = np.sum(row_i & nonzero_counts[j])",
        "                    if common_count >= 3:",
        "                        relations.append((terms[i], 'SimilarTo', terms[j], float(similarities[i, j])))",
        "",
        "    elif n_terms > 1:"
      ],
      "context_after": [
        "        magnitudes: Dict[str, float] = {}",
        "        for term in terms:",
        "            vec = context_vectors[term]",
        "            mag = math.sqrt(sum(v * v for v in vec.values()))",
        "            magnitudes[term] = mag",
        "",
        "",
        "            vec1 = context_vectors[t1]",
        "            mag1 = magnitudes[t1]",
        "            keys1 = key_sets[t1]",
        "",
        "                mag2 = magnitudes[t2]",
        "",
        "                common = keys1 & key_sets[t2]",
        "                    vec2 = context_vectors[t2]",
        "                    dot = sum(vec1[k] * vec2[k] for k in common)",
        "                    sim = dot / (mag1 * mag2)",
        "                    if sim > 0.3:",
        "                        relations.append((t1, 'SimilarTo', t2, sim))",
        "",
        "    # Extract commonsense relations from text patterns",
        "    if use_pattern_extraction:",
        "        valid_terms = set(layer0.minicolumns.keys())",
        "        pattern_relations = extract_pattern_relations(",
        "            documents,",
        "            valid_terms,",
        "            min_confidence=min_pattern_confidence",
        "        )",
        "        relations.extend(pattern_relations)"
      ],
      "change_type": "modify"
    }
  ],
  "hour_of_day": 22,
  "day_of_week": "Thursday",
  "seconds_since_last_commit": -314083,
  "is_merge": false,
  "is_initial": false,
  "parent_count": 1,
  "session_id": null,
  "related_chats": [],
  "ci_result": null,
  "reverted": false,
  "amended": false
}