{
  "hash": "3cd94d51dba5f555863d97bfd30f7232a43542a5",
  "message": "Merge pull request #66 from scrawlsbenches/claude/merge-friendly-task-ids-015NYkCkkSHZXKhTTdUq8wZu",
  "author": "scrawlsbenches",
  "timestamp": "2025-12-13 18:29:44 -0500",
  "branch": "claude/multi-index-design-DvifZ",
  "files_changed": [
    ".claude/skills/task-manager/SKILL.md",
    ".claude/workflows/bugfix.yaml",
    ".claude/workflows/feature.yaml",
    ".claude/workflows/refactor.yaml",
    ".gitignore",
    "docs/merge-friendly-tasks.md",
    "scripts/consolidate_tasks.py",
    "scripts/new_task.py",
    "scripts/task_utils.py",
    "scripts/workflow.py",
    "tasks/2025-12-13_22-32-34_e233.json",
    "tasks/2025-12-13_22-33-34_2d89.json",
    "tasks/2025-12-13_22-42-20_6ac7.json",
    "tasks/2025-12-13_22-50-18_cdd1.json",
    "tests/integration/test_task_integration.py",
    "tests/unit/test_task_utils.py"
  ],
  "insertions": 3055,
  "deletions": 0,
  "hunks": [
    {
      "file": ".claude/skills/task-manager/SKILL.md",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "---",
        "name: task-manager",
        "description: Manage tasks with merge-friendly IDs for parallel agent workflows. Use when creating, updating, or querying tasks during development. Prevents conflicts when multiple agents run in parallel.",
        "allowed-tools: Read, Bash, Write",
        "---",
        "# Task Manager Skill",
        "",
        "This skill enables **merge-friendly task management** for parallel agent workflows. It uses timestamp+session IDs that can't conflict when multiple agents work simultaneously.",
        "",
        "## Key Capabilities",
        "",
        "- **Conflict-free task creation**: Each agent writes to its own session file",
        "- **Unique task IDs**: `T-YYYYMMDD-HHMMSS-XXXX` format",
        "- **Session tracking**: All tasks from one session share a suffix",
        "- **Consolidation**: Merge task files like `git gc`",
        "- **TASK_LIST.md compatible**: Can generate markdown summaries",
        "",
        "## When to Use",
        "",
        "- Starting a new piece of work that needs tracking",
        "- Creating tasks from parallel agent workflows",
        "- Consolidating tasks after multi-agent runs",
        "- Querying task status across sessions",
        "",
        "## Quick Start",
        "",
        "### Create Tasks",
        "",
        "```python",
        "# In Python",
        "from scripts.task_utils import TaskSession",
        "",
        "session = TaskSession()",
        "task = session.create_task(",
        "    title=\"Implement feature X\",",
        "    priority=\"high\",",
        "    category=\"arch\",",
        "    description=\"Add new capability to processor\"",
        ")",
        "print(f\"Created: {task.id}\")  # T-20251213-143052-a1b2",
        "session.save()  # â†’ tasks/2025-12-13_14-30-52_a1b2.json",
        "```",
        "",
        "### Generate Task ID (CLI)",
        "",
        "```bash",
        "# Full format",
        "python scripts/task_utils.py generate",
        "# Output: T-20251213-143052-a1b2",
        "",
        "# Short format",
        "python scripts/task_utils.py generate --short",
        "# Output: T-a1b2c3d4",
        "```",
        "",
        "### List All Tasks",
        "",
        "```bash",
        "python scripts/task_utils.py list",
        "python scripts/task_utils.py list --status pending",
        "```",
        "",
        "### Consolidate Tasks",
        "",
        "```bash",
        "# See summary",
        "python scripts/consolidate_tasks.py --summary",
        "",
        "# Consolidate and deduplicate",
        "python scripts/consolidate_tasks.py --update --auto-merge",
        "",
        "# Archive old session files",
        "python scripts/consolidate_tasks.py --update --archive",
        "```",
        "",
        "## Task Structure",
        "",
        "```json",
        "{",
        "  \"id\": \"T-20251213-143052-a1b2\",",
        "  \"title\": \"Implement feature X\",",
        "  \"status\": \"pending\",",
        "  \"priority\": \"high\",",
        "  \"category\": \"arch\",",
        "  \"description\": \"...\",",
        "  \"depends_on\": [\"T-20251213-143000-c3d4\"],",
        "  \"effort\": \"medium\",",
        "  \"context\": {",
        "    \"files\": [\"cortical/processor.py\"],",
        "    \"methods\": [\"compute_all()\"]",
        "  }",
        "}",
        "```",
        "",
        "## Integration with TASK_LIST.md",
        "",
        "The new system can coexist with the legacy `TASK_LIST.md`:",
        "- Legacy tasks keep `#123` format",
        "- New parallel work uses `T-...` format",
        "- Both can be referenced and tracked",
        "",
        "## Tips",
        "",
        "1. **Create session at workflow start** - all tasks share session suffix",
        "2. **Save before commit** - persist tasks to disk",
        "3. **Consolidate weekly** - merge sessions, resolve duplicates",
        "4. **Use context field** - add file/method references for quick navigation"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": ".claude/workflows/bugfix.yaml",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "# Workflow: Bug Fix",
        "# Creates a standard set of tasks for fixing bugs",
        "",
        "name: \"Bug Fix\"",
        "description: \"Investigation -> Fix -> Test -> Document\"",
        "category: \"bugfix\"",
        "",
        "# Variables that will be prompted or provided",
        "variables:",
        "  - name: bug_title",
        "    description: \"Brief description of the bug\"",
        "    required: true",
        "  - name: priority",
        "    description: \"Bug priority\"",
        "    default: \"high\"",
        "    choices: [\"high\", \"medium\", \"low\"]",
        "  - name: affected_file",
        "    description: \"Primary file affected (optional)\"",
        "    required: false",
        "",
        "tasks:",
        "  - id: investigate",
        "    title: \"Investigate: {bug_title}\"",
        "    category: \"research\"",
        "    priority: \"{priority}\"",
        "    effort: \"small\"",
        "    description: |",
        "      Investigate the root cause of: {bug_title}",
        "",
        "      Steps:",
        "      1. Reproduce the issue",
        "      2. Identify the root cause",
        "      3. Document findings in task notes",
        "",
        "  - id: fix",
        "    title: \"Fix: {bug_title}\"",
        "    category: \"bugfix\"",
        "    priority: \"{priority}\"",
        "    effort: \"medium\"",
        "    depends_on: [investigate]",
        "    description: |",
        "      Implement the fix for: {bug_title}",
        "",
        "      Checklist:",
        "      - [ ] Fix implemented",
        "      - [ ] Code follows existing patterns",
        "      - [ ] No new warnings introduced",
        "",
        "  - id: test",
        "    title: \"Add regression test for: {bug_title}\"",
        "    category: \"test\"",
        "    priority: \"high\"",
        "    effort: \"small\"",
        "    depends_on: [fix]",
        "    description: |",
        "      Add test(s) to prevent regression.",
        "",
        "      Requirements:",
        "      - Test should fail without the fix",
        "      - Test should pass with the fix",
        "      - Test covers edge cases",
        "",
        "  - id: document",
        "    title: \"Document fix for: {bug_title}\"",
        "    category: \"docs\"",
        "    priority: \"low\"",
        "    effort: \"small\"",
        "    depends_on: [fix]",
        "    description: |",
        "      Update any relevant documentation.",
        "",
        "      Consider:",
        "      - CHANGELOG entry",
        "      - Code comments if complex",
        "      - Update troubleshooting docs if user-facing"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": ".claude/workflows/feature.yaml",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "# Workflow: Feature Implementation",
        "# Creates tasks for implementing a new feature",
        "",
        "name: \"Feature\"",
        "description: \"Design -> Implement -> Test -> Review\"",
        "category: \"feature\"",
        "",
        "variables:",
        "  - name: feature_name",
        "    description: \"Name of the feature\"",
        "    required: true",
        "  - name: priority",
        "    description: \"Feature priority\"",
        "    default: \"medium\"",
        "    choices: [\"high\", \"medium\", \"low\"]",
        "  - name: effort",
        "    description: \"Overall effort estimate\"",
        "    default: \"large\"",
        "    choices: [\"small\", \"medium\", \"large\"]",
        "",
        "tasks:",
        "  - id: design",
        "    title: \"Design: {feature_name}\"",
        "    category: \"arch\"",
        "    priority: \"{priority}\"",
        "    effort: \"medium\"",
        "    description: |",
        "      Design the API and architecture for: {feature_name}",
        "",
        "      Deliverables:",
        "      - API design (function signatures, data structures)",
        "      - Integration points with existing code",
        "      - Edge cases to handle",
        "",
        "  - id: implement",
        "    title: \"Implement: {feature_name}\"",
        "    category: \"feature\"",
        "    priority: \"{priority}\"",
        "    effort: \"{effort}\"",
        "    depends_on: [design]",
        "    description: |",
        "      Implement the core functionality for: {feature_name}",
        "",
        "      Checklist:",
        "      - [ ] Core logic implemented",
        "      - [ ] Error handling added",
        "      - [ ] Follows existing code patterns",
        "",
        "  - id: unit_tests",
        "    title: \"Unit tests for: {feature_name}\"",
        "    category: \"test\"",
        "    priority: \"high\"",
        "    effort: \"medium\"",
        "    depends_on: [implement]",
        "    description: |",
        "      Write comprehensive unit tests.",
        "",
        "      Coverage requirements:",
        "      - Happy path scenarios",
        "      - Edge cases",
        "      - Error conditions",
        "      - Target: 90%+ coverage for new code",
        "",
        "  - id: integration_tests",
        "    title: \"Integration tests for: {feature_name}\"",
        "    category: \"test\"",
        "    priority: \"high\"",
        "    effort: \"medium\"",
        "    depends_on: [implement]",
        "    description: |",
        "      Write integration tests verifying feature works with existing system.",
        "",
        "      Test scenarios:",
        "      - End-to-end workflows",
        "      - Interaction with other components",
        "      - Performance characteristics",
        "",
        "  - id: documentation",
        "    title: \"Documentation for: {feature_name}\"",
        "    category: \"docs\"",
        "    priority: \"medium\"",
        "    effort: \"small\"",
        "    depends_on: [unit_tests, integration_tests]",
        "    description: |",
        "      Document the new feature.",
        "",
        "      Include:",
        "      - Usage examples",
        "      - API reference updates",
        "      - CLAUDE.md updates if applicable"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": ".claude/workflows/refactor.yaml",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "# Workflow: Refactoring",
        "# Creates tasks for safe refactoring",
        "",
        "name: \"Refactor\"",
        "description: \"Analyze -> Refactor -> Verify -> Cleanup\"",
        "category: \"refactor\"",
        "",
        "variables:",
        "  - name: refactor_target",
        "    description: \"What is being refactored (module, function, pattern)\"",
        "    required: true",
        "  - name: priority",
        "    description: \"Refactor priority\"",
        "    default: \"medium\"",
        "    choices: [\"high\", \"medium\", \"low\"]",
        "",
        "tasks:",
        "  - id: analyze",
        "    title: \"Analyze before refactor: {refactor_target}\"",
        "    category: \"research\"",
        "    priority: \"{priority}\"",
        "    effort: \"small\"",
        "    description: |",
        "      Understand current state before refactoring.",
        "",
        "      Document:",
        "      - Current behavior and edge cases",
        "      - All callers/dependencies",
        "      - Test coverage baseline",
        "",
        "  - id: refactor",
        "    title: \"Refactor: {refactor_target}\"",
        "    category: \"refactor\"",
        "    priority: \"{priority}\"",
        "    effort: \"medium\"",
        "    depends_on: [analyze]",
        "    description: |",
        "      Perform the refactoring.",
        "",
        "      Guidelines:",
        "      - Make small, incremental changes",
        "      - Run tests after each change",
        "      - Keep commits atomic",
        "",
        "  - id: verify",
        "    title: \"Verify refactor: {refactor_target}\"",
        "    category: \"test\"",
        "    priority: \"high\"",
        "    effort: \"small\"",
        "    depends_on: [refactor]",
        "    description: |",
        "      Verify refactoring didn't break anything.",
        "",
        "      Checklist:",
        "      - [ ] All existing tests pass",
        "      - [ ] Coverage didn't decrease",
        "      - [ ] No new warnings",
        "      - [ ] Performance not degraded",
        "",
        "  - id: cleanup",
        "    title: \"Cleanup after refactor: {refactor_target}\"",
        "    category: \"codequal\"",
        "    priority: \"low\"",
        "    effort: \"small\"",
        "    depends_on: [verify]",
        "    description: |",
        "      Final cleanup tasks.",
        "",
        "      Consider:",
        "      - Remove dead code",
        "      - Update comments",
        "      - Remove temporary scaffolding"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": ".gitignore",
      "function": "CodeCoverage/",
      "start_line": 79,
      "lines_added": [
        "tasks/.current_session.json"
      ],
      "lines_removed": [],
      "context_before": [
        "[Tt]est[Rr]esult*/",
        "[Bb]uild[Ll]og.*",
        "",
        "# NUnit",
        "*.VisualState.xml",
        "TestResult.xml",
        "nunit-*.xml",
        "# Indexer progress files",
        ".index_progress.json",
        ".index_incremental_progress.json"
      ],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "docs/merge-friendly-tasks.md",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "# Merge-Friendly Task Management",
        "",
        "This document describes the merge-friendly task ID system for parallel agent workflows.",
        "",
        "## The Problem",
        "",
        "When multiple Claude agents run in parallel on the same repository:",
        "- Both might create task `#239` (next sequential ID)",
        "- Both modify `TASK_LIST.md` simultaneously",
        "- Git merge conflicts are guaranteed",
        "",
        "## The Solution",
        "",
        "Use **timestamp-based, session-scoped task IDs** that can't collide:",
        "",
        "```",
        "T-20251213-143052-a1b2",
        "â”‚ â”‚        â”‚      â”‚",
        "â”‚ â”‚        â”‚      â””â”€â”€ 4-char session suffix (unique per agent session)",
        "â”‚ â”‚        â””â”€â”€ Time created (HHMMSS)",
        "â”‚ â””â”€â”€ Date created (YYYYMMDD)",
        "â””â”€â”€ Task prefix",
        "```",
        "",
        "Combined with **per-session task files**:",
        "",
        "```",
        "tasks/",
        "â”œâ”€â”€ 2025-12-13_14-30-52_a1b2.json    # Agent A's session",
        "â”œâ”€â”€ 2025-12-13_14-31-05_c3d4.json    # Agent B's session",
        "â””â”€â”€ ...",
        "```",
        "",
        "## How It Works",
        "",
        "### 1. Each Agent Creates Its Own Session",
        "",
        "```python",
        "from scripts.task_utils import TaskSession",
        "",
        "# Start a session (gets unique suffix like \"a1b2\")",
        "session = TaskSession()",
        "",
        "# Create tasks (all get same suffix)",
        "task1 = session.create_task(",
        "    title=\"Implement feature X\",",
        "    priority=\"high\",",
        "    category=\"arch\",",
        "    description=\"...\",",
        "    effort=\"medium\"",
        ")",
        "",
        "task2 = session.create_task(",
        "    title=\"Add tests for feature X\",",
        "    priority=\"medium\",",
        "    category=\"test\",",
        "    depends_on=[task1.id]",
        ")",
        "",
        "# Save to tasks/2025-12-13_14-30-52_a1b2.json",
        "session.save()",
        "```",
        "",
        "### 2. No Merge Conflicts",
        "",
        "Each agent writes to a **unique filename**:",
        "- Agent A: `tasks/2025-12-13_14-30-52_a1b2.json`",
        "- Agent B: `tasks/2025-12-13_14-31-05_c3d4.json`",
        "",
        "Files never conflict because:",
        "1. Timestamps are different (even by milliseconds)",
        "2. Session IDs are randomly generated",
        "3. Each agent only writes to its own file",
        "",
        "### 3. Consolidation (Like `git gc`)",
        "",
        "Periodically consolidate task files:",
        "",
        "```bash",
        "# Show summary of all tasks",
        "python scripts/consolidate_tasks.py --summary",
        "",
        "# Auto-merge duplicates and consolidate",
        "python scripts/consolidate_tasks.py --update --auto-merge",
        "",
        "# Archive old session files after consolidation",
        "python scripts/consolidate_tasks.py --update --archive",
        "```",
        "",
        "## Task ID Formats",
        "",
        "### Full Format (Default)",
        "```",
        "T-20251213-143052-a1b2",
        "```",
        "- Sortable by creation time",
        "- Self-documenting (when it was created)",
        "- Session-traceable (which agent created it)",
        "",
        "### Short Format",
        "```",
        "T-a1b2c3d4",
        "```",
        "- More compact (8 hex chars)",
        "- Still practically unique",
        "- Good for quick references",
        "",
        "```python",
        "from scripts.task_utils import generate_short_task_id",
        "task_id = generate_short_task_id()  # T-a1b2c3d4",
        "```",
        "",
        "## CLI Commands",
        "",
        "### Generate Task ID",
        "```bash",
        "# Full format",
        "python scripts/task_utils.py generate",
        "# Output: T-20251213-143052-a1b2",
        "",
        "# Short format",
        "python scripts/task_utils.py generate --short",
        "# Output: T-a1b2c3d4",
        "```",
        "",
        "### List All Tasks",
        "```bash",
        "# List all tasks",
        "python scripts/task_utils.py list",
        "",
        "# Filter by status",
        "python scripts/task_utils.py list --status pending",
        "```",
        "",
        "### Consolidate Tasks",
        "```bash",
        "# Dry run (see what would happen)",
        "python scripts/consolidate_tasks.py --dry-run",
        "",
        "# Consolidate with summary",
        "python scripts/consolidate_tasks.py --update",
        "",
        "# Auto-merge duplicates",
        "python scripts/consolidate_tasks.py --update --auto-merge",
        "```",
        "",
        "## Comparison with Legacy System",
        "",
        "| Aspect | Legacy (`#133`) | New (`T-a1b2c3d4`) |",
        "|--------|-----------------|---------------------|",
        "| Collision risk | High (parallel agents) | ~Zero |",
        "| Human readable | Very easy | Moderate |",
        "| Git-friendly | Conflicts guaranteed | No conflicts |",
        "| Sorting | Natural numeric | Chronological |",
        "| Traceability | None | Session + timestamp |",
        "",
        "## Best Practices",
        "",
        "### For Parallel Agents",
        "",
        "1. **Always create a session** at the start of your work",
        "2. **Save the session** before your work is committed",
        "3. **Reference tasks by full ID** in commits and comments",
        "",
        "### For Consolidation",
        "",
        "1. **Run consolidation weekly** (or after parallel agent runs)",
        "2. **Use `--auto-merge`** to deduplicate similar tasks",
        "3. **Archive old files** to keep the directory clean",
        "",
        "### For Migration",
        "",
        "The new system can coexist with legacy `TASK_LIST.md`:",
        "- Legacy tasks keep their `#123` format",
        "- New tasks use `T-...` format",
        "- Both can be referenced and tracked",
        "",
        "## Architecture",
        "",
        "```",
        "tasks/",
        "â”œâ”€â”€ 2025-12-13_14-30-52_a1b2.json    # Agent sessions (append-only)",
        "â”œâ”€â”€ 2025-12-13_14-31-05_c3d4.json",
        "â”œâ”€â”€ consolidated_2025-12-13.json     # Periodic consolidation",
        "â””â”€â”€ archive/                          # Archived old files",
        "    â””â”€â”€ ...",
        "",
        "TASK_LIST.md                          # Optional: human-readable summary",
        "```",
        "",
        "This mirrors the `chunk_index.py` architecture for corpus indexing.",
        "",
        "## Task File Format",
        "",
        "```json",
        "{",
        "  \"version\": 1,",
        "  \"session_id\": \"a1b2\",",
        "  \"started_at\": \"2025-12-13T14:30:52\",",
        "  \"saved_at\": \"2025-12-13T14:35:00\",",
        "  \"tasks\": [",
        "    {",
        "      \"id\": \"T-20251213-143052-a1b2\",",
        "      \"title\": \"Implement feature X\",",
        "      \"status\": \"pending\",",
        "      \"priority\": \"high\",",
        "      \"category\": \"arch\",",
        "      \"description\": \"Detailed description...\",",
        "      \"depends_on\": [],",
        "      \"effort\": \"medium\",",
        "      \"created_at\": \"2025-12-13T14:30:52\",",
        "      \"updated_at\": null,",
        "      \"completed_at\": null,",
        "      \"context\": {",
        "        \"files\": [\"cortical/processor.py\"],",
        "        \"methods\": [\"compute_all()\"]",
        "      }",
        "    }",
        "  ]",
        "}",
        "```",
        "",
        "## Future Enhancements",
        "",
        "1. **Real-time sync**: Watch for file changes and auto-consolidate",
        "2. **Web UI**: Visual task board from consolidated data",
        "3. **GitHub Issues sync**: Two-way sync with GitHub Issues",
        "4. **Task dependencies**: Topological sorting for execution order",
        "",
        "---",
        "",
        "*This system follows the same principles as `cortical/chunk_index.py` - append-only, git-friendly, merge-conflict-free.*"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "scripts/consolidate_tasks.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "#!/usr/bin/env python3",
        "\"\"\"",
        "Consolidate task files from parallel agent sessions.",
        "",
        "This script merges task files created by parallel agents into a unified",
        "view, resolving any conflicts and generating an updated TASK_LIST.md.",
        "",
        "Similar to `git gc` for chunk files, this consolidates distributed task",
        "state into a coherent whole.",
        "",
        "Usage:",
        "    # Show what would be consolidated (dry run)",
        "    python scripts/consolidate_tasks.py --dry-run",
        "",
        "    # Consolidate and update TASK_LIST.md",
        "    python scripts/consolidate_tasks.py --update",
        "",
        "    # Just generate a summary without modifying anything",
        "    python scripts/consolidate_tasks.py --summary",
        "",
        "    # Consolidate tasks from a specific directory",
        "    python scripts/consolidate_tasks.py --dir tasks/ --update",
        "",
        "Architecture:",
        "    tasks/",
        "    â”œâ”€â”€ 2025-12-13_14-30-52_a1b2.json    # Agent A's session",
        "    â”œâ”€â”€ 2025-12-13_14-31-05_c3d4.json    # Agent B's session",
        "    â””â”€â”€ ...",
        "",
        "    After consolidation:",
        "    â”œâ”€â”€ consolidated_2025-12-13_15-00-00.json  # Merged state",
        "    â””â”€â”€ (old files can be archived or removed)",
        "\"\"\"",
        "",
        "import argparse",
        "import json",
        "import os",
        "import shutil",
        "from collections import defaultdict",
        "from datetime import datetime",
        "from pathlib import Path",
        "from typing import Dict, List, Optional, Tuple, Any",
        "",
        "from task_utils import (",
        "    Task, TaskSession, load_all_tasks, consolidate_tasks,",
        "    DEFAULT_TASKS_DIR, generate_session_id",
        ")",
        "",
        "",
        "def find_conflicts(tasks: List[Task]) -> Dict[str, List[Task]]:",
        "    \"\"\"",
        "    Find tasks that might be duplicates or conflicts.",
        "",
        "    Returns dict of potential duplicate groups (same title, different IDs).",
        "    \"\"\"",
        "    by_title = defaultdict(list)",
        "    for task in tasks:",
        "        # Normalize title for comparison",
        "        normalized = task.title.lower().strip()",
        "        by_title[normalized].append(task)",
        "",
        "    # Return only groups with potential conflicts",
        "    return {title: tasks for title, tasks in by_title.items() if len(tasks) > 1}",
        "",
        "",
        "def merge_duplicate_tasks(tasks: List[Task]) -> Task:",
        "    \"\"\"",
        "    Merge potentially duplicate tasks into one.",
        "",
        "    Strategy:",
        "    - Keep the earliest creation time",
        "    - Use the most complete description",
        "    - Prefer higher priority",
        "    - Prefer \"in_progress\" or \"completed\" status over \"pending\"",
        "    \"\"\"",
        "    if len(tasks) == 1:",
        "        return tasks[0]",
        "",
        "    # Sort by creation time (keep earliest ID)",
        "    sorted_tasks = sorted(tasks, key=lambda t: t.created_at)",
        "    merged = Task(",
        "        id=sorted_tasks[0].id,",
        "        title=sorted_tasks[0].title,",
        "        created_at=sorted_tasks[0].created_at",
        "    )",
        "",
        "    # Merge fields from all tasks",
        "    priority_order = {\"high\": 0, \"medium\": 1, \"low\": 2}",
        "    status_order = {\"completed\": 0, \"in_progress\": 1, \"pending\": 2, \"deferred\": 3}",
        "",
        "    best_priority = min(tasks, key=lambda t: priority_order.get(t.priority, 1))",
        "    best_status = min(tasks, key=lambda t: status_order.get(t.status, 2))",
        "",
        "    merged.priority = best_priority.priority",
        "    merged.status = best_status.status",
        "    merged.category = sorted_tasks[0].category",
        "",
        "    # Use longest description",
        "    merged.description = max(tasks, key=lambda t: len(t.description)).description",
        "",
        "    # Merge dependencies",
        "    all_deps = set()",
        "    for task in tasks:",
        "        all_deps.update(task.depends_on)",
        "    merged.depends_on = list(all_deps)",
        "",
        "    # Merge context",
        "    merged.context = {}",
        "    for task in tasks:",
        "        merged.context.update(task.context)",
        "",
        "    # Track completion",
        "    completed = [t for t in tasks if t.completed_at]",
        "    if completed:",
        "        merged.completed_at = min(t.completed_at for t in completed)",
        "",
        "    return merged",
        "",
        "",
        "def consolidate_and_dedupe(",
        "    tasks_dir: str = DEFAULT_TASKS_DIR,",
        "    auto_merge: bool = False",
        ") -> Tuple[List[Task], Dict[str, List[Task]]]:",
        "    \"\"\"",
        "    Load all tasks and identify/resolve duplicates.",
        "",
        "    Args:",
        "        tasks_dir: Directory containing task session files",
        "        auto_merge: If True, automatically merge duplicates",
        "",
        "    Returns:",
        "        Tuple of (final task list, conflicts dict)",
        "    \"\"\"",
        "    all_tasks = load_all_tasks(tasks_dir)",
        "    conflicts = find_conflicts(all_tasks)",
        "",
        "    if not auto_merge or not conflicts:",
        "        return all_tasks, conflicts",
        "",
        "    # Auto-merge duplicates",
        "    merged_ids = set()",
        "    final_tasks = []",
        "",
        "    for title, conflict_group in conflicts.items():",
        "        merged = merge_duplicate_tasks(conflict_group)",
        "        final_tasks.append(merged)",
        "        merged_ids.update(t.id for t in conflict_group)",
        "",
        "    # Add non-conflicting tasks",
        "    for task in all_tasks:",
        "        if task.id not in merged_ids:",
        "            final_tasks.append(task)",
        "",
        "    return final_tasks, conflicts",
        "",
        "",
        "def generate_markdown_section(",
        "    tasks: List[Task],",
        "    status_filter: str,",
        "    priority_filter: Optional[str] = None",
        ") -> List[str]:",
        "    \"\"\"Generate markdown table rows for tasks matching filters.\"\"\"",
        "    filtered = [t for t in tasks if t.status == status_filter]",
        "    if priority_filter:",
        "        filtered = [t for t in filtered if t.priority == priority_filter]",
        "",
        "    if not filtered:",
        "        return []",
        "",
        "    # Sort by priority then creation time",
        "    priority_order = {\"high\": 0, \"medium\": 1, \"low\": 2}",
        "    filtered.sort(key=lambda t: (priority_order.get(t.priority, 1), t.created_at))",
        "",
        "    lines = []",
        "    for task in filtered:",
        "        deps = \", \".join(task.depends_on) if task.depends_on else \"-\"",
        "        lines.append(",
        "            f\"| {task.id} | {task.title} | {task.category} | {deps} | {task.effort} |\"",
        "        )",
        "",
        "    return lines",
        "",
        "",
        "def write_consolidated_file(",
        "    tasks: List[Task],",
        "    output_dir: str,",
        "    session_id: Optional[str] = None",
        ") -> Path:",
        "    \"\"\"Write consolidated tasks to a single JSON file.\"\"\"",
        "    dir_path = Path(output_dir)",
        "    dir_path.mkdir(parents=True, exist_ok=True)",
        "",
        "    sid = session_id or generate_session_id()",
        "    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")",
        "    filename = f\"consolidated_{timestamp}_{sid}.json\"",
        "",
        "    filepath = dir_path / filename",
        "",
        "    data = {",
        "        \"version\": 1,",
        "        \"type\": \"consolidated\",",
        "        \"session_id\": sid,",
        "        \"created_at\": datetime.now().isoformat(),",
        "        \"task_count\": len(tasks),",
        "        \"tasks\": [t.to_dict() for t in tasks]",
        "    }",
        "",
        "    # Atomic write: temp file then rename",
        "    temp_filepath = filepath.with_suffix('.json.tmp')",
        "    try:",
        "        with open(temp_filepath, 'w') as f:",
        "            json.dump(data, f, indent=2)",
        "            f.flush()",
        "            os.fsync(f.fileno())",
        "        temp_filepath.rename(filepath)",
        "    except Exception:",
        "        if temp_filepath.exists():",
        "            temp_filepath.unlink()",
        "        raise",
        "",
        "    return filepath",
        "",
        "",
        "def _validate_archive_path(tasks_dir: Path, archive_path: Path) -> None:",
        "    \"\"\"",
        "    Validate that archive_path is within or under tasks_dir.",
        "",
        "    Raises:",
        "        ValueError: If archive_path escapes tasks_dir boundary",
        "    \"\"\"",
        "    # Resolve to absolute paths for comparison",
        "    tasks_resolved = tasks_dir.resolve()",
        "    archive_resolved = archive_path.resolve()",
        "",
        "    # Check that archive is within tasks directory",
        "    try:",
        "        archive_resolved.relative_to(tasks_resolved)",
        "    except ValueError:",
        "        raise ValueError(",
        "            f\"Archive path '{archive_path}' must be within tasks directory '{tasks_dir}'. \"",
        "            f\"Path traversal is not allowed for security reasons.\"",
        "        )",
        "",
        "",
        "def archive_old_session_files(",
        "    tasks_dir: str,",
        "    archive_dir: Optional[str] = None,",
        "    keep_consolidated: bool = True",
        ") -> List[Path]:",
        "    \"\"\"",
        "    Move old session files to archive after consolidation.",
        "",
        "    Args:",
        "        tasks_dir: Directory containing task files",
        "        archive_dir: Where to move old files (default: tasks/archive/).",
        "                     Must be within tasks_dir for security.",
        "        keep_consolidated: Don't archive consolidated_*.json files",
        "",
        "    Returns:",
        "        List of archived file paths",
        "",
        "    Raises:",
        "        ValueError: If archive_dir attempts path traversal outside tasks_dir",
        "    \"\"\"",
        "    dir_path = Path(tasks_dir)",
        "",
        "    # Default to subdirectory, validate if custom path provided",
        "    if archive_dir is None:",
        "        archive_path = dir_path / \"archive\"",
        "    else:",
        "        archive_path = Path(archive_dir)",
        "        # Security: validate path stays within tasks directory",
        "        _validate_archive_path(dir_path, archive_path)",
        "",
        "    archive_path.mkdir(parents=True, exist_ok=True)",
        "",
        "    archived = []",
        "    for filepath in dir_path.glob(\"*.json\"):",
        "        if keep_consolidated and filepath.name.startswith(\"consolidated_\"):",
        "            continue",
        "",
        "        dest = archive_path / filepath.name",
        "        shutil.move(str(filepath), str(dest))",
        "        archived.append(dest)",
        "",
        "    return archived",
        "",
        "",
        "def print_summary(tasks: List[Task], conflicts: Dict[str, List[Task]]) -> None:",
        "    \"\"\"Print a summary of task state.\"\"\"",
        "    by_status = defaultdict(list)",
        "    for task in tasks:",
        "        by_status[task.status].append(task)",
        "",
        "    print(\"\\n=== Task Summary ===\\n\")",
        "    print(f\"Total tasks: {len(tasks)}\")",
        "    print(f\"  In Progress: {len(by_status['in_progress'])}\")",
        "    print(f\"  Pending:     {len(by_status['pending'])}\")",
        "    print(f\"  Completed:   {len(by_status['completed'])}\")",
        "    print(f\"  Deferred:    {len(by_status['deferred'])}\")",
        "",
        "    if conflicts:",
        "        print(f\"\\nâš ï¸  Found {len(conflicts)} potential duplicate groups:\")",
        "        for title, group in conflicts.items():",
        "            print(f\"  - \\\"{title[:50]}...\\\" ({len(group)} tasks)\")",
        "            for task in group:",
        "                print(f\"      {task.id} [{task.status}]\")",
        "",
        "",
        "def main():",
        "    parser = argparse.ArgumentParser(",
        "        description=\"Consolidate task files from parallel agent sessions\"",
        "    )",
        "    parser.add_argument(",
        "        \"--dir\", default=DEFAULT_TASKS_DIR,",
        "        help=\"Tasks directory (default: tasks/)\"",
        "    )",
        "    parser.add_argument(",
        "        \"--dry-run\", action=\"store_true\",",
        "        help=\"Show what would be done without making changes\"",
        "    )",
        "    parser.add_argument(",
        "        \"--summary\", action=\"store_true\",",
        "        help=\"Show summary only\"",
        "    )",
        "    parser.add_argument(",
        "        \"--update\", action=\"store_true\",",
        "        help=\"Write consolidated file and archive old files\"",
        "    )",
        "    parser.add_argument(",
        "        \"--auto-merge\", action=\"store_true\",",
        "        help=\"Automatically merge duplicate tasks\"",
        "    )",
        "    parser.add_argument(",
        "        \"--output\", help=\"Output file for consolidated JSON\"",
        "    )",
        "    parser.add_argument(",
        "        \"--archive\", action=\"store_true\",",
        "        help=\"Archive old session files after consolidation\"",
        "    )",
        "",
        "    args = parser.parse_args()",
        "",
        "    # Check if tasks directory exists",
        "    if not Path(args.dir).exists():",
        "        print(f\"Tasks directory '{args.dir}' does not exist.\")",
        "        print(\"No tasks to consolidate. Use task_utils.py to create tasks first.\")",
        "        return",
        "",
        "    # Load and analyze tasks",
        "    tasks, conflicts = consolidate_and_dedupe(args.dir, args.auto_merge)",
        "",
        "    if not tasks:",
        "        print(\"No tasks found.\")",
        "        return",
        "",
        "    # Always show summary",
        "    print_summary(tasks, conflicts)",
        "",
        "    if args.summary or args.dry_run:",
        "        if args.dry_run and args.update:",
        "            print(\"\\n[Dry run] Would consolidate to:\")",
        "            print(f\"  {args.dir}/consolidated_TIMESTAMP_XXXX.json\")",
        "            if args.archive:",
        "                print(f\"  Would archive {len(list(Path(args.dir).glob('*.json')))} files\")",
        "        return",
        "",
        "    if args.update:",
        "        # Write consolidated file",
        "        output_path = write_consolidated_file(tasks, args.dir)",
        "        print(f\"\\nâœ… Consolidated to: {output_path}\")",
        "",
        "        if args.archive:",
        "            archived = archive_old_session_files(args.dir)",
        "            print(f\"ðŸ“¦ Archived {len(archived)} session files\")",
        "",
        "    if conflicts and not args.auto_merge:",
        "        print(\"\\nðŸ’¡ Tip: Use --auto-merge to automatically resolve duplicates\")",
        "",
        "",
        "if __name__ == \"__main__\":",
        "    main()"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "scripts/new_task.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "#!/usr/bin/env python3",
        "\"\"\"",
        "Quick task creation from command line.",
        "",
        "Usage:",
        "    # Create a task (interactive prompts)",
        "    python scripts/new_task.py",
        "",
        "    # Create with title only",
        "    python scripts/new_task.py \"Fix the login bug\"",
        "",
        "    # Create with options",
        "    python scripts/new_task.py \"Fix login bug\" --priority high --category bugfix",
        "",
        "    # Show current session tasks",
        "    python scripts/new_task.py --list",
        "",
        "    # Complete a task",
        "    python scripts/new_task.py --complete T-20251213-123456-a1b2-01",
        "",
        "Examples:",
        "    $ python scripts/new_task.py \"Add dark mode\" --priority medium --category feature",
        "    Created: T-20251213-143052-a1b2-01 - Add dark mode",
        "    Saved to: tasks/2025-12-13_14-30-52_a1b2.json",
        "\"\"\"",
        "",
        "import argparse",
        "import json",
        "import os",
        "import sys",
        "from pathlib import Path",
        "",
        "# Add scripts to path",
        "sys.path.insert(0, str(Path(__file__).parent))",
        "",
        "from task_utils import (",
        "    TaskSession,",
        "    Task,",
        "    load_all_tasks,",
        "    consolidate_tasks,",
        "    get_task_by_id,",
        "    DEFAULT_TASKS_DIR,",
        ")",
        "",
        "",
        "# Session file to persist between calls",
        "SESSION_FILE = Path(DEFAULT_TASKS_DIR) / \".current_session.json\"",
        "",
        "",
        "def get_or_create_session() -> TaskSession:",
        "    \"\"\"Get existing session or create new one.\"\"\"",
        "    if SESSION_FILE.exists():",
        "        try:",
        "            with open(SESSION_FILE) as f:",
        "                data = json.load(f)",
        "            session = TaskSession(",
        "                session_id=data[\"session_id\"],",
        "                started_at=data[\"started_at\"]",
        "            )",
        "            session._task_counter = data.get(\"task_counter\", 0)",
        "            # Load existing tasks",
        "            if Path(DEFAULT_TASKS_DIR).exists():",
        "                for filepath in Path(DEFAULT_TASKS_DIR).glob(f\"*_{data['session_id']}.json\"):",
        "                    loaded = TaskSession.load(filepath)",
        "                    session.tasks = loaded.tasks",
        "                    break",
        "            return session",
        "        except (json.JSONDecodeError, KeyError):",
        "            pass",
        "",
        "    # Create new session",
        "    session = TaskSession()",
        "    save_session_state(session)",
        "    return session",
        "",
        "",
        "def save_session_state(session: TaskSession) -> None:",
        "    \"\"\"Save session state for persistence.\"\"\"",
        "    Path(DEFAULT_TASKS_DIR).mkdir(parents=True, exist_ok=True)",
        "    with open(SESSION_FILE, \"w\") as f:",
        "        json.dump({",
        "            \"session_id\": session.session_id,",
        "            \"started_at\": session.started_at,",
        "            \"task_counter\": session._task_counter",
        "        }, f)",
        "",
        "",
        "def create_task(",
        "    title: str,",
        "    priority: str = \"medium\",",
        "    category: str = \"general\",",
        "    description: str = \"\",",
        "    effort: str = \"medium\"",
        ") -> Task:",
        "    \"\"\"Create a task in the current session.\"\"\"",
        "    session = get_or_create_session()",
        "",
        "    task = session.create_task(",
        "        title=title,",
        "        priority=priority,",
        "        category=category,",
        "        description=description,",
        "        effort=effort",
        "    )",
        "",
        "    filepath = session.save()",
        "    save_session_state(session)",
        "",
        "    return task, filepath",
        "",
        "",
        "def list_tasks(status_filter: str = None) -> None:",
        "    \"\"\"List all tasks.\"\"\"",
        "    tasks = load_all_tasks()",
        "",
        "    if status_filter:",
        "        tasks = [t for t in tasks if t.status == status_filter]",
        "",
        "    if not tasks:",
        "        print(\"No tasks found.\")",
        "        return",
        "",
        "    # Group by status",
        "    by_status = {}",
        "    for task in tasks:",
        "        by_status.setdefault(task.status, []).append(task)",
        "",
        "    status_emoji = {",
        "        \"in_progress\": \"ðŸ”„\",",
        "        \"pending\": \"ðŸ“‹\",",
        "        \"completed\": \"âœ…\",",
        "        \"deferred\": \"â¸ï¸\"",
        "    }",
        "",
        "    for status in [\"in_progress\", \"pending\", \"completed\", \"deferred\"]:",
        "        if status not in by_status:",
        "            continue",
        "        print(f\"\\n{status_emoji.get(status, 'â€¢')} {status.upper()}\")",
        "        for task in by_status[status]:",
        "            priority_marker = {\"high\": \"ðŸ”´\", \"medium\": \"ðŸŸ¡\", \"low\": \"ðŸŸ¢\"}.get(task.priority, \"\")",
        "            print(f\"  {priority_marker} {task.id}: {task.title}\")",
        "",
        "",
        "def complete_task(task_id: str) -> bool:",
        "    \"\"\"Mark a task as completed.\"\"\"",
        "    # Find the task",
        "    task = get_task_by_id(task_id)",
        "    if not task:",
        "        print(f\"Task not found: {task_id}\")",
        "        return False",
        "",
        "    # Load the session file containing this task",
        "    for filepath in Path(DEFAULT_TASKS_DIR).glob(\"*.json\"):",
        "        if filepath.name.startswith(\".\"):",
        "            continue",
        "        try:",
        "            session = TaskSession.load(filepath)",
        "            for t in session.tasks:",
        "                if t.id == task_id:",
        "                    t.mark_complete()",
        "                    session.save(DEFAULT_TASKS_DIR)",
        "                    print(f\"âœ… Completed: {task_id} - {t.title}\")",
        "                    return True",
        "        except:",
        "            continue",
        "",
        "    print(f\"Could not update task: {task_id}\")",
        "    return False",
        "",
        "",
        "def new_session() -> None:",
        "    \"\"\"Start a new session.\"\"\"",
        "    if SESSION_FILE.exists():",
        "        SESSION_FILE.unlink()",
        "    session = get_or_create_session()",
        "    print(f\"Started new session: {session.session_id}\")",
        "",
        "",
        "def main():",
        "    parser = argparse.ArgumentParser(",
        "        description=\"Quick task creation for parallel agent workflows\",",
        "        formatter_class=argparse.RawDescriptionHelpFormatter,",
        "        epilog=__doc__",
        "    )",
        "",
        "    parser.add_argument(\"title\", nargs=\"?\", help=\"Task title\")",
        "    parser.add_argument(\"-p\", \"--priority\", choices=[\"high\", \"medium\", \"low\"],",
        "                        default=\"medium\", help=\"Task priority\")",
        "    parser.add_argument(\"-c\", \"--category\", default=\"general\", help=\"Task category\")",
        "    parser.add_argument(\"-d\", \"--description\", default=\"\", help=\"Task description\")",
        "    parser.add_argument(\"-e\", \"--effort\", choices=[\"small\", \"medium\", \"large\"],",
        "                        default=\"medium\", help=\"Effort estimate\")",
        "    parser.add_argument(\"-l\", \"--list\", action=\"store_true\", help=\"List all tasks\")",
        "    parser.add_argument(\"-s\", \"--status\", help=\"Filter by status when listing\")",
        "    parser.add_argument(\"--complete\", metavar=\"TASK_ID\", help=\"Mark task as completed\")",
        "    parser.add_argument(\"--new-session\", action=\"store_true\", help=\"Start a new session\")",
        "    parser.add_argument(\"--summary\", action=\"store_true\", help=\"Show task summary\")",
        "",
        "    args = parser.parse_args()",
        "",
        "    # Ensure tasks directory exists",
        "    Path(DEFAULT_TASKS_DIR).mkdir(parents=True, exist_ok=True)",
        "",
        "    if args.new_session:",
        "        new_session()",
        "    elif args.list:",
        "        list_tasks(args.status)",
        "    elif args.complete:",
        "        complete_task(args.complete)",
        "    elif args.summary:",
        "        grouped = consolidate_tasks()",
        "        print(\"\\n=== Task Summary ===\")",
        "        for status, tasks in grouped.items():",
        "            if tasks:",
        "                print(f\"{status}: {len(tasks)}\")",
        "    elif args.title:",
        "        task, filepath = create_task(",
        "            title=args.title,",
        "            priority=args.priority,",
        "            category=args.category,",
        "            description=args.description,",
        "            effort=args.effort",
        "        )",
        "        print(f\"Created: {task.id} - {task.title}\")",
        "        print(f\"Saved to: {filepath}\")",
        "    else:",
        "        # Interactive mode",
        "        print(\"Create a new task (Ctrl+C to cancel)\")",
        "        title = input(\"Title: \").strip()",
        "        if not title:",
        "            print(\"Title is required\")",
        "            return",
        "",
        "        priority = input(\"Priority [high/medium/low] (medium): \").strip() or \"medium\"",
        "        category = input(\"Category (general): \").strip() or \"general\"",
        "        description = input(\"Description (optional): \").strip()",
        "",
        "        task, filepath = create_task(",
        "            title=title,",
        "            priority=priority,",
        "            category=category,",
        "            description=description",
        "        )",
        "        print(f\"\\nCreated: {task.id} - {task.title}\")",
        "        print(f\"Saved to: {filepath}\")",
        "",
        "",
        "if __name__ == \"__main__\":",
        "    main()"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "scripts/task_utils.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "#!/usr/bin/env python3",
        "\"\"\"",
        "Merge-friendly task ID management utilities.",
        "",
        "This module provides utilities for generating unique task IDs that won't",
        "conflict when multiple agents work in parallel. Follows the same pattern",
        "as cortical/chunk_index.py for append-only, git-friendly storage.",
        "",
        "Task ID Format:",
        "    Standalone: T-YYYYMMDD-HHMMSS-XXXX",
        "    Session:    T-YYYYMMDD-HHMMSS-XXXX-NN",
        "",
        "    Where:",
        "    - T = Task prefix",
        "    - YYYYMMDD = Date created",
        "    - HHMMSS = Time created",
        "    - XXXX = 4-char random suffix (from session UUID)",
        "    - NN = Task number within session (01, 02, etc.)",
        "",
        "Example:",
        "    T-20251213-143052-a1b2       # Standalone",
        "    T-20251213-143052-a1b2-01    # Session task 1",
        "    T-20251213-143052-a1b2-02    # Session task 2",
        "",
        "Usage:",
        "    from scripts.task_utils import generate_task_id, TaskSession",
        "",
        "    # Simple ID generation (standalone)",
        "    task_id = generate_task_id()  # T-20251213-143052-a1b2",
        "",
        "    # Session-based (guaranteed unique within session)",
        "    session = TaskSession()",
        "    task1 = session.new_task_id()  # T-20251213-143052-a1b2-01",
        "    task2 = session.new_task_id()  # T-20251213-143052-a1b2-02",
        "\"\"\"",
        "",
        "import json",
        "import os",
        "import uuid",
        "from dataclasses import dataclass, field, asdict",
        "from datetime import datetime",
        "from pathlib import Path",
        "from typing import Dict, List, Optional, Any",
        "",
        "",
        "# Directory for per-session task files",
        "DEFAULT_TASKS_DIR = \"tasks\"",
        "",
        "",
        "def generate_session_id() -> str:",
        "    \"\"\"Generate a short session ID (4 hex chars).\"\"\"",
        "    return uuid.uuid4().hex[:4]",
        "",
        "",
        "def generate_task_id(session_id: Optional[str] = None) -> str:",
        "    \"\"\"",
        "    Generate a unique, merge-friendly task ID.",
        "",
        "    Args:",
        "        session_id: Optional session suffix. If None, generates random suffix.",
        "",
        "    Returns:",
        "        Task ID in format T-YYYYMMDD-HHMMSS-XXXX",
        "",
        "    Example:",
        "        >>> generate_task_id()",
        "        'T-20251213-143052-a1b2'",
        "        >>> generate_task_id(\"test\")",
        "        'T-20251213-143052-test'",
        "    \"\"\"",
        "    now = datetime.now()",
        "    date_str = now.strftime(\"%Y%m%d\")",
        "    time_str = now.strftime(\"%H%M%S\")",
        "    suffix = session_id or generate_session_id()",
        "    return f\"T-{date_str}-{time_str}-{suffix}\"",
        "",
        "",
        "def generate_short_task_id() -> str:",
        "    \"\"\"",
        "    Generate a shorter unique task ID (8 hex chars).",
        "",
        "    Returns:",
        "        Task ID in format T-XXXXXXXX",
        "",
        "    Example:",
        "        >>> generate_short_task_id()",
        "        'T-a1b2c3d4'",
        "    \"\"\"",
        "    return f\"T-{uuid.uuid4().hex[:8]}\"",
        "",
        "",
        "@dataclass",
        "class Task:",
        "    \"\"\"A single task with merge-friendly ID.\"\"\"",
        "    id: str",
        "    title: str",
        "    status: str = \"pending\"  # pending, in_progress, completed, deferred",
        "    priority: str = \"medium\"  # high, medium, low",
        "    category: str = \"general\"",
        "    description: str = \"\"",
        "    depends_on: List[str] = field(default_factory=list)",
        "    effort: str = \"medium\"  # small, medium, large",
        "    created_at: str = field(default_factory=lambda: datetime.now().isoformat())",
        "    updated_at: Optional[str] = None",
        "    completed_at: Optional[str] = None",
        "    context: Dict[str, Any] = field(default_factory=dict)",
        "",
        "    def to_dict(self) -> Dict[str, Any]:",
        "        \"\"\"Convert to dictionary for serialization.\"\"\"",
        "        return asdict(self)",
        "",
        "    @classmethod",
        "    def from_dict(cls, d: Dict[str, Any]) -> 'Task':",
        "        \"\"\"Create Task from dictionary.\"\"\"",
        "        return cls(**d)",
        "",
        "    def mark_complete(self) -> None:",
        "        \"\"\"Mark task as completed.\"\"\"",
        "        self.status = \"completed\"",
        "        self.completed_at = datetime.now().isoformat()",
        "        self.updated_at = self.completed_at",
        "",
        "    def mark_in_progress(self) -> None:",
        "        \"\"\"Mark task as in progress.\"\"\"",
        "        self.status = \"in_progress\"",
        "        self.updated_at = datetime.now().isoformat()",
        "",
        "",
        "@dataclass",
        "class TaskSession:",
        "    \"\"\"",
        "    A session for creating tasks with consistent session suffix.",
        "",
        "    All tasks created in a session share the same suffix, making it",
        "    easy to identify which tasks were created together.",
        "",
        "    Example:",
        "        session = TaskSession()",
        "        task1 = session.create_task(\"Implement feature X\")",
        "        task2 = session.create_task(\"Add tests for feature X\")",
        "        session.save()  # Writes to tasks/2025-12-13_14-30-52_a1b2.json",
        "    \"\"\"",
        "    session_id: str = field(default_factory=generate_session_id)",
        "    tasks: List[Task] = field(default_factory=list)",
        "    started_at: str = field(default_factory=lambda: datetime.now().isoformat())",
        "    tasks_dir: str = DEFAULT_TASKS_DIR",
        "    _task_counter: int = field(default=0, repr=False)",
        "",
        "    def new_task_id(self) -> str:",
        "        \"\"\"Generate a new task ID with this session's suffix and counter.",
        "",
        "        The counter ensures unique IDs even when multiple tasks are created",
        "        within the same second.",
        "        \"\"\"",
        "        self._task_counter += 1",
        "        now = datetime.now()",
        "        date_str = now.strftime(\"%Y%m%d\")",
        "        time_str = now.strftime(\"%H%M%S\")",
        "        # Format: T-YYYYMMDD-HHMMSS-SSSS-NNN where NNN is task number (supports 999 tasks)",
        "        return f\"T-{date_str}-{time_str}-{self.session_id}-{self._task_counter:03d}\"",
        "",
        "    def create_task(",
        "        self,",
        "        title: str,",
        "        priority: str = \"medium\",",
        "        category: str = \"general\",",
        "        description: str = \"\",",
        "        depends_on: Optional[List[str]] = None,",
        "        effort: str = \"medium\",",
        "        context: Optional[Dict[str, Any]] = None",
        "    ) -> Task:",
        "        \"\"\"",
        "        Create a new task in this session.",
        "",
        "        Args:",
        "            title: Task title/summary",
        "            priority: high, medium, low",
        "            category: Task category (arch, devex, codequal, etc.)",
        "            description: Detailed description",
        "            depends_on: List of task IDs this depends on",
        "            effort: small, medium, large",
        "            context: Quick context dict (files, methods, etc.)",
        "",
        "        Returns:",
        "            The created Task object",
        "        \"\"\"",
        "        task = Task(",
        "            id=self.new_task_id(),",
        "            title=title,",
        "            priority=priority,",
        "            category=category,",
        "            description=description,",
        "            depends_on=depends_on or [],",
        "            effort=effort,",
        "            context=context or {}",
        "        )",
        "        self.tasks.append(task)",
        "        return task",
        "",
        "    def get_filename(self) -> str:",
        "        \"\"\"Get the session filename.\"\"\"",
        "        dt = datetime.fromisoformat(self.started_at)",
        "        timestamp = dt.strftime(\"%Y-%m-%d_%H-%M-%S\")",
        "        return f\"{timestamp}_{self.session_id}.json\"",
        "",
        "    def save(self, tasks_dir: Optional[str] = None) -> Path:",
        "        \"\"\"",
        "        Save session tasks to a JSON file atomically.",
        "",
        "        Uses write-to-temp-then-rename pattern to prevent data loss",
        "        if the process crashes during write.",
        "",
        "        Args:",
        "            tasks_dir: Directory for task files (default: tasks/)",
        "",
        "        Returns:",
        "            Path to the saved file",
        "",
        "        Raises:",
        "            OSError: If write or rename fails",
        "        \"\"\"",
        "        dir_path = Path(tasks_dir or self.tasks_dir)",
        "        dir_path.mkdir(parents=True, exist_ok=True)",
        "",
        "        filepath = dir_path / self.get_filename()",
        "        temp_filepath = filepath.with_suffix('.json.tmp')",
        "",
        "        data = {",
        "            \"version\": 1,",
        "            \"session_id\": self.session_id,",
        "            \"started_at\": self.started_at,",
        "            \"saved_at\": datetime.now().isoformat(),",
        "            \"tasks\": [t.to_dict() for t in self.tasks]",
        "        }",
        "",
        "        try:",
        "            # Write to temp file first",
        "            with open(temp_filepath, 'w') as f:",
        "                json.dump(data, f, indent=2)",
        "                f.flush()",
        "                os.fsync(f.fileno())  # Ensure data is on disk",
        "",
        "            # Atomic rename (on POSIX systems)",
        "            temp_filepath.rename(filepath)",
        "        except Exception:",
        "            # Clean up temp file on failure",
        "            if temp_filepath.exists():",
        "                temp_filepath.unlink()",
        "            raise",
        "",
        "        return filepath",
        "",
        "    @classmethod",
        "    def load(cls, filepath: Path) -> 'TaskSession':",
        "        \"\"\"Load a session from file.\"\"\"",
        "        with open(filepath) as f:",
        "            data = json.load(f)",
        "",
        "        session = cls(",
        "            session_id=data['session_id'],",
        "            started_at=data['started_at']",
        "        )",
        "        session.tasks = [Task.from_dict(t) for t in data['tasks']]",
        "        return session",
        "",
        "",
        "def load_all_tasks(tasks_dir: str = DEFAULT_TASKS_DIR) -> List[Task]:",
        "    \"\"\"",
        "    Load all tasks from all session files.",
        "",
        "    Args:",
        "        tasks_dir: Directory containing task session files",
        "",
        "    Returns:",
        "        List of all tasks, sorted by creation time",
        "    \"\"\"",
        "    dir_path = Path(tasks_dir)",
        "    if not dir_path.exists():",
        "        return []",
        "",
        "    all_tasks = []",
        "    for filepath in sorted(dir_path.glob(\"*.json\")):",
        "        try:",
        "            session = TaskSession.load(filepath)",
        "            all_tasks.extend(session.tasks)",
        "        except (json.JSONDecodeError, KeyError) as e:",
        "            print(f\"Warning: Could not load {filepath}: {e}\")",
        "",
        "    # Sort by creation time",
        "    all_tasks.sort(key=lambda t: t.created_at)",
        "    return all_tasks",
        "",
        "",
        "def get_task_by_id(task_id: str, tasks_dir: str = DEFAULT_TASKS_DIR) -> Optional[Task]:",
        "    \"\"\"Find a task by its ID across all session files.\"\"\"",
        "    for task in load_all_tasks(tasks_dir):",
        "        if task.id == task_id:",
        "            return task",
        "    return None",
        "",
        "",
        "def consolidate_tasks(",
        "    tasks_dir: str = DEFAULT_TASKS_DIR,",
        "    output_file: Optional[str] = None",
        ") -> Dict[str, List[Task]]:",
        "    \"\"\"",
        "    Consolidate all tasks from session files into a summary.",
        "",
        "    Args:",
        "        tasks_dir: Directory containing task session files",
        "        output_file: Optional path to write consolidated markdown",
        "",
        "    Returns:",
        "        Dict of tasks grouped by status",
        "    \"\"\"",
        "    all_tasks = load_all_tasks(tasks_dir)",
        "",
        "    # Group by status",
        "    grouped = {",
        "        \"in_progress\": [],",
        "        \"pending\": [],",
        "        \"completed\": [],",
        "        \"deferred\": []",
        "    }",
        "",
        "    for task in all_tasks:",
        "        status = task.status if task.status in grouped else \"pending\"",
        "        grouped[status].append(task)",
        "",
        "    # Sort within groups by priority",
        "    priority_order = {\"high\": 0, \"medium\": 1, \"low\": 2}",
        "    for status in grouped:",
        "        grouped[status].sort(key=lambda t: priority_order.get(t.priority, 1))",
        "",
        "    if output_file:",
        "        _write_consolidated_markdown(grouped, output_file)",
        "",
        "    return grouped",
        "",
        "",
        "def _write_consolidated_markdown(",
        "    grouped: Dict[str, List[Task]],",
        "    output_file: str",
        ") -> None:",
        "    \"\"\"Write consolidated tasks to markdown file.\"\"\"",
        "    lines = [",
        "        \"# Consolidated Task List\",",
        "        \"\",",
        "        f\"**Generated:** {datetime.now().isoformat()}\",",
        "        \"\",",
        "        \"---\",",
        "        \"\"",
        "    ]",
        "",
        "    status_headers = {",
        "        \"in_progress\": \"## ðŸ”„ In Progress\",",
        "        \"pending\": \"## ðŸ“‹ Pending\",",
        "        \"completed\": \"## âœ… Completed\",",
        "        \"deferred\": \"## â¸ï¸ Deferred\"",
        "    }",
        "",
        "    for status, tasks in grouped.items():",
        "        if not tasks:",
        "            continue",
        "",
        "        lines.append(status_headers.get(status, f\"## {status.title()}\"))",
        "        lines.append(\"\")",
        "        lines.append(\"| ID | Title | Priority | Category | Effort |\")",
        "        lines.append(\"|---|------|----------|----------|--------|\")",
        "",
        "        for task in tasks:",
        "            lines.append(",
        "                f\"| {task.id} | {task.title} | {task.priority} | \"",
        "                f\"{task.category} | {task.effort} |\"",
        "            )",
        "        lines.append(\"\")",
        "",
        "    with open(output_file, 'w') as f:",
        "        f.write('\\n'.join(lines))",
        "",
        "",
        "# CLI interface",
        "if __name__ == \"__main__\":",
        "    import argparse",
        "",
        "    parser = argparse.ArgumentParser(",
        "        description=\"Merge-friendly task management utilities\"",
        "    )",
        "    subparsers = parser.add_subparsers(dest=\"command\", help=\"Commands\")",
        "",
        "    # generate command",
        "    gen_parser = subparsers.add_parser(\"generate\", help=\"Generate a task ID\")",
        "    gen_parser.add_argument(\"--short\", action=\"store_true\", help=\"Short format (T-XXXXXXXX)\")",
        "",
        "    # consolidate command",
        "    cons_parser = subparsers.add_parser(\"consolidate\", help=\"Consolidate task files\")",
        "    cons_parser.add_argument(\"--dir\", default=DEFAULT_TASKS_DIR, help=\"Tasks directory\")",
        "    cons_parser.add_argument(\"--output\", help=\"Output markdown file\")",
        "",
        "    # list command",
        "    list_parser = subparsers.add_parser(\"list\", help=\"List all tasks\")",
        "    list_parser.add_argument(\"--dir\", default=DEFAULT_TASKS_DIR, help=\"Tasks directory\")",
        "    list_parser.add_argument(\"--status\", help=\"Filter by status\")",
        "",
        "    args = parser.parse_args()",
        "",
        "    if args.command == \"generate\":",
        "        if args.short:",
        "            print(generate_short_task_id())",
        "        else:",
        "            print(generate_task_id())",
        "",
        "    elif args.command == \"consolidate\":",
        "        grouped = consolidate_tasks(args.dir, args.output)",
        "        for status, tasks in grouped.items():",
        "            print(f\"{status}: {len(tasks)} tasks\")",
        "        if args.output:",
        "            print(f\"\\nWritten to {args.output}\")",
        "",
        "    elif args.command == \"list\":",
        "        tasks = load_all_tasks(args.dir)",
        "        if args.status:",
        "            tasks = [t for t in tasks if t.status == args.status]",
        "",
        "        for task in tasks:",
        "            print(f\"[{task.status}] {task.id}: {task.title}\")",
        "",
        "    else:",
        "        parser.print_help()"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "scripts/workflow.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "#!/usr/bin/env python3",
        "\"\"\"",
        "Workflow template engine for task creation.",
        "",
        "Spawns multiple linked tasks from YAML workflow templates.",
        "",
        "Usage:",
        "    # List available workflows",
        "    python scripts/workflow.py list",
        "",
        "    # Run a workflow",
        "    python scripts/workflow.py run bugfix --bug_title \"Login crashes on special chars\"",
        "",
        "    # Run with all options",
        "    python scripts/workflow.py run feature \\\\",
        "        --feature_name \"Dark mode\" \\\\",
        "        --priority high \\\\",
        "        --effort large",
        "",
        "    # Dry run (show tasks without creating)",
        "    python scripts/workflow.py run bugfix --bug_title \"Test\" --dry-run",
        "\"\"\"",
        "",
        "import argparse",
        "import sys",
        "from dataclasses import dataclass",
        "from pathlib import Path",
        "from typing import Any, Dict, List, Optional",
        "",
        "# Try to import yaml, fall back to basic parsing if not available",
        "try:",
        "    import yaml",
        "    HAS_YAML = True",
        "except ImportError:",
        "    HAS_YAML = False",
        "",
        "# Add scripts to path",
        "sys.path.insert(0, str(Path(__file__).parent))",
        "",
        "from task_utils import TaskSession, Task",
        "",
        "",
        "# Default workflows directory",
        "WORKFLOWS_DIR = Path(__file__).parent.parent / \".claude\" / \"workflows\"",
        "",
        "",
        "@dataclass",
        "class WorkflowVariable:",
        "    \"\"\"A variable in a workflow template.\"\"\"",
        "    name: str",
        "    description: str",
        "    required: bool = True",
        "    default: Optional[str] = None",
        "    choices: Optional[List[str]] = None",
        "",
        "",
        "@dataclass",
        "class WorkflowTask:",
        "    \"\"\"A task template in a workflow.\"\"\"",
        "    id: str",
        "    title: str",
        "    category: str = \"general\"",
        "    priority: str = \"medium\"",
        "    effort: str = \"medium\"",
        "    description: str = \"\"",
        "    depends_on: List[str] = None",
        "",
        "    def __post_init__(self):",
        "        if self.depends_on is None:",
        "            self.depends_on = []",
        "",
        "",
        "@dataclass",
        "class Workflow:",
        "    \"\"\"A workflow template that creates multiple tasks.\"\"\"",
        "    name: str",
        "    description: str",
        "    category: str",
        "    variables: List[WorkflowVariable]",
        "    tasks: List[WorkflowTask]",
        "",
        "    @classmethod",
        "    def from_dict(cls, data: Dict[str, Any]) -> 'Workflow':",
        "        \"\"\"Create workflow from parsed YAML dict.\"\"\"",
        "        variables = [",
        "            WorkflowVariable(",
        "                name=v['name'],",
        "                description=v.get('description', ''),",
        "                required=v.get('required', True),",
        "                default=v.get('default'),",
        "                choices=v.get('choices')",
        "            )",
        "            for v in data.get('variables', [])",
        "        ]",
        "",
        "        tasks = [",
        "            WorkflowTask(",
        "                id=t['id'],",
        "                title=t['title'],",
        "                category=t.get('category', 'general'),",
        "                priority=t.get('priority', 'medium'),",
        "                effort=t.get('effort', 'medium'),",
        "                description=t.get('description', ''),",
        "                depends_on=t.get('depends_on', [])",
        "            )",
        "            for t in data.get('tasks', [])",
        "        ]",
        "",
        "        return cls(",
        "            name=data['name'],",
        "            description=data.get('description', ''),",
        "            category=data.get('category', 'general'),",
        "            variables=variables,",
        "            tasks=tasks",
        "        )",
        "",
        "    @classmethod",
        "    def load(cls, filepath: Path) -> 'Workflow':",
        "        \"\"\"Load workflow from YAML file.\"\"\"",
        "        if not HAS_YAML:",
        "            raise ImportError(",
        "                \"PyYAML is required for workflow templates. \"",
        "                \"Install with: pip install pyyaml\"",
        "            )",
        "",
        "        with open(filepath) as f:",
        "            data = yaml.safe_load(f)",
        "",
        "        return cls.from_dict(data)",
        "",
        "",
        "def list_workflows(workflows_dir: Path = WORKFLOWS_DIR) -> List[Workflow]:",
        "    \"\"\"List all available workflow templates.\"\"\"",
        "    workflows = []",
        "",
        "    if not workflows_dir.exists():",
        "        return workflows",
        "",
        "    for filepath in sorted(workflows_dir.glob(\"*.yaml\")):",
        "        try:",
        "            workflow = Workflow.load(filepath)",
        "            workflows.append(workflow)",
        "        except Exception as e:",
        "            print(f\"Warning: Could not load {filepath}: {e}\")",
        "",
        "    return workflows",
        "",
        "",
        "def substitute_variables(text: str, variables: Dict[str, str]) -> str:",
        "    \"\"\"Substitute {variable} placeholders in text.\"\"\"",
        "    result = text",
        "    for name, value in variables.items():",
        "        result = result.replace(f\"{{{name}}}\", value)",
        "    return result",
        "",
        "",
        "def run_workflow(",
        "    workflow: Workflow,",
        "    variables: Dict[str, str],",
        "    tasks_dir: str = \"tasks\",",
        "    dry_run: bool = False",
        ") -> List[Task]:",
        "    \"\"\"",
        "    Execute a workflow template, creating all tasks.",
        "",
        "    Args:",
        "        workflow: The workflow to execute",
        "        variables: Variable values to substitute",
        "        tasks_dir: Directory to save tasks",
        "        dry_run: If True, show tasks but don't create",
        "",
        "    Returns:",
        "        List of created Task objects",
        "    \"\"\"",
        "    # Validate required variables",
        "    for var in workflow.variables:",
        "        if var.required and var.name not in variables:",
        "            if var.default:",
        "                variables[var.name] = var.default",
        "            else:",
        "                raise ValueError(f\"Missing required variable: {var.name}\")",
        "",
        "        # Validate choices",
        "        if var.choices and var.name in variables:",
        "            if variables[var.name] not in var.choices:",
        "                raise ValueError(",
        "                    f\"Invalid value for {var.name}: {variables[var.name]}. \"",
        "                    f\"Must be one of: {var.choices}\"",
        "                )",
        "",
        "    # Create session",
        "    session = TaskSession()",
        "",
        "    # Map workflow task IDs to actual task IDs",
        "    id_mapping: Dict[str, str] = {}",
        "",
        "    # Create tasks in order",
        "    created_tasks = []",
        "    for wf_task in workflow.tasks:",
        "        # Substitute variables",
        "        title = substitute_variables(wf_task.title, variables)",
        "        description = substitute_variables(wf_task.description, variables)",
        "        priority = substitute_variables(wf_task.priority, variables)",
        "        effort = substitute_variables(wf_task.effort, variables)",
        "",
        "        # Resolve dependencies to actual task IDs",
        "        depends_on = [",
        "            id_mapping[dep_id]",
        "            for dep_id in wf_task.depends_on",
        "            if dep_id in id_mapping",
        "        ]",
        "",
        "        # Create task",
        "        task = session.create_task(",
        "            title=title,",
        "            category=wf_task.category,",
        "            priority=priority,",
        "            effort=effort,",
        "            description=description,",
        "            depends_on=depends_on",
        "        )",
        "",
        "        id_mapping[wf_task.id] = task.id",
        "        created_tasks.append(task)",
        "",
        "    if dry_run:",
        "        print(f\"\\n[Dry Run] Would create {len(created_tasks)} tasks:\\n\")",
        "        for task in created_tasks:",
        "            deps = f\" (depends on: {len(task.depends_on)})\" if task.depends_on else \"\"",
        "            print(f\"  [{task.priority.upper()}] {task.title}{deps}\")",
        "            if task.description:",
        "                # Show first line of description",
        "                first_line = task.description.strip().split('\\n')[0]",
        "                print(f\"           {first_line[:60]}...\")",
        "        return created_tasks",
        "",
        "    # Save",
        "    filepath = session.save(tasks_dir)",
        "    print(f\"\\nCreated {len(created_tasks)} tasks from '{workflow.name}' workflow\")",
        "    print(f\"Saved to: {filepath}\\n\")",
        "",
        "    for task in created_tasks:",
        "        deps = f\" (depends on: {len(task.depends_on)})\" if task.depends_on else \"\"",
        "        print(f\"  {task.id}: {task.title}{deps}\")",
        "",
        "    return created_tasks",
        "",
        "",
        "def main():",
        "    parser = argparse.ArgumentParser(",
        "        description=\"Workflow template engine for task creation\",",
        "        formatter_class=argparse.RawDescriptionHelpFormatter,",
        "        epilog=__doc__",
        "    )",
        "",
        "    subparsers = parser.add_subparsers(dest=\"command\", help=\"Commands\")",
        "",
        "    # List command",
        "    list_parser = subparsers.add_parser(\"list\", help=\"List available workflows\")",
        "    list_parser.add_argument(",
        "        \"--dir\", default=str(WORKFLOWS_DIR),",
        "        help=\"Workflows directory\"",
        "    )",
        "",
        "    # Run command",
        "    run_parser = subparsers.add_parser(\"run\", help=\"Run a workflow\")",
        "    run_parser.add_argument(\"workflow\", help=\"Workflow name (e.g., 'bugfix', 'feature')\")",
        "    run_parser.add_argument(\"--dry-run\", action=\"store_true\", help=\"Show tasks without creating\")",
        "    run_parser.add_argument(\"--tasks-dir\", default=\"tasks\", help=\"Tasks directory\")",
        "",
        "    # Parse known args first to get workflow name",
        "    args, remaining = parser.parse_known_args()",
        "",
        "    if args.command == \"list\":",
        "        workflows = list_workflows(Path(args.dir))",
        "        if not workflows:",
        "            print(\"No workflows found.\")",
        "            print(f\"Add .yaml files to: {args.dir}\")",
        "            return",
        "",
        "        print(\"\\nAvailable Workflows:\\n\")",
        "        for wf in workflows:",
        "            print(f\"  {wf.name.lower().replace(' ', '_')}\")",
        "            print(f\"    {wf.description}\")",
        "            print(f\"    Tasks: {len(wf.tasks)}\")",
        "            if wf.variables:",
        "                var_names = [v.name for v in wf.variables]",
        "                print(f\"    Variables: {', '.join(var_names)}\")",
        "            print()",
        "",
        "    elif args.command == \"run\":",
        "        # Find workflow file",
        "        workflow_name = args.workflow.lower().replace(' ', '_')",
        "        workflow_path = WORKFLOWS_DIR / f\"{workflow_name}.yaml\"",
        "",
        "        if not workflow_path.exists():",
        "            print(f\"Workflow not found: {workflow_name}\")",
        "            print(f\"Available: {', '.join(p.stem for p in WORKFLOWS_DIR.glob('*.yaml'))}\")",
        "            return",
        "",
        "        workflow = Workflow.load(workflow_path)",
        "",
        "        # Add workflow-specific arguments dynamically",
        "        for var in workflow.variables:",
        "            arg_name = f\"--{var.name}\"",
        "            help_text = var.description",
        "            if var.default:",
        "                help_text += f\" (default: {var.default})\"",
        "            if var.choices:",
        "                help_text += f\" (choices: {', '.join(var.choices)})\"",
        "",
        "            run_parser.add_argument(",
        "                arg_name,",
        "                default=var.default,",
        "                required=var.required and not var.default,",
        "                help=help_text",
        "            )",
        "",
        "        # Re-parse with dynamic arguments",
        "        args = parser.parse_args()",
        "",
        "        # Collect variables",
        "        variables = {}",
        "        for var in workflow.variables:",
        "            value = getattr(args, var.name, None)",
        "            if value:",
        "                variables[var.name] = value",
        "",
        "        # Run workflow",
        "        try:",
        "            run_workflow(",
        "                workflow,",
        "                variables,",
        "                tasks_dir=args.tasks_dir,",
        "                dry_run=args.dry_run",
        "            )",
        "        except ValueError as e:",
        "            print(f\"Error: {e}\")",
        "            return",
        "",
        "    else:",
        "        parser.print_help()",
        "",
        "",
        "if __name__ == \"__main__\":",
        "    main()"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "tasks/2025-12-13_22-32-34_e233.json",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "{",
        "  \"version\": 1,",
        "  \"session_id\": \"e233\",",
        "  \"started_at\": \"2025-12-13T22:32:34.072421\",",
        "  \"saved_at\": \"2025-12-13T22:33:26.107227\",",
        "  \"tasks\": [",
        "    {",
        "      \"id\": \"T-20251213-223234-e233-01\",",
        "      \"title\": \"Document dog-fooding workflow\",",
        "      \"status\": \"pending\",",
        "      \"priority\": \"high\",",
        "      \"category\": \"docs\",",
        "      \"description\": \"Create a practical guide for using the task system in daily work\",",
        "      \"depends_on\": [],",
        "      \"effort\": \"small\",",
        "      \"created_at\": \"2025-12-13T22:32:34.072535\",",
        "      \"updated_at\": null,",
        "      \"completed_at\": null,",
        "      \"context\": {",
        "        \"files\": [",
        "          \"docs/merge-friendly-tasks.md\"",
        "        ]",
        "      }",
        "    },",
        "    {",
        "      \"id\": \"T-20251213-223234-e233-02\",",
        "      \"title\": \"Add convenience script for quick task creation\",",
        "      \"status\": \"completed\",",
        "      \"priority\": \"medium\",",
        "      \"category\": \"devex\",",
        "      \"description\": \"One-liner to create task from command line\",",
        "      \"depends_on\": [",
        "        \"T-20251213-223234-e233-01\"",
        "      ],",
        "      \"effort\": \"small\",",
        "      \"created_at\": \"2025-12-13T22:32:34.072592\",",
        "      \"updated_at\": \"2025-12-13T22:33:26.107132\",",
        "      \"completed_at\": \"2025-12-13T22:33:26.107132\",",
        "      \"context\": {}",
        "    },",
        "    {",
        "      \"id\": \"T-20251213-223234-e233-03\",",
        "      \"title\": \"Test task recovery from crash scenario\",",
        "      \"status\": \"pending\",",
        "      \"priority\": \"medium\",",
        "      \"category\": \"test\",",
        "      \"description\": \"Verify tasks persist correctly if agent crashes mid-work\",",
        "      \"depends_on\": [],",
        "      \"effort\": \"medium\",",
        "      \"created_at\": \"2025-12-13T22:32:34.072623\",",
        "      \"updated_at\": null,",
        "      \"completed_at\": null,",
        "      \"context\": {}",
        "    }",
        "  ]",
        "}"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "tasks/2025-12-13_22-33-34_2d89.json",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "{",
        "  \"version\": 1,",
        "  \"session_id\": \"2d89\",",
        "  \"started_at\": \"2025-12-13T22:33:34.431014\",",
        "  \"saved_at\": \"2025-12-13T23:25:22.253602\",",
        "  \"tasks\": [",
        "    {",
        "      \"id\": \"T-20251213-223334-2d89-01\",",
        "      \"title\": \"Investigate performance bottleneck in search\",",
        "      \"status\": \"pending\",",
        "      \"priority\": \"high\",",
        "      \"category\": \"perf\",",
        "      \"description\": \"\",",
        "      \"depends_on\": [],",
        "      \"effort\": \"medium\",",
        "      \"created_at\": \"2025-12-13T22:33:34.431621\",",
        "      \"updated_at\": null,",
        "      \"completed_at\": null,",
        "      \"context\": {}",
        "    },",
        "    {",
        "      \"id\": \"T-20251213-232522-2d89-002\",",
        "      \"title\": \"Update CI to output pending tasks intelligently\",",
        "      \"status\": \"pending\",",
        "      \"priority\": \"medium\",",
        "      \"category\": \"automation\",",
        "      \"description\": \"Add a CI step that shows pending tasks in a smart way: grouped by priority, showing blockers first, with context about what's ready to work on next. Could integrate with the workflow system to show task chains and dependencies.\",",
        "      \"depends_on\": [],",
        "      \"effort\": \"medium\",",
        "      \"created_at\": \"2025-12-13T23:25:22.253447\",",
        "      \"updated_at\": null,",
        "      \"completed_at\": null,",
        "      \"context\": {}",
        "    }",
        "  ]",
        "}"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "tasks/2025-12-13_22-42-20_6ac7.json",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "{",
        "  \"version\": 1,",
        "  \"session_id\": \"6ac7\",",
        "  \"started_at\": \"2025-12-13T22:42:20.986896\",",
        "  \"saved_at\": \"2025-12-13T22:50:36.977690\",",
        "  \"tasks\": [",
        "    {",
        "      \"id\": \"T-20251213-224220-6ac7-01\",",
        "      \"title\": \"Fix non-atomic file writes (data loss risk)\",",
        "      \"status\": \"completed\",",
        "      \"priority\": \"high\",",
        "      \"category\": \"bugfix\",",
        "      \"description\": \"TaskSession.save() should write to .tmp then atomic rename\",",
        "      \"depends_on\": [],",
        "      \"effort\": \"small\",",
        "      \"created_at\": \"2025-12-13T22:42:20.986937\",",
        "      \"updated_at\": \"2025-12-13T22:48:20.519513\",",
        "      \"completed_at\": \"2025-12-13T22:48:20.519513\",",
        "      \"context\": {",
        "        \"files\": [",
        "          \"scripts/task_utils.py\"",
        "        ],",
        "        \"line\": 229",
        "      }",
        "    },",
        "    {",
        "      \"id\": \"T-20251213-224220-6ac7-02\",",
        "      \"title\": \"Fix path traversal vulnerability in archive\",",
        "      \"status\": \"completed\",",
        "      \"priority\": \"high\",",
        "      \"category\": \"security\",",
        "      \"description\": \"archive_old_session_files() must validate paths stay within tasks/\",",
        "      \"depends_on\": [],",
        "      \"effort\": \"small\",",
        "      \"created_at\": \"2025-12-13T22:42:20.986947\",",
        "      \"updated_at\": \"2025-12-13T22:48:20.628391\",",
        "      \"completed_at\": \"2025-12-13T22:48:20.628391\",",
        "      \"context\": {",
        "        \"files\": [",
        "          \"scripts/consolidate_tasks.py\"",
        "        ],",
        "        \"line\": 214",
        "      }",
        "    },",
        "    {",
        "      \"id\": \"T-20251213-224220-6ac7-03\",",
        "      \"title\": \"Fix task counter overflow at 100 tasks\",",
        "      \"status\": \"completed\",",
        "      \"priority\": \"high\",",
        "      \"category\": \"bugfix\",",
        "      \"description\": \"Expand counter format from 02d to 03d or use base36\",",
        "      \"depends_on\": [],",
        "      \"effort\": \"small\",",
        "      \"created_at\": \"2025-12-13T22:42:20.986955\",",
        "      \"updated_at\": \"2025-12-13T22:48:20.763992\",",
        "      \"completed_at\": \"2025-12-13T22:48:20.763992\",",
        "      \"context\": {",
        "        \"files\": [",
        "          \"scripts/task_utils.py\"",
        "        ],",
        "        \"line\": 156",
        "      }",
        "    },",
        "    {",
        "      \"id\": \"T-20251213-224220-6ac7-04\",",
        "      \"title\": \"Add workflow templates (bugfix, feature, refactor)\",",
        "      \"status\": \"completed\",",
        "      \"priority\": \"medium\",",
        "      \"category\": \"feature\",",
        "      \"description\": \"YAML templates in .claude/workflows/ that spawn task chains\",",
        "      \"depends_on\": [",
        "        \"T-20251213-224220-6ac7-01\",",
        "        \"T-20251213-224220-6ac7-02\",",
        "        \"T-20251213-224220-6ac7-03\"",
        "      ],",
        "      \"effort\": \"medium\",",
        "      \"created_at\": \"2025-12-13T22:42:20.986963\",",
        "      \"updated_at\": \"2025-12-13T22:50:36.977578\",",
        "      \"completed_at\": \"2025-12-13T22:50:36.977578\",",
        "      \"context\": {}",
        "    },",
        "    {",
        "      \"id\": \"T-20251213-224220-6ac7-05\",",
        "      \"title\": \"Add auto-task creation from CI test failures\",",
        "      \"status\": \"pending\",",
        "      \"priority\": \"medium\",",
        "      \"category\": \"automation\",",
        "      \"description\": \"Parse pytest failures, create bugfix tasks automatically\",",
        "      \"depends_on\": [],",
        "      \"effort\": \"medium\",",
        "      \"created_at\": \"2025-12-13T22:42:20.986976\",",
        "      \"updated_at\": null,",
        "      \"completed_at\": null,",
        "      \"context\": {",
        "        \"files\": [",
        "          \".github/workflows/ci.yml\",",
        "          \"scripts/create_tasks_from_ci.py\"",
        "        ]",
        "      }",
        "    },",
        "    {",
        "      \"id\": \"T-20251213-224220-6ac7-06\",",
        "      \"title\": \"Add task retrospective metadata capture\",",
        "      \"status\": \"pending\",",
        "      \"priority\": \"medium\",",
        "      \"category\": \"feature\",",
        "      \"description\": \"Track files_touched, duration, tests_written for learning\",",
        "      \"depends_on\": [],",
        "      \"effort\": \"large\",",
        "      \"created_at\": \"2025-12-13T22:42:20.986983\",",
        "      \"updated_at\": null,",
        "      \"completed_at\": null,",
        "      \"context\": {}",
        "    }",
        "  ]",
        "}"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "tasks/2025-12-13_22-50-18_cdd1.json",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "{",
        "  \"version\": 1,",
        "  \"session_id\": \"cdd1\",",
        "  \"started_at\": \"2025-12-13T22:50:18.707702\",",
        "  \"saved_at\": \"2025-12-13T22:50:37.198957\",",
        "  \"tasks\": [",
        "    {",
        "      \"id\": \"T-20251213-225018-cdd1-001\",",
        "      \"title\": \"Design: Workflow templates\",",
        "      \"status\": \"completed\",",
        "      \"priority\": \"medium\",",
        "      \"category\": \"arch\",",
        "      \"description\": \"Design the API and architecture for: Workflow templates\\n\\nDeliverables:\\n- API design (function signatures, data structures)\\n- Integration points with existing code\\n- Edge cases to handle\\n\",",
        "      \"depends_on\": [],",
        "      \"effort\": \"medium\",",
        "      \"created_at\": \"2025-12-13T22:50:18.707760\",",
        "      \"updated_at\": \"2025-12-13T22:50:37.089387\",",
        "      \"completed_at\": \"2025-12-13T22:50:37.089387\",",
        "      \"context\": {}",
        "    },",
        "    {",
        "      \"id\": \"T-20251213-225018-cdd1-002\",",
        "      \"title\": \"Implement: Workflow templates\",",
        "      \"status\": \"completed\",",
        "      \"priority\": \"medium\",",
        "      \"category\": \"feature\",",
        "      \"description\": \"Implement the core functionality for: Workflow templates\\n\\nChecklist:\\n- [ ] Core logic implemented\\n- [ ] Error handling added\\n- [ ] Follows existing code patterns\\n\",",
        "      \"depends_on\": [",
        "        \"T-20251213-225018-cdd1-001\"",
        "      ],",
        "      \"effort\": \"medium\",",
        "      \"created_at\": \"2025-12-13T22:50:18.707774\",",
        "      \"updated_at\": \"2025-12-13T22:50:37.198821\",",
        "      \"completed_at\": \"2025-12-13T22:50:37.198821\",",
        "      \"context\": {}",
        "    },",
        "    {",
        "      \"id\": \"T-20251213-225018-cdd1-003\",",
        "      \"title\": \"Unit tests for: Workflow templates\",",
        "      \"status\": \"pending\",",
        "      \"priority\": \"high\",",
        "      \"category\": \"test\",",
        "      \"description\": \"Write comprehensive unit tests.\\n\\nCoverage requirements:\\n- Happy path scenarios\\n- Edge cases\\n- Error conditions\\n- Target: 90%+ coverage for new code\\n\",",
        "      \"depends_on\": [",
        "        \"T-20251213-225018-cdd1-002\"",
        "      ],",
        "      \"effort\": \"medium\",",
        "      \"created_at\": \"2025-12-13T22:50:18.707790\",",
        "      \"updated_at\": null,",
        "      \"completed_at\": null,",
        "      \"context\": {}",
        "    },",
        "    {",
        "      \"id\": \"T-20251213-225018-cdd1-004\",",
        "      \"title\": \"Integration tests for: Workflow templates\",",
        "      \"status\": \"pending\",",
        "      \"priority\": \"high\",",
        "      \"category\": \"test\",",
        "      \"description\": \"Write integration tests verifying feature works with existing system.\\n\\nTest scenarios:\\n- End-to-end workflows\\n- Interaction with other components\\n- Performance characteristics\\n\",",
        "      \"depends_on\": [",
        "        \"T-20251213-225018-cdd1-002\"",
        "      ],",
        "      \"effort\": \"medium\",",
        "      \"created_at\": \"2025-12-13T22:50:18.707805\",",
        "      \"updated_at\": null,",
        "      \"completed_at\": null,",
        "      \"context\": {}",
        "    },",
        "    {",
        "      \"id\": \"T-20251213-225018-cdd1-005\",",
        "      \"title\": \"Documentation for: Workflow templates\",",
        "      \"status\": \"pending\",",
        "      \"priority\": \"medium\",",
        "      \"category\": \"docs\",",
        "      \"description\": \"Document the new feature.\\n\\nInclude:\\n- Usage examples\\n- API reference updates\\n- CLAUDE.md updates if applicable\\n\",",
        "      \"depends_on\": [",
        "        \"T-20251213-225018-cdd1-003\",",
        "        \"T-20251213-225018-cdd1-004\"",
        "      ],",
        "      \"effort\": \"small\",",
        "      \"created_at\": \"2025-12-13T22:50:18.707823\",",
        "      \"updated_at\": null,",
        "      \"completed_at\": null,",
        "      \"context\": {}",
        "    }",
        "  ]",
        "}"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "tests/integration/test_task_integration.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "\"\"\"",
        "Integration tests for merge-friendly task utilities.",
        "",
        "These tests verify that the task system works correctly in realistic",
        "multi-agent scenarios where multiple sessions create tasks concurrently.",
        "",
        "Run with: pytest tests/integration/test_task_integration.py -v",
        "\"\"\"",
        "",
        "import json",
        "import os",
        "import shutil",
        "import sys",
        "import tempfile",
        "import threading",
        "import time",
        "import unittest",
        "from concurrent.futures import ThreadPoolExecutor, as_completed",
        "from pathlib import Path",
        "",
        "# Add scripts to path",
        "sys.path.insert(0, str(Path(__file__).parent.parent.parent / \"scripts\"))",
        "",
        "from task_utils import (",
        "    TaskSession,",
        "    Task,",
        "    load_all_tasks,",
        "    consolidate_tasks,",
        "    generate_task_id,",
        "    generate_short_task_id,",
        ")",
        "from consolidate_tasks import (",
        "    consolidate_and_dedupe,",
        "    find_conflicts,",
        "    merge_duplicate_tasks,",
        "    write_consolidated_file,",
        "    archive_old_session_files,",
        ")",
        "",
        "",
        "class TestMultiSessionConcurrency(unittest.TestCase):",
        "    \"\"\"Test that multiple sessions don't conflict when run concurrently.\"\"\"",
        "",
        "    def setUp(self):",
        "        \"\"\"Create temporary directory for task files.\"\"\"",
        "        self.temp_dir = tempfile.mkdtemp()",
        "",
        "    def tearDown(self):",
        "        \"\"\"Clean up temporary directory.\"\"\"",
        "        shutil.rmtree(self.temp_dir)",
        "",
        "    def test_parallel_sessions_create_unique_files(self):",
        "        \"\"\"Multiple sessions running in parallel should create unique files.\"\"\"",
        "        num_sessions = 10",
        "        sessions_created = []",
        "",
        "        def create_session_with_tasks():",
        "            session = TaskSession()",
        "            session.create_task(title=f\"Task from session {session.session_id}\")",
        "            filepath = session.save(self.temp_dir)",
        "            return str(filepath)",
        "",
        "        # Run sessions in parallel",
        "        with ThreadPoolExecutor(max_workers=num_sessions) as executor:",
        "            futures = [executor.submit(create_session_with_tasks) for _ in range(num_sessions)]",
        "            for future in as_completed(futures):",
        "                sessions_created.append(future.result())",
        "",
        "        # All files should be unique",
        "        self.assertEqual(len(set(sessions_created)), num_sessions)",
        "",
        "        # All files should exist",
        "        for filepath in sessions_created:",
        "            self.assertTrue(Path(filepath).exists())",
        "",
        "    def test_parallel_task_id_generation_high_uniqueness(self):",
        "        \"\"\"Task IDs generated in parallel should be mostly unique.",
        "",
        "        Note: Same-second generation may cause collisions (timestamp-based).",
        "        We verify >95% uniqueness which is sufficient for real-world use",
        "        where agents run for longer durations.",
        "        \"\"\"",
        "        num_ids = 1000",
        "        generated_ids = []",
        "",
        "        def generate_ids():",
        "            return [generate_task_id() for _ in range(100)]",
        "",
        "        with ThreadPoolExecutor(max_workers=10) as executor:",
        "            futures = [executor.submit(generate_ids) for _ in range(10)]",
        "            for future in as_completed(futures):",
        "                generated_ids.extend(future.result())",
        "",
        "        # At least 95% should be unique (same-second collisions expected)",
        "        unique_count = len(set(generated_ids))",
        "        uniqueness_ratio = unique_count / num_ids",
        "        self.assertGreater(uniqueness_ratio, 0.95,",
        "            f\"Only {uniqueness_ratio*100:.1f}% unique IDs\")",
        "",
        "    def test_concurrent_session_saves_no_corruption(self):",
        "        \"\"\"Concurrent saves should not corrupt files.\"\"\"",
        "        sessions = [TaskSession() for _ in range(5)]",
        "",
        "        # Add tasks to each session",
        "        for i, session in enumerate(sessions):",
        "            for j in range(10):",
        "                session.create_task(title=f\"Session {i} Task {j}\")",
        "",
        "        # Save all concurrently",
        "        def save_session(session):",
        "            return session.save(self.temp_dir)",
        "",
        "        with ThreadPoolExecutor(max_workers=5) as executor:",
        "            futures = [executor.submit(save_session, s) for s in sessions]",
        "            filepaths = [f.result() for f in as_completed(futures)]",
        "",
        "        # Verify all files are valid JSON",
        "        for filepath in filepaths:",
        "            with open(filepath) as f:",
        "                data = json.load(f)",
        "                self.assertIn(\"tasks\", data)",
        "                self.assertEqual(len(data[\"tasks\"]), 10)",
        "",
        "",
        "class TestConsolidationIntegration(unittest.TestCase):",
        "    \"\"\"Test task consolidation across multiple session files.\"\"\"",
        "",
        "    def setUp(self):",
        "        \"\"\"Create temporary directory with multiple session files.\"\"\"",
        "        self.temp_dir = tempfile.mkdtemp()",
        "",
        "    def tearDown(self):",
        "        \"\"\"Clean up temporary directory.\"\"\"",
        "        shutil.rmtree(self.temp_dir)",
        "",
        "    def test_consolidate_preserves_all_tasks(self):",
        "        \"\"\"Consolidation should preserve all tasks from all sessions.\"\"\"",
        "        total_tasks = 0",
        "",
        "        # Create multiple sessions with varying task counts",
        "        for i in range(5):",
        "            session = TaskSession()",
        "            task_count = (i + 1) * 3  # 3, 6, 9, 12, 15 tasks",
        "            for j in range(task_count):",
        "                session.create_task(title=f\"Session {i} Task {j}\")",
        "            total_tasks += task_count",
        "            session.save(self.temp_dir)",
        "",
        "        # Load all tasks",
        "        all_tasks = load_all_tasks(self.temp_dir)",
        "        self.assertEqual(len(all_tasks), total_tasks)",
        "",
        "    def test_consolidate_detects_duplicates(self):",
        "        \"\"\"Consolidation should detect tasks with same title as potential duplicates.\"\"\"",
        "        # Create two sessions with same task title",
        "        session1 = TaskSession()",
        "        session1.create_task(title=\"Fix authentication bug\")",
        "        session1.save(self.temp_dir)",
        "",
        "        session2 = TaskSession()",
        "        session2.create_task(title=\"Fix authentication bug\")  # Duplicate!",
        "        session2.save(self.temp_dir)",
        "",
        "        # Load and check for conflicts",
        "        all_tasks = load_all_tasks(self.temp_dir)",
        "        conflicts = find_conflicts(all_tasks)",
        "",
        "        self.assertEqual(len(conflicts), 1)",
        "        self.assertIn(\"fix authentication bug\", conflicts)",
        "        self.assertEqual(len(conflicts[\"fix authentication bug\"]), 2)",
        "",
        "    def test_auto_merge_resolves_duplicates(self):",
        "        \"\"\"Auto-merge should combine duplicate tasks intelligently.\"\"\"",
        "        # Create two sessions with same task, different metadata",
        "        session1 = TaskSession()",
        "        task1 = session1.create_task(",
        "            title=\"Fix authentication bug\",",
        "            priority=\"low\",",
        "            description=\"Short description\"",
        "        )",
        "        session1.save(self.temp_dir)",
        "",
        "        # Wait a bit to ensure different timestamp",
        "        time.sleep(0.01)",
        "",
        "        session2 = TaskSession()",
        "        task2 = session2.create_task(",
        "            title=\"Fix authentication bug\",",
        "            priority=\"high\",  # Higher priority",
        "            description=\"Much longer and more detailed description of the bug\"",
        "        )",
        "        task2.mark_in_progress()  # More advanced status",
        "        session2.save(self.temp_dir)",
        "",
        "        # Consolidate with auto-merge",
        "        tasks, conflicts = consolidate_and_dedupe(self.temp_dir, auto_merge=True)",
        "",
        "        # Should have merged to one task",
        "        auth_tasks = [t for t in tasks if \"authentication\" in t.title.lower()]",
        "        self.assertEqual(len(auth_tasks), 1)",
        "",
        "        merged = auth_tasks[0]",
        "        # Should keep higher priority",
        "        self.assertEqual(merged.priority, \"high\")",
        "        # Should keep more advanced status",
        "        self.assertEqual(merged.status, \"in_progress\")",
        "        # Should keep longer description",
        "        self.assertIn(\"longer\", merged.description)",
        "",
        "    def test_consolidated_file_is_valid(self):",
        "        \"\"\"Written consolidated file should be valid and loadable.\"\"\"",
        "        # Create some sessions",
        "        for i in range(3):",
        "            session = TaskSession()",
        "            session.create_task(title=f\"Task {i}\")",
        "            session.save(self.temp_dir)",
        "",
        "        all_tasks = load_all_tasks(self.temp_dir)",
        "",
        "        # Write consolidated file",
        "        consolidated_path = write_consolidated_file(all_tasks, self.temp_dir)",
        "",
        "        # Verify it's valid JSON",
        "        with open(consolidated_path) as f:",
        "            data = json.load(f)",
        "",
        "        self.assertEqual(data[\"type\"], \"consolidated\")",
        "        self.assertEqual(data[\"task_count\"], 3)",
        "        self.assertEqual(len(data[\"tasks\"]), 3)",
        "",
        "    def test_archive_moves_files_correctly(self):",
        "        \"\"\"Archiving should move session files but keep consolidated.\"\"\"",
        "        # Create sessions",
        "        session_files = []",
        "        for i in range(3):",
        "            session = TaskSession()",
        "            session.create_task(title=f\"Task {i}\")",
        "            path = session.save(self.temp_dir)",
        "            session_files.append(path)",
        "",
        "        # Write consolidated file",
        "        all_tasks = load_all_tasks(self.temp_dir)",
        "        consolidated_path = write_consolidated_file(all_tasks, self.temp_dir)",
        "",
        "        # Archive old files",
        "        archived = archive_old_session_files(self.temp_dir)",
        "",
        "        # Session files should be moved",
        "        self.assertEqual(len(archived), 3)",
        "        for path in session_files:",
        "            self.assertFalse(path.exists())",
        "",
        "        # Consolidated file should remain",
        "        self.assertTrue(consolidated_path.exists())",
        "",
        "        # Archive directory should exist",
        "        archive_dir = Path(self.temp_dir) / \"archive\"",
        "        self.assertTrue(archive_dir.exists())",
        "        self.assertEqual(len(list(archive_dir.glob(\"*.json\"))), 3)",
        "",
        "",
        "class TestTaskLifecycleIntegration(unittest.TestCase):",
        "    \"\"\"Test complete task lifecycle: create -> update -> complete -> consolidate.\"\"\"",
        "",
        "    def setUp(self):",
        "        \"\"\"Create temporary directory for task files.\"\"\"",
        "        self.temp_dir = tempfile.mkdtemp()",
        "",
        "    def tearDown(self):",
        "        \"\"\"Clean up temporary directory.\"\"\"",
        "        shutil.rmtree(self.temp_dir)",
        "",
        "    def test_full_task_lifecycle(self):",
        "        \"\"\"Test complete lifecycle of task management.\"\"\"",
        "        # 1. Create tasks in session",
        "        session = TaskSession()",
        "        task1 = session.create_task(",
        "            title=\"Implement feature\",",
        "            priority=\"high\",",
        "            category=\"feature\"",
        "        )",
        "        task2 = session.create_task(",
        "            title=\"Write tests\",",
        "            priority=\"medium\",",
        "            depends_on=[task1.id]",
        "        )",
        "",
        "        # 2. Update task status before saving",
        "        task1.mark_in_progress()",
        "        task1.mark_complete()",
        "        self.assertEqual(task1.status, \"completed\")",
        "        self.assertIsNotNone(task1.completed_at)",
        "",
        "        # 3. Save and verify persistence",
        "        session.save(self.temp_dir)",
        "",
        "        # 4. Load and verify",
        "        all_tasks = load_all_tasks(self.temp_dir)",
        "        self.assertEqual(len(all_tasks), 2)",
        "",
        "        # 5. Consolidate and check grouping",
        "        grouped = consolidate_tasks(self.temp_dir)",
        "        self.assertEqual(len(grouped[\"pending\"]), 1)  # task2",
        "        self.assertEqual(len(grouped[\"completed\"]), 1)  # task1",
        "",
        "    def test_multi_agent_workflow_simulation(self):",
        "        \"\"\"Simulate a realistic multi-agent workflow.\"\"\"",
        "        # Agent A: Research phase",
        "        agent_a = TaskSession()",
        "        a1 = agent_a.create_task(",
        "            title=\"Research existing codebase\",",
        "            priority=\"high\",",
        "            category=\"research\"",
        "        )",
        "        a2 = agent_a.create_task(",
        "            title=\"Identify integration points\",",
        "            depends_on=[a1.id]",
        "        )",
        "        agent_a.save(self.temp_dir)",
        "",
        "        # Agent B: Implementation phase (concurrent)",
        "        agent_b = TaskSession()",
        "        b1 = agent_b.create_task(",
        "            title=\"Implement core feature\",",
        "            priority=\"high\",",
        "            category=\"implementation\"",
        "        )",
        "        b2 = agent_b.create_task(",
        "            title=\"Add unit tests\",",
        "            depends_on=[b1.id]",
        "        )",
        "        agent_b.save(self.temp_dir)",
        "",
        "        # Agent C: Documentation (concurrent)",
        "        agent_c = TaskSession()",
        "        c1 = agent_c.create_task(",
        "            title=\"Update documentation\",",
        "            priority=\"medium\",",
        "            category=\"docs\"",
        "        )",
        "        agent_c.save(self.temp_dir)",
        "",
        "        # Verify all tasks exist and are unique",
        "        all_tasks = load_all_tasks(self.temp_dir)",
        "        self.assertEqual(len(all_tasks), 5)",
        "",
        "        # All IDs should be unique",
        "        all_ids = [t.id for t in all_tasks]",
        "        self.assertEqual(len(set(all_ids)), 5)",
        "",
        "        # Dependencies should reference valid IDs",
        "        for task in all_tasks:",
        "            for dep_id in task.depends_on:",
        "                self.assertTrue(",
        "                    any(t.id == dep_id for t in all_tasks),",
        "                    f\"Dependency {dep_id} not found\"",
        "                )",
        "",
        "",
        "class TestFileSystemResilience(unittest.TestCase):",
        "    \"\"\"Test resilience to file system edge cases.\"\"\"",
        "",
        "    def setUp(self):",
        "        \"\"\"Create temporary directory for task files.\"\"\"",
        "        self.temp_dir = tempfile.mkdtemp()",
        "",
        "    def tearDown(self):",
        "        \"\"\"Clean up temporary directory.\"\"\"",
        "        shutil.rmtree(self.temp_dir)",
        "",
        "    def test_handles_missing_directory(self):",
        "        \"\"\"load_all_tasks should handle missing directory gracefully.\"\"\"",
        "        tasks = load_all_tasks(\"/nonexistent/path/that/does/not/exist\")",
        "        self.assertEqual(tasks, [])",
        "",
        "    def test_handles_empty_directory(self):",
        "        \"\"\"load_all_tasks should handle empty directory.\"\"\"",
        "        tasks = load_all_tasks(self.temp_dir)",
        "        self.assertEqual(tasks, [])",
        "",
        "    def test_handles_corrupt_file(self):",
        "        \"\"\"load_all_tasks should skip corrupt files.\"\"\"",
        "        # Create valid session",
        "        session = TaskSession()",
        "        session.create_task(title=\"Valid task\")",
        "        session.save(self.temp_dir)",
        "",
        "        # Create corrupt file",
        "        corrupt_path = Path(self.temp_dir) / \"2025-12-13_00-00-00_xxxx.json\"",
        "        with open(corrupt_path, \"w\") as f:",
        "            f.write(\"{ this is not valid json }\")",
        "",
        "        # Should load valid tasks and skip corrupt",
        "        tasks = load_all_tasks(self.temp_dir)",
        "        self.assertEqual(len(tasks), 1)",
        "",
        "    def test_creates_directory_on_save(self):",
        "        \"\"\"save() should create directory if it doesn't exist.\"\"\"",
        "        nested_dir = Path(self.temp_dir) / \"nested\" / \"deep\" / \"tasks\"",
        "        session = TaskSession()",
        "        session.create_task(title=\"Task\")",
        "        filepath = session.save(str(nested_dir))",
        "",
        "        self.assertTrue(filepath.exists())",
        "        self.assertTrue(nested_dir.exists())",
        "",
        "",
        "class TestSecurityAndRobustness(unittest.TestCase):",
        "    \"\"\"Test security fixes and robustness improvements.\"\"\"",
        "",
        "    def setUp(self):",
        "        \"\"\"Create temporary directory for task files.\"\"\"",
        "        self.temp_dir = tempfile.mkdtemp()",
        "",
        "    def tearDown(self):",
        "        \"\"\"Clean up temporary directory.\"\"\"",
        "        shutil.rmtree(self.temp_dir)",
        "",
        "    def test_path_traversal_rejected(self):",
        "        \"\"\"Archive should reject path traversal attempts.\"\"\"",
        "        # Create a session with tasks",
        "        session = TaskSession()",
        "        session.create_task(title=\"Task\")",
        "        session.save(self.temp_dir)",
        "",
        "        # Attempt path traversal with ..",
        "        evil_archive = str(Path(self.temp_dir) / \"..\" / \"evil_dir\")",
        "        with self.assertRaises(ValueError) as cm:",
        "            archive_old_session_files(self.temp_dir, archive_dir=evil_archive)",
        "        self.assertIn(\"path traversal\", str(cm.exception).lower())",
        "",
        "    def test_absolute_path_outside_tasks_rejected(self):",
        "        \"\"\"Archive should reject absolute paths outside tasks directory.\"\"\"",
        "        session = TaskSession()",
        "        session.create_task(title=\"Task\")",
        "        session.save(self.temp_dir)",
        "",
        "        # Attempt to use /tmp as archive (outside tasks_dir)",
        "        with self.assertRaises(ValueError) as cm:",
        "            archive_old_session_files(self.temp_dir, archive_dir=\"/tmp\")",
        "        self.assertIn(\"must be within\", str(cm.exception).lower())",
        "",
        "    def test_subdirectory_archive_allowed(self):",
        "        \"\"\"Archive within tasks directory should be allowed.\"\"\"",
        "        session = TaskSession()",
        "        session.create_task(title=\"Task\")",
        "        session.save(self.temp_dir)",
        "",
        "        # Subdirectory should work fine",
        "        sub_archive = str(Path(self.temp_dir) / \"deep\" / \"archive\")",
        "        archived = archive_old_session_files(self.temp_dir, archive_dir=sub_archive)",
        "        self.assertEqual(len(archived), 1)",
        "        self.assertTrue(Path(sub_archive).exists())",
        "",
        "    def test_counter_supports_100_plus_tasks(self):",
        "        \"\"\"Session should support 100+ tasks without format issues.\"\"\"",
        "        session = TaskSession()",
        "",
        "        # Create 150 tasks",
        "        for i in range(150):",
        "            task = session.create_task(title=f\"Task {i}\")",
        "",
        "        # All IDs should be unique and follow pattern",
        "        all_ids = [t.id for t in session.tasks]",
        "        self.assertEqual(len(set(all_ids)), 150)",
        "",
        "        # Check format consistency (3-digit counter)",
        "        for task in session.tasks:",
        "            parts = task.id.split(\"-\")",
        "            self.assertEqual(len(parts), 5)  # T-YYYYMMDD-HHMMSS-XXXX-NNN",
        "            counter = parts[-1]",
        "            self.assertEqual(len(counter), 3)  # Always 3 digits",
        "",
        "    def test_atomic_write_no_temp_file_left_on_success(self):",
        "        \"\"\"Successful save should not leave temp files.\"\"\"",
        "        session = TaskSession()",
        "        session.create_task(title=\"Task\")",
        "        filepath = session.save(self.temp_dir)",
        "",
        "        # Check no .tmp files exist",
        "        tmp_files = list(Path(self.temp_dir).glob(\"*.tmp\"))",
        "        self.assertEqual(len(tmp_files), 0)",
        "",
        "        # Main file should exist",
        "        self.assertTrue(filepath.exists())",
        "",
        "    def test_save_creates_valid_json(self):",
        "        \"\"\"Saved file should be valid JSON after atomic write.\"\"\"",
        "        session = TaskSession()",
        "        for i in range(10):",
        "            session.create_task(title=f\"Task {i}\")",
        "        filepath = session.save(self.temp_dir)",
        "",
        "        # Should load without error",
        "        with open(filepath) as f:",
        "            data = json.load(f)",
        "",
        "        self.assertEqual(len(data[\"tasks\"]), 10)",
        "        self.assertEqual(data[\"session_id\"], session.session_id)",
        "",
        "",
        "if __name__ == \"__main__\":",
        "    unittest.main()"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "tests/unit/test_task_utils.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "\"\"\"Unit tests for merge-friendly task ID utilities.\"\"\"",
        "",
        "import json",
        "import os",
        "import sys",
        "import tempfile",
        "import unittest",
        "from datetime import datetime",
        "from pathlib import Path",
        "",
        "# Add scripts to path",
        "sys.path.insert(0, str(Path(__file__).parent.parent.parent / \"scripts\"))",
        "",
        "from task_utils import (",
        "    generate_task_id,",
        "    generate_short_task_id,",
        "    generate_session_id,",
        "    Task,",
        "    TaskSession,",
        "    load_all_tasks,",
        "    get_task_by_id,",
        "    consolidate_tasks,",
        ")",
        "",
        "",
        "class TestTaskIdGeneration(unittest.TestCase):",
        "    \"\"\"Tests for task ID generation.\"\"\"",
        "",
        "    def test_generate_task_id_format(self):",
        "        \"\"\"Task ID should have correct format.\"\"\"",
        "        task_id = generate_task_id()",
        "        # Format: T-YYYYMMDD-HHMMSS-XXXX",
        "        self.assertTrue(task_id.startswith(\"T-\"))",
        "        parts = task_id.split(\"-\")",
        "        self.assertEqual(len(parts), 4)",
        "        self.assertEqual(len(parts[1]), 8)  # YYYYMMDD",
        "        self.assertEqual(len(parts[2]), 6)  # HHMMSS",
        "        self.assertEqual(len(parts[3]), 4)  # session suffix",
        "",
        "    def test_generate_task_id_with_session(self):",
        "        \"\"\"Task ID should use provided session suffix.\"\"\"",
        "        task_id = generate_task_id(\"test\")",
        "        self.assertTrue(task_id.endswith(\"-test\"))",
        "",
        "    def test_generate_short_task_id(self):",
        "        \"\"\"Short task ID should be 10 characters.\"\"\"",
        "        task_id = generate_short_task_id()",
        "        # Format: T-XXXXXXXX",
        "        self.assertTrue(task_id.startswith(\"T-\"))",
        "        self.assertEqual(len(task_id), 10)",
        "",
        "    def test_generate_session_id(self):",
        "        \"\"\"Session ID should be 4 hex characters.\"\"\"",
        "        session_id = generate_session_id()",
        "        self.assertEqual(len(session_id), 4)",
        "        # Should be valid hex",
        "        int(session_id, 16)",
        "",
        "    def test_unique_task_ids(self):",
        "        \"\"\"Generated task IDs should be unique.\"\"\"",
        "        ids = {generate_task_id() for _ in range(100)}",
        "        self.assertEqual(len(ids), 100)",
        "",
        "",
        "class TestTask(unittest.TestCase):",
        "    \"\"\"Tests for Task dataclass.\"\"\"",
        "",
        "    def test_task_creation(self):",
        "        \"\"\"Task should be created with required fields.\"\"\"",
        "        task = Task(id=\"T-test\", title=\"Test task\")",
        "        self.assertEqual(task.id, \"T-test\")",
        "        self.assertEqual(task.title, \"Test task\")",
        "        self.assertEqual(task.status, \"pending\")",
        "",
        "    def test_task_to_dict(self):",
        "        \"\"\"Task should serialize to dict.\"\"\"",
        "        task = Task(",
        "            id=\"T-test\",",
        "            title=\"Test task\",",
        "            priority=\"high\",",
        "            category=\"arch\"",
        "        )",
        "        d = task.to_dict()",
        "        self.assertEqual(d[\"id\"], \"T-test\")",
        "        self.assertEqual(d[\"title\"], \"Test task\")",
        "        self.assertEqual(d[\"priority\"], \"high\")",
        "",
        "    def test_task_from_dict(self):",
        "        \"\"\"Task should deserialize from dict.\"\"\"",
        "        d = {",
        "            \"id\": \"T-test\",",
        "            \"title\": \"Test task\",",
        "            \"status\": \"completed\",",
        "            \"priority\": \"low\",",
        "            \"category\": \"test\",",
        "            \"description\": \"\",",
        "            \"depends_on\": [],",
        "            \"effort\": \"small\",",
        "            \"created_at\": \"2025-12-13T00:00:00\",",
        "            \"updated_at\": None,",
        "            \"completed_at\": None,",
        "            \"context\": {}",
        "        }",
        "        task = Task.from_dict(d)",
        "        self.assertEqual(task.id, \"T-test\")",
        "        self.assertEqual(task.status, \"completed\")",
        "",
        "    def test_mark_complete(self):",
        "        \"\"\"mark_complete should update status and timestamp.\"\"\"",
        "        task = Task(id=\"T-test\", title=\"Test task\")",
        "        self.assertEqual(task.status, \"pending\")",
        "        self.assertIsNone(task.completed_at)",
        "",
        "        task.mark_complete()",
        "        self.assertEqual(task.status, \"completed\")",
        "        self.assertIsNotNone(task.completed_at)",
        "",
        "    def test_mark_in_progress(self):",
        "        \"\"\"mark_in_progress should update status and timestamp.\"\"\"",
        "        task = Task(id=\"T-test\", title=\"Test task\")",
        "        task.mark_in_progress()",
        "        self.assertEqual(task.status, \"in_progress\")",
        "        self.assertIsNotNone(task.updated_at)",
        "",
        "",
        "class TestTaskSession(unittest.TestCase):",
        "    \"\"\"Tests for TaskSession.\"\"\"",
        "",
        "    def setUp(self):",
        "        \"\"\"Create temporary directory for task files.\"\"\"",
        "        self.temp_dir = tempfile.mkdtemp()",
        "",
        "    def tearDown(self):",
        "        \"\"\"Clean up temporary directory.\"\"\"",
        "        import shutil",
        "        shutil.rmtree(self.temp_dir)",
        "",
        "    def test_session_id_consistency(self):",
        "        \"\"\"All tasks in session should share same session suffix.\"\"\"",
        "        session = TaskSession()",
        "        id1 = session.new_task_id()",
        "        id2 = session.new_task_id()",
        "",
        "        # Format: T-YYYYMMDD-HHMMSS-XXXX-NN",
        "        # Session suffix is second to last part",
        "        parts1 = id1.split(\"-\")",
        "        parts2 = id2.split(\"-\")",
        "",
        "        session_suffix1 = parts1[-2]",
        "        session_suffix2 = parts2[-2]",
        "",
        "        self.assertEqual(session_suffix1, session_suffix2)",
        "        self.assertEqual(session_suffix1, session.session_id)",
        "",
        "        # Task counters should be different (3-digit format)",
        "        counter1 = parts1[-1]",
        "        counter2 = parts2[-1]",
        "        self.assertNotEqual(counter1, counter2)",
        "        self.assertEqual(counter1, \"001\")",
        "        self.assertEqual(counter2, \"002\")",
        "",
        "    def test_create_task(self):",
        "        \"\"\"create_task should add task to session.\"\"\"",
        "        session = TaskSession()",
        "        task = session.create_task(",
        "            title=\"Test task\",",
        "            priority=\"high\"",
        "        )",
        "",
        "        self.assertEqual(len(session.tasks), 1)",
        "        self.assertEqual(task.title, \"Test task\")",
        "        self.assertEqual(task.priority, \"high\")",
        "",
        "    def test_save_and_load(self):",
        "        \"\"\"Session should save and load correctly.\"\"\"",
        "        session = TaskSession()",
        "        session.create_task(title=\"Task 1\")",
        "        session.create_task(title=\"Task 2\")",
        "",
        "        filepath = session.save(self.temp_dir)",
        "        self.assertTrue(filepath.exists())",
        "",
        "        loaded = TaskSession.load(filepath)",
        "        self.assertEqual(len(loaded.tasks), 2)",
        "        self.assertEqual(loaded.tasks[0].title, \"Task 1\")",
        "",
        "    def test_session_filename_format(self):",
        "        \"\"\"Session filename should have correct format.\"\"\"",
        "        session = TaskSession()",
        "        filename = session.get_filename()",
        "        # Format: YYYY-MM-DD_HH-MM-SS_XXXX.json",
        "        self.assertTrue(filename.endswith(\".json\"))",
        "        parts = filename[:-5].split(\"_\")  # Remove .json",
        "        self.assertEqual(len(parts), 3)",
        "",
        "",
        "class TestTaskLoading(unittest.TestCase):",
        "    \"\"\"Tests for loading tasks from multiple files.\"\"\"",
        "",
        "    def setUp(self):",
        "        \"\"\"Create temporary directory with task files.\"\"\"",
        "        self.temp_dir = tempfile.mkdtemp()",
        "",
        "        # Create two sessions",
        "        session1 = TaskSession()",
        "        session1.create_task(title=\"Session 1 Task 1\")",
        "        session1.create_task(title=\"Session 1 Task 2\")",
        "        session1.save(self.temp_dir)",
        "",
        "        session2 = TaskSession()",
        "        session2.create_task(title=\"Session 2 Task 1\")",
        "        session2.save(self.temp_dir)",
        "",
        "    def tearDown(self):",
        "        \"\"\"Clean up temporary directory.\"\"\"",
        "        import shutil",
        "        shutil.rmtree(self.temp_dir)",
        "",
        "    def test_load_all_tasks(self):",
        "        \"\"\"load_all_tasks should load from all session files.\"\"\"",
        "        tasks = load_all_tasks(self.temp_dir)",
        "        self.assertEqual(len(tasks), 3)",
        "",
        "    def test_load_empty_directory(self):",
        "        \"\"\"load_all_tasks should return empty list for missing directory.\"\"\"",
        "        tasks = load_all_tasks(\"/nonexistent\")",
        "        self.assertEqual(tasks, [])",
        "",
        "    def test_get_task_by_id(self):",
        "        \"\"\"get_task_by_id should find task across sessions.\"\"\"",
        "        tasks = load_all_tasks(self.temp_dir)",
        "        target_id = tasks[0].id",
        "",
        "        found = get_task_by_id(target_id, self.temp_dir)",
        "        self.assertIsNotNone(found)",
        "        self.assertEqual(found.id, target_id)",
        "",
        "    def test_get_task_by_id_not_found(self):",
        "        \"\"\"get_task_by_id should return None for missing ID.\"\"\"",
        "        found = get_task_by_id(\"T-nonexistent\", self.temp_dir)",
        "        self.assertIsNone(found)",
        "",
        "",
        "class TestConsolidation(unittest.TestCase):",
        "    \"\"\"Tests for task consolidation.\"\"\"",
        "",
        "    def setUp(self):",
        "        \"\"\"Create temporary directory with task files.\"\"\"",
        "        self.temp_dir = tempfile.mkdtemp()",
        "",
        "    def tearDown(self):",
        "        \"\"\"Clean up temporary directory.\"\"\"",
        "        import shutil",
        "        shutil.rmtree(self.temp_dir)",
        "",
        "    def test_consolidate_groups_by_status(self):",
        "        \"\"\"consolidate_tasks should group by status.\"\"\"",
        "        session = TaskSession()",
        "        task1 = session.create_task(title=\"Pending task\")",
        "        task2 = session.create_task(title=\"In progress task\")",
        "        task2.mark_in_progress()",
        "        task3 = session.create_task(title=\"Completed task\")",
        "        task3.mark_complete()",
        "        session.save(self.temp_dir)",
        "",
        "        grouped = consolidate_tasks(self.temp_dir)",
        "",
        "        self.assertEqual(len(grouped[\"pending\"]), 1)",
        "        self.assertEqual(len(grouped[\"in_progress\"]), 1)",
        "        self.assertEqual(len(grouped[\"completed\"]), 1)",
        "",
        "",
        "if __name__ == \"__main__\":",
        "    unittest.main()"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    }
  ],
  "hour_of_day": 23,
  "day_of_week": "Saturday",
  "seconds_since_last_commit": -137704,
  "is_merge": true,
  "is_initial": false,
  "parent_count": 2,
  "session_id": null,
  "related_chats": [],
  "ci_result": null,
  "reverted": false,
  "amended": false
}