{
  "hash": "731ace797a43254045cafdd803f64b330f0666ed",
  "message": "Implement 3 high-priority tasks from dog-fooding review",
  "author": "Claude",
  "timestamp": "2025-12-12 01:24:47 +0000",
  "branch": "claude/multi-index-design-DvifZ",
  "files_changed": [
    "cortical/embeddings.py",
    "cortical/processor.py",
    "cortical/query/search.py",
    "cortical/tokenizer.py",
    "showcase.py"
  ],
  "insertions": 232,
  "deletions": 44,
  "hunks": [
    {
      "file": "cortical/embeddings.py",
      "function": "import math",
      "start_line": 15,
      "lines_added": [
        "    method: str = 'adjacency',",
        "    max_terms: Optional[int] = None",
        "",
        "        method: 'adjacency', 'random_walk', 'spectral', or 'fast'",
        "        max_terms: If set, only compute embeddings for top N terms by PageRank.",
        "                   This significantly speeds up computation for large corpora.",
        "                   Recommended: 1000-2000 for large corpora (5000+ tokens).",
        "",
        "",
        "    # Sample top terms if max_terms is specified",
        "    if max_terms is not None and max_terms < layer0.column_count():",
        "        sorted_cols = sorted(layer0.minicolumns.values(), key=lambda c: c.pagerank, reverse=True)",
        "        sampled_terms = {col.content for col in sorted_cols[:max_terms]}",
        "    else:",
        "        sampled_terms = None",
        "",
        "    if method == 'fast':",
        "        # Fast direct adjacency without multi-hop propagation",
        "        embeddings = _fast_adjacency_embeddings(layer0, dimensions, sampled_terms)",
        "    elif method == 'adjacency':",
        "        embeddings = _adjacency_embeddings(layer0, dimensions, sampled_terms)",
        "        embeddings = _random_walk_embeddings(layer0, dimensions, sampled_terms)",
        "        embeddings = _spectral_embeddings(layer0, dimensions, sampled_terms)",
        "",
        "        'terms_embedded': len(embeddings),",
        "        'max_terms': max_terms,",
        "        'sampled': max_terms is not None and max_terms < layer0.column_count()",
        "",
        "def _fast_adjacency_embeddings(",
        "    layer: HierarchicalLayer,",
        "    dimensions: int,",
        "    sampled_terms: Optional[set] = None",
        ") -> Dict[str, List[float]]:",
        "    \"\"\"",
        "    Fast direct adjacency embeddings without multi-hop propagation.",
        "",
        "    Much faster than full adjacency but less expressive. Good for large corpora",
        "    where speed is more important than embedding quality.",
        "",
        "    Args:",
        "        layer: Layer to compute embeddings for",
        "        dimensions: Number of embedding dimensions (= number of landmarks)",
        "        sampled_terms: If set, only compute embeddings for these terms",
        "    \"\"\"",
        "    embeddings: Dict[str, List[float]] = {}",
        "",
        "    sorted_cols = sorted(layer.minicolumns.values(), key=lambda c: c.pagerank, reverse=True)",
        "    landmarks = sorted_cols[:dimensions]",
        "    landmark_ids = {lm.id: i for i, lm in enumerate(landmarks)}",
        "",
        "    cols_to_process = layer.minicolumns.values()",
        "    if sampled_terms is not None:",
        "        cols_to_process = [c for c in cols_to_process if c.content in sampled_terms]",
        "",
        "    for col in cols_to_process:",
        "        vec = [0.0] * dimensions",
        "",
        "        # Direct connections only",
        "        for lm_id, lm_idx in landmark_ids.items():",
        "            if lm_id in col.lateral_connections:",
        "                vec[lm_idx] = col.lateral_connections[lm_id]",
        "",
        "        # Normalize",
        "        mag = math.sqrt(sum(v*v for v in vec)) + 1e-10",
        "        embeddings[col.content] = [v / mag for v in vec]",
        "",
        "    return embeddings",
        "",
        "",
        "    sampled_terms: Optional[set] = None,",
        "        sampled_terms: If set, only compute embeddings for these terms",
        "    cols_to_process = layer.minicolumns.values()",
        "    if sampled_terms is not None:",
        "        cols_to_process = [c for c in cols_to_process if c.content in sampled_terms]",
        "",
        "    for col in cols_to_process:"
      ],
      "lines_removed": [
        "    method: str = 'adjacency'",
        "    ",
        "        method: 'adjacency', 'random_walk', or 'spectral'",
        "        ",
        "    ",
        "    if method == 'adjacency':",
        "        embeddings = _adjacency_embeddings(layer0, dimensions)",
        "        embeddings = _random_walk_embeddings(layer0, dimensions)",
        "        embeddings = _spectral_embeddings(layer0, dimensions)",
        "    ",
        "        'terms_embedded': len(embeddings)",
        "    ",
        "    for col in layer.minicolumns.values():"
      ],
      "context_before": [
        "import random",
        "from typing import Any, Dict, List, Tuple, Optional",
        "from collections import defaultdict",
        "",
        "from .layers import CorticalLayer, HierarchicalLayer",
        "",
        "",
        "def compute_graph_embeddings(",
        "    layers: Dict[CorticalLayer, HierarchicalLayer],",
        "    dimensions: int = 64,"
      ],
      "context_after": [
        ") -> Tuple[Dict[str, List[float]], Dict[str, Any]]:",
        "    \"\"\"",
        "    Compute embeddings for tokens based on graph structure.",
        "    Args:",
        "        layers: Dictionary of layers (needs TOKENS)",
        "        dimensions: Number of embedding dimensions",
        "    Returns:",
        "        Tuple of (embeddings dict, statistics dict)",
        "    \"\"\"",
        "    layer0 = layers[CorticalLayer.TOKENS]",
        "    elif method == 'random_walk':",
        "    elif method == 'spectral':",
        "    else:",
        "        raise ValueError(f\"Unknown embedding method: {method}\")",
        "    stats = {",
        "        'method': method,",
        "        'dimensions': dimensions,",
        "    }",
        "    return embeddings, stats",
        "",
        "",
        "def _adjacency_embeddings(",
        "    layer: HierarchicalLayer,",
        "    dimensions: int,",
        "    propagation_steps: int = 2,",
        "    damping: float = 0.5",
        ") -> Dict[str, List[float]]:",
        "    \"\"\"",
        "    Compute embeddings using multi-hop adjacency to landmark nodes.",
        "",
        "    Improves over simple direct adjacency by propagating through the graph,",
        "    which handles sparse graphs better and produces more meaningful embeddings.",
        "",
        "    Args:",
        "        layer: Layer to compute embeddings for",
        "        dimensions: Number of embedding dimensions (= number of landmarks)",
        "        propagation_steps: Number of propagation steps (default 2)",
        "        damping: Weight decay per step (default 0.5)",
        "    \"\"\"",
        "    embeddings: Dict[str, List[float]] = {}",
        "",
        "    sorted_cols = sorted(layer.minicolumns.values(), key=lambda c: c.pagerank, reverse=True)",
        "    landmarks = sorted_cols[:dimensions]",
        "    landmark_ids = {lm.id: i for i, lm in enumerate(landmarks)}",
        "",
        "    # Build adjacency lookup for efficient propagation",
        "    id_to_col = {col.id: col for col in layer.minicolumns.values()}",
        "",
        "        vec = [0.0] * dimensions",
        "",
        "        # Direct connections (weight = 1.0)",
        "        for lm_id, lm_idx in landmark_ids.items():",
        "            if lm_id in col.lateral_connections:",
        "                vec[lm_idx] += col.lateral_connections[lm_id]",
        "",
        "        # Multi-hop propagation: reach landmarks through neighbors",
        "        current_weight = damping",
        "        frontier = list(col.lateral_connections.items())"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/embeddings.py",
      "function": "def _adjacency_embeddings(",
      "start_line": 123,
      "lines_added": [
        "    sampled_terms: Optional[set] = None,",
        "",
        "    # Only walk from sampled terms if specified",
        "    cols_to_walk = layer.minicolumns.values()",
        "    if sampled_terms is not None:",
        "        cols_to_walk = [c for c in cols_to_walk if c.content in sampled_terms]",
        "",
        "    for col in cols_to_walk:",
        "",
        "",
        "    terms_to_embed = layer.minicolumns.keys() if sampled_terms is None else sampled_terms",
        "    for term in terms_to_embed:",
        "        if term in layer.minicolumns:",
        "            vec = [cooccurrence[term].get(lm, 0) for lm in landmarks]",
        "            mag = math.sqrt(sum(v*v for v in vec)) + 1e-10",
        "            embeddings[term] = [v / mag for v in vec]",
        ""
      ],
      "lines_removed": [
        "    ",
        "    for col in layer.minicolumns.values():",
        "    ",
        "    ",
        "    for term in layer.minicolumns:",
        "        vec = [cooccurrence[term].get(lm, 0) for lm in landmarks]",
        "        mag = math.sqrt(sum(v*v for v in vec)) + 1e-10",
        "        embeddings[term] = [v / mag for v in vec]",
        "    "
      ],
      "context_before": [
        "        # Normalize",
        "        mag = math.sqrt(sum(v*v for v in vec)) + 1e-10",
        "        embeddings[col.content] = [v / mag for v in vec]",
        "",
        "    return embeddings",
        "",
        "",
        "def _random_walk_embeddings(",
        "    layer: HierarchicalLayer,",
        "    dimensions: int,"
      ],
      "context_after": [
        "    walks_per_node: int = 10,",
        "    walk_length: int = 40,",
        "    window_size: int = 5",
        ") -> Dict[str, List[float]]:",
        "    \"\"\"Compute embeddings using random walks (DeepWalk-inspired).\"\"\"",
        "    embeddings: Dict[str, List[float]] = {}",
        "    id_to_term = {col.id: col.content for col in layer.minicolumns.values()}",
        "    cooccurrence: Dict[str, Dict[str, float]] = defaultdict(lambda: defaultdict(float))",
        "        for _ in range(walks_per_node):",
        "            walk = _weighted_random_walk(col, layer, walk_length, id_to_term)",
        "            for i, term in enumerate(walk):",
        "                for j in range(max(0, i - window_size), min(len(walk), i + window_size + 1)):",
        "                    if i != j:",
        "                        cooccurrence[term][walk[j]] += 1.0",
        "    sorted_cols = sorted(layer.minicolumns.values(), key=lambda c: c.pagerank, reverse=True)",
        "    landmarks = [c.content for c in sorted_cols[:dimensions]]",
        "    return embeddings",
        "",
        "",
        "def _weighted_random_walk(start_col, layer: HierarchicalLayer, length: int, id_to_term: Dict[str, str]) -> List[str]:",
        "    \"\"\"Perform a weighted random walk from a starting column.\"\"\"",
        "    walk = [start_col.content]",
        "    current = start_col",
        "    ",
        "    for _ in range(length - 1):",
        "        if not current.lateral_connections:"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/embeddings.py",
      "function": "def _weighted_random_walk(start_col, layer: HierarchicalLayer, length: int, id_t",
      "start_line": 183,
      "lines_added": [
        "def _spectral_embeddings(",
        "    layer: HierarchicalLayer,",
        "    dimensions: int,",
        "    sampled_terms: Optional[set] = None,",
        "    iterations: int = 50",
        ") -> Dict[str, List[float]]:",
        "    \"\"\"Compute embeddings using spectral methods (graph Laplacian).",
        "",
        "    Note: This is inherently O(dimensions × iterations × n²) so it's slow for large graphs.",
        "    When sampled_terms is provided, only those terms get embeddings but the full graph",
        "    structure is still used for computation.",
        "    \"\"\"",
        "",
        "    # If sampling, use only sampled terms for the graph",
        "    if sampled_terms is not None:",
        "        terms = [t for t in layer.minicolumns.keys() if t in sampled_terms]",
        "    else:",
        "        terms = list(layer.minicolumns.keys())",
        "",
        "",
        "",
        "    for term in terms:",
        "        col = layer.minicolumns[term]",
        "",
        "",
        "",
        "",
        "",
        ""
      ],
      "lines_removed": [
        "def _spectral_embeddings(layer: HierarchicalLayer, dimensions: int, iterations: int = 100) -> Dict[str, List[float]]:",
        "    \"\"\"Compute embeddings using spectral methods (graph Laplacian).\"\"\"",
        "    terms = list(layer.minicolumns.keys())",
        "    ",
        "    ",
        "    for term, col in layer.minicolumns.items():",
        "    ",
        "    ",
        "        ",
        "            ",
        "        ",
        "    "
      ],
      "context_before": [
        "        next_term = id_to_term.get(next_id)",
        "        if next_term and next_term in layer.minicolumns:",
        "            current = layer.minicolumns[next_term]",
        "            walk.append(next_term)",
        "        else:",
        "            break",
        "    ",
        "    return walk",
        "",
        ""
      ],
      "context_after": [
        "    embeddings: Dict[str, List[float]] = {}",
        "    n = len(terms)",
        "    if n == 0:",
        "        return embeddings",
        "    term_to_idx = {t: i for i, t in enumerate(terms)}",
        "    adjacency: Dict[int, Dict[int, float]] = defaultdict(dict)",
        "    degrees = [0.0] * n",
        "        i = term_to_idx[term]",
        "        for neighbor_id, weight in col.lateral_connections.items():",
        "            neighbor = layer.get_by_id(neighbor_id)",
        "            if neighbor and neighbor.content in term_to_idx:",
        "                j = term_to_idx[neighbor.content]",
        "                adjacency[i][j] = weight",
        "                degrees[i] += weight",
        "    degrees = [d if d > 0 else 1.0 for d in degrees]",
        "    actual_dims = min(dimensions, n)",
        "    vectors = []",
        "    for d in range(actual_dims):",
        "        vec = [random.gauss(0, 1) for _ in range(n)]",
        "        for prev in vectors:",
        "            dot = sum(v * p for v, p in zip(vec, prev))",
        "            vec = [v - dot * p for v, p in zip(vec, prev)]",
        "        mag = math.sqrt(sum(v*v for v in vec)) + 1e-10",
        "        vec = [v / mag for v in vec]",
        "        for _ in range(iterations):",
        "            new_vec = [0.0] * n",
        "            for i in range(n):",
        "                for j, weight in adjacency[i].items():",
        "                    norm_weight = weight / math.sqrt(degrees[i] * degrees[j])",
        "                    new_vec[i] -= norm_weight * vec[j]",
        "                new_vec[i] += vec[i]",
        "            for prev in vectors:",
        "                dot = sum(v * p for v, p in zip(new_vec, prev))",
        "                new_vec = [v - dot * p for v, p in zip(new_vec, prev)]",
        "            mag = math.sqrt(sum(v*v for v in new_vec)) + 1e-10",
        "            vec = [v / mag for v in new_vec]",
        "        vectors.append(vec)",
        "    for term in terms:",
        "        i = term_to_idx[term]",
        "        embeddings[term] = [vectors[d][i] if d < len(vectors) else 0.0 for d in range(dimensions)]",
        "    ",
        "    return embeddings",
        "",
        "",
        "def embedding_similarity(embeddings: Dict[str, List[float]], term1: str, term2: str) -> float:",
        "    \"\"\"Compute cosine similarity between two term embeddings.\"\"\"",
        "    if term1 not in embeddings or term2 not in embeddings:"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/processor.py",
      "function": "class CorticalTextProcessor:",
      "start_line": 1349,
      "lines_added": [
        "    def compute_graph_embeddings(",
        "        self,",
        "        dimensions: int = 64,",
        "        method: str = 'fast',",
        "        max_terms: Optional[int] = None,",
        "        verbose: bool = True",
        "    ) -> Dict:",
        "        \"\"\"",
        "        Compute graph embeddings for tokens.",
        "",
        "        Args:",
        "            dimensions: Number of embedding dimensions (default 64)",
        "            method: Embedding method - 'fast', 'adjacency', 'random_walk', or 'spectral'",
        "                   'fast' is recommended for large corpora (>3000 tokens)",
        "            max_terms: Maximum number of terms to embed (by PageRank).",
        "                      If None, auto-selects based on corpus size:",
        "                      - <2000 tokens: embed all",
        "                      - 2000-5000 tokens: embed top 1500",
        "                      - >5000 tokens: embed top 1000",
        "            verbose: Print progress messages",
        "",
        "        Returns:",
        "            Statistics dict with method, dimensions, terms_embedded",
        "        \"\"\"",
        "        # Auto-select max_terms based on corpus size",
        "        token_count = self.layers[CorticalLayer.TOKENS].column_count()",
        "        if max_terms is None:",
        "            if token_count < 2000:",
        "                max_terms = None  # Embed all",
        "            elif token_count < 5000:",
        "                max_terms = 1500",
        "            else:",
        "                max_terms = 1000",
        "",
        "        self.embeddings, stats = emb_module.compute_graph_embeddings(",
        "            self.layers, dimensions, method, max_terms",
        "        )",
        "        if verbose:",
        "            sampled = stats.get('sampled', False)",
        "            sample_info = f\", sampled top {max_terms}\" if sampled else \"\"",
        "            print(f\"Computed {stats['terms_embedded']} embeddings ({method}{sample_info})\")",
        ""
      ],
      "lines_removed": [
        "    def compute_graph_embeddings(self, dimensions: int = 64, method: str = 'adjacency', verbose: bool = True) -> Dict:",
        "        self.embeddings, stats = emb_module.compute_graph_embeddings(self.layers, dimensions, method)",
        "        if verbose: print(f\"Computed {stats['terms_embedded']} embeddings ({method})\")",
        "    "
      ],
      "context_before": [
        "            >>> # Both inherit \"living\" from \"animal\", so similarity > 0",
        "        \"\"\"",
        "        if not self.semantic_relations:",
        "            return 0.0",
        "",
        "        # Compute inherited properties on the fly if needed",
        "        inherited = semantics.inherit_properties(self.semantic_relations)",
        "",
        "        return semantics.compute_property_similarity(term1, term2, inherited)",
        "    "
      ],
      "context_after": [
        "        return stats",
        "    def retrofit_embeddings(self, iterations: int = 10, alpha: float = 0.4, verbose: bool = True) -> Dict:",
        "        if not self.embeddings: self.compute_graph_embeddings(verbose=False)",
        "        if not self.semantic_relations: self.extract_corpus_semantics(verbose=False)",
        "        stats = semantics.retrofit_embeddings(self.embeddings, self.semantic_relations, iterations, alpha)",
        "        if verbose: print(f\"Retrofitted embeddings (moved {stats['total_movement']:.2f} total)\")",
        "        return stats",
        "    ",
        "    def embedding_similarity(self, term1: str, term2: str) -> float:",
        "        return emb_module.embedding_similarity(self.embeddings, term1, term2)",
        "    "
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/query/search.py",
      "function": "from ..code_concepts import get_related_terms",
      "start_line": 22,
      "lines_added": [
        "    use_semantic: bool = True,",
        "    doc_name_boost: float = 2.0",
        "        doc_name_boost: Multiplier for documents whose name matches query terms (default 2.0)"
      ],
      "lines_removed": [
        "    use_semantic: bool = True"
      ],
      "context_before": [
        "from .expansion import expand_query, get_expanded_query_terms",
        "",
        "",
        "def find_documents_for_query(",
        "    query_text: str,",
        "    layers: Dict[CorticalLayer, HierarchicalLayer],",
        "    tokenizer: Tokenizer,",
        "    top_n: int = 5,",
        "    use_expansion: bool = True,",
        "    semantic_relations: Optional[List[Tuple[str, str, str, float]]] = None,"
      ],
      "context_after": [
        ") -> List[Tuple[str, float]]:",
        "    \"\"\"",
        "    Find documents most relevant to a query using TF-IDF and optional expansion.",
        "",
        "    Args:",
        "        query_text: Search query",
        "        layers: Dictionary of layers",
        "        tokenizer: Tokenizer instance",
        "        top_n: Number of documents to return",
        "        use_expansion: Whether to expand query terms using lateral connections",
        "        semantic_relations: Optional list of semantic relations for expansion",
        "        use_semantic: Whether to use semantic relations for expansion (if available)",
        "",
        "    Returns:",
        "        List of (doc_id, score) tuples ranked by relevance",
        "    \"\"\"",
        "    layer0 = layers[CorticalLayer.TOKENS]",
        "",
        "    query_terms = get_expanded_query_terms(",
        "        query_text, layers, tokenizer,",
        "        use_expansion=use_expansion,",
        "        semantic_relations=semantic_relations,"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/query/search.py",
      "function": "def find_documents_for_query(",
      "start_line": 58,
      "lines_added": [
        "    # Boost documents whose name matches query terms",
        "    if doc_name_boost > 1.0:",
        "        query_tokens = set(tokenizer.tokenize(query_text))",
        "        for doc_id in doc_scores:",
        "            # Tokenize document ID (handle underscores as separators)",
        "            doc_name_tokens = set(tokenizer.tokenize(doc_id.replace('_', ' ')))",
        "            # Count how many query tokens appear in doc name",
        "            matches = len(query_tokens & doc_name_tokens)",
        "            if matches > 0:",
        "                # Boost proportional to match ratio",
        "                match_ratio = matches / len(query_tokens) if query_tokens else 0",
        "                boost = 1 + (doc_name_boost - 1) * match_ratio",
        "                doc_scores[doc_id] *= boost",
        "",
        "    use_code_concepts: bool = True,",
        "    doc_name_boost: float = 2.0",
        "        doc_name_boost: Multiplier for documents whose name matches query terms (default 2.0)",
        "    query_tokens = set(tokens)",
        ""
      ],
      "lines_removed": [
        "    use_code_concepts: bool = True"
      ],
      "context_before": [
        "    # Score each document",
        "    doc_scores: Dict[str, float] = defaultdict(float)",
        "",
        "    for term, term_weight in query_terms.items():",
        "        col = layer0.get_minicolumn(term)",
        "        if col:",
        "            for doc_id in col.document_ids:",
        "                tfidf = col.tfidf_per_doc.get(doc_id, col.tfidf)",
        "                doc_scores[doc_id] += tfidf * term_weight",
        ""
      ],
      "context_after": [
        "    sorted_docs = sorted(doc_scores.items(), key=lambda x: -x[1])",
        "    return sorted_docs[:top_n]",
        "",
        "",
        "def fast_find_documents(",
        "    query_text: str,",
        "    layers: Dict[CorticalLayer, HierarchicalLayer],",
        "    tokenizer: Tokenizer,",
        "    top_n: int = 5,",
        "    candidate_multiplier: int = 3,",
        ") -> List[Tuple[str, float]]:",
        "    \"\"\"",
        "    Fast document search using candidate filtering.",
        "",
        "    Optimizes search by:",
        "    1. Using set intersection to find candidate documents",
        "    2. Only scoring top candidates fully",
        "    3. Using code concept expansion for better recall",
        "",
        "    This is ~2-3x faster than full search on large corpora while",
        "    maintaining similar result quality.",
        "",
        "    Args:",
        "        query_text: Search query",
        "        layers: Dictionary of layers",
        "        tokenizer: Tokenizer instance",
        "        top_n: Number of results to return",
        "        candidate_multiplier: Multiplier for candidate set size",
        "        use_code_concepts: Whether to use code concept expansion",
        "",
        "    Returns:",
        "        List of (doc_id, score) tuples ranked by relevance",
        "    \"\"\"",
        "    layer0 = layers[CorticalLayer.TOKENS]",
        "",
        "    # Tokenize query",
        "    tokens = tokenizer.tokenize(query_text)",
        "    if not tokens:",
        "        return []",
        "",
        "    # Phase 1: Find candidate documents (fast set operations)",
        "    # Get documents containing ANY query term",
        "    candidate_docs: Dict[str, int] = defaultdict(int)  # doc_id -> match count",
        "",
        "    for token in tokens:",
        "        col = layer0.get_minicolumn(token)",
        "        if col:",
        "            for doc_id in col.document_ids:",
        "                candidate_docs[doc_id] += 1",
        ""
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/query/search.py",
      "function": "def fast_find_documents(",
      "start_line": 146,
      "lines_added": [
        "        score *= (1 + 0.5 * coverage_boost)",
        "",
        "        # Boost documents whose name matches query terms",
        "        if doc_name_boost > 1.0:",
        "            doc_name_tokens = set(tokenizer.tokenize(doc_id.replace('_', ' ')))",
        "            matches = len(query_tokens & doc_name_tokens)",
        "            if matches > 0:",
        "                match_ratio = matches / len(query_tokens)",
        "                boost = 1 + (doc_name_boost - 1) * match_ratio",
        "                score *= boost",
        "",
        "        doc_scores[doc_id] = score"
      ],
      "lines_removed": [
        "        doc_scores[doc_id] = score * (1 + 0.5 * coverage_boost)"
      ],
      "context_before": [
        "    for doc_id, match_count in top_candidates:",
        "        score = 0.0",
        "        for token in tokens:",
        "            col = layer0.get_minicolumn(token)",
        "            if col and doc_id in col.document_ids:",
        "                tfidf = col.tfidf_per_doc.get(doc_id, col.tfidf)",
        "                score += tfidf",
        "",
        "        # Boost by match coverage",
        "        coverage_boost = match_count / len(tokens)"
      ],
      "context_after": [
        "",
        "    # Return top results",
        "    sorted_docs = sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)",
        "    return sorted_docs[:top_n]",
        "",
        "",
        "def build_document_index(",
        "    layers: Dict[CorticalLayer, HierarchicalLayer]",
        ") -> Dict[str, Dict[str, float]]:",
        "    \"\"\""
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/tokenizer.py",
      "function": "from typing import List, Set, Optional, Dict, Tuple",
      "start_line": 17,
      "lines_added": [
        "# Very common code tokens that should be filtered from corpus analysis",
        "# when mixed text/code documents are present. These dominate PageRank/TF-IDF",
        "# due to appearing in almost every method/function.",
        "CODE_NOISE_TOKENS = frozenset({",
        "    # Python-specific",
        "    'self', 'cls', 'args', 'kwargs',",
        "    'def', 'class', 'return', 'pass',",
        "    'none', 'true', 'false',",
        "    'str', 'int', 'float', 'bool', 'list', 'dict', 'set', 'tuple',",
        "    'len', 'range', 'print', 'type', 'isinstance', 'hasattr',",
        "    # Test framework noise",
        "    'assertequal', 'asserttrue', 'assertfalse', 'assertnone',",
        "    'assertis', 'assertisnot', 'assertin', 'assertnotin',",
        "    'assertraises', 'setup', 'teardown', 'unittest',",
        "    # Common variable names that are too generic",
        "    'result', 'value', 'item', 'obj', 'data', 'func',",
        "})",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "# These appear in almost every Python method/function, so they add noise",
        "# rather than signal when expanding queries for code search",
        "CODE_EXPANSION_STOP_WORDS = frozenset({",
        "    'self', 'cls',              # Class method parameters",
        "    'args', 'kwargs',           # Variadic parameters",
        "    'none', 'true', 'false',    # Literals (too common)",
        "    'return', 'pass',           # Control flow (too common)",
        "    'def', 'class',             # Definitions (search for these explicitly)",
        "})",
        ""
      ],
      "context_after": [
        "",
        "# Programming keywords that should be preserved even if in stop words",
        "PROGRAMMING_KEYWORDS = frozenset({",
        "    'def', 'class', 'function', 'return', 'import', 'from', 'if', 'else',",
        "    'elif', 'for', 'while', 'try', 'except', 'finally', 'with', 'as',",
        "    'yield', 'async', 'await', 'lambda', 'pass', 'break', 'continue',",
        "    'raise', 'assert', 'global', 'nonlocal', 'del', 'true', 'false',",
        "    'none', 'null', 'void', 'int', 'str', 'float', 'bool', 'list',",
        "    'dict', 'set', 'tuple', 'self', 'cls', 'init', 'main', 'args',",
        "    'kwargs', 'super', 'property', 'staticmethod', 'classmethod',"
      ],
      "change_type": "add"
    },
    {
      "file": "cortical/tokenizer.py",
      "function": "class Tokenizer:",
      "start_line": 198,
      "lines_added": [
        "        split_identifiers: bool = False,",
        "        filter_code_noise: bool = False",
        "            filter_code_noise: If True, filter out common code tokens (self, def, etc.)",
        "                              that dominate PageRank/TF-IDF in mixed text/code corpora.",
        "        base_stop_words = stop_words if stop_words is not None else self.DEFAULT_STOP_WORDS",
        "        # Add code noise tokens to stop words if filtering is enabled",
        "        if filter_code_noise:",
        "            self.stop_words = base_stop_words | CODE_NOISE_TOKENS",
        "        else:",
        "            self.stop_words = base_stop_words",
        "        self.filter_code_noise = filter_code_noise"
      ],
      "lines_removed": [
        "        split_identifiers: bool = False",
        "        self.stop_words = stop_words if stop_words is not None else self.DEFAULT_STOP_WORDS"
      ],
      "context_before": [
        "        # Common adjectives (too generic)",
        "        'new', 'old', 'good', 'bad', 'great', 'small', 'large', 'big', 'long',",
        "        'high', 'low', 'right', 'left', 'possible', 'important', 'major',",
        "        'available', 'able', 'like', 'different', 'similar'",
        "    })",
        "    ",
        "    def __init__(",
        "        self,",
        "        stop_words: Optional[Set[str]] = None,",
        "        min_word_length: int = 3,"
      ],
      "context_after": [
        "    ):",
        "        \"\"\"",
        "        Initialize tokenizer.",
        "",
        "        Args:",
        "            stop_words: Set of words to filter out. Uses defaults if None.",
        "            min_word_length: Minimum word length to keep.",
        "            split_identifiers: If True, split camelCase/underscore_style and include",
        "                               both original and component tokens.",
        "        \"\"\"",
        "        self.min_word_length = min_word_length",
        "        self.split_identifiers = split_identifiers",
        "        ",
        "        # Simple suffix rules for stemming (Porter-lite)",
        "        self._suffix_rules = [",
        "            ('ational', 'ate'), ('tional', 'tion'), ('enci', 'ence'),",
        "            ('anci', 'ance'), ('izer', 'ize'), ('isation', 'ize'),",
        "            ('ization', 'ize'), ('ation', 'ate'), ('ator', 'ate'),",
        "            ('alism', 'al'), ('iveness', 'ive'), ('fulness', 'ful'),",
        "            ('ousness', 'ous'), ('aliti', 'al'), ('iviti', 'ive'),",
        "            ('biliti', 'ble'), ('ement', ''), ('ment', ''), ('ness', ''),",
        "            ('ling', ''), ('ing', ''), ('ies', 'y'), ('ied', 'y'),"
      ],
      "change_type": "modify"
    },
    {
      "file": "showcase.py",
      "function": "def render_bar(value: float, max_value: float, width: int = 30) -> str:",
      "start_line": 58,
      "lines_added": [
        "        # Use code noise filtering to exclude common Python keywords",
        "        # that pollute PageRank/TF-IDF in mixed text/code corpora",
        "        from cortical.tokenizer import Tokenizer",
        "        tokenizer = Tokenizer(filter_code_noise=True)",
        "        self.processor = CorticalTextProcessor(tokenizer=tokenizer)"
      ],
      "lines_removed": [
        "        self.processor = CorticalTextProcessor()"
      ],
      "context_before": [
        "        return \" \" * width",
        "    filled = int((value / max_value) * width)",
        "    return \"█\" * filled + \"░\" * (width - filled)",
        "",
        "",
        "class CorticalShowcase:",
        "    \"\"\"Showcases the cortical text processor with interesting analysis.\"\"\"",
        "",
        "    def __init__(self, samples_dir: str = \"samples\"):",
        "        self.samples_dir = samples_dir"
      ],
      "context_after": [
        "        self.loaded_files = []",
        "        self.timer = Timer()",
        "",
        "    def run(self):",
        "        \"\"\"Run the complete demo.\"\"\"",
        "        self.print_intro()",
        "",
        "        if not self.ingest_corpus():",
        "            print(\"No documents found!\")",
        "            return"
      ],
      "change_type": "modify"
    }
  ],
  "hour_of_day": 1,
  "day_of_week": "Friday",
  "seconds_since_last_commit": -303601,
  "is_merge": false,
  "is_initial": false,
  "parent_count": 1,
  "session_id": null,
  "related_chats": [],
  "ci_result": null,
  "reverted": false,
  "amended": false
}