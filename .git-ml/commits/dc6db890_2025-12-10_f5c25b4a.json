{
  "hash": "dc6db89035881b2a9eef9a6bba442d57ca5f1b76",
  "message": "Implement dog-fooding: search codebase with its own IR system (Task #47)",
  "author": "Claude",
  "timestamp": "2025-12-10 13:23:42 +0000",
  "branch": "claude/multi-index-design-DvifZ",
  "files_changed": [
    ".claude/skills/codebase-search/SKILL.md",
    ".claude/skills/corpus-indexer/SKILL.md",
    ".gitignore",
    "CLAUDE.md",
    "TASK_LIST.md",
    "scripts/index_codebase.py",
    "scripts/search_codebase.py"
  ],
  "insertions": 587,
  "deletions": 32,
  "hunks": [
    {
      "file": ".claude/skills/codebase-search/SKILL.md",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "---",
        "name: codebase-search",
        "description: Search the Cortical Text Processor codebase using semantic search. Use when looking for code patterns, understanding how features work, or finding relevant implementations. This skill uses the system's own IR algorithms to search its own codebase (dog-fooding).",
        "allowed-tools: Read, Bash, Glob",
        "---",
        "# Codebase Search Skill",
        "",
        "This skill enables semantic search over the Cortical Text Processor codebase using the system's own IR algorithms.",
        "",
        "## When to Use",
        "",
        "- Finding implementations of specific features",
        "- Understanding how algorithms work",
        "- Locating relevant code for modifications",
        "- Discovering related functions and classes",
        "- Exploring the codebase structure",
        "",
        "## Prerequisites",
        "",
        "Before using search, ensure the corpus is indexed:",
        "",
        "```bash",
        "python scripts/index_codebase.py",
        "```",
        "",
        "This creates `corpus_dev.pkl` with the indexed codebase.",
        "",
        "## Search Commands",
        "",
        "### Basic Search",
        "",
        "```bash",
        "python scripts/search_codebase.py \"your query here\"",
        "```",
        "",
        "### Options",
        "",
        "- `--top N` or `-n N`: Number of results (default: 5)",
        "- `--verbose` or `-v`: Show full passage text",
        "- `--expand` or `-e`: Show query expansion terms",
        "- `--interactive` or `-i`: Interactive search mode",
        "",
        "### Example Queries",
        "",
        "```bash",
        "# Find PageRank implementation",
        "python scripts/search_codebase.py \"PageRank algorithm implementation\"",
        "",
        "# Find bigram handling code",
        "python scripts/search_codebase.py \"bigram separator\" --verbose",
        "",
        "# Explore query expansion",
        "python scripts/search_codebase.py \"query expansion semantic\" --expand",
        "",
        "# Interactive exploration",
        "python scripts/search_codebase.py --interactive",
        "```",
        "",
        "## Understanding Results",
        "",
        "Results include:",
        "- **File:Line** reference (e.g., `cortical/analysis.py:127`)",
        "- **Score** indicating relevance (higher is better)",
        "- **Passage** showing relevant code or text",
        "",
        "## Tips",
        "",
        "1. Use natural language queries - the system understands concepts",
        "2. Check query expansion (`--expand`) to see related terms",
        "3. Start with broad queries, then refine",
        "4. Use interactive mode for exploration sessions"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": ".claude/skills/corpus-indexer/SKILL.md",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "---",
        "name: corpus-indexer",
        "description: Index or re-index the codebase for semantic search. Use after making significant code changes to keep the search corpus up-to-date.",
        "allowed-tools: Bash",
        "---",
        "# Corpus Indexer Skill",
        "",
        "This skill manages the codebase index used by the semantic search system.",
        "",
        "## When to Use",
        "",
        "- After adding new files to the codebase",
        "- After significant code changes",
        "- When search results seem outdated",
        "- To verify indexing statistics",
        "",
        "## Index the Codebase",
        "",
        "```bash",
        "python scripts/index_codebase.py",
        "```",
        "",
        "### Options",
        "",
        "- `--output FILE` or `-o FILE`: Custom output path (default: corpus_dev.pkl)",
        "- `--verbose` or `-v`: Show detailed indexing progress",
        "",
        "### Example",
        "",
        "```bash",
        "# Standard indexing",
        "python scripts/index_codebase.py",
        "",
        "# Verbose output to see what's being indexed",
        "python scripts/index_codebase.py --verbose",
        "",
        "# Custom output location",
        "python scripts/index_codebase.py --output my_corpus.pkl",
        "```",
        "",
        "## What Gets Indexed",
        "",
        "The indexer processes:",
        "- All Python files in `cortical/` (source code)",
        "- All Python files in `tests/` (test code)",
        "- Documentation: `CLAUDE.md`, `TASK_LIST.md`, `README.md`, `KNOWLEDGE_TRANSFER.md`",
        "",
        "## Output Statistics",
        "",
        "After indexing, you'll see:",
        "- Number of documents indexed",
        "- Total lines of code",
        "- Token count (unique terms)",
        "- Bigram count (word pairs)",
        "- Concept clusters",
        "- Semantic relations extracted",
        "",
        "## Maintenance",
        "",
        "Re-index periodically to keep search accurate:",
        "- After adding new modules",
        "- After major refactoring",
        "- Before deep codebase exploration sessions"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": ".gitignore",
      "function": "__pycache__/",
      "start_line": 3,
      "lines_added": [
        "# Generated corpus files",
        "corpus_dev.pkl",
        "*.pkl",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "*.py[cod]",
        "*$py.class",
        "*.so",
        ".Python",
        "*.egg-info/",
        ".eggs/",
        "dist/",
        "build/",
        ".pytest_cache/",
        ""
      ],
      "context_after": [
        "# Coverage",
        ".coverage",
        ".coverage.*",
        "coverage.xml",
        "htmlcov/",
        "",
        "## A streamlined .gitignore for modern .NET projects",
        "## including temporary files, build results, and",
        "## files generated by popular .NET tools. If you are",
        "## developing with Visual Studio, the VS .gitignore"
      ],
      "change_type": "add"
    },
    {
      "file": "CLAUDE.md",
      "function": "cortical/",
      "start_line": 52,
      "lines_added": [
        "### Fixed Bugs (2025-12-10)",
        "The bigram separator mismatch bugs in `query.py:1442-1468` and `analysis.py:927` have been **fixed**. Bigrams now correctly use space separators throughout the codebase."
      ],
      "lines_removed": [
        "### Known Bug (Unfixed)",
        "**Bigram separator mismatch in analogy completion** (`query.py:1442-1468`):",
        "```python",
        "# BUG: Uses underscore, but bigrams are stored with spaces",
        "ab_bigram = f\"{term_a}_{term_b}\"  # Wrong: \"neural_networks\"",
        "# Should be:",
        "ab_bigram = f\"{term_a} {term_b}\"  # Correct: \"neural networks\"",
        "```"
      ],
      "context_before": [
        "",
        "**Key data structures:**",
        "- `Minicolumn`: Core unit with `lateral_connections`, `typed_connections`, `feedforward_connections`, `feedback_connections`",
        "- `Edge`: Typed connection with `relation_type`, `weight`, `confidence`, `source`",
        "- `HierarchicalLayer`: Container with `minicolumns` dict and `_id_index` for O(1) lookups",
        "",
        "---",
        "",
        "## Critical Knowledge",
        ""
      ],
      "context_after": [
        "",
        "### Important Implementation Details",
        "",
        "1. **Bigrams use SPACE separators** (from `tokenizer.py:179`):",
        "   ```python",
        "   ' '.join(tokens[i:i+n])  # \"neural networks\", not \"neural_networks\"",
        "   ```",
        "",
        "2. **Global `col.tfidf` is NOT per-document TF-IDF** - it uses total corpus occurrence count. Use `col.tfidf_per_doc[doc_id]` for true per-document TF-IDF.",
        ""
      ],
      "change_type": "modify"
    },
    {
      "file": "CLAUDE.md",
      "function": "for t1, rel, t2, weight in processor.semantic_relations[:10]:",
      "start_line": 290,
      "lines_added": [
        "## Dog-Fooding: Search the Codebase",
        "",
        "The Cortical Text Processor can index and search its own codebase, providing semantic search capabilities during development.",
        "",
        "### Quick Start",
        "",
        "```bash",
        "# Index the codebase (creates corpus_dev.pkl)",
        "python scripts/index_codebase.py",
        "",
        "# Search for code",
        "python scripts/search_codebase.py \"PageRank algorithm\"",
        "python scripts/search_codebase.py \"bigram separator\" --verbose",
        "python scripts/search_codebase.py --interactive",
        "```",
        "",
        "### Claude Skills",
        "",
        "Two skills are available in `.claude/skills/`:",
        "",
        "1. **codebase-search**: Search the indexed codebase for code patterns and implementations",
        "2. **corpus-indexer**: Re-index the codebase after making changes",
        "",
        "### Search Options",
        "",
        "| Option | Description |",
        "|--------|-------------|",
        "| `--top N` | Number of results (default: 5) |",
        "| `--verbose` | Show full passage text |",
        "| `--expand` | Show query expansion terms |",
        "| `--interactive` | Interactive search mode |",
        "",
        "### Interactive Mode Commands",
        "",
        "| Command | Description |",
        "|---------|-------------|",
        "| `/expand <query>` | Show query expansion |",
        "| `/concepts` | List concept clusters |",
        "| `/stats` | Show corpus statistics |",
        "| `/quit` | Exit interactive mode |",
        "",
        "### Example Queries",
        "",
        "```bash",
        "# Find how PageRank is implemented",
        "python scripts/search_codebase.py \"compute pagerank damping factor\"",
        "",
        "# Find test patterns",
        "python scripts/search_codebase.py \"unittest setUp processor\"",
        "",
        "# Explore query expansion code",
        "python scripts/search_codebase.py \"expand query semantic lateral\"",
        "```",
        "",
        "---",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "| Build network | `processor.compute_all()` |",
        "| Search | `processor.find_documents_for_query(query)` |",
        "| RAG passages | `processor.find_passages_for_query(query)` |",
        "| Save state | `processor.save(\"corpus.pkl\")` |",
        "| Load state | `processor = CorticalTextProcessor.load(\"corpus.pkl\")` |",
        "| Run tests | `python -m unittest discover -s tests -v` |",
        "| Run showcase | `python showcase.py` |",
        "",
        "---",
        ""
      ],
      "context_after": [
        "## File Quick Links",
        "",
        "- **Main API**: `cortical/processor.py` - `CorticalTextProcessor` class",
        "- **Graph algorithms**: `cortical/analysis.py` - PageRank, TF-IDF, clustering",
        "- **Search**: `cortical/query.py` - query expansion, document retrieval",
        "- **Data structures**: `cortical/minicolumn.py` - `Minicolumn`, `Edge`",
        "- **Tests**: `tests/test_processor.py` - most comprehensive test file",
        "- **Demo**: `showcase.py` - interactive demonstration",
        "",
        "---"
      ],
      "change_type": "add"
    },
    {
      "file": "TASK_LIST.md",
      "function": "results = processor.complete_analogy(",
      "start_line": 1111,
      "lines_added": [
        "**Files:** New `scripts/index_codebase.py`, `scripts/search_codebase.py`, `.claude/skills/`",
        "**Status:** [x] Completed (2025-12-10)",
        "Use the Cortical Text Processor to index and search its own codebase during development.",
        "",
        "**Solution Applied:**",
        "1. Created `scripts/index_codebase.py`:",
        "   - Indexes all 19 Python files in `cortical/` and `tests/`",
        "   - Indexes 4 documentation files (CLAUDE.md, TASK_LIST.md, README.md, KNOWLEDGE_TRANSFER.md)",
        "   - Saves indexed corpus to `corpus_dev.pkl` (23 documents, ~15,600 lines)",
        "   - Computes semantic PageRank, TF-IDF, concepts, and semantic relations",
        "",
        "2. Created `scripts/search_codebase.py`:",
        "   - Loads indexed corpus and performs semantic search",
        "   - Returns file:line references for each result",
        "   - Supports `--top N`, `--verbose`, `--expand`, `--interactive` options",
        "   - Interactive mode with `/expand`, `/concepts`, `/stats`, `/quit` commands",
        "3. Created Claude Skills in `.claude/skills/`:",
        "   - `codebase-search/SKILL.md` - Search skill for finding code patterns",
        "   - `corpus-indexer/SKILL.md` - Indexing skill for updating corpus",
        "4. Updated `CLAUDE.md` with Dog-Fooding section documenting usage",
        "python scripts/search_codebase.py \"PageRank algorithm\" --top 3",
        "python scripts/search_codebase.py \"bigram separator\" --expand",
        "python scripts/search_codebase.py --interactive",
        "**Success Criteria:** All met",
        "- Passages include accurate file:line references (e.g., `cortical/analysis.py:127`)",
        "- Identified usability issue: return value order in find_passages_for_query (fixed)"
      ],
      "lines_removed": [
        "**Files:** New `scripts/index_codebase.py`, usage in development workflow",
        "**Status:** [ ] Not Started",
        "Use the Cortical Text Processor to index and search its own codebase during development. This validates the system in a real-world scenario and identifies usability issues.",
        "**Implementation:**",
        "1. Create `scripts/index_codebase.py` that:",
        "   - Indexes all `.py` files in `cortical/` and `tests/`",
        "   - Indexes `CLAUDE.md`, `TASK_LIST.md`, and `README.md`",
        "   - Saves the indexed corpus to `corpus_dev.pkl`",
        "2. Create `scripts/search_codebase.py` for interactive search:",
        "   - Load the indexed corpus",
        "   - Accept query from command line",
        "   - Return relevant code passages with file:line references",
        "# Index the codebase",
        "",
        "# Search for code patterns",
        "python scripts/search_codebase.py \"how does PageRank work\"",
        "python scripts/search_codebase.py \"bigram separator\"",
        "python scripts/search_codebase.py \"find documents for query\"",
        "**Success Criteria:**",
        "- Passages include accurate file:line references",
        "- Identifies any edge cases or usability issues"
      ],
      "context_before": [
        "---",
        "",
        "## Actionable Tasks (2025-12-10)",
        "",
        "The following tasks were identified during comprehensive code review and are prioritized for implementation:",
        "",
        "---",
        "",
        "### 47. Dog-Food the System During Development",
        ""
      ],
      "context_after": [
        "**Priority:** High",
        "",
        "**Goal:**",
        "",
        "",
        "",
        "**Example Usage:**",
        "```bash",
        "python scripts/index_codebase.py",
        "```",
        "",
        "- Can find relevant code when searching for concepts",
        "- System handles its own codebase without errors",
        "",
        "---",
        "",
        "### 37. Create Dedicated Query Module Tests",
        "",
        "**File:** `tests/test_query.py` (new file)",
        "**Status:** [x] Completed (2025-12-10)",
        "**Priority:** High",
        "",
        "**Problem:**"
      ],
      "change_type": "modify"
    },
    {
      "file": "TASK_LIST.md",
      "function": "Currently these are always 0 due to the bug.",
      "start_line": 1627,
      "lines_added": [
        "| 47 | **High** | Dog-food the system during development | ✅ Completed | Validation |",
        "**Completed:** 4/13 tasks",
        "**High Priority Remaining:** 1 task"
      ],
      "lines_removed": [
        "| 47 | **High** | Dog-food the system during development | [ ] Not Started | Validation |",
        "**Completed:** 3/13 tasks",
        "**High Priority Remaining:** 2 tasks"
      ],
      "context_before": [
        "---",
        "",
        "---",
        "",
        "## New Task Summary (2025-12-10)",
        "",
        "| # | Priority | Task | Status | Category |",
        "|---|----------|------|--------|----------|",
        "| 34 | **Critical** | Fix bigram separator in analogy completion | ✅ Completed | Bug Fix |",
        "| 35 | **Critical** | Fix bigram separator in bigram connections | ✅ Completed | Bug Fix |"
      ],
      "context_after": [
        "| 37 | **High** | Create dedicated query module tests | ✅ Completed | Testing |",
        "| 38 | **High** | Add input validation to public API | [ ] Not Started | Code Quality |",
        "| 40 | Medium | Add parameter range validation | [ ] Not Started | Code Quality |",
        "| 41 | Medium | Create configuration dataclass | [ ] Not Started | Architecture |",
        "| 43 | Medium | Optimize chunk scoring performance | [ ] Not Started | Performance |",
        "| 45 | Medium | Add LRU cache for query results | [ ] Not Started | Performance |",
        "| 39 | Low | Move inline imports to module top | [ ] Not Started | Code Quality |",
        "| 42 | Low | Add simple query language support | [ ] Not Started | Feature |",
        "| 44 | Low | Remove deprecated feedforward_sources | [ ] Not Started | Cleanup |",
        "| 46 | Low | Standardize return types with dataclasses | [ ] Not Started | API |",
        "",
        "**Medium Priority Remaining:** 4 tasks",
        "**Low Priority Remaining:** 4 tasks",
        "",
        "**Total Tests:** 388 (all passing)",
        "",
        "---",
        "",
        "*Updated from comprehensive code review on 2025-12-10*"
      ],
      "change_type": "modify"
    },
    {
      "file": "scripts/index_codebase.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "#!/usr/bin/env python3",
        "\"\"\"",
        "Index the Cortical Text Processor codebase for dog-fooding.",
        "",
        "This script indexes all Python files and documentation to enable",
        "semantic search over the codebase using the Cortical Text Processor itself.",
        "",
        "Usage:",
        "    python scripts/index_codebase.py [--output corpus_dev.pkl]",
        "\"\"\"",
        "",
        "import argparse",
        "import os",
        "import sys",
        "from pathlib import Path",
        "",
        "# Add parent directory to path for imports",
        "sys.path.insert(0, str(Path(__file__).parent.parent))",
        "",
        "from cortical.processor import CorticalTextProcessor",
        "",
        "",
        "def get_python_files(base_path: Path) -> list:",
        "    \"\"\"Get all Python files in cortical/ and tests/ directories.\"\"\"",
        "    files = []",
        "    for directory in ['cortical', 'tests']:",
        "        dir_path = base_path / directory",
        "        if dir_path.exists():",
        "            for py_file in dir_path.rglob('*.py'):",
        "                if not py_file.name.startswith('__'):",
        "                    files.append(py_file)",
        "    return sorted(files)",
        "",
        "",
        "def get_doc_files(base_path: Path) -> list:",
        "    \"\"\"Get documentation files.\"\"\"",
        "    doc_files = ['CLAUDE.md', 'TASK_LIST.md', 'README.md', 'KNOWLEDGE_TRANSFER.md']",
        "    files = []",
        "    for doc in doc_files:",
        "        doc_path = base_path / doc",
        "        if doc_path.exists():",
        "            files.append(doc_path)",
        "    return files",
        "",
        "",
        "def create_doc_id(file_path: Path, base_path: Path) -> str:",
        "    \"\"\"Create a document ID from file path.\"\"\"",
        "    rel_path = file_path.relative_to(base_path)",
        "    return str(rel_path)",
        "",
        "",
        "def index_file(processor: CorticalTextProcessor, file_path: Path, base_path: Path) -> dict:",
        "    \"\"\"Index a single file with line number metadata.\"\"\"",
        "    doc_id = create_doc_id(file_path, base_path)",
        "",
        "    try:",
        "        content = file_path.read_text(encoding='utf-8')",
        "    except Exception as e:",
        "        print(f\"  Warning: Could not read {doc_id}: {e}\")",
        "        return None",
        "",
        "    # Create metadata with file info",
        "    metadata = {",
        "        'file_path': str(file_path),",
        "        'relative_path': doc_id,",
        "        'file_type': file_path.suffix,",
        "        'line_count': content.count('\\n') + 1,",
        "    }",
        "",
        "    # For Python files, extract additional metadata",
        "    if file_path.suffix == '.py':",
        "        metadata['language'] = 'python'",
        "        # Count functions and classes",
        "        metadata['function_count'] = content.count('\\ndef ')",
        "        metadata['class_count'] = content.count('\\nclass ')",
        "",
        "    processor.process_document(doc_id, content, metadata=metadata)",
        "    return metadata",
        "",
        "",
        "def main():",
        "    parser = argparse.ArgumentParser(description='Index the codebase for semantic search')",
        "    parser.add_argument('--output', '-o', default='corpus_dev.pkl',",
        "                        help='Output file path (default: corpus_dev.pkl)')",
        "    parser.add_argument('--verbose', '-v', action='store_true',",
        "                        help='Show verbose output')",
        "    args = parser.parse_args()",
        "",
        "    base_path = Path(__file__).parent.parent",
        "    output_path = base_path / args.output",
        "",
        "    print(\"Cortical Text Processor - Codebase Indexer\")",
        "    print(\"=\" * 50)",
        "",
        "    # Initialize processor",
        "    processor = CorticalTextProcessor()",
        "",
        "    # Get files to index",
        "    python_files = get_python_files(base_path)",
        "    doc_files = get_doc_files(base_path)",
        "    all_files = python_files + doc_files",
        "",
        "    print(f\"\\nFound {len(python_files)} Python files and {len(doc_files)} documentation files\")",
        "",
        "    # Index all files",
        "    print(\"\\nIndexing files...\")",
        "    indexed = 0",
        "    total_lines = 0",
        "",
        "    for file_path in all_files:",
        "        if args.verbose:",
        "            print(f\"  Indexing: {create_doc_id(file_path, base_path)}\")",
        "",
        "        metadata = index_file(processor, file_path, base_path)",
        "        if metadata:",
        "            indexed += 1",
        "            total_lines += metadata.get('line_count', 0)",
        "",
        "    print(f\"  Indexed {indexed} files ({total_lines:,} total lines)\")",
        "",
        "    # Compute all analysis",
        "    print(\"\\nComputing analysis...\")",
        "    processor.compute_all(",
        "        build_concepts=True,",
        "        pagerank_method='semantic',",
        "        connection_strategy='hybrid',",
        "        verbose=args.verbose",
        "    )",
        "",
        "    # Extract semantic relations",
        "    print(\"Extracting semantic relations...\")",
        "    processor.extract_corpus_semantics(",
        "        use_pattern_extraction=True,",
        "        verbose=args.verbose",
        "    )",
        "",
        "    # Print statistics",
        "    print(\"\\nCorpus Statistics:\")",
        "    print(f\"  Documents: {len(processor.documents)}\")",
        "    print(f\"  Tokens (Layer 0): {processor.layers[0].column_count()}\")",
        "    print(f\"  Bigrams (Layer 1): {processor.layers[1].column_count()}\")",
        "    print(f\"  Concepts (Layer 2): {processor.layers[2].column_count()}\")",
        "    print(f\"  Semantic relations: {len(processor.semantic_relations)}\")",
        "",
        "    # Save corpus",
        "    print(f\"\\nSaving corpus to {output_path}...\")",
        "    processor.save(str(output_path))",
        "",
        "    file_size = output_path.stat().st_size / 1024",
        "    print(f\"  Saved ({file_size:.1f} KB)\")",
        "",
        "    print(\"\\nDone! Use search_codebase.py to query the indexed corpus.\")",
        "",
        "",
        "if __name__ == '__main__':",
        "    main()"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "scripts/search_codebase.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "#!/usr/bin/env python3",
        "\"\"\"",
        "Search the indexed codebase using Cortical Text Processor.",
        "",
        "This script provides semantic search over the codebase with file:line references.",
        "",
        "Usage:",
        "    python scripts/search_codebase.py \"how does PageRank work\"",
        "    python scripts/search_codebase.py \"bigram separator\" --top 10",
        "    python scripts/search_codebase.py --interactive",
        "\"\"\"",
        "",
        "import argparse",
        "import os",
        "import sys",
        "from pathlib import Path",
        "",
        "# Add parent directory to path for imports",
        "sys.path.insert(0, str(Path(__file__).parent.parent))",
        "",
        "from cortical.processor import CorticalTextProcessor",
        "",
        "",
        "def find_line_number(doc_content: str, passage_start: int) -> int:",
        "    \"\"\"Find the line number for a character position.\"\"\"",
        "    return doc_content[:passage_start].count('\\n') + 1",
        "",
        "",
        "def format_passage(passage: str, max_width: int = 80) -> str:",
        "    \"\"\"Format a passage for display, truncating long lines.\"\"\"",
        "    lines = passage.split('\\n')",
        "    formatted = []",
        "    for line in lines[:10]:  # Limit to 10 lines",
        "        if len(line) > max_width:",
        "            line = line[:max_width - 3] + '...'",
        "        formatted.append(line)",
        "    if len(lines) > 10:",
        "        formatted.append(f'  ... ({len(lines) - 10} more lines)')",
        "    return '\\n'.join(formatted)",
        "",
        "",
        "def search_codebase(processor: CorticalTextProcessor, query: str,",
        "                    top_n: int = 5, chunk_size: int = 400) -> list:",
        "    \"\"\"",
        "    Search the codebase and return results with file:line references.",
        "",
        "    Returns:",
        "        List of (file_path, line_number, passage, score) tuples",
        "    \"\"\"",
        "    results = processor.find_passages_for_query(",
        "        query,",
        "        top_n=top_n,",
        "        chunk_size=chunk_size,",
        "        overlap=100",
        "    )",
        "",
        "    formatted_results = []",
        "    for passage, doc_id, start, end, score in results:",
        "        # Get the full document content to find line number",
        "        doc_content = processor.documents.get(doc_id, '')",
        "        line_num = find_line_number(doc_content, start)",
        "",
        "        formatted_results.append({",
        "            'file': doc_id,",
        "            'line': line_num,",
        "            'passage': passage,",
        "            'score': score,",
        "            'reference': f\"{doc_id}:{line_num}\"",
        "        })",
        "",
        "    return formatted_results",
        "",
        "",
        "def display_results(results: list, verbose: bool = False):",
        "    \"\"\"Display search results.\"\"\"",
        "    if not results:",
        "        print(\"No results found.\")",
        "        return",
        "",
        "    print(f\"\\nFound {len(results)} relevant passages:\\n\")",
        "",
        "    for i, result in enumerate(results, 1):",
        "        print(\"=\" * 60)",
        "        print(f\"[{i}] {result['reference']}\")",
        "        print(f\"    Score: {result['score']:.3f}\")",
        "        print(\"-\" * 60)",
        "",
        "        if verbose:",
        "            print(format_passage(result['passage']))",
        "        else:",
        "            # Show first 5 lines",
        "            lines = result['passage'].split('\\n')[:5]",
        "            for line in lines:",
        "                if len(line) > 76:",
        "                    line = line[:73] + '...'",
        "                print(f\"  {line}\")",
        "            if len(result['passage'].split('\\n')) > 5:",
        "                print(f\"  ... ({len(result['passage'].split(chr(10))) - 5} more lines)\")",
        "        print()",
        "",
        "",
        "def expand_query_display(processor: CorticalTextProcessor, query: str):",
        "    \"\"\"Show expanded query terms.\"\"\"",
        "    expanded = processor.expand_query(query, max_expansions=10)",
        "    print(\"\\nQuery expansion:\")",
        "    for term, weight in sorted(expanded.items(), key=lambda x: -x[1])[:10]:",
        "        print(f\"  {term}: {weight:.3f}\")",
        "",
        "",
        "def interactive_mode(processor: CorticalTextProcessor):",
        "    \"\"\"Run interactive search mode.\"\"\"",
        "    print(\"\\nInteractive Search Mode\")",
        "    print(\"=\" * 40)",
        "    print(\"Commands:\")",
        "    print(\"  /expand <query>  - Show query expansion\")",
        "    print(\"  /concepts        - List concept clusters\")",
        "    print(\"  /stats           - Show corpus statistics\")",
        "    print(\"  /help            - Show this help\")",
        "    print(\"  /quit            - Exit\")",
        "    print()",
        "",
        "    while True:",
        "        try:",
        "            query = input(\"Search> \").strip()",
        "        except (EOFError, KeyboardInterrupt):",
        "            print(\"\\nGoodbye!\")",
        "            break",
        "",
        "        if not query:",
        "            continue",
        "",
        "        if query.startswith('/'):",
        "            cmd_parts = query.split(maxsplit=1)",
        "            cmd = cmd_parts[0].lower()",
        "",
        "            if cmd == '/quit' or cmd == '/exit':",
        "                print(\"Goodbye!\")",
        "                break",
        "            elif cmd == '/help':",
        "                print(\"Commands: /expand, /concepts, /stats, /quit\")",
        "            elif cmd == '/stats':",
        "                print(f\"\\nCorpus Statistics:\")",
        "                print(f\"  Documents: {len(processor.documents)}\")",
        "                print(f\"  Tokens: {processor.layers[0].column_count()}\")",
        "                print(f\"  Bigrams: {processor.layers[1].column_count()}\")",
        "                print(f\"  Concepts: {processor.layers[2].column_count()}\")",
        "                print(f\"  Relations: {len(processor.semantic_relations)}\")",
        "            elif cmd == '/expand' and len(cmd_parts) > 1:",
        "                expand_query_display(processor, cmd_parts[1])",
        "            elif cmd == '/concepts':",
        "                layer2 = processor.layers[2]",
        "                concepts = list(layer2.minicolumns.values())[:10]",
        "                print(f\"\\nTop concepts ({layer2.column_count()} total):\")",
        "                for c in concepts:",
        "                    print(f\"  {c.content[:50]}\")",
        "            else:",
        "                print(f\"Unknown command: {cmd}\")",
        "        else:",
        "            results = search_codebase(processor, query, top_n=5)",
        "            display_results(results, verbose=True)",
        "",
        "",
        "def main():",
        "    parser = argparse.ArgumentParser(description='Search the indexed codebase')",
        "    parser.add_argument('query', nargs='?', help='Search query')",
        "    parser.add_argument('--corpus', '-c', default='corpus_dev.pkl',",
        "                        help='Corpus file path (default: corpus_dev.pkl)')",
        "    parser.add_argument('--top', '-n', type=int, default=5,",
        "                        help='Number of results (default: 5)')",
        "    parser.add_argument('--verbose', '-v', action='store_true',",
        "                        help='Show full passage text')",
        "    parser.add_argument('--expand', '-e', action='store_true',",
        "                        help='Show query expansion')",
        "    parser.add_argument('--interactive', '-i', action='store_true',",
        "                        help='Interactive search mode')",
        "    args = parser.parse_args()",
        "",
        "    base_path = Path(__file__).parent.parent",
        "    corpus_path = base_path / args.corpus",
        "",
        "    # Check if corpus exists",
        "    if not corpus_path.exists():",
        "        print(f\"Error: Corpus file not found: {corpus_path}\")",
        "        print(\"Run 'python scripts/index_codebase.py' first to create it.\")",
        "        sys.exit(1)",
        "",
        "    # Load corpus",
        "    print(f\"Loading corpus from {corpus_path}...\")",
        "    processor = CorticalTextProcessor.load(str(corpus_path))",
        "    print(f\"Loaded {len(processor.documents)} documents\\n\")",
        "",
        "    if args.interactive:",
        "        interactive_mode(processor)",
        "    elif args.query:",
        "        if args.expand:",
        "            expand_query_display(processor, args.query)",
        "            print()",
        "",
        "        results = search_codebase(processor, args.query, top_n=args.top)",
        "        display_results(results, verbose=args.verbose)",
        "    else:",
        "        parser.print_help()",
        "",
        "",
        "if __name__ == '__main__':",
        "    main()"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    }
  ],
  "hour_of_day": 13,
  "day_of_week": "Wednesday",
  "seconds_since_last_commit": -433266,
  "is_merge": false,
  "is_initial": false,
  "parent_count": 1,
  "session_id": null,
  "related_chats": [],
  "ci_result": null,
  "reverted": false,
  "amended": false
}