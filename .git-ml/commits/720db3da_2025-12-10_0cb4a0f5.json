{
  "hash": "720db3dac40a7dca67eca2a7e00b6038ce507afe",
  "message": "Add comprehensive documentation for Claude agents",
  "author": "Claude",
  "timestamp": "2025-12-10 21:28:42 +0000",
  "branch": "claude/multi-index-design-DvifZ",
  "files_changed": [
    "docs/claude-usage.md",
    "docs/cookbook.md",
    "docs/query-guide.md"
  ],
  "insertions": 1753,
  "deletions": 0,
  "hunks": [
    {
      "file": "docs/claude-usage.md",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "# Claude Usage Guide: Semantic Search System",
        "",
        "This guide is written specifically for Claude (AI agents) to understand how to effectively use the Cortical Text Processor's semantic search system when working with this codebase.",
        "",
        "## Overview",
        "",
        "The Cortical Text Processor can index and semantically search its own codebase, providing meaning-based retrieval instead of simple keyword matching. This guide explains how to use this capability strategically during development tasks.",
        "",
        "**Key principle:** The system finds code by understanding intent and concepts, not just exact keywords. \"Fetch\", \"get\", \"load\", and \"retrieve\" are treated as semantically similar.",
        "",
        "---",
        "",
        "## Table of Contents",
        "",
        "1. [When to Use Codebase-Search](#when-to-use-codebase-search)",
        "2. [When to Use Direct File Reading](#when-to-use-direct-file-reading)",
        "3. [Formulating Effective Search Queries](#formulating-effective-search-queries)",
        "4. [Understanding Search Results](#understanding-search-results)",
        "5. [When to Re-Index](#when-to-re-index)",
        "6. [Handling No Results](#handling-no-results)",
        "7. [Iterative Search Strategy](#iterative-search-strategy)",
        "8. [Query Expansion Leverage](#query-expansion-leverage)",
        "9. [System Limitations and Workarounds](#system-limitations-and-workarounds)",
        "10. [Common Code Query Patterns](#common-code-query-patterns)",
        "11. [Performance Considerations](#performance-considerations)",
        "",
        "---",
        "",
        "## When to Use Codebase-Search",
        "",
        "Use the **codebase-search** skill when you need to:",
        "",
        "### 1. Find implementations of concepts",
        "```",
        "\"How does PageRank algorithm work?\"",
        "\"How is TF-IDF computed?\"",
        "\"How are bigrams created?\"",
        "```",
        "",
        "The system will find relevant code passages even if your exact words don't match the implementation. For example, searching for \"importance scoring\" will find PageRank code.",
        "",
        "### 2. Locate functionality by intent",
        "```",
        "\"Where do we handle errors?\"",
        "\"Where do we validate input?\"",
        "\"Where do we tokenize text?\"",
        "```",
        "",
        "Intent-based queries parse the natural language structure and find code implementing that action.",
        "",
        "### 3. Understand relationships between components",
        "```",
        "\"What connects to the tokenizer?\"",
        "\"How do layers interact?\"",
        "\"What uses layer 2 concepts?\"",
        "```",
        "",
        "The system understands component relationships through graph connections.",
        "",
        "### 4. Explore semantic concepts across the codebase",
        "```",
        "\"Neural network terminology\"",
        "\"Graph algorithms\"",
        "\"Performance optimization patterns\"",
        "```",
        "",
        "Query expansion automatically includes related terms, finding all discussions of a concept.",
        "",
        "### 5. When you need to understand code context",
        "You want to see how something is actually implemented, not just read the file directly. The search system gives you relevant passages in context.",
        "",
        "**Cost consideration:** Search is fast (~1 second for typical queries), so it's efficient for exploratory research.",
        "",
        "---",
        "",
        "## When to Use Direct File Reading",
        "",
        "Use **direct file reading** (Read tool) when you:",
        "",
        "### 1. Know the exact file location",
        "If you already know the file path (e.g., `cortical/processor.py`), reading directly is faster than searching.",
        "",
        "### 2. Need the complete file context",
        "When you need to see the entire file structure, imports, and all methods in a class, reading the file is more efficient than multiple targeted searches.",
        "",
        "### 3. Are implementing a pattern you've already found",
        "After a search tells you the file location, switch to direct reading to implement your changes.",
        "",
        "### 4. Need accurate line numbers for edits",
        "While search provides file:line references, reading the file confirms the exact content at those lines.",
        "",
        "### 5. The concept is very common",
        "If the concept appears frequently (like \"process\" or \"handle\"), search may return many results. Direct reading is faster when you know where to look.",
        "",
        "**Workflow:** Search → Find file → Read file → Implement",
        "",
        "---",
        "",
        "## Formulating Effective Search Queries",
        "",
        "### Query Structure",
        "",
        "The system parses queries into three components:",
        "",
        "1. **Question word** (optional): \"where\", \"how\", \"what\", \"why\" → affects intent",
        "2. **Action verb** (optional): \"handle\", \"process\", \"create\", \"validate\" → narrows scope",
        "3. **Subject**: The main concept you're searching for",
        "",
        "Examples:",
        "- \"where do we validate input?\" → Intent: location, Action: validate, Subject: input",
        "- \"how are bigrams created?\" → Intent: implementation, Action: create, Subject: bigrams",
        "- \"PageRank algorithm\" → Intent: general, Subject: PageRank algorithm",
        "",
        "### Writing Effective Queries",
        "",
        "**✓ DO:** Use natural language as you would ask a colleague",
        "```",
        "\"How does query expansion find related terms?\"",
        "\"Where do we compute document relevance?\"",
        "\"What's the structure of a minicolumn?\"",
        "```",
        "",
        "**✓ DO:** Include multiple related terms",
        "```",
        "\"PageRank importance scoring algorithm\" (better than just \"PageRank\")",
        "\"TF-IDF term weighting relevance\" (better than just \"TF-IDF\")",
        "```",
        "",
        "**✓ DO:** Use intent words",
        "```",
        "\"Find implementations of label propagation\"",
        "\"Locate the tokenizer code\"",
        "\"Show me how errors are handled\"",
        "```",
        "",
        "**✗ DON'T:** Use only exact technical names without context",
        "```",
        "\"L0\" (too abstract - use \"token layer\" instead)",
        "\"col\" (use \"minicolumn\" or \"column\")",
        "```",
        "",
        "**✗ DON'T:** Use implementation details you're not sure about",
        "```",
        "\"Use lateral_connections\" (search for the concept instead: \"related terms\")",
        "\"_id_index lookup\" (search for: \"ID lookup performance\")",
        "```",
        "",
        "**✗ DON'T:** Search for very common words alone",
        "```",
        "\"the\" or \"and\" (these appear everywhere)",
        "\"layer\" (almost every file mentions layers - add context: \"layer connections\")",
        "```",
        "",
        "### Query Length",
        "",
        "- **Short queries (1-3 words):** Fast, but may return many results",
        "  - Good for: \"PageRank\", \"stemming\", \"TF-IDF\"",
        "  - Problem: High recall, may need filtering",
        "",
        "- **Medium queries (4-6 words):** Optimal for most cases",
        "  - Good for: \"how bigrams are created\", \"Layer 0 token structure\"",
        "  - Sweet spot for precision and recall",
        "",
        "- **Long queries (7+ words):** Very specific, low recall",
        "  - Good for: Complete question phrases",
        "  - Problem: May miss results if wording doesn't match docs",
        "",
        "**Best practice:** Start with 4-5 word queries; adjust based on results.",
        "",
        "---",
        "",
        "## Understanding Search Results",
        "",
        "### Result Format",
        "",
        "Each result shows:",
        "",
        "```",
        "[N] cortical/processor.py:1265",
        "    Score: 0.847",
        "  - Passage text showing relevant code",
        "  - Up to 5 lines displayed by default",
        "```",
        "",
        "### Score Interpretation",
        "",
        "Scores range from 0.0 to 1.0:",
        "",
        "| Score | Meaning | What to do |",
        "|-------|---------|-----------|",
        "| 0.9-1.0 | Excellent match | This is what you're looking for |",
        "| 0.75-0.89 | Strong match | Very relevant, likely useful |",
        "| 0.6-0.74 | Good match | Relevant but may need context |",
        "| 0.45-0.59 | Weak match | May be tangentially related |",
        "| <0.45 | Poor match | Likely noise, but sometimes useful |",
        "",
        "**Note:** Scores depend on query quality and corpus structure. A 0.75 for a common topic may be more relevant than a 0.95 for a niche query.",
        "",
        "### File:Line References",
        "",
        "The format `filename:linenumber` tells you:",
        "- Which file to examine",
        "- Approximately where to look (line number may be off by ±10 lines due to chunking)",
        "",
        "**Action:** When you get a file:line reference:",
        "1. Use Read tool on that file",
        "2. Look around the suggested line (±5 lines on each side)",
        "3. If not found, search again with different terms",
        "",
        "### Passage Text",
        "",
        "The system shows relevant passages of code in context:",
        "",
        "- **In brief mode** (default): First 5 lines of the passage",
        "- **In verbose mode** (`--verbose` flag): Up to 10 lines",
        "",
        "**Interpreting passages:**",
        "- Look for function definitions, class declarations, and key logic",
        "- Passages may be partial—read the full file for complete understanding",
        "- Comments in passages are usually significant (the system ranks them highly)",
        "",
        "---",
        "",
        "## When to Re-Index",
        "",
        "The semantic search uses a pre-built index (`corpus_dev.pkl`) created from your codebase. It's not real-time—it reflects the state when the index was last built.",
        "",
        "### Use corpus-indexer After:",
        "",
        "**1. You make code changes** (Most important)",
        "```",
        "- Add a new function",
        "- Modify algorithm logic",
        "- Change class structure",
        "- Add new documentation",
        "```",
        "",
        "**When:** Use `--incremental` flag for speed (1-2 seconds vs 2-3 seconds for full rebuild)",
        "```python",
        "# In your task: \"Use corpus-indexer with --incremental flag\"",
        "```",
        "",
        "**2. You add new files**",
        "The indexer automatically detects new files in `cortical/`, `tests/`, and `docs/`.",
        "",
        "**When:** After adding `new_feature.py` or `test_new_feature.py`",
        "",
        "**3. Major refactoring**",
        "If you restructure multiple files, use `--force` flag to ensure clean rebuild.",
        "",
        "### When Index Staleness Matters",
        "",
        "Search results won't reflect changes until re-indexing. This is fine for:",
        "- Reading old code",
        "- Understanding historical implementation",
        "- Learning the architecture",
        "",
        "This is problematic for:",
        "- Verifying your own changes are searchable",
        "- Finding newly added functionality",
        "- Debugging code you just wrote",
        "",
        "### Index Staleness Detection",
        "",
        "Before using search, check if the index is stale:",
        "",
        "```bash",
        "# Check what would change",
        "python scripts/index_codebase.py --status",
        "```",
        "",
        "If files changed since last index, results may be out of date.",
        "",
        "---",
        "",
        "## Handling No Results",
        "",
        "When a search returns no results, try these strategies in order:",
        "",
        "### Strategy 1: Broaden Your Query",
        "",
        "**Narrow query with no results:**",
        "```",
        "\"compute_semantic_pagerank with damping factor\"",
        "```",
        "",
        "**Broadened version:**",
        "```",
        "\"PageRank algorithm\"",
        "```",
        "",
        "**Action:** Remove specific implementation details and search for the concept.",
        "",
        "### Strategy 2: Use Synonym/Related Terms",
        "",
        "**Query with no results:**",
        "```",
        "\"fetch documents from corpus\"",
        "```",
        "",
        "**Synonym version:**",
        "```",
        "\"retrieve documents relevance\"",
        "```",
        "",
        "**Action:** Replace implementation-specific words with general synonyms.",
        "",
        "### Strategy 3: Search Different Layers",
        "",
        "**Technical terms not found:**",
        "```",
        "\"minicolumn lateral connection weight\"",
        "```",
        "",
        "**Higher-level concept:**",
        "```",
        "\"related terms word associations\"",
        "```",
        "",
        "**Action:** Describe the concept instead of the implementation.",
        "",
        "### Strategy 4: Check if Index Exists",
        "",
        "**Problem:** \"Error: Corpus file not found\"",
        "",
        "**Solution:**",
        "```bash",
        "python scripts/index_codebase.py",
        "```",
        "",
        "This creates `corpus_dev.pkl` (~2-3 seconds).",
        "",
        "### Strategy 5: Use Direct File Search",
        "",
        "If semantic search fails, fall back to:",
        "",
        "1. **Grep search** for exact keywords:",
        "   ```",
        "   grep -r \"function_name\" cortical/",
        "   ```",
        "",
        "2. **Direct file reading** if you know the likely file:",
        "   ```",
        "   Read cortical/analysis.py",
        "   ```",
        "",
        "### Strategy 6: Check Query Expansion",
        "",
        "Use `--expand` flag to see what the system is actually searching for:",
        "",
        "```bash",
        "python scripts/search_codebase.py \"your query\" --expand",
        "```",
        "",
        "This shows the expanded terms. If expansion is incorrect, try a different query.",
        "",
        "### Why No Results Happen",
        "",
        "1. **Concept doesn't exist in codebase** - You're asking for something that isn't implemented",
        "2. **Different terminology** - The codebase uses different words than you're using",
        "3. **Index is stale** - Recent changes haven't been indexed",
        "4. **Query too specific** - You're combining terms that don't co-occur",
        "5. **Implementation detail** - You're searching for internal variable names instead of the concept",
        "",
        "---",
        "",
        "## Iterative Search Strategy",
        "",
        "When researching a complex topic, use iterative searching:",
        "",
        "### Iteration 1: Broad Exploration",
        "```",
        "Query: \"PageRank\"",
        "Goal: Find where PageRank is implemented",
        "Action: Choose the most relevant result file",
        "Result: cortical/analysis.py:22",
        "```",
        "",
        "### Iteration 2: Find Related Components",
        "```",
        "Query: \"how does PageRank use connections\"",
        "Goal: Understand what PageRank operates on",
        "Action: Search results show \"lateral connections\" and \"weighted edges\"",
        "Result: Learn that PageRank uses graph structure",
        "```",
        "",
        "### Iteration 3: Understand Integration",
        "```",
        "Query: \"where is PageRank computed in processor\"",
        "Goal: Find where PageRank is called",
        "Action: Results show processor.py lines that trigger compute_pagerank",
        "Result: Understand when PageRank runs (after corpus changes)",
        "```",
        "",
        "### Iteration 4: Deep Dive",
        "```",
        "Query: \"PageRank damping factor convergence\"",
        "Goal: Understand algorithm parameters",
        "Action: Read the full analysis.py function",
        "Result: Understand implementation details",
        "```",
        "",
        "**Pattern:** Start broad → narrow down → deepen understanding → read full files",
        "",
        "---",
        "",
        "## Query Expansion Leverage",
        "",
        "The system automatically expands queries using:",
        "",
        "1. **Lateral connections** - Terms frequently appearing together",
        "2. **Concept clusters** - Semantic groupings",
        "3. **Word variants** - Plurals, stems, related forms",
        "4. **Code concepts** - Programming synonyms (get/fetch/load)",
        "",
        "### How to Leverage Expansion",
        "",
        "**1. Use umbrella terms**",
        "",
        "Rather than searching for specific functions:",
        "```",
        "# Instead of: \"expand_query\"",
        "# Search for: \"query expansion\"",
        "```",
        "",
        "The system will automatically find `expand_query`, `get_expanded_query_terms`, etc.",
        "",
        "**2. Use related terminology**",
        "",
        "Expansion finds connections:",
        "```",
        "\"authentication\" → also finds \"login\", \"credential\", \"token\", \"session\"",
        "\"fetch\" → also finds \"get\", \"load\", \"retrieve\", \"access\"",
        "```",
        "",
        "**3. Check what's actually being searched**",
        "",
        "Use `--expand` flag:",
        "```bash",
        "python scripts/search_codebase.py \"PageRank\" --expand",
        "```",
        "",
        "Output shows:",
        "```",
        "pagerank: 1.000",
        "importance: 0.847",
        "score: 0.812",
        "rank: 0.791",
        "...",
        "```",
        "",
        "These are the actual terms being searched.",
        "",
        "**4. Add expansion hints to queries**",
        "",
        "If expansion misses terms, add them explicitly:",
        "```",
        "# Instead of: \"PageRank\"",
        "# Try: \"PageRank importance scoring algorithm\"",
        "```",
        "",
        "Now expansion includes more related terms.",
        "",
        "### Expansion Limitations",
        "",
        "Expansion works well for:",
        "- Common terms (appear in many documents)",
        "- Concepts with multiple discussions",
        "- Well-connected terms in the knowledge graph",
        "",
        "Expansion works poorly for:",
        "- Rare specialized terms (appear in 1-2 documents)",
        "- Very new features (not yet well-connected)",
        "- Acronyms (expansion may not handle well)",
        "",
        "---",
        "",
        "## System Limitations and Workarounds",
        "",
        "### Limitation 1: Exact Matches Don't Always Score Highest",
        "",
        "**Problem:** When you search for a function name exactly, variations sometimes score higher.",
        "",
        "```",
        "Query: \"find_documents_for_query\"",
        "Top result: \"fast_find_documents\" (unrelated function)",
        "```",
        "",
        "**Reason:** The system ranks by relevance semantically, not by exact match.",
        "",
        "**Workaround:** Read the file you found or refine your query:",
        "```",
        "\"find_documents relevance scoring\"",
        "```",
        "",
        "### Limitation 2: Code Structure Queries May Miss Abstract Concepts",
        "",
        "**Problem:** Searching for the structure of a data type:",
        "```",
        "\"what fields does Minicolumn have\"",
        "```",
        "",
        "May not find the class definition as well as you'd hope.",
        "",
        "**Reason:** The definition doesn't discuss relationships; it just declares fields.",
        "",
        "**Workaround:** Search for the concept instead:",
        "```",
        "\"minicolumn structure representation\"",
        "```",
        "",
        "Or use direct file reading for data structure definitions:",
        "```",
        "Read cortical/minicolumn.py",
        "```",
        "",
        "### Limitation 3: Semantic Similarity Can Be Too Broad",
        "",
        "**Problem:** Searching for common concepts returns too many results:",
        "```",
        "Query: \"connection\"",
        "Result: Returns all mentions of \"connections\" (hundreds)",
        "```",
        "",
        "**Reason:** \"Connection\" is a core concept mentioned everywhere.",
        "",
        "**Workaround:** Be more specific:",
        "```",
        "\"lateral connections co-occurrence\"",
        "\"feedforward connections hierarchy\"",
        "```",
        "",
        "### Limitation 4: Fast Mode Only Returns Documents, Not Passages",
        "",
        "**Problem:** When using `--fast` flag, you only get file names, not specific passages.",
        "",
        "```bash",
        "python scripts/search_codebase.py \"PageRank\" --fast",
        "# Returns: cortical/analysis.py:1 (without specific passage)",
        "```",
        "",
        "**Reason:** Fast mode skips passage extraction for speed (~2-3x faster).",
        "",
        "**Workaround:** Use without `--fast` for specific passages, or read the file directly after getting the filename.",
        "",
        "### Limitation 5: Index Doesn't Cover Git History",
        "",
        "**Problem:** You can't search for how code looked before changes.",
        "",
        "**Reason:** The index is built from current files only.",
        "",
        "**Workaround:** Use git history for temporal queries:",
        "```bash",
        "git log -p cortical/query.py | grep \"function_name\"",
        "```",
        "",
        "### Limitation 6: Documentation May Be Outdated",
        "",
        "**Problem:** Docs in the index reflect what was written, not necessarily what code actually does.",
        "",
        "```",
        "Query: \"how layer computation works\"",
        "Result: May find outdated documentation",
        "```",
        "",
        "**Reason:** Docs and code can drift.",
        "",
        "**Workaround:** Verify by reading the actual code after finding relevant documentation.",
        "",
        "### Limitation 7: Very New Code May Not Be Discoverable",
        "",
        "**Problem:** Code you just wrote won't be found until re-indexing.",
        "",
        "**Workaround:** Re-index with `--incremental` after writing code:",
        "```bash",
        "python scripts/index_codebase.py --incremental",
        "```",
        "",
        "---",
        "",
        "## Common Code Query Patterns",
        "",
        "### Finding Algorithm Implementations",
        "",
        "**Goal:** Understand how a specific algorithm works",
        "",
        "```",
        "\"PageRank importance scoring\"",
        "\"TF-IDF term weighting\"",
        "\"label propagation clustering\"",
        "```",
        "",
        "**What to expect:** Functions implementing the algorithm, parameter documentation",
        "",
        "### Finding Bug Locations",
        "",
        "**Goal:** Locate where a bug might be",
        "",
        "```",
        "\"bigram separator space\" (if debugging bigram issues)",
        "\"layer ID index lookup\" (if debugging lookups)",
        "\"tokenizer stemming\" (if debugging tokenization)",
        "```",
        "",
        "**What to expect:** Code that handles the buggy component",
        "",
        "### Finding Integration Points",
        "",
        "**Goal:** Understand how components connect",
        "",
        "```",
        "\"where PageRank results used\"",
        "\"TF-IDF score returned\"",
        "\"minicolumn connected to layer\"",
        "```",
        "",
        "**What to expect:** Code that calls or uses the component",
        "",
        "### Finding Test Patterns",
        "",
        "**Goal:** Understand how to test a feature",
        "",
        "```",
        "\"test PageRank computation\"",
        "\"unittest layer structure\"",
        "\"assert results valid\"",
        "```",
        "",
        "**What to expect:** Test files showing testing patterns",
        "",
        "### Finding Performance Optimizations",
        "",
        "**Goal:** Understand efficiency strategies",
        "",
        "```",
        "\"fast search document only\"",
        "\"incremental indexing changes\"",
        "\"O(1) ID lookup cache\"",
        "```",
        "",
        "**What to expect:** Code with performance-related comments/optimization",
        "",
        "### Finding Data Structure Details",
        "",
        "**Goal:** Understand internal representations",
        "",
        "```",
        "\"minicolumn connections fields\"",
        "\"layer minicolumns dictionary\"",
        "\"document ID format\"",
        "```",
        "",
        "**What to expect:** Class definitions, docstrings explaining structure",
        "",
        "---",
        "",
        "## Performance Considerations",
        "",
        "### When to Use Each Search Method",
        "",
        "| Method | Speed | Use Case |",
        "|--------|-------|----------|",
        "| Normal search | 1-2s | Default, accurate passage extraction |",
        "| Fast search (`--fast`) | 0.2-0.5s | Need just documents, not passages |",
        "| Direct file read | <0.1s | Know exact file location |",
        "| Interactive mode | 0.5-1s per query | Exploratory research sessions |",
        "",
        "### Batching Queries",
        "",
        "If you have multiple searches, use interactive mode instead of multiple CLI calls:",
        "",
        "```bash",
        "python scripts/search_codebase.py --interactive",
        "# Then issue multiple queries in one session",
        "# More efficient than multiple command calls",
        "```",
        "",
        "### Caching Expansion",
        "",
        "If you're searching for related terms repeatedly:",
        "",
        "```python",
        "# In code, use:",
        "processor.expand_query_cached(query)",
        "```",
        "",
        "Instead of:",
        "```python",
        "processor.expand_query(query)",
        "```",
        "",
        "The cached version uses LRU cache for repeated queries.",
        "",
        "### Index Size Trade-offs",
        "",
        "**Fast mode (default):**",
        "- Smaller index (~30MB)",
        "- Faster indexing (2-3 seconds)",
        "- Fast search (0.5-1s)",
        "- No bigram connections, no concept analysis",
        "",
        "**Full analysis mode:**",
        "- Larger index (~100+MB)",
        "- Slow indexing (10+ minutes)",
        "- More comprehensive results",
        "- Use only when you need deep exploration",
        "",
        "For normal development: **Use fast mode**. Use `--full-analysis` only for research sessions.",
        "",
        "---",
        "",
        "## Decision Tree: How to Find Code",
        "",
        "```",
        "Do you know the exact file?",
        "├─ YES: Use Read tool directly",
        "└─ NO: Continue...",
        "",
        "Do you know what to search for?",
        "├─ YES: Use codebase-search with query",
        "└─ NO: Continue...",
        "",
        "Is it a well-known component?",
        "├─ YES: Search for the component name",
        "└─ NO: Continue...",
        "",
        "Can you describe what it does?",
        "├─ YES: Search for the concept/behavior",
        "└─ NO: Use grep or browse manually",
        "",
        "Is the search too slow?",
        "├─ YES: Use --fast flag or break into narrower queries",
        "└─ NO: Proceed normally",
        "",
        "Did you get results?",
        "├─ YES: Pick the best match, read full file",
        "└─ NO: Go to \"Handling No Results\" section",
        "```",
        "",
        "---",
        "",
        "## Summary for Claude",
        "",
        "When working with this codebase:",
        "",
        "1. **Start with search, not reading** - The semantic search is fast and gives you context",
        "2. **Use natural language queries** - Write queries as you would ask a colleague",
        "3. **Trust the expansion** - The system automatically finds related terms",
        "4. **Check scores, but don't over-interpret** - High scores are good, but context matters more",
        "5. **Re-index after changes** - Always use `--incremental` after making code changes",
        "6. **Fall back to direct reading** - Once you have a file:line reference, switch to Read",
        "7. **Broaden when stuck** - If search returns nothing, remove implementation details and try again",
        "8. **Use iterative refinement** - Start broad, then narrow based on what you learn",
        "",
        "The semantic search system is designed to accelerate your understanding of the codebase by making it searchable by meaning, not just keywords. Use it as your primary tool for exploration and learning.",
        "",
        "---",
        "",
        "*Last updated: 2025-12-10*",
        "*For the Cortical Text Processor codebase*"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "docs/cookbook.md",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "# Cortical Text Processor Cookbook",
        "",
        "A practical guide to common patterns and recipes for using the Cortical Text Processor effectively.",
        "",
        "---",
        "",
        "## Table of Contents",
        "",
        "1. [Document Processing Patterns](#document-processing-patterns)",
        "2. [Search Optimization Recipes](#search-optimization-recipes)",
        "3. [Corpus Maintenance Patterns](#corpus-maintenance-patterns)",
        "4. [Query Expansion Tuning](#query-expansion-tuning)",
        "5. [Clustering Configuration](#clustering-configuration)",
        "6. [Performance Optimization](#performance-optimization)",
        "7. [RAG Integration Patterns](#rag-integration-patterns)",
        "",
        "---",
        "",
        "## Document Processing Patterns",
        "",
        "### Recipe 1: Batch Processing (Recommended)",
        "",
        "**When to use:** Adding multiple documents at once (initial corpus loading, bulk imports).",
        "",
        "```python",
        "from cortical import CorticalTextProcessor",
        "",
        "processor = CorticalTextProcessor()",
        "",
        "# Prepare documents as list of (doc_id, content, metadata) tuples",
        "documents = [",
        "    (\"doc1\", \"Neural networks process information.\", {\"source\": \"book1\"}),",
        "    (\"doc2\", \"Deep learning enables pattern recognition.\", {\"source\": \"book1\"}),",
        "    (\"doc3\", \"Machine learning algorithms learn from data.\", {\"source\": \"book2\"}),",
        "]",
        "",
        "# Add all documents and recompute once",
        "stats = processor.add_documents_batch(",
        "    documents,",
        "    recompute='full',  # 'full', 'tfidf', or 'none'",
        "    verbose=True",
        ")",
        "",
        "print(f\"Added {stats['documents_added']} documents\")",
        "```",
        "",
        "**Expected outcome:**",
        "- Single recomputation pass instead of per-document recomputation",
        "- ~3-5x faster than calling `process_document()` in a loop",
        "",
        "**Recomputation options:**",
        "- `recompute='full'`: Slowest, most accurate (includes all graph algorithms)",
        "- `recompute='tfidf'`: Fast, good for search quality",
        "- `recompute='none'`: Fastest, but computations marked stale",
        "",
        "---",
        "",
        "### Recipe 2: Incremental Updates (Live Systems)",
        "",
        "**When to use:** Adding documents to an already-built corpus (RAG systems, streaming data).",
        "",
        "```python",
        "# Start with existing corpus",
        "processor = CorticalTextProcessor.load(\"corpus.pkl\")",
        "",
        "# Add new document without full recomputation",
        "processor.add_document_incremental(",
        "    \"new_doc\",",
        "    \"New document content.\",",
        "    metadata={\"timestamp\": \"2025-12-10\"},",
        "    recompute='tfidf'  # Only recompute TF-IDF for search quality",
        ")",
        "",
        "# Later: full recomputation when needed",
        "processor.recompute(level='full', verbose=True)",
        "```",
        "",
        "---",
        "",
        "### Recipe 3: Document Removal",
        "",
        "**When to use:** Delete outdated documents, remove duplicates.",
        "",
        "```python",
        "# Remove single document",
        "result = processor.remove_document(\"old_doc\", verbose=True)",
        "print(f\"Tokens affected: {result['tokens_affected']}\")",
        "",
        "# Remove multiple documents efficiently",
        "doc_ids_to_remove = [\"old_doc1\", \"old_doc2\", \"old_doc3\"]",
        "result = processor.remove_documents_batch(",
        "    doc_ids_to_remove,",
        "    recompute='tfidf',",
        "    verbose=True",
        ")",
        "```",
        "",
        "---",
        "",
        "## Search Optimization Recipes",
        "",
        "### Recipe 4: Choosing the Right Search Method",
        "",
        "**Decision tree:**",
        "",
        "```",
        "Searching repeatedly on same corpus?",
        "├─ YES → fast_find_documents() or build_search_index()",
        "└─ NO  → find_documents_for_query()",
        "",
        "Need text passages for RAG?",
        "├─ YES → find_passages_for_query()",
        "└─ NO  → find_documents_for_query()",
        "",
        "Large corpus (1000+ docs)?",
        "└─ YES → fast_find_documents() for ~2-3x speedup",
        "```",
        "",
        "---",
        "",
        "### Recipe 5: Fast Document Search",
        "",
        "**When to use:** Large corpora, need sub-100ms response time.",
        "",
        "```python",
        "# Fast search with candidate filtering",
        "results = processor.fast_find_documents(",
        "    \"neural networks\",",
        "    top_n=5,",
        "    candidate_multiplier=3,  # 5 * 3 = 15 candidates examined",
        "    use_code_concepts=True   # Enable for code search",
        ")",
        "",
        "for doc_id, score in results:",
        "    print(f\"{doc_id}: {score:.3f}\")",
        "```",
        "",
        "**Tuning `candidate_multiplier`:**",
        "- `1`: Aggressive (may miss relevant documents)",
        "- `3`: Balanced (recommended)",
        "- `5`: Conservative (slower but higher recall)",
        "",
        "---",
        "",
        "### Recipe 6: Pre-Built Search Index (Fastest)",
        "",
        "**When to use:** Repeated searching on stable corpus.",
        "",
        "```python",
        "# Build index once",
        "index = processor.build_search_index()",
        "",
        "# Use for fast searches",
        "queries = [\"neural networks\", \"machine learning\", \"deep learning\"]",
        "for query in queries:",
        "    results = processor.search_with_index(query, index, top_n=5)",
        "    print(f\"{query}: {len(results)} results\")",
        "```",
        "",
        "**Note:** Rebuild index after `add_documents_batch()` or `remove_document()`.",
        "",
        "---",
        "",
        "### Recipe 7: Passage Retrieval for RAG",
        "",
        "**When to use:** Building retrieval-augmented generation systems.",
        "",
        "```python",
        "results = processor.find_passages_for_query(",
        "    \"neural network training\",",
        "    top_n=5,",
        "    chunk_size=512,      # Characters per chunk",
        "    overlap=128,         # Overlap between chunks",
        "    use_expansion=True",
        ")",
        "",
        "# Results: (passage_text, doc_id, start_char, end_char, score)",
        "for passage, doc_id, start, end, score in results:",
        "    print(f\"[{doc_id}:{start}-{end}] Score: {score:.3f}\")",
        "    print(passage[:100] + \"...\")",
        "```",
        "",
        "**Chunk size tuning:**",
        "- `256`: Small, precise passages",
        "- `512`: Balanced (recommended)",
        "- `1024`: Large, more context",
        "",
        "---",
        "",
        "## Corpus Maintenance Patterns",
        "",
        "### Recipe 8: Detecting Stale Computations",
        "",
        "**When to use:** Understand what needs recomputation after changes.",
        "",
        "```python",
        "# Check what's stale",
        "stale = processor.get_stale_computations()",
        "print(f\"Stale: {stale}\")",
        "",
        "if 'tfidf' in stale:",
        "    print(\"TF-IDF scores are outdated - search quality affected\")",
        "    processor.compute_tfidf(verbose=True)",
        "",
        "if 'pagerank' in stale:",
        "    print(\"PageRank scores are outdated\")",
        "    processor.compute_importance(verbose=True)",
        "```",
        "",
        "---",
        "",
        "### Recipe 9: Save and Load Corpus",
        "",
        "**When to use:** Persist trained corpus for deployment.",
        "",
        "```python",
        "# Build and save",
        "processor = CorticalTextProcessor()",
        "processor.add_documents_batch(documents, recompute='full')",
        "processor.save(\"production_corpus.pkl\", verbose=True)",
        "",
        "# Load in production",
        "loaded = CorticalTextProcessor.load(\"production_corpus.pkl\")",
        "results = loaded.find_documents_for_query(\"query\")",
        "```",
        "",
        "---",
        "",
        "## Query Expansion Tuning",
        "",
        "### Recipe 10: Understanding Expansion",
        "",
        "```python",
        "# See what expansion adds",
        "expanded = processor.expand_query(\"neural\", max_expansions=10)",
        "",
        "print(\"Original term: neural\")",
        "print(\"Expanded with:\")",
        "for term, weight in sorted(expanded.items(), key=lambda x: -x[1]):",
        "    if term != \"neural\":",
        "        print(f\"  {term}: {weight:.3f}\")",
        "```",
        "",
        "**Expansion sources:**",
        "- **Lateral connections** (0.6x): Terms appearing near query term",
        "- **Concept membership** (0.4x): Terms in same semantic cluster",
        "- **Code concepts** (0.6x): Programming synonyms (get/fetch/load)",
        "",
        "---",
        "",
        "### Recipe 11: Tuning Expansion Parameters",
        "",
        "```python",
        "# Conservative expansion (higher precision)",
        "conservative = processor.expand_query(",
        "    \"neural networks\",",
        "    max_expansions=3,",
        "    use_variants=False",
        ")",
        "",
        "# Aggressive expansion (higher recall)",
        "aggressive = processor.expand_query(",
        "    \"neural networks\",",
        "    max_expansions=20,",
        "    use_variants=True,",
        "    use_code_concepts=True",
        ")",
        "",
        "# Balanced (recommended)",
        "balanced = processor.expand_query(",
        "    \"neural networks\",",
        "    max_expansions=10,",
        "    use_variants=True",
        ")",
        "```",
        "",
        "---",
        "",
        "### Recipe 12: Multi-Hop Expansion",
        "",
        "**When to use:** Discover distantly related terms through semantic relations.",
        "",
        "```python",
        "# Extract semantic relations first",
        "processor.extract_corpus_semantics()",
        "",
        "# Multi-hop expansion",
        "expanded = processor.expand_query_multihop(",
        "    \"neural\",",
        "    max_hops=2,         # Follow 2 relation hops",
        "    max_expansions=15,",
        "    decay_factor=0.5    # Weight decreases per hop",
        ")",
        "```",
        "",
        "---",
        "",
        "## Clustering Configuration",
        "",
        "### Recipe 13: Tuning Cluster Strictness",
        "",
        "```python",
        "# Strict clustering (more separate clusters)",
        "processor.compute_all(",
        "    build_concepts=True,",
        "    cluster_strictness=1.0,",
        "    bridge_weight=0.0",
        ")",
        "",
        "# Loose clustering (fewer, larger clusters)",
        "processor.compute_all(",
        "    build_concepts=True,",
        "    cluster_strictness=0.5,",
        "    bridge_weight=0.3",
        ")",
        "```",
        "",
        "**Strictness guide:**",
        "- `1.0`: Strict (more clusters, stronger topic separation)",
        "- `0.5`: Balanced (recommended)",
        "- `0.0`: Loose (fewer clusters, more topic mixing)",
        "",
        "**Bridge weight effects:**",
        "- `0.0`: No synthetic connections (isolated topics)",
        "- `0.1-0.3`: Light bridging (recommended)",
        "- `0.5+`: Strong bridging (may create spurious links)",
        "",
        "---",
        "",
        "## Performance Optimization",
        "",
        "### Recipe 14: Profiling Corpus Size",
        "",
        "```python",
        "summary = processor.get_corpus_summary()",
        "",
        "print(f\"Documents: {summary['documents']}\")",
        "print(f\"Total columns: {summary['total_columns']}\")",
        "print(f\"Layer breakdown:\")",
        "print(f\"  Tokens: {summary['layer_stats'].get(0, {}).get('minicolumns', 0)}\")",
        "print(f\"  Bigrams: {summary['layer_stats'].get(1, {}).get('minicolumns', 0)}\")",
        "",
        "# Optimization strategy",
        "if summary['documents'] < 100:",
        "    print(\"Small corpus: use standard methods\")",
        "elif summary['documents'] < 1000:",
        "    print(\"Medium corpus: consider fast_find_documents()\")",
        "else:",
        "    print(\"Large corpus: use search index\")",
        "```",
        "",
        "---",
        "",
        "### Recipe 15: Query Cache Management",
        "",
        "```python",
        "# Enable query caching",
        "processor.set_query_cache_size(100)",
        "",
        "# Cached expansion (instant for repeated queries)",
        "results1 = processor.expand_query_cached(\"neural networks\")",
        "results2 = processor.expand_query_cached(\"neural networks\")  # From cache",
        "",
        "# Clear cache when corpus changes",
        "processor.clear_query_cache()",
        "```",
        "",
        "---",
        "",
        "## RAG Integration Patterns",
        "",
        "### Recipe 16: Simple RAG Backend",
        "",
        "```python",
        "def rag_retrieve(processor, query: str, top_n: int = 5) -> str:",
        "    \"\"\"Retrieve context for RAG system.\"\"\"",
        "    passages = processor.find_passages_for_query(",
        "        query,",
        "        top_n=top_n,",
        "        chunk_size=512,",
        "        overlap=128",
        "    )",
        "",
        "    context = \"Context from knowledge base:\\n\\n\"",
        "    for passage, doc_id, _, _, score in passages:",
        "        context += f\"[{doc_id}] {passage}\\n\\n\"",
        "",
        "    return context",
        "",
        "# Use in RAG loop",
        "context = rag_retrieve(processor, \"How do neural networks learn?\")",
        "# Pass to LLM with question",
        "```",
        "",
        "---",
        "",
        "### Recipe 17: Multi-Stage RAG Ranking",
        "",
        "**When to use:** Maximum quality ranking combining multiple signals.",
        "",
        "```python",
        "results = processor.multi_stage_rank(",
        "    \"neural networks\",",
        "    top_n=5,",
        "    chunk_size=512,",
        "    concept_boost=0.3  # Weight for concept relevance",
        ")",
        "",
        "for passage, doc_id, start, end, score, stages in results:",
        "    print(f\"[{doc_id}] Final: {score:.3f}\")",
        "    print(f\"  Concept: {stages['concept_score']:.3f}\")",
        "    print(f\"  Document: {stages['doc_score']:.3f}\")",
        "    print(f\"  Passage: {stages['chunk_score']:.3f}\")",
        "```",
        "",
        "---",
        "",
        "## Quick Reference",
        "",
        "| Task | Best Method |",
        "|------|-------------|",
        "| Multiple documents | `add_documents_batch()` |",
        "| Incremental updates | `add_document_incremental()` |",
        "| Document removal | `remove_documents_batch()` |",
        "| General search | `find_documents_for_query()` |",
        "| Large corpus search | `fast_find_documents()` |",
        "| Repeated searches | `build_search_index()` |",
        "| RAG passages | `find_passages_for_query()` |",
        "| High-quality RAG | `multi_stage_rank()` |",
        "| Query debugging | `expand_query()` |",
        "| Intent search | `search_by_intent()` |",
        "",
        "---",
        "",
        "## Troubleshooting",
        "",
        "### No Results Found",
        "",
        "```python",
        "# Check if query terms exist",
        "layer0 = processor.get_layer(CorticalLayer.TOKENS)",
        "for term in processor.tokenizer.tokenize(query):",
        "    if not layer0.get_minicolumn(term):",
        "        print(f\"'{term}' not in corpus\")",
        "",
        "# Try with expansion",
        "results = processor.find_documents_for_query(query, use_expansion=True)",
        "```",
        "",
        "### Search is Slow",
        "",
        "```python",
        "# Use fast search",
        "results = processor.fast_find_documents(query, top_n=5)",
        "",
        "# Or build index",
        "index = processor.build_search_index()",
        "results = processor.search_with_index(query, index)",
        "```",
        "",
        "### Stale Results",
        "",
        "```python",
        "# Check and recompute",
        "stale = processor.get_stale_computations()",
        "if stale:",
        "    processor.recompute(level='full')",
        "```",
        "",
        "---",
        "",
        "*See also: [Query Guide](query-guide.md) for detailed query formulation, [Claude Usage Guide](claude-usage.md) for AI agent usage.*"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "docs/query-guide.md",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "# Query Guide",
        "",
        "A comprehensive guide to formulating effective search queries and understanding how the query system works internally.",
        "",
        "---",
        "",
        "## Table of Contents",
        "",
        "1. [How Queries Work Internally](#how-queries-work-internally)",
        "2. [Query Syntax and Patterns](#query-syntax-and-patterns)",
        "3. [Understanding Query Expansion](#understanding-query-expansion)",
        "4. [Single-Word vs Multi-Word Queries](#single-word-vs-multi-word-queries)",
        "5. [Code Patterns vs Concept Searches](#code-patterns-vs-concept-searches)",
        "6. [Intent-Based Queries](#intent-based-queries)",
        "7. [Interpreting Relevance Scores](#interpreting-relevance-scores)",
        "8. [When Queries Fail](#when-queries-fail)",
        "9. [Advanced Techniques](#advanced-techniques)",
        "",
        "---",
        "",
        "## How Queries Work Internally",
        "",
        "### The Query Pipeline",
        "",
        "When you submit a query, the system performs a multi-stage pipeline:",
        "",
        "```",
        "Query Text",
        "    |",
        "[1. Tokenization] -> Split into words, remove stop words",
        "    |",
        "[2. Term Matching] -> Look up terms in token layer",
        "    |",
        "[3. Expansion] -> Add related terms via lateral connections",
        "    |",
        "[4. Document Scoring] -> TF-IDF weighting",
        "    |",
        "[5. Ranking] -> Sort by relevance score",
        "    |",
        "Results",
        "```",
        "",
        "### Stage 1: Tokenization",
        "",
        "Your query is tokenized using the same rules as document processing:",
        "",
        "```python",
        "# \"neural networks process data\" becomes:",
        "[\"neural\", \"networks\", \"process\", \"data\"]",
        "",
        "# Stop words are removed: \"the\", \"a\", \"in\", \"of\", \"is\"",
        "# Short words (< 3 characters) are removed",
        "```",
        "",
        "**Key points:**",
        "- Tokenization is **case-insensitive**",
        "- Punctuation is removed",
        "- Words shorter than 3 characters are filtered",
        "",
        "### Stage 2: Term Matching",
        "",
        "The system looks up each query token in Layer 0:",
        "",
        "```",
        "Token         Found?   Status",
        "\"neural\"      YES      Exact match in corpus",
        "\"networks\"    YES      Exact match in corpus",
        "```",
        "",
        "If a token doesn't exist, the system tries **word variants**:",
        "- Stemmed versions",
        "- Plural forms",
        "- Common aliases",
        "",
        "### Stage 3: Expansion",
        "",
        "The query is expanded using three methods:",
        "",
        "**Method A: Lateral Connections (Default)**",
        "- Terms co-occurring with query terms",
        "- Weights: connection strength x neighbor PageRank x 0.6",
        "",
        "**Method B: Concept Clustering**",
        "- Terms in same semantic cluster",
        "- Weights: concept PageRank x member PageRank x 0.4",
        "",
        "**Method C: Code Concepts (Optional)**",
        "- Programming synonyms (get/fetch/load)",
        "- Only enabled with `use_code_concepts=True`",
        "",
        "### Stage 4: Document Scoring",
        "",
        "```",
        "doc_score = sum(term_weight x tfidf_per_doc)",
        "            for each term in expanded_query",
        "",
        "where:",
        "  term_weight = original terms: 1.0",
        "              = expanded terms: 0.3-0.8",
        "```",
        "",
        "---",
        "",
        "## Query Syntax and Patterns",
        "",
        "The system uses **simple, natural language-based syntax**. No special operators needed.",
        "",
        "### Basic Patterns",
        "",
        "| Pattern | Example | Effect |",
        "|---------|---------|--------|",
        "| **Single word** | `neural` | Search term and related concepts |",
        "| **Multi-word** | `neural networks` | All words must match (AND logic) |",
        "| **Question words** | `where authentication` | Intent-based search |",
        "| **Action verbs** | `how validate input` | Parse action + subject |",
        "",
        "### What Doesn't Work",
        "",
        "```python",
        "# NOT supported:",
        "\"neural\" OR \"learning\"         # No boolean operators",
        "\"neural*\"                      # No wildcards",
        "\"exact phrase match\"           # No phrase searching",
        "```",
        "",
        "### How Multi-Word Queries Work",
        "",
        "Multi-word queries use **AND logic** at document level:",
        "",
        "```",
        "Query: \"neural networks\"",
        "",
        "Step 1: Find docs with \"neural\"   -> [doc1, doc3, doc5]",
        "Step 2: Find docs with \"networks\" -> [doc1, doc3, doc6]",
        "Step 3: Intersection              -> [doc1, doc3]",
        "Step 4: Rank by combined score",
        "```",
        "",
        "---",
        "",
        "## Understanding Query Expansion",
        "",
        "Query expansion is **the core secret** to finding relevant results even when your query doesn't exactly match.",
        "",
        "### How Expansion Works",
        "",
        "Given query `\"fetch user\"`:",
        "",
        "```",
        "Original Terms (weight 1.0):",
        "  - fetch",
        "  - user",
        "",
        "Lateral Connection Expansion:",
        "",
        "Neighbors of \"fetch\":",
        "  - get: 0.45",
        "  - load: 0.42",
        "  - data: 0.38",
        "",
        "Neighbors of \"user\":",
        "  - profile: 0.52",
        "  - account: 0.48",
        "  - authenticate: 0.35",
        "",
        "Final Query Terms:",
        "{",
        "  \"fetch\": 1.0,        # Original",
        "  \"user\": 1.0,         # Original",
        "  \"get\": 0.45,         # Expansion",
        "  \"profile\": 0.52,     # Expansion",
        "  ...",
        "}",
        "```",
        "",
        "### Controlling Expansion",
        "",
        "```python",
        "# With lateral connections only",
        "results = processor.find_documents_for_query(",
        "    \"neural networks\",",
        "    use_expansion=True,",
        "    use_semantic=False",
        ")",
        "",
        "# No expansion (exact match)",
        "results = processor.find_documents_for_query(",
        "    \"neural networks\",",
        "    use_expansion=False",
        ")",
        "",
        "# Code-specific expansion",
        "results = processor.expand_query_for_code(\"fetch user credentials\")",
        "```",
        "",
        "### Debugging Expansion",
        "",
        "```python",
        "expanded = processor.expand_query(\"neural networks\", max_expansions=10)",
        "",
        "for term, weight in sorted(expanded.items(), key=lambda x: -x[1]):",
        "    print(f\"  {term}: {weight:.3f}\")",
        "```",
        "",
        "---",
        "",
        "## Single-Word vs Multi-Word Queries",
        "",
        "### Single-Word Queries",
        "",
        "**Advantages:**",
        "- Faster execution",
        "- Broader matching",
        "- Better for exploratory search",
        "",
        "**Disadvantages:**",
        "- May return less relevant results if term is ambiguous",
        "",
        "```python",
        "Query: \"learning\"",
        "# Finds all documents with \"learning\" and related terms",
        "```",
        "",
        "### Multi-Word Queries",
        "",
        "**Advantages:**",
        "- More specific results (AND logic)",
        "- Provides disambiguation context",
        "",
        "**Disadvantages:**",
        "- Harder to match (all terms must exist)",
        "",
        "```python",
        "Query: \"machine learning\"",
        "# Returns only docs with BOTH terms",
        "```",
        "",
        "### Strategy: Combining Both",
        "",
        "```python",
        "# Broad search first",
        "broad = processor.find_documents_for_query(\"learning\", top_n=20)",
        "",
        "# Narrow with multi-word",
        "narrow = processor.find_documents_for_query(\"machine learning\", top_n=5)",
        "",
        "# Use narrow if available, fall back to broad",
        "results = narrow if narrow else broad",
        "```",
        "",
        "---",
        "",
        "## Code Patterns vs Concept Searches",
        "",
        "### Concept Searches (General Text)",
        "",
        "Best for finding semantic topics:",
        "",
        "```python",
        "processor.find_documents_for_query(\"authentication\")",
        "processor.find_documents_for_query(\"neural networks\")",
        "```",
        "",
        "Uses:",
        "- Lateral connections",
        "- Concept clusters",
        "- Natural language semantics",
        "",
        "### Code Pattern Searches",
        "",
        "Best for finding implementations:",
        "",
        "```python",
        "processor.expand_query_for_code(\"get user credentials\")",
        "processor.expand_query_for_code(\"validate input\")",
        "```",
        "",
        "Uses:",
        "- Code concept groups (get/fetch/load)",
        "- Programming keywords",
        "- Identifier splitting",
        "",
        "### When to Use Each",
        "",
        "| Type | Use Case | Method |",
        "|------|----------|--------|",
        "| **Concept** | Find ideas, topics | `find_documents_for_query()` |",
        "| **Code** | Find implementations | `expand_query_for_code()` |",
        "| **Intent** | Find by action | `search_by_intent()` |",
        "| **Passage** | Find specific text | `find_passages_for_query()` |",
        "",
        "---",
        "",
        "## Intent-Based Queries",
        "",
        "Intent-based queries use **natural language patterns** to understand what you're looking for.",
        "",
        "### Supported Question Words",
        "",
        "| Word | Intent | Example |",
        "|------|--------|---------|",
        "| **where** | location | \"where do we handle authentication?\" |",
        "| **how** | implementation | \"how does validation work?\" |",
        "| **what** | definition | \"what is a concept cluster?\" |",
        "| **why** | rationale | \"why do we use PageRank?\" |",
        "| **when** | lifecycle | \"when do we compute TF-IDF?\" |",
        "",
        "### How Intent Parsing Works",
        "",
        "```",
        "Query: \"where do we handle authentication?\"",
        "",
        "Step 1: Detect \"where\" -> intent = \"location\"",
        "Step 2: Extract content words -> handle, authentication",
        "Step 3: Identify action verb -> \"handle\"",
        "Step 4: Identify subject -> \"authentication\"",
        "Step 5: Build expanded terms",
        "Step 6: Search with weighted terms",
        "```",
        "",
        "### Using Intent Queries",
        "",
        "```python",
        "results = processor.search_by_intent(\"where do we validate input?\", top_n=5)",
        "",
        "parsed = processor.parse_intent_query(\"how does PageRank work?\")",
        "# {",
        "#   'action': 'work',",
        "#   'subject': 'pagerank',",
        "#   'intent': 'implementation',",
        "#   'expanded_terms': ['work', 'pagerank', 'rank', ...]",
        "# }",
        "```",
        "",
        "---",
        "",
        "## Interpreting Relevance Scores",
        "",
        "### Score Meaning",
        "",
        "```",
        "Score > 0.80   Very relevant - high confidence match",
        "Score 0.50-0.80  Relevant - good match",
        "Score 0.25-0.50  Somewhat relevant - weak connection",
        "Score < 0.25   Marginally relevant",
        "```",
        "",
        "### How Scores Are Calculated",
        "",
        "```python",
        "# TF-IDF Score:",
        "tf_idf = (term_count_in_doc / total_terms) x log(total_docs / docs_with_term)",
        "",
        "# Query Score:",
        "doc_score = sum(term_weight x term_tfidf_per_doc)",
        "```",
        "",
        "### Factors Affecting Scores",
        "",
        "1. **Term Frequency (TF):** More occurrences = higher score",
        "2. **Inverse Document Frequency (IDF):** Rarer terms = higher weight",
        "3. **Query Term Weight:** Original (1.0) vs expansion (0.3-0.6)",
        "4. **Concept overlap:** Documents in same cluster score higher",
        "",
        "---",
        "",
        "## When Queries Fail",
        "",
        "### Problem 1: No Results Found",
        "",
        "**Diagnosis:**",
        "```python",
        "layer0 = processor.get_layer(CorticalLayer.TOKENS)",
        "for term in processor.tokenizer.tokenize(query):",
        "    if not layer0.get_minicolumn(term):",
        "        print(f\"{term}: NOT FOUND\")",
        "```",
        "",
        "**Solutions:**",
        "1. Try variant forms: `\"getUserData\"` -> `\"get user data\"`",
        "2. Enable code splitting in tokenizer",
        "3. Use related concepts instead",
        "",
        "### Problem 2: Wrong Documents Returned",
        "",
        "**Diagnosis:**",
        "```python",
        "expanded = processor.expand_query(\"authentication\")",
        "# Check for unexpected expansion terms",
        "```",
        "",
        "**Solutions:**",
        "1. Use multi-word queries for specificity",
        "2. Disable expansion: `use_expansion=False`",
        "3. Use intent-based search",
        "",
        "### Problem 3: Missing Relevant Documents",
        "",
        "**Solutions:**",
        "1. Enable semantic expansion:",
        "   ```python",
        "   processor.extract_corpus_semantics()",
        "   results = processor.find_documents_for_query(",
        "       query,",
        "       use_semantic=True",
        "   )",
        "   ```",
        "",
        "2. Use multi-hop expansion:",
        "   ```python",
        "   expanded = processor.expand_query_multihop(query, max_hops=2)",
        "   ```",
        "",
        "### Problem 4: Slow Queries",
        "",
        "**Solutions:**",
        "1. Use `fast_find_documents()`",
        "2. Pre-build search index",
        "3. Use narrower queries",
        "",
        "---",
        "",
        "## Advanced Techniques",
        "",
        "### Technique 1: Multi-Hop Expansion",
        "",
        "```python",
        "processor.extract_corpus_semantics()",
        "",
        "expanded = processor.expand_query_multihop(",
        "    \"neural\",",
        "    max_hops=2,",
        "    max_expansions=15",
        ")",
        "",
        "# Hop 0: neural",
        "# Hop 1: networks, learning, brain",
        "# Hop 2: deep (via learning), cortex (via brain)",
        "```",
        "",
        "### Technique 2: Passage Retrieval (RAG)",
        "",
        "```python",
        "results = processor.find_passages_for_query(",
        "    \"neural network training\",",
        "    top_n=5,",
        "    chunk_size=512,",
        "    overlap=128",
        ")",
        "",
        "for passage, doc_id, start, end, score in results:",
        "    print(f\"[{doc_id}:{start}-{end}] Score: {score:.3f}\")",
        "    print(passage)",
        "```",
        "",
        "### Technique 3: Multi-Stage Ranking",
        "",
        "```python",
        "results = processor.multi_stage_rank(",
        "    \"neural networks\",",
        "    top_n=5,",
        "    concept_boost=0.3",
        ")",
        "",
        "for passage, doc_id, start, end, score, stages in results:",
        "    print(f\"Concept: {stages['concept_score']:.3f}\")",
        "    print(f\"Document: {stages['doc_score']:.3f}\")",
        "    print(f\"Passage: {stages['chunk_score']:.3f}\")",
        "```",
        "",
        "### Technique 4: Batch Queries",
        "",
        "```python",
        "queries = [\"neural networks\", \"machine learning\", \"deep learning\"]",
        "results = processor.find_documents_batch(queries, top_n=5)",
        "# ~2-3x faster for multiple queries",
        "```",
        "",
        "---",
        "",
        "## Quick Reference",
        "",
        "### Common Methods",
        "",
        "```python",
        "# Basic search",
        "processor.find_documents_for_query(query, top_n=5)",
        "",
        "# Fast search (large corpora)",
        "processor.fast_find_documents(query, top_n=5)",
        "",
        "# Intent-based",
        "processor.search_by_intent(\"where do we...?\", top_n=5)",
        "",
        "# Passages (RAG)",
        "processor.find_passages_for_query(query, top_n=5, chunk_size=512)",
        "",
        "# Code-specific",
        "processor.expand_query_for_code(query)",
        "",
        "# Multi-hop",
        "processor.expand_query_multihop(query, max_hops=2)",
        "",
        "# Batch queries",
        "processor.find_documents_batch(queries, top_n=5)",
        "",
        "# Debug expansion",
        "processor.expand_query(query, max_expansions=10)",
        "```",
        "",
        "### Query Tips",
        "",
        "1. **Start simple** - Single keywords first",
        "2. **Add specificity** - Multi-word if needed",
        "3. **Use intent words** - \"where\", \"how\", \"what\"",
        "4. **Check expansion** - See what terms are added",
        "5. **Trust the system** - Expansion finds related terms",
        "",
        "---",
        "",
        "*For practical recipes, see [Cookbook](cookbook.md). For Claude-specific usage, see [Claude Usage Guide](claude-usage.md).*"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    }
  ],
  "hour_of_day": 21,
  "day_of_week": "Wednesday",
  "seconds_since_last_commit": -404166,
  "is_merge": false,
  "is_initial": false,
  "parent_count": 1,
  "session_id": null,
  "related_chats": [],
  "ci_result": null,
  "reverted": false,
  "amended": false
}