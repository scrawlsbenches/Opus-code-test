{
  "hash": "ad9223552cd866420b2cd3e96de3e93e204667fe",
  "message": "feat(#134): Add Protocol Buffers serialization for cross-language support",
  "author": "Claude",
  "timestamp": "2025-12-14 09:14:41 +0000",
  "branch": "claude/multi-index-design-DvifZ",
  "files_changed": [
    "cortical/persistence.py",
    "cortical/proto/__init__.py",
    "cortical/proto/schema.proto",
    "cortical/proto/schema_pb2.py",
    "cortical/proto/serialization.py",
    "requirements.txt",
    "tests/unit/test_protobuf_serialization.py"
  ],
  "insertions": 1399,
  "deletions": 40,
  "hunks": [
    {
      "file": "cortical/persistence.py",
      "function": "logger = logging.getLogger(__name__)",
      "start_line": 23,
      "lines_added": [
        "    verbose: bool = True,",
        "    format: str = 'pickle'",
        "        format: Serialization format ('pickle' or 'protobuf'). Default: 'pickle'",
        "",
        "    Raises:",
        "        ValueError: If format is not 'pickle' or 'protobuf'",
        "        ImportError: If format='protobuf' but protobuf package is not installed",
        "    if format not in ['pickle', 'protobuf']:",
        "        raise ValueError(f\"Invalid format '{format}'. Must be 'pickle' or 'protobuf'.\")",
        "",
        "    if format == 'pickle':",
        "        # Original pickle serialization",
        "        state = {",
        "            'version': '2.2',",
        "            'layers': {},",
        "            'documents': documents,",
        "            'document_metadata': document_metadata or {},",
        "            'embeddings': embeddings or {},",
        "            'semantic_relations': semantic_relations or [],",
        "            'metadata': metadata or {}",
        "        }",
        "        # Serialize layers",
        "        for layer_enum, layer in layers.items():",
        "            state['layers'][layer_enum.value] = layer.to_dict()",
        "",
        "        with open(filepath, 'wb') as f:",
        "            pickle.dump(state, f, protocol=pickle.HIGHEST_PROTOCOL)",
        "    elif format == 'protobuf':",
        "        # Protocol Buffers serialization",
        "        try:",
        "            from .proto.serialization import to_proto",
        "        except ImportError as e:",
        "            raise ImportError(",
        "                \"protobuf package is required for Protocol Buffers serialization. \"",
        "                \"Install it with: pip install protobuf\"",
        "            ) from e",
        "",
        "        proto_state = to_proto(",
        "            layers, documents, document_metadata,",
        "            embeddings, semantic_relations, metadata",
        "        )",
        "",
        "        with open(filepath, 'wb') as f:",
        "            f.write(proto_state.SerializeToString())",
        "        logger.info(f\"✓ Saved processor to {filepath} (format: {format})\")",
        "    verbose: bool = True,",
        "    format: Optional[str] = None",
        "        format: Serialization format ('pickle' or 'protobuf'). If None, auto-detect.",
        "        ValueError: If layer values are invalid (must be 0-3) or format is invalid",
        "        ImportError: If format='protobuf' but protobuf package is not installed",
        "    # Auto-detect format if not specified",
        "    if format is None:",
        "        # Try to detect based on file content",
        "        with open(filepath, 'rb') as f:",
        "            # Read first few bytes",
        "            header = f.read(16)",
        "            f.seek(0)",
        "",
        "            # Pickle files start with 0x80 (protocol marker)",
        "            # Protobuf files have different structure",
        "            if header[0:1] == b'\\x80':",
        "                format = 'pickle'",
        "            else:",
        "                # Try protobuf",
        "                format = 'protobuf'",
        "",
        "    if format not in ['pickle', 'protobuf']:",
        "        raise ValueError(f\"Invalid format '{format}'. Must be 'pickle' or 'protobuf'.\")",
        "",
        "    if format == 'pickle':",
        "        # Original pickle deserialization",
        "        with open(filepath, 'rb') as f:",
        "            state = pickle.load(f)",
        "",
        "        # Reconstruct layers",
        "        layers = {}",
        "        for level_value, layer_data in state.get('layers', {}).items():",
        "            # Validate layer value before creating enum",
        "            level_int = int(level_value)",
        "            if level_int not in [0, 1, 2, 3]:",
        "                raise ValueError(",
        "                    f\"Invalid layer value {level_int} in saved state. \"",
        "                    f\"Layer values must be 0-3 (TOKENS=0, BIGRAMS=1, CONCEPTS=2, DOCUMENTS=3).\"",
        "                )",
        "            layer = HierarchicalLayer.from_dict(layer_data)",
        "            layers[CorticalLayer(level_int)] = layer",
        "",
        "        documents = state.get('documents', {})",
        "        document_metadata = state.get('document_metadata', {})",
        "        embeddings = state.get('embeddings', {})",
        "        semantic_relations = state.get('semantic_relations', [])",
        "        metadata = state.get('metadata', {})",
        "",
        "    elif format == 'protobuf':",
        "        # Protocol Buffers deserialization",
        "        try:",
        "            from .proto.serialization import from_proto",
        "            from .proto import schema_pb2",
        "        except ImportError as e:",
        "            raise ImportError(",
        "                \"protobuf package is required for Protocol Buffers deserialization. \"",
        "                \"Install it with: pip install protobuf\"",
        "            ) from e",
        "",
        "        with open(filepath, 'rb') as f:",
        "            proto_bytes = f.read()",
        "",
        "        proto_state = schema_pb2.ProcessorState()",
        "        proto_state.ParseFromString(proto_bytes)",
        "",
        "        layers, documents, document_metadata, embeddings, semantic_relations, metadata = from_proto(proto_state)",
        "        logger.info(f\"✓ Loaded processor from {filepath} (format: {format})\")"
      ],
      "lines_removed": [
        "    verbose: bool = True",
        "    state = {",
        "        'version': '2.2',",
        "        'layers': {},",
        "        'documents': documents,",
        "        'document_metadata': document_metadata or {},",
        "        'embeddings': embeddings or {},",
        "        'semantic_relations': semantic_relations or [],",
        "        'metadata': metadata or {}",
        "    }",
        "    # Serialize layers",
        "    for layer_enum, layer in layers.items():",
        "        state['layers'][layer_enum.value] = layer.to_dict()",
        "    with open(filepath, 'wb') as f:",
        "        pickle.dump(state, f, protocol=pickle.HIGHEST_PROTOCOL)",
        "        logger.info(f\"✓ Saved processor to {filepath}\")",
        "    verbose: bool = True",
        "        ValueError: If layer values are invalid (must be 0-3)",
        "    with open(filepath, 'rb') as f:",
        "        state = pickle.load(f)",
        "",
        "    # Reconstruct layers",
        "    layers = {}",
        "    for level_value, layer_data in state.get('layers', {}).items():",
        "        # Validate layer value before creating enum",
        "        level_int = int(level_value)",
        "        if level_int not in [0, 1, 2, 3]:",
        "            raise ValueError(",
        "                f\"Invalid layer value {level_int} in saved state. \"",
        "                f\"Layer values must be 0-3 (TOKENS=0, BIGRAMS=1, CONCEPTS=2, DOCUMENTS=3).\"",
        "            )",
        "        layer = HierarchicalLayer.from_dict(layer_data)",
        "        layers[CorticalLayer(level_int)] = layer",
        "",
        "    documents = state.get('documents', {})",
        "    document_metadata = state.get('document_metadata', {})",
        "    embeddings = state.get('embeddings', {})",
        "    semantic_relations = state.get('semantic_relations', [])",
        "    metadata = state.get('metadata', {})",
        "        logger.info(f\"✓ Loaded processor from {filepath}\")"
      ],
      "context_before": [
        "",
        "",
        "def save_processor(",
        "    filepath: str,",
        "    layers: Dict[CorticalLayer, HierarchicalLayer],",
        "    documents: Dict[str, str],",
        "    document_metadata: Optional[Dict[str, Dict[str, Any]]] = None,",
        "    embeddings: Optional[Dict[str, list]] = None,",
        "    semantic_relations: Optional[list] = None,",
        "    metadata: Optional[Dict] = None,"
      ],
      "context_after": [
        ") -> None:",
        "    \"\"\"",
        "    Save processor state to a file.",
        "",
        "    Args:",
        "        filepath: Path to save file",
        "        layers: Dictionary of all layers",
        "        documents: Document collection",
        "        document_metadata: Per-document metadata (source, timestamp, etc.)",
        "        embeddings: Graph embeddings for terms (optional)",
        "        semantic_relations: Extracted semantic relations (optional)",
        "        metadata: Optional processor metadata (version, settings, etc.)",
        "        verbose: Print progress",
        "    \"\"\"",
        "",
        "",
        "",
        "    if verbose:",
        "        total_cols = sum(len(layer.minicolumns) for layer in layers.values())",
        "        total_conns = sum(layer.total_connections() for layer in layers.values())",
        "        logger.info(f\"  - {len(documents)} documents\")",
        "        logger.info(f\"  - {total_cols} minicolumns\")",
        "        logger.info(f\"  - {total_conns} connections\")",
        "        if embeddings:",
        "            logger.info(f\"  - {len(embeddings)} embeddings\")",
        "        if semantic_relations:",
        "            logger.info(f\"  - {len(semantic_relations)} semantic relations\")",
        "",
        "",
        "def load_processor(",
        "    filepath: str,",
        ") -> tuple:",
        "    \"\"\"",
        "    Load processor state from a file.",
        "",
        "    Args:",
        "        filepath: Path to saved file",
        "        verbose: Print progress",
        "",
        "    Returns:",
        "        Tuple of (layers, documents, document_metadata, embeddings, semantic_relations, metadata)",
        "",
        "    Raises:",
        "    \"\"\"",
        "",
        "    if verbose:",
        "        total_cols = sum(len(layer.minicolumns) for layer in layers.values())",
        "        total_conns = sum(layer.total_connections() for layer in layers.values())",
        "        logger.info(f\"  - {len(documents)} documents\")",
        "        logger.info(f\"  - {total_cols} minicolumns\")",
        "        logger.info(f\"  - {total_conns} connections\")",
        "        if embeddings:",
        "            logger.info(f\"  - {len(embeddings)} embeddings\")",
        "        if semantic_relations:",
        "            logger.info(f\"  - {len(semantic_relations)} semantic relations\")",
        "",
        "    return layers, documents, document_metadata, embeddings, semantic_relations, metadata",
        ""
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/proto/__init__.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "\"\"\"",
        "Protocol Buffers Serialization Module",
        "=====================================",
        "",
        "Provides Protocol Buffers serialization for cross-language corpus sharing.",
        "",
        "This module enables the Cortical Text Processor to serialize and deserialize",
        "its state using Protocol Buffers, allowing corpus data to be shared across",
        "different programming languages and platforms.",
        "",
        "Usage:",
        "    from cortical.proto.serialization import to_proto, from_proto",
        "",
        "    # Convert processor state to protobuf",
        "    proto_state = to_proto(layers, documents, document_metadata,",
        "                           embeddings, semantic_relations, metadata)",
        "",
        "    # Serialize to bytes",
        "    serialized = proto_state.SerializeToString()",
        "",
        "    # Deserialize from bytes",
        "    proto_state = ProcessorState()",
        "    proto_state.ParseFromString(serialized)",
        "",
        "    # Convert back to Python objects",
        "    state = from_proto(proto_state)",
        "\"\"\"",
        "",
        "try:",
        "    from .serialization import to_proto, from_proto",
        "    __all__ = ['to_proto', 'from_proto']",
        "except ImportError:",
        "    # Protobuf not installed - this is OK for core library functionality",
        "    __all__ = []"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "cortical/proto/schema.proto",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "syntax = \"proto3\";",
        "",
        "package cortical;",
        "",
        "// Edge represents a typed connection with metadata",
        "message Edge {",
        "  string target_id = 1;",
        "  double weight = 2;",
        "  string relation_type = 3;",
        "  double confidence = 4;",
        "  string source = 5;",
        "}",
        "",
        "// Minicolumn represents a cortical minicolumn",
        "message Minicolumn {",
        "  string id = 1;",
        "  string content = 2;",
        "  int32 layer = 3;",
        "  double activation = 4;",
        "  int32 occurrence_count = 5;",
        "  repeated string document_ids = 6;",
        "  map<string, double> lateral_connections = 7;",
        "  map<string, Edge> typed_connections = 8;",
        "  repeated string feedforward_sources = 9;",
        "  map<string, double> feedforward_connections = 10;",
        "  map<string, double> feedback_connections = 11;",
        "  double tfidf = 12;",
        "  map<string, double> tfidf_per_doc = 13;",
        "  double pagerank = 14;",
        "  optional int32 cluster_id = 15;",
        "  map<string, int32> doc_occurrence_counts = 16;",
        "  repeated string name_tokens = 17;",
        "}",
        "",
        "// HierarchicalLayer represents a layer in the cortical hierarchy",
        "message HierarchicalLayer {",
        "  int32 level = 1;",
        "  map<string, Minicolumn> minicolumns = 2;",
        "}",
        "",
        "// FloatList represents a list of floats (for embeddings)",
        "message FloatList {",
        "  repeated double values = 1;",
        "}",
        "",
        "// SemanticRelation represents a semantic relation tuple",
        "message SemanticRelation {",
        "  string term1 = 1;",
        "  string relation_type = 2;",
        "  string term2 = 3;",
        "  double weight = 4;",
        "}",
        "",
        "// AnyValue represents arbitrary metadata values",
        "message AnyValue {",
        "  oneof value {",
        "    string string_value = 1;",
        "    int64 int_value = 2;",
        "    double double_value = 3;",
        "    bool bool_value = 4;",
        "    AnyDict dict_value = 5;",
        "    AnyList list_value = 6;",
        "  }",
        "}",
        "",
        "// AnyDict represents arbitrary metadata dictionaries",
        "message AnyDict {",
        "  map<string, AnyValue> values = 1;",
        "}",
        "",
        "// AnyList represents arbitrary metadata lists",
        "message AnyList {",
        "  repeated AnyValue values = 1;",
        "}",
        "",
        "// ProcessorState represents the complete state of a CorticalTextProcessor",
        "message ProcessorState {",
        "  string version = 1;",
        "  map<int32, HierarchicalLayer> layers = 2;",
        "  map<string, string> documents = 3;",
        "  map<string, AnyDict> document_metadata = 4;",
        "  map<string, FloatList> embeddings = 5;",
        "  repeated SemanticRelation semantic_relations = 6;",
        "  AnyDict metadata = 7;",
        "}"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "cortical/proto/schema_pb2.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "# -*- coding: utf-8 -*-",
        "# Generated by the protocol buffer compiler.  DO NOT EDIT!",
        "# NO CHECKED-IN PROTOBUF GENCODE",
        "# source: cortical/proto/schema.proto",
        "# Protobuf Python Version: 6.31.1",
        "\"\"\"Generated protocol buffer code.\"\"\"",
        "from google.protobuf import descriptor as _descriptor",
        "from google.protobuf import descriptor_pool as _descriptor_pool",
        "from google.protobuf import runtime_version as _runtime_version",
        "from google.protobuf import symbol_database as _symbol_database",
        "from google.protobuf.internal import builder as _builder",
        "_runtime_version.ValidateProtobufRuntimeVersion(",
        "    _runtime_version.Domain.PUBLIC,",
        "    6,",
        "    31,",
        "    1,",
        "    '',",
        "    'cortical/proto/schema.proto'",
        ")",
        "# @@protoc_insertion_point(imports)",
        "",
        "_sym_db = _symbol_database.Default()",
        "",
        "",
        "",
        "",
        "DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\\n\\x1b\\x63ortical/proto/schema.proto\\x12\\x08\\x63ortical\\\"d\\n\\x04\\x45\\x64ge\\x12\\x11\\n\\ttarget_id\\x18\\x01 \\x01(\\t\\x12\\x0e\\n\\x06weight\\x18\\x02 \\x01(\\x01\\x12\\x15\\n\\rrelation_type\\x18\\x03 \\x01(\\t\\x12\\x12\\n\\nconfidence\\x18\\x04 \\x01(\\x01\\x12\\x0e\\n\\x06source\\x18\\x05 \\x01(\\t\\\"\\xa4\\x08\\n\\nMinicolumn\\x12\\n\\n\\x02id\\x18\\x01 \\x01(\\t\\x12\\x0f\\n\\x07\\x63ontent\\x18\\x02 \\x01(\\t\\x12\\r\\n\\x05layer\\x18\\x03 \\x01(\\x05\\x12\\x12\\n\\nactivation\\x18\\x04 \\x01(\\x01\\x12\\x18\\n\\x10occurrence_count\\x18\\x05 \\x01(\\x05\\x12\\x14\\n\\x0c\\x64ocument_ids\\x18\\x06 \\x03(\\t\\x12I\\n\\x13lateral_connections\\x18\\x07 \\x03(\\x0b\\x32,.cortical.Minicolumn.LateralConnectionsEntry\\x12\\x45\\n\\x11typed_connections\\x18\\x08 \\x03(\\x0b\\x32*.cortical.Minicolumn.TypedConnectionsEntry\\x12\\x1b\\n\\x13\\x66\\x65\\x65\\x64\\x66orward_sources\\x18\\t \\x03(\\t\\x12Q\\n\\x17\\x66\\x65\\x65\\x64\\x66orward_connections\\x18\\n \\x03(\\x0b\\x32\\x30.cortical.Minicolumn.FeedforwardConnectionsEntry\\x12K\\n\\x14\\x66\\x65\\x65\\x64\\x62\\x61\\x63k_connections\\x18\\x0b \\x03(\\x0b\\x32-.cortical.Minicolumn.FeedbackConnectionsEntry\\x12\\r\\n\\x05tfidf\\x18\\x0c \\x01(\\x01\\x12<\\n\\rtfidf_per_doc\\x18\\r \\x03(\\x0b\\x32%.cortical.Minicolumn.TfidfPerDocEntry\\x12\\x10\\n\\x08pagerank\\x18\\x0e \\x01(\\x01\\x12\\x17\\n\\ncluster_id\\x18\\x0f \\x01(\\x05H\\x00\\x88\\x01\\x01\\x12L\\n\\x15\\x64oc_occurrence_counts\\x18\\x10 \\x03(\\x0b\\x32-.cortical.Minicolumn.DocOccurrenceCountsEntry\\x12\\x13\\n\\x0bname_tokens\\x18\\x11 \\x03(\\t\\x1a\\x39\\n\\x17LateralConnectionsEntry\\x12\\x0b\\n\\x03key\\x18\\x01 \\x01(\\t\\x12\\r\\n\\x05value\\x18\\x02 \\x01(\\x01:\\x02\\x38\\x01\\x1aG\\n\\x15TypedConnectionsEntry\\x12\\x0b\\n\\x03key\\x18\\x01 \\x01(\\t\\x12\\x1d\\n\\x05value\\x18\\x02 \\x01(\\x0b\\x32\\x0e.cortical.Edge:\\x02\\x38\\x01\\x1a=\\n\\x1b\\x46\\x65\\x65\\x64\\x66orwardConnectionsEntry\\x12\\x0b\\n\\x03key\\x18\\x01 \\x01(\\t\\x12\\r\\n\\x05value\\x18\\x02 \\x01(\\x01:\\x02\\x38\\x01\\x1a:\\n\\x18\\x46\\x65\\x65\\x64\\x62\\x61\\x63kConnectionsEntry\\x12\\x0b\\n\\x03key\\x18\\x01 \\x01(\\t\\x12\\r\\n\\x05value\\x18\\x02 \\x01(\\x01:\\x02\\x38\\x01\\x1a\\x32\\n\\x10TfidfPerDocEntry\\x12\\x0b\\n\\x03key\\x18\\x01 \\x01(\\t\\x12\\r\\n\\x05value\\x18\\x02 \\x01(\\x01:\\x02\\x38\\x01\\x1a:\\n\\x18\\x44ocOccurrenceCountsEntry\\x12\\x0b\\n\\x03key\\x18\\x01 \\x01(\\t\\x12\\r\\n\\x05value\\x18\\x02 \\x01(\\x05:\\x02\\x38\\x01\\x42\\r\\n\\x0b_cluster_id\\\"\\xaf\\x01\\n\\x11HierarchicalLayer\\x12\\r\\n\\x05level\\x18\\x01 \\x01(\\x05\\x12\\x41\\n\\x0bminicolumns\\x18\\x02 \\x03(\\x0b\\x32,.cortical.HierarchicalLayer.MinicolumnsEntry\\x1aH\\n\\x10MinicolumnsEntry\\x12\\x0b\\n\\x03key\\x18\\x01 \\x01(\\t\\x12#\\n\\x05value\\x18\\x02 \\x01(\\x0b\\x32\\x14.cortical.Minicolumn:\\x02\\x38\\x01\\\"\\x1b\\n\\tFloatList\\x12\\x0e\\n\\x06values\\x18\\x01 \\x03(\\x01\\\"W\\n\\x10SemanticRelation\\x12\\r\\n\\x05term1\\x18\\x01 \\x01(\\t\\x12\\x15\\n\\rrelation_type\\x18\\x02 \\x01(\\t\\x12\\r\\n\\x05term2\\x18\\x03 \\x01(\\t\\x12\\x0e\\n\\x06weight\\x18\\x04 \\x01(\\x01\\\"\\xc0\\x01\\n\\x08\\x41nyValue\\x12\\x16\\n\\x0cstring_value\\x18\\x01 \\x01(\\tH\\x00\\x12\\x13\\n\\tint_value\\x18\\x02 \\x01(\\x03H\\x00\\x12\\x16\\n\\x0c\\x64ouble_value\\x18\\x03 \\x01(\\x01H\\x00\\x12\\x14\\n\\nbool_value\\x18\\x04 \\x01(\\x08H\\x00\\x12\\'\\n\\ndict_value\\x18\\x05 \\x01(\\x0b\\x32\\x11.cortical.AnyDictH\\x00\\x12\\'\\n\\nlist_value\\x18\\x06 \\x01(\\x0b\\x32\\x11.cortical.AnyListH\\x00\\x42\\x07\\n\\x05value\\\"{\\n\\x07\\x41nyDict\\x12-\\n\\x06values\\x18\\x01 \\x03(\\x0b\\x32\\x1d.cortical.AnyDict.ValuesEntry\\x1a\\x41\\n\\x0bValuesEntry\\x12\\x0b\\n\\x03key\\x18\\x01 \\x01(\\t\\x12!\\n\\x05value\\x18\\x02 \\x01(\\x0b\\x32\\x12.cortical.AnyValue:\\x02\\x38\\x01\\\"-\\n\\x07\\x41nyList\\x12\\\"\\n\\x06values\\x18\\x01 \\x03(\\x0b\\x32\\x12.cortical.AnyValue\\\"\\x8b\\x05\\n\\x0eProcessorState\\x12\\x0f\\n\\x07version\\x18\\x01 \\x01(\\t\\x12\\x34\\n\\x06layers\\x18\\x02 \\x03(\\x0b\\x32$.cortical.ProcessorState.LayersEntry\\x12:\\n\\tdocuments\\x18\\x03 \\x03(\\x0b\\x32\\'.cortical.ProcessorState.DocumentsEntry\\x12I\\n\\x11\\x64ocument_metadata\\x18\\x04 \\x03(\\x0b\\x32..cortical.ProcessorState.DocumentMetadataEntry\\x12<\\n\\nembeddings\\x18\\x05 \\x03(\\x0b\\x32(.cortical.ProcessorState.EmbeddingsEntry\\x12\\x36\\n\\x12semantic_relations\\x18\\x06 \\x03(\\x0b\\x32\\x1a.cortical.SemanticRelation\\x12#\\n\\x08metadata\\x18\\x07 \\x01(\\x0b\\x32\\x11.cortical.AnyDict\\x1aJ\\n\\x0bLayersEntry\\x12\\x0b\\n\\x03key\\x18\\x01 \\x01(\\x05\\x12*\\n\\x05value\\x18\\x02 \\x01(\\x0b\\x32\\x1b.cortical.HierarchicalLayer:\\x02\\x38\\x01\\x1a\\x30\\n\\x0e\\x44ocumentsEntry\\x12\\x0b\\n\\x03key\\x18\\x01 \\x01(\\t\\x12\\r\\n\\x05value\\x18\\x02 \\x01(\\t:\\x02\\x38\\x01\\x1aJ\\n\\x15\\x44ocumentMetadataEntry\\x12\\x0b\\n\\x03key\\x18\\x01 \\x01(\\t\\x12 \\n\\x05value\\x18\\x02 \\x01(\\x0b\\x32\\x11.cortical.AnyDict:\\x02\\x38\\x01\\x1a\\x46\\n\\x0f\\x45mbeddingsEntry\\x12\\x0b\\n\\x03key\\x18\\x01 \\x01(\\t\\x12\\\"\\n\\x05value\\x18\\x02 \\x01(\\x0b\\x32\\x13.cortical.FloatList:\\x02\\x38\\x01\\x62\\x06proto3')",
        "",
        "_globals = globals()",
        "_builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, _globals)",
        "_builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, 'cortical.proto.schema_pb2', _globals)",
        "if not _descriptor._USE_C_DESCRIPTORS:",
        "  DESCRIPTOR._loaded_options = None",
        "  _globals['_MINICOLUMN_LATERALCONNECTIONSENTRY']._loaded_options = None",
        "  _globals['_MINICOLUMN_LATERALCONNECTIONSENTRY']._serialized_options = b'8\\001'",
        "  _globals['_MINICOLUMN_TYPEDCONNECTIONSENTRY']._loaded_options = None",
        "  _globals['_MINICOLUMN_TYPEDCONNECTIONSENTRY']._serialized_options = b'8\\001'",
        "  _globals['_MINICOLUMN_FEEDFORWARDCONNECTIONSENTRY']._loaded_options = None",
        "  _globals['_MINICOLUMN_FEEDFORWARDCONNECTIONSENTRY']._serialized_options = b'8\\001'",
        "  _globals['_MINICOLUMN_FEEDBACKCONNECTIONSENTRY']._loaded_options = None",
        "  _globals['_MINICOLUMN_FEEDBACKCONNECTIONSENTRY']._serialized_options = b'8\\001'",
        "  _globals['_MINICOLUMN_TFIDFPERDOCENTRY']._loaded_options = None",
        "  _globals['_MINICOLUMN_TFIDFPERDOCENTRY']._serialized_options = b'8\\001'",
        "  _globals['_MINICOLUMN_DOCOCCURRENCECOUNTSENTRY']._loaded_options = None",
        "  _globals['_MINICOLUMN_DOCOCCURRENCECOUNTSENTRY']._serialized_options = b'8\\001'",
        "  _globals['_HIERARCHICALLAYER_MINICOLUMNSENTRY']._loaded_options = None",
        "  _globals['_HIERARCHICALLAYER_MINICOLUMNSENTRY']._serialized_options = b'8\\001'",
        "  _globals['_ANYDICT_VALUESENTRY']._loaded_options = None",
        "  _globals['_ANYDICT_VALUESENTRY']._serialized_options = b'8\\001'",
        "  _globals['_PROCESSORSTATE_LAYERSENTRY']._loaded_options = None",
        "  _globals['_PROCESSORSTATE_LAYERSENTRY']._serialized_options = b'8\\001'",
        "  _globals['_PROCESSORSTATE_DOCUMENTSENTRY']._loaded_options = None",
        "  _globals['_PROCESSORSTATE_DOCUMENTSENTRY']._serialized_options = b'8\\001'",
        "  _globals['_PROCESSORSTATE_DOCUMENTMETADATAENTRY']._loaded_options = None",
        "  _globals['_PROCESSORSTATE_DOCUMENTMETADATAENTRY']._serialized_options = b'8\\001'",
        "  _globals['_PROCESSORSTATE_EMBEDDINGSENTRY']._loaded_options = None",
        "  _globals['_PROCESSORSTATE_EMBEDDINGSENTRY']._serialized_options = b'8\\001'",
        "  _globals['_EDGE']._serialized_start=41",
        "  _globals['_EDGE']._serialized_end=141",
        "  _globals['_MINICOLUMN']._serialized_start=144",
        "  _globals['_MINICOLUMN']._serialized_end=1204",
        "  _globals['_MINICOLUMN_LATERALCONNECTIONSENTRY']._serialized_start=824",
        "  _globals['_MINICOLUMN_LATERALCONNECTIONSENTRY']._serialized_end=881",
        "  _globals['_MINICOLUMN_TYPEDCONNECTIONSENTRY']._serialized_start=883",
        "  _globals['_MINICOLUMN_TYPEDCONNECTIONSENTRY']._serialized_end=954",
        "  _globals['_MINICOLUMN_FEEDFORWARDCONNECTIONSENTRY']._serialized_start=956",
        "  _globals['_MINICOLUMN_FEEDFORWARDCONNECTIONSENTRY']._serialized_end=1017",
        "  _globals['_MINICOLUMN_FEEDBACKCONNECTIONSENTRY']._serialized_start=1019",
        "  _globals['_MINICOLUMN_FEEDBACKCONNECTIONSENTRY']._serialized_end=1077",
        "  _globals['_MINICOLUMN_TFIDFPERDOCENTRY']._serialized_start=1079",
        "  _globals['_MINICOLUMN_TFIDFPERDOCENTRY']._serialized_end=1129",
        "  _globals['_MINICOLUMN_DOCOCCURRENCECOUNTSENTRY']._serialized_start=1131",
        "  _globals['_MINICOLUMN_DOCOCCURRENCECOUNTSENTRY']._serialized_end=1189",
        "  _globals['_HIERARCHICALLAYER']._serialized_start=1207",
        "  _globals['_HIERARCHICALLAYER']._serialized_end=1382",
        "  _globals['_HIERARCHICALLAYER_MINICOLUMNSENTRY']._serialized_start=1310",
        "  _globals['_HIERARCHICALLAYER_MINICOLUMNSENTRY']._serialized_end=1382",
        "  _globals['_FLOATLIST']._serialized_start=1384",
        "  _globals['_FLOATLIST']._serialized_end=1411",
        "  _globals['_SEMANTICRELATION']._serialized_start=1413",
        "  _globals['_SEMANTICRELATION']._serialized_end=1500",
        "  _globals['_ANYVALUE']._serialized_start=1503",
        "  _globals['_ANYVALUE']._serialized_end=1695",
        "  _globals['_ANYDICT']._serialized_start=1697",
        "  _globals['_ANYDICT']._serialized_end=1820",
        "  _globals['_ANYDICT_VALUESENTRY']._serialized_start=1755",
        "  _globals['_ANYDICT_VALUESENTRY']._serialized_end=1820",
        "  _globals['_ANYLIST']._serialized_start=1822",
        "  _globals['_ANYLIST']._serialized_end=1867",
        "  _globals['_PROCESSORSTATE']._serialized_start=1870",
        "  _globals['_PROCESSORSTATE']._serialized_end=2521",
        "  _globals['_PROCESSORSTATE_LAYERSENTRY']._serialized_start=2249",
        "  _globals['_PROCESSORSTATE_LAYERSENTRY']._serialized_end=2323",
        "  _globals['_PROCESSORSTATE_DOCUMENTSENTRY']._serialized_start=2325",
        "  _globals['_PROCESSORSTATE_DOCUMENTSENTRY']._serialized_end=2373",
        "  _globals['_PROCESSORSTATE_DOCUMENTMETADATAENTRY']._serialized_start=2375",
        "  _globals['_PROCESSORSTATE_DOCUMENTMETADATAENTRY']._serialized_end=2449",
        "  _globals['_PROCESSORSTATE_EMBEDDINGSENTRY']._serialized_start=2451",
        "  _globals['_PROCESSORSTATE_EMBEDDINGSENTRY']._serialized_end=2521",
        "# @@protoc_insertion_point(module_scope)"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "cortical/proto/serialization.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "\"\"\"",
        "Protocol Buffers Serialization",
        "===============================",
        "",
        "Converts between Python data structures and Protocol Buffer messages.",
        "",
        "This module provides serialization and deserialization functions for the",
        "Cortical Text Processor state, enabling cross-language corpus sharing.",
        "\"\"\"",
        "",
        "import os",
        "import sys",
        "from typing import Dict, List, Optional, Any, Tuple",
        "from pathlib import Path",
        "",
        "try:",
        "    import google.protobuf",
        "    from google.protobuf import descriptor_pool",
        "    from google.protobuf import symbol_database",
        "    from google.protobuf.descriptor import FileDescriptor",
        "    from google.protobuf import message_factory",
        "    PROTOBUF_AVAILABLE = True",
        "except ImportError:",
        "    PROTOBUF_AVAILABLE = False",
        "    raise ImportError(",
        "        \"protobuf package is required for Protocol Buffers serialization. \"",
        "        \"Install it with: pip install protobuf\"",
        "    )",
        "",
        "from ..layers import CorticalLayer, HierarchicalLayer",
        "from ..minicolumn import Minicolumn, Edge",
        "",
        "",
        "# Dynamically compile and load proto definitions",
        "def _load_proto_definitions():",
        "    \"\"\"",
        "    Load Protocol Buffer definitions from schema.proto.",
        "",
        "    Uses runtime proto compilation to avoid requiring protoc at build time.",
        "",
        "    Returns:",
        "        Tuple of (ProcessorState class, message factory)",
        "    \"\"\"",
        "    proto_dir = Path(__file__).parent",
        "    proto_file = proto_dir / \"schema.proto\"",
        "",
        "    if not proto_file.exists():",
        "        raise FileNotFoundError(f\"schema.proto not found at {proto_file}\")",
        "",
        "    # Read the proto file content",
        "    with open(proto_file, 'r') as f:",
        "        proto_content = f.read()",
        "",
        "    # Try to use compiled protos first (if available)",
        "    try:",
        "        # Import generated pb2 module if it exists",
        "        from . import schema_pb2",
        "        return (",
        "            schema_pb2.ProcessorState,",
        "            schema_pb2.Edge,",
        "            schema_pb2.Minicolumn,",
        "            schema_pb2.HierarchicalLayer,",
        "            schema_pb2.FloatList,",
        "            schema_pb2.SemanticRelation,",
        "            schema_pb2.AnyValue,",
        "            schema_pb2.AnyDict,",
        "            schema_pb2.AnyList",
        "        )",
        "    except ImportError:",
        "        # Compiled protos not available - use runtime compilation",
        "        # This is a fallback that requires the 'protoc' compiler",
        "        import subprocess",
        "        import tempfile",
        "",
        "        # Create temporary directory for compilation",
        "        with tempfile.TemporaryDirectory() as tmpdir:",
        "            # Write proto file to temp location",
        "            temp_proto = Path(tmpdir) / \"schema.proto\"",
        "            temp_proto.write_text(proto_content)",
        "",
        "            # Compile proto file",
        "            try:",
        "                subprocess.run(",
        "                    [",
        "                        'protoc',",
        "                        f'--python_out={tmpdir}',",
        "                        f'--proto_path={tmpdir}',",
        "                        str(temp_proto)",
        "                    ],",
        "                    check=True,",
        "                    capture_output=True,",
        "                    text=True",
        "                )",
        "            except FileNotFoundError:",
        "                raise RuntimeError(",
        "                    \"protoc compiler not found. Please install Protocol Buffers compiler:\\n\"",
        "                    \"  Ubuntu/Debian: sudo apt-get install protobuf-compiler\\n\"",
        "                    \"  macOS: brew install protobuf\\n\"",
        "                    \"  Or compile protos manually: protoc --python_out=. schema.proto\"",
        "                )",
        "            except subprocess.CalledProcessError as e:",
        "                raise RuntimeError(f\"Failed to compile schema.proto: {e.stderr}\")",
        "",
        "            # Import the generated module",
        "            sys.path.insert(0, tmpdir)",
        "            try:",
        "                import schema_pb2",
        "                return (",
        "                    schema_pb2.ProcessorState,",
        "                    schema_pb2.Edge,",
        "                    schema_pb2.Minicolumn,",
        "                    schema_pb2.HierarchicalLayer,",
        "                    schema_pb2.FloatList,",
        "                    schema_pb2.SemanticRelation,",
        "                    schema_pb2.AnyValue,",
        "                    schema_pb2.AnyDict,",
        "                    schema_pb2.AnyList",
        "                )",
        "            finally:",
        "                sys.path.pop(0)",
        "",
        "",
        "# Load proto message classes",
        "(",
        "    ProcessorStateProto,",
        "    EdgeProto,",
        "    MinicolumnProto,",
        "    HierarchicalLayerProto,",
        "    FloatListProto,",
        "    SemanticRelationProto,",
        "    AnyValueProto,",
        "    AnyDictProto,",
        "    AnyListProto",
        ") = _load_proto_definitions()",
        "",
        "",
        "def _python_value_to_any_value(value: Any) -> Any:",
        "    \"\"\"",
        "    Convert a Python value to AnyValue protobuf message.",
        "",
        "    Args:",
        "        value: Python value (str, int, float, bool, dict, list)",
        "",
        "    Returns:",
        "        AnyValue protobuf message",
        "    \"\"\"",
        "    any_val = AnyValueProto()",
        "",
        "    if isinstance(value, str):",
        "        any_val.string_value = value",
        "    elif isinstance(value, bool):  # Check bool before int (bool is subclass of int)",
        "        any_val.bool_value = value",
        "    elif isinstance(value, int):",
        "        any_val.int_value = value",
        "    elif isinstance(value, float):",
        "        any_val.double_value = value",
        "    elif isinstance(value, dict):",
        "        any_dict = AnyDictProto()",
        "        for k, v in value.items():",
        "            any_dict.values[k].CopyFrom(_python_value_to_any_value(v))",
        "        any_val.dict_value.CopyFrom(any_dict)",
        "    elif isinstance(value, (list, tuple)):",
        "        any_list = AnyListProto()",
        "        for item in value:",
        "            any_list.values.append(_python_value_to_any_value(item))",
        "        any_val.list_value.CopyFrom(any_list)",
        "    else:",
        "        # Fallback: convert to string",
        "        any_val.string_value = str(value)",
        "",
        "    return any_val",
        "",
        "",
        "def _any_value_to_python(any_val: Any) -> Any:",
        "    \"\"\"",
        "    Convert AnyValue protobuf message to Python value.",
        "",
        "    Args:",
        "        any_val: AnyValue protobuf message",
        "",
        "    Returns:",
        "        Python value",
        "    \"\"\"",
        "    which = any_val.WhichOneof('value')",
        "",
        "    if which == 'string_value':",
        "        return any_val.string_value",
        "    elif which == 'int_value':",
        "        return any_val.int_value",
        "    elif which == 'double_value':",
        "        return any_val.double_value",
        "    elif which == 'bool_value':",
        "        return any_val.bool_value",
        "    elif which == 'dict_value':",
        "        return {",
        "            k: _any_value_to_python(v)",
        "            for k, v in any_val.dict_value.values.items()",
        "        }",
        "    elif which == 'list_value':",
        "        return [_any_value_to_python(v) for v in any_val.list_value.values]",
        "    else:",
        "        return None",
        "",
        "",
        "def edge_to_proto(edge: Edge) -> Any:",
        "    \"\"\"",
        "    Convert an Edge to protobuf message.",
        "",
        "    Args:",
        "        edge: Edge object",
        "",
        "    Returns:",
        "        Edge protobuf message",
        "    \"\"\"",
        "    proto = EdgeProto()",
        "    proto.target_id = edge.target_id",
        "    proto.weight = edge.weight",
        "    proto.relation_type = edge.relation_type",
        "    proto.confidence = edge.confidence",
        "    proto.source = edge.source",
        "    return proto",
        "",
        "",
        "def edge_from_proto(proto: Any) -> Edge:",
        "    \"\"\"",
        "    Convert Edge protobuf message to Edge object.",
        "",
        "    Args:",
        "        proto: Edge protobuf message",
        "",
        "    Returns:",
        "        Edge object",
        "    \"\"\"",
        "    return Edge(",
        "        target_id=proto.target_id,",
        "        weight=proto.weight,",
        "        relation_type=proto.relation_type,",
        "        confidence=proto.confidence,",
        "        source=proto.source",
        "    )",
        "",
        "",
        "def minicolumn_to_proto(col: Minicolumn) -> Any:",
        "    \"\"\"",
        "    Convert a Minicolumn to protobuf message.",
        "",
        "    Args:",
        "        col: Minicolumn object",
        "",
        "    Returns:",
        "        Minicolumn protobuf message",
        "    \"\"\"",
        "    proto = MinicolumnProto()",
        "    proto.id = col.id",
        "    proto.content = col.content",
        "    proto.layer = col.layer",
        "    proto.activation = col.activation",
        "    proto.occurrence_count = col.occurrence_count",
        "    proto.document_ids.extend(list(col.document_ids))",
        "",
        "    # Lateral connections",
        "    for target_id, weight in col.lateral_connections.items():",
        "        proto.lateral_connections[target_id] = weight",
        "",
        "    # Typed connections",
        "    for target_id, edge in col.typed_connections.items():",
        "        proto.typed_connections[target_id].CopyFrom(edge_to_proto(edge))",
        "",
        "    # Feedforward",
        "    proto.feedforward_sources.extend(list(col.feedforward_sources))",
        "    for target_id, weight in col.feedforward_connections.items():",
        "        proto.feedforward_connections[target_id] = weight",
        "",
        "    # Feedback",
        "    for target_id, weight in col.feedback_connections.items():",
        "        proto.feedback_connections[target_id] = weight",
        "",
        "    # TF-IDF",
        "    proto.tfidf = col.tfidf",
        "    for doc_id, score in col.tfidf_per_doc.items():",
        "        proto.tfidf_per_doc[doc_id] = score",
        "",
        "    # PageRank",
        "    proto.pagerank = col.pagerank",
        "",
        "    # Cluster ID (optional)",
        "    if col.cluster_id is not None:",
        "        proto.cluster_id = col.cluster_id",
        "",
        "    # Doc occurrence counts",
        "    for doc_id, count in col.doc_occurrence_counts.items():",
        "        proto.doc_occurrence_counts[doc_id] = count",
        "",
        "    # Name tokens (optional)",
        "    if col.name_tokens is not None:",
        "        proto.name_tokens.extend(list(col.name_tokens))",
        "",
        "    return proto",
        "",
        "",
        "def minicolumn_from_proto(proto: Any) -> Minicolumn:",
        "    \"\"\"",
        "    Convert Minicolumn protobuf message to Minicolumn object.",
        "",
        "    Args:",
        "        proto: Minicolumn protobuf message",
        "",
        "    Returns:",
        "        Minicolumn object",
        "    \"\"\"",
        "    col = Minicolumn(proto.id, proto.content, proto.layer)",
        "    col.activation = proto.activation",
        "    col.occurrence_count = proto.occurrence_count",
        "    col.document_ids = set(proto.document_ids)",
        "",
        "    # Typed connections (primary)",
        "    for target_id, edge_proto in proto.typed_connections.items():",
        "        col.typed_connections[target_id] = edge_from_proto(edge_proto)",
        "",
        "    # If no typed connections but lateral_connections exists (old format), convert",
        "    if not col.typed_connections and proto.lateral_connections:",
        "        for target_id, weight in proto.lateral_connections.items():",
        "            col.typed_connections[target_id] = Edge(",
        "                target_id=target_id,",
        "                weight=weight,",
        "                relation_type='co_occurrence',",
        "                confidence=1.0,",
        "                source='corpus'",
        "            )",
        "",
        "    # Invalidate cache to rebuild from typed_connections",
        "    col._lateral_cache_valid = False",
        "",
        "    # Feedforward",
        "    col.feedforward_sources = set(proto.feedforward_sources)",
        "    col.feedforward_connections = dict(proto.feedforward_connections)",
        "",
        "    # Feedback",
        "    col.feedback_connections = dict(proto.feedback_connections)",
        "",
        "    # TF-IDF",
        "    col.tfidf = proto.tfidf",
        "    col.tfidf_per_doc = dict(proto.tfidf_per_doc)",
        "",
        "    # PageRank",
        "    col.pagerank = proto.pagerank",
        "",
        "    # Cluster ID (optional)",
        "    if proto.HasField('cluster_id'):",
        "        col.cluster_id = proto.cluster_id",
        "",
        "    # Doc occurrence counts",
        "    col.doc_occurrence_counts = dict(proto.doc_occurrence_counts)",
        "",
        "    # Name tokens (optional)",
        "    if proto.name_tokens:",
        "        col.name_tokens = set(proto.name_tokens)",
        "",
        "    return col",
        "",
        "",
        "def layer_to_proto(layer: HierarchicalLayer) -> Any:",
        "    \"\"\"",
        "    Convert a HierarchicalLayer to protobuf message.",
        "",
        "    Args:",
        "        layer: HierarchicalLayer object",
        "",
        "    Returns:",
        "        HierarchicalLayer protobuf message",
        "    \"\"\"",
        "    proto = HierarchicalLayerProto()",
        "    proto.level = layer.level",
        "",
        "    for content, col in layer.minicolumns.items():",
        "        proto.minicolumns[content].CopyFrom(minicolumn_to_proto(col))",
        "",
        "    return proto",
        "",
        "",
        "def layer_from_proto(proto: Any) -> HierarchicalLayer:",
        "    \"\"\"",
        "    Convert HierarchicalLayer protobuf message to HierarchicalLayer object.",
        "",
        "    Args:",
        "        proto: HierarchicalLayer protobuf message",
        "",
        "    Returns:",
        "        HierarchicalLayer object",
        "    \"\"\"",
        "    layer = HierarchicalLayer(CorticalLayer(proto.level))",
        "",
        "    for content, col_proto in proto.minicolumns.items():",
        "        col = minicolumn_from_proto(col_proto)",
        "        layer.minicolumns[content] = col",
        "        layer._id_index[col.id] = content  # Rebuild ID index",
        "",
        "    return layer",
        "",
        "",
        "def to_proto(",
        "    layers: Dict[CorticalLayer, HierarchicalLayer],",
        "    documents: Dict[str, str],",
        "    document_metadata: Optional[Dict[str, Dict[str, Any]]] = None,",
        "    embeddings: Optional[Dict[str, list]] = None,",
        "    semantic_relations: Optional[list] = None,",
        "    metadata: Optional[Dict] = None",
        ") -> Any:",
        "    \"\"\"",
        "    Convert processor state to Protocol Buffer message.",
        "",
        "    Args:",
        "        layers: Dictionary of all layers",
        "        documents: Document collection",
        "        document_metadata: Per-document metadata",
        "        embeddings: Graph embeddings for terms",
        "        semantic_relations: Extracted semantic relations",
        "        metadata: Processor metadata",
        "",
        "    Returns:",
        "        ProcessorState protobuf message",
        "    \"\"\"",
        "    proto = ProcessorStateProto()",
        "    proto.version = '2.2'",
        "",
        "    # Serialize layers",
        "    for layer_enum, layer in layers.items():",
        "        proto.layers[layer_enum.value].CopyFrom(layer_to_proto(layer))",
        "",
        "    # Documents",
        "    for doc_id, text in documents.items():",
        "        proto.documents[doc_id] = text",
        "",
        "    # Document metadata",
        "    if document_metadata:",
        "        for doc_id, meta in document_metadata.items():",
        "            any_dict = AnyDictProto()",
        "            for k, v in meta.items():",
        "                any_dict.values[k].CopyFrom(_python_value_to_any_value(v))",
        "            proto.document_metadata[doc_id].CopyFrom(any_dict)",
        "",
        "    # Embeddings",
        "    if embeddings:",
        "        for term, embedding in embeddings.items():",
        "            float_list = FloatListProto()",
        "            float_list.values.extend(embedding)",
        "            proto.embeddings[term].CopyFrom(float_list)",
        "",
        "    # Semantic relations",
        "    if semantic_relations:",
        "        for rel in semantic_relations:",
        "            if len(rel) >= 4:",
        "                rel_proto = SemanticRelationProto()",
        "                rel_proto.term1 = rel[0]",
        "                rel_proto.relation_type = rel[1]",
        "                rel_proto.term2 = rel[2]",
        "                rel_proto.weight = float(rel[3])",
        "                proto.semantic_relations.append(rel_proto)",
        "",
        "    # Metadata",
        "    if metadata:",
        "        any_dict = AnyDictProto()",
        "        for k, v in metadata.items():",
        "            any_dict.values[k].CopyFrom(_python_value_to_any_value(v))",
        "        proto.metadata.CopyFrom(any_dict)",
        "",
        "    return proto",
        "",
        "",
        "def from_proto(proto: Any) -> Tuple:",
        "    \"\"\"",
        "    Convert Protocol Buffer message to processor state.",
        "",
        "    Args:",
        "        proto: ProcessorState protobuf message",
        "",
        "    Returns:",
        "        Tuple of (layers, documents, document_metadata, embeddings, semantic_relations, metadata)",
        "    \"\"\"",
        "    # Reconstruct layers",
        "    layers = {}",
        "    for level_value, layer_proto in proto.layers.items():",
        "        layer = layer_from_proto(layer_proto)",
        "        layers[CorticalLayer(level_value)] = layer",
        "",
        "    # Documents",
        "    documents = dict(proto.documents)",
        "",
        "    # Document metadata",
        "    document_metadata = {}",
        "    for doc_id, any_dict_proto in proto.document_metadata.items():",
        "        document_metadata[doc_id] = {",
        "            k: _any_value_to_python(v)",
        "            for k, v in any_dict_proto.values.items()",
        "        }",
        "",
        "    # Embeddings",
        "    embeddings = {}",
        "    for term, float_list_proto in proto.embeddings.items():",
        "        embeddings[term] = list(float_list_proto.values)",
        "",
        "    # Semantic relations",
        "    semantic_relations = []",
        "    for rel_proto in proto.semantic_relations:",
        "        semantic_relations.append((",
        "            rel_proto.term1,",
        "            rel_proto.relation_type,",
        "            rel_proto.term2,",
        "            rel_proto.weight",
        "        ))",
        "",
        "    # Metadata",
        "    metadata = {}",
        "    if proto.HasField('metadata'):",
        "        metadata = {",
        "            k: _any_value_to_python(v)",
        "            for k, v in proto.metadata.values.items()",
        "        }",
        "",
        "    return layers, documents, document_metadata, embeddings, semantic_relations, metadata"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "requirements.txt",
      "function": null,
      "start_line": 6,
      "lines_added": [
        "",
        "# Optional dependencies",
        "protobuf>=4.0        # Protocol Buffers serialization (optional, for cross-language support)",
        "grpcio-tools>=1.0    # Protocol Buffers compiler (optional, for schema compilation)"
      ],
      "lines_removed": [],
      "context_before": [
        "# No pip install required for production use.",
        "#",
        "# DEVELOPMENT: Install with `pip install -e \".[dev]\"`",
        "# Or install these manually:",
        "",
        "# Development dependencies",
        "coverage>=7.0        # Test coverage reporting",
        "pytest>=7.0          # Test framework (required by tests/unit/)",
        "mcp>=1.0             # MCP server tests (Model Context Protocol)",
        "pyyaml>=6.0          # Workflow integration tests"
      ],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "tests/unit/test_protobuf_serialization.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "\"\"\"",
        "Unit tests for Protocol Buffers serialization.",
        "",
        "Tests the protobuf serialization and deserialization functionality",
        "for cross-language corpus sharing.",
        "\"\"\"",
        "",
        "import unittest",
        "import tempfile",
        "import os",
        "from pathlib import Path",
        "",
        "# Import core modules",
        "from cortical.layers import CorticalLayer, HierarchicalLayer",
        "from cortical.minicolumn import Minicolumn, Edge",
        "from cortical.persistence import save_processor, load_processor",
        "",
        "# Try to import protobuf modules",
        "try:",
        "    from cortical.proto.serialization import (",
        "        to_proto, from_proto,",
        "        edge_to_proto, edge_from_proto,",
        "        minicolumn_to_proto, minicolumn_from_proto,",
        "        layer_to_proto, layer_from_proto",
        "    )",
        "    PROTOBUF_AVAILABLE = True",
        "except ImportError:",
        "    PROTOBUF_AVAILABLE = False",
        "",
        "",
        "@unittest.skipIf(not PROTOBUF_AVAILABLE, \"protobuf package not installed\")",
        "class TestEdgeSerialization(unittest.TestCase):",
        "    \"\"\"Test Edge protobuf serialization.\"\"\"",
        "",
        "    def test_edge_round_trip(self):",
        "        \"\"\"Test Edge serialization and deserialization.\"\"\"",
        "        edge = Edge(",
        "            target_id=\"L0_network\",",
        "            weight=0.8,",
        "            relation_type=\"RelatedTo\",",
        "            confidence=0.9,",
        "            source=\"semantic\"",
        "        )",
        "",
        "        # Convert to protobuf and back",
        "        proto = edge_to_proto(edge)",
        "        restored = edge_from_proto(proto)",
        "",
        "        # Verify all fields",
        "        self.assertEqual(restored.target_id, edge.target_id)",
        "        self.assertEqual(restored.weight, edge.weight)",
        "        self.assertEqual(restored.relation_type, edge.relation_type)",
        "        self.assertEqual(restored.confidence, edge.confidence)",
        "        self.assertEqual(restored.source, edge.source)",
        "",
        "    def test_edge_default_values(self):",
        "        \"\"\"Test Edge with default values.\"\"\"",
        "        edge = Edge(target_id=\"L0_test\")",
        "",
        "        proto = edge_to_proto(edge)",
        "        restored = edge_from_proto(proto)",
        "",
        "        self.assertEqual(restored.target_id, \"L0_test\")",
        "        self.assertEqual(restored.weight, 1.0)",
        "        self.assertEqual(restored.relation_type, \"co_occurrence\")",
        "        self.assertEqual(restored.confidence, 1.0)",
        "        self.assertEqual(restored.source, \"corpus\")",
        "",
        "",
        "@unittest.skipIf(not PROTOBUF_AVAILABLE, \"protobuf package not installed\")",
        "class TestMinicolumnSerialization(unittest.TestCase):",
        "    \"\"\"Test Minicolumn protobuf serialization.\"\"\"",
        "",
        "    def test_minicolumn_basic(self):",
        "        \"\"\"Test basic Minicolumn serialization.\"\"\"",
        "        col = Minicolumn(\"L0_neural\", \"neural\", 0)",
        "        col.occurrence_count = 15",
        "        col.activation = 0.5",
        "        col.pagerank = 0.01",
        "        col.tfidf = 2.5",
        "        col.document_ids.add(\"doc1\")",
        "        col.document_ids.add(\"doc2\")",
        "",
        "        # Add connections",
        "        col.add_lateral_connection(\"L0_network\", 3.0)",
        "        col.add_typed_connection(\"L0_brain\", 2.0, relation_type=\"RelatedTo\", confidence=0.85)",
        "",
        "        # Convert to protobuf and back",
        "        proto = minicolumn_to_proto(col)",
        "        restored = minicolumn_from_proto(proto)",
        "",
        "        # Verify basic fields",
        "        self.assertEqual(restored.id, col.id)",
        "        self.assertEqual(restored.content, col.content)",
        "        self.assertEqual(restored.layer, col.layer)",
        "        self.assertEqual(restored.occurrence_count, col.occurrence_count)",
        "        self.assertEqual(restored.activation, col.activation)",
        "        self.assertEqual(restored.pagerank, col.pagerank)",
        "        self.assertEqual(restored.tfidf, col.tfidf)",
        "        self.assertEqual(restored.document_ids, col.document_ids)",
        "",
        "        # Verify connections",
        "        self.assertEqual(len(restored.lateral_connections), 2)",
        "        self.assertEqual(restored.lateral_connections[\"L0_network\"], 3.0)",
        "        self.assertEqual(restored.lateral_connections[\"L0_brain\"], 2.0)",
        "",
        "        # Verify typed connections",
        "        self.assertEqual(len(restored.typed_connections), 2)",
        "        brain_edge = restored.typed_connections[\"L0_brain\"]",
        "        self.assertEqual(brain_edge.relation_type, \"RelatedTo\")",
        "        self.assertEqual(brain_edge.confidence, 0.85)",
        "",
        "    def test_minicolumn_with_feedforward_feedback(self):",
        "        \"\"\"Test Minicolumn with feedforward/feedback connections.\"\"\"",
        "        col = Minicolumn(\"L1_neural_network\", \"neural network\", 1)",
        "        col.add_feedforward_connection(\"L0_neural\", 1.0)",
        "        col.add_feedforward_connection(\"L0_network\", 1.0)",
        "        col.add_feedback_connection(\"L2_cluster_5\", 0.5)",
        "",
        "        proto = minicolumn_to_proto(col)",
        "        restored = minicolumn_from_proto(proto)",
        "",
        "        self.assertEqual(len(restored.feedforward_connections), 2)",
        "        self.assertEqual(restored.feedforward_connections[\"L0_neural\"], 1.0)",
        "        self.assertEqual(len(restored.feedback_connections), 1)",
        "        self.assertEqual(restored.feedback_connections[\"L2_cluster_5\"], 0.5)",
        "",
        "    def test_minicolumn_with_tfidf_per_doc(self):",
        "        \"\"\"Test Minicolumn with per-document TF-IDF scores.\"\"\"",
        "        col = Minicolumn(\"L0_test\", \"test\", 0)",
        "        col.tfidf_per_doc = {\"doc1\": 1.5, \"doc2\": 2.0, \"doc3\": 0.8}",
        "        col.doc_occurrence_counts = {\"doc1\": 3, \"doc2\": 5, \"doc3\": 1}",
        "",
        "        proto = minicolumn_to_proto(col)",
        "        restored = minicolumn_from_proto(proto)",
        "",
        "        self.assertEqual(restored.tfidf_per_doc, col.tfidf_per_doc)",
        "        self.assertEqual(restored.doc_occurrence_counts, col.doc_occurrence_counts)",
        "",
        "    def test_minicolumn_with_cluster(self):",
        "        \"\"\"Test Minicolumn with cluster ID.\"\"\"",
        "        col = Minicolumn(\"L0_test\", \"test\", 0)",
        "        col.cluster_id = 42",
        "",
        "        proto = minicolumn_to_proto(col)",
        "        restored = minicolumn_from_proto(proto)",
        "",
        "        self.assertEqual(restored.cluster_id, 42)",
        "",
        "    def test_minicolumn_with_name_tokens(self):",
        "        \"\"\"Test Minicolumn with name tokens (for documents).\"\"\"",
        "        col = Minicolumn(\"L3_doc1\", \"doc1\", 3)",
        "        col.name_tokens = {\"test\", \"document\", \"sample\"}",
        "",
        "        proto = minicolumn_to_proto(col)",
        "        restored = minicolumn_from_proto(proto)",
        "",
        "        self.assertEqual(restored.name_tokens, col.name_tokens)",
        "",
        "    def test_empty_minicolumn(self):",
        "        \"\"\"Test Minicolumn with no connections or metadata.\"\"\"",
        "        col = Minicolumn(\"L0_empty\", \"empty\", 0)",
        "",
        "        proto = minicolumn_to_proto(col)",
        "        restored = minicolumn_from_proto(proto)",
        "",
        "        self.assertEqual(restored.id, col.id)",
        "        self.assertEqual(len(restored.lateral_connections), 0)",
        "        self.assertEqual(len(restored.typed_connections), 0)",
        "",
        "",
        "@unittest.skipIf(not PROTOBUF_AVAILABLE, \"protobuf package not installed\")",
        "class TestLayerSerialization(unittest.TestCase):",
        "    \"\"\"Test HierarchicalLayer protobuf serialization.\"\"\"",
        "",
        "    def test_layer_basic(self):",
        "        \"\"\"Test basic HierarchicalLayer serialization.\"\"\"",
        "        layer = HierarchicalLayer(CorticalLayer.TOKENS)",
        "        col1 = layer.get_or_create_minicolumn(\"neural\")",
        "        col1.occurrence_count = 10",
        "        col2 = layer.get_or_create_minicolumn(\"network\")",
        "        col2.occurrence_count = 8",
        "        col1.add_lateral_connection(\"L0_network\", 5.0)",
        "",
        "        proto = layer_to_proto(layer)",
        "        restored = layer_from_proto(proto)",
        "",
        "        # Verify layer structure",
        "        self.assertEqual(restored.level, CorticalLayer.TOKENS)",
        "        self.assertEqual(len(restored.minicolumns), 2)",
        "        self.assertIn(\"neural\", restored.minicolumns)",
        "        self.assertIn(\"network\", restored.minicolumns)",
        "",
        "        # Verify minicolumn data",
        "        restored_col1 = restored.minicolumns[\"neural\"]",
        "        self.assertEqual(restored_col1.occurrence_count, 10)",
        "        self.assertEqual(len(restored_col1.lateral_connections), 1)",
        "",
        "        # Verify ID index was rebuilt",
        "        self.assertEqual(restored.get_by_id(\"L0_neural\"), restored_col1)",
        "",
        "    def test_empty_layer(self):",
        "        \"\"\"Test empty HierarchicalLayer serialization.\"\"\"",
        "        layer = HierarchicalLayer(CorticalLayer.CONCEPTS)",
        "",
        "        proto = layer_to_proto(layer)",
        "        restored = layer_from_proto(proto)",
        "",
        "        self.assertEqual(restored.level, CorticalLayer.CONCEPTS)",
        "        self.assertEqual(len(restored.minicolumns), 0)",
        "",
        "",
        "@unittest.skipIf(not PROTOBUF_AVAILABLE, \"protobuf package not installed\")",
        "class TestProcessorStateSerialization(unittest.TestCase):",
        "    \"\"\"Test complete processor state protobuf serialization.\"\"\"",
        "",
        "    def setUp(self):",
        "        \"\"\"Create test processor state.\"\"\"",
        "        # Create layers",
        "        self.layers = {",
        "            CorticalLayer.TOKENS: HierarchicalLayer(CorticalLayer.TOKENS),",
        "            CorticalLayer.BIGRAMS: HierarchicalLayer(CorticalLayer.BIGRAMS),",
        "            CorticalLayer.CONCEPTS: HierarchicalLayer(CorticalLayer.CONCEPTS),",
        "            CorticalLayer.DOCUMENTS: HierarchicalLayer(CorticalLayer.DOCUMENTS)",
        "        }",
        "",
        "        # Add some data to token layer",
        "        layer0 = self.layers[CorticalLayer.TOKENS]",
        "        col1 = layer0.get_or_create_minicolumn(\"neural\")",
        "        col1.occurrence_count = 10",
        "        col1.pagerank = 0.05",
        "        col1.tfidf = 2.5",
        "        col1.document_ids.add(\"doc1\")",
        "        col1.add_lateral_connection(\"L0_network\", 3.0)",
        "",
        "        col2 = layer0.get_or_create_minicolumn(\"network\")",
        "        col2.occurrence_count = 8",
        "        col2.pagerank = 0.04",
        "        col2.tfidf = 2.0",
        "        col2.document_ids.add(\"doc1\")",
        "",
        "        # Documents",
        "        self.documents = {",
        "            \"doc1\": \"Neural networks are powerful.\",",
        "            \"doc2\": \"Machine learning is fascinating.\"",
        "        }",
        "",
        "        # Document metadata",
        "        self.document_metadata = {",
        "            \"doc1\": {\"source\": \"test\", \"timestamp\": \"2025-01-01\"},",
        "            \"doc2\": {\"source\": \"test\", \"timestamp\": \"2025-01-02\"}",
        "        }",
        "",
        "        # Embeddings",
        "        self.embeddings = {",
        "            \"neural\": [0.1, 0.2, 0.3],",
        "            \"network\": [0.15, 0.25, 0.35]",
        "        }",
        "",
        "        # Semantic relations",
        "        self.semantic_relations = [",
        "            (\"neural\", \"RelatedTo\", \"network\", 0.8),",
        "            (\"neural\", \"UsedFor\", \"learning\", 0.5)",
        "        ]",
        "",
        "        # Metadata",
        "        self.metadata = {",
        "            \"created\": \"2025-01-01\",",
        "            \"version\": \"1.0\",",
        "            \"settings\": {\"alpha\": 0.85, \"enabled\": True}",
        "        }",
        "",
        "    def test_full_state_round_trip(self):",
        "        \"\"\"Test full processor state serialization.\"\"\"",
        "        # Convert to protobuf",
        "        proto = to_proto(",
        "            self.layers, self.documents, self.document_metadata,",
        "            self.embeddings, self.semantic_relations, self.metadata",
        "        )",
        "",
        "        # Convert back to Python",
        "        (restored_layers, restored_docs, restored_meta,",
        "         restored_embeddings, restored_relations, restored_metadata) = from_proto(proto)",
        "",
        "        # Verify layers",
        "        self.assertEqual(len(restored_layers), 4)",
        "        layer0 = restored_layers[CorticalLayer.TOKENS]",
        "        self.assertEqual(len(layer0.minicolumns), 2)",
        "",
        "        neural = layer0.minicolumns[\"neural\"]",
        "        self.assertEqual(neural.occurrence_count, 10)",
        "        self.assertEqual(neural.pagerank, 0.05)",
        "        self.assertEqual(neural.tfidf, 2.5)",
        "",
        "        # Verify documents",
        "        self.assertEqual(restored_docs, self.documents)",
        "",
        "        # Verify document metadata",
        "        self.assertEqual(len(restored_meta), 2)",
        "        self.assertEqual(restored_meta[\"doc1\"][\"source\"], \"test\")",
        "",
        "        # Verify embeddings",
        "        self.assertEqual(len(restored_embeddings), 2)",
        "        self.assertEqual(restored_embeddings[\"neural\"], [0.1, 0.2, 0.3])",
        "",
        "        # Verify semantic relations",
        "        self.assertEqual(len(restored_relations), 2)",
        "        self.assertEqual(restored_relations[0][0], \"neural\")",
        "        self.assertEqual(restored_relations[0][1], \"RelatedTo\")",
        "        self.assertEqual(restored_relations[0][2], \"network\")",
        "        self.assertEqual(restored_relations[0][3], 0.8)",
        "",
        "        # Verify metadata",
        "        self.assertEqual(restored_metadata[\"created\"], \"2025-01-01\")",
        "        self.assertEqual(restored_metadata[\"settings\"][\"alpha\"], 0.85)",
        "        self.assertEqual(restored_metadata[\"settings\"][\"enabled\"], True)",
        "",
        "    def test_minimal_state(self):",
        "        \"\"\"Test minimal processor state (layers and documents only).\"\"\"",
        "        proto = to_proto(self.layers, self.documents)",
        "        (restored_layers, restored_docs, restored_meta,",
        "         restored_embeddings, restored_relations, restored_metadata) = from_proto(proto)",
        "",
        "        self.assertEqual(len(restored_layers), 4)",
        "        self.assertEqual(restored_docs, self.documents)",
        "        self.assertEqual(restored_meta, {})",
        "        self.assertEqual(restored_embeddings, {})",
        "        self.assertEqual(restored_relations, [])",
        "        self.assertEqual(restored_metadata, {})",
        "",
        "",
        "@unittest.skipIf(not PROTOBUF_AVAILABLE, \"protobuf package not installed\")",
        "class TestPersistenceIntegration(unittest.TestCase):",
        "    \"\"\"Test protobuf integration with persistence module.\"\"\"",
        "",
        "    def setUp(self):",
        "        \"\"\"Create test data.\"\"\"",
        "        self.layers = {",
        "            CorticalLayer.TOKENS: HierarchicalLayer(CorticalLayer.TOKENS),",
        "            CorticalLayer.BIGRAMS: HierarchicalLayer(CorticalLayer.BIGRAMS),",
        "            CorticalLayer.CONCEPTS: HierarchicalLayer(CorticalLayer.CONCEPTS),",
        "            CorticalLayer.DOCUMENTS: HierarchicalLayer(CorticalLayer.DOCUMENTS)",
        "        }",
        "",
        "        layer0 = self.layers[CorticalLayer.TOKENS]",
        "        col = layer0.get_or_create_minicolumn(\"test\")",
        "        col.occurrence_count = 5",
        "        col.add_lateral_connection(\"L0_data\", 2.0)",
        "",
        "        self.documents = {\"doc1\": \"Test data\"}",
        "        self.document_metadata = {\"doc1\": {\"source\": \"test\"}}",
        "        self.embeddings = {\"test\": [0.1, 0.2]}",
        "        self.semantic_relations = [(\"test\", \"RelatedTo\", \"data\", 0.7)]",
        "        self.metadata = {\"version\": \"1.0\"}",
        "",
        "    def test_save_and_load_protobuf(self):",
        "        \"\"\"Test saving and loading with protobuf format.\"\"\"",
        "        with tempfile.NamedTemporaryFile(delete=False, suffix='.pb') as f:",
        "            filepath = f.name",
        "",
        "        try:",
        "            # Save with protobuf format",
        "            save_processor(",
        "                filepath, self.layers, self.documents,",
        "                self.document_metadata, self.embeddings,",
        "                self.semantic_relations, self.metadata,",
        "                verbose=False, format='protobuf'",
        "            )",
        "",
        "            # Load with protobuf format",
        "            (restored_layers, restored_docs, restored_meta,",
        "             restored_embeddings, restored_relations, restored_metadata) = load_processor(",
        "                filepath, verbose=False, format='protobuf'",
        "            )",
        "",
        "            # Verify data",
        "            self.assertEqual(len(restored_layers), 4)",
        "            layer0 = restored_layers[CorticalLayer.TOKENS]",
        "            self.assertEqual(len(layer0.minicolumns), 1)  # Only \"test\" minicolumn",
        "",
        "            test_col = layer0.minicolumns[\"test\"]",
        "            self.assertEqual(test_col.occurrence_count, 5)",
        "            self.assertEqual(len(test_col.lateral_connections), 1)",
        "            self.assertIn(\"L0_data\", test_col.lateral_connections)",
        "",
        "            self.assertEqual(restored_docs, self.documents)",
        "            self.assertEqual(restored_meta, self.document_metadata)",
        "            self.assertEqual(restored_embeddings, self.embeddings)",
        "            self.assertEqual(len(restored_relations), 1)",
        "",
        "        finally:",
        "            if os.path.exists(filepath):",
        "                os.unlink(filepath)",
        "",
        "    def test_save_and_load_pickle(self):",
        "        \"\"\"Test saving and loading with pickle format (backward compatibility).\"\"\"",
        "        with tempfile.NamedTemporaryFile(delete=False, suffix='.pkl') as f:",
        "            filepath = f.name",
        "",
        "        try:",
        "            # Save with pickle format (default)",
        "            save_processor(",
        "                filepath, self.layers, self.documents,",
        "                self.document_metadata, self.embeddings,",
        "                self.semantic_relations, self.metadata,",
        "                verbose=False, format='pickle'",
        "            )",
        "",
        "            # Load with pickle format",
        "            (restored_layers, restored_docs, restored_meta,",
        "             restored_embeddings, restored_relations, restored_metadata) = load_processor(",
        "                filepath, verbose=False, format='pickle'",
        "            )",
        "",
        "            # Verify data",
        "            self.assertEqual(len(restored_layers), 4)",
        "            self.assertEqual(restored_docs, self.documents)",
        "",
        "        finally:",
        "            if os.path.exists(filepath):",
        "                os.unlink(filepath)",
        "",
        "    def test_auto_detect_format(self):",
        "        \"\"\"Test automatic format detection.\"\"\"",
        "        with tempfile.NamedTemporaryFile(delete=False, suffix='.pkl') as f:",
        "            pickle_file = f.name",
        "        with tempfile.NamedTemporaryFile(delete=False, suffix='.pb') as f:",
        "            proto_file = f.name",
        "",
        "        try:",
        "            # Save in both formats",
        "            save_processor(",
        "                pickle_file, self.layers, self.documents,",
        "                verbose=False, format='pickle'",
        "            )",
        "            save_processor(",
        "                proto_file, self.layers, self.documents,",
        "                verbose=False, format='protobuf'",
        "            )",
        "",
        "            # Load without specifying format (auto-detect)",
        "            (layers1, docs1, _, _, _, _) = load_processor(pickle_file, verbose=False)",
        "            (layers2, docs2, _, _, _, _) = load_processor(proto_file, verbose=False)",
        "",
        "            # Both should load successfully",
        "            self.assertEqual(len(layers1), 4)",
        "            self.assertEqual(len(layers2), 4)",
        "            self.assertEqual(docs1, self.documents)",
        "            self.assertEqual(docs2, self.documents)",
        "",
        "        finally:",
        "            if os.path.exists(pickle_file):",
        "                os.unlink(pickle_file)",
        "            if os.path.exists(proto_file):",
        "                os.unlink(proto_file)",
        "",
        "    def test_invalid_format_error(self):",
        "        \"\"\"Test error handling for invalid format.\"\"\"",
        "        with tempfile.NamedTemporaryFile(delete=False) as f:",
        "            filepath = f.name",
        "",
        "        try:",
        "            with self.assertRaises(ValueError):",
        "                save_processor(",
        "                    filepath, self.layers, self.documents,",
        "                    verbose=False, format='json'",
        "                )",
        "        finally:",
        "            if os.path.exists(filepath):",
        "                os.unlink(filepath)",
        "",
        "",
        "@unittest.skipIf(not PROTOBUF_AVAILABLE, \"protobuf package not installed\")",
        "class TestComplexDataStructures(unittest.TestCase):",
        "    \"\"\"Test protobuf serialization with complex data structures.\"\"\"",
        "",
        "    def test_nested_metadata(self):",
        "        \"\"\"Test nested dictionaries in metadata.\"\"\"",
        "        layers = {",
        "            CorticalLayer.TOKENS: HierarchicalLayer(CorticalLayer.TOKENS)",
        "        }",
        "        documents = {\"doc1\": \"test\"}",
        "        metadata = {",
        "            \"config\": {",
        "                \"params\": {",
        "                    \"alpha\": 0.85,",
        "                    \"beta\": 0.15,",
        "                    \"nested\": {",
        "                        \"value\": 42,",
        "                        \"enabled\": True",
        "                    }",
        "                },",
        "                \"lists\": [1, 2, 3, \"four\", 5.0]",
        "            }",
        "        }",
        "",
        "        proto = to_proto(layers, documents, metadata=metadata)",
        "        (_, _, _, _, _, restored_metadata) = from_proto(proto)",
        "",
        "        self.assertEqual(restored_metadata[\"config\"][\"params\"][\"alpha\"], 0.85)",
        "        self.assertEqual(restored_metadata[\"config\"][\"params\"][\"nested\"][\"value\"], 42)",
        "        self.assertEqual(restored_metadata[\"config\"][\"params\"][\"nested\"][\"enabled\"], True)",
        "        self.assertEqual(restored_metadata[\"config\"][\"lists\"], [1, 2, 3, \"four\", 5.0])",
        "",
        "    def test_large_embeddings(self):",
        "        \"\"\"Test large embedding vectors.\"\"\"",
        "        layers = {",
        "            CorticalLayer.TOKENS: HierarchicalLayer(CorticalLayer.TOKENS)",
        "        }",
        "        documents = {\"doc1\": \"test\"}",
        "        embeddings = {",
        "            \"term1\": [float(i) for i in range(300)],  # 300-dimensional embedding",
        "            \"term2\": [float(i * 0.5) for i in range(300)]",
        "        }",
        "",
        "        proto = to_proto(layers, documents, embeddings=embeddings)",
        "        (_, _, _, restored_embeddings, _, _) = from_proto(proto)",
        "",
        "        self.assertEqual(len(restored_embeddings[\"term1\"]), 300)",
        "        self.assertEqual(restored_embeddings[\"term1\"][0], 0.0)",
        "        self.assertEqual(restored_embeddings[\"term1\"][299], 299.0)",
        "",
        "    def test_many_semantic_relations(self):",
        "        \"\"\"Test many semantic relations.\"\"\"",
        "        layers = {",
        "            CorticalLayer.TOKENS: HierarchicalLayer(CorticalLayer.TOKENS)",
        "        }",
        "        documents = {\"doc1\": \"test\"}",
        "        relations = [",
        "            (f\"term{i}\", \"RelatedTo\", f\"term{i+1}\", float(i) / 100.0)",
        "            for i in range(100)",
        "        ]",
        "",
        "        proto = to_proto(layers, documents, semantic_relations=relations)",
        "        (_, _, _, _, restored_relations, _) = from_proto(proto)",
        "",
        "        self.assertEqual(len(restored_relations), 100)",
        "        self.assertEqual(restored_relations[50][0], \"term50\")",
        "        self.assertEqual(restored_relations[50][2], \"term51\")",
        "        self.assertEqual(restored_relations[50][3], 0.5)",
        "",
        "",
        "if __name__ == '__main__':",
        "    unittest.main()"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    }
  ],
  "hour_of_day": 9,
  "day_of_week": "Sunday",
  "seconds_since_last_commit": -102607,
  "is_merge": false,
  "is_initial": false,
  "parent_count": 1,
  "session_id": null,
  "related_chats": [],
  "ci_result": null,
  "reverted": false,
  "amended": false
}