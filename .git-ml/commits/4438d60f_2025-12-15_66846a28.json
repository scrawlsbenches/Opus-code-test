{
  "hash": "4438d60fd867cf10e23752e3554f6eb2a5bf217d",
  "message": "fix: Harden ML data collector with critical fixes",
  "author": "Claude",
  "timestamp": "2025-12-15 03:37:24 +0000",
  "branch": "claude/multi-index-design-DvifZ",
  "files_changed": [
    "scripts/ml_data_collector.py"
  ],
  "insertions": 151,
  "deletions": 54,
  "hunks": [
    {
      "file": "scripts/ml_data_collector.py",
      "function": "Usage:",
      "start_line": 17,
      "lines_added": [
        "import tempfile",
        "import uuid",
        "",
        "class GitCommandError(Exception):",
        "    \"\"\"Raised when a git command fails.\"\"\"",
        "    pass",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "",
        "    # Estimate when training is viable",
        "    python scripts/ml_data_collector.py estimate",
        "\"\"\"",
        "",
        "import json",
        "import os",
        "import subprocess",
        "import hashlib",
        "import re"
      ],
      "context_after": [
        "from datetime import datetime",
        "from pathlib import Path",
        "from typing import Dict, List, Optional, Any",
        "from dataclasses import dataclass, asdict, field",
        "",
        "# ============================================================================",
        "# CONFIGURATION",
        "# ============================================================================",
        "",
        "ML_DATA_DIR = Path(\".git-ml\")",
        "COMMITS_DIR = ML_DATA_DIR / \"commits\"",
        "SESSIONS_DIR = ML_DATA_DIR / \"sessions\"",
        "CHATS_DIR = ML_DATA_DIR / \"chats\"",
        "ACTIONS_DIR = ML_DATA_DIR / \"actions\"",
        ""
      ],
      "change_type": "add"
    },
    {
      "file": "scripts/ml_data_collector.py",
      "function": "class CommitContext:",
      "start_line": 79,
      "lines_added": [
        "    # Commit type detection",
        "    is_merge: bool = False",
        "    is_initial: bool = False",
        "    parent_count: int = 1",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "    deletions: int",
        "",
        "    # Diff structure",
        "    hunks: List[Dict]",
        "",
        "    # Temporal context",
        "    hour_of_day: int",
        "    day_of_week: str",
        "    seconds_since_last_commit: Optional[int]",
        ""
      ],
      "context_after": [
        "    # Session context (if available)",
        "    session_id: Optional[str] = None",
        "    related_chats: List[str] = field(default_factory=list)",
        "",
        "    # Outcome tracking (filled in later)",
        "    ci_result: Optional[str] = None",
        "    reverted: bool = False",
        "    amended: bool = False",
        "",
        ""
      ],
      "change_type": "add"
    },
    {
      "file": "scripts/ml_data_collector.py",
      "function": "class ActionEntry:",
      "start_line": 144,
      "lines_added": [
        "def run_git(args: List[str], check: bool = True) -> str:",
        "    \"\"\"Run a git command and return output.",
        "",
        "    Args:",
        "        args: Git command arguments (without 'git' prefix)",
        "        check: If True, raise GitCommandError on non-zero exit",
        "",
        "    Returns:",
        "        Command stdout stripped of whitespace",
        "",
        "    Raises:",
        "        GitCommandError: If check=True and command fails",
        "    \"\"\"",
        "    if check and result.returncode != 0:",
        "        raise GitCommandError(",
        "            f\"git {args[0]} failed (exit {result.returncode}): {result.stderr.strip()}\"",
        "        )",
        "def parse_diff_hunks(commit_hash: str, is_merge: bool = False) -> List[Dict]:",
        "    \"\"\"Parse diff hunks from a commit into structured data.",
        "",
        "    Args:",
        "        commit_hash: Git commit hash",
        "        is_merge: If True, use --first-parent to get meaningful diff",
        "",
        "    Returns:",
        "        List of hunk dictionaries with file, function, lines, etc.",
        "    \"\"\"",
        "    # Use -U10 for more context (better for ML training)",
        "    # For merge commits, use --first-parent to get the actual changes",
        "    args = [\"show\", \"--format=\", \"-U10\", commit_hash]",
        "    if is_merge:",
        "        args.insert(2, \"--first-parent\")",
        "",
        "    diff_output = run_git(args, check=False)  # Don't fail on empty diffs"
      ],
      "lines_removed": [
        "def run_git(args: List[str]) -> str:",
        "    \"\"\"Run a git command and return output.\"\"\"",
        "def parse_diff_hunks(commit_hash: str) -> List[Dict]:",
        "    \"\"\"Parse diff hunks from a commit into structured data.\"\"\"",
        "    diff_output = run_git([\"show\", \"--format=\", \"-U3\", commit_hash])"
      ],
      "context_before": [
        "# ============================================================================",
        "# DATA COLLECTION FUNCTIONS",
        "# ============================================================================",
        "",
        "def ensure_dirs():",
        "    \"\"\"Create data directories if they don't exist.\"\"\"",
        "    for dir_path in [COMMITS_DIR, SESSIONS_DIR, CHATS_DIR, ACTIONS_DIR]:",
        "        dir_path.mkdir(parents=True, exist_ok=True)",
        "",
        ""
      ],
      "context_after": [
        "    result = subprocess.run(",
        "        [\"git\"] + args,",
        "        capture_output=True,",
        "        text=True,",
        "        cwd=str(Path.cwd())",
        "    )",
        "    return result.stdout.strip()",
        "",
        "",
        "def get_last_commit_time() -> Optional[datetime]:",
        "    \"\"\"Get timestamp of the previous commit.\"\"\"",
        "    output = run_git([\"log\", \"-2\", \"--format=%ct\"])",
        "    lines = output.strip().split(\"\\n\")",
        "    if len(lines) >= 2:",
        "        return datetime.fromtimestamp(int(lines[1]))",
        "    return None",
        "",
        "",
        "",
        "    hunks = []",
        "    current_file = None",
        "    current_hunk = None",
        "",
        "    for line in diff_output.split(\"\\n\"):",
        "        # New file",
        "        if line.startswith(\"diff --git\"):",
        "            if current_hunk:",
        "                hunks.append(current_hunk)"
      ],
      "change_type": "modify"
    },
    {
      "file": "scripts/ml_data_collector.py",
      "function": "def parse_diff_hunks(commit_hash: str) -> List[Dict]:",
      "start_line": 241,
      "lines_added": [
        "    branch = run_git([\"rev-parse\", \"--abbrev-ref\", \"HEAD\"], check=False) or \"HEAD\"",
        "",
        "    # Detect merge/initial commit by counting parents",
        "    parents_output = run_git([\"rev-list\", \"--parents\", \"-n1\", commit_hash])",
        "    parents = parents_output.split()",
        "    parent_count = len(parents) - 1  # First element is the commit itself",
        "    is_merge = parent_count > 1",
        "    is_initial = parent_count == 0",
        "",
        "    # Files and stats - for merges, use --first-parent for meaningful diff",
        "    if is_merge:",
        "        files_output = run_git(",
        "            [\"diff\", \"--name-only\", f\"{commit_hash}^1\", commit_hash],",
        "            check=False",
        "        )",
        "        stats = run_git(",
        "            [\"diff\", \"--stat\", f\"{commit_hash}^1\", commit_hash],",
        "            check=False",
        "        )",
        "    else:",
        "        files_output = run_git([\"show\", \"--name-only\", \"--format=\", commit_hash])",
        "        stats = run_git([\"show\", \"--stat\", \"--format=\", commit_hash])",
        "    # Temporal context - use commit timestamp, not current time for backfill",
        "    commit_time = run_git([\"log\", \"-1\", \"--format=%ct\", commit_hash])",
        "    try:",
        "        commit_dt = datetime.fromtimestamp(int(commit_time))",
        "    except (ValueError, OSError):",
        "        commit_dt = datetime.now()",
        "",
        "        seconds_since = int((commit_dt - last_commit).total_seconds())",
        "    # Parse diff hunks (pass is_merge flag for proper handling)",
        "    hunks = parse_diff_hunks(commit_hash, is_merge=is_merge)",
        "        hour_of_day=commit_dt.hour,",
        "        day_of_week=commit_dt.strftime(\"%A\"),",
        "        is_merge=is_merge,",
        "        is_initial=is_initial,",
        "        parent_count=parent_count,",
        "def atomic_write_json(filepath: Path, data: dict):",
        "    \"\"\"Write JSON atomically using temp file + rename.",
        "",
        "    This prevents data corruption if the process is interrupted.",
        "    \"\"\"",
        "    # Write to temp file in same directory (for same-filesystem rename)",
        "    temp_fd, temp_path = tempfile.mkstemp(",
        "        suffix=\".tmp\",",
        "        prefix=filepath.stem + \"_\",",
        "        dir=filepath.parent",
        "    )",
        "    try:",
        "        with os.fdopen(temp_fd, \"w\", encoding=\"utf-8\") as f:",
        "            json.dump(data, f, indent=2, ensure_ascii=False)",
        "        # Atomic rename (on POSIX systems)",
        "        os.replace(temp_path, filepath)",
        "    except Exception:",
        "        # Clean up temp file on failure",
        "        if os.path.exists(temp_path):",
        "            os.unlink(temp_path)",
        "        raise",
        "",
        "",
        "    \"\"\"Save commit context to disk atomically.\"\"\"",
        "    # Use full hash + UUID suffix to prevent collisions",
        "    unique_id = uuid.uuid4().hex[:8]",
        "    filename = f\"{context.hash[:8]}_{context.timestamp[:10]}_{unique_id}.json\"",
        "    atomic_write_json(filepath, asdict(context))",
        "    \"\"\"Save a chat entry to disk atomically.\"\"\"",
        "    atomic_write_json(filepath, asdict(entry))"
      ],
      "lines_removed": [
        "    branch = run_git([\"rev-parse\", \"--abbrev-ref\", \"HEAD\"])",
        "    # Files and stats",
        "    files_output = run_git([\"show\", \"--name-only\", \"--format=\", commit_hash])",
        "    stats = run_git([\"show\", \"--stat\", \"--format=\", commit_hash])",
        "    # Temporal context",
        "    now = datetime.now()",
        "        seconds_since = int((now - last_commit).total_seconds())",
        "    # Parse diff hunks",
        "    hunks = parse_diff_hunks(commit_hash)",
        "        hour_of_day=now.hour,",
        "        day_of_week=now.strftime(\"%A\"),",
        "    \"\"\"Save commit context to disk.\"\"\"",
        "    filename = f\"{context.hash[:8]}_{context.timestamp[:10]}.json\"",
        "    with open(filepath, \"w\") as f:",
        "        json.dump(asdict(context), f, indent=2)",
        "",
        "    \"\"\"Save a chat entry to disk.\"\"\"",
        "    with open(filepath, \"w\") as f:",
        "        json.dump(asdict(entry), f, indent=2)",
        ""
      ],
      "context_before": [
        "",
        "def collect_commit_data(commit_hash: Optional[str] = None) -> CommitContext:",
        "    \"\"\"Collect rich context for a commit.\"\"\"",
        "    if commit_hash is None:",
        "        commit_hash = run_git([\"rev-parse\", \"HEAD\"])",
        "",
        "    # Basic metadata",
        "    message = run_git([\"log\", \"-1\", \"--format=%s\", commit_hash])",
        "    author = run_git([\"log\", \"-1\", \"--format=%an\", commit_hash])",
        "    timestamp = run_git([\"log\", \"-1\", \"--format=%ci\", commit_hash])"
      ],
      "context_after": [
        "",
        "    files_changed = [f for f in files_output.split(\"\\n\") if f]",
        "",
        "    insertions = 0",
        "    deletions = 0",
        "    if stats:",
        "        match = re.search(r\"(\\d+) insertion\", stats)",
        "        if match:",
        "            insertions = int(match.group(1))",
        "        match = re.search(r\"(\\d+) deletion\", stats)",
        "        if match:",
        "            deletions = int(match.group(1))",
        "",
        "    last_commit = get_last_commit_time()",
        "    seconds_since = None",
        "    if last_commit:",
        "",
        "",
        "    return CommitContext(",
        "        hash=commit_hash,",
        "        message=message,",
        "        author=author,",
        "        timestamp=timestamp,",
        "        branch=branch,",
        "        files_changed=files_changed,",
        "        insertions=insertions,",
        "        deletions=deletions,",
        "        hunks=hunks,",
        "        seconds_since_last_commit=seconds_since,",
        "    )",
        "",
        "",
        "def save_commit_data(context: CommitContext):",
        "    ensure_dirs()",
        "",
        "    filepath = COMMITS_DIR / filename",
        "",
        "    print(f\"Saved commit data to {filepath}\")",
        "",
        "",
        "def generate_chat_id() -> str:",
        "    \"\"\"Generate unique chat entry ID.\"\"\"",
        "    timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")",
        "    suffix = hashlib.sha256(str(datetime.now().timestamp()).encode()).hexdigest()[:6]",
        "    return f\"chat-{timestamp}-{suffix}\"",
        "",
        "",
        "def generate_session_id() -> str:",
        "    \"\"\"Generate unique session ID.\"\"\"",
        "    return hashlib.sha256(str(datetime.now().timestamp()).encode()).hexdigest()[:8]",
        "",
        "",
        "def save_chat_entry(entry: ChatEntry):",
        "    ensure_dirs()",
        "",
        "    # Organize by date",
        "    date_dir = CHATS_DIR / entry.timestamp[:10]",
        "    date_dir.mkdir(exist_ok=True)",
        "",
        "    filename = f\"{entry.id}.json\"",
        "    filepath = date_dir / filename",
        "",
        "    print(f\"Saved chat entry to {filepath}\")",
        "",
        "",
        "def log_chat(",
        "    query: str,",
        "    response: str,",
        "    session_id: Optional[str] = None,",
        "    files_referenced: Optional[List[str]] = None,",
        "    files_modified: Optional[List[str]] = None,",
        "    tools_used: Optional[List[str]] = None,"
      ],
      "change_type": "modify"
    },
    {
      "file": "scripts/ml_data_collector.py",
      "function": "def log_chat(",
      "start_line": 356,
      "lines_added": [
        "    \"\"\"Save an action entry to disk atomically.\"\"\"",
        "    atomic_write_json(filepath, asdict(entry))"
      ],
      "lines_removed": [
        "    \"\"\"Save an action entry to disk.\"\"\"",
        "    with open(filepath, \"w\") as f:",
        "        json.dump(asdict(entry), f, indent=2)"
      ],
      "context_before": [
        "        user_feedback=user_feedback,",
        "        query_tokens=len(query.split()),  # Rough estimate",
        "        response_tokens=len(response.split()),",
        "    )",
        "",
        "    save_chat_entry(entry)",
        "    return entry",
        "",
        "",
        "def save_action(entry: ActionEntry):"
      ],
      "context_after": [
        "    ensure_dirs()",
        "",
        "    date_dir = ACTIONS_DIR / entry.timestamp[:10]",
        "    date_dir.mkdir(exist_ok=True)",
        "",
        "    filename = f\"{entry.id}.json\"",
        "    filepath = date_dir / filename",
        "",
        "",
        "",
        "def log_action(",
        "    action_type: str,",
        "    target: str,",
        "    session_id: Optional[str] = None,",
        "    context: Optional[Dict] = None,",
        "    success: bool = True,",
        "    result_summary: Optional[str] = None,",
        ") -> ActionEntry:"
      ],
      "change_type": "modify"
    },
    {
      "file": "scripts/ml_data_collector.py",
      "function": "def estimate_project_size():",
      "start_line": 575,
      "lines_added": [
        "ML_HOOK_MARKER = \"# ML-DATA-COLLECTOR-HOOK\"",
        "",
        "POST_COMMIT_SNIPPET = '''",
        "# ML-DATA-COLLECTOR-HOOK",
        "python scripts/ml_data_collector.py commit 2>/dev/null || true",
        "# END-ML-DATA-COLLECTOR-HOOK",
        "PRE_PUSH_SNIPPET = '''",
        "# ML-DATA-COLLECTOR-HOOK",
        "# END-ML-DATA-COLLECTOR-HOOK",
        "    \"\"\"Install git hooks for data collection, merging with existing hooks.\"\"\"",
        "    for hook_name, snippet in [(\"post-commit\", POST_COMMIT_SNIPPET), (\"pre-push\", PRE_PUSH_SNIPPET)]:",
        "        hook_path = hooks_dir / hook_name",
        "",
        "        if hook_path.exists():",
        "            existing = hook_path.read_text(encoding=\"utf-8\")",
        "",
        "            # Check if our hook is already installed",
        "            if ML_HOOK_MARKER in existing:",
        "                print(f\"âœ“ {hook_name}: ML hook already installed\")",
        "                continue",
        "",
        "            # Append to existing hook",
        "            with open(hook_path, \"a\", encoding=\"utf-8\") as f:",
        "                f.write(snippet)",
        "            print(f\"âœ“ {hook_name}: Added ML hook to existing hook\")",
        "",
        "        else:",
        "            # Create new hook with shebang",
        "            with open(hook_path, \"w\", encoding=\"utf-8\") as f:",
        "                f.write(\"#!/bin/bash\\n\")",
        "                f.write(snippet)",
        "                f.write(\"\\nexit 0\\n\")",
        "            hook_path.chmod(0o755)",
        "            print(f\"âœ“ {hook_name}: Created new hook\")",
        "",
        "    print(\"\\nML hooks installed! Commit data will be collected automatically.\")"
      ],
      "lines_removed": [
        "POST_COMMIT_HOOK = '''#!/bin/bash",
        "",
        "python scripts/ml_data_collector.py commit",
        "",
        "# Don't block the commit if collection fails",
        "exit 0",
        "PRE_PUSH_HOOK = '''#!/bin/bash",
        "",
        "",
        "exit 0",
        "    \"\"\"Install git hooks for data collection.\"\"\"",
        "    # Post-commit hook",
        "    post_commit = hooks_dir / \"post-commit\"",
        "    with open(post_commit, \"w\") as f:",
        "        f.write(POST_COMMIT_HOOK)",
        "    post_commit.chmod(0o755)",
        "    print(f\"âœ“ Installed {post_commit}\")",
        "",
        "    # Pre-push hook",
        "    pre_push = hooks_dir / \"pre-push\"",
        "    with open(pre_push, \"w\") as f:",
        "        f.write(PRE_PUSH_HOOK)",
        "    pre_push.chmod(0o755)",
        "    print(f\"âœ“ Installed {pre_push}\")",
        "",
        "    print(\"\\nGit hooks installed! Commit data will be collected automatically.\")"
      ],
      "context_before": [
        "    print(f\"   At current rate:     ~{int(days_needed)} days ({int(days_needed/30)} months)\")",
        "    print(f\"   With active use:     ~{int(days_needed * 0.5)} days (more chatting)\")",
        "",
        "    print(\"\\n\" + \"=\" * 60)",
        "",
        "",
        "# ============================================================================",
        "# GIT HOOKS",
        "# ============================================================================",
        ""
      ],
      "context_after": [
        "# ML Data Collection - Post-Commit Hook",
        "# Automatically collects enriched commit data for model training",
        "'''",
        "",
        "# ML Data Collection - Pre-Push Hook",
        "# Validates data collection is working before push",
        "if [ -d \".git-ml/commits\" ]; then",
        "    count=$(ls -1 .git-ml/commits/*.json 2>/dev/null | wc -l)",
        "    echo \"ðŸ“Š ML Data: $count commits collected\"",
        "fi",
        "'''",
        "",
        "",
        "def install_hooks():",
        "    hooks_dir = Path(\".git/hooks\")",
        "",
        "",
        "",
        "# ============================================================================",
        "# CLI",
        "# ============================================================================",
        "",
        "def main():",
        "    import sys",
        "",
        "    if len(sys.argv) < 2:"
      ],
      "change_type": "modify"
    }
  ],
  "hour_of_day": 3,
  "day_of_week": "Monday",
  "seconds_since_last_commit": -36444,
  "is_merge": false,
  "is_initial": false,
  "parent_count": 1,
  "session_id": null,
  "related_chats": [],
  "ci_result": null,
  "reverted": false,
  "amended": false
}