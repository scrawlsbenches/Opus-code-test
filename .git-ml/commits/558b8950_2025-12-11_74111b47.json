{
  "hash": "558b895066324a246440cff4c6cc2367206c6fad",
  "message": "Implement tasks #41, #56, #61, #62: Config dataclass, patterns docs, chunk warning",
  "author": "Claude",
  "timestamp": "2025-12-11 00:34:05 +0000",
  "branch": "claude/multi-index-design-DvifZ",
  "files_changed": [
    ".claude/skills/corpus-indexer/SKILL.md",
    "CLAUDE.md",
    "TASK_LIST.md",
    "cortical/__init__.py",
    "cortical/chunk_index.py",
    "cortical/config.py",
    "docs/patterns.md",
    "tests/test_chunk_indexing.py",
    "tests/test_config.py"
  ],
  "insertions": 1364,
  "deletions": 41,
  "hunks": [
    {
      "file": ".claude/skills/corpus-indexer/SKILL.md",
      "function": "This enables fast incremental updates by detecting only changed files.",
      "start_line": 101,
      "lines_added": [
        "# Check chunk status",
        "python scripts/index_codebase.py --status --use-chunks",
        "",
        "## Chunk Compaction",
        "",
        "Over time, chunk files accumulate. Use compaction to consolidate them (like `git gc`):",
        "",
        "**When to compact:**",
        "- After 10+ chunk files accumulate",
        "- When you see size warnings during save",
        "- Before merging branches with chunk histories",
        "- To clean up old deleted entries",
        "",
        "**Commands:**",
        "```bash",
        "# Compact all chunks into one",
        "python scripts/index_codebase.py --compact --use-chunks",
        "",
        "# Compact chunks before a date",
        "python scripts/index_codebase.py --compact --before 2025-12-01 --use-chunks",
        "```",
        "",
        "**What happens:**",
        "1. All chunks are read in timestamp order",
        "2. Operations are replayed (later timestamps win)",
        "3. A single compacted chunk is created",
        "4. Old chunk files are removed",
        "5. Cache is preserved if valid",
        "",
        "**Recommended frequency:**",
        "- Weekly for active development",
        "- Monthly for maintenance mode",
        "- Before major releases"
      ],
      "lines_removed": [
        "# Compact old chunks (reduces history size)",
        "python scripts/index_codebase.py --compact --before 2025-12-01"
      ],
      "context_before": [
        "- Use `--full-analysis` before deep exploration sessions",
        "",
        "## Git-Compatible Chunk Storage",
        "",
        "For team collaboration, use `--use-chunks` to store changes as git-friendly JSON:",
        "",
        "```bash",
        "# Index with chunk storage",
        "python scripts/index_codebase.py --incremental --use-chunks",
        ""
      ],
      "context_after": [
        "```",
        "",
        "**Benefits:**",
        "- No merge conflicts (unique timestamp filenames)",
        "- Shared indexed state across branches",
        "- Fast startup when cache is valid",
        "",
        "**Files Created:**",
        "- `corpus_chunks/*.json` - Tracked in git (append-only changes)",
        "- `corpus_dev.pkl` - NOT tracked (local cache)",
        "- `corpus_dev.pkl.hash` - NOT tracked (cache validation)"
      ],
      "change_type": "modify"
    },
    {
      "file": "CLAUDE.md",
      "function": "corpus_dev.pkl                        # NOT tracked (local cache)",
      "start_line": 442,
      "lines_added": [
        "### Chunk Compaction",
        "",
        "Over time, chunk files accumulate. Use compaction to consolidate them, similar to `git gc`:",
        "",
        "**When to compact:**",
        "- After many indexing sessions (10+ chunk files)",
        "- When you see size warnings during indexing",
        "- Before merging branches with different chunk histories",
        "- To clean up deleted/modified document history",
        "",
        "**Compaction commands:**",
        "```bash",
        "# Compact all chunks into a single consolidated file",
        "python scripts/index_codebase.py --compact --use-chunks",
        "",
        "# Compact only chunks created before a specific date",
        "python scripts/index_codebase.py --compact --before 2025-12-01 --use-chunks",
        "",
        "# Check chunk status before compacting",
        "python scripts/index_codebase.py --status --use-chunks",
        "```",
        "",
        "**How compaction works:**",
        "1. Reads all chunk files (sorted by timestamp)",
        "2. Replays operations in order (later timestamps override)",
        "3. Creates a single compacted chunk with final state",
        "4. Removes old chunk files",
        "5. Preserves cache if still valid",
        "",
        "**Recommended frequency:**",
        "- Weekly for active development",
        "- Monthly for maintenance repositories",
        "- Before major releases",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "corpus_dev.pkl.hash                   # NOT tracked (cache validation)",
        "```",
        "",
        "**Benefits:**",
        "- No merge conflicts (unique timestamp+session filenames)",
        "- Shared indexed state across team/branches",
        "- Fast startup when cache is valid",
        "- Git-friendly (small JSON, append-only)",
        "- Periodic compaction like `git gc`",
        ""
      ],
      "context_after": [
        "---",
        "",
        "## File Quick Links",
        "",
        "- **Main API**: `cortical/processor.py` - `CorticalTextProcessor` class",
        "- **Graph algorithms**: `cortical/analysis.py` - PageRank, TF-IDF, clustering",
        "- **Search**: `cortical/query.py` - query expansion, document retrieval",
        "- **Data structures**: `cortical/minicolumn.py` - `Minicolumn`, `Edge`",
        "- **Tests**: `tests/test_processor.py` - most comprehensive test file",
        "- **Demo**: `showcase.py` - interactive demonstration"
      ],
      "change_type": "add"
    },
    {
      "file": "TASK_LIST.md",
      "function": "Added validation to key functions:",
      "start_line": 1264,
      "lines_added": [
        "**Status:** [x] Completed (2025-12-11)",
        "**Solution Applied:**",
        "1. Created `cortical/config.py` with `CorticalConfig` dataclass",
        "2. Centralized all magic numbers and defaults:",
        "   - PageRank: damping, iterations, tolerance",
        "   - Clustering: min_cluster_size, cluster_strictness",
        "   - Gap detection: isolation_threshold, well_connected_threshold, etc.",
        "   - Chunking: chunk_size, chunk_overlap",
        "   - Query expansion: max_query_expansions, semantic_expansion_discount",
        "   - Bigram connections: component_weight, chain_weight, cooccurrence_weight",
        "   - Concept connections: min_shared_docs, min_jaccard, embedding_threshold",
        "   - Multi-hop: max_hops, decay_factor, min_path_score",
        "   - Property inheritance: decay_factor, max_depth, boost_factor",
        "   - Relation weights dictionary",
        "3. Added validation in `__post_init__()` for all parameters",
        "4. Added `copy()`, `to_dict()`, and `from_dict()` methods",
        "5. Moved `VALID_RELATION_CHAINS` to config module",
        "6. Updated `__init__.py` to export new classes",
        "7. Created 29 tests in `tests/test_config.py`",
        "",
        "**Usage:**",
        "from cortical import CorticalTextProcessor, CorticalConfig",
        "",
        "# Custom configuration",
        "config = CorticalConfig(",
        "    pagerank_damping=0.9,",
        "    min_cluster_size=5,",
        "    isolation_threshold=0.03",
        ")",
        "processor = CorticalTextProcessor(config=config)"
      ],
      "lines_removed": [
        "**Status:** [ ] Not Started",
        "**Solution:**",
        "@dataclass",
        "class CorticalConfig:",
        "    # PageRank",
        "    pagerank_damping: float = 0.85",
        "    pagerank_iterations: int = 20",
        "    pagerank_tolerance: float = 1e-6",
        "",
        "    # Clustering",
        "    min_cluster_size: int = 3",
        "    cluster_strictness: float = 1.0",
        "",
        "    # Gap detection",
        "    isolation_threshold: float = 0.02",
        "    well_connected_threshold: float = 0.03"
      ],
      "context_before": [
        "- `retrofit_embeddings()`: alpha must be in range (0, 1]",
        "- `create_chunks()`: chunk_size > 0, overlap >= 0, overlap < chunk_size",
        "",
        "Added 9 new tests for parameter validation.",
        "",
        "---",
        "",
        "### 41. Create Configuration Dataclass",
        "",
        "**Files:** New `cortical/config.py`"
      ],
      "context_after": [
        "**Priority:** Medium",
        "",
        "**Problem:**",
        "Magic numbers scattered across modules with no central configuration:",
        "- `gaps.py`: ISOLATION_THRESHOLD=0.02, WELL_CONNECTED_THRESHOLD=0.03",
        "- `query.py`: VALID_RELATION_CHAINS (15 entries)",
        "- `analysis.py`: damping=0.85, iterations=20, tolerance=1e-6",
        "",
        "```python",
        "```",
        "",
        "---",
        "",
        "### 42. Add Simple Query Language Support",
        "",
        "**File:** `cortical/query.py`",
        "**Status:** [ ] Not Started",
        "**Priority:** Low",
        ""
      ],
      "change_type": "modify"
    },
    {
      "file": "TASK_LIST.md",
      "function": "Define terminology used throughout the codebase so searches for concepts find re",
      "start_line": 1918,
      "lines_added": [
        "**Status:** [x] Completed (2025-12-11)",
        "**Solution Applied:**",
        "Created `docs/patterns.md` with 15 usage patterns covering:",
        "",
        "1. **Code Search Patterns** (Patterns 1-4):",
        "   - Code-aware tokenization with identifier splitting",
        "   - Programming concept expansion",
        "   - Intent-based code search",
        "   - Combined code search",
        "",
        "2. **Fingerprint Comparison** (Patterns 5-8):",
        "   - Basic fingerprinting",
        "   - Explain similarity",
        "   - Find similar code blocks",
        "   - Code deduplication",
        "",
        "3. **Intent-Based Querying** (Patterns 9-10):",
        "   - Query intent detection",
        "   - Intent-aware search",
        "",
        "4. **Document Type Boosting** (Patterns 11-12):",
        "   - Boost documentation",
        "   - Search with type filtering",
        "",
        "5. **Configuration Patterns** (Patterns 13-15):",
        "   - Custom configuration",
        "   - Save and restore configuration",
        "   - Domain-specific configurations",
        "",
        "Note: Basic document processing, RAG retrieval, batch operations, and incremental",
        "updates are already covered in `docs/cookbook.md`."
      ],
      "lines_removed": [
        "**Status:** [ ] Not Started",
        "**Patterns:**",
        "- Basic document processing workflow",
        "- RAG retrieval with passages",
        "- Code search with intent parsing",
        "- Fingerprint comparison for similarity",
        "- Batch operations for performance",
        "- Incremental updates"
      ],
      "context_before": [
        "- Feedforward/feedback connections",
        "- PageRank, TF-IDF, damping factor",
        "- Semantic relations (IsA, PartOf, etc.)",
        "- Query expansion, spreading activation",
        "",
        "---",
        "",
        "### 56. Create Usage Patterns Documentation",
        "",
        "**File:** New `docs/patterns.md`"
      ],
      "context_after": [
        "**Priority:** Medium",
        "",
        "**Goal:**",
        "Document common usage patterns and code examples that help answer \"how do I...\" queries.",
        "",
        "",
        "---",
        "",
        "### 58. Git-Compatible Chunk-Based Indexing",
        "",
        "**Files:** `scripts/index_codebase.py`, `cortical/chunk_index.py` (new), `tests/test_chunk_indexing.py` (new)",
        "**Status:** [x] Completed (2025-12-10)",
        "**Priority:** High",
        "",
        "**Problem:**"
      ],
      "change_type": "modify"
    },
    {
      "file": "TASK_LIST.md",
      "function": "Added cross-platform timeout implementation:",
      "start_line": 2070,
      "lines_added": [
        "**Status:** [x] Completed (2025-12-11)",
        "**Solution Applied:**",
        "1. Added `DEFAULT_WARN_SIZE_KB = 1024` constant (1MB default threshold)",
        "2. Added `warn_size_kb` parameter to `ChunkWriter.save()` method",
        "3. Added warning emission when chunk file exceeds threshold",
        "4. Warning includes helpful message suggesting `--compact`",
        "5. Warning can be disabled by passing `warn_size_kb=0`",
        "6. Added 3 tests for warning functionality",
        "",
        "**Files Modified:**",
        "- `cortical/chunk_index.py` - Added warning logic",
        "- `tests/test_chunk_indexing.py` - Added 3 tests",
        "",
        "**Usage:**",
        "writer = ChunkWriter('corpus_chunks')",
        "writer.add_document('large_doc', 'x' * 2_000_000)",
        "",
        "# Default: warn if > 1MB",
        "writer.save()  # May emit warning",
        "",
        "# Custom threshold",
        "writer.save(warn_size_kb=500)  # Warn if > 500KB",
        "",
        "# Disable warning",
        "writer.save(warn_size_kb=0)  # Never warn",
        "**Status:** [x] Completed (2025-12-11)",
        "**Solution Applied:**",
        "Added comprehensive compaction documentation to both files covering:",
        "",
        "1. **When to compact:**",
        "   - After 10+ chunk files accumulate",
        "   - When size warnings appear",
        "   - Before merging branches",
        "   - To clean up deleted entries",
        "",
        "2. **How compaction works:**",
        "   - Reads chunks in timestamp order",
        "   - Replays operations (later timestamps win)",
        "   - Creates single compacted chunk",
        "   - Removes old chunk files",
        "   - Preserves valid cache",
        "",
        "3. **Example commands:**",
        "   - `--compact --use-chunks` for full compaction",
        "   - `--compact --before DATE --use-chunks` for date-based",
        "",
        "4. **Recommended frequency:**",
        "   - Weekly for active development",
        "   - Monthly for maintenance",
        "   - Before major releases",
        "",
        "**Files Modified:**",
        "- `CLAUDE.md` - Added \"Chunk Compaction\" section",
        "- `.claude/skills/corpus-indexer/SKILL.md` - Added compaction section"
      ],
      "lines_removed": [
        "**Status:** [ ] Not Started",
        "**Solution:**",
        "Add a warning when saving chunks that exceed a configurable threshold (e.g., 1MB):",
        "def save(self, warn_size_kb: int = 1024) -> Optional[Path]:",
        "    # ... save logic ...",
        "    if file_path.stat().st_size > warn_size_kb * 1024:",
        "        warnings.warn(f\"Chunk file exceeds {warn_size_kb}KB. Consider running --compact.\")",
        "**Status:** [ ] Not Started",
        "**Solution:**",
        "Add documentation explaining:",
        "- When to use `--compact`",
        "- How compaction works (merges chunks, preserves latest state)",
        "- Recommended compaction frequency",
        "- Example commands"
      ],
      "context_before": [
        "",
        "4. **Limitation documented**: Windows implementation cannot interrupt blocking I/O operations",
        "",
        "5. **Also addressed Task #59**: Renamed `TimeoutError` to `IndexingTimeoutError` to avoid shadowing the built-in",
        "",
        "---",
        "",
        "### 61. Add Chunk Size Warning for Large Chunks",
        "",
        "**Files:** `cortical/chunk_index.py`"
      ],
      "context_after": [
        "**Priority:** Low",
        "",
        "**Problem:**",
        "Large chunk files in git can bloat repository history. There's no warning when chunks exceed a reasonable size threshold.",
        "",
        "```python",
        "```",
        "",
        "---",
        "",
        "### 62. Add Chunk Compaction Documentation",
        "",
        "**Files:** `CLAUDE.md`, `.claude/skills/corpus-indexer/SKILL.md`",
        "**Priority:** Low",
        "",
        "**Problem:**",
        "The `--compact` feature is implemented but not documented in CLAUDE.md or the corpus-indexer skill.",
        "",
        "",
        "---",
        "",
        "## Dog-Fooding Findings (2025-12-10)",
        "",
        "The following issues were identified during a dog-fooding session reviewing the docs folder and testing search quality.",
        "",
        "---",
        "",
        "### 65. Add Document Metadata to Chunk-Based Indexing (Prerequisite)"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/__init__.py",
      "function": "Example:",
      "start_line": 11,
      "lines_added": [
        "from .config import CorticalConfig, get_default_config, VALID_RELATION_CHAINS",
        "    \"CorticalConfig\",",
        "    \"get_default_config\",",
        "    \"VALID_RELATION_CHAINS\","
      ],
      "lines_removed": [],
      "context_before": [
        "    processor = CorticalTextProcessor()",
        "    processor.process_document(\"doc1\", \"Neural networks process information...\")",
        "    processor.compute_all()",
        "    results = processor.find_documents_for_query(\"neural processing\")",
        "\"\"\"",
        "",
        "from .tokenizer import Tokenizer",
        "from .minicolumn import Minicolumn, Edge",
        "from .layers import CorticalLayer, HierarchicalLayer",
        "from .processor import CorticalTextProcessor"
      ],
      "context_after": [
        "",
        "__version__ = \"2.0.0\"",
        "__all__ = [",
        "    \"CorticalTextProcessor\",",
        "    \"CorticalLayer\",",
        "    \"HierarchicalLayer\",",
        "    \"Minicolumn\",",
        "    \"Edge\",",
        "    \"Tokenizer\",",
        "]"
      ],
      "change_type": "add"
    },
    {
      "file": "cortical/chunk_index.py",
      "function": "Chunk Format:",
      "start_line": 25,
      "lines_added": [
        "import warnings",
        "# Default size threshold for chunk size warnings (in KB)",
        "# Chunks larger than this may bloat git history",
        "DEFAULT_WARN_SIZE_KB = 1024  # 1MB",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "            {\"op\": \"delete\", \"doc_id\": \"...\"}",
        "        ]",
        "    }",
        "\"\"\"",
        "",
        "import hashlib",
        "import json",
        "import os",
        "import subprocess",
        "import uuid"
      ],
      "context_after": [
        "from dataclasses import dataclass, field, asdict",
        "from datetime import datetime",
        "from pathlib import Path",
        "from typing import Dict, List, Optional, Tuple, Any",
        "",
        "",
        "# Chunk format version",
        "CHUNK_VERSION = 1",
        "",
        "",
        "@dataclass",
        "class ChunkOperation:",
        "    \"\"\"A single operation in a chunk (add, modify, or delete).\"\"\"",
        "    op: str  # 'add', 'modify', 'delete'",
        "    doc_id: str",
        "    content: Optional[str] = None  # None for delete operations",
        "    mtime: Optional[float] = None  # Modification time",
        "    metadata: Optional[Dict[str, Any]] = None  # Document metadata (doc_type, headings, etc.)",
        ""
      ],
      "change_type": "add"
    },
    {
      "file": "cortical/chunk_index.py",
      "function": "class ChunkWriter:",
      "start_line": 182,
      "lines_added": [
        "    def save(self, warn_size_kb: int = DEFAULT_WARN_SIZE_KB) -> Optional[Path]:",
        "        Args:",
        "            warn_size_kb: Emit a warning if the saved chunk exceeds this size",
        "                in kilobytes. Set to 0 to disable warning. Default is 1024 KB (1MB).",
        ""
      ],
      "lines_removed": [
        "    def save(self) -> Optional[Path]:"
      ],
      "context_before": [
        "        \"\"\"Record a delete operation.\"\"\"",
        "        self.operations.append(ChunkOperation(",
        "            op='delete',",
        "            doc_id=doc_id",
        "        ))",
        "",
        "    def has_operations(self) -> bool:",
        "        \"\"\"Check if any operations were recorded.\"\"\"",
        "        return len(self.operations) > 0",
        ""
      ],
      "context_after": [
        "        \"\"\"",
        "        Save chunk to file.",
        "",
        "        Returns:",
        "            Path to saved chunk file, or None if no operations.",
        "        \"\"\"",
        "        if not self.operations:",
        "            return None",
        "",
        "        # Create chunks directory if needed",
        "        self.chunks_dir.mkdir(parents=True, exist_ok=True)",
        "",
        "        # Create chunk"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/chunk_index.py",
      "function": "class ChunkWriter:",
      "start_line": 209,
      "lines_added": [
        "        # Check file size and warn if too large",
        "        if warn_size_kb > 0:",
        "            file_size_bytes = filepath.stat().st_size",
        "            file_size_kb = file_size_bytes / 1024",
        "            if file_size_kb > warn_size_kb:",
        "                warnings.warn(",
        "                    f\"Chunk file '{filepath.name}' is {file_size_kb:.1f}KB \"",
        "                    f\"(exceeds {warn_size_kb}KB threshold). \"",
        "                    f\"Large chunks may bloat git history. \"",
        "                    f\"Consider running --compact to consolidate old chunks.\",",
        "                    UserWarning",
        "                )",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "            session_id=self.session_id,",
        "            branch=self.branch,",
        "            operations=self.operations",
        "        )",
        "",
        "        # Write to file",
        "        filepath = self.chunks_dir / chunk.get_filename()",
        "        with open(filepath, 'w', encoding='utf-8') as f:",
        "            json.dump(chunk.to_dict(), f, indent=2, ensure_ascii=False)",
        ""
      ],
      "context_after": [
        "        return filepath",
        "",
        "",
        "class ChunkLoader:",
        "    \"\"\"",
        "    Loads and combines chunks to rebuild document state.",
        "",
        "    Usage:",
        "        loader = ChunkLoader(chunks_dir='corpus_chunks')",
        "        documents = loader.load_all()  # Returns {doc_id: content}"
      ],
      "change_type": "add"
    },
    {
      "file": "cortical/config.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "\"\"\"",
        "Configuration Module",
        "====================",
        "",
        "Centralized configuration for the Cortical Text Processor.",
        "",
        "This module provides a dataclass-based configuration system that allows",
        "users to customize algorithm parameters, thresholds, and defaults without",
        "modifying the source code.",
        "",
        "Example:",
        "    from cortical import CorticalTextProcessor, CorticalConfig",
        "",
        "    # Use custom configuration",
        "    config = CorticalConfig(",
        "        pagerank_damping=0.9,",
        "        min_cluster_size=5,",
        "        isolation_threshold=0.03",
        "    )",
        "    processor = CorticalTextProcessor(config=config)",
        "",
        "    # Or modify defaults",
        "    config = CorticalConfig()",
        "    config.pagerank_iterations = 50",
        "    processor = CorticalTextProcessor(config=config)",
        "\"\"\"",
        "",
        "from dataclasses import dataclass, field",
        "from typing import Dict, Tuple, FrozenSet",
        "",
        "",
        "@dataclass",
        "class CorticalConfig:",
        "    \"\"\"",
        "    Configuration settings for the Cortical Text Processor.",
        "",
        "    All values have sensible defaults that work well for typical text corpora.",
        "    Adjust these based on your specific use case:",
        "    - Smaller corpora may need lower thresholds",
        "    - Specialized domains may need different relation weights",
        "    - Performance-critical applications may want fewer iterations",
        "",
        "    Attributes:",
        "        pagerank_damping: Damping factor for PageRank (0-1). Higher values",
        "            give more weight to link structure vs uniform distribution.",
        "        pagerank_iterations: Maximum PageRank iterations before stopping.",
        "        pagerank_tolerance: Convergence threshold for PageRank. Algorithm",
        "            stops when max change between iterations is below this value.",
        "",
        "        min_cluster_size: Minimum nodes required to form a concept cluster.",
        "            Smaller values create more fine-grained concepts.",
        "        cluster_strictness: Controls clustering aggressiveness (0.0-1.0).",
        "            Lower values allow more cross-topic mixing.",
        "",
        "        isolation_threshold: Documents below this average similarity are",
        "            considered isolated from the corpus.",
        "        well_connected_threshold: Documents above this average similarity",
        "            are considered well-integrated.",
        "        weak_topic_tfidf_threshold: Terms above this TF-IDF are considered",
        "            significant topics.",
        "        bridge_similarity_min: Minimum similarity for bridge opportunities.",
        "        bridge_similarity_max: Maximum similarity for bridge opportunities.",
        "",
        "        chunk_size: Default chunk size for passage retrieval (in characters).",
        "        chunk_overlap: Default overlap between chunks (in characters).",
        "",
        "        max_query_expansions: Maximum expansion terms to add to queries.",
        "        semantic_expansion_discount: Weight discount for semantic expansions",
        "            relative to lateral connection expansions.",
        "",
        "        cross_layer_damping: Damping at layer boundaries for hierarchical",
        "            PageRank propagation.",
        "",
        "        relation_weights: Weights for semantic relation types. Higher weights",
        "            increase influence of that relation type in algorithms.",
        "    \"\"\"",
        "",
        "    # PageRank settings",
        "    pagerank_damping: float = 0.85",
        "    pagerank_iterations: int = 20",
        "    pagerank_tolerance: float = 1e-6",
        "",
        "    # Clustering settings",
        "    min_cluster_size: int = 3",
        "    cluster_strictness: float = 1.0",
        "",
        "    # Gap detection thresholds",
        "    isolation_threshold: float = 0.02",
        "    well_connected_threshold: float = 0.03",
        "    weak_topic_tfidf_threshold: float = 0.005",
        "    bridge_similarity_min: float = 0.005",
        "    bridge_similarity_max: float = 0.03",
        "",
        "    # Chunking settings for RAG",
        "    chunk_size: int = 512",
        "    chunk_overlap: int = 128",
        "",
        "    # Query expansion settings",
        "    max_query_expansions: int = 10",
        "    semantic_expansion_discount: float = 0.7",
        "",
        "    # Cross-layer propagation",
        "    cross_layer_damping: float = 0.7",
        "",
        "    # Bigram connection weights",
        "    bigram_component_weight: float = 0.5",
        "    bigram_chain_weight: float = 0.7",
        "    bigram_cooccurrence_weight: float = 0.3",
        "",
        "    # Concept connection thresholds",
        "    concept_min_shared_docs: int = 1",
        "    concept_min_jaccard: float = 0.1",
        "    concept_embedding_threshold: float = 0.3",
        "",
        "    # Multi-hop expansion settings",
        "    multihop_max_hops: int = 2",
        "    multihop_decay_factor: float = 0.5",
        "    multihop_min_path_score: float = 0.3",
        "",
        "    # Property inheritance settings",
        "    inheritance_decay_factor: float = 0.7",
        "    inheritance_max_depth: int = 5",
        "    inheritance_boost_factor: float = 0.3",
        "",
        "    # Relation weights for semantic algorithms",
        "    relation_weights: Dict[str, float] = field(default_factory=lambda: {",
        "        'IsA': 1.5,",
        "        'PartOf': 1.2,",
        "        'HasA': 1.0,",
        "        'UsedFor': 0.8,",
        "        'CapableOf': 0.7,",
        "        'HasProperty': 1.1,",
        "        'SimilarTo': 1.3,",
        "        'RelatedTo': 1.0,",
        "        'Causes': 1.0,",
        "        'Antonym': 0.3,",
        "        'DerivedFrom': 1.1,",
        "        'AtLocation': 0.9,",
        "        'CoOccurs': 0.8,",
        "    })",
        "",
        "    def __post_init__(self):",
        "        \"\"\"Validate configuration values after initialization.\"\"\"",
        "        self._validate()",
        "",
        "    def _validate(self):",
        "        \"\"\"",
        "        Validate configuration values are within acceptable ranges.",
        "",
        "        Raises:",
        "            ValueError: If any configuration value is invalid.",
        "        \"\"\"",
        "        # PageRank validation",
        "        if not (0 < self.pagerank_damping < 1):",
        "            raise ValueError(",
        "                f\"pagerank_damping must be between 0 and 1, got {self.pagerank_damping}\"",
        "            )",
        "        if self.pagerank_iterations < 1:",
        "            raise ValueError(",
        "                f\"pagerank_iterations must be at least 1, got {self.pagerank_iterations}\"",
        "            )",
        "        if self.pagerank_tolerance <= 0:",
        "            raise ValueError(",
        "                f\"pagerank_tolerance must be positive, got {self.pagerank_tolerance}\"",
        "            )",
        "",
        "        # Clustering validation",
        "        if self.min_cluster_size < 1:",
        "            raise ValueError(",
        "                f\"min_cluster_size must be at least 1, got {self.min_cluster_size}\"",
        "            )",
        "        if not (0 <= self.cluster_strictness <= 1):",
        "            raise ValueError(",
        "                f\"cluster_strictness must be between 0 and 1, got {self.cluster_strictness}\"",
        "            )",
        "",
        "        # Threshold validation",
        "        if self.isolation_threshold < 0:",
        "            raise ValueError(",
        "                f\"isolation_threshold must be non-negative, got {self.isolation_threshold}\"",
        "            )",
        "        if self.well_connected_threshold < 0:",
        "            raise ValueError(",
        "                f\"well_connected_threshold must be non-negative, got {self.well_connected_threshold}\"",
        "            )",
        "        if self.weak_topic_tfidf_threshold < 0:",
        "            raise ValueError(",
        "                f\"weak_topic_tfidf_threshold must be non-negative, got {self.weak_topic_tfidf_threshold}\"",
        "            )",
        "",
        "        # Chunking validation",
        "        if self.chunk_size < 1:",
        "            raise ValueError(",
        "                f\"chunk_size must be at least 1, got {self.chunk_size}\"",
        "            )",
        "        if self.chunk_overlap < 0:",
        "            raise ValueError(",
        "                f\"chunk_overlap must be non-negative, got {self.chunk_overlap}\"",
        "            )",
        "        if self.chunk_overlap >= self.chunk_size:",
        "            raise ValueError(",
        "                f\"chunk_overlap ({self.chunk_overlap}) must be less than chunk_size ({self.chunk_size})\"",
        "            )",
        "",
        "        # Query expansion validation",
        "        if self.max_query_expansions < 0:",
        "            raise ValueError(",
        "                f\"max_query_expansions must be non-negative, got {self.max_query_expansions}\"",
        "            )",
        "        if not (0 <= self.semantic_expansion_discount <= 1):",
        "            raise ValueError(",
        "                f\"semantic_expansion_discount must be between 0 and 1, got {self.semantic_expansion_discount}\"",
        "            )",
        "",
        "        # Cross-layer damping validation",
        "        if not (0 < self.cross_layer_damping < 1):",
        "            raise ValueError(",
        "                f\"cross_layer_damping must be between 0 and 1, got {self.cross_layer_damping}\"",
        "            )",
        "",
        "    def copy(self) -> 'CorticalConfig':",
        "        \"\"\"",
        "        Create a copy of this configuration.",
        "",
        "        Returns:",
        "            A new CorticalConfig instance with the same values.",
        "        \"\"\"",
        "        return CorticalConfig(",
        "            pagerank_damping=self.pagerank_damping,",
        "            pagerank_iterations=self.pagerank_iterations,",
        "            pagerank_tolerance=self.pagerank_tolerance,",
        "            min_cluster_size=self.min_cluster_size,",
        "            cluster_strictness=self.cluster_strictness,",
        "            isolation_threshold=self.isolation_threshold,",
        "            well_connected_threshold=self.well_connected_threshold,",
        "            weak_topic_tfidf_threshold=self.weak_topic_tfidf_threshold,",
        "            bridge_similarity_min=self.bridge_similarity_min,",
        "            bridge_similarity_max=self.bridge_similarity_max,",
        "            chunk_size=self.chunk_size,",
        "            chunk_overlap=self.chunk_overlap,",
        "            max_query_expansions=self.max_query_expansions,",
        "            semantic_expansion_discount=self.semantic_expansion_discount,",
        "            cross_layer_damping=self.cross_layer_damping,",
        "            bigram_component_weight=self.bigram_component_weight,",
        "            bigram_chain_weight=self.bigram_chain_weight,",
        "            bigram_cooccurrence_weight=self.bigram_cooccurrence_weight,",
        "            concept_min_shared_docs=self.concept_min_shared_docs,",
        "            concept_min_jaccard=self.concept_min_jaccard,",
        "            concept_embedding_threshold=self.concept_embedding_threshold,",
        "            multihop_max_hops=self.multihop_max_hops,",
        "            multihop_decay_factor=self.multihop_decay_factor,",
        "            multihop_min_path_score=self.multihop_min_path_score,",
        "            inheritance_decay_factor=self.inheritance_decay_factor,",
        "            inheritance_max_depth=self.inheritance_max_depth,",
        "            inheritance_boost_factor=self.inheritance_boost_factor,",
        "            relation_weights=dict(self.relation_weights),",
        "        )",
        "",
        "    def to_dict(self) -> Dict:",
        "        \"\"\"",
        "        Convert configuration to a dictionary for serialization.",
        "",
        "        Returns:",
        "            Dictionary representation of the configuration.",
        "        \"\"\"",
        "        return {",
        "            'pagerank_damping': self.pagerank_damping,",
        "            'pagerank_iterations': self.pagerank_iterations,",
        "            'pagerank_tolerance': self.pagerank_tolerance,",
        "            'min_cluster_size': self.min_cluster_size,",
        "            'cluster_strictness': self.cluster_strictness,",
        "            'isolation_threshold': self.isolation_threshold,",
        "            'well_connected_threshold': self.well_connected_threshold,",
        "            'weak_topic_tfidf_threshold': self.weak_topic_tfidf_threshold,",
        "            'bridge_similarity_min': self.bridge_similarity_min,",
        "            'bridge_similarity_max': self.bridge_similarity_max,",
        "            'chunk_size': self.chunk_size,",
        "            'chunk_overlap': self.chunk_overlap,",
        "            'max_query_expansions': self.max_query_expansions,",
        "            'semantic_expansion_discount': self.semantic_expansion_discount,",
        "            'cross_layer_damping': self.cross_layer_damping,",
        "            'bigram_component_weight': self.bigram_component_weight,",
        "            'bigram_chain_weight': self.bigram_chain_weight,",
        "            'bigram_cooccurrence_weight': self.bigram_cooccurrence_weight,",
        "            'concept_min_shared_docs': self.concept_min_shared_docs,",
        "            'concept_min_jaccard': self.concept_min_jaccard,",
        "            'concept_embedding_threshold': self.concept_embedding_threshold,",
        "            'multihop_max_hops': self.multihop_max_hops,",
        "            'multihop_decay_factor': self.multihop_decay_factor,",
        "            'multihop_min_path_score': self.multihop_min_path_score,",
        "            'inheritance_decay_factor': self.inheritance_decay_factor,",
        "            'inheritance_max_depth': self.inheritance_max_depth,",
        "            'inheritance_boost_factor': self.inheritance_boost_factor,",
        "            'relation_weights': dict(self.relation_weights),",
        "        }",
        "",
        "    @classmethod",
        "    def from_dict(cls, data: Dict) -> 'CorticalConfig':",
        "        \"\"\"",
        "        Create configuration from a dictionary.",
        "",
        "        Args:",
        "            data: Dictionary with configuration values.",
        "",
        "        Returns:",
        "            CorticalConfig instance.",
        "        \"\"\"",
        "        return cls(**data)",
        "",
        "",
        "# Valid relation chains for multi-hop inference",
        "# Maps (relation1, relation2) -> validity score (0.0 to 1.0)",
        "# Higher scores indicate more semantically valid inference chains",
        "VALID_RELATION_CHAINS: Dict[Tuple[str, str], float] = {",
        "    # Transitive hierarchies",
        "    ('IsA', 'IsA'): 1.0,           # dog IsA animal IsA living_thing",
        "    ('PartOf', 'PartOf'): 1.0,     # wheel PartOf car PartOf vehicle",
        "    ('IsA', 'HasProperty'): 0.9,   # dog IsA animal HasProperty alive",
        "    ('PartOf', 'HasProperty'): 0.8,  # wheel PartOf car HasProperty fast",
        "",
        "    # Association chains",
        "    ('RelatedTo', 'RelatedTo'): 0.6,",
        "    ('SimilarTo', 'SimilarTo'): 0.7,",
        "    ('CoOccurs', 'CoOccurs'): 0.5,",
        "    ('RelatedTo', 'IsA'): 0.7,",
        "    ('RelatedTo', 'SimilarTo'): 0.7,",
        "",
        "    # Causal chains",
        "    ('Causes', 'Causes'): 0.8,",
        "    ('Causes', 'HasProperty'): 0.7,",
        "",
        "    # Derivation chains",
        "    ('DerivedFrom', 'DerivedFrom'): 0.8,",
        "    ('DerivedFrom', 'IsA'): 0.7,",
        "",
        "    # Invalid/contradictory chains (low scores)",
        "    ('Antonym', 'IsA'): 0.1,       # Contradictory: opposite → type",
        "    ('Antonym', 'Antonym'): 0.4,   # Double negation",
        "}",
        "",
        "# Default validity score for unlisted relation chain pairs",
        "DEFAULT_CHAIN_VALIDITY: float = 0.4",
        "",
        "",
        "def get_default_config() -> CorticalConfig:",
        "    \"\"\"",
        "    Get a new instance of the default configuration.",
        "",
        "    Returns:",
        "        CorticalConfig with default values.",
        "    \"\"\"",
        "    return CorticalConfig()"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "docs/patterns.md",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "# Usage Patterns Guide",
        "",
        "Advanced usage patterns for the Cortical Text Processor, focusing on code-aware search, semantic fingerprinting, and intent-based querying.",
        "",
        "---",
        "",
        "## Table of Contents",
        "",
        "1. [Code Search Patterns](#code-search-patterns)",
        "2. [Fingerprint Comparison](#fingerprint-comparison)",
        "3. [Intent-Based Querying](#intent-based-querying)",
        "4. [Document Type Boosting](#document-type-boosting)",
        "5. [Configuration Patterns](#configuration-patterns)",
        "",
        "---",
        "",
        "## Code Search Patterns",
        "",
        "### Pattern 1: Code-Aware Tokenization",
        "",
        "Enable identifier splitting to search for code patterns:",
        "",
        "```python",
        "from cortical import CorticalTextProcessor, Tokenizer",
        "",
        "# Create processor with code-aware tokenizer",
        "tokenizer = Tokenizer(split_identifiers=True)",
        "processor = CorticalTextProcessor()",
        "",
        "# Process code with identifier splitting",
        "processor.process_document(",
        "    \"auth.py\",",
        "    \"\"\"",
        "    def getUserCredentials(userId):",
        "        return fetchUserFromDB(userId).credentials",
        "    \"\"\"",
        ")",
        "",
        "# Now searches for \"user\" will find \"getUserCredentials\"",
        "# because it was split into [\"get\", \"user\", \"credentials\"]",
        "```",
        "",
        "**What identifier splitting does:**",
        "- `getUserCredentials` → `[\"getusercredentials\", \"get\", \"user\", \"credentials\"]`",
        "- `fetch_user_from_db` → `[\"fetch\", \"user\", \"from\", \"db\"]`",
        "- `HTTPResponseCode` → `[\"httpresponsecode\", \"http\", \"response\", \"code\"]`",
        "",
        "---",
        "",
        "### Pattern 2: Programming Concept Expansion",
        "",
        "Expand queries with programming synonyms:",
        "",
        "```python",
        "# Search with code concept expansion",
        "results = processor.find_documents_for_query(",
        "    \"fetch user data\",",
        "    use_code_concepts=True",
        ")",
        "",
        "# Or use dedicated method",
        "results = processor.expand_query_for_code(\"fetch user data\")",
        "# Expands \"fetch\" to include: get, retrieve, load, read, query",
        "# Expands \"user\" to include: account, profile, member",
        "```",
        "",
        "**Built-in concept groups:**",
        "",
        "| Concept Group | Terms |",
        "|--------------|-------|",
        "| retrieval | get, fetch, retrieve, load, read, query |",
        "| storage | save, store, write, persist, cache, put |",
        "| deletion | delete, remove, drop, clear, purge, erase |",
        "| auth | auth, authenticate, authorize, login, signin |",
        "| error | error, exception, fail, invalid, wrong |",
        "| validation | validate, check, verify, assert, ensure |",
        "| transform | convert, transform, parse, serialize, encode |",
        "| async | async, await, promise, future, callback |",
        "",
        "---",
        "",
        "### Pattern 3: Intent-Based Code Search",
        "",
        "Search by developer intent rather than exact keywords:",
        "",
        "```python",
        "# Parse natural language query into structured intent",
        "parsed = processor.parse_intent_query(\"where do we handle authentication?\")",
        "print(parsed)",
        "# {",
        "#   'intent': 'location',        # What type of answer expected",
        "#   'action': 'handle',          # The action verb",
        "#   'subject': 'authentication', # What the action operates on",
        "#   'question_word': 'where',",
        "#   'expanded_terms': ['handle', 'authentication', 'auth', 'login', ...]",
        "# }",
        "",
        "# Search with intent understanding",
        "results = processor.search_by_intent(\"how do we validate user input?\")",
        "# Returns documents relevant to validation, input checking, assertions",
        "```",
        "",
        "**Supported intent types:**",
        "",
        "| Question Word | Intent Type | Typical Results |",
        "|--------------|-------------|-----------------|",
        "| where | location | File paths, module locations |",
        "| how | implementation | Code implementation details |",
        "| what | definition | Type definitions, interfaces |",
        "| why | rationale | Comments, design decisions |",
        "| when | lifecycle | Initialization, cleanup code |",
        "",
        "---",
        "",
        "### Pattern 4: Combined Code Search",
        "",
        "Combine multiple code search features:",
        "",
        "```python",
        "# Full code search with all features",
        "results = processor.find_documents_for_query(",
        "    \"authentication handler\",",
        "    use_expansion=True,           # Lateral connection expansion",
        "    use_code_concepts=True,       # Programming synonyms",
        "    use_semantic=True             # Semantic relations",
        ")",
        "",
        "# Or use intent search for natural language",
        "results = processor.search_by_intent(",
        "    \"where is the password validation logic?\"",
        ")",
        "```",
        "",
        "---",
        "",
        "## Fingerprint Comparison",
        "",
        "Semantic fingerprinting enables comparing the meaning of code blocks without embedding models.",
        "",
        "### Pattern 5: Basic Fingerprinting",
        "",
        "```python",
        "# Get semantic fingerprint of a code block",
        "code1 = \"\"\"",
        "def authenticate_user(username, password):",
        "    user = database.find_user(username)",
        "    if user and verify_password(password, user.hash):",
        "        return create_session(user)",
        "    return None",
        "\"\"\"",
        "",
        "code2 = \"\"\"",
        "def login(name, pwd):",
        "    account = db.get_account(name)",
        "    if account and check_pwd(pwd, account.password_hash):",
        "        return generate_token(account)",
        "    return None",
        "\"\"\"",
        "",
        "fp1 = processor.get_fingerprint(code1)",
        "fp2 = processor.get_fingerprint(code2)",
        "",
        "# Compare fingerprints",
        "comparison = processor.compare_fingerprints(fp1, fp2)",
        "print(f\"Similarity: {comparison['similarity']:.2%}\")",
        "# Output: Similarity: 78.5%",
        "```",
        "",
        "**Fingerprint contents:**",
        "```python",
        "{",
        "    'terms': {'user': 0.8, 'password': 0.7, 'authenticate': 0.6, ...},",
        "    'concepts': ['L2_authentication', 'L2_database_operations'],",
        "    'bigrams': ['find user', 'verify password', ...],",
        "    'top_terms': [('user', 0.8), ('password', 0.7), ...]",
        "}",
        "```",
        "",
        "---",
        "",
        "### Pattern 6: Explain Similarity",
        "",
        "Get human-readable explanation of why two code blocks are similar:",
        "",
        "```python",
        "explanation = processor.explain_similarity(fp1, fp2)",
        "print(explanation)",
        "# Output:",
        "# Shared terms: user (0.8), password (0.7), database (0.5)",
        "# Shared concepts: authentication (2), database_operations (1)",
        "# Both use patterns: user lookup, password verification",
        "",
        "# Or explain a single fingerprint",
        "fp_explanation = processor.explain_fingerprint(fp1)",
        "print(fp_explanation)",
        "# Output:",
        "# Main concepts: authentication, database access",
        "# Key terms: user (0.8), password (0.7), authenticate (0.6)",
        "# Notable bigrams: verify password, create session",
        "```",
        "",
        "---",
        "",
        "### Pattern 7: Find Similar Code Blocks",
        "",
        "Search for similar code across the corpus:",
        "",
        "```python",
        "# Find code blocks similar to a reference",
        "target_code = \"\"\"",
        "def validate_email(email):",
        "    pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'",
        "    return re.match(pattern, email) is not None",
        "\"\"\"",
        "",
        "similar = processor.find_similar_texts(",
        "    target_code,",
        "    top_n=5,",
        "    min_similarity=0.3",
        ")",
        "",
        "for doc_id, similarity in similar:",
        "    print(f\"{doc_id}: {similarity:.2%} similar\")",
        "```",
        "",
        "---",
        "",
        "### Pattern 8: Code Deduplication",
        "",
        "Use fingerprints to detect duplicate or near-duplicate code:",
        "",
        "```python",
        "def find_duplicates(processor, min_similarity=0.85):",
        "    \"\"\"Find potentially duplicate code blocks.\"\"\"",
        "    docs = processor.documents",
        "    fingerprints = {",
        "        doc_id: processor.get_fingerprint(content)",
        "        for doc_id, content in docs.items()",
        "    }",
        "",
        "    duplicates = []",
        "    doc_ids = list(fingerprints.keys())",
        "",
        "    for i, doc_id1 in enumerate(doc_ids):",
        "        for doc_id2 in doc_ids[i+1:]:",
        "            result = processor.compare_fingerprints(",
        "                fingerprints[doc_id1],",
        "                fingerprints[doc_id2]",
        "            )",
        "            if result['similarity'] >= min_similarity:",
        "                duplicates.append((doc_id1, doc_id2, result['similarity']))",
        "",
        "    return sorted(duplicates, key=lambda x: -x[2])",
        "",
        "# Find duplicates",
        "dupes = find_duplicates(processor, min_similarity=0.9)",
        "for doc1, doc2, sim in dupes:",
        "    print(f\"Possible duplicate: {doc1} <-> {doc2} ({sim:.1%})\")",
        "```",
        "",
        "---",
        "",
        "## Intent-Based Querying",
        "",
        "### Pattern 9: Query Intent Detection",
        "",
        "Let the system auto-detect query intent:",
        "",
        "```python",
        "# The system detects query type",
        "queries = [",
        "    \"what is PageRank\",           # Conceptual (wants definition)",
        "    \"where is PageRank computed\", # Implementation (wants location)",
        "    \"how does PageRank work\",     # Implementation (wants details)",
        "]",
        "",
        "for query in queries:",
        "    is_conceptual = processor.is_conceptual_query(query)",
        "    query_type = \"conceptual\" if is_conceptual else \"implementation\"",
        "    print(f\"{query} -> {query_type}\")",
        "```",
        "",
        "---",
        "",
        "### Pattern 10: Intent-Aware Search",
        "",
        "Search with intent understanding:",
        "",
        "```python",
        "# Conceptual query: boost documentation",
        "results = processor.search_by_intent(\"what is the 4-layer architecture?\")",
        "# Will prefer: docs/architecture.md over cortical/processor.py",
        "",
        "# Implementation query: boost code",
        "results = processor.search_by_intent(\"where is PageRank computed?\")",
        "# Will prefer: cortical/analysis.py over docs/algorithms.md",
        "```",
        "",
        "---",
        "",
        "## Document Type Boosting",
        "",
        "### Pattern 11: Boost Documentation",
        "",
        "When searching for concepts, boost documentation files:",
        "",
        "```python",
        "# Auto-detect intent and boost accordingly",
        "results = processor.find_documents_with_boost(",
        "    \"PageRank algorithm\",",
        "    auto_detect_intent=True,  # Auto-boost docs for conceptual queries",
        "    top_n=5",
        ")",
        "",
        "# Always prefer documentation",
        "results = processor.find_documents_with_boost(",
        "    \"PageRank algorithm\",",
        "    prefer_docs=True,         # Always boost docs",
        "    top_n=5",
        ")",
        "",
        "# Custom boost factors",
        "results = processor.find_documents_with_boost(",
        "    \"PageRank algorithm\",",
        "    custom_boosts={",
        "        'docs': 2.0,    # Double weight for docs/ folder",
        "        'root_docs': 1.5,  # 1.5x for root-level .md",
        "        'code': 1.0,    # Normal weight for code",
        "        'test': 0.5     # Half weight for tests",
        "    }",
        ")",
        "```",
        "",
        "---",
        "",
        "### Pattern 12: Search with Type Filtering",
        "",
        "Limit search to specific document types:",
        "",
        "```python",
        "# Search only in documentation",
        "doc_results = [",
        "    (doc_id, score)",
        "    for doc_id, score in processor.find_documents_for_query(\"PageRank\")",
        "    if doc_id.endswith('.md') or doc_id.startswith('docs/')",
        "]",
        "",
        "# Search only in code",
        "code_results = [",
        "    (doc_id, score)",
        "    for doc_id, score in processor.find_documents_for_query(\"PageRank\")",
        "    if doc_id.endswith('.py') and not doc_id.startswith('tests/')",
        "]",
        "```",
        "",
        "---",
        "",
        "## Configuration Patterns",
        "",
        "### Pattern 13: Custom Configuration",
        "",
        "Use custom configuration for specific use cases:",
        "",
        "```python",
        "from cortical import CorticalTextProcessor, CorticalConfig",
        "",
        "# High-precision configuration (less expansion, stricter clustering)",
        "precision_config = CorticalConfig(",
        "    max_query_expansions=5,",
        "    cluster_strictness=1.0,",
        "    pagerank_damping=0.85",
        ")",
        "",
        "# High-recall configuration (more expansion, looser clustering)",
        "recall_config = CorticalConfig(",
        "    max_query_expansions=20,",
        "    cluster_strictness=0.5,",
        "    semantic_expansion_discount=0.8",
        ")",
        "",
        "# Create processor with configuration",
        "processor = CorticalTextProcessor(config=precision_config)",
        "```",
        "",
        "---",
        "",
        "### Pattern 14: Save and Restore Configuration",
        "",
        "```python",
        "# Save configuration with corpus",
        "config = CorticalConfig(pagerank_damping=0.9, min_cluster_size=5)",
        "processor = CorticalTextProcessor(config=config)",
        "processor.add_documents_batch(docs, recompute='full')",
        "processor.save(\"corpus.pkl\")  # Config saved with corpus",
        "",
        "# Load and check configuration",
        "loaded = CorticalTextProcessor.load(\"corpus.pkl\")",
        "print(f\"Loaded config: {loaded.config.pagerank_damping}\")",
        "",
        "# Or export/import config separately",
        "config_dict = config.to_dict()",
        "# Save to JSON",
        "import json",
        "with open(\"config.json\", \"w\") as f:",
        "    json.dump(config_dict, f)",
        "",
        "# Restore",
        "with open(\"config.json\") as f:",
        "    restored_config = CorticalConfig.from_dict(json.load(f))",
        "```",
        "",
        "---",
        "",
        "### Pattern 15: Domain-Specific Configurations",
        "",
        "```python",
        "# Code search configuration",
        "code_config = CorticalConfig(",
        "    chunk_size=256,              # Smaller chunks for code",
        "    chunk_overlap=64,",
        "    max_query_expansions=15,     # More expansion for code synonyms",
        ")",
        "",
        "# Documentation search configuration",
        "docs_config = CorticalConfig(",
        "    chunk_size=1024,             # Larger chunks for prose",
        "    chunk_overlap=256,",
        "    max_query_expansions=8,",
        ")",
        "",
        "# RAG-optimized configuration",
        "rag_config = CorticalConfig(",
        "    chunk_size=512,",
        "    chunk_overlap=128,",
        "    pagerank_iterations=30,      # More iterations for stability",
        "    cluster_strictness=0.7,      # Balanced clustering",
        ")",
        "```",
        "",
        "---",
        "",
        "## Quick Reference",
        "",
        "| Pattern | Use Case | Key Method |",
        "|---------|----------|------------|",
        "| Code tokenization | Index code files | `Tokenizer(split_identifiers=True)` |",
        "| Code concepts | Expand with synonyms | `expand_query_for_code()` |",
        "| Intent search | Natural language | `search_by_intent()` |",
        "| Fingerprinting | Compare code blocks | `get_fingerprint()`, `compare_fingerprints()` |",
        "| Similarity search | Find duplicates | `find_similar_texts()` |",
        "| Doc boosting | Prefer documentation | `find_documents_with_boost()` |",
        "| Custom config | Tune behavior | `CorticalConfig()` |",
        "",
        "---",
        "",
        "*See also: [Cookbook](cookbook.md) for more recipes, [Query Guide](query-guide.md) for query details.*"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "tests/test_chunk_indexing.py",
      "function": "class TestChunkWriter(unittest.TestCase):",
      "start_line": 215,
      "lines_added": [
        "    def test_save_no_warning_small_chunk(self):",
        "        \"\"\"Test that small chunks don't trigger a warning.\"\"\"",
        "        import warnings",
        "        with tempfile.TemporaryDirectory() as tmpdir:",
        "            writer = ChunkWriter(tmpdir)",
        "            writer.add_document('doc1', 'small content')",
        "",
        "            # Should not warn",
        "            with warnings.catch_warnings(record=True) as w:",
        "                warnings.simplefilter(\"always\")",
        "                filepath = writer.save(warn_size_kb=100)  # 100KB threshold",
        "                # Small chunk should not trigger warning",
        "                self.assertEqual(len(w), 0)",
        "                self.assertIsNotNone(filepath)",
        "",
        "    def test_save_warning_large_chunk(self):",
        "        \"\"\"Test that large chunks trigger a warning.\"\"\"",
        "        import warnings",
        "        with tempfile.TemporaryDirectory() as tmpdir:",
        "            writer = ChunkWriter(tmpdir)",
        "            # Add large content (>1KB)",
        "            large_content = 'x' * 2000  # 2KB+ of content",
        "            writer.add_document('doc1', large_content)",
        "",
        "            with warnings.catch_warnings(record=True) as w:",
        "                warnings.simplefilter(\"always\")",
        "                filepath = writer.save(warn_size_kb=1)  # 1KB threshold",
        "                # Should trigger warning",
        "                self.assertEqual(len(w), 1)",
        "                self.assertIn('exceeds', str(w[0].message))",
        "                self.assertIn('compact', str(w[0].message).lower())",
        "                self.assertIsNotNone(filepath)",
        "",
        "    def test_save_warning_disabled(self):",
        "        \"\"\"Test that warning can be disabled with warn_size_kb=0.\"\"\"",
        "        import warnings",
        "        with tempfile.TemporaryDirectory() as tmpdir:",
        "            writer = ChunkWriter(tmpdir)",
        "            large_content = 'x' * 2000",
        "            writer.add_document('doc1', large_content)",
        "",
        "            with warnings.catch_warnings(record=True) as w:",
        "                warnings.simplefilter(\"always\")",
        "                filepath = writer.save(warn_size_kb=0)  # Disabled",
        "                # Should not warn even for large chunk",
        "                self.assertEqual(len(w), 0)",
        "                self.assertIsNotNone(filepath)",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "        \"\"\"Test saving creates the chunks directory if needed.\"\"\"",
        "        with tempfile.TemporaryDirectory() as tmpdir:",
        "            chunks_dir = os.path.join(tmpdir, 'new_chunks')",
        "            writer = ChunkWriter(chunks_dir)",
        "            writer.add_document('doc1', 'content')",
        "            filepath = writer.save()",
        "",
        "            self.assertTrue(os.path.exists(chunks_dir))",
        "            self.assertTrue(filepath.exists())",
        ""
      ],
      "context_after": [
        "",
        "class TestChunkLoader(unittest.TestCase):",
        "    \"\"\"Test ChunkLoader class.\"\"\"",
        "",
        "    def test_loader_empty_directory(self):",
        "        \"\"\"Test loading from empty directory.\"\"\"",
        "        with tempfile.TemporaryDirectory() as tmpdir:",
        "            loader = ChunkLoader(tmpdir)",
        "            docs = loader.load_all()",
        "            self.assertEqual(len(docs), 0)"
      ],
      "change_type": "add"
    },
    {
      "file": "tests/test_config.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "\"\"\"",
        "Tests for the configuration module.",
        "\"\"\"",
        "",
        "import unittest",
        "",
        "from cortical.config import (",
        "    CorticalConfig,",
        "    get_default_config,",
        "    VALID_RELATION_CHAINS,",
        "    DEFAULT_CHAIN_VALIDITY,",
        ")",
        "",
        "",
        "class TestCorticalConfig(unittest.TestCase):",
        "    \"\"\"Tests for CorticalConfig dataclass.\"\"\"",
        "",
        "    def test_default_values(self):",
        "        \"\"\"Test that default values are set correctly.\"\"\"",
        "        config = CorticalConfig()",
        "",
        "        # PageRank defaults",
        "        self.assertEqual(config.pagerank_damping, 0.85)",
        "        self.assertEqual(config.pagerank_iterations, 20)",
        "        self.assertEqual(config.pagerank_tolerance, 1e-6)",
        "",
        "        # Clustering defaults",
        "        self.assertEqual(config.min_cluster_size, 3)",
        "        self.assertEqual(config.cluster_strictness, 1.0)",
        "",
        "        # Chunking defaults",
        "        self.assertEqual(config.chunk_size, 512)",
        "        self.assertEqual(config.chunk_overlap, 128)",
        "",
        "    def test_custom_values(self):",
        "        \"\"\"Test creating config with custom values.\"\"\"",
        "        config = CorticalConfig(",
        "            pagerank_damping=0.9,",
        "            min_cluster_size=5,",
        "            chunk_size=1024",
        "        )",
        "",
        "        self.assertEqual(config.pagerank_damping, 0.9)",
        "        self.assertEqual(config.min_cluster_size, 5)",
        "        self.assertEqual(config.chunk_size, 1024)",
        "        # Other defaults still apply",
        "        self.assertEqual(config.pagerank_iterations, 20)",
        "",
        "    def test_relation_weights_default(self):",
        "        \"\"\"Test that relation weights have sensible defaults.\"\"\"",
        "        config = CorticalConfig()",
        "",
        "        self.assertIn('IsA', config.relation_weights)",
        "        self.assertIn('PartOf', config.relation_weights)",
        "        self.assertIn('RelatedTo', config.relation_weights)",
        "",
        "        # IsA should have high weight",
        "        self.assertGreater(config.relation_weights['IsA'], 1.0)",
        "        # Antonym should have low weight",
        "        self.assertLess(config.relation_weights['Antonym'], 1.0)",
        "",
        "",
        "class TestConfigValidation(unittest.TestCase):",
        "    \"\"\"Tests for configuration validation.\"\"\"",
        "",
        "    def test_invalid_pagerank_damping_too_high(self):",
        "        \"\"\"Test that damping > 1 raises ValueError.\"\"\"",
        "        with self.assertRaises(ValueError) as ctx:",
        "            CorticalConfig(pagerank_damping=1.5)",
        "        self.assertIn('pagerank_damping', str(ctx.exception))",
        "",
        "    def test_invalid_pagerank_damping_too_low(self):",
        "        \"\"\"Test that damping <= 0 raises ValueError.\"\"\"",
        "        with self.assertRaises(ValueError) as ctx:",
        "            CorticalConfig(pagerank_damping=0)",
        "        self.assertIn('pagerank_damping', str(ctx.exception))",
        "",
        "    def test_invalid_pagerank_damping_negative(self):",
        "        \"\"\"Test that negative damping raises ValueError.\"\"\"",
        "        with self.assertRaises(ValueError) as ctx:",
        "            CorticalConfig(pagerank_damping=-0.5)",
        "        self.assertIn('pagerank_damping', str(ctx.exception))",
        "",
        "    def test_invalid_pagerank_iterations(self):",
        "        \"\"\"Test that iterations < 1 raises ValueError.\"\"\"",
        "        with self.assertRaises(ValueError) as ctx:",
        "            CorticalConfig(pagerank_iterations=0)",
        "        self.assertIn('pagerank_iterations', str(ctx.exception))",
        "",
        "    def test_invalid_pagerank_tolerance(self):",
        "        \"\"\"Test that tolerance <= 0 raises ValueError.\"\"\"",
        "        with self.assertRaises(ValueError) as ctx:",
        "            CorticalConfig(pagerank_tolerance=0)",
        "        self.assertIn('pagerank_tolerance', str(ctx.exception))",
        "",
        "    def test_invalid_min_cluster_size(self):",
        "        \"\"\"Test that min_cluster_size < 1 raises ValueError.\"\"\"",
        "        with self.assertRaises(ValueError) as ctx:",
        "            CorticalConfig(min_cluster_size=0)",
        "        self.assertIn('min_cluster_size', str(ctx.exception))",
        "",
        "    def test_invalid_cluster_strictness_too_high(self):",
        "        \"\"\"Test that cluster_strictness > 1 raises ValueError.\"\"\"",
        "        with self.assertRaises(ValueError) as ctx:",
        "            CorticalConfig(cluster_strictness=1.5)",
        "        self.assertIn('cluster_strictness', str(ctx.exception))",
        "",
        "    def test_invalid_cluster_strictness_negative(self):",
        "        \"\"\"Test that cluster_strictness < 0 raises ValueError.\"\"\"",
        "        with self.assertRaises(ValueError) as ctx:",
        "            CorticalConfig(cluster_strictness=-0.1)",
        "        self.assertIn('cluster_strictness', str(ctx.exception))",
        "",
        "    def test_invalid_chunk_size(self):",
        "        \"\"\"Test that chunk_size < 1 raises ValueError.\"\"\"",
        "        with self.assertRaises(ValueError) as ctx:",
        "            CorticalConfig(chunk_size=0)",
        "        self.assertIn('chunk_size', str(ctx.exception))",
        "",
        "    def test_invalid_chunk_overlap_negative(self):",
        "        \"\"\"Test that negative chunk_overlap raises ValueError.\"\"\"",
        "        with self.assertRaises(ValueError) as ctx:",
        "            CorticalConfig(chunk_overlap=-1)",
        "        self.assertIn('chunk_overlap', str(ctx.exception))",
        "",
        "    def test_invalid_chunk_overlap_too_large(self):",
        "        \"\"\"Test that chunk_overlap >= chunk_size raises ValueError.\"\"\"",
        "        with self.assertRaises(ValueError) as ctx:",
        "            CorticalConfig(chunk_size=100, chunk_overlap=100)",
        "        self.assertIn('chunk_overlap', str(ctx.exception))",
        "",
        "    def test_invalid_cross_layer_damping(self):",
        "        \"\"\"Test that cross_layer_damping outside (0,1) raises ValueError.\"\"\"",
        "        with self.assertRaises(ValueError) as ctx:",
        "            CorticalConfig(cross_layer_damping=1.0)",
        "        self.assertIn('cross_layer_damping', str(ctx.exception))",
        "",
        "    def test_invalid_semantic_expansion_discount(self):",
        "        \"\"\"Test that semantic_expansion_discount outside [0,1] raises ValueError.\"\"\"",
        "        with self.assertRaises(ValueError) as ctx:",
        "            CorticalConfig(semantic_expansion_discount=1.5)",
        "        self.assertIn('semantic_expansion_discount', str(ctx.exception))",
        "",
        "    def test_valid_boundary_values(self):",
        "        \"\"\"Test that valid boundary values are accepted.\"\"\"",
        "        # Should not raise",
        "        config = CorticalConfig(",
        "            pagerank_damping=0.99,",
        "            cluster_strictness=0.0,",
        "            semantic_expansion_discount=0.0,",
        "            chunk_overlap=0",
        "        )",
        "        self.assertEqual(config.pagerank_damping, 0.99)",
        "        self.assertEqual(config.cluster_strictness, 0.0)",
        "",
        "",
        "class TestConfigCopy(unittest.TestCase):",
        "    \"\"\"Tests for configuration copying.\"\"\"",
        "",
        "    def test_copy_creates_new_instance(self):",
        "        \"\"\"Test that copy creates a new independent instance.\"\"\"",
        "        original = CorticalConfig(pagerank_damping=0.9)",
        "        copied = original.copy()",
        "",
        "        self.assertIsNot(original, copied)",
        "        self.assertEqual(original.pagerank_damping, copied.pagerank_damping)",
        "",
        "    def test_copy_is_independent(self):",
        "        \"\"\"Test that modifying copy doesn't affect original.\"\"\"",
        "        original = CorticalConfig()",
        "        copied = original.copy()",
        "",
        "        # Modify the copy's relation weights",
        "        copied.relation_weights['IsA'] = 999.0",
        "",
        "        # Original should be unchanged",
        "        self.assertNotEqual(original.relation_weights['IsA'], 999.0)",
        "",
        "    def test_copy_preserves_all_values(self):",
        "        \"\"\"Test that copy preserves all configuration values.\"\"\"",
        "        original = CorticalConfig(",
        "            pagerank_damping=0.9,",
        "            min_cluster_size=5,",
        "            chunk_size=1024,",
        "            isolation_threshold=0.05",
        "        )",
        "        copied = original.copy()",
        "",
        "        self.assertEqual(copied.pagerank_damping, 0.9)",
        "        self.assertEqual(copied.min_cluster_size, 5)",
        "        self.assertEqual(copied.chunk_size, 1024)",
        "        self.assertEqual(copied.isolation_threshold, 0.05)",
        "",
        "",
        "class TestConfigSerialization(unittest.TestCase):",
        "    \"\"\"Tests for configuration serialization.\"\"\"",
        "",
        "    def test_to_dict(self):",
        "        \"\"\"Test converting config to dictionary.\"\"\"",
        "        config = CorticalConfig(pagerank_damping=0.9)",
        "        data = config.to_dict()",
        "",
        "        self.assertIsInstance(data, dict)",
        "        self.assertEqual(data['pagerank_damping'], 0.9)",
        "        self.assertIn('relation_weights', data)",
        "",
        "    def test_from_dict(self):",
        "        \"\"\"Test creating config from dictionary.\"\"\"",
        "        data = {",
        "            'pagerank_damping': 0.9,",
        "            'min_cluster_size': 5,",
        "            'pagerank_iterations': 20,",
        "            'pagerank_tolerance': 1e-6,",
        "            'cluster_strictness': 1.0,",
        "            'isolation_threshold': 0.02,",
        "            'well_connected_threshold': 0.03,",
        "            'weak_topic_tfidf_threshold': 0.005,",
        "            'bridge_similarity_min': 0.005,",
        "            'bridge_similarity_max': 0.03,",
        "            'chunk_size': 512,",
        "            'chunk_overlap': 128,",
        "            'max_query_expansions': 10,",
        "            'semantic_expansion_discount': 0.7,",
        "            'cross_layer_damping': 0.7,",
        "            'bigram_component_weight': 0.5,",
        "            'bigram_chain_weight': 0.7,",
        "            'bigram_cooccurrence_weight': 0.3,",
        "            'concept_min_shared_docs': 1,",
        "            'concept_min_jaccard': 0.1,",
        "            'concept_embedding_threshold': 0.3,",
        "            'multihop_max_hops': 2,",
        "            'multihop_decay_factor': 0.5,",
        "            'multihop_min_path_score': 0.3,",
        "            'inheritance_decay_factor': 0.7,",
        "            'inheritance_max_depth': 5,",
        "            'inheritance_boost_factor': 0.3,",
        "            'relation_weights': {'IsA': 1.5, 'RelatedTo': 1.0}",
        "        }",
        "        config = CorticalConfig.from_dict(data)",
        "",
        "        self.assertEqual(config.pagerank_damping, 0.9)",
        "        self.assertEqual(config.min_cluster_size, 5)",
        "",
        "    def test_roundtrip(self):",
        "        \"\"\"Test that to_dict and from_dict are inverses.\"\"\"",
        "        original = CorticalConfig(",
        "            pagerank_damping=0.9,",
        "            min_cluster_size=5,",
        "            chunk_size=1024",
        "        )",
        "",
        "        data = original.to_dict()",
        "        restored = CorticalConfig.from_dict(data)",
        "",
        "        self.assertEqual(original.pagerank_damping, restored.pagerank_damping)",
        "        self.assertEqual(original.min_cluster_size, restored.min_cluster_size)",
        "        self.assertEqual(original.chunk_size, restored.chunk_size)",
        "",
        "",
        "class TestGetDefaultConfig(unittest.TestCase):",
        "    \"\"\"Tests for get_default_config function.\"\"\"",
        "",
        "    def test_returns_config_instance(self):",
        "        \"\"\"Test that get_default_config returns a CorticalConfig.\"\"\"",
        "        config = get_default_config()",
        "        self.assertIsInstance(config, CorticalConfig)",
        "",
        "    def test_returns_new_instance_each_time(self):",
        "        \"\"\"Test that get_default_config returns new instances.\"\"\"",
        "        config1 = get_default_config()",
        "        config2 = get_default_config()",
        "        self.assertIsNot(config1, config2)",
        "",
        "",
        "class TestValidRelationChains(unittest.TestCase):",
        "    \"\"\"Tests for VALID_RELATION_CHAINS constant.\"\"\"",
        "",
        "    def test_transitive_chains_high_score(self):",
        "        \"\"\"Test that transitive chains have high validity scores.\"\"\"",
        "        self.assertEqual(VALID_RELATION_CHAINS[('IsA', 'IsA')], 1.0)",
        "        self.assertEqual(VALID_RELATION_CHAINS[('PartOf', 'PartOf')], 1.0)",
        "",
        "    def test_contradictory_chains_low_score(self):",
        "        \"\"\"Test that contradictory chains have low validity scores.\"\"\"",
        "        self.assertLess(VALID_RELATION_CHAINS[('Antonym', 'IsA')], 0.5)",
        "",
        "    def test_association_chains_medium_score(self):",
        "        \"\"\"Test that association chains have medium validity scores.\"\"\"",
        "        score = VALID_RELATION_CHAINS[('RelatedTo', 'RelatedTo')]",
        "        self.assertGreater(score, 0.3)",
        "        self.assertLess(score, 0.8)",
        "",
        "    def test_default_chain_validity(self):",
        "        \"\"\"Test DEFAULT_CHAIN_VALIDITY value.\"\"\"",
        "        self.assertEqual(DEFAULT_CHAIN_VALIDITY, 0.4)",
        "",
        "",
        "if __name__ == '__main__':",
        "    unittest.main()"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    }
  ],
  "hour_of_day": 0,
  "day_of_week": "Thursday",
  "seconds_since_last_commit": -393043,
  "is_merge": false,
  "is_initial": false,
  "parent_count": 1,
  "session_id": null,
  "related_chats": [],
  "ci_result": null,
  "reverted": false,
  "amended": false
}