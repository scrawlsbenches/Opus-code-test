{
  "hash": "HEAD",
  "message": "fix(benchmarks): Add safeguards to prevent accidental model overwrites",
  "author": "Claude",
  "timestamp": "2025-12-27 02:08:07 +0000",
  "branch": "claude/accept-handoff-ctrSI",
  "files_changed": [
    "benchmarks/codebase_slm/train_augmented.py",
    "benchmarks/codebase_slm/train_slm.py"
  ],
  "insertions": 275,
  "deletions": 23,
  "hunks": [
    {
      "file": "benchmarks/codebase_slm/train_augmented.py",
      "function": null,
      "start_line": 1,
      "lines_added": [
        "",
        "IMPORTANT: This script saves models to disk. Use --dry-run to evaluate",
        "without saving, or --output to specify a custom output path.",
        "",
        "Usage:",
        "    # Dry run (evaluate only, no save)",
        "    python -m benchmarks.codebase_slm.train_augmented --dry-run",
        "",
        "    # Save to custom path (recommended)",
        "    python -m benchmarks.codebase_slm.train_augmented --output models/my_model.json",
        "",
        "    # Save to default path (creates backup first)",
        "    python -m benchmarks.codebase_slm.train_augmented",
        "",
        "    # Force overwrite without backup",
        "    python -m benchmarks.codebase_slm.train_augmented --force",
        "import argparse",
        "import hashlib",
        "import shutil",
        "from datetime import datetime",
        "# Default output path",
        "DEFAULT_MODEL_PATH = PROJECT_ROOT / \"benchmarks\" / \"codebase_slm\" / \"models\" / \"prism_augmented.json\"",
        "BACKUP_DIR = PROJECT_ROOT / \"benchmarks\" / \"codebase_slm\" / \"models\" / \"backups\"",
        "",
        "        return [], None",
        "    return lines, str(corpus_path)",
        "        return [], None"
      ],
      "lines_removed": [
        "        return []",
        "    return lines",
        "        return []"
      ],
      "context_before": [
        "#!/usr/bin/env python3",
        "\"\"\"",
        "Train PRISM-SLM with augmented corpus and run benchmarks."
      ],
      "context_after": [
        "\"\"\"",
        "",
        "import json",
        "import sys",
        "from pathlib import Path",
        "",
        "PROJECT_ROOT = Path(__file__).parent.parent.parent",
        "sys.path.insert(0, str(PROJECT_ROOT))",
        "",
        "from cortical.spark import NGramModel",
        "",
        "",
        "def load_augmented_corpus():",
        "    \"\"\"Load the augmented training corpus.\"\"\"",
        "    corpus_path = PROJECT_ROOT / \"benchmarks\" / \"codebase_slm\" / \"data\" / \"augmented_corpus.txt\"",
        "",
        "    if not corpus_path.exists():",
        "        print(\"ERROR: Augmented corpus not found. Run data_augmentation.py first.\")",
        "",
        "    with open(corpus_path) as f:",
        "        lines = [line.strip() for line in f if line.strip()]",
        "",
        "    print(f\"Loaded {len(lines)} augmented training lines\")",
        "",
        "",
        "def load_existing_patterns():",
        "    \"\"\"Load existing training patterns.\"\"\"",
        "    patterns_path = PROJECT_ROOT / \"benchmarks\" / \"codebase_slm\" / \"corpus\" / \"training_patterns.jsonl\"",
        "",
        "    if not patterns_path.exists():",
        "        print(\"No existing patterns found\")",
        "",
        "    patterns = []",
        "    with open(patterns_path) as f:",
        "        for line in f:",
        "            try:",
        "                p = json.loads(line)",
        "                # Format as training text",
        "                ptype = p.get('pattern_type', '')",
        "                input_text = p.get('input_text', '')",
        "                target = p.get('target_text', '')"
      ],
      "change_type": "modify"
    },
    {
      "file": "benchmarks/codebase_slm/train_augmented.py",
      "function": "def load_existing_patterns():",
      "start_line": 49,
      "lines_added": [
        "    return patterns, str(patterns_path)"
      ],
      "lines_removed": [
        "    return patterns"
      ],
      "context_before": [
        "                if ptype == 'qa':",
        "                    text = f\"Q: {input_text} A: {target}\"",
        "                else:",
        "                    text = f\"{input_text} {target}\"",
        "",
        "                patterns.append(text)",
        "            except:",
        "                continue",
        "",
        "    print(f\"Loaded {len(patterns)} existing patterns\")"
      ],
      "context_after": [
        "",
        "",
        "def train_model(corpus):",
        "    \"\"\"Train NGramModel on combined corpus.\"\"\"",
        "    print(f\"\\nTraining on {len(corpus)} patterns...\")",
        "",
        "    model = NGramModel(n=3)",
        "    model.train(corpus)",
        "",
        "    print(f\"  Vocabulary size: {len(model.vocab)}\")"
      ],
      "change_type": "modify"
    },
    {
      "file": "benchmarks/codebase_slm/train_augmented.py",
      "function": "def test_model(model):",
      "start_line": 147,
      "lines_added": [
        "def backup_existing_model(model_path: Path) -> Path | None:",
        "    \"\"\"Create a backup of the existing model before overwriting.\"\"\"",
        "    if not model_path.exists():",
        "        return None",
        "",
        "    BACKUP_DIR.mkdir(parents=True, exist_ok=True)",
        "",
        "    # Create timestamped backup filename",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")",
        "    backup_name = f\"prism_augmented_{timestamp}.json\"",
        "    backup_path = BACKUP_DIR / backup_name",
        "",
        "    shutil.copy2(model_path, backup_path)",
        "    print(f\"‚ö†Ô∏è  Backed up existing model to: {backup_path}\")",
        "",
        "    # Also keep only last 5 backups to avoid bloat",
        "    backups = sorted(BACKUP_DIR.glob(\"prism_augmented_*.json\"))",
        "    if len(backups) > 5:",
        "        for old_backup in backups[:-5]:",
        "            old_backup.unlink()",
        "            print(f\"   Removed old backup: {old_backup.name}\")",
        "",
        "    return backup_path",
        "",
        "",
        "def compute_corpus_hash(corpus: list) -> str:",
        "    \"\"\"Compute a hash of the training corpus for provenance.\"\"\"",
        "    content = \"\\n\".join(sorted(set(corpus)))  # Dedupe and sort for consistency",
        "    return hashlib.sha256(content.encode()).hexdigest()[:16]",
        "",
        "",
        "def save_model(model, path: Path, provenance: dict):",
        "    \"\"\"Save trained model with provenance metadata.\"\"\"",
        "        # Provenance metadata (new!)",
        "        '_provenance': {",
        "            'trained_at': datetime.now().isoformat(),",
        "            'corpus_hash': provenance.get('corpus_hash', 'unknown'),",
        "            'corpus_size': provenance.get('corpus_size', 0),",
        "            'sources': provenance.get('sources', []),",
        "            'script': 'train_augmented.py',",
        "            'model_type': 'NGramModel',",
        "        },",
        "        # Model data",
        "    path.parent.mkdir(parents=True, exist_ok=True)",
        "",
        "    print(f\"\\n‚úì Model saved to {path}\")",
        "    print(f\"  Corpus hash: {provenance.get('corpus_hash', 'unknown')}\")",
        "    print(f\"  Corpus size: {provenance.get('corpus_size', 0)} patterns\")",
        "    parser = argparse.ArgumentParser(",
        "        description='Train PRISM-SLM with augmented corpus',",
        "        formatter_class=argparse.RawDescriptionHelpFormatter,",
        "        epilog=\"\"\"",
        "Examples:",
        "  %(prog)s --dry-run              # Evaluate only, don't save",
        "  %(prog)s --output my_model.json # Save to custom path",
        "  %(prog)s                        # Save to default (with backup)",
        "  %(prog)s --force                # Overwrite without backup",
        "        \"\"\"",
        "    )",
        "    parser.add_argument('--output', '-o', type=str,",
        "                        help='Output path for trained model (default: models/prism_augmented.json)')",
        "    parser.add_argument('--dry-run', action='store_true',",
        "                        help='Evaluate only, do not save model')",
        "    parser.add_argument('--force', '-f', action='store_true',",
        "                        help='Overwrite existing model without backup')",
        "    parser.add_argument('--no-existing', action='store_true',",
        "                        help='Train only on augmented corpus, skip existing patterns')",
        "    args = parser.parse_args()",
        "",
        "    if args.dry_run:",
        "        print(\"‚ö†Ô∏è  DRY RUN MODE - Model will NOT be saved\")",
        "",
        "    augmented, aug_source = load_augmented_corpus()",
        "",
        "    if args.no_existing:",
        "        existing, exist_source = [], None",
        "        print(\"Skipping existing patterns (--no-existing)\")",
        "    else:",
        "        existing, exist_source = load_existing_patterns()",
        "    if not combined:",
        "        print(\"ERROR: No training data available\")",
        "        return 1",
        "",
        "    # Save (unless dry-run)",
        "    if not args.dry_run:",
        "        # Determine output path",
        "        if args.output:",
        "            model_path = Path(args.output)",
        "            if not model_path.is_absolute():",
        "                model_path = PROJECT_ROOT / \"benchmarks\" / \"codebase_slm\" / \"models\" / model_path",
        "        else:",
        "            model_path = DEFAULT_MODEL_PATH",
        "",
        "        # Backup existing model (unless --force)",
        "        if model_path.exists() and not args.force:",
        "            backup_existing_model(model_path)",
        "        elif model_path.exists() and args.force:",
        "            print(\"‚ö†Ô∏è  --force specified, skipping backup\")",
        "",
        "        # Build provenance",
        "        sources = []",
        "        if aug_source:",
        "            sources.append(aug_source)",
        "        if exist_source:",
        "            sources.append(exist_source)",
        "",
        "        provenance = {",
        "            'corpus_hash': compute_corpus_hash(combined),",
        "            'corpus_size': len(combined),",
        "            'sources': sources,",
        "        }",
        "",
        "        save_model(model, model_path, provenance)",
        "    else:",
        "        print(\"\\n‚ö†Ô∏è  DRY RUN - Model was NOT saved\")"
      ],
      "lines_removed": [
        "def save_model(model, path):",
        "    \"\"\"Save trained model.\"\"\"",
        "    print(f\"\\nModel saved to {path}\")",
        "    augmented = load_augmented_corpus()",
        "    existing = load_existing_patterns()",
        "    # Save",
        "    model_path = PROJECT_ROOT / \"benchmarks\" / \"codebase_slm\" / \"models\" / \"prism_augmented.json\"",
        "    model_path.parent.mkdir(parents=True, exist_ok=True)",
        "    save_model(model, model_path)"
      ],
      "context_before": [
        "    for category, scores in results.items():",
        "        avg = sum(scores) / len(scores) if scores else 0",
        "        print(f\"  {category}: {avg:.0%}\")",
        "",
        "    overall = sum(sum(s) for s in results.values()) / sum(len(s) for s in results.values())",
        "    print(f\"\\n  OVERALL: {overall:.0%}\")",
        "",
        "    return results",
        "",
        ""
      ],
      "context_after": [
        "    model_data = {",
        "        'vocab': list(model.vocab),",
        "        'counts': {",
        "            ' '.join(ctx): dict(counter)",
        "            for ctx, counter in model.counts.items()",
        "        },",
        "        'context_totals': {",
        "            ' '.join(ctx): total",
        "            for ctx, total in model.context_totals.items()",
        "        },",
        "        'total_tokens': model.total_tokens,",
        "        'total_documents': model.total_documents,",
        "        'n': model.n,",
        "    }",
        "",
        "    with open(path, 'w') as f:",
        "        json.dump(model_data, f, indent=2)",
        "",
        "",
        "",
        "def main():",
        "    print(\"=\" * 60)",
        "    print(\"PRISM-SLM AUGMENTED TRAINING\")",
        "    print(\"=\" * 60)",
        "",
        "    # Load corpora",
        "",
        "    # Combine (augmented has higher weight due to oversampling)",
        "    combined = augmented + existing",
        "    print(f\"\\nTotal training corpus: {len(combined)} patterns\")",
        "",
        "    # Train",
        "    model = train_model(combined)",
        "",
        "    # Test",
        "    results = test_model(model)",
        "",
        "",
        "    # Compare with baseline",
        "    print(\"\\n\" + \"=\" * 60)",
        "    print(\"COMPARISON WITH BASELINE\")",
        "    print(\"=\" * 60)",
        "",
        "    baseline_results = {",
        "        'file_location': 0.875,",
        "        'concept': 0.0,",
        "        'how_to': 0.50,"
      ],
      "change_type": "modify"
    },
    {
      "file": "benchmarks/codebase_slm/train_augmented.py",
      "function": "def main():",
      "start_line": 217,
      "lines_added": [
        "    return 0",
        "",
        "    sys.exit(main())"
      ],
      "lines_removed": [
        "    main()"
      ],
      "context_before": [
        "    print(\"\\n| Category      | Baseline | Augmented | Change |\")",
        "    print(\"|---------------|----------|-----------|--------|\")",
        "",
        "    for category in ['concept', 'file_location']:",
        "        baseline = baseline_results.get(category, 0)",
        "        current = sum(results[category]) / len(results[category]) if results.get(category) else 0",
        "        change = current - baseline",
        "        change_str = f\"+{change:.0%}\" if change >= 0 else f\"{change:.0%}\"",
        "        print(f\"| {category:13} | {baseline:.0%}      | {current:.0%}       | {change_str:6} |\")",
        ""
      ],
      "context_after": [
        "",
        "if __name__ == \"__main__\":"
      ],
      "change_type": "modify"
    },
    {
      "file": "benchmarks/codebase_slm/train_slm.py",
      "function": null,
      "start_line": 1,
      "lines_added": [
        "This script trains a domain-specific SLM that understands",
        "IMPORTANT: Use --dry-run to evaluate without saving, or --output to",
        "specify a custom output path.",
        "",
        "    # Quick training (sample corpus) - evaluate only",
        "    python -m benchmarks.codebase_slm.train_slm --quick --dry-run",
        "    # Full training with model save",
        "    python -m benchmarks.codebase_slm.train_slm --full --output prism_full.json",
        "    python -m benchmarks.codebase_slm.train_slm --quick --interactive",
        "import hashlib",
        "import shutil",
        "from datetime import datetime",
        "from typing import List, Dict, Any, Optional",
        "PROJECT_ROOT = Path(__file__).parent.parent.parent",
        "sys.path.insert(0, str(PROJECT_ROOT))",
        "# Default paths",
        "DEFAULT_MODEL_PATH = PROJECT_ROOT / \"benchmarks\" / \"codebase_slm\" / \"models\" / \"prism_slm.json\"",
        "BACKUP_DIR = PROJECT_ROOT / \"benchmarks\" / \"codebase_slm\" / \"models\" / \"backups\"",
        ""
      ],
      "lines_removed": [
        "This script demonstrates training a domain-specific SLM that understands",
        "    # Quick training (sample corpus)",
        "    python -m benchmarks.codebase_slm.train_slm --quick",
        "    # Full training",
        "    python -m benchmarks.codebase_slm.train_slm --full",
        "    python -m benchmarks.codebase_slm.train_slm --interactive",
        "from typing import List, Dict, Any",
        "sys.path.insert(0, str(Path(__file__).parent.parent.parent))"
      ],
      "context_before": [
        "#!/usr/bin/env python3",
        "\"\"\"",
        "Train PRISM-SLM on the generated repository corpus.",
        ""
      ],
      "context_after": [
        "the repository's code, documentation, and structure.",
        "",
        "Usage:",
        "",
        "",
        "    # Interactive mode after training",
        "\"\"\"",
        "",
        "import argparse",
        "import json",
        "import sys",
        "import time",
        "from pathlib import Path",
        "",
        "# Add project root to path",
        "",
        "from cortical.reasoning.prism_slm import PRISMLanguageModel",
        "",
        "",
        "def load_training_corpus(corpus_path: Path, limit: int = None) -> List[str]:",
        "    \"\"\"Load training patterns from corpus file.",
        "",
        "    Uses Q: / A: format to create clear separation between",
        "    questions and answers for n-gram learning.",
        "    \"\"\"",
        "    patterns = []",
        "",
        "    if not corpus_path.exists():"
      ],
      "change_type": "modify"
    },
    {
      "file": "benchmarks/codebase_slm/train_slm.py",
      "function": "def interactive_mode(model: PRISMLanguageModel):",
      "start_line": 147,
      "lines_added": [
        "def backup_existing_model(model_path: Path) -> Optional[Path]:",
        "    \"\"\"Create a backup of the existing model before overwriting.\"\"\"",
        "    if not model_path.exists():",
        "        return None",
        "",
        "    BACKUP_DIR.mkdir(parents=True, exist_ok=True)",
        "",
        "    # Create timestamped backup filename",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")",
        "    backup_name = f\"{model_path.stem}_{timestamp}.json\"",
        "    backup_path = BACKUP_DIR / backup_name",
        "",
        "    shutil.copy2(model_path, backup_path)",
        "    print(f\"‚ö†Ô∏è  Backed up existing model to: {backup_path}\")",
        "",
        "    # Keep only last 5 backups per model type",
        "    pattern = f\"{model_path.stem}_*.json\"",
        "    backups = sorted(BACKUP_DIR.glob(pattern))",
        "    if len(backups) > 5:",
        "        for old_backup in backups[:-5]:",
        "            old_backup.unlink()",
        "            print(f\"   Removed old backup: {old_backup.name}\")",
        "",
        "    return backup_path",
        "",
        "",
        "def compute_corpus_hash(patterns: List[str]) -> str:",
        "    \"\"\"Compute a hash of the training corpus for provenance.\"\"\"",
        "    content = \"\\n\".join(sorted(set(patterns)))",
        "    return hashlib.sha256(content.encode()).hexdigest()[:16]",
        "",
        "",
        "def save_model(model: PRISMLanguageModel, path: Path, provenance: dict):",
        "    \"\"\"Save trained model with provenance metadata.\"\"\"",
        "    # Get model internal state",
        "    model_data = {",
        "        # Provenance metadata",
        "        '_provenance': {",
        "            'trained_at': datetime.now().isoformat(),",
        "            'corpus_hash': provenance.get('corpus_hash', 'unknown'),",
        "            'corpus_size': provenance.get('corpus_size', 0),",
        "            'corpus_path': provenance.get('corpus_path', 'unknown'),",
        "            'script': 'train_slm.py',",
        "            'model_type': 'PRISMLanguageModel',",
        "            'context_size': model.context_size,",
        "        },",
        "        # Model data",
        "        'vocab_size': model.vocab_size,",
        "        'context_size': model.context_size,",
        "        'transitions': {",
        "            ' '.join(ctx): dict(trans)",
        "            for ctx, trans in model.graph._transitions.items()",
        "        },",
        "    }",
        "",
        "    path.parent.mkdir(parents=True, exist_ok=True)",
        "",
        "    with open(path, 'w') as f:",
        "        json.dump(model_data, f, indent=2)",
        "",
        "    print(f\"\\n‚úì Model saved to {path}\")",
        "    print(f\"  Corpus hash: {provenance.get('corpus_hash', 'unknown')}\")",
        "    print(f\"  Corpus size: {provenance.get('corpus_size', 0)} patterns\")",
        "",
        "",
        "    parser = argparse.ArgumentParser(",
        "        description='Train PRISM-SLM on repository corpus',",
        "        formatter_class=argparse.RawDescriptionHelpFormatter,",
        "        epilog=\"\"\"",
        "Examples:",
        "  %(prog)s --quick --dry-run           # Quick eval, no save",
        "  %(prog)s --full --output my_model.json  # Full training, save to file",
        "  %(prog)s --quick --interactive       # Quick train + interactive mode",
        "        \"\"\"",
        "    )",
        "    parser.add_argument('--output', '-o', type=str,",
        "                        help='Save trained model to this path')",
        "    parser.add_argument('--dry-run', action='store_true',",
        "                        help='Evaluate only, do not save model')",
        "    parser.add_argument('--force', '-f', action='store_true',",
        "                        help='Overwrite existing model without backup')",
        "    if not corpus_path.is_absolute():",
        "        corpus_path = PROJECT_ROOT / corpus_path",
        "",
        "    if args.dry_run:",
        "        print(\"‚ö†Ô∏è  DRY RUN MODE - Model will NOT be saved\")"
      ],
      "lines_removed": [
        "    parser = argparse.ArgumentParser(description='Train PRISM-SLM on repository corpus')"
      ],
      "context_before": [
        "            print()",
        "",
        "        except KeyboardInterrupt:",
        "            break",
        "        except EOFError:",
        "            break",
        "",
        "    print(\"\\nGoodbye!\")",
        "",
        ""
      ],
      "context_after": [
        "def main():",
        "    parser.add_argument('--quick', action='store_true', help='Quick training (1000 patterns)')",
        "    parser.add_argument('--full', action='store_true', help='Full training (all patterns)')",
        "    parser.add_argument('--interactive', action='store_true', help='Interactive mode after training')",
        "    parser.add_argument('--corpus', type=str,",
        "                        default='benchmarks/codebase_slm/corpus/training_patterns.jsonl',",
        "                        help='Path to training corpus')",
        "    parser.add_argument('--context-size', type=int, default=3, help='Context window size')",
        "    args = parser.parse_args()",
        "",
        "    corpus_path = Path(args.corpus)",
        "",
        "    print(\"=\" * 60)",
        "    print(\"Repository-Native SLM Training\")",
        "    print(\"=\" * 60)",
        "    print()",
        "",
        "    # Determine pattern limit",
        "    limit = 1000 if args.quick else None",
        "",
        "    # Load corpus",
        "    print(f\"Loading corpus from {corpus_path}...\")",
        "    start = time.time()",
        "    patterns = load_training_corpus(corpus_path, limit=limit)",
        ""
      ],
      "change_type": "modify"
    },
    {
      "file": "benchmarks/codebase_slm/train_slm.py",
      "function": "def main():",
      "start_line": 237,
      "lines_added": [
        "    # Save model if --output specified (and not --dry-run)",
        "    if args.output and not args.dry_run:",
        "        model_path = Path(args.output)",
        "        if not model_path.is_absolute():",
        "            model_path = PROJECT_ROOT / \"benchmarks\" / \"codebase_slm\" / \"models\" / model_path",
        "",
        "        # Backup existing model (unless --force)",
        "        if model_path.exists() and not args.force:",
        "            backup_existing_model(model_path)",
        "        elif model_path.exists() and args.force:",
        "            print(\"‚ö†Ô∏è  --force specified, skipping backup\")",
        "",
        "        provenance = {",
        "            'corpus_hash': compute_corpus_hash(patterns),",
        "            'corpus_size': len(patterns),",
        "            'corpus_path': str(corpus_path),",
        "        }",
        "",
        "        save_model(model, model_path, provenance)",
        "    elif args.output and args.dry_run:",
        "        print(\"\\n‚ö†Ô∏è  DRY RUN - Model was NOT saved (would save to: {})\".format(args.output))",
        "    elif not args.output and not args.dry_run:",
        "        print(\"\\nüí° Tip: Use --output <path> to save the trained model\")",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "        print(f\"\\n[{q['category']}] {status}\")",
        "        print(f\"  Prompt: {q['prompt']}\")",
        "        print(f\"  Generated: {generated}\")",
        "        print(f\"  Expected terms: {q['expected']}\")",
        "        print(f\"  Match: {matches}/{len(expected_terms)} terms ({match_pct:.0f}%)\")",
        "",
        "    print(f\"\\n{'=' * 60}\")",
        "    print(f\"RESULTS: {correct}/{len(test_queries)} queries matched (‚â•50% of terms)\")",
        "    print(f\"{'=' * 60}\")",
        ""
      ],
      "context_after": [
        "    # Interactive mode",
        "    if args.interactive:",
        "        interactive_mode(model)",
        "",
        "    return 0",
        "",
        "",
        "if __name__ == '__main__':",
        "    sys.exit(main())"
      ],
      "change_type": "add"
    }
  ],
  "hour_of_day": 2,
  "day_of_week": "Saturday",
  "seconds_since_last_commit": 677,
  "is_merge": false,
  "is_initial": false,
  "parent_count": 1,
  "session_id": "c7e7f25e",
  "related_chats": [
    "chat-20251227-015100-015a56"
  ],
  "ci_result": null,
  "reverted": false,
  "amended": false
}