{
  "hash": "0f75675e99ae2fae5084ccde73d287ee304fc613",
  "message": "Add Python code samples and update showcase for code search features",
  "author": "Claude",
  "timestamp": "2025-12-11 10:15:14 +0000",
  "branch": "claude/multi-index-design-DvifZ",
  "files_changed": [
    "samples/data_processor.py",
    "samples/search_engine.py",
    "samples/test_data_processor.py",
    "showcase.py"
  ],
  "insertions": 789,
  "deletions": 5,
  "hunks": [
    {
      "file": "samples/data_processor.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "\"\"\"",
        "Data Processor Module - Sample code for demonstrating code search features.",
        "",
        "This module provides utilities for processing and transforming data records.",
        "\"\"\"",
        "",
        "from typing import Dict, List, Optional, Any",
        "from dataclasses import dataclass",
        "from collections import defaultdict",
        "",
        "",
        "@dataclass",
        "class DataRecord:",
        "    \"\"\"Represents a single data record with metadata.",
        "",
        "    Attributes:",
        "        id: Unique identifier for the record",
        "        content: The main content of the record",
        "        metadata: Optional dictionary of metadata fields",
        "        tags: List of tags associated with the record",
        "    \"\"\"",
        "    id: str",
        "    content: str",
        "    metadata: Optional[Dict[str, Any]] = None",
        "    tags: List[str] = None",
        "",
        "    def __post_init__(self):",
        "        if self.tags is None:",
        "            self.tags = []",
        "        if self.metadata is None:",
        "            self.metadata = {}",
        "",
        "",
        "class DataProcessor:",
        "    \"\"\"Main processor for handling data records.",
        "",
        "    The DataProcessor provides methods for filtering, transforming,",
        "    and aggregating data records efficiently.",
        "",
        "    Example:",
        "        processor = DataProcessor()",
        "        processor.add_record(DataRecord(\"1\", \"Hello world\"))",
        "        results = processor.filter_by_content(\"hello\")",
        "    \"\"\"",
        "",
        "    def __init__(self):",
        "        \"\"\"Initialize the data processor with empty storage.\"\"\"",
        "        self._records: Dict[str, DataRecord] = {}",
        "        self._tag_index: Dict[str, List[str]] = defaultdict(list)",
        "",
        "    def add_record(self, record: DataRecord) -> None:",
        "        \"\"\"Add a record to the processor.",
        "",
        "        Args:",
        "            record: The DataRecord to add",
        "        \"\"\"",
        "        self._records[record.id] = record",
        "        for tag in record.tags:",
        "            self._tag_index[tag].append(record.id)",
        "",
        "    def get_record(self, record_id: str) -> Optional[DataRecord]:",
        "        \"\"\"Retrieve a record by its ID.",
        "",
        "        Args:",
        "            record_id: The unique identifier of the record",
        "",
        "        Returns:",
        "            The DataRecord if found, None otherwise",
        "        \"\"\"",
        "        return self._records.get(record_id)",
        "",
        "    def filter_by_content(self, query: str) -> List[DataRecord]:",
        "        \"\"\"Filter records by content matching.",
        "",
        "        Args:",
        "            query: Search string to match against content",
        "",
        "        Returns:",
        "            List of matching DataRecord objects",
        "        \"\"\"",
        "        query_lower = query.lower()",
        "        return [",
        "            record for record in self._records.values()",
        "            if query_lower in record.content.lower()",
        "        ]",
        "",
        "    def filter_by_tag(self, tag: str) -> List[DataRecord]:",
        "        \"\"\"Filter records by tag.",
        "",
        "        Args:",
        "            tag: The tag to filter by",
        "",
        "        Returns:",
        "            List of DataRecord objects with the specified tag",
        "        \"\"\"",
        "        record_ids = self._tag_index.get(tag, [])",
        "        return [self._records[rid] for rid in record_ids if rid in self._records]",
        "",
        "    def transform_content(self, transformer_func) -> List[DataRecord]:",
        "        \"\"\"Apply a transformation function to all record contents.",
        "",
        "        Args:",
        "            transformer_func: Callable that takes content string and returns transformed string",
        "",
        "        Returns:",
        "            List of new DataRecord objects with transformed content",
        "        \"\"\"",
        "        results = []",
        "        for record in self._records.values():",
        "            new_content = transformer_func(record.content)",
        "            new_record = DataRecord(",
        "                id=record.id,",
        "                content=new_content,",
        "                metadata=record.metadata.copy(),",
        "                tags=record.tags.copy()",
        "            )",
        "            results.append(new_record)",
        "        return results",
        "",
        "    def aggregate_by_tag(self) -> Dict[str, int]:",
        "        \"\"\"Count records per tag.",
        "",
        "        Returns:",
        "            Dictionary mapping tag names to record counts",
        "        \"\"\"",
        "        return {tag: len(ids) for tag, ids in self._tag_index.items()}",
        "",
        "    def clear(self) -> None:",
        "        \"\"\"Remove all records from the processor.\"\"\"",
        "        self._records.clear()",
        "        self._tag_index.clear()",
        "",
        "",
        "def calculate_statistics(records: List[DataRecord]) -> Dict[str, Any]:",
        "    \"\"\"Calculate statistics for a list of records.",
        "",
        "    Args:",
        "        records: List of DataRecord objects to analyze",
        "",
        "    Returns:",
        "        Dictionary containing:",
        "            - count: Number of records",
        "            - avg_content_length: Average content length",
        "            - unique_tags: Set of all unique tags",
        "            - records_with_metadata: Count of records with non-empty metadata",
        "    \"\"\"",
        "    if not records:",
        "        return {",
        "            'count': 0,",
        "            'avg_content_length': 0,",
        "            'unique_tags': set(),",
        "            'records_with_metadata': 0",
        "        }",
        "",
        "    total_length = sum(len(r.content) for r in records)",
        "    all_tags = set()",
        "    metadata_count = 0",
        "",
        "    for record in records:",
        "        all_tags.update(record.tags)",
        "        if record.metadata:",
        "            metadata_count += 1",
        "",
        "    return {",
        "        'count': len(records),",
        "        'avg_content_length': total_length / len(records),",
        "        'unique_tags': all_tags,",
        "        'records_with_metadata': metadata_count",
        "    }",
        "",
        "",
        "def merge_records(records: List[DataRecord], separator: str = '\\n') -> DataRecord:",
        "    \"\"\"Merge multiple records into a single record.",
        "",
        "    Args:",
        "        records: List of DataRecord objects to merge",
        "        separator: String to use between merged contents",
        "",
        "    Returns:",
        "        A new DataRecord with combined content, merged metadata, and all tags",
        "    \"\"\"",
        "    if not records:",
        "        return DataRecord(id='merged', content='')",
        "",
        "    merged_content = separator.join(r.content for r in records)",
        "    merged_metadata = {}",
        "    merged_tags = []",
        "",
        "    for record in records:",
        "        merged_metadata.update(record.metadata)",
        "        merged_tags.extend(record.tags)",
        "",
        "    return DataRecord(",
        "        id='merged_' + '_'.join(r.id for r in records[:3]),",
        "        content=merged_content,",
        "        metadata=merged_metadata,",
        "        tags=list(set(merged_tags))",
        "    )"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "samples/search_engine.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "\"\"\"",
        "Search Engine Module - Sample code for demonstrating code search features.",
        "",
        "This module implements a simple search engine with indexing and ranking.",
        "\"\"\"",
        "",
        "from typing import Dict, List, Tuple, Set, Optional",
        "from collections import defaultdict",
        "import math",
        "",
        "",
        "class SearchIndex:",
        "    \"\"\"Inverted index for text search.",
        "",
        "    The SearchIndex maintains an inverted index mapping terms to documents,",
        "    enabling fast full-text search with TF-IDF ranking.",
        "",
        "    Example:",
        "        index = SearchIndex()",
        "        index.add_document(\"doc1\", \"hello world\")",
        "        results = index.search(\"hello\")",
        "    \"\"\"",
        "",
        "    def __init__(self):",
        "        \"\"\"Initialize empty search index.\"\"\"",
        "        self._inverted_index: Dict[str, Set[str]] = defaultdict(set)",
        "        self._documents: Dict[str, str] = {}",
        "        self._term_frequencies: Dict[str, Dict[str, int]] = defaultdict(lambda: defaultdict(int))",
        "        self._document_lengths: Dict[str, int] = {}",
        "",
        "    def add_document(self, doc_id: str, content: str) -> None:",
        "        \"\"\"Add a document to the index.",
        "",
        "        Args:",
        "            doc_id: Unique document identifier",
        "            content: Text content to index",
        "        \"\"\"",
        "        self._documents[doc_id] = content",
        "        tokens = self._tokenize(content)",
        "        self._document_lengths[doc_id] = len(tokens)",
        "",
        "        for token in tokens:",
        "            self._inverted_index[token].add(doc_id)",
        "            self._term_frequencies[doc_id][token] += 1",
        "",
        "    def remove_document(self, doc_id: str) -> bool:",
        "        \"\"\"Remove a document from the index.",
        "",
        "        Args:",
        "            doc_id: The document ID to remove",
        "",
        "        Returns:",
        "            True if document was removed, False if not found",
        "        \"\"\"",
        "        if doc_id not in self._documents:",
        "            return False",
        "",
        "        # Remove from inverted index",
        "        for term, doc_ids in self._inverted_index.items():",
        "            doc_ids.discard(doc_id)",
        "",
        "        # Clean up empty term entries",
        "        empty_terms = [t for t, ids in self._inverted_index.items() if not ids]",
        "        for term in empty_terms:",
        "            del self._inverted_index[term]",
        "",
        "        del self._documents[doc_id]",
        "        del self._term_frequencies[doc_id]",
        "        del self._document_lengths[doc_id]",
        "        return True",
        "",
        "    def search(self, query: str, top_k: int = 10) -> List[Tuple[str, float]]:",
        "        \"\"\"Search for documents matching the query.",
        "",
        "        Args:",
        "            query: Search query string",
        "            top_k: Maximum number of results to return",
        "",
        "        Returns:",
        "            List of (doc_id, score) tuples sorted by relevance",
        "        \"\"\"",
        "        query_tokens = self._tokenize(query)",
        "        if not query_tokens:",
        "            return []",
        "",
        "        scores = defaultdict(float)",
        "",
        "        for token in query_tokens:",
        "            if token not in self._inverted_index:",
        "                continue",
        "",
        "            idf = self._compute_idf(token)",
        "            for doc_id in self._inverted_index[token]:",
        "                tf = self._compute_tf(token, doc_id)",
        "                scores[doc_id] += tf * idf",
        "",
        "        # Sort by score descending",
        "        ranked = sorted(scores.items(), key=lambda x: x[1], reverse=True)",
        "        return ranked[:top_k]",
        "",
        "    def get_document(self, doc_id: str) -> Optional[str]:",
        "        \"\"\"Retrieve document content by ID.",
        "",
        "        Args:",
        "            doc_id: The document identifier",
        "",
        "        Returns:",
        "            Document content string if found, None otherwise",
        "        \"\"\"",
        "        return self._documents.get(doc_id)",
        "",
        "    def _tokenize(self, text: str) -> List[str]:",
        "        \"\"\"Tokenize text into lowercase terms.",
        "",
        "        Args:",
        "            text: Input text to tokenize",
        "",
        "        Returns:",
        "            List of lowercase tokens",
        "        \"\"\"",
        "        # Simple whitespace tokenization",
        "        return [t.lower().strip('.,!?;:()[]{}') for t in text.split() if t.strip()]",
        "",
        "    def _compute_tf(self, term: str, doc_id: str) -> float:",
        "        \"\"\"Compute term frequency for a term in a document.",
        "",
        "        Args:",
        "            term: The term to compute TF for",
        "            doc_id: The document identifier",
        "",
        "        Returns:",
        "            Normalized term frequency",
        "        \"\"\"",
        "        raw_tf = self._term_frequencies[doc_id][term]",
        "        doc_length = self._document_lengths[doc_id]",
        "        return raw_tf / doc_length if doc_length > 0 else 0",
        "",
        "    def _compute_idf(self, term: str) -> float:",
        "        \"\"\"Compute inverse document frequency for a term.",
        "",
        "        Args:",
        "            term: The term to compute IDF for",
        "",
        "        Returns:",
        "            IDF value using log scaling",
        "        \"\"\"",
        "        n_docs = len(self._documents)",
        "        doc_freq = len(self._inverted_index.get(term, set()))",
        "        if doc_freq == 0:",
        "            return 0",
        "        return math.log(n_docs / doc_freq)",
        "",
        "",
        "class QueryParser:",
        "    \"\"\"Parser for search queries with advanced syntax.",
        "",
        "    Supports:",
        "        - Simple terms: hello world",
        "        - Phrase queries: \"hello world\"",
        "        - Required terms: +important",
        "        - Excluded terms: -spam",
        "    \"\"\"",
        "",
        "    def __init__(self):",
        "        \"\"\"Initialize the query parser.\"\"\"",
        "        self._operators = {'+', '-', '\"'}",
        "",
        "    def parse(self, query: str) -> Dict[str, List[str]]:",
        "        \"\"\"Parse a query string into components.",
        "",
        "        Args:",
        "            query: The query string to parse",
        "",
        "        Returns:",
        "            Dictionary with keys:",
        "                - required: Terms that must appear",
        "                - excluded: Terms that must not appear",
        "                - optional: Regular search terms",
        "                - phrases: Exact phrase matches",
        "        \"\"\"",
        "        result = {",
        "            'required': [],",
        "            'excluded': [],",
        "            'optional': [],",
        "            'phrases': []",
        "        }",
        "",
        "        i = 0",
        "        tokens = query.split()",
        "",
        "        while i < len(tokens):",
        "            token = tokens[i]",
        "",
        "            if token.startswith('+'):",
        "                result['required'].append(token[1:].lower())",
        "            elif token.startswith('-'):",
        "                result['excluded'].append(token[1:].lower())",
        "            elif token.startswith('\"'):",
        "                # Handle phrase - collect until closing quote",
        "                phrase_tokens = [token[1:]]",
        "                i += 1",
        "                while i < len(tokens) and not tokens[i].endswith('\"'):",
        "                    phrase_tokens.append(tokens[i])",
        "                    i += 1",
        "                if i < len(tokens):",
        "                    phrase_tokens.append(tokens[i][:-1])",
        "                result['phrases'].append(' '.join(phrase_tokens).lower())",
        "            else:",
        "                result['optional'].append(token.lower())",
        "",
        "            i += 1",
        "",
        "        return result",
        "",
        "",
        "def compute_bm25_score(",
        "    term: str,",
        "    doc_id: str,",
        "    index: SearchIndex,",
        "    k1: float = 1.5,",
        "    b: float = 0.75",
        ") -> float:",
        "    \"\"\"Compute BM25 score for a term in a document.",
        "",
        "    BM25 is a ranking function used by search engines to estimate",
        "    the relevance of documents to a given search query.",
        "",
        "    Args:",
        "        term: The search term",
        "        doc_id: The document identifier",
        "        index: The SearchIndex to use",
        "        k1: Term frequency saturation parameter",
        "        b: Length normalization parameter",
        "",
        "    Returns:",
        "        BM25 score for the term-document pair",
        "    \"\"\"",
        "    tf = index._term_frequencies[doc_id].get(term, 0)",
        "    doc_length = index._document_lengths[doc_id]",
        "    avg_doc_length = sum(index._document_lengths.values()) / len(index._document_lengths)",
        "",
        "    idf = index._compute_idf(term)",
        "",
        "    numerator = tf * (k1 + 1)",
        "    denominator = tf + k1 * (1 - b + b * doc_length / avg_doc_length)",
        "",
        "    return idf * numerator / denominator",
        "",
        "",
        "def highlight_matches(text: str, query_terms: List[str], marker: str = '**') -> str:",
        "    \"\"\"Highlight query term matches in text.",
        "",
        "    Args:",
        "        text: The text to highlight",
        "        query_terms: List of terms to highlight",
        "        marker: String to use for highlighting (wraps matches)",
        "",
        "    Returns:",
        "        Text with query terms wrapped in markers",
        "    \"\"\"",
        "    result = text",
        "    for term in query_terms:",
        "        # Case-insensitive replacement",
        "        import re",
        "        pattern = re.compile(re.escape(term), re.IGNORECASE)",
        "        result = pattern.sub(f'{marker}{term}{marker}', result)",
        "    return result"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "samples/test_data_processor.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "\"\"\"",
        "Unit tests for the DataProcessor module.",
        "",
        "This test file demonstrates various testing patterns for the data processing",
        "functionality including fixtures, edge cases, and integration tests.",
        "\"\"\"",
        "",
        "import unittest",
        "from typing import List",
        "",
        "# Note: In a real project, this would import from the actual module",
        "# from data_processor import DataProcessor, DataRecord, calculate_statistics",
        "",
        "",
        "class MockDataRecord:",
        "    \"\"\"Mock DataRecord for testing purposes.\"\"\"",
        "    def __init__(self, id: str, content: str, metadata=None, tags=None):",
        "        self.id = id",
        "        self.content = content",
        "        self.metadata = metadata or {}",
        "        self.tags = tags or []",
        "",
        "",
        "class TestDataProcessorBasics(unittest.TestCase):",
        "    \"\"\"Test basic DataProcessor functionality.\"\"\"",
        "",
        "    def setUp(self):",
        "        \"\"\"Set up test fixtures.\"\"\"",
        "        self.processor = MockDataProcessor()",
        "        self.sample_record = MockDataRecord(",
        "            id=\"test1\",",
        "            content=\"Hello world test content\",",
        "            tags=[\"test\", \"sample\"]",
        "        )",
        "",
        "    def test_add_record(self):",
        "        \"\"\"Test adding a single record.\"\"\"",
        "        self.processor.add_record(self.sample_record)",
        "        result = self.processor.get_record(\"test1\")",
        "        self.assertIsNotNone(result)",
        "        self.assertEqual(result.content, \"Hello world test content\")",
        "",
        "    def test_add_multiple_records(self):",
        "        \"\"\"Test adding multiple records.\"\"\"",
        "        records = [",
        "            MockDataRecord(\"r1\", \"First record\"),",
        "            MockDataRecord(\"r2\", \"Second record\"),",
        "            MockDataRecord(\"r3\", \"Third record\"),",
        "        ]",
        "        for record in records:",
        "            self.processor.add_record(record)",
        "",
        "        self.assertEqual(len(self.processor._records), 3)",
        "",
        "    def test_get_nonexistent_record(self):",
        "        \"\"\"Test retrieving a record that doesn't exist.\"\"\"",
        "        result = self.processor.get_record(\"nonexistent\")",
        "        self.assertIsNone(result)",
        "",
        "",
        "class TestDataProcessorFiltering(unittest.TestCase):",
        "    \"\"\"Test filtering functionality.\"\"\"",
        "",
        "    def setUp(self):",
        "        \"\"\"Set up test data with various records.\"\"\"",
        "        self.processor = MockDataProcessor()",
        "        self.processor.add_record(MockDataRecord(\"1\", \"Python programming\"))",
        "        self.processor.add_record(MockDataRecord(\"2\", \"JavaScript development\"))",
        "        self.processor.add_record(MockDataRecord(\"3\", \"Python web development\"))",
        "        self.processor.add_record(MockDataRecord(\"4\", \"Database design\", tags=[\"db\"]))",
        "",
        "    def test_filter_by_content_single_match(self):",
        "        \"\"\"Test content filter with single match.\"\"\"",
        "        results = self.processor.filter_by_content(\"JavaScript\")",
        "        self.assertEqual(len(results), 1)",
        "        self.assertEqual(results[0].id, \"2\")",
        "",
        "    def test_filter_by_content_multiple_matches(self):",
        "        \"\"\"Test content filter with multiple matches.\"\"\"",
        "        results = self.processor.filter_by_content(\"Python\")",
        "        self.assertEqual(len(results), 2)",
        "",
        "    def test_filter_by_content_case_insensitive(self):",
        "        \"\"\"Test that content filtering is case insensitive.\"\"\"",
        "        results = self.processor.filter_by_content(\"python\")",
        "        self.assertEqual(len(results), 2)",
        "",
        "    def test_filter_by_content_no_match(self):",
        "        \"\"\"Test content filter with no matches.\"\"\"",
        "        results = self.processor.filter_by_content(\"Ruby\")",
        "        self.assertEqual(len(results), 0)",
        "",
        "    def test_filter_by_tag(self):",
        "        \"\"\"Test filtering by tag.\"\"\"",
        "        results = self.processor.filter_by_tag(\"db\")",
        "        self.assertEqual(len(results), 1)",
        "        self.assertEqual(results[0].id, \"4\")",
        "",
        "",
        "class TestDataProcessorTransformation(unittest.TestCase):",
        "    \"\"\"Test transformation functionality.\"\"\"",
        "",
        "    def setUp(self):",
        "        \"\"\"Set up test processor.\"\"\"",
        "        self.processor = MockDataProcessor()",
        "        self.processor.add_record(MockDataRecord(\"1\", \"hello world\"))",
        "        self.processor.add_record(MockDataRecord(\"2\", \"test content\"))",
        "",
        "    def test_transform_uppercase(self):",
        "        \"\"\"Test transforming content to uppercase.\"\"\"",
        "        results = self.processor.transform_content(str.upper)",
        "        self.assertEqual(results[0].content, \"HELLO WORLD\")",
        "",
        "    def test_transform_preserves_metadata(self):",
        "        \"\"\"Test that transformation preserves record metadata.\"\"\"",
        "        self.processor._records[\"1\"].metadata = {\"key\": \"value\"}",
        "        results = self.processor.transform_content(str.upper)",
        "        self.assertEqual(results[0].metadata, {\"key\": \"value\"})",
        "",
        "",
        "class TestStatisticsCalculation(unittest.TestCase):",
        "    \"\"\"Test statistics calculation functions.\"\"\"",
        "",
        "    def test_empty_records(self):",
        "        \"\"\"Test statistics with empty record list.\"\"\"",
        "        stats = mock_calculate_statistics([])",
        "        self.assertEqual(stats['count'], 0)",
        "        self.assertEqual(stats['avg_content_length'], 0)",
        "",
        "    def test_single_record(self):",
        "        \"\"\"Test statistics with single record.\"\"\"",
        "        records = [MockDataRecord(\"1\", \"Hello\")]",
        "        stats = mock_calculate_statistics(records)",
        "        self.assertEqual(stats['count'], 1)",
        "        self.assertEqual(stats['avg_content_length'], 5)",
        "",
        "    def test_multiple_records(self):",
        "        \"\"\"Test statistics with multiple records.\"\"\"",
        "        records = [",
        "            MockDataRecord(\"1\", \"Hi\", tags=[\"a\"]),",
        "            MockDataRecord(\"2\", \"Hello\", tags=[\"a\", \"b\"]),",
        "            MockDataRecord(\"3\", \"Goodbye\", tags=[\"c\"]),",
        "        ]",
        "        stats = mock_calculate_statistics(records)",
        "        self.assertEqual(stats['count'], 3)",
        "        self.assertEqual(stats['unique_tags'], {\"a\", \"b\", \"c\"})",
        "",
        "",
        "class TestEdgeCases(unittest.TestCase):",
        "    \"\"\"Test edge cases and boundary conditions.\"\"\"",
        "",
        "    def test_empty_content(self):",
        "        \"\"\"Test record with empty content.\"\"\"",
        "        processor = MockDataProcessor()",
        "        record = MockDataRecord(\"empty\", \"\")",
        "        processor.add_record(record)",
        "        self.assertEqual(processor.get_record(\"empty\").content, \"\")",
        "",
        "    def test_special_characters(self):",
        "        \"\"\"Test content with special characters.\"\"\"",
        "        processor = MockDataProcessor()",
        "        record = MockDataRecord(\"special\", \"Hello! @#$% World?\")",
        "        processor.add_record(record)",
        "        results = processor.filter_by_content(\"@#$%\")",
        "        self.assertEqual(len(results), 1)",
        "",
        "    def test_unicode_content(self):",
        "        \"\"\"Test content with unicode characters.\"\"\"",
        "        processor = MockDataProcessor()",
        "        record = MockDataRecord(\"unicode\", \"Hello ä¸–ç•Œ ðŸŒ\")",
        "        processor.add_record(record)",
        "        results = processor.filter_by_content(\"ä¸–ç•Œ\")",
        "        self.assertEqual(len(results), 1)",
        "",
        "",
        "# Mock implementations for testing",
        "class MockDataProcessor:",
        "    \"\"\"Mock implementation of DataProcessor for testing.\"\"\"",
        "",
        "    def __init__(self):",
        "        self._records = {}",
        "        self._tag_index = {}",
        "",
        "    def add_record(self, record):",
        "        self._records[record.id] = record",
        "        for tag in record.tags:",
        "            if tag not in self._tag_index:",
        "                self._tag_index[tag] = []",
        "            self._tag_index[tag].append(record.id)",
        "",
        "    def get_record(self, record_id):",
        "        return self._records.get(record_id)",
        "",
        "    def filter_by_content(self, query):",
        "        query_lower = query.lower()",
        "        return [r for r in self._records.values() if query_lower in r.content.lower()]",
        "",
        "    def filter_by_tag(self, tag):",
        "        ids = self._tag_index.get(tag, [])",
        "        return [self._records[i] for i in ids if i in self._records]",
        "",
        "    def transform_content(self, func):",
        "        results = []",
        "        for r in self._records.values():",
        "            new_record = MockDataRecord(r.id, func(r.content), r.metadata.copy(), r.tags.copy())",
        "            results.append(new_record)",
        "        return results",
        "",
        "",
        "def mock_calculate_statistics(records: List) -> dict:",
        "    \"\"\"Mock implementation of calculate_statistics.\"\"\"",
        "    if not records:",
        "        return {'count': 0, 'avg_content_length': 0, 'unique_tags': set(), 'records_with_metadata': 0}",
        "",
        "    total_length = sum(len(r.content) for r in records)",
        "    all_tags = set()",
        "    for r in records:",
        "        all_tags.update(r.tags)",
        "",
        "    return {",
        "        'count': len(records),",
        "        'avg_content_length': total_length / len(records),",
        "        'unique_tags': all_tags,",
        "        'records_with_metadata': sum(1 for r in records if r.metadata)",
        "    }",
        "",
        "",
        "if __name__ == '__main__':",
        "    unittest.main()"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "showcase.py",
      "function": "class CorticalShowcase:",
      "start_line": 107,
      "lines_added": [
        "        # Load both .txt and .py files",
        "        py_files = sorted([f for f in os.listdir(self.samples_dir) if f.endswith('.py')])",
        "        all_files = txt_files + py_files",
        "        if not all_files:",
        "        for filename in all_files:",
        "            # Handle both .txt and .py extensions",
        "            doc_id = filename.replace('.txt', '').replace('.py', '')"
      ],
      "lines_removed": [
        "        if not txt_files:",
        "        for filename in txt_files:",
        "            doc_id = filename.replace('.txt', '')"
      ],
      "context_before": [
        "        print_header(\"DOCUMENT INGESTION\", \"â•\")",
        "",
        "        print(f\"Loading documents from: {self.samples_dir}\")",
        "        print(\"Processing through cortical hierarchy...\")",
        "        print(\"(Like visual information flowing V1 â†’ V2 â†’ V4 â†’ IT)\\n\")",
        "",
        "        if not os.path.exists(self.samples_dir):",
        "            print(f\"  âŒ Directory not found: {self.samples_dir}\")",
        "            return False",
        ""
      ],
      "context_after": [
        "        txt_files = sorted([f for f in os.listdir(self.samples_dir) if f.endswith('.txt')])",
        "",
        "            return False",
        "",
        "        # Time document loading",
        "        self.timer.start('document_loading')",
        "            filepath = os.path.join(self.samples_dir, filename)",
        "            with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:",
        "                content = f.read()",
        "",
        "            self.processor.process_document(doc_id, content)",
        "            word_count = len(content.split())",
        "            self.loaded_files.append((doc_id, word_count))",
        "            print(f\"  ðŸ“„ {doc_id:30} ({word_count:3} words)\")",
        "        load_time = self.timer.stop()",
        "",
        "        # Run all computations with hybrid strategy for better Layer 2 connectivity",
        "        print(\"\\nComputing cortical representations...\")",
        "        self.timer.start('compute_all')",
        "        self.processor.compute_all("
      ],
      "change_type": "modify"
    },
    {
      "file": "showcase.py",
      "function": "class CorticalShowcase:",
      "start_line": 425,
      "lines_added": [
        "        # 2. Definition search (NEW - Task #84)",
        "        print_subheader(\"\\nðŸ”Ž Definition Search\")",
        "        print(\"    Find class/function definitions directly in code:\\n\")",
        "",
        "        definition_queries = [\"class DataProcessor\", \"def calculate_statistics\", \"class SearchIndex\"]",
        "",
        "        for query in definition_queries:",
        "            is_def, def_type, identifier = self.processor.is_definition_query(query)",
        "            print(f\"    Query: \\\"{query}\\\"\")",
        "            print(f\"      Is definition query: {is_def} ({def_type} '{identifier}')\" if is_def else f\"      Is definition query: {is_def}\")",
        "",
        "            if is_def:",
        "                passages = self.processor.find_definition_passages(query)",
        "                if passages:",
        "                    text, doc_id, start, end, score = passages[0]",
        "                    # Show first line of the definition",
        "                    first_line = text.strip().split('\\n')[0][:60]",
        "                    print(f\"      Found in: {doc_id}\")",
        "                    print(f\"      Match: {first_line}...\")",
        "                else:",
        "                    print(f\"      (No definition found in corpus)\")",
        "            print()",
        "",
        "        # 3. Doc-type boosting (NEW - Task #66)",
        "        print_subheader(\"ðŸ“Š Doc-Type Boosting\")",
        "        print(\"    Apply different weights to docs, code, and test files:\\n\")",
        "",
        "        query = \"filter data records\"",
        "        print(f\"    Query: \\\"{query}\\\"\\n\")",
        "",
        "        # Without boosting",
        "        results_normal = self.processor.find_passages_for_query(",
        "            query, top_n=3, apply_doc_boost=False",
        "        )",
        "",
        "        # With boosting (prefer docs over tests)",
        "        results_boosted = self.processor.find_passages_for_query(",
        "            query, top_n=3, apply_doc_boost=True, prefer_docs=True",
        "        )",
        "",
        "        print(\"    Without doc-type boost:\")",
        "        for text, doc_id, start, end, score in results_normal[:3]:",
        "            is_test = 'test' in doc_id.lower()",
        "            marker = \"ðŸ§ª\" if is_test else \"ðŸ“„\"",
        "            print(f\"      {marker} {doc_id}: {score:.3f}\")",
        "",
        "        print(\"\\n    With doc-type boost (prefer docs, penalize tests):\")",
        "        for text, doc_id, start, end, score in results_boosted[:3]:",
        "            is_test = 'test' in doc_id.lower()",
        "            marker = \"ðŸ§ª\" if is_test else \"ðŸ“„\"",
        "            print(f\"      {marker} {doc_id}: {score:.3f}\")",
        "",
        "        print(\"\\n    ðŸ’¡ Test files receive a 0.5x penalty to surface source files first.\")",
        "",
        "        # 4. Code-aware chunking (NEW - Task #86)",
        "        print_subheader(\"\\nâœ‚ï¸  Code-Aware Chunking\")",
        "        print(\"    Split code at semantic boundaries (class/function defs):\\n\")",
        "",
        "        # Find a Python file",
        "        code_doc_id = None",
        "        for doc_id, _ in self.loaded_files:",
        "            if doc_id in ['data_processor', 'search_engine']:",
        "                code_doc_id = doc_id",
        "                break",
        "",
        "        if code_doc_id:",
        "            content = self.processor.documents.get(code_doc_id, \"\")",
        "",
        "            # Regular chunking",
        "            from cortical.query import create_chunks, create_code_aware_chunks",
        "            regular_chunks = create_chunks(content, chunk_size=300, overlap=50)",
        "",
        "            # Code-aware chunking",
        "            code_chunks = create_code_aware_chunks(content, max_size=300)",
        "",
        "            print(f\"    File: {code_doc_id}\")",
        "            print(f\"    Regular chunks: {len(regular_chunks)} (fixed 300-char boundaries)\")",
        "            print(f\"    Code-aware chunks: {len(code_chunks)} (semantic boundaries)\\n\")",
        "",
        "            print(\"    Code-aware chunk boundaries:\")",
        "            for i, chunk in enumerate(code_chunks[:4]):",
        "                first_line = chunk.strip().split('\\n')[0][:50]",
        "                print(f\"      [{i+1}] {first_line}...\")",
        "        else:",
        "            print(\"    (No Python files in corpus)\")",
        "",
        "        # 5. Code-aware query expansion"
      ],
      "lines_removed": [
        "        # 2. Code-aware query expansion"
      ],
      "context_before": [
        "",
        "        for query, expected_conceptual in test_queries:",
        "            is_conceptual = self.processor.is_conceptual_query(query)",
        "            intent = \"conceptual\" if is_conceptual else \"implementation\"",
        "            marker = \"ðŸ“–\" if is_conceptual else \"ðŸ’»\"",
        "            print(f\"    {marker} \\\"{query}\\\" â†’ {intent}\")",
        "",
        "        print(\"\\n    ðŸ’¡ Use case: Boost documentation for conceptual queries,\")",
        "        print(\"                 boost code files for implementation queries.\")",
        ""
      ],
      "context_after": [
        "        print_subheader(\"\\nðŸ”§ Code-Aware Query Expansion\")",
        "        print(\"    Programming synonyms expand queries for better code search:\\n\")",
        "",
        "        code_queries = [\"fetch data\", \"get results\", \"process input\"]",
        "",
        "        for query in code_queries:",
        "            # Regular expansion",
        "            regular = self.processor.expand_query(query, max_expansions=5)",
        "            # Code-aware expansion",
        "            code_exp = self.processor.expand_query_for_code(query, max_expansions=8)"
      ],
      "change_type": "modify"
    },
    {
      "file": "showcase.py",
      "function": "class CorticalShowcase:",
      "start_line": 450,
      "lines_added": [
        "        # 6. Semantic fingerprinting"
      ],
      "lines_removed": [
        "        # 3. Semantic fingerprinting"
      ],
      "context_before": [
        "            new_terms = code_terms - regular_terms",
        "",
        "            print(f\"    Query: \\\"{query}\\\"\")",
        "            if new_terms:",
        "                new_list = sorted(new_terms, key=lambda t: -code_exp.get(t, 0))[:4]",
        "                print(f\"      + Code terms: {', '.join(new_list)}\")",
        "            else:",
        "                print(f\"      (corpus lacks programming synonyms for this query)\")",
        "            print()",
        ""
      ],
      "context_after": [
        "        print_subheader(\"ðŸ” Semantic Fingerprinting\")",
        "        print(\"    Compare text similarity using semantic fingerprints:\\n\")",
        "",
        "        # Get two related documents",
        "        if len(self.loaded_files) >= 2:",
        "            doc1_id = \"neural_pagerank\" if \"neural_pagerank\" in self.processor.documents else self.loaded_files[0][0]",
        "            doc2_id = \"pagerank_fundamentals\" if \"pagerank_fundamentals\" in self.processor.documents else self.loaded_files[1][0]",
        "",
        "            doc1_content = self.processor.documents.get(doc1_id, \"\")[:500]",
        "            doc2_content = self.processor.documents.get(doc2_id, \"\")[:500]"
      ],
      "change_type": "modify"
    }
  ],
  "hour_of_day": 10,
  "day_of_week": "Thursday",
  "seconds_since_last_commit": -358174,
  "is_merge": false,
  "is_initial": false,
  "parent_count": 1,
  "session_id": null,
  "related_chats": [],
  "ci_result": null,
  "reverted": false,
  "amended": false
}