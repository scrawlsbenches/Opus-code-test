{
  "hash": "774d6a9f3e37392ded2dd6b641a664904be8802b",
  "message": "Merge pull request #39 from scrawlsbenches/claude/background-analysis-progress-01X6fp1f13rGb2fFd7ftU6dV",
  "author": "scrawlsbenches",
  "timestamp": "2025-12-11 14:00:19 -0500",
  "branch": "claude/multi-index-design-DvifZ",
  "files_changed": [
    ".gitignore",
    "scripts/index_codebase.py"
  ],
  "insertions": 973,
  "deletions": 4,
  "hunks": [
    {
      "file": ".gitignore",
      "function": "CodeCoverage/",
      "start_line": 75,
      "lines_added": [
        "nunit-*.xml",
        "# Indexer progress files",
        ".index_progress.json",
        ".index_incremental_progress.json"
      ],
      "lines_removed": [
        "nunit-*.xml"
      ],
      "context_before": [
        "# MSBuild Binary and Structured Log",
        "*.binlog",
        "",
        "# MSTest test Results",
        "[Tt]est[Rr]esult*/",
        "[Bb]uild[Ll]og.*",
        "",
        "# NUnit",
        "*.VisualState.xml",
        "TestResult.xml"
      ],
      "context_after": [],
      "change_type": "modify"
    },
    {
      "file": "scripts/index_codebase.py",
      "function": "This script indexes all Python files and documentation to enable",
      "start_line": 6,
      "lines_added": [
        "",
        "Background Full Analysis (for long-running environments):",
        "    python scripts/index_codebase.py --full-analysis  # Start in background",
        "    python scripts/index_codebase.py --full-analysis  # Check progress (run again)",
        "    python scripts/index_codebase.py --full-analysis --foreground  # Run synchronously",
        "",
        "Incremental Full Analysis (resumable, for short-lived processes):",
        "    python scripts/index_codebase.py --full-analysis --batch  # Process one batch",
        "    python scripts/index_codebase.py --full-analysis --batch  # Continue (run again)",
        "    python scripts/index_codebase.py --full-analysis --batch --batch-size 10  # Smaller batches",
        "    python scripts/index_codebase.py --full-analysis --batch --status  # Check progress",
        "",
        "The --full-analysis flag runs semantic PageRank and hybrid connections, which",
        "can take several minutes. Two modes are available:",
        "",
        "1. Background mode (default): Spawns a background process. Best for environments",
        "   that support long-running processes. Progress file: .index_progress.json",
        "",
        "2. Batch mode (--batch): Processes files in batches, saving after each batch.",
        "   Run the command multiple times to complete. Best for environments with",
        "   process timeouts. Progress file: .index_incremental_progress.json",
        "import multiprocessing"
      ],
      "lines_removed": [],
      "context_before": [
        "semantic search over the codebase using the Cortical Text Processor itself.",
        "",
        "Supports incremental indexing to only re-index changed files.",
        "",
        "Usage:",
        "    python scripts/index_codebase.py [--output corpus_dev.pkl]",
        "    python scripts/index_codebase.py --incremental  # Only index changes",
        "    python scripts/index_codebase.py --status       # Show what would change",
        "    python scripts/index_codebase.py --force        # Force full rebuild",
        "    python scripts/index_codebase.py --log indexer.log  # Log to file"
      ],
      "context_after": [
        "\"\"\"",
        "",
        "import argparse",
        "import json",
        "import logging",
        "import os",
        "import platform",
        "import signal",
        "import sys",
        "import threading",
        "import time",
        "from contextlib import contextmanager",
        "from dataclasses import dataclass, field",
        "from datetime import datetime",
        "from pathlib import Path",
        "from typing import Dict, List, Optional, Tuple, Any",
        "",
        "# Add parent directory to path for imports",
        "sys.path.insert(0, str(Path(__file__).parent.parent))",
        "",
        "from cortical.processor import CorticalTextProcessor",
        "from cortical.tokenizer import Tokenizer",
        "from cortical.chunk_index import (",
        "    ChunkWriter, ChunkLoader, ChunkCompactor,",
        "    get_changes_from_manifest as get_chunk_changes",
        ")"
      ],
      "change_type": "add"
    },
    {
      "file": "scripts/index_codebase.py",
      "function": "def timeout_handler(seconds: int, tracker: Optional[ProgressTracker] = None):",
      "start_line": 327,
      "lines_added": [
        "# =============================================================================",
        "# Background Analysis Progress Tracking",
        "# =============================================================================",
        "",
        "# Default progress file path",
        "DEFAULT_PROGRESS_FILE = '.index_progress.json'",
        "",
        "",
        "@dataclass",
        "class BackgroundProgress:",
        "    \"\"\"Progress state for background full-analysis runs.\"\"\"",
        "    status: str = \"not_started\"  # not_started, running, completed, failed",
        "    started_at: Optional[str] = None",
        "    completed_at: Optional[str] = None",
        "    pid: Optional[int] = None",
        "    current_phase: str = \"\"",
        "    progress_percent: float = 0.0",
        "    phases_completed: List[str] = field(default_factory=list)",
        "    phases_pending: List[str] = field(default_factory=list)",
        "    error: Optional[str] = None",
        "    output_path: Optional[str] = None",
        "",
        "    def to_dict(self) -> Dict[str, Any]:",
        "        return {",
        "            'status': self.status,",
        "            'started_at': self.started_at,",
        "            'completed_at': self.completed_at,",
        "            'pid': self.pid,",
        "            'current_phase': self.current_phase,",
        "            'progress_percent': self.progress_percent,",
        "            'phases_completed': self.phases_completed,",
        "            'phases_pending': self.phases_pending,",
        "            'error': self.error,",
        "            'output_path': self.output_path,",
        "        }",
        "",
        "    @classmethod",
        "    def from_dict(cls, data: Dict[str, Any]) -> 'BackgroundProgress':",
        "        return cls(",
        "            status=data.get('status', 'not_started'),",
        "            started_at=data.get('started_at'),",
        "            completed_at=data.get('completed_at'),",
        "            pid=data.get('pid'),",
        "            current_phase=data.get('current_phase', ''),",
        "            progress_percent=data.get('progress_percent', 0.0),",
        "            phases_completed=data.get('phases_completed', []),",
        "            phases_pending=data.get('phases_pending', []),",
        "            error=data.get('error'),",
        "            output_path=data.get('output_path'),",
        "        )",
        "",
        "",
        "def get_progress_file_path(base_path: Path) -> Path:",
        "    \"\"\"Get the progress file path.\"\"\"",
        "    return base_path / DEFAULT_PROGRESS_FILE",
        "",
        "",
        "def load_background_progress(progress_path: Path) -> Optional[BackgroundProgress]:",
        "    \"\"\"Load background progress from file.\"\"\"",
        "    if not progress_path.exists():",
        "        return None",
        "    try:",
        "        with open(progress_path, 'r') as f:",
        "            data = json.load(f)",
        "        return BackgroundProgress.from_dict(data)",
        "    except (json.JSONDecodeError, IOError):",
        "        return None",
        "",
        "",
        "def save_background_progress(progress_path: Path, progress: BackgroundProgress) -> None:",
        "    \"\"\"Save background progress to file.\"\"\"",
        "    with open(progress_path, 'w') as f:",
        "        json.dump(progress.to_dict(), f, indent=2)",
        "",
        "",
        "def is_process_alive(pid: int) -> bool:",
        "    \"\"\"Check if a process with given PID is still running.\"\"\"",
        "    if pid is None:",
        "        return False",
        "    try:",
        "        os.kill(pid, 0)  # Signal 0 checks existence without killing",
        "        return True",
        "    except (OSError, ProcessLookupError):",
        "        return False",
        "",
        "",
        "def display_progress(progress: BackgroundProgress, progress_path: Path) -> None:",
        "    \"\"\"Display current progress to the user.\"\"\"",
        "    print(\"\\n\" + \"=\" * 50)",
        "    print(\"BACKGROUND FULL-ANALYSIS STATUS\")",
        "    print(\"=\" * 50)",
        "",
        "    print(f\"\\nStatus: {progress.status.upper()}\")",
        "",
        "    if progress.started_at:",
        "        print(f\"Started: {progress.started_at}\")",
        "",
        "    if progress.status == \"running\":",
        "        # Check if process is actually alive",
        "        if progress.pid and not is_process_alive(progress.pid):",
        "            print(\"\\n⚠️  WARNING: Background process appears to have died unexpectedly!\")",
        "            print(\"   The analysis may have crashed. Check logs for details.\")",
        "            print(f\"   Progress file: {progress_path}\")",
        "        else:",
        "            print(f\"Process ID: {progress.pid}\")",
        "            print(f\"\\nCurrent phase: {progress.current_phase}\")",
        "            print(f\"Progress: {progress.progress_percent:.1f}%\")",
        "",
        "            # Progress bar",
        "            bar_width = 40",
        "            filled = int(bar_width * progress.progress_percent / 100)",
        "            bar = \"█\" * filled + \"░\" * (bar_width - filled)",
        "            print(f\"[{bar}]\")",
        "",
        "            if progress.phases_completed:",
        "                print(f\"\\nCompleted phases: {', '.join(progress.phases_completed)}\")",
        "            if progress.phases_pending:",
        "                print(f\"Remaining phases: {', '.join(progress.phases_pending)}\")",
        "",
        "    elif progress.status == \"completed\":",
        "        print(f\"Completed: {progress.completed_at}\")",
        "        print(f\"\\n✓ Full analysis completed successfully!\")",
        "        if progress.output_path:",
        "            print(f\"Output saved to: {progress.output_path}\")",
        "",
        "        if progress.phases_completed:",
        "            print(f\"\\nPhases completed: {', '.join(progress.phases_completed)}\")",
        "",
        "    elif progress.status == \"failed\":",
        "        print(f\"\\n✗ Full analysis failed!\")",
        "        if progress.error:",
        "            print(f\"Error: {progress.error}\")",
        "",
        "    print(f\"\\nProgress file: {progress_path}\")",
        "    print(\"Run the command again to check for updates.\\n\")",
        "",
        "",
        "class BackgroundProgressTracker(ProgressTracker):",
        "    \"\"\"",
        "    Extended ProgressTracker that writes progress to a file for background monitoring.",
        "    \"\"\"",
        "",
        "    def __init__(",
        "        self,",
        "        progress_path: Path,",
        "        output_path: Path,",
        "        log_file: Optional[str] = None,",
        "        verbose: bool = False,",
        "        quiet: bool = True,  # Default quiet for background",
        "        total_phases: int = 6",
        "    ):",
        "        super().__init__(log_file=log_file, verbose=verbose, quiet=quiet)",
        "        self.progress_path = progress_path",
        "        self.output_path = output_path",
        "        self.total_phases = total_phases",
        "        self.completed_phase_count = 0",
        "",
        "        # Define the phases for full analysis",
        "        self.all_phases = [",
        "            \"Discovering files\",",
        "            \"Indexing files\",",
        "            \"Computing analysis (full mode)\",",
        "            \"Extracting semantic relations\",",
        "            \"Saving corpus\",",
        "            \"Saving manifest\"",
        "        ]",
        "",
        "        # Initialize progress",
        "        self.bg_progress = BackgroundProgress(",
        "            status=\"running\",",
        "            started_at=datetime.now().isoformat(),",
        "            pid=os.getpid(),",
        "            output_path=str(output_path),",
        "            phases_pending=self.all_phases.copy(),",
        "        )",
        "        self._save_progress()",
        "",
        "    def _save_progress(self):",
        "        \"\"\"Save current progress to file.\"\"\"",
        "        try:",
        "            save_background_progress(self.progress_path, self.bg_progress)",
        "        except Exception:",
        "            pass  # Don't let progress save failures stop the analysis",
        "",
        "    def start_phase(self, name: str, total_items: int = 0):",
        "        \"\"\"Start a new phase and update progress file.\"\"\"",
        "        super().start_phase(name, total_items)",
        "        self.bg_progress.current_phase = name",
        "",
        "        # Calculate progress based on phase",
        "        base_progress = (self.completed_phase_count / self.total_phases) * 100",
        "        self.bg_progress.progress_percent = base_progress",
        "        self._save_progress()",
        "",
        "    def end_phase(self, name: Optional[str] = None, status: str = \"completed\"):",
        "        \"\"\"End a phase and update progress file.\"\"\"",
        "        super().end_phase(name, status)",
        "        phase_name = name or self.current_phase",
        "",
        "        if status == \"completed\" and phase_name:",
        "            self.completed_phase_count += 1",
        "            if phase_name not in self.bg_progress.phases_completed:",
        "                self.bg_progress.phases_completed.append(phase_name)",
        "            if phase_name in self.bg_progress.phases_pending:",
        "                self.bg_progress.phases_pending.remove(phase_name)",
        "",
        "            # Update progress",
        "            self.bg_progress.progress_percent = (",
        "                self.completed_phase_count / self.total_phases",
        "            ) * 100",
        "            self._save_progress()",
        "",
        "    def update_progress(self, items_processed: int, item_name: Optional[str] = None):",
        "        \"\"\"Update progress within current phase.\"\"\"",
        "        super().update_progress(items_processed, item_name)",
        "",
        "        if self.current_phase and self.current_phase in self.phases:",
        "            phase = self.phases[self.current_phase]",
        "            if phase.items_total > 0:",
        "                # Calculate intra-phase progress",
        "                phase_progress = items_processed / phase.items_total",
        "                base_progress = (self.completed_phase_count / self.total_phases) * 100",
        "                phase_contribution = (1 / self.total_phases) * 100 * phase_progress",
        "                self.bg_progress.progress_percent = base_progress + phase_contribution",
        "                self._save_progress()",
        "",
        "    def mark_completed(self):",
        "        \"\"\"Mark the analysis as completed.\"\"\"",
        "        self.bg_progress.status = \"completed\"",
        "        self.bg_progress.completed_at = datetime.now().isoformat()",
        "        self.bg_progress.progress_percent = 100.0",
        "        self.bg_progress.current_phase = \"\"",
        "        self._save_progress()",
        "",
        "    def mark_failed(self, error: str):",
        "        \"\"\"Mark the analysis as failed.\"\"\"",
        "        self.bg_progress.status = \"failed\"",
        "        self.bg_progress.completed_at = datetime.now().isoformat()",
        "        self.bg_progress.error = error",
        "        self._save_progress()",
        "",
        "",
        "def run_background_analysis(",
        "    base_path: Path,",
        "    output_path: Path,",
        "    progress_path: Path,",
        "    use_chunks: bool,",
        "    chunks_dir: str,",
        "    timeout: int,",
        "    verbose: bool,",
        "    log_file: Optional[str]",
        ") -> None:",
        "    \"\"\"",
        "    Run full analysis in a background-compatible way.",
        "",
        "    This function is designed to be called from a background thread/process.",
        "    It writes progress updates to a file that can be monitored.",
        "    \"\"\"",
        "    # Initialize background progress tracker",
        "    tracker = BackgroundProgressTracker(",
        "        progress_path=progress_path,",
        "        output_path=output_path,",
        "        log_file=log_file,",
        "        verbose=verbose,",
        "        quiet=True,  # Background mode is quiet to console",
        "    )",
        "",
        "    try:",
        "        tracker.log(\"Cortical Text Processor - Background Full Analysis\")",
        "        tracker.log(\"=\" * 50)",
        "        tracker.log(f\"Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")",
        "        tracker.log(f\"Progress file: {progress_path}\")",
        "",
        "        # Get files to index",
        "        tracker.start_phase(\"Discovering files\")",
        "        python_files = get_python_files(base_path)",
        "        doc_files = get_doc_files(base_path)",
        "        all_files = python_files + doc_files",
        "        tracker.end_phase(\"Discovering files\")",
        "",
        "        tracker.log(f\"\\nFound {len(python_files)} Python files and {len(doc_files)} documentation files\")",
        "",
        "        # Create processor",
        "        processor = create_code_processor()",
        "",
        "        # Index all files",
        "        tracker.start_phase(\"Indexing files\", len(all_files))",
        "        indexed = 0",
        "        total_lines = 0",
        "        file_mtimes = {}",
        "",
        "        for i, file_path in enumerate(all_files, 1):",
        "            doc_id = create_doc_id(file_path, base_path)",
        "            tracker.update_progress(i, doc_id)",
        "",
        "            metadata = index_file(processor, file_path, base_path, tracker)",
        "            if metadata:",
        "                indexed += 1",
        "                total_lines += metadata.get('line_count', 0)",
        "                file_mtimes[doc_id] = metadata.get('mtime', 0)",
        "",
        "        tracker.end_phase(\"Indexing files\")",
        "        tracker.log(f\"  Indexed {indexed} files ({total_lines:,} total lines)\")",
        "",
        "        # Run FULL analysis (the slow part)",
        "        tracker.start_phase(\"Computing analysis (full mode)\")",
        "        processor.compute_all(",
        "            build_concepts=True,",
        "            pagerank_method='semantic',",
        "            connection_strategy='hybrid',",
        "            verbose=False",
        "        )",
        "        tracker.end_phase(\"Computing analysis (full mode)\")",
        "",
        "        # Extract semantic relations",
        "        tracker.start_phase(\"Extracting semantic relations\")",
        "        processor.extract_corpus_semantics(",
        "            use_pattern_extraction=True,",
        "            verbose=False",
        "        )",
        "        tracker.end_phase(\"Extracting semantic relations\")",
        "",
        "        # Print corpus statistics",
        "        tracker.log(\"\\nCorpus Statistics:\")",
        "        tracker.log(f\"  Documents: {len(processor.documents)}\")",
        "        tracker.log(f\"  Tokens (Layer 0): {processor.layers[0].column_count()}\")",
        "        tracker.log(f\"  Bigrams (Layer 1): {processor.layers[1].column_count()}\")",
        "        tracker.log(f\"  Concepts (Layer 2): {processor.layers[2].column_count()}\")",
        "        tracker.log(f\"  Semantic relations: {len(processor.semantic_relations)}\")",
        "",
        "        # Save corpus",
        "        tracker.start_phase(\"Saving corpus\")",
        "        processor.save(str(output_path))",
        "        file_size = output_path.stat().st_size / 1024",
        "        tracker.log(f\"  Saved to {output_path.name} ({file_size:.1f} KB)\")",
        "        tracker.end_phase(\"Saving corpus\")",
        "",
        "        # Save manifest",
        "        manifest_path = get_manifest_path(output_path)",
        "        tracker.start_phase(\"Saving manifest\")",
        "        stats = {",
        "            'documents': len(processor.documents),",
        "            'tokens': processor.layers[0].column_count(),",
        "            'bigrams': processor.layers[1].column_count(),",
        "            'concepts': processor.layers[2].column_count(),",
        "            'semantic_relations': len(processor.semantic_relations),",
        "            'full_analysis': True,",
        "        }",
        "        save_manifest(manifest_path, file_mtimes, str(output_path), stats, tracker)",
        "        tracker.end_phase(\"Saving manifest\")",
        "",
        "        # Mark as completed",
        "        tracker.mark_completed()",
        "        tracker.print_summary()",
        "        tracker.log(\"\\n✓ Background full analysis completed successfully!\")",
        "        tracker.log(f\"Output saved to: {output_path}\")",
        "",
        "    except Exception as e:",
        "        tracker.mark_failed(str(e))",
        "        tracker.error(f\"Background analysis failed: {e}\")",
        "        tracker.print_summary()",
        "        raise",
        "",
        "",
        "def start_background_analysis(",
        "    base_path: Path,",
        "    output_path: Path,",
        "    progress_path: Path,",
        "    use_chunks: bool = False,",
        "    chunks_dir: str = 'corpus_chunks',",
        "    timeout: int = 0,",
        "    verbose: bool = False,",
        "    log_file: Optional[str] = None",
        ") -> None:",
        "    \"\"\"",
        "    Start full analysis in a background thread.",
        "",
        "    The analysis runs in a daemon thread that will continue running",
        "    even after the main process returns. Progress can be monitored",
        "    via the progress file.",
        "    \"\"\"",
        "    # Use a separate process for true background execution",
        "    # This ensures the analysis continues even if the main script exits",
        "    process = multiprocessing.Process(",
        "        target=run_background_analysis,",
        "        args=(",
        "            base_path,",
        "            output_path,",
        "            progress_path,",
        "            use_chunks,",
        "            chunks_dir,",
        "            timeout,",
        "            verbose,",
        "            log_file,",
        "        ),",
        "        daemon=False,  # Non-daemon so it survives parent exit",
        "    )",
        "    process.start()",
        "",
        "    # Give it a moment to start and create progress file",
        "    time.sleep(0.5)",
        "",
        "    print(\"\\n\" + \"=\" * 50)",
        "    print(\"FULL ANALYSIS STARTED IN BACKGROUND\")",
        "    print(\"=\" * 50)",
        "    print(f\"\\nProcess ID: {process.pid}\")",
        "    print(f\"Progress file: {progress_path}\")",
        "    print(f\"Output will be saved to: {output_path}\")",
        "    print(\"\\nThe analysis is running in the background.\")",
        "    print(\"You can safely close this terminal.\")",
        "    print(\"\\nTo check progress, run:\")",
        "    print(f\"  python scripts/index_codebase.py --full-analysis\")",
        "    print(\"\\nOr monitor the progress file directly:\")",
        "    print(f\"  cat {progress_path}\")",
        "    print(\"=\" * 50 + \"\\n\")",
        "",
        "",
        "# =============================================================================",
        "# Incremental Full Analysis (Resumable Batch Mode)",
        "# =============================================================================",
        "",
        "DEFAULT_BATCH_SIZE = 20",
        "",
        "# Analysis phases for incremental mode",
        "PHASE_INDEXING = \"indexing\"",
        "PHASE_FAST_ANALYSIS = \"fast_analysis\"",
        "PHASE_FULL_ANALYSIS = \"full_analysis\"",
        "PHASE_SEMANTIC_RELATIONS = \"semantic_relations\"",
        "PHASE_COMPLETED = \"completed\"",
        "",
        "",
        "@dataclass",
        "class IncrementalProgress:",
        "    \"\"\"Progress state for incremental full-analysis runs.\"\"\"",
        "    status: str = \"not_started\"  # not_started, in_progress, completed",
        "    started_at: Optional[str] = None",
        "    last_updated: Optional[str] = None",
        "    completed_at: Optional[str] = None",
        "",
        "    # File tracking",
        "    total_files: int = 0",
        "    files_indexed: List[str] = field(default_factory=list)",
        "    files_pending: List[str] = field(default_factory=list)",
        "",
        "    # Phase tracking",
        "    current_phase: str = PHASE_INDEXING",
        "    phases_completed: List[str] = field(default_factory=list)",
        "",
        "    # Stats",
        "    batch_count: int = 0",
        "    total_lines: int = 0",
        "    error: Optional[str] = None",
        "    output_path: Optional[str] = None",
        "",
        "    def to_dict(self) -> Dict[str, Any]:",
        "        return {",
        "            'status': self.status,",
        "            'started_at': self.started_at,",
        "            'last_updated': self.last_updated,",
        "            'completed_at': self.completed_at,",
        "            'total_files': self.total_files,",
        "            'files_indexed': self.files_indexed,",
        "            'files_pending': self.files_pending,",
        "            'current_phase': self.current_phase,",
        "            'phases_completed': self.phases_completed,",
        "            'batch_count': self.batch_count,",
        "            'total_lines': self.total_lines,",
        "            'error': self.error,",
        "            'output_path': self.output_path,",
        "        }",
        "",
        "    @classmethod",
        "    def from_dict(cls, data: Dict[str, Any]) -> 'IncrementalProgress':",
        "        return cls(",
        "            status=data.get('status', 'not_started'),",
        "            started_at=data.get('started_at'),",
        "            last_updated=data.get('last_updated'),",
        "            completed_at=data.get('completed_at'),",
        "            total_files=data.get('total_files', 0),",
        "            files_indexed=data.get('files_indexed', []),",
        "            files_pending=data.get('files_pending', []),",
        "            current_phase=data.get('current_phase', PHASE_INDEXING),",
        "            phases_completed=data.get('phases_completed', []),",
        "            batch_count=data.get('batch_count', 0),",
        "            total_lines=data.get('total_lines', 0),",
        "            error=data.get('error'),",
        "            output_path=data.get('output_path'),",
        "        )",
        "",
        "    @property",
        "    def progress_percent(self) -> float:",
        "        \"\"\"Calculate overall progress percentage.\"\"\"",
        "        # Phases: indexing (60%), fast_analysis (10%), full_analysis (20%), semantic (10%)",
        "        phase_weights = {",
        "            PHASE_INDEXING: 0.6,",
        "            PHASE_FAST_ANALYSIS: 0.1,",
        "            PHASE_FULL_ANALYSIS: 0.2,",
        "            PHASE_SEMANTIC_RELATIONS: 0.1,",
        "        }",
        "",
        "        progress = 0.0",
        "",
        "        # Add completed phases",
        "        for phase in self.phases_completed:",
        "            progress += phase_weights.get(phase, 0) * 100",
        "",
        "        # Add current phase progress",
        "        if self.current_phase == PHASE_INDEXING and self.total_files > 0:",
        "            file_progress = len(self.files_indexed) / self.total_files",
        "            progress += phase_weights[PHASE_INDEXING] * 100 * file_progress",
        "",
        "        return min(progress, 100.0)",
        "",
        "",
        "def get_incremental_progress_path(base_path: Path) -> Path:",
        "    \"\"\"Get the incremental progress file path.\"\"\"",
        "    return base_path / '.index_incremental_progress.json'",
        "",
        "",
        "def load_incremental_progress(progress_path: Path) -> Optional[IncrementalProgress]:",
        "    \"\"\"Load incremental progress from file.\"\"\"",
        "    if not progress_path.exists():",
        "        return None",
        "    try:",
        "        with open(progress_path, 'r') as f:",
        "            data = json.load(f)",
        "        return IncrementalProgress.from_dict(data)",
        "    except (json.JSONDecodeError, IOError):",
        "        return None",
        "",
        "",
        "def save_incremental_progress(progress_path: Path, progress: IncrementalProgress) -> None:",
        "    \"\"\"Save incremental progress to file.\"\"\"",
        "    progress.last_updated = datetime.now().isoformat()",
        "    with open(progress_path, 'w') as f:",
        "        json.dump(progress.to_dict(), f, indent=2)",
        "",
        "",
        "def display_incremental_progress(progress: IncrementalProgress, progress_path: Path) -> None:",
        "    \"\"\"Display current incremental progress to the user.\"\"\"",
        "    print(\"\\n\" + \"=\" * 50)",
        "    print(\"INCREMENTAL FULL-ANALYSIS STATUS\")",
        "    print(\"=\" * 50)",
        "",
        "    print(f\"\\nStatus: {progress.status.upper()}\")",
        "    print(f\"Current phase: {progress.current_phase}\")",
        "",
        "    if progress.started_at:",
        "        print(f\"Started: {progress.started_at}\")",
        "    if progress.last_updated:",
        "        print(f\"Last updated: {progress.last_updated}\")",
        "",
        "    # Progress bar",
        "    pct = progress.progress_percent",
        "    bar_width = 40",
        "    filled = int(bar_width * pct / 100)",
        "    bar = \"█\" * filled + \"░\" * (bar_width - filled)",
        "    print(f\"\\nProgress: {pct:.1f}%\")",
        "    print(f\"[{bar}]\")",
        "",
        "    # File stats",
        "    indexed = len(progress.files_indexed)",
        "    pending = len(progress.files_pending)",
        "    total = progress.total_files",
        "    print(f\"\\nFiles: {indexed}/{total} indexed, {pending} pending\")",
        "    print(f\"Batches completed: {progress.batch_count}\")",
        "    print(f\"Total lines: {progress.total_lines:,}\")",
        "",
        "    # Phase status",
        "    if progress.phases_completed:",
        "        print(f\"\\nPhases completed: {', '.join(progress.phases_completed)}\")",
        "",
        "    if progress.status == \"completed\":",
        "        print(f\"\\n✓ Full analysis completed successfully!\")",
        "        if progress.output_path:",
        "            print(f\"Output saved to: {progress.output_path}\")",
        "    elif progress.status == \"in_progress\":",
        "        print(f\"\\nRun the command again to continue processing.\")",
        "",
        "    if progress.error:",
        "        print(f\"\\nLast error: {progress.error}\")",
        "",
        "    print(f\"\\nProgress file: {progress_path}\")",
        "    print(\"\")",
        "",
        "",
        "def run_incremental_full_analysis(",
        "    base_path: Path,",
        "    output_path: Path,",
        "    progress_path: Path,",
        "    batch_size: int,",
        "    tracker: ProgressTracker",
        ") -> bool:",
        "    \"\"\"",
        "    Run one batch of the incremental full analysis.",
        "",
        "    Returns True if there's more work to do, False if complete.",
        "    \"\"\"",
        "    # Load or create progress",
        "    progress = load_incremental_progress(progress_path)",
        "",
        "    if progress is None:",
        "        # First run - discover files and initialize",
        "        tracker.log(\"\\nInitializing incremental full analysis...\")",
        "        tracker.start_phase(\"Discovering files\")",
        "",
        "        python_files = get_python_files(base_path)",
        "        doc_files = get_doc_files(base_path)",
        "        all_files = python_files + doc_files",
        "",
        "        # Create list of doc_ids",
        "        all_doc_ids = [create_doc_id(f, base_path) for f in all_files]",
        "",
        "        progress = IncrementalProgress(",
        "            status=\"in_progress\",",
        "            started_at=datetime.now().isoformat(),",
        "            total_files=len(all_files),",
        "            files_pending=all_doc_ids,",
        "            current_phase=PHASE_INDEXING,",
        "            output_path=str(output_path),",
        "        )",
        "        save_incremental_progress(progress_path, progress)",
        "        tracker.end_phase(\"Discovering files\")",
        "        tracker.log(f\"  Found {len(all_files)} files to index\")",
        "",
        "    # Check if already complete",
        "    if progress.status == \"completed\":",
        "        tracker.log(\"\\n✓ Incremental full analysis already complete!\")",
        "        display_incremental_progress(progress, progress_path)",
        "        return False",
        "",
        "    # Load or create processor",
        "    if output_path.exists() and len(progress.files_indexed) > 0:",
        "        tracker.log(f\"\\nLoading existing corpus ({len(progress.files_indexed)} files indexed)...\")",
        "        try:",
        "            processor = CorticalTextProcessor.load(str(output_path))",
        "        except Exception as e:",
        "            tracker.warn(f\"Could not load corpus: {e}. Starting fresh.\")",
        "            processor = create_code_processor()",
        "            progress.files_indexed = []",
        "            progress.files_pending = [create_doc_id(f, base_path)",
        "                                      for f in get_python_files(base_path) + get_doc_files(base_path)]",
        "    else:",
        "        processor = create_code_processor()",
        "",
        "    # Phase: Indexing files",
        "    if progress.current_phase == PHASE_INDEXING:",
        "        if progress.files_pending:",
        "            # Process next batch",
        "            batch = progress.files_pending[:batch_size]",
        "            tracker.start_phase(f\"Indexing batch {progress.batch_count + 1}\", len(batch))",
        "",
        "            batch_lines = 0",
        "            for i, doc_id in enumerate(batch, 1):",
        "                file_path = base_path / doc_id",
        "                tracker.update_progress(i, doc_id)",
        "",
        "                if file_path.exists():",
        "                    metadata = index_file(processor, file_path, base_path, tracker)",
        "                    if metadata:",
        "                        batch_lines += metadata.get('line_count', 0)",
        "                        progress.files_indexed.append(doc_id)",
        "                else:",
        "                    tracker.warn(f\"File not found: {doc_id}\")",
        "",
        "                # Remove from pending",
        "                if doc_id in progress.files_pending:",
        "                    progress.files_pending.remove(doc_id)",
        "",
        "            progress.batch_count += 1",
        "            progress.total_lines += batch_lines",
        "            tracker.end_phase(f\"Indexing batch {progress.batch_count}\")",
        "            tracker.log(f\"  Indexed {len(batch)} files ({batch_lines:,} lines)\")",
        "",
        "            # Run fast analysis after each batch so corpus is usable",
        "            tracker.start_phase(\"Fast analysis (batch)\")",
        "            processor.propagate_activation(verbose=False)",
        "            processor.compute_importance(verbose=False)",
        "            processor.compute_tfidf(verbose=False)",
        "            tracker.end_phase(\"Fast analysis (batch)\")",
        "",
        "            # Save corpus and progress",
        "            tracker.start_phase(\"Saving checkpoint\")",
        "            processor.save(str(output_path))",
        "            save_incremental_progress(progress_path, progress)",
        "            tracker.end_phase(\"Saving checkpoint\")",
        "",
        "            # Show status",
        "            display_incremental_progress(progress, progress_path)",
        "",
        "            if progress.files_pending:",
        "                tracker.log(f\"\\n→ {len(progress.files_pending)} files remaining. Run again to continue.\")",
        "                return True",
        "",
        "        # All files indexed - move to next phase",
        "        progress.phases_completed.append(PHASE_INDEXING)",
        "        progress.current_phase = PHASE_FAST_ANALYSIS",
        "        save_incremental_progress(progress_path, progress)",
        "        tracker.log(\"\\n✓ All files indexed! Moving to analysis phase...\")",
        "",
        "    # Phase: Fast analysis (full corpus)",
        "    if progress.current_phase == PHASE_FAST_ANALYSIS:",
        "        tracker.start_phase(\"Fast analysis (full corpus)\")",
        "        processor.propagate_activation(verbose=False)",
        "        processor.compute_importance(verbose=False)",
        "        processor.compute_tfidf(verbose=False)",
        "        processor.compute_document_connections(verbose=False)",
        "        tracker.end_phase(\"Fast analysis (full corpus)\")",
        "",
        "        progress.phases_completed.append(PHASE_FAST_ANALYSIS)",
        "        progress.current_phase = PHASE_FULL_ANALYSIS",
        "",
        "        # Save checkpoint",
        "        processor.save(str(output_path))",
        "        save_incremental_progress(progress_path, progress)",
        "",
        "        tracker.log(\"\\n→ Fast analysis complete. Run again for full semantic analysis.\")",
        "        display_incremental_progress(progress, progress_path)",
        "        return True",
        "",
        "    # Phase: Full analysis (expensive)",
        "    if progress.current_phase == PHASE_FULL_ANALYSIS:",
        "        tracker.start_phase(\"Full semantic analysis\")",
        "        tracker.log(\"  This may take a few minutes...\")",
        "",
        "        try:",
        "            processor.compute_all(",
        "                build_concepts=True,",
        "                pagerank_method='semantic',",
        "                connection_strategy='hybrid',",
        "                verbose=False",
        "            )",
        "            tracker.end_phase(\"Full semantic analysis\")",
        "",
        "            progress.phases_completed.append(PHASE_FULL_ANALYSIS)",
        "            progress.current_phase = PHASE_SEMANTIC_RELATIONS",
        "",
        "            # Save checkpoint",
        "            processor.save(str(output_path))",
        "            save_incremental_progress(progress_path, progress)",
        "",
        "            tracker.log(\"\\n→ Full analysis complete. Run again for semantic relations.\")",
        "            display_incremental_progress(progress, progress_path)",
        "            return True",
        "",
        "        except Exception as e:",
        "            progress.error = str(e)",
        "            save_incremental_progress(progress_path, progress)",
        "            tracker.error(f\"Full analysis failed: {e}\")",
        "            tracker.log(\"  You can retry by running the command again.\")",
        "            return True",
        "",
        "    # Phase: Semantic relations",
        "    if progress.current_phase == PHASE_SEMANTIC_RELATIONS:",
        "        tracker.start_phase(\"Extracting semantic relations\")",
        "",
        "        try:",
        "            processor.extract_corpus_semantics(",
        "                use_pattern_extraction=True,",
        "                verbose=False",
        "            )",
        "            tracker.end_phase(\"Extracting semantic relations\")",
        "",
        "            progress.phases_completed.append(PHASE_SEMANTIC_RELATIONS)",
        "            progress.current_phase = PHASE_COMPLETED",
        "            progress.status = \"completed\"",
        "            progress.completed_at = datetime.now().isoformat()",
        "            progress.error = None",
        "",
        "            # Final save",
        "            processor.save(str(output_path))",
        "",
        "            # Save manifest",
        "            manifest_path = get_manifest_path(output_path)",
        "            file_mtimes = {}",
        "            for doc_id in progress.files_indexed:",
        "                file_path = base_path / doc_id",
        "                if file_path.exists():",
        "                    file_mtimes[doc_id] = get_file_mtime(file_path)",
        "",
        "            stats = {",
        "                'documents': len(processor.documents),",
        "                'tokens': processor.layers[0].column_count(),",
        "                'bigrams': processor.layers[1].column_count(),",
        "                'concepts': processor.layers[2].column_count(),",
        "                'semantic_relations': len(processor.semantic_relations),",
        "                'full_analysis': True,",
        "                'incremental': True,",
        "            }",
        "            save_manifest(manifest_path, file_mtimes, str(output_path), stats, tracker)",
        "            save_incremental_progress(progress_path, progress)",
        "",
        "            tracker.log(\"\\n\" + \"=\" * 50)",
        "            tracker.log(\"✓ INCREMENTAL FULL ANALYSIS COMPLETE!\")",
        "            tracker.log(\"=\" * 50)",
        "            tracker.log(f\"\\nCorpus Statistics:\")",
        "            tracker.log(f\"  Documents: {len(processor.documents)}\")",
        "            tracker.log(f\"  Tokens: {processor.layers[0].column_count()}\")",
        "            tracker.log(f\"  Bigrams: {processor.layers[1].column_count()}\")",
        "            tracker.log(f\"  Concepts: {processor.layers[2].column_count()}\")",
        "            tracker.log(f\"  Semantic relations: {len(processor.semantic_relations)}\")",
        "            tracker.log(f\"\\nOutput saved to: {output_path}\")",
        "",
        "            return False",
        "",
        "        except Exception as e:",
        "            progress.error = str(e)",
        "            save_incremental_progress(progress_path, progress)",
        "            tracker.error(f\"Semantic extraction failed: {e}\")",
        "            tracker.log(\"  You can retry by running the command again.\")",
        "            return True",
        "",
        "    return False",
        "",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "        signal.alarm(seconds)",
        "",
        "        try:",
        "            yield",
        "        finally:",
        "            # Restore the old handler and cancel the alarm",
        "            signal.alarm(0)",
        "            signal.signal(signal.SIGALRM, old_handler)",
        "",
        ""
      ],
      "context_after": [
        "# =============================================================================",
        "# Manifest Operations",
        "# =============================================================================",
        "",
        "def get_manifest_path(corpus_path: Path) -> Path:",
        "    \"\"\"Get the manifest file path based on corpus path.\"\"\"",
        "    return corpus_path.with_suffix('.manifest.json')",
        "",
        "",
        "def load_manifest("
      ],
      "change_type": "add"
    },
    {
      "file": "scripts/index_codebase.py",
      "function": "def show_chunk_status(",
      "start_line": 1066,
      "lines_added": [
        "  python scripts/index_codebase.py                   # Full rebuild (fast mode)",
        "",
        "Background Full Analysis (for long-running environments):",
        "  python scripts/index_codebase.py --full-analysis              # Start in background",
        "  python scripts/index_codebase.py --full-analysis              # Check progress",
        "  python scripts/index_codebase.py --full-analysis --foreground # Run synchronously",
        "",
        "Incremental Full Analysis (resumable, for short-lived processes):",
        "  python scripts/index_codebase.py --full-analysis --batch      # Process one batch",
        "  python scripts/index_codebase.py --full-analysis --batch      # Continue (run again)",
        "  python scripts/index_codebase.py --full-analysis --batch --batch-size 10  # Smaller batches",
        "  python scripts/index_codebase.py --full-analysis --batch --status  # Check progress",
        "                        help='Use full semantic analysis (runs in background by default)')",
        "    parser.add_argument('--foreground', action='store_true',",
        "                        help='Run full analysis in foreground (blocking) instead of background')",
        "    parser.add_argument('--progress-file', type=str, default=None,",
        "                        help='Custom path for progress file (default: .index_progress.json)')",
        "",
        "    # Incremental batch mode options",
        "    parser.add_argument('--batch', action='store_true',",
        "                        help='Use incremental batch mode (resumable, for short-lived processes)')",
        "    parser.add_argument('--batch-size', type=int, default=DEFAULT_BATCH_SIZE,",
        "                        help=f'Number of files per batch (default: {DEFAULT_BATCH_SIZE})')",
        "    # Set up progress file path",
        "    if args.progress_file:",
        "        progress_path = Path(args.progress_file)",
        "        if not progress_path.is_absolute():",
        "            progress_path = base_path / args.progress_file",
        "    else:",
        "        progress_path = get_progress_file_path(base_path)",
        "",
        "    # Handle incremental batch mode for full-analysis",
        "    if args.full_analysis and args.batch:",
        "        incremental_progress_path = get_incremental_progress_path(base_path)",
        "",
        "        # Status check only",
        "        if args.status:",
        "            existing = load_incremental_progress(incremental_progress_path)",
        "            if existing:",
        "                display_incremental_progress(existing, incremental_progress_path)",
        "            else:",
        "                print(\"\\nNo incremental full-analysis in progress.\")",
        "                print(f\"Run with --full-analysis --batch to start.\\n\")",
        "            return",
        "",
        "        # Force restart",
        "        if args.force:",
        "            if incremental_progress_path.exists():",
        "                incremental_progress_path.unlink()",
        "                print(\"Cleared previous incremental progress.\\n\")",
        "",
        "        # Initialize tracker for batch mode",
        "        tracker = ProgressTracker(",
        "            log_file=log_path,",
        "            verbose=args.verbose,",
        "            quiet=args.quiet",
        "        )",
        "",
        "        tracker.log(\"Cortical Text Processor - Incremental Full Analysis\")",
        "        tracker.log(\"=\" * 50)",
        "        tracker.log(f\"Batch size: {args.batch_size} files\")",
        "",
        "        # Run one batch",
        "        more_work = run_incremental_full_analysis(",
        "            base_path=base_path,",
        "            output_path=output_path,",
        "            progress_path=incremental_progress_path,",
        "            batch_size=args.batch_size,",
        "            tracker=tracker",
        "        )",
        "",
        "        tracker.print_summary()",
        "",
        "        if more_work:",
        "            print(\"\\nRun the command again to continue processing.\")",
        "        return",
        "",
        "    # Handle background full-analysis mode",
        "    if args.full_analysis and not args.foreground and not args.batch:",
        "        # Check for existing progress",
        "        existing_progress = load_background_progress(progress_path)",
        "",
        "        if existing_progress:",
        "            if existing_progress.status == \"running\":",
        "                # Check if the process is actually alive",
        "                if existing_progress.pid and is_process_alive(existing_progress.pid):",
        "                    # Show progress and exit",
        "                    display_progress(existing_progress, progress_path)",
        "                    return",
        "                else:",
        "                    # Process died, show warning and offer to restart",
        "                    print(\"\\n\" + \"=\" * 50)",
        "                    print(\"PREVIOUS BACKGROUND ANALYSIS\")",
        "                    print(\"=\" * 50)",
        "                    print(f\"\\nStatus: STALE (process {existing_progress.pid} no longer running)\")",
        "                    print(f\"Started: {existing_progress.started_at}\")",
        "                    print(f\"Last phase: {existing_progress.current_phase}\")",
        "                    print(f\"Progress: {existing_progress.progress_percent:.1f}%\")",
        "                    print(\"\\nThe previous background process appears to have stopped.\")",
        "                    print(\"Starting a new background analysis...\\n\")",
        "",
        "            elif existing_progress.status == \"completed\":",
        "                # Show completion and ask if they want to re-run",
        "                display_progress(existing_progress, progress_path)",
        "                print(\"To start a fresh analysis, delete the progress file first:\")",
        "                print(f\"  rm {progress_path}\")",
        "                print(\"Or run with --force to overwrite.\\n\")",
        "                if not args.force:",
        "                    return",
        "",
        "            elif existing_progress.status == \"failed\":",
        "                # Show failure and start new",
        "                print(\"\\n\" + \"=\" * 50)",
        "                print(\"PREVIOUS ANALYSIS FAILED\")",
        "                print(\"=\" * 50)",
        "                print(f\"\\nError: {existing_progress.error}\")",
        "                print(\"\\nStarting a new background analysis...\\n\")",
        "",
        "        # Start new background analysis",
        "        start_background_analysis(",
        "            base_path=base_path,",
        "            output_path=output_path,",
        "            progress_path=progress_path,",
        "            use_chunks=args.use_chunks,",
        "            chunks_dir=args.chunks_dir,",
        "            timeout=args.timeout,",
        "            verbose=args.verbose,",
        "            log_file=log_path,",
        "        )",
        "        return",
        "",
        "    # Initialize progress tracker for foreground operation"
      ],
      "lines_removed": [
        "  python scripts/index_codebase.py                   # Full rebuild",
        "                        help='Use full semantic analysis (slower but more accurate)')",
        "    # Initialize progress tracker"
      ],
      "context_before": [
        "# =============================================================================",
        "# Main Entry Point",
        "# =============================================================================",
        "",
        "def main():",
        "    parser = argparse.ArgumentParser(",
        "        description='Index the codebase for semantic search',",
        "        formatter_class=argparse.RawDescriptionHelpFormatter,",
        "        epilog=\"\"\"",
        "Examples:"
      ],
      "context_after": [
        "  python scripts/index_codebase.py --incremental    # Update changed files only",
        "  python scripts/index_codebase.py --status         # Show what would change",
        "  python scripts/index_codebase.py --force          # Force full rebuild",
        "  python scripts/index_codebase.py --log index.log  # Log to file",
        "  python scripts/index_codebase.py --timeout 60     # Timeout after 60s",
        "        \"\"\"",
        "    )",
        "    parser.add_argument('--output', '-o', default='corpus_dev.pkl',",
        "                        help='Output file path (default: corpus_dev.pkl)')",
        "    parser.add_argument('--incremental', '-i', action='store_true',",
        "                        help='Only index changed files (requires existing corpus)')",
        "    parser.add_argument('--force', '-f', action='store_true',",
        "                        help='Force full rebuild even if manifest exists')",
        "    parser.add_argument('--status', '-s', action='store_true',",
        "                        help='Show what would change without indexing')",
        "    parser.add_argument('--verbose', '-v', action='store_true',",
        "                        help='Show verbose output (per-file progress)')",
        "    parser.add_argument('--quiet', '-q', action='store_true',",
        "                        help='Suppress console output (still writes to log)')",
        "    parser.add_argument('--log', '-l', type=str, default=None,",
        "                        help='Log file path (writes detailed log)')",
        "    parser.add_argument('--timeout', '-t', type=int, default=DEFAULT_TIMEOUT,",
        "                        help=f'Timeout in seconds (0=none, default={DEFAULT_TIMEOUT})')",
        "    parser.add_argument('--full-analysis', action='store_true',",
        "",
        "    # Chunk-based indexing options",
        "    parser.add_argument('--use-chunks', action='store_true',",
        "                        help='Use git-compatible chunk-based indexing')",
        "    parser.add_argument('--chunks-dir', default='corpus_chunks',",
        "                        help='Directory for chunk files (default: corpus_chunks)')",
        "    parser.add_argument('--compact', action='store_true',",
        "                        help='Compact old chunks into a single file')",
        "    parser.add_argument('--compact-before', type=str, default=None,",
        "                        help='Only compact chunks before this date (YYYY-MM-DD)')",
        "    parser.add_argument('--compact-keep', type=int, default=0,",
        "                        help='Keep this many recent chunks when compacting')",
        "",
        "    args = parser.parse_args()",
        "",
        "    base_path = Path(__file__).parent.parent",
        "    output_path = base_path / args.output",
        "    manifest_path = get_manifest_path(output_path)",
        "",
        "    # Set up log file path",
        "    log_path = None",
        "    if args.log:",
        "        log_path = args.log if os.path.isabs(args.log) else str(base_path / args.log)",
        "",
        "    tracker = ProgressTracker(",
        "        log_file=log_path,",
        "        verbose=args.verbose,",
        "        quiet=args.quiet",
        "    )",
        "",
        "    tracker.log(\"Cortical Text Processor - Codebase Indexer\")",
        "    tracker.log(\"=\" * 50)",
        "    tracker.log(f\"Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")",
        "    if args.timeout > 0:"
      ],
      "change_type": "modify"
    }
  ],
  "hour_of_day": 19,
  "day_of_week": "Thursday",
  "seconds_since_last_commit": -326669,
  "is_merge": true,
  "is_initial": false,
  "parent_count": 2,
  "session_id": null,
  "related_chats": [],
  "ci_result": null,
  "reverted": false,
  "amended": false
}