{
  "hash": "67f09e5ba27f7c414518efc446033f221ab4756b",
  "message": "Implement git-friendly state storage (Phase 1 of Task #206)",
  "author": "Claude",
  "timestamp": "2025-12-14 00:27:15 +0000",
  "branch": "claude/multi-index-design-DvifZ",
  "files_changed": [
    "cortical/state_storage.py",
    "tasks/2025-12-14_00-21-46_8d66.json",
    "tests/unit/test_state_storage.py"
  ],
  "insertions": 1389,
  "deletions": 0,
  "hunks": [
    {
      "file": "cortical/state_storage.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "\"\"\"",
        "Git-friendly State Storage Module",
        "=================================",
        "",
        "Replaces pickle-based persistence with JSON files that:",
        "- Can be diff'd and reviewed in git",
        "- Won't cause merge conflicts",
        "- Support incremental updates",
        "- Are language/version independent",
        "",
        "Architecture:",
        "    corpus_state/",
        "    ├── manifest.json           # Version, checksums, staleness",
        "    ├── documents.json          # Document content and metadata",
        "    ├── layers/",
        "    │   ├── L0_tokens.json      # Token minicolumns",
        "    │   ├── L1_bigrams.json     # Bigram minicolumns",
        "    │   ├── L2_concepts.json    # Concept clusters",
        "    │   └── L3_documents.json   # Document minicolumns",
        "    └── computed/",
        "        ├── semantic_relations.json",
        "        └── embeddings.json",
        "",
        "Usage:",
        "    # Save processor state",
        "    writer = StateWriter('corpus_state')",
        "    writer.save_processor(processor)",
        "",
        "    # Load processor state",
        "    loader = StateLoader('corpus_state')",
        "    layers, documents, metadata, embeddings, relations = loader.load_all()",
        "\"\"\"",
        "",
        "import hashlib",
        "import json",
        "import os",
        "import logging",
        "from dataclasses import dataclass, field, asdict",
        "from datetime import datetime",
        "from pathlib import Path",
        "from typing import Dict, List, Optional, Any, Tuple, Set",
        "",
        "from .layers import CorticalLayer, HierarchicalLayer",
        "from .minicolumn import Minicolumn",
        "",
        "logger = logging.getLogger(__name__)",
        "",
        "# Version for state format (increment on breaking changes)",
        "STATE_VERSION = 1",
        "",
        "# Layer enum value to filename mapping",
        "LAYER_FILENAMES = {",
        "    0: 'L0_tokens.json',",
        "    1: 'L1_bigrams.json',",
        "    2: 'L2_concepts.json',",
        "    3: 'L3_documents.json',",
        "}",
        "",
        "",
        "@dataclass",
        "class StateManifest:",
        "    \"\"\"",
        "    Manifest file tracking state version and component checksums.",
        "",
        "    Used to:",
        "    - Detect which components changed since last save",
        "    - Validate loaded state integrity",
        "    - Track staleness of computed values",
        "    \"\"\"",
        "    version: int = STATE_VERSION",
        "    created_at: str = field(default_factory=lambda: datetime.now().isoformat())",
        "    updated_at: str = field(default_factory=lambda: datetime.now().isoformat())",
        "    checksums: Dict[str, str] = field(default_factory=dict)",
        "    stale_computations: List[str] = field(default_factory=list)",
        "    document_count: int = 0",
        "    layer_stats: Dict[str, int] = field(default_factory=dict)",
        "",
        "    def to_dict(self) -> Dict[str, Any]:",
        "        \"\"\"Convert to dictionary for JSON serialization.\"\"\"",
        "        return asdict(self)",
        "",
        "    @classmethod",
        "    def from_dict(cls, data: Dict[str, Any]) -> 'StateManifest':",
        "        \"\"\"Create manifest from dictionary.\"\"\"",
        "        return cls(",
        "            version=data.get('version', STATE_VERSION),",
        "            created_at=data.get('created_at', datetime.now().isoformat()),",
        "            updated_at=data.get('updated_at', datetime.now().isoformat()),",
        "            checksums=data.get('checksums', {}),",
        "            stale_computations=data.get('stale_computations', []),",
        "            document_count=data.get('document_count', 0),",
        "            layer_stats=data.get('layer_stats', {})",
        "        )",
        "",
        "    def update_checksum(self, component: str, content: str) -> bool:",
        "        \"\"\"",
        "        Update checksum for a component.",
        "",
        "        Args:",
        "            component: Component name (e.g., 'L0_tokens', 'documents')",
        "            content: Serialized content to hash",
        "",
        "        Returns:",
        "            True if checksum changed, False if same",
        "        \"\"\"",
        "        new_hash = hashlib.sha256(content.encode('utf-8')).hexdigest()[:16]",
        "        old_hash = self.checksums.get(component)",
        "        self.checksums[component] = new_hash",
        "        self.updated_at = datetime.now().isoformat()",
        "        return new_hash != old_hash",
        "",
        "",
        "class StateWriter:",
        "    \"\"\"",
        "    Writes processor state to git-friendly JSON files.",
        "",
        "    Features:",
        "    - Incremental saves (only writes changed components)",
        "    - Atomic writes (write to temp, then rename)",
        "    - Content hashing for change detection",
        "",
        "    Usage:",
        "        writer = StateWriter('corpus_state')",
        "        writer.save_all(layers, documents, metadata, embeddings, relations)",
        "",
        "        # Or incrementally:",
        "        writer.save_layer(layers[CorticalLayer.TOKENS])",
        "        writer.save_documents(documents, metadata)",
        "        writer.save_manifest()",
        "    \"\"\"",
        "",
        "    def __init__(self, state_dir: str):",
        "        \"\"\"",
        "        Initialize state writer.",
        "",
        "        Args:",
        "            state_dir: Directory to write state files",
        "        \"\"\"",
        "        self.state_dir = Path(state_dir)",
        "        self.layers_dir = self.state_dir / 'layers'",
        "        self.computed_dir = self.state_dir / 'computed'",
        "        self.manifest: Optional[StateManifest] = None",
        "        self._load_or_create_manifest()",
        "",
        "    def _load_or_create_manifest(self) -> None:",
        "        \"\"\"Load existing manifest or create new one.\"\"\"",
        "        manifest_path = self.state_dir / 'manifest.json'",
        "        if manifest_path.exists():",
        "            try:",
        "                with open(manifest_path, 'r', encoding='utf-8') as f:",
        "                    data = json.load(f)",
        "                self.manifest = StateManifest.from_dict(data)",
        "            except (json.JSONDecodeError, IOError):",
        "                self.manifest = StateManifest()",
        "        else:",
        "            self.manifest = StateManifest()",
        "",
        "    def _ensure_dirs(self) -> None:",
        "        \"\"\"Create directories if they don't exist.\"\"\"",
        "        self.state_dir.mkdir(parents=True, exist_ok=True)",
        "        self.layers_dir.mkdir(exist_ok=True)",
        "        self.computed_dir.mkdir(exist_ok=True)",
        "",
        "    def _atomic_write(self, filepath: Path, content: str) -> None:",
        "        \"\"\"",
        "        Write content atomically using temp file + rename.",
        "",
        "        This prevents data corruption if the process crashes mid-write.",
        "        \"\"\"",
        "        temp_path = filepath.with_suffix('.json.tmp')",
        "        try:",
        "            with open(temp_path, 'w', encoding='utf-8') as f:",
        "                f.write(content)",
        "            temp_path.replace(filepath)",
        "        except Exception:",
        "            if temp_path.exists():",
        "                temp_path.unlink()",
        "            raise",
        "",
        "    def save_layer(",
        "        self,",
        "        layer: HierarchicalLayer,",
        "        force: bool = False",
        "    ) -> bool:",
        "        \"\"\"",
        "        Save a single layer to its JSON file.",
        "",
        "        Args:",
        "            layer: The layer to save",
        "            force: Save even if checksum unchanged",
        "",
        "        Returns:",
        "            True if file was written, False if skipped (unchanged)",
        "        \"\"\"",
        "        self._ensure_dirs()",
        "",
        "        filename = LAYER_FILENAMES.get(layer.level)",
        "        if not filename:",
        "            raise ValueError(f\"Unknown layer level: {layer.level}\")",
        "",
        "        filepath = self.layers_dir / filename",
        "        component_name = f\"layer_{layer.level}\"",
        "",
        "        # Serialize layer",
        "        layer_data = layer.to_dict()",
        "        content = json.dumps(layer_data, indent=2, ensure_ascii=False)",
        "",
        "        # Check if changed",
        "        changed = self.manifest.update_checksum(component_name, content)",
        "",
        "        if not changed and not force:",
        "            return False",
        "",
        "        self._atomic_write(filepath, content)",
        "        self.manifest.layer_stats[f\"L{layer.level}\"] = len(layer.minicolumns)",
        "",
        "        return True",
        "",
        "    def save_documents(",
        "        self,",
        "        documents: Dict[str, str],",
        "        document_metadata: Optional[Dict[str, Dict[str, Any]]] = None,",
        "        force: bool = False",
        "    ) -> bool:",
        "        \"\"\"",
        "        Save documents and metadata.",
        "",
        "        Args:",
        "            documents: Document ID to content mapping",
        "            document_metadata: Document ID to metadata mapping",
        "            force: Save even if unchanged",
        "",
        "        Returns:",
        "            True if file was written",
        "        \"\"\"",
        "        self._ensure_dirs()",
        "",
        "        filepath = self.state_dir / 'documents.json'",
        "",
        "        data = {",
        "            'documents': documents,",
        "            'metadata': document_metadata or {}",
        "        }",
        "        content = json.dumps(data, indent=2, ensure_ascii=False)",
        "",
        "        changed = self.manifest.update_checksum('documents', content)",
        "",
        "        if not changed and not force:",
        "            return False",
        "",
        "        self._atomic_write(filepath, content)",
        "        self.manifest.document_count = len(documents)",
        "",
        "        return True",
        "",
        "    def save_semantic_relations(",
        "        self,",
        "        relations: List[Tuple],",
        "        force: bool = False",
        "    ) -> bool:",
        "        \"\"\"",
        "        Save semantic relations.",
        "",
        "        Args:",
        "            relations: List of (term1, relation, term2, weight) tuples",
        "            force: Save even if unchanged",
        "",
        "        Returns:",
        "            True if file was written",
        "        \"\"\"",
        "        self._ensure_dirs()",
        "",
        "        filepath = self.computed_dir / 'semantic_relations.json'",
        "",
        "        # Convert tuples to lists for JSON",
        "        data = {",
        "            'relations': [list(r) for r in relations],",
        "            'count': len(relations)",
        "        }",
        "        content = json.dumps(data, indent=2, ensure_ascii=False)",
        "",
        "        changed = self.manifest.update_checksum('semantic_relations', content)",
        "",
        "        if not changed and not force:",
        "            return False",
        "",
        "        self._atomic_write(filepath, content)",
        "        return True",
        "",
        "    def save_embeddings(",
        "        self,",
        "        embeddings: Dict[str, List[float]],",
        "        force: bool = False",
        "    ) -> bool:",
        "        \"\"\"",
        "        Save graph embeddings.",
        "",
        "        Args:",
        "            embeddings: Term to embedding vector mapping",
        "            force: Save even if unchanged",
        "",
        "        Returns:",
        "            True if file was written",
        "        \"\"\"",
        "        self._ensure_dirs()",
        "",
        "        filepath = self.computed_dir / 'embeddings.json'",
        "",
        "        data = {",
        "            'embeddings': embeddings,",
        "            'dimensions': len(next(iter(embeddings.values()))) if embeddings else 0,",
        "            'count': len(embeddings)",
        "        }",
        "        content = json.dumps(data, indent=2, ensure_ascii=False)",
        "",
        "        changed = self.manifest.update_checksum('embeddings', content)",
        "",
        "        if not changed and not force:",
        "            return False",
        "",
        "        self._atomic_write(filepath, content)",
        "        return True",
        "",
        "    def save_manifest(self) -> None:",
        "        \"\"\"Save the manifest file.\"\"\"",
        "        self._ensure_dirs()",
        "        filepath = self.state_dir / 'manifest.json'",
        "        content = json.dumps(self.manifest.to_dict(), indent=2, ensure_ascii=False)",
        "        self._atomic_write(filepath, content)",
        "",
        "    def save_all(",
        "        self,",
        "        layers: Dict[CorticalLayer, HierarchicalLayer],",
        "        documents: Dict[str, str],",
        "        document_metadata: Optional[Dict[str, Dict[str, Any]]] = None,",
        "        embeddings: Optional[Dict[str, List[float]]] = None,",
        "        semantic_relations: Optional[List[Tuple]] = None,",
        "        stale_computations: Optional[Set[str]] = None,",
        "        force: bool = False,",
        "        verbose: bool = True",
        "    ) -> Dict[str, bool]:",
        "        \"\"\"",
        "        Save all processor state.",
        "",
        "        Args:",
        "            layers: Dictionary of all layers",
        "            documents: Document collection",
        "            document_metadata: Per-document metadata",
        "            embeddings: Graph embeddings",
        "            semantic_relations: Extracted relations",
        "            stale_computations: Set of stale computation names",
        "            force: Force save even if unchanged",
        "            verbose: Log progress",
        "",
        "        Returns:",
        "            Dictionary of component -> was_written",
        "        \"\"\"",
        "        results = {}",
        "",
        "        # Save layers",
        "        for layer_enum, layer in layers.items():",
        "            key = f\"layer_{layer_enum.value}\"",
        "            results[key] = self.save_layer(layer, force=force)",
        "            if verbose and results[key]:",
        "                logger.info(f\"  Saved {LAYER_FILENAMES[layer_enum.value]}: {len(layer.minicolumns)} minicolumns\")",
        "",
        "        # Save documents",
        "        results['documents'] = self.save_documents(documents, document_metadata, force=force)",
        "        if verbose and results['documents']:",
        "            logger.info(f\"  Saved documents.json: {len(documents)} documents\")",
        "",
        "        # Save computed values",
        "        if semantic_relations is not None:",
        "            results['semantic_relations'] = self.save_semantic_relations(semantic_relations, force=force)",
        "            if verbose and results['semantic_relations']:",
        "                logger.info(f\"  Saved semantic_relations.json: {len(semantic_relations)} relations\")",
        "",
        "        if embeddings is not None:",
        "            results['embeddings'] = self.save_embeddings(embeddings, force=force)",
        "            if verbose and results['embeddings']:",
        "                logger.info(f\"  Saved embeddings.json: {len(embeddings)} embeddings\")",
        "",
        "        # Update staleness tracking",
        "        if stale_computations is not None:",
        "            self.manifest.stale_computations = list(stale_computations)",
        "",
        "        # Save manifest",
        "        self.save_manifest()",
        "",
        "        if verbose:",
        "            saved_count = sum(1 for v in results.values() if v)",
        "            logger.info(f\"✓ Saved state to {self.state_dir} ({saved_count} files updated)\")",
        "",
        "        return results",
        "",
        "",
        "class StateLoader:",
        "    \"\"\"",
        "    Loads processor state from git-friendly JSON files.",
        "",
        "    Features:",
        "    - Validates checksums before loading",
        "    - Reports missing or corrupted components",
        "    - Provides incremental loading (load only what you need)",
        "",
        "    Usage:",
        "        loader = StateLoader('corpus_state')",
        "",
        "        # Load everything",
        "        state = loader.load_all()",
        "",
        "        # Or load selectively",
        "        layer0 = loader.load_layer(0)",
        "        docs = loader.load_documents()",
        "    \"\"\"",
        "",
        "    def __init__(self, state_dir: str):",
        "        \"\"\"",
        "        Initialize state loader.",
        "",
        "        Args:",
        "            state_dir: Directory containing state files",
        "        \"\"\"",
        "        self.state_dir = Path(state_dir)",
        "        self.layers_dir = self.state_dir / 'layers'",
        "        self.computed_dir = self.state_dir / 'computed'",
        "        self.manifest: Optional[StateManifest] = None",
        "",
        "    def exists(self) -> bool:",
        "        \"\"\"Check if state directory exists and has manifest.\"\"\"",
        "        return (self.state_dir / 'manifest.json').exists()",
        "",
        "    def load_manifest(self) -> StateManifest:",
        "        \"\"\"",
        "        Load the manifest file.",
        "",
        "        Returns:",
        "            StateManifest object",
        "",
        "        Raises:",
        "            FileNotFoundError: If manifest doesn't exist",
        "        \"\"\"",
        "        manifest_path = self.state_dir / 'manifest.json'",
        "        if not manifest_path.exists():",
        "            raise FileNotFoundError(f\"No manifest found at {manifest_path}\")",
        "",
        "        with open(manifest_path, 'r', encoding='utf-8') as f:",
        "            data = json.load(f)",
        "",
        "        self.manifest = StateManifest.from_dict(data)",
        "        return self.manifest",
        "",
        "    def validate_checksum(self, component: str, filepath: Path) -> bool:",
        "        \"\"\"",
        "        Validate a component's checksum.",
        "",
        "        Args:",
        "            component: Component name",
        "            filepath: Path to the component file",
        "",
        "        Returns:",
        "            True if checksum matches, False otherwise",
        "        \"\"\"",
        "        if self.manifest is None:",
        "            self.load_manifest()",
        "",
        "        expected = self.manifest.checksums.get(component)",
        "        if expected is None:",
        "            return True  # No checksum stored, assume valid",
        "",
        "        if not filepath.exists():",
        "            return False",
        "",
        "        with open(filepath, 'r', encoding='utf-8') as f:",
        "            content = f.read()",
        "",
        "        actual = hashlib.sha256(content.encode('utf-8')).hexdigest()[:16]",
        "        return actual == expected",
        "",
        "    def load_layer(self, level: int) -> HierarchicalLayer:",
        "        \"\"\"",
        "        Load a single layer.",
        "",
        "        Args:",
        "            level: Layer level (0-3)",
        "",
        "        Returns:",
        "            HierarchicalLayer object",
        "",
        "        Raises:",
        "            FileNotFoundError: If layer file doesn't exist",
        "            ValueError: If layer data is invalid",
        "        \"\"\"",
        "        filename = LAYER_FILENAMES.get(level)",
        "        if not filename:",
        "            raise ValueError(f\"Unknown layer level: {level}\")",
        "",
        "        filepath = self.layers_dir / filename",
        "        if not filepath.exists():",
        "            raise FileNotFoundError(f\"Layer file not found: {filepath}\")",
        "",
        "        with open(filepath, 'r', encoding='utf-8') as f:",
        "            data = json.load(f)",
        "",
        "        return HierarchicalLayer.from_dict(data)",
        "",
        "    def load_documents(self) -> Tuple[Dict[str, str], Dict[str, Dict[str, Any]]]:",
        "        \"\"\"",
        "        Load documents and metadata.",
        "",
        "        Returns:",
        "            Tuple of (documents, metadata)",
        "",
        "        Raises:",
        "            FileNotFoundError: If documents file doesn't exist",
        "        \"\"\"",
        "        filepath = self.state_dir / 'documents.json'",
        "        if not filepath.exists():",
        "            raise FileNotFoundError(f\"Documents file not found: {filepath}\")",
        "",
        "        with open(filepath, 'r', encoding='utf-8') as f:",
        "            data = json.load(f)",
        "",
        "        return data.get('documents', {}), data.get('metadata', {})",
        "",
        "    def load_semantic_relations(self) -> List[Tuple]:",
        "        \"\"\"",
        "        Load semantic relations.",
        "",
        "        Returns:",
        "            List of (term1, relation, term2, weight) tuples",
        "        \"\"\"",
        "        filepath = self.computed_dir / 'semantic_relations.json'",
        "        if not filepath.exists():",
        "            return []",
        "",
        "        with open(filepath, 'r', encoding='utf-8') as f:",
        "            data = json.load(f)",
        "",
        "        # Convert lists back to tuples",
        "        return [tuple(r) for r in data.get('relations', [])]",
        "",
        "    def load_embeddings(self) -> Dict[str, List[float]]:",
        "        \"\"\"",
        "        Load graph embeddings.",
        "",
        "        Returns:",
        "            Term to embedding vector mapping",
        "        \"\"\"",
        "        filepath = self.computed_dir / 'embeddings.json'",
        "        if not filepath.exists():",
        "            return {}",
        "",
        "        with open(filepath, 'r', encoding='utf-8') as f:",
        "            data = json.load(f)",
        "",
        "        return data.get('embeddings', {})",
        "",
        "    def load_all(",
        "        self,",
        "        validate: bool = True,",
        "        verbose: bool = True",
        "    ) -> Tuple[",
        "        Dict[CorticalLayer, HierarchicalLayer],",
        "        Dict[str, str],",
        "        Dict[str, Dict[str, Any]],",
        "        Dict[str, List[float]],",
        "        List[Tuple],",
        "        Dict[str, Any]",
        "    ]:",
        "        \"\"\"",
        "        Load all processor state.",
        "",
        "        Args:",
        "            validate: Validate checksums before loading",
        "            verbose: Log progress",
        "",
        "        Returns:",
        "            Tuple of (layers, documents, metadata, embeddings, relations, manifest_data)",
        "",
        "        Raises:",
        "            FileNotFoundError: If required files don't exist",
        "            ValueError: If checksums don't match (when validate=True)",
        "        \"\"\"",
        "        # Load manifest first",
        "        manifest = self.load_manifest()",
        "",
        "        if verbose:",
        "            logger.info(f\"Loading state from {self.state_dir}\")",
        "",
        "        # Load layers",
        "        layers = {}",
        "        for level in range(4):",
        "            try:",
        "                layer = self.load_layer(level)",
        "                layers[CorticalLayer(level)] = layer",
        "                if verbose:",
        "                    logger.info(f\"  Loaded {LAYER_FILENAMES[level]}: {len(layer.minicolumns)} minicolumns\")",
        "            except FileNotFoundError:",
        "                if verbose:",
        "                    logger.warning(f\"  Layer {level} not found, creating empty\")",
        "                layers[CorticalLayer(level)] = HierarchicalLayer(CorticalLayer(level))",
        "",
        "        # Load documents",
        "        try:",
        "            documents, metadata = self.load_documents()",
        "            if verbose:",
        "                logger.info(f\"  Loaded documents.json: {len(documents)} documents\")",
        "        except FileNotFoundError:",
        "            documents = {}",
        "            metadata = {}",
        "            if verbose:",
        "                logger.warning(\"  Documents not found, starting empty\")",
        "",
        "        # Load computed values",
        "        relations = self.load_semantic_relations()",
        "        if verbose and relations:",
        "            logger.info(f\"  Loaded semantic_relations.json: {len(relations)} relations\")",
        "",
        "        embeddings = self.load_embeddings()",
        "        if verbose and embeddings:",
        "            logger.info(f\"  Loaded embeddings.json: {len(embeddings)} embeddings\")",
        "",
        "        if verbose:",
        "            logger.info(f\"✓ Loaded state from {self.state_dir}\")",
        "",
        "        # Build metadata dict similar to pkl format",
        "        manifest_data = {",
        "            'version': manifest.version,",
        "            'stale_computations': set(manifest.stale_computations)",
        "        }",
        "",
        "        return layers, documents, metadata, embeddings, relations, manifest_data",
        "",
        "    def get_stats(self) -> Dict[str, Any]:",
        "        \"\"\"",
        "        Get statistics about stored state without loading everything.",
        "",
        "        Returns:",
        "            Dictionary of statistics",
        "        \"\"\"",
        "        if self.manifest is None:",
        "            try:",
        "                self.load_manifest()",
        "            except FileNotFoundError:",
        "                return {'exists': False}",
        "",
        "        return {",
        "            'exists': True,",
        "            'version': self.manifest.version,",
        "            'created_at': self.manifest.created_at,",
        "            'updated_at': self.manifest.updated_at,",
        "            'document_count': self.manifest.document_count,",
        "            'layer_stats': self.manifest.layer_stats,",
        "            'stale_computations': self.manifest.stale_computations,",
        "            'components': list(self.manifest.checksums.keys())",
        "        }",
        "",
        "",
        "def migrate_pkl_to_json(",
        "    pkl_path: str,",
        "    json_dir: str,",
        "    verbose: bool = True",
        ") -> bool:",
        "    \"\"\"",
        "    Migrate a pickle file to git-friendly JSON format.",
        "",
        "    Args:",
        "        pkl_path: Path to existing .pkl file",
        "        json_dir: Directory to write JSON state",
        "        verbose: Log progress",
        "",
        "    Returns:",
        "        True if migration successful",
        "",
        "    Raises:",
        "        FileNotFoundError: If pkl file doesn't exist",
        "    \"\"\"",
        "    from .persistence import load_processor",
        "",
        "    if verbose:",
        "        logger.info(f\"Migrating {pkl_path} to {json_dir}\")",
        "",
        "    # Load from pkl",
        "    layers, documents, metadata, embeddings, relations, pkl_metadata = load_processor(",
        "        pkl_path, verbose=False",
        "    )",
        "",
        "    # Get stale computations from pkl metadata if present",
        "    stale = pkl_metadata.get('stale_computations', set()) if pkl_metadata else set()",
        "",
        "    # Save as JSON",
        "    writer = StateWriter(json_dir)",
        "    writer.save_all(",
        "        layers=layers,",
        "        documents=documents,",
        "        document_metadata=metadata,",
        "        embeddings=embeddings,",
        "        semantic_relations=relations,",
        "        stale_computations=stale,",
        "        force=True,",
        "        verbose=verbose",
        "    )",
        "",
        "    if verbose:",
        "        logger.info(f\"✓ Migration complete: {pkl_path} → {json_dir}\")",
        "",
        "    return True"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "tasks/2025-12-14_00-21-46_8d66.json",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "{",
        "  \"version\": 1,",
        "  \"session_id\": \"8d66\",",
        "  \"started_at\": \"2025-12-14T00:21:46.447704\",",
        "  \"saved_at\": \"2025-12-14T00:21:46.447937\",",
        "  \"tasks\": [",
        "    {",
        "      \"id\": \"T-20251214-002146-8d66-001\",",
        "      \"title\": \"Add session context generator for agent handoff\",",
        "      \"status\": \"pending\",",
        "      \"priority\": \"high\",",
        "      \"category\": \"agent-dx\",",
        "      \"description\": \"When a new agent session starts, generate a concise \\\"catch-up\\\" summary:\\n- What work was done in the last N sessions\\n- Current pending tasks sorted by priority\\n- Recent file changes with semantic summaries\\n- Key decisions made (from commit messages)\\n\\nThis reduces the 'cold start' problem where agents spend time re-understanding context.\\n\\nImplementation:\\n- scripts/session_context.py with SessionContextGenerator class\\n- Read from tasks/*.json for pending/completed work\\n- Use git log for recent changes\\n- Output: markdown summary suitable for agent consumption\",",
        "      \"depends_on\": [],",
        "      \"effort\": \"medium\",",
        "      \"created_at\": \"2025-12-14T00:21:46.447746\",",
        "      \"updated_at\": null,",
        "      \"completed_at\": null,",
        "      \"context\": {",
        "        \"files\": [",
        "          \"scripts/task_utils.py\",",
        "          \"scripts/consolidate_tasks.py\"",
        "        ],",
        "        \"patterns\": [",
        "          \"chunk_index.py for similar git-friendly storage\"",
        "        ]",
        "      }",
        "    },",
        "    {",
        "      \"id\": \"T-20251214-002146-8d66-002\",",
        "      \"title\": \"Implement state_storage.py (Phase 1 of Task #206)\",",
        "      \"status\": \"pending\",",
        "      \"priority\": \"high\",",
        "      \"category\": \"arch\",",
        "      \"description\": \"Create cortical/state_storage.py with StateWriter and StateLoader classes.\\nThis is Phase 1 of Task #206 (git-friendly pkl replacement).\\n\\nKey classes:\\n- StateWriter: serialize layers, connections, computed values to JSON\\n- StateLoader: load state with hash validation  \\n- StateManifest: track versions, checksums, staleness\\n\\nDesign follows chunk_index.py pattern:\\n- Append-only where possible\\n- Content hashing for change detection\\n- Split large state into multiple files\\n\\nFiles to create:\\n- cortical/state_storage.py (main module)\\n- tests/unit/test_state_storage.py (unit tests)\\n\\nLeverage existing:\\n- Minicolumn.to_dict()/from_dict()\\n- HierarchicalLayer.to_dict()/from_dict()\\n- ChunkWriter/ChunkLoader patterns\",",
        "      \"depends_on\": [],",
        "      \"effort\": \"large\",",
        "      \"created_at\": \"2025-12-14T00:21:46.447756\",",
        "      \"updated_at\": null,",
        "      \"completed_at\": null,",
        "      \"context\": {",
        "        \"files\": [",
        "          \"cortical/persistence.py\",",
        "          \"cortical/chunk_index.py\",",
        "          \"cortical/minicolumn.py\",",
        "          \"cortical/layers.py\"",
        "        ],",
        "        \"methods\": [",
        "          \"save_processor()\",",
        "          \"load_processor()\",",
        "          \"ChunkWriter\",",
        "          \"ChunkLoader\"",
        "        ]",
        "      }",
        "    },",
        "    {",
        "      \"id\": \"T-20251214-002146-8d66-003\",",
        "      \"title\": \"Add progress checkpointing for compute_all()\",",
        "      \"status\": \"pending\",",
        "      \"priority\": \"medium\",",
        "      \"category\": \"agent-dx\",",
        "      \"description\": \"When compute_all() runs on large corpora, it can take 10+ minutes.\\nIf the agent session times out, all work is lost.\\n\\nSolution: Add checkpointing after each major phase:\\n1. After TF-IDF computation - save partial state\\n2. After bigram connections - save partial state  \\n3. After PageRank - save partial state\\n4. After concepts - save partial state\\n5. After semantics - final save\\n\\nBenefits:\\n- Resume from checkpoint if session times out\\n- Progress visibility (which phase are we on)\\n- Partial results usable even if later phases fail\\n\\nImplementation:\\n- Add checkpoint_dir parameter to compute_all()\\n- Save phase completions as JSON markers\\n- Add resume_from_checkpoint() method\",",
        "      \"depends_on\": [],",
        "      \"effort\": \"medium\",",
        "      \"created_at\": \"2025-12-14T00:21:46.447764\",",
        "      \"updated_at\": null,",
        "      \"completed_at\": null,",
        "      \"context\": {",
        "        \"files\": [",
        "          \"cortical/processor.py\"",
        "        ],",
        "        \"methods\": [",
        "          \"compute_all()\",",
        "          \"compute_tfidf()\",",
        "          \"compute_bigram_connections()\"",
        "        ]",
        "      }",
        "    },",
        "    {",
        "      \"id\": \"T-20251214-002146-8d66-004\",",
        "      \"title\": \"Auto-suggest tasks from code changes\",",
        "      \"status\": \"pending\",",
        "      \"priority\": \"low\",",
        "      \"category\": \"agent-dx\",",
        "      \"description\": \"When code is modified, automatically suggest follow-up tasks:\\n- Tests added? Suggest running test suite\\n- New public method? Suggest adding docstring\\n- Performance-sensitive code changed? Suggest profiling\\n- Validation logic changed? Suggest checking all related tests\\n\\nUses the semantic understanding from Cortical to identify patterns.\\n\\nImplementation:\\n- scripts/suggest_tasks.py\\n- Hook into git pre-commit or post-commit\\n- Output task suggestions in merge-friendly format\",",
        "      \"depends_on\": [],",
        "      \"effort\": \"medium\",",
        "      \"created_at\": \"2025-12-14T00:21:46.447781\",",
        "      \"updated_at\": null,",
        "      \"completed_at\": null,",
        "      \"context\": {",
        "        \"files\": [",
        "          \"scripts/task_utils.py\",",
        "          \"cortical/semantics.py\"",
        "        ],",
        "        \"patterns\": [",
        "          \"Could use search_codebase.py patterns\"",
        "        ]",
        "      }",
        "    }",
        "  ]",
        "}"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "tests/unit/test_state_storage.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "\"\"\"",
        "Unit tests for cortical/state_storage.py",
        "",
        "Tests the git-friendly JSON state storage system that replaces pickle-based persistence.",
        "\"\"\"",
        "",
        "import json",
        "import os",
        "import tempfile",
        "import shutil",
        "import unittest",
        "from pathlib import Path",
        "",
        "from cortical.state_storage import (",
        "    StateManifest,",
        "    StateWriter,",
        "    StateLoader,",
        "    migrate_pkl_to_json,",
        "    STATE_VERSION,",
        "    LAYER_FILENAMES,",
        ")",
        "from cortical.layers import CorticalLayer, HierarchicalLayer",
        "from cortical.minicolumn import Minicolumn, Edge",
        "",
        "",
        "class TestStateManifest(unittest.TestCase):",
        "    \"\"\"Tests for StateManifest dataclass.\"\"\"",
        "",
        "    def test_default_values(self):",
        "        \"\"\"Test manifest creates with sensible defaults.\"\"\"",
        "        manifest = StateManifest()",
        "        self.assertEqual(manifest.version, STATE_VERSION)",
        "        self.assertIsNotNone(manifest.created_at)",
        "        self.assertIsNotNone(manifest.updated_at)",
        "        self.assertEqual(manifest.checksums, {})",
        "        self.assertEqual(manifest.stale_computations, [])",
        "        self.assertEqual(manifest.document_count, 0)",
        "",
        "    def test_to_dict_from_dict_roundtrip(self):",
        "        \"\"\"Test manifest serialization roundtrip.\"\"\"",
        "        manifest = StateManifest(",
        "            version=1,",
        "            checksums={'layer_0': 'abc123'},",
        "            stale_computations=['pagerank', 'tfidf'],",
        "            document_count=42",
        "        )",
        "        data = manifest.to_dict()",
        "        restored = StateManifest.from_dict(data)",
        "",
        "        self.assertEqual(restored.version, manifest.version)",
        "        self.assertEqual(restored.checksums, manifest.checksums)",
        "        self.assertEqual(restored.stale_computations, manifest.stale_computations)",
        "        self.assertEqual(restored.document_count, manifest.document_count)",
        "",
        "    def test_update_checksum_detects_change(self):",
        "        \"\"\"Test checksum update returns True for new/changed content.\"\"\"",
        "        manifest = StateManifest()",
        "",
        "        # First update should return True (new)",
        "        changed = manifest.update_checksum('test', 'content1')",
        "        self.assertTrue(changed)",
        "",
        "        # Same content should return False",
        "        changed = manifest.update_checksum('test', 'content1')",
        "        self.assertFalse(changed)",
        "",
        "        # Different content should return True",
        "        changed = manifest.update_checksum('test', 'content2')",
        "        self.assertTrue(changed)",
        "",
        "    def test_update_checksum_updates_timestamp(self):",
        "        \"\"\"Test that updating checksum updates the timestamp.\"\"\"",
        "        manifest = StateManifest()",
        "        original_time = manifest.updated_at",
        "",
        "        # Small delay to ensure different timestamp",
        "        import time",
        "        time.sleep(0.01)",
        "",
        "        manifest.update_checksum('test', 'content')",
        "        self.assertNotEqual(manifest.updated_at, original_time)",
        "",
        "",
        "class TestStateWriter(unittest.TestCase):",
        "    \"\"\"Tests for StateWriter class.\"\"\"",
        "",
        "    def setUp(self):",
        "        \"\"\"Create a temporary directory for each test.\"\"\"",
        "        self.temp_dir = tempfile.mkdtemp()",
        "        self.state_dir = os.path.join(self.temp_dir, 'corpus_state')",
        "",
        "    def tearDown(self):",
        "        \"\"\"Clean up temporary directory.\"\"\"",
        "        shutil.rmtree(self.temp_dir)",
        "",
        "    def _create_test_layer(self, level: int, num_cols: int = 3) -> HierarchicalLayer:",
        "        \"\"\"Create a test layer with sample minicolumns.\"\"\"",
        "        layer = HierarchicalLayer(CorticalLayer(level))",
        "        for i in range(num_cols):",
        "            col = Minicolumn(f\"L{level}_term{i}\", f\"term{i}\", level)",
        "            col.occurrence_count = i + 1",
        "            col.tfidf = 0.5 + i * 0.1",
        "            col.pagerank = 0.1 + i * 0.05",
        "            layer.minicolumns[f\"term{i}\"] = col",
        "            layer._id_index[col.id] = f\"term{i}\"",
        "        return layer",
        "",
        "    def test_creates_directory_structure(self):",
        "        \"\"\"Test that writer creates required directories.\"\"\"",
        "        writer = StateWriter(self.state_dir)",
        "        layer = self._create_test_layer(0)",
        "        writer.save_layer(layer)",
        "",
        "        self.assertTrue(os.path.exists(self.state_dir))",
        "        self.assertTrue(os.path.exists(os.path.join(self.state_dir, 'layers')))",
        "        self.assertTrue(os.path.exists(os.path.join(self.state_dir, 'computed')))",
        "",
        "    def test_save_layer(self):",
        "        \"\"\"Test saving a single layer.\"\"\"",
        "        writer = StateWriter(self.state_dir)",
        "        layer = self._create_test_layer(0, num_cols=5)",
        "",
        "        result = writer.save_layer(layer)",
        "",
        "        self.assertTrue(result)  # File was written",
        "        filepath = os.path.join(self.state_dir, 'layers', 'L0_tokens.json')",
        "        self.assertTrue(os.path.exists(filepath))",
        "",
        "        # Verify content",
        "        with open(filepath) as f:",
        "            data = json.load(f)",
        "        self.assertEqual(data['level'], 0)",
        "        self.assertEqual(len(data['minicolumns']), 5)",
        "",
        "    def test_save_layer_skips_unchanged(self):",
        "        \"\"\"Test that saving same layer twice skips second write.\"\"\"",
        "        writer = StateWriter(self.state_dir)",
        "        layer = self._create_test_layer(0)",
        "",
        "        first_result = writer.save_layer(layer)",
        "        second_result = writer.save_layer(layer)",
        "",
        "        self.assertTrue(first_result)",
        "        self.assertFalse(second_result)  # Skipped, unchanged",
        "",
        "    def test_save_layer_force_writes_unchanged(self):",
        "        \"\"\"Test that force=True writes even if unchanged.\"\"\"",
        "        writer = StateWriter(self.state_dir)",
        "        layer = self._create_test_layer(0)",
        "",
        "        writer.save_layer(layer)",
        "        result = writer.save_layer(layer, force=True)",
        "",
        "        self.assertTrue(result)  # Force write",
        "",
        "    def test_save_documents(self):",
        "        \"\"\"Test saving documents and metadata.\"\"\"",
        "        writer = StateWriter(self.state_dir)",
        "        documents = {'doc1': 'Content one', 'doc2': 'Content two'}",
        "        metadata = {'doc1': {'source': 'test'}}",
        "",
        "        result = writer.save_documents(documents, metadata)",
        "",
        "        self.assertTrue(result)",
        "        filepath = os.path.join(self.state_dir, 'documents.json')",
        "        self.assertTrue(os.path.exists(filepath))",
        "",
        "        with open(filepath) as f:",
        "            data = json.load(f)",
        "        self.assertEqual(data['documents'], documents)",
        "        self.assertEqual(data['metadata'], metadata)",
        "",
        "    def test_save_semantic_relations(self):",
        "        \"\"\"Test saving semantic relations.\"\"\"",
        "        writer = StateWriter(self.state_dir)",
        "        relations = [",
        "            ('neural', 'RelatedTo', 'network', 0.8),",
        "            ('machine', 'PartOf', 'learning', 0.9),",
        "        ]",
        "",
        "        result = writer.save_semantic_relations(relations)",
        "",
        "        self.assertTrue(result)",
        "        filepath = os.path.join(self.state_dir, 'computed', 'semantic_relations.json')",
        "        self.assertTrue(os.path.exists(filepath))",
        "",
        "        with open(filepath) as f:",
        "            data = json.load(f)",
        "        self.assertEqual(len(data['relations']), 2)",
        "        self.assertEqual(data['count'], 2)",
        "",
        "    def test_save_embeddings(self):",
        "        \"\"\"Test saving embeddings.\"\"\"",
        "        writer = StateWriter(self.state_dir)",
        "        embeddings = {",
        "            'neural': [0.1, 0.2, 0.3],",
        "            'network': [0.4, 0.5, 0.6],",
        "        }",
        "",
        "        result = writer.save_embeddings(embeddings)",
        "",
        "        self.assertTrue(result)",
        "        filepath = os.path.join(self.state_dir, 'computed', 'embeddings.json')",
        "        self.assertTrue(os.path.exists(filepath))",
        "",
        "        with open(filepath) as f:",
        "            data = json.load(f)",
        "        self.assertEqual(data['embeddings'], embeddings)",
        "        self.assertEqual(data['dimensions'], 3)",
        "        self.assertEqual(data['count'], 2)",
        "",
        "    def test_save_all(self):",
        "        \"\"\"Test saving complete processor state.\"\"\"",
        "        writer = StateWriter(self.state_dir)",
        "",
        "        layers = {",
        "            CorticalLayer(i): self._create_test_layer(i)",
        "            for i in range(4)",
        "        }",
        "        documents = {'doc1': 'Test content'}",
        "        metadata = {'doc1': {'source': 'unit_test'}}",
        "        embeddings = {'term': [0.1, 0.2]}",
        "        relations = [('a', 'rel', 'b', 0.5)]",
        "",
        "        results = writer.save_all(",
        "            layers=layers,",
        "            documents=documents,",
        "            document_metadata=metadata,",
        "            embeddings=embeddings,",
        "            semantic_relations=relations,",
        "            stale_computations={'pagerank'},",
        "            verbose=False",
        "        )",
        "",
        "        # All components should be written",
        "        self.assertTrue(results['layer_0'])",
        "        self.assertTrue(results['layer_1'])",
        "        self.assertTrue(results['layer_2'])",
        "        self.assertTrue(results['layer_3'])",
        "        self.assertTrue(results['documents'])",
        "        self.assertTrue(results['embeddings'])",
        "        self.assertTrue(results['semantic_relations'])",
        "",
        "        # Manifest should exist",
        "        manifest_path = os.path.join(self.state_dir, 'manifest.json')",
        "        self.assertTrue(os.path.exists(manifest_path))",
        "",
        "    def test_atomic_write_creates_valid_file(self):",
        "        \"\"\"Test atomic write produces valid JSON.\"\"\"",
        "        writer = StateWriter(self.state_dir)",
        "        layer = self._create_test_layer(0)",
        "        writer.save_layer(layer)",
        "",
        "        filepath = os.path.join(self.state_dir, 'layers', 'L0_tokens.json')",
        "",
        "        # Should be valid JSON",
        "        with open(filepath) as f:",
        "            data = json.load(f)",
        "        self.assertIsInstance(data, dict)",
        "",
        "        # No temp files should remain",
        "        temp_path = filepath + '.tmp'",
        "        self.assertFalse(os.path.exists(temp_path))",
        "",
        "",
        "class TestStateLoader(unittest.TestCase):",
        "    \"\"\"Tests for StateLoader class.\"\"\"",
        "",
        "    def setUp(self):",
        "        \"\"\"Create temp directory and write test state.\"\"\"",
        "        self.temp_dir = tempfile.mkdtemp()",
        "        self.state_dir = os.path.join(self.temp_dir, 'corpus_state')",
        "        self._write_test_state()",
        "",
        "    def tearDown(self):",
        "        \"\"\"Clean up temporary directory.\"\"\"",
        "        shutil.rmtree(self.temp_dir)",
        "",
        "    def _create_test_layer(self, level: int, num_cols: int = 3) -> HierarchicalLayer:",
        "        \"\"\"Create a test layer with sample minicolumns.\"\"\"",
        "        layer = HierarchicalLayer(CorticalLayer(level))",
        "        for i in range(num_cols):",
        "            col = Minicolumn(f\"L{level}_term{i}\", f\"term{i}\", level)",
        "            col.occurrence_count = i + 1",
        "            col.tfidf = 0.5 + i * 0.1",
        "            col.pagerank = 0.1 + i * 0.05",
        "            layer.minicolumns[f\"term{i}\"] = col",
        "            layer._id_index[col.id] = f\"term{i}\"",
        "        return layer",
        "",
        "    def _write_test_state(self):",
        "        \"\"\"Write test state files.\"\"\"",
        "        writer = StateWriter(self.state_dir)",
        "        layers = {",
        "            CorticalLayer(i): self._create_test_layer(i)",
        "            for i in range(4)",
        "        }",
        "        documents = {'doc1': 'Test content one', 'doc2': 'Test content two'}",
        "        metadata = {'doc1': {'source': 'test'}}",
        "        embeddings = {'term0': [0.1, 0.2, 0.3]}",
        "        relations = [('neural', 'RelatedTo', 'network', 0.8)]",
        "",
        "        writer.save_all(",
        "            layers=layers,",
        "            documents=documents,",
        "            document_metadata=metadata,",
        "            embeddings=embeddings,",
        "            semantic_relations=relations,",
        "            stale_computations={'pagerank'},",
        "            verbose=False",
        "        )",
        "",
        "    def test_exists(self):",
        "        \"\"\"Test exists() returns True for valid state.\"\"\"",
        "        loader = StateLoader(self.state_dir)",
        "        self.assertTrue(loader.exists())",
        "",
        "    def test_exists_false_for_missing(self):",
        "        \"\"\"Test exists() returns False for missing state.\"\"\"",
        "        loader = StateLoader('/nonexistent/path')",
        "        self.assertFalse(loader.exists())",
        "",
        "    def test_load_manifest(self):",
        "        \"\"\"Test loading manifest file.\"\"\"",
        "        loader = StateLoader(self.state_dir)",
        "        manifest = loader.load_manifest()",
        "",
        "        self.assertEqual(manifest.version, STATE_VERSION)",
        "        self.assertEqual(manifest.document_count, 2)",
        "        self.assertIn('pagerank', manifest.stale_computations)",
        "",
        "    def test_load_layer(self):",
        "        \"\"\"Test loading a single layer.\"\"\"",
        "        loader = StateLoader(self.state_dir)",
        "        layer = loader.load_layer(0)",
        "",
        "        self.assertEqual(layer.level, 0)",
        "        self.assertEqual(len(layer.minicolumns), 3)",
        "        self.assertIn('term0', layer.minicolumns)",
        "",
        "    def test_load_layer_invalid_level(self):",
        "        \"\"\"Test loading invalid layer level raises error.\"\"\"",
        "        loader = StateLoader(self.state_dir)",
        "",
        "        with self.assertRaises(ValueError):",
        "            loader.load_layer(5)",
        "",
        "    def test_load_documents(self):",
        "        \"\"\"Test loading documents and metadata.\"\"\"",
        "        loader = StateLoader(self.state_dir)",
        "        documents, metadata = loader.load_documents()",
        "",
        "        self.assertEqual(len(documents), 2)",
        "        self.assertIn('doc1', documents)",
        "        self.assertEqual(metadata['doc1']['source'], 'test')",
        "",
        "    def test_load_semantic_relations(self):",
        "        \"\"\"Test loading semantic relations.\"\"\"",
        "        loader = StateLoader(self.state_dir)",
        "        relations = loader.load_semantic_relations()",
        "",
        "        self.assertEqual(len(relations), 1)",
        "        self.assertEqual(relations[0], ('neural', 'RelatedTo', 'network', 0.8))",
        "",
        "    def test_load_semantic_relations_empty(self):",
        "        \"\"\"Test loading when no relations file exists.\"\"\"",
        "        # Remove the relations file",
        "        relations_path = os.path.join(self.state_dir, 'computed', 'semantic_relations.json')",
        "        os.remove(relations_path)",
        "",
        "        loader = StateLoader(self.state_dir)",
        "        relations = loader.load_semantic_relations()",
        "",
        "        self.assertEqual(relations, [])",
        "",
        "    def test_load_embeddings(self):",
        "        \"\"\"Test loading embeddings.\"\"\"",
        "        loader = StateLoader(self.state_dir)",
        "        embeddings = loader.load_embeddings()",
        "",
        "        self.assertIn('term0', embeddings)",
        "        self.assertEqual(embeddings['term0'], [0.1, 0.2, 0.3])",
        "",
        "    def test_load_all(self):",
        "        \"\"\"Test loading complete state.\"\"\"",
        "        loader = StateLoader(self.state_dir)",
        "        layers, documents, metadata, embeddings, relations, manifest_data = loader.load_all(verbose=False)",
        "",
        "        # Check layers",
        "        self.assertEqual(len(layers), 4)",
        "        for level in range(4):",
        "            self.assertIn(CorticalLayer(level), layers)",
        "            self.assertEqual(len(layers[CorticalLayer(level)].minicolumns), 3)",
        "",
        "        # Check documents",
        "        self.assertEqual(len(documents), 2)",
        "        self.assertIn('doc1', documents)",
        "",
        "        # Check computed values",
        "        self.assertEqual(len(relations), 1)",
        "        self.assertIn('term0', embeddings)",
        "",
        "        # Check manifest data",
        "        self.assertEqual(manifest_data['version'], STATE_VERSION)",
        "        self.assertIn('pagerank', manifest_data['stale_computations'])",
        "",
        "    def test_get_stats(self):",
        "        \"\"\"Test getting state statistics.\"\"\"",
        "        loader = StateLoader(self.state_dir)",
        "        stats = loader.get_stats()",
        "",
        "        self.assertTrue(stats['exists'])",
        "        self.assertEqual(stats['version'], STATE_VERSION)",
        "        self.assertEqual(stats['document_count'], 2)",
        "        self.assertIn('layer_0', stats['components'])",
        "",
        "    def test_get_stats_missing_state(self):",
        "        \"\"\"Test getting stats for missing state.\"\"\"",
        "        loader = StateLoader('/nonexistent')",
        "        stats = loader.get_stats()",
        "",
        "        self.assertFalse(stats['exists'])",
        "",
        "",
        "class TestStateRoundtrip(unittest.TestCase):",
        "    \"\"\"Tests for complete save/load roundtrip.\"\"\"",
        "",
        "    def setUp(self):",
        "        \"\"\"Create temp directory.\"\"\"",
        "        self.temp_dir = tempfile.mkdtemp()",
        "        self.state_dir = os.path.join(self.temp_dir, 'corpus_state')",
        "",
        "    def tearDown(self):",
        "        \"\"\"Clean up.\"\"\"",
        "        shutil.rmtree(self.temp_dir)",
        "",
        "    def test_full_roundtrip(self):",
        "        \"\"\"Test complete save and load cycle preserves data.\"\"\"",
        "        # Create test data",
        "        layers = {}",
        "        for level in range(4):",
        "            layer = HierarchicalLayer(CorticalLayer(level))",
        "            for i in range(5):",
        "                col = Minicolumn(f\"L{level}_term{i}\", f\"term{i}\", level)",
        "                col.occurrence_count = i * 10",
        "                col.tfidf = 0.1 * (i + 1)",
        "                col.pagerank = 0.05 * (i + 1)",
        "                col.document_ids = {f\"doc{j}\" for j in range(i + 1)}",
        "                layer.minicolumns[f\"term{i}\"] = col",
        "                layer._id_index[col.id] = f\"term{i}\"",
        "            layers[CorticalLayer(level)] = layer",
        "",
        "        documents = {f\"doc{i}\": f\"Content for document {i}\" for i in range(10)}",
        "        metadata = {f\"doc{i}\": {'index': i, 'source': 'test'} for i in range(10)}",
        "        embeddings = {f\"term{i}\": [0.1 * i, 0.2 * i, 0.3 * i] for i in range(5)}",
        "        relations = [",
        "            ('term0', 'RelatedTo', 'term1', 0.8),",
        "            ('term1', 'PartOf', 'term2', 0.6),",
        "        ]",
        "",
        "        # Save",
        "        writer = StateWriter(self.state_dir)",
        "        writer.save_all(",
        "            layers=layers,",
        "            documents=documents,",
        "            document_metadata=metadata,",
        "            embeddings=embeddings,",
        "            semantic_relations=relations,",
        "            stale_computations={'pagerank', 'concepts'},",
        "            verbose=False",
        "        )",
        "",
        "        # Load",
        "        loader = StateLoader(self.state_dir)",
        "        (loaded_layers, loaded_docs, loaded_meta,",
        "         loaded_embed, loaded_rels, manifest_data) = loader.load_all(verbose=False)",
        "",
        "        # Verify layers",
        "        self.assertEqual(len(loaded_layers), 4)",
        "        for level in range(4):",
        "            original = layers[CorticalLayer(level)]",
        "            loaded = loaded_layers[CorticalLayer(level)]",
        "            self.assertEqual(len(loaded.minicolumns), len(original.minicolumns))",
        "",
        "            for content, orig_col in original.minicolumns.items():",
        "                loaded_col = loaded.minicolumns[content]",
        "                self.assertEqual(loaded_col.id, orig_col.id)",
        "                self.assertEqual(loaded_col.occurrence_count, orig_col.occurrence_count)",
        "                self.assertAlmostEqual(loaded_col.tfidf, orig_col.tfidf, places=5)",
        "                self.assertAlmostEqual(loaded_col.pagerank, orig_col.pagerank, places=5)",
        "",
        "        # Verify documents",
        "        self.assertEqual(loaded_docs, documents)",
        "        self.assertEqual(loaded_meta, metadata)",
        "",
        "        # Verify computed values",
        "        self.assertEqual(loaded_embed, embeddings)",
        "        self.assertEqual(loaded_rels, relations)",
        "",
        "        # Verify staleness",
        "        self.assertEqual(manifest_data['stale_computations'], {'pagerank', 'concepts'})",
        "",
        "",
        "class TestMigration(unittest.TestCase):",
        "    \"\"\"Tests for pkl to JSON migration.\"\"\"",
        "",
        "    def setUp(self):",
        "        \"\"\"Create temp directory.\"\"\"",
        "        self.temp_dir = tempfile.mkdtemp()",
        "",
        "    def tearDown(self):",
        "        \"\"\"Clean up.\"\"\"",
        "        shutil.rmtree(self.temp_dir)",
        "",
        "    def test_migrate_requires_pkl_file(self):",
        "        \"\"\"Test migration fails for missing pkl file.\"\"\"",
        "        with self.assertRaises(FileNotFoundError):",
        "            migrate_pkl_to_json(",
        "                '/nonexistent/file.pkl',",
        "                os.path.join(self.temp_dir, 'output'),",
        "                verbose=False",
        "            )",
        "",
        "",
        "class TestIncrementalSave(unittest.TestCase):",
        "    \"\"\"Tests for incremental saving behavior.\"\"\"",
        "",
        "    def setUp(self):",
        "        \"\"\"Create temp directory.\"\"\"",
        "        self.temp_dir = tempfile.mkdtemp()",
        "        self.state_dir = os.path.join(self.temp_dir, 'corpus_state')",
        "",
        "    def tearDown(self):",
        "        \"\"\"Clean up.\"\"\"",
        "        shutil.rmtree(self.temp_dir)",
        "",
        "    def test_only_changed_layers_written(self):",
        "        \"\"\"Test that only modified layers are written on second save.\"\"\"",
        "        # Initial save",
        "        writer = StateWriter(self.state_dir)",
        "",
        "        layer0 = HierarchicalLayer(CorticalLayer.TOKENS)",
        "        col0 = Minicolumn(\"L0_test\", \"test\", 0)",
        "        layer0.minicolumns[\"test\"] = col0",
        "        layer0._id_index[col0.id] = \"test\"",
        "",
        "        layer1 = HierarchicalLayer(CorticalLayer.BIGRAMS)",
        "        col1 = Minicolumn(\"L1_test bigram\", \"test bigram\", 1)",
        "        layer1.minicolumns[\"test bigram\"] = col1",
        "        layer1._id_index[col1.id] = \"test bigram\"",
        "",
        "        layers = {",
        "            CorticalLayer.TOKENS: layer0,",
        "            CorticalLayer.BIGRAMS: layer1,",
        "        }",
        "",
        "        writer.save_layer(layer0)",
        "        writer.save_layer(layer1)",
        "",
        "        # Get initial modification times",
        "        l0_path = os.path.join(self.state_dir, 'layers', 'L0_tokens.json')",
        "        l1_path = os.path.join(self.state_dir, 'layers', 'L1_bigrams.json')",
        "",
        "        # Modify only layer0",
        "        col_new = Minicolumn(\"L0_new\", \"new\", 0)",
        "        layer0.minicolumns[\"new\"] = col_new",
        "        layer0._id_index[col_new.id] = \"new\"",
        "",
        "        # Save both again",
        "        result0 = writer.save_layer(layer0)",
        "        result1 = writer.save_layer(layer1)",
        "",
        "        # Only layer0 should be written",
        "        self.assertTrue(result0)",
        "        self.assertFalse(result1)",
        "",
        "",
        "if __name__ == '__main__':",
        "    unittest.main()"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    }
  ],
  "hour_of_day": 0,
  "day_of_week": "Sunday",
  "seconds_since_last_commit": -134253,
  "is_merge": false,
  "is_initial": false,
  "parent_count": 1,
  "session_id": null,
  "related_chats": [],
  "ci_result": null,
  "reverted": false,
  "amended": false
}