{
  "hash": "e18850802e4764d425a9bd3f6d075c3a64e617dd",
  "message": "feat: Add privacy features to ML data collection",
  "author": "Claude",
  "timestamp": "2025-12-15 13:54:34 +0000",
  "branch": "claude/multi-index-design-DvifZ",
  "files_changed": [
    "README.md",
    "scripts/ml_data_collector.py"
  ],
  "insertions": 628,
  "deletions": 1,
  "hunks": [
    {
      "file": "README.md",
      "function": "If you prefer not to participate:",
      "start_line": 540,
      "lines_added": [
        "### Privacy & Security",
        "**Automatic Redaction:** Sensitive data is automatically scrubbed before storage:",
        "- API keys, tokens, and secrets",
        "- Passwords and credentials",
        "- Private keys (RSA, SSH, etc.)",
        "- Database connection strings",
        "- GitHub/Slack tokens, JWTs",
        "",
        "```bash",
        "# Test what gets redacted",
        "python scripts/ml_data_collector.py redact-test --text \"api_key=sk-abc123secret\"",
        "```",
        "",
        "**Data Retention:** Old data is automatically cleaned up:",
        "```bash",
        "# Preview what would be deleted (90 days default)",
        "python scripts/ml_data_collector.py cleanup --dry-run",
        "",
        "# Clean up data older than 60 days",
        "python scripts/ml_data_collector.py cleanup --days 60",
        "```",
        "",
        "**Privacy Guarantees:**",
        "### Contributing Your Data",
        "",
        "When ready to help train the shared model:",
        "",
        "```bash",
        "# Preview what would be shared (with redaction applied)",
        "python scripts/ml_data_collector.py contribute preview",
        "",
        "# Opt-in to contribute (requires explicit consent)",
        "python scripts/ml_data_collector.py contribute enable --name \"Your Name\"",
        "",
        "# Check your contribution status",
        "python scripts/ml_data_collector.py contribute status",
        "",
        "# Opt-out anytime",
        "python scripts/ml_data_collector.py contribute disable",
        "```",
        ""
      ],
      "lines_removed": [
        "### Privacy Considerations"
      ],
      "context_before": [
        "",
        "```bash",
        "# Option 1: Environment variable (per-session or in shell profile)",
        "export ML_COLLECTION_ENABLED=0",
        "",
        "# Option 2: Remove the Stop hook from .claude/settings.json",
        "```",
        "",
        "When disabled, no data is written to `.git-ml/`. You can still use all other features normally.",
        ""
      ],
      "context_after": [
        "",
        "- **Local storage only**: Data never leaves your machine automatically",
        "- **Gitignored**: `.git-ml/` is excluded from version control",
        "- **Regeneratable**: All commit data can be backfilled from git history",
        "- **Full control**: Delete `.git-ml/` anytime to remove all collected data",
        "- **Transparent**: All collection code is in `scripts/ml_data_collector.py`",
        "",
        "See [CLAUDE.md](CLAUDE.md) for detailed documentation on the ML data collection system.",
        "",
        "## Development History",
        "",
        "This project evolved through systematic improvements:",
        "",
        "1. **Initial Release**: Core hierarchical text processing",
        "2. **Code Review & Fixes**: TF-IDF calculation, O(1) lookups, type annotations",
        "3. **RAG Enhancements**: Chunk-level retrieval, metadata support, concept clustering",
        "4. **ConceptNet Integration**: Typed edges, relation-weighted PageRank, multi-hop inference"
      ],
      "change_type": "modify"
    },
    {
      "file": "scripts/ml_data_collector.py",
      "function": null,
      "start_line": 1,
      "lines_added": [
        "PRIVACY: Sensitive data (API keys, passwords, tokens) is automatically redacted",
        "before storage. See REDACTION_PATTERNS for the full list.",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "#!/usr/bin/env python3",
        "\"\"\"",
        "ML Data Collector for Project-Specific Language Model Training",
        "",
        "This module collects enriched data from git commits, chat sessions, and developer",
        "actions to train a micro-model specific to this project.",
        ""
      ],
      "context_after": [
        "Usage:",
        "    # Collect commit data (call from git hook)",
        "    python scripts/ml_data_collector.py commit",
        "",
        "    # Log a chat session",
        "    python scripts/ml_data_collector.py chat --query \"...\" --response \"...\"",
        "",
        "    # Show statistics",
        "    python scripts/ml_data_collector.py stats",
        ""
      ],
      "change_type": "add"
    },
    {
      "file": "scripts/ml_data_collector.py",
      "function": "Usage:",
      "start_line": 25,
      "lines_added": [
        "",
        "    # Clean up old data (default: 90 days retention)",
        "    python scripts/ml_data_collector.py cleanup [--days 90] [--dry-run]",
        "",
        "    # Manage contribution consent",
        "    python scripts/ml_data_collector.py contribute status     # Check consent status",
        "    python scripts/ml_data_collector.py contribute enable     # Opt-in to share data",
        "    python scripts/ml_data_collector.py contribute disable    # Opt-out of sharing",
        "    python scripts/ml_data_collector.py contribute preview    # Preview what would be shared",
        "",
        "    # Test redaction patterns",
        "    python scripts/ml_data_collector.py redact-test --text \"api_key=secret123\""
      ],
      "lines_removed": [],
      "context_before": [
        "    python scripts/ml_data_collector.py handoff",
        "",
        "    # Add feedback to a chat",
        "    python scripts/ml_data_collector.py feedback --chat-id <id> --rating good [--comment \"text\"]",
        "",
        "    # List recent chats and their feedback status",
        "    python scripts/ml_data_collector.py feedback --list [--limit 20]",
        "",
        "    # Export data for training",
        "    python scripts/ml_data_collector.py export --format jsonl --output training_data.jsonl"
      ],
      "context_after": [
        "\"\"\"",
        "",
        "import json",
        "import os",
        "import subprocess",
        "import hashlib",
        "import re",
        "import shlex",
        "import tempfile",
        "import uuid"
      ],
      "change_type": "add"
    },
    {
      "file": "scripts/ml_data_collector.py",
      "function": "CHAT_SCHEMA = {",
      "start_line": 109,
      "lines_added": [
        "# Data retention configuration (days)",
        "DEFAULT_RETENTION_DAYS = 90",
        "CONSENT_FILE = ML_DATA_DIR / \"contribution_consent.json\"",
        "",
        "# Sensitive data patterns to redact before storage",
        "# These patterns are applied to query/response text before saving",
        "REDACTION_PATTERNS = [",
        "    # API keys and tokens",
        "    (r'(?i)(api[_-]?key|apikey|api_secret|secret[_-]?key)\\s*[=:]\\s*[\"\\']?[\\w\\-]{20,}[\"\\']?', r'\\1=<REDACTED>'),",
        "    (r'(?i)(token|bearer|authorization)\\s*[=:]\\s*[\"\\']?[\\w\\-\\.]{20,}[\"\\']?', r'\\1=<REDACTED>'),",
        "    # Passwords and secrets",
        "    (r'(?i)(password|passwd|pwd|secret)\\s*[=:]\\s*[\"\\']?[^\\s\"\\']{8,}[\"\\']?', r'\\1=<REDACTED>'),",
        "    # AWS credentials",
        "    (r'(?i)(aws[_-]?access[_-]?key[_-]?id|aws[_-]?secret[_-]?access[_-]?key)\\s*[=:]\\s*[\"\\']?[\\w\\+/]{16,}[\"\\']?', r'\\1=<REDACTED>'),",
        "    (r'AKIA[0-9A-Z]{16}', '<AWS_KEY_REDACTED>'),",
        "    # Private keys",
        "    (r'-----BEGIN\\s+(RSA\\s+)?PRIVATE KEY-----[\\s\\S]*?-----END\\s+(RSA\\s+)?PRIVATE KEY-----', '<PRIVATE_KEY_REDACTED>'),",
        "    (r'-----BEGIN\\s+OPENSSH\\s+PRIVATE KEY-----[\\s\\S]*?-----END\\s+OPENSSH\\s+PRIVATE KEY-----', '<SSH_KEY_REDACTED>'),",
        "    # Database connection strings",
        "    (r'(?i)(mongodb|postgresql|mysql|redis)://[^\\s\"\\']+', r'\\1://<CONNECTION_REDACTED>'),",
        "    # Generic credentials in URLs",
        "    (r'://[^:]+:[^@]+@', '://<CREDENTIALS>@'),",
        "    # GitHub tokens",
        "    (r'ghp_[a-zA-Z0-9]{36}', '<GITHUB_TOKEN_REDACTED>'),",
        "    (r'gho_[a-zA-Z0-9]{36}', '<GITHUB_OAUTH_REDACTED>'),",
        "    # Slack tokens",
        "    (r'xox[baprs]-[0-9a-zA-Z\\-]+', '<SLACK_TOKEN_REDACTED>'),",
        "    # JWT tokens (basic pattern)",
        "    (r'eyJ[a-zA-Z0-9_-]*\\.eyJ[a-zA-Z0-9_-]*\\.[a-zA-Z0-9_-]*', '<JWT_REDACTED>'),",
        "]",
        "",
        "# ============================================================================",
        "# SENSITIVE DATA REDACTION",
        "# ============================================================================",
        "",
        "def redact_sensitive_data(text: str) -> str:",
        "    \"\"\"Redact sensitive data patterns from text before storage.",
        "",
        "    Args:",
        "        text: The text to redact",
        "",
        "    Returns:",
        "        Text with sensitive patterns replaced with redaction markers",
        "    \"\"\"",
        "    if not text:",
        "        return text",
        "",
        "    redacted = text",
        "    for pattern, replacement in REDACTION_PATTERNS:",
        "        redacted = re.sub(pattern, replacement, redacted)",
        "    return redacted",
        "",
        "",
        "def count_redactions(original: str, redacted: str) -> int:",
        "    \"\"\"Count how many redactions were made.\"\"\"",
        "    if not original or not redacted:",
        "        return 0",
        "    # Count occurrences of redaction markers",
        "    markers = ['<REDACTED>', '<AWS_KEY_REDACTED>', '<PRIVATE_KEY_REDACTED>',",
        "               '<SSH_KEY_REDACTED>', '<CONNECTION_REDACTED>', '<CREDENTIALS>',",
        "               '<GITHUB_TOKEN_REDACTED>', '<GITHUB_OAUTH_REDACTED>',",
        "               '<SLACK_TOKEN_REDACTED>', '<JWT_REDACTED>']",
        "    return sum(redacted.count(m) for m in markers)",
        "",
        "",
        "# ============================================================================",
        "# DATA RETENTION & CLEANUP",
        "# ============================================================================",
        "",
        "def cleanup_old_data(",
        "    retention_days: int = DEFAULT_RETENTION_DAYS,",
        "    dry_run: bool = False,",
        "    verbose: bool = False",
        ") -> Dict[str, Any]:",
        "    \"\"\"Remove data older than retention_days.",
        "",
        "    Args:",
        "        retention_days: Days to keep data (default: 90)",
        "        dry_run: If True, only report what would be deleted",
        "        verbose: Show detailed output",
        "",
        "    Returns:",
        "        Dict with counts of files removed/would be removed",
        "    \"\"\"",
        "    from datetime import timedelta",
        "",
        "    cutoff_date = datetime.now() - timedelta(days=retention_days)",
        "    cutoff_str = cutoff_date.strftime('%Y-%m-%d')",
        "",
        "    results = {",
        "        'retention_days': retention_days,",
        "        'cutoff_date': cutoff_str,",
        "        'dry_run': dry_run,",
        "        'commits_removed': 0,",
        "        'chats_removed': 0,",
        "        'actions_removed': 0,",
        "        'sessions_removed': 0,",
        "        'bytes_freed': 0,",
        "    }",
        "",
        "    # Cleanup commits",
        "    if COMMITS_DIR.exists():",
        "        for f in COMMITS_DIR.glob(\"*.json\"):",
        "            try:",
        "                with open(f, 'r', encoding='utf-8') as fp:",
        "                    data = json.load(fp)",
        "                timestamp = data.get('timestamp', '')[:10]  # YYYY-MM-DD",
        "                if timestamp < cutoff_str:",
        "                    file_size = f.stat().st_size",
        "                    if verbose:",
        "                        print(f\"  {'Would remove' if dry_run else 'Removing'}: {f.name} ({timestamp})\")",
        "                    if not dry_run:",
        "                        f.unlink()",
        "                    results['commits_removed'] += 1",
        "                    results['bytes_freed'] += file_size",
        "            except (json.JSONDecodeError, IOError):",
        "                continue",
        "",
        "    # Cleanup chats (organized by date directories)",
        "    if CHATS_DIR.exists():",
        "        for date_dir in CHATS_DIR.iterdir():",
        "            if date_dir.is_dir():",
        "                dir_date = date_dir.name  # YYYY-MM-DD",
        "                if dir_date < cutoff_str:",
        "                    for f in date_dir.glob(\"*.json\"):",
        "                        file_size = f.stat().st_size",
        "                        if verbose:",
        "                            print(f\"  {'Would remove' if dry_run else 'Removing'}: chats/{dir_date}/{f.name}\")",
        "                        if not dry_run:",
        "                            f.unlink()",
        "                        results['chats_removed'] += 1",
        "                        results['bytes_freed'] += file_size",
        "                    # Remove empty directory",
        "                    if not dry_run and not any(date_dir.iterdir()):",
        "                        date_dir.rmdir()",
        "",
        "    # Cleanup actions (organized by date directories)",
        "    if ACTIONS_DIR.exists():",
        "        for date_dir in ACTIONS_DIR.iterdir():",
        "            if date_dir.is_dir():",
        "                dir_date = date_dir.name",
        "                if dir_date < cutoff_str:",
        "                    for f in date_dir.glob(\"*.json\"):",
        "                        file_size = f.stat().st_size",
        "                        if verbose:",
        "                            print(f\"  {'Would remove' if dry_run else 'Removing'}: actions/{dir_date}/{f.name}\")",
        "                        if not dry_run:",
        "                            f.unlink()",
        "                        results['actions_removed'] += 1",
        "                        results['bytes_freed'] += file_size",
        "                    if not dry_run and not any(date_dir.iterdir()):",
        "                        date_dir.rmdir()",
        "",
        "    # Cleanup old sessions",
        "    if SESSIONS_DIR.exists():",
        "        for f in SESSIONS_DIR.glob(\"*.json\"):",
        "            try:",
        "                with open(f, 'r', encoding='utf-8') as fp:",
        "                    data = json.load(fp)",
        "                ended_at = data.get('ended_at', data.get('started_at', ''))[:10]",
        "                if ended_at and ended_at < cutoff_str:",
        "                    file_size = f.stat().st_size",
        "                    if verbose:",
        "                        print(f\"  {'Would remove' if dry_run else 'Removing'}: sessions/{f.name}\")",
        "                    if not dry_run:",
        "                        f.unlink()",
        "                    results['sessions_removed'] += 1",
        "                    results['bytes_freed'] += file_size",
        "            except (json.JSONDecodeError, IOError):",
        "                continue",
        "",
        "    return results",
        "",
        "",
        "# ============================================================================",
        "# CONTRIBUTION CONSENT MANAGEMENT",
        "# ============================================================================",
        "",
        "def get_contribution_consent() -> Optional[Dict[str, Any]]:",
        "    \"\"\"Get the current contribution consent status.",
        "",
        "    Returns:",
        "        Consent data dict or None if no consent recorded",
        "    \"\"\"",
        "    if not CONSENT_FILE.exists():",
        "        return None",
        "    try:",
        "        with open(CONSENT_FILE, 'r', encoding='utf-8') as f:",
        "            return json.load(f)",
        "    except (json.JSONDecodeError, IOError):",
        "        return None",
        "",
        "",
        "def set_contribution_consent(",
        "    consented: bool,",
        "    contributor_name: Optional[str] = None,",
        "    contributor_email: Optional[str] = None,",
        "    notes: Optional[str] = None",
        ") -> Dict[str, Any]:",
        "    \"\"\"Record contribution consent decision.",
        "",
        "    Args:",
        "        consented: Whether user consents to contribute data",
        "        contributor_name: Optional name for attribution",
        "        contributor_email: Optional email for contact",
        "        notes: Optional notes about the consent",
        "",
        "    Returns:",
        "        The consent record",
        "    \"\"\"",
        "    ensure_dirs()",
        "",
        "    consent_data = {",
        "        'consented': consented,",
        "        'timestamp': datetime.now().isoformat(),",
        "        'contributor_name': contributor_name,",
        "        'contributor_email': contributor_email,",
        "        'notes': notes,",
        "        'version': '1.0',",
        "    }",
        "",
        "    atomic_write_json(CONSENT_FILE, consent_data)",
        "    return consent_data",
        "",
        "",
        "def preview_contribution_data(",
        "    max_samples: int = 5,",
        "    include_full_text: bool = False",
        ") -> Dict[str, Any]:",
        "    \"\"\"Preview what data would be shared if contributing.",
        "",
        "    Args:",
        "        max_samples: Maximum samples to show per category",
        "        include_full_text: Whether to include full query/response text",
        "",
        "    Returns:",
        "        Preview of contribution data with statistics",
        "    \"\"\"",
        "    ensure_dirs()",
        "",
        "    preview = {",
        "        'summary': {",
        "            'total_commits': 0,",
        "            'total_chats': 0,",
        "            'total_sessions': 0,",
        "            'date_range': {'earliest': None, 'latest': None},",
        "            'unique_files': set(),",
        "            'unique_tools': set(),",
        "        },",
        "        'sample_commits': [],",
        "        'sample_chats': [],",
        "        'redaction_stats': {",
        "            'total_redactions': 0,",
        "            'chats_with_redactions': 0,",
        "        }",
        "    }",
        "",
        "    all_timestamps = []",
        "",
        "    # Sample commits",
        "    if COMMITS_DIR.exists():",
        "        commit_files = sorted(COMMITS_DIR.glob(\"*.json\"), reverse=True)",
        "        preview['summary']['total_commits'] = len(commit_files)",
        "",
        "        for f in commit_files[:max_samples]:",
        "            try:",
        "                with open(f, 'r', encoding='utf-8') as fp:",
        "                    data = json.load(fp)",
        "                all_timestamps.append(data.get('timestamp', ''))",
        "                preview['summary']['unique_files'].update(data.get('files_changed', []))",
        "",
        "                sample = {",
        "                    'hash': data.get('hash', '')[:12],",
        "                    'message': data.get('message', '')[:100] + ('...' if len(data.get('message', '')) > 100 else ''),",
        "                    'files_count': len(data.get('files_changed', [])),",
        "                    'timestamp': data.get('timestamp', '')[:10],",
        "                }",
        "                preview['sample_commits'].append(sample)",
        "            except (json.JSONDecodeError, IOError):",
        "                continue",
        "",
        "    # Sample chats",
        "    if CHATS_DIR.exists():",
        "        chat_files = sorted(CHATS_DIR.glob(\"**/*.json\"), reverse=True)",
        "        preview['summary']['total_chats'] = len(chat_files)",
        "",
        "        for f in chat_files[:max_samples]:",
        "            try:",
        "                with open(f, 'r', encoding='utf-8') as fp:",
        "                    data = json.load(fp)",
        "                all_timestamps.append(data.get('timestamp', ''))",
        "                preview['summary']['unique_tools'].update(data.get('tools_used', []))",
        "",
        "                query = data.get('query', '')",
        "                response = data.get('response', '')",
        "",
        "                # Check for redactions",
        "                redacted_query = redact_sensitive_data(query)",
        "                redacted_response = redact_sensitive_data(response)",
        "                redactions = count_redactions(query, redacted_query) + count_redactions(response, redacted_response)",
        "",
        "                if redactions > 0:",
        "                    preview['redaction_stats']['total_redactions'] += redactions",
        "                    preview['redaction_stats']['chats_with_redactions'] += 1",
        "",
        "                sample = {",
        "                    'id': data.get('id', ''),",
        "                    'timestamp': data.get('timestamp', '')[:10],",
        "                    'tools_used': data.get('tools_used', []),",
        "                    'files_count': len(data.get('files_referenced', [])) + len(data.get('files_modified', [])),",
        "                    'redactions_applied': redactions,",
        "                }",
        "",
        "                if include_full_text:",
        "                    sample['query'] = redacted_query[:500] + ('...' if len(redacted_query) > 500 else '')",
        "                    sample['response_preview'] = redacted_response[:200] + ('...' if len(redacted_response) > 200 else '')",
        "                else:",
        "                    sample['query_preview'] = redacted_query[:100] + ('...' if len(redacted_query) > 100 else '')",
        "",
        "                preview['sample_chats'].append(sample)",
        "            except (json.JSONDecodeError, IOError):",
        "                continue",
        "",
        "    # Count sessions",
        "    if SESSIONS_DIR.exists():",
        "        preview['summary']['total_sessions'] = len(list(SESSIONS_DIR.glob(\"*.json\")))",
        "",
        "    # Calculate date range",
        "    if all_timestamps:",
        "        sorted_ts = sorted([t for t in all_timestamps if t])",
        "        if sorted_ts:",
        "            preview['summary']['date_range']['earliest'] = sorted_ts[0][:10]",
        "            preview['summary']['date_range']['latest'] = sorted_ts[-1][:10]",
        "",
        "    # Convert sets to counts for JSON serialization",
        "    preview['summary']['unique_files'] = len(preview['summary']['unique_files'])",
        "    preview['summary']['unique_tools'] = len(preview['summary']['unique_tools'])",
        "",
        "    return preview",
        "",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "}",
        "",
        "ACTION_SCHEMA = {",
        "    \"required\": [\"id\", \"timestamp\", \"session_id\", \"action_type\", \"target\"],",
        "    \"types\": {",
        "        \"id\": str, \"timestamp\": str, \"session_id\": str,",
        "        \"action_type\": str, \"target\": str, \"success\": bool,",
        "    }",
        "}",
        ""
      ],
      "context_after": [
        "",
        "def validate_schema(data: dict, schema: dict, data_type: str) -> List[str]:",
        "    \"\"\"Validate data against a schema. Returns list of errors (empty if valid).\"\"\"",
        "    errors = []",
        "    for field in schema[\"required\"]:",
        "        if field not in data:",
        "            errors.append(f\"{data_type}: missing required field '{field}'\")",
        "    for field, expected in schema[\"types\"].items():",
        "        if field in data and data[field] is not None:",
        "            if isinstance(expected, tuple):",
        "                if not isinstance(data[field], expected):",
        "                    errors.append(f\"{data_type}: field '{field}' has wrong type\")",
        "            elif not isinstance(data[field], expected):",
        "                errors.append(f\"{data_type}: field '{field}' has type {type(data[field]).__name__}, expected {expected.__name__}\")",
        "    return errors",
        "",
        "",
        "# ============================================================================",
        "# FILE LOCKING (for concurrent access safety)",
        "# ============================================================================",
        "",
        "@contextmanager",
        "def file_lock(filepath: Path, exclusive: bool = True):",
        "    \"\"\"Context manager for file locking to prevent race conditions.",
        "",
        "    Args:",
        "        filepath: Path to lock file"
      ],
      "change_type": "add"
    },
    {
      "file": "scripts/ml_data_collector.py",
      "function": "def save_chat_entry(entry: ChatEntry, validate: bool = True):",
      "start_line": 1360,
      "lines_added": [
        "    skip_redaction: bool = False,",
        "",
        "    Sensitive data (API keys, passwords, tokens) is automatically redacted before storage",
        "    unless skip_redaction=True.",
        "    # Redact sensitive data before storage",
        "    if not skip_redaction:",
        "        query = redact_sensitive_data(query)",
        "        response = redact_sensitive_data(response)",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "",
        "",
        "def log_chat(",
        "    query: str,",
        "    response: str,",
        "    session_id: Optional[str] = None,",
        "    files_referenced: Optional[List[str]] = None,",
        "    files_modified: Optional[List[str]] = None,",
        "    tools_used: Optional[List[str]] = None,",
        "    user_feedback: Optional[str] = None,"
      ],
      "context_after": [
        ") -> ChatEntry:",
        "    \"\"\"Log a chat query/response pair.",
        "",
        "    If no session_id is provided, uses the current session (creating one if needed).",
        "    The chat is automatically registered with the session for commit linking.",
        "    \"\"\"",
        "    # Use current session or create one",
        "    if session_id is None:",
        "        session_id = get_or_create_session()",
        "",
        "    entry = ChatEntry(",
        "        id=generate_chat_id(),",
        "        timestamp=datetime.now().isoformat(),",
        "        session_id=session_id,",
        "        query=query,",
        "        response=response,",
        "        files_referenced=files_referenced or [],",
        "        files_modified=files_modified or [],",
        "        tools_used=tools_used or [],",
        "        user_feedback=user_feedback,"
      ],
      "change_type": "add"
    },
    {
      "file": "scripts/ml_data_collector.py",
      "function": "def main():",
      "start_line": 2633,
      "lines_added": [
        "    elif command == \"cleanup\":",
        "        # Remove data older than retention period",
        "        import argparse",
        "        parser = argparse.ArgumentParser()",
        "        parser.add_argument(\"--days\", type=int, default=DEFAULT_RETENTION_DAYS,",
        "                            help=f\"Retention period in days (default: {DEFAULT_RETENTION_DAYS})\")",
        "        parser.add_argument(\"--dry-run\", action=\"store_true\",",
        "                            help=\"Show what would be deleted without deleting\")",
        "        parser.add_argument(\"--verbose\", \"-v\", action=\"store_true\",",
        "                            help=\"Show each file being removed\")",
        "        args = parser.parse_args(sys.argv[2:])",
        "",
        "        print(f\"\\n{'='*60}\")",
        "        print(\"DATA CLEANUP\")",
        "        print(f\"{'='*60}\")",
        "        print(f\"Retention period: {args.days} days\")",
        "        if args.dry_run:",
        "            print(\"Mode: DRY RUN (no files will be deleted)\")",
        "        print()",
        "",
        "        results = cleanup_old_data(",
        "            retention_days=args.days,",
        "            dry_run=args.dry_run,",
        "            verbose=args.verbose",
        "        )",
        "",
        "        total_removed = (results['commits_removed'] + results['chats_removed'] +",
        "                        results['actions_removed'] + results['sessions_removed'])",
        "",
        "        if total_removed == 0:",
        "            print(\"âœ“ No data older than cutoff date found.\")",
        "        else:",
        "            action = \"Would remove\" if args.dry_run else \"Removed\"",
        "            print(f\"\\n{action}:\")",
        "            print(f\"   Commits:  {results['commits_removed']}\")",
        "            print(f\"   Chats:    {results['chats_removed']}\")",
        "            print(f\"   Actions:  {results['actions_removed']}\")",
        "            print(f\"   Sessions: {results['sessions_removed']}\")",
        "",
        "            if results['bytes_freed'] > 1024 * 1024:",
        "                print(f\"   Space:    {results['bytes_freed'] / 1024 / 1024:.2f} MB\")",
        "            elif results['bytes_freed'] > 1024:",
        "                print(f\"   Space:    {results['bytes_freed'] / 1024:.2f} KB\")",
        "            else:",
        "                print(f\"   Space:    {results['bytes_freed']} bytes\")",
        "",
        "        print(f\"{'='*60}\\n\")",
        "",
        "    elif command == \"contribute\":",
        "        # Manage contribution consent and preview data",
        "        import argparse",
        "        parser = argparse.ArgumentParser()",
        "        parser.add_argument(\"action\", nargs=\"?\", choices=[\"status\", \"enable\", \"disable\", \"preview\"],",
        "                            default=\"status\", help=\"Contribution action\")",
        "        parser.add_argument(\"--name\", help=\"Your name for attribution\")",
        "        parser.add_argument(\"--email\", help=\"Contact email\")",
        "        parser.add_argument(\"--notes\", help=\"Additional notes\")",
        "        parser.add_argument(\"--samples\", type=int, default=5,",
        "                            help=\"Number of samples to show in preview\")",
        "        parser.add_argument(\"--full-text\", action=\"store_true\",",
        "                            help=\"Show full query/response text in preview\")",
        "        args = parser.parse_args(sys.argv[2:])",
        "",
        "        if args.action == \"status\":",
        "            consent = get_contribution_consent()",
        "            print(f\"\\n{'='*60}\")",
        "            print(\"CONTRIBUTION STATUS\")",
        "            print(f\"{'='*60}\")",
        "",
        "            if consent is None:",
        "                print(\"\\nðŸ“‹ Status: NOT CONFIGURED\")",
        "                print(\"\\n   You haven't decided whether to contribute data yet.\")",
        "                print(\"   Run 'contribute enable' to opt-in or 'contribute disable' to opt-out.\")",
        "            else:",
        "                status = \"âœ… ENABLED\" if consent.get('consented') else \"âŒ DISABLED\"",
        "                print(f\"\\nðŸ“‹ Status: {status}\")",
        "                print(f\"   Recorded: {consent.get('timestamp', 'unknown')[:10]}\")",
        "                if consent.get('contributor_name'):",
        "                    print(f\"   Name: {consent['contributor_name']}\")",
        "                if consent.get('contributor_email'):",
        "                    print(f\"   Email: {consent['contributor_email']}\")",
        "                if consent.get('notes'):",
        "                    print(f\"   Notes: {consent['notes']}\")",
        "",
        "            print(f\"\\n{'='*60}\\n\")",
        "",
        "        elif args.action == \"enable\":",
        "            print(f\"\\n{'='*60}\")",
        "            print(\"ENABLE DATA CONTRIBUTION\")",
        "            print(f\"{'='*60}\")",
        "            print(\"\"\"",
        "By enabling contribution, you agree to share your collected data",
        "(commits, chats, sessions) with the project maintainers for training",
        "a project-specific micro-model.",
        "",
        "WHAT GETS SHARED:",
        "- Commit messages and file change patterns",
        "- Query/response pairs (with sensitive data automatically redacted)",
        "- Tool usage patterns and session metadata",
        "",
        "WHAT DOESN'T GET SHARED:",
        "- Raw file contents",
        "- Actual code diffs (only patterns)",
        "- Any data matching redaction patterns (API keys, passwords, etc.)",
        "",
        "Your name and email (if provided) will be used for:",
        "- Attribution in the trained model's credits",
        "- Contact if we have questions about your contributions",
        "\"\"\")",
        "",
        "            # Get confirmation",
        "            response = input(\"Do you consent to contributing your data? [y/N] \")",
        "            if response.lower() != 'y':",
        "                print(\"\\nContribution not enabled. Your data stays local.\")",
        "                sys.exit(0)",
        "",
        "            consent = set_contribution_consent(",
        "                consented=True,",
        "                contributor_name=args.name,",
        "                contributor_email=args.email,",
        "                notes=args.notes",
        "            )",
        "",
        "            print(f\"\\nâœ… Contribution ENABLED\")",
        "            print(f\"   Recorded at: {consent['timestamp'][:19]}\")",
        "            if args.name:",
        "                print(f\"   Name: {args.name}\")",
        "            print(\"\\nYour data will be included in the next collection round.\")",
        "            print(\"Run 'contribute preview' to see what will be shared.\")",
        "            print(f\"{'='*60}\\n\")",
        "",
        "        elif args.action == \"disable\":",
        "            consent = set_contribution_consent(",
        "                consented=False,",
        "                notes=args.notes or \"User opted out\"",
        "            )",
        "",
        "            print(f\"\\n{'='*60}\")",
        "            print(\"CONTRIBUTION DISABLED\")",
        "            print(f\"{'='*60}\")",
        "            print(\"\\nâŒ Your data will NOT be shared with the project.\")",
        "            print(\"   Local collection continues (for your own use).\")",
        "            print(\"   You can re-enable anytime with 'contribute enable'.\")",
        "            print(f\"{'='*60}\\n\")",
        "",
        "        elif args.action == \"preview\":",
        "            print(f\"\\n{'='*60}\")",
        "            print(\"CONTRIBUTION DATA PREVIEW\")",
        "            print(f\"{'='*60}\")",
        "            print(\"\\nThis shows what data would be shared if you contribute:\\n\")",
        "",
        "            preview = preview_contribution_data(",
        "                max_samples=args.samples,",
        "                include_full_text=args.full_text",
        "            )",
        "",
        "            summary = preview['summary']",
        "            print(\"ðŸ“Š Summary:\")",
        "            print(f\"   Total commits:  {summary['total_commits']}\")",
        "            print(f\"   Total chats:    {summary['total_chats']}\")",
        "            print(f\"   Total sessions: {summary['total_sessions']}\")",
        "            print(f\"   Unique files:   {summary['unique_files']}\")",
        "            print(f\"   Unique tools:   {summary['unique_tools']}\")",
        "            if summary['date_range']['earliest']:",
        "                print(f\"   Date range:     {summary['date_range']['earliest']} to {summary['date_range']['latest']}\")",
        "",
        "            redaction = preview['redaction_stats']",
        "            if redaction['total_redactions'] > 0:",
        "                print(f\"\\nðŸ”’ Redaction Stats:\")",
        "                print(f\"   Chats with redactions: {redaction['chats_with_redactions']}\")",
        "                print(f\"   Total redactions made: {redaction['total_redactions']}\")",
        "",
        "            if preview['sample_commits']:",
        "                print(f\"\\nðŸ“ Sample Commits (latest {len(preview['sample_commits'])}):\")",
        "                for c in preview['sample_commits']:",
        "                    print(f\"   [{c['timestamp']}] {c['hash']} - {c['message']}\")",
        "                    print(f\"            Files: {c['files_count']}\")",
        "",
        "            if preview['sample_chats']:",
        "                print(f\"\\nðŸ’¬ Sample Chats (latest {len(preview['sample_chats'])}):\")",
        "                for c in preview['sample_chats']:",
        "                    print(f\"   [{c['timestamp']}] {c['id']}\")",
        "                    if 'query' in c:",
        "                        print(f\"      Query: {c['query']}\")",
        "                    elif 'query_preview' in c:",
        "                        print(f\"      Query: {c['query_preview']}\")",
        "                    print(f\"      Tools: {', '.join(c['tools_used']) or 'none'}\")",
        "                    if c['redactions_applied'] > 0:",
        "                        print(f\"      ðŸ”’ {c['redactions_applied']} sensitive item(s) redacted\")",
        "",
        "            print(f\"\\n{'='*60}\\n\")",
        "",
        "    elif command == \"redact-test\":",
        "        # Test redaction patterns on sample text",
        "        import argparse",
        "        parser = argparse.ArgumentParser()",
        "        parser.add_argument(\"--text\", \"-t\", help=\"Text to test redaction on\")",
        "        parser.add_argument(\"--file\", \"-f\", help=\"File containing text to test\")",
        "        args = parser.parse_args(sys.argv[2:])",
        "",
        "        if args.file:",
        "            with open(args.file, 'r', encoding='utf-8') as f:",
        "                text = f.read()",
        "        elif args.text:",
        "            text = args.text",
        "        else:",
        "            print(\"Provide --text or --file to test redaction patterns\")",
        "            sys.exit(1)",
        "",
        "        print(f\"\\n{'='*60}\")",
        "        print(\"REDACTION TEST\")",
        "        print(f\"{'='*60}\")",
        "        print(f\"\\nOriginal ({len(text)} chars):\")",
        "        print(text[:500] + ('...' if len(text) > 500 else ''))",
        "",
        "        redacted = redact_sensitive_data(text)",
        "        redaction_count = count_redactions(text, redacted)",
        "",
        "        print(f\"\\nRedacted ({redaction_count} patterns matched):\")",
        "        print(redacted[:500] + ('...' if len(redacted) > 500 else ''))",
        "        print(f\"{'='*60}\\n\")",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "                        sys.exit(1)",
        "                    else:",
        "                        print(f\"âœ— Chat {args.chat_id} already has feedback.\")",
        "                        print(\"   Use --force to overwrite.\")",
        "                        sys.exit(1)",
        "",
        "            except ValueError as e:",
        "                print(f\"âœ— {e}\")",
        "                sys.exit(1)",
        ""
      ],
      "context_after": [
        "    else:",
        "        print(f\"Unknown command: {command}\")",
        "        print(__doc__)",
        "",
        "",
        "if __name__ == \"__main__\":",
        "    main()"
      ],
      "change_type": "add"
    }
  ],
  "hour_of_day": 13,
  "day_of_week": "Monday",
  "seconds_since_last_commit": 586,
  "is_merge": false,
  "is_initial": false,
  "parent_count": 1,
  "session_id": null,
  "related_chats": [],
  "ci_result": null,
  "reverted": false,
  "amended": false
}