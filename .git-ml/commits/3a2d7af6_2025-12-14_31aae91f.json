{
  "hash": "3a2d7af64d73a9eedd324fb1039385436b22de81",
  "message": "Merge pull request #80 from scrawlsbenches/claude/resume-dog-fooding-9RPIV",
  "author": "scrawlsbenches",
  "timestamp": "2025-12-14 12:48:06 -0500",
  "branch": "claude/multi-index-design-DvifZ",
  "files_changed": [
    ".claude/commands/director.md",
    ".claude/commands/knowledge-transfer.md",
    ".claude/skills/memory-manager/SKILL.md",
    ".gitignore",
    "CLAUDE.md",
    "cortical/config.py",
    "docs/text-as-memories.md",
    "samples/decisions/adr-microseconds-task-id.md",
    "samples/memories/2025-12-14-dogfooding-session.md",
    "samples/memories/2025-12-14-session-security-and-memories.md",
    "samples/memories/concept-hebbian-text-processing.md",
    "scripts/task_utils.py",
    "tasks/2025-12-14_11-15-01_41d5.json",
    "tasks/2025-12-14_17-13-01_6aa8.json",
    "tests/security/__init__.py",
    "tests/security/test_fuzzing.py",
    "tests/security/test_security.py",
    "tests/unit/test_task_utils.py"
  ],
  "insertions": 2582,
  "deletions": 15,
  "hunks": [
    {
      "file": ".claude/commands/director.md",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "# Director Agent: Intelligent Task Orchestration",
        "",
        "You are a Director Agent responsible for orchestrating complex work across multiple sub-agents. Your role is to analyze tasks, create optimal execution batches, delegate effectively, verify results, and adapt plans based on outcomes.",
        "",
        "## Core Principles",
        "",
        "### 1. Understand Before Planning",
        "- Read the full task requirements before creating any plan",
        "- Identify dependencies, risks, and verification criteria",
        "- Check existing tasks: `python scripts/task_utils.py list`",
        "- Search for relevant context: `python scripts/search_codebase.py \"query\"`",
        "",
        "### 2. Batch for Parallelism, Sequence for Dependencies",
        "- **Parallel**: Tasks with no shared dependencies can run simultaneously",
        "- **Sequential**: Tasks where output of one feeds into another",
        "- **Hybrid**: Mix of parallel batches with sequential checkpoints",
        "",
        "### 3. Verify Early and Often",
        "- Don't wait until the end to verify",
        "- Each batch should have clear success criteria",
        "- Failed verification triggers replanning, not blind retry",
        "",
        "---",
        "",
        "## Task Analysis Framework",
        "",
        "When given a complex task, analyze it using this framework:",
        "",
        "```",
        "┌─────────────────────────────────────────────────────────────┐",
        "│                    TASK DECOMPOSITION                        │",
        "├─────────────────────────────────────────────────────────────┤",
        "│ 1. What is the end goal? (success looks like...)            │",
        "│ 2. What are the major components?                           │",
        "│ 3. What are the dependencies between components?            │",
        "│ 4. What can fail? How will we know?                         │",
        "│ 5. What existing code/docs are relevant?                    │",
        "└─────────────────────────────────────────────────────────────┘",
        "```",
        "",
        "### Dependency Graph",
        "",
        "Create a mental (or written) dependency graph:",
        "",
        "```",
        "[Task A] ──┐",
        "           ├──► [Task D] ──► [Task F]",
        "[Task B] ──┘         │",
        "                     ▼",
        "[Task C] ────────► [Task E] ──► [Task G]",
        "```",
        "",
        "**Batching from graph:**",
        "- Batch 1 (parallel): A, B, C",
        "- Batch 2 (parallel after 1): D, E",
        "- Batch 3 (sequential): F, G",
        "",
        "---",
        "",
        "## Delegation Patterns",
        "",
        "### Pattern 1: Research Batch",
        "Use when you need information before implementing.",
        "",
        "```",
        "Spawn agents in PARALLEL:",
        "├── Agent 1: \"Research how X is currently implemented in cortical/\"",
        "├── Agent 2: \"Find all tests related to Y in tests/\"",
        "└── Agent 3: \"Check docs/ for existing documentation on Z\"",
        "",
        "WAIT for all results, then SYNTHESIZE before next batch.",
        "```",
        "",
        "### Pattern 2: Implementation Batch",
        "Use when you have clear specs and independent components.",
        "",
        "```",
        "Spawn agents in PARALLEL:",
        "├── Agent 1: \"Implement function X in module A. Do NOT modify other files.\"",
        "├── Agent 2: \"Implement function Y in module B. Do NOT modify other files.\"",
        "└── Agent 3: \"Write tests for X and Y in tests/unit/. Do NOT implement X or Y.\"",
        "",
        "VERIFY: All agents complete, no file conflicts, tests reference correct functions.",
        "```",
        "",
        "### Pattern 3: Sequential Pipeline",
        "Use when each step depends on the previous.",
        "",
        "```",
        "Step 1: Agent researches and returns findings",
        "        ↓ (Director reviews)",
        "Step 2: Agent implements based on findings",
        "        ↓ (Director verifies)",
        "Step 3: Agent writes tests for implementation",
        "        ↓ (Director runs tests)",
        "Step 4: Agent documents the feature",
        "```",
        "",
        "### Pattern 4: Verify-and-Fix Loop",
        "Use when quality is critical.",
        "",
        "```",
        "LOOP until success or max_attempts:",
        "    1. Agent implements/fixes",
        "    2. Director runs verification (tests, linting, etc.)",
        "    3. IF pass: break",
        "       ELSE: Provide failure details to agent for next iteration",
        "```",
        "",
        "---",
        "",
        "## Delegation Prompt Templates",
        "",
        "### For Research Agents",
        "```",
        "You are researching [TOPIC] in the codebase.",
        "",
        "SEARCH these locations:",
        "- [specific directories or files]",
        "",
        "FIND:",
        "- [specific information needed]",
        "",
        "RETURN a structured report with:",
        "1. Summary (2-3 sentences)",
        "2. Key findings (bullet points)",
        "3. Relevant file paths with line numbers",
        "4. Recommendations for next steps",
        "",
        "Do NOT modify any files. Research only.",
        "```",
        "",
        "### For Implementation Agents",
        "```",
        "You are implementing [FEATURE].",
        "",
        "CONTEXT:",
        "- [relevant background from research phase]",
        "- [dependencies and constraints]",
        "",
        "IMPLEMENT:",
        "- [specific function/class/module]",
        "- Location: [exact file path]",
        "",
        "CONSTRAINTS:",
        "- Do NOT modify files outside [allowed paths]",
        "- Follow existing code patterns in [reference file]",
        "- Include type hints and docstrings",
        "",
        "WHEN DONE:",
        "- List all files modified",
        "- Describe what was implemented",
        "- Note any concerns or edge cases",
        "```",
        "",
        "### For Testing Agents",
        "```",
        "You are writing tests for [FEATURE].",
        "",
        "IMPLEMENTATION DETAILS:",
        "- [summary of what was implemented]",
        "- [file locations]",
        "",
        "WRITE TESTS covering:",
        "- Happy path",
        "- Edge cases: [specific cases]",
        "- Error conditions: [expected errors]",
        "",
        "LOCATION: [test file path]",
        "",
        "FOLLOW patterns from: [existing test file for reference]",
        "",
        "VERIFY by running: python -m pytest [test file] -v",
        "```",
        "",
        "### For Verification Agents",
        "```",
        "You are verifying [FEATURE/CHANGE].",
        "",
        "CHECK:",
        "1. All tests pass: python -m pytest tests/ -x",
        "2. No type errors: (if applicable)",
        "3. Code follows patterns in CLAUDE.md",
        "4. Documentation is updated",
        "",
        "REPORT:",
        "- Pass/Fail status",
        "- If fail: exact error messages and file locations",
        "- Suggestions for fixes",
        "```",
        "",
        "---",
        "",
        "## Verification Strategies",
        "",
        "### After Each Batch",
        "```python",
        "def verify_batch(batch_results):",
        "    checks = []",
        "",
        "    # 1. All agents completed",
        "    checks.append(all(r.completed for r in batch_results))",
        "",
        "    # 2. No conflicting file modifications",
        "    modified_files = [f for r in batch_results for f in r.modified_files]",
        "    checks.append(len(modified_files) == len(set(modified_files)))",
        "",
        "    # 3. Tests still pass",
        "    checks.append(run_tests())",
        "",
        "    # 4. Git status is clean (no untracked important files)",
        "    checks.append(verify_git_status())",
        "",
        "    return all(checks)",
        "```",
        "",
        "### Verification Commands",
        "```bash",
        "# Quick sanity check",
        "python -m pytest tests/smoke/ -v",
        "",
        "# Full test suite",
        "python -m pytest tests/ -x -q",
        "",
        "# Check for uncommitted changes",
        "git status",
        "",
        "# Verify no regressions",
        "python -m pytest tests/regression/ -v",
        "```",
        "",
        "---",
        "",
        "## Replanning Triggers",
        "",
        "### When to Replan",
        "",
        "1. **Agent reports blocker**: Missing dependency, unclear requirement",
        "2. **Verification fails**: Tests fail, conflicts detected",
        "3. **New information**: Agent discovers something that changes the approach",
        "4. **Scope creep**: Task is larger than estimated",
        "",
        "### Replanning Process",
        "",
        "```",
        "1. STOP current batch (don't spawn more agents)",
        "",
        "2. GATHER information:",
        "   - What succeeded?",
        "   - What failed and why?",
        "   - What new information do we have?",
        "",
        "3. ANALYZE:",
        "   - Is the original goal still valid?",
        "   - Do we need to adjust the approach?",
        "   - Are there new dependencies?",
        "",
        "4. CREATE new plan:",
        "   - Incorporate lessons learned",
        "   - Adjust batch composition",
        "   - Update success criteria",
        "",
        "5. COMMUNICATE:",
        "   - Summarize what changed and why",
        "   - Get user confirmation if major pivot",
        "",
        "6. RESUME execution with new plan",
        "```",
        "",
        "### Replanning Example",
        "",
        "```",
        "ORIGINAL PLAN:",
        "  Batch 1: [Implement feature X]",
        "  Batch 2: [Write tests for X]",
        "  Batch 3: [Document X]",
        "",
        "FAILURE: Agent reports X requires modifying core module Y",
        "",
        "REPLAN:",
        "  Batch 1: [Research module Y dependencies]  ← NEW",
        "  Batch 2: [Implement Y changes, implement X]  ← MODIFIED",
        "  Batch 3: [Write tests for Y and X]  ← MODIFIED",
        "  Batch 4: [Document X and Y changes]  ← MODIFIED",
        "```",
        "",
        "---",
        "",
        "## Orchestration Checklist",
        "",
        "Before starting:",
        "- [ ] Understand the full scope of work",
        "- [ ] Identify all dependencies",
        "- [ ] Define success criteria for each component",
        "- [ ] Check for existing relevant code/tests/docs",
        "",
        "For each batch:",
        "- [ ] Tasks in batch are truly independent",
        "- [ ] Each agent has clear, scoped instructions",
        "- [ ] Success criteria are verifiable",
        "- [ ] Failure handling is defined",
        "",
        "After each batch:",
        "- [ ] All agents completed",
        "- [ ] No file conflicts",
        "- [ ] Tests pass",
        "- [ ] Results match expectations",
        "",
        "Before declaring done:",
        "- [ ] All success criteria met",
        "- [ ] Full test suite passes",
        "- [ ] Documentation updated",
        "- [ ] Changes committed and pushed",
        "- [ ] Knowledge transfer created (if significant work)",
        "",
        "---",
        "",
        "## Example: Complete Orchestration",
        "",
        "**Task**: \"Add a new CLI command for memory creation\"",
        "",
        "### Phase 1: Research (Parallel)",
        "```",
        "Spawn 3 agents:",
        "1. \"Find existing CLI commands in scripts/. Note patterns and conventions.\"",
        "2. \"Research memory system in samples/memories/ and .claude/skills/memory-manager/\"",
        "3. \"Check CLAUDE.md and docs/ for CLI documentation requirements\"",
        "```",
        "",
        "### Phase 2: Synthesize (Director)",
        "```",
        "Review findings:",
        "- CLI pattern: argparse in scripts/, follows new_task.py pattern",
        "- Memory format: YYYY-MM-DD-topic.md with frontmatter",
        "- Docs needed: Update CLAUDE.md quick reference table",
        "```",
        "",
        "### Phase 3: Implement (Parallel)",
        "```",
        "Spawn 2 agents:",
        "1. \"Create scripts/new_memory.py following new_task.py pattern.",
        "    Generate merge-safe filenames with timestamps.\"",
        "2. \"Write tests in tests/unit/test_new_memory.py covering:",
        "    - Filename generation",
        "    - Template creation",
        "    - Argument parsing\"",
        "```",
        "",
        "### Phase 4: Verify (Director)",
        "```",
        "Run: python -m pytest tests/unit/test_new_memory.py -v",
        "Run: python scripts/new_memory.py --help",
        "Run: python scripts/new_memory.py \"Test memory\" --dry-run",
        "```",
        "",
        "### Phase 5: Document (Sequential)",
        "```",
        "Agent: \"Update CLAUDE.md to add new_memory.py to quick reference.",
        "        Update .claude/skills/memory-manager/SKILL.md with CLI usage.\"",
        "```",
        "",
        "### Phase 6: Finalize (Director)",
        "```",
        "- Run full test suite",
        "- Commit changes",
        "- Create knowledge transfer if significant",
        "```",
        "",
        "---",
        "",
        "## Anti-Patterns to Avoid",
        "",
        "❌ **Spawning too many agents at once**",
        "- Hard to track, likely conflicts",
        "- Better: 2-4 agents per batch maximum",
        "",
        "❌ **Vague instructions**",
        "- \"Fix the bug\" → Agent doesn't know which bug",
        "- Better: \"Fix issue where X returns Y instead of Z in file.py:123\"",
        "",
        "❌ **No verification between batches**",
        "- Errors compound, harder to debug",
        "- Better: Verify after each batch before proceeding",
        "",
        "❌ **Ignoring agent feedback**",
        "- Agent says \"this is risky\" → Director proceeds anyway",
        "- Better: Pause, understand concern, adjust if needed",
        "",
        "❌ **Replanning without understanding failure**",
        "- Test failed → immediately try something else",
        "- Better: Understand WHY it failed, then adjust",
        "",
        "---",
        "",
        "## Quick Reference",
        "",
        "| Situation | Action |",
        "|-----------|--------|",
        "| Need information | Research batch (parallel) |",
        "| Independent implementations | Implementation batch (parallel) |",
        "| Step-by-step process | Sequential pipeline |",
        "| Quality critical | Verify-and-fix loop |",
        "| Something failed | Stop, analyze, replan |",
        "| Major scope change | Confirm with user first |",
        "| Work complete | Verify all criteria, commit, knowledge transfer |",
        "",
        "---",
        "",
        "*\"The best director doesn't do the work—they ensure the right work gets done, in the right order, by the right agents, with the right verification.\"*"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": ".claude/commands/knowledge-transfer.md",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "# Knowledge Transfer Generator",
        "",
        "Generate a knowledge transfer document based on the current session context.",
        "",
        "## Instructions",
        "",
        "Analyze the conversation history and create a comprehensive knowledge transfer document. Save it to `samples/memories/` with today's date.",
        "",
        "### Document Structure",
        "",
        "Create the document with this structure:",
        "",
        "```markdown",
        "# Session Knowledge Transfer: [DATE] [BRIEF-TOPIC]",
        "",
        "**Date:** YYYY-MM-DD",
        "**Session:** [Describe the session focus]",
        "**Branch:** [Current git branch if applicable]",
        "",
        "## Summary",
        "",
        "2-3 sentences capturing what was accomplished and why it matters.",
        "",
        "## What Was Accomplished",
        "",
        "### Completed Tasks",
        "- List tasks that were completed with their IDs",
        "- Include brief description of what each involved",
        "",
        "### Code Changes",
        "- Files created or modified",
        "- Key functions/classes added",
        "- Bug fixes applied",
        "",
        "### Documentation Added",
        "- New docs created",
        "- Existing docs updated",
        "",
        "## Key Decisions Made",
        "",
        "| Decision | Rationale | Alternatives Considered |",
        "|----------|-----------|------------------------|",
        "| ... | ... | ... |",
        "",
        "## Problems Encountered & Solutions",
        "",
        "### Problem 1: [Title]",
        "**Symptom:** What was observed",
        "**Root Cause:** What was actually wrong",
        "**Solution:** How it was fixed",
        "**Lesson:** What to remember for next time",
        "",
        "## Technical Insights",
        "",
        "- Key technical learnings from this session",
        "- Non-obvious discoveries",
        "- Performance findings",
        "- Security considerations",
        "",
        "## Context for Next Session",
        "",
        "### Current State",
        "- What's working",
        "- What's in progress",
        "- Any uncommitted changes",
        "",
        "### Suggested Next Steps",
        "1. Prioritized list of what to do next",
        "2. Any blockers to be aware of",
        "3. Related tasks to consider",
        "",
        "### Files to Review",
        "- Key files that were central to this work",
        "- Entry points for understanding the changes",
        "",
        "## Connections to Existing Knowledge",
        "",
        "- Links to related memories: [[memory-name.md]]",
        "- Related concepts: [[concept-name.md]]",
        "- Relevant decisions: [[adr-NNN.md]]",
        "",
        "## Tags",
        "",
        "`tag1`, `tag2`, `tag3`",
        "```",
        "",
        "### Output Location",
        "",
        "Save to: `samples/memories/YYYY-MM-DD-session-[topic].md`",
        "",
        "Where `[topic]` is a brief kebab-case description of the main focus.",
        "",
        "### After Creating",
        "",
        "1. Show the user the generated document",
        "2. Suggest: `git add samples/memories/ && git commit -m \"memory: session knowledge transfer\"`",
        "3. Remind to re-index: `python scripts/index_codebase.py --incremental`",
        "",
        "### Tips for Good Knowledge Transfers",
        "",
        "- **Be specific** - Include file paths, function names, line numbers",
        "- **Capture the \"why\"** - Decisions without rationale are less useful",
        "- **Note surprises** - Things that were unexpected are often valuable",
        "- **Link generously** - Cross-references strengthen the knowledge graph",
        "- **Think forward** - What would help the next person (or future you)?"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": ".claude/skills/memory-manager/SKILL.md",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "# Memory Manager Skill",
        "",
        "Create, search, and manage knowledge memories stored in git.",
        "",
        "## Overview",
        "",
        "This skill helps you work with the text-as-memories system - treating documents as persistent, versioned, searchable memories that build institutional knowledge.",
        "",
        "## Memory Types",
        "",
        "### 1. Daily Memories (`samples/memories/YYYY-MM-DD-topic.md`)",
        "Capture learnings, discoveries, and insights from development sessions.",
        "",
        "### 2. Decision Records (`samples/decisions/adr-NNN-title.md`)",
        "Document architectural decisions with context, options, and rationale.",
        "",
        "### 3. Concept Documents (`samples/memories/concept-topic.md`)",
        "Consolidated knowledge about specific topics, refined from multiple memories.",
        "",
        "## Commands",
        "",
        "### Create a Memory Entry",
        "",
        "When the user wants to capture a learning or insight:",
        "",
        "```bash",
        "# Create the file",
        "cat > samples/memories/$(date +%Y-%m-%d)-TOPIC.md << 'EOF'",
        "# Memory Entry: $(date +%Y-%m-%d) TOPIC",
        "",
        "**Tags:** `tag1`, `tag2`",
        "**Related:** [[other-memory.md]], [[concept.md]]",
        "",
        "---",
        "",
        "## Context",
        "What prompted this learning?",
        "",
        "## What I Learned",
        "- Key insight 1",
        "- Key insight 2",
        "",
        "## Connections Made",
        "- How this relates to other knowledge",
        "",
        "## Future Exploration",
        "- [ ] Follow-up item",
        "",
        "---",
        "*Committed to memory at: $(date -Iseconds)*",
        "EOF",
        "```",
        "",
        "### Create a Decision Record",
        "",
        "When the user makes an architectural decision:",
        "",
        "```bash",
        "# Find next ADR number",
        "NEXT_NUM=$(ls samples/decisions/adr-*.md 2>/dev/null | wc -l)",
        "NEXT_NUM=$((NEXT_NUM + 1))",
        "PADDED=$(printf \"%03d\" $NEXT_NUM)",
        "",
        "cat > samples/decisions/adr-${PADDED}-TITLE.md << 'EOF'",
        "# ADR-${PADDED}: TITLE",
        "",
        "**Status:** Proposed | Accepted | Deprecated | Superseded",
        "**Date:** $(date +%Y-%m-%d)",
        "**Deciders:** Team/Person",
        "**Tags:** `tag1`, `tag2`",
        "",
        "---",
        "",
        "## Context and Problem Statement",
        "What is the issue?",
        "",
        "## Decision Drivers",
        "1. Driver 1",
        "2. Driver 2",
        "",
        "## Considered Options",
        "### Option 1: Name",
        "**Pros:** ...",
        "**Cons:** ...",
        "",
        "### Option 2: Name",
        "**Pros:** ...",
        "**Cons:** ...",
        "",
        "## Decision Outcome",
        "**Chosen Option:** Option X",
        "",
        "**Rationale:** Why this option?",
        "",
        "## Consequences",
        "### Positive",
        "- ...",
        "",
        "### Negative",
        "- ...",
        "",
        "---",
        "EOF",
        "```",
        "",
        "### Search Memories",
        "",
        "```bash",
        "# Search indexed memories (requires corpus_dev.pkl)",
        "python scripts/search_codebase.py \"query terms\" --top 10",
        "",
        "# Search with expansion to see related terms",
        "python scripts/search_codebase.py \"query\" --expand",
        "```",
        "",
        "### Index New Memories",
        "",
        "After creating memories, re-index for search:",
        "",
        "```bash",
        "python scripts/index_codebase.py --incremental",
        "```",
        "",
        "## Best Practices",
        "",
        "1. **Write memories immediately** - Capture insights while fresh",
        "2. **Use consistent tags** - Makes searching easier",
        "3. **Link related memories** - Use `[[wiki-style]]` references",
        "4. **Include context** - Future you won't remember why",
        "5. **Consolidate periodically** - Merge related memories into concepts",
        "6. **Commit memories** - They're only persistent once in git",
        "",
        "## Integration with Tasks",
        "",
        "When completing a task, consider creating a memory:",
        "",
        "```markdown",
        "## What I Learned (from Task T-XXXXX)",
        "- The retrospective from the task",
        "- Additional insights discovered",
        "- Links to files modified",
        "```",
        "",
        "## Example Workflow",
        "",
        "1. **During session**: Notice something interesting",
        "2. **Capture**: Create a memory entry with the insight",
        "3. **Connect**: Add `[[links]]` to related knowledge",
        "4. **Commit**: `git add samples/ && git commit -m \"memory: ...\"`",
        "5. **Index**: `python scripts/index_codebase.py --incremental`",
        "6. **Later**: Search to recall: `python scripts/search_codebase.py \"that thing\"`",
        "",
        "## File Locations",
        "",
        "- Memories: `samples/memories/`",
        "- Decisions: `samples/decisions/`",
        "- Main guide: `docs/text-as-memories.md`"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": ".gitignore",
      "function": null,
      "start_line": 2,
      "lines_added": [
        ".hypothesis/"
      ],
      "lines_removed": [],
      "context_before": [
        "__pycache__/",
        "*.py[cod]",
        "*$py.class",
        "*.so",
        ".Python",
        "*.egg-info/",
        ".eggs/",
        "dist/",
        "build/",
        ".pytest_cache/"
      ],
      "context_after": [
        "",
        "# Generated corpus files",
        "corpus_dev.pkl",
        "*.pkl",
        "*.manifest.json",
        "*.pkl.hash",
        "",
        "# AI metadata files (generated, not tracked)",
        "*.ai_meta",
        ""
      ],
      "change_type": "add"
    },
    {
      "file": "CLAUDE.md",
      "function": "python scripts/index_codebase.py",
      "start_line": 952,
      "lines_added": [
        "Four skills are available in `.claude/skills/`:",
        "4. **memory-manager**: Create and manage knowledge memories (learnings, decisions, concepts)"
      ],
      "lines_removed": [
        "Three skills are available in `.claude/skills/`:"
      ],
      "context_before": [
        "python scripts/index_codebase.py --incremental",
        "",
        "# Search for code",
        "python scripts/search_codebase.py \"PageRank algorithm\"",
        "python scripts/search_codebase.py \"bigram separator\" --verbose",
        "python scripts/search_codebase.py --interactive",
        "```",
        "",
        "### Claude Skills",
        ""
      ],
      "context_after": [
        "",
        "1. **codebase-search**: Search the indexed codebase for code patterns and implementations",
        "2. **corpus-indexer**: Re-index the codebase after making changes",
        "3. **ai-metadata**: View pre-generated module metadata for rapid understanding",
        "",
        "### Indexer Options",
        "",
        "| Option | Description |",
        "|--------|-------------|",
        "| `--incremental`, `-i` | Only re-index changed files (fastest) |",
        "| `--status`, `-s` | Show what would change without indexing |",
        "| `--force`, `-f` | Force full rebuild |",
        "| `--log FILE` | Write detailed log to file |",
        "| `--verbose`, `-v` | Show per-file progress |"
      ],
      "change_type": "modify"
    },
    {
      "file": "CLAUDE.md",
      "function": "python scripts/index_codebase.py --status --use-chunks",
      "start_line": 1070,
      "lines_added": [
        "## Text-as-Memories: Knowledge Management",
        "",
        "The project uses a text-as-memories system to capture and preserve institutional knowledge. Documents are treated as memories that, when stored in git, form a persistent, searchable knowledge base.",
        "",
        "### Memory Types",
        "",
        "| Type | Location | Purpose |",
        "|------|----------|---------|",
        "| **Daily Memories** | `samples/memories/YYYY-MM-DD-*.md` | Capture daily learnings and insights |",
        "| **Decision Records** | `samples/decisions/adr-NNN-*.md` | Document architectural decisions |",
        "| **Concept Docs** | `samples/memories/concept-*.md` | Consolidated knowledge on topics |",
        "",
        "### Creating Memories",
        "",
        "**Daily Memory:**",
        "```bash",
        "# Capture a learning",
        "cat > samples/memories/$(date +%Y-%m-%d)-topic.md << 'EOF'",
        "# Memory Entry: YYYY-MM-DD Topic",
        "",
        "**Tags:** `tag1`, `tag2`",
        "**Related:** [[other-doc.md]]",
        "",
        "## What I Learned",
        "- Key insight here",
        "",
        "## Connections",
        "- How this relates to other knowledge",
        "EOF",
        "git add samples/ && git commit -m \"memory: topic insight\"",
        "```",
        "",
        "**Decision Record:**",
        "```bash",
        "# Document a decision",
        "cat > samples/decisions/adr-001-title.md << 'EOF'",
        "# ADR-001: Title",
        "",
        "**Status:** Accepted",
        "**Date:** YYYY-MM-DD",
        "",
        "## Context",
        "What problem are we solving?",
        "",
        "## Decision",
        "What did we decide?",
        "",
        "## Consequences",
        "What are the trade-offs?",
        "EOF",
        "```",
        "",
        "### Searching Memories",
        "",
        "```bash",
        "# Index memories for search",
        "python scripts/index_codebase.py --incremental",
        "",
        "# Search across code AND memories",
        "python scripts/search_codebase.py \"what did we learn about validation\"",
        "```",
        "",
        "### Best Practices",
        "",
        "1. **Write immediately** - Capture insights while fresh",
        "2. **Use consistent tags** - Improves searchability",
        "3. **Link related docs** - Use `[[wiki-style]]` references",
        "4. **Commit to git** - Memories are only persistent once committed",
        "5. **Consolidate periodically** - Merge related memories into concept docs",
        "",
        "### Integration with Tasks",
        "",
        "When completing a task, consider creating a memory entry from the retrospective:",
        "- What was learned?",
        "- What connections were made?",
        "- What should future developers know?",
        "",
        "See `docs/text-as-memories.md` for the full guide.",
        "",
        "---",
        "",
        "- **Text-as-Memories**: `docs/text-as-memories.md` - knowledge management guide"
      ],
      "lines_removed": [],
      "context_before": [
        "4. Removes old chunk files",
        "5. Preserves cache if still valid",
        "",
        "**Recommended frequency:**",
        "- Weekly for active development",
        "- Monthly for maintenance repositories",
        "- Before major releases",
        "",
        "---",
        ""
      ],
      "context_after": [
        "## File Quick Links",
        "",
        "- **Main API**: `cortical/processor.py` - `CorticalTextProcessor` class",
        "- **Graph algorithms**: `cortical/analysis.py` - PageRank, TF-IDF, clustering",
        "- **Search**: `cortical/query.py` - query expansion, document retrieval",
        "- **Data structures**: `cortical/minicolumn.py` - `Minicolumn`, `Edge`",
        "- **Configuration**: `cortical/config.py` - `CorticalConfig` dataclass",
        "- **Tests**: `tests/test_processor.py` - most comprehensive test file",
        "- **Demo**: `showcase.py` - interactive demonstration",
        "",
        "**Process Documentation:**",
        "- **Getting Started**: `docs/quickstart.md` - 5-minute tutorial for newcomers",
        "- **Contributing**: `CONTRIBUTING.md` - how to contribute (fork, test, PR workflow)",
        "- **Ethics**: `docs/code-of-ethics.md` - documentation, testing, and completion standards",
        "- **Dog-fooding**: `docs/dogfooding-checklist.md` - checklist for testing with real usage",
        "- **Definition of Done**: `docs/definition-of-done.md` - when is a task truly complete?",
        "- **Task Archive**: `TASK_ARCHIVE.md` - completed tasks history",
        "",
        "---",
        "",
        "*Remember: Measure before optimizing, test before committing, and document what you discover.*"
      ],
      "change_type": "add"
    },
    {
      "file": "cortical/config.py",
      "function": "Example:",
      "start_line": 18,
      "lines_added": [
        "import math"
      ],
      "lines_removed": [],
      "context_before": [
        "        isolation_threshold=0.03",
        "    )",
        "    processor = CorticalTextProcessor(config=config)",
        "",
        "    # Or modify defaults",
        "    config = CorticalConfig()",
        "    config.pagerank_iterations = 50",
        "    processor = CorticalTextProcessor(config=config)",
        "\"\"\"",
        ""
      ],
      "context_after": [
        "from dataclasses import dataclass, field",
        "from typing import Dict, Tuple, FrozenSet",
        "",
        "",
        "@dataclass",
        "class CorticalConfig:",
        "    \"\"\"",
        "    Configuration settings for the Cortical Text Processor.",
        "",
        "    All values have sensible defaults that work well for typical text corpora."
      ],
      "change_type": "add"
    },
    {
      "file": "cortical/config.py",
      "function": "class CorticalConfig:",
      "start_line": 171,
      "lines_added": [
        "        if math.isnan(self.louvain_resolution) or math.isinf(self.louvain_resolution):",
        "            raise ValueError(",
        "                f\"louvain_resolution must be a finite number, got {self.louvain_resolution}\"",
        "            )"
      ],
      "lines_removed": [],
      "context_before": [
        "",
        "        # Clustering validation",
        "        if self.min_cluster_size < 1:",
        "            raise ValueError(",
        "                f\"min_cluster_size must be at least 1, got {self.min_cluster_size}\"",
        "            )",
        "        if not (0 <= self.cluster_strictness <= 1):",
        "            raise ValueError(",
        "                f\"cluster_strictness must be between 0 and 1, got {self.cluster_strictness}\"",
        "            )"
      ],
      "context_after": [
        "        if self.louvain_resolution <= 0:",
        "            raise ValueError(",
        "                f\"louvain_resolution must be positive, got {self.louvain_resolution}\"",
        "            )",
        "        if self.louvain_resolution > 20:",
        "            import warnings",
        "            warnings.warn(",
        "                f\"louvain_resolution={self.louvain_resolution} is very high. \"",
        "                f\"This may produce hundreds of tiny clusters. \"",
        "                f\"Typical range is 1.0-10.0.\""
      ],
      "change_type": "add"
    },
    {
      "file": "docs/text-as-memories.md",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "# Text as Memories: A Knowledge Transfer Guide",
        "",
        "**Date:** 2025-12-14",
        "**Status:** Complete",
        "**Related Tasks:** Search relevance investigation (T-20251214-171301-6aa8-001)",
        "",
        "## Executive Summary",
        "",
        "This document explores how text documents can be conceptualized as **memories**—discrete units of knowledge that, when stored in git, form a persistent, versioned, and interconnected knowledge base. The Cortical Text Processor provides the algorithmic foundation for this metaphor, treating documents as \"episodes\" and terms as \"associations\" that strengthen through co-occurrence.",
        "",
        "---",
        "",
        "## The Memory Metaphor",
        "",
        "### Human Memory vs. Text Systems",
        "",
        "| Human Memory | Text in Git | Cortical Text Processor |",
        "|--------------|-------------|-------------------------|",
        "| **Encoding** | Writing a document | `process_document()` |",
        "| **Storage** | Committing to git | Minicolumns + connections |",
        "| **Consolidation** | Merging branches | PageRank identifies hubs |",
        "| **Retrieval** | Git search / blame | Query expansion + ranking |",
        "| **Association** | Related memories activate | Lateral connections fire |",
        "| **Forgetting** | Deleting files | `remove_document()` |",
        "",
        "### Why This Matters",
        "",
        "When you commit text to git, you're not just storing files—you're creating a **persistent memory system** that:",
        "",
        "1. **Tracks evolution**: Git history shows how ideas develop over time",
        "2. **Enables collaboration**: Multiple minds contribute to shared memory",
        "3. **Supports retrieval**: Semantic search finds related concepts",
        "4. **Preserves context**: Blame shows when and why knowledge was added",
        "",
        "---",
        "",
        "## The Cortical Processing Model",
        "",
        "### Four Layers of Memory",
        "",
        "The processor organizes text into hierarchical layers, inspired by how the visual cortex processes information:",
        "",
        "```",
        "Layer 3: DOCUMENTS   →  \"Episodes\" (complete experiences)",
        "         ↑",
        "Layer 2: CONCEPTS    →  \"Schemas\" (organized knowledge structures)",
        "         ↑",
        "Layer 1: BIGRAMS     →  \"Associations\" (paired ideas)",
        "         ↑",
        "Layer 0: TOKENS      →  \"Features\" (atomic elements)",
        "```",
        "",
        "**Example progression:**",
        "",
        "```",
        "Document: \"Neural networks learn patterns from data\"",
        "",
        "Layer 0 (Features):     [neural, networks, learn, patterns, data]",
        "Layer 1 (Associations): [neural networks, networks learn, learn patterns, patterns data]",
        "Layer 2 (Schema):       [machine_learning_concepts]",
        "Layer 3 (Episode):      [doc_ml_intro_001]",
        "```",
        "",
        "### Hebbian Learning: \"Neurons That Fire Together, Wire Together\"",
        "",
        "The processor applies this neuroscience principle to text:",
        "",
        "```python",
        "# When \"neural\" and \"networks\" appear together repeatedly:",
        "col_neural.lateral_connections[\"networks\"] += 1",
        "",
        "# After processing 100 documents:",
        "# \"neural\" → \"networks\": weight 87",
        "# \"neural\" → \"learning\": weight 45",
        "# \"neural\" → \"artificial\": weight 23",
        "```",
        "",
        "**Result**: Terms that co-occur form stronger associations, enabling retrieval through related concepts.",
        "",
        "---",
        "",
        "## Git as Memory Infrastructure",
        "",
        "### The Version Control Memory Model",
        "",
        "```",
        "Repository = Long-term Memory Store",
        "├── main branch = Consolidated, verified knowledge",
        "├── feature branches = Working memory (experimental ideas)",
        "├── commits = Memory encoding events",
        "├── merges = Memory consolidation",
        "└── tags = Landmark memories (releases, milestones)",
        "```",
        "",
        "### Memory Operations Mapped to Git",
        "",
        "| Memory Operation | Git Command | Effect |",
        "|-----------------|-------------|--------|",
        "| **Encode new memory** | `git add && git commit` | Store new knowledge |",
        "| **Recall** | `git log --grep` or semantic search | Retrieve by content |",
        "| **Update memory** | `git commit --amend` | Modify recent memory |",
        "| **Consolidate** | `git merge` | Integrate new with existing |",
        "| **Create association** | Cross-reference in docs | Link related memories |",
        "| **Time travel** | `git checkout <hash>` | Access past states |",
        "| **Attribution** | `git blame` | Who knew what when |",
        "",
        "### Practical Pattern: Memory Journaling",
        "",
        "Store thoughts, decisions, and learnings as versioned documents:",
        "",
        "```bash",
        "# Create a memory entry",
        "echo \"Today I learned that PageRank...\" > memories/2025-12-14-pagerank.md",
        "git add memories/",
        "git commit -m \"memory: PageRank insight from debugging session\"",
        "",
        "# Later, search your memories",
        "git log --all --grep=\"PageRank\" --oneline",
        "python scripts/search_codebase.py \"PageRank importance calculation\"",
        "```",
        "",
        "---",
        "",
        "## Building a Personal Knowledge Base",
        "",
        "### Directory Structure for Memory Storage",
        "",
        "```",
        "knowledge-base/",
        "├── memories/                    # Daily/episodic memories",
        "│   ├── 2025-12-14-security.md",
        "│   └── 2025-12-15-debugging.md",
        "├── concepts/                    # Consolidated knowledge",
        "│   ├── algorithms/",
        "│   │   ├── pagerank.md",
        "│   │   └── tfidf.md",
        "│   └── patterns/",
        "│       └── hebbian-learning.md",
        "├── decisions/                   # Architectural decisions",
        "│   └── adr-001-layer-structure.md",
        "└── corpus_dev.pkl              # Indexed for semantic search",
        "```",
        "",
        "### Indexing Your Memories",
        "",
        "```python",
        "from cortical import CorticalTextProcessor",
        "import glob",
        "",
        "# Create processor",
        "processor = CorticalTextProcessor()",
        "",
        "# Index all memories",
        "for filepath in glob.glob(\"knowledge-base/**/*.md\", recursive=True):",
        "    with open(filepath) as f:",
        "        processor.process_document(filepath, f.read())",
        "",
        "# Build connections",
        "processor.compute_all()",
        "",
        "# Save for future sessions",
        "processor.save(\"knowledge-base/corpus_dev.pkl\")",
        "```",
        "",
        "### Querying Your Knowledge",
        "",
        "```python",
        "# Load your memory index",
        "processor = CorticalTextProcessor.load(\"knowledge-base/corpus_dev.pkl\")",
        "",
        "# Semantic search",
        "results = processor.find_documents_for_query(\"debugging network issues\")",
        "for doc_id, score in results:",
        "    print(f\"{doc_id}: {score:.2f}\")",
        "",
        "# Find related concepts",
        "expanded = processor.expand_query(\"network\")",
        "# Returns: {network: 1.0, connection: 0.67, layer: 0.45, ...}",
        "```",
        "",
        "---",
        "",
        "## Memory Consolidation Patterns",
        "",
        "### Pattern 1: Daily Memory Capture",
        "",
        "```markdown",
        "<!-- memories/2025-12-14.md -->",
        "# Memory Entry: 2025-12-14",
        "",
        "## What I Learned",
        "- Fuzzing found a bug in config validation",
        "- NaN and infinity weren't being rejected",
        "",
        "## Connections",
        "- Related to: [[concepts/validation.md]]",
        "- Builds on: [[memories/2025-12-13.md]]",
        "",
        "## Future Exploration",
        "- [ ] Apply fuzzing to other validation code",
        "```",
        "",
        "### Pattern 2: Concept Consolidation",
        "",
        "When a topic appears in multiple memories, consolidate:",
        "",
        "```markdown",
        "<!-- concepts/input-validation.md -->",
        "# Input Validation",
        "",
        "## Core Principle",
        "Always validate at system boundaries.",
        "",
        "## Learned From",
        "- 2025-12-14: Fuzzing found NaN/inf bug",
        "- 2025-12-10: Path traversal prevention added",
        "",
        "## Implementation",
        "See: cortical/validation.py",
        "",
        "## Related Concepts",
        "- [[concepts/security.md]]",
        "- [[concepts/testing.md]]",
        "```",
        "",
        "### Pattern 3: Decision Records",
        "",
        "Capture **why** decisions were made:",
        "",
        "```markdown",
        "<!-- decisions/adr-003-microseconds-in-task-id.md -->",
        "# ADR-003: Add Microseconds to Task IDs",
        "",
        "## Status",
        "Accepted (2025-12-14)",
        "",
        "## Context",
        "Task IDs were colliding when generated in tight loops.",
        "Format was: T-YYYYMMDD-HHMMSS-XXXX (seconds precision)",
        "",
        "## Decision",
        "Add microseconds: T-YYYYMMDD-HHMMSSffffff-XXXX",
        "",
        "## Consequences",
        "- Pro: No more collisions in tight loops",
        "- Pro: 1,000,000x more unique IDs per second",
        "- Con: Longer IDs (6 more characters)",
        "```",
        "",
        "---",
        "",
        "## Semantic Search as Memory Recall",
        "",
        "### How Query Expansion Models Association",
        "",
        "When you search for \"neural\", the processor:",
        "",
        "1. **Activates** the \"neural\" minicolumn",
        "2. **Spreads** activation through lateral connections",
        "3. **Includes** associated terms (networks, learning, patterns)",
        "4. **Ranks** documents by cumulative relevance",
        "",
        "```python",
        "# Query: \"neural\"",
        "# Expanded: {neural: 1.0, networks: 0.87, learning: 0.45, artificial: 0.23}",
        "#",
        "# This mimics how thinking of \"neural\" naturally brings",
        "# \"networks\" and \"learning\" to mind",
        "```",
        "",
        "### Improving Recall with Metadata",
        "",
        "Add tags and links to strengthen associations:",
        "",
        "```markdown",
        "---",
        "tags: [machine-learning, neural-networks, debugging]",
        "related: [memories/2025-12-13.md, concepts/pagerank.md]",
        "---",
        "",
        "# Today's Learning",
        "",
        "The PageRank algorithm can be applied to...",
        "```",
        "",
        "---",
        "",
        "## Integration with Development Workflow",
        "",
        "### Memory-Aware Git Hooks",
        "",
        "```bash",
        "#!/bin/bash",
        "# .git/hooks/post-commit",
        "",
        "# Re-index after commits to docs/memories",
        "if git diff --cached --name-only | grep -q \"^docs/\\|^memories/\"; then",
        "    python scripts/index_codebase.py --incremental",
        "fi",
        "```",
        "",
        "### Semantic Commit Messages",
        "",
        "Treat commits as memory encoding:",
        "",
        "```bash",
        "# Bad: \"fix bug\"",
        "# Good: \"memory: discovered that TF-IDF scores need normalization\"",
        "",
        "git commit -m \"memory: fuzzing revealed NaN acceptance in config validation\"",
        "```",
        "",
        "### Branch as Working Memory",
        "",
        "```bash",
        "# Create a working memory space",
        "git checkout -b memory/exploring-pagerank",
        "",
        "# Capture discoveries",
        "echo \"# PageRank Exploration...\" > memories/pagerank-deep-dive.md",
        "git add memories/",
        "git commit -m \"memory: initial pagerank exploration\"",
        "",
        "# When ready, consolidate to main",
        "git checkout main",
        "git merge memory/exploring-pagerank",
        "```",
        "",
        "---",
        "",
        "## Benefits of Text-as-Memories",
        "",
        "1. **Searchable**: Semantic search finds related knowledge",
        "2. **Versioned**: See how understanding evolved",
        "3. **Shareable**: Collaborate on shared knowledge",
        "4. **Persistent**: Never lose insights",
        "5. **Attributable**: Know when and why you learned something",
        "6. **Interconnected**: Cross-references strengthen recall",
        "",
        "---",
        "",
        "## Getting Started",
        "",
        "1. **Create a memories directory** in your repo",
        "2. **Index with the processor**: `python scripts/index_codebase.py`",
        "3. **Search semantically**: `python scripts/search_codebase.py \"your query\"`",
        "4. **Link related concepts** with `[[wiki-style]]` references",
        "5. **Review periodically** to consolidate into concept documents",
        "",
        "---",
        "",
        "## Related Documentation",
        "",
        "- [Architecture Guide](architecture.md) - How layers work",
        "- [Algorithms Guide](algorithms.md) - PageRank, TF-IDF explained",
        "- [Dogfooding Guide](dogfooding.md) - Using the system on itself",
        "- [Query Guide](query-guide.md) - Search techniques",
        "",
        "---",
        "",
        "*\"The palest ink is better than the best memory.\" — Chinese proverb*",
        "",
        "*But versioned, indexed ink is even better.*"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "samples/decisions/adr-microseconds-task-id.md",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "# ADR-001: Add Microseconds to Task ID Generation",
        "",
        "**Status:** Accepted",
        "**Date:** 2025-12-14",
        "**Deciders:** Development team",
        "**Tags:** `task-management`, `uniqueness`, `concurrency`",
        "",
        "---",
        "",
        "## Context and Problem Statement",
        "",
        "Task IDs were generated with second-precision timestamps plus a 4-character hex suffix:",
        "",
        "```",
        "T-YYYYMMDD-HHMMSS-XXXX",
        "Example: T-20251214-163052-a1b2",
        "```",
        "",
        "During CI testing, the `test_unique_task_ids` test was intermittently failing:",
        "",
        "```",
        "AssertionError: 99 != 100",
        "```",
        "",
        "When generating 100 task IDs in a tight loop (same second), collisions occurred because:",
        "- Same timestamp for all IDs in that second",
        "- Only 4 hex chars = 65,536 possible suffixes",
        "- Birthday paradox: P(collision) ≈ 7% for 100 items from 65,536",
        "",
        "## Decision Drivers",
        "",
        "1. **Reliability**: Tests must not be flaky",
        "2. **Uniqueness**: Task IDs must be unique even under concurrent generation",
        "3. **Backwards Compatibility**: Existing task IDs shouldn't break",
        "4. **Readability**: IDs should remain human-readable",
        "",
        "## Considered Options",
        "",
        "### Option 1: Increase Random Suffix Length",
        "",
        "```",
        "T-YYYYMMDD-HHMMSS-XXXXXXXX  (8 hex chars)",
        "```",
        "",
        "**Pros:**",
        "- Simple change",
        "- 4 billion possibilities per second",
        "",
        "**Cons:**",
        "- Longer IDs",
        "- Doesn't leverage timestamp ordering",
        "",
        "### Option 2: Add Microseconds to Timestamp",
        "",
        "```",
        "T-YYYYMMDD-HHMMSSffffff-XXXX",
        "Example: T-20251214-163052123456-a1b2",
        "```",
        "",
        "**Pros:**",
        "- Timestamps remain sortable",
        "- 1 million unique timestamps per second",
        "- Combined with 4 hex suffix = practically unlimited uniqueness",
        "",
        "**Cons:**",
        "- IDs are 6 characters longer",
        "- Existing code parsing IDs needs update",
        "",
        "### Option 3: Use UUID Only",
        "",
        "```",
        "T-a1b2c3d4-e5f6-7890-abcd-ef1234567890",
        "```",
        "",
        "**Pros:**",
        "- Guaranteed uniqueness",
        "- Standard format",
        "",
        "**Cons:**",
        "- Not human-readable",
        "- Loses temporal ordering",
        "- Much longer",
        "",
        "## Decision Outcome",
        "",
        "**Chosen Option:** Option 2 - Add Microseconds to Timestamp",
        "",
        "**Rationale:**",
        "- Preserves temporal ordering (IDs sort chronologically)",
        "- Microseconds provide 1M unique slots per second",
        "- Combined with 4 hex chars: virtually collision-proof",
        "- Minimal change to existing format",
        "",
        "## Implementation",
        "",
        "```python",
        "def generate_task_id(session_id: Optional[str] = None) -> str:",
        "    now = datetime.now()",
        "    date_str = now.strftime(\"%Y%m%d\")",
        "    time_str = now.strftime(\"%H%M%S%f\")  # Added %f for microseconds",
        "    suffix = session_id or generate_session_id()",
        "    return f\"T-{date_str}-{time_str}-{suffix}\"",
        "```",
        "",
        "## Consequences",
        "",
        "### Positive",
        "- Tests no longer flaky",
        "- IDs unique even under heavy concurrent generation",
        "- Temporal ordering preserved",
        "",
        "### Negative",
        "- IDs 6 characters longer",
        "- Tests checking ID format needed update",
        "",
        "### Neutral",
        "- Existing IDs continue to work (no migration needed)",
        "- No performance impact",
        "",
        "## Validation",
        "",
        "```python",
        "# Verify uniqueness with 1000 IDs",
        "ids = {generate_task_id() for _ in range(1000)}",
        "assert len(ids) == 1000  # All unique",
        "```",
        "",
        "## Related Decisions",
        "",
        "- Task management system design (LEGACY-047)",
        "- Merge-friendly task format (docs/merge-friendly-tasks.md)",
        "",
        "---",
        "",
        "*This decision was made after a flaky test exposed the birthday paradox collision probability.*"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "samples/memories/2025-12-14-dogfooding-session.md",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "# Memory Entry: 2025-12-14 Dog-Fooding Session",
        "",
        "**Tags:** `security`, `testing`, `fuzzing`, `dog-fooding`",
        "**Related:** [[../decisions/adr-microseconds-task-id.md]], [[2025-12-13-security-review.md]]",
        "",
        "---",
        "",
        "## Context",
        "",
        "Resumed dog-fooding the Cortical Text Processor. Goal was to use the system to test itself and complete pending security tasks.",
        "",
        "## What I Learned",
        "",
        "### 1. Hypothesis Fuzzing Finds Real Bugs",
        "",
        "Property-based testing with Hypothesis discovered that `CorticalConfig` accepted `NaN` and `infinity` for `louvain_resolution`. The validation check:",
        "",
        "```python",
        "if self.louvain_resolution <= 0:  # BUG: NaN comparisons are always False!",
        "```",
        "",
        "**Fix:** Added explicit checks:",
        "```python",
        "if math.isnan(self.louvain_resolution) or math.isinf(self.louvain_resolution):",
        "    raise ValueError(...)",
        "```",
        "",
        "**Lesson:** Fuzzing with extreme values (NaN, inf, empty strings, unicode) catches edge cases that manual tests miss.",
        "",
        "### 2. Timestamp Precision Matters",
        "",
        "Task ID generation was using seconds precision:",
        "```",
        "T-20251214-163052-a1b2  # Only unique per second",
        "```",
        "",
        "When generating 100 IDs in a tight loop, collisions occurred (~7% probability via birthday paradox with 65,536 possible suffixes).",
        "",
        "**Fix:** Added microseconds:",
        "```",
        "T-20251214-163052123456-a1b2  # Unique per microsecond",
        "```",
        "",
        "### 3. Semantic Search Has Blind Spots",
        "",
        "Searching for \"security test fuzzing\" returned staleness tests instead of actual security code. The search seems to over-weight common terms like \"test\".",
        "",
        "**Created task:** T-20251214-171301-6aa8-001 to investigate.",
        "",
        "## Connections Made",
        "",
        "- **Fuzzing → Validation**: Property-based testing is essential for numeric validation",
        "- **Timestamps → Uniqueness**: Sub-second precision needed for concurrent operations",
        "- **Search → Relevance**: Domain-specific term weighting improves results",
        "",
        "## Emotional State",
        "",
        "Satisfying session. Finding a real bug through fuzzing validated the investment in security testing. The birthday paradox collision was a nice teachable moment about probability.",
        "",
        "## Future Exploration",
        "",
        "- [ ] Apply NaN/inf checks to other float config parameters",
        "- [ ] Investigate TF-IDF weighting for common programming terms",
        "- [ ] Consider security-specific synonym expansion",
        "",
        "## Artifacts Created",
        "",
        "- `tests/security/test_security.py` (22 tests)",
        "- `tests/security/test_fuzzing.py` (17 Hypothesis tests)",
        "- Task: T-20251214-171301-6aa8-001",
        "",
        "---",
        "",
        "*Committed to memory at: 2025-12-14T17:15:00Z*"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "samples/memories/2025-12-14-session-security-and-memories.md",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "# Session Knowledge Transfer: 2025-12-14 Security Testing & Memory System",
        "",
        "**Date:** 2025-12-14",
        "**Session:** Dog-fooding, security testing, and knowledge management system",
        "**Branch:** `claude/resume-dog-fooding-9RPIV`",
        "",
        "## Summary",
        "",
        "Completed security test suite (SEC-009, SEC-010) using Hypothesis fuzzing which discovered a real config validation bug. Extended the project with a comprehensive \"text-as-memories\" knowledge management system including documentation, samples, a Claude skill, and a slash command for generating knowledge transfers.",
        "",
        "## What Was Accomplished",
        "",
        "### Completed Tasks",
        "- **SEC-009**: Security-focused test suite (22 tests)",
        "- **SEC-010**: Hypothesis fuzzing tests (17 tests)",
        "- **T-20251214-171301-6aa8-001**: Created task for search relevance investigation",
        "- **T-*-002 through T-*-007**: Created 6 future tasks for memory system integration",
        "",
        "### Code Changes",
        "",
        "**New Files:**",
        "- `tests/security/__init__.py` - Security test package",
        "- `tests/security/test_security.py` - 22 security tests (path traversal, input validation, DoS prevention)",
        "- `tests/security/test_fuzzing.py` - 17 Hypothesis property-based tests",
        "- `docs/text-as-memories.md` - Knowledge management guide",
        "- `samples/memories/*.md` - Example memory documents",
        "- `samples/decisions/adr-microseconds-task-id.md` - Example ADR",
        "- `.claude/skills/memory-manager/SKILL.md` - Memory management skill",
        "- `.claude/commands/knowledge-transfer.md` - This slash command",
        "",
        "**Bug Fix:**",
        "- `cortical/config.py:182-185` - Added NaN/infinity validation for `louvain_resolution`",
        "- `scripts/task_utils.py:74` - Added microseconds to task ID to prevent collisions",
        "",
        "**Documentation Updates:**",
        "- `CLAUDE.md` - Added memory-manager skill, Text-as-Memories section",
        "- `.gitignore` - Added `.hypothesis/` directory",
        "",
        "### Test Results",
        "- 2859 tests passing",
        "- 39 security tests (22 + 17 fuzzing)",
        "- 19 skipped (optional dependencies)",
        "",
        "## Key Decisions Made",
        "",
        "| Decision | Rationale | Alternatives Considered |",
        "|----------|-----------|------------------------|",
        "| Add microseconds to task IDs | Birthday paradox caused ~7% collision rate in tight loops | Longer random suffix, UUID-only IDs |",
        "| Use Hypothesis for fuzzing | Property-based testing finds edge cases manual tests miss | Only manual security tests |",
        "| Create text-as-memories system | Institutional knowledge was being lost between sessions | Wiki, separate knowledge base tool |",
        "| Slash command for knowledge transfer | Lower friction than manual document creation | Skill-only, manual templates |",
        "",
        "## Problems Encountered & Solutions",
        "",
        "### Problem 1: Flaky Task ID Test",
        "**Symptom:** `test_unique_task_ids` failing intermittently (99 != 100)",
        "**Root Cause:** Task IDs used seconds precision; 100 IDs in same second had ~7% collision probability",
        "**Solution:** Added microseconds to timestamp (`%H%M%S%f`)",
        "**Lesson:** Birthday paradox applies to any ID generation in tight loops",
        "",
        "### Problem 2: Config Accepted NaN/Infinity",
        "**Symptom:** Hypothesis fuzzing found `CorticalConfig(louvain_resolution=nan)` was accepted",
        "**Root Cause:** Python comparison `nan <= 0` returns `False`, bypassing validation",
        "**Solution:** Added explicit `math.isnan()` and `math.isinf()` checks",
        "**Lesson:** Always fuzz numeric validation with NaN, inf, -inf",
        "",
        "### Problem 3: Pytest Not Installed",
        "**Symptom:** Smoke tests failing with ModuleNotFoundError",
        "**Solution:** `pip install pytest hypothesis`",
        "**Lesson:** Document test dependencies clearly",
        "",
        "## Technical Insights",
        "",
        "- **Fuzzing finds real bugs**: The NaN/inf bug would likely never be found by manual testing",
        "- **Semantic search has blind spots**: Searching \"security test fuzzing\" returned staleness tests - need domain-specific boosting",
        "- **Timestamps need sub-second precision** for concurrent ID generation",
        "- **Property-based tests complement unit tests**: Different failure modes discovered",
        "",
        "## Context for Next Session",
        "",
        "### Current State",
        "- All tests passing (2859 + 39 security)",
        "- Memory system fully documented and tooled",
        "- 7 future tasks created for memory integration",
        "- Branch ready for PR",
        "",
        "### Suggested Next Steps",
        "1. Merge PR for this branch",
        "2. Investigate search relevance (T-20251214-171301-6aa8-001)",
        "3. Implement memory templates CLI (T-*-002)",
        "4. Index memories in semantic search (T-*-003)",
        "",
        "### Files to Review",
        "- `docs/text-as-memories.md` - Main concept guide",
        "- `.claude/skills/memory-manager/SKILL.md` - Usage instructions",
        "- `tests/security/test_fuzzing.py` - Fuzzing patterns to reuse",
        "",
        "## Connections to Existing Knowledge",
        "",
        "- Related to: [[concept-hebbian-text-processing.md]] - How connections form",
        "- Decision record: [[adr-microseconds-task-id.md]] - Why IDs changed",
        "- Previous session: Security features were added in earlier PRs",
        "",
        "## Tags",
        "",
        "`security`, `testing`, `fuzzing`, `hypothesis`, `knowledge-management`, `documentation`, `dog-fooding`",
        "",
        "---",
        "",
        "*Generated via `/knowledge-transfer` command*"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "samples/memories/concept-hebbian-text-processing.md",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "# Concept: Hebbian Learning in Text Processing",
        "",
        "**Tags:** `algorithms`, `neuroscience`, `information-retrieval`",
        "**Consolidated From:** Multiple sessions exploring the Cortical Text Processor",
        "",
        "---",
        "",
        "## The Core Principle",
        "",
        "> \"Neurons that fire together, wire together.\" — Donald Hebb, 1949",
        "",
        "In text processing, this translates to:",
        "",
        "> **Words that appear together, link together.**",
        "",
        "## How It Works",
        "",
        "### In Biology",
        "",
        "When two neurons are active simultaneously, their synaptic connection strengthens. Over time, activating one neuron makes the other more likely to fire.",
        "",
        "### In Text Processing",
        "",
        "When two words co-occur within a context window, their connection weight increases:",
        "",
        "```python",
        "# Processing: \"neural networks process data efficiently\"",
        "# Context window: ±3 words",
        "",
        "# Result:",
        "connections[\"neural\"][\"networks\"] += 1.0   # Adjacent",
        "connections[\"neural\"][\"process\"] += 1.0    # Within window",
        "connections[\"neural\"][\"data\"] += 1.0       # Within window",
        "connections[\"networks\"][\"process\"] += 1.0",
        "# etc.",
        "```",
        "",
        "### The Accumulation Effect",
        "",
        "After processing thousands of documents:",
        "",
        "```",
        "\"neural\" connections:",
        "  → \"networks\":    weight 87  (very strong - always together)",
        "  → \"learning\":    weight 45  (often together)",
        "  → \"artificial\":  weight 23  (sometimes together)",
        "  → \"bread\":       weight 0   (never together)",
        "```",
        "",
        "## Why This Matters",
        "",
        "### 1. Query Expansion",
        "",
        "When a user searches for \"neural\", the system can suggest:",
        "- \"networks\" (strong association)",
        "- \"learning\" (moderate association)",
        "",
        "This improves recall without requiring exact matches.",
        "",
        "### 2. Semantic Similarity",
        "",
        "Documents with similar Hebbian connection patterns are semantically related, even without shared vocabulary.",
        "",
        "### 3. Knowledge Discovery",
        "",
        "Strong unexpected connections reveal insights:",
        "- \"yeast\" → \"bread\" makes sense",
        "- \"yeast\" → \"genetic research\" reveals scientific usage",
        "",
        "## Implementation in the Cortical Processor",
        "",
        "From `cortical/processor/documents.py`:",
        "",
        "```python",
        "def _build_lateral_connections(self, tokens, layer0):",
        "    \"\"\"Build Hebbian-style connections from co-occurrence.\"\"\"",
        "    window_size = 3",
        "",
        "    for i, token in enumerate(tokens):",
        "        col = layer0.get_minicolumn(token)",
        "        if not col:",
        "            continue",
        "",
        "        # Connect to tokens within window",
        "        for j in range(max(0, i - window_size),",
        "                       min(len(tokens), i + window_size + 1)):",
        "            if i != j:",
        "                other = layer0.get_minicolumn(tokens[j])",
        "                if other:",
        "                    # Strengthen the connection",
        "                    col.add_lateral_connection(other.id, weight=1.0)",
        "```",
        "",
        "## Limitations",
        "",
        "### 1. Common Word Pollution",
        "",
        "Frequent words (\"the\", \"is\", \"self\") connect to everything, creating noise.",
        "",
        "**Solution:** Stop word filtering, TF-IDF weighting",
        "",
        "### 2. O(n²) Scaling",
        "",
        "Every pair within a window creates a connection.",
        "",
        "**Solution:** Limits on connections per term (`max_bigrams_per_term`)",
        "",
        "### 3. Context Blindness",
        "",
        "\"bank\" (financial) and \"bank\" (river) create the same connections.",
        "",
        "**Solution:** Bigram layer provides some disambiguation",
        "",
        "## Related Concepts",
        "",
        "- [[pagerank.md]] - Uses connection structure to rank importance",
        "- [[tfidf.md]] - Weights terms by distinctiveness",
        "- [[louvain-clustering.md]] - Groups connected terms into concepts",
        "",
        "## Sources",
        "",
        "- Hebb, D.O. (1949). *The Organization of Behavior*",
        "- Cortical Text Processor source: `cortical/processor/documents.py:88-106`",
        "- README.md visualization of connection weights",
        "",
        "---",
        "",
        "*Consolidated: 2025-12-14*",
        "*Last updated: 2025-12-14*"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "scripts/task_utils.py",
      "function": "def generate_session_id() -> str:",
      "start_line": 53,
      "lines_added": [
        "        Task ID in format T-YYYYMMDD-HHMMSSffffff-XXXX (with microseconds)",
        "        'T-20251213-143052123456-a1b2'",
        "        'T-20251213-143052123456-test'",
        "    # Include microseconds to avoid collisions in tight loops",
        "    time_str = now.strftime(\"%H%M%S%f\")"
      ],
      "lines_removed": [
        "        Task ID in format T-YYYYMMDD-HHMMSS-XXXX",
        "        'T-20251213-143052-a1b2'",
        "        'T-20251213-143052-test'",
        "    time_str = now.strftime(\"%H%M%S\")"
      ],
      "context_before": [
        "",
        "",
        "def generate_task_id(session_id: Optional[str] = None) -> str:",
        "    \"\"\"",
        "    Generate a unique, merge-friendly task ID.",
        "",
        "    Args:",
        "        session_id: Optional session suffix. If None, generates random suffix.",
        "",
        "    Returns:"
      ],
      "context_after": [
        "",
        "    Example:",
        "        >>> generate_task_id()",
        "        >>> generate_task_id(\"test\")",
        "    \"\"\"",
        "    now = datetime.now()",
        "    date_str = now.strftime(\"%Y%m%d\")",
        "    suffix = session_id or generate_session_id()",
        "    return f\"T-{date_str}-{time_str}-{suffix}\"",
        "",
        "",
        "def generate_short_task_id() -> str:",
        "    \"\"\"",
        "    Generate a shorter unique task ID (8 hex chars).",
        "",
        "    Returns:",
        "        Task ID in format T-XXXXXXXX"
      ],
      "change_type": "modify"
    },
    {
      "file": "tasks/2025-12-14_11-15-01_41d5.json",
      "function": null,
      "start_line": 178,
      "lines_added": [
        "      \"status\": \"completed\",",
        "      \"updated_at\": \"2025-12-14T16:45:00.000000\",",
        "      \"completed_at\": \"2025-12-14T16:45:00.000000\",",
        "      \"retrospective\": {",
        "        \"notes\": \"Created tests/security/ directory with test_security.py containing 22 tests: TestPathTraversalPrevention (3 tests), TestInputValidation (6 tests), TestLargeInputHandling (5 tests), TestMaliciousInputRejection (3 tests), TestConfigurationSecurity (2 tests), TestSignatureVerificationIntegration (3 tests). All tests pass.\"",
        "      }",
        "      \"status\": \"completed\",",
        "      \"updated_at\": \"2025-12-14T16:45:00.000000\",",
        "      \"completed_at\": \"2025-12-14T16:45:00.000000\",",
        "      \"retrospective\": {",
        "        \"notes\": \"Created test_fuzzing.py with 17 Hypothesis-based fuzz tests covering: document processing, queries, config validation, persistence, tokenization, and layer operations. Fuzzing discovered a bug: CorticalConfig accepted NaN and inf for louvain_resolution. Fixed by adding math.isnan/math.isinf checks in config.py validation.\"",
        "      }"
      ],
      "lines_removed": [
        "      \"status\": \"pending\",",
        "      \"updated_at\": null,",
        "      \"completed_at\": null,",
        "      \"retrospective\": null",
        "      \"status\": \"pending\",",
        "      \"updated_at\": null,",
        "      \"completed_at\": null,",
        "      \"retrospective\": null"
      ],
      "context_before": [
        "          \"cortical/persistence.py\"",
        "        ]",
        "      },",
        "      \"retrospective\": {",
        "        \"notes\": \"Implemented in commit 90b989f\"",
        "      }",
        "    },",
        "    {",
        "      \"id\": \"T-20251214-111501-41d5-009\",",
        "      \"title\": \"SEC-009: Add security-focused test suite\","
      ],
      "context_after": [
        "      \"priority\": \"low\",",
        "      \"category\": \"security\",",
        "      \"description\": \"Create tests/security/ directory with security-focused tests.\\n\\nTest cases:\\n- Path traversal prevention in file operations\\n- Input validation on public API methods\\n- Pickle signature verification (if SEC-003 implemented)\\n- Large input handling (DoS prevention)\\n\\nEffort: 4 hours\",",
        "      \"depends_on\": [],",
        "      \"effort\": \"medium\",",
        "      \"created_at\": \"2025-12-14T11:15:01.281877\",",
        "      \"context\": {",
        "        \"files\": [",
        "          \"tests/security/\"",
        "        ]",
        "      },",
        "    },",
        "    {",
        "      \"id\": \"T-20251214-111501-41d5-010\",",
        "      \"title\": \"SEC-010: Implement input fuzzing with Hypothesis\",",
        "      \"priority\": \"low\",",
        "      \"category\": \"security\",",
        "      \"description\": \"Add property-based testing with Hypothesis for input fuzzing.\\n\\nTarget methods:\\n- process_document() with random content\\n- find_documents_for_query() with malformed queries\\n- expand_query() with edge case inputs\\n\\nInstall: pip install hypothesis\\nAdd to tests/security/test_fuzzing.py\\n\\nEffort: 8 hours\",",
        "      \"depends_on\": [],",
        "      \"effort\": \"large\",",
        "      \"created_at\": \"2025-12-14T11:15:01.281884\",",
        "      \"context\": {",
        "        \"files\": [",
        "          \"tests/security/test_fuzzing.py\"",
        "        ]",
        "      },",
        "    }",
        "  ]",
        "}"
      ],
      "change_type": "modify"
    },
    {
      "file": "tasks/2025-12-14_17-13-01_6aa8.json",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "{",
        "  \"version\": 1,",
        "  \"session_id\": \"6aa8\",",
        "  \"started_at\": \"2025-12-14T17:13:01.505357\",",
        "  \"saved_at\": \"2025-12-14T17:45:30.067738\",",
        "  \"tasks\": [",
        "    {",
        "      \"id\": \"T-20251214-171301-6aa8-001\",",
        "      \"title\": \"Investigate semantic search relevance for domain-specific queries\",",
        "      \"status\": \"pending\",",
        "      \"priority\": \"medium\",",
        "      \"category\": \"arch\",",
        "      \"description\": \"During dog-fooding, searching for 'security test fuzzing' returned staleness tests instead of actual security-related code. The search seems to weight common terms like 'test' too heavily.\\n\\nInvestigation areas:\\n- Query expansion may be pulling in too many generic terms\\n- TF-IDF weighting may not properly discount common programming terms\\n- Domain-specific boosting could improve relevance for security/testing queries\\n\\nRelated: The code_concepts.py has programming synonyms but may need security-specific terms.\\n\\nDiscovered during: Dog-fooding session 2025-12-14\",",
        "      \"depends_on\": [],",
        "      \"effort\": \"medium\",",
        "      \"created_at\": \"2025-12-14T17:13:01.505725\",",
        "      \"updated_at\": null,",
        "      \"completed_at\": null,",
        "      \"context\": {},",
        "      \"retrospective\": null",
        "    },",
        "    {",
        "      \"id\": \"T-20251214-172207-6aa8-002\",",
        "      \"title\": \"Add memory document templates and CLI\",",
        "      \"status\": \"pending\",",
        "      \"priority\": \"medium\",",
        "      \"category\": \"feature\",",
        "      \"description\": \"Create templates and CLI tooling for the text-as-memories system:\\n\\n1. Memory entry template (memories/YYYY-MM-DD-topic.md)\\n2. Decision record template (decisions/adr-NNN-title.md)\\n3. Concept consolidation template (concepts/topic.md)\\n\\nCLI commands:\\n- python scripts/new_memory.py 'What I learned today'\\n- python scripts/new_decision.py 'ADR: Choose X over Y'\\n\\nAuto-populate:\\n- Date, tags placeholder, related links section\\n- Git author info\\n- Link to recent tasks\\n\\nSee: docs/text-as-memories.md for format reference\",",
        "      \"depends_on\": [],",
        "      \"effort\": \"medium\",",
        "      \"created_at\": \"2025-12-14T17:22:07.596921\",",
        "      \"updated_at\": null,",
        "      \"completed_at\": null,",
        "      \"context\": {},",
        "      \"retrospective\": null",
        "    },",
        "    {",
        "      \"id\": \"T-20251214-172212-6aa8-003\",",
        "      \"title\": \"Index memories and decisions in semantic search\",",
        "      \"status\": \"pending\",",
        "      \"priority\": \"medium\",",
        "      \"category\": \"feature\",",
        "      \"description\": \"Extend the codebase indexer to include memory documents:\\n\\n1. Add samples/memories/ and samples/decisions/ to indexed paths\\n2. Add doc-type detection for memory vs decision vs concept docs\\n3. Apply appropriate boosting (decisions may be more authoritative)\\n4. Support [[wiki-link]] extraction for cross-references\\n\\nIntegration points:\\n- scripts/index_codebase.py: Add memory paths\\n- cortical/processor: Add memory-specific metadata\\n- scripts/search_codebase.py: Show memory type in results\\n\\nEnables: Searching across code AND institutional knowledge\",",
        "      \"depends_on\": [],",
        "      \"effort\": \"medium\",",
        "      \"created_at\": \"2025-12-14T17:22:12.010789\",",
        "      \"updated_at\": null,",
        "      \"completed_at\": null,",
        "      \"context\": {},",
        "      \"retrospective\": null",
        "    },",
        "    {",
        "      \"id\": \"T-20251214-172216-6aa8-004\",",
        "      \"title\": \"Auto-generate memory entries from completed tasks\",",
        "      \"status\": \"pending\",",
        "      \"priority\": \"low\",",
        "      \"category\": \"feature\",",
        "      \"description\": \"When a task is marked complete, offer to generate a memory entry:\\n\\n1. Extract task title, description, retrospective\\n2. Create memory document with learnings\\n3. Link to related files from task context\\n4. Add tags from task category\\n\\nIntegration:\\n- TaskSession.complete_task() could trigger this\\n- Optional flag: --create-memory\\n- Template pulls from task retrospective field\\n\\nBenefits:\\n- Captures learnings automatically\\n- Builds institutional memory from task work\\n- Creates searchable knowledge base\",",
        "      \"depends_on\": [],",
        "      \"effort\": \"medium\",",
        "      \"created_at\": \"2025-12-14T17:22:16.074170\",",
        "      \"updated_at\": null,",
        "      \"completed_at\": null,",
        "      \"context\": {},",
        "      \"retrospective\": null",
        "    },",
        "    {",
        "      \"id\": \"T-20251214-172238-6aa8-005\",",
        "      \"title\": \"Add wiki-link cross-reference resolution\",",
        "      \"status\": \"pending\",",
        "      \"priority\": \"low\",",
        "      \"category\": \"feature\",",
        "      \"description\": \"Parse and resolve [[wiki-style]] links in memory documents:\\n\\n1. Extract [[link]] patterns from markdown files\\n2. Resolve to actual file paths (fuzzy matching)\\n3. Build bidirectional link graph\\n4. Add 'backlinks' section showing what links TO a document\\n\\nUse cases:\\n- [[concepts/pagerank.md]] resolves to actual path\\n- [[2025-12-14]] finds memory entries for that date\\n- Search results show connection strength via links\\n\\nImplementation:\\n- Add link extraction to tokenizer or separate module\\n- Store links as typed_connections (relation_type='references')\\n- Query can traverse link graph for related docs\",",
        "      \"depends_on\": [],",
        "      \"effort\": \"medium\",",
        "      \"created_at\": \"2025-12-14T17:22:38.635912\",",
        "      \"updated_at\": null,",
        "      \"completed_at\": null,",
        "      \"context\": {},",
        "      \"retrospective\": null",
        "    },",
        "    {",
        "      \"id\": \"T-20251214-172243-6aa8-006\",",
        "      \"title\": \"Memory consolidation suggestions\",",
        "      \"status\": \"pending\",",
        "      \"priority\": \"low\",",
        "      \"category\": \"feature\",",
        "      \"description\": \"Analyze memories to suggest consolidation opportunities:\\n\\n1. Find memories with high term overlap (similar topics)\\n2. Identify repeated concepts across multiple entries\\n3. Suggest creating concept documents from clusters\\n4. Track 'memory age' - old unconsolidated memories\\n\\nAlgorithm:\\n- Use existing Louvain clustering on memory documents\\n- Memories in same cluster = consolidation candidates\\n- High PageRank terms across cluster = concept name\\n\\nOutput:\\n- 'These 5 memories all discuss PageRank, consider creating concepts/pagerank.md'\\n- 'Memory from 2025-12-10 has 80% overlap with 2025-12-14, merge?'\\n\\nCLI: python scripts/suggest_consolidation.py\",",
        "      \"depends_on\": [],",
        "      \"effort\": \"medium\",",
        "      \"created_at\": \"2025-12-14T17:22:43.647861\",",
        "      \"updated_at\": null,",
        "      \"completed_at\": null,",
        "      \"context\": {},",
        "      \"retrospective\": null",
        "    },",
        "    {",
        "      \"id\": \"T-20251214-172247-6aa8-007\",",
        "      \"title\": \"Session handoff memory generator\",",
        "      \"status\": \"pending\",",
        "      \"priority\": \"medium\",",
        "      \"category\": \"feature\",",
        "      \"description\": \"Generate memory documents for agent session continuity:\\n\\nWhen ending a session, create a handoff document containing:\\n1. What was accomplished (from completed tasks)\\n2. Current state (uncommitted changes, running processes)\\n3. Blockers or issues encountered\\n4. Suggested next steps\\n5. Key context the next session needs\\n\\nIntegration:\\n- Hook into session end (or manual trigger)\\n- Pull from task retrospectives\\n- Include git status summary\\n- Link to relevant code locations\\n\\nFormat: memories/session-handoff-YYYY-MM-DD-HHMM.md\\n\\nThis enables smooth continuation across agent sessions\\nand builds institutional memory of development flow.\",",
        "      \"depends_on\": [],",
        "      \"effort\": \"medium\",",
        "      \"created_at\": \"2025-12-14T17:22:47.634689\",",
        "      \"updated_at\": null,",
        "      \"completed_at\": null,",
        "      \"context\": {},",
        "      \"retrospective\": null",
        "    },",
        "    {",
        "      \"id\": \"T-20251214-173831-6aa8-008\",",
        "      \"title\": \"Make memory/decision filenames merge-safe\",",
        "      \"status\": \"pending\",",
        "      \"priority\": \"medium\",",
        "      \"category\": \"arch\",",
        "      \"description\": \"Apply the same merge-safe pattern from tasks to memories and decisions.\\n\\nCurrent patterns (can conflict):\\n- memories: YYYY-MM-DD-topic.md\\n- decisions: adr-NNN-title.md\\n\\nProposed patterns (merge-safe):\\n- memories: YYYY-MM-DD_HH-MM-SS_XXXX-topic.md\\n- decisions: adr-YYYYMMDD-HHMMSS-XXXX-title.md\\n\\nChanges needed:\\n1. Update memory-manager skill templates\\n2. Update /knowledge-transfer command\\n3. Update docs/text-as-memories.md examples\\n4. Add helper: scripts/new_memory.py with auto-generated safe names\\n5. Add helper: scripts/new_decision.py with auto-generated safe names\\n\\nAlternative: Use topic-hash suffix instead of timestamp\\nExample: 2025-12-14-dogfooding-a1b2.md\\n\\nThe session ID approach mirrors the proven task system design.\",",
        "      \"depends_on\": [],",
        "      \"effort\": \"medium\",",
        "      \"created_at\": \"2025-12-14T17:38:31.813288\",",
        "      \"updated_at\": null,",
        "      \"completed_at\": null,",
        "      \"context\": {},",
        "      \"retrospective\": null",
        "    },",
        "    {",
        "      \"id\": \"T-20251214-174107-6aa8-009\",",
        "      \"title\": \"Update README.md with comprehensive project overview\",",
        "      \"status\": \"pending\",",
        "      \"priority\": \"high\",",
        "      \"category\": \"docs\",",
        "      \"description\": \"The README.md needs to be updated to reflect current project state and be more engaging.\\n\\nCurrent issues:\\n- May be outdated with recent features\\n- Missing text-as-memories system\\n- Missing security features (HMAC signing, Bandit CI)\\n- Missing Claude skills and commands\\n\\nREADME should include:\\n1. Compelling intro with the 'visual cortex for text' metaphor\\n2. Quick start (5-line example)\\n3. Key features table with badges\\n4. Architecture diagram (ASCII or link to docs)\\n5. Installation instructions\\n6. Links to docs/quickstart.md for detailed guide\\n7. Security section (pickle warnings, HMAC signing)\\n8. Claude Code integration (skills, commands)\\n9. Contributing section\\n10. License\\n\\nMake it visually appealing with:\\n- Feature badges\\n- Code examples that actually run\\n- Clear navigation to detailed docs\\n- The 'neurons that fire together' quote\",",
        "      \"depends_on\": [],",
        "      \"effort\": \"medium\",",
        "      \"created_at\": \"2025-12-14T17:41:07.348022\",",
        "      \"updated_at\": null,",
        "      \"completed_at\": null,",
        "      \"context\": {},",
        "      \"retrospective\": null",
        "    },",
        "    {",
        "      \"id\": \"T-20251214-174112-6aa8-010\",",
        "      \"title\": \"Audit markdown files for staleness\",",
        "      \"status\": \"pending\",",
        "      \"priority\": \"medium\",",
        "      \"category\": \"docs\",",
        "      \"description\": \"Review all markdown documentation for outdated content.\\n\\nFiles to audit:\\n- README.md - main project readme\\n- CLAUDE.md - development guide\\n- CONTRIBUTING.md - contribution guide\\n- docs/*.md - all documentation files\\n- samples/**/*.md - sample documents\\n\\nCheck for:\\n1. Outdated code examples (do they still run?)\\n2. Missing new features (text-as-memories, security, skills)\\n3. Broken internal links ([[wiki-style]] and markdown links)\\n4. Incorrect file paths or line numbers\\n5. Deprecated APIs still documented\\n6. Missing new configuration options\\n7. Outdated task references\\n\\nCreate a staleness report:\\n- List files with issues\\n- Categorize by severity (broken vs outdated vs minor)\\n- Link to specific lines needing updates\\n\\nConsider adding a CI check for:\\n- Broken markdown links\\n- Code blocks that don't parse\\n- References to non-existent files\",",
        "      \"depends_on\": [],",
        "      \"effort\": \"medium\",",
        "      \"created_at\": \"2025-12-14T17:41:12.741156\",",
        "      \"updated_at\": null,",
        "      \"completed_at\": null,",
        "      \"context\": {},",
        "      \"retrospective\": null",
        "    },",
        "    {",
        "      \"id\": \"T-20251214-174116-6aa8-011\",",
        "      \"title\": \"Add markdown link checker to CI\",",
        "      \"status\": \"pending\",",
        "      \"priority\": \"low\",",
        "      \"category\": \"infra\",",
        "      \"description\": \"Add automated checking for broken links in markdown files.\\n\\nTools to consider:\\n- markdown-link-check (npm)\\n- mlc (rust-based, fast)\\n- lychee (rust-based, comprehensive)\\n\\nCI integration:\\n```yaml\\nmarkdown-lint:\\n  runs-on: ubuntu-latest\\n  steps:\\n    - uses: actions/checkout@v4\\n    - name: Check markdown links\\n      uses: gaurav-nelson/github-action-markdown-link-check@v1\\n      with:\\n        use-quiet-mode: 'yes'\\n        config-file: '.markdown-link-check.json'\\n```\\n\\nConfig file should:\\n- Ignore external URLs (or check with timeout)\\n- Check internal file references\\n- Check anchor links (#section-name)\\n- Whitelist known-good external domains\\n\\nAlso check:\\n- [[wiki-style]] links resolve to real files\\n- Code file references (cortical/foo.py:123) exist\",",
        "      \"depends_on\": [],",
        "      \"effort\": \"medium\",",
        "      \"created_at\": \"2025-12-14T17:41:16.926225\",",
        "      \"updated_at\": null,",
        "      \"completed_at\": null,",
        "      \"context\": {},",
        "      \"retrospective\": null",
        "    },",
        "    {",
        "      \"id\": \"T-20251214-174530-6aa8-012\",",
        "      \"title\": \"Enhance director orchestration with execution tracking\",",
        "      \"status\": \"pending\",",
        "      \"priority\": \"low\",",
        "      \"category\": \"feature\",",
        "      \"description\": \"Extend the director prompt with tooling support:\\n\\n1. Execution tracking file:\\n   - .claude/orchestration/current-plan.json\\n   - Track: batches, agents, status, results\\n\\n2. Verification automation:\\n   - scripts/verify_batch.py - run standard checks\\n   - Integrate with CI for parallel agent validation\\n\\n3. Replanning assistant:\\n   - Analyze failure patterns\\n   - Suggest alternative approaches\\n   - Track what was tried\\n\\n4. Metrics collection:\\n   - Time per batch\\n   - Success/failure rates\\n   - Common failure modes\\n\\n5. Integration with task system:\\n   - Auto-create tasks for each batch\\n   - Link to parent orchestration task\\n   - Capture retrospectives automatically\\n\\nSee: .claude/commands/director.md for current prompt\",",
        "      \"depends_on\": [],",
        "      \"effort\": \"medium\",",
        "      \"created_at\": \"2025-12-14T17:45:30.067633\",",
        "      \"updated_at\": null,",
        "      \"completed_at\": null,",
        "      \"context\": {},",
        "      \"retrospective\": null",
        "    }",
        "  ]",
        "}"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "tests/security/__init__.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "\"\"\"",
        "Security-focused tests for the Cortical Text Processor.",
        "",
        "SEC-009: Test cases covering:",
        "- Path traversal prevention in file operations",
        "- Input validation on public API methods",
        "- Large input handling (DoS prevention)",
        "- Signature verification (see tests/unit/test_persistence.py for comprehensive coverage)",
        "\"\"\""
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "tests/security/test_fuzzing.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "\"\"\"",
        "Property-based fuzzing tests using Hypothesis.",
        "",
        "SEC-010: Automatically generates random inputs to find edge cases",
        "and potential security issues that manual tests might miss.",
        "",
        "Target methods:",
        "- process_document() with random content",
        "- find_documents_for_query() with malformed queries",
        "- expand_query() with edge case inputs",
        "\"\"\"",
        "",
        "import os",
        "import sys",
        "import tempfile",
        "import string",
        "",
        "import pytest",
        "from hypothesis import given, settings, assume, HealthCheck",
        "from hypothesis import strategies as st",
        "",
        "# Add parent directory to path for imports",
        "sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.dirname(__file__))))",
        "",
        "from cortical import CorticalTextProcessor, CorticalConfig",
        "",
        "",
        "# Custom strategies for generating test data",
        "# Text that's likely to be valid for processing",
        "safe_text = st.text(",
        "    alphabet=string.ascii_letters + string.digits + \" .,!?'\\\"-\",",
        "    min_size=1,",
        "    max_size=10000",
        ")",
        "",
        "# Document IDs - any string that isn't empty",
        "doc_ids = st.text(min_size=1, max_size=100)",
        "",
        "# Queries - non-empty text",
        "queries = st.text(min_size=1, max_size=1000)",
        "",
        "# Unicode text including potentially problematic characters",
        "unicode_text = st.text(min_size=1, max_size=1000)",
        "",
        "# Numbers for numeric parameters",
        "positive_ints = st.integers(min_value=1, max_value=1000)",
        "non_negative_ints = st.integers(min_value=0, max_value=1000)",
        "floats_0_1 = st.floats(min_value=0.001, max_value=0.999)",
        "",
        "",
        "class TestProcessDocumentFuzzing:",
        "    \"\"\"Fuzz testing for process_document() method.\"\"\"",
        "",
        "    @given(doc_id=doc_ids, content=safe_text)",
        "    @settings(max_examples=100, suppress_health_check=[HealthCheck.too_slow])",
        "    def test_process_document_never_crashes(self, doc_id, content):",
        "        \"\"\"process_document should never crash with valid-ish inputs.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "",
        "        # Skip empty strings (known to be invalid)",
        "        assume(doc_id.strip())",
        "        assume(content.strip())",
        "",
        "        try:",
        "            processor.process_document(doc_id, content)",
        "            # Should not crash",
        "            assert True",
        "        except ValueError:",
        "            # ValueError for invalid input is acceptable",
        "            pass",
        "",
        "    @given(doc_id=doc_ids, content=unicode_text)",
        "    @settings(max_examples=50, suppress_health_check=[HealthCheck.too_slow])",
        "    def test_process_document_handles_unicode(self, doc_id, content):",
        "        \"\"\"process_document should handle arbitrary Unicode safely.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "",
        "        assume(doc_id.strip())",
        "        assume(content.strip())",
        "",
        "        try:",
        "            processor.process_document(doc_id, content)",
        "            # If successful, document should be stored",
        "            assert doc_id in processor.documents",
        "        except (ValueError, UnicodeError):",
        "            # Rejection of problematic Unicode is acceptable",
        "            pass",
        "",
        "    @given(count=st.integers(min_value=1, max_value=50))",
        "    @settings(max_examples=10, suppress_health_check=[HealthCheck.too_slow])",
        "    def test_multiple_documents_never_crash(self, count):",
        "        \"\"\"Adding multiple documents should never crash.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "",
        "        for i in range(count):",
        "            processor.process_document(f\"doc_{i}\", f\"Content for document {i}.\")",
        "",
        "        assert len(processor.documents) == count",
        "",
        "",
        "class TestQueryFuzzing:",
        "    \"\"\"Fuzz testing for query methods.\"\"\"",
        "",
        "    @given(query=queries)",
        "    @settings(max_examples=100, suppress_health_check=[HealthCheck.too_slow])",
        "    def test_find_documents_never_crashes(self, query):",
        "        \"\"\"find_documents_for_query should never crash with valid queries.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(\"doc1\", \"Test document with some content.\")",
        "        processor.compute_all()",
        "",
        "        # Skip whitespace-only queries (known to be invalid)",
        "        assume(query.strip())",
        "",
        "        try:",
        "            results = processor.find_documents_for_query(query)",
        "            # Should return a list",
        "            assert isinstance(results, list)",
        "        except ValueError:",
        "            # ValueError for invalid query is acceptable",
        "            pass",
        "",
        "    @given(query=unicode_text)",
        "    @settings(max_examples=50, suppress_health_check=[HealthCheck.too_slow])",
        "    def test_query_handles_unicode(self, query):",
        "        \"\"\"Query methods should handle arbitrary Unicode safely.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(\"doc1\", \"Test document.\")",
        "        processor.compute_all()",
        "",
        "        assume(query.strip())",
        "",
        "        try:",
        "            results = processor.find_documents_for_query(query)",
        "            assert isinstance(results, list)",
        "        except (ValueError, UnicodeError):",
        "            # Rejection is acceptable",
        "            pass",
        "",
        "    @given(top_n=st.integers(min_value=-100, max_value=1000))",
        "    @settings(max_examples=50, suppress_health_check=[HealthCheck.too_slow])",
        "    def test_top_n_boundaries(self, top_n):",
        "        \"\"\"top_n parameter should handle boundary values safely.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(\"doc1\", \"Test document.\")",
        "        processor.compute_all()",
        "",
        "        try:",
        "            if top_n <= 0:",
        "                # Should reject non-positive values",
        "                with pytest.raises(ValueError):",
        "                    processor.find_documents_for_query(\"test\", top_n=top_n)",
        "            else:",
        "                results = processor.find_documents_for_query(\"test\", top_n=top_n)",
        "                assert len(results) <= top_n",
        "        except (ValueError, OverflowError):",
        "            # These are acceptable responses to boundary values",
        "            pass",
        "",
        "",
        "class TestExpandQueryFuzzing:",
        "    \"\"\"Fuzz testing for expand_query() method.\"\"\"",
        "",
        "    @given(query=queries)",
        "    @settings(max_examples=100, suppress_health_check=[HealthCheck.too_slow])",
        "    def test_expand_query_never_crashes(self, query):",
        "        \"\"\"expand_query should never crash with arbitrary queries.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(\"doc1\", \"Neural networks process data efficiently.\")",
        "        processor.compute_all()",
        "",
        "        assume(query.strip())",
        "",
        "        try:",
        "            expanded = processor.expand_query(query)",
        "            # Should return a dict",
        "            assert isinstance(expanded, dict)",
        "        except ValueError:",
        "            # Rejection is acceptable",
        "            pass",
        "",
        "    @given(max_expansions=st.integers(min_value=-10, max_value=100))",
        "    @settings(max_examples=50, suppress_health_check=[HealthCheck.too_slow])",
        "    def test_max_expansions_boundaries(self, max_expansions):",
        "        \"\"\"max_expansions parameter should handle boundary values.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(\"doc1\", \"Neural networks machine learning.\")",
        "        processor.compute_all()",
        "",
        "        try:",
        "            if max_expansions <= 0:",
        "                # May reject or return minimal results",
        "                expanded = processor.expand_query(\"neural\", max_expansions=max_expansions)",
        "                # If it doesn't reject, should return limited results",
        "                assert len(expanded) <= max(0, max_expansions) or len(expanded) >= 1",
        "            else:",
        "                expanded = processor.expand_query(\"neural\", max_expansions=max_expansions)",
        "                assert len(expanded) <= max_expansions + 10  # Some tolerance",
        "        except ValueError:",
        "            # Rejection is acceptable",
        "            pass",
        "",
        "",
        "class TestConfigFuzzing:",
        "    \"\"\"Fuzz testing for configuration validation.\"\"\"",
        "",
        "    @given(damping=st.floats(allow_nan=True, allow_infinity=True))",
        "    @settings(max_examples=100)",
        "    def test_pagerank_damping_validation(self, damping):",
        "        \"\"\"pagerank_damping should reject invalid values.\"\"\"",
        "        import math",
        "",
        "        try:",
        "            config = CorticalConfig(pagerank_damping=damping)",
        "            # If it accepted, must be valid",
        "            assert 0 < damping < 1",
        "            assert not math.isnan(damping)",
        "            assert not math.isinf(damping)",
        "        except (ValueError, TypeError):",
        "            # Rejection is expected for invalid values",
        "            pass",
        "",
        "    @given(resolution=st.floats(allow_nan=True, allow_infinity=True))",
        "    @settings(max_examples=100)",
        "    def test_louvain_resolution_validation(self, resolution):",
        "        \"\"\"louvain_resolution should reject invalid values.\"\"\"",
        "        import math",
        "        import warnings",
        "",
        "        try:",
        "            with warnings.catch_warnings():",
        "                warnings.simplefilter(\"ignore\")  # High values trigger warnings",
        "                config = CorticalConfig(louvain_resolution=resolution)",
        "            # If it accepted, must be valid (positive, finite)",
        "            assert resolution > 0",
        "            assert not math.isnan(resolution)",
        "            assert not math.isinf(resolution)",
        "        except (ValueError, TypeError):",
        "            # Rejection is expected for invalid values",
        "            pass",
        "",
        "    @given(iterations=st.integers(min_value=-1000, max_value=1000))",
        "    @settings(max_examples=100)",
        "    def test_pagerank_iterations_validation(self, iterations):",
        "        \"\"\"pagerank_iterations should reject invalid values.\"\"\"",
        "        try:",
        "            config = CorticalConfig(pagerank_iterations=iterations)",
        "            # If it accepted, must be valid",
        "            assert iterations >= 1",
        "        except ValueError:",
        "            # Rejection is expected for invalid values",
        "            pass",
        "",
        "",
        "class TestPersistenceFuzzing:",
        "    \"\"\"Fuzz testing for save/load operations.\"\"\"",
        "",
        "    @given(content=safe_text)",
        "    @settings(max_examples=20, suppress_health_check=[HealthCheck.too_slow])",
        "    def test_save_load_roundtrip(self, content):",
        "        \"\"\"Save/load roundtrip should preserve data.\"\"\"",
        "        assume(content.strip())",
        "",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(\"doc1\", content)",
        "",
        "        with tempfile.TemporaryDirectory() as tmpdir:",
        "            path = os.path.join(tmpdir, \"test.pkl\")",
        "            processor.save(path)",
        "",
        "            loaded = CorticalTextProcessor.load(path)",
        "            assert loaded.documents.get(\"doc1\") == content",
        "",
        "    @given(key=st.binary(min_size=16, max_size=64))",
        "    @settings(max_examples=20, suppress_health_check=[HealthCheck.too_slow])",
        "    def test_signed_save_load_with_random_keys(self, key):",
        "        \"\"\"Signed save/load should work with random keys.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(\"doc1\", \"Test content.\")",
        "",
        "        with tempfile.TemporaryDirectory() as tmpdir:",
        "            path = os.path.join(tmpdir, \"test.pkl\")",
        "            processor.save(path, signing_key=key)",
        "",
        "            loaded = CorticalTextProcessor.load(path, verify_key=key)",
        "            assert \"doc1\" in loaded.documents",
        "",
        "",
        "class TestTokenizerFuzzing:",
        "    \"\"\"Fuzz testing for tokenization.\"\"\"",
        "",
        "    @given(text=unicode_text)",
        "    @settings(max_examples=100, suppress_health_check=[HealthCheck.too_slow])",
        "    def test_tokenize_never_crashes(self, text):",
        "        \"\"\"Tokenization should never crash with arbitrary input.\"\"\"",
        "        from cortical import Tokenizer",
        "",
        "        tokenizer = Tokenizer()",
        "",
        "        try:",
        "            tokens = tokenizer.tokenize(text)",
        "            # Should return a list",
        "            assert isinstance(tokens, list)",
        "            # All tokens should be strings",
        "            assert all(isinstance(t, str) for t in tokens)",
        "        except (ValueError, UnicodeError):",
        "            # Rejection is acceptable",
        "            pass",
        "",
        "    @given(text=st.text(min_size=0, max_size=100))",
        "    @settings(max_examples=100)",
        "    def test_extract_ngrams_never_crashes(self, text):",
        "        \"\"\"N-gram extraction should never crash.\"\"\"",
        "        from cortical import Tokenizer",
        "",
        "        tokenizer = Tokenizer()",
        "",
        "        try:",
        "            # First tokenize, then extract ngrams",
        "            tokens = tokenizer.tokenize(text)",
        "            if tokens:  # Only extract ngrams if there are tokens",
        "                bigrams = tokenizer.extract_ngrams(tokens, n=2)",
        "                # Should return a list",
        "                assert isinstance(bigrams, list)",
        "        except (ValueError, UnicodeError):",
        "            # Rejection is acceptable",
        "            pass",
        "",
        "",
        "class TestLayerFuzzing:",
        "    \"\"\"Fuzz testing for layer operations.\"\"\"",
        "",
        "    @given(term=st.text(min_size=1, max_size=100))",
        "    @settings(max_examples=100)",
        "    def test_get_minicolumn_never_crashes(self, term):",
        "        \"\"\"get_minicolumn should never crash with arbitrary terms.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(\"doc1\", \"Test content.\")",
        "",
        "        from cortical import CorticalLayer",
        "",
        "        layer = processor.layers[CorticalLayer.TOKENS]",
        "",
        "        # Should return None for missing terms, never crash",
        "        result = layer.get_minicolumn(term)",
        "        assert result is None or hasattr(result, 'content')",
        "",
        "    @given(col_id=st.text(min_size=1, max_size=100))",
        "    @settings(max_examples=100)",
        "    def test_get_by_id_never_crashes(self, col_id):",
        "        \"\"\"get_by_id should never crash with arbitrary IDs.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(\"doc1\", \"Test content.\")",
        "",
        "        from cortical import CorticalLayer",
        "",
        "        layer = processor.layers[CorticalLayer.TOKENS]",
        "",
        "        # Should return None for missing IDs, never crash",
        "        result = layer.get_by_id(col_id)",
        "        assert result is None or hasattr(result, 'id')",
        "",
        "",
        "if __name__ == \"__main__\":",
        "    pytest.main([__file__, \"-v\", \"--hypothesis-show-statistics\"])"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "tests/security/test_security.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "\"\"\"",
        "Security-focused test suite for the Cortical Text Processor.",
        "",
        "SEC-009: Comprehensive security tests covering:",
        "- Path traversal prevention",
        "- Input validation",
        "- Large input handling (DoS prevention)",
        "- Malicious input rejection",
        "\"\"\"",
        "",
        "import os",
        "import sys",
        "import tempfile",
        "import unittest",
        "from unittest.mock import patch, MagicMock",
        "",
        "import pytest",
        "",
        "# Add parent directory to path for imports",
        "sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.dirname(__file__))))",
        "",
        "from cortical import CorticalTextProcessor, CorticalConfig",
        "from cortical.persistence import (",
        "    save_processor,",
        "    load_processor,",
        "    SignatureVerificationError,",
        ")",
        "from cortical.validation import (",
        "    validate_non_empty_string,",
        "    validate_positive_int,",
        "    validate_range,",
        ")",
        "",
        "",
        "class TestPathTraversalPrevention:",
        "    \"\"\"",
        "    Test that file operations prevent path traversal attacks.",
        "",
        "    Path traversal attacks attempt to access files outside the intended",
        "    directory by using sequences like '../' or absolute paths.",
        "    \"\"\"",
        "",
        "    def test_save_rejects_path_traversal_sequences(self):",
        "        \"\"\"Save should not allow path traversal in filenames.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(\"test\", \"Test content\")",
        "",
        "        # Attempting path traversal with ../ should either:",
        "        # 1. Fail (if the system prevents it)",
        "        # 2. Create the file in a safe location (normalized path)",
        "        with tempfile.TemporaryDirectory() as tmpdir:",
        "            # Try to escape the directory",
        "            malicious_path = os.path.join(tmpdir, \"..\", \"escaped.pkl\")",
        "",
        "            # This should either raise an error or normalize the path",
        "            # The important thing is it shouldn't create files outside tmpdir parent",
        "            try:",
        "                processor.save(malicious_path)",
        "                # If it succeeded, verify the file is in a reasonable location",
        "                # (os.path normalizes the path, so \"../escaped.pkl\" becomes parent/escaped.pkl)",
        "                # This is acceptable as long as we document the behavior",
        "                assert os.path.exists(malicious_path) or os.path.exists(",
        "                    os.path.normpath(malicious_path)",
        "                )",
        "            except (OSError, ValueError):",
        "                # Rejection is also acceptable",
        "                pass",
        "",
        "    def test_save_handles_absolute_paths_safely(self):",
        "        \"\"\"Save with absolute path should work normally.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(\"test\", \"Test content\")",
        "",
        "        with tempfile.TemporaryDirectory() as tmpdir:",
        "            abs_path = os.path.join(tmpdir, \"test.pkl\")",
        "            processor.save(abs_path)",
        "            assert os.path.exists(abs_path)",
        "",
        "    def test_document_id_with_path_characters(self):",
        "        \"\"\"Document IDs with path-like characters should be sanitized or rejected.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "",
        "        # These should either be accepted (if IDs are just keys, not files)",
        "        # or rejected if they could cause security issues",
        "        suspicious_ids = [",
        "            \"../etc/passwd\",",
        "            \"/etc/passwd\",",
        "            \"..\\\\..\\\\windows\\\\system32\",",
        "            \"doc|id\",  # Pipe character",
        "            \"doc\\x00id\",  # Null byte",
        "        ]",
        "",
        "        for doc_id in suspicious_ids:",
        "            try:",
        "                processor.process_document(doc_id, \"Test content\")",
        "                # If accepted, verify it's stored as-is (just a dict key)",
        "                # This is acceptable since doc_ids are not used as filenames",
        "                assert doc_id in processor.documents",
        "            except (ValueError, KeyError):",
        "                # Rejection is also acceptable",
        "                pass",
        "",
        "",
        "class TestInputValidation:",
        "    \"\"\"",
        "    Test input validation on public API methods.",
        "",
        "    Public APIs should validate inputs to prevent:",
        "    - Type confusion attacks",
        "    - Buffer overflow-like attacks (very large inputs)",
        "    - Injection attacks",
        "    \"\"\"",
        "",
        "    def test_empty_query_rejected(self):",
        "        \"\"\"Empty queries should be rejected.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(\"doc1\", \"Test content\")",
        "        processor.compute_all()",
        "",
        "        with pytest.raises(ValueError):",
        "            processor.find_documents_for_query(\"\")",
        "",
        "        with pytest.raises(ValueError):",
        "            processor.find_documents_for_query(\"   \")  # Whitespace only",
        "",
        "    def test_none_query_rejected(self):",
        "        \"\"\"None query should be rejected.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(\"doc1\", \"Test content\")",
        "        processor.compute_all()",
        "",
        "        with pytest.raises((ValueError, TypeError, AttributeError)):",
        "            processor.find_documents_for_query(None)",
        "",
        "    def test_invalid_top_n_rejected(self):",
        "        \"\"\"Invalid top_n values should be rejected.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(\"doc1\", \"Test content\")",
        "        processor.compute_all()",
        "",
        "        with pytest.raises(ValueError):",
        "            processor.find_documents_for_query(\"test\", top_n=0)",
        "",
        "        with pytest.raises(ValueError):",
        "            processor.find_documents_for_query(\"test\", top_n=-1)",
        "",
        "    def test_non_string_document_rejected(self):",
        "        \"\"\"Non-string document content should be rejected.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "",
        "        with pytest.raises((ValueError, TypeError, AttributeError)):",
        "            processor.process_document(\"doc1\", None)",
        "",
        "        with pytest.raises((ValueError, TypeError, AttributeError)):",
        "            processor.process_document(\"doc1\", 12345)",
        "",
        "        with pytest.raises((ValueError, TypeError, AttributeError)):",
        "            processor.process_document(\"doc1\", [\"list\", \"of\", \"words\"])",
        "",
        "    def test_empty_document_id_rejected(self):",
        "        \"\"\"Empty document IDs should be rejected.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "",
        "        with pytest.raises(ValueError):",
        "            processor.process_document(\"\", \"Test content\")",
        "",
        "    def test_validation_decorators(self):",
        "        \"\"\"Test validation utility functions.\"\"\"",
        "        # validate_non_empty_string",
        "        with pytest.raises(ValueError):",
        "            validate_non_empty_string(\"\", \"test\")",
        "",
        "        with pytest.raises(ValueError):",
        "            validate_non_empty_string(None, \"test\")",
        "",
        "        with pytest.raises(ValueError):",
        "            validate_non_empty_string(123, \"test\")",
        "",
        "        # Should not raise",
        "        validate_non_empty_string(\"valid\", \"test\")",
        "",
        "        # validate_positive_int",
        "        with pytest.raises(ValueError):",
        "            validate_positive_int(0, \"test\")",
        "",
        "        with pytest.raises(ValueError):",
        "            validate_positive_int(-1, \"test\")",
        "",
        "        with pytest.raises(ValueError):",
        "            validate_positive_int(1.5, \"test\")",
        "",
        "        # Should not raise",
        "        validate_positive_int(1, \"test\")",
        "",
        "        # validate_range",
        "        with pytest.raises(ValueError):",
        "            validate_range(1.5, \"test\", min_val=0.0, max_val=1.0)",
        "",
        "        with pytest.raises(ValueError):",
        "            validate_range(-0.1, \"test\", min_val=0.0)",
        "",
        "",
        "class TestLargeInputHandling:",
        "    \"\"\"",
        "    Test handling of very large inputs (DoS prevention).",
        "",
        "    Large inputs should either:",
        "    - Be processed reasonably",
        "    - Be rejected with clear limits",
        "    - Not cause crashes or excessive memory usage",
        "    \"\"\"",
        "",
        "    def test_very_large_document_handled(self):",
        "        \"\"\"Very large documents should be handled without crashing.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "",
        "        # Create a large document (100KB of text)",
        "        large_text = \"test word \" * 10000",
        "",
        "        # Should not crash",
        "        processor.process_document(\"large_doc\", large_text)",
        "",
        "        # Should have processed tokens",
        "        assert processor.layers is not None",
        "",
        "    def test_many_documents_handled(self):",
        "        \"\"\"Many documents should be handled without crashing.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "",
        "        # Add 100 documents",
        "        for i in range(100):",
        "            processor.process_document(f\"doc_{i}\", f\"Document number {i} with some content.\")",
        "",
        "        # Should have all documents",
        "        assert len(processor.documents) == 100",
        "",
        "    def test_very_long_query_handled(self):",
        "        \"\"\"Very long queries should be handled without crashing.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(\"doc1\", \"Test content for searching.\")",
        "        processor.compute_all()",
        "",
        "        # Create a long query (1000 words)",
        "        long_query = \" \".join([\"word\"] * 1000)",
        "",
        "        # Should not crash (may return empty results, but shouldn't crash)",
        "        try:",
        "            results = processor.find_documents_for_query(long_query)",
        "            assert isinstance(results, list)",
        "        except ValueError:",
        "            # Rejecting very long queries is also acceptable",
        "            pass",
        "",
        "    def test_repeated_same_document_id(self):",
        "        \"\"\"Processing the same document ID multiple times should be handled.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "",
        "        # Process the same document 100 times",
        "        for i in range(100):",
        "            processor.process_document(\"doc1\", f\"Version {i} of the document.\")",
        "",
        "        # Should only have one document (latest version)",
        "        assert len(processor.documents) == 1",
        "        assert \"Version 99\" in processor.documents[\"doc1\"]",
        "",
        "    def test_unicode_document_handling(self):",
        "        \"\"\"Unicode characters in documents should be handled safely.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "",
        "        # Various Unicode test cases",
        "        unicode_tests = [",
        "            \"Hello 世界 🌍\",  # Chinese + emoji",
        "            \"مرحبا بالعالم\",  # Arabic (RTL)",
        "            \"Привет мир\",  # Cyrillic",
        "            \"\\u0000\\u0001\\u0002\",  # Control characters",
        "            \"a\" * 10000 + \"🔥\",  # Long string with emoji",
        "        ]",
        "",
        "        for i, text in enumerate(unicode_tests):",
        "            # Should not crash",
        "            try:",
        "                processor.process_document(f\"unicode_{i}\", text)",
        "            except (ValueError, UnicodeError):",
        "                # Rejection of invalid Unicode is acceptable",
        "                pass",
        "",
        "",
        "class TestMaliciousInputRejection:",
        "    \"\"\"",
        "    Test rejection of potentially malicious inputs.",
        "",
        "    This includes:",
        "    - SQL injection-like patterns (shouldn't affect us, but test anyway)",
        "    - Script injection patterns",
        "    - Format string attacks",
        "    \"\"\"",
        "",
        "    def test_script_like_content_safe(self):",
        "        \"\"\"Script-like content should be stored safely as text.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "",
        "        script_content = \"\"\"",
        "        <script>alert('xss')</script>",
        "        <?php system('whoami'); ?>",
        "        $(rm -rf /)",
        "        \"\"\"",
        "",
        "        # Should be stored as plain text",
        "        processor.process_document(\"script_doc\", script_content)",
        "        assert processor.documents[\"script_doc\"] == script_content",
        "",
        "    def test_sql_like_content_safe(self):",
        "        \"\"\"SQL-like content should be stored safely as text.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "",
        "        sql_content = \"'; DROP TABLE users; --\"",
        "",
        "        # Should be stored as plain text (we don't use SQL)",
        "        processor.process_document(\"sql_doc\", sql_content)",
        "        assert processor.documents[\"sql_doc\"] == sql_content",
        "",
        "    def test_format_string_content_safe(self):",
        "        \"\"\"Format string patterns should be stored safely.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "",
        "        format_content = \"%s%s%s%n%n%n{0}{1}{2}\"",
        "",
        "        # Should be stored as plain text",
        "        processor.process_document(\"format_doc\", format_content)",
        "        assert processor.documents[\"format_doc\"] == format_content",
        "",
        "",
        "class TestConfigurationSecurity:",
        "    \"\"\"Test security aspects of configuration handling.\"\"\"",
        "",
        "    def test_config_rejects_invalid_ranges(self):",
        "        \"\"\"Configuration should reject values outside valid ranges.\"\"\"",
        "        # pagerank_damping should be in (0, 1) exclusive",
        "        with pytest.raises(ValueError):",
        "            CorticalConfig(pagerank_damping=0.0)",
        "",
        "        with pytest.raises(ValueError):",
        "            CorticalConfig(pagerank_damping=1.0)",
        "",
        "        with pytest.raises(ValueError):",
        "            CorticalConfig(pagerank_damping=-0.5)",
        "",
        "        with pytest.raises(ValueError):",
        "            CorticalConfig(pagerank_damping=1.5)",
        "",
        "    def test_config_rejects_negative_counts(self):",
        "        \"\"\"Configuration should reject negative count values.\"\"\"",
        "        with pytest.raises(ValueError):",
        "            CorticalConfig(pagerank_iterations=-1)",
        "",
        "        with pytest.raises(ValueError):",
        "            CorticalConfig(louvain_resolution=-1.0)",
        "",
        "",
        "class TestSignatureVerificationIntegration:",
        "    \"\"\"",
        "    Integration tests for signature verification.",
        "",
        "    Note: Unit tests are in tests/unit/test_persistence.py.",
        "    These tests verify the full save/load workflow.",
        "    \"\"\"",
        "",
        "    def test_signed_save_load_roundtrip(self):",
        "        \"\"\"Signed files should load correctly with the right key.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(\"doc1\", \"Test content\")",
        "",
        "        key = b\"test_secret_key_32bytes_minimum!\"",
        "",
        "        with tempfile.TemporaryDirectory() as tmpdir:",
        "            path = os.path.join(tmpdir, \"signed.pkl\")",
        "",
        "            # Save with signature",
        "            processor.save(path, signing_key=key)",
        "",
        "            # Should have created signature file",
        "            assert os.path.exists(f\"{path}.sig\")",
        "",
        "            # Load with verification",
        "            loaded = CorticalTextProcessor.load(path, verify_key=key)",
        "            assert \"doc1\" in loaded.documents",
        "",
        "    def test_tampered_file_rejected(self):",
        "        \"\"\"Tampered files should be rejected.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(\"doc1\", \"Test content\")",
        "",
        "        key = b\"test_secret_key_32bytes_minimum!\"",
        "",
        "        with tempfile.TemporaryDirectory() as tmpdir:",
        "            path = os.path.join(tmpdir, \"signed.pkl\")",
        "",
        "            # Save with signature",
        "            processor.save(path, signing_key=key)",
        "",
        "            # Tamper with the file",
        "            with open(path, 'ab') as f:",
        "                f.write(b\"tampered\")",
        "",
        "            # Load should fail",
        "            with pytest.raises(SignatureVerificationError):",
        "                CorticalTextProcessor.load(path, verify_key=key)",
        "",
        "    def test_wrong_key_rejected(self):",
        "        \"\"\"Loading with wrong key should be rejected.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(\"doc1\", \"Test content\")",
        "",
        "        key1 = b\"correct_key_here_32bytes_minimum\"",
        "        key2 = b\"wrong_key_here_32_bytes_minimum!\"",
        "",
        "        with tempfile.TemporaryDirectory() as tmpdir:",
        "            path = os.path.join(tmpdir, \"signed.pkl\")",
        "",
        "            # Save with key1",
        "            processor.save(path, signing_key=key1)",
        "",
        "            # Load with key2 should fail",
        "            with pytest.raises(SignatureVerificationError):",
        "                CorticalTextProcessor.load(path, verify_key=key2)",
        "",
        "",
        "if __name__ == \"__main__\":",
        "    pytest.main([__file__, \"-v\"])"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "tests/unit/test_task_utils.py",
      "function": "from task_utils import (",
      "start_line": 22,
      "lines_added": [
        "        # Format: T-YYYYMMDD-HHMMSSffffff-XXXX (with microseconds)",
        "        self.assertEqual(len(parts[2]), 12)  # HHMMSSffffff (with microseconds)"
      ],
      "lines_removed": [
        "        # Format: T-YYYYMMDD-HHMMSS-XXXX",
        "        self.assertEqual(len(parts[2]), 6)  # HHMMSS"
      ],
      "context_before": [
        "    consolidate_tasks,",
        ")",
        "",
        "",
        "class TestTaskIdGeneration(unittest.TestCase):",
        "    \"\"\"Tests for task ID generation.\"\"\"",
        "",
        "    def test_generate_task_id_format(self):",
        "        \"\"\"Task ID should have correct format.\"\"\"",
        "        task_id = generate_task_id()"
      ],
      "context_after": [
        "        self.assertTrue(task_id.startswith(\"T-\"))",
        "        parts = task_id.split(\"-\")",
        "        self.assertEqual(len(parts), 4)",
        "        self.assertEqual(len(parts[1]), 8)  # YYYYMMDD",
        "        self.assertEqual(len(parts[3]), 4)  # session suffix",
        "",
        "    def test_generate_task_id_with_session(self):",
        "        \"\"\"Task ID should use provided session suffix.\"\"\"",
        "        task_id = generate_task_id(\"test\")",
        "        self.assertTrue(task_id.endswith(\"-test\"))",
        "",
        "    def test_generate_short_task_id(self):",
        "        \"\"\"Short task ID should be 10 characters.\"\"\"",
        "        task_id = generate_short_task_id()"
      ],
      "change_type": "modify"
    }
  ],
  "hour_of_day": 17,
  "day_of_week": "Sunday",
  "seconds_since_last_commit": -71802,
  "is_merge": true,
  "is_initial": false,
  "parent_count": 2,
  "session_id": null,
  "related_chats": [],
  "ci_result": null,
  "reverted": false,
  "amended": false
}