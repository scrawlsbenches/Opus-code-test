{
  "hash": "a819131dcf247566556cd47afee267df9929607b",
  "message": "Add LRU cache for query expansion results (Task #45)",
  "author": "Claude",
  "timestamp": "2025-12-10 15:01:30 +0000",
  "branch": "claude/multi-index-design-DvifZ",
  "files_changed": [
    "cortical/processor.py",
    "tests/test_processor.py"
  ],
  "insertions": 181,
  "deletions": 0,
  "hunks": [
    {
      "file": "cortical/processor.py",
      "function": "class CorticalTextProcessor:",
      "start_line": 40,
      "lines_added": [
        "        # LRU cache for query expansion results",
        "        self._query_expansion_cache: Dict[str, Dict[str, float]] = {}",
        "        self._query_cache_max_size: int = 100"
      ],
      "lines_removed": [],
      "context_before": [
        "            CorticalLayer.BIGRAMS: HierarchicalLayer(CorticalLayer.BIGRAMS),",
        "            CorticalLayer.CONCEPTS: HierarchicalLayer(CorticalLayer.CONCEPTS),",
        "            CorticalLayer.DOCUMENTS: HierarchicalLayer(CorticalLayer.DOCUMENTS),",
        "        }",
        "        self.documents: Dict[str, str] = {}",
        "        self.document_metadata: Dict[str, Dict[str, Any]] = {}",
        "        self.embeddings: Dict[str, List[float]] = {}",
        "        self.semantic_relations: List[Tuple[str, str, str, float]] = []",
        "        # Track which computations are stale and need recomputation",
        "        self._stale_computations: set = set()"
      ],
      "context_after": [
        "",
        "    def process_document(",
        "        self,",
        "        doc_id: str,",
        "        content: str,",
        "        metadata: Optional[Dict[str, Any]] = None",
        "    ) -> Dict[str, int]:",
        "        \"\"\"",
        "        Process a document and add it to the corpus.",
        ""
      ],
      "change_type": "add"
    },
    {
      "file": "cortical/processor.py",
      "function": "class CorticalTextProcessor:",
      "start_line": 576,
      "lines_added": [
        "",
        "        # Invalidate query cache since corpus state changed",
        "        self._query_expansion_cache.clear()",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "        fresh_comps = [",
        "            self.COMP_ACTIVATION,",
        "            self.COMP_PAGERANK,",
        "            self.COMP_TFIDF,",
        "            self.COMP_DOC_CONNECTIONS,",
        "            self.COMP_BIGRAM_CONNECTIONS,",
        "        ]",
        "        if build_concepts:",
        "            fresh_comps.append(self.COMP_CONCEPTS)",
        "        self._mark_fresh(*fresh_comps)"
      ],
      "context_after": [
        "        if verbose:",
        "            print(\"Done.\")",
        "",
        "        return stats",
        "    ",
        "    def propagate_activation(self, iterations: int = 3, decay: float = 0.8, verbose: bool = True) -> None:",
        "        analysis.propagate_activation(self.layers, iterations, decay)",
        "        if verbose: print(f\"Propagated activation ({iterations} iterations)\")",
        "    ",
        "    def compute_importance(self, verbose: bool = True) -> None:"
      ],
      "change_type": "add"
    },
    {
      "file": "cortical/processor.py",
      "function": "class CorticalTextProcessor:",
      "start_line": 1139,
      "lines_added": [
        "    def expand_query_cached(",
        "        self,",
        "        query_text: str,",
        "        max_expansions: int = 10,",
        "        use_variants: bool = True,",
        "        use_code_concepts: bool = False",
        "    ) -> Dict[str, float]:",
        "        \"\"\"",
        "        Expand a query with caching for faster repeated lookups.",
        "",
        "        Uses an LRU-style cache to avoid recomputing expansion for",
        "        frequently repeated queries. Useful in RAG loops where the",
        "        same queries may be issued multiple times.",
        "",
        "        Args:",
        "            query_text: Original query string",
        "            max_expansions: Maximum expansion terms to add",
        "            use_variants: Try word variants when direct match fails",
        "            use_code_concepts: Include programming synonym expansions",
        "",
        "        Returns:",
        "            Dict mapping terms to weights",
        "        \"\"\"",
        "        # Create cache key from parameters",
        "        cache_key = f\"{query_text}|{max_expansions}|{use_variants}|{use_code_concepts}\"",
        "",
        "        # Check cache",
        "        if cache_key in self._query_expansion_cache:",
        "            return self._query_expansion_cache[cache_key].copy()",
        "",
        "        # Compute expansion",
        "        result = query_module.expand_query(",
        "            query_text,",
        "            self.layers,",
        "            self.tokenizer,",
        "            max_expansions=max_expansions,",
        "            use_variants=use_variants,",
        "            use_code_concepts=use_code_concepts",
        "        )",
        "",
        "        # Add to cache (with LRU eviction if at max size)",
        "        if len(self._query_expansion_cache) >= self._query_cache_max_size:",
        "            # Remove oldest entry (first key in dict - approximates LRU)",
        "            oldest_key = next(iter(self._query_expansion_cache))",
        "            del self._query_expansion_cache[oldest_key]",
        "",
        "        self._query_expansion_cache[cache_key] = result.copy()",
        "        return result",
        "",
        "    def clear_query_cache(self) -> int:",
        "        \"\"\"",
        "        Clear the query expansion cache.",
        "",
        "        Should be called after modifying the corpus (adding documents,",
        "        recomputing connections) to ensure fresh expansions.",
        "",
        "        Returns:",
        "            Number of cache entries cleared",
        "        \"\"\"",
        "        count = len(self._query_expansion_cache)",
        "        self._query_expansion_cache.clear()",
        "        return count",
        "",
        "    def set_query_cache_size(self, max_size: int) -> None:",
        "        \"\"\"",
        "        Set the maximum size of the query expansion cache.",
        "",
        "        Args:",
        "            max_size: Maximum number of queries to cache (must be > 0)",
        "",
        "        Raises:",
        "            ValueError: If max_size <= 0",
        "        \"\"\"",
        "        if max_size <= 0:",
        "            raise ValueError(f\"max_size must be positive, got {max_size}\")",
        "        self._query_cache_max_size = max_size",
        "",
        "        # Trim cache if it exceeds new size",
        "        while len(self._query_expansion_cache) > max_size:",
        "            oldest_key = next(iter(self._query_expansion_cache))",
        "            del self._query_expansion_cache[oldest_key]",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "        \"\"\"",
        "        return query_module.expand_query(",
        "            query_text,",
        "            self.layers,",
        "            self.tokenizer,",
        "            max_expansions=max_expansions,",
        "            use_variants=True,",
        "            use_code_concepts=True",
        "        )",
        ""
      ],
      "context_after": [
        "    def parse_intent_query(self, query_text: str) -> Dict:",
        "        \"\"\"",
        "        Parse a natural language query to extract intent and searchable terms.",
        "",
        "        Analyzes queries like \"where do we handle authentication?\" to identify:",
        "        - Question word (where) -> intent type (location)",
        "        - Action verb (handle) -> search for handling code",
        "        - Subject (authentication) -> main topic with synonyms",
        "",
        "        Args:"
      ],
      "change_type": "add"
    },
    {
      "file": "tests/test_processor.py",
      "function": "class TestInputValidation(unittest.TestCase):",
      "start_line": 2691,
      "lines_added": [
        "class TestQueryCache(unittest.TestCase):",
        "    \"\"\"Test query expansion caching functionality.\"\"\"",
        "",
        "    @classmethod",
        "    def setUpClass(cls):",
        "        \"\"\"Set up test processor.\"\"\"",
        "        cls.processor = CorticalTextProcessor()",
        "        cls.processor.process_document(\"doc1\", \"Neural networks process data.\")",
        "        cls.processor.process_document(\"doc2\", \"Machine learning algorithms.\")",
        "        cls.processor.compute_all(verbose=False)",
        "",
        "    def test_expand_query_cached_returns_dict(self):",
        "        \"\"\"expand_query_cached should return a dict.\"\"\"",
        "        result = self.processor.expand_query_cached(\"neural\")",
        "        self.assertIsInstance(result, dict)",
        "",
        "    def test_expand_query_cached_same_result(self):",
        "        \"\"\"expand_query_cached should return same result for same query.\"\"\"",
        "        result1 = self.processor.expand_query_cached(\"neural networks\")",
        "        result2 = self.processor.expand_query_cached(\"neural networks\")",
        "        self.assertEqual(result1, result2)",
        "",
        "    def test_expand_query_cached_different_params(self):",
        "        \"\"\"Different parameters should use different cache entries.\"\"\"",
        "        result1 = self.processor.expand_query_cached(\"neural\", use_code_concepts=False)",
        "        result2 = self.processor.expand_query_cached(\"neural\", use_code_concepts=True)",
        "        # Results may differ (code concepts add synonyms)",
        "        self.assertIsInstance(result1, dict)",
        "        self.assertIsInstance(result2, dict)",
        "",
        "    def test_clear_query_cache(self):",
        "        \"\"\"clear_query_cache should return count and clear cache.\"\"\"",
        "        # Populate cache",
        "        self.processor.expand_query_cached(\"test1\")",
        "        self.processor.expand_query_cached(\"test2\")",
        "",
        "        # Clear and verify",
        "        count = self.processor.clear_query_cache()",
        "        self.assertGreaterEqual(count, 2)",
        "",
        "        # Verify cache is empty",
        "        count2 = self.processor.clear_query_cache()",
        "        self.assertEqual(count2, 0)",
        "",
        "    def test_set_query_cache_size(self):",
        "        \"\"\"set_query_cache_size should update max size.\"\"\"",
        "        self.processor.set_query_cache_size(50)",
        "        self.assertEqual(self.processor._query_cache_max_size, 50)",
        "",
        "        # Reset to default",
        "        self.processor.set_query_cache_size(100)",
        "",
        "    def test_set_query_cache_size_invalid(self):",
        "        \"\"\"set_query_cache_size should reject invalid sizes.\"\"\"",
        "        with self.assertRaises(ValueError):",
        "            self.processor.set_query_cache_size(0)",
        "        with self.assertRaises(ValueError):",
        "            self.processor.set_query_cache_size(-10)",
        "",
        "    def test_cache_lru_eviction(self):",
        "        \"\"\"Cache should evict oldest entries when full.\"\"\"",
        "        self.processor.clear_query_cache()",
        "        self.processor.set_query_cache_size(3)",
        "",
        "        # Fill cache",
        "        self.processor.expand_query_cached(\"query1\")",
        "        self.processor.expand_query_cached(\"query2\")",
        "        self.processor.expand_query_cached(\"query3\")",
        "",
        "        # Add another - should evict oldest",
        "        self.processor.expand_query_cached(\"query4\")",
        "",
        "        # Cache should still be at max size",
        "        self.assertLessEqual(len(self.processor._query_expansion_cache), 3)",
        "",
        "        # Reset",
        "        self.processor.set_query_cache_size(100)",
        "        self.processor.clear_query_cache()",
        "",
        "    def test_compute_all_invalidates_cache(self):",
        "        \"\"\"compute_all should clear the query cache.\"\"\"",
        "        # Populate cache",
        "        self.processor.expand_query_cached(\"cached_query\")",
        "        self.assertGreater(len(self.processor._query_expansion_cache), 0)",
        "",
        "        # Recompute",
        "        self.processor.compute_all(verbose=False)",
        "",
        "        # Cache should be cleared",
        "        self.assertEqual(len(self.processor._query_expansion_cache), 0)",
        "",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "    def test_add_documents_batch_valid_input(self):",
        "        \"\"\"add_documents_batch should accept valid input.\"\"\"",
        "        docs = [",
        "            (\"doc1\", \"First document.\", None),",
        "            (\"doc2\", \"Second document.\", {\"source\": \"test\"}),",
        "        ]",
        "        stats = self.processor.add_documents_batch(docs, recompute='none', verbose=False)",
        "        self.assertEqual(stats['documents_added'], 2)",
        "",
        ""
      ],
      "context_after": [
        "if __name__ == \"__main__\":",
        "    unittest.main(verbosity=2)"
      ],
      "change_type": "add"
    }
  ],
  "hour_of_day": 15,
  "day_of_week": "Wednesday",
  "seconds_since_last_commit": -427398,
  "is_merge": false,
  "is_initial": false,
  "parent_count": 1,
  "session_id": null,
  "related_chats": [],
  "ci_result": null,
  "reverted": false,
  "amended": false
}