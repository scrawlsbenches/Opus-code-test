{
  "hash": "a9ee1d0f689f8ca1dd15244c2cfcba85cb01110c",
  "message": "Add background execution support for --full-analysis",
  "author": "Claude",
  "timestamp": "2025-12-11 18:44:52 +0000",
  "branch": "claude/multi-index-design-DvifZ",
  "files_changed": [
    "scripts/index_codebase.py"
  ],
  "insertions": 505,
  "deletions": 3,
  "hunks": [
    {
      "file": "scripts/index_codebase.py",
      "function": "This script indexes all Python files and documentation to enable",
      "start_line": 6,
      "lines_added": [
        "",
        "Background Full Analysis:",
        "    python scripts/index_codebase.py --full-analysis  # Start in background",
        "    python scripts/index_codebase.py --full-analysis  # Check progress (run again)",
        "    python scripts/index_codebase.py --full-analysis --foreground  # Run synchronously",
        "",
        "The --full-analysis flag runs semantic PageRank and hybrid connections, which",
        "can take several minutes. By default it runs in a background process, allowing",
        "you to continue working. Run the same command again to check progress.",
        "",
        "Progress is tracked in .index_progress.json and can be monitored with:",
        "    cat .index_progress.json",
        "import multiprocessing"
      ],
      "lines_removed": [],
      "context_before": [
        "semantic search over the codebase using the Cortical Text Processor itself.",
        "",
        "Supports incremental indexing to only re-index changed files.",
        "",
        "Usage:",
        "    python scripts/index_codebase.py [--output corpus_dev.pkl]",
        "    python scripts/index_codebase.py --incremental  # Only index changes",
        "    python scripts/index_codebase.py --status       # Show what would change",
        "    python scripts/index_codebase.py --force        # Force full rebuild",
        "    python scripts/index_codebase.py --log indexer.log  # Log to file"
      ],
      "context_after": [
        "\"\"\"",
        "",
        "import argparse",
        "import json",
        "import logging",
        "import os",
        "import platform",
        "import signal",
        "import sys",
        "import threading",
        "import time",
        "from contextlib import contextmanager",
        "from dataclasses import dataclass, field",
        "from datetime import datetime",
        "from pathlib import Path",
        "from typing import Dict, List, Optional, Tuple, Any",
        "",
        "# Add parent directory to path for imports",
        "sys.path.insert(0, str(Path(__file__).parent.parent))",
        "",
        "from cortical.processor import CorticalTextProcessor",
        "from cortical.tokenizer import Tokenizer",
        "from cortical.chunk_index import (",
        "    ChunkWriter, ChunkLoader, ChunkCompactor,",
        "    get_changes_from_manifest as get_chunk_changes",
        ")"
      ],
      "change_type": "add"
    },
    {
      "file": "scripts/index_codebase.py",
      "function": "def timeout_handler(seconds: int, tracker: Optional[ProgressTracker] = None):",
      "start_line": 327,
      "lines_added": [
        "# =============================================================================",
        "# Background Analysis Progress Tracking",
        "# =============================================================================",
        "",
        "# Default progress file path",
        "DEFAULT_PROGRESS_FILE = '.index_progress.json'",
        "",
        "",
        "@dataclass",
        "class BackgroundProgress:",
        "    \"\"\"Progress state for background full-analysis runs.\"\"\"",
        "    status: str = \"not_started\"  # not_started, running, completed, failed",
        "    started_at: Optional[str] = None",
        "    completed_at: Optional[str] = None",
        "    pid: Optional[int] = None",
        "    current_phase: str = \"\"",
        "    progress_percent: float = 0.0",
        "    phases_completed: List[str] = field(default_factory=list)",
        "    phases_pending: List[str] = field(default_factory=list)",
        "    error: Optional[str] = None",
        "    output_path: Optional[str] = None",
        "",
        "    def to_dict(self) -> Dict[str, Any]:",
        "        return {",
        "            'status': self.status,",
        "            'started_at': self.started_at,",
        "            'completed_at': self.completed_at,",
        "            'pid': self.pid,",
        "            'current_phase': self.current_phase,",
        "            'progress_percent': self.progress_percent,",
        "            'phases_completed': self.phases_completed,",
        "            'phases_pending': self.phases_pending,",
        "            'error': self.error,",
        "            'output_path': self.output_path,",
        "        }",
        "",
        "    @classmethod",
        "    def from_dict(cls, data: Dict[str, Any]) -> 'BackgroundProgress':",
        "        return cls(",
        "            status=data.get('status', 'not_started'),",
        "            started_at=data.get('started_at'),",
        "            completed_at=data.get('completed_at'),",
        "            pid=data.get('pid'),",
        "            current_phase=data.get('current_phase', ''),",
        "            progress_percent=data.get('progress_percent', 0.0),",
        "            phases_completed=data.get('phases_completed', []),",
        "            phases_pending=data.get('phases_pending', []),",
        "            error=data.get('error'),",
        "            output_path=data.get('output_path'),",
        "        )",
        "",
        "",
        "def get_progress_file_path(base_path: Path) -> Path:",
        "    \"\"\"Get the progress file path.\"\"\"",
        "    return base_path / DEFAULT_PROGRESS_FILE",
        "",
        "",
        "def load_background_progress(progress_path: Path) -> Optional[BackgroundProgress]:",
        "    \"\"\"Load background progress from file.\"\"\"",
        "    if not progress_path.exists():",
        "        return None",
        "    try:",
        "        with open(progress_path, 'r') as f:",
        "            data = json.load(f)",
        "        return BackgroundProgress.from_dict(data)",
        "    except (json.JSONDecodeError, IOError):",
        "        return None",
        "",
        "",
        "def save_background_progress(progress_path: Path, progress: BackgroundProgress) -> None:",
        "    \"\"\"Save background progress to file.\"\"\"",
        "    with open(progress_path, 'w') as f:",
        "        json.dump(progress.to_dict(), f, indent=2)",
        "",
        "",
        "def is_process_alive(pid: int) -> bool:",
        "    \"\"\"Check if a process with given PID is still running.\"\"\"",
        "    if pid is None:",
        "        return False",
        "    try:",
        "        os.kill(pid, 0)  # Signal 0 checks existence without killing",
        "        return True",
        "    except (OSError, ProcessLookupError):",
        "        return False",
        "",
        "",
        "def display_progress(progress: BackgroundProgress, progress_path: Path) -> None:",
        "    \"\"\"Display current progress to the user.\"\"\"",
        "    print(\"\\n\" + \"=\" * 50)",
        "    print(\"BACKGROUND FULL-ANALYSIS STATUS\")",
        "    print(\"=\" * 50)",
        "",
        "    print(f\"\\nStatus: {progress.status.upper()}\")",
        "",
        "    if progress.started_at:",
        "        print(f\"Started: {progress.started_at}\")",
        "",
        "    if progress.status == \"running\":",
        "        # Check if process is actually alive",
        "        if progress.pid and not is_process_alive(progress.pid):",
        "            print(\"\\n⚠️  WARNING: Background process appears to have died unexpectedly!\")",
        "            print(\"   The analysis may have crashed. Check logs for details.\")",
        "            print(f\"   Progress file: {progress_path}\")",
        "        else:",
        "            print(f\"Process ID: {progress.pid}\")",
        "            print(f\"\\nCurrent phase: {progress.current_phase}\")",
        "            print(f\"Progress: {progress.progress_percent:.1f}%\")",
        "",
        "            # Progress bar",
        "            bar_width = 40",
        "            filled = int(bar_width * progress.progress_percent / 100)",
        "            bar = \"█\" * filled + \"░\" * (bar_width - filled)",
        "            print(f\"[{bar}]\")",
        "",
        "            if progress.phases_completed:",
        "                print(f\"\\nCompleted phases: {', '.join(progress.phases_completed)}\")",
        "            if progress.phases_pending:",
        "                print(f\"Remaining phases: {', '.join(progress.phases_pending)}\")",
        "",
        "    elif progress.status == \"completed\":",
        "        print(f\"Completed: {progress.completed_at}\")",
        "        print(f\"\\n✓ Full analysis completed successfully!\")",
        "        if progress.output_path:",
        "            print(f\"Output saved to: {progress.output_path}\")",
        "",
        "        if progress.phases_completed:",
        "            print(f\"\\nPhases completed: {', '.join(progress.phases_completed)}\")",
        "",
        "    elif progress.status == \"failed\":",
        "        print(f\"\\n✗ Full analysis failed!\")",
        "        if progress.error:",
        "            print(f\"Error: {progress.error}\")",
        "",
        "    print(f\"\\nProgress file: {progress_path}\")",
        "    print(\"Run the command again to check for updates.\\n\")",
        "",
        "",
        "class BackgroundProgressTracker(ProgressTracker):",
        "    \"\"\"",
        "    Extended ProgressTracker that writes progress to a file for background monitoring.",
        "    \"\"\"",
        "",
        "    def __init__(",
        "        self,",
        "        progress_path: Path,",
        "        output_path: Path,",
        "        log_file: Optional[str] = None,",
        "        verbose: bool = False,",
        "        quiet: bool = True,  # Default quiet for background",
        "        total_phases: int = 6",
        "    ):",
        "        super().__init__(log_file=log_file, verbose=verbose, quiet=quiet)",
        "        self.progress_path = progress_path",
        "        self.output_path = output_path",
        "        self.total_phases = total_phases",
        "        self.completed_phase_count = 0",
        "",
        "        # Define the phases for full analysis",
        "        self.all_phases = [",
        "            \"Discovering files\",",
        "            \"Indexing files\",",
        "            \"Computing analysis (full mode)\",",
        "            \"Extracting semantic relations\",",
        "            \"Saving corpus\",",
        "            \"Saving manifest\"",
        "        ]",
        "",
        "        # Initialize progress",
        "        self.bg_progress = BackgroundProgress(",
        "            status=\"running\",",
        "            started_at=datetime.now().isoformat(),",
        "            pid=os.getpid(),",
        "            output_path=str(output_path),",
        "            phases_pending=self.all_phases.copy(),",
        "        )",
        "        self._save_progress()",
        "",
        "    def _save_progress(self):",
        "        \"\"\"Save current progress to file.\"\"\"",
        "        try:",
        "            save_background_progress(self.progress_path, self.bg_progress)",
        "        except Exception:",
        "            pass  # Don't let progress save failures stop the analysis",
        "",
        "    def start_phase(self, name: str, total_items: int = 0):",
        "        \"\"\"Start a new phase and update progress file.\"\"\"",
        "        super().start_phase(name, total_items)",
        "        self.bg_progress.current_phase = name",
        "",
        "        # Calculate progress based on phase",
        "        base_progress = (self.completed_phase_count / self.total_phases) * 100",
        "        self.bg_progress.progress_percent = base_progress",
        "        self._save_progress()",
        "",
        "    def end_phase(self, name: Optional[str] = None, status: str = \"completed\"):",
        "        \"\"\"End a phase and update progress file.\"\"\"",
        "        super().end_phase(name, status)",
        "        phase_name = name or self.current_phase",
        "",
        "        if status == \"completed\" and phase_name:",
        "            self.completed_phase_count += 1",
        "            if phase_name not in self.bg_progress.phases_completed:",
        "                self.bg_progress.phases_completed.append(phase_name)",
        "            if phase_name in self.bg_progress.phases_pending:",
        "                self.bg_progress.phases_pending.remove(phase_name)",
        "",
        "            # Update progress",
        "            self.bg_progress.progress_percent = (",
        "                self.completed_phase_count / self.total_phases",
        "            ) * 100",
        "            self._save_progress()",
        "",
        "    def update_progress(self, items_processed: int, item_name: Optional[str] = None):",
        "        \"\"\"Update progress within current phase.\"\"\"",
        "        super().update_progress(items_processed, item_name)",
        "",
        "        if self.current_phase and self.current_phase in self.phases:",
        "            phase = self.phases[self.current_phase]",
        "            if phase.items_total > 0:",
        "                # Calculate intra-phase progress",
        "                phase_progress = items_processed / phase.items_total",
        "                base_progress = (self.completed_phase_count / self.total_phases) * 100",
        "                phase_contribution = (1 / self.total_phases) * 100 * phase_progress",
        "                self.bg_progress.progress_percent = base_progress + phase_contribution",
        "                self._save_progress()",
        "",
        "    def mark_completed(self):",
        "        \"\"\"Mark the analysis as completed.\"\"\"",
        "        self.bg_progress.status = \"completed\"",
        "        self.bg_progress.completed_at = datetime.now().isoformat()",
        "        self.bg_progress.progress_percent = 100.0",
        "        self.bg_progress.current_phase = \"\"",
        "        self._save_progress()",
        "",
        "    def mark_failed(self, error: str):",
        "        \"\"\"Mark the analysis as failed.\"\"\"",
        "        self.bg_progress.status = \"failed\"",
        "        self.bg_progress.completed_at = datetime.now().isoformat()",
        "        self.bg_progress.error = error",
        "        self._save_progress()",
        "",
        "",
        "def run_background_analysis(",
        "    base_path: Path,",
        "    output_path: Path,",
        "    progress_path: Path,",
        "    use_chunks: bool,",
        "    chunks_dir: str,",
        "    timeout: int,",
        "    verbose: bool,",
        "    log_file: Optional[str]",
        ") -> None:",
        "    \"\"\"",
        "    Run full analysis in a background-compatible way.",
        "",
        "    This function is designed to be called from a background thread/process.",
        "    It writes progress updates to a file that can be monitored.",
        "    \"\"\"",
        "    # Initialize background progress tracker",
        "    tracker = BackgroundProgressTracker(",
        "        progress_path=progress_path,",
        "        output_path=output_path,",
        "        log_file=log_file,",
        "        verbose=verbose,",
        "        quiet=True,  # Background mode is quiet to console",
        "    )",
        "",
        "    try:",
        "        tracker.log(\"Cortical Text Processor - Background Full Analysis\")",
        "        tracker.log(\"=\" * 50)",
        "        tracker.log(f\"Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")",
        "        tracker.log(f\"Progress file: {progress_path}\")",
        "",
        "        # Get files to index",
        "        tracker.start_phase(\"Discovering files\")",
        "        python_files = get_python_files(base_path)",
        "        doc_files = get_doc_files(base_path)",
        "        all_files = python_files + doc_files",
        "        tracker.end_phase(\"Discovering files\")",
        "",
        "        tracker.log(f\"\\nFound {len(python_files)} Python files and {len(doc_files)} documentation files\")",
        "",
        "        # Create processor",
        "        processor = create_code_processor()",
        "",
        "        # Index all files",
        "        tracker.start_phase(\"Indexing files\", len(all_files))",
        "        indexed = 0",
        "        total_lines = 0",
        "        file_mtimes = {}",
        "",
        "        for i, file_path in enumerate(all_files, 1):",
        "            doc_id = create_doc_id(file_path, base_path)",
        "            tracker.update_progress(i, doc_id)",
        "",
        "            metadata = index_file(processor, file_path, base_path, tracker)",
        "            if metadata:",
        "                indexed += 1",
        "                total_lines += metadata.get('line_count', 0)",
        "                file_mtimes[doc_id] = metadata.get('mtime', 0)",
        "",
        "        tracker.end_phase(\"Indexing files\")",
        "        tracker.log(f\"  Indexed {indexed} files ({total_lines:,} total lines)\")",
        "",
        "        # Run FULL analysis (the slow part)",
        "        tracker.start_phase(\"Computing analysis (full mode)\")",
        "        processor.compute_all(",
        "            build_concepts=True,",
        "            pagerank_method='semantic',",
        "            connection_strategy='hybrid',",
        "            verbose=False",
        "        )",
        "        tracker.end_phase(\"Computing analysis (full mode)\")",
        "",
        "        # Extract semantic relations",
        "        tracker.start_phase(\"Extracting semantic relations\")",
        "        processor.extract_corpus_semantics(",
        "            use_pattern_extraction=True,",
        "            verbose=False",
        "        )",
        "        tracker.end_phase(\"Extracting semantic relations\")",
        "",
        "        # Print corpus statistics",
        "        tracker.log(\"\\nCorpus Statistics:\")",
        "        tracker.log(f\"  Documents: {len(processor.documents)}\")",
        "        tracker.log(f\"  Tokens (Layer 0): {processor.layers[0].column_count()}\")",
        "        tracker.log(f\"  Bigrams (Layer 1): {processor.layers[1].column_count()}\")",
        "        tracker.log(f\"  Concepts (Layer 2): {processor.layers[2].column_count()}\")",
        "        tracker.log(f\"  Semantic relations: {len(processor.semantic_relations)}\")",
        "",
        "        # Save corpus",
        "        tracker.start_phase(\"Saving corpus\")",
        "        processor.save(str(output_path))",
        "        file_size = output_path.stat().st_size / 1024",
        "        tracker.log(f\"  Saved to {output_path.name} ({file_size:.1f} KB)\")",
        "        tracker.end_phase(\"Saving corpus\")",
        "",
        "        # Save manifest",
        "        manifest_path = get_manifest_path(output_path)",
        "        tracker.start_phase(\"Saving manifest\")",
        "        stats = {",
        "            'documents': len(processor.documents),",
        "            'tokens': processor.layers[0].column_count(),",
        "            'bigrams': processor.layers[1].column_count(),",
        "            'concepts': processor.layers[2].column_count(),",
        "            'semantic_relations': len(processor.semantic_relations),",
        "            'full_analysis': True,",
        "        }",
        "        save_manifest(manifest_path, file_mtimes, str(output_path), stats, tracker)",
        "        tracker.end_phase(\"Saving manifest\")",
        "",
        "        # Mark as completed",
        "        tracker.mark_completed()",
        "        tracker.print_summary()",
        "        tracker.log(\"\\n✓ Background full analysis completed successfully!\")",
        "        tracker.log(f\"Output saved to: {output_path}\")",
        "",
        "    except Exception as e:",
        "        tracker.mark_failed(str(e))",
        "        tracker.error(f\"Background analysis failed: {e}\")",
        "        tracker.print_summary()",
        "        raise",
        "",
        "",
        "def start_background_analysis(",
        "    base_path: Path,",
        "    output_path: Path,",
        "    progress_path: Path,",
        "    use_chunks: bool = False,",
        "    chunks_dir: str = 'corpus_chunks',",
        "    timeout: int = 0,",
        "    verbose: bool = False,",
        "    log_file: Optional[str] = None",
        ") -> None:",
        "    \"\"\"",
        "    Start full analysis in a background thread.",
        "",
        "    The analysis runs in a daemon thread that will continue running",
        "    even after the main process returns. Progress can be monitored",
        "    via the progress file.",
        "    \"\"\"",
        "    # Use a separate process for true background execution",
        "    # This ensures the analysis continues even if the main script exits",
        "    process = multiprocessing.Process(",
        "        target=run_background_analysis,",
        "        args=(",
        "            base_path,",
        "            output_path,",
        "            progress_path,",
        "            use_chunks,",
        "            chunks_dir,",
        "            timeout,",
        "            verbose,",
        "            log_file,",
        "        ),",
        "        daemon=False,  # Non-daemon so it survives parent exit",
        "    )",
        "    process.start()",
        "",
        "    # Give it a moment to start and create progress file",
        "    time.sleep(0.5)",
        "",
        "    print(\"\\n\" + \"=\" * 50)",
        "    print(\"FULL ANALYSIS STARTED IN BACKGROUND\")",
        "    print(\"=\" * 50)",
        "    print(f\"\\nProcess ID: {process.pid}\")",
        "    print(f\"Progress file: {progress_path}\")",
        "    print(f\"Output will be saved to: {output_path}\")",
        "    print(\"\\nThe analysis is running in the background.\")",
        "    print(\"You can safely close this terminal.\")",
        "    print(\"\\nTo check progress, run:\")",
        "    print(f\"  python scripts/index_codebase.py --full-analysis\")",
        "    print(\"\\nOr monitor the progress file directly:\")",
        "    print(f\"  cat {progress_path}\")",
        "    print(\"=\" * 50 + \"\\n\")",
        "",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "        signal.alarm(seconds)",
        "",
        "        try:",
        "            yield",
        "        finally:",
        "            # Restore the old handler and cancel the alarm",
        "            signal.alarm(0)",
        "            signal.signal(signal.SIGALRM, old_handler)",
        "",
        ""
      ],
      "context_after": [
        "# =============================================================================",
        "# Manifest Operations",
        "# =============================================================================",
        "",
        "def get_manifest_path(corpus_path: Path) -> Path:",
        "    \"\"\"Get the manifest file path based on corpus path.\"\"\"",
        "    return corpus_path.with_suffix('.manifest.json')",
        "",
        "",
        "def load_manifest("
      ],
      "change_type": "add"
    },
    {
      "file": "scripts/index_codebase.py",
      "function": "def show_chunk_status(",
      "start_line": 1066,
      "lines_added": [
        "  python scripts/index_codebase.py                   # Full rebuild (fast mode)",
        "",
        "Background Full Analysis:",
        "  python scripts/index_codebase.py --full-analysis              # Start in background",
        "  python scripts/index_codebase.py --full-analysis              # Check progress",
        "  python scripts/index_codebase.py --full-analysis --foreground # Run synchronously",
        "  python scripts/index_codebase.py --full-analysis --force      # Restart even if completed",
        "                        help='Use full semantic analysis (runs in background by default)')",
        "    parser.add_argument('--foreground', action='store_true',",
        "                        help='Run full analysis in foreground (blocking) instead of background')",
        "    parser.add_argument('--progress-file', type=str, default=None,",
        "                        help='Custom path for progress file (default: .index_progress.json)')",
        "    # Set up progress file path",
        "    if args.progress_file:",
        "        progress_path = Path(args.progress_file)",
        "        if not progress_path.is_absolute():",
        "            progress_path = base_path / args.progress_file",
        "    else:",
        "        progress_path = get_progress_file_path(base_path)",
        "",
        "    # Handle background full-analysis mode",
        "    if args.full_analysis and not args.foreground:",
        "        # Check for existing progress",
        "        existing_progress = load_background_progress(progress_path)",
        "",
        "        if existing_progress:",
        "            if existing_progress.status == \"running\":",
        "                # Check if the process is actually alive",
        "                if existing_progress.pid and is_process_alive(existing_progress.pid):",
        "                    # Show progress and exit",
        "                    display_progress(existing_progress, progress_path)",
        "                    return",
        "                else:",
        "                    # Process died, show warning and offer to restart",
        "                    print(\"\\n\" + \"=\" * 50)",
        "                    print(\"PREVIOUS BACKGROUND ANALYSIS\")",
        "                    print(\"=\" * 50)",
        "                    print(f\"\\nStatus: STALE (process {existing_progress.pid} no longer running)\")",
        "                    print(f\"Started: {existing_progress.started_at}\")",
        "                    print(f\"Last phase: {existing_progress.current_phase}\")",
        "                    print(f\"Progress: {existing_progress.progress_percent:.1f}%\")",
        "                    print(\"\\nThe previous background process appears to have stopped.\")",
        "                    print(\"Starting a new background analysis...\\n\")",
        "",
        "            elif existing_progress.status == \"completed\":",
        "                # Show completion and ask if they want to re-run",
        "                display_progress(existing_progress, progress_path)",
        "                print(\"To start a fresh analysis, delete the progress file first:\")",
        "                print(f\"  rm {progress_path}\")",
        "                print(\"Or run with --force to overwrite.\\n\")",
        "                if not args.force:",
        "                    return",
        "",
        "            elif existing_progress.status == \"failed\":",
        "                # Show failure and start new",
        "                print(\"\\n\" + \"=\" * 50)",
        "                print(\"PREVIOUS ANALYSIS FAILED\")",
        "                print(\"=\" * 50)",
        "                print(f\"\\nError: {existing_progress.error}\")",
        "                print(\"\\nStarting a new background analysis...\\n\")",
        "",
        "        # Start new background analysis",
        "        start_background_analysis(",
        "            base_path=base_path,",
        "            output_path=output_path,",
        "            progress_path=progress_path,",
        "            use_chunks=args.use_chunks,",
        "            chunks_dir=args.chunks_dir,",
        "            timeout=args.timeout,",
        "            verbose=args.verbose,",
        "            log_file=log_path,",
        "        )",
        "        return",
        "",
        "    # Initialize progress tracker for foreground operation"
      ],
      "lines_removed": [
        "  python scripts/index_codebase.py                   # Full rebuild",
        "                        help='Use full semantic analysis (slower but more accurate)')",
        "    # Initialize progress tracker"
      ],
      "context_before": [
        "# =============================================================================",
        "# Main Entry Point",
        "# =============================================================================",
        "",
        "def main():",
        "    parser = argparse.ArgumentParser(",
        "        description='Index the codebase for semantic search',",
        "        formatter_class=argparse.RawDescriptionHelpFormatter,",
        "        epilog=\"\"\"",
        "Examples:"
      ],
      "context_after": [
        "  python scripts/index_codebase.py --incremental    # Update changed files only",
        "  python scripts/index_codebase.py --status         # Show what would change",
        "  python scripts/index_codebase.py --force          # Force full rebuild",
        "  python scripts/index_codebase.py --log index.log  # Log to file",
        "  python scripts/index_codebase.py --timeout 60     # Timeout after 60s",
        "        \"\"\"",
        "    )",
        "    parser.add_argument('--output', '-o', default='corpus_dev.pkl',",
        "                        help='Output file path (default: corpus_dev.pkl)')",
        "    parser.add_argument('--incremental', '-i', action='store_true',",
        "                        help='Only index changed files (requires existing corpus)')",
        "    parser.add_argument('--force', '-f', action='store_true',",
        "                        help='Force full rebuild even if manifest exists')",
        "    parser.add_argument('--status', '-s', action='store_true',",
        "                        help='Show what would change without indexing')",
        "    parser.add_argument('--verbose', '-v', action='store_true',",
        "                        help='Show verbose output (per-file progress)')",
        "    parser.add_argument('--quiet', '-q', action='store_true',",
        "                        help='Suppress console output (still writes to log)')",
        "    parser.add_argument('--log', '-l', type=str, default=None,",
        "                        help='Log file path (writes detailed log)')",
        "    parser.add_argument('--timeout', '-t', type=int, default=DEFAULT_TIMEOUT,",
        "                        help=f'Timeout in seconds (0=none, default={DEFAULT_TIMEOUT})')",
        "    parser.add_argument('--full-analysis', action='store_true',",
        "",
        "    # Chunk-based indexing options",
        "    parser.add_argument('--use-chunks', action='store_true',",
        "                        help='Use git-compatible chunk-based indexing')",
        "    parser.add_argument('--chunks-dir', default='corpus_chunks',",
        "                        help='Directory for chunk files (default: corpus_chunks)')",
        "    parser.add_argument('--compact', action='store_true',",
        "                        help='Compact old chunks into a single file')",
        "    parser.add_argument('--compact-before', type=str, default=None,",
        "                        help='Only compact chunks before this date (YYYY-MM-DD)')",
        "    parser.add_argument('--compact-keep', type=int, default=0,",
        "                        help='Keep this many recent chunks when compacting')",
        "",
        "    args = parser.parse_args()",
        "",
        "    base_path = Path(__file__).parent.parent",
        "    output_path = base_path / args.output",
        "    manifest_path = get_manifest_path(output_path)",
        "",
        "    # Set up log file path",
        "    log_path = None",
        "    if args.log:",
        "        log_path = args.log if os.path.isabs(args.log) else str(base_path / args.log)",
        "",
        "    tracker = ProgressTracker(",
        "        log_file=log_path,",
        "        verbose=args.verbose,",
        "        quiet=args.quiet",
        "    )",
        "",
        "    tracker.log(\"Cortical Text Processor - Codebase Indexer\")",
        "    tracker.log(\"=\" * 50)",
        "    tracker.log(f\"Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")",
        "    if args.timeout > 0:"
      ],
      "change_type": "modify"
    }
  ],
  "hour_of_day": 18,
  "day_of_week": "Thursday",
  "seconds_since_last_commit": -327596,
  "is_merge": false,
  "is_initial": false,
  "parent_count": 1,
  "session_id": null,
  "related_chats": [],
  "ci_result": null,
  "reverted": false,
  "amended": false
}