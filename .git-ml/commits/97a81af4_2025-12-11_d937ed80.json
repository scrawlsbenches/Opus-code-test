{
  "hash": "97a81af47412180203df0d2dc8841aa51f9b2569",
  "message": "Merge pull request #40 from scrawlsbenches/claude/dog-fooding-01MVUyfMSevs1xrTFX68mfFN",
  "author": "scrawlsbenches",
  "timestamp": "2025-12-11 18:03:56 -0500",
  "branch": "claude/multi-index-design-DvifZ",
  "files_changed": [
    "CLAUDE.md",
    "TASK_ARCHIVE.md",
    "TASK_LIST.md",
    "corpus_dev.pkl",
    "cortical/analysis.py",
    "cortical/processor.py",
    "cortical/query.py",
    "cortical/semantics.py",
    "scripts/profile_full_analysis.py",
    "tests/test_query.py",
    "tests/test_semantics.py"
  ],
  "insertions": 947,
  "deletions": 36,
  "hunks": [
    {
      "file": "CLAUDE.md",
      "function": null,
      "start_line": 1,
      "lines_added": [
        "## Persona & Working Philosophy",
        "### Core Principles",
        "",
        "**Scientific Rigor First**",
        "- Verify claims with data, not assumptions",
        "- When something \"seems slow,\" profile it before optimizing",
        "- Be skeptical of intuitionsâ€”measure, then act",
        "",
        "**Understand Before Acting**",
        "- Read relevant code before proposing changes",
        "- Trace data flow through the system",
        "- Check TASK_LIST.md to avoid duplicate work",
        "",
        "**Deep Analysis Over Trial-and-Error**",
        "- When debugging, build a complete picture before running fixes",
        "- Profile bottlenecks systematically; the obvious culprit often isn't the real one",
        "- Document findings even when they contradict initial hypotheses",
        "",
        "**Test-Driven Confidence**",
        "- Maintain >89% code coverage before optimizations",
        "- Run the full test suite after every change",
        "- Write tests for the bug before writing the fix",
        "",
        "**Dog-Food Everything**",
        "- Use the system to test itself when possible",
        "- Real usage reveals issues that unit tests miss",
        "- Document all findings in TASK_LIST.md",
        "",
        "**Honest Assessment**",
        "- Acknowledge when something isn't working",
        "- Say \"I don't know\" when uncertain, then investigate",
        "- Correct course based on evidence, not pride",
        "",
        "When you see \"neural\" or \"cortical\" in this codebase, remember: these are metaphors for standard IR algorithms, not actual neural implementations.",
        "- **Louvain community detection** for concept clustering (`analysis.py`)"
      ],
      "lines_removed": [
        "## Persona",
        "Approach every task with **scientific rigor** - verify claims, check edge cases, and be skeptical of assumptions. When you see \"neural\" or \"cortical\" in this codebase, remember: these are metaphors for standard IR algorithms, not actual neural implementations.",
        "- **Label propagation** for concept clustering (`analysis.py`)"
      ],
      "context_before": [
        "# CLAUDE.md - Cortical Text Processor Development Guide",
        ""
      ],
      "context_after": [
        "",
        "You are a **senior computational neuroscience engineer** with deep expertise in:",
        "- Information retrieval algorithms (PageRank, TF-IDF, BM25)",
        "- Graph theory and network analysis",
        "- Natural language processing without ML dependencies",
        "- Biologically-inspired computing architectures",
        "- Python best practices and clean code principles",
        "",
        "",
        "---",
        "",
        "## Project Overview",
        "",
        "**Cortical Text Processor** is a zero-dependency Python library for hierarchical text analysis. It organizes text through 4 layers inspired by visual cortex organization:",
        "",
        "```",
        "Layer 0 (TOKENS)    â†’ Individual words        [V1 analogy: edges]",
        "Layer 1 (BIGRAMS)   â†’ Word pairs              [V2 analogy: patterns]",
        "Layer 2 (CONCEPTS)  â†’ Semantic clusters       [V4 analogy: shapes]",
        "Layer 3 (DOCUMENTS) â†’ Full documents          [IT analogy: objects]",
        "```",
        "",
        "**Core algorithms:**",
        "- **PageRank** for term importance (`analysis.py`)",
        "- **TF-IDF** for document relevance (`analysis.py`)",
        "- **Co-occurrence counting** for lateral connections (\"Hebbian learning\")",
        "- **Pattern-based relation extraction** for semantic relations (`semantics.py`)",
        "",
        "---",
        "",
        "## AI Agent Onboarding",
        "",
        "**New to this codebase?** Follow these steps to get oriented quickly:",
        "",
        "### Step 1: Generate AI Metadata (if missing)"
      ],
      "change_type": "modify"
    },
    {
      "file": "CLAUDE.md",
      "function": "cortical/",
      "start_line": 156,
      "lines_added": [
        "### Performance Lessons Learned (2025-12-11)",
        "",
        "**Profile before optimizing.** During dog-fooding, `compute_all()` was hanging. Initial suspicion was Louvain clustering (the most complex algorithm). Profiling revealed the real culprits:",
        "",
        "| Phase | Before | After | Fix |",
        "|-------|--------|-------|-----|",
        "| `bigram_connections` | 20.85s timeout | 10.79s | `max_bigrams_per_term=100`, `max_bigrams_per_doc=500` |",
        "| `semantics` | 30.05s timeout | 5.56s | `max_similarity_pairs=100000`, `min_context_keys=3` |",
        "| `louvain` | 2.2s | 2.2s | Not the bottleneck! |",
        "",
        "**Root cause:** O(nÂ²) complexity from common terms like \"self\" creating millions of pairs.",
        "",
        "### Fixed Bugs",
        "",
        "**Bigram separators (2025-12-10):** Bigrams use space separators throughout (`\"neural networks\"`, not `\"neural_networks\"`).",
        "",
        "**Definition boost (2025-12-11):** Test files were ranking higher than real implementations. Fixed with `is_test_file()` detection and `test_file_penalty` parameter."
      ],
      "lines_removed": [
        "### Fixed Bugs (2025-12-10)",
        "Historical note: Bigram separator mismatch bugs have been **fixed**. Bigrams now correctly use space separators throughout the codebase (see `tokenizer.py:extract_ngrams` and `analysis.py:compute_bigram_connections`)."
      ],
      "context_before": [
        "| Fingerprinting | `tests/test_fingerprint.py` |",
        "| Code concepts | `tests/test_code_concepts.py` |",
        "| Chunk indexing | `tests/test_chunk_indexing.py` |",
        "| Incremental updates | `tests/test_incremental_indexing.py` |",
        "| Intent queries | `tests/test_intent_query.py` |",
        "",
        "---",
        "",
        "## Critical Knowledge",
        ""
      ],
      "context_after": [
        "",
        "### Important Implementation Details",
        "",
        "1. **Bigrams use SPACE separators** (from `tokenizer.py:319-332`):",
        "   ```python",
        "   ' '.join(tokens[i:i+n])  # \"neural networks\", not \"neural_networks\"",
        "   ```",
        "",
        "2. **Global `col.tfidf` is NOT per-document TF-IDF** - it uses total corpus occurrence count. Use `col.tfidf_per_doc[doc_id]` for true per-document TF-IDF.",
        ""
      ],
      "change_type": "modify"
    },
    {
      "file": "CLAUDE.md",
      "function": "if processor.is_stale(processor.COMP_PAGERANK):",
      "start_line": 246,
      "lines_added": [
        "4. **Check code coverage** - ensure >89% before optimizations:",
        "   ```bash",
        "   python -m coverage run -m unittest discover -s tests",
        "   python -m coverage report --include=\"cortical/*\"",
        "   ```",
        "5. **Trace data flow** - follow how data moves through layers",
        "",
        "### When Debugging Performance Issues",
        "",
        "1. **Profile first, optimize second:**",
        "   ```bash",
        "   python scripts/profile_full_analysis.py",
        "   ```",
        "2. **Question assumptions** - the obvious culprit often isn't the real one",
        "3. **Build a complete picture** before running fixes",
        "4. **Document findings** in TASK_LIST.md even if they contradict initial hypotheses",
        "2. **Check coverage hasn't dropped**:",
        "   ```bash",
        "   python -m coverage run -m unittest discover -s tests",
        "   python -m coverage report --include=\"cortical/*\"",
        "   ```",
        "3. **Run the showcase** to verify integration:",
        "4. **Check for regressions** in related functionality",
        "5. **Dog-food the feature** - test with real usage (see [dogfooding-checklist.md](docs/dogfooding-checklist.md))",
        "6. **Document all findings** - add issues to TASK_LIST.md (see [code-of-ethics.md](docs/code-of-ethics.md))",
        "7. **Verify completion** - use [definition-of-done.md](docs/definition-of-done.md) checklist"
      ],
      "lines_removed": [
        "4. **Trace data flow** - follow how data moves through layers",
        "2. **Run the showcase** to verify integration:",
        "3. **Check for regressions** in related functionality",
        "4. **Dog-food the feature** - test with real usage (see [dogfooding-checklist.md](docs/dogfooding-checklist.md))",
        "5. **Document all findings** - add issues to TASK_LIST.md (see [code-of-ethics.md](docs/code-of-ethics.md))",
        "6. **Verify completion** - use [definition-of-done.md](docs/definition-of-done.md) checklist"
      ],
      "context_before": [
        "## Development Workflow",
        "",
        "### Before Writing Code",
        "",
        "1. **Read the relevant module** - understand existing patterns",
        "2. **Check TASK_LIST.md** - see if work is already planned/done",
        "3. **Run tests first** to establish baseline:",
        "   ```bash",
        "   python -m unittest discover -s tests -v",
        "   ```"
      ],
      "context_after": [
        "",
        "### When Implementing Features",
        "",
        "1. **Follow existing patterns** - this codebase is consistent",
        "2. **Add type hints** - the codebase uses them extensively",
        "3. **Write docstrings** - Google style with Args/Returns sections",
        "4. **Update staleness tracking** if adding new computation:",
        "   ```python",
        "   # In processor.py, add constant:",
        "   COMP_YOUR_FEATURE = 'your_feature'",
        "   # Mark stale in _mark_all_stale()",
        "   # Mark fresh after computation",
        "   ```",
        "",
        "### After Writing Code",
        "",
        "1. **Run the full test suite**:",
        "   ```bash",
        "   python -m unittest discover -s tests -v",
        "   ```",
        "   ```bash",
        "   python showcase.py",
        "   ```",
        "",
        "---",
        "",
        "## Testing Patterns",
        "",
        "Tests follow `unittest` conventions in `tests/` directory:",
        "",
        "```python",
        "class TestYourFeature(unittest.TestCase):",
        "    def setUp(self):"
      ],
      "change_type": "modify"
    },
    {
      "file": "CLAUDE.md",
      "function": "def find_documents(",
      "start_line": 393,
      "lines_added": [
        "8. **Watch for O(nÂ²) patterns** in loops over connectionsâ€”use limits like `max_bigrams_per_term`"
      ],
      "lines_removed": [],
      "context_before": [
        "",
        "## Performance Considerations",
        "",
        "1. **Use `get_by_id()` for ID lookups** - O(1) vs O(n) iteration",
        "2. **Batch document additions** with `add_documents_batch()` for bulk imports",
        "3. **Use incremental updates** with `add_document_incremental()` for live systems",
        "4. **Cache query expansions** when processing multiple similar queries",
        "5. **Pre-compute chunks** in `find_passages_batch()` to avoid redundant work",
        "6. **Use `fast_find_documents()`** for ~2-3x faster search on large corpora",
        "7. **Pre-build index** with `build_search_index()` for fastest repeated queries"
      ],
      "context_after": [
        "",
        "---",
        "",
        "## Code Search Capabilities",
        "",
        "### Code-Aware Tokenization",
        "```python",
        "# Enable identifier splitting for code search",
        "tokenizer = Tokenizer(split_identifiers=True)",
        "tokens = tokenizer.tokenize(\"getUserCredentials\")"
      ],
      "change_type": "add"
    },
    {
      "file": "CLAUDE.md",
      "function": "for term, weight in sorted(expanded.items(), key=lambda x: -x[1]):",
      "start_line": 479,
      "lines_added": [
        "### Profiling Performance",
        "```bash",
        "# Profile full analysis phases with timeout detection",
        "python scripts/profile_full_analysis.py",
        "",
        "# This reveals which phases are slow and helps identify O(nÂ²) bottlenecks",
        "```",
        "",
        "| Check coverage | `python -m coverage run -m unittest discover -s tests && python -m coverage report --include=\"cortical/*\"` |",
        "| Profile analysis | `python scripts/profile_full_analysis.py` |"
      ],
      "lines_removed": [],
      "context_before": [
        "    print(f\"  {term}: {weight:.3f}\")",
        "```",
        "",
        "### Checking Semantic Relations",
        "```python",
        "processor.extract_corpus_semantics()",
        "for t1, rel, t2, weight in processor.semantic_relations[:10]:",
        "    print(f\"{t1} --{rel}--> {t2} ({weight:.2f})\")",
        "```",
        ""
      ],
      "context_after": [
        "---",
        "",
        "## Quick Reference",
        "",
        "| Task | Command/Method |",
        "|------|----------------|",
        "| Process document | `processor.process_document(id, text)` |",
        "| Build network | `processor.compute_all()` |",
        "| Search | `processor.find_documents_for_query(query)` |",
        "| Fast search | `processor.fast_find_documents(query)` |",
        "| Code search | `processor.expand_query_for_code(query)` |",
        "| Intent search | `processor.search_by_intent(\"where do we...\")` |",
        "| RAG passages | `processor.find_passages_for_query(query)` |",
        "| Fingerprint | `processor.get_fingerprint(text)` |",
        "| Compare | `processor.compare_fingerprints(fp1, fp2)` |",
        "| Save state | `processor.save(\"corpus.pkl\")` |",
        "| Load state | `processor = CorticalTextProcessor.load(\"corpus.pkl\")` |",
        "| Run tests | `python -m unittest discover -s tests -v` |",
        "| Run showcase | `python showcase.py` |",
        "",
        "---",
        "",
        "## Dog-Fooding: Search the Codebase",
        "",
        "The Cortical Text Processor can index and search its own codebase, providing semantic search capabilities during development.",
        "",
        "### Quick Start",
        "",
        "```bash"
      ],
      "change_type": "add"
    },
    {
      "file": "CLAUDE.md",
      "function": "python scripts/index_codebase.py --status --use-chunks",
      "start_line": 660,
      "lines_added": [
        "*Remember: Measure before optimizing, test before committing, and document what you discover.*"
      ],
      "lines_removed": [
        "*Remember: Be skeptical, verify assumptions, and always run the tests.*"
      ],
      "context_before": [
        "**Process Documentation:**",
        "- **Getting Started**: `docs/quickstart.md` - 5-minute tutorial for newcomers",
        "- **Contributing**: `CONTRIBUTING.md` - how to contribute (fork, test, PR workflow)",
        "- **Ethics**: `docs/code-of-ethics.md` - documentation, testing, and completion standards",
        "- **Dog-fooding**: `docs/dogfooding-checklist.md` - checklist for testing with real usage",
        "- **Definition of Done**: `docs/definition-of-done.md` - when is a task truly complete?",
        "- **Task Archive**: `TASK_ARCHIVE.md` - completed tasks history",
        "",
        "---",
        ""
      ],
      "context_after": [],
      "change_type": "modify"
    },
    {
      "file": "TASK_ARCHIVE.md",
      "function": "Completed tasks moved from TASK_LIST.md. Search here for historical context and",
      "start_line": 4,
      "lines_added": [
        "| 136 | Optimize semantics O(nÂ²) similarity - add sampling/early-exit | Perf | 2025-12-11 |"
      ],
      "lines_removed": [],
      "context_before": [
        "",
        "**Archive Created:** 2025-12-11",
        "**Tasks Archived:** 75+ completed tasks",
        "",
        "---",
        "",
        "## Quick Reference: Completed Tasks",
        "",
        "| # | Task | Category | Completed |",
        "|---|------|----------|-----------|"
      ],
      "context_after": [
        "| 1 | Fix Per-Document TF-IDF Calculation Bug | Bug Fix | 2025-12-10 |",
        "| 2 | Add ID-to-Minicolumn Lookup Optimization | Performance | 2025-12-10 |",
        "| 3 | Fix Type Annotation Errors | Bug Fix | 2025-12-10 |",
        "| 4 | Remove Unused Import | Cleanup | 2025-12-10 |",
        "| 5 | Fix Unconditional Print in Export | Bug Fix | 2025-12-10 |",
        "| 6 | Add Missing Test Coverage | Testing | 2025-12-10 |",
        "| 8 | Implement Chunk-Level Retrieval | RAG | 2025-12-10 |",
        "| 9 | Add Document Metadata Support | RAG | 2025-12-10 |",
        "| 10 | Activate Layer 2 Concept Clustering | RAG | 2025-12-10 |",
        "| 11 | Integrate Semantic Relations into Retrieval | RAG | 2025-12-10 |"
      ],
      "change_type": "add"
    },
    {
      "file": "TASK_LIST.md",
      "function": null,
      "start_line": 1,
      "lines_added": [
        "**Pending Tasks:** 38",
        "**Completed Tasks:** 90+ (see archive)"
      ],
      "lines_removed": [
        "**Pending Tasks:** 32",
        "**Completed Tasks:** 88+ (see archive)"
      ],
      "context_before": [
        "# Task List: Cortical Text Processor",
        "",
        "Active backlog for the Cortical Text Processor project. Completed tasks are archived in [TASK_ARCHIVE.md](TASK_ARCHIVE.md).",
        "",
        "**Last Updated:** 2025-12-11"
      ],
      "context_after": [
        "",
        "---",
        "",
        "## Active Backlog",
        "",
        "<!-- Machine-parseable format for automation -->",
        "",
        "### ðŸ”´ Critical (Do Now)",
        "",
        "| # | Task | Category | Depends | Effort |"
      ],
      "change_type": "modify"
    },
    {
      "file": "TASK_LIST.md",
      "function": "Active backlog for the Cortical Text Processor project. Completed tasks are arch",
      "start_line": 24,
      "lines_added": [
        "| 137 | Cap bigram connections to top-K per bigram | Perf | - | Small |",
        "| 138 | Use sparse matrix multiplication for bigram connections | Perf | - | Medium |",
        "| 139 | Batch bigram connection updates to reduce dict overhead | Perf | - | Small |",
        "| 133 | Implement WAL + snapshot persistence (fault-tolerant rebuild) | Arch | 132 | Large |",
        "| 134 | Implement protobuf serialization for corpus | Arch | 132 | Medium |",
        "| 135 | Implement chunked parallel processing for full-analysis | Arch | 132 | Large |"
      ],
      "lines_removed": [],
      "context_before": [
        "| # | Task | Category | Depends | Effort |",
        "|---|------|----------|---------|--------|",
        "| 94 | Split query.py into focused modules | Arch | - | Large |",
        "| 97 | Integrate CorticalConfig into processor | Arch | - | Medium |",
        "| 127 | Create cluster coverage evaluation script | DevEx | 125 | Medium |",
        "",
        "### ðŸŸ¡ Medium (Do This Month)",
        "",
        "| # | Task | Category | Depends | Effort |",
        "|---|------|----------|---------|--------|"
      ],
      "context_after": [
        "| 91 | Create docs/README.md index | Docs | - | Small |",
        "| 92 | Add badges to README.md | DevEx | - | Small |",
        "| 93 | Update README with docs references | Docs | 91 | Small |",
        "| 95 | Split processor.py into modules | Arch | 97 | Large |",
        "| 96 | Centralize duplicate constants | CodeQual | - | Small |",
        "| 98 | Replace print() with logging | CodeQual | - | Medium |",
        "| 99 | Add input validation to public methods | CodeQual | - | Medium |",
        "| 102 | Add tests for edge cases | Testing | - | Medium |",
        "| 107 | Add Quick Context to tasks | TaskMgmt | - | Medium |",
        "| 113 | Document staleness tracking system | AINav | - | Small |"
      ],
      "change_type": "add"
    },
    {
      "file": "TASK_LIST.md",
      "function": "Active backlog for the Cortical Text Processor project. Completed tasks are arch",
      "start_line": 55,
      "lines_added": [
        "| 140 | Analyze customer service cluster quality | Research | 127 | Small |"
      ],
      "lines_removed": [
        "| 128 | Analyze customer service cluster quality | Research | 127 | Small |"
      ],
      "context_before": [
        "| 76 | Add \"Suggest Related Files\" feature | DevEx | - | Medium |",
        "| 78 | Add code pattern detection | DevEx | - | Large |",
        "| 79 | Add corpus health dashboard | DevEx | - | Medium |",
        "| 80 | Add \"Learning Mode\" for contributors | DevEx | - | Large |",
        "| 100 | Implement plugin/extension registry | Arch | - | Large |",
        "| 101 | Automate staleness tracking | Arch | - | Medium |",
        "| 106 | Add task dependency graph | TaskMgmt | - | Small |",
        "| 108 | Create task selection script | TaskMgmt | - | Medium |",
        "| 117 | Create debugging cookbook | AINav | - | Medium |",
        "| 118 | Add function complexity annotations | AINav | - | Small |"
      ],
      "context_after": [
        "| 129 | Test customer service retrieval quality | Testing | - | Small |",
        "| 130 | Expand customer service sample cluster | Samples | - | Medium |",
        "| 131 | Investigate cross-domain semantic bridges | Research | - | Medium |",
        "",
        "### â¸ï¸ Deferred",
        "",
        "| # | Task | Reason |",
        "|---|------|--------|",
        "| 110 | Add section markers to large files | Superseded by #119 (AI metadata generator) |",
        "| 111 | Add \"See Also\" cross-references | Superseded by #119 (AI metadata generator) |"
      ],
      "change_type": "modify"
    },
    {
      "file": "TASK_LIST.md",
      "function": "Active backlog for the Cortical Text Processor project. Completed tasks are arch",
      "start_line": 84,
      "lines_added": [
        "| 128 | Fix definition boost that favors test mocks over real implementations | 2025-12-11 | Added is_test_file() and test file penalty |",
        "| 132 | Profile full-analysis bottleneck (bigram, semantics O(nÂ²)) | 2025-12-11 | Created profile_full_analysis.py, fixed bottlenecks |",
        "| 136 | Optimize semantics O(nÂ²) similarity with early termination | 2025-12-11 | Added max_similarity_pairs, min_context_keys |"
      ],
      "lines_removed": [],
      "context_before": [
        "| # | Task | Started | Notes |",
        "|---|------|---------|-------|",
        "| 87 | Add Python code samples and showcase | 2025-12-11 | samples/*.py created |",
        "",
        "---",
        "",
        "## Recently Completed (Last 7 Days)",
        "",
        "| # | Task | Completed | Notes |",
        "|---|------|-----------|-------|"
      ],
      "context_after": [
        "| 126 | Investigate optimal Louvain resolution for sample corpus | 2025-12-11 | Research confirms default 1.0 is optimal |",
        "| 123 | Replace label propagation with Louvain community detection | 2025-12-11 | Implemented Louvain algorithm, 34 clusters for 92 docs |",
        "| 122 | Investigate Concept Layer & Embeddings regressions | 2025-12-11 | Fixed inverted strictness, improved embeddings |",
        "| 119 | Create AI metadata generator script | 2025-12-11 | scripts/generate_ai_metadata.py with tests |",
        "| 120 | Add AI metadata loader to Claude skills | 2025-12-11 | ai-metadata skill created |",
        "| 121 | Auto-regenerate AI metadata on changes | 2025-12-11 | Documented in CLAUDE.md, skills |",
        "| 88 | Create package installation files | 2025-12-11 | pyproject.toml, requirements.txt |",
        "| 89 | Create CONTRIBUTING.md | 2025-12-11 | Contribution guide |",
        "| 90 | Create docs/quickstart.md | 2025-12-11 | 5-minute tutorial |",
        "| 103 | Add Priority Backlog Summary | 2025-12-11 | TASK_LIST.md restructure |"
      ],
      "change_type": "add"
    },
    {
      "file": "cortical/analysis.py",
      "function": "def compute_concept_connections(",
      "start_line": 1211,
      "lines_added": [
        "    cooccurrence_weight: float = 0.3,",
        "    max_bigrams_per_term: int = 100,",
        "    max_bigrams_per_doc: int = 500",
        "        max_bigrams_per_term: Skip terms appearing in more than this many bigrams",
        "            to avoid O(nÂ²) explosion from common terms like \"self\", \"return\" (default 100)",
        "        max_bigrams_per_doc: Skip documents with more than this many bigrams for",
        "            co-occurrence connections to avoid O(nÂ²) explosion (default 500)",
        "        - skipped_common_terms: Number of terms skipped due to max_bigrams_per_term",
        "        - skipped_large_docs: Number of docs skipped due to max_bigrams_per_doc",
        "            'cooccurrence_connections': 0,",
        "            'skipped_common_terms': 0,",
        "            'skipped_large_docs': 0"
      ],
      "lines_removed": [
        "    cooccurrence_weight: float = 0.3",
        "            'cooccurrence_connections': 0"
      ],
      "context_before": [
        "        'semantic_connections': semantic_connections,",
        "        'embedding_connections': embedding_connections",
        "    }",
        "",
        "",
        "def compute_bigram_connections(",
        "    layers: Dict[CorticalLayer, HierarchicalLayer],",
        "    min_shared_docs: int = 1,",
        "    component_weight: float = 0.5,",
        "    chain_weight: float = 0.7,"
      ],
      "context_after": [
        ") -> Dict[str, Any]:",
        "    \"\"\"",
        "    Build lateral connections between bigrams in Layer 1.",
        "",
        "    Bigrams are connected based on:",
        "    1. Shared component terms (\"neural_networks\" â†” \"neural_processing\")",
        "    2. Document co-occurrence (appear in same documents)",
        "    3. Chains (\"machine_learning\" â†” \"learning_algorithms\" where right=left)",
        "",
        "    Args:",
        "        layers: Dictionary of all layers",
        "        min_shared_docs: Minimum shared documents for co-occurrence connection",
        "        component_weight: Weight for shared component connections (default 0.5)",
        "        chain_weight: Weight for chain connections (default 0.7)",
        "        cooccurrence_weight: Weight for document co-occurrence (default 0.3)",
        "",
        "    Returns:",
        "        Statistics about connections created:",
        "        - connections_created: Total bidirectional connections",
        "        - component_connections: Connections from shared components",
        "        - chain_connections: Connections from chains",
        "        - cooccurrence_connections: Connections from document co-occurrence",
        "    \"\"\"",
        "    layer1 = layers[CorticalLayer.BIGRAMS]",
        "",
        "    if layer1.column_count() == 0:",
        "        return {",
        "            'connections_created': 0,",
        "            'bigrams': 0,",
        "            'component_connections': 0,",
        "            'chain_connections': 0,",
        "        }",
        "",
        "    bigrams = list(layer1.minicolumns.values())",
        "",
        "    # Build indexes for efficient lookup",
        "    # left_component_index: {\"neural\": [bigram1, bigram2, ...]}",
        "    # right_component_index: {\"networks\": [bigram1, bigram3, ...]}",
        "    # Note: Bigrams use space separators (e.g., \"neural networks\")",
        "    left_index: Dict[str, List[Minicolumn]] = defaultdict(list)",
        "    right_index: Dict[str, List[Minicolumn]] = defaultdict(list)"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/analysis.py",
      "function": "def compute_bigram_connections(",
      "start_line": 1293,
      "lines_added": [
        "    # Track skipped common terms for statistics",
        "    skipped_common_terms = 0",
        "",
        "        # Skip overly common terms to avoid O(nÂ²) explosion",
        "        if len(bigram_list) > max_bigrams_per_term:",
        "            skipped_common_terms += 1",
        "            continue",
        "        # Skip overly common terms to avoid O(nÂ²) explosion",
        "        if len(bigram_list) > max_bigrams_per_term:",
        "            skipped_common_terms += 1",
        "            continue",
        "            # Skip overly common terms",
        "            if len(left_index[term]) > max_bigrams_per_term or len(right_index[term]) > max_bigrams_per_term:",
        "                continue",
        "    skipped_large_docs = 0",
        "        # Skip documents with too many bigrams to avoid O(nÂ²) explosion",
        "        if len(doc_bigrams) > max_bigrams_per_doc:",
        "            skipped_large_docs += 1",
        "            continue",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "",
        "        if conn_type == 'component':",
        "            component_connections += 1",
        "        elif conn_type == 'chain':",
        "            chain_connections += 1",
        "        elif conn_type == 'cooccurrence':",
        "            cooccurrence_connections += 1",
        "",
        "        return True",
        ""
      ],
      "context_after": [
        "    # 1. Connect bigrams sharing a component",
        "    # Left component matches: \"neural_networks\" â†” \"neural_processing\"",
        "    for component, bigram_list in left_index.items():",
        "        for i, b1 in enumerate(bigram_list):",
        "            for b2 in bigram_list[i+1:]:",
        "                # Weight by component's PageRank importance (if available)",
        "                weight = component_weight",
        "                add_connection(b1, b2, weight, 'component')",
        "",
        "    # Right component matches: \"deep_learning\" â†” \"machine_learning\"",
        "    for component, bigram_list in right_index.items():",
        "        for i, b1 in enumerate(bigram_list):",
        "            for b2 in bigram_list[i+1:]:",
        "                weight = component_weight",
        "                add_connection(b1, b2, weight, 'component')",
        "",
        "    # 2. Connect chain bigrams (right of one = left of other)",
        "    # \"machine_learning\" â†” \"learning_algorithms\"",
        "    for term in left_index:",
        "        if term in right_index:",
        "            # term appears as right component in some bigrams and left in others",
        "            for b_left in right_index[term]:  # ends with term",
        "                for b_right in left_index[term]:  # starts with term",
        "                    if b_left.id != b_right.id:",
        "                        add_connection(b_left, b_right, chain_weight, 'chain')",
        "",
        "    # 3. Connect bigrams that co-occur in the same documents",
        "    # Use inverted index for O(d * bÂ²) instead of O(nÂ²) where d=docs, b=bigrams per doc",
        "    doc_to_bigrams: Dict[str, List[Minicolumn]] = defaultdict(list)",
        "    for bigram in bigrams:",
        "        for doc_id in bigram.document_ids:",
        "            doc_to_bigrams[doc_id].append(bigram)",
        "",
        "    # Track pairs we've already processed to avoid duplicate work",
        "    cooccur_processed: Set[Tuple[str, str]] = set()",
        "",
        "    for doc_id, doc_bigrams in doc_to_bigrams.items():",
        "        # Only compare bigrams within the same document",
        "        for i, b1 in enumerate(doc_bigrams):",
        "            docs1 = b1.document_ids",
        "            for b2 in doc_bigrams[i+1:]:",
        "                # Skip if already processed this pair",
        "                pair_key = tuple(sorted([b1.id, b2.id]))",
        "                if pair_key in cooccur_processed:",
        "                    continue",
        "                cooccur_processed.add(pair_key)",
        ""
      ],
      "change_type": "add"
    },
    {
      "file": "cortical/analysis.py",
      "function": "def compute_bigram_connections(",
      "start_line": 1353,
      "lines_added": [
        "        'cooccurrence_connections': cooccurrence_connections,",
        "        'skipped_common_terms': skipped_common_terms,",
        "        'skipped_large_docs': skipped_large_docs"
      ],
      "lines_removed": [
        "        'cooccurrence_connections': cooccurrence_connections"
      ],
      "context_before": [
        "                    # Weight by Jaccard similarity of document sets",
        "                    jaccard = len(shared_docs) / len(docs1 | docs2)",
        "                    weight = cooccurrence_weight * jaccard",
        "                    add_connection(b1, b2, weight, 'cooccurrence')",
        "",
        "    return {",
        "        'connections_created': len(connected_pairs),",
        "        'bigrams': len(bigrams),",
        "        'component_connections': component_connections,",
        "        'chain_connections': chain_connections,"
      ],
      "context_after": [
        "    }",
        "",
        "",
        "def compute_document_connections(",
        "    layers: Dict[CorticalLayer, HierarchicalLayer],",
        "    documents: Dict[str, str],",
        "    min_shared_terms: int = 3",
        ") -> None:",
        "    \"\"\"",
        "    Build lateral connections between documents."
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/processor.py",
      "function": "class CorticalTextProcessor:",
      "start_line": 886,
      "lines_added": [
        "        max_bigrams_per_term: int = 100,",
        "        max_bigrams_per_doc: int = 500,",
        "            max_bigrams_per_term: Skip terms appearing in more than this many bigrams",
        "                to avoid O(nÂ²) explosion from common terms like \"self\", \"return\" (default 100)",
        "            max_bigrams_per_doc: Skip documents with more than this many bigrams for",
        "                co-occurrence connections to avoid O(nÂ²) explosion (default 500)",
        "            - skipped_common_terms: Number of terms skipped due to max_bigrams_per_term",
        "            - skipped_large_docs: Number of docs skipped due to max_bigrams_per_doc",
        "            cooccurrence_weight=cooccurrence_weight,",
        "            max_bigrams_per_term=max_bigrams_per_term,",
        "            max_bigrams_per_doc=max_bigrams_per_doc",
        "            skipped_terms = stats.get('skipped_common_terms', 0)",
        "            skipped_docs = stats.get('skipped_large_docs', 0)",
        "            skip_parts = []",
        "            if skipped_terms:",
        "                skip_parts.append(f\"{skipped_terms} common terms\")",
        "            if skipped_docs:",
        "                skip_parts.append(f\"{skipped_docs} large docs\")",
        "            skip_msg = f\", skipped {', '.join(skip_parts)}\" if skip_parts else \"\"",
        "                  f\"cooccur: {stats['cooccurrence_connections']}{skip_msg})\")"
      ],
      "lines_removed": [
        "            cooccurrence_weight=cooccurrence_weight",
        "                  f\"cooccur: {stats['cooccurrence_connections']})\")"
      ],
      "context_before": [
        "    def compute_document_connections(self, min_shared_terms: int = 3, verbose: bool = True) -> None:",
        "        analysis.compute_document_connections(self.layers, self.documents, min_shared_terms)",
        "        if verbose: print(\"Computed document connections\")",
        "",
        "    def compute_bigram_connections(",
        "        self,",
        "        min_shared_docs: int = 1,",
        "        component_weight: float = 0.5,",
        "        chain_weight: float = 0.7,",
        "        cooccurrence_weight: float = 0.3,"
      ],
      "context_after": [
        "        verbose: bool = True",
        "    ) -> Dict[str, Any]:",
        "        \"\"\"",
        "        Build lateral connections between bigrams based on shared components and co-occurrence.",
        "",
        "        Bigrams are connected when they:",
        "        - Share a component term (\"neural_networks\" â†” \"neural_processing\")",
        "        - Form chains (\"machine_learning\" â†” \"learning_algorithms\")",
        "        - Co-occur in the same documents",
        "",
        "        Args:",
        "            min_shared_docs: Minimum shared documents for co-occurrence connection",
        "            component_weight: Weight for shared component connections (default 0.5)",
        "            chain_weight: Weight for chain connections (default 0.7)",
        "            cooccurrence_weight: Weight for document co-occurrence (default 0.3)",
        "            verbose: Print progress messages",
        "",
        "        Returns:",
        "            Statistics about connections created:",
        "            - connections_created: Total bidirectional connections",
        "            - component_connections: Connections from shared components",
        "            - chain_connections: Connections from chains",
        "            - cooccurrence_connections: Connections from document co-occurrence",
        "",
        "        Example:",
        "            >>> stats = processor.compute_bigram_connections()",
        "            >>> print(f\"Created {stats['connections_created']} bigram connections\")",
        "            >>> print(f\"  Component: {stats['component_connections']}\")",
        "            >>> print(f\"  Chain: {stats['chain_connections']}\")",
        "            >>> print(f\"  Co-occurrence: {stats['cooccurrence_connections']}\")",
        "        \"\"\"",
        "        stats = analysis.compute_bigram_connections(",
        "            self.layers,",
        "            min_shared_docs=min_shared_docs,",
        "            component_weight=component_weight,",
        "            chain_weight=chain_weight,",
        "        )",
        "        if verbose:",
        "            print(f\"Created {stats['connections_created']} bigram connections \"",
        "                  f\"(component: {stats['component_connections']}, \"",
        "                  f\"chain: {stats['chain_connections']}, \"",
        "        return stats",
        "",
        "    def build_concept_clusters(",
        "        self,",
        "        min_cluster_size: int = 3,",
        "        clustering_method: str = 'louvain',",
        "        cluster_strictness: float = 1.0,",
        "        bridge_weight: float = 0.0,",
        "        resolution: float = 1.0,",
        "        verbose: bool = True"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/processor.py",
      "function": "class CorticalTextProcessor:",
      "start_line": 1085,
      "lines_added": [
        "        max_similarity_pairs: int = 100000,",
        "        min_context_keys: int = 3,",
        "            max_similarity_pairs: Maximum pairs to check for SimilarTo relations.",
        "                Set to 0 for unlimited (may be slow for large corpora). Default 100000.",
        "            min_context_keys: Minimum context keys for a term to be considered for",
        "                SimilarTo relations. Terms with fewer keys are skipped. Default 3.",
        "            min_pattern_confidence=min_pattern_confidence,",
        "            max_similarity_pairs=max_similarity_pairs,",
        "            min_context_keys=min_context_keys"
      ],
      "lines_removed": [
        "            min_pattern_confidence=min_pattern_confidence"
      ],
      "context_before": [
        "                parts.append(f\"semantic: {stats['semantic_connections']}\")",
        "            if stats.get('embedding_connections', 0) > 0:",
        "                parts.append(f\"embedding: {stats['embedding_connections']}\")",
        "            print(\", \".join(parts) if len(parts) > 1 else parts[0])",
        "        return stats",
        "",
        "    def extract_corpus_semantics(",
        "        self,",
        "        use_pattern_extraction: bool = True,",
        "        min_pattern_confidence: float = 0.6,"
      ],
      "context_after": [
        "        verbose: bool = True",
        "    ) -> int:",
        "        \"\"\"",
        "        Extract semantic relations from the corpus.",
        "",
        "        Combines co-occurrence analysis with pattern-based extraction to discover",
        "        semantic relationships like IsA, HasA, UsedFor, Causes, etc.",
        "",
        "        Args:",
        "            use_pattern_extraction: Extract relations from text patterns (e.g., \"X is a Y\")",
        "            min_pattern_confidence: Minimum confidence for pattern-based relations",
        "            verbose: Print progress messages",
        "",
        "        Returns:",
        "            Number of relations extracted",
        "",
        "        Example:",
        "            >>> count = processor.extract_corpus_semantics(verbose=False)",
        "            >>> print(f\"Found {count} semantic relations\")",
        "        \"\"\"",
        "        self.semantic_relations = semantics.extract_corpus_semantics(",
        "            self.layers,",
        "            self.documents,",
        "            self.tokenizer,",
        "            use_pattern_extraction=use_pattern_extraction,",
        "        )",
        "        if verbose:",
        "            print(f\"Extracted {len(self.semantic_relations)} semantic relations\")",
        "        return len(self.semantic_relations)",
        "",
        "    def extract_pattern_relations(",
        "        self,",
        "        min_confidence: float = 0.6,",
        "        verbose: bool = True",
        "    ) -> List[Tuple[str, str, str, float]]:"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/query.py",
      "function": "def apply_definition_boost(",
      "start_line": 619,
      "lines_added": [
        "def is_test_file(doc_id: str) -> bool:",
        "    \"\"\"",
        "    Detect if a document ID represents a test file.",
        "",
        "    Checks for common test file patterns:",
        "    - Path contains 'tests/' or 'test/'",
        "    - Filename starts with 'test_' or ends with '_test.py'",
        "    - Path contains 'mock' or 'fixture'",
        "",
        "    Args:",
        "        doc_id: Document identifier (typically a file path)",
        "",
        "    Returns:",
        "        True if the document appears to be a test file",
        "    \"\"\"",
        "    doc_lower = doc_id.lower()",
        "",
        "    # Check path components",
        "    if '/tests/' in doc_lower or '/test/' in doc_lower:",
        "        return True",
        "",
        "    # Check filename patterns",
        "    filename = doc_lower.split('/')[-1] if '/' in doc_lower else doc_lower",
        "    if filename.startswith('test_') or filename.endswith('_test.py'):",
        "        return True",
        "    if 'mock' in filename or 'fixture' in filename:",
        "        return True",
        "",
        "    return False",
        "",
        "",
        "    boost_factor: float = 2.0,",
        "    test_file_boost_factor: float = 0.5,",
        "    test_file_penalty: float = 0.7",
        "    For definition queries:",
        "    - Source files with the definition pattern get boost_factor (default 2.0x)",
        "    - Test files with the definition pattern get test_file_boost_factor (default 0.5x)",
        "    - All other test files get test_file_penalty (default 0.7x) to deprioritize them",
        "",
        "        boost_factor: Multiplier for definition-containing source docs (default 2.0)",
        "        test_file_boost_factor: Multiplier for test files with definition (default 0.5)",
        "        test_file_penalty: Multiplier for test files without definition (default 0.7)",
        "            Set to 1.0 to disable test file penalty.",
        "        has_definition = pattern.search(doc_text)",
        "        is_test = is_test_file(doc_id)",
        "",
        "        if has_definition:",
        "            if is_test:",
        "                # Test file with definition: apply reduced boost",
        "                boosted_docs.append((doc_id, score * test_file_boost_factor))",
        "            else:",
        "                # Source file with definition: apply full boost",
        "                boosted_docs.append((doc_id, score * boost_factor))",
        "        elif is_test:",
        "            # Test file without definition: apply penalty to deprioritize",
        "            boosted_docs.append((doc_id, score * test_file_penalty))",
        "            # Source file without definition: keep original score"
      ],
      "lines_removed": [
        "    boost_factor: float = 2.0",
        "        boost_factor: Multiplier for definition-containing docs (default 2.0)",
        "        if pattern.search(doc_text):",
        "            # This document contains the actual definition",
        "            boosted_docs.append((doc_id, score * boost_factor))"
      ],
      "context_before": [
        "            # This passage contains the actual definition",
        "            boosted_passages.append((text, doc_id, start, end, score * boost_factor))",
        "        else:",
        "            boosted_passages.append((text, doc_id, start, end, score))",
        "",
        "    # Re-sort by boosted scores",
        "    boosted_passages.sort(key=lambda x: x[4], reverse=True)",
        "    return boosted_passages",
        "",
        ""
      ],
      "context_after": [
        "def boost_definition_documents(",
        "    doc_results: List[Tuple[str, float]],",
        "    query_text: str,",
        "    documents: Dict[str, str],",
        ") -> List[Tuple[str, float]]:",
        "    \"\"\"",
        "    Boost documents that contain the actual definition being searched for.",
        "",
        "    This helps ensure the source file containing a class/function definition",
        "    is included in the document candidates, even if test files mention the",
        "    identifier more frequently.",
        "",
        "    Args:",
        "        doc_results: List of (doc_id, score) tuples",
        "        query_text: The original search query",
        "        documents: Dict mapping doc_id to document text",
        "",
        "    Returns:",
        "        Re-scored document results with definition boost applied",
        "    \"\"\"",
        "    definition_info = detect_definition_query(query_text)",
        "",
        "    if not definition_info['is_definition_query'] or not definition_info['pattern']:",
        "        return doc_results",
        "",
        "    pattern = re.compile(definition_info['pattern'], re.IGNORECASE)",
        "    boosted_docs = []",
        "",
        "    for doc_id, score in doc_results:",
        "        doc_text = documents.get(doc_id, '')",
        "        else:",
        "            boosted_docs.append((doc_id, score))",
        "",
        "    # Re-sort by boosted scores",
        "    boosted_docs.sort(key=lambda x: x[1], reverse=True)",
        "    return boosted_docs",
        "",
        "",
        "def parse_intent_query(query_text: str) -> ParsedIntent:",
        "    \"\"\"",
        "    Parse a natural language query to extract intent and searchable terms."
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/semantics.py",
      "function": "def get_pattern_statistics(relations: List[Tuple[str, str, str, float]]) -> Dict",
      "start_line": 216,
      "lines_added": [
        "    min_pattern_confidence: float = 0.6,",
        "    max_similarity_pairs: int = 100000,",
        "    min_context_keys: int = 3",
        "        max_similarity_pairs: Maximum pairs to check for SimilarTo relations.",
        "            Set to 0 for unlimited (may be slow for large corpora). Default 100000.",
        "        min_context_keys: Minimum context keys for a term to be considered for",
        "            SimilarTo relations. Terms with fewer keys are skipped. Default 3."
      ],
      "lines_removed": [
        "    min_pattern_confidence: float = 0.6"
      ],
      "context_before": [
        "    }",
        "",
        "",
        "def extract_corpus_semantics(",
        "    layers: Dict[CorticalLayer, HierarchicalLayer],",
        "    documents: Dict[str, str],",
        "    tokenizer,",
        "    window_size: int = 5,",
        "    min_cooccurrence: int = 2,",
        "    use_pattern_extraction: bool = True,"
      ],
      "context_after": [
        ") -> List[Tuple[str, str, str, float]]:",
        "    \"\"\"",
        "    Extract semantic relations from corpus co-occurrence patterns.",
        "",
        "    Analyzes word co-occurrences to infer semantic relationships:",
        "    - Words appearing together frequently â†’ CoOccurs",
        "    - Words appearing in similar contexts â†’ SimilarTo",
        "    - Pattern-based extraction â†’ IsA, HasA, UsedFor, Causes, etc.",
        "",
        "    Args:",
        "        layers: Dictionary of layers (needs TOKENS)",
        "        documents: Dictionary of documents",
        "        tokenizer: Tokenizer instance for processing text",
        "        window_size: Co-occurrence window size",
        "        min_cooccurrence: Minimum co-occurrences to form relation",
        "        use_pattern_extraction: Whether to extract relations from text patterns",
        "        min_pattern_confidence: Minimum confidence for pattern-based extraction",
        "",
        "    Returns:",
        "        List of (term1, relation, term2, weight) tuples",
        "    \"\"\"",
        "    layer0 = layers[CorticalLayer.TOKENS]",
        "    relations: List[Tuple[str, str, str, float]] = []",
        "    ",
        "    # Track co-occurrences within window",
        "    cooccurrence: Dict[Tuple[str, str], int] = defaultdict(int)",
        "    "
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/semantics.py",
      "function": "def extract_corpus_semantics(",
      "start_line": 326,
      "lines_added": [
        "        # Fallback: pure Python implementation with optimizations",
        "        # Pre-filter terms by minimum context keys",
        "        key_sets: Dict[str, set] = {}",
        "",
        "            keys = set(vec.keys())",
        "            # Skip terms with too few context keys (can't meet min_context_keys threshold)",
        "            if len(keys) < min_context_keys:",
        "                continue",
        "            key_sets[term] = keys",
        "        # Get filtered terms with enough context",
        "        filtered_terms = [t for t in terms if t in key_sets and magnitudes.get(t, 0) > 0]",
        "",
        "        # Track pairs checked for early termination",
        "        pairs_checked = 0",
        "        for i, t1 in enumerate(filtered_terms):",
        "            for t2 in filtered_terms[i+1:]:",
        "                # Check pair limit",
        "                if max_similarity_pairs > 0 and pairs_checked >= max_similarity_pairs:",
        "                    break",
        "",
        "                pairs_checked += 1",
        "                if len(common) >= min_context_keys:",
        "            # Also check outer loop for pair limit",
        "            if max_similarity_pairs > 0 and pairs_checked >= max_similarity_pairs:",
        "                break",
        ""
      ],
      "lines_removed": [
        "        # Fallback: pure Python implementation",
        "        key_sets: Dict[str, set] = {term: set(context_vectors[term].keys()) for term in terms}",
        "        for i, t1 in enumerate(terms):",
        "            if mag1 == 0:",
        "                continue",
        "            for t2 in terms[i+1:]:",
        "                if mag2 == 0:",
        "                    continue",
        "                if len(common) >= 3:"
      ],
      "context_before": [
        "        # Extract pairs with similarity > 0.3 and at least 3 common keys",
        "        for i in range(n_terms):",
        "            row_i = nonzero_counts[i]",
        "            for j in range(i + 1, n_terms):",
        "                if similarities[i, j] > 0.3:",
        "                    common_count = np.sum(row_i & nonzero_counts[j])",
        "                    if common_count >= 3:",
        "                        relations.append((terms[i], 'SimilarTo', terms[j], float(similarities[i, j])))",
        "",
        "    elif n_terms > 1:"
      ],
      "context_after": [
        "        magnitudes: Dict[str, float] = {}",
        "        for term in terms:",
        "            vec = context_vectors[term]",
        "            mag = math.sqrt(sum(v * v for v in vec.values()))",
        "            magnitudes[term] = mag",
        "",
        "",
        "            vec1 = context_vectors[t1]",
        "            mag1 = magnitudes[t1]",
        "            keys1 = key_sets[t1]",
        "",
        "                mag2 = magnitudes[t2]",
        "",
        "                common = keys1 & key_sets[t2]",
        "                    vec2 = context_vectors[t2]",
        "                    dot = sum(vec1[k] * vec2[k] for k in common)",
        "                    sim = dot / (mag1 * mag2)",
        "                    if sim > 0.3:",
        "                        relations.append((t1, 'SimilarTo', t2, sim))",
        "",
        "    # Extract commonsense relations from text patterns",
        "    if use_pattern_extraction:",
        "        valid_terms = set(layer0.minicolumns.keys())",
        "        pattern_relations = extract_pattern_relations(",
        "            documents,",
        "            valid_terms,",
        "            min_confidence=min_pattern_confidence",
        "        )",
        "        relations.extend(pattern_relations)",
        ""
      ],
      "change_type": "modify"
    },
    {
      "file": "scripts/profile_full_analysis.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "#!/usr/bin/env python3",
        "\"\"\"",
        "Profile full-analysis to identify bottlenecks.",
        "",
        "This script profiles each phase of compute_all() separately to identify",
        "where time is being spent and why full-analysis hangs.",
        "",
        "Usage:",
        "    python scripts/profile_full_analysis.py",
        "    python scripts/profile_full_analysis.py --phase louvain",
        "    python scripts/profile_full_analysis.py --phase semantics",
        "    python scripts/profile_full_analysis.py --timeout 30",
        "\"\"\"",
        "",
        "import argparse",
        "import cProfile",
        "import pstats",
        "import io",
        "import time",
        "import signal",
        "import sys",
        "from pathlib import Path",
        "from contextlib import contextmanager",
        "from typing import Optional, Callable, Any",
        "",
        "# Add parent directory to path",
        "sys.path.insert(0, str(Path(__file__).parent.parent))",
        "",
        "from cortical.processor import CorticalTextProcessor",
        "from cortical.analysis import cluster_by_louvain, cluster_by_label_propagation",
        "from cortical.layers import CorticalLayer",
        "",
        "",
        "class TimeoutError(Exception):",
        "    \"\"\"Raised when operation times out.\"\"\"",
        "    pass",
        "",
        "",
        "@contextmanager",
        "def timeout(seconds: int, operation: str = \"operation\"):",
        "    \"\"\"Context manager for timing out long operations.\"\"\"",
        "    def handler(signum, frame):",
        "        raise TimeoutError(f\"{operation} timed out after {seconds}s\")",
        "",
        "    old_handler = signal.signal(signal.SIGALRM, handler)",
        "    signal.alarm(seconds)",
        "    try:",
        "        yield",
        "    finally:",
        "        signal.alarm(0)",
        "        signal.signal(signal.SIGALRM, old_handler)",
        "",
        "",
        "def profile_function(func: Callable, *args, timeout_sec: int = 60, **kwargs) -> tuple:",
        "    \"\"\"",
        "    Profile a function and return stats.",
        "",
        "    Returns:",
        "        (result, elapsed_time, profile_stats, timed_out)",
        "    \"\"\"",
        "    profiler = cProfile.Profile()",
        "    start = time.time()",
        "    result = None",
        "    timed_out = False",
        "",
        "    try:",
        "        with timeout(timeout_sec, func.__name__):",
        "            profiler.enable()",
        "            result = func(*args, **kwargs)",
        "            profiler.disable()",
        "    except TimeoutError as e:",
        "        profiler.disable()",
        "        timed_out = True",
        "        print(f\"\\nâš ï¸  {e}\")",
        "",
        "    elapsed = time.time() - start",
        "",
        "    # Get stats",
        "    stream = io.StringIO()",
        "    stats = pstats.Stats(profiler, stream=stream)",
        "    stats.sort_stats('cumulative')",
        "    stats.print_stats(20)",
        "",
        "    return result, elapsed, stream.getvalue(), timed_out",
        "",
        "",
        "def load_corpus(corpus_path: str) -> CorticalTextProcessor:",
        "    \"\"\"Load the corpus for profiling.\"\"\"",
        "    print(f\"Loading corpus from {corpus_path}...\")",
        "    processor = CorticalTextProcessor.load(corpus_path)",
        "",
        "    # Print corpus stats",
        "    layer0 = processor.layers.get(CorticalLayer.TOKENS)",
        "    layer1 = processor.layers.get(CorticalLayer.BIGRAMS)",
        "",
        "    print(f\"  Documents: {len(processor.documents)}\")",
        "    print(f\"  Tokens (Layer 0): {layer0.column_count() if layer0 else 0}\")",
        "    print(f\"  Bigrams (Layer 1): {layer1.column_count() if layer1 else 0}\")",
        "    print()",
        "",
        "    return processor",
        "",
        "",
        "def profile_louvain(processor: CorticalTextProcessor, timeout_sec: int = 60) -> dict:",
        "    \"\"\"Profile Louvain clustering.\"\"\"",
        "    print(\"=\" * 60)",
        "    print(\"PROFILING: Louvain Clustering\")",
        "    print(\"=\" * 60)",
        "",
        "    layer0 = processor.layers.get(CorticalLayer.TOKENS)",
        "    if not layer0:",
        "        print(\"No token layer found!\")",
        "        return {}",
        "",
        "    print(f\"Layer 0 has {layer0.column_count()} minicolumns\")",
        "    print(f\"Total connections: {layer0.total_connections()}\")",
        "    print()",
        "",
        "    result, elapsed, stats, timed_out = profile_function(",
        "        cluster_by_louvain,",
        "        layer0,",
        "        min_cluster_size=3,",
        "        resolution=1.0,",
        "        timeout_sec=timeout_sec",
        "    )",
        "",
        "    print(f\"\\nElapsed: {elapsed:.2f}s {'(TIMED OUT)' if timed_out else ''}\")",
        "    if result:",
        "        print(f\"Clusters found: {len(result)}\")",
        "    print(\"\\nTop 20 functions by cumulative time:\")",
        "    print(stats)",
        "",
        "    return {",
        "        'phase': 'louvain',",
        "        'elapsed': elapsed,",
        "        'timed_out': timed_out,",
        "        'clusters': len(result) if result else 0",
        "    }",
        "",
        "",
        "def profile_label_propagation(processor: CorticalTextProcessor, timeout_sec: int = 60) -> dict:",
        "    \"\"\"Profile label propagation clustering (legacy).\"\"\"",
        "    print(\"=\" * 60)",
        "    print(\"PROFILING: Label Propagation Clustering\")",
        "    print(\"=\" * 60)",
        "",
        "    layer0 = processor.layers.get(CorticalLayer.TOKENS)",
        "    if not layer0:",
        "        print(\"No token layer found!\")",
        "        return {}",
        "",
        "    print(f\"Layer 0 has {layer0.column_count()} minicolumns\")",
        "    print()",
        "",
        "    result, elapsed, stats, timed_out = profile_function(",
        "        cluster_by_label_propagation,",
        "        layer0,",
        "        min_cluster_size=3,",
        "        timeout_sec=timeout_sec",
        "    )",
        "",
        "    print(f\"\\nElapsed: {elapsed:.2f}s {'(TIMED OUT)' if timed_out else ''}\")",
        "    if result:",
        "        print(f\"Clusters found: {len(result)}\")",
        "    print(\"\\nTop 20 functions by cumulative time:\")",
        "    print(stats)",
        "",
        "    return {",
        "        'phase': 'label_propagation',",
        "        'elapsed': elapsed,",
        "        'timed_out': timed_out,",
        "        'clusters': len(result) if result else 0",
        "    }",
        "",
        "",
        "def profile_semantics(processor: CorticalTextProcessor, timeout_sec: int = 60) -> dict:",
        "    \"\"\"Profile semantic relation extraction.\"\"\"",
        "    print(\"=\" * 60)",
        "    print(\"PROFILING: Semantic Relation Extraction\")",
        "    print(\"=\" * 60)",
        "",
        "    print(f\"Documents: {len(processor.documents)}\")",
        "    total_chars = sum(len(doc) for doc in processor.documents.values())",
        "    print(f\"Total characters: {total_chars:,}\")",
        "    print()",
        "",
        "    result, elapsed, stats, timed_out = profile_function(",
        "        processor.extract_corpus_semantics,",
        "        use_pattern_extraction=True,",
        "        verbose=False,",
        "        timeout_sec=timeout_sec",
        "    )",
        "",
        "    print(f\"\\nElapsed: {elapsed:.2f}s {'(TIMED OUT)' if timed_out else ''}\")",
        "    print(f\"Relations extracted: {len(processor.semantic_relations)}\")",
        "    print(\"\\nTop 20 functions by cumulative time:\")",
        "    print(stats)",
        "",
        "    return {",
        "        'phase': 'semantics',",
        "        'elapsed': elapsed,",
        "        'timed_out': timed_out,",
        "        'relations': len(processor.semantic_relations)",
        "    }",
        "",
        "",
        "def profile_bigram_connections(processor: CorticalTextProcessor, timeout_sec: int = 60) -> dict:",
        "    \"\"\"Profile bigram connection computation.\"\"\"",
        "    print(\"=\" * 60)",
        "    print(\"PROFILING: Bigram Connections\")",
        "    print(\"=\" * 60)",
        "",
        "    layer1 = processor.layers.get(CorticalLayer.BIGRAMS)",
        "    if layer1:",
        "        print(f\"Bigrams: {layer1.column_count()}\")",
        "    print()",
        "",
        "    result, elapsed, stats, timed_out = profile_function(",
        "        processor.compute_bigram_connections,",
        "        verbose=False,",
        "        timeout_sec=timeout_sec",
        "    )",
        "",
        "    print(f\"\\nElapsed: {elapsed:.2f}s {'(TIMED OUT)' if timed_out else ''}\")",
        "    print(\"\\nTop 20 functions by cumulative time:\")",
        "    print(stats)",
        "",
        "    return {",
        "        'phase': 'bigram_connections',",
        "        'elapsed': elapsed,",
        "        'timed_out': timed_out",
        "    }",
        "",
        "",
        "def profile_all_phases(processor: CorticalTextProcessor, timeout_sec: int = 30) -> list:",
        "    \"\"\"Profile all phases of full-analysis.\"\"\"",
        "    results = []",
        "",
        "    # Fast phases first",
        "    print(\"\\n\" + \"=\" * 60)",
        "    print(\"PHASE 1: Activation Propagation\")",
        "    print(\"=\" * 60)",
        "    start = time.time()",
        "    processor.propagate_activation(verbose=False)",
        "    elapsed = time.time() - start",
        "    print(f\"Elapsed: {elapsed:.2f}s\")",
        "    results.append({'phase': 'activation', 'elapsed': elapsed, 'timed_out': False})",
        "",
        "    print(\"\\n\" + \"=\" * 60)",
        "    print(\"PHASE 2: PageRank\")",
        "    print(\"=\" * 60)",
        "    start = time.time()",
        "    processor.compute_importance(verbose=False)",
        "    elapsed = time.time() - start",
        "    print(f\"Elapsed: {elapsed:.2f}s\")",
        "    results.append({'phase': 'pagerank', 'elapsed': elapsed, 'timed_out': False})",
        "",
        "    print(\"\\n\" + \"=\" * 60)",
        "    print(\"PHASE 3: TF-IDF\")",
        "    print(\"=\" * 60)",
        "    start = time.time()",
        "    processor.compute_tfidf(verbose=False)",
        "    elapsed = time.time() - start",
        "    print(f\"Elapsed: {elapsed:.2f}s\")",
        "    results.append({'phase': 'tfidf', 'elapsed': elapsed, 'timed_out': False})",
        "",
        "    print(\"\\n\" + \"=\" * 60)",
        "    print(\"PHASE 4: Document Connections\")",
        "    print(\"=\" * 60)",
        "    start = time.time()",
        "    processor.compute_document_connections(verbose=False)",
        "    elapsed = time.time() - start",
        "    print(f\"Elapsed: {elapsed:.2f}s\")",
        "    results.append({'phase': 'doc_connections', 'elapsed': elapsed, 'timed_out': False})",
        "",
        "    # Potentially slow phases",
        "    results.append(profile_bigram_connections(processor, timeout_sec))",
        "    results.append(profile_louvain(processor, timeout_sec))",
        "    results.append(profile_semantics(processor, timeout_sec))",
        "",
        "    return results",
        "",
        "",
        "def print_summary(results: list):",
        "    \"\"\"Print profiling summary.\"\"\"",
        "    print(\"\\n\" + \"=\" * 60)",
        "    print(\"PROFILING SUMMARY\")",
        "    print(\"=\" * 60)",
        "",
        "    total_time = sum(r['elapsed'] for r in results)",
        "",
        "    print(f\"\\n{'Phase':<25} {'Time':>10} {'Status':>15}\")",
        "    print(\"-\" * 50)",
        "",
        "    for r in results:",
        "        status = \"TIMED OUT\" if r.get('timed_out') else \"OK\"",
        "        print(f\"{r['phase']:<25} {r['elapsed']:>9.2f}s {status:>15}\")",
        "",
        "    print(\"-\" * 50)",
        "    print(f\"{'TOTAL':<25} {total_time:>9.2f}s\")",
        "",
        "    # Identify bottleneck",
        "    slowest = max(results, key=lambda x: x['elapsed'])",
        "    print(f\"\\nðŸ” BOTTLENECK: {slowest['phase']} ({slowest['elapsed']:.2f}s)\")",
        "",
        "    timed_out = [r for r in results if r.get('timed_out')]",
        "    if timed_out:",
        "        print(f\"\\nâš ï¸  TIMED OUT PHASES: {', '.join(r['phase'] for r in timed_out)}\")",
        "",
        "",
        "def main():",
        "    parser = argparse.ArgumentParser(description=\"Profile full-analysis bottlenecks\")",
        "    parser.add_argument('--corpus', default='corpus_dev.pkl',",
        "                       help='Path to corpus file')",
        "    parser.add_argument('--phase', choices=['all', 'louvain', 'label_propagation',",
        "                                            'semantics', 'bigram'],",
        "                       default='all', help='Phase to profile')",
        "    parser.add_argument('--timeout', type=int, default=30,",
        "                       help='Timeout per phase in seconds')",
        "",
        "    args = parser.parse_args()",
        "",
        "    corpus_path = Path(args.corpus)",
        "    if not corpus_path.exists():",
        "        print(f\"Corpus not found: {corpus_path}\")",
        "        print(\"Run: python scripts/index_codebase.py first\")",
        "        sys.exit(1)",
        "",
        "    processor = load_corpus(str(corpus_path))",
        "",
        "    if args.phase == 'all':",
        "        results = profile_all_phases(processor, args.timeout)",
        "        print_summary(results)",
        "    elif args.phase == 'louvain':",
        "        profile_louvain(processor, args.timeout)",
        "    elif args.phase == 'label_propagation':",
        "        profile_label_propagation(processor, args.timeout)",
        "    elif args.phase == 'semantics':",
        "        profile_semantics(processor, args.timeout)",
        "    elif args.phase == 'bigram':",
        "        profile_bigram_connections(processor, args.timeout)",
        "",
        "",
        "if __name__ == '__main__':",
        "    main()"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "tests/test_query.py",
      "function": "class TestQueryRelatedDocuments(unittest.TestCase):",
      "start_line": 1789,
      "lines_added": [
        "class TestIsTestFile(unittest.TestCase):",
        "    \"\"\"Test the is_test_file detection function.\"\"\"",
        "",
        "    def test_tests_directory(self):",
        "        \"\"\"Test detection of files in tests/ directory.\"\"\"",
        "        from cortical.query import is_test_file",
        "",
        "        self.assertTrue(is_test_file(\"tests/test_query.py\"))",
        "        self.assertTrue(is_test_file(\"project/tests/test_module.py\"))",
        "        self.assertTrue(is_test_file(\"/home/user/tests/helpers.py\"))",
        "",
        "    def test_test_directory(self):",
        "        \"\"\"Test detection of files in test/ directory.\"\"\"",
        "        from cortical.query import is_test_file",
        "",
        "        self.assertTrue(is_test_file(\"test/test_query.py\"))",
        "        self.assertTrue(is_test_file(\"project/test/conftest.py\"))",
        "",
        "    def test_test_prefix_filename(self):",
        "        \"\"\"Test detection of test_ prefixed files.\"\"\"",
        "        from cortical.query import is_test_file",
        "",
        "        self.assertTrue(is_test_file(\"test_module.py\"))",
        "        self.assertTrue(is_test_file(\"src/test_helpers.py\"))",
        "",
        "    def test_test_suffix_filename(self):",
        "        \"\"\"Test detection of _test.py suffixed files.\"\"\"",
        "        from cortical.query import is_test_file",
        "",
        "        self.assertTrue(is_test_file(\"module_test.py\"))",
        "        self.assertTrue(is_test_file(\"src/helpers_test.py\"))",
        "",
        "    def test_mock_files(self):",
        "        \"\"\"Test detection of mock files.\"\"\"",
        "        from cortical.query import is_test_file",
        "",
        "        self.assertTrue(is_test_file(\"mock_service.py\"))",
        "        self.assertTrue(is_test_file(\"mocks/mock_data.py\"))",
        "",
        "    def test_fixture_files(self):",
        "        \"\"\"Test detection of fixture files.\"\"\"",
        "        from cortical.query import is_test_file",
        "",
        "        self.assertTrue(is_test_file(\"fixtures.py\"))",
        "        self.assertTrue(is_test_file(\"test_fixtures.py\"))",
        "",
        "    def test_source_files_not_detected(self):",
        "        \"\"\"Test that source files are not detected as test files.\"\"\"",
        "        from cortical.query import is_test_file",
        "",
        "        self.assertFalse(is_test_file(\"cortical/query.py\"))",
        "        self.assertFalse(is_test_file(\"cortical/analysis.py\"))",
        "        self.assertFalse(is_test_file(\"src/module.py\"))",
        "        self.assertFalse(is_test_file(\"main.py\"))",
        "        self.assertFalse(is_test_file(\"scripts/run.py\"))",
        "",
        "",
        "class TestBoostDefinitionDocumentsTestFilePenalty(unittest.TestCase):",
        "    \"\"\"Test that definition boost correctly penalizes test files.\"\"\"",
        "",
        "    def test_source_file_boosted_over_test_file(self):",
        "        \"\"\"Test that source files are ranked higher than test files with same base score.\"\"\"",
        "        from cortical.query import boost_definition_documents",
        "",
        "        # Simulate documents with the same base relevance score",
        "        doc_results = [",
        "            (\"tests/test_analysis.py\", 10.0),",
        "            (\"cortical/analysis.py\", 10.0),",
        "        ]",
        "",
        "        documents = {",
        "            \"tests/test_analysis.py\": \"def compute_pagerank(layers, damping=0.85): pass  # mock\",",
        "            \"cortical/analysis.py\": \"def compute_pagerank(layers, damping=0.85):\\n    '''Real implementation'''\\n    result = do_stuff()\",",
        "        }",
        "",
        "        boosted = boost_definition_documents(",
        "            doc_results,",
        "            \"def compute_pagerank\",",
        "            documents,",
        "            boost_factor=2.0,",
        "            test_file_boost_factor=0.5",
        "        )",
        "",
        "        # Source file should be ranked first after boosting",
        "        self.assertEqual(boosted[0][0], \"cortical/analysis.py\")",
        "        # Source file gets 2.0x boost: 10.0 * 2.0 = 20.0",
        "        self.assertEqual(boosted[0][1], 20.0)",
        "        # Test file gets 0.5x penalty: 10.0 * 0.5 = 5.0",
        "        self.assertEqual(boosted[1][1], 5.0)",
        "",
        "    def test_test_file_penalty_can_be_disabled(self):",
        "        \"\"\"Test that test_file_boost_factor=1.0 disables the penalty.\"\"\"",
        "        from cortical.query import boost_definition_documents",
        "",
        "        doc_results = [",
        "            (\"tests/test_module.py\", 10.0),",
        "            (\"src/module.py\", 10.0),",
        "        ]",
        "",
        "        documents = {",
        "            \"tests/test_module.py\": \"def my_func(): pass\",",
        "            \"src/module.py\": \"def my_func(): return 42\",",
        "        }",
        "",
        "        boosted = boost_definition_documents(",
        "            doc_results,",
        "            \"def my_func\",",
        "            documents,",
        "            boost_factor=2.0,",
        "            test_file_boost_factor=1.0  # No penalty",
        "        )",
        "",
        "        # Both should get the same boost when penalty is disabled",
        "        scores = {doc_id: score for doc_id, score in boosted}",
        "        # Test file doesn't get full boost, it gets test_file_boost_factor (1.0 here means no change)",
        "        # Wait, if test_file_boost_factor=1.0, test file gets 10.0 * 1.0 = 10.0",
        "        # Source file gets 10.0 * 2.0 = 20.0",
        "        self.assertEqual(scores[\"src/module.py\"], 20.0)",
        "        self.assertEqual(scores[\"tests/test_module.py\"], 10.0)",
        "",
        "    def test_non_definition_query_unchanged(self):",
        "        \"\"\"Test that non-definition queries are not affected.\"\"\"",
        "        from cortical.query import boost_definition_documents",
        "",
        "        doc_results = [",
        "            (\"tests/test_query.py\", 10.0),",
        "            (\"cortical/query.py\", 8.0),",
        "        ]",
        "",
        "        documents = {",
        "            \"tests/test_query.py\": \"testing query functionality\",",
        "            \"cortical/query.py\": \"query implementation code\",",
        "        }",
        "",
        "        # Non-definition query (no \"def\", \"class\", etc.)",
        "        boosted = boost_definition_documents(",
        "            doc_results,",
        "            \"query functionality\",",
        "            documents,",
        "            boost_factor=2.0",
        "        )",
        "",
        "        # Scores should be unchanged",
        "        self.assertEqual(boosted[0], (\"tests/test_query.py\", 10.0))",
        "        self.assertEqual(boosted[1], (\"cortical/query.py\", 8.0))",
        "",
        "    def test_test_files_without_definition_penalized(self):",
        "        \"\"\"Test that test files without the definition are penalized for definition queries.\"\"\"",
        "        from cortical.query import boost_definition_documents",
        "",
        "        # Test files that just mention the function but don't define it",
        "        doc_results = [",
        "            (\"tests/test_processor.py\", 100.0),  # High score, no definition",
        "            (\"cortical/analysis.py\", 80.0),       # Lower score, has definition",
        "        ]",
        "",
        "        documents = {",
        "            \"tests/test_processor.py\": \"from analysis import compute_pagerank; result = compute_pagerank()\",",
        "            \"cortical/analysis.py\": \"def compute_pagerank(layers, damping=0.85):\\n    return pagerank_impl()\",",
        "        }",
        "",
        "        boosted = boost_definition_documents(",
        "            doc_results,",
        "            \"def compute_pagerank\",",
        "            documents,",
        "            boost_factor=2.0,",
        "            test_file_boost_factor=0.5,",
        "            test_file_penalty=0.7",
        "        )",
        "",
        "        # Source file with definition should now rank first",
        "        # Source file gets 2.0x: 80.0 * 2.0 = 160.0",
        "        # Test file without definition gets 0.7x penalty: 100.0 * 0.7 = 70.0",
        "        self.assertEqual(boosted[0][0], \"cortical/analysis.py\")",
        "        self.assertEqual(boosted[0][1], 160.0)",
        "        self.assertEqual(boosted[1][1], 70.0)",
        "",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "",
        "        related = find_related_documents(",
        "            \"doc1\",",
        "            self.processor.layers",
        "        )",
        "",
        "        # Should return a list",
        "        self.assertIsInstance(related, list)",
        "",
        ""
      ],
      "context_after": [
        "if __name__ == '__main__':",
        "    unittest.main()"
      ],
      "change_type": "add"
    },
    {
      "file": "tests/test_semantics.py",
      "function": "class TestProcessorPatternExtraction(unittest.TestCase):",
      "start_line": 834,
      "lines_added": [
        "class TestSimilarToRelationExtraction(unittest.TestCase):",
        "    \"\"\"Test SimilarTo relation extraction with context similarity.\"\"\"",
        "",
        "    def test_similarto_with_shared_context(self):",
        "        \"\"\"Test SimilarTo extraction when terms share context.\"\"\"",
        "        from cortical.processor import CorticalTextProcessor",
        "        from cortical.semantics import extract_corpus_semantics",
        "",
        "        # Create corpus with terms that share context",
        "        # \"apple\" and \"orange\" both appear near \"fruit\", \"eat\", \"fresh\", \"juice\"",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(\"doc1\",",
        "            \"I eat fresh apple fruit. Apple juice is healthy. The apple is fresh.\")",
        "        processor.process_document(\"doc2\",",
        "            \"I eat fresh orange fruit. Orange juice is healthy. The orange is fresh.\")",
        "        processor.process_document(\"doc3\",",
        "            \"Fresh fruit juice from apple and orange. Eat fresh fruit daily.\")",
        "        processor.compute_all(verbose=False)",
        "",
        "        relations = extract_corpus_semantics(",
        "            processor.layers,",
        "            processor.documents,",
        "            processor.tokenizer,",
        "            window_size=5,",
        "            min_cooccurrence=2,",
        "            use_pattern_extraction=False  # Only test similarity",
        "        )",
        "",
        "        # Check that we get some relations",
        "        self.assertIsInstance(relations, list)",
        "",
        "        # Check relation types",
        "        relation_types = set(r[1] for r in relations)",
        "        # Should have CoOccurs at minimum",
        "        self.assertIn('CoOccurs', relation_types)",
        "",
        "    def test_extract_corpus_semantics_similarto_threshold(self):",
        "        \"\"\"Test that SimilarTo respects similarity threshold.\"\"\"",
        "        from cortical.processor import CorticalTextProcessor",
        "        from cortical.semantics import extract_corpus_semantics",
        "",
        "        # Create documents with overlapping terms",
        "        processor = CorticalTextProcessor()",
        "        for i in range(5):",
        "            processor.process_document(f\"doc{i}\",",
        "                f\"The quick brown fox jumps over the lazy dog. \"",
        "                f\"Quick foxes are brown and lazy dogs sleep. \"",
        "                f\"Brown quick lazy fox dog jump sleep.\")",
        "        processor.compute_all(verbose=False)",
        "",
        "        relations = extract_corpus_semantics(",
        "            processor.layers,",
        "            processor.documents,",
        "            processor.tokenizer,",
        "            window_size=3,",
        "            min_cooccurrence=2,",
        "            use_pattern_extraction=False",
        "        )",
        "",
        "        # Verify relation structure",
        "        for rel in relations:",
        "            self.assertEqual(len(rel), 4)",
        "            term1, rel_type, term2, weight = rel",
        "            self.assertIn(rel_type, ['CoOccurs', 'SimilarTo'])",
        "            self.assertGreater(weight, 0)",
        "            # SimilarTo is 0-1, but CoOccurs can be higher (count-based)",
        "            if rel_type == 'SimilarTo':",
        "                self.assertLessEqual(weight, 1.0)",
        "",
        "",
        "class TestBigramConnectionsVerbose(unittest.TestCase):",
        "    \"\"\"Test bigram connection verbose output and new parameters.\"\"\"",
        "",
        "    def test_max_bigrams_per_term_parameter(self):",
        "        \"\"\"Test that max_bigrams_per_term skips common terms.\"\"\"",
        "        from cortical.processor import CorticalTextProcessor",
        "",
        "        processor = CorticalTextProcessor()",
        "        # Create documents with common bigram prefix \"data\" (not a stop word)",
        "        for i in range(20):",
        "            processor.process_document(f\"doc{i}\",",
        "                f\"data processing data analysis data mining data science \"",
        "                f\"data engineering data storage data pipeline data flow\")",
        "        processor.compute_all(verbose=False, build_concepts=False)",
        "",
        "        # With very low threshold, should skip \"data\" as it appears in many bigrams",
        "        stats = processor.compute_bigram_connections(",
        "            max_bigrams_per_term=3,",
        "            verbose=False",
        "        )",
        "",
        "        self.assertIn('skipped_common_terms', stats)",
        "        self.assertGreater(stats['skipped_common_terms'], 0)",
        "",
        "    def test_max_bigrams_per_doc_parameter(self):",
        "        \"\"\"Test that max_bigrams_per_doc skips large documents.\"\"\"",
        "        from cortical.processor import CorticalTextProcessor",
        "",
        "        processor = CorticalTextProcessor()",
        "        # Create one large document and several small ones",
        "        large_doc = \" \".join([f\"word{i} word{i+1}\" for i in range(200)])",
        "        processor.process_document(\"large\", large_doc)",
        "        for i in range(5):",
        "            processor.process_document(f\"small{i}\", \"simple short document here\")",
        "        processor.compute_all(verbose=False, build_concepts=False)",
        "",
        "        # With low threshold, should skip the large document",
        "        stats = processor.compute_bigram_connections(",
        "            max_bigrams_per_doc=50,",
        "            verbose=False",
        "        )",
        "",
        "        self.assertIn('skipped_large_docs', stats)",
        "        self.assertGreater(stats['skipped_large_docs'], 0)",
        "",
        "    def test_bigram_connections_returns_all_stats(self):",
        "        \"\"\"Test that bigram connections returns complete statistics.\"\"\"",
        "        from cortical.processor import CorticalTextProcessor",
        "",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(\"doc1\", \"machine learning algorithms work well\")",
        "        processor.process_document(\"doc2\", \"deep learning neural networks train fast\")",
        "        processor.compute_all(verbose=False, build_concepts=False)",
        "",
        "        stats = processor.compute_bigram_connections(verbose=False)",
        "",
        "        # Check all expected keys",
        "        expected_keys = [",
        "            'connections_created', 'bigrams', 'component_connections',",
        "            'chain_connections', 'cooccurrence_connections',",
        "            'skipped_common_terms', 'skipped_large_docs'",
        "        ]",
        "        for key in expected_keys:",
        "            self.assertIn(key, stats)",
        "",
        "",
        "class TestProcessorVerboseOutput(unittest.TestCase):",
        "    \"\"\"Test verbose output messages.\"\"\"",
        "",
        "    def test_compute_bigram_connections_verbose_skipped(self):",
        "        \"\"\"Test verbose output includes skipped info.\"\"\"",
        "        import io",
        "        import sys",
        "        from cortical.processor import CorticalTextProcessor",
        "",
        "        processor = CorticalTextProcessor()",
        "        for i in range(15):",
        "            processor.process_document(f\"doc{i}\",",
        "                f\"the quick brown fox jumps over the lazy dog number {i}\")",
        "        processor.compute_all(verbose=False, build_concepts=False)",
        "",
        "        # Capture stdout",
        "        captured = io.StringIO()",
        "        sys.stdout = captured",
        "        try:",
        "            processor.compute_bigram_connections(",
        "                max_bigrams_per_term=3,",
        "                verbose=True",
        "            )",
        "        finally:",
        "            sys.stdout = sys.__stdout__",
        "",
        "        output = captured.getvalue()",
        "        # Should mention \"bigram connections\"",
        "        self.assertIn('bigram connections', output)",
        "",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "",
        "        relations_high = self.processor.extract_pattern_relations(",
        "            min_confidence=0.9,",
        "            verbose=False",
        "        )",
        "",
        "        # Lower confidence should find at least as many",
        "        self.assertGreaterEqual(len(relations_low), len(relations_high))",
        "",
        ""
      ],
      "context_after": [
        "if __name__ == \"__main__\":",
        "    unittest.main(verbosity=2)"
      ],
      "change_type": "add"
    }
  ],
  "hour_of_day": 23,
  "day_of_week": "Thursday",
  "seconds_since_last_commit": -312052,
  "is_merge": true,
  "is_initial": false,
  "parent_count": 2,
  "session_id": null,
  "related_chats": [],
  "ci_result": null,
  "reverted": false,
  "amended": false
}