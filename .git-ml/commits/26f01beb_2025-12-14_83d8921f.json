{
  "hash": "26f01bebff031b5649975bbb06c9022ae79f1893",
  "message": "Merge pull request #73 from scrawlsbenches/claude/security-review-84jN0",
  "author": "scrawlsbenches",
  "timestamp": "2025-12-14 05:52:10 -0500",
  "branch": "claude/multi-index-design-DvifZ",
  "files_changed": [
    "CODE_REVIEW.md",
    "SECURITY_REVIEW.md",
    "docs/security-knowledge-transfer.md",
    "samples/api_design_security.txt",
    "samples/application_security_fundamentals.txt",
    "samples/authentication_patterns.txt",
    "samples/configuration_management.txt",
    "samples/dependency_management_practices.txt",
    "samples/devsecops_practices.txt",
    "samples/incident_response_procedures.txt",
    "samples/input_validation_patterns.txt",
    "samples/network_security_fundamentals.txt",
    "samples/penetration_testing_methodology.txt",
    "samples/secrets_management.txt",
    "samples/secure_code_review.txt",
    "samples/secure_deserialization.txt",
    "samples/secure_development_lifecycle.txt",
    "samples/security_compliance_frameworks.txt",
    "samples/static_analysis_tools.txt",
    "samples/supply_chain_security.txt",
    "samples/threat_modeling.txt",
    "secureshowcase.py"
  ],
  "insertions": 2209,
  "deletions": 159,
  "hunks": [
    {
      "file": "CODE_REVIEW.md",
      "function": null,
      "start_line": 1,
      "lines_added": [
        "# Code Quality Review Report",
        "**Date:** 2025-12-14",
        "**Reviewer:** Claude (Automated Code Review)",
        "**Scope:** Code smells, clean code issues, symbolic misinterpretations",
        "This review identifies code quality issues across the Cortical Text Processor codebase. The code is generally well-structured with good documentation, but several patterns indicate opportunities for improvement.",
        "| Category | Severity | Count |",
        "|----------|----------|-------|",
        "| God Class | High | 1 |",
        "| Deprecated Code Still Used | Medium | 1 |",
        "| Naming Inconsistencies | Medium | 4 |",
        "| Code Duplication | Medium | 3 |",
        "| Magic Numbers | Low | 5 |",
        "| Minor Clean Code Issues | Low | 6 |",
        "## 1. God Class Anti-Pattern",
        "",
        "### Issue: CorticalTextProcessor is Too Large",
        "",
        "**File:** `cortical/processor.py`",
        "**Lines:** 3115 lines",
        "**Methods:** 70+ public methods",
        "",
        "The `CorticalTextProcessor` class violates the Single Responsibility Principle. It handles:",
        "- Document processing",
        "- TF-IDF computation",
        "- PageRank computation",
        "- Query expansion",
        "- Semantic analysis",
        "- Fingerprinting",
        "- Persistence",
        "- Concept clustering",
        "- Graph embeddings",
        "- And more...",
        "",
        "**Symptoms:**",
        "- File is over 3000 lines",
        "- Class has 70+ methods",
        "- Many methods are thin delegators to other modules",
        "- Difficult to test individual components",
        "",
        "**Recommendation:**",
        "Consider extracting cohesive functionality into separate classes:",
        "```",
        "CorticalTextProcessor (core orchestration only)",
        "â”œâ”€â”€ DocumentManager (add/remove/batch documents)",
        "â”œâ”€â”€ ComputationEngine (TF-IDF, PageRank, embeddings)",
        "â”œâ”€â”€ QueryEngine (search, expansion, ranking)",
        "â”œâ”€â”€ SemanticAnalyzer (relations, concepts, retrofitting)",
        "â””â”€â”€ PersistenceManager (save/load)",
        "```",
        "",
        "---",
        "## 2. Deprecated Code Still in Use",
        "### Issue: feedforward_sources is Deprecated but Actively Used",
        "**File:** `cortical/minicolumn.py:76, 118`",
        "feedforward_sources: IDs of columns that feed into this one (deprecated, use feedforward_connections)",
        "...",
        "self.feedforward_sources: Set[str] = set()  # Deprecated: use feedforward_connections",
        "**Problem:**",
        "The field is marked as deprecated in comments, but:",
        "- It's still maintained in `add_feedforward_connection()` (line 390-391)",
        "- It's still serialized in `to_dict()` (line 448)",
        "- It's still used in `analysis.py:967` and `analysis.py:1507`",
        "- It's used in 20+ test files",
        "",
        "**Impact:**",
        "- Maintenance burden (must keep both in sync)",
        "- Confusion for developers",
        "- Memory overhead (duplicate data)",
        "",
        "**Recommendation:**",
        "Either:",
        "1. Remove the deprecated field completely and migrate all usages",
        "2. Or remove the deprecation comment if it's still needed",
        "",
        "---",
        "## 3. Naming Inconsistencies",
        "### 3.1 Layer Variable Naming",
        "",
        "**Pattern:** `layer0`, `layer1`, `layer2`, `layer3` vs semantic names",
        "",
        "```python",
        "# In processor.py",
        "layer0 = self.layers[CorticalLayer.TOKENS]",
        "layer1 = self.layers[CorticalLayer.BIGRAMS]",
        "layer3 = self.layers[CorticalLayer.DOCUMENTS]  # Note: layer2 skipped",
        "",
        "# Better naming would be:",
        "token_layer = self.layers[CorticalLayer.TOKENS]",
        "bigram_layer = self.layers[CorticalLayer.BIGRAMS]",
        "document_layer = self.layers[CorticalLayer.DOCUMENTS]",
        "**Files affected:** `processor.py`, `showcase.py`, `analysis.py`, multiple test files",
        "",
        "**Issue:**",
        "- Numeric names don't convey meaning",
        "- `layer2` (CONCEPTS) is often skipped, making the pattern confusing",
        "- Code uses `layer3` for documents but also `doc_layer` in some places",
        "",
        "---",
        "",
        "### 3.2 Inconsistent Abbreviations",
        "",
        "| Full Name | Abbreviations Used |",
        "|-----------|-------------------|",
        "| document | `doc`, `document`, `docs` |",
        "| column | `col`, `column`, `minicolumn` |",
        "| connection | `conn`, `connection`, `conns` |",
        "**Example inconsistency:**",
        "# In minicolumn.py",
        "lateral_connections  # Full name",
        "feedforward_connections  # Full name",
        "doc_occurrence_counts  # Abbreviated",
        "",
        "# In analysis.py",
        "col_entries  # Abbreviated",
        "column_count()  # Full name",
        "### 3.3 Boolean Parameter Name Confusion",
        "**File:** `cortical/processor.py`",
        "",
        "```python",
        "def find_documents_for_query(",
        "    ...",
        "    use_expansion: bool = True,",
        "    use_semantic: bool = True,",
        "    ...",
        ")",
        "```",
        "**Issue:** `use_semantic` is ambiguous - semantic what? It controls whether semantic *relations* are used for *expansion*.",
        "**Better names:**",
        "- `expand_with_semantics: bool`",
        "- `include_semantic_relations: bool`",
        "### 3.4 Symbolic Misinterpretation: \"Minicolumn\"",
        "",
        "The term \"minicolumn\" comes from neuroscience (vertical columns of ~80-100 neurons), but in this codebase it represents:",
        "- A token (word)",
        "- A bigram",
        "- A concept cluster",
        "- A document",
        "**Issue:** The biological analogy breaks down at the document level - documents aren't \"mini\" anything.",
        "**Recommendation:** Consider renaming to more generic terms like `Node`, `Unit`, or `Feature` for the generic structure, with type-specific terms in documentation.",
        "---",
        "## 4. Code Duplication",
        "### 4.1 Checkpoint Handling Duplication",
        "**File:** `cortical/processor.py` (lines 800-960)",
        "The `compute_all()` method has highly repetitive checkpoint handling:",
        "```python",
        "# Repeated pattern ~10 times:",
        "phase_name = \"phase_x\"",
        "if phase_name in completed_phases:",
        "    if verbose:",
        "        logger.info(\"  Skipping X (already checkpointed)\")",
        "else:",
        "    progress.start_phase(\"X\")",
        "    if verbose:",
        "        logger.info(\"Computing X...\")",
        "    self.compute_x(verbose=False)",
        "    progress.update(100)",
        "    progress.complete_phase()",
        "    if checkpoint_dir:",
        "        self._save_checkpoint(checkpoint_dir, phase_name, verbose=verbose)",
        "```",
        "",
        "**Recommendation:** Extract to a helper method:",
        "```python",
        "def _run_phase(self, phase_name, compute_fn, description, ...):",
        "    if phase_name in completed_phases:",
        "        self._log_skip(phase_name)",
        "    else:",
        "        self._run_and_checkpoint(phase_name, compute_fn, description)",
        "```",
        "### 4.2 Input Validation Duplication",
        "**File:** `cortical/processor.py`",
        "Same validation pattern repeated in multiple methods:",
        "```python",
        "# Repeated in 10+ methods:",
        "if not isinstance(query_text, str) or not query_text.strip():",
        "    raise ValueError(\"...\")",
        "if not isinstance(top_n, int) or top_n < 1:",
        "    raise ValueError(\"...\")",
        "```",
        "**Recommendation:** Use the existing `validation.py` decorators more consistently:",
        "@validate_params(",
        "    query_text=lambda x: validate_non_empty_string(x, 'query_text'),",
        "    top_n=lambda x: validate_positive_int(x, 'top_n')",
        ")",
        "def find_documents_for_query(self, query_text: str, top_n: int = 5):",
        "    ...",
        "---",
        "",
        "### 4.3 Layer Access Pattern Duplication",
        "",
        "**Pattern:** Getting layer references is done inconsistently:",
        "",
        "# Pattern 1: Direct dictionary access",
        "layer0 = self.layers[CorticalLayer.TOKENS]",
        "",
        "# Pattern 2: Using get_layer method",
        "token_layer = self.get_layer(CorticalLayer.TOKENS)",
        "",
        "# Pattern 3: Re-importing in function",
        "from .layers import CorticalLayer",
        "layer0 = layers[CorticalLayer.TOKENS]",
        "**Recommendation:** Standardize on one approach, preferably using `get_layer()` for consistency and future extensibility.",
        "---",
        "## 5. Magic Numbers",
        "### 5.1 Hardcoded Thresholds",
        "**File:** `cortical/processor.py`",
        "```python",
        "# Line 145: Hardcoded window size",
        "for j in range(max(0, i-3), min(len(tokens), i+4)):  # Magic: 3, 4",
        "# Line 79: Cache size",
        "self._query_cache_max_size: int = 100  # Magic: 100",
        "```",
        "",
        "**Recommendation:** Move to `CorticalConfig`:",
        "```python",
        "@dataclass",
        "class CorticalConfig:",
        "    lateral_window_size: int = 3",
        "    query_cache_max_size: int = 100",
        "```",
        "### 5.2 Scattered Default Values",
        "Default values are scattered throughout the codebase:",
        "```python",
        "# processor.py",
        "def find_documents_for_query(..., doc_name_boost: float = 2.0):",
        "# query/search.py (same function)",
        "def find_documents_for_query(..., doc_name_boost: float = 2.0):",
        "# query/ranking.py",
        "candidate_multiplier: int = 3  # Default here too",
        "```",
        "**Issue:** If defaults need to change, multiple files must be updated.",
        "**Recommendation:** Centralize in `CorticalConfig` and reference from there.",
        "## 6. Clean Code Issues",
        "### 6.1 Long Parameter Lists",
        "",
        "**File:** `cortical/processor.py`",
        "",
        "```python",
        "def compute_all(",
        "    self,",
        "    verbose: bool = True,",
        "    show_progress: bool = True,",
        "    progress_callback: Optional[Callable[[str, float], None]] = None,",
        "    build_concepts: bool = True,",
        "    cluster_strictness: float = 1.0,",
        "    connection_strategy: str = 'hybrid',",
        "    bridge_weight: float = 0.5,",
        "    checkpoint_dir: Optional[str] = None",
        ") -> Dict[str, Any]:",
        "```",
        "",
        "**Issue:** 8 parameters is difficult to remember and use correctly.",
        "",
        "**Recommendation:** Use a configuration object:",
        "```python",
        "@dataclass",
        "class ComputeOptions:",
        "    verbose: bool = True",
        "    show_progress: bool = True",
        "    build_concepts: bool = True",
        "    cluster_strictness: float = 1.0",
        "    connection_strategy: str = 'hybrid'",
        "    bridge_weight: float = 0.5",
        "    checkpoint_dir: Optional[str] = None",
        "",
        "def compute_all(self, options: Optional[ComputeOptions] = None):",
        "    options = options or ComputeOptions()",
        "```",
        "### 6.2 Boolean Parameter Confusion",
        "**File:** `cortical/processor.py`",
        "```python",
        "# What does this call do?",
        "processor.compute_all(True, True, None, True, 1.0, 'hybrid', 0.5, None)",
        "```",
        "",
        "**Issue:** Positional booleans are unreadable.",
        "",
        "**Recommendation:** Always use keyword arguments for booleans:",
        "```python",
        "processor.compute_all(",
        "    verbose=True,",
        "    show_progress=True,",
        "    build_concepts=True",
        ")",
        "```",
        "### 6.3 Return Type Inconsistency",
        "Some methods return different structures for success vs failure:",
        "```python",
        "# MCP server returns dict with 'error' key on failure",
        "return {\"error\": str(e), \"results\": [], \"count\": 0}",
        "# But processor methods raise exceptions",
        "raise ValueError(\"doc_id must be a non-empty string\")",
        "```",
        "",
        "**Recommendation:** Be consistent - either always use exceptions or always use result objects.",
        "### 6.4 Comments That Should Be Code",
        "**File:** `cortical/minicolumn.py:390-391`",
        "```python",
        "# Also maintain legacy feedforward_sources for backward compatibility",
        "self.feedforward_sources.add(target_id)",
        "**Issue:** The comment explains what the code does, not why. The deprecation status should be in a migration plan, not a comment.",
        "### 6.5 Dead Code: Unused Imports",
        "",
        "**File:** Various",
        "",
        "```python",
        "# processor.py line 418 - imports inside function",
        "from .layers import CorticalLayer  # Already imported at top of file",
        "```",
        "---",
        "### 6.6 Inconsistent Error Messages",
        "```python",
        "# Some use 'must be'",
        "raise ValueError(\"doc_id must be a non-empty string\")",
        "",
        "# Some use 'is required'",
        "raise ValueError(\"query_text is required\")",
        "",
        "# Some include type info",
        "raise ValueError(f\"{param_name} must be a string, got {type(value).__name__}\")",
        "",
        "# Some don't",
        "raise ValueError(\"content must be a string\")",
        "```",
        "",
        "**Recommendation:** Standardize error message format.",
        "",
        "---",
        "",
        "## 7. Good Practices Observed",
        "",
        "Despite the issues above, the codebase demonstrates several good practices:",
        "",
        "1. **Comprehensive Documentation:** Docstrings with Args, Returns, Examples",
        "2. **Type Hints:** Consistent use of typing annotations",
        "3. **Centralized Configuration:** `CorticalConfig` dataclass with validation",
        "4. **Separation of Concerns:** Query, analysis, persistence in separate modules",
        "5. **Backward Compatibility:** `from_dict` handles old formats gracefully",
        "6. **Test Coverage:** Extensive test suite with unit and integration tests",
        "7. **Validation Module:** Reusable validators in `validation.py`",
        "## Recommendations Summary",
        "",
        "### High Priority",
        "1. Extract functionality from `CorticalTextProcessor` into focused classes",
        "2. Remove or properly deprecate `feedforward_sources`",
        "",
        "### Medium Priority",
        "3. Standardize layer variable naming (use semantic names)",
        "4. Reduce checkpoint handling duplication with helper methods",
        "5. Use `validation.py` decorators consistently",
        "",
        "### Low Priority",
        "6. Move magic numbers to configuration",
        "7. Standardize error message format",
        "8. Consider renaming \"Minicolumn\" for non-neuroscience contexts",
        "## Metrics",
        "",
        "| Metric | Value | Target |",
        "|--------|-------|--------|",
        "| Largest file (processor.py) | 3115 lines | < 500 lines |",
        "| Methods in CorticalTextProcessor | 70+ | < 20 |",
        "| Duplicate validation blocks | ~15 | 0 |",
        "| Deprecated code still used | 1 | 0 |"
      ],
      "lines_removed": [
        "# Cortical Text Processor - Expert Code Review",
        "**Reviewer**: Expert AI Code Reviewer",
        "**Date**: 2025-12-10",
        "**Commit**: HEAD on branch `claude/expert-code-review-014LRTYwziGPnUKnD6UnJwtj`",
        "I conducted a thorough review of this biologically-inspired NLP library. The codebase is **well-architected and generally high quality**, with 337 passing tests and solid documentation. However, I discovered **one critical bug** and identified several areas where documentation claims don't fully match implementation reality.",
        "**Overall Assessment**: 7.5/10 - A solid, functional library with one significant bug and some marketing overstatements.",
        "## Critical Findings",
        "### ðŸ”´ BUG: Bigram Separator Mismatch in Analogy Completion",
        "**Severity**: Critical",
        "**Location**: `cortical/query.py:1442-1468`",
        "**The Problem**: The analogy completion functions use underscore-separated bigram lookups, but bigrams are stored with space separators.",
        "# In query.py complete_analogy_simple() - INCORRECT",
        "ab_bigram = f\"{term_a}_{term_b}\"   # Creates \"neural_networks\"",
        "ab_col = layer1.get_minicolumn(ab_bigram)  # Looks for \"neural_networks\"",
        "",
        "# But bigrams are stored as (from tokenizer.py line 179):",
        "' '.join(tokens[i:i+n])  # Creates \"neural networks\" (SPACE separator)",
        "**Verified Reproduction**:",
        "```python",
        "from cortical import CorticalTextProcessor",
        "p = CorticalTextProcessor()",
        "p.process_document('d1', 'Neural networks learn from data.')",
        "p.compute_all()",
        "layer1 = p.layers[1]  # BIGRAMS layer",
        "print(layer1.get_minicolumn('neural_networks'))  # None - NOT FOUND",
        "print(layer1.get_minicolumn('neural networks'))  # Minicolumn - FOUND",
        "**Impact**: The bigram-based strategy in `complete_analogy_simple()` will never find matching bigrams, silently degrading analogy quality. The function falls back to other strategies but loses a valuable signal.",
        "**Fix Required**: Change lines 1442-1446 and 1453 to use space separators:",
        "ab_bigram = f\"{term_a} {term_b}\"  # Space, not underscore",
        "parts = bigram.split(' ')         # Split on space, not underscore",
        "## Verification of Claimed Bug Fixes",
        "I verified all bug fixes listed in `TASK_LIST.md`:",
        "| Task | Claim | Status | Location |",
        "|------|-------|--------|----------|",
        "| TF-IDF per-doc calculation | Fixed to use actual doc counts | âœ… Verified | `analysis.py:412-413` |",
        "| O(1) ID lookup | Added `_id_index` to `HierarchicalLayer` | âœ… Verified | `layers.py` |",
        "| Type annotations | Fixed `any` â†’ `Any` | âœ… Verified | `semantics.py`, `embeddings.py` |",
        "| Unused Counter import | Removed | âœ… Verified | `analysis.py` |",
        "| Verbose parameter | Added to `export_graph_json()` | âœ… Verified | `persistence.py` |",
        "**All claimed fixes are legitimately implemented.**",
        "## Architecture Assessment",
        "### Strengths",
        "1. **Clean Module Separation**: Each module has a clear responsibility",
        "   - `processor.py`: Orchestration",
        "   - `analysis.py`: Graph algorithms (PageRank, TF-IDF)",
        "   - `query.py`: Search and retrieval",
        "   - `semantics.py`: Relation extraction",
        "   - `persistence.py`: Save/load",
        "2. **Good Data Structure Design**:",
        "   - `Minicolumn` with `__slots__` for memory efficiency",
        "   - `Edge` dataclass for typed connections",
        "   - `_id_index` for O(1) lookups (confirmed working)",
        "3. **Comprehensive Testing**: 337 tests with good coverage of edge cases",
        "4. **Zero Dependencies**: Truly uses only stdlib - a genuine achievement",
        "### Concerns",
        "1. **`processor.py` Size**: At ~1600 lines, this module is becoming a \"god object\". Consider splitting incremental indexing and batch operations into separate modules as noted in TASK_LIST.md item #31.",
        "2. **Semantic Lookup Memory**: `compute_concept_connections()` builds a double-nested dict for bidirectional semantic lookup. For 10K+ relations, this could be memory-optimized.",
        "## Technical Accuracy Review",
        "### PageRank Implementation âœ…",
        "The PageRank implementation (`analysis.py:22-89`) is correct:",
        "- Standard power iteration method",
        "- Damping factor of 0.85 (matches original PageRank paper)",
        "- Proper convergence checking with configurable tolerance",
        "- Correct normalization",
        "### TF-IDF Implementation âš ï¸",
        "**Per-document TF-IDF** (`tfidf_per_doc`) is correctly implemented:",
        "# analysis.py:412-413",
        "doc_tf = col.doc_occurrence_counts.get(doc_id, 1)",
        "col.tfidf_per_doc[doc_id] = math.log1p(doc_tf) * idf  # Correct!",
        "**Global TF-IDF** (`col.tfidf`) is **not standard TF-IDF**:",
        "# analysis.py:404",
        "tf = math.log1p(col.occurrence_count)  # Total across ALL docs, not per-doc",
        "col.tfidf = tf * idf  # This is corpus-wide importance, not TF-IDF",
        "This global value is a corpus-wide importance metric, which is useful but misnamed. The query code correctly uses `tfidf_per_doc` for document ranking, so functionality is correct - but the naming could be clearer.",
        "### Cosine Similarity âœ…",
        "Correctly implemented for sparse vectors (`analysis.py:1075-1102`).",
        "### Label Propagation âœ…",
        "Correctly implements community detection with configurable strictness.",
        "### Graph Embeddings âœ…",
        "All three methods (adjacency, random walk, spectral) are correctly implemented with proper normalization.",
        "## Claims vs. Reality",
        "| Claim | Reality |",
        "|-------|---------|",
        "| \"Zero dependencies\" | âœ… **True** - uses only stdlib |",
        "| \"337 tests passing\" | âœ… **Verified** - all pass |",
        "| \"O(1) ID lookups\" | âœ… **Implemented** via `_id_index` |",
        "| \"PageRank importance\" | âœ… **Correctly implemented** |",
        "| \"Neocortex-inspired\" | âš ï¸ **Overstated** - uses standard NLP algorithms with neuroscience naming |",
        "| \"ConceptNet-style relations\" | âœ… **Works correctly** - pattern extraction and typed edges |",
        "| \"Hebbian learning\" | âš ï¸ **Simplified** - co-occurrence counting, not actual Hebbian rules |",
        "### On the \"Neocortex\" Claim",
        "The library primarily uses:",
        "- TF-IDF (statistical, 1970s)",
        "- PageRank (graph theory, 1998)",
        "- Label propagation (community detection, 2002)",
        "- Co-occurrence counting (called \"Hebbian connections\")",
        "While the 4-layer hierarchy is a reasonable abstraction inspired by cortical organization (V1â†’V2â†’V4â†’IT), calling this \"neocortex-inspired processing\" is marketing. The visual cortex doesn't compute TF-IDF or run PageRank iterations.",
        "**The documentation states**: \"mimicking how the visual cortex organizes information\" - this is an overstatement. The algorithms are standard information retrieval techniques organized into a hierarchical structure.",
        "**Recommendation**: Describe as \"hierarchical text processing with a design inspired by cortical organization\" rather than claiming biological similarity.",
        "## Code Quality Metrics",
        "| Metric | Value | Assessment |",
        "|--------|-------|------------|",
        "| Lines of Code (cortical/) | ~7,070 | Reasonable |",
        "| Lines of Tests | ~4,944 | Excellent coverage |",
        "| Test Count | 337 | Comprehensive |",
        "| Test Pass Rate | 100% | âœ… Perfect |",
        "| Type Hints | Extensive | Good practice |",
        "| Docstrings | Comprehensive | Excellent documentation |",
        "| Average Function Length | ~30 lines | Acceptable |",
        "## Security Assessment",
        "**No security vulnerabilities identified.**",
        "The library:",
        "- Uses only stdlib (no supply chain risk)",
        "- Doesn't execute arbitrary code",
        "- Doesn't access network resources",
        "- Uses pickle for persistence (standard for ML, but noted for awareness)",
        "- Properly sanitizes user input in tokenizer",
        "## Recommendations",
        "### Must Fix (Critical)",
        "1. **Fix bigram separator bug** in `complete_analogy_simple()` (`query.py:1442-1468`)",
        "   - Change underscore separators to spaces to match actual bigram storage",
        "### Should Fix (Important)",
        "2. **Clarify global TF-IDF** - add comment or rename `col.tfidf` to indicate it's corpus importance, not per-doc TF-IDF",
        "3. **Add integration test** for analogy completion to catch this class of bug",
        "### Consider (Enhancements)",
        "4. **Split `processor.py`** - extract batch operations and incremental indexing as noted in TASK_LIST.md",
        "5. **Temper neuroscience claims** - describe as \"inspired by\" rather than \"mimicking\"",
        "6. **Optimize semantic lookup memory** - use frozenset keys instead of bidirectional nested dicts",
        "## Test Suite Verification",
        "```bash",
        "$ python -m unittest discover -s tests -v",
        "...",
        "Ran 337 tests in 0.309s",
        "OK",
        "All tests pass. The test suite is comprehensive and includes:",
        "- Unit tests for all modules",
        "- Edge cases (empty corpus, single document)",
        "- Integration tests for full pipeline",
        "- Regression tests for fixed bugs",
        "## Conclusion",
        "This is a **well-engineered library** with one significant bug (bigram separator mismatch) and some documentation issues. The core algorithms (PageRank, per-document TF-IDF, label propagation, query expansion) are correctly implemented.",
        "The \"neocortex\" branding is marketing - the algorithms are standard information retrieval techniques - but the hierarchical abstraction is valid and useful for organizing text processing.",
        "**Recommendation**: Fix the bigram bug, clarify the TF-IDF documentation, and consider moderating the biological claims. The library is otherwise **production-ready** for text processing and RAG applications.",
        "## Appendix: Files Reviewed",
        "",
        "| File | Lines | Assessment |",
        "|------|-------|------------|",
        "| `cortical/processor.py` | 1,596 | Main orchestrator, well-organized but large |",
        "| `cortical/analysis.py` | 1,102 | Graph algorithms, correct implementations |",
        "| `cortical/query.py` | 1,503 | Search/retrieval, **contains bigram bug** |",
        "| `cortical/semantics.py` | 904 | Relation extraction, working correctly |",
        "| `cortical/persistence.py` | 606 | Save/load, well-implemented |",
        "| `cortical/minicolumn.py` | 357 | Core data structure, good design |",
        "| `cortical/layers.py` | 273 | Layer management with O(1) lookups |",
        "| `cortical/gaps.py` | 245 | Gap detection, working correctly |",
        "| `cortical/tokenizer.py` | 245 | Tokenization, correct |",
        "| `cortical/embeddings.py` | 209 | Graph embeddings, correct implementations |",
        "*Review completed 2025-12-10*"
      ],
      "context_before": [],
      "context_after": [
        "",
        "",
        "---",
        "",
        "## Executive Summary",
        "",
        "",
        "",
        "---",
        "",
        "",
        "",
        "",
        "",
        "```python",
        "```",
        "",
        "",
        "",
        "```",
        "",
        "",
        "```python",
        "```",
        "",
        "---",
        "",
        "",
        "",
        "",
        "",
        "---",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "---",
        "",
        "",
        "",
        "",
        "",
        "```python",
        "```",
        "",
        "```python",
        "```",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "---",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "---",
        "",
        "",
        "",
        "---",
        "",
        "",
        "",
        "",
        "---",
        "",
        "",
        "",
        "",
        "",
        "---",
        "",
        "",
        "",
        "```",
        "",
        "",
        "---",
        "",
        "",
        "",
        "",
        "",
        "---",
        "",
        "",
        "---",
        ""
      ],
      "change_type": "modify"
    },
    {
      "file": "SECURITY_REVIEW.md",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "# Security Review Report",
        "",
        "**Date:** 2025-12-14",
        "**Reviewer:** Claude (Automated Security Review)",
        "**Scope:** Comprehensive git history and code security review",
        "",
        "---",
        "",
        "## Executive Summary",
        "",
        "This security review analyzed the Cortical Text Processor codebase for vulnerabilities in both the git history and source code. Overall, the codebase demonstrates good security practices with one notable medium-risk finding related to pickle deserialization.",
        "",
        "| Category | Risk Level | Count |",
        "|----------|------------|-------|",
        "| Critical | - | 0 |",
        "| High | - | 0 |",
        "| Medium | - | 1 |",
        "| Low | - | 2 |",
        "| Informational | - | 3 |",
        "",
        "---",
        "",
        "## Git History Review",
        "",
        "### Secrets and Credentials Scan",
        "",
        "**Status: PASSED**",
        "",
        "Searched the entire git history (363 commits) for:",
        "- Passwords, API keys, tokens, secrets",
        "- AWS/Azure/GCP credentials",
        "- Base64-encoded secrets",
        "- Private keys and certificates",
        "- Environment files (.env, secrets.*)",
        "",
        "**Findings:**",
        "- No leaked credentials or secrets found",
        "- No sensitive files in deleted history",
        "- References to \"password\" are only in customer service sample documents and test fixtures",
        "- No hardcoded API keys or tokens",
        "",
        "### Suspicious Patterns",
        "",
        "**Status: PASSED**",
        "",
        "- No force-pushed commits that could hide malicious changes",
        "- No commits from unknown or suspicious authors",
        "- Commit history follows normal development patterns",
        "",
        "---",
        "",
        "## Code Security Review",
        "",
        "### 1. Pickle Deserialization (MEDIUM RISK)",
        "",
        "**File:** `cortical/persistence.py:156`",
        "",
        "```python",
        "with open(filepath, 'rb') as f:",
        "    state = pickle.load(f)",
        "```",
        "",
        "**Issue:** Python's `pickle.load()` can execute arbitrary code during deserialization. If an attacker can control the pickle file being loaded, they can achieve remote code execution.",
        "",
        "**Risk Assessment:**",
        "- **Severity:** Medium",
        "- **Exploitability:** Requires attacker to replace/modify a corpus pickle file",
        "- **Impact:** Arbitrary code execution",
        "",
        "**Recommendation:**",
        "1. Add a warning in documentation that users should only load trusted pickle files",
        "2. Consider implementing pickle file signing/verification",
        "3. Long-term: Migrate fully to the safer JSON-based `StateLoader` for loading corpus data",
        "4. The protobuf text format (`format='protobuf'`) is a safer alternative already available",
        "",
        "---",
        "",
        "### 2. Subprocess Usage (LOW RISK - Mitigated)",
        "",
        "**Files:** `cortical/cli_wrapper.py`, `cortical/chunk_index.py`, `scripts/*.py`",
        "",
        "**Findings:**",
        "- All subprocess calls use list-style command arguments (not shell strings)",
        "- No use of `shell=True` which would enable shell injection",
        "- All subprocess calls use hardcoded commands, not user-supplied input",
        "",
        "**Example (Safe Usage):**",
        "```python",
        "result = subprocess.run(",
        "    ['git', 'rev-parse', '--is-inside-work-tree'],",
        "    capture_output=True,",
        "    text=True,",
        "    timeout=5",
        ")",
        "```",
        "",
        "**Status:** No vulnerabilities found. Current implementation follows security best practices.",
        "",
        "---",
        "",
        "### 3. Path Traversal Protection (LOW RISK - Mitigated)",
        "",
        "**File:** `scripts/consolidate_tasks.py:224-242`",
        "",
        "**Findings:**",
        "- Path traversal protection is properly implemented",
        "- Uses `Path.resolve()` for canonical path comparison",
        "- Validates paths stay within allowed boundaries",
        "",
        "**Example (Proper Protection):**",
        "```python",
        "def _validate_archive_path(tasks_dir: Path, archive_path: Path) -> None:",
        "    tasks_resolved = tasks_dir.resolve()",
        "    archive_resolved = archive_path.resolve()",
        "    try:",
        "        archive_resolved.relative_to(tasks_resolved)",
        "    except ValueError:",
        "        raise ValueError(\"Path traversal is not allowed for security reasons.\")",
        "```",
        "",
        "**Status:** Well-implemented security control.",
        "",
        "---",
        "",
        "### 4. Input Validation (INFORMATIONAL)",
        "",
        "**File:** `cortical/validation.py`, `cortical/mcp_server.py`",
        "",
        "**Findings:**",
        "- Input validation utilities exist in `validation.py`",
        "- MCP server validates query strings for empty values",
        "- Type checking is performed on function parameters",
        "",
        "**Positive Examples:**",
        "```python",
        "if not query or not query.strip():",
        "    return {\"error\": \"Query must be a non-empty string\", ...}",
        "",
        "if top_n < 1:",
        "    return {\"error\": \"top_n must be at least 1\", ...}",
        "```",
        "",
        "**Status:** Good practices observed.",
        "",
        "---",
        "",
        "### 5. File Operations (INFORMATIONAL)",
        "",
        "**Findings:**",
        "- Atomic writes implemented using temp file + rename pattern",
        "- Prevents data corruption on crash/interrupt",
        "- Temporary files are properly cleaned up",
        "",
        "**Example (Atomic Write):**",
        "```python",
        "def _atomic_write(self, filepath: Path, content: str) -> None:",
        "    temp_path = filepath.with_suffix('.json.tmp')",
        "    try:",
        "        with open(temp_path, 'w', encoding='utf-8') as f:",
        "            f.write(content)",
        "        temp_path.replace(filepath)",
        "    except Exception:",
        "        if temp_path.exists():",
        "            temp_path.unlink()",
        "        raise",
        "```",
        "",
        "**Status:** Good security practice.",
        "",
        "---",
        "",
        "### 6. Network and External Communication (INFORMATIONAL)",
        "",
        "**Findings:**",
        "- No HTTP client libraries (requests, urllib) used in production code",
        "- No external API calls made",
        "- MCP server uses stdio transport by default (local only)",
        "",
        "**Status:** Minimal attack surface for network-based attacks.",
        "",
        "---",
        "",
        "## Items Not Found (Good)",
        "",
        "The following common vulnerabilities were NOT found:",
        "",
        "- SQL Injection (No database usage)",
        "- Cross-Site Scripting (No web frontend)",
        "- Command Injection (Subprocess calls are safe)",
        "- Hardcoded Credentials (None found)",
        "- Insecure Random (Uses uuid.uuid4() appropriately)",
        "- XML External Entity (No XML parsing)",
        "- SSRF (No HTTP client usage)",
        "",
        "---",
        "",
        "## Recommendations",
        "",
        "### Immediate Actions",
        "",
        "1. **Document pickle security warning**",
        "   - Add documentation warning users to only load trusted pickle files",
        "   - Consider adding a verification step for corpus files",
        "",
        "### Short-term Improvements",
        "",
        "2. **Prefer JSON/Protobuf over Pickle**",
        "   - The codebase already has `StateLoader` (JSON) and protobuf support",
        "   - Consider deprecating pickle format for new deployments",
        "",
        "3. **Add security documentation**",
        "   - Document the security model for MCP server deployments",
        "   - Include guidance on file permissions for corpus data",
        "",
        "### Long-term Considerations",
        "",
        "4. **Pickle file signing**",
        "   - If pickle must be supported, implement HMAC signing",
        "   - Verify signature before loading",
        "",
        "5. **Security testing**",
        "   - Add security-focused test cases",
        "   - Consider fuzzing for input validation",
        "",
        "---",
        "",
        "## Testing Coverage",
        "",
        "Security-relevant tests observed:",
        "- `tests/integration/test_task_integration.py` - Path traversal tests",
        "- `tests/integration/test_task_recovery.py` - Atomic write tests",
        "- `tests/unit/test_validation.py` - Input validation tests",
        "",
        "---",
        "",
        "## Conclusion",
        "",
        "The Cortical Text Processor codebase demonstrates **good overall security hygiene**. The main concern is pickle deserialization, which is a known Python security issue. The codebase already provides safer alternatives (JSON state storage, protobuf) that should be preferred for untrusted environments.",
        "",
        "No critical or high-severity vulnerabilities were identified. The development team has implemented proper security controls for path traversal, input validation, and file operations.",
        "",
        "---",
        "",
        "## Deep Security Scan - Hidden Binary/Malware Detection",
        "",
        "**Date:** 2025-12-14",
        "**Scope:** Comprehensive scan for hidden binary data, embedded malware, and overlooked attack vectors",
        "",
        "This deep scan specifically targeted locations commonly exploited by attackers that security professionals often overlook.",
        "",
        "### Scan Coverage",
        "",
        "| Location | Status | Details |",
        "|----------|--------|---------|",
        "| Git Hooks (`/.git/hooks/`) | âœ… CLEAN | Only sample files present, no active hooks |",
        "| Hidden Dotfiles | âœ… CLEAN | Only `.gitignore` found with standard content |",
        "| Text Files for Binary Headers | âœ… CLEAN | All 100+ .txt/.py/.md files verified as text |",
        "| PE/ELF/Mach-O Headers | âœ… CLEAN | No executable headers embedded in any file |",
        "| Base64 Encoded Payloads | âœ… CLEAN | No suspicious long base64 strings |",
        "| Hex Escape Sequences | âœ… CLEAN | No embedded `\\x` byte sequences |",
        "| CI/CD Workflows | âœ… CLEAN | Standard GitHub Actions, no injection vectors |",
        "| Sample Data Files | âœ… CLEAN | All sample .txt files are proper ASCII/UTF-8 text |",
        "| Test Fixtures | âœ… CLEAN | Synthetic test data, no hidden payloads |",
        "| Corpus Chunks (JSON) | âœ… CLEAN | Proper JSON structure, no embedded data |",
        "| Zero-Width Unicode | âœ… CLEAN | No RTL override, zero-width, or homoglyph attacks |",
        "| Obfuscated Python | âœ… CLEAN | No malicious `exec()`, `eval()`, `__import__` |",
        "| Reverse Shell Patterns | âœ… CLEAN | No `socket.connect`, `pty.spawn`, `shell=True` |",
        "| Symlinks | âœ… CLEAN | No symlinks that could escape directory boundaries |",
        "| Executable Files | âœ… CLEAN | No binary executables in repository |",
        "| `.pth` Files | âœ… CLEAN | None present (these auto-execute on import) |",
        "| `pyproject.toml` | âœ… CLEAN | Zero runtime dependencies, legitimate dev deps |",
        "| `conftest.py` | âœ… CLEAN | Standard pytest configuration |",
        "| `.claude/` Directory | âœ… CLEAN | Clean markdown/yaml workflow definitions |",
        "",
        "### Specific Checks Performed",
        "",
        "1. **Git Hooks Analysis**",
        "   - Verified `/.git/hooks/` contains only `.sample` files",
        "   - No active pre-commit, post-checkout, or other hooks",
        "   - Repository safe to clone without auto-execution",
        "",
        "2. **Binary Detection**",
        "   - Scanned all files with `file` command",
        "   - Searched for magic bytes: `MZ` (PE), `\\x7fELF`, `cafebabe` (Mach-O)",
        "   - Searched for `PK\\x03\\x04` (ZIP), `JFIF` (JPEG), `PNG`, `GIF8`",
        "   - No embedded binaries found",
        "",
        "3. **Encoding Attacks**",
        "   - Searched for base64 strings >50 characters",
        "   - Searched for hex escape sequences (`\\x00` patterns)",
        "   - Searched for Unicode exploits (U+200B zero-width, U+202E RTL override)",
        "   - No encoding-based attacks found",
        "",
        "4. **Code Injection Vectors**",
        "   - Checked for `exec()`, `eval()`, `compile()` misuse",
        "   - Verified `getattr()` usage is legitimate (standard Python patterns)",
        "   - No dynamic code execution vulnerabilities",
        "",
        "5. **Network/Shell Patterns**",
        "   - Searched for `curl`, `wget`, `nc`, `netcat`, `/dev/tcp`",
        "   - Searched for `subprocess.Popen.*shell=True`",
        "   - No reverse shell or command injection patterns",
        "",
        "6. **Supply Chain Analysis**",
        "   - `pyproject.toml` has **zero runtime dependencies**",
        "   - Dev dependencies are legitimate packages: `pytest`, `coverage`, `mcp`, `pyyaml`",
        "   - No typosquatted or suspicious package names",
        "",
        "### Security Positives",
        "",
        "1. **Zero Runtime Dependencies** - Minimal attack surface from third-party code",
        "2. **No Active Git Hooks** - Cannot execute code on clone/commit/push",
        "3. **No Shell Scripts** - No `.sh` files in repository",
        "4. **Standard Library Only** - Production code uses only Python stdlib",
        "5. **No Network Code** - No `socket`, `requests`, `urllib` in production",
        "6. **Clean File Types** - All files match expected types",
        "",
        "### Conclusion",
        "",
        "**No hidden binary data, embedded malware, or suspicious payloads detected.** The codebase is clean across all commonly exploited attack vectors that security professionals typically overlook.",
        "",
        "---",
        "",
        "## Comprehensive Checklist of All Security Checks Performed",
        "",
        "### Git History Security (363 commits analyzed)",
        "",
        "| Check | Command/Method | Result |",
        "|-------|----------------|--------|",
        "| Leaked passwords | `git log -p --all -S \"password\"` | âœ… Only in sample docs |",
        "| API keys | `git log -p --all -S \"api_key\\|apikey\\|API_KEY\"` | âœ… None found |",
        "| AWS credentials | `git log -p --all -S \"AKIA\\|aws_secret\"` | âœ… None found |",
        "| Private keys | `git log -p --all -S \"BEGIN.*PRIVATE KEY\"` | âœ… None found |",
        "| Tokens/secrets | `git log -p --all -S \"token\\|secret\\|credential\"` | âœ… None found |",
        "| .env files | `git log --all --name-only \\| grep -E \"\\.env\"` | âœ… None found |",
        "| Force pushes | `git reflog` analysis | âœ… Clean history |",
        "| Deleted large files | `git log --diff-filter=D` | âœ… Only corpus_dev.pkl (legitimate) |",
        "| Git config hooks | `cat .git/config` | âœ… No malicious hooks/remotes |",
        "",
        "### File System Security",
        "",
        "| Check | Command/Method | Result |",
        "|-------|----------------|--------|",
        "| Active git hooks | `ls .git/hooks/ \\| grep -v sample` | âœ… None active |",
        "| Hidden dotfiles | `find . -name \".*\" -type f` | âœ… Only .gitignore |",
        "| Symlinks | `find . -type l` | âœ… None found |",
        "| Executable files | `find . -type f -perm /111` | âœ… None (except .git/) |",
        "| .pth files | `find . -name \"*.pth\"` | âœ… None found |",
        "| .pyc committed | `find . -name \"*.pyc\" ! -path \"*/.git/*\"` | âœ… None committed |",
        "| Non-ASCII filenames | `find . -type f \\| LC_ALL=C grep -E '[^\\x00-\\x7F]'` | âœ… None found |",
        "| Large files | `find . -size +10M` | âœ… Only corpus chunks (legitimate JSON) |",
        "",
        "### Binary/Malware Detection",
        "",
        "| Check | Command/Method | Result |",
        "|-------|----------------|--------|",
        "| File type verification | `file *.txt *.py *.md` | âœ… All proper text |",
        "| PE headers (Windows exe) | Search for `MZ` magic bytes | âœ… None found |",
        "| ELF headers (Linux exe) | Search for `\\x7fELF` magic bytes | âœ… None found |",
        "| Mach-O headers (macOS) | Search for `cafebabe`/`feedface` | âœ… None found |",
        "| ZIP embedded | Search for `PK\\x03\\x04` | âœ… None found |",
        "| Image headers | Search for `JFIF`, `PNG`, `GIF8` | âœ… None found |",
        "| Hex dump analysis | `xxd -l 4` on all files | âœ… No binary headers |",
        "",
        "### Encoding Attack Detection",
        "",
        "| Check | Command/Method | Result |",
        "|-------|----------------|--------|",
        "| Base64 payloads | Regex `[A-Za-z0-9+/]{50,}={0,2}` | âœ… None suspicious |",
        "| Hex escape sequences | Regex `\\\\x[0-9a-fA-F]{2}` repeated | âœ… None found |",
        "| Zero-width chars | Search for U+200B, U+200C, U+200D | âœ… None found |",
        "| RTL override | Search for U+202E | âœ… None found |",
        "| Homoglyph attacks | Non-ASCII in identifiers | âœ… None found |",
        "| Unicode escapes in MD | Regex `\\\\u[0-9a-fA-F]{4}` | âœ… None found |",
        "",
        "### Python Code Security",
        "",
        "| Check | Command/Method | Result |",
        "|-------|----------------|--------|",
        "| `exec()` usage | `grep -r \"exec(\"` | âœ… None (only \"execute\" words) |",
        "| `eval()` usage | `grep -r \"eval(\"` | âœ… None (only \"evaluate\" words) |",
        "| `compile()` misuse | `grep -r \"compile(.*exec\"` | âœ… None found |",
        "| `__import__()` | `grep -r \"__import__\"` | âœ… None found |",
        "| Dangerous `getattr` | `grep -r \"getattr\\(.*,.*\\)\"` | âœ… Legitimate usage only |",
        "| `pickle.loads` on user input | Code review | âœ… Only file-based (medium risk) |",
        "| `subprocess` shell=True | `grep -r \"shell=True\"` | âœ… None found |",
        "| `os.system()` | `grep -r \"os\\.system\"` | âœ… None found |",
        "| `pty.spawn()` | `grep -r \"pty\\.spawn\"` | âœ… None found |",
        "",
        "### Network/Shell Pattern Detection",
        "",
        "| Check | Command/Method | Result |",
        "|-------|----------------|--------|",
        "| Socket connections | `grep -r \"socket\\.connect\"` | âœ… None found |",
        "| HTTP clients | `grep -r \"requests\\.\\|urllib\\.\"` | âœ… None in production |",
        "| curl/wget | `grep -r \"curl\\|wget\"` | âœ… None found |",
        "| netcat | `grep -r \"nc \\|netcat\"` | âœ… None found |",
        "| /dev/tcp | `grep -r \"/dev/tcp\"` | âœ… None found |",
        "| Reverse shell patterns | Combined regex search | âœ… None found |",
        "",
        "### Supply Chain Security",
        "",
        "| Check | Command/Method | Result |",
        "|-------|----------------|--------|",
        "| Runtime dependencies | `pyproject.toml` review | âœ… Zero dependencies |",
        "| Dev dependencies | Package name verification | âœ… All legitimate |",
        "| Typosquatting check | Manual review of package names | âœ… None suspicious |",
        "| setup.py hooks | Check for malicious install hooks | âœ… No setup.py (uses pyproject.toml) |",
        "| Import hijacking | Check for shadowed stdlib names | âœ… None found |",
        "",
        "### CI/CD Security",
        "",
        "| Check | Command/Method | Result |",
        "|-------|----------------|--------|",
        "| Workflow injection | Review `.github/workflows/*.yml` | âœ… Standard actions only |",
        "| Hardcoded secrets | Search workflow files | âœ… None found |",
        "| Unsafe script execution | Review `run:` blocks | âœ… All safe |",
        "| Third-party actions | Verify action sources | âœ… Official actions only |",
        "| Environment leakage | Check env variable handling | âœ… Proper usage |",
        "",
        "### Configuration & Skill Files",
        "",
        "| Check | Command/Method | Result |",
        "|-------|----------------|--------|",
        "| CLAUDE.md.potential | Content review | âœ… Standard dev documentation |",
        "| Claude Skills | Review allowed-tools | âœ… Appropriately restricted |",
        "| Workflow definitions | Review .claude/workflows/ | âœ… Standard YAML |",
        "| Command definitions | Review .claude/commands/ | âœ… Standard Markdown |",
        "",
        "### Regex Security (ReDoS)",
        "",
        "| Check | Command/Method | Result |",
        "|-------|----------------|--------|",
        "| Nested quantifiers | Search for `(.*)+` patterns | âœ… None found |",
        "| Catastrophic backtracking | Review regex in semantics.py | âœ… Safe patterns |",
        "| Unbounded repetition | Review all regex patterns | âœ… All bounded |",
        "",
        "### Additional Checks",
        "",
        "| Check | Command/Method | Result |",
        "|-------|----------------|--------|",
        "| Deleted git objects | `git fsck` | âœ… Clean |",
        "| Orphaned commits | `git reflog` | âœ… None suspicious |",
        "| Large blob history | `git rev-list --objects --all` | âœ… Only legitimate files |",
        "| conftest.py review | Content verification | âœ… Standard pytest config |",
        "| __init__.py review | Check for hidden imports | âœ… Clean module exports |",
        "| Test fixtures | Review test data files | âœ… Synthetic, clean data |",
        "",
        "---",
        "",
        "## Security Improvement Task List",
        "",
        "### Priority: HIGH",
        "",
        "- [ ] **SEC-001: Add pickle security warning to documentation**",
        "  - File: `README.md`, `docs/quickstart.md`",
        "  - Add warning: \"Only load pickle files from trusted sources\"",
        "  - Effort: Low (30 min)",
        "  - Risk addressed: Arbitrary code execution via malicious pickle",
        "",
        "- [ ] **SEC-002: Add Bandit to CI pipeline**",
        "  - File: `.github/workflows/ci.yml`",
        "  - Add Python SAST scanning job",
        "  - Catches: SQL injection, hardcoded passwords, unsafe deserialization",
        "  - Effort: Low (1 hour)",
        "",
        "### Priority: MEDIUM",
        "",
        "- [ ] **SEC-003: Implement pickle file verification**",
        "  - File: `cortical/persistence.py`",
        "  - Add optional HMAC signature verification before loading",
        "  - Effort: Medium (4 hours)",
        "  - Risk addressed: Tampered corpus files",
        "",
        "- [ ] **SEC-004: Add dependency scanning to CI**",
        "  - File: `.github/workflows/ci.yml`",
        "  - Add `pip-audit` and `safety` checks",
        "  - Catches: Known CVEs in dependencies",
        "  - Effort: Low (1 hour)",
        "",
        "- [ ] **SEC-005: Add secret scanning to CI**",
        "  - File: `.github/workflows/ci.yml`",
        "  - Add `detect-secrets` or `truffleHog`",
        "  - Catches: Accidentally committed credentials",
        "  - Effort: Low (1 hour)",
        "",
        "- [ ] **SEC-006: Document MCP server security model**",
        "  - File: `docs/security.md` (new)",
        "  - Document: Transport security, authentication, file permissions",
        "  - Effort: Medium (2 hours)",
        "",
        "### Priority: LOW",
        "",
        "- [ ] **SEC-007: Restrict task-manager skill Write access**",
        "  - File: `.claude/skills/task-manager/SKILL.md`",
        "  - Limit Write tool to `tasks/` directory only",
        "  - Risk addressed: Arbitrary file writes via skill",
        "  - Effort: Low (30 min)",
        "",
        "- [ ] **SEC-008: Deprecate pickle format for new deployments**",
        "  - File: `cortical/persistence.py`, documentation",
        "  - Recommend JSON/protobuf as default",
        "  - Add deprecation warning when pickle is used",
        "  - Effort: Medium (2 hours)",
        "",
        "- [ ] **SEC-009: Add security-focused test cases**",
        "  - File: `tests/security/` (new directory)",
        "  - Tests: Path traversal, input validation edge cases",
        "  - Effort: Medium (4 hours)",
        "",
        "- [ ] **SEC-010: Implement input fuzzing**",
        "  - Tool: `hypothesis` or custom fuzzer",
        "  - Target: Query functions, document processing",
        "  - Catches: Unexpected crashes, edge cases",
        "  - Effort: High (8 hours)",
        "",
        "### Priority: INFORMATIONAL",
        "",
        "- [ ] **SEC-011: Add SECURITY.md file**",
        "  - Standard security policy document",
        "  - Include: Reporting vulnerabilities, security contacts",
        "  - Effort: Low (1 hour)",
        "",
        "- [ ] **SEC-012: Review and document file permission requirements**",
        "  - Document: Required permissions for corpus files",
        "  - Best practice: 600 for pickle files, 644 for JSON",
        "  - Effort: Low (30 min)",
        "",
        "---",
        "",
        "## CI Security Job Template",
        "",
        "Add this job to `.github/workflows/ci.yml`:",
        "",
        "```yaml",
        "security-scan:",
        "  name: \"ðŸ”’ Security Scan\"",
        "  runs-on: ubuntu-latest",
        "  needs: smoke-tests",
        "  steps:",
        "  - uses: actions/checkout@v4",
        "",
        "  - name: Set up Python 3.11",
        "    uses: actions/setup-python@v5",
        "    with:",
        "      python-version: '3.11'",
        "",
        "  - name: Install security tools",
        "    run: |",
        "      pip install bandit safety pip-audit detect-secrets",
        "",
        "  - name: Run Bandit (SAST)",
        "    run: |",
        "      echo \"=== Running Bandit Security Scan ===\"",
        "      bandit -r cortical/ -ll -f txt",
        "      echo \"âœ… Bandit scan passed\"",
        "",
        "  - name: Check Dependencies",
        "    run: |",
        "      echo \"=== Checking Dependencies ===\"",
        "      pip install -e \".[dev]\"",
        "      pip-audit --desc || echo \"âš ï¸ Dependency warnings (review required)\"",
        "      safety check || echo \"âš ï¸ Safety warnings (review required)\"",
        "",
        "  - name: Scan for Secrets",
        "    run: |",
        "      echo \"=== Scanning for Secrets ===\"",
        "      detect-secrets scan --all-files --exclude-files '\\.git/.*' || true",
        "```",
        "",
        "---",
        "",
        "## Review Sign-off",
        "",
        "| Reviewer | Date | Scope | Status |",
        "|----------|------|-------|--------|",
        "| Claude (Automated) | 2025-12-14 | Git history, code, binary scan | âœ… Complete |",
        "",
        "**Next Review Recommended:** After implementing SEC-001 through SEC-005"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "docs/security-knowledge-transfer.md",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "# Security Knowledge Transfer",
        "",
        "**Date:** 2025-12-14",
        "**Scope:** Comprehensive security review, security corpus development, and security-focused tooling",
        "**Branch:** `claude/security-review-84jN0`",
        "",
        "---",
        "",
        "## Executive Summary",
        "",
        "This document captures the complete security review performed on the Cortical Text Processor codebase, including methodology, findings, remediation recommendations, and the security-focused sample corpus created to demonstrate semantic search capabilities for security knowledge bases.",
        "",
        "### Key Outcomes",
        "",
        "| Deliverable | Status | Location |",
        "|-------------|--------|----------|",
        "| Security Review Report | Complete | `SECURITY_REVIEW.md` |",
        "| Code Quality Review | Complete | `CODE_REVIEW.md` |",
        "| Security Sample Corpus | 18 documents | `samples/*.txt` |",
        "| Security Showcase | Working | `secureshowcase.py` |",
        "| CI Security Template | Ready | `SECURITY_REVIEW.md` |",
        "",
        "---",
        "",
        "## Part 1: Security Review",
        "",
        "### 1.1 Scope and Methodology",
        "",
        "The security review covered:",
        "",
        "1. **Git History Analysis** (363 commits)",
        "   - Secrets and credentials scanning",
        "   - Suspicious pattern detection",
        "   - Deleted file analysis",
        "",
        "2. **Source Code Review**",
        "   - Injection vulnerabilities",
        "   - Authentication/authorization",
        "   - Cryptographic usage",
        "   - Input validation",
        "   - File operations",
        "",
        "3. **Deep Binary/Malware Scan**",
        "   - Git hooks inspection",
        "   - Hidden dotfiles",
        "   - Embedded binary detection",
        "   - Encoding attack detection",
        "   - Supply chain analysis",
        "",
        "4. **Configuration Review**",
        "   - CI/CD workflows",
        "   - Claude skills and commands",
        "   - Build configuration",
        "",
        "### 1.2 Tools and Techniques Used",
        "",
        "| Category | Tools/Commands |",
        "|----------|----------------|",
        "| Git History | `git log -p --all -S`, `git rev-list --objects` |",
        "| File Analysis | `file`, `xxd`, `grep`, `find` |",
        "| Binary Detection | Magic byte scanning (MZ, ELF, Mach-O) |",
        "| Encoding Detection | Base64/hex pattern regex |",
        "| Code Patterns | `grep -r` for dangerous functions |",
        "| Supply Chain | `pyproject.toml` review |",
        "",
        "### 1.3 Findings Summary",
        "",
        "| Severity | Count | Description |",
        "|----------|-------|-------------|",
        "| Critical | 0 | None identified |",
        "| High | 0 | None identified |",
        "| Medium | 1 | Pickle deserialization (CWE-502) |",
        "| Low | 2 | Mitigated subprocess/path traversal |",
        "| Informational | 3 | Good practices documented |",
        "",
        "### 1.4 Primary Finding: Pickle Deserialization",
        "",
        "**Location:** `cortical/persistence.py:156`",
        "",
        "```python",
        "with open(filepath, 'rb') as f:",
        "    state = pickle.load(f)",
        "```",
        "",
        "**Risk:** Python's `pickle.load()` can execute arbitrary code during deserialization. If an attacker controls the pickle file, they achieve RCE.",
        "",
        "**Mitigations Available:**",
        "1. Use JSON-based `StateLoader` (already implemented)",
        "2. Use protobuf format (`format='protobuf'`)",
        "3. Add HMAC verification before loading",
        "4. Document security warning for users",
        "",
        "### 1.5 Security Positives",
        "",
        "The codebase demonstrates strong security practices:",
        "",
        "- **Zero runtime dependencies** - Minimal attack surface",
        "- **No active git hooks** - Safe to clone",
        "- **Proper subprocess usage** - List arguments, no `shell=True`",
        "- **Path traversal protection** - `Path.resolve()` + `relative_to()`",
        "- **Input validation** - Centralized in `validation.py`",
        "- **Atomic file writes** - Temp file + rename pattern",
        "- **No network code** - No HTTP clients in production",
        "",
        "---",
        "",
        "## Part 2: Code Quality Review",
        "",
        "### 2.1 Code Smells Identified",
        "",
        "| Issue | Location | Severity |",
        "|-------|----------|----------|",
        "| God Class | `processor.py` (3115 lines, 70+ methods) | High |",
        "| Deprecated Code | `minicolumn.py` feedforward_sources | Medium |",
        "| Naming Inconsistency | `layer0/layer1/layer3` vs semantic names | Low |",
        "| Magic Numbers | Various hardcoded values | Low |",
        "| Code Duplication | Checkpoint handling patterns | Low |",
        "",
        "### 2.2 Symbolic Misinterpretation",
        "",
        "The term \"Minicolumn\" comes from neuroscience (cortical columns) but the actual implementation is a standard graph node/document representation. This may cause confusion for developers unfamiliar with the metaphor.",
        "",
        "**Recommendation:** Add documentation clarifying that neuroscience terms are metaphors for IR algorithms.",
        "",
        "---",
        "",
        "## Part 3: Security Sample Corpus",
        "",
        "### 3.1 Document Inventory",
        "",
        "18 security-focused sample documents organized by category:",
        "",
        "#### Fundamentals (2 documents)",
        "| Document | Topics |",
        "|----------|--------|",
        "| `application_security_fundamentals.txt` | OWASP Top 10, input validation, auth, crypto |",
        "| `threat_modeling.txt` | STRIDE, attack trees, risk assessment |",
        "",
        "#### Secure Development (4 documents)",
        "| Document | Topics |",
        "|----------|--------|",
        "| `secure_development_lifecycle.txt` | SSDLC phases, shift-left security |",
        "| `secure_code_review.txt` | Auth review, input handling, crypto |",
        "| `devsecops_practices.txt` | CI/CD security, SAST, SCA |",
        "| `static_analysis_tools.txt` | Linters, Bandit, type checking |",
        "",
        "#### Authentication & Secrets (2 documents)",
        "| Document | Topics |",
        "|----------|--------|",
        "| `authentication_patterns.txt` | Passwords, MFA, sessions, JWT, OAuth |",
        "| `secrets_management.txt` | Vault, rotation, git-secrets |",
        "",
        "#### Input/Output Security (3 documents)",
        "| Document | Topics |",
        "|----------|--------|",
        "| `input_validation_patterns.txt` | Whitelisting, encoding, canonicalization |",
        "| `secure_deserialization.txt` | Pickle dangers, HMAC, allowlists |",
        "| `api_design_security.txt` | OAuth, rate limiting, validation |",
        "",
        "#### Infrastructure (2 documents)",
        "| Document | Topics |",
        "|----------|--------|",
        "| `network_security_fundamentals.txt` | Firewall, IDS/IPS, VPN, zero trust |",
        "| `configuration_management.txt` | Secrets managers, IaC, defaults |",
        "",
        "#### Supply Chain (2 documents)",
        "| Document | Topics |",
        "|----------|--------|",
        "| `supply_chain_security.txt` | SBOM, typosquatting, build security |",
        "| `dependency_management_practices.txt` | Pinning, scanning, updates |",
        "",
        "#### Operations (2 documents)",
        "| Document | Topics |",
        "|----------|--------|",
        "| `incident_response_procedures.txt` | Detection, containment, recovery |",
        "| `penetration_testing_methodology.txt` | Recon, exploitation, reporting |",
        "",
        "#### Governance (1 document)",
        "| Document | Topics |",
        "|----------|--------|",
        "| `security_compliance_frameworks.txt` | PCI-DSS, GDPR, SOC 2, NIST |",
        "",
        "### 3.2 Concept Overlap Design",
        "",
        "Documents were designed with intentional concept overlap to strengthen semantic connections:",
        "",
        "```",
        "authentication â†â†’ api_design_security",
        "                â†•",
        "         secrets_management",
        "                â†•",
        "       configuration_management",
        "",
        "injection â†â†’ input_validation_patterns",
        "           â†•",
        "    secure_code_review",
        "           â†•",
        "    static_analysis_tools",
        "```",
        "",
        "Key bridging concepts appearing in 3+ documents:",
        "- Input validation (4 docs)",
        "- Authentication/OAuth/JWT (4 docs)",
        "- Secrets management (3 docs)",
        "- Vulnerability scanning (3 docs)",
        "- SAST/static analysis (3 docs)",
        "",
        "---",
        "",
        "## Part 4: Security Showcase",
        "",
        "### 4.1 Running the Showcase",
        "",
        "```bash",
        "python secureshowcase.py",
        "```",
        "",
        "### 4.2 Showcase Features",
        "",
        "| Section | What It Demonstrates |",
        "|---------|---------------------|",
        "| Corpus Ingestion | Loads 18 security documents, builds knowledge graph |",
        "| Knowledge Hierarchy | 4-layer security concept organization |",
        "| Key Concepts (PageRank) | Central terms: security, authentication, validation |",
        "| Threat Analysis (TF-IDF) | Distinctive terms: deserialization, pickle, injection |",
        "| Concept Associations | Semantic links: injectionâ†”SQL, authâ†”identity |",
        "| Vulnerability Search | Query expansion for security topics |",
        "| STRIDE Mapping | Threat categories to documents |",
        "| Secure Coding Guidance | Passage retrieval for practices |",
        "| OWASP Coverage | 10/10 category assessment |",
        "| Knowledge Gaps | Topic coverage analysis |",
        "",
        "### 4.3 Sample Output",
        "",
        "```",
        "âœ“ Processed 18 security documents",
        "âœ“ Created 1,200+ security term minicolumns",
        "âœ“ Formed 150,000+ semantic connections",
        "",
        "Key Security Concepts (PageRank):",
        "  1. security           â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 0.0211",
        "  2. authentication     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 0.0075",
        "  3. validation         â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 0.0071",
        "  4. vulnerabilities    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 0.0067",
        "",
        "OWASP Coverage: 10/10 categories addressed",
        "Security Topic Audit: 12/12 essential topics present",
        "```",
        "",
        "### 4.4 Use Cases for Security Teams",
        "",
        "1. **Security Knowledge Base** - Query documentation for threat info",
        "2. **Secure Coding Reference** - Find relevant coding practices",
        "3. **Gap Analysis** - Identify missing security documentation",
        "4. **RAG Systems** - Build security Q&A with passage retrieval",
        "5. **Threat Modeling Support** - Cross-reference threats with mitigations",
        "6. **Training Material** - Comprehensive security topic coverage",
        "",
        "---",
        "",
        "## Part 5: CI/CD Security Recommendations",
        "",
        "### 5.1 Recommended Security Job",
        "",
        "Add to `.github/workflows/ci.yml`:",
        "",
        "```yaml",
        "security-scan:",
        "  name: \"ðŸ”’ Security Scan\"",
        "  runs-on: ubuntu-latest",
        "  needs: smoke-tests",
        "  steps:",
        "  - uses: actions/checkout@v4",
        "",
        "  - name: Set up Python 3.11",
        "    uses: actions/setup-python@v5",
        "    with:",
        "      python-version: '3.11'",
        "",
        "  - name: Install security tools",
        "    run: pip install bandit safety pip-audit detect-secrets",
        "",
        "  - name: Run Bandit (SAST)",
        "    run: bandit -r cortical/ -ll -f txt",
        "",
        "  - name: Check Dependencies",
        "    run: |",
        "      pip install -e \".[dev]\"",
        "      pip-audit --desc || echo \"âš ï¸ Review required\"",
        "",
        "  - name: Scan for Secrets",
        "    run: detect-secrets scan --all-files --exclude-files '\\.git/.*'",
        "```",
        "",
        "### 5.2 Security Tools Summary",
        "",
        "| Tool | Purpose | Catches |",
        "|------|---------|---------|",
        "| Bandit | Python SAST | Injection, hardcoded secrets, unsafe deserialize |",
        "| pip-audit | Dependency CVEs | Known vulnerabilities in packages |",
        "| safety | PyPI CVE database | Security issues in installed packages |",
        "| detect-secrets | Secret scanning | API keys, passwords in code |",
        "",
        "---",
        "",
        "## Part 6: Remediation Task List",
        "",
        "### Priority: HIGH",
        "",
        "| ID | Task | File | Effort |",
        "|----|------|------|--------|",
        "| SEC-001 | Add pickle security warning to docs | README.md | 30 min |",
        "| SEC-002 | Add Bandit to CI pipeline | ci.yml | 1 hour |",
        "",
        "### Priority: MEDIUM",
        "",
        "| ID | Task | File | Effort |",
        "|----|------|------|--------|",
        "| SEC-003 | Implement pickle HMAC verification | persistence.py | 4 hours |",
        "| SEC-004 | Add pip-audit to CI | ci.yml | 1 hour |",
        "| SEC-005 | Add secret scanning to CI | ci.yml | 1 hour |",
        "| SEC-006 | Document MCP server security model | docs/security.md | 2 hours |",
        "",
        "### Priority: LOW",
        "",
        "| ID | Task | File | Effort |",
        "|----|------|------|--------|",
        "| SEC-007 | Restrict task-manager skill Write access | SKILL.md | 30 min |",
        "| SEC-008 | Deprecate pickle format | persistence.py | 2 hours |",
        "| SEC-009 | Add security-focused tests | tests/security/ | 4 hours |",
        "| SEC-010 | Implement input fuzzing | hypothesis | 8 hours |",
        "",
        "---",
        "",
        "## Part 7: Extending the Security Corpus",
        "",
        "### 7.1 Adding New Security Documents",
        "",
        "1. Create document in `samples/` directory:",
        "   ```bash",
        "   vim samples/your_security_topic.txt",
        "   ```",
        "",
        "2. Add to `secureshowcase.py` SECURITY_SAMPLES list:",
        "   ```python",
        "   SECURITY_SAMPLES = [",
        "       ...",
        "       \"your_security_topic\",",
        "   ]",
        "   ```",
        "",
        "3. Re-run showcase to verify integration:",
        "   ```bash",
        "   python secureshowcase.py",
        "   ```",
        "",
        "### 7.2 Document Guidelines",
        "",
        "- **Length:** 250-350 words (optimal for chunking)",
        "- **Structure:** Topic sentence per paragraph",
        "- **Terminology:** Use standard security terms",
        "- **Overlap:** Include 3-5 terms from existing documents",
        "- **Specificity:** Balance general concepts with concrete examples",
        "",
        "### 7.3 Suggested Future Topics",
        "",
        "| Topic | Concepts to Include |",
        "|-------|---------------------|",
        "| Cloud Security | IAM, S3 policies, VPC, shared responsibility |",
        "| Container Security | Docker, Kubernetes, image scanning, runtime |",
        "| Mobile Security | OWASP Mobile Top 10, certificate pinning |",
        "| IoT Security | Firmware, protocols, physical security |",
        "| Privacy Engineering | Data minimization, anonymization, consent |",
        "",
        "---",
        "",
        "## Part 8: Files Reference",
        "",
        "### Created/Modified Files",
        "",
        "| File | Purpose |",
        "|------|---------|",
        "| `SECURITY_REVIEW.md` | Complete security audit report |",
        "| `CODE_REVIEW.md` | Code quality findings |",
        "| `secureshowcase.py` | Security-focused demonstration |",
        "| `samples/application_security_fundamentals.txt` | OWASP, auth, crypto |",
        "| `samples/supply_chain_security.txt` | Dependencies, SBOM |",
        "| `samples/secure_deserialization.txt` | Pickle dangers |",
        "| `samples/secrets_management.txt` | Vault, rotation |",
        "| `samples/threat_modeling.txt` | STRIDE, attack trees |",
        "| `samples/secure_code_review.txt` | Review practices |",
        "| `samples/secure_development_lifecycle.txt` | SSDLC phases |",
        "| `samples/devsecops_practices.txt` | CI/CD security |",
        "| `samples/api_design_security.txt` | OAuth, validation |",
        "| `samples/dependency_management_practices.txt` | Pinning, scanning |",
        "| `samples/configuration_management.txt` | IaC, secrets |",
        "| `samples/static_analysis_tools.txt` | Bandit, linters |",
        "| `samples/input_validation_patterns.txt` | Whitelisting |",
        "| `samples/authentication_patterns.txt` | MFA, JWT |",
        "| `samples/network_security_fundamentals.txt` | Firewall, IDS |",
        "| `samples/incident_response_procedures.txt` | IR lifecycle |",
        "| `samples/penetration_testing_methodology.txt` | Pentest phases |",
        "| `samples/security_compliance_frameworks.txt` | PCI, GDPR, SOC2 |",
        "| `docs/security-knowledge-transfer.md` | This document |",
        "",
        "---",
        "",
        "## Part 9: Checklist for Knowledge Transfer",
        "",
        "### For New Team Members",
        "",
        "- [ ] Read `SECURITY_REVIEW.md` for security posture overview",
        "- [ ] Run `python secureshowcase.py` to see security corpus in action",
        "- [ ] Review `CODE_REVIEW.md` for code quality context",
        "- [ ] Understand pickle risk in `persistence.py:156`",
        "- [ ] Know safer alternatives: JSON StateLoader, protobuf format",
        "",
        "### For Security Engineers",
        "",
        "- [ ] Review 60+ security checks in `SECURITY_REVIEW.md`",
        "- [ ] Implement CI security job from template",
        "- [ ] Prioritize SEC-001 through SEC-005 tasks",
        "- [ ] Consider extending security sample corpus",
        "- [ ] Set up automated dependency scanning",
        "",
        "### For DevOps/Platform",
        "",
        "- [ ] Add security-scan job to CI pipeline",
        "- [ ] Configure Bandit, pip-audit, detect-secrets",
        "- [ ] Set up secret scanning pre-commit hooks",
        "- [ ] Document corpus file permissions (600 for pickle)",
        "",
        "---",
        "",
        "## Appendix A: Quick Commands",
        "",
        "```bash",
        "# Run security showcase",
        "python secureshowcase.py",
        "",
        "# Run regular showcase",
        "python showcase.py",
        "",
        "# Check for secrets in git history",
        "git log -p --all -S \"password\\|secret\\|api_key\"",
        "",
        "# Scan Python code with Bandit",
        "pip install bandit && bandit -r cortical/ -ll",
        "",
        "# Check dependencies for vulnerabilities",
        "pip install pip-audit && pip-audit",
        "",
        "# Find pickle usage",
        "grep -rn \"pickle.load\" cortical/",
        "```",
        "",
        "---",
        "",
        "## Appendix B: Security Review Checklist Summary",
        "",
        "60+ checks performed across categories:",
        "",
        "- **Git History:** 9 checks (secrets, credentials, force pushes)",
        "- **File System:** 8 checks (hooks, dotfiles, symlinks, executables)",
        "- **Binary Detection:** 7 checks (PE, ELF, Mach-O, ZIP, images)",
        "- **Encoding Attacks:** 6 checks (base64, hex, Unicode)",
        "- **Python Code:** 9 checks (exec, eval, pickle, subprocess)",
        "- **Network/Shell:** 6 checks (sockets, HTTP, curl, netcat)",
        "- **Supply Chain:** 5 checks (dependencies, typosquatting)",
        "- **CI/CD:** 5 checks (workflows, secrets, actions)",
        "- **Configuration:** 4 checks (CLAUDE.md, skills)",
        "- **Regex (ReDoS):** 3 checks (nested quantifiers)",
        "",
        "All checks passed. See `SECURITY_REVIEW.md` for full details.",
        "",
        "---",
        "",
        "*Document generated as part of security review session on 2025-12-14*"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "samples/api_design_security.txt",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "API Design and Security",
        "",
        "API security protects the interfaces through which applications communicate. As APIs expose application functionality to external consumers, they represent critical attack surfaces requiring careful design. RESTful APIs, GraphQL endpoints, and RPC services all face similar security challenges around authentication, authorization, and input handling.",
        "",
        "Authentication verifies the identity of API consumers. API keys provide simple authentication but offer limited security and no user context. OAuth 2.0 enables delegated authorization where users grant limited access without sharing credentials. JSON Web Tokens (JWT) carry signed claims about identity and permissions. Mutual TLS authenticates both client and server using certificates.",
        "",
        "Authorization determines what authenticated consumers can access. Role-based access control maps users to roles with defined permissions. Attribute-based access control evaluates policies based on user attributes, resource properties, and context. API gateways centralize authorization enforcement across multiple backend services.",
        "",
        "Input validation protects APIs from malicious payloads. Schema validation ensures requests match expected structure. Type checking prevents type confusion attacks. Length limits prevent buffer overflows and denial of service. Content-type validation rejects unexpected data formats. All validation should occur server-side since client-side checks are easily bypassed.",
        "",
        "Rate limiting prevents abuse and ensures availability. Token bucket algorithms allow burst traffic while enforcing average limits. Per-user limits prevent individual accounts from monopolizing resources. Graduated responses warn before blocking, giving legitimate users opportunity to reduce load.",
        "",
        "Output encoding prevents injection when API responses are rendered. JSON encoding handles special characters safely. XML encoding prevents entity injection. Error responses avoid leaking sensitive information like stack traces, database schemas, or internal network topology. Consistent error formats help consumers handle failures gracefully without revealing implementation details."
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "samples/application_security_fundamentals.txt",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "Application Security Fundamentals",
        "",
        "Application security encompasses the measures taken to improve the security of applications by finding, fixing, and preventing security vulnerabilities. The OWASP Top 10 represents the most critical security risks to web applications, including injection attacks, broken authentication, sensitive data exposure, and cross-site scripting.",
        "",
        "Input validation is the first line of defense against many attacks. All user input should be treated as untrusted and validated against expected formats, lengths, and character sets. Whitelisting acceptable input is preferred over blacklisting known bad patterns, as attackers constantly discover new attack vectors.",
        "",
        "Authentication verifies user identity through something they know (passwords), something they have (tokens), or something they are (biometrics). Multi-factor authentication combines these methods for stronger security. Session management must ensure tokens are unpredictable, properly scoped, and securely transmitted.",
        "",
        "Authorization determines what authenticated users can access. The principle of least privilege dictates granting only the minimum permissions necessary. Role-based access control (RBAC) and attribute-based access control (ABAC) provide frameworks for managing permissions at scale.",
        "",
        "Secure coding practices include parameterized queries to prevent SQL injection, output encoding to prevent XSS, and proper error handling that doesn't leak sensitive information. Security should be built into the development lifecycle through threat modeling, code review, and automated security testing.",
        "",
        "Cryptography protects data confidentiality and integrity. Encryption at rest protects stored data while encryption in transit (TLS) protects network communications. Hash functions verify data integrity, and digital signatures provide non-repudiation. Using well-tested cryptographic libraries is essential since implementing cryptography correctly is extremely difficult."
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "samples/authentication_patterns.txt",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "Authentication Patterns and Implementation",
        "",
        "Authentication verifies that users are who they claim to be, forming the foundation for access control decisions. Modern authentication systems balance security with usability, providing strong identity verification without creating friction that drives users to insecure workarounds.",
        "",
        "Password-based authentication remains common despite known weaknesses. Secure password storage uses adaptive hashing algorithms like bcrypt, scrypt, or Argon2 that are intentionally slow to prevent brute force attacks. Salting ensures identical passwords produce different hashes. Password policies should encourage length over complexity since longer passphrases are both more secure and more memorable.",
        "",
        "Multi-factor authentication combines multiple verification methods. Something you know (passwords), something you have (tokens, phones), and something you are (biometrics) provide independent factors. TOTP apps generate time-based codes. Hardware security keys provide phishing-resistant authentication. SMS codes are better than passwords alone but vulnerable to SIM swapping.",
        "",
        "Session management maintains authentication state across requests. Session tokens must be unpredictable, using cryptographically secure random generation. Secure cookie attributes prevent theft: HttpOnly blocks JavaScript access, Secure requires HTTPS, SameSite prevents cross-site request forgery. Session expiration limits the window for token theft exploitation.",
        "",
        "Token-based authentication suits API and microservice architectures. JSON Web Tokens carry signed claims about identity and permissions. Short expiration times limit exposure from token theft. Refresh tokens enable obtaining new access tokens without re-authentication. Token revocation requires maintaining state, often via short expiration plus refresh token rotation.",
        "",
        "Federated identity delegates authentication to trusted identity providers. OAuth 2.0 enables authorization without sharing credentials. OpenID Connect adds identity layer atop OAuth. SAML provides enterprise single sign-on. Federation reduces password fatigue while centralizing authentication security improvements.",
        "",
        "Account recovery is often the weakest link. Security questions are easily researched. Email-based recovery depends on email account security. SMS recovery is vulnerable to SIM swapping. Recovery codes stored securely provide a fallback when other methods fail. Balance recovery convenience against the risk of unauthorized access."
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "samples/configuration_management.txt",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "Configuration Management Principles",
        "",
        "Configuration management controls the settings that determine application behavior across environments. Proper configuration management ensures consistency, enables automation, and prevents security misconfigurations. The principle of separating configuration from code allows the same codebase to run differently in development, staging, and production.",
        "",
        "Environment-specific configuration adapts applications to their deployment context. Database connection strings, API endpoints, and feature flags vary between environments. Twelve-factor app methodology recommends storing configuration in environment variables. Configuration files work for complex settings but must be managed carefully to prevent sensitive data exposure.",
        "",
        "Secrets require special handling distinct from regular configuration. Database passwords, API keys, and encryption keys must never appear in source code or configuration files committed to version control. Secrets managers like HashiCorp Vault, AWS Secrets Manager, and Azure Key Vault provide secure storage with access control and audit logging. Applications retrieve secrets at runtime rather than storing them locally.",
        "",
        "Configuration validation catches errors before they cause production incidents. Schema validation ensures configuration matches expected structure. Range checking verifies numeric values are sensible. Dependency checking confirms referenced resources exist. Fail-fast behavior surfaces configuration problems immediately rather than causing subtle runtime failures.",
        "",
        "Infrastructure as Code applies version control to infrastructure configuration. Terraform, CloudFormation, and Pulumi define infrastructure declaratively. Changes undergo code review before application. Drift detection alerts when actual infrastructure diverges from defined state. This approach enables consistent, auditable, reproducible infrastructure deployments.",
        "",
        "Default security requires secure configuration out of the box. Applications should start in their most secure state, requiring explicit action to reduce security. Default passwords must never exist. Debug modes should require explicit enablement. Error messages should be minimal in production. Documentation should guide users toward secure configuration choices."
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "samples/dependency_management_practices.txt",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "Dependency Management Best Practices",
        "",
        "Modern software relies heavily on third-party libraries, making dependency management critical for both functionality and security. The average application includes hundreds of transitive dependencies, each representing potential vulnerabilities or supply chain risks. Effective dependency management balances using community code efficiently while managing associated risks.",
        "",
        "Version pinning ensures reproducible builds by specifying exact dependency versions. Lock files capture the complete dependency tree including transitive dependencies. Semantic versioning conventions help predict compatibility: patch versions fix bugs, minor versions add features, major versions may break compatibility. However, any version update can introduce vulnerabilities or malicious code.",
        "",
        "Vulnerability scanning identifies known security issues in dependencies. Tools like pip-audit, npm audit, and Snyk check packages against vulnerability databases. Continuous monitoring alerts when new vulnerabilities affect existing dependencies. Severity scoring helps prioritize remediation efforts. Not all vulnerabilities are exploitable in every context, requiring assessment of actual risk.",
        "",
        "Update strategies balance security with stability. Automated updates with testing catch vulnerabilities quickly but risk breaking changes. Scheduled update cycles provide predictable maintenance windows. Security-only updates minimize change while addressing critical issues. Whatever strategy chosen, having a process ensures dependencies don't become dangerously stale.",
        "",
        "Supply chain attacks target the dependency ecosystem itself. Typosquatting publishes malicious packages with names similar to popular libraries. Account compromise lets attackers push malicious updates to legitimate packages. Build system attacks inject malware during package creation. Defenses include verifying package signatures, using trusted registries, and reviewing dependency changes carefully.",
        "",
        "Minimal dependencies reduce attack surface. Each dependency adds code that must be trusted and maintained. Evaluate whether functionality justifies the dependency or could be implemented directly. Remove unused dependencies regularly. Consider vendoring critical dependencies to control updates and reduce external trust requirements."
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "samples/devsecops_practices.txt",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "DevSecOps Practices and Automation",
        "",
        "DevSecOps integrates security into DevOps pipelines, making security checks automatic and continuous rather than manual gates. Security becomes everyone's responsibility, not just the security team's. Automation enables fast feedback loops where developers learn about vulnerabilities within minutes of committing code.",
        "",
        "Pipeline security scanning runs multiple tools in parallel. Static Application Security Testing (SAST) analyzes source code for vulnerability patterns like SQL injection, command injection, and insecure deserialization. Software Composition Analysis (SCA) checks dependencies against vulnerability databases. Secret scanning detects accidentally committed credentials, API keys, and tokens.",
        "",
        "Container security addresses the unique risks of containerized deployments. Image scanning checks base images and installed packages for known vulnerabilities. Dockerfile linting enforces security best practices like non-root users and minimal base images. Runtime security monitors container behavior for anomalies suggesting compromise.",
        "",
        "Infrastructure as Code security applies the same rigor to infrastructure definitions. Terraform and CloudFormation templates undergo security review and scanning. Policy as code enforces security requirements automatically. Drift detection alerts when production configurations diverge from approved definitions.",
        "",
        "Dependency management automation keeps third-party code secure. Automated pull requests update vulnerable dependencies. Lock files ensure reproducible builds with known-good versions. Software Bill of Materials (SBOM) generation documents all components for supply chain transparency. Typosquatting detection prevents installation of malicious packages with similar names.",
        "",
        "Security testing in CI/CD provides fast feedback without blocking velocity. Critical vulnerabilities fail the build immediately. Medium-severity findings create tracked issues for remediation. False positive management prevents alert fatigue. Security dashboards track vulnerability trends over time, measuring improvement and identifying systemic issues."
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "samples/incident_response_procedures.txt",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "Incident Response Procedures",
        "",
        "Incident response provides a structured approach to handling security breaches and cyberattacks. Preparation, detection, containment, eradication, recovery, and lessons learned form the incident response lifecycle. Having documented procedures before incidents occur enables faster, more effective response under pressure.",
        "",
        "Preparation establishes incident response capabilities before they're needed. Response teams with defined roles and communication channels coordinate effectively during incidents. Playbooks document procedures for common incident types. Regular tabletop exercises and simulations validate preparedness. Relationships with external resources like forensics firms and law enforcement facilitate response when needed.",
        "",
        "Detection and analysis identify that an incident has occurred and assess its scope. Security monitoring, intrusion detection, and user reports surface potential incidents. Triage prioritizes response based on severity and business impact. Initial analysis determines attack vectors, affected systems, and whether the attack is ongoing. Preserving evidence enables later forensic analysis and potential legal action.",
        "",
        "Containment limits damage while maintaining evidence for investigation. Short-term containment stops immediate harm, perhaps by isolating affected systems. Long-term containment allows continued operation while preparing permanent fixes. Containment strategies balance business continuity against security risks. Documentation captures actions taken and their effects.",
        "",
        "Eradication removes the attacker's presence from the environment. Malware removal, account cleanup, and vulnerability patching eliminate the initial access vector. Validation confirms removal was complete. Root cause analysis identifies how the attack succeeded to prevent recurrence. System hardening addresses weaknesses that enabled the breach.",
        "",
        "Recovery restores normal operations with confidence that threats are eliminated. Phased restoration prioritizes critical systems. Monitoring for reinfection or attacker return continues through recovery. Business validation confirms systems function correctly. Communication with stakeholders maintains trust and meets disclosure requirements.",
        "",
        "Post-incident activities improve future response. Lessons learned sessions identify what worked and what needs improvement. Metrics track response effectiveness over time. Updated procedures incorporate lessons learned. Threat intelligence sharing helps the broader community defend against similar attacks."
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "samples/input_validation_patterns.txt",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "Input Validation Patterns and Practices",
        "",
        "Input validation is the first line of defense against injection attacks, data corruption, and application crashes. All data from external sources should be treated as untrusted and validated before use. Effective validation combines multiple techniques at appropriate layers to catch malicious and malformed input.",
        "",
        "Whitelisting validates that input matches expected patterns, rejecting anything else. This positive security model is more robust than blacklisting known-bad patterns because attackers constantly discover new attack vectors. Regular expressions define acceptable formats for emails, phone numbers, and identifiers. Enumeration restricts values to predefined sets.",
        "",
        "Type coercion and validation ensure data matches expected types. String inputs parsed as numbers should fail gracefully on non-numeric content. Date parsing should handle format variations while rejecting invalid dates. Boolean parsing should have clear true/false semantics. Type validation prevents type confusion vulnerabilities.",
        "",
        "Length and size limits prevent resource exhaustion and buffer overflows. Maximum string lengths bound memory allocation. Array size limits prevent algorithmic complexity attacks. File upload size limits prevent storage exhaustion. Nested structure depth limits prevent stack overflows in recursive processing.",
        "",
        "Encoding and escaping neutralize special characters that could be interpreted as code. SQL parameterization separates data from queries, preventing SQL injection. HTML encoding prevents cross-site scripting when displaying user content. Shell escaping prevents command injection. The appropriate encoding depends on the output context.",
        "",
        "Validation layers provide defense in depth. Client-side validation improves user experience with immediate feedback but cannot be trusted for security. Server-side validation enforces security rules before processing. Database constraints provide final validation ensuring data integrity. Each layer catches different issues and compensates for potential bypasses of other layers.",
        "",
        "Canonicalization converts input to standard form before validation. Path canonicalization resolves directory traversal sequences like \"../\". URL normalization handles encoding variations. Unicode normalization addresses equivalent character representations. Without canonicalization, validation may miss attacks using alternate representations."
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "samples/network_security_fundamentals.txt",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "Network Security Fundamentals",
        "",
        "Network security protects the infrastructure that connects systems and transmits data. Defense in depth applies multiple security layers so that breach of one control doesn't compromise the entire network. Perimeter security, internal segmentation, and endpoint protection work together to detect and prevent attacks.",
        "",
        "A firewall controls traffic flow between network segments based on rules. Firewalls filter packets using configured policies. Stateful firewalls track connection state to allow return traffic for established sessions. Next-generation firewalls add application awareness, inspecting packet contents to identify applications regardless of port. Web application firewalls specifically protect HTTP traffic against injection, cross-site scripting, and other web attacks.",
        "",
        "Network segmentation isolates sensitive systems from general network traffic. VLANs separate broadcast domains logically. Microsegmentation extends this to individual workloads, limiting lateral movement after initial compromise. Zero trust architecture assumes breach and verifies every access request regardless of network location.",
        "",
        "Intrusion detection systems monitor network traffic for suspicious patterns. Signature-based detection matches known attack patterns while anomaly-based detection identifies deviations from normal behavior. Intrusion prevention systems can automatically block detected attacks. Security information and event management aggregates logs for correlation and alerting.",
        "",
        "Encryption protects data in transit across networks. TLS secures web traffic, email, and many application protocols. VPNs create encrypted tunnels for remote access or site-to-site connectivity. Certificate management ensures proper authentication of encrypted connections. Perfect forward secrecy prevents decryption of past sessions if keys are later compromised.",
        "",
        "DNS security prevents attacks that exploit name resolution. DNSSEC cryptographically validates DNS responses. DNS filtering blocks access to known malicious domains. DNS over HTTPS encrypts queries to prevent eavesdropping. Monitoring DNS traffic reveals command and control communications and data exfiltration attempts."
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "samples/penetration_testing_methodology.txt",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "Penetration Testing Methodology",
        "",
        "Penetration testing simulates real attacks to identify vulnerabilities before malicious actors exploit them. Unlike vulnerability scanning which finds known issues automatically, penetration testing applies human creativity and persistence to discover complex attack paths. Professional penetration testers follow methodologies that ensure thorough, safe, and valuable assessments.",
        "",
        "Engagement scoping defines testing boundaries and objectives. Rules of engagement specify which systems are in scope, permitted attack techniques, and emergency contacts. Time windows coordinate testing with operations teams. Legal agreements protect both testers and organizations. Clear scope prevents misunderstandings and ensures relevant coverage.",
        "",
        "Reconnaissance gathers information about targets before active testing begins. Passive reconnaissance collects publicly available information: DNS records, employee names, technology stacks, and leaked credentials. Active reconnaissance probes target systems for open ports, services, and versions. Thorough reconnaissance enables focused, effective attacks during later phases.",
        "",
        "Vulnerability identification discovers weaknesses that could enable unauthorized access. Automated scanners find known vulnerabilities efficiently. Manual testing discovers logic flaws, business process vulnerabilities, and issues scanners miss. Authentication testing probes for weak credentials and session management flaws. Each discovered vulnerability is evaluated for exploitability.",
        "",
        "Exploitation attempts to leverage vulnerabilities to gain access. Proof-of-concept exploits demonstrate risk without causing damage. Privilege escalation seeks higher access levels after initial compromise. Lateral movement explores what an attacker could reach from compromised systems. Careful exploitation avoids disrupting production systems.",
        "",
        "Post-exploitation assesses the impact of successful attacks. Data access determines what sensitive information attackers could steal. Persistence mechanisms show how attackers maintain access. Documentation captures attack paths for remediation guidance. Clean-up removes any artifacts created during testing.",
        "",
        "Reporting communicates findings to enable remediation. Executive summaries convey risk to leadership. Technical details guide remediation efforts. Prioritization helps organizations address the most critical issues first. Remediation verification confirms fixes are effective. Quality reports transform testing into security improvements."
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "samples/secrets_management.txt",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "Secrets Management Best Practices",
        "",
        "Secrets management involves securely storing, accessing, and rotating sensitive credentials like API keys, database passwords, encryption keys, and certificates. Hardcoded secrets in source code are a leading cause of security breaches, as code repositories are frequently exposed through misconfiguration or insider threats.",
        "",
        "Environment variables separate secrets from code but have limitations. They're visible to all processes in the same environment and often logged accidentally. Container orchestration platforms may expose environment variables in configuration. Environment variables work for simple deployments but don't scale well.",
        "",
        "Dedicated secrets managers like HashiCorp Vault, AWS Secrets Manager, and Azure Key Vault provide centralized, audited secrets storage. They offer encryption at rest, fine-grained access control, and automatic rotation. Applications authenticate to retrieve secrets at runtime rather than storing them locally.",
        "",
        "Git history preservation means secrets committed even briefly remain discoverable. Tools like git-secrets, truffleHog, and detect-secrets scan repositories for accidentally committed credentials. Pre-commit hooks prevent secrets from being committed in the first place. If secrets are exposed, they must be rotated immediately.",
        "",
        "Secret rotation limits the window of opportunity for compromised credentials. Automated rotation changes secrets on a schedule without manual intervention. Zero-downtime rotation ensures applications always have valid credentials. Short-lived credentials like JWT tokens naturally expire, forcing regular renewal.",
        "",
        "Principle of least privilege applies to secrets: applications should only access the secrets they need. Separate secrets per environment prevent development credentials from accessing production data. Service accounts with minimal permissions limit blast radius if compromised. Audit logs track all secret access for forensic analysis."
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "samples/secure_code_review.txt",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "Secure Code Review Practices",
        "",
        "Secure code review systematically examines source code to identify security vulnerabilities before deployment. It complements automated scanning by catching logic flaws and context-dependent issues that tools miss. Effective code review requires security knowledge, attention to detail, and understanding of the application's threat model.",
        "",
        "Authentication and session management deserve careful scrutiny. Review password hashing algorithms, ensuring use of bcrypt, scrypt, or Argon2 rather than MD5 or SHA1. Check session token generation for sufficient randomness. Verify session expiration and invalidation on logout. Look for authentication bypass through parameter manipulation.",
        "",
        "Input handling is a common source of vulnerabilities. Trace all user input from entry points through processing. Verify validation occurs before use in dangerous operations. Check for injection vulnerabilities in SQL queries, shell commands, LDAP queries, and XML parsers. Ensure output encoding matches the context (HTML, JavaScript, URL, CSS).",
        "",
        "Access control logic often contains subtle flaws. Verify authorization checks occur on every sensitive operation, not just UI elements. Look for insecure direct object references where user-controlled IDs access unauthorized resources. Check that privilege escalation paths are properly restricted.",
        "",
        "Cryptographic implementation requires expert review. Verify use of standard algorithms with appropriate key sizes. Check for hardcoded keys, weak random number generation, and improper IV handling. Ensure TLS configuration uses modern protocols and cipher suites. Review certificate validation for proper hostname and chain verification.",
        "",
        "Error handling and logging need security attention. Ensure errors don't leak sensitive information like stack traces, database schemas, or internal paths. Verify logging captures security-relevant events without recording sensitive data. Check that error conditions don't create exploitable states."
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "samples/secure_deserialization.txt",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "Secure Deserialization Practices",
        "",
        "Deserialization vulnerabilities occur when applications reconstruct objects from untrusted data without proper validation. Insecure deserialization can lead to remote code execution, denial of service, and authentication bypass. Many programming languages have built-in serialization that can instantiate arbitrary objects during deserialization.",
        "",
        "Python's pickle module is particularly dangerous because it can execute arbitrary code during deserialization. The pickle protocol allows defining __reduce__ methods that specify how to reconstruct objects, which attackers exploit to execute malicious commands. Never unpickle data from untrusted sources.",
        "",
        "Safer alternatives exist for most serialization needs. JSON provides a text-based format that only supports primitive types and cannot execute code. Protocol Buffers and MessagePack offer efficient binary formats with strict schemas. These formats separate data from behavior, preventing code execution attacks.",
        "",
        "When deserialization of complex objects is necessary, implement defense in depth. Cryptographic signatures verify data hasn't been tampered with. HMAC authentication ensures data comes from trusted sources. Schema validation confirms data matches expected structure before full deserialization.",
        "",
        "Allowlists restrict which classes can be deserialized. Java's ObjectInputFilter and similar mechanisms in other languages reject unexpected object types. This prevents attackers from instantiating dangerous classes even if they control the serialized data.",
        "",
        "Sandboxing limits the impact of successful attacks. Running deserialization in isolated processes with minimal privileges contains potential damage. Container isolation and seccomp filters restrict what compromised code can do. Monitoring and alerting detect exploitation attempts."
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "samples/secure_development_lifecycle.txt",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "Secure Software Development Lifecycle",
        "",
        "The Secure Software Development Lifecycle (SSDLC) integrates security practices into every phase of software creation. Rather than treating security as an afterthought, SSDLC embeds threat modeling, secure coding, and vulnerability testing from requirements through deployment. This shift-left approach catches vulnerabilities when they're cheapest to fix.",
        "",
        "Requirements phase establishes security objectives alongside functional requirements. Security user stories capture authentication needs, data protection requirements, and compliance constraints. Abuse cases document how attackers might misuse the system. These artifacts inform later threat modeling and testing activities.",
        "",
        "Design phase applies secure architecture patterns. Defense in depth layers multiple controls so single failures don't compromise security. Principle of least privilege restricts access to minimum necessary permissions. Input validation boundaries define where untrusted data enters the system. Cryptographic decisions specify algorithms, key management, and certificate handling.",
        "",
        "Implementation phase follows secure coding standards. Developers use parameterized queries to prevent SQL injection, output encoding to prevent cross-site scripting, and proper error handling that doesn't leak sensitive information. Code review catches logic flaws and validates security control implementation. Static analysis tools identify common vulnerability patterns automatically.",
        "",
        "Testing phase verifies security controls work as designed. Unit tests exercise input validation and access control logic. Integration tests verify authentication flows and session management. Dynamic application security testing (DAST) probes running applications for vulnerabilities. Penetration testing simulates real attacks against the complete system.",
        "",
        "Deployment phase ensures secure configuration in production. Secrets management provides credentials without hardcoding. Infrastructure as code enables consistent, auditable deployments. Monitoring and logging capture security events for incident response. Vulnerability scanning continues throughout the operational lifetime."
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "samples/security_compliance_frameworks.txt",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "Security Compliance Frameworks",
        "",
        "Security compliance frameworks provide structured approaches to managing information security risks. Regulatory requirements like GDPR, HIPAA, and PCI-DSS mandate specific controls for certain data types. Voluntary frameworks like SOC 2, ISO 27001, and NIST CSF demonstrate security commitment to customers and partners.",
        "",
        "Risk assessment forms the foundation of compliance programs. Asset inventories identify what needs protection. Threat modeling considers who might attack and how. Vulnerability assessments find weaknesses in defenses. Risk scoring prioritizes remediation based on likelihood and impact. Continuous risk monitoring adapts to evolving threats.",
        "",
        "Control frameworks organize security requirements into manageable categories. Administrative controls include policies, procedures, and training. Technical controls implement security through technology: encryption, access controls, and monitoring. Physical controls protect facilities and hardware. Layered controls provide defense in depth.",
        "",
        "PCI DSS protects payment card data through twelve requirement categories. Network segmentation isolates cardholder data environments. Encryption protects stored and transmitted card data. Access controls limit who can view payment information. Regular testing validates control effectiveness. Compliance validation occurs through self-assessment or external audit depending on transaction volume.",
        "",
        "GDPR governs personal data processing for EU residents. Lawful basis justifies data collection and use. Data subject rights include access, correction, and deletion. Privacy by design embeds protection into systems from the start. Data breach notification requires reporting significant incidents within 72 hours. Penalties for non-compliance can reach 4% of global revenue.",
        "",
        "SOC 2 audits evaluate service organization controls against trust service criteria. Security, availability, processing integrity, confidentiality, and privacy form the five categories. Type 1 reports assess control design at a point in time. Type 2 reports evaluate control operation over a period. SOC 2 reports help customers evaluate vendor security posture.",
        "",
        "Compliance automation reduces manual effort and improves accuracy. Continuous monitoring tracks control status in real-time. Evidence collection automation supports audit preparation. Policy as code enforces compliance requirements automatically. Compliance dashboards provide visibility into program status. Integration with security tools ensures compliance reflects actual security posture."
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "samples/static_analysis_tools.txt",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "Static Analysis for Code Quality and Security",
        "",
        "Static analysis examines source code without executing it, finding bugs, security vulnerabilities, and style violations automatically. Unlike testing, which verifies specific scenarios, static analysis reasons about all possible execution paths. Integrating static analysis into development workflows catches issues early when they're cheapest to fix.",
        "",
        "Linters enforce coding standards and catch common mistakes. They identify unused variables, unreachable code, and style inconsistencies. Language-specific linters understand idioms and best practices. Configuration allows customizing rules to project conventions. Consistent style improves readability and reduces cognitive load during code review.",
        "",
        "Security-focused static analysis (SAST) identifies vulnerability patterns. Tools like Bandit for Python, ESLint security plugins for JavaScript, and commercial products scan for injection vulnerabilities, authentication flaws, and insecure cryptography. Pattern matching finds known-bad code constructs. Data flow analysis tracks tainted input through the program to dangerous sinks.",
        "",
        "Type checking catches errors that would otherwise surface at runtime. Static type checkers like mypy for Python and TypeScript for JavaScript verify type consistency. Gradual typing allows incremental adoption. Type annotations serve as documentation while enabling automated verification. Strict type checking prevents null pointer exceptions and type confusion bugs.",
        "",
        "Complexity metrics identify code that's difficult to understand and maintain. Cyclomatic complexity counts decision points, with high values indicating code needing refactoring. Cognitive complexity weights nested structures that strain human comprehension. Function length and parameter counts flag overly complex interfaces. Addressing complexity issues improves maintainability and reduces bug density.",
        "",
        "Integration with development workflows maximizes static analysis value. IDE plugins provide immediate feedback as developers write code. Pre-commit hooks prevent committing code with violations. CI pipeline gates ensure issues are addressed before merging. Baseline management allows adopting analysis on existing codebases without requiring immediate fixes of all legacy issues."
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "samples/supply_chain_security.txt",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "Software Supply Chain Security",
        "",
        "Software supply chain security addresses risks introduced through third-party code, build systems, and distribution channels. Modern applications depend on hundreds of external packages, each representing a potential attack vector. The SolarWinds and Log4Shell incidents demonstrated how supply chain compromises can have widespread impact.",
        "",
        "Dependency management requires vigilance against typosquatting, where attackers publish malicious packages with names similar to popular libraries. Lock files pin exact dependency versions to prevent unexpected updates. Software Bill of Materials (SBOM) documents all components in a software product, enabling rapid response to newly discovered vulnerabilities.",
        "",
        "Package integrity verification ensures downloaded code matches what the author published. Cryptographic signatures on packages prove authenticity. Hash verification confirms the package hasn't been modified. Trusted registries and mirrors reduce the risk of compromised distribution channels.",
        "",
        "Build system security protects the software creation process. Reproducible builds allow independent verification that source code produces the expected binary. Isolated build environments prevent contamination from compromised developer machines. Build provenance attestations document how software was built and by whom.",
        "",
        "Vulnerability scanning identifies known security issues in dependencies. Tools like pip-audit, npm audit, and Snyk check packages against vulnerability databases. Continuous monitoring alerts when new vulnerabilities affect existing dependencies. Automated updates with testing help maintain secure versions.",
        "",
        "Zero-trust principles apply to supply chains: verify everything, trust nothing implicitly. Vendor security assessments evaluate third-party security practices. Code review of critical dependencies catches malicious or vulnerable code. Runtime protection monitors for suspicious behavior from dependencies."
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "samples/threat_modeling.txt",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "Threat Modeling Methodology",
        "",
        "Threat modeling is a structured approach to identifying security threats, understanding their potential impact, and designing appropriate mitigations. It shifts security left in the development lifecycle, addressing vulnerabilities when they're cheapest to fix. Effective threat modeling requires understanding the system, identifying threats, and prioritizing responses.",
        "",
        "STRIDE is a mnemonic for six threat categories: Spoofing identity, Tampering with data, Repudiation of actions, Information disclosure, Denial of service, and Elevation of privilege. Each category maps to security properties: authentication prevents spoofing, integrity prevents tampering, non-repudiation prevents denial, confidentiality prevents disclosure, availability prevents DoS, and authorization prevents privilege escalation.",
        "",
        "Data flow diagrams visualize how information moves through systems. They identify trust boundaries where data crosses between different security contexts. External entities, processes, data stores, and data flows are analyzed for potential vulnerabilities. Attack surfaces emerge where untrusted input enters the system.",
        "",
        "Attack trees decompose high-level threats into specific attack paths. The root node represents the attacker's goal, while child nodes show ways to achieve it. AND nodes require all children to succeed; OR nodes need only one. Attack trees help identify the most likely and damaging attack scenarios.",
        "",
        "Risk assessment combines likelihood and impact to prioritize threats. DREAD (Damage, Reproducibility, Exploitability, Affected users, Discoverability) provides a scoring framework. Not all threats warrant immediate attention; risk-based prioritization focuses resources on the most critical issues.",
        "",
        "Mitigations address identified threats through technical controls, process changes, or accepted risk. Defense in depth layers multiple controls so failure of one doesn't compromise security. Threat models should be living documents, updated as systems evolve and new threats emerge."
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "secureshowcase.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "\"\"\"",
        "Security-Focused Cortical Text Processor Showcase",
        "==================================================",
        "",
        "This showcase demonstrates the cortical text processor's capabilities",
        "for security-related content: threat analysis, vulnerability patterns,",
        "secure coding practices, and security knowledge retrieval.",
        "\"\"\"",
        "",
        "import os",
        "import sys",
        "import time",
        "from typing import Dict, List, Tuple",
        "",
        "from cortical import CorticalTextProcessor, CorticalLayer",
        "",
        "",
        "class Timer:",
        "    \"\"\"Simple timer for measuring operation durations.\"\"\"",
        "",
        "    def __init__(self):",
        "        self.times: Dict[str, float] = {}",
        "        self._start: float = 0",
        "",
        "    def start(self, name: str):",
        "        \"\"\"Start timing an operation.\"\"\"",
        "        self._start = time.perf_counter()",
        "        self._current = name",
        "",
        "    def stop(self) -> float:",
        "        \"\"\"Stop timing and record the duration.\"\"\"",
        "        elapsed = time.perf_counter() - self._start",
        "        self.times[self._current] = elapsed",
        "        return elapsed",
        "",
        "    def get(self, name: str) -> float:",
        "        \"\"\"Get recorded time for an operation.\"\"\"",
        "        return self.times.get(name, 0)",
        "",
        "",
        "def print_header(title: str, char: str = \"=\"):",
        "    \"\"\"Print a formatted section header.\"\"\"",
        "    width = 70",
        "    print(f\"\\n{char * width}\")",
        "    print(f\"{title:^{width}}\")",
        "    print(f\"{char * width}\\n\")",
        "",
        "",
        "def print_subheader(title: str):",
        "    \"\"\"Print a formatted subsection header.\"\"\"",
        "    print(f\"\\n{title}\")",
        "    print(\"-\" * len(title))",
        "",
        "",
        "def render_bar(value: float, max_value: float, width: int = 30) -> str:",
        "    \"\"\"Render a text-based progress bar.\"\"\"",
        "    if max_value == 0:",
        "        return \" \" * width",
        "    filled = int((value / max_value) * width)",
        "    return \"â–ˆ\" * filled + \"â–‘\" * (width - filled)",
        "",
        "",
        "class SecurityShowcase:",
        "    \"\"\"Security-focused showcase of the cortical text processor.\"\"\"",
        "",
        "    # Security-related sample files to load",
        "    SECURITY_SAMPLES = [",
        "        \"application_security_fundamentals\",",
        "        \"supply_chain_security\",",
        "        \"secure_deserialization\",",
        "        \"secrets_management\",",
        "        \"threat_modeling\",",
        "        \"secure_code_review\",",
        "        \"secure_development_lifecycle\",",
        "        \"devsecops_practices\",",
        "        \"api_design_security\",",
        "        \"dependency_management_practices\",",
        "        \"configuration_management\",",
        "        \"static_analysis_tools\",",
        "        \"input_validation_patterns\",",
        "        \"authentication_patterns\",",
        "        \"network_security_fundamentals\",",
        "        \"incident_response_procedures\",",
        "        \"penetration_testing_methodology\",",
        "        \"security_compliance_frameworks\",",
        "    ]",
        "",
        "    def __init__(self, samples_dir: str = \"samples\"):",
        "        self.samples_dir = samples_dir",
        "        from cortical.tokenizer import Tokenizer",
        "        tokenizer = Tokenizer(filter_code_noise=True)",
        "        self.processor = CorticalTextProcessor(tokenizer=tokenizer)",
        "        self.loaded_files = []",
        "        self.timer = Timer()",
        "",
        "    def run(self):",
        "        \"\"\"Run the complete security demo.\"\"\"",
        "        self.print_intro()",
        "",
        "        if not self.ingest_security_corpus():",
        "            print(\"No security documents found!\")",
        "            return",
        "",
        "        self.analyze_hierarchy()",
        "        self.discover_security_concepts()",
        "        self.analyze_threat_categories()",
        "        self.find_security_associations()",
        "        self.demonstrate_vulnerability_queries()",
        "        self.demonstrate_threat_modeling()",
        "        self.demonstrate_secure_coding_search()",
        "        self.demonstrate_compliance_queries()",
        "        self.analyze_security_coverage()",
        "        self.print_security_insights()",
        "",
        "    def print_intro(self):",
        "        \"\"\"Print introduction.\"\"\"",
        "        print(\"\"\"",
        "    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—",
        "    â•‘                                                                      â•‘",
        "    â•‘          ðŸ”’  SECURITY-FOCUSED CORTICAL SHOWCASE  ðŸ”’                  â•‘",
        "    â•‘                                                                      â•‘",
        "    â•‘     Semantic search and analysis for security knowledge bases        â•‘",
        "    â•‘                                                                      â•‘",
        "    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•",
        "        \"\"\")",
        "",
        "    def ingest_security_corpus(self) -> bool:",
        "        \"\"\"Ingest security-focused documents from the corpus.\"\"\"",
        "        print_header(\"SECURITY CORPUS INGESTION\", \"â•\")",
        "",
        "        print(f\"Loading security documents from: {self.samples_dir}\")",
        "        print(\"Building security knowledge graph...\\n\")",
        "",
        "        if not os.path.exists(self.samples_dir):",
        "            print(f\"  âŒ Directory not found: {self.samples_dir}\")",
        "            return False",
        "",
        "        # Load security-related sample files",
        "        self.timer.start('document_loading')",
        "        loaded_count = 0",
        "",
        "        for sample_name in self.SECURITY_SAMPLES:",
        "            filepath = os.path.join(self.samples_dir, f\"{sample_name}.txt\")",
        "            if os.path.exists(filepath):",
        "                with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:",
        "                    content = f.read()",
        "",
        "                self.processor.process_document(sample_name, content)",
        "                word_count = len(content.split())",
        "                self.loaded_files.append((sample_name, word_count))",
        "                print(f\"  ðŸ” {sample_name:40} ({word_count:3} words)\")",
        "                loaded_count += 1",
        "            else:",
        "                print(f\"  âš ï¸  {sample_name:40} (not found)\")",
        "",
        "        load_time = self.timer.stop()",
        "",
        "        if loaded_count == 0:",
        "            return False",
        "",
        "        # Compute all analysis",
        "        print(\"\\nBuilding security concept network...\")",
        "        self.timer.start('compute_all')",
        "        self.processor.compute_all(",
        "            verbose=False,",
        "            connection_strategy='hybrid',",
        "            cluster_strictness=0.5,",
        "            bridge_weight=0.3",
        "        )",
        "        compute_time = self.timer.stop()",
        "",
        "        layer0 = self.processor.get_layer(CorticalLayer.TOKENS)",
        "        layer1 = self.processor.get_layer(CorticalLayer.BIGRAMS)",
        "",
        "        total_conns = sum(",
        "            layer.total_connections()",
        "            for layer in self.processor.layers.values()",
        "        )",
        "",
        "        print(f\"\\nâœ“ Processed {loaded_count} security documents\")",
        "        print(f\"âœ“ Created {layer0.column_count()} security term minicolumns\")",
        "        print(f\"âœ“ Created {layer1.column_count()} concept pair minicolumns\")",
        "        print(f\"âœ“ Formed {total_conns:,} semantic connections\")",
        "        print(f\"\\nâ±  Document loading: {load_time:.2f}s\")",
        "        print(f\"â±  Network analysis: {compute_time:.2f}s\")",
        "",
        "        return True",
        "",
        "    def analyze_hierarchy(self):",
        "        \"\"\"Show the hierarchical structure of security knowledge.\"\"\"",
        "        print_header(\"SECURITY KNOWLEDGE HIERARCHY\", \"â•\")",
        "",
        "        print(\"Security concepts organized in hierarchical layers:\\n\")",
        "",
        "        layers = [",
        "            (CorticalLayer.TOKENS, \"Terms\", \"Security terms (authentication, injection, etc.)\"),",
        "            (CorticalLayer.BIGRAMS, \"Patterns\", \"Term pairs (SQL injection, access control, etc.)\"),",
        "            (CorticalLayer.CONCEPTS, \"Categories\", \"Security domains (OWASP, STRIDE, etc.)\"),",
        "            (CorticalLayer.DOCUMENTS, \"Knowledge\", \"Complete security documents\"),",
        "        ]",
        "",
        "        for layer_enum, name, desc in layers:",
        "            layer = self.processor.get_layer(layer_enum)",
        "            count = layer.column_count()",
        "            conns = layer.total_connections()",
        "            print(f\"  Layer {layer_enum.value}: {name}\")",
        "            print(f\"         {count:,} nodes, {conns:,} connections\")",
        "            print(f\"         {desc}\")",
        "            print()",
        "",
        "    def discover_security_concepts(self):",
        "        \"\"\"Show most important security concepts via PageRank.\"\"\"",
        "        print_header(\"KEY SECURITY CONCEPTS (PageRank)\", \"â•\")",
        "",
        "        print(\"Central security concepts - highly connected in the knowledge graph:\")",
        "        print(\"(Terms that bridge multiple security domains)\\n\")",
        "",
        "        layer0 = self.processor.get_layer(CorticalLayer.TOKENS)",
        "",
        "        # Get top tokens by pagerank",
        "        top_tokens = sorted(layer0.minicolumns.values(),",
        "                           key=lambda c: c.pagerank, reverse=True)[:20]",
        "",
        "        if top_tokens:",
        "            max_pr = top_tokens[0].pagerank",
        "            print(\"  Rank  Security Concept    PageRank\")",
        "            print(\"  \" + \"â”€\" * 50)",
        "",
        "            for i, col in enumerate(top_tokens, 1):",
        "                bar = render_bar(col.pagerank, max_pr, 20)",
        "                print(f\"  {i:>3}.  {col.content:<18} {bar} {col.pagerank:.4f}\")",
        "",
        "    def analyze_threat_categories(self):",
        "        \"\"\"Analyze threat categories using TF-IDF.\"\"\"",
        "        print_header(\"THREAT & VULNERABILITY ANALYSIS (TF-IDF)\", \"â•\")",
        "",
        "        print(\"Distinctive security terms - specific to certain threat domains:\")",
        "        print(\"(High TF-IDF = important in specific security contexts)\\n\")",
        "",
        "        layer0 = self.processor.get_layer(CorticalLayer.TOKENS)",
        "",
        "        # Security-relevant terms to highlight",
        "        security_terms = [",
        "            \"injection\", \"authentication\", \"deserialization\", \"pickle\",",
        "            \"vulnerability\", \"exploit\", \"malicious\", \"tampering\",",
        "            \"encryption\", \"credential\", \"token\", \"sanitize\",",
        "            \"whitelist\", \"validation\", \"hmac\", \"signature\"",
        "        ]",
        "",
        "        found_terms = []",
        "        for term in security_terms:",
        "            col = layer0.get_minicolumn(term)",
        "            if col:",
        "                found_terms.append((term, col.tfidf, len(col.document_ids)))",
        "",
        "        if found_terms:",
        "            found_terms.sort(key=lambda x: x[1], reverse=True)",
        "            max_tfidf = found_terms[0][1] if found_terms else 1",
        "",
        "            print(\"  Term               TF-IDF              Documents\")",
        "            print(\"  \" + \"â”€\" * 55)",
        "",
        "            for term, tfidf, doc_count in found_terms[:15]:",
        "                bar = render_bar(tfidf, max_tfidf, 20)",
        "                print(f\"  {term:<18} {bar} {tfidf:.4f}  ({doc_count} docs)\")",
        "",
        "    def find_security_associations(self):",
        "        \"\"\"Show associations between security concepts.\"\"\"",
        "        print_header(\"SECURITY CONCEPT ASSOCIATIONS\", \"â•\")",
        "",
        "        print(\"Semantic connections between security concepts:\")",
        "        print(\"(Terms that co-occur in security contexts)\\n\")",
        "",
        "        layer0 = self.processor.get_layer(CorticalLayer.TOKENS)",
        "",
        "        # Key security concepts to explore",
        "        security_concepts = [\"authentication\", \"injection\", \"encryption\", \"validation\"]",
        "",
        "        for concept in security_concepts:",
        "            col = layer0.get_minicolumn(concept)",
        "            if col and col.lateral_connections:",
        "                print_subheader(f\"ðŸ”— '{concept}' associates with:\")",
        "",
        "                sorted_conns = sorted(col.lateral_connections.items(),",
        "                                     key=lambda x: x[1], reverse=True)[:6]",
        "",
        "                for neighbor_id, weight in sorted_conns:",
        "                    neighbor = layer0.get_by_id(neighbor_id)",
        "                    if neighbor:",
        "                        bar_len = int(min(weight, 10) * 3)",
        "                        bar = \"â”€\" * bar_len + \">\"",
        "                        print(f\"    {bar} {neighbor.content} (weight: {weight:.2f})\")",
        "                print()",
        "",
        "    def demonstrate_vulnerability_queries(self):",
        "        \"\"\"Demonstrate vulnerability and attack pattern queries.\"\"\"",
        "        print_header(\"VULNERABILITY SEARCH\", \"â•\")",
        "",
        "        print(\"Finding information about specific vulnerabilities:\\n\")",
        "",
        "        vulnerability_queries = [",
        "            \"SQL injection prevention\",",
        "            \"pickle deserialization attack\",",
        "            \"cross-site scripting XSS\",",
        "            \"authentication bypass\",",
        "        ]",
        "",
        "        total_query_time = 0",
        "",
        "        for query in vulnerability_queries:",
        "            print_subheader(f\"ðŸ” Query: '{query}'\")",
        "",
        "            start = time.perf_counter()",
        "",
        "            # Show query expansion",
        "            expanded = self.processor.expand_query(query, max_expansions=6)",
        "            original = set(self.processor.tokenizer.tokenize(query))",
        "            new_terms = [t for t in expanded.keys() if t not in original]",
        "",
        "            if new_terms:",
        "                print(f\"    Expanded with: {', '.join(new_terms[:5])}\")",
        "",
        "            # Find relevant documents",
        "            results = self.processor.find_documents_for_query(query, top_n=3)",
        "            elapsed = time.perf_counter() - start",
        "            total_query_time += elapsed",
        "",
        "            print(f\"\\n    Relevant security documents:\")",
        "            for doc_id, score in results:",
        "                print(f\"      ðŸ” {doc_id} (relevance: {score:.3f})\")",
        "            print(f\"    â±  {elapsed*1000:.1f}ms\")",
        "            print()",
        "",
        "        self.timer.times['vuln_queries'] = total_query_time",
        "",
        "    def demonstrate_threat_modeling(self):",
        "        \"\"\"Demonstrate STRIDE threat modeling queries.\"\"\"",
        "        print_header(\"STRIDE THREAT MODELING\", \"â•\")",
        "",
        "        print(\"Searching for STRIDE threat categories:\\n\")",
        "        print(\"  S - Spoofing    | T - Tampering   | R - Repudiation\")",
        "        print(\"  I - Info Disc   | D - Denial Svc  | E - Elev Privilege\\n\")",
        "",
        "        stride_queries = [",
        "            (\"Spoofing\", \"identity spoofing authentication bypass\"),",
        "            (\"Tampering\", \"data tampering integrity modification\"),",
        "            (\"Repudiation\", \"repudiation audit logging non-repudiation\"),",
        "            (\"Information Disclosure\", \"information disclosure data leak exposure\"),",
        "            (\"Denial of Service\", \"denial of service availability rate limiting\"),",
        "            (\"Elevation of Privilege\", \"privilege escalation authorization bypass\"),",
        "        ]",
        "",
        "        for threat_name, query in stride_queries:",
        "            results = self.processor.find_documents_for_query(query, top_n=2)",
        "",
        "            print(f\"  [{threat_name[0]}] {threat_name}:\")",
        "            if results:",
        "                for doc_id, score in results:",
        "                    print(f\"      â†’ {doc_id} ({score:.3f})\")",
        "            else:",
        "                print(f\"      â†’ (no specific coverage)\")",
        "            print()",
        "",
        "    def demonstrate_secure_coding_search(self):",
        "        \"\"\"Demonstrate secure coding practice searches.\"\"\"",
        "        print_header(\"SECURE CODING PRACTICES\", \"â•\")",
        "",
        "        print(\"Retrieving secure coding guidance:\\n\")",
        "",
        "        coding_queries = [",
        "            \"input validation sanitization\",",
        "            \"password hashing bcrypt\",",
        "            \"secure session management\",",
        "            \"parameterized queries prepared statements\",",
        "        ]",
        "",
        "        for query in coding_queries:",
        "            print_subheader(f\"ðŸ›¡ï¸  '{query}'\")",
        "",
        "            # Get passages for detailed guidance",
        "            passages = self.processor.find_passages_for_query(",
        "                query,",
        "                top_n=2,",
        "                chunk_size=250,",
        "                overlap=30",
        "            )",
        "",
        "            if passages:",
        "                for i, (text, doc_id, start, end, score) in enumerate(passages, 1):",
        "                    print(f\"\\n    [{i}] From: {doc_id} (score: {score:.3f})\")",
        "                    print(\"    \" + \"â”€\" * 45)",
        "",
        "                    # Show truncated passage",
        "                    lines = text.strip().split('\\n')[:3]",
        "                    for line in lines:",
        "                        if len(line) > 55:",
        "                            line = line[:52] + \"...\"",
        "                        print(f\"      {line}\")",
        "            print()",
        "",
        "    def demonstrate_compliance_queries(self):",
        "        \"\"\"Demonstrate security compliance and best practice queries.\"\"\"",
        "        print_header(\"SECURITY BEST PRACTICES\", \"â•\")",
        "",
        "        print(\"Finding security best practices and standards:\\n\")",
        "",
        "        # Check for OWASP-related concepts",
        "        print_subheader(\"ðŸ“‹ OWASP Top 10 Coverage\")",
        "",
        "        owasp_categories = [",
        "            (\"A01 - Broken Access Control\", \"access control authorization\"),",
        "            (\"A02 - Cryptographic Failures\", \"encryption cryptography key management\"),",
        "            (\"A03 - Injection\", \"injection SQL command LDAP\"),",
        "            (\"A04 - Insecure Design\", \"threat modeling secure design\"),",
        "            (\"A05 - Security Misconfiguration\", \"configuration hardening default\"),",
        "            (\"A06 - Vulnerable Components\", \"dependency vulnerability supply chain\"),",
        "            (\"A07 - Auth Failures\", \"authentication session management\"),",
        "            (\"A08 - Integrity Failures\", \"deserialization integrity verification\"),",
        "            (\"A09 - Logging Failures\", \"logging monitoring audit\"),",
        "            (\"A10 - SSRF\", \"server-side request forgery SSRF\"),",
        "        ]",
        "",
        "        coverage_count = 0",
        "        for category, query in owasp_categories:",
        "            results = self.processor.find_documents_for_query(query, top_n=1)",
        "            if results and results[0][1] > 0.1:",
        "                coverage_count += 1",
        "                status = \"âœ…\"",
        "                doc = results[0][0][:25]",
        "            else:",
        "                status = \"âš ï¸\"",
        "                doc = \"(limited coverage)\"",
        "            print(f\"    {status} {category}: {doc}\")",
        "",
        "        print(f\"\\n    Coverage: {coverage_count}/10 OWASP categories addressed\")",
        "",
        "    def analyze_security_coverage(self):",
        "        \"\"\"Analyze coverage of security topics.\"\"\"",
        "        print_header(\"SECURITY KNOWLEDGE GAPS\", \"â•\")",
        "",
        "        print(\"Analyzing security topic coverage:\\n\")",
        "",
        "        gaps = self.processor.analyze_knowledge_gaps()",
        "",
        "        print(f\"  Knowledge Coverage: {gaps['coverage_score']:.1%}\")",
        "        print(f\"  Topic Connectivity: {gaps['connectivity_score']:.4f}\")",
        "",
        "        summary = gaps['summary']",
        "        print(f\"\\n  Total documents: {summary['total_documents']}\")",
        "        print(f\"  Well-connected: {summary['well_connected_count']}\")",
        "        print(f\"  Isolated topics: {summary['isolated_count']}\")",
        "",
        "        if gaps['weak_topics']:",
        "            print(\"\\n  ðŸ“ Topics needing more coverage:\")",
        "            for topic in gaps['weak_topics'][:5]:",
        "                print(f\"    â€¢ '{topic['term']}' - only {topic['doc_count']} doc(s)\")",
        "",
        "        # Check for missing security topics",
        "        print_subheader(\"\\nðŸ” Security Topic Audit\")",
        "",
        "        essential_topics = [",
        "            \"authentication\", \"authorization\", \"encryption\", \"injection\",",
        "            \"validation\", \"secrets\", \"vulnerability\", \"threat\",",
        "            \"audit\", \"compliance\", \"firewall\", \"penetration\"",
        "        ]",
        "",
        "        layer0 = self.processor.get_layer(CorticalLayer.TOKENS)",
        "        present = []",
        "        missing = []",
        "",
        "        for topic in essential_topics:",
        "            if layer0.get_minicolumn(topic):",
        "                present.append(topic)",
        "            else:",
        "                missing.append(topic)",
        "",
        "        print(f\"    Present ({len(present)}): {', '.join(present[:8])}...\")",
        "        if missing:",
        "            print(f\"    Consider adding: {', '.join(missing)}\")",
        "",
        "    def print_security_insights(self):",
        "        \"\"\"Print final security insights and summary.\"\"\"",
        "        print_header(\"SECURITY KNOWLEDGE SUMMARY\", \"â•\")",
        "",
        "        layer0 = self.processor.get_layer(CorticalLayer.TOKENS)",
        "        layer1 = self.processor.get_layer(CorticalLayer.BIGRAMS)",
        "        layer3 = self.processor.get_layer(CorticalLayer.DOCUMENTS)",
        "",
        "        total_conns = sum(",
        "            layer.total_connections()",
        "            for layer in self.processor.layers.values()",
        "        )",
        "",
        "        print(\"ðŸ“Š SECURITY CORPUS ANALYSIS\\n\")",
        "",
        "        print(f\"  Security documents:      {len(self.loaded_files)}\")",
        "        print(f\"  Security terms:          {layer0.column_count()}\")",
        "        print(f\"  Term combinations:       {layer1.column_count()}\")",
        "        print(f\"  Semantic connections:    {total_conns:,}\")",
        "",
        "        # Find most central security term",
        "        top_token = max(layer0.minicolumns.values(), key=lambda c: c.pagerank)",
        "        print(f\"\\n  Most central concept: '{top_token.content}'\")",
        "",
        "        # Find most connected document",
        "        if layer3.column_count() > 0:",
        "            top_doc = max(layer3.minicolumns.values(), key=lambda c: c.connection_count())",
        "            print(f\"  Most connected topic: '{top_doc.content}'\")",
        "",
        "        # Performance summary",
        "        print(\"\\nâ±  PERFORMANCE SUMMARY\\n\")",
        "        if 'document_loading' in self.timer.times:",
        "            print(f\"  Corpus loading:      {self.timer.get('document_loading'):.2f}s\")",
        "        if 'compute_all' in self.timer.times:",
        "            print(f\"  Network analysis:    {self.timer.get('compute_all'):.2f}s\")",
        "        if 'vuln_queries' in self.timer.times:",
        "            avg_query = self.timer.get('vuln_queries') / 4 * 1000",
        "            print(f\"  Avg query time:      {avg_query:.1f}ms\")",
        "",
        "        print(\"\\n\" + \"â•\" * 70)",
        "        print(\"Security showcase complete! The system successfully:\")",
        "        print(\"  âœ“ Built security knowledge hierarchy\")",
        "        print(\"  âœ“ Identified key security concepts via PageRank\")",
        "        print(\"  âœ“ Analyzed threat and vulnerability patterns\")",
        "        print(\"  âœ“ Found security concept associations\")",
        "        print(\"  âœ“ Searched vulnerability documentation\")",
        "        print(\"  âœ“ Mapped STRIDE threat categories\")",
        "        print(\"  âœ“ Retrieved secure coding guidance\")",
        "        print(\"  âœ“ Assessed OWASP Top 10 coverage\")",
        "        print(\"  âœ“ Identified security knowledge gaps\")",
        "        print(\"â•\" * 70)",
        "",
        "        print(\"\\nðŸ’¡ USE CASES FOR SECURITY TEAMS:\\n\")",
        "        print(\"  â€¢ Query security knowledge base for threat info\")",
        "        print(\"  â€¢ Find relevant secure coding practices\")",
        "        print(\"  â€¢ Identify gaps in security documentation\")",
        "        print(\"  â€¢ Build RAG systems for security Q&A\")",
        "        print(\"  â€¢ Cross-reference vulnerabilities with mitigations\")",
        "        print(\"  â€¢ Support threat modeling exercises\")",
        "        print()",
        "",
        "",
        "if __name__ == \"__main__\":",
        "    showcase = SecurityShowcase(samples_dir=\"samples\")",
        "    showcase.run()"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    }
  ],
  "hour_of_day": 10,
  "day_of_week": "Sunday",
  "seconds_since_last_commit": -96758,
  "is_merge": true,
  "is_initial": false,
  "parent_count": 2,
  "session_id": null,
  "related_chats": [],
  "ci_result": null,
  "reverted": false,
  "amended": false
}