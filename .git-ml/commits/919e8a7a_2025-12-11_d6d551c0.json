{
  "hash": "919e8a7a45f0353dd3c393e2aa451b997a13616c",
  "message": "Fix cluster_strictness inversion and improve embeddings (Task #122)",
  "author": "Claude",
  "timestamp": "2025-12-11 13:58:02 +0000",
  "branch": "claude/multi-index-design-DvifZ",
  "files_changed": [
    "cortical/analysis.py",
    "cortical/embeddings.py",
    "showcase.py",
    "tests/test_embeddings.py",
    "tests/test_processor.py"
  ],
  "insertions": 199,
  "deletions": 11,
  "hunks": [
    {
      "file": "cortical/analysis.py",
      "function": "def cluster_by_label_propagation(",
      "start_line": 574,
      "lines_added": [
        "    # This means higher strictness → higher threshold → more clusters (topics stay separate)",
        "    change_threshold = cluster_strictness * 0.3",
        "            if current_label in label_weights and cluster_strictness > 0.0:",
        "                # Higher strictness = stronger bias toward current label (resist change)",
        "                label_weights[current_label] *= (1 + cluster_strictness * 2)"
      ],
      "lines_removed": [
        "    change_threshold = (1.0 - cluster_strictness) * 0.3",
        "            if current_label in label_weights and cluster_strictness < 1.0:",
        "                # Lower strictness = stronger bias toward current label",
        "                label_weights[current_label] *= (1 + (1 - cluster_strictness) * 2)"
      ],
      "context_before": [
        "                    for t2 in tokens2[:sample_size]:",
        "                        if t1 != t2:",
        "                            # Add weak bidirectional bridge",
        "                            current = augmented_connections[t1].get(t2, 0)",
        "                            augmented_connections[t1][t2] = current + bridge_weight * 0.5",
        "                            current = augmented_connections[t2].get(t1, 0)",
        "                            augmented_connections[t2][t1] = current + bridge_weight * 0.5",
        "",
        "    # Calculate label change threshold based on strictness",
        "    # Higher strictness = requires stronger evidence to change label"
      ],
      "context_after": [
        "",
        "    for iteration in range(max_iterations):",
        "        changed = False",
        "",
        "        # Process in order (could shuffle for better results)",
        "        for content in columns:",
        "            # Count neighbor labels weighted by connection strength",
        "            label_weights: Dict[int, float] = defaultdict(float)",
        "",
        "            for neighbor_content, weight in augmented_connections[content].items():",
        "                if neighbor_content in labels:",
        "                    label_weights[labels[neighbor_content]] += weight",
        "",
        "            # Apply strictness: current label gets a bonus based on strictness",
        "            current_label = labels[content]",
        "",
        "            # Adopt most common label",
        "            if label_weights:",
        "                best_label, best_weight = max(label_weights.items(), key=lambda x: x[1])",
        "                current_weight = label_weights.get(current_label, 0)",
        "",
        "                # Only change if the improvement exceeds threshold",
        "                if best_label != current_label:",
        "                    if current_weight == 0 or (best_weight / max(current_weight, 0.001) - 1) > change_threshold:",
        "                        labels[content] = best_label"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/embeddings.py",
      "function": "def compute_graph_embeddings(",
      "start_line": 48,
      "lines_added": [
        "def _adjacency_embeddings(",
        "    layer: HierarchicalLayer,",
        "    dimensions: int,",
        "    propagation_steps: int = 2,",
        "    damping: float = 0.5",
        ") -> Dict[str, List[float]]:",
        "    \"\"\"",
        "    Compute embeddings using multi-hop adjacency to landmark nodes.",
        "",
        "    Improves over simple direct adjacency by propagating through the graph,",
        "    which handles sparse graphs better and produces more meaningful embeddings.",
        "",
        "    Args:",
        "        layer: Layer to compute embeddings for",
        "        dimensions: Number of embedding dimensions (= number of landmarks)",
        "        propagation_steps: Number of propagation steps (default 2)",
        "        damping: Weight decay per step (default 0.5)",
        "    \"\"\"",
        "",
        "    landmark_ids = {lm.id: i for i, lm in enumerate(landmarks)}",
        "",
        "    # Build adjacency lookup for efficient propagation",
        "    id_to_col = {col.id: col for col in layer.minicolumns.values()}",
        "",
        "        vec = [0.0] * dimensions",
        "",
        "        # Direct connections (weight = 1.0)",
        "        for lm_id, lm_idx in landmark_ids.items():",
        "            if lm_id in col.lateral_connections:",
        "                vec[lm_idx] += col.lateral_connections[lm_id]",
        "",
        "        # Multi-hop propagation: reach landmarks through neighbors",
        "        current_weight = damping",
        "        frontier = list(col.lateral_connections.items())",
        "        visited = {col.id}",
        "",
        "        for step in range(propagation_steps):",
        "            next_frontier = []",
        "            for neighbor_id, edge_weight in frontier:",
        "                if neighbor_id in visited:",
        "                    continue",
        "                visited.add(neighbor_id)",
        "",
        "                neighbor = id_to_col.get(neighbor_id)",
        "                if not neighbor:",
        "                    continue",
        "",
        "                # Check if this neighbor connects to any landmark",
        "                for lm_id, lm_idx in landmark_ids.items():",
        "                    if lm_id in neighbor.lateral_connections:",
        "                        # Add propagated weight (damped by distance)",
        "                        vec[lm_idx] += edge_weight * neighbor.lateral_connections[lm_id] * current_weight",
        "",
        "                # Add neighbor's neighbors to next frontier",
        "                for next_id, next_weight in neighbor.lateral_connections.items():",
        "                    if next_id not in visited:",
        "                        next_frontier.append((next_id, edge_weight * next_weight * current_weight))",
        "",
        "            frontier = next_frontier",
        "            current_weight *= damping",
        "",
        "        # Normalize",
        ""
      ],
      "lines_removed": [
        "def _adjacency_embeddings(layer: HierarchicalLayer, dimensions: int) -> Dict[str, List[float]]:",
        "    \"\"\"Compute embeddings using adjacency to landmark nodes.\"\"\"",
        "    ",
        "    ",
        "        vec = [col.lateral_connections.get(lm.id, 0) for lm in landmarks]",
        "    "
      ],
      "context_before": [
        "    ",
        "    stats = {",
        "        'method': method,",
        "        'dimensions': dimensions,",
        "        'terms_embedded': len(embeddings)",
        "    }",
        "    ",
        "    return embeddings, stats",
        "",
        ""
      ],
      "context_after": [
        "    embeddings: Dict[str, List[float]] = {}",
        "    sorted_cols = sorted(layer.minicolumns.values(), key=lambda c: c.pagerank, reverse=True)",
        "    landmarks = sorted_cols[:dimensions]",
        "    for col in layer.minicolumns.values():",
        "        mag = math.sqrt(sum(v*v for v in vec)) + 1e-10",
        "        embeddings[col.content] = [v / mag for v in vec]",
        "    return embeddings",
        "",
        "",
        "def _random_walk_embeddings(",
        "    layer: HierarchicalLayer,",
        "    dimensions: int,",
        "    walks_per_node: int = 10,",
        "    walk_length: int = 40,",
        "    window_size: int = 5",
        ") -> Dict[str, List[float]]:"
      ],
      "change_type": "modify"
    },
    {
      "file": "showcase.py",
      "function": "class CorticalShowcase:",
      "start_line": 604,
      "lines_added": [
        "            dimensions=32, method='random_walk', verbose=False"
      ],
      "lines_removed": [
        "            dimensions=32, method='adjacency', verbose=False"
      ],
      "context_before": [
        "            for topic in gaps['weak_topics'][:5]:",
        "                print(f\"    • '{topic['term']}' - only {topic['doc_count']} doc(s)\")",
        "    ",
        "    def demonstrate_embeddings(self):",
        "        \"\"\"Show embedding-based similarity.\"\"\"",
        "        print_header(\"GRAPH EMBEDDINGS\", \"═\")",
        "        ",
        "        print(\"Computing embeddings from graph structure...\\n\")",
        "        ",
        "        stats = self.processor.compute_graph_embeddings("
      ],
      "context_after": [
        "        )",
        "        print(f\"  Created {stats['terms_embedded']} term embeddings\")",
        "        ",
        "        # Find similar terms",
        "        test_terms = [\"neural\", \"learning\", \"data\"]",
        "        ",
        "        for term in test_terms:",
        "            similar = self.processor.find_similar_by_embedding(term, top_n=5)",
        "            if similar:",
        "                print(f\"\\n  Terms similar to '{term}':\")"
      ],
      "change_type": "modify"
    },
    {
      "file": "tests/test_embeddings.py",
      "function": "class TestEmbeddingsEmptyLayer(unittest.TestCase):",
      "start_line": 188,
      "lines_added": [
        "class TestEmbeddingSemanticQuality(unittest.TestCase):",
        "    \"\"\"Regression tests for embedding semantic quality (Task #122).\"\"\"",
        "",
        "    @classmethod",
        "    def setUpClass(cls):",
        "        \"\"\"Set up processor with semantically distinct documents.\"\"\"",
        "        cls.processor = CorticalTextProcessor()",
        "        # Machine learning documents",
        "        cls.processor.process_document(\"ml1\", \"\"\"",
        "            Neural networks process information through multiple layers.",
        "            Deep learning enables automatic feature extraction.",
        "            Training neural networks requires gradient descent optimization.",
        "        \"\"\")",
        "        cls.processor.process_document(\"ml2\", \"\"\"",
        "            Machine learning algorithms learn patterns from data.",
        "            Neural networks are inspired by biological neurons.",
        "            Deep learning models use backpropagation for training.",
        "        \"\"\")",
        "        # Cooking documents (semantically different)",
        "        cls.processor.process_document(\"cook1\", \"\"\"",
        "            Bread baking requires yeast and flour for fermentation.",
        "            Sourdough bread has a tangy flavor from natural fermentation.",
        "        \"\"\")",
        "        cls.processor.process_document(\"cook2\", \"\"\"",
        "            Pasta is made from durum wheat semolina and water.",
        "            Italian cuisine features many regional pasta variations.",
        "        \"\"\")",
        "        cls.processor.compute_all(verbose=False)",
        "",
        "    def test_random_walk_semantic_similarity(self):",
        "        \"\"\"Test that random_walk embeddings capture semantic relationships.",
        "",
        "        Regression test: 'neural' should be more similar to 'networks'",
        "        than to unrelated words like 'bread'.",
        "        \"\"\"",
        "        embeddings, _ = compute_graph_embeddings(",
        "            self.processor.layers,",
        "            dimensions=16,",
        "            method='random_walk'",
        "        )",
        "",
        "        if 'neural' in embeddings and 'networks' in embeddings:",
        "            neural_networks_sim = embedding_similarity(embeddings, 'neural', 'networks')",
        "",
        "            # Check against unrelated terms",
        "            for unrelated in ['bread', 'pasta', 'yeast', 'flour']:",
        "                if unrelated in embeddings:",
        "                    neural_unrelated_sim = embedding_similarity(embeddings, 'neural', unrelated)",
        "                    # Neural should be more similar to networks than to cooking terms",
        "                    self.assertGreater(",
        "                        neural_networks_sim, neural_unrelated_sim,",
        "                        f\"'neural' should be more similar to 'networks' ({neural_networks_sim:.3f}) \"",
        "                        f\"than to '{unrelated}' ({neural_unrelated_sim:.3f})\"",
        "                    )",
        "",
        "    def test_adjacency_produces_nonzero_embeddings(self):",
        "        \"\"\"Test that adjacency method produces meaningful (non-sparse) embeddings.",
        "",
        "        Regression test: After multi-hop propagation fix, adjacency embeddings",
        "        should have multiple non-zero dimensions.",
        "        \"\"\"",
        "        import math",
        "",
        "        embeddings, _ = compute_graph_embeddings(",
        "            self.processor.layers,",
        "            dimensions=16,",
        "            method='adjacency'",
        "        )",
        "",
        "        # Check that embeddings have multiple non-zero dimensions",
        "        for term, vec in list(embeddings.items())[:10]:",
        "            nonzero_dims = sum(1 for v in vec if abs(v) > 1e-6)",
        "            # With multi-hop propagation, should have more than just 1-2 non-zero dims",
        "            self.assertGreater(",
        "                nonzero_dims, 0,",
        "                f\"Term '{term}' has all-zero embedding\"",
        "            )",
        "",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "        processor = CorticalTextProcessor()",
        "        embeddings, stats = compute_graph_embeddings(",
        "            processor.layers,",
        "            dimensions=16,",
        "            method='adjacency'",
        "        )",
        "        self.assertEqual(len(embeddings), 0)",
        "        self.assertEqual(stats['terms_embedded'], 0)",
        "",
        ""
      ],
      "context_after": [
        "if __name__ == \"__main__\":",
        "    unittest.main(verbosity=2)"
      ],
      "change_type": "add"
    },
    {
      "file": "tests/test_processor.py",
      "function": "class TestConceptClustering(unittest.TestCase):",
      "start_line": 1384,
      "lines_added": [
        "    def test_cluster_strictness_direction(self):",
        "        \"\"\"Regression test: Higher strictness should create MORE clusters.",
        "",
        "        Task #122 fix: The cluster_strictness logic was inverted.",
        "        This test ensures the correct behavior:",
        "        - strictness=1.0 (strict) → MORE clusters (topics stay separate)",
        "        - strictness=0.0 (loose) → FEWER clusters (topics merge)",
        "        \"\"\"",
        "        processor = CorticalTextProcessor()",
        "        # Create multiple distinct topics",
        "        processor.process_document(\"ml1\", \"\"\"",
        "            Neural networks process information through layers.",
        "            Deep learning enables pattern recognition in data.",
        "            Training neural networks requires gradient descent.",
        "        \"\"\")",
        "        processor.process_document(\"ml2\", \"\"\"",
        "            Machine learning algorithms learn from training data.",
        "            Neural networks are inspired by biological neurons.",
        "        \"\"\")",
        "        processor.process_document(\"cook1\", \"\"\"",
        "            Bread baking requires yeast and flour for fermentation.",
        "            Sourdough bread develops complex flavors over time.",
        "        \"\"\")",
        "        processor.process_document(\"cook2\", \"\"\"",
        "            Pasta is made from durum wheat semolina and water.",
        "            Italian cuisine features many regional pasta dishes.",
        "        \"\"\")",
        "        processor.compute_importance(verbose=False)",
        "        processor.compute_tfidf(verbose=False)",
        "",
        "        # Strict clustering should create more clusters",
        "        clusters_strict = processor.build_concept_clusters(",
        "            cluster_strictness=1.0, verbose=False",
        "        )",
        "        count_strict = len(clusters_strict)",
        "",
        "        # Reset concepts layer",
        "        processor.layers[CorticalLayer.CONCEPTS] = HierarchicalLayer(CorticalLayer.CONCEPTS)",
        "",
        "        # Loose clustering should create fewer clusters",
        "        clusters_loose = processor.build_concept_clusters(",
        "            cluster_strictness=0.0, verbose=False",
        "        )",
        "        count_loose = len(clusters_loose)",
        "",
        "        # Strict should have >= loose clusters (topics stay separate vs merge)",
        "        self.assertGreaterEqual(",
        "            count_strict, count_loose,",
        "            f\"Strict clustering ({count_strict}) should create >= clusters than loose ({count_loose})\"",
        "        )",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "",
        "        # Loose clustering",
        "        clusters_loose = processor.build_concept_clusters(",
        "            cluster_strictness=0.3, verbose=False",
        "        )",
        "",
        "        # Both should return valid cluster dictionaries",
        "        self.assertIsInstance(clusters_strict, dict)",
        "        self.assertIsInstance(clusters_loose, dict)",
        ""
      ],
      "context_after": [
        "    def test_bridge_weight_parameter(self):",
        "        \"\"\"Test that bridge_weight enables cross-document connections.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(",
        "            \"doc1\", \"Neural networks learn patterns from data.\"",
        "        )",
        "        processor.process_document(",
        "            \"doc2\", \"Bread baking requires yeast and flour.\"",
        "        )",
        "        processor.compute_importance(verbose=False)"
      ],
      "change_type": "add"
    }
  ],
  "hour_of_day": 13,
  "day_of_week": "Thursday",
  "seconds_since_last_commit": -344806,
  "is_merge": false,
  "is_initial": false,
  "parent_count": 1,
  "session_id": null,
  "related_chats": [],
  "ci_result": null,
  "reverted": false,
  "amended": false
}