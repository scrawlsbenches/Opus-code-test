{
  "hash": "87f53e76be6c96c985fa66f2f4053421b126db61",
  "message": "Merge pull request #23 from scrawlsbenches/claude/incremental-codebase-indexing-01FXTSSDbweigs6bjjMVS9VG",
  "author": "scrawlsbenches",
  "timestamp": "2025-12-10 18:18:10 -0500",
  "branch": "claude/multi-index-design-DvifZ",
  "files_changed": [
    ".claude/skills/corpus-indexer/SKILL.md",
    ".gitignore",
    "CLAUDE.md",
    "TASK_LIST.md",
    "corpus_chunks/2025-12-10_23-06-01_b408da4e.json",
    "cortical/chunk_index.py",
    "cortical/layers.py",
    "cortical/processor.py",
    "docs/claude-usage.md",
    "docs/cookbook.md",
    "docs/dogfooding.md",
    "docs/query-guide.md",
    "scripts/index_codebase.py",
    "tests/test_chunk_indexing.py",
    "tests/test_incremental_indexing.py"
  ],
  "insertions": 5916,
  "deletions": 65,
  "hunks": [
    {
      "file": ".claude/skills/corpus-indexer/SKILL.md",
      "function": "allowed-tools: Bash",
      "start_line": 7,
      "lines_added": [
        "## Quick Commands",
        "# Fast incremental update (only changed files, ~2s)",
        "python scripts/index_codebase.py --incremental",
        "",
        "# Check what would be indexed without doing it",
        "python scripts/index_codebase.py --status",
        "",
        "# Full rebuild (still fast, ~2s)",
        "",
        "# Force full rebuild even if no changes detected",
        "python scripts/index_codebase.py --force",
        "## Options",
        "| Option | Description |",
        "|--------|-------------|",
        "| `--incremental`, `-i` | Only re-index changed files (fastest) |",
        "| `--status`, `-s` | Show what would change without indexing |",
        "| `--force`, `-f` | Force full rebuild even if up-to-date |",
        "| `--verbose`, `-v` | Show per-file progress |",
        "| `--log FILE`, `-l FILE` | Write detailed log to file |",
        "| `--timeout N`, `-t N` | Timeout in seconds (default: 300) |",
        "| `--full-analysis` | Use complete semantic analysis (slower) |",
        "| `--output FILE`, `-o FILE` | Custom output path (default: corpus_dev.pkl) |",
        "## Examples",
        "# Standard incremental update",
        "python scripts/index_codebase.py --incremental",
        "",
        "# Verbose with logging (good for debugging)",
        "python scripts/index_codebase.py --verbose --log index.log",
        "# See what files changed",
        "python scripts/index_codebase.py --status",
        "# Full semantic analysis (takes longer, more accurate)",
        "python scripts/index_codebase.py --full-analysis",
        "- Intelligence docs in `docs/` directory",
        "- Concept clusters (in full-analysis mode)",
        "- Semantic relations (in full-analysis mode)",
        "",
        "## Manifest File",
        "",
        "The indexer creates `corpus_dev.manifest.json` to track file modification times.",
        "This enables fast incremental updates by detecting only changed files.",
        "",
        "## Performance",
        "",
        "| Mode | Time | Use Case |",
        "|------|------|----------|",
        "| Incremental | ~1-2s | After small edits |",
        "| Full rebuild (fast) | ~2-3s | Default mode |",
        "| Full analysis | ~10+ min | Complete semantic analysis |",
        "- Use `--incremental` for quick updates during development",
        "- Use `--status` to check if re-indexing is needed",
        "- Use `--force` after major refactoring",
        "- Use `--full-analysis` before deep exploration sessions"
      ],
      "lines_removed": [
        "## Index the Codebase",
        "### Options",
        "- `--output FILE` or `-o FILE`: Custom output path (default: corpus_dev.pkl)",
        "- `--verbose` or `-v`: Show detailed indexing progress",
        "### Example",
        "# Standard indexing",
        "python scripts/index_codebase.py",
        "# Verbose output to see what's being indexed",
        "python scripts/index_codebase.py --verbose",
        "# Custom output location",
        "python scripts/index_codebase.py --output my_corpus.pkl",
        "- Concept clusters",
        "- Semantic relations extracted",
        "Re-index periodically to keep search accurate:",
        "- After adding new modules",
        "- After major refactoring",
        "- Before deep codebase exploration sessions"
      ],
      "context_before": [
        "",
        "This skill manages the codebase index used by the semantic search system.",
        "",
        "## When to Use",
        "",
        "- After adding new files to the codebase",
        "- After significant code changes",
        "- When search results seem outdated",
        "- To verify indexing statistics",
        ""
      ],
      "context_after": [
        "",
        "```bash",
        "python scripts/index_codebase.py",
        "```",
        "",
        "",
        "",
        "",
        "```bash",
        "",
        "",
        "```",
        "",
        "## What Gets Indexed",
        "",
        "The indexer processes:",
        "- All Python files in `cortical/` (source code)",
        "- All Python files in `tests/` (test code)",
        "- Documentation: `CLAUDE.md`, `TASK_LIST.md`, `README.md`, `KNOWLEDGE_TRANSFER.md`",
        "",
        "## Output Statistics",
        "",
        "After indexing, you'll see:",
        "- Number of documents indexed",
        "- Total lines of code",
        "- Token count (unique terms)",
        "- Bigram count (word pairs)",
        "",
        "## Maintenance",
        ""
      ],
      "change_type": "modify"
    },
    {
      "file": ".gitignore",
      "function": "__pycache__/",
      "start_line": 6,
      "lines_added": [
        "*.manifest.json",
        "*.pkl.hash",
        "",
        "# Indexer logs",
        "index.log",
        "*.log"
      ],
      "lines_removed": [],
      "context_before": [
        ".Python",
        "*.egg-info/",
        ".eggs/",
        "dist/",
        "build/",
        ".pytest_cache/",
        "",
        "# Generated corpus files",
        "corpus_dev.pkl",
        "*.pkl"
      ],
      "context_after": [
        "",
        "# Coverage",
        ".coverage",
        ".coverage.*",
        "coverage.xml",
        "htmlcov/",
        "",
        "## A streamlined .gitignore for modern .NET projects",
        "## including temporary files, build results, and",
        "## files generated by popular .NET tools. If you are"
      ],
      "change_type": "add"
    },
    {
      "file": "CLAUDE.md",
      "function": "for t1, rel, t2, weight in processor.semantic_relations[:10]:",
      "start_line": 347,
      "lines_added": [
        "# Index the codebase (creates corpus_dev.pkl, ~2s)",
        "# Incremental update (only changed files)",
        "python scripts/index_codebase.py --incremental",
        "",
        "### Indexer Options",
        "",
        "| Option | Description |",
        "|--------|-------------|",
        "| `--incremental`, `-i` | Only re-index changed files (fastest) |",
        "| `--status`, `-s` | Show what would change without indexing |",
        "| `--force`, `-f` | Force full rebuild |",
        "| `--log FILE` | Write detailed log to file |",
        "| `--verbose`, `-v` | Show per-file progress |",
        ""
      ],
      "lines_removed": [
        "# Index the codebase (creates corpus_dev.pkl)"
      ],
      "context_before": [
        "",
        "---",
        "",
        "## Dog-Fooding: Search the Codebase",
        "",
        "The Cortical Text Processor can index and search its own codebase, providing semantic search capabilities during development.",
        "",
        "### Quick Start",
        "",
        "```bash"
      ],
      "context_after": [
        "python scripts/index_codebase.py",
        "",
        "# Search for code",
        "python scripts/search_codebase.py \"PageRank algorithm\"",
        "python scripts/search_codebase.py \"bigram separator\" --verbose",
        "python scripts/search_codebase.py --interactive",
        "```",
        "",
        "### Claude Skills",
        "",
        "Two skills are available in `.claude/skills/`:",
        "",
        "1. **codebase-search**: Search the indexed codebase for code patterns and implementations",
        "2. **corpus-indexer**: Re-index the codebase after making changes",
        "",
        "### Search Options",
        "",
        "| Option | Description |",
        "|--------|-------------|",
        "| `--top N` | Number of results (default: 5) |",
        "| `--verbose` | Show full passage text |",
        "| `--expand` | Show query expansion terms |",
        "| `--interactive` | Interactive search mode |",
        "",
        "### Interactive Mode Commands"
      ],
      "change_type": "modify"
    },
    {
      "file": "TASK_LIST.md",
      "function": "Currently these are always 0 due to the bug.",
      "start_line": 1659,
      "lines_added": [
        "**Total Tests:** 593 (all passing)",
        "",
        "---",
        "",
        "### 57. Add Incremental Codebase Indexing",
        "",
        "**Files:** `scripts/index_codebase.py`, `cortical/processor.py`, `cortical/layers.py`, `tests/test_incremental_indexing.py`",
        "**Status:** [x] Completed (2025-12-10)",
        "**Priority:** High",
        "",
        "**Problem:**",
        "The codebase indexer had to rebuild the entire corpus on every run, even for small changes. This was slow and inefficient for iterative development.",
        "",
        "**Solution Applied:**",
        "1. Added manifest file (`corpus_dev.manifest.json`) to track file modification times",
        "2. Added `--incremental` flag to only re-index changed files",
        "3. Added `--status` flag to show what would change without indexing",
        "4. Added `--force` flag to force full rebuild",
        "5. Added `remove_document()` and `remove_documents_batch()` methods to processor",
        "6. Added `remove_minicolumn()` method to HierarchicalLayer",
        "7. Added robust progress tracking with `ProgressTracker` class",
        "8. Added phase timing and logging support (`--log FILE`)",
        "9. Added timeout support (`--timeout N`)",
        "10. Added fast mode (default) that skips slow bigram connections",
        "",
        "**Performance Fix:**",
        "Identified that `compute_bigram_connections()` has O(n²) complexity with large corpora (26,000+ bigrams), causing hangs. Fast mode skips this operation:",
        "- Before: >10 minutes (hung)",
        "- After: ~2.3 seconds",
        "",
        "**Files Modified:**",
        "- `scripts/index_codebase.py` - Complete rewrite with incremental support (~840 lines)",
        "- `cortical/processor.py` - Added `remove_document()`, `remove_documents_batch()` (~160 lines)",
        "- `cortical/layers.py` - Added `remove_minicolumn()` (~20 lines)",
        "- `tests/test_incremental_indexing.py` - 47 comprehensive tests",
        "- `.claude/skills/corpus-indexer/SKILL.md` - Updated documentation",
        "- `CLAUDE.md` - Updated Dog-Fooding section",
        "",
        "**Usage:**",
        "```bash",
        "# Fast incremental update (~1-2s)",
        "python scripts/index_codebase.py --incremental",
        "",
        "# Check what would change",
        "python scripts/index_codebase.py --status",
        "",
        "# Full rebuild with logging",
        "python scripts/index_codebase.py --force --log index.log",
        "",
        "# With timeout safeguard",
        "python scripts/index_codebase.py --timeout 60",
        "```"
      ],
      "lines_removed": [
        "**Total Tests:** 546 (all passing)"
      ],
      "context_before": [
        "| 43 | Medium | Optimize chunk scoring performance | ✅ Completed | Performance |",
        "| 44 | Low | Remove deprecated feedforward_sources | [ ] Not Started | Cleanup |",
        "| 45 | Medium | Add LRU cache for query results | ✅ Completed | Performance |",
        "| 46 | Low | Standardize return types with dataclasses | [ ] Not Started | API |",
        "",
        "**Completed:** 9/13 tasks",
        "**High Priority Remaining:** 0 tasks",
        "**Medium Priority Remaining:** 1 task (#41)",
        "**Low Priority Remaining:** 3 tasks (#42, #44, #46)",
        ""
      ],
      "context_after": [
        "",
        "---",
        "",
        "## Intent-Based Code Search Enhancements",
        "",
        "The following tasks enhance the system's ability to understand developer intent and retrieve code by meaning rather than exact keyword matching.",
        "",
        "---",
        "",
        "### 48. Add Code-Aware Tokenization"
      ],
      "change_type": "modify"
    },
    {
      "file": "TASK_LIST.md",
      "function": "Document common usage patterns and code examples that help answer \"how do I...\"",
      "start_line": 1883,
      "lines_added": [
        "### 58. Git-Compatible Chunk-Based Indexing",
        "",
        "**Files:** `scripts/index_codebase.py`, `cortical/chunk_index.py` (new), `tests/test_chunk_indexing.py` (new)",
        "**Status:** [ ] In Progress",
        "**Priority:** High",
        "",
        "**Problem:**",
        "The current pkl-based index cannot be tracked in git (binary, merge conflicts). This prevents sharing indexed state across branches and team members, requiring full rebuilds.",
        "",
        "**Solution:**",
        "Implement append-only, time-stamped JSON chunks that can be safely committed to git and merged without conflicts.",
        "",
        "**Architecture:**",
        "```",
        "corpus_chunks/                        # Tracked in git",
        "├── 2025-12-10_21-53-45_a1b2.json    # Session 1 changes",
        "├── 2025-12-10_22-15-30_c3d4.json    # Session 2 changes",
        "└── 2025-12-10_23-00-00_e5f6.json    # Session 3 changes",
        "",
        "corpus_dev.pkl                        # NOT tracked (local cache)",
        "```",
        "",
        "**Chunk Format:**",
        "```json",
        "{",
        "  \"timestamp\": \"2025-12-10T21:53:45\",",
        "  \"session_id\": \"a1b2c3d4\",",
        "  \"branch\": \"feature-x\",",
        "  \"operations\": [",
        "    {\"op\": \"add\", \"doc_id\": \"docs/new.md\", \"content\": \"...\", \"mtime\": 1234567890},",
        "    {\"op\": \"modify\", \"doc_id\": \"query.py\", \"content\": \"...\", \"mtime\": 1234567891},",
        "    {\"op\": \"delete\", \"doc_id\": \"old.md\"}",
        "  ]",
        "}",
        "```",
        "",
        "**Implementation Tasks:**",
        "1. [ ] Create `ChunkWriter` class - save session changes as timestamped JSON",
        "2. [ ] Create `ChunkLoader` class - combine chunks on startup (later timestamps win)",
        "3. [ ] Add cache validator - check if pkl matches combined chunk hash",
        "4. [ ] Add `--compact` command - merge old chunks into single file",
        "5. [ ] Update CLI with `--use-chunks` flag",
        "6. [ ] Handle deletions with tombstones",
        "7. [ ] Add `.gitignore` entry for `corpus_dev.pkl` (keep chunks tracked)",
        "8. [ ] Add comprehensive tests",
        "",
        "**Startup Flow:**",
        "```",
        "1. Load all chunk files (sorted by timestamp)",
        "2. Replay operations → build document set",
        "   - Later timestamps win for conflicts",
        "   - Deletes remove documents",
        "3. Check if pkl cache is valid (hash of combined docs)",
        "   - Valid: load pkl (fast)",
        "   - Invalid: recompute analysis (~2s)",
        "```",
        "",
        "**Benefits:**",
        "- No merge conflicts (unique timestamp+session names)",
        "- Shared indexed state across team/branches",
        "- Fast startup when cache valid",
        "- Git-friendly (small JSON, append-only)",
        "- Periodic compaction like `git gc`",
        "",
        "**Usage (planned):**",
        "```bash",
        "# Index with chunks (creates timestamped JSON)",
        "python scripts/index_codebase.py --incremental --use-chunks",
        "",
        "# Compact old chunks",
        "python scripts/index_codebase.py --compact --before 2025-12-01",
        "",
        "# Status including chunk info",
        "python scripts/index_codebase.py --status --use-chunks",
        "```",
        "",
        "---",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "**Patterns:**",
        "- Basic document processing workflow",
        "- RAG retrieval with passages",
        "- Code search with intent parsing",
        "- Fingerprint comparison for similarity",
        "- Batch operations for performance",
        "- Incremental updates",
        "",
        "---",
        ""
      ],
      "context_after": [
        "*Updated 2025-12-10*"
      ],
      "change_type": "add"
    },
    {
      "file": "corpus_chunks/2025-12-10_23-06-01_b408da4e.json",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "{",
        "  \"version\": 1,",
        "  \"timestamp\": \"2025-12-10T23:06:01\",",
        "  \"session_id\": \"b408da4e858b4eeb\",",
        "  \"branch\": \"claude/incremental-codebase-indexing-01FXTSSDbweigs6bjjMVS9VG\",",
        "  \"operations\": [",
        "    {",
        "      \"op\": \"add\",",
        "      \"doc_id\": \"tests/test_persistence.py\",",
        "      \"content\": \"\\\"\\\"\\\"Tests for the persistence module.\\\"\\\"\\\"\\n\\nimport unittest\\nimport tempfile\\nimport os\\nimport json\\nimport sys\\nsys.path.insert(0, '..')\\n\\nfrom cortical import CorticalTextProcessor, CorticalLayer\\nfrom cortical.persistence import (\\n    save_processor,\\n    load_processor,\\n    export_graph_json,\\n    export_embeddings_json,\\n    get_state_summary,\\n    export_conceptnet_json,\\n    LAYER_COLORS,\\n    LAYER_NAMES\\n)\\nfrom cortical.embeddings import compute_graph_embeddings\\n\\n\\nclass TestSaveLoad(unittest.TestCase):\\n    \\\"\\\"\\\"Test save and load functionality.\\\"\\\"\\\"\\n\\n    def test_save_and_load(self):\\n        \\\"\\\"\\\"Test saving and loading processor state.\\\"\\\"\\\"\\n        processor = CorticalTextProcessor()\\n        processor.process_document(\\\"doc1\\\", \\\"Neural networks process information.\\\")\\n        processor.process_document(\\\"doc2\\\", \\\"Machine learning algorithms learn.\\\")\\n        processor.compute_all(verbose=False)\\n\\n        with tempfile.TemporaryDirectory() as tmpdir:\\n            filepath = os.path.join(tmpdir, \\\"test.pkl\\\")\\n            save_processor(\\n                filepath, processor.layers, processor.documents,\\n                processor.document_metadata, processor.embeddings,\\n                processor.semantic_relations, verbose=False\\n            )\\n\\n            result = load_processor(filepath, verbose=False)\\n            layers, documents, document_metadata, embeddings, semantic_relations, metadata = result\\n\\n            self.assertEqual(len(documents), 2)\\n            self.assertIn(\\\"doc1\\\", documents)\\n            self.assertIn(\\\"doc2\\\", documents)\\n\\n            # Check layers were restored\\n            layer0 = layers[CorticalLayer.TOKENS]\\n            self.assertGreater(len(layer0.minicolumns), 0)\\n\\n    def test_save_load_preserves_id_index(self):\\n        \\\"\\\"\\\"Test that save/load preserves the ID index.\\\"\\\"\\\"\\n        processor = CorticalTextProcessor()\\n        processor.process_document(\\\"doc1\\\", \\\"neural networks deep learning\\\")\\n        processor.compute_all(verbose=False)\\n\\n        with tempfile.TemporaryDirectory() as tmpdir:\\n            filepath = os.path.join(tmpdir, \\\"test.pkl\\\")\\n            save_processor(\\n                filepath, processor.layers, processor.documents,\\n                processor.document_metadata, processor.embeddings,\\n                processor.semantic_relations, verbose=False\\n            )\\n\\n            result = load_processor(filepath, verbose=False)\\n            layers, documents, document_metadata, embeddings, semantic_relations, metadata = result\\n\\n            layer0 = layers[CorticalLayer.TOKENS]\\n            neural = layer0.get_minicolumn(\\\"neural\\\")\\n\\n            # get_by_id should work after load\\n            retrieved = layer0.get_by_id(neural.id)\\n            self.assertEqual(retrieved.content, \\\"neural\\\")\\n\\n    def test_save_load_preserves_doc_occurrence_counts(self):\\n        \\\"\\\"\\\"Test that save/load preserves doc_occurrence_counts.\\\"\\\"\\\"\\n        processor = CorticalTextProcessor()\\n        processor.process_document(\\\"doc1\\\", \\\"neural neural neural\\\")  # 3 times\\n        processor.process_document(\\\"doc2\\\", \\\"neural\\\")  # 1 time\\n        processor.compute_all(verbose=False)\\n\\n        with tempfile.TemporaryDirectory() as tmpdir:\\n            filepath = os.path.join(tmpdir, \\\"test.pkl\\\")\\n            save_processor(\\n                filepath, processor.layers, processor.documents,\\n                processor.document_metadata, processor.embeddings,\\n                processor.semantic_relations, verbose=False\\n            )\\n\\n            result = load_processor(filepath, verbose=False)\\n            layers, documents, document_metadata, embeddings, semantic_relations, metadata = result\\n\\n            layer0 = layers[CorticalLayer.TOKENS]\\n            neural = layer0.get_minicolumn(\\\"neural\\\")\\n\\n            self.assertEqual(neural.doc_occurrence_counts.get(\\\"doc1\\\"), 3)\\n            self.assertEqual(neural.doc_occurrence_counts.get(\\\"doc2\\\"), 1)\\n\\n    def test_save_load_empty_processor(self):\\n        \\\"\\\"\\\"Test saving and loading empty processor.\\\"\\\"\\\"\\n        processor = CorticalTextProcessor()\\n\\n        with tempfile.TemporaryDirectory() as tmpdir:\\n            filepath = os.path.join(tmpdir, \\\"test.pkl\\\")\\n            save_processor(\\n                filepath, processor.layers, processor.documents,\\n                processor.document_metadata, processor.embeddings,\\n                processor.semantic_relations, verbose=False\\n            )\\n\\n            result = load_processor(filepath, verbose=False)\\n            layers, documents, document_metadata, embeddings, semantic_relations, metadata = result\\n\\n            self.assertEqual(len(documents), 0)\\n\\n    def test_save_load_preserves_document_metadata(self):\\n        \\\"\\\"\\\"Test that save/load preserves document metadata.\\\"\\\"\\\"\\n        processor = CorticalTextProcessor()\\n        processor.process_document(\\n            \\\"doc1\\\", \\\"Neural networks process information.\\\",\\n            metadata={\\\"source\\\": \\\"https://example.com\\\", \\\"author\\\": \\\"Test\\\"}\\n        )\\n        processor.compute_all(verbose=False)\\n\\n        with tempfile.TemporaryDirectory() as tmpdir:\\n            filepath = os.path.join(tmpdir, \\\"test.pkl\\\")\\n            save_processor(\\n                filepath, processor.layers, processor.documents,\\n                processor.document_metadata, processor.embeddings,\\n                processor.semantic_relations, verbose=False\\n            )\\n\\n            result = load_processor(filepath, verbose=False)\\n            layers, documents, document_metadata, embeddings, semantic_relations, metadata = result\\n\\n            self.assertEqual(document_metadata[\\\"doc1\\\"][\\\"source\\\"], \\\"https://example.com\\\")\\n            self.assertEqual(document_metadata[\\\"doc1\\\"][\\\"author\\\"], \\\"Test\\\")\\n\\n    def test_save_load_preserves_embeddings(self):\\n        \\\"\\\"\\\"Test that save/load preserves graph embeddings.\\\"\\\"\\\"\\n        processor = CorticalTextProcessor()\\n        processor.process_document(\\\"doc1\\\", \\\"Neural networks process information.\\\")\\n        processor.compute_all(verbose=False)\\n        processor.compute_graph_embeddings(dimensions=16, verbose=False)\\n\\n        with tempfile.TemporaryDirectory() as tmpdir:\\n            filepath = os.path.join(tmpdir, \\\"test.pkl\\\")\\n            processor.save(filepath, verbose=False)\\n\\n            loaded = CorticalTextProcessor.load(filepath, verbose=False)\\n\\n            self.assertEqual(len(loaded.embeddings), len(processor.embeddings))\\n            # Check a specific embedding is preserved\\n            for term in processor.embeddings:\\n                self.assertIn(term, loaded.embeddings)\\n                self.assertEqual(processor.embeddings[term], loaded.embeddings[term])\\n\\n    def test_save_load_preserves_semantic_relations(self):\\n        \\\"\\\"\\\"Test that save/load preserves semantic relations.\\\"\\\"\\\"\\n        processor = CorticalTextProcessor()\\n        processor.process_document(\\\"doc1\\\", \\\"Neural networks are computational models.\\\")\\n        processor.process_document(\\\"doc2\\\", \\\"Deep learning uses neural networks.\\\")\\n        processor.compute_all(verbose=False)\\n        processor.extract_corpus_semantics(verbose=False)\\n\\n        with tempfile.TemporaryDirectory() as tmpdir:\\n            filepath = os.path.join(tmpdir, \\\"test.pkl\\\")\\n            processor.save(filepath, verbose=False)\\n\\n            loaded = CorticalTextProcessor.load(filepath, verbose=False)\\n\\n            self.assertEqual(len(loaded.semantic_relations), len(processor.semantic_relations))\\n\\n\\nclass TestExportGraphJSON(unittest.TestCase):\\n    \\\"\\\"\\\"Test graph JSON export.\\\"\\\"\\\"\\n\\n    def test_export_graph_json(self):\\n        \\\"\\\"\\\"Test exporting graph to JSON.\\\"\\\"\\\"\\n        processor = CorticalTextProcessor()\\n        processor.process_document(\\\"doc1\\\", \\\"neural networks learning\\\")\\n        processor.process_document(\\\"doc2\\\", \\\"machine learning algorithms\\\")\\n        processor.compute_all(verbose=False)\\n\\n        with tempfile.TemporaryDirectory() as tmpdir:\\n            filepath = os.path.join(tmpdir, \\\"graph.json\\\")\\n            result = export_graph_json(filepath, processor.layers, verbose=False)\\n\\n            # Check file was created\\n            self.assertTrue(os.path.exists(filepath))\\n\\n            # Check result structure\\n            self.assertIn('nodes', result)\\n            self.assertIn('edges', result)\\n            self.assertIn('metadata', result)\\n\\n            # Verify file contents\\n            with open(filepath) as f:\\n                data = json.load(f)\\n            self.assertEqual(data['metadata']['node_count'], len(data['nodes']))\\n\\n    def test_export_graph_json_layer_filter(self):\\n        \\\"\\\"\\\"Test exporting specific layer.\\\"\\\"\\\"\\n        processor = CorticalTextProcessor()\\n        processor.process_document(\\\"doc1\\\", \\\"neural networks learning\\\")\\n        processor.compute_all(verbose=False)\\n\\n        with tempfile.TemporaryDirectory() as tmpdir:\\n            filepath = os.path.join(tmpdir, \\\"graph.json\\\")\\n            result = export_graph_json(\\n                filepath,\\n                processor.layers,\\n                layer_filter=CorticalLayer.TOKENS,\\n                verbose=False\\n            )\\n\\n            # All nodes should be from layer 0\\n            for node in result['nodes']:\\n                self.assertEqual(node['layer'], 0)\\n\\n    def test_export_graph_json_min_weight(self):\\n        \\\"\\\"\\\"Test filtering edges by minimum weight.\\\"\\\"\\\"\\n        processor = CorticalTextProcessor()\\n        processor.process_document(\\\"doc1\\\", \\\"neural networks learning deep\\\")\\n        processor.compute_all(verbose=False)\\n\\n        with tempfile.TemporaryDirectory() as tmpdir:\\n            filepath = os.path.join(tmpdir, \\\"graph.json\\\")\\n            result = export_graph_json(\\n                filepath,\\n                processor.layers,\\n                min_weight=0.5,\\n                verbose=False\\n            )\\n\\n            # All edges should have weight >= 0.5\\n            for edge in result['edges']:\\n                self.assertGreaterEqual(edge['weight'], 0.5)\\n\\n    def test_export_graph_json_max_nodes(self):\\n        \\\"\\\"\\\"Test limiting number of nodes.\\\"\\\"\\\"\\n        processor = CorticalTextProcessor()\\n        processor.process_document(\\\"doc1\\\", \\\"word1 word2 word3 word4 word5 word6 word7 word8\\\")\\n        processor.compute_all(verbose=False)\\n\\n        with tempfile.TemporaryDirectory() as tmpdir:\\n            filepath = os.path.join(tmpdir, \\\"graph.json\\\")\\n            result = export_graph_json(\\n                filepath,\\n                processor.layers,\\n                max_nodes=3,\\n                verbose=False\\n            )\\n\\n            self.assertLessEqual(len(result['nodes']), 3)\\n\\n    def test_export_graph_json_verbose_false(self):\\n        \\\"\\\"\\\"Test that verbose=False suppresses output.\\\"\\\"\\\"\\n        processor = CorticalTextProcessor()\\n        processor.process_document(\\\"doc1\\\", \\\"test content\\\")\\n        processor.compute_all(verbose=False)\\n\\n        with tempfile.TemporaryDirectory() as tmpdir:\\n            filepath = os.path.join(tmpdir, \\\"graph.json\\\")\\n            # This should not print anything\\n            export_graph_json(filepath, processor.layers, verbose=False)\\n\\n\\nclass TestExportEmbeddingsJSON(unittest.TestCase):\\n    \\\"\\\"\\\"Test embeddings JSON export.\\\"\\\"\\\"\\n\\n    def test_export_embeddings_json(self):\\n        \\\"\\\"\\\"Test exporting embeddings to JSON.\\\"\\\"\\\"\\n        processor = CorticalTextProcessor()\\n        processor.process_document(\\\"doc1\\\", \\\"neural networks learning\\\")\\n        processor.compute_all(verbose=False)\\n\\n        embeddings, _ = compute_graph_embeddings(\\n            processor.layers,\\n            dimensions=16,\\n            method='adjacency'\\n        )\\n\\n        with tempfile.TemporaryDirectory() as tmpdir:\\n            filepath = os.path.join(tmpdir, \\\"embeddings.json\\\")\\n            export_embeddings_json(filepath, embeddings)\\n\\n            # Check file was created\\n            self.assertTrue(os.path.exists(filepath))\\n\\n            # Check file contents\\n            with open(filepath) as f:\\n                data = json.load(f)\\n            self.assertIn('embeddings', data)\\n            self.assertIn('metadata', data)\\n\\n    def test_export_embeddings_json_with_metadata(self):\\n        \\\"\\\"\\\"Test exporting embeddings with custom metadata.\\\"\\\"\\\"\\n        embeddings = {'term1': [1.0, 2.0], 'term2': [3.0, 4.0]}\\n        metadata = {'custom_key': 'custom_value'}\\n\\n        with tempfile.TemporaryDirectory() as tmpdir:\\n            filepath = os.path.join(tmpdir, \\\"embeddings.json\\\")\\n            export_embeddings_json(filepath, embeddings, metadata)\\n\\n            with open(filepath) as f:\\n                data = json.load(f)\\n            self.assertIn('custom_key', data['metadata'])\\n\\n\\nclass TestGetStateSummary(unittest.TestCase):\\n    \\\"\\\"\\\"Test state summary functionality.\\\"\\\"\\\"\\n\\n    def test_get_state_summary(self):\\n        \\\"\\\"\\\"Test getting state summary.\\\"\\\"\\\"\\n        processor = CorticalTextProcessor()\\n        processor.process_document(\\\"doc1\\\", \\\"neural networks learning\\\")\\n        processor.process_document(\\\"doc2\\\", \\\"machine learning algorithms\\\")\\n        processor.compute_all(verbose=False)\\n\\n        summary = get_state_summary(processor.layers, processor.documents)\\n\\n        # Check expected keys (actual keys from get_state_summary)\\n        self.assertIn('documents', summary)\\n        self.assertIn('layers', summary)\\n        self.assertIn('total_connections', summary)\\n        self.assertIn('total_columns', summary)\\n\\n        self.assertEqual(summary['documents'], 2)\\n\\n    def test_get_state_summary_empty(self):\\n        \\\"\\\"\\\"Test summary for empty processor.\\\"\\\"\\\"\\n        processor = CorticalTextProcessor()\\n\\n        summary = get_state_summary(processor.layers, processor.documents)\\n\\n        self.assertEqual(summary['documents'], 0)\\n\\n\\nclass TestExportConceptNetJSON(unittest.TestCase):\\n    \\\"\\\"\\\"Test ConceptNet-style graph export.\\\"\\\"\\\"\\n\\n    @classmethod\\n    def setUpClass(cls):\\n        \\\"\\\"\\\"Set up processor with sample data.\\\"\\\"\\\"\\n        cls.processor = CorticalTextProcessor()\\n        cls.processor.process_document(\\\"doc1\\\", \\\"\\\"\\\"\\n            Neural networks are a type of machine learning model.\\n            Deep learning uses neural networks for pattern recognition.\\n        \\\"\\\"\\\")\\n        cls.processor.process_document(\\\"doc2\\\", \\\"\\\"\\\"\\n            Machine learning algorithms process data efficiently.\\n            Pattern recognition is used for image classification.\\n        \\\"\\\"\\\")\\n        cls.processor.compute_all(verbose=False)\\n        cls.processor.extract_corpus_semantics(verbose=False)\\n\\n    def test_export_conceptnet_json_creates_file(self):\\n        \\\"\\\"\\\"Test that export creates a JSON file.\\\"\\\"\\\"\\n        with tempfile.TemporaryDirectory() as tmpdir:\\n            filepath = os.path.join(tmpdir, \\\"conceptnet.json\\\")\\n            export_conceptnet_json(filepath, self.processor.layers, verbose=False)\\n            self.assertTrue(os.path.exists(filepath))\\n\\n    def test_export_conceptnet_json_structure(self):\\n        \\\"\\\"\\\"Test exported JSON structure.\\\"\\\"\\\"\\n        with tempfile.TemporaryDirectory() as tmpdir:\\n            filepath = os.path.join(tmpdir, \\\"conceptnet.json\\\")\\n            result = export_conceptnet_json(filepath, self.processor.layers, verbose=False)\\n\\n            self.assertIn('nodes', result)\\n            self.assertIn('edges', result)\\n            self.assertIn('metadata', result)\\n\\n            # Check metadata\\n            self.assertIn('node_count', result['metadata'])\\n            self.assertIn('edge_count', result['metadata'])\\n            self.assertIn('layers', result['metadata'])\\n            self.assertIn('edge_types', result['metadata'])\\n            self.assertIn('relation_types', result['metadata'])\\n\\n    def test_export_conceptnet_json_node_structure(self):\\n        \\\"\\\"\\\"Test node structure in exported JSON.\\\"\\\"\\\"\\n        with tempfile.TemporaryDirectory() as tmpdir:\\n            filepath = os.path.join(tmpdir, \\\"conceptnet.json\\\")\\n            result = export_conceptnet_json(filepath, self.processor.layers, verbose=False)\\n\\n            for node in result['nodes']:\\n                self.assertIn('id', node)\\n                self.assertIn('label', node)\\n                self.assertIn('layer', node)\\n                self.assertIn('layer_name', node)\\n                self.assertIn('color', node)\\n                self.assertIn('pagerank', node)\\n                # Color should be valid hex\\n                self.assertTrue(node['color'].startswith('#'))\\n\\n    def test_export_conceptnet_json_edge_structure(self):\\n        \\\"\\\"\\\"Test edge structure in exported JSON.\\\"\\\"\\\"\\n        with tempfile.TemporaryDirectory() as tmpdir:\\n            filepath = os.path.join(tmpdir, \\\"conceptnet.json\\\")\\n            result = export_conceptnet_json(filepath, self.processor.layers, verbose=False)\\n\\n            for edge in result['edges']:\\n                self.assertIn('source', edge)\\n                self.assertIn('target', edge)\\n                self.assertIn('weight', edge)\\n                self.assertIn('relation_type', edge)\\n                self.assertIn('edge_type', edge)\\n                self.assertIn('color', edge)\\n\\n    def test_export_conceptnet_json_layer_colors(self):\\n        \\\"\\\"\\\"Test that nodes have correct layer colors.\\\"\\\"\\\"\\n        with tempfile.TemporaryDirectory() as tmpdir:\\n            filepath = os.path.join(tmpdir, \\\"conceptnet.json\\\")\\n            result = export_conceptnet_json(filepath, self.processor.layers, verbose=False)\\n\\n            for node in result['nodes']:\\n                layer = CorticalLayer(node['layer'])\\n                expected_color = LAYER_COLORS.get(layer, '#808080')\\n                self.assertEqual(node['color'], expected_color)\\n\\n    def test_export_conceptnet_json_with_semantic_relations(self):\\n        \\\"\\\"\\\"Test export with semantic relations included.\\\"\\\"\\\"\\n        with tempfile.TemporaryDirectory() as tmpdir:\\n            filepath = os.path.join(tmpdir, \\\"conceptnet.json\\\")\\n            result = export_conceptnet_json(\\n                filepath,\\n                self.processor.layers,\\n                semantic_relations=self.processor.semantic_relations,\\n                verbose=False\\n            )\\n\\n            # Should have edges\\n            self.assertGreater(len(result['edges']), 0)\\n\\n    def test_export_conceptnet_json_cross_layer_edges(self):\\n        \\\"\\\"\\\"Test that cross-layer edges are included when requested.\\\"\\\"\\\"\\n        with tempfile.TemporaryDirectory() as tmpdir:\\n            filepath = os.path.join(tmpdir, \\\"conceptnet.json\\\")\\n            result = export_conceptnet_json(\\n                filepath,\\n                self.processor.layers,\\n                include_cross_layer=True,\\n                verbose=False\\n            )\\n\\n            edge_types = result['metadata'].get('edge_types', {})\\n            # May have cross_layer edges if there are feedforward/feedback connections\\n            self.assertIsInstance(edge_types, dict)\\n\\n    def test_export_conceptnet_json_no_cross_layer(self):\\n        \\\"\\\"\\\"Test export without cross-layer edges.\\\"\\\"\\\"\\n        with tempfile.TemporaryDirectory() as tmpdir:\\n            filepath = os.path.join(tmpdir, \\\"conceptnet.json\\\")\\n            result = export_conceptnet_json(\\n                filepath,\\n                self.processor.layers,\\n                include_cross_layer=False,\\n                verbose=False\\n            )\\n\\n            # No cross_layer edges should be present\\n            cross_layer_count = result['metadata'].get('edge_types', {}).get('cross_layer', 0)\\n            self.assertEqual(cross_layer_count, 0)\\n\\n    def test_export_conceptnet_json_max_nodes(self):\\n        \\\"\\\"\\\"Test limiting nodes per layer.\\\"\\\"\\\"\\n        with tempfile.TemporaryDirectory() as tmpdir:\\n            filepath = os.path.join(tmpdir, \\\"conceptnet.json\\\")\\n            result = export_conceptnet_json(\\n                filepath,\\n                self.processor.layers,\\n                max_nodes_per_layer=5,\\n                verbose=False\\n            )\\n\\n            # Count nodes per layer\\n            layer_counts = {}\\n            for node in result['nodes']:\\n                layer = node['layer']\\n                layer_counts[layer] = layer_counts.get(layer, 0) + 1\\n\\n            # Each layer should have at most 5 nodes\\n            for layer, count in layer_counts.items():\\n                self.assertLessEqual(count, 5)\\n\\n    def test_export_conceptnet_json_min_weight(self):\\n        \\\"\\\"\\\"Test filtering edges by minimum weight.\\\"\\\"\\\"\\n        with tempfile.TemporaryDirectory() as tmpdir:\\n            filepath = os.path.join(tmpdir, \\\"conceptnet.json\\\")\\n            result = export_conceptnet_json(\\n                filepath,\\n                self.processor.layers,\\n                min_weight=0.5,\\n                verbose=False\\n            )\\n\\n            for edge in result['edges']:\\n                self.assertGreaterEqual(edge['weight'], 0.5)\\n\\n    def test_layer_colors_constant(self):\\n        \\\"\\\"\\\"Test that LAYER_COLORS constant is defined.\\\"\\\"\\\"\\n        self.assertIn(CorticalLayer.TOKENS, LAYER_COLORS)\\n        self.assertIn(CorticalLayer.BIGRAMS, LAYER_COLORS)\\n        self.assertIn(CorticalLayer.CONCEPTS, LAYER_COLORS)\\n        self.assertIn(CorticalLayer.DOCUMENTS, LAYER_COLORS)\\n\\n    def test_layer_names_constant(self):\\n        \\\"\\\"\\\"Test that LAYER_NAMES constant is defined.\\\"\\\"\\\"\\n        self.assertIn(CorticalLayer.TOKENS, LAYER_NAMES)\\n        self.assertEqual(LAYER_NAMES[CorticalLayer.TOKENS], 'Tokens')\\n        self.assertEqual(LAYER_NAMES[CorticalLayer.BIGRAMS], 'Bigrams')\\n\\n    def test_processor_export_conceptnet_json(self):\\n        \\\"\\\"\\\"Test processor-level export method.\\\"\\\"\\\"\\n        with tempfile.TemporaryDirectory() as tmpdir:\\n            filepath = os.path.join(tmpdir, \\\"conceptnet.json\\\")\\n            result = self.processor.export_conceptnet_json(filepath, verbose=False)\\n\\n            self.assertIn('nodes', result)\\n            self.assertIn('edges', result)\\n            self.assertTrue(os.path.exists(filepath))\\n\\n\\nif __name__ == \\\"__main__\\\":\\n    unittest.main(verbosity=2)\\n\",",
        "      \"mtime\": 1765375584.0",
        "    },",
        "    {",
        "      \"op\": \"add\",",
        "      \"doc_id\": \"cortical/embeddings.py\",",
        "      \"content\": \"\\\"\\\"\\\"\\nEmbeddings Module\\n=================\\n\\nGraph-based embeddings for the cortical network.\\n\\nImplements three methods for computing term embeddings from the\\nconnection graph structure:\\n1. Adjacency: Direct connection weights to landmark nodes\\n2. Random Walk: DeepWalk-inspired walk co-occurrence\\n3. Spectral: Graph Laplacian eigenvector approximation\\n\\\"\\\"\\\"\\n\\nimport math\\nimport random\\nfrom typing import Any, Dict, List, Tuple, Optional\\nfrom collections import defaultdict\\n\\nfrom .layers import CorticalLayer, HierarchicalLayer\\n\\n\\ndef compute_graph_embeddings(\\n    layers: Dict[CorticalLayer, HierarchicalLayer],\\n    dimensions: int = 64,\\n    method: str = 'adjacency'\\n) -> Tuple[Dict[str, List[float]], Dict[str, Any]]:\\n    \\\"\\\"\\\"\\n    Compute embeddings for tokens based on graph structure.\\n    \\n    Args:\\n        layers: Dictionary of layers (needs TOKENS)\\n        dimensions: Number of embedding dimensions\\n        method: 'adjacency', 'random_walk', or 'spectral'\\n        \\n    Returns:\\n        Tuple of (embeddings dict, statistics dict)\\n    \\\"\\\"\\\"\\n    layer0 = layers[CorticalLayer.TOKENS]\\n    \\n    if method == 'adjacency':\\n        embeddings = _adjacency_embeddings(layer0, dimensions)\\n    elif method == 'random_walk':\\n        embeddings = _random_walk_embeddings(layer0, dimensions)\\n    elif method == 'spectral':\\n        embeddings = _spectral_embeddings(layer0, dimensions)\\n    else:\\n        raise ValueError(f\\\"Unknown embedding method: {method}\\\")\\n    \\n    stats = {\\n        'method': method,\\n        'dimensions': dimensions,\\n        'terms_embedded': len(embeddings)\\n    }\\n    \\n    return embeddings, stats\\n\\n\\ndef _adjacency_embeddings(layer: HierarchicalLayer, dimensions: int) -> Dict[str, List[float]]:\\n    \\\"\\\"\\\"Compute embeddings using adjacency to landmark nodes.\\\"\\\"\\\"\\n    embeddings: Dict[str, List[float]] = {}\\n    \\n    sorted_cols = sorted(layer.minicolumns.values(), key=lambda c: c.pagerank, reverse=True)\\n    landmarks = sorted_cols[:dimensions]\\n    \\n    for col in layer.minicolumns.values():\\n        vec = [col.lateral_connections.get(lm.id, 0) for lm in landmarks]\\n        mag = math.sqrt(sum(v*v for v in vec)) + 1e-10\\n        embeddings[col.content] = [v / mag for v in vec]\\n    \\n    return embeddings\\n\\n\\ndef _random_walk_embeddings(\\n    layer: HierarchicalLayer,\\n    dimensions: int,\\n    walks_per_node: int = 10,\\n    walk_length: int = 40,\\n    window_size: int = 5\\n) -> Dict[str, List[float]]:\\n    \\\"\\\"\\\"Compute embeddings using random walks (DeepWalk-inspired).\\\"\\\"\\\"\\n    embeddings: Dict[str, List[float]] = {}\\n    id_to_term = {col.id: col.content for col in layer.minicolumns.values()}\\n    cooccurrence: Dict[str, Dict[str, float]] = defaultdict(lambda: defaultdict(float))\\n    \\n    for col in layer.minicolumns.values():\\n        for _ in range(walks_per_node):\\n            walk = _weighted_random_walk(col, layer, walk_length, id_to_term)\\n            for i, term in enumerate(walk):\\n                for j in range(max(0, i - window_size), min(len(walk), i + window_size + 1)):\\n                    if i != j:\\n                        cooccurrence[term][walk[j]] += 1.0\\n    \\n    sorted_cols = sorted(layer.minicolumns.values(), key=lambda c: c.pagerank, reverse=True)\\n    landmarks = [c.content for c in sorted_cols[:dimensions]]\\n    \\n    for term in layer.minicolumns:\\n        vec = [cooccurrence[term].get(lm, 0) for lm in landmarks]\\n        mag = math.sqrt(sum(v*v for v in vec)) + 1e-10\\n        embeddings[term] = [v / mag for v in vec]\\n    \\n    return embeddings\\n\\n\\ndef _weighted_random_walk(start_col, layer: HierarchicalLayer, length: int, id_to_term: Dict[str, str]) -> List[str]:\\n    \\\"\\\"\\\"Perform a weighted random walk from a starting column.\\\"\\\"\\\"\\n    walk = [start_col.content]\\n    current = start_col\\n    \\n    for _ in range(length - 1):\\n        if not current.lateral_connections:\\n            break\\n        neighbors = list(current.lateral_connections.items())\\n        total_weight = sum(w for _, w in neighbors)\\n        if total_weight == 0:\\n            break\\n        \\n        r = random.random() * total_weight\\n        cumsum = 0.0\\n        next_id = neighbors[0][0]\\n        for neighbor_id, weight in neighbors:\\n            cumsum += weight\\n            if cumsum >= r:\\n                next_id = neighbor_id\\n                break\\n        \\n        next_term = id_to_term.get(next_id)\\n        if next_term and next_term in layer.minicolumns:\\n            current = layer.minicolumns[next_term]\\n            walk.append(next_term)\\n        else:\\n            break\\n    \\n    return walk\\n\\n\\ndef _spectral_embeddings(layer: HierarchicalLayer, dimensions: int, iterations: int = 100) -> Dict[str, List[float]]:\\n    \\\"\\\"\\\"Compute embeddings using spectral methods (graph Laplacian).\\\"\\\"\\\"\\n    embeddings: Dict[str, List[float]] = {}\\n    terms = list(layer.minicolumns.keys())\\n    n = len(terms)\\n    if n == 0:\\n        return embeddings\\n    \\n    term_to_idx = {t: i for i, t in enumerate(terms)}\\n    adjacency: Dict[int, Dict[int, float]] = defaultdict(dict)\\n    degrees = [0.0] * n\\n    \\n    for term, col in layer.minicolumns.items():\\n        i = term_to_idx[term]\\n        for neighbor_id, weight in col.lateral_connections.items():\\n            neighbor = layer.get_by_id(neighbor_id)\\n            if neighbor and neighbor.content in term_to_idx:\\n                j = term_to_idx[neighbor.content]\\n                adjacency[i][j] = weight\\n                degrees[i] += weight\\n    \\n    degrees = [d if d > 0 else 1.0 for d in degrees]\\n    actual_dims = min(dimensions, n)\\n    vectors = []\\n    \\n    for d in range(actual_dims):\\n        vec = [random.gauss(0, 1) for _ in range(n)]\\n        for prev in vectors:\\n            dot = sum(v * p for v, p in zip(vec, prev))\\n            vec = [v - dot * p for v, p in zip(vec, prev)]\\n        mag = math.sqrt(sum(v*v for v in vec)) + 1e-10\\n        vec = [v / mag for v in vec]\\n        \\n        for _ in range(iterations):\\n            new_vec = [0.0] * n\\n            for i in range(n):\\n                for j, weight in adjacency[i].items():\\n                    norm_weight = weight / math.sqrt(degrees[i] * degrees[j])\\n                    new_vec[i] -= norm_weight * vec[j]\\n                new_vec[i] += vec[i]\\n            \\n            for prev in vectors:\\n                dot = sum(v * p for v, p in zip(new_vec, prev))\\n                new_vec = [v - dot * p for v, p in zip(new_vec, prev)]\\n            mag = math.sqrt(sum(v*v for v in new_vec)) + 1e-10\\n            vec = [v / mag for v in new_vec]\\n        \\n        vectors.append(vec)\\n    \\n    for term in terms:\\n        i = term_to_idx[term]\\n        embeddings[term] = [vectors[d][i] if d < len(vectors) else 0.0 for d in range(dimensions)]\\n    \\n    return embeddings\\n\\n\\ndef embedding_similarity(embeddings: Dict[str, List[float]], term1: str, term2: str) -> float:\\n    \\\"\\\"\\\"Compute cosine similarity between two term embeddings.\\\"\\\"\\\"\\n    if term1 not in embeddings or term2 not in embeddings:\\n        return 0.0\\n    vec1, vec2 = embeddings[term1], embeddings[term2]\\n    dot = sum(a * b for a, b in zip(vec1, vec2))\\n    mag1 = math.sqrt(sum(a * a for a in vec1))\\n    mag2 = math.sqrt(sum(b * b for b in vec2))\\n    return dot / (mag1 * mag2) if mag1 > 0 and mag2 > 0 else 0.0\\n\\n\\ndef find_similar_by_embedding(embeddings: Dict[str, List[float]], term: str, top_n: int = 10) -> List[Tuple[str, float]]:\\n    \\\"\\\"\\\"Find terms most similar to a given term by embedding.\\\"\\\"\\\"\\n    if term not in embeddings:\\n        return []\\n    similarities = [(t, embedding_similarity(embeddings, term, t)) for t in embeddings if t != term]\\n    similarities.sort(key=lambda x: x[1], reverse=True)\\n    return similarities[:top_n]\\n\",",
        "      \"mtime\": 1765375584.0",
        "    },",
        "    {",
        "      \"op\": \"add\",",
        "      \"doc_id\": \"docs/cookbook.md\",",
        "      \"content\": \"# Cortical Text Processor Cookbook\\n\\nA practical guide to common patterns and recipes for using the Cortical Text Processor effectively.\\n\\n---\\n\\n## Table of Contents\\n\\n1. [Document Processing Patterns](#document-processing-patterns)\\n2. [Search Optimization Recipes](#search-optimization-recipes)\\n3. [Corpus Maintenance Patterns](#corpus-maintenance-patterns)\\n4. [Query Expansion Tuning](#query-expansion-tuning)\\n5. [Clustering Configuration](#clustering-configuration)\\n6. [Performance Optimization](#performance-optimization)\\n7. [RAG Integration Patterns](#rag-integration-patterns)\\n\\n---\\n\\n## Document Processing Patterns\\n\\n### Recipe 1: Batch Processing (Recommended)\\n\\n**When to use:** Adding multiple documents at once (initial corpus loading, bulk imports).\\n\\n```python\\nfrom cortical import CorticalTextProcessor\\n\\nprocessor = CorticalTextProcessor()\\n\\n# Prepare documents as list of (doc_id, content, metadata) tuples\\ndocuments = [\\n    (\\\"doc1\\\", \\\"Neural networks process information.\\\", {\\\"source\\\": \\\"book1\\\"}),\\n    (\\\"doc2\\\", \\\"Deep learning enables pattern recognition.\\\", {\\\"source\\\": \\\"book1\\\"}),\\n    (\\\"doc3\\\", \\\"Machine learning algorithms learn from data.\\\", {\\\"source\\\": \\\"book2\\\"}),\\n]\\n\\n# Add all documents and recompute once\\nstats = processor.add_documents_batch(\\n    documents,\\n    recompute='full',  # 'full', 'tfidf', or 'none'\\n    verbose=True\\n)\\n\\nprint(f\\\"Added {stats['documents_added']} documents\\\")\\n```\\n\\n**Expected outcome:**\\n- Single recomputation pass instead of per-document recomputation\\n- ~3-5x faster than calling `process_document()` in a loop\\n\\n**Recomputation options:**\\n- `recompute='full'`: Slowest, most accurate (includes all graph algorithms)\\n- `recompute='tfidf'`: Fast, good for search quality\\n- `recompute='none'`: Fastest, but computations marked stale\\n\\n---\\n\\n### Recipe 2: Incremental Updates (Live Systems)\\n\\n**When to use:** Adding documents to an already-built corpus (RAG systems, streaming data).\\n\\n```python\\n# Start with existing corpus\\nprocessor = CorticalTextProcessor.load(\\\"corpus.pkl\\\")\\n\\n# Add new document without full recomputation\\nprocessor.add_document_incremental(\\n    \\\"new_doc\\\",\\n    \\\"New document content.\\\",\\n    metadata={\\\"timestamp\\\": \\\"2025-12-10\\\"},\\n    recompute='tfidf'  # Only recompute TF-IDF for search quality\\n)\\n\\n# Later: full recomputation when needed\\nprocessor.recompute(level='full', verbose=True)\\n```\\n\\n---\\n\\n### Recipe 3: Document Removal\\n\\n**When to use:** Delete outdated documents, remove duplicates.\\n\\n```python\\n# Remove single document\\nresult = processor.remove_document(\\\"old_doc\\\", verbose=True)\\nprint(f\\\"Tokens affected: {result['tokens_affected']}\\\")\\n\\n# Remove multiple documents efficiently\\ndoc_ids_to_remove = [\\\"old_doc1\\\", \\\"old_doc2\\\", \\\"old_doc3\\\"]\\nresult = processor.remove_documents_batch(\\n    doc_ids_to_remove,\\n    recompute='tfidf',\\n    verbose=True\\n)\\n```\\n\\n---\\n\\n## Search Optimization Recipes\\n\\n### Recipe 4: Choosing the Right Search Method\\n\\n**Decision tree:**\\n\\n```\\nSearching repeatedly on same corpus?\\n├─ YES → fast_find_documents() or build_search_index()\\n└─ NO  → find_documents_for_query()\\n\\nNeed text passages for RAG?\\n├─ YES → find_passages_for_query()\\n└─ NO  → find_documents_for_query()\\n\\nLarge corpus (1000+ docs)?\\n└─ YES → fast_find_documents() for ~2-3x speedup\\n```\\n\\n---\\n\\n### Recipe 5: Fast Document Search\\n\\n**When to use:** Large corpora, need sub-100ms response time.\\n\\n```python\\n# Fast search with candidate filtering\\nresults = processor.fast_find_documents(\\n    \\\"neural networks\\\",\\n    top_n=5,\\n    candidate_multiplier=3,  # 5 * 3 = 15 candidates examined\\n    use_code_concepts=True   # Enable for code search\\n)\\n\\nfor doc_id, score in results:\\n    print(f\\\"{doc_id}: {score:.3f}\\\")\\n```\\n\\n**Tuning `candidate_multiplier`:**\\n- `1`: Aggressive (may miss relevant documents)\\n- `3`: Balanced (recommended)\\n- `5`: Conservative (slower but higher recall)\\n\\n---\\n\\n### Recipe 6: Pre-Built Search Index (Fastest)\\n\\n**When to use:** Repeated searching on stable corpus.\\n\\n```python\\n# Build index once\\nindex = processor.build_search_index()\\n\\n# Use for fast searches\\nqueries = [\\\"neural networks\\\", \\\"machine learning\\\", \\\"deep learning\\\"]\\nfor query in queries:\\n    results = processor.search_with_index(query, index, top_n=5)\\n    print(f\\\"{query}: {len(results)} results\\\")\\n```\\n\\n**Note:** Rebuild index after `add_documents_batch()` or `remove_document()`.\\n\\n---\\n\\n### Recipe 7: Passage Retrieval for RAG\\n\\n**When to use:** Building retrieval-augmented generation systems.\\n\\n```python\\nresults = processor.find_passages_for_query(\\n    \\\"neural network training\\\",\\n    top_n=5,\\n    chunk_size=512,      # Characters per chunk\\n    overlap=128,         # Overlap between chunks\\n    use_expansion=True\\n)\\n\\n# Results: (passage_text, doc_id, start_char, end_char, score)\\nfor passage, doc_id, start, end, score in results:\\n    print(f\\\"[{doc_id}:{start}-{end}] Score: {score:.3f}\\\")\\n    print(passage[:100] + \\\"...\\\")\\n```\\n\\n**Chunk size tuning:**\\n- `256`: Small, precise passages\\n- `512`: Balanced (recommended)\\n- `1024`: Large, more context\\n\\n---\\n\\n## Corpus Maintenance Patterns\\n\\n### Recipe 8: Detecting Stale Computations\\n\\n**When to use:** Understand what needs recomputation after changes.\\n\\n```python\\n# Check what's stale\\nstale = processor.get_stale_computations()\\nprint(f\\\"Stale: {stale}\\\")\\n\\nif 'tfidf' in stale:\\n    print(\\\"TF-IDF scores are outdated - search quality affected\\\")\\n    processor.compute_tfidf(verbose=True)\\n\\nif 'pagerank' in stale:\\n    print(\\\"PageRank scores are outdated\\\")\\n    processor.compute_importance(verbose=True)\\n```\\n\\n---\\n\\n### Recipe 9: Save and Load Corpus\\n\\n**When to use:** Persist trained corpus for deployment.\\n\\n```python\\n# Build and save\\nprocessor = CorticalTextProcessor()\\nprocessor.add_documents_batch(documents, recompute='full')\\nprocessor.save(\\\"production_corpus.pkl\\\", verbose=True)\\n\\n# Load in production\\nloaded = CorticalTextProcessor.load(\\\"production_corpus.pkl\\\")\\nresults = loaded.find_documents_for_query(\\\"query\\\")\\n```\\n\\n---\\n\\n## Query Expansion Tuning\\n\\n### Recipe 10: Understanding Expansion\\n\\n```python\\n# See what expansion adds\\nexpanded = processor.expand_query(\\\"neural\\\", max_expansions=10)\\n\\nprint(\\\"Original term: neural\\\")\\nprint(\\\"Expanded with:\\\")\\nfor term, weight in sorted(expanded.items(), key=lambda x: -x[1]):\\n    if term != \\\"neural\\\":\\n        print(f\\\"  {term}: {weight:.3f}\\\")\\n```\\n\\n**Expansion sources:**\\n- **Lateral connections** (0.6x): Terms appearing near query term\\n- **Concept membership** (0.4x): Terms in same semantic cluster\\n- **Code concepts** (0.6x): Programming synonyms (get/fetch/load)\\n\\n---\\n\\n### Recipe 11: Tuning Expansion Parameters\\n\\n```python\\n# Conservative expansion (higher precision)\\nconservative = processor.expand_query(\\n    \\\"neural networks\\\",\\n    max_expansions=3,\\n    use_variants=False\\n)\\n\\n# Aggressive expansion (higher recall)\\naggressive = processor.expand_query(\\n    \\\"neural networks\\\",\\n    max_expansions=20,\\n    use_variants=True,\\n    use_code_concepts=True\\n)\\n\\n# Balanced (recommended)\\nbalanced = processor.expand_query(\\n    \\\"neural networks\\\",\\n    max_expansions=10,\\n    use_variants=True\\n)\\n```\\n\\n---\\n\\n### Recipe 12: Multi-Hop Expansion\\n\\n**When to use:** Discover distantly related terms through semantic relations.\\n\\n```python\\n# Extract semantic relations first\\nprocessor.extract_corpus_semantics()\\n\\n# Multi-hop expansion\\nexpanded = processor.expand_query_multihop(\\n    \\\"neural\\\",\\n    max_hops=2,         # Follow 2 relation hops\\n    max_expansions=15,\\n    decay_factor=0.5    # Weight decreases per hop\\n)\\n```\\n\\n---\\n\\n## Clustering Configuration\\n\\n### Recipe 13: Tuning Cluster Strictness\\n\\n```python\\n# Strict clustering (more separate clusters)\\nprocessor.compute_all(\\n    build_concepts=True,\\n    cluster_strictness=1.0,\\n    bridge_weight=0.0\\n)\\n\\n# Loose clustering (fewer, larger clusters)\\nprocessor.compute_all(\\n    build_concepts=True,\\n    cluster_strictness=0.5,\\n    bridge_weight=0.3\\n)\\n```\\n\\n**Strictness guide:**\\n- `1.0`: Strict (more clusters, stronger topic separation)\\n- `0.5`: Balanced (recommended)\\n- `0.0`: Loose (fewer clusters, more topic mixing)\\n\\n**Bridge weight effects:**\\n- `0.0`: No synthetic connections (isolated topics)\\n- `0.1-0.3`: Light bridging (recommended)\\n- `0.5+`: Strong bridging (may create spurious links)\\n\\n---\\n\\n## Performance Optimization\\n\\n### Recipe 14: Profiling Corpus Size\\n\\n```python\\nsummary = processor.get_corpus_summary()\\n\\nprint(f\\\"Documents: {summary['documents']}\\\")\\nprint(f\\\"Total columns: {summary['total_columns']}\\\")\\nprint(f\\\"Layer breakdown:\\\")\\nprint(f\\\"  Tokens: {summary['layer_stats'].get(0, {}).get('minicolumns', 0)}\\\")\\nprint(f\\\"  Bigrams: {summary['layer_stats'].get(1, {}).get('minicolumns', 0)}\\\")\\n\\n# Optimization strategy\\nif summary['documents'] < 100:\\n    print(\\\"Small corpus: use standard methods\\\")\\nelif summary['documents'] < 1000:\\n    print(\\\"Medium corpus: consider fast_find_documents()\\\")\\nelse:\\n    print(\\\"Large corpus: use search index\\\")\\n```\\n\\n---\\n\\n### Recipe 15: Query Cache Management\\n\\n```python\\n# Enable query caching\\nprocessor.set_query_cache_size(100)\\n\\n# Cached expansion (instant for repeated queries)\\nresults1 = processor.expand_query_cached(\\\"neural networks\\\")\\nresults2 = processor.expand_query_cached(\\\"neural networks\\\")  # From cache\\n\\n# Clear cache when corpus changes\\nprocessor.clear_query_cache()\\n```\\n\\n---\\n\\n## RAG Integration Patterns\\n\\n### Recipe 16: Simple RAG Backend\\n\\n```python\\ndef rag_retrieve(processor, query: str, top_n: int = 5) -> str:\\n    \\\"\\\"\\\"Retrieve context for RAG system.\\\"\\\"\\\"\\n    passages = processor.find_passages_for_query(\\n        query,\\n        top_n=top_n,\\n        chunk_size=512,\\n        overlap=128\\n    )\\n\\n    context = \\\"Context from knowledge base:\\\\n\\\\n\\\"\\n    for passage, doc_id, _, _, score in passages:\\n        context += f\\\"[{doc_id}] {passage}\\\\n\\\\n\\\"\\n\\n    return context\\n\\n# Use in RAG loop\\ncontext = rag_retrieve(processor, \\\"How do neural networks learn?\\\")\\n# Pass to LLM with question\\n```\\n\\n---\\n\\n### Recipe 17: Multi-Stage RAG Ranking\\n\\n**When to use:** Maximum quality ranking combining multiple signals.\\n\\n```python\\nresults = processor.multi_stage_rank(\\n    \\\"neural networks\\\",\\n    top_n=5,\\n    chunk_size=512,\\n    concept_boost=0.3  # Weight for concept relevance\\n)\\n\\nfor passage, doc_id, start, end, score, stages in results:\\n    print(f\\\"[{doc_id}] Final: {score:.3f}\\\")\\n    print(f\\\"  Concept: {stages['concept_score']:.3f}\\\")\\n    print(f\\\"  Document: {stages['doc_score']:.3f}\\\")\\n    print(f\\\"  Passage: {stages['chunk_score']:.3f}\\\")\\n```\\n\\n---\\n\\n## Quick Reference\\n\\n| Task | Best Method |\\n|------|-------------|\\n| Multiple documents | `add_documents_batch()` |\\n| Incremental updates | `add_document_incremental()` |\\n| Document removal | `remove_documents_batch()` |\\n| General search | `find_documents_for_query()` |\\n| Large corpus search | `fast_find_documents()` |\\n| Repeated searches | `build_search_index()` |\\n| RAG passages | `find_passages_for_query()` |\\n| High-quality RAG | `multi_stage_rank()` |\\n| Query debugging | `expand_query()` |\\n| Intent search | `search_by_intent()` |\\n\\n---\\n\\n## Troubleshooting\\n\\n### No Results Found\\n\\n```python\\n# Check if query terms exist\\nlayer0 = processor.get_layer(CorticalLayer.TOKENS)\\nfor term in processor.tokenizer.tokenize(query):\\n    if not layer0.get_minicolumn(term):\\n        print(f\\\"'{term}' not in corpus\\\")\\n\\n# Try with expansion\\nresults = processor.find_documents_for_query(query, use_expansion=True)\\n```\\n\\n### Search is Slow\\n\\n```python\\n# Use fast search\\nresults = processor.fast_find_documents(query, top_n=5)\\n\\n# Or build index\\nindex = processor.build_search_index()\\nresults = processor.search_with_index(query, index)\\n```\\n\\n### Stale Results\\n\\n```python\\n# Check and recompute\\nstale = processor.get_stale_computations()\\nif stale:\\n    processor.recompute(level='full')\\n```\\n\\n---\\n\\n*See also: [Query Guide](query-guide.md) for detailed query formulation, [Claude Usage Guide](claude-usage.md) for AI agent usage.*\\n\",",
        "      \"mtime\": 1765401947.0",
        "    },",
        "    {",
        "      \"op\": \"add\",",
        "      \"doc_id\": \"cortical/tokenizer.py\",",
        "      \"content\": \"\\\"\\\"\\\"\\nTokenizer Module\\n================\\n\\nText tokenization with stemming and word variant support.\\n\\nLike early visual processing, the tokenizer extracts basic features\\n(words) from raw input, filtering noise (stop words) and normalizing\\nrepresentations (lowercase, stemming).\\n\\\"\\\"\\\"\\n\\nimport re\\nfrom typing import List, Set, Optional, Dict, Tuple\\n\\n\\n# Programming keywords that should be preserved even if in stop words\\nPROGRAMMING_KEYWORDS = frozenset({\\n    'def', 'class', 'function', 'return', 'import', 'from', 'if', 'else',\\n    'elif', 'for', 'while', 'try', 'except', 'finally', 'with', 'as',\\n    'yield', 'async', 'await', 'lambda', 'pass', 'break', 'continue',\\n    'raise', 'assert', 'global', 'nonlocal', 'del', 'true', 'false',\\n    'none', 'null', 'void', 'int', 'str', 'float', 'bool', 'list',\\n    'dict', 'set', 'tuple', 'self', 'cls', 'init', 'main', 'args',\\n    'kwargs', 'super', 'property', 'staticmethod', 'classmethod',\\n    'isinstance', 'hasattr', 'getattr', 'setattr', 'len', 'range',\\n    'enumerate', 'zip', 'map', 'filter', 'print', 'open', 'read',\\n    'write', 'close', 'append', 'extend', 'insert', 'remove', 'pop',\\n    'const', 'let', 'var', 'public', 'private', 'protected', 'static',\\n    'final', 'abstract', 'interface', 'implements', 'extends', 'new',\\n    'this', 'constructor', 'module', 'export', 'require', 'package',\\n    # Common identifier components that shouldn't be filtered\\n    'get', 'set', 'add', 'put', 'has', 'can', 'run', 'max', 'min',\\n})\\n\\n\\ndef split_identifier(identifier: str) -> List[str]:\\n    \\\"\\\"\\\"\\n    Split a code identifier into component words.\\n\\n    Handles camelCase, PascalCase, underscore_style, and CONSTANT_STYLE.\\n\\n    Args:\\n        identifier: A code identifier like \\\"getUserCredentials\\\" or \\\"get_user_data\\\"\\n\\n    Returns:\\n        List of component words in lowercase\\n\\n    Examples:\\n        >>> split_identifier(\\\"getUserCredentials\\\")\\n        ['get', 'user', 'credentials']\\n        >>> split_identifier(\\\"get_user_data\\\")\\n        ['get', 'user', 'data']\\n        >>> split_identifier(\\\"XMLParser\\\")\\n        ['xml', 'parser']\\n        >>> split_identifier(\\\"parseHTTPResponse\\\")\\n        ['parse', 'http', 'response']\\n    \\\"\\\"\\\"\\n    if not identifier:\\n        return []\\n\\n    # Handle underscore_style and CONSTANT_STYLE\\n    if '_' in identifier:\\n        parts = [p for p in identifier.split('_') if p]\\n        # Recursively split any camelCase parts\\n        result = []\\n        for part in parts:\\n            if any(c.isupper() for c in part):  # Has any capitals - could be camelCase\\n                result.extend(split_identifier(part))\\n            else:\\n                result.append(part.lower())\\n        return [p for p in result if p]\\n\\n    # Handle camelCase and PascalCase\\n    # Insert space before uppercase letters, handling acronyms\\n    # \\\"parseHTTPResponse\\\" -> \\\"parse HTTP Response\\\" -> [\\\"parse\\\", \\\"http\\\", \\\"response\\\"]\\n    result = []\\n    current = []\\n\\n    for i, char in enumerate(identifier):\\n        if char.isupper():\\n            # Check if this starts a new word\\n            if current:\\n                # If previous was lowercase, this starts a new word\\n                if current[-1].islower():\\n                    result.append(''.join(current).lower())\\n                    current = [char]\\n                # If next char is lowercase, this uppercase starts a new word (end of acronym)\\n                elif i + 1 < len(identifier) and identifier[i + 1].islower():\\n                    result.append(''.join(current).lower())\\n                    current = [char]\\n                else:\\n                    # Continue building acronym\\n                    current.append(char)\\n            else:\\n                current.append(char)\\n        else:\\n            current.append(char)\\n\\n    if current:\\n        result.append(''.join(current).lower())\\n\\n    return [p for p in result if p]\\n\\n\\nclass Tokenizer:\\n    \\\"\\\"\\\"\\n    Text tokenizer with stemming and word variant support.\\n    \\n    Extracts tokens from text, filters stop words, and provides\\n    word variant expansion for query normalization.\\n    \\n    Attributes:\\n        stop_words: Set of words to filter out\\n        min_word_length: Minimum word length to keep\\n        \\n    Example:\\n        tokenizer = Tokenizer()\\n        tokens = tokenizer.tokenize(\\\"Neural networks process information\\\")\\n        # ['neural', 'networks', 'process', 'information']\\n        \\n        variants = tokenizer.get_word_variants(\\\"bread\\\")\\n        # ['bread', 'sourdough', 'dough', 'flour', 'baking', 'breads']\\n    \\\"\\\"\\\"\\n    \\n    DEFAULT_STOP_WORDS = frozenset({\\n        # Articles and conjunctions\\n        'the', 'a', 'an', 'and', 'or', 'but', 'nor', 'yet', 'so',\\n        # Prepositions\\n        'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'from', 'as',\\n        'into', 'through', 'during', 'before', 'after', 'above', 'below',\\n        'between', 'under', 'over', 'again', 'against', 'about', 'within',\\n        'without', 'toward', 'towards', 'upon', 'across', 'along', 'around',\\n        'behind', 'beside', 'beyond', 'down', 'inside', 'outside', 'throughout',\\n        # Verbs (auxiliary and common)\\n        'is', 'was', 'are', 'were', 'been', 'be', 'being',\\n        'have', 'has', 'had', 'having',\\n        'do', 'does', 'did', 'doing', 'done',\\n        'will', 'would', 'could', 'should', 'may', 'might', 'must', 'shall', 'can',\\n        'need', 'needs', 'needed',\\n        'get', 'gets', 'got', 'getting',\\n        'make', 'makes', 'made', 'making',\\n        'take', 'takes', 'took', 'taking', 'taken',\\n        'come', 'comes', 'came', 'coming',\\n        'give', 'gives', 'gave', 'giving', 'given',\\n        'use', 'uses', 'used', 'using',\\n        # Pronouns\\n        'that', 'this', 'these', 'those', 'it', 'its', 'itself',\\n        'they', 'them', 'their', 'theirs', 'themselves',\\n        'he', 'she', 'him', 'her', 'his', 'hers', 'himself', 'herself',\\n        'we', 'us', 'our', 'ours', 'ourselves',\\n        'you', 'your', 'yours', 'yourself', 'yourselves',\\n        'i', 'me', 'my', 'mine', 'myself',\\n        'who', 'whom', 'whose', 'what', 'which', 'when', 'where', 'why', 'how',\\n        # Adverbs and modifiers\\n        'not', 'no', 'yes', 'so', 'if', 'then', 'than', 'too', 'very', 'just',\\n        'also', 'only', 'even', 'still', 'already', 'always', 'never', 'ever',\\n        'often', 'sometimes', 'usually', 'now', 'here', 'there', 'well', 'much',\\n        'more', 'most', 'less', 'least', 'rather', 'quite', 'almost', 'nearly',\\n        'really', 'actually', 'especially', 'particularly', 'generally',\\n        # Common transitional words\\n        'while', 'although', 'though', 'however', 'therefore', 'thus', 'hence',\\n        'moreover', 'furthermore', 'nevertheless', 'nonetheless', 'meanwhile',\\n        'otherwise', 'instead', 'besides', 'whereas', 'whether', 'unless',\\n        # Common verbs\\n        'include', 'includes', 'including', 'included',\\n        'provide', 'provides', 'provided', 'providing',\\n        'require', 'requires', 'required', 'requiring',\\n        'enable', 'enables', 'enabled', 'enabling',\\n        'allow', 'allows', 'allowed', 'allowing',\\n        'create', 'creates', 'created', 'creating',\\n        'become', 'becomes', 'became', 'becoming',\\n        'remain', 'remains', 'remained', 'remaining',\\n        'offer', 'offers', 'offered', 'offering',\\n        'support', 'supports', 'supported', 'supporting',\\n        # Quantifiers and determiners\\n        'each', 'every', 'any', 'some', 'all', 'both', 'few', 'many', 'several',\\n        'such', 'other', 'another', 'same', 'different', 'own', 'certain',\\n        'one', 'two', 'three', 'first', 'second', 'third', 'last', 'next',\\n        # Common nouns (too generic)\\n        'way', 'ways', 'thing', 'things', 'time', 'times', 'year', 'years',\\n        'day', 'days', 'place', 'part', 'parts', 'case', 'cases', 'point',\\n        'fact', 'kind', 'type', 'form', 'forms', 'level', 'area', 'areas',\\n        # Common adjectives (too generic)\\n        'new', 'old', 'good', 'bad', 'great', 'small', 'large', 'big', 'long',\\n        'high', 'low', 'right', 'left', 'possible', 'important', 'major',\\n        'available', 'able', 'like', 'different', 'similar'\\n    })\\n    \\n    def __init__(\\n        self,\\n        stop_words: Optional[Set[str]] = None,\\n        min_word_length: int = 3,\\n        split_identifiers: bool = False\\n    ):\\n        \\\"\\\"\\\"\\n        Initialize tokenizer.\\n\\n        Args:\\n            stop_words: Set of words to filter out. Uses defaults if None.\\n            min_word_length: Minimum word length to keep.\\n            split_identifiers: If True, split camelCase/underscore_style and include\\n                               both original and component tokens.\\n        \\\"\\\"\\\"\\n        self.stop_words = stop_words if stop_words is not None else self.DEFAULT_STOP_WORDS\\n        self.min_word_length = min_word_length\\n        self.split_identifiers = split_identifiers\\n        \\n        # Simple suffix rules for stemming (Porter-lite)\\n        self._suffix_rules = [\\n            ('ational', 'ate'), ('tional', 'tion'), ('enci', 'ence'),\\n            ('anci', 'ance'), ('izer', 'ize'), ('isation', 'ize'),\\n            ('ization', 'ize'), ('ation', 'ate'), ('ator', 'ate'),\\n            ('alism', 'al'), ('iveness', 'ive'), ('fulness', 'ful'),\\n            ('ousness', 'ous'), ('aliti', 'al'), ('iviti', 'ive'),\\n            ('biliti', 'ble'), ('ement', ''), ('ment', ''), ('ness', ''),\\n            ('ling', ''), ('ing', ''), ('ies', 'y'), ('ied', 'y'),\\n            ('es', ''), ('ed', ''), ('ly', ''), ('er', ''), ('est', ''),\\n            ('ful', ''), ('less', ''), ('able', ''), ('ible', ''),\\n            ('ness', ''), ('ment', ''), ('ity', ''),\\n        ]\\n        \\n        # Common word mappings for query normalization\\n        self._word_mappings: Dict[str, List[str]] = {\\n            # Bread/baking related\\n            'bread': ['sourdough', 'dough', 'flour', 'baking', 'loaf'],\\n            'baking': ['sourdough', 'bread', 'dough', 'flour'],\\n            # Neural/brain related\\n            'brain': ['neural', 'cortical', 'neurons', 'cognitive'],\\n            'ai': ['neural', 'learning', 'artificial', 'intelligence'],\\n            'ml': ['learning', 'machine', 'neural', 'training'],\\n            # Database/storage\\n            'database': ['storage', 'data', 'query', 'index'],\\n            'db': ['database', 'storage', 'data'],\\n            # Common abbreviations\\n            'nlp': ['natural', 'language', 'processing', 'text'],\\n            'api': ['interface', 'endpoint', 'service'],\\n            # Synonyms\\n            'fast': ['quick', 'rapid', 'speed'],\\n            'slow': ['latency', 'delay'],\\n            'big': ['large', 'scale', 'massive'],\\n            'small': ['tiny', 'minimal', 'compact'],\\n        }\\n    \\n    def tokenize(self, text: str, split_identifiers: Optional[bool] = None) -> List[str]:\\n        \\\"\\\"\\\"\\n        Extract tokens from text.\\n\\n        Args:\\n            text: Input text to tokenize.\\n            split_identifiers: Override instance setting. If True, split\\n                              camelCase/underscore_style identifiers into components.\\n\\n        Returns:\\n            List of filtered, lowercase tokens.\\n\\n        Examples:\\n            >>> t = Tokenizer(split_identifiers=True)\\n            >>> t.tokenize(\\\"getUserCredentials fetches data\\\")\\n            ['getusercredentials', 'get', 'user', 'credentials', 'fetches', 'data']\\n        \\\"\\\"\\\"\\n        should_split = split_identifiers if split_identifiers is not None else self.split_identifiers\\n\\n        # Extract potential identifiers (including camelCase with internal caps)\\n        # Pattern matches: word2vec, getUserData, get_user_data, XMLParser\\n        raw_tokens = re.findall(r'\\\\b[a-zA-Z][a-zA-Z0-9_]*\\\\b', text)\\n\\n        result = []\\n        seen_splits = set()  # Only track splits to avoid duplicates from them\\n\\n        for token in raw_tokens:\\n            token_lower = token.lower()\\n\\n            # Skip stop words and short words\\n            if token_lower in self.stop_words or len(token_lower) < self.min_word_length:\\n                continue\\n\\n            # Add the original token (allow duplicates for proper bigram extraction)\\n            result.append(token_lower)\\n            # Track this token to prevent splits from duplicating it\\n            seen_splits.add(token_lower)\\n\\n            # Split identifier if enabled and token looks like an identifier\\n            if should_split and (\\n                '_' in token or\\n                any(c.isupper() for c in token[1:])  # Has internal capitals\\n            ):\\n                parts = split_identifier(token)\\n                for part in parts:\\n                    # Allow programming keywords even if in stop words\\n                    is_programming_keyword = part in PROGRAMMING_KEYWORDS\\n                    # Only add split parts once per token to avoid bloating\\n                    if (\\n                        part not in seen_splits and\\n                        part != token_lower and  # Don't duplicate the original\\n                        (is_programming_keyword or part not in self.stop_words) and\\n                        len(part) >= self.min_word_length\\n                    ):\\n                        result.append(part)\\n                        seen_splits.add(part)\\n\\n        return result\\n    \\n    def extract_ngrams(self, tokens: List[str], n: int = 2) -> List[str]:\\n        \\\"\\\"\\\"\\n        Extract n-grams from token list.\\n        \\n        Args:\\n            tokens: List of tokens.\\n            n: Size of n-grams to extract.\\n            \\n        Returns:\\n            List of n-gram strings (tokens joined by space).\\n        \\\"\\\"\\\"\\n        if len(tokens) < n:\\n            return []\\n        return [' '.join(tokens[i:i+n]) for i in range(len(tokens) - n + 1)]\\n    \\n    def stem(self, word: str) -> str:\\n        \\\"\\\"\\\"\\n        Apply simple suffix stripping (Porter-lite stemming).\\n        \\n        Args:\\n            word: Word to stem\\n            \\n        Returns:\\n            Stemmed word\\n        \\\"\\\"\\\"\\n        if len(word) <= 4:\\n            return word\\n        \\n        for suffix, replacement in self._suffix_rules:\\n            if word.endswith(suffix):\\n                stemmed = word[:-len(suffix)] + replacement\\n                if len(stemmed) >= 3:\\n                    return stemmed\\n        \\n        return word\\n    \\n    def get_word_variants(self, word: str) -> List[str]:\\n        \\\"\\\"\\\"\\n        Get related words/variants for query expansion.\\n        \\n        Args:\\n            word: Input word\\n            \\n        Returns:\\n            List of related words including the original\\n        \\\"\\\"\\\"\\n        word_lower = word.lower()\\n        variants = [word_lower]\\n        \\n        # Add mapped variants\\n        if word_lower in self._word_mappings:\\n            variants.extend(self._word_mappings[word_lower])\\n        \\n        # Add stemmed version\\n        stemmed = self.stem(word_lower)\\n        if stemmed != word_lower:\\n            variants.append(stemmed)\\n        \\n        # Add common variations\\n        if word_lower.endswith('s') and len(word_lower) > 3:\\n            variants.append(word_lower[:-1])  # Remove plural\\n        elif not word_lower.endswith('s'):\\n            variants.append(word_lower + 's')  # Add plural\\n        \\n        return list(set(variants))\\n    \\n    def add_word_mapping(self, word: str, variants: List[str]) -> None:\\n        \\\"\\\"\\\"\\n        Add a custom word mapping for query expansion.\\n        \\n        Args:\\n            word: The source word\\n            variants: List of variant words to map to\\n        \\\"\\\"\\\"\\n        word_lower = word.lower()\\n        if word_lower in self._word_mappings:\\n            self._word_mappings[word_lower].extend(variants)\\n            self._word_mappings[word_lower] = list(set(self._word_mappings[word_lower]))\\n        else:\\n            self._word_mappings[word_lower] = variants\\n\",",
        "      \"mtime\": 1765392877.0",
        "    },",
        "    {",
        "      \"op\": \"add\",",
        "      \"doc_id\": \"docs/glossary.md\",",
        "      \"content\": \"# Glossary\\n\\nThis glossary defines terminology used throughout the Cortical Text Processor codebase. Terms are organized by category for easy reference.\\n\\n---\\n\\n## Core Data Structures\\n\\n### Minicolumn\\nThe fundamental unit of representation at each layer. Named after cortical minicolumns in neuroscience, but implemented as a data structure holding connections, statistics, and metadata.\\n\\n**Location:** `minicolumn.py:56-357`\\n\\n**Fields:**\\n- `id`: Unique identifier (e.g., \\\"L0_neural\\\")\\n- `content`: The actual content (word, bigram, concept name, or doc_id)\\n- `layer`: Layer number (0-3)\\n- Various connection dictionaries and statistics\\n\\n### Edge\\nA typed connection with metadata, used for ConceptNet-style semantic edges.\\n\\n**Location:** `minicolumn.py:16-53`\\n\\n**Fields:**\\n- `target_id`: Target minicolumn ID\\n- `weight`: Connection strength\\n- `relation_type`: Semantic type ('IsA', 'PartOf', 'CoOccurs', etc.)\\n- `confidence`: Reliability score [0.0, 1.0]\\n- `source`: Origin ('corpus', 'semantic', 'inferred')\\n\\n### HierarchicalLayer\\nContainer that holds all minicolumns at a specific layer level.\\n\\n**Location:** `layers.py:59-273`\\n\\n**Key Features:**\\n- `minicolumns` dict maps content to Minicolumn objects\\n- `_id_index` provides O(1) lookup by minicolumn ID\\n- Methods: `get_or_create_minicolumn()`, `get_by_id()`, `column_count()`\\n\\n### CorticalLayer\\nEnumeration defining the 4 processing layers.\\n\\n**Location:** `layers.py:21-56`\\n\\n```\\nTOKENS = 0      # Individual words\\nBIGRAMS = 1     # Word pairs\\nCONCEPTS = 2    # Semantic clusters\\nDOCUMENTS = 3   # Full documents\\n```\\n\\n---\\n\\n## Connection Types\\n\\n### Lateral Connections\\n**Within-layer** connections between minicolumns at the same level. Built from co-occurrence patterns (tokens appearing near each other in text).\\n\\n**Storage:** `minicolumn.lateral_connections: Dict[str, float]`\\n\\n**Use:** Query expansion, PageRank computation, spreading activation.\\n\\n### Typed Connections\\n**Within-layer** connections with semantic metadata. Store relation type, confidence, and source information.\\n\\n**Storage:** `minicolumn.typed_connections: Dict[str, Edge]`\\n\\n**Use:** Semantic PageRank, ConceptNet-style reasoning.\\n\\n### Feedforward Connections\\n**Cross-layer** connections pointing downward (higher layer → lower layer). Connect containers to their components.\\n\\n**Storage:** `minicolumn.feedforward_connections: Dict[str, float]`\\n\\n**Examples:**\\n- Bigram → component tokens\\n- Concept → member tokens\\n- Document → contained tokens\\n\\n### Feedback Connections\\n**Cross-layer** connections pointing upward (lower layer → higher layer). Connect components to their containers.\\n\\n**Storage:** `minicolumn.feedback_connections: Dict[str, float]`\\n\\n**Examples:**\\n- Token → containing bigrams\\n- Token → containing concepts\\n- Token → containing documents\\n\\n---\\n\\n## Algorithms\\n\\n### PageRank\\nGraph algorithm measuring importance based on connection structure. Terms connected to other important terms receive higher scores.\\n\\n**Formula:** `PR(i) = (1-d)/n + d × Σ(PR(j) × w(j→i) / out(j))`\\n\\n**Location:** `analysis.py:22-95`\\n\\n**Variants:**\\n- Standard PageRank: Equal edge weights\\n- Semantic PageRank: Weights edges by relation type\\n- Hierarchical PageRank: Propagates across layers\\n\\n### TF-IDF\\nTerm Frequency - Inverse Document Frequency. Measures how distinctive a term is to documents in the corpus.\\n\\n**Formula:** `TF-IDF = log(1 + count) × log(num_docs / doc_frequency)`\\n\\n**Location:** `analysis.py:394-433`\\n\\n**Variants:**\\n- Global: Uses total corpus occurrence (`col.tfidf`)\\n- Per-document: Uses document-specific count (`col.tfidf_per_doc[doc_id]`)\\n\\n### Label Propagation\\nCommunity detection algorithm for clustering. Tokens adopt the most common label among their neighbors, causing related tokens to converge to the same cluster.\\n\\n**Location:** `analysis.py:502-636`\\n\\n**Parameters:**\\n- `cluster_strictness`: Higher = more separate clusters\\n- `bridge_weight`: Synthetic inter-document connections\\n\\n### Damping Factor\\nPageRank parameter (default 0.85) representing probability of following a link vs. random jump. Lower damping = more randomness in importance distribution.\\n\\n### Query Expansion\\nProcess of adding related terms to a search query based on lateral connections, concept membership, or semantic relations.\\n\\n**Location:** `query.py:55-176`\\n\\n### Spreading Activation\\nInformation propagation through connections. Activation starts at query terms and spreads to connected nodes, simulating neural activation patterns.\\n\\n---\\n\\n## Semantic Relations\\n\\n### IsA\\nHypernym/hyponym relationship. \\\"A dog IsA animal\\\" means dog is a type of animal.\\n\\n**Weight:** 1.5 (highest)\\n\\n### PartOf\\nMeronym/holonym relationship. \\\"Wheel PartOf car\\\" means wheel is a component of car.\\n\\n**Weight:** 1.3\\n\\n### HasA / HasProperty\\nProperty or component ownership. \\\"Dog HasProperty loyal\\\" or \\\"Dog HasA tail\\\".\\n\\n**Weight:** 1.2\\n\\n### SimilarTo\\nSimilarity without hierarchy. \\\"Dog SimilarTo cat\\\" - both are pets/animals.\\n\\n**Weight:** 1.4\\n\\n### RelatedTo\\nGeneral association from co-occurrence. Default relation type.\\n\\n**Weight:** 1.0\\n\\n### CoOccurs\\nStatistical co-occurrence in text. Lower confidence than explicit relations.\\n\\n**Weight:** 0.8\\n\\n### Causes\\nCausal relationship. \\\"Rain Causes floods\\\".\\n\\n**Weight:** 1.1\\n\\n### UsedFor\\nFunctional purpose. \\\"Hammer UsedFor nailing\\\".\\n\\n**Weight:** 1.0\\n\\n### Antonym\\nOpposition/contrast. \\\"Big Antonym small\\\".\\n\\n**Weight:** 0.3 (penalized)\\n\\n### DerivedFrom\\nMorphological or etymological derivation.\\n\\n**Weight:** 1.2\\n\\n---\\n\\n## Processing Concepts\\n\\n### Tokenization\\nBreaking text into individual word tokens. Includes lowercasing, stop word removal, and optional stemming.\\n\\n**Location:** `tokenizer.py`\\n\\n### Bigram\\nA pair of consecutive tokens. Stored with SPACE separator: \\\"neural networks\\\" (not underscore).\\n\\n**Location:** `tokenizer.py:303-316`\\n\\n### Concept Cluster\\nGroup of semantically related tokens discovered through label propagation. Becomes a minicolumn in Layer 2.\\n\\n### Corpus\\nThe collection of all documents processed by the system.\\n\\n### Retrofitting\\nPost-processing that adjusts lateral connection weights to align with semantic relations. Blends co-occurrence patterns with semantic knowledge.\\n\\n**Location:** `semantics.py:378-476`\\n\\n---\\n\\n## Architecture Concepts\\n\\n### 4-Layer Hierarchy\\nThe core architecture organizing text at increasing abstraction levels:\\n- Layer 0: TOKENS (words)\\n- Layer 1: BIGRAMS (word pairs)\\n- Layer 2: CONCEPTS (topic clusters)\\n- Layer 3: DOCUMENTS (full texts)\\n\\n### Cortical Metaphor\\nThe naming convention draws from neuroscience (V1→V2→V4→IT visual cortex pathway) but implementations are standard IR algorithms, not neural models.\\n\\n### Staleness Tracking\\nSystem for knowing which computations need rerunning after corpus changes. Prevents unnecessary recomputation.\\n\\n**Location:** `processor.py:49`\\n\\n---\\n\\n## Search Concepts\\n\\n### Intent Parsing\\nExtracting user intent from natural language queries. Maps question words to intent types (where→location, how→implementation).\\n\\n**Location:** `query.py:179-284`\\n\\n### Multi-hop Expansion\\nQuery expansion through chains of semantic relations. Finds terms 2+ hops away through valid relation paths.\\n\\n**Location:** `query.py:407-531`\\n\\n### Chunk\\nA segment of document text for passage retrieval. Created with configurable size and overlap.\\n\\n**Location:** `query.py:937-978`\\n\\n### Inverted Index\\nPre-computed mapping from terms to containing documents. Enables fast candidate filtering.\\n\\n**Location:** `query.py` (fast search functions)\\n\\n---\\n\\n## Code Concepts\\n\\n### Programming Concept Groups\\nCollections of synonymous programming terms. \\\"get\\\", \\\"fetch\\\", \\\"load\\\", \\\"retrieve\\\" are grouped together.\\n\\n**Location:** `code_concepts.py`\\n\\n### Code-Aware Tokenization\\nTokenization that splits identifiers: `getUserName` → `[\\\"getusername\\\", \\\"get\\\", \\\"user\\\", \\\"name\\\"]`.\\n\\n**Location:** `tokenizer.py` (split_identifiers parameter)\\n\\n### Semantic Fingerprint\\nVector representation of a text's semantic content for similarity comparison.\\n\\n**Location:** `fingerprint.py`\\n\\n---\\n\\n## Performance Concepts\\n\\n### O(1) ID Lookup\\nUsing `layer.get_by_id(col_id)` instead of iterating minicolumns. Critical for algorithm performance.\\n\\n### Query Cache\\nLRU cache storing query expansion results to avoid recomputation for repeated queries.\\n\\n**Location:** `processor.py:51-52`\\n\\n### Batch Processing\\nProcessing multiple queries or documents together to amortize overhead.\\n\\n**Functions:** `find_documents_batch()`, `find_passages_batch()`, `add_documents_batch()`\\n\\n---\\n\\n## File Locations Quick Reference\\n\\n| Term | Primary File |\\n|------|--------------|\\n| Minicolumn | `minicolumn.py` |\\n| Edge | `minicolumn.py` |\\n| HierarchicalLayer | `layers.py` |\\n| CorticalLayer | `layers.py` |\\n| PageRank | `analysis.py` |\\n| TF-IDF | `analysis.py` |\\n| Label Propagation | `analysis.py` |\\n| Query Expansion | `query.py` |\\n| Relation Extraction | `semantics.py` |\\n| Retrofitting | `semantics.py` |\\n| Tokenization | `tokenizer.py` |\\n| Fingerprint | `fingerprint.py` |\\n| Code Concepts | `code_concepts.py` |\\n\",",
        "      \"mtime\": 1765392877.0",
        "    },",
        "    {",
        "      \"op\": \"add\",",
        "      \"doc_id\": \"tests/test_layers.py\",",
        "      \"content\": \"\\\"\\\"\\\"Tests for Minicolumn, Edge, and Layer classes.\\\"\\\"\\\"\\n\\nimport unittest\\nimport sys\\nsys.path.insert(0, '..')\\n\\nfrom cortical import Minicolumn, Edge, CorticalLayer, HierarchicalLayer\\n\\n\\nclass TestMinicolumn(unittest.TestCase):\\n    \\\"\\\"\\\"Test the Minicolumn class.\\\"\\\"\\\"\\n    \\n    def test_creation(self):\\n        \\\"\\\"\\\"Test basic minicolumn creation.\\\"\\\"\\\"\\n        col = Minicolumn(\\\"L0_test\\\", \\\"test\\\", 0)\\n        self.assertEqual(col.id, \\\"L0_test\\\")\\n        self.assertEqual(col.content, \\\"test\\\")\\n        self.assertEqual(col.layer, 0)\\n        self.assertEqual(col.activation, 0.0)\\n    \\n    def test_lateral_connections(self):\\n        \\\"\\\"\\\"Test adding lateral connections.\\\"\\\"\\\"\\n        col = Minicolumn(\\\"L0_test\\\", \\\"test\\\", 0)\\n        col.add_lateral_connection(\\\"L0_other\\\", 0.5)\\n        self.assertIn(\\\"L0_other\\\", col.lateral_connections)\\n        self.assertEqual(col.lateral_connections[\\\"L0_other\\\"], 0.5)\\n    \\n    def test_connection_strengthening(self):\\n        \\\"\\\"\\\"Test that repeated connections strengthen.\\\"\\\"\\\"\\n        col = Minicolumn(\\\"L0_test\\\", \\\"test\\\", 0)\\n        col.add_lateral_connection(\\\"L0_other\\\", 0.5)\\n        col.add_lateral_connection(\\\"L0_other\\\", 0.3)\\n        self.assertEqual(col.lateral_connections[\\\"L0_other\\\"], 0.8)\\n    \\n    def test_connection_count(self):\\n        \\\"\\\"\\\"Test connection count.\\\"\\\"\\\"\\n        col = Minicolumn(\\\"L0_test\\\", \\\"test\\\", 0)\\n        col.add_lateral_connection(\\\"L0_a\\\", 1.0)\\n        col.add_lateral_connection(\\\"L0_b\\\", 1.0)\\n        self.assertEqual(col.connection_count(), 2)\\n    \\n    def test_document_ids(self):\\n        \\\"\\\"\\\"Test document ID tracking.\\\"\\\"\\\"\\n        col = Minicolumn(\\\"L0_test\\\", \\\"test\\\", 0)\\n        col.document_ids.add(\\\"doc1\\\")\\n        col.document_ids.add(\\\"doc2\\\")\\n        self.assertEqual(len(col.document_ids), 2)\\n    \\n    def test_serialization(self):\\n        \\\"\\\"\\\"Test to_dict and from_dict.\\\"\\\"\\\"\\n        col = Minicolumn(\\\"L0_test\\\", \\\"test\\\", 0)\\n        col.activation = 5.0\\n        col.occurrence_count = 10\\n        col.document_ids.add(\\\"doc1\\\")\\n        col.add_lateral_connection(\\\"L0_other\\\", 2.0)\\n        \\n        data = col.to_dict()\\n        restored = Minicolumn.from_dict(data)\\n        \\n        self.assertEqual(restored.id, col.id)\\n        self.assertEqual(restored.content, col.content)\\n        self.assertEqual(restored.activation, col.activation)\\n        self.assertEqual(restored.occurrence_count, col.occurrence_count)\\n\\n\\nclass TestHierarchicalLayer(unittest.TestCase):\\n    \\\"\\\"\\\"Test the HierarchicalLayer class.\\\"\\\"\\\"\\n    \\n    def test_creation(self):\\n        \\\"\\\"\\\"Test layer creation.\\\"\\\"\\\"\\n        layer = HierarchicalLayer(CorticalLayer.TOKENS)\\n        self.assertEqual(layer.level, CorticalLayer.TOKENS)\\n        self.assertEqual(len(layer.minicolumns), 0)\\n    \\n    def test_get_or_create(self):\\n        \\\"\\\"\\\"Test get_or_create_minicolumn.\\\"\\\"\\\"\\n        layer = HierarchicalLayer(CorticalLayer.TOKENS)\\n        col = layer.get_or_create_minicolumn(\\\"test\\\")\\n        self.assertEqual(col.content, \\\"test\\\")\\n        \\n        # Should return same column\\n        col2 = layer.get_or_create_minicolumn(\\\"test\\\")\\n        self.assertIs(col, col2)\\n    \\n    def test_get_minicolumn(self):\\n        \\\"\\\"\\\"Test get_minicolumn returns None for missing.\\\"\\\"\\\"\\n        layer = HierarchicalLayer(CorticalLayer.TOKENS)\\n        self.assertIsNone(layer.get_minicolumn(\\\"missing\\\"))\\n        \\n        layer.get_or_create_minicolumn(\\\"exists\\\")\\n        self.assertIsNotNone(layer.get_minicolumn(\\\"exists\\\"))\\n    \\n    def test_column_count(self):\\n        \\\"\\\"\\\"Test column counting.\\\"\\\"\\\"\\n        layer = HierarchicalLayer(CorticalLayer.TOKENS)\\n        layer.get_or_create_minicolumn(\\\"a\\\")\\n        layer.get_or_create_minicolumn(\\\"b\\\")\\n        layer.get_or_create_minicolumn(\\\"c\\\")\\n        self.assertEqual(layer.column_count(), 3)\\n    \\n    def test_iteration(self):\\n        \\\"\\\"\\\"Test iterating over layer.\\\"\\\"\\\"\\n        layer = HierarchicalLayer(CorticalLayer.TOKENS)\\n        layer.get_or_create_minicolumn(\\\"a\\\")\\n        layer.get_or_create_minicolumn(\\\"b\\\")\\n        \\n        contents = [col.content for col in layer]\\n        self.assertEqual(set(contents), {\\\"a\\\", \\\"b\\\"})\\n\\n\\nclass TestCorticalLayerEnum(unittest.TestCase):\\n    \\\"\\\"\\\"Test the CorticalLayer enum.\\\"\\\"\\\"\\n\\n    def test_values(self):\\n        \\\"\\\"\\\"Test layer values.\\\"\\\"\\\"\\n        self.assertEqual(CorticalLayer.TOKENS.value, 0)\\n        self.assertEqual(CorticalLayer.BIGRAMS.value, 1)\\n        self.assertEqual(CorticalLayer.CONCEPTS.value, 2)\\n        self.assertEqual(CorticalLayer.DOCUMENTS.value, 3)\\n\\n    def test_description(self):\\n        \\\"\\\"\\\"Test layer descriptions.\\\"\\\"\\\"\\n        self.assertIn(\\\"Token\\\", CorticalLayer.TOKENS.description)\\n        self.assertIn(\\\"Document\\\", CorticalLayer.DOCUMENTS.description)\\n\\n\\nclass TestEdge(unittest.TestCase):\\n    \\\"\\\"\\\"Test the Edge dataclass.\\\"\\\"\\\"\\n\\n    def test_edge_creation(self):\\n        \\\"\\\"\\\"Test basic Edge creation.\\\"\\\"\\\"\\n        edge = Edge(\\\"L0_target\\\", 0.5)\\n        self.assertEqual(edge.target_id, \\\"L0_target\\\")\\n        self.assertEqual(edge.weight, 0.5)\\n        self.assertEqual(edge.relation_type, 'co_occurrence')\\n        self.assertEqual(edge.confidence, 1.0)\\n        self.assertEqual(edge.source, 'corpus')\\n\\n    def test_edge_with_metadata(self):\\n        \\\"\\\"\\\"Test Edge creation with full metadata.\\\"\\\"\\\"\\n        edge = Edge(\\n            target_id=\\\"L0_target\\\",\\n            weight=0.8,\\n            relation_type='IsA',\\n            confidence=0.9,\\n            source='semantic'\\n        )\\n        self.assertEqual(edge.relation_type, 'IsA')\\n        self.assertEqual(edge.confidence, 0.9)\\n        self.assertEqual(edge.source, 'semantic')\\n\\n    def test_edge_serialization(self):\\n        \\\"\\\"\\\"Test Edge to_dict and from_dict.\\\"\\\"\\\"\\n        edge = Edge(\\\"L0_target\\\", 0.8, 'RelatedTo', 0.9, 'semantic')\\n        data = edge.to_dict()\\n\\n        restored = Edge.from_dict(data)\\n        self.assertEqual(restored.target_id, edge.target_id)\\n        self.assertEqual(restored.weight, edge.weight)\\n        self.assertEqual(restored.relation_type, edge.relation_type)\\n        self.assertEqual(restored.confidence, edge.confidence)\\n        self.assertEqual(restored.source, edge.source)\\n\\n    def test_edge_from_dict_defaults(self):\\n        \\\"\\\"\\\"Test Edge.from_dict with minimal data.\\\"\\\"\\\"\\n        data = {'target_id': 'L0_test'}\\n        edge = Edge.from_dict(data)\\n        self.assertEqual(edge.target_id, 'L0_test')\\n        self.assertEqual(edge.weight, 1.0)\\n        self.assertEqual(edge.relation_type, 'co_occurrence')\\n\\n\\nclass TestTypedConnections(unittest.TestCase):\\n    \\\"\\\"\\\"Test typed connection functionality on Minicolumn.\\\"\\\"\\\"\\n\\n    def test_add_typed_connection(self):\\n        \\\"\\\"\\\"Test adding a typed connection.\\\"\\\"\\\"\\n        col = Minicolumn(\\\"L0_test\\\", \\\"test\\\", 0)\\n        col.add_typed_connection(\\\"L0_other\\\", 0.5, relation_type='RelatedTo')\\n\\n        self.assertIn(\\\"L0_other\\\", col.typed_connections)\\n        edge = col.typed_connections[\\\"L0_other\\\"]\\n        self.assertEqual(edge.weight, 0.5)\\n        self.assertEqual(edge.relation_type, 'RelatedTo')\\n\\n    def test_typed_connection_also_updates_lateral(self):\\n        \\\"\\\"\\\"Test that typed connections also update lateral_connections.\\\"\\\"\\\"\\n        col = Minicolumn(\\\"L0_test\\\", \\\"test\\\", 0)\\n        col.add_typed_connection(\\\"L0_other\\\", 0.5, relation_type='RelatedTo')\\n\\n        # Should also be in lateral_connections\\n        self.assertIn(\\\"L0_other\\\", col.lateral_connections)\\n        self.assertEqual(col.lateral_connections[\\\"L0_other\\\"], 0.5)\\n\\n    def test_typed_connection_weight_accumulation(self):\\n        \\\"\\\"\\\"Test that typed connection weights accumulate.\\\"\\\"\\\"\\n        col = Minicolumn(\\\"L0_test\\\", \\\"test\\\", 0)\\n        col.add_typed_connection(\\\"L0_other\\\", 0.5, relation_type='RelatedTo')\\n        col.add_typed_connection(\\\"L0_other\\\", 0.3, relation_type='RelatedTo')\\n\\n        edge = col.typed_connections[\\\"L0_other\\\"]\\n        self.assertEqual(edge.weight, 0.8)\\n\\n    def test_typed_connection_relation_type_priority(self):\\n        \\\"\\\"\\\"Test that specific relation types take priority over co_occurrence.\\\"\\\"\\\"\\n        col = Minicolumn(\\\"L0_test\\\", \\\"test\\\", 0)\\n        col.add_typed_connection(\\\"L0_other\\\", 0.5, relation_type='co_occurrence')\\n        col.add_typed_connection(\\\"L0_other\\\", 0.3, relation_type='IsA')\\n\\n        edge = col.typed_connections[\\\"L0_other\\\"]\\n        self.assertEqual(edge.relation_type, 'IsA')\\n\\n    def test_typed_connection_source_priority(self):\\n        \\\"\\\"\\\"Test that semantic/inferred sources take priority over corpus.\\\"\\\"\\\"\\n        col = Minicolumn(\\\"L0_test\\\", \\\"test\\\", 0)\\n        col.add_typed_connection(\\\"L0_other\\\", 0.5, source='corpus')\\n        col.add_typed_connection(\\\"L0_other\\\", 0.3, source='semantic')\\n\\n        edge = col.typed_connections[\\\"L0_other\\\"]\\n        self.assertEqual(edge.source, 'semantic')\\n\\n    def test_typed_connection_confidence_max(self):\\n        \\\"\\\"\\\"Test that confidence uses max value.\\\"\\\"\\\"\\n        col = Minicolumn(\\\"L0_test\\\", \\\"test\\\", 0)\\n        col.add_typed_connection(\\\"L0_other\\\", 0.5, confidence=0.7)\\n        col.add_typed_connection(\\\"L0_other\\\", 0.3, confidence=0.9)\\n\\n        edge = col.typed_connections[\\\"L0_other\\\"]\\n        self.assertEqual(edge.confidence, 0.9)\\n\\n    def test_get_typed_connection(self):\\n        \\\"\\\"\\\"Test retrieving a typed connection.\\\"\\\"\\\"\\n        col = Minicolumn(\\\"L0_test\\\", \\\"test\\\", 0)\\n        col.add_typed_connection(\\\"L0_other\\\", 0.5, relation_type='IsA')\\n\\n        edge = col.get_typed_connection(\\\"L0_other\\\")\\n        self.assertIsNotNone(edge)\\n        self.assertEqual(edge.relation_type, 'IsA')\\n\\n        # Non-existent connection\\n        self.assertIsNone(col.get_typed_connection(\\\"L0_missing\\\"))\\n\\n    def test_get_connections_by_type(self):\\n        \\\"\\\"\\\"Test filtering connections by relation type.\\\"\\\"\\\"\\n        col = Minicolumn(\\\"L0_test\\\", \\\"test\\\", 0)\\n        col.add_typed_connection(\\\"L0_a\\\", 0.5, relation_type='IsA')\\n        col.add_typed_connection(\\\"L0_b\\\", 0.3, relation_type='IsA')\\n        col.add_typed_connection(\\\"L0_c\\\", 0.4, relation_type='PartOf')\\n\\n        is_a_edges = col.get_connections_by_type('IsA')\\n        self.assertEqual(len(is_a_edges), 2)\\n\\n        part_of_edges = col.get_connections_by_type('PartOf')\\n        self.assertEqual(len(part_of_edges), 1)\\n\\n    def test_get_connections_by_source(self):\\n        \\\"\\\"\\\"Test filtering connections by source.\\\"\\\"\\\"\\n        col = Minicolumn(\\\"L0_test\\\", \\\"test\\\", 0)\\n        col.add_typed_connection(\\\"L0_a\\\", 0.5, source='corpus')\\n        col.add_typed_connection(\\\"L0_b\\\", 0.3, source='semantic')\\n        col.add_typed_connection(\\\"L0_c\\\", 0.4, source='semantic')\\n\\n        corpus_edges = col.get_connections_by_source('corpus')\\n        self.assertEqual(len(corpus_edges), 1)\\n\\n        semantic_edges = col.get_connections_by_source('semantic')\\n        self.assertEqual(len(semantic_edges), 2)\\n\\n    def test_typed_connections_serialization(self):\\n        \\\"\\\"\\\"Test that typed connections survive serialization.\\\"\\\"\\\"\\n        col = Minicolumn(\\\"L0_test\\\", \\\"test\\\", 0)\\n        col.add_typed_connection(\\\"L0_other\\\", 0.8, relation_type='IsA', confidence=0.9)\\n\\n        data = col.to_dict()\\n        restored = Minicolumn.from_dict(data)\\n\\n        self.assertIn(\\\"L0_other\\\", restored.typed_connections)\\n        edge = restored.typed_connections[\\\"L0_other\\\"]\\n        self.assertEqual(edge.weight, 0.8)\\n        self.assertEqual(edge.relation_type, 'IsA')\\n        self.assertEqual(edge.confidence, 0.9)\\n\\n    def test_empty_typed_connections_serialization(self):\\n        \\\"\\\"\\\"Test serialization with no typed connections.\\\"\\\"\\\"\\n        col = Minicolumn(\\\"L0_test\\\", \\\"test\\\", 0)\\n\\n        data = col.to_dict()\\n        self.assertEqual(data['typed_connections'], {})\\n\\n        restored = Minicolumn.from_dict(data)\\n        self.assertEqual(restored.typed_connections, {})\\n\\n\\nif __name__ == \\\"__main__\\\":\\n    unittest.main(verbosity=2)\\n\",",
        "      \"mtime\": 1765375584.0",
        "    },",
        "    {",
        "      \"op\": \"add\",",
        "      \"doc_id\": \"tests/test_code_concepts.py\",",
        "      \"content\": \"\\\"\\\"\\\"\\nTests for code_concepts module.\\n\\nTests the programming concept groups and expansion functions\\nused for semantic code search.\\n\\\"\\\"\\\"\\n\\nimport unittest\\nfrom cortical.code_concepts import (\\n    CODE_CONCEPT_GROUPS,\\n    get_related_terms,\\n    expand_code_concepts,\\n    get_concept_group,\\n    list_concept_groups,\\n    get_group_terms,\\n)\\n\\n\\nclass TestCodeConceptGroups(unittest.TestCase):\\n    \\\"\\\"\\\"Test the CODE_CONCEPT_GROUPS structure.\\\"\\\"\\\"\\n\\n    def test_groups_exist(self):\\n        \\\"\\\"\\\"Test that concept groups are defined.\\\"\\\"\\\"\\n        self.assertGreater(len(CODE_CONCEPT_GROUPS), 0)\\n\\n    def test_retrieval_group(self):\\n        \\\"\\\"\\\"Test the retrieval concept group.\\\"\\\"\\\"\\n        self.assertIn('retrieval', CODE_CONCEPT_GROUPS)\\n        retrieval = CODE_CONCEPT_GROUPS['retrieval']\\n        self.assertIn('get', retrieval)\\n        self.assertIn('fetch', retrieval)\\n        self.assertIn('load', retrieval)\\n        self.assertIn('retrieve', retrieval)\\n\\n    def test_storage_group(self):\\n        \\\"\\\"\\\"Test the storage concept group.\\\"\\\"\\\"\\n        self.assertIn('storage', CODE_CONCEPT_GROUPS)\\n        storage = CODE_CONCEPT_GROUPS['storage']\\n        self.assertIn('save', storage)\\n        self.assertIn('store', storage)\\n        self.assertIn('write', storage)\\n        self.assertIn('persist', storage)\\n\\n    def test_auth_group(self):\\n        \\\"\\\"\\\"Test the authentication concept group.\\\"\\\"\\\"\\n        self.assertIn('auth', CODE_CONCEPT_GROUPS)\\n        auth = CODE_CONCEPT_GROUPS['auth']\\n        self.assertIn('login', auth)\\n        self.assertIn('credentials', auth)\\n        self.assertIn('token', auth)\\n\\n    def test_error_group(self):\\n        \\\"\\\"\\\"Test the error handling concept group.\\\"\\\"\\\"\\n        self.assertIn('error', CODE_CONCEPT_GROUPS)\\n        error = CODE_CONCEPT_GROUPS['error']\\n        self.assertIn('exception', error)\\n        self.assertIn('catch', error)\\n        self.assertIn('throw', error)\\n\\n    def test_groups_are_frozensets(self):\\n        \\\"\\\"\\\"Test that groups are immutable frozensets.\\\"\\\"\\\"\\n        for group_name, terms in CODE_CONCEPT_GROUPS.items():\\n            self.assertIsInstance(terms, frozenset)\\n\\n\\nclass TestGetRelatedTerms(unittest.TestCase):\\n    \\\"\\\"\\\"Test the get_related_terms function.\\\"\\\"\\\"\\n\\n    def test_fetch_related_terms(self):\\n        \\\"\\\"\\\"Test getting terms related to 'fetch'.\\\"\\\"\\\"\\n        related = get_related_terms('fetch', max_terms=10)\\n        self.assertIn('get', related)\\n        self.assertIn('load', related)\\n        self.assertNotIn('fetch', related)  # Should not include input term\\n\\n    def test_save_related_terms(self):\\n        \\\"\\\"\\\"Test getting terms related to 'save'.\\\"\\\"\\\"\\n        related = get_related_terms('save', max_terms=12)\\n        self.assertIn('store', related)\\n        self.assertIn('write', related)\\n        self.assertNotIn('save', related)\\n\\n    def test_unknown_term(self):\\n        \\\"\\\"\\\"Test with a term not in any concept group.\\\"\\\"\\\"\\n        related = get_related_terms('xyzabc123')\\n        self.assertEqual(related, [])\\n\\n    def test_max_terms_limit(self):\\n        \\\"\\\"\\\"Test that max_terms limits the output.\\\"\\\"\\\"\\n        related = get_related_terms('get', max_terms=3)\\n        self.assertLessEqual(len(related), 3)\\n\\n    def test_case_insensitive(self):\\n        \\\"\\\"\\\"Test that lookup is case insensitive.\\\"\\\"\\\"\\n        related_lower = get_related_terms('fetch')\\n        related_upper = get_related_terms('FETCH')\\n        related_mixed = get_related_terms('Fetch')\\n        self.assertEqual(set(related_lower), set(related_upper))\\n        self.assertEqual(set(related_lower), set(related_mixed))\\n\\n\\nclass TestExpandCodeConcepts(unittest.TestCase):\\n    \\\"\\\"\\\"Test the expand_code_concepts function.\\\"\\\"\\\"\\n\\n    def test_expand_single_term(self):\\n        \\\"\\\"\\\"Test expanding a single term.\\\"\\\"\\\"\\n        expanded = expand_code_concepts(['fetch'], max_expansions_per_term=10)\\n        self.assertIn('get', expanded)\\n        self.assertIn('load', expanded)\\n        self.assertNotIn('fetch', expanded)  # Input terms not in output\\n\\n    def test_expand_multiple_terms(self):\\n        \\\"\\\"\\\"Test expanding multiple terms.\\\"\\\"\\\"\\n        expanded = expand_code_concepts(['fetch', 'save'], max_expansions_per_term=10)\\n        # Should have terms from both retrieval and storage groups\\n        self.assertIn('get', expanded)\\n        self.assertIn('store', expanded)\\n\\n    def test_expand_empty_list(self):\\n        \\\"\\\"\\\"Test expanding empty list.\\\"\\\"\\\"\\n        expanded = expand_code_concepts([])\\n        self.assertEqual(expanded, {})\\n\\n    def test_expand_unknown_terms(self):\\n        \\\"\\\"\\\"Test expanding terms not in any group.\\\"\\\"\\\"\\n        expanded = expand_code_concepts(['xyzabc123'])\\n        self.assertEqual(expanded, {})\\n\\n    def test_weights_are_floats(self):\\n        \\\"\\\"\\\"Test that expansion weights are floats.\\\"\\\"\\\"\\n        expanded = expand_code_concepts(['fetch'])\\n        for term, weight in expanded.items():\\n            self.assertIsInstance(weight, float)\\n            self.assertGreater(weight, 0.0)\\n            self.assertLessEqual(weight, 1.0)\\n\\n    def test_custom_weight(self):\\n        \\\"\\\"\\\"Test custom weight parameter.\\\"\\\"\\\"\\n        expanded = expand_code_concepts(['fetch'], weight=0.8)\\n        for term, weight in expanded.items():\\n            self.assertEqual(weight, 0.8)\\n\\n    def test_max_expansions_per_term(self):\\n        \\\"\\\"\\\"Test limiting expansions per term.\\\"\\\"\\\"\\n        expanded = expand_code_concepts(['fetch'], max_expansions_per_term=2)\\n        self.assertLessEqual(len(expanded), 2)\\n\\n    def test_no_duplicate_original_terms(self):\\n        \\\"\\\"\\\"Test that original terms are not in expansions.\\\"\\\"\\\"\\n        terms = ['get', 'fetch', 'load']\\n        expanded = expand_code_concepts(terms)\\n        for term in terms:\\n            self.assertNotIn(term, expanded)\\n\\n\\nclass TestGetConceptGroup(unittest.TestCase):\\n    \\\"\\\"\\\"Test the get_concept_group function.\\\"\\\"\\\"\\n\\n    def test_single_group_membership(self):\\n        \\\"\\\"\\\"Test term that belongs to one group.\\\"\\\"\\\"\\n        groups = get_concept_group('fetch')\\n        self.assertIn('retrieval', groups)\\n\\n    def test_multiple_group_membership(self):\\n        \\\"\\\"\\\"Test term that might belong to multiple groups.\\\"\\\"\\\"\\n        # 'validate' is in both 'validation' and possibly 'testing'\\n        groups = get_concept_group('validate')\\n        self.assertIn('validation', groups)\\n\\n    def test_unknown_term(self):\\n        \\\"\\\"\\\"Test unknown term returns empty list.\\\"\\\"\\\"\\n        groups = get_concept_group('xyzabc123')\\n        self.assertEqual(groups, [])\\n\\n    def test_case_insensitive(self):\\n        \\\"\\\"\\\"Test case insensitive lookup.\\\"\\\"\\\"\\n        groups_lower = get_concept_group('fetch')\\n        groups_upper = get_concept_group('FETCH')\\n        self.assertEqual(groups_lower, groups_upper)\\n\\n\\nclass TestListConceptGroups(unittest.TestCase):\\n    \\\"\\\"\\\"Test the list_concept_groups function.\\\"\\\"\\\"\\n\\n    def test_returns_list(self):\\n        \\\"\\\"\\\"Test that function returns a list.\\\"\\\"\\\"\\n        groups = list_concept_groups()\\n        self.assertIsInstance(groups, list)\\n\\n    def test_contains_known_groups(self):\\n        \\\"\\\"\\\"Test that known groups are in the list.\\\"\\\"\\\"\\n        groups = list_concept_groups()\\n        self.assertIn('retrieval', groups)\\n        self.assertIn('storage', groups)\\n        self.assertIn('auth', groups)\\n        self.assertIn('error', groups)\\n\\n    def test_list_is_sorted(self):\\n        \\\"\\\"\\\"Test that list is sorted alphabetically.\\\"\\\"\\\"\\n        groups = list_concept_groups()\\n        self.assertEqual(groups, sorted(groups))\\n\\n\\nclass TestGetGroupTerms(unittest.TestCase):\\n    \\\"\\\"\\\"Test the get_group_terms function.\\\"\\\"\\\"\\n\\n    def test_retrieval_terms(self):\\n        \\\"\\\"\\\"Test getting terms from retrieval group.\\\"\\\"\\\"\\n        terms = get_group_terms('retrieval')\\n        self.assertIn('get', terms)\\n        self.assertIn('fetch', terms)\\n\\n    def test_unknown_group(self):\\n        \\\"\\\"\\\"Test unknown group returns empty list.\\\"\\\"\\\"\\n        terms = get_group_terms('nonexistent_group')\\n        self.assertEqual(terms, [])\\n\\n    def test_terms_are_sorted(self):\\n        \\\"\\\"\\\"Test that terms are sorted alphabetically.\\\"\\\"\\\"\\n        terms = get_group_terms('retrieval')\\n        self.assertEqual(terms, sorted(terms))\\n\\n\\nclass TestQueryExpansionIntegration(unittest.TestCase):\\n    \\\"\\\"\\\"Test code concepts integration with query expansion.\\\"\\\"\\\"\\n\\n    def setUp(self):\\n        \\\"\\\"\\\"Set up test processor.\\\"\\\"\\\"\\n        from cortical import CorticalTextProcessor\\n        self.processor = CorticalTextProcessor()\\n        # Use terms that won't be filtered as stop words\\n        self.processor.process_document(\\\"doc1\\\", \\\"\\\"\\\"\\n            The retrieve function obtains user information from the database.\\n            It will fetch data and load settings internally.\\n            The query method returns user profiles.\\n        \\\"\\\"\\\")\\n        self.processor.process_document(\\\"doc2\\\", \\\"\\\"\\\"\\n            The persist function stores user information to the database.\\n            It handles save operations and caching of user profiles.\\n            The store method writes data.\\n        \\\"\\\"\\\")\\n        self.processor.compute_all()\\n\\n    def test_expand_query_with_code_concepts(self):\\n        \\\"\\\"\\\"Test expand_query with use_code_concepts enabled.\\\"\\\"\\\"\\n        expanded = self.processor.expand_query(\\n            \\\"fetch data\\\",\\n            use_code_concepts=True\\n        )\\n        # Should include original terms\\n        self.assertIn('fetch', expanded)\\n        self.assertIn('data', expanded)\\n        # With code concepts enabled, should also include related terms\\n        # like 'load', 'retrieve' (if expansion finds them)\\n\\n    def test_expand_query_for_code(self):\\n        \\\"\\\"\\\"Test the expand_query_for_code convenience method.\\\"\\\"\\\"\\n        expanded = self.processor.expand_query_for_code(\\\"fetch data\\\")\\n        self.assertIn('fetch', expanded)\\n        self.assertIn('data', expanded)\\n\\n    def test_code_concepts_adds_synonyms(self):\\n        \\\"\\\"\\\"Test that code concepts adds programming synonyms.\\\"\\\"\\\"\\n        # Expand 'fetch' with code concepts - not a stop word\\n        expanded_with_code = self.processor.expand_query(\\n            \\\"fetch\\\",\\n            use_code_concepts=True,\\n            max_expansions=20\\n        )\\n        # Should include 'fetch' as original term\\n        self.assertIn('fetch', expanded_with_code)\\n        # Code concepts should add related retrieval terms\\n        # Check that at least one synonym is added\\n        retrieval_synonyms = {'load', 'retrieve', 'query', 'obtain'}\\n        has_synonym = any(s in expanded_with_code for s in retrieval_synonyms)\\n        self.assertTrue(has_synonym, f\\\"Expected synonyms in {expanded_with_code}\\\")\\n\\n    def test_code_concepts_disabled_by_default(self):\\n        \\\"\\\"\\\"Test that code concepts are disabled by default.\\\"\\\"\\\"\\n        # This test verifies the parameter exists and doesn't crash\\n        expanded_default = self.processor.expand_query(\\\"fetch\\\")\\n        self.assertIn('fetch', expanded_default)\\n\\n\\nif __name__ == '__main__':\\n    unittest.main()\\n\",",
        "      \"mtime\": 1765392877.0",
        "    },",
        "    {",
        "      \"op\": \"add\",",
        "      \"doc_id\": \"README.md\",",
        "      \"content\": \"# Cortical Text Processor\\n\\nA neocortex-inspired text processing library with **zero external dependencies** for semantic analysis, document retrieval, and knowledge gap detection.\\n\\n---\\n\\n> *\\\"What if we built a text search engine the way evolution built a brain?\\\"*\\n\\nYour visual cortex doesn't grep through pixels looking for cats. It builds hierarchies—edges become patterns, patterns become shapes, shapes become objects. This library applies the same principle to text.\\n\\nFeed it documents. It tokenizes them into \\\"minicolumns\\\" (Layer 0), connects co-occurring words through Hebbian learning (\\\"neurons that fire together, wire together\\\"), clusters them into concepts (Layer 2), and links documents by shared meaning (Layer 3). The result: a graph that understands your corpus well enough to expand queries, complete analogies, and tell you where your knowledge has gaps.\\n\\nNo PyTorch. No transformers. No API keys. Just 337 tests, 7000 lines of pure Python, and a data structure that would make a neuroscientist squint approvingly.\\n\\n---\\n\\n## Overview\\n\\nThis library provides a biologically-inspired approach to text processing, organizing information through a hierarchical structure similar to the visual cortex:\\n\\n| Layer | Name | Analogy | Purpose |\\n|-------|------|---------|---------|\\n| 0 | Tokens | V1 (edges) | Individual words |\\n| 1 | Bigrams | V2 (patterns) | Word pairs |\\n| 2 | Concepts | V4 (shapes) | Semantic clusters |\\n| 3 | Documents | IT (objects) | Full documents |\\n\\n## Key Features\\n\\n- **Hierarchical Processing**: Feedforward, feedback, and lateral connections like the neocortex\\n- **PageRank Importance**: Graph-based term importance with relation-weighted and cross-layer propagation\\n- **TF-IDF Weighting**: Statistical term distinctiveness with per-document occurrence tracking\\n- **Corpus-Derived Semantics**: Pattern-based commonsense relation extraction without external knowledge bases\\n- **Graph Embeddings**: Multiple embedding methods (adjacency, spectral, random walk) with semantic retrofitting\\n- **ConceptNet-Style Relations**: Typed edges (IsA, HasA, PartOf, etc.) with multi-hop inference\\n- **Concept Inheritance**: IsA hierarchy propagation for concept properties\\n- **Analogy Completion**: Relation matching and vector arithmetic for analogical reasoning\\n- **Gap Detection**: Find weak spots and isolated documents in your corpus\\n- **Query Expansion**: Smart retrieval with synonym handling and semantic relations\\n- **RAG System Support**: Chunk-level passage retrieval, document metadata, and multi-stage ranking\\n- **Zero Dependencies**: Pure Python, no pip installs required\\n\\n## Installation\\n\\nInstall from source:\\n\\n```bash\\ngit clone <repository-url>\\ncd cortical-text-processor\\npip install -e .\\n```\\n\\nOr simply copy the `cortical/` directory into your project—zero dependencies means no pip required.\\n\\n## Quick Start\\n\\nRun the showcase to see the processor analyze 92 documents covering everything from neural networks to medieval falconry:\\n\\n```bash\\npython showcase.py\\n```\\n\\n**Output:**\\n```\\n    ╔══════════════════════════════════════════════════════════════════════╗\\n    ║            🧠  CORTICAL TEXT PROCESSOR SHOWCASE  🧠                  ║\\n    ║     Mimicking how the neocortex processes and understands text       ║\\n    ╚══════════════════════════════════════════════════════════════════════╝\\n\\nLoading documents from: samples\\nProcessing through cortical hierarchy...\\n(Like visual information flowing V1 → V2 → V4 → IT)\\n\\n  📄 comprehensive_machine_learning (2445 words)\\n  📄 attention_mechanism_research   (644 words)\\n  📄 neural_network_optimization    (648 words)\\n  ... 89 more documents ...\\n\\n✓ Processed 92 documents\\n✓ Created 6,506 token minicolumns\\n✓ Created 20,114 bigram minicolumns\\n✓ Formed 116,332 lateral connections\\n\\n══════════════════════════════════════════════════════════════════════\\n                       KEY CONCEPTS (PageRank)\\n══════════════════════════════════════════════════════════════════════\\n\\nPageRank identifies central concepts - highly connected 'hub' words:\\n\\n  Rank  Concept            PageRank\\n  ─────────────────────────────────────────────\\n    1.  data               ████████████████████ 0.0046\\n    2.  model              ███████████████████░ 0.0044\\n    3.  learning           ██████████████████░░ 0.0041\\n    ...\\n\\n══════════════════════════════════════════════════════════════════════\\n                        QUERY DEMONSTRATION\\n══════════════════════════════════════════════════════════════════════\\n\\n🔍 Query: 'neural networks'\\n   Expanded with: knowledge, data, graph, network, deep, artificial\\n\\n   Top documents:\\n     • comprehensive_machine_learning (score: 26.384)\\n     • attention_mechanism_research (score: 19.178)\\n     • cortical_semantic_networks (score: 18.470)\\n```\\n\\n### Programmatic Usage\\n\\n```python\\nfrom cortical import CorticalTextProcessor\\n\\nprocessor = CorticalTextProcessor()\\n\\n# Add documents\\nprocessor.process_document(\\\"doc1\\\", \\\"Neural networks process information hierarchically.\\\")\\nprocessor.process_document(\\\"doc2\\\", \\\"The brain uses layers of neurons for processing.\\\")\\nprocessor.process_document(\\\"doc3\\\", \\\"Machine learning enables pattern recognition.\\\")\\n\\n# Build the network\\nprocessor.compute_all()\\n\\n# Query\\nresults = processor.find_documents_for_query(\\\"neural processing\\\")\\nprint(results)  # [('doc1', 0.877), ('doc2', 0.832)]\\n\\n# Save for later\\nprocessor.save(\\\"my_corpus.pkl\\\")\\n```\\n\\n## Core API\\n\\n### Document Processing\\n\\n```python\\nprocessor.process_document(doc_id, content, metadata=None)\\nprocessor.add_document_incremental(doc_id, content)  # Incremental indexing\\nprocessor.add_documents_batch([(doc_id, content, metadata), ...])  # Batch processing\\n```\\n\\n### Network Building\\n\\n```python\\n# All-in-one computation with connection strategies\\nprocessor.compute_all(\\n    verbose=False,\\n    connection_strategy='hybrid',  # 'document_overlap', 'semantic', 'embedding', 'hybrid'\\n    cluster_strictness=0.5,        # 0.0-1.0, lower = fewer, larger clusters\\n    bridge_weight=0.3              # 0.0-1.0, cross-document bridging\\n)\\n\\n# Individual computations\\nprocessor.propagate_activation()      # Spread activation\\nprocessor.compute_importance()        # PageRank scores\\nprocessor.compute_tfidf()             # TF-IDF weights\\nprocessor.build_concept_clusters()    # Cluster tokens\\nprocessor.compute_document_connections()  # Link documents\\nprocessor.compute_bigram_connections()    # Bigram lateral connections\\n```\\n\\n### Semantics & Embeddings\\n\\n```python\\nprocessor.extract_corpus_semantics()  # Extract relations\\nprocessor.retrofit_connections()      # Blend with semantics\\nprocessor.compute_graph_embeddings(dimensions=32, method='adjacency')\\nprocessor.retrofit_embeddings()       # Improve embeddings\\nprocessor.expand_query_multihop(query, max_hops=2)  # Multi-hop query expansion\\nprocessor.complete_analogy(a, b, c)   # Analogy completion (a:b :: c:?)\\n```\\n\\n### Query & Retrieval\\n\\n```python\\nprocessor.expand_query(text, max_expansions=10)  # Expand query\\nprocessor.find_documents_for_query(text, top_n=5)  # Search\\nprocessor.find_related_documents(doc_id)  # Related docs\\nprocessor.find_documents_batch(queries)  # Process multiple queries\\nprocessor.find_passages_for_query(query, top_n=5)  # Chunk-level RAG retrieval\\n```\\n\\n### Analysis\\n\\n```python\\nprocessor.analyze_knowledge_gaps()  # Find gaps\\nprocessor.detect_anomalies(threshold=0.1)  # Find outliers\\nprocessor.get_corpus_summary()      # Corpus statistics\\nprocessor.export_conceptnet_json(filepath)  # ConceptNet-style visualization export\\n```\\n\\n## Connection Strategies\\n\\nFor documents with different topics or minimal overlap, use connection strategies:\\n\\n```python\\n# Hybrid strategy combines all methods for maximum connectivity\\nprocessor.compute_all(\\n    connection_strategy='hybrid',\\n    cluster_strictness=0.5,\\n    bridge_weight=0.3\\n)\\n```\\n\\n| Strategy | Description |\\n|----------|-------------|\\n| `document_overlap` | Traditional Jaccard similarity (default) |\\n| `semantic` | Connect via semantic relations between members |\\n| `embedding` | Connect via embedding centroid similarity |\\n| `hybrid` | Combine all three for maximum connectivity |\\n\\n## Performance\\n\\nTested with 92 sample documents covering topics from neural networks to medieval falconry to sourdough breadmaking.\\n\\n| Metric | Value |\\n|--------|-------|\\n| Documents processed | 92 |\\n| Token minicolumns | 6,506 |\\n| Bigram minicolumns | 20,114 |\\n| Lateral connections | 116,332 |\\n| Test coverage | 337 tests passing |\\n| Graph algorithms | O(1) ID lookups |\\n\\n**What the processor discovers:**\\n- Most central concept: `data` (PageRank: 0.0046)\\n- Most distinctive terms: `gradient`, `pagerank`, `patent` (high TF-IDF, rare but meaningful)\\n- Most connected document: `comprehensive_machine_learning` (91 connections to other docs)\\n- Isolated outliers detected: `sumo_wrestling`, `medieval_falconry` (low similarity to corpus)\\n\\n## Package Structure\\n\\n```\\ncortical/\\n├── __init__.py      # Public API (v2.0.0)\\n├── processor.py     # Main orchestrator\\n├── tokenizer.py     # Tokenization + stemming\\n├── minicolumn.py    # Core data structure with typed edges\\n├── layers.py        # Hierarchical layers with O(1) lookups\\n├── analysis.py      # PageRank, TF-IDF, cross-layer propagation\\n├── semantics.py     # Semantic extraction, inference, analogy\\n├── embeddings.py    # Graph embeddings with retrofitting\\n├── query.py         # Search, retrieval, batch processing\\n├── gaps.py          # Gap detection and anomalies\\n└── persistence.py   # Save/load with full state\\n\\nevaluation/\\n└── evaluator.py     # Evaluation framework\\n\\ntests/               # 337 comprehensive tests\\nshowcase.py          # Interactive demonstration (run it!)\\nsamples/             # 92 documents: from quantum computing to cheese affinage\\n```\\n\\n## Development History\\n\\nThis project evolved through systematic improvements:\\n\\n1. **Initial Release**: Core hierarchical text processing\\n2. **Code Review & Fixes**: TF-IDF calculation, O(1) lookups, type annotations\\n3. **RAG Enhancements**: Chunk-level retrieval, metadata support, concept clustering\\n4. **ConceptNet Integration**: Typed edges, relation-weighted PageRank, multi-hop inference\\n5. **Connection Strategies**: Multiple strategies for Layer 2 concept connections\\n6. **Showcase & Polish**: Interactive demo with real corpus analysis\\n\\n## Running the Showcase\\n\\n```bash\\npython showcase.py\\n```\\n\\nThe showcase processes 92 diverse sample documents and demonstrates every major feature. Here's what you'll see:\\n\\n### Concept Associations (Hebbian Learning)\\n\\nThe processor discovers that `neural` connects to `networks` (weight: 23), `artificial` (7), `knowledge` (7)—while `bread` meekly connects to `beer`, `wine`, and `pyruvate` (weight: 1 each). Neurons that fire together really do wire together.\\n\\n### Query Expansion in Action\\n\\n```\\n🔍 Query: 'neural networks'\\n   Expanded with: knowledge, data, graph, network, deep, artificial\\n\\n   Top documents:\\n     • comprehensive_machine_learning (score: 26.384)\\n     • attention_mechanism_research (score: 19.178)\\n     • cortical_semantic_networks (score: 18.470)\\n```\\n\\n### The Polysemy Problem\\n\\nSearch for \\\"candle sticks\\\" and you'll find `candlestick_patterns` (trading charts) at the top—but also `letterpress_printing` (composing sticks) and `wine_tasting_vocabulary`. The query tokenizes to `['candle', 'sticks']`: \\\"candle\\\" matches the trading document (which discusses \\\"single candle patterns\\\"), while \\\"sticks\\\" matches the printing document. Classic information retrieval challenge: compound words fragment, partial matches surface, and the system can't read your mind about intent.\\n\\n### Knowledge Gap Detection\\n\\nThe analyzer flags `sumo_wrestling` and `medieval_falconry` as isolated documents—they don't fit well with the rest of the corpus. It also identifies weak topics: terms like `patent` appear in only 1 document. This is how you find holes in your knowledge base.\\n\\n## Running Tests\\n\\n```bash\\npython -m unittest discover -s tests -v\\n```\\n\\n## License\\n\\nMIT License\\n\",",
        "      \"mtime\": 1765375584.0",
        "    },",
        "    {",
        "      \"op\": \"add\",",
        "      \"doc_id\": \"cortical/gaps.py\",",
        "      \"content\": \"\\\"\\\"\\\"\\nGaps Module\\n===========\\n\\nKnowledge gap detection and anomaly analysis.\\n\\nIdentifies:\\n- Isolated documents that don't connect well to the corpus\\n- Weakly covered topics (few documents)\\n- Bridge opportunities between document clusters\\n- Anomalous documents that may be miscategorized\\n\\\"\\\"\\\"\\n\\nimport math\\nfrom typing import Dict, List, Tuple, Set, Optional\\nfrom collections import defaultdict\\n\\nfrom .layers import CorticalLayer, HierarchicalLayer\\nfrom .analysis import cosine_similarity\\n\\n\\n# =============================================================================\\n# Gap Detection Thresholds\\n# =============================================================================\\n# These thresholds were empirically determined for typical text corpora.\\n# They may need adjustment for specialized domains or very small/large corpora.\\n\\n# Isolation threshold: Documents with average cosine similarity below this value\\n# are considered \\\"isolated\\\" from the rest of the corpus. Value of 0.02 means\\n# the document shares very little vocabulary overlap with other documents.\\n# Typical range: 0.01 (very strict) to 0.05 (more lenient)\\nISOLATION_THRESHOLD = 0.02\\n\\n# Well-connected threshold: Documents with average similarity above this value\\n# are considered well-integrated into the corpus.\\n# Typical range: 0.02 to 0.05\\nWELL_CONNECTED_THRESHOLD = 0.03\\n\\n# Weak topic TF-IDF threshold: Terms with TF-IDF above this value are considered\\n# significant enough to represent a \\\"topic\\\". Lower values include more common terms.\\n# Typical range: 0.001 (include more terms) to 0.01 (only distinctive terms)\\nWEAK_TOPIC_TFIDF_THRESHOLD = 0.005\\n\\n# Bridge opportunity range: Document pairs with similarity in this range are\\n# candidates for \\\"bridging\\\" - they share some vocabulary but aren't closely related.\\n# Too similar (>0.03) means they're already well-connected; too dissimilar (<0.005)\\n# means there's no common ground to bridge.\\nBRIDGE_SIMILARITY_MIN = 0.005\\nBRIDGE_SIMILARITY_MAX = 0.03\\n\\n\\ndef analyze_knowledge_gaps(\\n    layers: Dict[CorticalLayer, HierarchicalLayer],\\n    documents: Dict[str, str]\\n) -> Dict:\\n    \\\"\\\"\\\"\\n    Analyze the corpus to identify potential knowledge gaps.\\n    \\n    Args:\\n        layers: Dictionary of layers\\n        documents: Dictionary of documents\\n        \\n    Returns:\\n        Dict with gap analysis results including isolated_documents,\\n        weak_topics, bridge_opportunities, coverage_score, etc.\\n    \\\"\\\"\\\"\\n    layer0 = layers[CorticalLayer.TOKENS]\\n    doc_ids = list(documents.keys())\\n    \\n    # 1. Find isolated documents\\n    isolated_docs = []\\n    doc_similarities: Dict[str, Dict[str, float]] = {}\\n    \\n    for doc_id in doc_ids:\\n        doc_vector = {col.content: col.tfidf_per_doc[doc_id] \\n                     for col in layer0.minicolumns.values() \\n                     if doc_id in col.tfidf_per_doc}\\n        \\n        similarities = []\\n        for other_id in doc_ids:\\n            if other_id != doc_id:\\n                other_vector = {col.content: col.tfidf_per_doc[other_id]\\n                               for col in layer0.minicolumns.values()\\n                               if other_id in col.tfidf_per_doc}\\n                sim = cosine_similarity(doc_vector, other_vector)\\n                similarities.append((other_id, sim))\\n        \\n        avg_sim = sum(s for _, s in similarities) / len(similarities) if similarities else 0\\n        max_sim = max((s for _, s in similarities), default=0)\\n        doc_similarities[doc_id] = {'avg': avg_sim, 'max': max_sim}\\n        \\n        if avg_sim < ISOLATION_THRESHOLD:\\n            isolated_docs.append({\\n                'doc_id': doc_id,\\n                'avg_similarity': avg_sim,\\n                'max_similarity': max_sim,\\n                'most_similar': max(similarities, key=lambda x: x[1])[0] if similarities else None\\n            })\\n    \\n    isolated_docs.sort(key=lambda x: x['avg_similarity'])\\n    \\n    # 2. Find weakly covered topics\\n    weak_topics = []\\n    for col in layer0.minicolumns.values():\\n        doc_count = len(col.document_ids)\\n        if col.tfidf > WEAK_TOPIC_TFIDF_THRESHOLD and 1 <= doc_count <= 2:\\n            weak_topics.append({\\n                'term': col.content,\\n                'tfidf': col.tfidf,\\n                'doc_count': doc_count,\\n                'documents': list(col.document_ids),\\n                'pagerank': col.pagerank\\n            })\\n    weak_topics.sort(key=lambda x: x['tfidf'] * x['pagerank'], reverse=True)\\n    \\n    # 3. Find bridge opportunities\\n    bridge_opportunities = []\\n    for i, doc1 in enumerate(doc_ids):\\n        vec1 = {col.content: col.tfidf_per_doc[doc1] \\n               for col in layer0.minicolumns.values() \\n               if doc1 in col.tfidf_per_doc}\\n        \\n        for doc2 in doc_ids[i+1:]:\\n            vec2 = {col.content: col.tfidf_per_doc[doc2]\\n                   for col in layer0.minicolumns.values()\\n                   if doc2 in col.tfidf_per_doc}\\n            \\n            sim = cosine_similarity(vec1, vec2)\\n            if BRIDGE_SIMILARITY_MIN < sim < BRIDGE_SIMILARITY_MAX:\\n                shared = set(vec1.keys()) & set(vec2.keys())\\n                bridge_opportunities.append({\\n                    'doc1': doc1,\\n                    'doc2': doc2,\\n                    'similarity': sim,\\n                    'shared_terms': list(shared)[:5]\\n                })\\n    \\n    bridge_opportunities.sort(key=lambda x: x['similarity'], reverse=True)\\n    \\n    # 4. Connector terms\\n    connector_terms = []\\n    isolated_doc_ids = {d['doc_id'] for d in isolated_docs[:5]}\\n    if isolated_doc_ids:\\n        for col in layer0.minicolumns.values():\\n            in_isolated = col.document_ids & isolated_doc_ids\\n            in_connected = col.document_ids - isolated_doc_ids\\n            if in_isolated and in_connected:\\n                connector_terms.append({\\n                    'term': col.content,\\n                    'bridges_isolated': list(in_isolated),\\n                    'connects_to': list(in_connected)[:3],\\n                    'pagerank': col.pagerank\\n                })\\n    connector_terms.sort(key=lambda x: len(x['bridges_isolated']), reverse=True)\\n    \\n    # 5. Coverage metrics\\n    total_docs = len(doc_ids)\\n    isolated_count = len([d for d in doc_similarities.values() if d['avg'] < ISOLATION_THRESHOLD])\\n    well_connected = len([d for d in doc_similarities.values() if d['avg'] >= WELL_CONNECTED_THRESHOLD])\\n    coverage_score = well_connected / total_docs if total_docs > 0 else 0\\n    \\n    all_avg_sims = [d['avg'] for d in doc_similarities.values()]\\n    connectivity_score = sum(all_avg_sims) / len(all_avg_sims) if all_avg_sims else 0\\n    \\n    return {\\n        'isolated_documents': isolated_docs[:10],\\n        'weak_topics': weak_topics[:10],\\n        'bridge_opportunities': bridge_opportunities[:10],\\n        'connector_terms': connector_terms[:10],\\n        'coverage_score': coverage_score,\\n        'connectivity_score': connectivity_score,\\n        'summary': {\\n            'total_documents': total_docs,\\n            'isolated_count': isolated_count,\\n            'well_connected_count': well_connected,\\n            'weak_topic_count': len(weak_topics)\\n        }\\n    }\\n\\n\\ndef detect_anomalies(\\n    layers: Dict[CorticalLayer, HierarchicalLayer],\\n    documents: Dict[str, str],\\n    threshold: float = 0.3\\n) -> List[Dict]:\\n    \\\"\\\"\\\"\\n    Detect documents that don't fit well with the rest of the corpus.\\n    \\n    Args:\\n        layers: Dictionary of layers\\n        documents: Dictionary of documents\\n        threshold: Similarity threshold below which docs are anomalies\\n        \\n    Returns:\\n        List of anomaly reports with explanations\\n    \\\"\\\"\\\"\\n    layer0 = layers[CorticalLayer.TOKENS]\\n    layer3 = layers.get(CorticalLayer.DOCUMENTS)\\n    anomalies = []\\n    \\n    for doc_id in documents:\\n        doc_col = layer3.get_minicolumn(doc_id) if layer3 else None\\n        connection_count = doc_col.connection_count() if doc_col else 0\\n        \\n        doc_vector = {col.content: col.tfidf_per_doc[doc_id]\\n                     for col in layer0.minicolumns.values()\\n                     if doc_id in col.tfidf_per_doc}\\n        \\n        similarities = []\\n        for other_id in documents:\\n            if other_id != doc_id:\\n                other_vector = {col.content: col.tfidf_per_doc[other_id]\\n                               for col in layer0.minicolumns.values()\\n                               if other_id in col.tfidf_per_doc}\\n                similarities.append(cosine_similarity(doc_vector, other_vector))\\n        \\n        avg_similarity = sum(similarities) / len(similarities) if similarities else 0\\n        max_similarity = max(similarities) if similarities else 0\\n        \\n        is_anomaly = False\\n        reasons = []\\n        \\n        if avg_similarity < threshold:\\n            is_anomaly = True\\n            reasons.append(f\\\"Low average similarity ({avg_similarity:.1%})\\\")\\n        if connection_count <= 1:\\n            is_anomaly = True\\n            reasons.append(f\\\"Few document connections ({connection_count})\\\")\\n        if max_similarity < threshold * 1.5:\\n            is_anomaly = True\\n            reasons.append(\\\"No closely related documents\\\")\\n        \\n        if is_anomaly:\\n            sig_terms = sorted(doc_vector.items(), key=lambda x: x[1], reverse=True)[:5]\\n            anomalies.append({\\n                'doc_id': doc_id,\\n                'avg_similarity': avg_similarity,\\n                'max_similarity': max_similarity,\\n                'connections': connection_count,\\n                'reasons': reasons,\\n                'distinctive_terms': [t for t, _ in sig_terms]\\n            })\\n    \\n    anomalies.sort(key=lambda x: x['avg_similarity'])\\n    return anomalies\\n\",",
        "      \"mtime\": 1765375584.0",
        "    },",
        "    {",
        "      \"op\": \"add\",",
        "      \"doc_id\": \"CLAUDE.md\",",
        "      \"content\": \"# CLAUDE.md - Cortical Text Processor Development Guide\\n\\n## Persona\\n\\nYou are a **senior computational neuroscience engineer** with deep expertise in:\\n- Information retrieval algorithms (PageRank, TF-IDF, BM25)\\n- Graph theory and network analysis\\n- Natural language processing without ML dependencies\\n- Biologically-inspired computing architectures\\n- Python best practices and clean code principles\\n\\nApproach every task with **scientific rigor** - verify claims, check edge cases, and be skeptical of assumptions. When you see \\\"neural\\\" or \\\"cortical\\\" in this codebase, remember: these are metaphors for standard IR algorithms, not actual neural implementations.\\n\\n---\\n\\n## Project Overview\\n\\n**Cortical Text Processor** is a zero-dependency Python library for hierarchical text analysis. It organizes text through 4 layers inspired by visual cortex organization:\\n\\n```\\nLayer 0 (TOKENS)    → Individual words        [V1 analogy: edges]\\nLayer 1 (BIGRAMS)   → Word pairs              [V2 analogy: patterns]\\nLayer 2 (CONCEPTS)  → Semantic clusters       [V4 analogy: shapes]\\nLayer 3 (DOCUMENTS) → Full documents          [IT analogy: objects]\\n```\\n\\n**Core algorithms:**\\n- **PageRank** for term importance (`analysis.py`)\\n- **TF-IDF** for document relevance (`analysis.py`)\\n- **Label propagation** for concept clustering (`analysis.py`)\\n- **Co-occurrence counting** for lateral connections (\\\"Hebbian learning\\\")\\n- **Pattern-based relation extraction** for semantic relations (`semantics.py`)\\n\\n---\\n\\n## Architecture Map\\n\\n```\\ncortical/\\n├── processor.py      # Main orchestrator (1,596 lines) - START HERE\\n│                     # CorticalTextProcessor is the public API\\n├── analysis.py       # Graph algorithms: PageRank, TF-IDF, clustering\\n├── query.py          # Search, retrieval, query expansion, analogies\\n├── semantics.py      # Relation extraction, inheritance, retrofitting\\n├── minicolumn.py     # Core data structure with typed Edge connections\\n├── layers.py         # HierarchicalLayer with O(1) ID lookups via _id_index\\n├── embeddings.py     # Graph embeddings (adjacency, spectral, random walk)\\n├── gaps.py           # Knowledge gap detection and anomaly analysis\\n├── persistence.py    # Save/load with full state preservation\\n└── tokenizer.py      # Tokenization, stemming, stop word removal\\n```\\n\\n**Key data structures:**\\n- `Minicolumn`: Core unit with `lateral_connections`, `typed_connections`, `feedforward_connections`, `feedback_connections`\\n- `Edge`: Typed connection with `relation_type`, `weight`, `confidence`, `source`\\n- `HierarchicalLayer`: Container with `minicolumns` dict and `_id_index` for O(1) lookups\\n\\n---\\n\\n## Critical Knowledge\\n\\n### Fixed Bugs (2025-12-10)\\nThe bigram separator mismatch bugs in `query.py:1442-1468` and `analysis.py:927` have been **fixed**. Bigrams now correctly use space separators throughout the codebase.\\n\\n### Important Implementation Details\\n\\n1. **Bigrams use SPACE separators** (from `tokenizer.py:179`):\\n   ```python\\n   ' '.join(tokens[i:i+n])  # \\\"neural networks\\\", not \\\"neural_networks\\\"\\n   ```\\n\\n2. **Global `col.tfidf` is NOT per-document TF-IDF** - it uses total corpus occurrence count. Use `col.tfidf_per_doc[doc_id]` for true per-document TF-IDF.\\n\\n3. **O(1) ID lookups**: Always use `layer.get_by_id(col_id)` instead of iterating `layer.minicolumns`. The `_id_index` provides O(1) access.\\n\\n4. **Layer enum values**:\\n   ```python\\n   CorticalLayer.TOKENS = 0\\n   CorticalLayer.BIGRAMS = 1\\n   CorticalLayer.CONCEPTS = 2\\n   CorticalLayer.DOCUMENTS = 3\\n   ```\\n\\n5. **Minicolumn IDs follow pattern**: `L{layer}_{content}` (e.g., `L0_neural`, `L1_neural networks`)\\n\\n---\\n\\n## Development Workflow\\n\\n### Before Writing Code\\n\\n1. **Read the relevant module** - understand existing patterns\\n2. **Check TASK_LIST.md** - see if work is already planned/done\\n3. **Run tests first** to establish baseline:\\n   ```bash\\n   python -m unittest discover -s tests -v\\n   ```\\n4. **Trace data flow** - follow how data moves through layers\\n\\n### When Implementing Features\\n\\n1. **Follow existing patterns** - this codebase is consistent\\n2. **Add type hints** - the codebase uses them extensively\\n3. **Write docstrings** - Google style with Args/Returns sections\\n4. **Update staleness tracking** if adding new computation:\\n   ```python\\n   # In processor.py, add constant:\\n   COMP_YOUR_FEATURE = 'your_feature'\\n   # Mark stale in _mark_all_stale()\\n   # Mark fresh after computation\\n   ```\\n\\n### After Writing Code\\n\\n1. **Run the full test suite**:\\n   ```bash\\n   python -m unittest discover -s tests -v\\n   ```\\n2. **Run the showcase** to verify integration:\\n   ```bash\\n   python showcase.py\\n   ```\\n3. **Check for regressions** in related functionality\\n\\n---\\n\\n## Testing Patterns\\n\\nTests follow `unittest` conventions in `tests/` directory:\\n\\n```python\\nclass TestYourFeature(unittest.TestCase):\\n    def setUp(self):\\n        self.processor = CorticalTextProcessor()\\n        self.processor.process_document(\\\"doc1\\\", \\\"Test content here.\\\")\\n        self.processor.compute_all()\\n\\n    def test_feature_basic(self):\\n        \\\"\\\"\\\"Test basic functionality.\\\"\\\"\\\"\\n        result = self.processor.your_feature()\\n        self.assertIsNotNone(result)\\n\\n    def test_feature_empty_corpus(self):\\n        \\\"\\\"\\\"Test with empty processor.\\\"\\\"\\\"\\n        empty = CorticalTextProcessor()\\n        result = empty.your_feature()\\n        self.assertEqual(result, expected_empty_value)\\n```\\n\\n**Always test:**\\n- Empty corpus case\\n- Single document case\\n- Multiple documents case\\n- Edge cases specific to your feature\\n\\n---\\n\\n## Common Tasks\\n\\n### Adding a New Analysis Function\\n\\n1. Add function to `analysis.py` with proper signature:\\n   ```python\\n   def compute_your_analysis(\\n       layers: Dict[CorticalLayer, HierarchicalLayer],\\n       **kwargs\\n   ) -> Dict[str, Any]:\\n       \\\"\\\"\\\"Your analysis description.\\\"\\\"\\\"\\n       layer0 = layers[CorticalLayer.TOKENS]\\n       # Implementation\\n       return {'result': ..., 'stats': ...}\\n   ```\\n\\n2. Add wrapper method to `CorticalTextProcessor` in `processor.py`:\\n   ```python\\n   def compute_your_analysis(self, **kwargs) -> Dict[str, Any]:\\n       \\\"\\\"\\\"Wrapper with docstring.\\\"\\\"\\\"\\n       return compute_your_analysis(self.layers, **kwargs)\\n   ```\\n\\n3. Add tests in `tests/test_analysis.py`\\n\\n### Adding a New Query Function\\n\\n1. Add to `query.py` following existing patterns\\n2. Use `get_expanded_query_terms()` helper for query expansion\\n3. Use `layer.get_by_id()` for O(1) lookups, not iteration\\n4. Add wrapper to `processor.py`\\n5. Add tests in `tests/test_processor.py`\\n\\n### Modifying Minicolumn Structure\\n\\n1. Update `Minicolumn` class in `minicolumn.py`\\n2. Update `to_dict()` and `from_dict()` for persistence\\n3. Update `__slots__` if adding new fields\\n4. Increment state version in `persistence.py` if breaking change\\n5. Add migration logic for backward compatibility\\n\\n---\\n\\n## Code Style Guidelines\\n\\n```python\\n# Imports: stdlib, then local\\nfrom typing import Dict, List, Optional, Tuple\\nfrom collections import defaultdict\\n\\nfrom .layers import CorticalLayer, HierarchicalLayer\\nfrom .minicolumn import Minicolumn\\n\\n# Type hints on all public functions\\ndef find_documents(\\n    query: str,\\n    layers: Dict[CorticalLayer, HierarchicalLayer],\\n    top_n: int = 5\\n) -> List[Tuple[str, float]]:\\n    \\\"\\\"\\\"\\n    Find documents matching query.\\n\\n    Args:\\n        query: Search query string\\n        layers: Dictionary of hierarchical layers\\n        top_n: Number of results to return\\n\\n    Returns:\\n        List of (doc_id, score) tuples sorted by relevance\\n    \\\"\\\"\\\"\\n    # Implementation\\n```\\n\\n---\\n\\n## Performance Considerations\\n\\n1. **Use `get_by_id()` for ID lookups** - O(1) vs O(n) iteration\\n2. **Batch document additions** with `add_documents_batch()` for bulk imports\\n3. **Use incremental updates** with `add_document_incremental()` for live systems\\n4. **Cache query expansions** when processing multiple similar queries\\n5. **Pre-compute chunks** in `find_passages_batch()` to avoid redundant work\\n6. **Use `fast_find_documents()`** for ~2-3x faster search on large corpora\\n7. **Pre-build index** with `build_search_index()` for fastest repeated queries\\n\\n---\\n\\n## Code Search Capabilities\\n\\n### Code-Aware Tokenization\\n```python\\n# Enable identifier splitting for code search\\ntokenizer = Tokenizer(split_identifiers=True)\\ntokens = tokenizer.tokenize(\\\"getUserCredentials\\\")\\n# ['getusercredentials', 'get', 'user', 'credentials']\\n```\\n\\n### Programming Concept Expansion\\n```python\\n# Expand queries with programming synonyms (get/fetch/load)\\nresults = processor.expand_query(\\\"fetch data\\\", use_code_concepts=True)\\n# Or use the convenience method\\nresults = processor.expand_query_for_code(\\\"fetch data\\\")\\n```\\n\\n### Intent-Based Search\\n```python\\n# Parse natural language queries\\nparsed = processor.parse_intent_query(\\\"where do we handle authentication?\\\")\\n# {'intent': 'location', 'action': 'handle', 'subject': 'authentication', ...}\\n\\n# Search with intent understanding\\nresults = processor.search_by_intent(\\\"how do we validate input?\\\")\\n```\\n\\n### Semantic Fingerprinting\\n```python\\n# Compare code similarity\\nfp1 = processor.get_fingerprint(code_block_1)\\nfp2 = processor.get_fingerprint(code_block_2)\\ncomparison = processor.compare_fingerprints(fp1, fp2)\\nexplanation = processor.explain_similarity(fp1, fp2)\\n```\\n\\n### Fast Search\\n```python\\n# Fast document search (~2-3x faster)\\nresults = processor.fast_find_documents(\\\"authentication\\\")\\n\\n# Pre-built index for fastest search\\nindex = processor.build_search_index()\\nresults = processor.search_with_index(\\\"query\\\", index)\\n```\\n\\n---\\n\\n## Debugging Tips\\n\\n### Inspecting Layer State\\n```python\\nprocessor = CorticalTextProcessor()\\nprocessor.process_document(\\\"test\\\", \\\"Neural networks process data.\\\")\\nprocessor.compute_all()\\n\\n# Check layer sizes\\nfor layer_enum, layer in processor.layers.items():\\n    print(f\\\"{layer_enum.name}: {layer.column_count()} minicolumns\\\")\\n\\n# Inspect a specific minicolumn\\ncol = processor.layers[CorticalLayer.TOKENS].get_minicolumn(\\\"neural\\\")\\nprint(f\\\"PageRank: {col.pagerank}\\\")\\nprint(f\\\"TF-IDF: {col.tfidf}\\\")\\nprint(f\\\"Connections: {len(col.lateral_connections)}\\\")\\nprint(f\\\"Documents: {col.document_ids}\\\")\\n```\\n\\n### Tracing Query Expansion\\n```python\\nexpanded = processor.expand_query(\\\"neural networks\\\", max_expansions=10)\\nfor term, weight in sorted(expanded.items(), key=lambda x: -x[1]):\\n    print(f\\\"  {term}: {weight:.3f}\\\")\\n```\\n\\n### Checking Semantic Relations\\n```python\\nprocessor.extract_corpus_semantics()\\nfor t1, rel, t2, weight in processor.semantic_relations[:10]:\\n    print(f\\\"{t1} --{rel}--> {t2} ({weight:.2f})\\\")\\n```\\n\\n---\\n\\n## Quick Reference\\n\\n| Task | Command/Method |\\n|------|----------------|\\n| Process document | `processor.process_document(id, text)` |\\n| Build network | `processor.compute_all()` |\\n| Search | `processor.find_documents_for_query(query)` |\\n| Fast search | `processor.fast_find_documents(query)` |\\n| Code search | `processor.expand_query_for_code(query)` |\\n| Intent search | `processor.search_by_intent(\\\"where do we...\\\")` |\\n| RAG passages | `processor.find_passages_for_query(query)` |\\n| Fingerprint | `processor.get_fingerprint(text)` |\\n| Compare | `processor.compare_fingerprints(fp1, fp2)` |\\n| Save state | `processor.save(\\\"corpus.pkl\\\")` |\\n| Load state | `processor = CorticalTextProcessor.load(\\\"corpus.pkl\\\")` |\\n| Run tests | `python -m unittest discover -s tests -v` |\\n| Run showcase | `python showcase.py` |\\n\\n---\\n\\n## Dog-Fooding: Search the Codebase\\n\\nThe Cortical Text Processor can index and search its own codebase, providing semantic search capabilities during development.\\n\\n### Quick Start\\n\\n```bash\\n# Index the codebase (creates corpus_dev.pkl, ~2s)\\npython scripts/index_codebase.py\\n\\n# Incremental update (only changed files)\\npython scripts/index_codebase.py --incremental\\n\\n# Search for code\\npython scripts/search_codebase.py \\\"PageRank algorithm\\\"\\npython scripts/search_codebase.py \\\"bigram separator\\\" --verbose\\npython scripts/search_codebase.py --interactive\\n```\\n\\n### Claude Skills\\n\\nTwo skills are available in `.claude/skills/`:\\n\\n1. **codebase-search**: Search the indexed codebase for code patterns and implementations\\n2. **corpus-indexer**: Re-index the codebase after making changes\\n\\n### Indexer Options\\n\\n| Option | Description |\\n|--------|-------------|\\n| `--incremental`, `-i` | Only re-index changed files (fastest) |\\n| `--status`, `-s` | Show what would change without indexing |\\n| `--force`, `-f` | Force full rebuild |\\n| `--log FILE` | Write detailed log to file |\\n| `--verbose`, `-v` | Show per-file progress |\\n\\n### Search Options\\n\\n| Option | Description |\\n|--------|-------------|\\n| `--top N` | Number of results (default: 5) |\\n| `--verbose` | Show full passage text |\\n| `--expand` | Show query expansion terms |\\n| `--interactive` | Interactive search mode |\\n\\n### Interactive Mode Commands\\n\\n| Command | Description |\\n|---------|-------------|\\n| `/expand <query>` | Show query expansion |\\n| `/concepts` | List concept clusters |\\n| `/stats` | Show corpus statistics |\\n| `/quit` | Exit interactive mode |\\n\\n### Example Queries\\n\\n```bash\\n# Find how PageRank is implemented\\npython scripts/search_codebase.py \\\"compute pagerank damping factor\\\"\\n\\n# Find test patterns\\npython scripts/search_codebase.py \\\"unittest setUp processor\\\"\\n\\n# Explore query expansion code\\npython scripts/search_codebase.py \\\"expand query semantic lateral\\\"\\n```\\n\\n---\\n\\n## File Quick Links\\n\\n- **Main API**: `cortical/processor.py` - `CorticalTextProcessor` class\\n- **Graph algorithms**: `cortical/analysis.py` - PageRank, TF-IDF, clustering\\n- **Search**: `cortical/query.py` - query expansion, document retrieval\\n- **Data structures**: `cortical/minicolumn.py` - `Minicolumn`, `Edge`\\n- **Tests**: `tests/test_processor.py` - most comprehensive test file\\n- **Demo**: `showcase.py` - interactive demonstration\\n\\n---\\n\\n*Remember: Be skeptical, verify assumptions, and always run the tests.*\\n\",",
        "      \"mtime\": 1765399191.0",
        "    },",
        "    {",
        "      \"op\": \"add\",",
        "      \"doc_id\": \"docs/architecture.md\",",
        "      \"content\": \"# System Architecture\\n\\nThis document describes the 4-layer hierarchical architecture of the Cortical Text Processor. The design is inspired by visual cortex organization, processing text at increasing levels of abstraction.\\n\\n## Layer Overview\\n\\n```\\nLayer 3 (DOCUMENTS)  ← Full documents        [IT analogy: objects]\\n    ↑↓\\nLayer 2 (CONCEPTS)   ← Semantic clusters     [V4 analogy: shapes]\\n    ↑↓\\nLayer 1 (BIGRAMS)    ← Word pairs            [V2 analogy: patterns]\\n    ↑↓\\nLayer 0 (TOKENS)     ← Individual words      [V1 analogy: edges]\\n```\\n\\nInformation flows both upward (abstraction) and downward (grounding) through feedforward and feedback connections.\\n\\n---\\n\\n## Core Data Structures\\n\\n### CorticalLayer Enum\\n\\n**Location:** `layers.py:21-56`\\n\\n```python\\nclass CorticalLayer(Enum):\\n    TOKENS = 0      # Individual words\\n    BIGRAMS = 1     # Word pairs\\n    CONCEPTS = 2    # Semantic clusters\\n    DOCUMENTS = 3   # Full documents\\n```\\n\\n### HierarchicalLayer\\n\\n**Location:** `layers.py:59-273`\\n\\nContainer for minicolumns at each layer:\\n\\n```python\\nclass HierarchicalLayer:\\n    layer_type: CorticalLayer\\n    minicolumns: Dict[str, Minicolumn]  # content → minicolumn\\n    _id_index: Dict[str, str]           # id → content (O(1) lookup)\\n```\\n\\n**Key Methods:**\\n- `get_or_create_minicolumn(content)` - Create or retrieve minicolumn\\n- `get_minicolumn(content)` - Retrieve by content\\n- `get_by_id(col_id)` - O(1) lookup by ID (critical for performance)\\n- `column_count()` - Number of minicolumns\\n\\n### Minicolumn\\n\\n**Location:** `minicolumn.py:56-357`\\n\\nThe fundamental unit of representation:\\n\\n```python\\nclass Minicolumn:\\n    # Identity\\n    id: str              # \\\"L0_neural\\\", \\\"L1_neural networks\\\"\\n    content: str         # \\\"neural\\\", \\\"neural networks\\\"\\n    layer: int           # 0, 1, 2, or 3\\n\\n    # Statistics\\n    activation: float           # Neural activation level\\n    occurrence_count: int       # Total occurrences in corpus\\n    pagerank: float            # Importance score\\n    tfidf: float               # Global TF-IDF weight\\n    tfidf_per_doc: Dict[str, float]  # Per-document TF-IDF\\n\\n    # Document association\\n    document_ids: Set[str]     # Which documents contain this\\n    doc_occurrence_counts: Dict[str, int]  # Occurrences per document\\n\\n    # Connections (see Connection Types below)\\n    lateral_connections: Dict[str, float]\\n    typed_connections: Dict[str, Edge]\\n    feedforward_connections: Dict[str, float]\\n    feedback_connections: Dict[str, float]\\n\\n    # Clustering\\n    cluster_id: Optional[int]  # For Layer 0 tokens\\n```\\n\\n**ID Pattern:** `f\\\"L{layer}_{content}\\\"`\\n- Token: `\\\"L0_neural\\\"`\\n- Bigram: `\\\"L1_neural networks\\\"`\\n- Concept: `\\\"L2_neural/networks/learning\\\"`\\n- Document: `\\\"L3_doc_001\\\"`\\n\\n### Edge\\n\\n**Location:** `minicolumn.py:16-53`\\n\\nTyped connection with metadata (ConceptNet-style):\\n\\n```python\\n@dataclass\\nclass Edge:\\n    target_id: str                      # \\\"L0_network\\\"\\n    weight: float = 1.0                 # Connection strength\\n    relation_type: str = 'co_occurrence'  # 'IsA', 'PartOf', etc.\\n    confidence: float = 1.0             # [0.0, 1.0]\\n    source: str = 'corpus'              # 'corpus', 'semantic', 'inferred'\\n```\\n\\n---\\n\\n## Connection Types\\n\\n### 1. Lateral Connections\\n\\n**Within-layer** associations from co-occurrence.\\n\\n```python\\nminicolumn.lateral_connections: Dict[str, float]\\n# {\\\"L0_networks\\\": 0.8, \\\"L0_learning\\\": 0.5}\\n```\\n\\n- **Layer 0:** Tokens appearing near each other in text\\n- **Layer 1:** Bigrams sharing components or co-occurring\\n- **Layer 2:** Concepts with overlapping documents or semantics\\n- **Layer 3:** Documents sharing vocabulary\\n\\n### 2. Typed Connections\\n\\n**Within-layer** with semantic metadata.\\n\\n```python\\nminicolumn.typed_connections: Dict[str, Edge]\\n# {\\\"L0_animal\\\": Edge(weight=0.9, relation_type='IsA', confidence=0.95)}\\n```\\n\\nUsed for ConceptNet-style reasoning with relation types.\\n\\n### 3. Feedforward Connections\\n\\n**Downward** links to components (higher → lower layer).\\n\\n```python\\nminicolumn.feedforward_connections: Dict[str, float]\\n```\\n\\n- Bigram → component tokens: `\\\"neural networks\\\" → [\\\"neural\\\", \\\"networks\\\"]`\\n- Concept → member tokens: `\\\"neural/networks/learning\\\" → [member tokens]`\\n- Document → contained tokens: `\\\"doc1\\\" → [all tokens in doc1]`\\n\\n### 4. Feedback Connections\\n\\n**Upward** links to containers (lower → higher layer).\\n\\n```python\\nminicolumn.feedback_connections: Dict[str, float]\\n```\\n\\n- Token → containing bigrams: `\\\"neural\\\" → [\\\"neural networks\\\", \\\"neural processing\\\"]`\\n- Token → containing concepts: `\\\"neural\\\" → [\\\"neural/networks/learning\\\"]`\\n- Token → containing documents: `\\\"neural\\\" → [\\\"doc1\\\", \\\"doc2\\\"]`\\n\\n---\\n\\n## Data Flow\\n\\n### Document Processing\\n\\n**Location:** `processor.py:54-137`\\n\\nWhen a document is processed:\\n\\n```\\nINPUT: \\\"Neural networks process data.\\\"\\n\\n1. TOKENIZATION\\n   → [\\\"neural\\\", \\\"networks\\\", \\\"process\\\", \\\"data\\\"]\\n   → Create Layer 0 minicolumns\\n\\n2. DOCUMENT-TOKEN CONNECTIONS\\n   → doc.feedforward_connections[\\\"L0_neural\\\"] = 1.0\\n   → token.feedback_connections[\\\"L3_doc1\\\"] = 1.0\\n\\n3. LATERAL TOKEN CONNECTIONS\\n   → \\\"neural\\\" ↔ \\\"networks\\\" (co-occurrence)\\n   → \\\"networks\\\" ↔ \\\"process\\\" (co-occurrence)\\n\\n4. BIGRAM EXTRACTION\\n   → [\\\"neural networks\\\", \\\"networks process\\\", \\\"process data\\\"]\\n   → Create Layer 1 minicolumns\\n\\n5. BIGRAM-TOKEN CONNECTIONS\\n   → bigram.feedforward_connections[\\\"L0_neural\\\"] = 1.0\\n   → token.feedback_connections[\\\"L1_neural networks\\\"] = 1.0\\n```\\n\\n**Important:** Bigrams use SPACE separators: `\\\"neural networks\\\"`, not `\\\"neural_networks\\\"`.\\n\\n### Network Computation\\n\\n**Location:** `processor.py:452-596` (`compute_all()`)\\n\\nAfter processing documents, compute the full network:\\n\\n```\\n1. ACTIVATION PROPAGATION\\n   → Spread activation through connections\\n   → Simulates information flow\\n\\n2. PAGERANK\\n   → Compute importance for Layer 0 and Layer 1\\n   → Options: standard, semantic, hierarchical\\n\\n3. TF-IDF\\n   → Compute term weights for Layer 0\\n   → Both global and per-document variants\\n\\n4. DOCUMENT CONNECTIONS\\n   → Connect Layer 3 documents by shared vocabulary\\n   → Weight by sum of shared term TF-IDF scores\\n\\n5. BIGRAM CONNECTIONS\\n   → Connect Layer 1 bigrams by:\\n     - Shared components (\\\"neural networks\\\" ↔ \\\"neural processing\\\")\\n     - Chain patterns (\\\"machine learning\\\" ↔ \\\"learning algorithms\\\")\\n     - Document co-occurrence\\n\\n6. CONCEPT CLUSTERING\\n   → Run label propagation on Layer 0\\n   → Create Layer 2 concepts from clusters\\n   → Connect concepts to member tokens\\n\\n7. CONCEPT CONNECTIONS\\n   → Connect Layer 2 concepts by:\\n     - Document overlap (Jaccard similarity)\\n     - Semantic relations between members\\n     - Embedding similarity (optional)\\n```\\n\\n### Query Flow\\n\\n**Location:** `query.py`\\n\\nWhen a query is executed:\\n\\n```\\nINPUT: \\\"neural networks\\\"\\n\\n1. TOKENIZE QUERY\\n   → [\\\"neural\\\", \\\"networks\\\"]\\n\\n2. EXPAND QUERY\\n   → Add related terms from lateral connections\\n   → Add terms from concept clusters\\n   → Result: {\\\"neural\\\": 1.0, \\\"networks\\\": 1.0, \\\"learning\\\": 0.5, ...}\\n\\n3. SCORE DOCUMENTS\\n   → For each document, sum term scores:\\n     score = Σ(term_weight × token.tfidf_per_doc[doc_id])\\n\\n4. RANK AND RETURN\\n   → Sort documents by score\\n   → Return top_n results\\n```\\n\\n---\\n\\n## Layer Details\\n\\n### Layer 0: Tokens\\n\\n**Purpose:** Represent individual words after tokenization.\\n\\n**Content:** Lowercase stemmed words (stop words removed).\\n\\n**Connections:**\\n- Lateral: Co-occurring tokens within window\\n- Feedback: Containing bigrams, concepts, documents\\n- Feedforward: None (lowest layer)\\n\\n**Key Fields:**\\n- `occurrence_count`: Total times seen in corpus\\n- `document_ids`: Set of documents containing token\\n- `pagerank`: Importance score\\n- `tfidf`: Global TF-IDF weight\\n- `cluster_id`: Assigned concept cluster\\n\\n### Layer 1: Bigrams\\n\\n**Purpose:** Represent word pairs for phrase-level patterns.\\n\\n**Content:** Space-separated word pairs: `\\\"neural networks\\\"`.\\n\\n**Connections:**\\n- Lateral: Bigrams sharing components or co-occurring\\n- Feedforward: Component tokens\\n- Feedback: None typically (no Layer 2 → Layer 1 direct)\\n\\n**Key Fields:**\\n- Same as Layer 0\\n- Bigrams inherit properties from component tokens\\n\\n### Layer 2: Concepts\\n\\n**Purpose:** Represent semantic topic clusters.\\n\\n**Content:** Named by top members: `\\\"neural/networks/learning\\\"`.\\n\\n**Connections:**\\n- Lateral: Concepts with overlapping documents or semantics\\n- Feedforward: Member tokens\\n- Feedback: None typically\\n\\n**Creation:** Built by `build_concept_clusters()` using label propagation on Layer 0 tokens.\\n\\n### Layer 3: Documents\\n\\n**Purpose:** Represent full documents in the corpus.\\n\\n**Content:** Document ID string.\\n\\n**Connections:**\\n- Lateral: Documents sharing vocabulary\\n- Feedforward: All tokens in document\\n- Feedback: None (highest layer)\\n\\n**Key Fields:**\\n- `document_ids`: Contains only self\\n- `occurrence_count`: 1 (single document)\\n\\n---\\n\\n## Performance Patterns\\n\\n### O(1) ID Lookups\\n\\n**Critical:** Always use `layer.get_by_id(col_id)` instead of iterating:\\n\\n```python\\n# WRONG - O(n):\\nfor col in layer.minicolumns.values():\\n    if col.id == target_id:\\n        neighbor = col\\n\\n# RIGHT - O(1):\\nneighbor = layer.get_by_id(target_id)\\n```\\n\\nUsed throughout `analysis.py` and `query.py`.\\n\\n### Staleness Tracking\\n\\n**Location:** `processor.py:49`\\n\\n```python\\nself._stale_computations: set\\n```\\n\\nTracks which computations need rerunning after corpus changes:\\n- `COMP_TFIDF`\\n- `COMP_PAGERANK`\\n- `COMP_ACTIVATION`\\n- `COMP_DOC_CONNECTIONS`\\n- `COMP_BIGRAM_CONNECTIONS`\\n- `COMP_CONCEPTS`\\n\\n### Query Caching\\n\\n**Location:** `processor.py:51-52`\\n\\n```python\\nself._query_expansion_cache: Dict[str, Dict[str, float]]\\nself._query_cache_max_size: int = 100\\n```\\n\\nLRU cache for query expansion results. Cleared after `compute_all()`.\\n\\n---\\n\\n## File Reference\\n\\n| Component | File | Lines |\\n|-----------|------|-------|\\n| CorticalLayer enum | `layers.py` | 21-56 |\\n| HierarchicalLayer | `layers.py` | 59-273 |\\n| Minicolumn | `minicolumn.py` | 56-357 |\\n| Edge | `minicolumn.py` | 16-53 |\\n| process_document() | `processor.py` | 54-137 |\\n| compute_all() | `processor.py` | 452-596 |\\n| Tokenizer | `tokenizer.py` | Full file |\\n\\n---\\n\\n## Visual Summary\\n\\n```\\n┌─────────────────────────────────────────────────────────────┐\\n│                    Layer 3: DOCUMENTS                        │\\n│  ┌─────────┐    ┌─────────┐                                 │\\n│  │  doc1   │←──→│  doc2   │  (lateral: shared vocab)        │\\n│  └────┬────┘    └────┬────┘                                 │\\n│       │              │      (feedforward: contained tokens) │\\n└───────┼──────────────┼──────────────────────────────────────┘\\n        ↓              ↓\\n┌───────┼──────────────┼──────────────────────────────────────┐\\n│       │   Layer 2: CONCEPTS                                 │\\n│  ┌────┴────┐    ┌────┴────┐                                │\\n│  │ concept1│←──→│ concept2│  (lateral: doc overlap)        │\\n│  └────┬────┘    └────┬────┘                                │\\n│       │              │      (feedforward: member tokens)    │\\n└───────┼──────────────┼──────────────────────────────────────┘\\n        ↓              ↓\\n┌───────┼──────────────┼──────────────────────────────────────┐\\n│       │   Layer 1: BIGRAMS                                  │\\n│  ┌────┴──────┐  ┌────┴──────┐                              │\\n│  │neural     │←→│networks   │  (lateral: shared component) │\\n│  │networks   │  │process    │                              │\\n│  └────┬──────┘  └────┬──────┘                              │\\n│       │              │      (feedforward: component tokens) │\\n└───────┼──────────────┼──────────────────────────────────────┘\\n        ↓              ↓\\n┌───────┼──────────────┼──────────────────────────────────────┐\\n│       │   Layer 0: TOKENS                                   │\\n│  ┌────┴────┐ ┌──────┐ ┌────┴────┐ ┌────────┐              │\\n│  │ neural  │←→│networks│←→│ process │←→│  data  │           │\\n│  └─────────┘ └──────┘ └─────────┘ └────────┘              │\\n│              (lateral: co-occurrence within window)         │\\n└─────────────────────────────────────────────────────────────┘\\n```\\n\",",
        "      \"mtime\": 1765392877.0",
        "    },",
        "    {",
        "      \"op\": \"add\",",
        "      \"doc_id\": \"tests/test_processor.py\",",
        "      \"content\": \"\\\"\\\"\\\"Tests for the CorticalTextProcessor class.\\\"\\\"\\\"\\n\\nimport unittest\\nimport tempfile\\nimport os\\nimport sys\\nsys.path.insert(0, '..')\\n\\nfrom cortical import CorticalTextProcessor, CorticalLayer\\nfrom cortical.layers import HierarchicalLayer\\n\\n\\nclass TestProcessorBasic(unittest.TestCase):\\n    \\\"\\\"\\\"Test basic processor functionality.\\\"\\\"\\\"\\n    \\n    def setUp(self):\\n        self.processor = CorticalTextProcessor()\\n    \\n    def test_process_document(self):\\n        \\\"\\\"\\\"Test document processing.\\\"\\\"\\\"\\n        stats = self.processor.process_document(\\\"doc1\\\", \\\"Neural networks process information.\\\")\\n        self.assertGreater(stats['tokens'], 0)\\n        self.assertIn(\\\"doc1\\\", self.processor.documents)\\n    \\n    def test_multiple_documents(self):\\n        \\\"\\\"\\\"Test processing multiple documents.\\\"\\\"\\\"\\n        self.processor.process_document(\\\"doc1\\\", \\\"Neural networks learn.\\\")\\n        self.processor.process_document(\\\"doc2\\\", \\\"Deep learning models.\\\")\\n        self.assertEqual(len(self.processor.documents), 2)\\n    \\n    def test_token_layer_populated(self):\\n        \\\"\\\"\\\"Test that token layer is populated.\\\"\\\"\\\"\\n        self.processor.process_document(\\\"doc1\\\", \\\"Neural networks process information.\\\")\\n        layer0 = self.processor.get_layer(CorticalLayer.TOKENS)\\n        self.assertGreater(layer0.column_count(), 0)\\n    \\n    def test_lateral_connections(self):\\n        \\\"\\\"\\\"Test that lateral connections are formed.\\\"\\\"\\\"\\n        self.processor.process_document(\\\"doc1\\\", \\\"Neural networks process information.\\\")\\n        layer0 = self.processor.get_layer(CorticalLayer.TOKENS)\\n        \\n        neural = layer0.get_minicolumn(\\\"neural\\\")\\n        self.assertIsNotNone(neural)\\n        self.assertGreater(neural.connection_count(), 0)\\n\\n\\nclass TestProcessorComputation(unittest.TestCase):\\n    \\\"\\\"\\\"Test processor computation methods.\\\"\\\"\\\"\\n    \\n    @classmethod\\n    def setUpClass(cls):\\n        cls.processor = CorticalTextProcessor()\\n        cls.processor.process_document(\\\"doc1\\\", \\\"\\\"\\\"\\n            Neural networks process information through layers.\\n            Deep learning enables pattern recognition.\\n        \\\"\\\"\\\")\\n        cls.processor.process_document(\\\"doc2\\\", \\\"\\\"\\\"\\n            Machine learning models learn from data.\\n            Training neural networks requires optimization.\\n        \\\"\\\"\\\")\\n    \\n    def test_propagate_activation(self):\\n        \\\"\\\"\\\"Test activation propagation.\\\"\\\"\\\"\\n        self.processor.propagate_activation(iterations=3, verbose=False)\\n        layer0 = self.processor.get_layer(CorticalLayer.TOKENS)\\n        \\n        # Check some columns have activation\\n        activations = [col.activation for col in layer0]\\n        self.assertTrue(any(a > 0 for a in activations))\\n    \\n    def test_compute_importance(self):\\n        \\\"\\\"\\\"Test PageRank computation.\\\"\\\"\\\"\\n        self.processor.propagate_activation(iterations=3, verbose=False)\\n        self.processor.compute_importance(verbose=False)\\n        \\n        layer0 = self.processor.get_layer(CorticalLayer.TOKENS)\\n        pageranks = [col.pagerank for col in layer0]\\n        self.assertTrue(all(p > 0 for p in pageranks))\\n    \\n    def test_compute_tfidf(self):\\n        \\\"\\\"\\\"Test TF-IDF computation.\\\"\\\"\\\"\\n        # Create fresh processor for this test\\n        # Use 3 docs where 'neural' only appears in 2, so IDF > 0\\n        processor = CorticalTextProcessor()\\n        processor.process_document(\\\"doc1\\\", \\\"Neural networks process information.\\\")\\n        processor.process_document(\\\"doc2\\\", \\\"Machine learning neural models.\\\")\\n        processor.process_document(\\\"doc3\\\", \\\"Database systems store data efficiently.\\\")\\n        processor.compute_tfidf(verbose=False)\\n        \\n        layer0 = processor.get_layer(CorticalLayer.TOKENS)\\n        neural = layer0.get_minicolumn(\\\"neural\\\")\\n        self.assertIsNotNone(neural)\\n        # Now IDF = log(3/2) > 0\\n        self.assertGreater(neural.tfidf, 0)\\n    \\n    def test_compute_all(self):\\n        \\\"\\\"\\\"Test compute_all runs without error.\\\"\\\"\\\"\\n        processor = CorticalTextProcessor()\\n        processor.process_document(\\\"test\\\", \\\"Test document content.\\\")\\n        processor.compute_all(verbose=False)\\n\\n\\nclass TestProcessorQuery(unittest.TestCase):\\n    \\\"\\\"\\\"Test processor query functionality.\\\"\\\"\\\"\\n    \\n    @classmethod\\n    def setUpClass(cls):\\n        cls.processor = CorticalTextProcessor()\\n        cls.processor.process_document(\\\"neural_doc\\\", \\\"\\\"\\\"\\n            Neural networks process information through multiple layers.\\n            Deep learning enables complex pattern recognition.\\n            Backpropagation trains neural network weights.\\n        \\\"\\\"\\\")\\n        cls.processor.process_document(\\\"ml_doc\\\", \\\"\\\"\\\"\\n            Machine learning algorithms learn from data.\\n            Supervised learning uses labeled examples.\\n            Model training optimizes parameters.\\n        \\\"\\\"\\\")\\n        cls.processor.compute_all(verbose=False)\\n    \\n    def test_expand_query(self):\\n        \\\"\\\"\\\"Test query expansion.\\\"\\\"\\\"\\n        expanded = self.processor.expand_query(\\\"neural\\\", max_expansions=5)\\n        self.assertIn(\\\"neural\\\", expanded)\\n        self.assertGreater(len(expanded), 1)\\n    \\n    def test_find_documents(self):\\n        \\\"\\\"\\\"Test document finding.\\\"\\\"\\\"\\n        results = self.processor.find_documents_for_query(\\\"neural networks\\\", top_n=2)\\n        self.assertGreater(len(results), 0)\\n        self.assertEqual(results[0][0], \\\"neural_doc\\\")\\n    \\n    def test_query_expanded(self):\\n        \\\"\\\"\\\"Test expanded query.\\\"\\\"\\\"\\n        results = self.processor.query_expanded(\\\"learning\\\", top_n=5)\\n        self.assertIsInstance(results, list)\\n\\n\\nclass TestProcessorMetadata(unittest.TestCase):\\n    \\\"\\\"\\\"Test document metadata functionality.\\\"\\\"\\\"\\n\\n    def setUp(self):\\n        self.processor = CorticalTextProcessor()\\n\\n    def test_process_document_with_metadata(self):\\n        \\\"\\\"\\\"Test processing document with metadata.\\\"\\\"\\\"\\n        metadata = {\\\"source\\\": \\\"https://example.com\\\", \\\"author\\\": \\\"Test Author\\\"}\\n        self.processor.process_document(\\\"doc1\\\", \\\"Test content.\\\", metadata=metadata)\\n        retrieved = self.processor.get_document_metadata(\\\"doc1\\\")\\n        self.assertEqual(retrieved[\\\"source\\\"], \\\"https://example.com\\\")\\n        self.assertEqual(retrieved[\\\"author\\\"], \\\"Test Author\\\")\\n\\n    def test_set_document_metadata(self):\\n        \\\"\\\"\\\"Test setting metadata after processing.\\\"\\\"\\\"\\n        self.processor.process_document(\\\"doc1\\\", \\\"Test content.\\\")\\n        self.processor.set_document_metadata(\\\"doc1\\\", source=\\\"https://test.com\\\", timestamp=\\\"2025-12-09\\\")\\n        metadata = self.processor.get_document_metadata(\\\"doc1\\\")\\n        self.assertEqual(metadata[\\\"source\\\"], \\\"https://test.com\\\")\\n        self.assertEqual(metadata[\\\"timestamp\\\"], \\\"2025-12-09\\\")\\n\\n    def test_update_document_metadata(self):\\n        \\\"\\\"\\\"Test updating existing metadata.\\\"\\\"\\\"\\n        self.processor.process_document(\\\"doc1\\\", \\\"Test content.\\\", metadata={\\\"author\\\": \\\"Original\\\"})\\n        self.processor.set_document_metadata(\\\"doc1\\\", author=\\\"Updated\\\", category=\\\"AI\\\")\\n        metadata = self.processor.get_document_metadata(\\\"doc1\\\")\\n        self.assertEqual(metadata[\\\"author\\\"], \\\"Updated\\\")\\n        self.assertEqual(metadata[\\\"category\\\"], \\\"AI\\\")\\n\\n    def test_get_document_metadata_missing(self):\\n        \\\"\\\"\\\"Test getting metadata for nonexistent document.\\\"\\\"\\\"\\n        metadata = self.processor.get_document_metadata(\\\"nonexistent\\\")\\n        self.assertEqual(metadata, {})\\n\\n    def test_get_all_document_metadata(self):\\n        \\\"\\\"\\\"Test getting all document metadata.\\\"\\\"\\\"\\n        self.processor.process_document(\\\"doc1\\\", \\\"Content 1\\\", metadata={\\\"type\\\": \\\"article\\\"})\\n        self.processor.process_document(\\\"doc2\\\", \\\"Content 2\\\", metadata={\\\"type\\\": \\\"paper\\\"})\\n        all_metadata = self.processor.get_all_document_metadata()\\n        self.assertEqual(len(all_metadata), 2)\\n        self.assertEqual(all_metadata[\\\"doc1\\\"][\\\"type\\\"], \\\"article\\\")\\n        self.assertEqual(all_metadata[\\\"doc2\\\"][\\\"type\\\"], \\\"paper\\\")\\n\\n    def test_metadata_not_modified_externally(self):\\n        \\\"\\\"\\\"Test that get_all_document_metadata returns a copy.\\\"\\\"\\\"\\n        self.processor.process_document(\\\"doc1\\\", \\\"Content\\\", metadata={\\\"key\\\": \\\"value\\\"})\\n        all_metadata = self.processor.get_all_document_metadata()\\n        all_metadata[\\\"doc1\\\"][\\\"key\\\"] = \\\"modified\\\"\\n        # Original should be unchanged\\n        original = self.processor.get_document_metadata(\\\"doc1\\\")\\n        self.assertEqual(original[\\\"key\\\"], \\\"value\\\")\\n\\n\\nclass TestProcessorPersistence(unittest.TestCase):\\n    \\\"\\\"\\\"Test processor save/load functionality.\\\"\\\"\\\"\\n\\n    def test_save_and_load(self):\\n        \\\"\\\"\\\"Test saving and loading processor.\\\"\\\"\\\"\\n        processor = CorticalTextProcessor()\\n        processor.process_document(\\\"doc1\\\", \\\"Test document content.\\\")\\n        processor.compute_all(verbose=False)\\n\\n        with tempfile.TemporaryDirectory() as tmpdir:\\n            filepath = os.path.join(tmpdir, \\\"test.pkl\\\")\\n            processor.save(filepath, verbose=False)\\n\\n            loaded = CorticalTextProcessor.load(filepath, verbose=False)\\n            self.assertEqual(len(loaded.documents), 1)\\n            self.assertIn(\\\"doc1\\\", loaded.documents)\\n\\n    def test_save_and_load_with_metadata(self):\\n        \\\"\\\"\\\"Test that document metadata is preserved through save/load.\\\"\\\"\\\"\\n        processor = CorticalTextProcessor()\\n        processor.process_document(\\n            \\\"doc1\\\",\\n            \\\"Test document content.\\\",\\n            metadata={\\\"source\\\": \\\"https://example.com\\\", \\\"author\\\": \\\"Test Author\\\"}\\n        )\\n        processor.set_document_metadata(\\\"doc1\\\", category=\\\"test\\\")\\n        processor.compute_all(verbose=False)\\n\\n        with tempfile.TemporaryDirectory() as tmpdir:\\n            filepath = os.path.join(tmpdir, \\\"test.pkl\\\")\\n            processor.save(filepath, verbose=False)\\n\\n            loaded = CorticalTextProcessor.load(filepath, verbose=False)\\n            metadata = loaded.get_document_metadata(\\\"doc1\\\")\\n            self.assertEqual(metadata[\\\"source\\\"], \\\"https://example.com\\\")\\n            self.assertEqual(metadata[\\\"author\\\"], \\\"Test Author\\\")\\n            self.assertEqual(metadata[\\\"category\\\"], \\\"test\\\")\\n\\n\\nclass TestProcessorPassageRetrieval(unittest.TestCase):\\n    \\\"\\\"\\\"Test chunk-level passage retrieval for RAG systems.\\\"\\\"\\\"\\n\\n    @classmethod\\n    def setUpClass(cls):\\n        cls.processor = CorticalTextProcessor()\\n        # Create documents with distinct content for testing passage retrieval\\n        cls.processor.process_document(\\\"neural_doc\\\", \\\"\\\"\\\"\\n            Neural networks are computational models inspired by biological neurons.\\n            They process information through interconnected layers of nodes.\\n            Deep learning uses many layers to learn hierarchical representations.\\n            Backpropagation is the key algorithm for training neural networks.\\n            Convolutional neural networks excel at image recognition tasks.\\n        \\\"\\\"\\\")\\n        cls.processor.process_document(\\\"ml_doc\\\", \\\"\\\"\\\"\\n            Machine learning algorithms learn patterns from data automatically.\\n            Supervised learning requires labeled training examples.\\n            Unsupervised learning discovers hidden structure in unlabeled data.\\n            Reinforcement learning trains agents through rewards and penalties.\\n            Model evaluation uses metrics like accuracy and precision.\\n        \\\"\\\"\\\")\\n        cls.processor.process_document(\\\"data_doc\\\", \\\"\\\"\\\"\\n            Data preprocessing is essential for machine learning pipelines.\\n            Feature engineering creates meaningful input representations.\\n            Data normalization scales features to similar ranges.\\n            Missing value imputation handles incomplete datasets.\\n            Cross-validation ensures robust model performance estimates.\\n        \\\"\\\"\\\")\\n        cls.processor.compute_all(verbose=False)\\n\\n    def test_find_passages_returns_list(self):\\n        \\\"\\\"\\\"Test that find_passages_for_query returns a list.\\\"\\\"\\\"\\n        results = self.processor.find_passages_for_query(\\\"neural networks\\\")\\n        self.assertIsInstance(results, list)\\n\\n    def test_find_passages_returns_tuples(self):\\n        \\\"\\\"\\\"Test that results are tuples with correct structure.\\\"\\\"\\\"\\n        results = self.processor.find_passages_for_query(\\\"neural networks\\\", top_n=1)\\n        self.assertGreater(len(results), 0)\\n        passage, doc_id, start, end, score = results[0]\\n        self.assertIsInstance(passage, str)\\n        self.assertIsInstance(doc_id, str)\\n        self.assertIsInstance(start, int)\\n        self.assertIsInstance(end, int)\\n        self.assertIsInstance(score, float)\\n\\n    def test_find_passages_contains_text(self):\\n        \\\"\\\"\\\"Test that passages contain actual text.\\\"\\\"\\\"\\n        results = self.processor.find_passages_for_query(\\\"neural\\\", top_n=3)\\n        self.assertGreater(len(results), 0)\\n        passage, _, _, _, _ = results[0]\\n        self.assertGreater(len(passage), 0)\\n\\n    def test_find_passages_position_valid(self):\\n        \\\"\\\"\\\"Test that start/end positions are valid.\\\"\\\"\\\"\\n        results = self.processor.find_passages_for_query(\\\"learning\\\", top_n=3)\\n        for passage, doc_id, start, end, score in results:\\n            self.assertGreaterEqual(start, 0)\\n            self.assertGreater(end, start)\\n            self.assertEqual(len(passage), end - start)\\n\\n    def test_find_passages_top_n_limit(self):\\n        \\\"\\\"\\\"Test that top_n parameter limits results.\\\"\\\"\\\"\\n        results = self.processor.find_passages_for_query(\\\"learning\\\", top_n=2)\\n        self.assertLessEqual(len(results), 2)\\n\\n    def test_find_passages_chunk_size(self):\\n        \\\"\\\"\\\"Test that chunk_size parameter is respected.\\\"\\\"\\\"\\n        results = self.processor.find_passages_for_query(\\n            \\\"neural\\\", top_n=5, chunk_size=100, overlap=20\\n        )\\n        for passage, _, _, _, _ in results:\\n            self.assertLessEqual(len(passage), 100)\\n\\n    def test_find_passages_doc_filter(self):\\n        \\\"\\\"\\\"Test that doc_filter restricts search.\\\"\\\"\\\"\\n        results = self.processor.find_passages_for_query(\\n            \\\"learning\\\", top_n=10, doc_filter=[\\\"neural_doc\\\"]\\n        )\\n        for _, doc_id, _, _, _ in results:\\n            self.assertEqual(doc_id, \\\"neural_doc\\\")\\n\\n    def test_find_passages_scores_descending(self):\\n        \\\"\\\"\\\"Test that results are sorted by score descending.\\\"\\\"\\\"\\n        results = self.processor.find_passages_for_query(\\\"neural networks\\\", top_n=5)\\n        if len(results) > 1:\\n            scores = [score for _, _, _, _, score in results]\\n            self.assertEqual(scores, sorted(scores, reverse=True))\\n\\n    def test_find_passages_no_expansion(self):\\n        \\\"\\\"\\\"Test passage retrieval without query expansion.\\\"\\\"\\\"\\n        results = self.processor.find_passages_for_query(\\n            \\\"neural\\\", top_n=3, use_expansion=False\\n        )\\n        self.assertIsInstance(results, list)\\n\\n    def test_find_passages_empty_query(self):\\n        \\\"\\\"\\\"Test handling of queries with no matching terms.\\\"\\\"\\\"\\n        results = self.processor.find_passages_for_query(\\\"xyznonexistent123\\\")\\n        self.assertEqual(len(results), 0)\\n\\n\\nclass TestProcessorGaps(unittest.TestCase):\\n    \\\"\\\"\\\"Test gap detection functionality.\\\"\\\"\\\"\\n\\n    @classmethod\\n    def setUpClass(cls):\\n        cls.processor = CorticalTextProcessor()\\n        for i in range(3):\\n            cls.processor.process_document(f\\\"tech_{i}\\\", \\\"\\\"\\\"\\n                Machine learning neural networks deep learning.\\n                Training models data processing algorithms.\\n            \\\"\\\"\\\")\\n        cls.processor.process_document(\\\"outlier\\\", \\\"\\\"\\\"\\n            Medieval falconry birds hunting prey.\\n            Falcons hawks eagles training.\\n        \\\"\\\"\\\")\\n        cls.processor.compute_all(verbose=False)\\n\\n    def test_analyze_knowledge_gaps(self):\\n        \\\"\\\"\\\"Test gap analysis returns expected structure.\\\"\\\"\\\"\\n        gaps = self.processor.analyze_knowledge_gaps()\\n        self.assertIn('isolated_documents', gaps)\\n        self.assertIn('weak_topics', gaps)\\n        self.assertIn('coverage_score', gaps)\\n\\n    def test_detect_anomalies(self):\\n        \\\"\\\"\\\"Test anomaly detection.\\\"\\\"\\\"\\n        anomalies = self.processor.detect_anomalies(threshold=0.1)\\n        self.assertIsInstance(anomalies, list)\\n\\n\\nclass TestProcessorBatchQuery(unittest.TestCase):\\n    \\\"\\\"\\\"Test batch query functionality for efficient multi-query search.\\\"\\\"\\\"\\n\\n    @classmethod\\n    def setUpClass(cls):\\n        cls.processor = CorticalTextProcessor()\\n        cls.processor.process_document(\\\"neural_doc\\\", \\\"\\\"\\\"\\n            Neural networks are computational models inspired by biological neurons.\\n            Deep learning uses many layers to learn hierarchical representations.\\n            Backpropagation is the key algorithm for training neural networks.\\n        \\\"\\\"\\\")\\n        cls.processor.process_document(\\\"ml_doc\\\", \\\"\\\"\\\"\\n            Machine learning algorithms learn patterns from data automatically.\\n            Supervised learning requires labeled training examples.\\n            Model evaluation uses metrics like accuracy and precision.\\n        \\\"\\\"\\\")\\n        cls.processor.process_document(\\\"data_doc\\\", \\\"\\\"\\\"\\n            Data preprocessing is essential for machine learning pipelines.\\n            Feature engineering creates meaningful input representations.\\n            Data normalization scales features to similar ranges.\\n        \\\"\\\"\\\")\\n        cls.processor.compute_all(verbose=False)\\n\\n    def test_find_documents_batch_returns_list(self):\\n        \\\"\\\"\\\"Test that find_documents_batch returns a list of results.\\\"\\\"\\\"\\n        queries = [\\\"neural networks\\\", \\\"machine learning\\\"]\\n        results = self.processor.find_documents_batch(queries, top_n=2)\\n        self.assertIsInstance(results, list)\\n        self.assertEqual(len(results), 2)\\n\\n    def test_find_documents_batch_result_structure(self):\\n        \\\"\\\"\\\"Test that each result has correct structure.\\\"\\\"\\\"\\n        queries = [\\\"neural\\\", \\\"data\\\"]\\n        results = self.processor.find_documents_batch(queries, top_n=3)\\n        for result in results:\\n            self.assertIsInstance(result, list)\\n            for doc_id, score in result:\\n                self.assertIsInstance(doc_id, str)\\n                self.assertIsInstance(score, float)\\n\\n    def test_find_documents_batch_returns_relevant_docs(self):\\n        \\\"\\\"\\\"Test that batch queries return relevant documents.\\\"\\\"\\\"\\n        queries = [\\\"neural networks\\\", \\\"data preprocessing\\\"]\\n        results = self.processor.find_documents_batch(queries, top_n=1)\\n        # First query should find neural_doc\\n        self.assertGreater(len(results[0]), 0)\\n        self.assertEqual(results[0][0][0], \\\"neural_doc\\\")\\n        # Second query should find data_doc\\n        self.assertGreater(len(results[1]), 0)\\n        self.assertEqual(results[1][0][0], \\\"data_doc\\\")\\n\\n    def test_find_documents_batch_top_n(self):\\n        \\\"\\\"\\\"Test that top_n limits results per query.\\\"\\\"\\\"\\n        queries = [\\\"learning\\\", \\\"neural\\\"]\\n        results = self.processor.find_documents_batch(queries, top_n=2)\\n        for result in results:\\n            self.assertLessEqual(len(result), 2)\\n\\n    def test_find_documents_batch_empty_query_list(self):\\n        \\\"\\\"\\\"Test batch with empty query list.\\\"\\\"\\\"\\n        results = self.processor.find_documents_batch([], top_n=3)\\n        self.assertEqual(results, [])\\n\\n    def test_find_documents_batch_no_expansion(self):\\n        \\\"\\\"\\\"Test batch query without expansion.\\\"\\\"\\\"\\n        queries = [\\\"neural\\\", \\\"data\\\"]\\n        results = self.processor.find_documents_batch(\\n            queries, top_n=2, use_expansion=False\\n        )\\n        self.assertEqual(len(results), 2)\\n\\n    def test_find_passages_batch_returns_list(self):\\n        \\\"\\\"\\\"Test that find_passages_batch returns a list of results.\\\"\\\"\\\"\\n        queries = [\\\"neural networks\\\", \\\"machine learning\\\"]\\n        results = self.processor.find_passages_batch(queries, top_n=2)\\n        self.assertIsInstance(results, list)\\n        self.assertEqual(len(results), 2)\\n\\n    def test_find_passages_batch_result_structure(self):\\n        \\\"\\\"\\\"Test that each passage result has correct structure.\\\"\\\"\\\"\\n        queries = [\\\"neural\\\"]\\n        results = self.processor.find_passages_batch(queries, top_n=3)\\n        self.assertEqual(len(results), 1)\\n        for passage, doc_id, start, end, score in results[0]:\\n            self.assertIsInstance(passage, str)\\n            self.assertIsInstance(doc_id, str)\\n            self.assertIsInstance(start, int)\\n            self.assertIsInstance(end, int)\\n            self.assertIsInstance(score, float)\\n\\n    def test_find_passages_batch_top_n(self):\\n        \\\"\\\"\\\"Test that top_n limits passages per query.\\\"\\\"\\\"\\n        queries = [\\\"learning\\\", \\\"neural\\\"]\\n        results = self.processor.find_passages_batch(queries, top_n=2)\\n        for result in results:\\n            self.assertLessEqual(len(result), 2)\\n\\n    def test_find_passages_batch_chunk_size(self):\\n        \\\"\\\"\\\"Test that chunk_size is respected.\\\"\\\"\\\"\\n        queries = [\\\"neural\\\"]\\n        results = self.processor.find_passages_batch(\\n            queries, top_n=5, chunk_size=100, overlap=20\\n        )\\n        for passage, _, _, _, _ in results[0]:\\n            self.assertLessEqual(len(passage), 100)\\n\\n    def test_find_passages_batch_doc_filter(self):\\n        \\\"\\\"\\\"Test that doc_filter restricts results.\\\"\\\"\\\"\\n        queries = [\\\"learning\\\", \\\"neural\\\"]\\n        results = self.processor.find_passages_batch(\\n            queries, top_n=10, doc_filter=[\\\"neural_doc\\\"]\\n        )\\n        for result in results:\\n            for _, doc_id, _, _, _ in result:\\n                self.assertEqual(doc_id, \\\"neural_doc\\\")\\n\\n    def test_find_passages_batch_empty_query_list(self):\\n        \\\"\\\"\\\"Test batch with empty query list.\\\"\\\"\\\"\\n        results = self.processor.find_passages_batch([], top_n=3)\\n        self.assertEqual(results, [])\\n\\n    def test_batch_query_consistency(self):\\n        \\\"\\\"\\\"Test that batch results match individual queries.\\\"\\\"\\\"\\n        queries = [\\\"neural networks\\\", \\\"data processing\\\"]\\n        batch_results = self.processor.find_documents_batch(queries, top_n=3)\\n\\n        # Compare with individual queries\\n        for i, query in enumerate(queries):\\n            individual_result = self.processor.find_documents_for_query(query, top_n=3)\\n            # Results should be the same (or very close)\\n            self.assertEqual(len(batch_results[i]), len(individual_result))\\n            for j, (doc_id, score) in enumerate(batch_results[i]):\\n                self.assertEqual(doc_id, individual_result[j][0])\\n\\n    def test_batch_handles_nonexistent_terms(self):\\n        \\\"\\\"\\\"Test that batch handles queries with no matches.\\\"\\\"\\\"\\n        queries = [\\\"xyznonexistent123\\\", \\\"neural networks\\\"]\\n        results = self.processor.find_documents_batch(queries, top_n=3)\\n        self.assertEqual(len(results), 2)\\n        self.assertEqual(len(results[0]), 0)  # No matches for nonexistent\\n        self.assertGreater(len(results[1]), 0)  # Matches for neural networks\\n\\n\\nclass TestProcessorMultiStageRanking(unittest.TestCase):\\n    \\\"\\\"\\\"Test multi-stage ranking pipeline for RAG systems.\\\"\\\"\\\"\\n\\n    @classmethod\\n    def setUpClass(cls):\\n        cls.processor = CorticalTextProcessor()\\n        # Create a diverse corpus for testing multi-stage ranking\\n        cls.processor.process_document(\\\"neural_doc\\\", \\\"\\\"\\\"\\n            Neural networks are computational models inspired by biological neurons.\\n            Deep learning uses many layers to learn hierarchical representations.\\n            Backpropagation is the key algorithm for training neural networks.\\n            Convolutional neural networks excel at image recognition tasks.\\n        \\\"\\\"\\\")\\n        cls.processor.process_document(\\\"ml_doc\\\", \\\"\\\"\\\"\\n            Machine learning algorithms learn patterns from data automatically.\\n            Supervised learning requires labeled training examples.\\n            Unsupervised learning discovers hidden structure in data.\\n            Model evaluation uses metrics like accuracy precision and recall.\\n        \\\"\\\"\\\")\\n        cls.processor.process_document(\\\"data_doc\\\", \\\"\\\"\\\"\\n            Data preprocessing is essential for machine learning pipelines.\\n            Feature engineering creates meaningful input representations.\\n            Data normalization scales features to similar ranges.\\n            Cross-validation ensures robust model performance estimates.\\n        \\\"\\\"\\\")\\n        cls.processor.process_document(\\\"nlp_doc\\\", \\\"\\\"\\\"\\n            Natural language processing enables computers to understand text.\\n            Word embeddings capture semantic relationships between words.\\n            Transformers use attention mechanisms for sequence modeling.\\n            Language models can generate coherent text passages.\\n        \\\"\\\"\\\")\\n        cls.processor.compute_all(verbose=False)\\n\\n    def test_multi_stage_rank_returns_list(self):\\n        \\\"\\\"\\\"Test that multi_stage_rank returns a list.\\\"\\\"\\\"\\n        results = self.processor.multi_stage_rank(\\\"neural networks\\\", top_n=3)\\n        self.assertIsInstance(results, list)\\n\\n    def test_multi_stage_rank_result_structure(self):\\n        \\\"\\\"\\\"Test that results have correct 6-tuple structure.\\\"\\\"\\\"\\n        results = self.processor.multi_stage_rank(\\\"neural\\\", top_n=3)\\n        self.assertGreater(len(results), 0)\\n        passage, doc_id, start, end, score, stage_scores = results[0]\\n        self.assertIsInstance(passage, str)\\n        self.assertIsInstance(doc_id, str)\\n        self.assertIsInstance(start, int)\\n        self.assertIsInstance(end, int)\\n        self.assertIsInstance(score, float)\\n        self.assertIsInstance(stage_scores, dict)\\n\\n    def test_multi_stage_rank_stage_scores(self):\\n        \\\"\\\"\\\"Test that stage_scores contains expected keys.\\\"\\\"\\\"\\n        results = self.processor.multi_stage_rank(\\\"neural networks\\\", top_n=3)\\n        self.assertGreater(len(results), 0)\\n        _, _, _, _, _, stage_scores = results[0]\\n        self.assertIn('concept_score', stage_scores)\\n        self.assertIn('doc_score', stage_scores)\\n        self.assertIn('chunk_score', stage_scores)\\n        self.assertIn('final_score', stage_scores)\\n\\n    def test_multi_stage_rank_top_n(self):\\n        \\\"\\\"\\\"Test that top_n limits results.\\\"\\\"\\\"\\n        results = self.processor.multi_stage_rank(\\\"learning\\\", top_n=2)\\n        self.assertLessEqual(len(results), 2)\\n\\n    def test_multi_stage_rank_chunk_size(self):\\n        \\\"\\\"\\\"Test that chunk_size is respected.\\\"\\\"\\\"\\n        results = self.processor.multi_stage_rank(\\n            \\\"neural\\\", top_n=5, chunk_size=100, overlap=20\\n        )\\n        for passage, _, _, _, _, _ in results:\\n            self.assertLessEqual(len(passage), 100)\\n\\n    def test_multi_stage_rank_concept_boost(self):\\n        \\\"\\\"\\\"Test that concept_boost parameter is used.\\\"\\\"\\\"\\n        # Test with high concept boost vs low\\n        results_high = self.processor.multi_stage_rank(\\n            \\\"neural\\\", top_n=3, concept_boost=0.8\\n        )\\n        results_low = self.processor.multi_stage_rank(\\n            \\\"neural\\\", top_n=3, concept_boost=0.1\\n        )\\n        # Both should return results (exact ordering may differ)\\n        self.assertGreater(len(results_high), 0)\\n        self.assertGreater(len(results_low), 0)\\n\\n    def test_multi_stage_rank_sorted_descending(self):\\n        \\\"\\\"\\\"Test that results are sorted by score descending.\\\"\\\"\\\"\\n        results = self.processor.multi_stage_rank(\\\"neural networks\\\", top_n=5)\\n        if len(results) > 1:\\n            scores = [score for _, _, _, _, score, _ in results]\\n            self.assertEqual(scores, sorted(scores, reverse=True))\\n\\n    def test_multi_stage_rank_documents_returns_list(self):\\n        \\\"\\\"\\\"Test that multi_stage_rank_documents returns a list.\\\"\\\"\\\"\\n        results = self.processor.multi_stage_rank_documents(\\\"neural networks\\\", top_n=3)\\n        self.assertIsInstance(results, list)\\n\\n    def test_multi_stage_rank_documents_structure(self):\\n        \\\"\\\"\\\"Test that document results have correct 3-tuple structure.\\\"\\\"\\\"\\n        results = self.processor.multi_stage_rank_documents(\\\"neural\\\", top_n=3)\\n        self.assertGreater(len(results), 0)\\n        doc_id, score, stage_scores = results[0]\\n        self.assertIsInstance(doc_id, str)\\n        self.assertIsInstance(score, float)\\n        self.assertIsInstance(stage_scores, dict)\\n\\n    def test_multi_stage_rank_documents_stage_scores(self):\\n        \\\"\\\"\\\"Test that document stage_scores contains expected keys.\\\"\\\"\\\"\\n        results = self.processor.multi_stage_rank_documents(\\\"neural networks\\\", top_n=3)\\n        self.assertGreater(len(results), 0)\\n        _, _, stage_scores = results[0]\\n        self.assertIn('concept_score', stage_scores)\\n        self.assertIn('tfidf_score', stage_scores)\\n        self.assertIn('combined_score', stage_scores)\\n\\n    def test_multi_stage_rank_documents_top_n(self):\\n        \\\"\\\"\\\"Test that top_n limits document results.\\\"\\\"\\\"\\n        results = self.processor.multi_stage_rank_documents(\\\"learning\\\", top_n=2)\\n        self.assertLessEqual(len(results), 2)\\n\\n    def test_multi_stage_rank_documents_sorted(self):\\n        \\\"\\\"\\\"Test that document results are sorted by score descending.\\\"\\\"\\\"\\n        results = self.processor.multi_stage_rank_documents(\\\"neural networks\\\", top_n=5)\\n        if len(results) > 1:\\n            scores = [score for _, score, _ in results]\\n            self.assertEqual(scores, sorted(scores, reverse=True))\\n\\n    def test_multi_stage_rank_empty_query(self):\\n        \\\"\\\"\\\"Test handling of query with no matches.\\\"\\\"\\\"\\n        results = self.processor.multi_stage_rank(\\\"xyznonexistent123\\\", top_n=3)\\n        self.assertEqual(len(results), 0)\\n\\n    def test_multi_stage_rank_without_expansion(self):\\n        \\\"\\\"\\\"Test multi-stage ranking without query expansion.\\\"\\\"\\\"\\n        results = self.processor.multi_stage_rank(\\n            \\\"neural\\\", top_n=3, use_expansion=False\\n        )\\n        self.assertIsInstance(results, list)\\n\\n    def test_multi_stage_vs_flat_ranking(self):\\n        \\\"\\\"\\\"Test that multi-stage ranking produces results comparable to flat ranking.\\\"\\\"\\\"\\n        # Both should find relevant documents for the same query\\n        multi_results = self.processor.multi_stage_rank(\\\"neural networks\\\", top_n=3)\\n        flat_results = self.processor.find_passages_for_query(\\\"neural networks\\\", top_n=3)\\n\\n        # Both should return results\\n        self.assertGreater(len(multi_results), 0)\\n        self.assertGreater(len(flat_results), 0)\\n\\n        # Both should find the neural_doc\\n        multi_docs = {doc_id for _, doc_id, _, _, _, _ in multi_results}\\n        flat_docs = {doc_id for _, doc_id, _, _, _ in flat_results}\\n        self.assertIn(\\\"neural_doc\\\", multi_docs)\\n        self.assertIn(\\\"neural_doc\\\", flat_docs)\\n\\n\\nclass TestProcessorIncrementalIndexing(unittest.TestCase):\\n    \\\"\\\"\\\"Test incremental document indexing functionality.\\\"\\\"\\\"\\n\\n    def setUp(self):\\n        self.processor = CorticalTextProcessor()\\n\\n    def test_add_document_incremental_returns_stats(self):\\n        \\\"\\\"\\\"Test that add_document_incremental returns processing stats.\\\"\\\"\\\"\\n        stats = self.processor.add_document_incremental(\\n            \\\"doc1\\\", \\\"Neural networks process information.\\\", recompute='tfidf'\\n        )\\n        self.assertIn('tokens', stats)\\n        self.assertIn('bigrams', stats)\\n        self.assertIn('unique_tokens', stats)\\n        self.assertGreater(stats['tokens'], 0)\\n\\n    def test_add_document_incremental_with_metadata(self):\\n        \\\"\\\"\\\"Test incremental add with metadata.\\\"\\\"\\\"\\n        self.processor.add_document_incremental(\\n            \\\"doc1\\\",\\n            \\\"Test content.\\\",\\n            metadata={\\\"source\\\": \\\"test\\\", \\\"author\\\": \\\"AI\\\"},\\n            recompute='tfidf'\\n        )\\n        metadata = self.processor.get_document_metadata(\\\"doc1\\\")\\n        self.assertEqual(metadata[\\\"source\\\"], \\\"test\\\")\\n        self.assertEqual(metadata[\\\"author\\\"], \\\"AI\\\")\\n\\n    def test_add_document_incremental_recompute_none(self):\\n        \\\"\\\"\\\"Test that recompute='none' marks computations as stale.\\\"\\\"\\\"\\n        self.processor.add_document_incremental(\\n            \\\"doc1\\\", \\\"Test content.\\\", recompute='none'\\n        )\\n        # Should be stale\\n        self.assertTrue(self.processor.is_stale(CorticalTextProcessor.COMP_TFIDF))\\n        self.assertTrue(self.processor.is_stale(CorticalTextProcessor.COMP_PAGERANK))\\n\\n    def test_add_document_incremental_recompute_tfidf(self):\\n        \\\"\\\"\\\"Test that recompute='tfidf' only recomputes TF-IDF.\\\"\\\"\\\"\\n        self.processor.add_document_incremental(\\n            \\\"doc1\\\", \\\"Test content.\\\", recompute='tfidf'\\n        )\\n        # TF-IDF should be fresh\\n        self.assertFalse(self.processor.is_stale(CorticalTextProcessor.COMP_TFIDF))\\n        # Other computations should be stale\\n        self.assertTrue(self.processor.is_stale(CorticalTextProcessor.COMP_PAGERANK))\\n\\n    def test_add_document_incremental_recompute_full(self):\\n        \\\"\\\"\\\"Test that recompute='full' clears all staleness.\\\"\\\"\\\"\\n        self.processor.add_document_incremental(\\n            \\\"doc1\\\", \\\"Test content.\\\", recompute='full'\\n        )\\n        # All should be fresh\\n        self.assertFalse(self.processor.is_stale(CorticalTextProcessor.COMP_TFIDF))\\n        self.assertFalse(self.processor.is_stale(CorticalTextProcessor.COMP_PAGERANK))\\n        self.assertFalse(self.processor.is_stale(CorticalTextProcessor.COMP_ACTIVATION))\\n\\n    def test_add_documents_batch_returns_stats(self):\\n        \\\"\\\"\\\"Test that add_documents_batch returns batch statistics.\\\"\\\"\\\"\\n        docs = [\\n            (\\\"doc1\\\", \\\"First document content.\\\", {\\\"source\\\": \\\"web\\\"}),\\n            (\\\"doc2\\\", \\\"Second document content.\\\", None),\\n            (\\\"doc3\\\", \\\"Third document content.\\\", {\\\"author\\\": \\\"AI\\\"}),\\n        ]\\n        stats = self.processor.add_documents_batch(docs, recompute='full', verbose=False)\\n        self.assertEqual(stats['documents_added'], 3)\\n        self.assertIn('total_tokens', stats)\\n        self.assertIn('total_bigrams', stats)\\n        self.assertEqual(stats['recomputation'], 'full')\\n\\n    def test_add_documents_batch_preserves_metadata(self):\\n        \\\"\\\"\\\"Test that batch add preserves metadata for all documents.\\\"\\\"\\\"\\n        docs = [\\n            (\\\"doc1\\\", \\\"First content.\\\", {\\\"type\\\": \\\"article\\\"}),\\n            (\\\"doc2\\\", \\\"Second content.\\\", {\\\"type\\\": \\\"paper\\\"}),\\n        ]\\n        self.processor.add_documents_batch(docs, recompute='tfidf', verbose=False)\\n        self.assertEqual(self.processor.get_document_metadata(\\\"doc1\\\")[\\\"type\\\"], \\\"article\\\")\\n        self.assertEqual(self.processor.get_document_metadata(\\\"doc2\\\")[\\\"type\\\"], \\\"paper\\\")\\n\\n    def test_add_documents_batch_recompute_none(self):\\n        \\\"\\\"\\\"Test batch add with no recomputation.\\\"\\\"\\\"\\n        docs = [(\\\"doc1\\\", \\\"Content one.\\\", None), (\\\"doc2\\\", \\\"Content two.\\\", None)]\\n        self.processor.add_documents_batch(docs, recompute='none', verbose=False)\\n        self.assertTrue(self.processor.is_stale(CorticalTextProcessor.COMP_TFIDF))\\n        self.assertEqual(len(self.processor.documents), 2)\\n\\n    def test_recompute_full(self):\\n        \\\"\\\"\\\"Test recompute with level='full'.\\\"\\\"\\\"\\n        self.processor.add_document_incremental(\\\"doc1\\\", \\\"Test content.\\\", recompute='none')\\n        recomputed = self.processor.recompute(level='full', verbose=False)\\n        self.assertIn(CorticalTextProcessor.COMP_TFIDF, recomputed)\\n        self.assertIn(CorticalTextProcessor.COMP_PAGERANK, recomputed)\\n        self.assertFalse(self.processor.is_stale(CorticalTextProcessor.COMP_TFIDF))\\n\\n    def test_recompute_tfidf(self):\\n        \\\"\\\"\\\"Test recompute with level='tfidf'.\\\"\\\"\\\"\\n        self.processor.add_document_incremental(\\\"doc1\\\", \\\"Test content.\\\", recompute='none')\\n        recomputed = self.processor.recompute(level='tfidf', verbose=False)\\n        self.assertEqual(recomputed, {CorticalTextProcessor.COMP_TFIDF: True})\\n        self.assertFalse(self.processor.is_stale(CorticalTextProcessor.COMP_TFIDF))\\n        # Others still stale\\n        self.assertTrue(self.processor.is_stale(CorticalTextProcessor.COMP_PAGERANK))\\n\\n    def test_recompute_stale_only(self):\\n        \\\"\\\"\\\"Test recompute with level='stale' (only recomputes stale items).\\\"\\\"\\\"\\n        self.processor.add_document_incremental(\\\"doc1\\\", \\\"Test content.\\\", recompute='tfidf')\\n        # Now only pagerank, activation, etc. are stale\\n        recomputed = self.processor.recompute(level='stale', verbose=False)\\n        # TF-IDF should NOT be in recomputed (it was already fresh)\\n        self.assertNotIn(CorticalTextProcessor.COMP_TFIDF, recomputed)\\n        # Others should be recomputed\\n        self.assertIn(CorticalTextProcessor.COMP_PAGERANK, recomputed)\\n\\n    def test_get_stale_computations(self):\\n        \\\"\\\"\\\"Test get_stale_computations returns correct set.\\\"\\\"\\\"\\n        self.processor.add_document_incremental(\\\"doc1\\\", \\\"Test content.\\\", recompute='tfidf')\\n        stale = self.processor.get_stale_computations()\\n        self.assertNotIn(CorticalTextProcessor.COMP_TFIDF, stale)\\n        self.assertIn(CorticalTextProcessor.COMP_PAGERANK, stale)\\n\\n    def test_is_stale(self):\\n        \\\"\\\"\\\"Test is_stale returns correct boolean.\\\"\\\"\\\"\\n        self.processor.add_document_incremental(\\\"doc1\\\", \\\"Test content.\\\", recompute='none')\\n        self.assertTrue(self.processor.is_stale(CorticalTextProcessor.COMP_TFIDF))\\n        self.processor.compute_tfidf(verbose=False)\\n        self.processor._mark_fresh(CorticalTextProcessor.COMP_TFIDF)\\n        self.assertFalse(self.processor.is_stale(CorticalTextProcessor.COMP_TFIDF))\\n\\n    def test_incremental_workflow(self):\\n        \\\"\\\"\\\"Test typical incremental indexing workflow.\\\"\\\"\\\"\\n        # Initial corpus\\n        self.processor.process_document(\\\"doc1\\\", \\\"Neural networks process information.\\\")\\n        self.processor.compute_all(verbose=False)\\n\\n        # Add new documents incrementally\\n        self.processor.add_document_incremental(\\n            \\\"doc2\\\", \\\"Machine learning algorithms.\\\", recompute='tfidf'\\n        )\\n\\n        # Search should work\\n        results = self.processor.find_documents_for_query(\\\"neural\\\", top_n=2)\\n        self.assertIsInstance(results, list)\\n\\n        # Full recompute when needed\\n        self.processor.recompute(level='full', verbose=False)\\n        self.assertEqual(len(self.processor.get_stale_computations()), 0)\\n\\n    def test_batch_then_query(self):\\n        \\\"\\\"\\\"Test batch add followed by querying.\\\"\\\"\\\"\\n        docs = [\\n            (\\\"neural\\\", \\\"Neural networks deep learning AI.\\\", None),\\n            (\\\"ml\\\", \\\"Machine learning algorithms models.\\\", None),\\n            (\\\"data\\\", \\\"Data processing storage retrieval.\\\", None),\\n        ]\\n        self.processor.add_documents_batch(docs, recompute='full', verbose=False)\\n\\n        results = self.processor.find_documents_for_query(\\\"neural networks\\\", top_n=3)\\n        self.assertGreater(len(results), 0)\\n        # The neural doc should rank highest\\n        self.assertEqual(results[0][0], \\\"neural\\\")\\n\\n\\nclass TestCrossLayerConnections(unittest.TestCase):\\n    \\\"\\\"\\\"Test cross-layer feedforward and feedback connections.\\\"\\\"\\\"\\n\\n    def setUp(self):\\n        self.processor = CorticalTextProcessor()\\n        self.processor.process_document(\\\"doc1\\\", \\\"Neural networks process information efficiently.\\\")\\n        self.processor.process_document(\\\"doc2\\\", \\\"Deep learning neural models are powerful.\\\")\\n        self.processor.compute_all(verbose=False)\\n\\n    def test_bigram_feedforward_connections(self):\\n        \\\"\\\"\\\"Test that bigrams have feedforward connections to component tokens.\\\"\\\"\\\"\\n        layer0 = self.processor.get_layer(CorticalLayer.TOKENS)\\n        layer1 = self.processor.get_layer(CorticalLayer.BIGRAMS)\\n\\n        bigram = layer1.get_minicolumn(\\\"neural networks\\\")\\n        self.assertIsNotNone(bigram)\\n        self.assertGreater(len(bigram.feedforward_connections), 0)\\n\\n        # Should connect to both \\\"neural\\\" and \\\"networks\\\"\\n        neural = layer0.get_minicolumn(\\\"neural\\\")\\n        networks = layer0.get_minicolumn(\\\"networks\\\")\\n        self.assertIn(neural.id, bigram.feedforward_connections)\\n        self.assertIn(networks.id, bigram.feedforward_connections)\\n\\n    def test_bigram_feedforward_weights(self):\\n        \\\"\\\"\\\"Test that bigram feedforward connections have accumulated weights.\\\"\\\"\\\"\\n        layer1 = self.processor.get_layer(CorticalLayer.BIGRAMS)\\n\\n        bigram = layer1.get_minicolumn(\\\"neural networks\\\")\\n        self.assertIsNotNone(bigram)\\n\\n        # Weight should be >= 1.0 (accumulated from occurrences)\\n        for target_id, weight in bigram.feedforward_connections.items():\\n            self.assertGreaterEqual(weight, 1.0)\\n\\n    def test_token_feedback_to_bigrams(self):\\n        \\\"\\\"\\\"Test that tokens have feedback connections to bigrams.\\\"\\\"\\\"\\n        layer0 = self.processor.get_layer(CorticalLayer.TOKENS)\\n        layer1 = self.processor.get_layer(CorticalLayer.BIGRAMS)\\n\\n        neural = layer0.get_minicolumn(\\\"neural\\\")\\n        self.assertIsNotNone(neural)\\n        self.assertGreater(len(neural.feedback_connections), 0)\\n\\n        # Should connect back to bigrams containing \\\"neural\\\"\\n        bigram = layer1.get_minicolumn(\\\"neural networks\\\")\\n        if bigram:\\n            self.assertIn(bigram.id, neural.feedback_connections)\\n\\n    def test_document_feedforward_connections(self):\\n        \\\"\\\"\\\"Test that documents have feedforward connections to tokens.\\\"\\\"\\\"\\n        layer0 = self.processor.get_layer(CorticalLayer.TOKENS)\\n        layer3 = self.processor.get_layer(CorticalLayer.DOCUMENTS)\\n\\n        doc = layer3.get_minicolumn(\\\"doc1\\\")\\n        self.assertIsNotNone(doc)\\n        self.assertGreater(len(doc.feedforward_connections), 0)\\n\\n        # Document should connect to tokens in its content\\n        neural = layer0.get_minicolumn(\\\"neural\\\")\\n        self.assertIn(neural.id, doc.feedforward_connections)\\n\\n    def test_document_feedforward_weights(self):\\n        \\\"\\\"\\\"Test that document feedforward weights reflect token frequency.\\\"\\\"\\\"\\n        layer0 = self.processor.get_layer(CorticalLayer.TOKENS)\\n        layer3 = self.processor.get_layer(CorticalLayer.DOCUMENTS)\\n\\n        doc = layer3.get_minicolumn(\\\"doc1\\\")\\n        neural = layer0.get_minicolumn(\\\"neural\\\")\\n\\n        # Weight should match occurrence count\\n        weight = doc.feedforward_connections.get(neural.id, 0)\\n        self.assertGreaterEqual(weight, 1.0)\\n\\n    def test_token_feedback_to_documents(self):\\n        \\\"\\\"\\\"Test that tokens have feedback connections to documents.\\\"\\\"\\\"\\n        layer0 = self.processor.get_layer(CorticalLayer.TOKENS)\\n        layer3 = self.processor.get_layer(CorticalLayer.DOCUMENTS)\\n\\n        neural = layer0.get_minicolumn(\\\"neural\\\")\\n        self.assertIsNotNone(neural)\\n\\n        # Should connect to documents containing this token\\n        doc1 = layer3.get_minicolumn(\\\"doc1\\\")\\n        doc2 = layer3.get_minicolumn(\\\"doc2\\\")\\n        self.assertIn(doc1.id, neural.feedback_connections)\\n        self.assertIn(doc2.id, neural.feedback_connections)\\n\\n    def test_concept_feedforward_connections(self):\\n        \\\"\\\"\\\"Test that concepts have feedforward connections to member tokens.\\\"\\\"\\\"\\n        layer2 = self.processor.get_layer(CorticalLayer.CONCEPTS)\\n\\n        if layer2.column_count() > 0:\\n            # Get first concept\\n            concept = list(layer2.minicolumns.values())[0]\\n            self.assertGreater(len(concept.feedforward_connections), 0)\\n\\n            # All feedforward targets should be in feedforward_sources too\\n            for target_id in concept.feedforward_connections:\\n                self.assertIn(target_id, concept.feedforward_sources)\\n\\n    def test_concept_feedforward_weights_by_pagerank(self):\\n        \\\"\\\"\\\"Test that concept feedforward weights are based on token PageRank.\\\"\\\"\\\"\\n        layer0 = self.processor.get_layer(CorticalLayer.TOKENS)\\n        layer2 = self.processor.get_layer(CorticalLayer.CONCEPTS)\\n\\n        if layer2.column_count() > 0:\\n            concept = list(layer2.minicolumns.values())[0]\\n\\n            # Weights should be normalized (max = 1.0)\\n            max_weight = max(concept.feedforward_connections.values())\\n            self.assertLessEqual(max_weight, 1.0 + 0.001)  # Allow small float error\\n\\n    def test_token_feedback_to_concepts(self):\\n        \\\"\\\"\\\"Test that tokens have feedback connections to concepts.\\\"\\\"\\\"\\n        layer0 = self.processor.get_layer(CorticalLayer.TOKENS)\\n        layer2 = self.processor.get_layer(CorticalLayer.CONCEPTS)\\n\\n        if layer2.column_count() > 0:\\n            concept = list(layer2.minicolumns.values())[0]\\n\\n            # Get a member token\\n            if concept.feedforward_connections:\\n                member_id = list(concept.feedforward_connections.keys())[0]\\n                member = layer0.get_by_id(member_id)\\n                if member:\\n                    self.assertIn(concept.id, member.feedback_connections)\\n\\n    def test_cross_layer_bidirectional(self):\\n        \\\"\\\"\\\"Test that cross-layer connections are bidirectional.\\\"\\\"\\\"\\n        layer0 = self.processor.get_layer(CorticalLayer.TOKENS)\\n        layer1 = self.processor.get_layer(CorticalLayer.BIGRAMS)\\n\\n        bigram = layer1.get_minicolumn(\\\"neural networks\\\")\\n        if bigram:\\n            for target_id in bigram.feedforward_connections:\\n                token = layer0.get_by_id(target_id)\\n                if token:\\n                    self.assertIn(bigram.id, token.feedback_connections)\\n\\n    def test_persistence_cross_layer_connections(self):\\n        \\\"\\\"\\\"Test that cross-layer connections are saved and loaded correctly.\\\"\\\"\\\"\\n        import tempfile\\n\\n        layer1 = self.processor.get_layer(CorticalLayer.BIGRAMS)\\n        bigram = layer1.get_minicolumn(\\\"neural networks\\\")\\n        original_ff = dict(bigram.feedforward_connections) if bigram else {}\\n\\n        with tempfile.NamedTemporaryFile(suffix='.pkl', delete=False) as f:\\n            path = f.name\\n\\n        try:\\n            self.processor.save(path)\\n            loaded = CorticalTextProcessor.load(path)\\n\\n            loaded_layer1 = loaded.get_layer(CorticalLayer.BIGRAMS)\\n            loaded_bigram = loaded_layer1.get_minicolumn(\\\"neural networks\\\")\\n\\n            if bigram and loaded_bigram:\\n                self.assertEqual(\\n                    loaded_bigram.feedforward_connections,\\n                    original_ff\\n                )\\n        finally:\\n            os.unlink(path)\\n\\n    def test_cross_layer_connection_count(self):\\n        \\\"\\\"\\\"Test counting cross-layer connections.\\\"\\\"\\\"\\n        layer1 = self.processor.get_layer(CorticalLayer.BIGRAMS)\\n\\n        total_ff = 0\\n        for col in layer1.minicolumns.values():\\n            total_ff += len(col.feedforward_connections)\\n\\n        # Each bigram should have 2 feedforward connections (to its 2 tokens)\\n        # So total should be approximately 2 * number of bigrams\\n        self.assertGreater(total_ff, 0)\\n\\n\\nclass TestConceptConnections(unittest.TestCase):\\n    \\\"\\\"\\\"Test concept-level lateral connections.\\\"\\\"\\\"\\n\\n    def setUp(self):\\n        self.processor = CorticalTextProcessor()\\n        # Create documents with overlapping topics\\n        self.processor.process_document(\\\"neural_doc\\\",\\n            \\\"Neural networks process information using deep learning algorithms.\\\")\\n        self.processor.process_document(\\\"ml_doc\\\",\\n            \\\"Machine learning algorithms learn patterns from data using neural methods.\\\")\\n        self.processor.process_document(\\\"data_doc\\\",\\n            \\\"Data processing systems analyze information patterns efficiently.\\\")\\n        self.processor.process_document(\\\"unrelated_doc\\\",\\n            \\\"Ancient pottery techniques involve clay and firing in kilns.\\\")\\n        self.processor.compute_all(verbose=False)\\n\\n    def test_concepts_have_lateral_connections(self):\\n        \\\"\\\"\\\"Test that concepts have lateral connections when documents overlap.\\\"\\\"\\\"\\n        # Create a processor with documents that will create multiple overlapping concepts\\n        processor = CorticalTextProcessor()\\n        # Add many documents with overlapping terms to force multiple concept clusters\\n        processor.process_document(\\\"doc1\\\", \\\"Neural networks deep learning artificial intelligence models.\\\")\\n        processor.process_document(\\\"doc2\\\", \\\"Machine learning algorithms data science models.\\\")\\n        processor.process_document(\\\"doc3\\\", \\\"Deep learning neural networks training optimization.\\\")\\n        processor.process_document(\\\"doc4\\\", \\\"Data analysis machine learning statistical models.\\\")\\n        processor.process_document(\\\"doc5\\\", \\\"Artificial intelligence reasoning knowledge graphs.\\\")\\n        processor.process_document(\\\"doc6\\\", \\\"Knowledge representation semantic networks graphs.\\\")\\n        processor.compute_all(verbose=False)\\n\\n        layer2 = processor.get_layer(CorticalLayer.CONCEPTS)\\n\\n        # If we have multiple concepts with overlapping docs, they should connect\\n        if layer2.column_count() > 1:\\n            # Check if any concepts share documents\\n            concepts = list(layer2.minicolumns.values())\\n            has_overlap = False\\n            for i, c1 in enumerate(concepts):\\n                for c2 in concepts[i+1:]:\\n                    if c1.document_ids & c2.document_ids:\\n                        has_overlap = True\\n                        break\\n\\n            if has_overlap:\\n                total_connections = sum(\\n                    len(c.lateral_connections) for c in layer2.minicolumns.values()\\n                )\\n                self.assertGreater(total_connections, 0)\\n\\n    def test_concept_connections_based_on_jaccard(self):\\n        \\\"\\\"\\\"Test that concept connections are based on document overlap.\\\"\\\"\\\"\\n        layer2 = self.processor.get_layer(CorticalLayer.CONCEPTS)\\n\\n        if layer2.column_count() > 1:\\n            concepts = list(layer2.minicolumns.values())\\n            # Find concepts with connections\\n            connected_concepts = [c for c in concepts if c.lateral_connections]\\n\\n            for concept in connected_concepts:\\n                for target_id, weight in concept.lateral_connections.items():\\n                    # Weight should be based on Jaccard (0 < weight <= 1.5 with semantic boost)\\n                    self.assertGreater(weight, 0)\\n                    self.assertLessEqual(weight, 2.0)  # Max with semantic boost\\n\\n    def test_compute_concept_connections_method(self):\\n        \\\"\\\"\\\"Test the compute_concept_connections method directly.\\\"\\\"\\\"\\n        # Clear existing connections\\n        layer2 = self.processor.get_layer(CorticalLayer.CONCEPTS)\\n        for concept in layer2.minicolumns.values():\\n            concept.lateral_connections.clear()\\n\\n        # Recompute\\n        stats = self.processor.compute_concept_connections(verbose=False)\\n\\n        self.assertIn('connections_created', stats)\\n        self.assertIn('concepts', stats)\\n        self.assertGreaterEqual(stats['connections_created'], 0)\\n\\n    def test_concept_connections_with_semantics(self):\\n        \\\"\\\"\\\"Test that semantic relations boost connection weights.\\\"\\\"\\\"\\n        # Extract semantics first\\n        self.processor.extract_corpus_semantics(verbose=False)\\n\\n        # Clear and recompute with semantics\\n        layer2 = self.processor.get_layer(CorticalLayer.CONCEPTS)\\n        for concept in layer2.minicolumns.values():\\n            concept.lateral_connections.clear()\\n\\n        stats_with = self.processor.compute_concept_connections(\\n            use_semantics=True, verbose=False\\n        )\\n\\n        # Clear and recompute without semantics\\n        for concept in layer2.minicolumns.values():\\n            concept.lateral_connections.clear()\\n\\n        stats_without = self.processor.compute_concept_connections(\\n            use_semantics=False, verbose=False\\n        )\\n\\n        # Both should work\\n        self.assertGreaterEqual(stats_with['connections_created'], 0)\\n        self.assertGreaterEqual(stats_without['connections_created'], 0)\\n\\n    def test_concept_connections_min_jaccard_filter(self):\\n        \\\"\\\"\\\"Test that min_jaccard threshold filters connections.\\\"\\\"\\\"\\n        layer2 = self.processor.get_layer(CorticalLayer.CONCEPTS)\\n\\n        # Clear connections\\n        for concept in layer2.minicolumns.values():\\n            concept.lateral_connections.clear()\\n\\n        # With low threshold\\n        stats_low = self.processor.compute_concept_connections(\\n            min_jaccard=0.01, verbose=False\\n        )\\n\\n        # Clear again\\n        for concept in layer2.minicolumns.values():\\n            concept.lateral_connections.clear()\\n\\n        # With high threshold\\n        stats_high = self.processor.compute_concept_connections(\\n            min_jaccard=0.9, verbose=False\\n        )\\n\\n        # Low threshold should create >= high threshold connections\\n        self.assertGreaterEqual(\\n            stats_low['connections_created'],\\n            stats_high['connections_created']\\n        )\\n\\n    def test_concept_connections_bidirectional(self):\\n        \\\"\\\"\\\"Test that concept connections are bidirectional.\\\"\\\"\\\"\\n        layer2 = self.processor.get_layer(CorticalLayer.CONCEPTS)\\n\\n        for concept in layer2.minicolumns.values():\\n            for target_id, weight in concept.lateral_connections.items():\\n                target = layer2.get_by_id(target_id)\\n                if target:\\n                    # Target should have connection back to this concept\\n                    self.assertIn(concept.id, target.lateral_connections)\\n\\n    def test_concept_connections_empty_layer(self):\\n        \\\"\\\"\\\"Test concept connections with empty concept layer.\\\"\\\"\\\"\\n        processor = CorticalTextProcessor()\\n        processor.process_document(\\\"doc1\\\", \\\"Hello world.\\\")\\n        processor.compute_all(verbose=False, build_concepts=False)\\n\\n        layer2 = processor.get_layer(CorticalLayer.CONCEPTS)\\n        self.assertEqual(layer2.column_count(), 0)\\n\\n        # Should handle empty layer gracefully\\n        stats = processor.compute_concept_connections(verbose=False)\\n        self.assertEqual(stats['connections_created'], 0)\\n        self.assertEqual(stats['concepts'], 0)\\n\\n    def test_isolated_concepts_not_connected(self):\\n        \\\"\\\"\\\"Test that concepts with no document overlap don't connect.\\\"\\\"\\\"\\n        # The unrelated_doc about pottery should form isolated concepts\\n        layer2 = self.processor.get_layer(CorticalLayer.CONCEPTS)\\n\\n        if layer2.column_count() > 0:\\n            # At least some concepts should be isolated if topics are different\\n            # This is a soft test since clustering may group differently\\n            pass  # Concept isolation depends on clustering results\\n\\n    def test_concept_connections_zero_thresholds(self):\\n        \\\"\\\"\\\"Test that min_shared_docs=0 and min_jaccard=0 allow all connections.\\\"\\\"\\\"\\n        # Create processor with documents that have NO overlap but enough content\\n        # to form distinct concept clusters\\n        processor = CorticalTextProcessor()\\n        processor.process_document(\\n            \\\"doc1\\\",\\n            \\\"Neural networks learn patterns from data using algorithms. \\\"\\n            \\\"Deep learning models process information through layers. \\\"\\n            \\\"Machine learning systems train on examples to improve accuracy.\\\"\\n        )\\n        processor.process_document(\\n            \\\"doc2\\\",\\n            \\\"Bread baking requires yeast and flour for fermentation. \\\"\\n            \\\"Sourdough starters contain wild yeast and bacteria cultures. \\\"\\n            \\\"Kneading dough develops gluten structure for texture.\\\"\\n        )\\n        processor.compute_all(verbose=False, build_concepts=False)\\n        processor.build_concept_clusters(min_cluster_size=2, verbose=False)\\n\\n        layer2 = processor.get_layer(CorticalLayer.CONCEPTS)\\n        self.assertGreaterEqual(layer2.column_count(), 2, \\\"Need at least 2 concepts\\\")\\n\\n        # Clear connections\\n        for concept in layer2.minicolumns.values():\\n            concept.lateral_connections.clear()\\n\\n        # With default thresholds, should get 0 connections (no doc overlap)\\n        stats_default = processor.compute_concept_connections(verbose=False)\\n\\n        # Clear again\\n        for concept in layer2.minicolumns.values():\\n            concept.lateral_connections.clear()\\n\\n        # With zero thresholds, all pairs can connect (if they pass other checks)\\n        stats_zero = processor.compute_concept_connections(\\n            min_shared_docs=0,\\n            min_jaccard=0.0,\\n            verbose=False\\n        )\\n\\n        # Zero thresholds should allow at least as many connections\\n        self.assertGreaterEqual(\\n            stats_zero['connections_created'],\\n            stats_default['connections_created']\\n        )\\n\\n    def test_concept_connections_member_semantics(self):\\n        \\\"\\\"\\\"Test that use_member_semantics creates connections via semantic relations.\\\"\\\"\\\"\\n        processor = CorticalTextProcessor()\\n        # Create documents with semantically related but non-overlapping content\\n        processor.process_document(\\n            \\\"doc1\\\",\\n            \\\"Dogs are animals that bark and run in parks. \\\"\\n            \\\"Canines make loyal pets and companions for families. \\\"\\n            \\\"Puppies require training and socialization early.\\\"\\n        )\\n        processor.process_document(\\n            \\\"doc2\\\",\\n            \\\"Cats are animals that meow and climb on furniture. \\\"\\n            \\\"Felines are independent pets that groom themselves. \\\"\\n            \\\"Kittens play with toys and explore their surroundings.\\\"\\n        )\\n        processor.process_document(\\n            \\\"doc3\\\",\\n            \\\"Quantum physics studies subatomic particle behavior. \\\"\\n            \\\"Electrons orbit atomic nuclei in probability clouds. \\\"\\n            \\\"Wave functions describe quantum mechanical states.\\\"\\n        )\\n        processor.compute_all(verbose=False, build_concepts=False)\\n        processor.build_concept_clusters(min_cluster_size=2, verbose=False)\\n        processor.extract_corpus_semantics(verbose=False)\\n\\n        layer2 = processor.get_layer(CorticalLayer.CONCEPTS)\\n        self.assertGreaterEqual(layer2.column_count(), 2, \\\"Need at least 2 concepts\\\")\\n\\n        # Clear connections\\n        for concept in layer2.minicolumns.values():\\n            concept.lateral_connections.clear()\\n\\n        # With member semantics enabled\\n        stats = processor.compute_concept_connections(\\n            use_member_semantics=True,\\n            verbose=False\\n        )\\n\\n        # Should have statistics for semantic connections\\n        self.assertIn('semantic_connections', stats)\\n        self.assertIn('doc_overlap_connections', stats)\\n\\n    def test_concept_connections_embedding_similarity(self):\\n        \\\"\\\"\\\"Test that use_embedding_similarity creates connections via embeddings.\\\"\\\"\\\"\\n        processor = CorticalTextProcessor()\\n        processor.process_document(\\n            \\\"doc1\\\",\\n            \\\"Neural networks process information through layers. \\\"\\n            \\\"Artificial intelligence learns patterns from training data. \\\"\\n            \\\"Machine learning algorithms optimize model parameters.\\\"\\n        )\\n        processor.process_document(\\n            \\\"doc2\\\",\\n            \\\"Cooking recipes require specific ingredients and techniques. \\\"\\n            \\\"Chefs prepare dishes using various culinary methods. \\\"\\n            \\\"Kitchen equipment helps with food preparation tasks.\\\"\\n        )\\n        processor.compute_all(verbose=False, build_concepts=False)\\n        processor.build_concept_clusters(min_cluster_size=2, verbose=False)\\n        processor.compute_graph_embeddings(verbose=False)\\n\\n        layer2 = processor.get_layer(CorticalLayer.CONCEPTS)\\n        self.assertGreaterEqual(layer2.column_count(), 2, \\\"Need at least 2 concepts\\\")\\n\\n        # Clear connections\\n        for concept in layer2.minicolumns.values():\\n            concept.lateral_connections.clear()\\n\\n        # With embedding similarity enabled\\n        stats = processor.compute_concept_connections(\\n            use_embedding_similarity=True,\\n            embedding_threshold=0.1,  # Low threshold to catch similarities\\n            verbose=False\\n        )\\n\\n        # Should have statistics for embedding connections\\n        self.assertIn('embedding_connections', stats)\\n\\n    def test_concept_connections_combined_strategies(self):\\n        \\\"\\\"\\\"Test combining multiple connection strategies.\\\"\\\"\\\"\\n        processor = CorticalTextProcessor()\\n        processor.process_document(\\n            \\\"doc1\\\",\\n            \\\"Machine learning algorithms process data efficiently. \\\"\\n            \\\"Neural networks train on large datasets to find patterns. \\\"\\n            \\\"Supervised learning requires labeled training examples.\\\"\\n        )\\n        processor.process_document(\\n            \\\"doc2\\\",\\n            \\\"Ocean waves crash against rocky coastal shores. \\\"\\n            \\\"Marine biology studies creatures living underwater. \\\"\\n            \\\"Coral reefs provide habitat for diverse fish species.\\\"\\n        )\\n        processor.process_document(\\n            \\\"doc3\\\",\\n            \\\"Ancient history explores civilizations from the past. \\\"\\n            \\\"Archaeological excavations uncover buried artifacts. \\\"\\n            \\\"Museums preserve historical objects for education.\\\"\\n        )\\n        processor.compute_all(verbose=False, build_concepts=False)\\n        processor.build_concept_clusters(min_cluster_size=2, verbose=False)\\n        processor.extract_corpus_semantics(verbose=False)\\n        processor.compute_graph_embeddings(verbose=False)\\n\\n        layer2 = processor.get_layer(CorticalLayer.CONCEPTS)\\n        self.assertGreaterEqual(layer2.column_count(), 2, \\\"Need at least 2 concepts\\\")\\n\\n        # Clear connections\\n        for concept in layer2.minicolumns.values():\\n            concept.lateral_connections.clear()\\n\\n        # Enable all strategies\\n        stats = processor.compute_concept_connections(\\n            use_semantics=True,\\n            use_member_semantics=True,\\n            use_embedding_similarity=True,\\n            min_shared_docs=0,\\n            min_jaccard=0.0,\\n            embedding_threshold=0.1,\\n            verbose=False\\n        )\\n\\n        # Total should equal sum of individual strategy connections\\n        total = (\\n            stats.get('doc_overlap_connections', 0) +\\n            stats.get('semantic_connections', 0) +\\n            stats.get('embedding_connections', 0)\\n        )\\n        self.assertEqual(stats['connections_created'], total)\\n\\n    def test_concept_connections_returns_detailed_stats(self):\\n        \\\"\\\"\\\"Test that compute_concept_connections returns detailed statistics.\\\"\\\"\\\"\\n        stats = self.processor.compute_concept_connections(verbose=False)\\n\\n        # Check all expected keys are present\\n        self.assertIn('connections_created', stats)\\n        self.assertIn('concepts', stats)\\n        self.assertIn('doc_overlap_connections', stats)\\n        self.assertIn('semantic_connections', stats)\\n        self.assertIn('embedding_connections', stats)\\n\\n\\nclass TestConceptClustering(unittest.TestCase):\\n    \\\"\\\"\\\"Test concept clustering with strictness and bridging parameters.\\\"\\\"\\\"\\n\\n    def test_cluster_strictness_parameter(self):\\n        \\\"\\\"\\\"Test that cluster_strictness affects number of clusters.\\\"\\\"\\\"\\n        processor = CorticalTextProcessor()\\n        processor.process_document(\\n            \\\"doc1\\\", \\\"Neural networks process information using layers.\\\"\\n        )\\n        processor.process_document(\\n            \\\"doc2\\\", \\\"Machine learning algorithms process data patterns.\\\"\\n        )\\n        processor.compute_importance(verbose=False)\\n        processor.compute_tfidf(verbose=False)\\n\\n        # Strict clustering (default)\\n        clusters_strict = processor.build_concept_clusters(\\n            cluster_strictness=1.0, verbose=False\\n        )\\n\\n        # Reset concepts layer\\n        processor.layers[CorticalLayer.CONCEPTS] = HierarchicalLayer(CorticalLayer.CONCEPTS)\\n\\n        # Loose clustering\\n        clusters_loose = processor.build_concept_clusters(\\n            cluster_strictness=0.3, verbose=False\\n        )\\n\\n        # Both should return valid cluster dictionaries\\n        self.assertIsInstance(clusters_strict, dict)\\n        self.assertIsInstance(clusters_loose, dict)\\n\\n    def test_bridge_weight_parameter(self):\\n        \\\"\\\"\\\"Test that bridge_weight enables cross-document connections.\\\"\\\"\\\"\\n        processor = CorticalTextProcessor()\\n        processor.process_document(\\n            \\\"doc1\\\", \\\"Neural networks learn patterns from data.\\\"\\n        )\\n        processor.process_document(\\n            \\\"doc2\\\", \\\"Bread baking requires yeast and flour.\\\"\\n        )\\n        processor.compute_importance(verbose=False)\\n        processor.compute_tfidf(verbose=False)\\n\\n        # No bridging (default)\\n        clusters_no_bridge = processor.build_concept_clusters(\\n            bridge_weight=0.0, verbose=False\\n        )\\n\\n        # Reset concepts layer\\n        processor.layers[CorticalLayer.CONCEPTS] = HierarchicalLayer(CorticalLayer.CONCEPTS)\\n\\n        # With bridging\\n        clusters_with_bridge = processor.build_concept_clusters(\\n            bridge_weight=0.5, verbose=False\\n        )\\n\\n        # Both should produce valid results\\n        self.assertIsInstance(clusters_no_bridge, dict)\\n        self.assertIsInstance(clusters_with_bridge, dict)\\n\\n    def test_combined_clustering_parameters(self):\\n        \\\"\\\"\\\"Test combining strictness and bridging parameters.\\\"\\\"\\\"\\n        processor = CorticalTextProcessor()\\n        processor.process_document(\\n            \\\"doc1\\\", \\\"Neural networks are computational models.\\\"\\n        )\\n        processor.process_document(\\n            \\\"doc2\\\", \\\"Deep learning uses neural networks for AI.\\\"\\n        )\\n        processor.compute_importance(verbose=False)\\n        processor.compute_tfidf(verbose=False)\\n\\n        # Combined loose clustering with bridging\\n        clusters = processor.build_concept_clusters(\\n            cluster_strictness=0.5,\\n            bridge_weight=0.3,\\n            min_cluster_size=2,\\n            verbose=False\\n        )\\n\\n        self.assertIsInstance(clusters, dict)\\n\\n    def test_min_cluster_size_filter(self):\\n        \\\"\\\"\\\"Test that min_cluster_size filters small clusters.\\\"\\\"\\\"\\n        processor = CorticalTextProcessor()\\n        processor.process_document(\\n            \\\"doc1\\\", \\\"Neural networks process information efficiently.\\\"\\n        )\\n        processor.compute_importance(verbose=False)\\n        processor.compute_tfidf(verbose=False)\\n\\n        # Large minimum size should produce fewer clusters\\n        clusters_large_min = processor.build_concept_clusters(\\n            min_cluster_size=10, verbose=False\\n        )\\n\\n        # Reset concepts layer\\n        processor.layers[CorticalLayer.CONCEPTS] = HierarchicalLayer(CorticalLayer.CONCEPTS)\\n\\n        # Small minimum size\\n        clusters_small_min = processor.build_concept_clusters(\\n            min_cluster_size=2, verbose=False\\n        )\\n\\n        # Small min should allow at least as many clusters\\n        self.assertGreaterEqual(len(clusters_small_min), len(clusters_large_min))\\n\\n    def test_cluster_strictness_bounds(self):\\n        \\\"\\\"\\\"Test that cluster_strictness is clamped to valid range.\\\"\\\"\\\"\\n        processor = CorticalTextProcessor()\\n        processor.process_document(\\\"doc1\\\", \\\"Test document with words.\\\")\\n        processor.compute_importance(verbose=False)\\n        processor.compute_tfidf(verbose=False)\\n\\n        # Should handle out-of-range values gracefully\\n        clusters_negative = processor.build_concept_clusters(\\n            cluster_strictness=-0.5, verbose=False\\n        )\\n        self.assertIsInstance(clusters_negative, dict)\\n\\n        processor.layers[CorticalLayer.CONCEPTS] = HierarchicalLayer(CorticalLayer.CONCEPTS)\\n\\n        clusters_over = processor.build_concept_clusters(\\n            cluster_strictness=1.5, verbose=False\\n        )\\n        self.assertIsInstance(clusters_over, dict)\\n\\n\\nclass TestComputeAllStrategies(unittest.TestCase):\\n    \\\"\\\"\\\"Test compute_all with different connection strategies.\\\"\\\"\\\"\\n\\n    def test_compute_all_default_strategy(self):\\n        \\\"\\\"\\\"Test compute_all with default document_overlap strategy.\\\"\\\"\\\"\\n        processor = CorticalTextProcessor()\\n        processor.process_document(\\\"doc1\\\", \\\"Neural networks process information.\\\")\\n        processor.process_document(\\\"doc2\\\", \\\"Machine learning uses neural networks.\\\")\\n\\n        stats = processor.compute_all(verbose=False)\\n\\n        self.assertIsInstance(stats, dict)\\n        if 'concept_connections' in stats:\\n            self.assertIn('connections_created', stats['concept_connections'])\\n\\n    def test_compute_all_semantic_strategy(self):\\n        \\\"\\\"\\\"Test compute_all with semantic connection strategy.\\\"\\\"\\\"\\n        processor = CorticalTextProcessor()\\n        processor.process_document(\\\"doc1\\\", \\\"Dogs are animals that bark.\\\")\\n        processor.process_document(\\\"doc2\\\", \\\"Cats are animals that meow.\\\")\\n\\n        stats = processor.compute_all(\\n            connection_strategy='semantic',\\n            verbose=False\\n        )\\n\\n        self.assertIsInstance(stats, dict)\\n\\n    def test_compute_all_embedding_strategy(self):\\n        \\\"\\\"\\\"Test compute_all with embedding connection strategy.\\\"\\\"\\\"\\n        processor = CorticalTextProcessor()\\n        processor.process_document(\\\"doc1\\\", \\\"Neural networks learn patterns.\\\")\\n        processor.process_document(\\\"doc2\\\", \\\"Deep learning models train on data.\\\")\\n\\n        stats = processor.compute_all(\\n            connection_strategy='embedding',\\n            verbose=False\\n        )\\n\\n        self.assertIsInstance(stats, dict)\\n\\n    def test_compute_all_hybrid_strategy(self):\\n        \\\"\\\"\\\"Test compute_all with hybrid connection strategy.\\\"\\\"\\\"\\n        processor = CorticalTextProcessor()\\n        processor.process_document(\\\"doc1\\\", \\\"Neural networks process information.\\\")\\n        processor.process_document(\\\"doc2\\\", \\\"Bread baking requires yeast.\\\")\\n\\n        stats = processor.compute_all(\\n            connection_strategy='hybrid',\\n            cluster_strictness=0.5,\\n            bridge_weight=0.3,\\n            verbose=False\\n        )\\n\\n        self.assertIsInstance(stats, dict)\\n        if 'concept_connections' in stats:\\n            # Hybrid should have all connection type stats\\n            conn_stats = stats['concept_connections']\\n            self.assertIn('doc_overlap_connections', conn_stats)\\n            self.assertIn('semantic_connections', conn_stats)\\n            self.assertIn('embedding_connections', conn_stats)\\n\\n    def test_compute_all_returns_cluster_count(self):\\n        \\\"\\\"\\\"Test that compute_all returns cluster count in stats.\\\"\\\"\\\"\\n        processor = CorticalTextProcessor()\\n        processor.process_document(\\\"doc1\\\", \\\"Neural networks learn patterns from data.\\\")\\n        processor.process_document(\\\"doc2\\\", \\\"Machine learning algorithms process information.\\\")\\n\\n        stats = processor.compute_all(verbose=False)\\n\\n        if 'clusters_created' in stats:\\n            self.assertIsInstance(stats['clusters_created'], int)\\n            self.assertGreaterEqual(stats['clusters_created'], 0)\\n\\n    def test_compute_all_with_clustering_params(self):\\n        \\\"\\\"\\\"Test compute_all with clustering parameters.\\\"\\\"\\\"\\n        processor = CorticalTextProcessor()\\n        processor.process_document(\\\"doc1\\\", \\\"Neural networks are computational models.\\\")\\n        processor.process_document(\\\"doc2\\\", \\\"Deep learning uses neural architectures.\\\")\\n\\n        stats = processor.compute_all(\\n            cluster_strictness=0.3,\\n            bridge_weight=0.5,\\n            verbose=False\\n        )\\n\\n        self.assertIsInstance(stats, dict)\\n\\n\\nclass TestBigramConnections(unittest.TestCase):\\n    \\\"\\\"\\\"Test bigram lateral connection functionality.\\\"\\\"\\\"\\n\\n    @classmethod\\n    def setUpClass(cls):\\n        \\\"\\\"\\\"Set up processor with documents containing related bigrams.\\\"\\\"\\\"\\n        cls.processor = CorticalTextProcessor()\\n        # Documents with overlapping bigrams to test connections\\n        cls.processor.process_document(\\n            \\\"doc1\\\",\\n            \\\"Neural networks process information. Neural processing enables \\\"\\n            \\\"deep learning. Machine learning algorithms process data.\\\"\\n        )\\n        cls.processor.process_document(\\n            \\\"doc2\\\",\\n            \\\"Deep learning models use neural networks. Machine learning \\\"\\n            \\\"is related to deep learning and neural processing.\\\"\\n        )\\n        cls.processor.process_document(\\n            \\\"doc3\\\",\\n            \\\"Learning algorithms improve performance. Machine learning \\\"\\n            \\\"and deep learning are popular approaches.\\\"\\n        )\\n        cls.processor.compute_all(verbose=False)\\n\\n    def test_compute_bigram_connections_returns_stats(self):\\n        \\\"\\\"\\\"Test that compute_bigram_connections returns expected statistics.\\\"\\\"\\\"\\n        # Connections are already computed by compute_all, so create new processor\\n        processor = CorticalTextProcessor()\\n        processor.process_document(\\\"doc1\\\", \\\"Neural networks process data. Neural processing works.\\\")\\n        processor.compute_tfidf(verbose=False)\\n\\n        stats = processor.compute_bigram_connections(verbose=False)\\n\\n        self.assertIn('connections_created', stats)\\n        self.assertIn('bigrams', stats)\\n        self.assertIn('component_connections', stats)\\n        self.assertIn('chain_connections', stats)\\n        self.assertIn('cooccurrence_connections', stats)\\n\\n    def test_shared_left_component_connection(self):\\n        \\\"\\\"\\\"Test that bigrams sharing left component are connected.\\\"\\\"\\\"\\n        # \\\"neural networks\\\" and \\\"neural processing\\\" share \\\"neural\\\"\\n        # Note: bigrams use space separators (tokenizer.py:179)\\n        layer1 = self.processor.get_layer(CorticalLayer.BIGRAMS)\\n\\n        neural_networks = layer1.get_minicolumn(\\\"neural networks\\\")\\n        neural_processing = layer1.get_minicolumn(\\\"neural processing\\\")\\n\\n        # Verify bigrams exist\\n        self.assertIsNotNone(neural_networks, \\\"Bigram 'neural networks' should exist\\\")\\n        self.assertIsNotNone(neural_processing, \\\"Bigram 'neural processing' should exist\\\")\\n\\n        # They should be connected via shared \\\"neural\\\" component\\n        self.assertIn(neural_processing.id, neural_networks.lateral_connections)\\n        self.assertIn(neural_networks.id, neural_processing.lateral_connections)\\n\\n    def test_shared_right_component_connection(self):\\n        \\\"\\\"\\\"Test that bigrams sharing right component are connected.\\\"\\\"\\\"\\n        # \\\"machine learning\\\" and \\\"deep learning\\\" share \\\"learning\\\"\\n        layer1 = self.processor.get_layer(CorticalLayer.BIGRAMS)\\n\\n        machine_learning = layer1.get_minicolumn(\\\"machine learning\\\")\\n        deep_learning = layer1.get_minicolumn(\\\"deep learning\\\")\\n\\n        # Verify bigrams exist\\n        self.assertIsNotNone(machine_learning, \\\"Bigram 'machine learning' should exist\\\")\\n        self.assertIsNotNone(deep_learning, \\\"Bigram 'deep learning' should exist\\\")\\n\\n        # They should be connected via shared \\\"learning\\\" component\\n        self.assertIn(deep_learning.id, machine_learning.lateral_connections)\\n        self.assertIn(machine_learning.id, deep_learning.lateral_connections)\\n\\n    def test_chain_connections(self):\\n        \\\"\\\"\\\"Test that chain bigrams are connected (right of one = left of other).\\\"\\\"\\\"\\n        # \\\"machine learning\\\" and \\\"learning algorithms\\\" form a chain\\n        layer1 = self.processor.get_layer(CorticalLayer.BIGRAMS)\\n\\n        machine_learning = layer1.get_minicolumn(\\\"machine learning\\\")\\n        learning_algorithms = layer1.get_minicolumn(\\\"learning algorithms\\\")\\n\\n        # Verify bigrams exist\\n        self.assertIsNotNone(machine_learning, \\\"Bigram 'machine learning' should exist\\\")\\n        self.assertIsNotNone(learning_algorithms, \\\"Bigram 'learning algorithms' should exist\\\")\\n\\n        # They should be connected via chain relationship\\n        self.assertIn(learning_algorithms.id, machine_learning.lateral_connections)\\n        self.assertIn(machine_learning.id, learning_algorithms.lateral_connections)\\n\\n    def test_cooccurrence_connections(self):\\n        \\\"\\\"\\\"Test that bigrams co-occurring in documents are connected.\\\"\\\"\\\"\\n        layer1 = self.processor.get_layer(CorticalLayer.BIGRAMS)\\n\\n        # Bigrams that appear in same documents should have co-occurrence connections\\n        for bigram in layer1.minicolumns.values():\\n            if bigram.document_ids and len(bigram.lateral_connections) > 0:\\n                # If a bigram has connections, some should be from co-occurrence\\n                # This is a general check that connections exist\\n                break\\n\\n    def test_bidirectional_connections(self):\\n        \\\"\\\"\\\"Test that all bigram connections are bidirectional.\\\"\\\"\\\"\\n        layer1 = self.processor.get_layer(CorticalLayer.BIGRAMS)\\n\\n        for bigram in layer1.minicolumns.values():\\n            for target_id in bigram.lateral_connections:\\n                target = layer1.get_by_id(target_id)\\n                if target:\\n                    self.assertIn(\\n                        bigram.id, target.lateral_connections,\\n                        f\\\"Connection from {bigram.content} to {target.content} is not bidirectional\\\"\\n                    )\\n\\n    def test_empty_bigram_layer(self):\\n        \\\"\\\"\\\"Test bigram connections with empty bigram layer.\\\"\\\"\\\"\\n        processor = CorticalTextProcessor()\\n        processor.process_document(\\\"doc1\\\", \\\"Hello\\\")  # Single word, no bigrams\\n        processor.compute_tfidf(verbose=False)\\n\\n        stats = processor.compute_bigram_connections(verbose=False)\\n        self.assertEqual(stats['connections_created'], 0)\\n        self.assertEqual(stats['bigrams'], 0)\\n\\n    def test_compute_all_includes_bigram_connections(self):\\n        \\\"\\\"\\\"Test that compute_all includes bigram connections.\\\"\\\"\\\"\\n        processor = CorticalTextProcessor()\\n        processor.process_document(\\\"doc1\\\", \\\"Neural networks process data. Neural processing works.\\\")\\n        processor.compute_all(verbose=False)\\n\\n        # Check that bigram connections were marked fresh\\n        self.assertFalse(processor.is_stale(processor.COMP_BIGRAM_CONNECTIONS))\\n\\n    def test_custom_weights(self):\\n        \\\"\\\"\\\"Test that custom weights affect connection strengths.\\\"\\\"\\\"\\n        processor = CorticalTextProcessor()\\n        processor.process_document(\\\"doc1\\\", \\\"Neural networks neural processing neural analysis\\\")\\n        processor.compute_tfidf(verbose=False)\\n\\n        # Use different weights\\n        stats = processor.compute_bigram_connections(\\n            component_weight=1.0,\\n            chain_weight=1.5,\\n            cooccurrence_weight=0.5,\\n            verbose=False\\n        )\\n\\n        # Just verify it runs without error\\n        self.assertIsNotNone(stats)\\n\\n    def test_recompute_handles_bigram_connections(self):\\n        \\\"\\\"\\\"Test that recompute method handles bigram connections.\\\"\\\"\\\"\\n        processor = CorticalTextProcessor()\\n        processor.process_document(\\\"doc1\\\", \\\"Neural networks process data\\\")\\n\\n        # Mark as stale\\n        processor._mark_all_stale()\\n        self.assertTrue(processor.is_stale(processor.COMP_BIGRAM_CONNECTIONS))\\n\\n        # Recompute\\n        recomputed = processor.recompute(level='full', verbose=False)\\n        self.assertTrue(recomputed.get(processor.COMP_BIGRAM_CONNECTIONS, False))\\n        self.assertFalse(processor.is_stale(processor.COMP_BIGRAM_CONNECTIONS))\\n\\n    def test_bigram_connection_weights_accumulate(self):\\n        \\\"\\\"\\\"Test that connection weights accumulate for multiple reasons.\\\"\\\"\\\"\\n        layer1 = self.processor.get_layer(CorticalLayer.BIGRAMS)\\n\\n        # Find bigrams that could be connected by multiple reasons\\n        # (shared component AND co-occurrence)\\n        for bigram in layer1.minicolumns.values():\\n            for target_id, weight in bigram.lateral_connections.items():\\n                # Weights should be positive\\n                self.assertGreater(weight, 0)\\n\\n    def test_component_and_chain_connections_nonzero(self):\\n        \\\"\\\"\\\"Test that component and chain connections are created (verifies bigram separator fix).\\\"\\\"\\\"\\n        # Create fresh processor with bigrams that share components\\n        processor = CorticalTextProcessor()\\n        processor.process_document(\\n            \\\"doc1\\\",\\n            \\\"Neural networks are powerful. Neural processing enables deep learning. \\\"\\n            \\\"Machine learning is related to deep learning approaches.\\\"\\n        )\\n        processor.compute_tfidf(verbose=False)\\n\\n        stats = processor.compute_bigram_connections(verbose=False)\\n\\n        # With the bigram separator fix, component_connections should be > 0\\n        # because \\\"neural networks\\\" and \\\"neural processing\\\" share \\\"neural\\\"\\n        self.assertGreater(\\n            stats['component_connections'], 0,\\n            \\\"Component connections should be > 0 when bigrams share components. \\\"\\n            \\\"If this fails, check that bigram.content.split(' ') is used (not split('_')).\\\"\\n        )\\n\\n    def test_bigram_separator_is_space(self):\\n        \\\"\\\"\\\"Test that bigrams use space separator (regression test for separator bug).\\\"\\\"\\\"\\n        processor = CorticalTextProcessor()\\n        processor.process_document(\\\"doc1\\\", \\\"Neural networks process data\\\")\\n        processor.compute_all(verbose=False)\\n\\n        layer1 = processor.layers[CorticalLayer.BIGRAMS]\\n\\n        # Bigrams should use space separators\\n        self.assertIsNotNone(\\n            layer1.get_minicolumn(\\\"neural networks\\\"),\\n            \\\"Bigram 'neural networks' (with space) should exist\\\"\\n        )\\n        self.assertIsNone(\\n            layer1.get_minicolumn(\\\"neural_networks\\\"),\\n            \\\"Bigram 'neural_networks' (with underscore) should NOT exist\\\"\\n        )\\n\\n\\nclass TestSemanticPageRank(unittest.TestCase):\\n    \\\"\\\"\\\"Test semantic PageRank functionality.\\\"\\\"\\\"\\n\\n    @classmethod\\n    def setUpClass(cls):\\n        \\\"\\\"\\\"Set up processor with documents for semantic PageRank testing.\\\"\\\"\\\"\\n        cls.processor = CorticalTextProcessor()\\n        cls.processor.process_document(\\n            \\\"doc1\\\",\\n            \\\"Neural networks are a type of machine learning model. \\\"\\n            \\\"Deep learning uses neural networks for complex tasks.\\\"\\n        )\\n        cls.processor.process_document(\\n            \\\"doc2\\\",\\n            \\\"Machine learning algorithms process data patterns. \\\"\\n            \\\"Neural networks learn from examples.\\\"\\n        )\\n        cls.processor.process_document(\\n            \\\"doc3\\\",\\n            \\\"Deep learning is part of artificial intelligence. \\\"\\n            \\\"Machine learning models improve with data.\\\"\\n        )\\n        # Extract semantic relations first\\n        cls.processor.extract_corpus_semantics(verbose=False)\\n\\n    def test_compute_semantic_importance_returns_stats(self):\\n        \\\"\\\"\\\"Test that compute_semantic_importance returns expected statistics.\\\"\\\"\\\"\\n        processor = CorticalTextProcessor()\\n        processor.process_document(\\\"doc1\\\", \\\"Neural networks process data efficiently.\\\")\\n        processor.extract_corpus_semantics(verbose=False)\\n\\n        stats = processor.compute_semantic_importance(verbose=False)\\n\\n        self.assertIn('total_edges_with_relations', stats)\\n        self.assertIn('token_layer', stats)\\n        self.assertIn('bigram_layer', stats)\\n\\n    def test_semantic_pagerank_with_relations(self):\\n        \\\"\\\"\\\"Test that semantic PageRank uses relation weights.\\\"\\\"\\\"\\n        processor = CorticalTextProcessor()\\n        processor.process_document(\\n            \\\"doc1\\\",\\n            \\\"Neural networks learn patterns. Neural systems process data.\\\"\\n        )\\n        processor.extract_corpus_semantics(verbose=False)\\n\\n        # Get initial PageRank with standard method\\n        processor.compute_importance(verbose=False)\\n        layer0 = processor.get_layer(CorticalLayer.TOKENS)\\n        standard_pr = {col.content: col.pagerank for col in layer0.minicolumns.values()}\\n\\n        # Now compute with semantic method\\n        stats = processor.compute_semantic_importance(verbose=False)\\n\\n        # PageRank values should be updated\\n        semantic_pr = {col.content: col.pagerank for col in layer0.minicolumns.values()}\\n\\n        # Just verify it ran and produced valid PageRank values\\n        for content, pr in semantic_pr.items():\\n            self.assertGreater(pr, 0)\\n\\n    def test_semantic_pagerank_no_relations(self):\\n        \\\"\\\"\\\"Test semantic PageRank falls back when no relations exist.\\\"\\\"\\\"\\n        processor = CorticalTextProcessor()\\n        processor.process_document(\\\"doc1\\\", \\\"Hello world.\\\")\\n        # Don't extract semantic relations\\n\\n        stats = processor.compute_semantic_importance(verbose=False)\\n\\n        self.assertEqual(stats['total_edges_with_relations'], 0)\\n\\n    def test_compute_all_with_semantic_pagerank(self):\\n        \\\"\\\"\\\"Test compute_all with pagerank_method='semantic'.\\\"\\\"\\\"\\n        processor = CorticalTextProcessor()\\n        processor.process_document(\\n            \\\"doc1\\\",\\n            \\\"Neural networks process information efficiently.\\\"\\n        )\\n\\n        # Should work without errors\\n        processor.compute_all(verbose=False, pagerank_method='semantic')\\n\\n        # Verify computations ran\\n        self.assertFalse(processor.is_stale(processor.COMP_PAGERANK))\\n        self.assertFalse(processor.is_stale(processor.COMP_TFIDF))\\n\\n    def test_compute_all_standard_pagerank(self):\\n        \\\"\\\"\\\"Test compute_all with default pagerank_method='standard'.\\\"\\\"\\\"\\n        processor = CorticalTextProcessor()\\n        processor.process_document(\\n            \\\"doc1\\\",\\n            \\\"Neural networks process information efficiently.\\\"\\n        )\\n\\n        processor.compute_all(verbose=False, pagerank_method='standard')\\n\\n        self.assertFalse(processor.is_stale(processor.COMP_PAGERANK))\\n\\n    def test_custom_relation_weights(self):\\n        \\\"\\\"\\\"Test semantic PageRank with custom relation weights.\\\"\\\"\\\"\\n        processor = CorticalTextProcessor()\\n        processor.process_document(\\n            \\\"doc1\\\",\\n            \\\"Neural networks learn patterns. Machine learning improves.\\\"\\n        )\\n        processor.extract_corpus_semantics(verbose=False)\\n\\n        # Use custom weights\\n        custom_weights = {\\n            'CoOccurs': 2.0,  # Boost co-occurrence\\n            'RelatedTo': 0.1,  # Reduce related\\n        }\\n\\n        stats = processor.compute_semantic_importance(\\n            relation_weights=custom_weights,\\n            verbose=False\\n        )\\n\\n        # Should run without errors\\n        self.assertIsNotNone(stats)\\n\\n    def test_semantic_pagerank_empty_layer(self):\\n        \\\"\\\"\\\"Test semantic PageRank handles empty layer gracefully.\\\"\\\"\\\"\\n        from cortical.analysis import compute_semantic_pagerank\\n        from cortical.layers import HierarchicalLayer, CorticalLayer\\n\\n        empty_layer = HierarchicalLayer(CorticalLayer.TOKENS)\\n        relations = [(\\\"test\\\", \\\"RelatedTo\\\", \\\"example\\\", 0.5)]\\n\\n        result = compute_semantic_pagerank(empty_layer, relations)\\n\\n        self.assertEqual(result['pagerank'], {})\\n        self.assertEqual(result['iterations_run'], 0)\\n        self.assertEqual(result['edges_with_relations'], 0)\\n\\n    def test_semantic_pagerank_convergence(self):\\n        \\\"\\\"\\\"Test that semantic PageRank converges.\\\"\\\"\\\"\\n        from cortical.analysis import compute_semantic_pagerank\\n\\n        layer0 = self.processor.get_layer(CorticalLayer.TOKENS)\\n\\n        result = compute_semantic_pagerank(\\n            layer0,\\n            self.processor.semantic_relations,\\n            iterations=100,\\n            tolerance=1e-6\\n        )\\n\\n        # Should converge in less than max iterations\\n        self.assertLessEqual(result['iterations_run'], 100)\\n\\n    def test_relation_weights_applied(self):\\n        \\\"\\\"\\\"Test that different relation types get different weights.\\\"\\\"\\\"\\n        from cortical.analysis import RELATION_WEIGHTS\\n\\n        # Verify key relations have expected relative weights\\n        self.assertGreater(RELATION_WEIGHTS['IsA'], RELATION_WEIGHTS['RelatedTo'])\\n        self.assertGreater(RELATION_WEIGHTS['PartOf'], RELATION_WEIGHTS['CoOccurs'])\\n        self.assertLess(RELATION_WEIGHTS['Antonym'], RELATION_WEIGHTS['RelatedTo'])\\n\\n\\nclass TestHierarchicalPageRank(unittest.TestCase):\\n    \\\"\\\"\\\"Test hierarchical (cross-layer) PageRank functionality.\\\"\\\"\\\"\\n\\n    @classmethod\\n    def setUpClass(cls):\\n        \\\"\\\"\\\"Set up processor with documents for hierarchical PageRank testing.\\\"\\\"\\\"\\n        cls.processor = CorticalTextProcessor()\\n        cls.processor.process_document(\\n            \\\"doc1\\\",\\n            \\\"Neural networks are powerful machine learning models. \\\"\\n            \\\"Deep learning uses neural networks for complex tasks.\\\"\\n        )\\n        cls.processor.process_document(\\n            \\\"doc2\\\",\\n            \\\"Machine learning algorithms process data patterns. \\\"\\n            \\\"Neural networks learn from examples effectively.\\\"\\n        )\\n        cls.processor.process_document(\\n            \\\"doc3\\\",\\n            \\\"Deep learning is part of artificial intelligence. \\\"\\n            \\\"Machine learning models improve with more data.\\\"\\n        )\\n        cls.processor.compute_all(verbose=False, build_concepts=True)\\n\\n    def test_compute_hierarchical_importance_returns_stats(self):\\n        \\\"\\\"\\\"Test that compute_hierarchical_importance returns expected statistics.\\\"\\\"\\\"\\n        processor = CorticalTextProcessor()\\n        processor.process_document(\\\"doc1\\\", \\\"Neural networks process data efficiently.\\\")\\n        processor.compute_all(verbose=False)\\n\\n        stats = processor.compute_hierarchical_importance(verbose=False)\\n\\n        self.assertIn('iterations_run', stats)\\n        self.assertIn('converged', stats)\\n        self.assertIn('layer_stats', stats)\\n\\n    def test_hierarchical_pagerank_layer_stats(self):\\n        \\\"\\\"\\\"Test that layer stats contain expected fields.\\\"\\\"\\\"\\n        stats = self.processor.compute_hierarchical_importance(verbose=False)\\n\\n        for layer_name, layer_info in stats['layer_stats'].items():\\n            self.assertIn('nodes', layer_info)\\n            self.assertIn('max_pagerank', layer_info)\\n            self.assertIn('min_pagerank', layer_info)\\n            self.assertIn('avg_pagerank', layer_info)\\n\\n    def test_hierarchical_pagerank_convergence(self):\\n        \\\"\\\"\\\"Test that hierarchical PageRank converges.\\\"\\\"\\\"\\n        stats = self.processor.compute_hierarchical_importance(\\n            global_iterations=10,\\n            verbose=False\\n        )\\n\\n        # Should run at least one iteration\\n        self.assertGreaterEqual(stats['iterations_run'], 1)\\n\\n    def test_hierarchical_pagerank_affects_scores(self):\\n        \\\"\\\"\\\"Test that hierarchical PageRank updates scores across layers.\\\"\\\"\\\"\\n        processor = CorticalTextProcessor()\\n        processor.process_document(\\n            \\\"doc1\\\",\\n            \\\"Neural networks process information. Machine learning improves.\\\"\\n        )\\n        processor.compute_all(verbose=False, build_concepts=True)\\n\\n        # Get scores before hierarchical\\n        layer0 = processor.get_layer(CorticalLayer.TOKENS)\\n        before_scores = {col.content: col.pagerank for col in layer0.minicolumns.values()}\\n\\n        # Run hierarchical PageRank\\n        processor.compute_hierarchical_importance(verbose=False)\\n\\n        # Scores should be updated (normalized to sum to 1)\\n        after_scores = {col.content: col.pagerank for col in layer0.minicolumns.values()}\\n\\n        # Verify scores are valid probabilities\\n        total = sum(after_scores.values())\\n        self.assertAlmostEqual(total, 1.0, places=5)\\n\\n    def test_compute_all_with_hierarchical_pagerank(self):\\n        \\\"\\\"\\\"Test compute_all with pagerank_method='hierarchical'.\\\"\\\"\\\"\\n        processor = CorticalTextProcessor()\\n        processor.process_document(\\n            \\\"doc1\\\",\\n            \\\"Neural networks process information efficiently.\\\"\\n        )\\n\\n        # Should work without errors\\n        processor.compute_all(verbose=False, pagerank_method='hierarchical')\\n\\n        # Verify computations ran\\n        self.assertFalse(processor.is_stale(processor.COMP_PAGERANK))\\n        self.assertFalse(processor.is_stale(processor.COMP_TFIDF))\\n\\n    def test_hierarchical_empty_layers(self):\\n        \\\"\\\"\\\"Test hierarchical PageRank handles empty layers gracefully.\\\"\\\"\\\"\\n        from cortical.analysis import compute_hierarchical_pagerank\\n        from cortical.layers import HierarchicalLayer, CorticalLayer\\n\\n        # Create empty layers dict\\n        layers = {\\n            CorticalLayer.TOKENS: HierarchicalLayer(CorticalLayer.TOKENS),\\n            CorticalLayer.BIGRAMS: HierarchicalLayer(CorticalLayer.BIGRAMS),\\n        }\\n\\n        result = compute_hierarchical_pagerank(layers)\\n\\n        self.assertEqual(result['iterations_run'], 0)\\n        self.assertTrue(result['converged'])\\n        self.assertEqual(result['layer_stats'], {})\\n\\n    def test_cross_layer_damping(self):\\n        \\\"\\\"\\\"Test that cross-layer damping parameter affects propagation.\\\"\\\"\\\"\\n        processor = CorticalTextProcessor()\\n        processor.process_document(\\n            \\\"doc1\\\",\\n            \\\"Neural networks learn from data patterns.\\\"\\n        )\\n        processor.compute_all(verbose=False, build_concepts=True)\\n\\n        # Run with different damping values\\n        stats_low = processor.compute_hierarchical_importance(\\n            cross_layer_damping=0.3,\\n            verbose=False\\n        )\\n        stats_high = processor.compute_hierarchical_importance(\\n            cross_layer_damping=0.9,\\n            verbose=False\\n        )\\n\\n        # Both should produce valid results\\n        self.assertIsNotNone(stats_low)\\n        self.assertIsNotNone(stats_high)\\n\\n    def test_hierarchical_with_concepts(self):\\n        \\\"\\\"\\\"Test hierarchical PageRank includes concept layer.\\\"\\\"\\\"\\n        stats = self.processor.compute_hierarchical_importance(verbose=False)\\n\\n        # Should include CONCEPTS layer if it has nodes\\n        layer2 = self.processor.get_layer(CorticalLayer.CONCEPTS)\\n        if layer2.column_count() > 0:\\n            self.assertIn('CONCEPTS', stats['layer_stats'])\\n\\n    def test_feedforward_feedback_connections_used(self):\\n        \\\"\\\"\\\"Test that cross-layer connections are used in propagation.\\\"\\\"\\\"\\n        # Verify that tokens have feedback connections (to bigrams)\\n        layer0 = self.processor.get_layer(CorticalLayer.TOKENS)\\n\\n        has_feedback = any(\\n            col.feedback_connections\\n            for col in layer0.minicolumns.values()\\n        )\\n        self.assertTrue(has_feedback, \\\"Tokens should have feedback connections to bigrams\\\")\\n\\n\\nclass TestMultiHopSemanticInference(unittest.TestCase):\\n    \\\"\\\"\\\"Test multi-hop semantic inference query expansion.\\\"\\\"\\\"\\n\\n    @classmethod\\n    def setUpClass(cls):\\n        \\\"\\\"\\\"Set up processor with documents for multi-hop testing.\\\"\\\"\\\"\\n        cls.processor = CorticalTextProcessor()\\n        # Create a corpus with semantic chain potential\\n        cls.processor.process_document(\\n            \\\"doc1\\\",\\n            \\\"Neural networks are a type of machine learning model. \\\"\\n            \\\"Deep learning uses neural networks for complex pattern recognition.\\\"\\n        )\\n        cls.processor.process_document(\\n            \\\"doc2\\\",\\n            \\\"Machine learning algorithms process data efficiently. \\\"\\n            \\\"Pattern recognition is important for image classification.\\\"\\n        )\\n        cls.processor.process_document(\\n            \\\"doc3\\\",\\n            \\\"Deep learning is part of artificial intelligence research. \\\"\\n            \\\"Image classification improves with more training data.\\\"\\n        )\\n        cls.processor.process_document(\\n            \\\"doc4\\\",\\n            \\\"Artificial intelligence systems can learn from examples. \\\"\\n            \\\"Training data is essential for model accuracy.\\\"\\n        )\\n        cls.processor.compute_all(verbose=False)\\n        cls.processor.extract_corpus_semantics(verbose=False)\\n\\n    def test_expand_query_multihop_returns_dict(self):\\n        \\\"\\\"\\\"Test that expand_query_multihop returns a dictionary.\\\"\\\"\\\"\\n        expanded = self.processor.expand_query_multihop(\\\"neural\\\", max_hops=2)\\n        self.assertIsInstance(expanded, dict)\\n\\n    def test_original_terms_weight_one(self):\\n        \\\"\\\"\\\"Test that original query terms have weight 1.0.\\\"\\\"\\\"\\n        expanded = self.processor.expand_query_multihop(\\\"neural networks\\\", max_hops=2)\\n        self.assertEqual(expanded.get(\\\"neural\\\"), 1.0)\\n        self.assertEqual(expanded.get(\\\"networks\\\"), 1.0)\\n\\n    def test_hop_1_expansions(self):\\n        \\\"\\\"\\\"Test that single-hop expansions are included.\\\"\\\"\\\"\\n        expanded = self.processor.expand_query_multihop(\\\"neural\\\", max_hops=1)\\n\\n        # Should have original term\\n        self.assertIn(\\\"neural\\\", expanded)\\n\\n        # Should have some expansions (semantically related terms)\\n        expansion_count = len([k for k in expanded if k != \\\"neural\\\"])\\n        self.assertGreater(expansion_count, 0, \\\"Should have at least one expansion\\\")\\n\\n    def test_hop_2_expansions(self):\\n        \\\"\\\"\\\"Test that two-hop expansions discover more terms.\\\"\\\"\\\"\\n        expanded_1hop = self.processor.expand_query_multihop(\\\"neural\\\", max_hops=1)\\n        expanded_2hop = self.processor.expand_query_multihop(\\\"neural\\\", max_hops=2)\\n\\n        # 2-hop should have >= terms than 1-hop\\n        self.assertGreaterEqual(len(expanded_2hop), len(expanded_1hop))\\n\\n    def test_weight_decay_with_hops(self):\\n        \\\"\\\"\\\"Test that expansion weights decay with hop distance.\\\"\\\"\\\"\\n        expanded = self.processor.expand_query_multihop(\\n            \\\"neural\\\", max_hops=2, decay_factor=0.5\\n        )\\n\\n        # Original term should have weight 1.0\\n        self.assertEqual(expanded.get(\\\"neural\\\"), 1.0)\\n\\n        # All expansions should have weight < 1.0\\n        for term, weight in expanded.items():\\n            if term != \\\"neural\\\":\\n                self.assertLess(\\n                    weight, 1.0,\\n                    f\\\"Expansion '{term}' should have weight < 1.0, got {weight}\\\"\\n                )\\n\\n    def test_custom_decay_factor(self):\\n        \\\"\\\"\\\"Test that custom decay factor affects weights.\\\"\\\"\\\"\\n        expanded_slow = self.processor.expand_query_multihop(\\n            \\\"neural\\\", max_hops=2, decay_factor=0.8  # Slower decay\\n        )\\n        expanded_fast = self.processor.expand_query_multihop(\\n            \\\"neural\\\", max_hops=2, decay_factor=0.3  # Faster decay\\n        )\\n\\n        # Slower decay should give higher average weights to expansions\\n        slow_avg = sum(w for t, w in expanded_slow.items() if t != \\\"neural\\\")\\n        fast_avg = sum(w for t, w in expanded_fast.items() if t != \\\"neural\\\")\\n\\n        # If both have expansions, slow decay should have higher total\\n        if slow_avg > 0 and fast_avg > 0:\\n            self.assertGreater(slow_avg, fast_avg)\\n\\n    def test_max_expansions_limit(self):\\n        \\\"\\\"\\\"Test that max_expansions limits the number of expansion terms.\\\"\\\"\\\"\\n        expanded_3 = self.processor.expand_query_multihop(\\n            \\\"neural\\\", max_hops=2, max_expansions=3\\n        )\\n        expanded_10 = self.processor.expand_query_multihop(\\n            \\\"neural\\\", max_hops=2, max_expansions=10\\n        )\\n\\n        # Count expansions (non-original terms)\\n        expansions_3 = len([k for k in expanded_3 if k != \\\"neural\\\"])\\n        expansions_10 = len([k for k in expanded_10 if k != \\\"neural\\\"])\\n\\n        self.assertLessEqual(expansions_3, 3)\\n        self.assertLessEqual(expansions_10, 10)\\n\\n    def test_no_semantic_relations_fallback(self):\\n        \\\"\\\"\\\"Test fallback to regular expansion when no semantic relations.\\\"\\\"\\\"\\n        processor = CorticalTextProcessor()\\n        processor.process_document(\\\"doc1\\\", \\\"Neural networks process data.\\\")\\n        processor.compute_all(verbose=False)\\n        # Don't extract semantic relations\\n\\n        expanded = processor.expand_query_multihop(\\\"neural\\\", max_hops=2)\\n\\n        # Should fall back to regular expansion\\n        self.assertIn(\\\"neural\\\", expanded)\\n\\n    def test_unknown_query_term(self):\\n        \\\"\\\"\\\"Test handling of query terms not in corpus.\\\"\\\"\\\"\\n        expanded = self.processor.expand_query_multihop(\\\"xyznonexistent\\\", max_hops=2)\\n\\n        # Should return empty dict for unknown terms\\n        self.assertEqual(len(expanded), 0)\\n\\n    def test_min_path_score_filtering(self):\\n        \\\"\\\"\\\"Test that min_path_score filters low-validity paths.\\\"\\\"\\\"\\n        expanded_low = self.processor.expand_query_multihop(\\n            \\\"neural\\\", max_hops=2, min_path_score=0.1  # Low threshold\\n        )\\n        expanded_high = self.processor.expand_query_multihop(\\n            \\\"neural\\\", max_hops=2, min_path_score=0.8  # High threshold\\n        )\\n\\n        # Low threshold should allow more expansions\\n        self.assertGreaterEqual(len(expanded_low), len(expanded_high))\\n\\n    def test_multihop_integration_with_documents(self):\\n        \\\"\\\"\\\"Test that multi-hop expansion finds relevant documents.\\\"\\\"\\\"\\n        # Use multi-hop expansion to find documents\\n        expanded = self.processor.expand_query_multihop(\\\"neural\\\", max_hops=2)\\n\\n        # Use expanded terms to score documents\\n        layer0 = self.processor.get_layer(CorticalLayer.TOKENS)\\n        doc_scores = {}\\n\\n        for term, weight in expanded.items():\\n            col = layer0.get_minicolumn(term)\\n            if col:\\n                for doc_id in col.document_ids:\\n                    doc_scores[doc_id] = doc_scores.get(doc_id, 0) + weight * col.tfidf\\n\\n        # Should find at least doc1 which contains \\\"neural\\\"\\n        self.assertIn(\\\"doc1\\\", doc_scores)\\n\\n\\nclass TestMultiHopPathScoring(unittest.TestCase):\\n    \\\"\\\"\\\"Test relation path scoring for multi-hop inference.\\\"\\\"\\\"\\n\\n    def test_score_relation_path_empty(self):\\n        \\\"\\\"\\\"Test scoring empty path.\\\"\\\"\\\"\\n        from cortical.query import score_relation_path\\n        self.assertEqual(score_relation_path([]), 1.0)\\n\\n    def test_score_relation_path_single(self):\\n        \\\"\\\"\\\"Test scoring single-hop path.\\\"\\\"\\\"\\n        from cortical.query import score_relation_path\\n        self.assertEqual(score_relation_path(['IsA']), 1.0)\\n        self.assertEqual(score_relation_path(['RelatedTo']), 1.0)\\n\\n    def test_score_isa_chain(self):\\n        \\\"\\\"\\\"Test that IsA chains get high scores.\\\"\\\"\\\"\\n        from cortical.query import score_relation_path\\n        # IsA → IsA is a valid transitive chain\\n        score = score_relation_path(['IsA', 'IsA'])\\n        self.assertEqual(score, 1.0)\\n\\n    def test_score_mixed_chain(self):\\n        \\\"\\\"\\\"Test scoring mixed relation chains.\\\"\\\"\\\"\\n        from cortical.query import score_relation_path\\n        # IsA → HasProperty is a valid inference\\n        score = score_relation_path(['IsA', 'HasProperty'])\\n        self.assertGreater(score, 0.8)\\n\\n    def test_score_weak_chain(self):\\n        \\\"\\\"\\\"Test that weak chains get low scores.\\\"\\\"\\\"\\n        from cortical.query import score_relation_path\\n        # Antonym → IsA is contradictory\\n        score = score_relation_path(['Antonym', 'IsA'])\\n        self.assertLess(score, 0.3)\\n\\n    def test_score_default_relation(self):\\n        \\\"\\\"\\\"Test scoring unknown relation pairs.\\\"\\\"\\\"\\n        from cortical.query import score_relation_path\\n        # Unknown pair should get moderate default score\\n        score = score_relation_path(['UnknownRel', 'AnotherUnknown'])\\n        self.assertEqual(score, 0.4)  # Default moderate validity\\n\\n    def test_valid_relation_chains_constant(self):\\n        \\\"\\\"\\\"Test that VALID_RELATION_CHAINS is defined.\\\"\\\"\\\"\\n        from cortical.query import VALID_RELATION_CHAINS\\n        self.assertIsInstance(VALID_RELATION_CHAINS, dict)\\n        self.assertIn(('IsA', 'IsA'), VALID_RELATION_CHAINS)\\n        self.assertIn(('PartOf', 'PartOf'), VALID_RELATION_CHAINS)\\n\\n\\nclass TestAnalogyCompletion(unittest.TestCase):\\n    \\\"\\\"\\\"Test analogy completion functionality.\\\"\\\"\\\"\\n\\n    @classmethod\\n    def setUpClass(cls):\\n        \\\"\\\"\\\"Set up processor with documents for analogy testing.\\\"\\\"\\\"\\n        cls.processor = CorticalTextProcessor()\\n        # Create a corpus with semantic structure for analogies\\n        cls.processor.process_document(\\\"doc1\\\", \\\"\\\"\\\"\\n            Neural networks are powerful machine learning models.\\n            Deep learning uses neural networks for complex tasks.\\n            Knowledge graphs store semantic relationships.\\n        \\\"\\\"\\\")\\n        cls.processor.process_document(\\\"doc2\\\", \\\"\\\"\\\"\\n            Machine learning algorithms process data efficiently.\\n            Pattern recognition helps with image classification.\\n            Data processing transforms raw information.\\n        \\\"\\\"\\\")\\n        cls.processor.process_document(\\\"doc3\\\", \\\"\\\"\\\"\\n            Artificial intelligence enables intelligent systems.\\n            Natural language processing understands text.\\n            Computer vision analyzes images and video.\\n        \\\"\\\"\\\")\\n        cls.processor.compute_all(verbose=False)\\n        cls.processor.extract_corpus_semantics(verbose=False)\\n        cls.processor.compute_graph_embeddings(dimensions=16, verbose=False)\\n\\n    def test_complete_analogy_returns_list(self):\\n        \\\"\\\"\\\"Test that complete_analogy returns a list.\\\"\\\"\\\"\\n        results = self.processor.complete_analogy(\\n            \\\"neural\\\", \\\"networks\\\", \\\"machine\\\"\\n        )\\n        self.assertIsInstance(results, list)\\n\\n    def test_complete_analogy_result_format(self):\\n        \\\"\\\"\\\"Test that results have correct format (term, score, method).\\\"\\\"\\\"\\n        results = self.processor.complete_analogy(\\n            \\\"neural\\\", \\\"networks\\\", \\\"machine\\\", top_n=3\\n        )\\n\\n        for result in results:\\n            self.assertEqual(len(result), 3)\\n            term, score, method = result\\n            self.assertIsInstance(term, str)\\n            self.assertIsInstance(score, float)\\n            self.assertIsInstance(method, str)\\n            self.assertGreater(score, 0)\\n\\n    def test_complete_analogy_excludes_input_terms(self):\\n        \\\"\\\"\\\"Test that input terms are excluded from results.\\\"\\\"\\\"\\n        results = self.processor.complete_analogy(\\n            \\\"neural\\\", \\\"networks\\\", \\\"machine\\\"\\n        )\\n\\n        result_terms = [term for term, _, _ in results]\\n        self.assertNotIn(\\\"neural\\\", result_terms)\\n        self.assertNotIn(\\\"networks\\\", result_terms)\\n        self.assertNotIn(\\\"machine\\\", result_terms)\\n\\n    def test_complete_analogy_top_n_limit(self):\\n        \\\"\\\"\\\"Test that top_n limits the number of results.\\\"\\\"\\\"\\n        results_3 = self.processor.complete_analogy(\\n            \\\"neural\\\", \\\"networks\\\", \\\"machine\\\", top_n=3\\n        )\\n        results_5 = self.processor.complete_analogy(\\n            \\\"neural\\\", \\\"networks\\\", \\\"machine\\\", top_n=5\\n        )\\n\\n        self.assertLessEqual(len(results_3), 3)\\n        self.assertLessEqual(len(results_5), 5)\\n\\n    def test_complete_analogy_unknown_term(self):\\n        \\\"\\\"\\\"Test handling of unknown terms.\\\"\\\"\\\"\\n        results = self.processor.complete_analogy(\\n            \\\"xyznonexistent\\\", \\\"abcnonexistent\\\", \\\"machine\\\"\\n        )\\n        self.assertEqual(results, [])\\n\\n    def test_complete_analogy_with_embeddings_only(self):\\n        \\\"\\\"\\\"Test analogy completion using only embeddings.\\\"\\\"\\\"\\n        results = self.processor.complete_analogy(\\n            \\\"neural\\\", \\\"networks\\\", \\\"machine\\\",\\n            use_embeddings=True,\\n            use_relations=False\\n        )\\n        self.assertIsInstance(results, list)\\n\\n    def test_complete_analogy_with_relations_only(self):\\n        \\\"\\\"\\\"Test analogy completion using only relations.\\\"\\\"\\\"\\n        results = self.processor.complete_analogy(\\n            \\\"neural\\\", \\\"networks\\\", \\\"machine\\\",\\n            use_embeddings=False,\\n            use_relations=True\\n        )\\n        self.assertIsInstance(results, list)\\n\\n    def test_complete_analogy_simple_returns_list(self):\\n        \\\"\\\"\\\"Test that complete_analogy_simple returns a list.\\\"\\\"\\\"\\n        results = self.processor.complete_analogy_simple(\\n            \\\"neural\\\", \\\"networks\\\", \\\"machine\\\"\\n        )\\n        self.assertIsInstance(results, list)\\n\\n    def test_complete_analogy_simple_format(self):\\n        \\\"\\\"\\\"Test that simple results have correct format (term, score).\\\"\\\"\\\"\\n        results = self.processor.complete_analogy_simple(\\n            \\\"neural\\\", \\\"networks\\\", \\\"machine\\\", top_n=3\\n        )\\n\\n        for result in results:\\n            self.assertEqual(len(result), 2)\\n            term, score = result\\n            self.assertIsInstance(term, str)\\n            self.assertIsInstance(score, float)\\n\\n    def test_complete_analogy_simple_excludes_input(self):\\n        \\\"\\\"\\\"Test that input terms are excluded from simple results.\\\"\\\"\\\"\\n        results = self.processor.complete_analogy_simple(\\n            \\\"neural\\\", \\\"networks\\\", \\\"machine\\\"\\n        )\\n\\n        result_terms = [term for term, _ in results]\\n        self.assertNotIn(\\\"neural\\\", result_terms)\\n        self.assertNotIn(\\\"networks\\\", result_terms)\\n        self.assertNotIn(\\\"machine\\\", result_terms)\\n\\n    def test_complete_analogy_simple_uses_bigram_patterns(self):\\n        \\\"\\\"\\\"Test that analogy completion uses bigram patterns (verifies separator fix).\\\"\\\"\\\"\\n        # Create processor with documents that should create \\\"a b\\\" and \\\"c d\\\" bigrams\\n        processor = CorticalTextProcessor()\\n        processor.process_document(\\n            \\\"doc1\\\",\\n            \\\"Neural networks are powerful tools for data analysis. \\\"\\n            \\\"Machine learning helps neural networks improve performance.\\\"\\n        )\\n        processor.process_document(\\n            \\\"doc2\\\",\\n            \\\"Neural algorithms process neural signals. \\\"\\n            \\\"Machine processing uses machine algorithms.\\\"\\n        )\\n        processor.compute_all(verbose=False)\\n\\n        # Verify bigrams exist with space separators\\n        layer1 = processor.layers[CorticalLayer.BIGRAMS]\\n        self.assertIsNotNone(\\n            layer1.get_minicolumn(\\\"neural networks\\\"),\\n            \\\"Bigram 'neural networks' should exist for bigram strategy\\\"\\n        )\\n\\n        # The analogy should work using bigram patterns\\n        # a:b :: c:? where \\\"a b\\\" is a bigram and we look for \\\"c ?\\\" bigrams\\n        results = processor.complete_analogy_simple(\\\"neural\\\", \\\"networks\\\", \\\"machine\\\")\\n\\n        # Results should be a list (may be empty if no bigram pattern matches)\\n        self.assertIsInstance(results, list)\\n\\n\\nclass TestAnalogyHelperFunctions(unittest.TestCase):\\n    \\\"\\\"\\\"Test analogy helper functions.\\\"\\\"\\\"\\n\\n    def test_find_relation_between(self):\\n        \\\"\\\"\\\"Test finding relations between terms.\\\"\\\"\\\"\\n        from cortical.query import find_relation_between\\n\\n        relations = [\\n            (\\\"dog\\\", \\\"IsA\\\", \\\"animal\\\", 1.0),\\n            (\\\"cat\\\", \\\"IsA\\\", \\\"animal\\\", 1.0),\\n            (\\\"dog\\\", \\\"HasProperty\\\", \\\"loyal\\\", 0.8),\\n        ]\\n\\n        result = find_relation_between(\\\"dog\\\", \\\"animal\\\", relations)\\n        self.assertEqual(len(result), 1)\\n        self.assertEqual(result[0][0], \\\"IsA\\\")\\n\\n    def test_find_relation_between_no_match(self):\\n        \\\"\\\"\\\"Test finding relations with no match.\\\"\\\"\\\"\\n        from cortical.query import find_relation_between\\n\\n        relations = [\\n            (\\\"dog\\\", \\\"IsA\\\", \\\"animal\\\", 1.0),\\n        ]\\n\\n        result = find_relation_between(\\\"cat\\\", \\\"animal\\\", relations)\\n        self.assertEqual(len(result), 0)\\n\\n    def test_find_terms_with_relation(self):\\n        \\\"\\\"\\\"Test finding terms with specific relation.\\\"\\\"\\\"\\n        from cortical.query import find_terms_with_relation\\n\\n        relations = [\\n            (\\\"dog\\\", \\\"IsA\\\", \\\"animal\\\", 1.0),\\n            (\\\"cat\\\", \\\"IsA\\\", \\\"animal\\\", 0.9),\\n            (\\\"bird\\\", \\\"IsA\\\", \\\"animal\\\", 0.8),\\n        ]\\n\\n        result = find_terms_with_relation(\\\"animal\\\", \\\"IsA\\\", relations, direction='backward')\\n        self.assertEqual(len(result), 3)\\n        # Should be sorted by weight\\n        self.assertEqual(result[0][0], \\\"dog\\\")\\n\\n    def test_find_terms_with_relation_forward(self):\\n        \\\"\\\"\\\"Test finding terms with forward relation.\\\"\\\"\\\"\\n        from cortical.query import find_terms_with_relation\\n\\n        relations = [\\n            (\\\"dog\\\", \\\"HasProperty\\\", \\\"loyal\\\", 1.0),\\n            (\\\"dog\\\", \\\"HasProperty\\\", \\\"friendly\\\", 0.8),\\n        ]\\n\\n        result = find_terms_with_relation(\\\"dog\\\", \\\"HasProperty\\\", relations, direction='forward')\\n        self.assertEqual(len(result), 2)\\n\\n\\nclass TestInputValidation(unittest.TestCase):\\n    \\\"\\\"\\\"Test input validation for public API methods.\\\"\\\"\\\"\\n\\n    def setUp(self):\\n        self.processor = CorticalTextProcessor()\\n\\n    # Tests for process_document validation\\n    def test_process_document_empty_doc_id(self):\\n        \\\"\\\"\\\"process_document should reject empty doc_id.\\\"\\\"\\\"\\n        with self.assertRaises(ValueError) as ctx:\\n            self.processor.process_document(\\\"\\\", \\\"Some content\\\")\\n        self.assertIn(\\\"doc_id\\\", str(ctx.exception))\\n\\n    def test_process_document_none_doc_id(self):\\n        \\\"\\\"\\\"process_document should reject None doc_id.\\\"\\\"\\\"\\n        with self.assertRaises(ValueError) as ctx:\\n            self.processor.process_document(None, \\\"Some content\\\")\\n        self.assertIn(\\\"doc_id\\\", str(ctx.exception))\\n\\n    def test_process_document_non_string_doc_id(self):\\n        \\\"\\\"\\\"process_document should reject non-string doc_id.\\\"\\\"\\\"\\n        with self.assertRaises(ValueError) as ctx:\\n            self.processor.process_document(123, \\\"Some content\\\")\\n        self.assertIn(\\\"doc_id\\\", str(ctx.exception))\\n\\n    def test_process_document_empty_content(self):\\n        \\\"\\\"\\\"process_document should reject empty content.\\\"\\\"\\\"\\n        with self.assertRaises(ValueError) as ctx:\\n            self.processor.process_document(\\\"doc1\\\", \\\"\\\")\\n        self.assertIn(\\\"content\\\", str(ctx.exception))\\n\\n    def test_process_document_whitespace_content(self):\\n        \\\"\\\"\\\"process_document should reject whitespace-only content.\\\"\\\"\\\"\\n        with self.assertRaises(ValueError) as ctx:\\n            self.processor.process_document(\\\"doc1\\\", \\\"   \\\\n\\\\t  \\\")\\n        self.assertIn(\\\"content\\\", str(ctx.exception))\\n\\n    def test_process_document_non_string_content(self):\\n        \\\"\\\"\\\"process_document should reject non-string content.\\\"\\\"\\\"\\n        with self.assertRaises(ValueError) as ctx:\\n            self.processor.process_document(\\\"doc1\\\", 123)\\n        self.assertIn(\\\"content\\\", str(ctx.exception))\\n\\n    def test_process_document_valid_input(self):\\n        \\\"\\\"\\\"process_document should accept valid input.\\\"\\\"\\\"\\n        stats = self.processor.process_document(\\\"doc1\\\", \\\"Valid content here.\\\")\\n        self.assertIn(\\\"doc1\\\", self.processor.documents)\\n\\n    # Tests for find_documents_for_query validation\\n    def test_find_documents_empty_query(self):\\n        \\\"\\\"\\\"find_documents_for_query should reject empty query.\\\"\\\"\\\"\\n        self.processor.process_document(\\\"doc1\\\", \\\"Some content here.\\\")\\n        self.processor.compute_all()\\n\\n        with self.assertRaises(ValueError) as ctx:\\n            self.processor.find_documents_for_query(\\\"\\\")\\n        self.assertIn(\\\"query_text\\\", str(ctx.exception))\\n\\n    def test_find_documents_whitespace_query(self):\\n        \\\"\\\"\\\"find_documents_for_query should reject whitespace-only query.\\\"\\\"\\\"\\n        self.processor.process_document(\\\"doc1\\\", \\\"Some content here.\\\")\\n        self.processor.compute_all()\\n\\n        with self.assertRaises(ValueError) as ctx:\\n            self.processor.find_documents_for_query(\\\"   \\\")\\n        self.assertIn(\\\"query_text\\\", str(ctx.exception))\\n\\n    def test_find_documents_invalid_top_n(self):\\n        \\\"\\\"\\\"find_documents_for_query should reject invalid top_n.\\\"\\\"\\\"\\n        self.processor.process_document(\\\"doc1\\\", \\\"Some content here.\\\")\\n        self.processor.compute_all()\\n\\n        with self.assertRaises(ValueError) as ctx:\\n            self.processor.find_documents_for_query(\\\"content\\\", top_n=0)\\n        self.assertIn(\\\"top_n\\\", str(ctx.exception))\\n\\n        with self.assertRaises(ValueError) as ctx:\\n            self.processor.find_documents_for_query(\\\"content\\\", top_n=-1)\\n        self.assertIn(\\\"top_n\\\", str(ctx.exception))\\n\\n    def test_find_documents_valid_input(self):\\n        \\\"\\\"\\\"find_documents_for_query should accept valid input.\\\"\\\"\\\"\\n        self.processor.process_document(\\\"doc1\\\", \\\"Neural networks process data.\\\")\\n        self.processor.compute_all()\\n\\n        results = self.processor.find_documents_for_query(\\\"neural\\\", top_n=5)\\n        self.assertIsInstance(results, list)\\n\\n    # Tests for complete_analogy validation\\n    def test_complete_analogy_empty_term(self):\\n        \\\"\\\"\\\"complete_analogy should reject empty terms.\\\"\\\"\\\"\\n        self.processor.process_document(\\\"doc1\\\", \\\"Neural networks and data.\\\")\\n        self.processor.compute_all()\\n\\n        with self.assertRaises(ValueError) as ctx:\\n            self.processor.complete_analogy(\\\"\\\", \\\"b\\\", \\\"c\\\")\\n        self.assertIn(\\\"term_a\\\", str(ctx.exception))\\n\\n        with self.assertRaises(ValueError) as ctx:\\n            self.processor.complete_analogy(\\\"a\\\", \\\"\\\", \\\"c\\\")\\n        self.assertIn(\\\"term_b\\\", str(ctx.exception))\\n\\n        with self.assertRaises(ValueError) as ctx:\\n            self.processor.complete_analogy(\\\"a\\\", \\\"b\\\", \\\"\\\")\\n        self.assertIn(\\\"term_c\\\", str(ctx.exception))\\n\\n    def test_complete_analogy_invalid_top_n(self):\\n        \\\"\\\"\\\"complete_analogy should reject invalid top_n.\\\"\\\"\\\"\\n        self.processor.process_document(\\\"doc1\\\", \\\"Neural networks and data.\\\")\\n        self.processor.compute_all()\\n\\n        with self.assertRaises(ValueError) as ctx:\\n            self.processor.complete_analogy(\\\"a\\\", \\\"b\\\", \\\"c\\\", top_n=0)\\n        self.assertIn(\\\"top_n\\\", str(ctx.exception))\\n\\n    def test_complete_analogy_valid_input(self):\\n        \\\"\\\"\\\"complete_analogy should accept valid input.\\\"\\\"\\\"\\n        self.processor.process_document(\\\"doc1\\\", \\\"Neural networks process data.\\\")\\n        self.processor.compute_all()\\n\\n        results = self.processor.complete_analogy(\\\"neural\\\", \\\"networks\\\", \\\"data\\\", top_n=3)\\n        self.assertIsInstance(results, list)\\n\\n    # Tests for add_documents_batch validation\\n    def test_add_documents_batch_not_list(self):\\n        \\\"\\\"\\\"add_documents_batch should reject non-list input.\\\"\\\"\\\"\\n        with self.assertRaises(ValueError) as ctx:\\n            self.processor.add_documents_batch(\\\"not a list\\\")\\n        self.assertIn(\\\"must be a list\\\", str(ctx.exception))\\n\\n    def test_add_documents_batch_empty_list(self):\\n        \\\"\\\"\\\"add_documents_batch should reject empty list.\\\"\\\"\\\"\\n        with self.assertRaises(ValueError) as ctx:\\n            self.processor.add_documents_batch([])\\n        self.assertIn(\\\"must not be empty\\\", str(ctx.exception))\\n\\n    def test_add_documents_batch_invalid_recompute(self):\\n        \\\"\\\"\\\"add_documents_batch should reject invalid recompute level.\\\"\\\"\\\"\\n        with self.assertRaises(ValueError) as ctx:\\n            self.processor.add_documents_batch(\\n                [(\\\"doc1\\\", \\\"content\\\")],\\n                recompute='invalid'\\n            )\\n        self.assertIn(\\\"recompute\\\", str(ctx.exception))\\n\\n    def test_add_documents_batch_invalid_tuple(self):\\n        \\\"\\\"\\\"add_documents_batch should reject invalid tuple format.\\\"\\\"\\\"\\n        with self.assertRaises(ValueError) as ctx:\\n            self.processor.add_documents_batch([(\\\"only_one_element\\\",)])\\n        self.assertIn(\\\"documents[0]\\\", str(ctx.exception))\\n\\n    def test_add_documents_batch_invalid_doc_id(self):\\n        \\\"\\\"\\\"add_documents_batch should reject invalid doc_id in tuple.\\\"\\\"\\\"\\n        with self.assertRaises(ValueError) as ctx:\\n            self.processor.add_documents_batch([(123, \\\"content\\\")])\\n        self.assertIn(\\\"doc_id\\\", str(ctx.exception))\\n\\n    def test_add_documents_batch_valid_input(self):\\n        \\\"\\\"\\\"add_documents_batch should accept valid input.\\\"\\\"\\\"\\n        docs = [\\n            (\\\"doc1\\\", \\\"First document.\\\", None),\\n            (\\\"doc2\\\", \\\"Second document.\\\", {\\\"source\\\": \\\"test\\\"}),\\n        ]\\n        stats = self.processor.add_documents_batch(docs, recompute='none', verbose=False)\\n        self.assertEqual(stats['documents_added'], 2)\\n\\n\\nclass TestQueryCache(unittest.TestCase):\\n    \\\"\\\"\\\"Test query expansion caching functionality.\\\"\\\"\\\"\\n\\n    @classmethod\\n    def setUpClass(cls):\\n        \\\"\\\"\\\"Set up test processor.\\\"\\\"\\\"\\n        cls.processor = CorticalTextProcessor()\\n        cls.processor.process_document(\\\"doc1\\\", \\\"Neural networks process data.\\\")\\n        cls.processor.process_document(\\\"doc2\\\", \\\"Machine learning algorithms.\\\")\\n        cls.processor.compute_all(verbose=False)\\n\\n    def test_expand_query_cached_returns_dict(self):\\n        \\\"\\\"\\\"expand_query_cached should return a dict.\\\"\\\"\\\"\\n        result = self.processor.expand_query_cached(\\\"neural\\\")\\n        self.assertIsInstance(result, dict)\\n\\n    def test_expand_query_cached_same_result(self):\\n        \\\"\\\"\\\"expand_query_cached should return same result for same query.\\\"\\\"\\\"\\n        result1 = self.processor.expand_query_cached(\\\"neural networks\\\")\\n        result2 = self.processor.expand_query_cached(\\\"neural networks\\\")\\n        self.assertEqual(result1, result2)\\n\\n    def test_expand_query_cached_different_params(self):\\n        \\\"\\\"\\\"Different parameters should use different cache entries.\\\"\\\"\\\"\\n        result1 = self.processor.expand_query_cached(\\\"neural\\\", use_code_concepts=False)\\n        result2 = self.processor.expand_query_cached(\\\"neural\\\", use_code_concepts=True)\\n        # Results may differ (code concepts add synonyms)\\n        self.assertIsInstance(result1, dict)\\n        self.assertIsInstance(result2, dict)\\n\\n    def test_clear_query_cache(self):\\n        \\\"\\\"\\\"clear_query_cache should return count and clear cache.\\\"\\\"\\\"\\n        # Populate cache\\n        self.processor.expand_query_cached(\\\"test1\\\")\\n        self.processor.expand_query_cached(\\\"test2\\\")\\n\\n        # Clear and verify\\n        count = self.processor.clear_query_cache()\\n        self.assertGreaterEqual(count, 2)\\n\\n        # Verify cache is empty\\n        count2 = self.processor.clear_query_cache()\\n        self.assertEqual(count2, 0)\\n\\n    def test_set_query_cache_size(self):\\n        \\\"\\\"\\\"set_query_cache_size should update max size.\\\"\\\"\\\"\\n        self.processor.set_query_cache_size(50)\\n        self.assertEqual(self.processor._query_cache_max_size, 50)\\n\\n        # Reset to default\\n        self.processor.set_query_cache_size(100)\\n\\n    def test_set_query_cache_size_invalid(self):\\n        \\\"\\\"\\\"set_query_cache_size should reject invalid sizes.\\\"\\\"\\\"\\n        with self.assertRaises(ValueError):\\n            self.processor.set_query_cache_size(0)\\n        with self.assertRaises(ValueError):\\n            self.processor.set_query_cache_size(-10)\\n\\n    def test_cache_lru_eviction(self):\\n        \\\"\\\"\\\"Cache should evict oldest entries when full.\\\"\\\"\\\"\\n        self.processor.clear_query_cache()\\n        self.processor.set_query_cache_size(3)\\n\\n        # Fill cache\\n        self.processor.expand_query_cached(\\\"query1\\\")\\n        self.processor.expand_query_cached(\\\"query2\\\")\\n        self.processor.expand_query_cached(\\\"query3\\\")\\n\\n        # Add another - should evict oldest\\n        self.processor.expand_query_cached(\\\"query4\\\")\\n\\n        # Cache should still be at max size\\n        self.assertLessEqual(len(self.processor._query_expansion_cache), 3)\\n\\n        # Reset\\n        self.processor.set_query_cache_size(100)\\n        self.processor.clear_query_cache()\\n\\n    def test_compute_all_invalidates_cache(self):\\n        \\\"\\\"\\\"compute_all should clear the query cache.\\\"\\\"\\\"\\n        # Populate cache\\n        self.processor.expand_query_cached(\\\"cached_query\\\")\\n        self.assertGreater(len(self.processor._query_expansion_cache), 0)\\n\\n        # Recompute\\n        self.processor.compute_all(verbose=False)\\n\\n        # Cache should be cleared\\n        self.assertEqual(len(self.processor._query_expansion_cache), 0)\\n\\n\\nif __name__ == \\\"__main__\\\":\\n    unittest.main(verbosity=2)\\n\",",
        "      \"mtime\": 1765392877.0",
        "    },",
        "    {",
        "      \"op\": \"add\",",
        "      \"doc_id\": \"tests/test_embeddings.py\",",
        "      \"content\": \"\\\"\\\"\\\"Tests for the embeddings module.\\\"\\\"\\\"\\n\\nimport unittest\\nimport sys\\nsys.path.insert(0, '..')\\n\\nfrom cortical import CorticalTextProcessor, CorticalLayer\\nfrom cortical.embeddings import (\\n    compute_graph_embeddings,\\n    embedding_similarity,\\n    find_similar_by_embedding,\\n    _adjacency_embeddings,\\n    _random_walk_embeddings,\\n    _spectral_embeddings\\n)\\n\\n\\nclass TestEmbeddings(unittest.TestCase):\\n    \\\"\\\"\\\"Test the embeddings module.\\\"\\\"\\\"\\n\\n    @classmethod\\n    def setUpClass(cls):\\n        \\\"\\\"\\\"Set up processor with sample data.\\\"\\\"\\\"\\n        cls.processor = CorticalTextProcessor()\\n        cls.processor.process_document(\\\"doc1\\\", \\\"\\\"\\\"\\n            Neural networks process information through layers.\\n            Deep learning enables pattern recognition.\\n        \\\"\\\"\\\")\\n        cls.processor.process_document(\\\"doc2\\\", \\\"\\\"\\\"\\n            Machine learning algorithms learn from data.\\n            Training neural networks requires optimization.\\n        \\\"\\\"\\\")\\n        cls.processor.process_document(\\\"doc3\\\", \\\"\\\"\\\"\\n            Graph algorithms traverse nodes and edges.\\n            Network analysis reveals structure.\\n        \\\"\\\"\\\")\\n        cls.processor.compute_all(verbose=False)\\n\\n    def test_compute_graph_embeddings_adjacency(self):\\n        \\\"\\\"\\\"Test adjacency-based embeddings.\\\"\\\"\\\"\\n        embeddings, stats = compute_graph_embeddings(\\n            self.processor.layers,\\n            dimensions=16,\\n            method='adjacency'\\n        )\\n        self.assertIsInstance(embeddings, dict)\\n        self.assertGreater(len(embeddings), 0)\\n        self.assertEqual(stats['method'], 'adjacency')\\n        self.assertEqual(stats['dimensions'], 16)\\n        self.assertEqual(stats['terms_embedded'], len(embeddings))\\n\\n    def test_compute_graph_embeddings_random_walk(self):\\n        \\\"\\\"\\\"Test random walk embeddings.\\\"\\\"\\\"\\n        embeddings, stats = compute_graph_embeddings(\\n            self.processor.layers,\\n            dimensions=16,\\n            method='random_walk'\\n        )\\n        self.assertIsInstance(embeddings, dict)\\n        self.assertGreater(len(embeddings), 0)\\n        self.assertEqual(stats['method'], 'random_walk')\\n\\n    def test_compute_graph_embeddings_spectral(self):\\n        \\\"\\\"\\\"Test spectral embeddings.\\\"\\\"\\\"\\n        embeddings, stats = compute_graph_embeddings(\\n            self.processor.layers,\\n            dimensions=16,\\n            method='spectral'\\n        )\\n        self.assertIsInstance(embeddings, dict)\\n        self.assertGreater(len(embeddings), 0)\\n        self.assertEqual(stats['method'], 'spectral')\\n\\n    def test_compute_graph_embeddings_invalid_method(self):\\n        \\\"\\\"\\\"Test that invalid method raises error.\\\"\\\"\\\"\\n        with self.assertRaises(ValueError):\\n            compute_graph_embeddings(\\n                self.processor.layers,\\n                dimensions=16,\\n                method='invalid'\\n            )\\n\\n    def test_embedding_similarity(self):\\n        \\\"\\\"\\\"Test cosine similarity between embeddings.\\\"\\\"\\\"\\n        embeddings, _ = compute_graph_embeddings(\\n            self.processor.layers,\\n            dimensions=16,\\n            method='adjacency'\\n        )\\n\\n        # Find two terms that exist in embeddings\\n        terms = list(embeddings.keys())\\n        if len(terms) >= 2:\\n            sim = embedding_similarity(embeddings, terms[0], terms[1])\\n            self.assertIsInstance(sim, float)\\n            self.assertGreaterEqual(sim, -1.0)\\n            self.assertLessEqual(sim, 1.0)\\n\\n    def test_embedding_similarity_self(self):\\n        \\\"\\\"\\\"Test that a term has similarity 1.0 with itself.\\\"\\\"\\\"\\n        embeddings, _ = compute_graph_embeddings(\\n            self.processor.layers,\\n            dimensions=16,\\n            method='adjacency'\\n        )\\n\\n        terms = list(embeddings.keys())\\n        if terms:\\n            sim = embedding_similarity(embeddings, terms[0], terms[0])\\n            self.assertAlmostEqual(sim, 1.0, places=5)\\n\\n    def test_embedding_similarity_missing_term(self):\\n        \\\"\\\"\\\"Test similarity with missing term returns 0.\\\"\\\"\\\"\\n        embeddings, _ = compute_graph_embeddings(\\n            self.processor.layers,\\n            dimensions=16,\\n            method='adjacency'\\n        )\\n\\n        sim = embedding_similarity(embeddings, \\\"nonexistent_term\\\", \\\"another_missing\\\")\\n        self.assertEqual(sim, 0.0)\\n\\n    def test_find_similar_by_embedding(self):\\n        \\\"\\\"\\\"Test finding similar terms by embedding.\\\"\\\"\\\"\\n        embeddings, _ = compute_graph_embeddings(\\n            self.processor.layers,\\n            dimensions=16,\\n            method='adjacency'\\n        )\\n\\n        terms = list(embeddings.keys())\\n        if terms:\\n            similar = find_similar_by_embedding(embeddings, terms[0], top_n=5)\\n            self.assertIsInstance(similar, list)\\n            self.assertLessEqual(len(similar), 5)\\n\\n            # Check format of results\\n            for term, score in similar:\\n                self.assertIsInstance(term, str)\\n                self.assertIsInstance(score, float)\\n\\n    def test_find_similar_by_embedding_missing_term(self):\\n        \\\"\\\"\\\"Test finding similar for missing term returns empty list.\\\"\\\"\\\"\\n        embeddings, _ = compute_graph_embeddings(\\n            self.processor.layers,\\n            dimensions=16,\\n            method='adjacency'\\n        )\\n\\n        similar = find_similar_by_embedding(embeddings, \\\"nonexistent_term\\\", top_n=5)\\n        self.assertEqual(similar, [])\\n\\n    def test_embedding_dimensions(self):\\n        \\\"\\\"\\\"Test that embeddings have correct dimensions.\\\"\\\"\\\"\\n        dimensions = 16\\n        embeddings, stats = compute_graph_embeddings(\\n            self.processor.layers,\\n            dimensions=dimensions,\\n            method='adjacency'\\n        )\\n\\n        # Dimensions are min(requested, num_terms)\\n        expected_dims = min(dimensions, stats['terms_embedded'])\\n        for term, vec in embeddings.items():\\n            self.assertEqual(len(vec), expected_dims)\\n\\n    def test_embedding_normalization(self):\\n        \\\"\\\"\\\"Test that adjacency embeddings are normalized.\\\"\\\"\\\"\\n        import math\\n\\n        embeddings, _ = compute_graph_embeddings(\\n            self.processor.layers,\\n            dimensions=16,\\n            method='adjacency'\\n        )\\n\\n        for term, vec in embeddings.items():\\n            magnitude = math.sqrt(sum(v * v for v in vec))\\n            # Should be approximately 1.0 (normalized)\\n            self.assertAlmostEqual(magnitude, 1.0, places=5)\\n\\n\\nclass TestEmbeddingsEmptyLayer(unittest.TestCase):\\n    \\\"\\\"\\\"Test embeddings with empty layer.\\\"\\\"\\\"\\n\\n    def test_empty_layer_embeddings(self):\\n        \\\"\\\"\\\"Test embeddings on empty processor.\\\"\\\"\\\"\\n        processor = CorticalTextProcessor()\\n        embeddings, stats = compute_graph_embeddings(\\n            processor.layers,\\n            dimensions=16,\\n            method='adjacency'\\n        )\\n        self.assertEqual(len(embeddings), 0)\\n        self.assertEqual(stats['terms_embedded'], 0)\\n\\n\\nif __name__ == \\\"__main__\\\":\\n    unittest.main(verbosity=2)\\n\",",
        "      \"mtime\": 1765375584.0",
        "    },",
        "    {",
        "      \"op\": \"add\",",
        "      \"doc_id\": \"tests/test_gaps.py\",",
        "      \"content\": \"\\\"\\\"\\\"Tests for the gaps module.\\\"\\\"\\\"\\n\\nimport unittest\\nimport sys\\nsys.path.insert(0, '..')\\n\\nfrom cortical import CorticalTextProcessor, CorticalLayer\\nfrom cortical.gaps import analyze_knowledge_gaps, detect_anomalies\\n\\n\\nclass TestGaps(unittest.TestCase):\\n    \\\"\\\"\\\"Test the gaps module.\\\"\\\"\\\"\\n\\n    @classmethod\\n    def setUpClass(cls):\\n        \\\"\\\"\\\"Set up processor with sample data including an outlier.\\\"\\\"\\\"\\n        cls.processor = CorticalTextProcessor()\\n        # Create cluster of related documents\\n        for i in range(3):\\n            cls.processor.process_document(f\\\"tech_{i}\\\", \\\"\\\"\\\"\\n                Machine learning neural networks deep learning.\\n                Training models data processing algorithms.\\n                Pattern recognition artificial intelligence.\\n            \\\"\\\"\\\")\\n        # Add outlier document with different topic\\n        cls.processor.process_document(\\\"outlier\\\", \\\"\\\"\\\"\\n            Medieval falconry birds hunting prey.\\n            Falcons hawks eagles training techniques.\\n            Ancient hunting traditions wildlife.\\n        \\\"\\\"\\\")\\n        cls.processor.compute_all(verbose=False)\\n\\n    def test_analyze_knowledge_gaps_structure(self):\\n        \\\"\\\"\\\"Test that gap analysis returns expected structure.\\\"\\\"\\\"\\n        gaps = analyze_knowledge_gaps(\\n            self.processor.layers,\\n            self.processor.documents\\n        )\\n\\n        # Check all expected keys are present\\n        self.assertIn('isolated_documents', gaps)\\n        self.assertIn('weak_topics', gaps)\\n        self.assertIn('bridge_opportunities', gaps)\\n        self.assertIn('connector_terms', gaps)\\n        self.assertIn('coverage_score', gaps)\\n        self.assertIn('connectivity_score', gaps)\\n        self.assertIn('summary', gaps)\\n\\n    def test_analyze_knowledge_gaps_summary(self):\\n        \\\"\\\"\\\"Test that summary contains expected fields.\\\"\\\"\\\"\\n        gaps = analyze_knowledge_gaps(\\n            self.processor.layers,\\n            self.processor.documents\\n        )\\n\\n        summary = gaps['summary']\\n        self.assertIn('total_documents', summary)\\n        self.assertIn('isolated_count', summary)\\n        self.assertIn('well_connected_count', summary)\\n        self.assertIn('weak_topic_count', summary)\\n\\n        self.assertEqual(summary['total_documents'], 4)\\n\\n    def test_analyze_knowledge_gaps_isolated_documents(self):\\n        \\\"\\\"\\\"Test isolated documents detection.\\\"\\\"\\\"\\n        gaps = analyze_knowledge_gaps(\\n            self.processor.layers,\\n            self.processor.documents\\n        )\\n\\n        isolated = gaps['isolated_documents']\\n        self.assertIsInstance(isolated, list)\\n\\n        # Each isolated doc should have expected fields\\n        for doc in isolated:\\n            self.assertIn('doc_id', doc)\\n            self.assertIn('avg_similarity', doc)\\n            self.assertIn('max_similarity', doc)\\n\\n    def test_analyze_knowledge_gaps_weak_topics(self):\\n        \\\"\\\"\\\"Test weak topics detection.\\\"\\\"\\\"\\n        gaps = analyze_knowledge_gaps(\\n            self.processor.layers,\\n            self.processor.documents\\n        )\\n\\n        weak_topics = gaps['weak_topics']\\n        self.assertIsInstance(weak_topics, list)\\n\\n        for topic in weak_topics:\\n            self.assertIn('term', topic)\\n            self.assertIn('tfidf', topic)\\n            self.assertIn('doc_count', topic)\\n            self.assertIn('documents', topic)\\n\\n    def test_analyze_knowledge_gaps_coverage_score(self):\\n        \\\"\\\"\\\"Test coverage score is valid.\\\"\\\"\\\"\\n        gaps = analyze_knowledge_gaps(\\n            self.processor.layers,\\n            self.processor.documents\\n        )\\n\\n        self.assertIsInstance(gaps['coverage_score'], float)\\n        self.assertGreaterEqual(gaps['coverage_score'], 0.0)\\n        self.assertLessEqual(gaps['coverage_score'], 1.0)\\n\\n    def test_detect_anomalies_structure(self):\\n        \\\"\\\"\\\"Test anomaly detection returns expected structure.\\\"\\\"\\\"\\n        anomalies = detect_anomalies(\\n            self.processor.layers,\\n            self.processor.documents,\\n            threshold=0.3\\n        )\\n\\n        self.assertIsInstance(anomalies, list)\\n\\n        for anomaly in anomalies:\\n            self.assertIn('doc_id', anomaly)\\n            self.assertIn('avg_similarity', anomaly)\\n            self.assertIn('max_similarity', anomaly)\\n            self.assertIn('connections', anomaly)\\n            self.assertIn('reasons', anomaly)\\n            self.assertIn('distinctive_terms', anomaly)\\n\\n    def test_detect_anomalies_reasons(self):\\n        \\\"\\\"\\\"Test that anomalies have reasons.\\\"\\\"\\\"\\n        anomalies = detect_anomalies(\\n            self.processor.layers,\\n            self.processor.documents,\\n            threshold=0.3\\n        )\\n\\n        for anomaly in anomalies:\\n            self.assertIsInstance(anomaly['reasons'], list)\\n            # Each anomaly should have at least one reason\\n            self.assertGreater(len(anomaly['reasons']), 0)\\n\\n    def test_detect_anomalies_sorted(self):\\n        \\\"\\\"\\\"Test that anomalies are sorted by similarity (ascending).\\\"\\\"\\\"\\n        anomalies = detect_anomalies(\\n            self.processor.layers,\\n            self.processor.documents,\\n            threshold=0.5\\n        )\\n\\n        if len(anomalies) > 1:\\n            similarities = [a['avg_similarity'] for a in anomalies]\\n            self.assertEqual(similarities, sorted(similarities))\\n\\n    def test_detect_anomalies_threshold(self):\\n        \\\"\\\"\\\"Test that threshold affects anomaly detection.\\\"\\\"\\\"\\n        anomalies_low = detect_anomalies(\\n            self.processor.layers,\\n            self.processor.documents,\\n            threshold=0.1\\n        )\\n\\n        anomalies_high = detect_anomalies(\\n            self.processor.layers,\\n            self.processor.documents,\\n            threshold=0.5\\n        )\\n\\n        # Higher threshold should find more or equal anomalies\\n        self.assertGreaterEqual(len(anomalies_high), len(anomalies_low))\\n\\n\\nclass TestGapsEmptyCorpus(unittest.TestCase):\\n    \\\"\\\"\\\"Test gaps module with empty or minimal corpus.\\\"\\\"\\\"\\n\\n    def test_empty_corpus_gaps(self):\\n        \\\"\\\"\\\"Test gap analysis on empty processor.\\\"\\\"\\\"\\n        processor = CorticalTextProcessor()\\n        gaps = analyze_knowledge_gaps(\\n            processor.layers,\\n            processor.documents\\n        )\\n\\n        self.assertEqual(gaps['summary']['total_documents'], 0)\\n        self.assertEqual(gaps['isolated_documents'], [])\\n        self.assertEqual(gaps['weak_topics'], [])\\n\\n    def test_single_document_gaps(self):\\n        \\\"\\\"\\\"Test gap analysis with single document.\\\"\\\"\\\"\\n        processor = CorticalTextProcessor()\\n        processor.process_document(\\\"only_doc\\\", \\\"Single document content here.\\\")\\n        processor.compute_all(verbose=False)\\n\\n        gaps = analyze_knowledge_gaps(\\n            processor.layers,\\n            processor.documents\\n        )\\n\\n        self.assertEqual(gaps['summary']['total_documents'], 1)\\n\\n    def test_single_document_anomalies(self):\\n        \\\"\\\"\\\"Test anomaly detection with single document.\\\"\\\"\\\"\\n        processor = CorticalTextProcessor()\\n        processor.process_document(\\\"only_doc\\\", \\\"Single document content here.\\\")\\n        processor.compute_all(verbose=False)\\n\\n        anomalies = detect_anomalies(\\n            processor.layers,\\n            processor.documents,\\n            threshold=0.3\\n        )\\n\\n        # Single doc can't have similarity to others\\n        self.assertIsInstance(anomalies, list)\\n\\n\\nclass TestGapsBridgeOpportunities(unittest.TestCase):\\n    \\\"\\\"\\\"Test bridge opportunity detection.\\\"\\\"\\\"\\n\\n    def test_bridge_opportunities_format(self):\\n        \\\"\\\"\\\"Test bridge opportunities have correct format.\\\"\\\"\\\"\\n        processor = CorticalTextProcessor()\\n        processor.process_document(\\\"doc1\\\", \\\"neural networks learning deep\\\")\\n        processor.process_document(\\\"doc2\\\", \\\"machine learning algorithms\\\")\\n        processor.process_document(\\\"doc3\\\", \\\"database systems storage\\\")\\n        processor.compute_all(verbose=False)\\n\\n        gaps = analyze_knowledge_gaps(\\n            processor.layers,\\n            processor.documents\\n        )\\n\\n        bridges = gaps['bridge_opportunities']\\n        self.assertIsInstance(bridges, list)\\n\\n        for bridge in bridges:\\n            self.assertIn('doc1', bridge)\\n            self.assertIn('doc2', bridge)\\n            self.assertIn('similarity', bridge)\\n            self.assertIn('shared_terms', bridge)\\n\\n\\nif __name__ == \\\"__main__\\\":\\n    unittest.main(verbosity=2)\\n\",",
        "      \"mtime\": 1765375584.0",
        "    },",
        "    {",
        "      \"op\": \"add\",",
        "      \"doc_id\": \"cortical/persistence.py\",",
        "      \"content\": \"\\\"\\\"\\\"\\nPersistence Module\\n==================\\n\\nSave and load functionality for the cortical processor.\\n\\nSupports:\\n- Pickle serialization for full state\\n- JSON export for graph visualization\\n- Incremental updates\\n\\\"\\\"\\\"\\n\\nimport pickle\\nimport json\\nimport os\\nfrom typing import Dict, Optional, Any\\n\\nfrom .layers import CorticalLayer, HierarchicalLayer\\nfrom .minicolumn import Minicolumn\\n\\n\\ndef save_processor(\\n    filepath: str,\\n    layers: Dict[CorticalLayer, HierarchicalLayer],\\n    documents: Dict[str, str],\\n    document_metadata: Optional[Dict[str, Dict[str, Any]]] = None,\\n    embeddings: Optional[Dict[str, list]] = None,\\n    semantic_relations: Optional[list] = None,\\n    metadata: Optional[Dict] = None,\\n    verbose: bool = True\\n) -> None:\\n    \\\"\\\"\\\"\\n    Save processor state to a file.\\n\\n    Args:\\n        filepath: Path to save file\\n        layers: Dictionary of all layers\\n        documents: Document collection\\n        document_metadata: Per-document metadata (source, timestamp, etc.)\\n        embeddings: Graph embeddings for terms (optional)\\n        semantic_relations: Extracted semantic relations (optional)\\n        metadata: Optional processor metadata (version, settings, etc.)\\n        verbose: Print progress\\n    \\\"\\\"\\\"\\n    state = {\\n        'version': '2.2',\\n        'layers': {},\\n        'documents': documents,\\n        'document_metadata': document_metadata or {},\\n        'embeddings': embeddings or {},\\n        'semantic_relations': semantic_relations or [],\\n        'metadata': metadata or {}\\n    }\\n\\n    # Serialize layers\\n    for layer_enum, layer in layers.items():\\n        state['layers'][layer_enum.value] = layer.to_dict()\\n\\n    with open(filepath, 'wb') as f:\\n        pickle.dump(state, f, protocol=pickle.HIGHEST_PROTOCOL)\\n\\n    if verbose:\\n        total_cols = sum(len(layer.minicolumns) for layer in layers.values())\\n        total_conns = sum(layer.total_connections() for layer in layers.values())\\n        print(f\\\"✓ Saved processor to {filepath}\\\")\\n        print(f\\\"  - {len(documents)} documents\\\")\\n        print(f\\\"  - {total_cols} minicolumns\\\")\\n        print(f\\\"  - {total_conns} connections\\\")\\n        if embeddings:\\n            print(f\\\"  - {len(embeddings)} embeddings\\\")\\n        if semantic_relations:\\n            print(f\\\"  - {len(semantic_relations)} semantic relations\\\")\\n\\n\\ndef load_processor(\\n    filepath: str,\\n    verbose: bool = True\\n) -> tuple:\\n    \\\"\\\"\\\"\\n    Load processor state from a file.\\n\\n    Args:\\n        filepath: Path to saved file\\n        verbose: Print progress\\n\\n    Returns:\\n        Tuple of (layers, documents, document_metadata, embeddings, semantic_relations, metadata)\\n    \\\"\\\"\\\"\\n    with open(filepath, 'rb') as f:\\n        state = pickle.load(f)\\n\\n    # Reconstruct layers\\n    layers = {}\\n    for level_value, layer_data in state.get('layers', {}).items():\\n        layer = HierarchicalLayer.from_dict(layer_data)\\n        layers[CorticalLayer(int(level_value))] = layer\\n\\n    documents = state.get('documents', {})\\n    document_metadata = state.get('document_metadata', {})\\n    embeddings = state.get('embeddings', {})\\n    semantic_relations = state.get('semantic_relations', [])\\n    metadata = state.get('metadata', {})\\n\\n    if verbose:\\n        total_cols = sum(len(layer.minicolumns) for layer in layers.values())\\n        total_conns = sum(layer.total_connections() for layer in layers.values())\\n        print(f\\\"✓ Loaded processor from {filepath}\\\")\\n        print(f\\\"  - {len(documents)} documents\\\")\\n        print(f\\\"  - {total_cols} minicolumns\\\")\\n        print(f\\\"  - {total_conns} connections\\\")\\n        if embeddings:\\n            print(f\\\"  - {len(embeddings)} embeddings\\\")\\n        if semantic_relations:\\n            print(f\\\"  - {len(semantic_relations)} semantic relations\\\")\\n\\n    return layers, documents, document_metadata, embeddings, semantic_relations, metadata\\n\\n\\ndef export_graph_json(\\n    filepath: str,\\n    layers: Dict[CorticalLayer, HierarchicalLayer],\\n    layer_filter: Optional[CorticalLayer] = None,\\n    min_weight: float = 0.0,\\n    max_nodes: int = 500,\\n    verbose: bool = True\\n) -> Dict:\\n    \\\"\\\"\\\"\\n    Export graph structure as JSON for visualization.\\n\\n    Creates a format compatible with D3.js, vis.js, etc.\\n\\n    Args:\\n        filepath: Output file path\\n        layers: Dictionary of layers\\n        layer_filter: Only export specific layer (None = all)\\n        min_weight: Minimum edge weight to include\\n        max_nodes: Maximum nodes to export\\n        verbose: Print progress messages\\n\\n    Returns:\\n        The exported graph data\\n    \\\"\\\"\\\"\\n    nodes = []\\n    edges = []\\n    node_ids = set()\\n    \\n    # Determine which layers to export\\n    if layer_filter is not None:\\n        layer_list = [layers.get(layer_filter)]\\n    else:\\n        layer_list = list(layers.values())\\n    \\n    # Collect nodes (sorted by PageRank)\\n    all_columns = []\\n    for layer in layer_list:\\n        if layer:\\n            all_columns.extend(layer.minicolumns.values())\\n    \\n    all_columns.sort(key=lambda c: c.pagerank, reverse=True)\\n    \\n    # Take top nodes\\n    for col in all_columns[:max_nodes]:\\n        nodes.append({\\n            'id': col.id,\\n            'label': col.content,\\n            'layer': col.layer,\\n            'pagerank': col.pagerank,\\n            'tfidf': col.tfidf,\\n            'activation': col.activation,\\n            'documents': len(col.document_ids)\\n        })\\n        node_ids.add(col.id)\\n    \\n    # Collect edges\\n    for col in all_columns[:max_nodes]:\\n        for target_id, weight in col.lateral_connections.items():\\n            if weight >= min_weight and target_id in node_ids:\\n                edges.append({\\n                    'source': col.id,\\n                    'target': target_id,\\n                    'weight': weight\\n                })\\n    \\n    graph = {\\n        'nodes': nodes,\\n        'edges': edges,\\n        'metadata': {\\n            'node_count': len(nodes),\\n            'edge_count': len(edges),\\n            'layers': [l.value for l in layers.keys() if l is not None]\\n        }\\n    }\\n    \\n    with open(filepath, 'w') as f:\\n        json.dump(graph, f, indent=2)\\n\\n    if verbose:\\n        print(f\\\"Graph exported to {filepath}\\\")\\n        print(f\\\"  - {len(nodes)} nodes, {len(edges)} edges\\\")\\n\\n    return graph\\n\\n\\ndef export_embeddings_json(\\n    filepath: str,\\n    embeddings: Dict[str, list],\\n    metadata: Optional[Dict] = None\\n) -> None:\\n    \\\"\\\"\\\"\\n    Export embeddings as JSON.\\n    \\n    Args:\\n        filepath: Output file path\\n        embeddings: Dictionary of term -> embedding vector\\n        metadata: Optional metadata\\n    \\\"\\\"\\\"\\n    data = {\\n        'embeddings': embeddings,\\n        'dimensions': len(next(iter(embeddings.values()))) if embeddings else 0,\\n        'terms': len(embeddings),\\n        'metadata': metadata or {}\\n    }\\n    \\n    with open(filepath, 'w') as f:\\n        json.dump(data, f)\\n    \\n    print(f\\\"Embeddings exported to {filepath}\\\")\\n    print(f\\\"  - {len(embeddings)} terms, {data['dimensions']} dimensions\\\")\\n\\n\\ndef load_embeddings_json(filepath: str) -> Dict[str, list]:\\n    \\\"\\\"\\\"\\n    Load embeddings from JSON.\\n    \\n    Args:\\n        filepath: Input file path\\n        \\n    Returns:\\n        Dictionary of term -> embedding vector\\n    \\\"\\\"\\\"\\n    with open(filepath, 'r') as f:\\n        data = json.load(f)\\n    \\n    return data.get('embeddings', {})\\n\\n\\ndef export_semantic_relations_json(\\n    filepath: str,\\n    relations: list\\n) -> None:\\n    \\\"\\\"\\\"\\n    Export semantic relations as JSON.\\n    \\n    Args:\\n        filepath: Output file path\\n        relations: List of relation dictionaries\\n    \\\"\\\"\\\"\\n    with open(filepath, 'w') as f:\\n        json.dump({\\n            'relations': relations,\\n            'count': len(relations)\\n        }, f, indent=2)\\n    \\n    print(f\\\"Relations exported to {filepath}\\\")\\n    print(f\\\"  - {len(relations)} relations\\\")\\n\\n\\ndef load_semantic_relations_json(filepath: str) -> list:\\n    \\\"\\\"\\\"\\n    Load semantic relations from JSON.\\n    \\n    Args:\\n        filepath: Input file path\\n        \\n    Returns:\\n        List of relation dictionaries\\n    \\\"\\\"\\\"\\n    with open(filepath, 'r') as f:\\n        data = json.load(f)\\n    \\n    return data.get('relations', [])\\n\\n\\ndef get_state_summary(\\n    layers: Dict[CorticalLayer, HierarchicalLayer],\\n    documents: Dict[str, str]\\n) -> Dict:\\n    \\\"\\\"\\\"\\n    Get a summary of the current processor state.\\n\\n    Args:\\n        layers: Dictionary of layers\\n        documents: Document collection\\n\\n    Returns:\\n        Summary statistics\\n    \\\"\\\"\\\"\\n    summary = {\\n        'documents': len(documents),\\n        'layers': {}\\n    }\\n\\n    for layer_enum, layer in layers.items():\\n        summary['layers'][layer_enum.name] = {\\n            'columns': len(layer.minicolumns),\\n            'connections': layer.total_connections(),\\n            'avg_activation': layer.average_activation(),\\n            'sparsity': layer.sparsity()\\n        }\\n\\n    summary['total_columns'] = sum(\\n        len(layer.minicolumns) for layer in layers.values()\\n    )\\n    summary['total_connections'] = sum(\\n        layer.total_connections() for layer in layers.values()\\n    )\\n\\n    return summary\\n\\n\\n# Layer colors for visualization\\nLAYER_COLORS = {\\n    CorticalLayer.TOKENS: '#4169E1',     # Royal Blue\\n    CorticalLayer.BIGRAMS: '#228B22',    # Forest Green\\n    CorticalLayer.CONCEPTS: '#FF8C00',   # Dark Orange\\n    CorticalLayer.DOCUMENTS: '#DC143C',  # Crimson\\n}\\n\\n# Layer display names\\nLAYER_NAMES = {\\n    CorticalLayer.TOKENS: 'Tokens',\\n    CorticalLayer.BIGRAMS: 'Bigrams',\\n    CorticalLayer.CONCEPTS: 'Concepts',\\n    CorticalLayer.DOCUMENTS: 'Documents',\\n}\\n\\n\\ndef export_conceptnet_json(\\n    filepath: str,\\n    layers: Dict[CorticalLayer, HierarchicalLayer],\\n    semantic_relations: Optional[list] = None,\\n    include_cross_layer: bool = True,\\n    include_typed_edges: bool = True,\\n    min_weight: float = 0.0,\\n    min_confidence: float = 0.0,\\n    max_nodes_per_layer: int = 100,\\n    verbose: bool = True\\n) -> Dict[str, Any]:\\n    \\\"\\\"\\\"\\n    Export ConceptNet-style graph for visualization.\\n\\n    Creates a rich graph format with:\\n    - Color-coded nodes by layer\\n    - Typed edges with relation types and confidence\\n    - Cross-layer connections (feedforward/feedback)\\n    - D3.js/Cytoscape-compatible output\\n\\n    Args:\\n        filepath: Output file path (JSON)\\n        layers: Dictionary of layers\\n        semantic_relations: Optional list of (t1, rel, t2, weight) tuples\\n        include_cross_layer: Include feedforward/feedback edges\\n        include_typed_edges: Include typed_connections with relation types\\n        min_weight: Minimum edge weight to include\\n        min_confidence: Minimum confidence for typed edges\\n        max_nodes_per_layer: Maximum nodes per layer (by PageRank)\\n        verbose: Print progress messages\\n\\n    Returns:\\n        The exported graph data\\n\\n    Example:\\n        >>> export_conceptnet_json(\\n        ...     \\\"graph.json\\\", processor.layers,\\n        ...     semantic_relations=processor.semantic_relations\\n        ... )\\n    \\\"\\\"\\\"\\n    nodes = []\\n    edges = []\\n    node_ids = set()\\n    edge_set = set()  # Track unique edges\\n\\n    # Collect nodes from each layer\\n    for layer_enum, layer in layers.items():\\n        if layer is None or layer.column_count() == 0:\\n            continue\\n\\n        color = LAYER_COLORS.get(layer_enum, '#808080')\\n        layer_name = LAYER_NAMES.get(layer_enum, f'Layer {layer_enum.value}')\\n\\n        # Sort by PageRank and take top nodes\\n        sorted_cols = sorted(\\n            layer.minicolumns.values(),\\n            key=lambda c: c.pagerank,\\n            reverse=True\\n        )[:max_nodes_per_layer]\\n\\n        for col in sorted_cols:\\n            node = {\\n                'id': col.id,\\n                'label': col.content,\\n                'layer': layer_enum.value,\\n                'layer_name': layer_name,\\n                'color': color,\\n                'pagerank': round(col.pagerank, 6),\\n                'tfidf': round(col.tfidf, 6),\\n                'activation': round(col.activation, 6),\\n                'occurrence_count': col.occurrence_count,\\n                'document_count': len(col.document_ids),\\n                'cluster_id': col.cluster_id\\n            }\\n            nodes.append(node)\\n            node_ids.add(col.id)\\n\\n    # Collect lateral edges (same-layer connections)\\n    for layer_enum, layer in layers.items():\\n        if layer is None:\\n            continue\\n\\n        for col in layer.minicolumns.values():\\n            if col.id not in node_ids:\\n                continue\\n\\n            # Add typed edges with relation information\\n            if include_typed_edges:\\n                for target_id, edge_obj in col.typed_connections.items():\\n                    if target_id in node_ids and edge_obj.weight >= min_weight:\\n                        if edge_obj.confidence >= min_confidence:\\n                            edge_key = (col.id, target_id, edge_obj.relation_type)\\n                            if edge_key not in edge_set:\\n                                edge_set.add(edge_key)\\n                                edges.append({\\n                                    'source': col.id,\\n                                    'target': target_id,\\n                                    'weight': round(edge_obj.weight, 4),\\n                                    'relation_type': edge_obj.relation_type,\\n                                    'confidence': round(edge_obj.confidence, 4),\\n                                    'source_type': edge_obj.source,\\n                                    'edge_type': 'lateral',\\n                                    'color': _get_relation_color(edge_obj.relation_type)\\n                                })\\n\\n            # Add regular lateral connections (without typed info)\\n            for target_id, weight in col.lateral_connections.items():\\n                if target_id in node_ids and weight >= min_weight:\\n                    # Skip if already added as typed edge\\n                    if include_typed_edges and target_id in col.typed_connections:\\n                        continue\\n                    edge_key = (col.id, target_id, 'co_occurrence')\\n                    if edge_key not in edge_set:\\n                        edge_set.add(edge_key)\\n                        edges.append({\\n                            'source': col.id,\\n                            'target': target_id,\\n                            'weight': round(weight, 4),\\n                            'relation_type': 'co_occurrence',\\n                            'confidence': 1.0,\\n                            'source_type': 'corpus',\\n                            'edge_type': 'lateral',\\n                            'color': '#999999'\\n                        })\\n\\n    # Add cross-layer edges (feedforward/feedback)\\n    if include_cross_layer:\\n        for layer_enum, layer in layers.items():\\n            if layer is None:\\n                continue\\n\\n            for col in layer.minicolumns.values():\\n                if col.id not in node_ids:\\n                    continue\\n\\n                # Feedforward connections (to lower layers)\\n                for target_id, weight in col.feedforward_connections.items():\\n                    if target_id in node_ids and weight >= min_weight:\\n                        edge_key = (col.id, target_id, 'feedforward')\\n                        if edge_key not in edge_set:\\n                            edge_set.add(edge_key)\\n                            edges.append({\\n                                'source': col.id,\\n                                'target': target_id,\\n                                'weight': round(weight, 4),\\n                                'relation_type': 'feedforward',\\n                                'confidence': 1.0,\\n                                'source_type': 'structure',\\n                                'edge_type': 'cross_layer',\\n                                'color': '#4CAF50'  # Green\\n                            })\\n\\n                # Feedback connections (to higher layers)\\n                for target_id, weight in col.feedback_connections.items():\\n                    if target_id in node_ids and weight >= min_weight:\\n                        edge_key = (col.id, target_id, 'feedback')\\n                        if edge_key not in edge_set:\\n                            edge_set.add(edge_key)\\n                            edges.append({\\n                                'source': col.id,\\n                                'target': target_id,\\n                                'weight': round(weight, 4),\\n                                'relation_type': 'feedback',\\n                                'confidence': 1.0,\\n                                'source_type': 'structure',\\n                                'edge_type': 'cross_layer',\\n                                'color': '#9C27B0'  # Purple\\n                            })\\n\\n    # Add edges from semantic relations if provided\\n    if semantic_relations:\\n        for rel in semantic_relations:\\n            if len(rel) >= 4:\\n                t1, rel_type, t2, weight = rel[:4]\\n                # Find node IDs\\n                source_id = f\\\"L0_{t1}\\\"\\n                target_id = f\\\"L0_{t2}\\\"\\n                if source_id in node_ids and target_id in node_ids:\\n                    if weight >= min_weight:\\n                        edge_key = (source_id, target_id, rel_type)\\n                        if edge_key not in edge_set:\\n                            edge_set.add(edge_key)\\n                            edges.append({\\n                                'source': source_id,\\n                                'target': target_id,\\n                                'weight': round(weight, 4),\\n                                'relation_type': rel_type,\\n                                'confidence': 1.0,\\n                                'source_type': 'semantic',\\n                                'edge_type': 'semantic',\\n                                'color': _get_relation_color(rel_type)\\n                            })\\n\\n    # Build graph structure\\n    graph = {\\n        'nodes': nodes,\\n        'edges': edges,\\n        'metadata': {\\n            'node_count': len(nodes),\\n            'edge_count': len(edges),\\n            'layers': {\\n                layer_enum.value: {\\n                    'name': LAYER_NAMES.get(layer_enum, f'Layer {layer_enum.value}'),\\n                    'color': LAYER_COLORS.get(layer_enum, '#808080'),\\n                    'node_count': sum(1 for n in nodes if n['layer'] == layer_enum.value)\\n                }\\n                for layer_enum in layers.keys()\\n            },\\n            'edge_types': _count_edge_types(edges),\\n            'relation_types': _count_relation_types(edges),\\n            'format_version': '1.0',\\n            'compatible_with': ['D3.js', 'Cytoscape.js', 'vis.js', 'Gephi']\\n        }\\n    }\\n\\n    # Write to file\\n    with open(filepath, 'w') as f:\\n        json.dump(graph, f, indent=2)\\n\\n    if verbose:\\n        print(f\\\"ConceptNet-style graph exported to {filepath}\\\")\\n        print(f\\\"  Nodes: {len(nodes)}\\\")\\n        print(f\\\"  Edges: {len(edges)}\\\")\\n        print(f\\\"  Layers: {list(graph['metadata']['layers'].keys())}\\\")\\n        print(f\\\"  Edge types: {graph['metadata']['edge_types']}\\\")\\n\\n    return graph\\n\\n\\ndef _get_relation_color(relation_type: str) -> str:\\n    \\\"\\\"\\\"Get color for a relation type.\\\"\\\"\\\"\\n    relation_colors = {\\n        'IsA': '#E91E63',         # Pink\\n        'PartOf': '#9C27B0',      # Purple\\n        'HasA': '#673AB7',        # Deep Purple\\n        'UsedFor': '#3F51B5',     # Indigo\\n        'Causes': '#F44336',      # Red\\n        'HasProperty': '#FF9800', # Orange\\n        'AtLocation': '#4CAF50',  # Green\\n        'CapableOf': '#00BCD4',   # Cyan\\n        'SimilarTo': '#2196F3',   # Blue\\n        'Antonym': '#795548',     # Brown\\n        'RelatedTo': '#607D8B',   # Blue Grey\\n        'CoOccurs': '#9E9E9E',    # Grey\\n        'DerivedFrom': '#8BC34A', # Light Green\\n        'DefinedBy': '#FFEB3B',   # Yellow\\n        'feedforward': '#4CAF50', # Green\\n        'feedback': '#9C27B0',    # Purple\\n        'co_occurrence': '#999999',  # Grey\\n    }\\n    return relation_colors.get(relation_type, '#808080')\\n\\n\\ndef _count_edge_types(edges: list) -> Dict[str, int]:\\n    \\\"\\\"\\\"Count edges by edge_type.\\\"\\\"\\\"\\n    counts: Dict[str, int] = {}\\n    for edge in edges:\\n        edge_type = edge.get('edge_type', 'unknown')\\n        counts[edge_type] = counts.get(edge_type, 0) + 1\\n    return counts\\n\\n\\ndef _count_relation_types(edges: list) -> Dict[str, int]:\\n    \\\"\\\"\\\"Count edges by relation_type.\\\"\\\"\\\"\\n    counts: Dict[str, int] = {}\\n    for edge in edges:\\n        rel_type = edge.get('relation_type', 'unknown')\\n        counts[rel_type] = counts.get(rel_type, 0) + 1\\n    return counts\\n\",",
        "      \"mtime\": 1765375584.0",
        "    },",
        "    {",
        "      \"op\": \"add\",",
        "      \"doc_id\": \"docs/dogfooding.md\",",
        "      \"content\": \"# Dog-Fooding Guide\\n\\nThis guide explains how the Cortical Text Processor is used to build and improve itself - a practice known as \\\"dog-fooding.\\\" The system indexes its own codebase, enabling semantic search during development.\\n\\n---\\n\\n## Overview\\n\\n**Dog-fooding** means using your own product to develop it. The Cortical Text Processor can:\\n\\n1. **Index its own source code** - Build a searchable semantic model of the codebase\\n2. **Search semantically** - Find relevant code by meaning, not just keywords\\n3. **Update incrementally** - Keep the index current as code changes\\n4. **Integrate with Claude** - Provide semantic search via Claude skills\\n\\n```\\n┌─────────────────────────────────────────────────────────────────┐\\n│                    Dog-Fooding Workflow                          │\\n│                                                                  │\\n│  ┌──────────┐    ┌──────────┐    ┌──────────┐    ┌──────────┐  │\\n│  │  Index   │───▶│  Search  │───▶│ Develop  │───▶│ Re-index │  │\\n│  │ Codebase │    │   Code   │    │   Code   │    │  Changes │  │\\n│  └──────────┘    └──────────┘    └──────────┘    └────┬─────┘  │\\n│       ▲                                               │        │\\n│       └───────────────────────────────────────────────┘        │\\n└─────────────────────────────────────────────────────────────────┘\\n```\\n\\n---\\n\\n## Quick Start\\n\\n### 1. Index the Codebase\\n\\n```bash\\n# First time: Full index (~2-3 seconds)\\npython scripts/index_codebase.py\\n\\n# After changes: Incremental update (~1 second)\\npython scripts/index_codebase.py --incremental\\n```\\n\\n### 2. Search for Code\\n\\n```bash\\n# Basic search\\npython scripts/search_codebase.py \\\"PageRank algorithm\\\"\\n\\n# See query expansion\\npython scripts/search_codebase.py \\\"bigram separator\\\" --expand\\n\\n# Interactive mode\\npython scripts/search_codebase.py --interactive\\n```\\n\\n### 3. Use Claude Skills\\n\\nWhen using Claude Code in this project:\\n- **codebase-search**: Search for code patterns and implementations\\n- **corpus-indexer**: Re-index after making changes\\n\\n---\\n\\n## The Indexing System\\n\\n### What Gets Indexed\\n\\nThe indexer processes these files:\\n\\n| Category | Pattern | Purpose |\\n|----------|---------|---------|\\n| Source code | `cortical/*.py` | Core library implementation |\\n| Tests | `tests/*.py` | Test cases and examples |\\n| Documentation | `CLAUDE.md`, `README.md` | Project documentation |\\n| Intelligence | `docs/*.md` | Architecture docs |\\n| Task tracking | `TASK_LIST.md` | Development tasks |\\n\\n### Index Files Created\\n\\n| File | Purpose |\\n|------|---------|\\n| `corpus_dev.pkl` | Serialized processor state (searchable index) |\\n| `corpus_dev.manifest.json` | File modification times for incremental updates |\\n\\n### Indexer Options\\n\\n```bash\\n# Show what would be indexed without doing it\\npython scripts/index_codebase.py --status\\n\\n# Force full rebuild even if nothing changed\\npython scripts/index_codebase.py --force\\n\\n# See per-file progress\\npython scripts/index_codebase.py --verbose\\n\\n# Log to file for debugging\\npython scripts/index_codebase.py --log indexer.log\\n\\n# Set timeout (default 300s)\\npython scripts/index_codebase.py --timeout 60\\n\\n# Full semantic analysis (slower, more accurate)\\npython scripts/index_codebase.py --full-analysis\\n```\\n\\n---\\n\\n## Incremental Indexing\\n\\nIncremental indexing is the key to efficient dog-fooding. Instead of rebuilding the entire index, it only processes changes.\\n\\n### How It Works\\n\\n```\\n1. Load manifest (file modification times from last index)\\n2. Scan current files\\n3. Detect changes:\\n   - ADDED: Files that didn't exist before\\n   - MODIFIED: Files with newer modification times\\n   - DELETED: Files in manifest but no longer exist\\n4. Update only changed files:\\n   - Remove deleted docs from index\\n   - Re-index modified files (remove old, add new)\\n   - Add new files\\n5. Recompute analysis (PageRank, TF-IDF, etc.)\\n6. Save updated index and manifest\\n```\\n\\n### Performance\\n\\n| Operation | Time | Use Case |\\n|-----------|------|----------|\\n| No changes detected | ~0.1s | Check if re-index needed |\\n| Few files changed | ~1-2s | Normal development |\\n| Full rebuild (fast mode) | ~2-3s | After major refactoring |\\n| Full rebuild (full analysis) | ~10+ min | Before deep exploration |\\n\\n### When to Re-index\\n\\n| Scenario | Command |\\n|----------|---------|\\n| After editing code | `--incremental` |\\n| After adding new files | `--incremental` |\\n| After deleting files | `--incremental` |\\n| After major refactoring | `--force` |\\n| Before deep code exploration | `--full-analysis` |\\n| Search results seem stale | `--status` then decide |\\n\\n---\\n\\n## Search Capabilities\\n\\n### Basic Search\\n\\n```bash\\n# Find code related to a concept\\npython scripts/search_codebase.py \\\"query expansion\\\"\\n\\n# Output shows file:line references\\n# cortical/query.py:55  [0.847]\\n#   def get_expanded_query_terms(...)\\n```\\n\\n### Query Expansion\\n\\nThe search automatically expands queries with related terms:\\n\\n```bash\\npython scripts/search_codebase.py \\\"PageRank\\\" --expand\\n\\n# Shows: pagerank → importance, score, rank, algorithm, weight, ...\\n```\\n\\n### Interactive Mode\\n\\nFor exploratory searching:\\n\\n```bash\\npython scripts/search_codebase.py --interactive\\n\\n# Commands in interactive mode:\\n# /expand <query>  - Show query expansion terms\\n# /concepts        - List concept clusters\\n# /stats           - Show corpus statistics\\n# /quit            - Exit\\n```\\n\\n### Search Options\\n\\n| Option | Description |\\n|--------|-------------|\\n| `--top N` | Return N results (default: 5) |\\n| `--verbose` | Show full passage text |\\n| `--expand` | Show query expansion terms |\\n| `--fast` | Document-level search only (faster) |\\n| `--interactive` | Interactive search mode |\\n\\n---\\n\\n## Claude Skills Integration\\n\\n### codebase-search Skill\\n\\nUse this skill to search the indexed codebase from Claude:\\n\\n```\\n@claude: Use codebase-search to find how PageRank is implemented\\n```\\n\\nThe skill:\\n1. Loads the pre-built corpus (`corpus_dev.pkl`)\\n2. Executes semantic search\\n3. Returns file:line references with relevant passages\\n\\n### corpus-indexer Skill\\n\\nUse this skill to re-index after making changes:\\n\\n```\\n@claude: Use corpus-indexer to update the index\\n\\n# Or specifically:\\n@claude: Use corpus-indexer with --incremental flag\\n```\\n\\nThe skill runs `scripts/index_codebase.py` with appropriate options.\\n\\n---\\n\\n## Development Workflow\\n\\n### Typical Development Cycle\\n\\n```bash\\n# 1. Start by searching for relevant code\\npython scripts/search_codebase.py \\\"feature I want to modify\\\"\\n\\n# 2. Make changes to the code\\n# ... edit files ...\\n\\n# 3. Run tests\\npython -m unittest discover -s tests -v\\n\\n# 4. Re-index to update search\\npython scripts/index_codebase.py --incremental\\n\\n# 5. Verify changes are searchable\\npython scripts/search_codebase.py \\\"my new function\\\"\\n```\\n\\n### Adding a New Feature\\n\\n1. **Research existing code**\\n   ```bash\\n   python scripts/search_codebase.py \\\"related functionality\\\" --verbose\\n   ```\\n\\n2. **Check the task list**\\n   ```bash\\n   python scripts/search_codebase.py \\\"TASK_LIST feature name\\\"\\n   ```\\n\\n3. **Implement the feature**\\n   - Follow patterns found in search results\\n   - Add tests in `tests/`\\n\\n4. **Update the index**\\n   ```bash\\n   python scripts/index_codebase.py --incremental --verbose\\n   ```\\n\\n5. **Verify searchability**\\n   ```bash\\n   python scripts/search_codebase.py \\\"new feature name\\\"\\n   ```\\n\\n### Debugging with Search\\n\\nWhen debugging, use semantic search to find related code:\\n\\n```bash\\n# Find error handling patterns\\npython scripts/search_codebase.py \\\"handle error exception\\\"\\n\\n# Find similar implementations\\npython scripts/search_codebase.py \\\"implementation pattern I'm looking at\\\"\\n\\n# Find test patterns\\npython scripts/search_codebase.py \\\"test case for feature\\\"\\n```\\n\\n---\\n\\n## Technical Details\\n\\n### Fast Mode vs Full Analysis\\n\\n**Fast Mode** (default):\\n- Skips `compute_bigram_connections()` - O(n²) on large corpora\\n- Computes: PageRank, TF-IDF, document connections\\n- Time: ~2-3 seconds\\n- Good for: Development, quick searches\\n\\n**Full Analysis Mode** (`--full-analysis`):\\n- Runs complete `compute_all()` pipeline\\n- Includes: Bigram connections, concept clusters, semantic relations\\n- Time: ~10+ minutes for full codebase\\n- Good for: Deep exploration, research sessions\\n\\n### Manifest File Format\\n\\n```json\\n{\\n  \\\"cortical/processor.py\\\": 1702234567.89,\\n  \\\"tests/test_processor.py\\\": 1702234590.12,\\n  ...\\n}\\n```\\n\\nMaps relative file paths to Unix modification timestamps.\\n\\n### Index Contents\\n\\nThe `corpus_dev.pkl` file contains a serialized `CorticalTextProcessor` with:\\n\\n- **Layer 0 (TOKENS)**: ~6,000+ unique terms from source code\\n- **Layer 1 (BIGRAMS)**: ~26,000+ word pairs\\n- **Layer 2 (CONCEPTS)**: Semantic clusters (if full analysis)\\n- **Layer 3 (DOCUMENTS)**: Each indexed file\\n\\n---\\n\\n## Troubleshooting\\n\\n### Index Taking Too Long\\n\\n**Symptom:** Indexer hangs at \\\"Computing analysis\\\"\\n\\n**Cause:** `compute_bigram_connections()` has O(n²) complexity\\n\\n**Solution:** Use fast mode (default) or add `--timeout`:\\n```bash\\npython scripts/index_codebase.py --timeout 60\\n```\\n\\n### Search Results Seem Stale\\n\\n**Check index status:**\\n```bash\\npython scripts/search_codebase.py --status\\n```\\n\\n**Force rebuild:**\\n```bash\\npython scripts/index_codebase.py --force\\n```\\n\\n### \\\"No corpus found\\\" Error\\n\\n**Cause:** `corpus_dev.pkl` doesn't exist\\n\\n**Solution:** Run initial indexing:\\n```bash\\npython scripts/index_codebase.py\\n```\\n\\n### Memory Issues with Large Corpus\\n\\n**Cause:** Full analysis mode creates many connections\\n\\n**Solution:** Use fast mode or limit file count\\n\\n### Index File Too Large\\n\\n**Cause:** Full analysis mode creates extensive connection data\\n\\n**Solution:** Use fast mode which produces smaller indices\\n\\n---\\n\\n## Best Practices\\n\\n### 1. Index Frequently\\n\\nRun `--incremental` after every significant code change:\\n```bash\\npython scripts/index_codebase.py --incremental\\n```\\n\\n### 2. Use --status Before Decisions\\n\\nCheck what would change before rebuilding:\\n```bash\\npython scripts/index_codebase.py --status\\n```\\n\\n### 3. Log for Debugging\\n\\nWhen investigating issues, enable logging:\\n```bash\\npython scripts/index_codebase.py --verbose --log debug.log\\n```\\n\\n### 4. Use Interactive Mode for Exploration\\n\\nWhen researching unfamiliar code:\\n```bash\\npython scripts/search_codebase.py --interactive\\n```\\n\\n### 5. Trust the Expansion\\n\\nLet query expansion find related terms:\\n```bash\\npython scripts/search_codebase.py \\\"authentication\\\" --expand\\n# May find: auth, login, credential, token, session, ...\\n```\\n\\n### 6. Combine with Git\\n\\nIndex before major refactoring to capture baseline:\\n```bash\\ngit status\\npython scripts/index_codebase.py --force --log pre-refactor.log\\n```\\n\\n---\\n\\n## File Reference\\n\\n| File | Purpose |\\n|------|---------|\\n| `scripts/index_codebase.py` | Codebase indexer with incremental support |\\n| `scripts/search_codebase.py` | Semantic search CLI |\\n| `corpus_dev.pkl` | Serialized index (generated) |\\n| `corpus_dev.manifest.json` | File modification times (generated) |\\n| `.claude/skills/codebase-search/` | Claude search skill |\\n| `.claude/skills/corpus-indexer/` | Claude indexer skill |\\n\\n---\\n\\n## Summary\\n\\nDog-fooding the Cortical Text Processor creates a virtuous cycle:\\n\\n1. **The system searches itself** - Find relevant code by meaning\\n2. **Changes improve search** - Better algorithms help find code\\n3. **Incremental updates are fast** - Stay productive during development\\n4. **Claude integration automates** - Skills handle indexing and search\\n\\nThis self-referential capability accelerates development by making the codebase semantically searchable while actively improving it.\\n\\n---\\n\\n*Updated 2025-12-10*\\n\",",
        "      \"mtime\": 1765400937.0",
        "    },",
        "    {",
        "      \"op\": \"add\",",
        "      \"doc_id\": \"cortical/chunk_index.py\",",
        "      \"content\": \"\\\"\\\"\\\"\\nChunk-based indexing for git-compatible corpus storage.\\n\\nThis module provides append-only, time-stamped JSON chunks that can be\\nsafely committed to git without merge conflicts. Each indexing session\\ncreates a uniquely named chunk file containing document operations.\\n\\nArchitecture:\\n    corpus_chunks/                        # Tracked in git\\n    ├── 2025-12-10_21-53-45_a1b2.json    # Session 1 changes\\n    ├── 2025-12-10_22-15-30_c3d4.json    # Session 2 changes\\n    └── 2025-12-10_23-00-00_e5f6.json    # Session 3 changes\\n\\n    corpus_dev.pkl                        # NOT tracked (local cache)\\n\\nChunk Format:\\n    {\\n        \\\"version\\\": 1,\\n        \\\"timestamp\\\": \\\"2025-12-10T21:53:45\\\",\\n        \\\"session_id\\\": \\\"a1b2c3d4\\\",\\n        \\\"branch\\\": \\\"main\\\",\\n        \\\"operations\\\": [\\n            {\\\"op\\\": \\\"add\\\", \\\"doc_id\\\": \\\"...\\\", \\\"content\\\": \\\"...\\\", \\\"mtime\\\": 123},\\n            {\\\"op\\\": \\\"modify\\\", \\\"doc_id\\\": \\\"...\\\", \\\"content\\\": \\\"...\\\", \\\"mtime\\\": 124},\\n            {\\\"op\\\": \\\"delete\\\", \\\"doc_id\\\": \\\"...\\\"}\\n        ]\\n    }\\n\\\"\\\"\\\"\\n\\nimport hashlib\\nimport json\\nimport os\\nimport subprocess\\nimport uuid\\nfrom dataclasses import dataclass, field, asdict\\nfrom datetime import datetime\\nfrom pathlib import Path\\nfrom typing import Dict, List, Optional, Tuple, Any\\n\\n\\n# Chunk format version\\nCHUNK_VERSION = 1\\n\\n\\n@dataclass\\nclass ChunkOperation:\\n    \\\"\\\"\\\"A single operation in a chunk (add, modify, or delete).\\\"\\\"\\\"\\n    op: str  # 'add', 'modify', 'delete'\\n    doc_id: str\\n    content: Optional[str] = None  # None for delete operations\\n    mtime: Optional[float] = None  # Modification time\\n\\n    def to_dict(self) -> Dict[str, Any]:\\n        \\\"\\\"\\\"Convert to dictionary for JSON serialization.\\\"\\\"\\\"\\n        d = {'op': self.op, 'doc_id': self.doc_id}\\n        if self.content is not None:\\n            d['content'] = self.content\\n        if self.mtime is not None:\\n            d['mtime'] = self.mtime\\n        return d\\n\\n    @classmethod\\n    def from_dict(cls, d: Dict[str, Any]) -> 'ChunkOperation':\\n        \\\"\\\"\\\"Create from dictionary.\\\"\\\"\\\"\\n        return cls(\\n            op=d['op'],\\n            doc_id=d['doc_id'],\\n            content=d.get('content'),\\n            mtime=d.get('mtime')\\n        )\\n\\n\\n@dataclass\\nclass Chunk:\\n    \\\"\\\"\\\"A chunk containing operations from a single indexing session.\\\"\\\"\\\"\\n    version: int\\n    timestamp: str\\n    session_id: str\\n    branch: str\\n    operations: List[ChunkOperation] = field(default_factory=list)\\n\\n    def to_dict(self) -> Dict[str, Any]:\\n        \\\"\\\"\\\"Convert to dictionary for JSON serialization.\\\"\\\"\\\"\\n        return {\\n            'version': self.version,\\n            'timestamp': self.timestamp,\\n            'session_id': self.session_id,\\n            'branch': self.branch,\\n            'operations': [op.to_dict() for op in self.operations]\\n        }\\n\\n    @classmethod\\n    def from_dict(cls, d: Dict[str, Any]) -> 'Chunk':\\n        \\\"\\\"\\\"Create from dictionary.\\\"\\\"\\\"\\n        return cls(\\n            version=d.get('version', 1),\\n            timestamp=d['timestamp'],\\n            session_id=d['session_id'],\\n            branch=d.get('branch', 'unknown'),\\n            operations=[ChunkOperation.from_dict(op) for op in d['operations']]\\n        )\\n\\n    def get_filename(self) -> str:\\n        \\\"\\\"\\\"Generate filename for this chunk.\\\"\\\"\\\"\\n        # Format: YYYY-MM-DD_HH-MM-SS_sessionid.json\\n        ts = self.timestamp.replace(':', '-').replace('T', '_')\\n        short_id = self.session_id[:8]\\n        return f\\\"{ts}_{short_id}.json\\\"\\n\\n\\nclass ChunkWriter:\\n    \\\"\\\"\\\"\\n    Writes indexing session changes to timestamped JSON chunks.\\n\\n    Usage:\\n        writer = ChunkWriter(chunks_dir='corpus_chunks')\\n        writer.add_document('doc1', 'content here', mtime=1234567890)\\n        writer.modify_document('doc2', 'new content', mtime=1234567891)\\n        writer.delete_document('doc3')\\n        chunk_path = writer.save()\\n    \\\"\\\"\\\"\\n\\n    def __init__(self, chunks_dir: str = 'corpus_chunks'):\\n        self.chunks_dir = Path(chunks_dir)\\n        self.session_id = uuid.uuid4().hex[:16]\\n        self.timestamp = datetime.now().isoformat(timespec='seconds')\\n        self.branch = self._get_git_branch()\\n        self.operations: List[ChunkOperation] = []\\n\\n    def _get_git_branch(self) -> str:\\n        \\\"\\\"\\\"Get current git branch name.\\\"\\\"\\\"\\n        try:\\n            result = subprocess.run(\\n                ['git', 'rev-parse', '--abbrev-ref', 'HEAD'],\\n                capture_output=True,\\n                text=True,\\n                timeout=5\\n            )\\n            if result.returncode == 0:\\n                return result.stdout.strip()\\n        except (subprocess.TimeoutExpired, FileNotFoundError):\\n            pass\\n        return 'unknown'\\n\\n    def add_document(self, doc_id: str, content: str, mtime: Optional[float] = None):\\n        \\\"\\\"\\\"Record an add operation.\\\"\\\"\\\"\\n        self.operations.append(ChunkOperation(\\n            op='add',\\n            doc_id=doc_id,\\n            content=content,\\n            mtime=mtime\\n        ))\\n\\n    def modify_document(self, doc_id: str, content: str, mtime: Optional[float] = None):\\n        \\\"\\\"\\\"Record a modify operation.\\\"\\\"\\\"\\n        self.operations.append(ChunkOperation(\\n            op='modify',\\n            doc_id=doc_id,\\n            content=content,\\n            mtime=mtime\\n        ))\\n\\n    def delete_document(self, doc_id: str):\\n        \\\"\\\"\\\"Record a delete operation.\\\"\\\"\\\"\\n        self.operations.append(ChunkOperation(\\n            op='delete',\\n            doc_id=doc_id\\n        ))\\n\\n    def has_operations(self) -> bool:\\n        \\\"\\\"\\\"Check if any operations were recorded.\\\"\\\"\\\"\\n        return len(self.operations) > 0\\n\\n    def save(self) -> Optional[Path]:\\n        \\\"\\\"\\\"\\n        Save chunk to file.\\n\\n        Returns:\\n            Path to saved chunk file, or None if no operations.\\n        \\\"\\\"\\\"\\n        if not self.operations:\\n            return None\\n\\n        # Create chunks directory if needed\\n        self.chunks_dir.mkdir(parents=True, exist_ok=True)\\n\\n        # Create chunk\\n        chunk = Chunk(\\n            version=CHUNK_VERSION,\\n            timestamp=self.timestamp,\\n            session_id=self.session_id,\\n            branch=self.branch,\\n            operations=self.operations\\n        )\\n\\n        # Write to file\\n        filepath = self.chunks_dir / chunk.get_filename()\\n        with open(filepath, 'w', encoding='utf-8') as f:\\n            json.dump(chunk.to_dict(), f, indent=2, ensure_ascii=False)\\n\\n        return filepath\\n\\n\\nclass ChunkLoader:\\n    \\\"\\\"\\\"\\n    Loads and combines chunks to rebuild document state.\\n\\n    Usage:\\n        loader = ChunkLoader(chunks_dir='corpus_chunks')\\n        documents = loader.load_all()  # Returns {doc_id: content}\\n\\n        # Check if cache is valid\\n        if loader.is_cache_valid('corpus_dev.pkl'):\\n            # Load from pkl\\n        else:\\n            # Rebuild from documents\\n    \\\"\\\"\\\"\\n\\n    def __init__(self, chunks_dir: str = 'corpus_chunks'):\\n        self.chunks_dir = Path(chunks_dir)\\n        self._chunks: List[Chunk] = []\\n        self._documents: Dict[str, str] = {}\\n        self._mtimes: Dict[str, float] = {}\\n        self._loaded = False\\n\\n    def get_chunk_files(self) -> List[Path]:\\n        \\\"\\\"\\\"Get all chunk files sorted by timestamp.\\\"\\\"\\\"\\n        if not self.chunks_dir.exists():\\n            return []\\n\\n        files = list(self.chunks_dir.glob('*.json'))\\n        # Sort by filename (which starts with timestamp)\\n        return sorted(files, key=lambda p: p.name)\\n\\n    def load_chunk(self, filepath: Path) -> Chunk:\\n        \\\"\\\"\\\"Load a single chunk file.\\\"\\\"\\\"\\n        with open(filepath, 'r', encoding='utf-8') as f:\\n            data = json.load(f)\\n        return Chunk.from_dict(data)\\n\\n    def load_all(self) -> Dict[str, str]:\\n        \\\"\\\"\\\"\\n        Load all chunks and replay operations to get current document state.\\n\\n        Returns:\\n            Dictionary mapping doc_id to content.\\n        \\\"\\\"\\\"\\n        if self._loaded:\\n            return self._documents\\n\\n        self._chunks = []\\n        self._documents = {}\\n        self._mtimes = {}\\n\\n        for filepath in self.get_chunk_files():\\n            chunk = self.load_chunk(filepath)\\n            self._chunks.append(chunk)\\n\\n            # Replay operations\\n            for op in chunk.operations:\\n                if op.op == 'add':\\n                    self._documents[op.doc_id] = op.content\\n                    if op.mtime:\\n                        self._mtimes[op.doc_id] = op.mtime\\n                elif op.op == 'modify':\\n                    self._documents[op.doc_id] = op.content\\n                    if op.mtime:\\n                        self._mtimes[op.doc_id] = op.mtime\\n                elif op.op == 'delete':\\n                    self._documents.pop(op.doc_id, None)\\n                    self._mtimes.pop(op.doc_id, None)\\n\\n        self._loaded = True\\n        return self._documents\\n\\n    def get_documents(self) -> Dict[str, str]:\\n        \\\"\\\"\\\"Get loaded documents (calls load_all if needed).\\\"\\\"\\\"\\n        if not self._loaded:\\n            self.load_all()\\n        return self._documents\\n\\n    def get_mtimes(self) -> Dict[str, float]:\\n        \\\"\\\"\\\"Get document modification times.\\\"\\\"\\\"\\n        if not self._loaded:\\n            self.load_all()\\n        return self._mtimes\\n\\n    def get_chunks(self) -> List[Chunk]:\\n        \\\"\\\"\\\"Get loaded chunks.\\\"\\\"\\\"\\n        if not self._loaded:\\n            self.load_all()\\n        return self._chunks\\n\\n    def compute_hash(self) -> str:\\n        \\\"\\\"\\\"\\n        Compute hash of current document state.\\n\\n        Used to check if pkl cache is still valid.\\n        \\\"\\\"\\\"\\n        if not self._loaded:\\n            self.load_all()\\n\\n        # Hash based on sorted (doc_id, content) pairs\\n        hasher = hashlib.sha256()\\n        for doc_id in sorted(self._documents.keys()):\\n            hasher.update(doc_id.encode('utf-8'))\\n            hasher.update(self._documents[doc_id].encode('utf-8'))\\n\\n        return hasher.hexdigest()[:16]\\n\\n    def is_cache_valid(self, cache_path: str, cache_hash_path: Optional[str] = None) -> bool:\\n        \\\"\\\"\\\"\\n        Check if pkl cache is valid for current chunk state.\\n\\n        Args:\\n            cache_path: Path to pkl cache file\\n            cache_hash_path: Path to hash file (defaults to cache_path + '.hash')\\n\\n        Returns:\\n            True if cache exists and hash matches\\n        \\\"\\\"\\\"\\n        cache_file = Path(cache_path)\\n        if not cache_file.exists():\\n            return False\\n\\n        hash_file = Path(cache_hash_path or f\\\"{cache_path}.hash\\\")\\n        if not hash_file.exists():\\n            return False\\n\\n        try:\\n            with open(hash_file, 'r') as f:\\n                stored_hash = f.read().strip()\\n\\n            current_hash = self.compute_hash()\\n            return stored_hash == current_hash\\n        except (IOError, OSError):\\n            return False\\n\\n    def save_cache_hash(self, cache_path: str, cache_hash_path: Optional[str] = None):\\n        \\\"\\\"\\\"Save current document hash for cache validation.\\\"\\\"\\\"\\n        hash_file = Path(cache_hash_path or f\\\"{cache_path}.hash\\\")\\n        current_hash = self.compute_hash()\\n\\n        with open(hash_file, 'w') as f:\\n            f.write(current_hash)\\n\\n    def get_stats(self) -> Dict[str, Any]:\\n        \\\"\\\"\\\"Get statistics about loaded chunks.\\\"\\\"\\\"\\n        if not self._loaded:\\n            self.load_all()\\n\\n        total_ops = sum(len(c.operations) for c in self._chunks)\\n        add_ops = sum(\\n            1 for c in self._chunks\\n            for op in c.operations if op.op == 'add'\\n        )\\n        modify_ops = sum(\\n            1 for c in self._chunks\\n            for op in c.operations if op.op == 'modify'\\n        )\\n        delete_ops = sum(\\n            1 for c in self._chunks\\n            for op in c.operations if op.op == 'delete'\\n        )\\n\\n        return {\\n            'chunk_count': len(self._chunks),\\n            'document_count': len(self._documents),\\n            'total_operations': total_ops,\\n            'add_operations': add_ops,\\n            'modify_operations': modify_ops,\\n            'delete_operations': delete_ops,\\n            'hash': self.compute_hash()\\n        }\\n\\n\\nclass ChunkCompactor:\\n    \\\"\\\"\\\"\\n    Compacts multiple chunk files into a single file.\\n\\n    Usage:\\n        compactor = ChunkCompactor(chunks_dir='corpus_chunks')\\n        compactor.compact(before='2025-12-01')  # Compact old chunks\\n        compactor.compact()  # Compact all chunks into one\\n    \\\"\\\"\\\"\\n\\n    def __init__(self, chunks_dir: str = 'corpus_chunks'):\\n        self.chunks_dir = Path(chunks_dir)\\n\\n    def compact(\\n        self,\\n        before: Optional[str] = None,\\n        keep_recent: int = 0,\\n        dry_run: bool = False\\n    ) -> Dict[str, Any]:\\n        \\\"\\\"\\\"\\n        Compact chunks into a single chunk.\\n\\n        Args:\\n            before: Only compact chunks before this date (YYYY-MM-DD)\\n            keep_recent: Keep this many recent chunks uncompacted\\n            dry_run: If True, don't actually compact, just report what would happen\\n\\n        Returns:\\n            Statistics about the compaction\\n        \\\"\\\"\\\"\\n        loader = ChunkLoader(str(self.chunks_dir))\\n        chunk_files = loader.get_chunk_files()\\n\\n        if not chunk_files:\\n            return {'status': 'no_chunks', 'compacted': 0}\\n\\n        # Filter chunks to compact\\n        to_compact = []\\n        to_keep = []\\n\\n        for filepath in chunk_files:\\n            filename = filepath.name\\n            # Extract date from filename (YYYY-MM-DD_HH-MM-SS_...)\\n            file_date = filename[:10]\\n\\n            should_compact = True\\n\\n            if before:\\n                should_compact = file_date < before\\n\\n            if should_compact:\\n                to_compact.append(filepath)\\n            else:\\n                to_keep.append(filepath)\\n\\n        # Keep recent chunks if requested\\n        if keep_recent > 0 and len(to_compact) > keep_recent:\\n            # Move some from to_compact to to_keep\\n            to_keep = to_compact[-keep_recent:] + to_keep\\n            to_compact = to_compact[:-keep_recent]\\n\\n        if not to_compact:\\n            return {'status': 'nothing_to_compact', 'compacted': 0}\\n\\n        if dry_run:\\n            return {\\n                'status': 'dry_run',\\n                'would_compact': len(to_compact),\\n                'would_keep': len(to_keep),\\n                'files_to_compact': [str(f) for f in to_compact]\\n            }\\n\\n        # Load and merge chunks to compact\\n        documents = {}\\n        mtimes = {}\\n\\n        for filepath in to_compact:\\n            chunk = loader.load_chunk(filepath)\\n            for op in chunk.operations:\\n                if op.op in ('add', 'modify'):\\n                    documents[op.doc_id] = op.content\\n                    if op.mtime:\\n                        mtimes[op.doc_id] = op.mtime\\n                elif op.op == 'delete':\\n                    documents.pop(op.doc_id, None)\\n                    mtimes.pop(op.doc_id, None)\\n\\n        # Create compacted chunk with all remaining documents as 'add' operations\\n        writer = ChunkWriter(str(self.chunks_dir))\\n        writer.timestamp = datetime.now().isoformat(timespec='seconds')\\n        writer.session_id = 'compacted_' + uuid.uuid4().hex[:8]\\n\\n        for doc_id, content in sorted(documents.items()):\\n            writer.add_document(doc_id, content, mtimes.get(doc_id))\\n\\n        # Save compacted chunk\\n        compacted_path = None\\n        if writer.has_operations():\\n            compacted_path = writer.save()\\n\\n        # Delete old chunk files\\n        for filepath in to_compact:\\n            filepath.unlink()\\n\\n        return {\\n            'status': 'compacted',\\n            'compacted': len(to_compact),\\n            'kept': len(to_keep),\\n            'documents': len(documents),\\n            'compacted_file': str(compacted_path) if compacted_path else None\\n        }\\n\\n\\ndef get_changes_from_manifest(\\n    current_files: Dict[str, float],\\n    manifest: Dict[str, float]\\n) -> Tuple[List[str], List[str], List[str]]:\\n    \\\"\\\"\\\"\\n    Compare current files to manifest to find changes.\\n\\n    Args:\\n        current_files: Dict mapping file paths to modification times\\n        manifest: Dict mapping file paths to last indexed modification times\\n\\n    Returns:\\n        Tuple of (added, modified, deleted) file lists\\n    \\\"\\\"\\\"\\n    current_set = set(current_files.keys())\\n    manifest_set = set(manifest.keys())\\n\\n    added = list(current_set - manifest_set)\\n    deleted = list(manifest_set - current_set)\\n\\n    # Check for modified files\\n    modified = []\\n    for filepath in current_set & manifest_set:\\n        if current_files[filepath] > manifest[filepath]:\\n            modified.append(filepath)\\n\\n    return added, modified, deleted\\n\",",
        "      \"mtime\": 1765407699.3613257",
        "    },",
        "    {",
        "      \"op\": \"add\",",
        "      \"doc_id\": \"TASK_LIST.md\",",
        "      \"content\": \"# Task List: Bug Fixes & RAG Enhancements\\n\\nThis document tracks bug fixes and feature enhancements for the Cortical Text Processor.\\n\\n**Last Updated:** 2025-12-09\\n**Status:** Bug fixes complete | RAG enhancements planned\\n\\n---\\n\\n## Critical Priority\\n\\n### 1. Fix Per-Document TF-IDF Calculation Bug\\n\\n**File:** `cortical/analysis.py`\\n**Line:** 131\\n**Status:** [x] Completed\\n\\n**Problem:**\\nThe per-document term frequency calculation was incorrect. The code always returned 1.\\n\\n**Solution Applied:**\\n1. Added `doc_occurrence_counts: Dict[str, int]` field to `Minicolumn` class\\n2. Updated `processor.py` to track per-document token occurrences during document processing\\n3. Fixed TF-IDF calculation to use actual occurrence counts: `col.doc_occurrence_counts.get(doc_id, 1)`\\n\\n**Files Modified:**\\n- `cortical/minicolumn.py` - Added new field and serialization support\\n- `cortical/processor.py` - Track occurrences per document\\n- `cortical/analysis.py` - Use actual counts in TF-IDF calculation\\n\\n---\\n\\n## High Priority\\n\\n### 2. Add ID-to-Minicolumn Lookup Optimization\\n\\n**Files:** `cortical/layers.py`, `cortical/analysis.py`, `cortical/query.py`\\n**Status:** [x] Completed\\n\\n**Problem:**\\nMultiple O(n) linear searches occurred when looking up minicolumns by ID.\\n\\n**Solution Applied:**\\n1. Added `_id_index: Dict[str, str]` secondary index to `HierarchicalLayer`\\n2. Added `get_by_id()` method for O(1) lookups\\n3. Updated `from_dict()` to rebuild index when loading\\n4. Replaced all linear searches with `get_by_id()` calls\\n\\n**Files Modified:**\\n- `cortical/layers.py` - Added `_id_index` and `get_by_id()` method\\n- `cortical/analysis.py` - Updated PageRank, activation propagation, label propagation\\n- `cortical/query.py` - Updated query expansion, spreading activation, related documents\\n\\n**Performance Impact:** Graph algorithms improved from O(n²) to O(n)\\n\\n---\\n\\n## Medium Priority\\n\\n### 3. Fix Type Annotation Errors\\n\\n**File:** `cortical/semantics.py`\\n**Lines:** 153, 248\\n**Status:** [x] Completed\\n\\n**Solution Applied:**\\n1. Added `Any` to imports\\n2. Changed `any` to `Any` on both lines\\n\\n---\\n\\n### 4. Remove Unused Import\\n\\n**File:** `cortical/analysis.py`\\n**Line:** 16\\n**Status:** [x] Completed\\n\\n**Solution Applied:**\\nRemoved `Counter` from the import statement.\\n\\n---\\n\\n### 5. Fix Unconditional Print in Export Function\\n\\n**File:** `cortical/persistence.py`\\n**Lines:** 175-176\\n**Status:** [x] Completed\\n\\n**Solution Applied:**\\n1. Added `verbose: bool = True` parameter to `export_graph_json()`\\n2. Wrapped print statements in `if verbose:` conditional\\n\\n---\\n\\n## Low Priority\\n\\n### 6. Add Missing Test Coverage\\n\\n**Files:** `tests/test_embeddings.py`, `tests/test_semantics.py`, `tests/test_gaps.py`, `tests/test_analysis.py`, `tests/test_persistence.py`\\n**Status:** [x] Completed\\n\\n**Tests Added:**\\n\\n**test_embeddings.py (15 tests):**\\n- `test_compute_graph_embeddings_adjacency`\\n- `test_compute_graph_embeddings_random_walk`\\n- `test_compute_graph_embeddings_spectral`\\n- `test_compute_graph_embeddings_invalid_method`\\n- `test_embedding_similarity`\\n- `test_embedding_similarity_self`\\n- `test_embedding_similarity_missing_term`\\n- `test_find_similar_by_embedding`\\n- `test_find_similar_by_embedding_missing_term`\\n- `test_embedding_dimensions`\\n- `test_embedding_normalization`\\n- `test_empty_layer_embeddings`\\n\\n**test_semantics.py (12 tests):**\\n- `test_extract_corpus_semantics`\\n- `test_extract_corpus_semantics_cooccurs`\\n- `test_retrofit_connections`\\n- `test_retrofit_connections_affects_weights`\\n- `test_retrofit_embeddings`\\n- `test_get_relation_type_weight`\\n- `test_relation_weights_constant`\\n- `test_empty_corpus_semantics`\\n- `test_retrofit_empty_relations`\\n- `test_larger_window_more_relations`\\n\\n**test_gaps.py (15 tests):**\\n- `test_analyze_knowledge_gaps_structure`\\n- `test_analyze_knowledge_gaps_summary`\\n- `test_analyze_knowledge_gaps_isolated_documents`\\n- `test_analyze_knowledge_gaps_weak_topics`\\n- `test_analyze_knowledge_gaps_coverage_score`\\n- `test_detect_anomalies_structure`\\n- `test_detect_anomalies_reasons`\\n- `test_detect_anomalies_sorted`\\n- `test_detect_anomalies_threshold`\\n- `test_empty_corpus_gaps`\\n- `test_single_document_gaps`\\n- `test_single_document_anomalies`\\n- `test_bridge_opportunities_format`\\n\\n**test_analysis.py (17 tests):**\\n- `test_pagerank_empty_layer`\\n- `test_pagerank_single_node`\\n- `test_pagerank_multiple_nodes`\\n- `test_pagerank_convergence`\\n- `test_tfidf_empty_corpus`\\n- `test_tfidf_single_document`\\n- `test_tfidf_multiple_documents`\\n- `test_tfidf_per_document`\\n- `test_propagation_empty_layers`\\n- `test_propagation_preserves_activation`\\n- `test_clustering_empty_layer`\\n- `test_clustering_returns_dict`\\n- `test_clustering_min_size`\\n- `test_build_concept_clusters`\\n- `test_document_connections`\\n- `test_cosine_similarity` (5 sub-tests)\\n- `test_get_by_id_returns_correct_minicolumn`\\n- `test_get_by_id_returns_none_for_missing`\\n\\n**test_persistence.py (12 tests):**\\n- `test_save_and_load`\\n- `test_save_load_preserves_id_index`\\n- `test_save_load_preserves_doc_occurrence_counts`\\n- `test_save_load_empty_processor`\\n- `test_export_graph_json`\\n- `test_export_graph_json_layer_filter`\\n- `test_export_graph_json_min_weight`\\n- `test_export_graph_json_max_nodes`\\n- `test_export_graph_json_verbose_false`\\n- `test_export_embeddings_json`\\n- `test_export_embeddings_json_with_metadata`\\n- `test_get_state_summary`\\n- `test_get_state_summary_empty`\\n\\n**Test Coverage Summary:**\\n- Previous: 39 tests\\n- Added: 70 new tests\\n- **Total: 109 tests (all passing)**\\n\\n---\\n\\n### 7. Document Magic Numbers\\n\\n**File:** `cortical/gaps.py`\\n**Lines:** 62, 76, 99\\n**Status:** [ ] Deferred\\n\\n**Note:** This task remains as a future enhancement. The magic numbers are functional but could benefit from documentation or configuration options.\\n\\n---\\n\\n---\\n\\n# RAG System Enhancements\\n\\nThe following tasks are required to transform the Cortical Text Processor into a production-ready RAG (Retrieval-Augmented Generation) system.\\n\\n---\\n\\n## RAG Critical Priority\\n\\n### 8. Implement Chunk-Level Retrieval\\n\\n**Files:** `cortical/processor.py`, `cortical/query.py`\\n**Status:** [x] Completed\\n\\n**Problem:**\\nCurrent retrieval returns only document IDs and scores. RAG systems need actual text passages with position information for context windows and citations.\\n\\n**Current Behavior:**\\n```python\\nresults = processor.find_documents_for_query(\\\"neural networks\\\")\\n# Returns: [(\\\"doc1\\\", 3.47), (\\\"doc2\\\", 2.15)]  # Just IDs!\\n```\\n\\n**Required Behavior:**\\n```python\\nresults = processor.find_passages_for_query(\\\"neural networks\\\")\\n# Returns: [\\n#   (\\\"Neural networks process information...\\\", \\\"doc1\\\", 1500, 2000, 3.47),\\n#   (text, doc_id, start_char, end_char, score)\\n# ]\\n```\\n\\n**Implementation Steps:**\\n1. Add `find_passages_for_query()` method to `processor.py`\\n2. Add `_create_chunks()` helper for splitting documents with overlap\\n3. Add `_score_tokens()` helper for chunk-level scoring\\n4. Add corresponding function to `query.py` for standalone use\\n5. Support configurable `chunk_size` (default 512) and `overlap` (default 128)\\n\\n**Files to Modify:**\\n- `cortical/processor.py` - Add new methods (~50 lines)\\n- `cortical/query.py` - Add standalone function (~40 lines)\\n- `tests/test_processor.py` - Add tests for chunk retrieval\\n- `tests/test_query.py` - Add tests for passage finding\\n\\n---\\n\\n### 9. Add Document Metadata Support\\n\\n**Files:** `cortical/processor.py`, `cortical/persistence.py`\\n**Status:** [x] Completed\\n\\n**Problem:**\\nNo way to store or retrieve document metadata (source URL, timestamp, author, etc.). RAG systems need this for proper citations and filtering.\\n\\n**Current Data Model:**\\n```python\\nself.documents: Dict[str, str] = {}  # Only doc_id → text\\n```\\n\\n**Required Data Model:**\\n```python\\nself.documents: Dict[str, str] = {}\\nself.document_metadata: Dict[str, Dict[str, Any]] = {}\\n# Stores: source, timestamp, author, category, custom fields\\n```\\n\\n**Implementation Steps:**\\n1. Add `document_metadata` dict to `CorticalTextProcessor.__init__()`\\n2. Modify `process_document()` to accept optional `metadata` parameter\\n3. Add `set_document_metadata()` and `get_document_metadata()` methods\\n4. Update `persistence.py` to save/load metadata\\n5. Increment state version to `2.1`\\n\\n**Files to Modify:**\\n- `cortical/processor.py` - Add metadata storage and methods\\n- `cortical/persistence.py` - Update save/load functions\\n- `tests/test_persistence.py` - Add metadata persistence tests\\n\\n---\\n\\n## RAG High Priority\\n\\n### 10. Activate Layer 2 (Concept Clustering) by Default\\n\\n**Files:** `cortical/processor.py`, `cortical/query.py`\\n**Status:** [x] Completed\\n\\n**Problem:**\\nLayer 2 (Concepts) has clustering code but is never populated automatically. This layer could enable topic-based filtering and hierarchical search.\\n\\n**Current Behavior:**\\n- `compute_all()` does NOT call `build_concept_clusters()`\\n- Layer 2 remains empty with 0 minicolumns\\n- Query expansion code checks Layer 2 but finds nothing\\n\\n**Implementation Steps:**\\n1. Add `build_concepts: bool = True` parameter to `compute_all()`\\n2. Call `build_concept_clusters()` when enabled\\n3. Update query expansion to use concepts when available\\n4. Add concept-based document filtering option\\n\\n**Files to Modify:**\\n- `cortical/processor.py` - Update `compute_all()` (~10 lines)\\n- `cortical/query.py` - Enhance expansion logic (~20 lines)\\n\\n---\\n\\n### 11. Integrate Semantic Relations into Retrieval\\n\\n**Files:** `cortical/query.py`\\n**Status:** [x] Completed\\n\\n**Problem:**\\n`semantics.py` extracts relations (IsA, PartOf, RelatedTo, etc.) but they're only used for retrofitting embeddings, not for query expansion or retrieval.\\n\\n**Current State:**\\n- `expand_query_semantic()` exists in `query.py` (lines 127-174)\\n- This function is NEVER called by `find_documents_for_query()`\\n- Semantic relations are computed but ignored during search\\n\\n**Implementation Steps:**\\n1. Add `use_semantic: bool = True` parameter to `find_documents_for_query()`\\n2. Call `expand_query_semantic()` when semantic relations exist\\n3. Combine lateral connection expansion with semantic expansion\\n4. Weight semantic expansions appropriately\\n\\n**Files to Modify:**\\n- `cortical/query.py` - Integrate semantic expansion (~15 lines)\\n\\n---\\n\\n### 12. Persist Full Computed State\\n\\n**Files:** `cortical/persistence.py`, `cortical/processor.py`\\n**Status:** [x] Completed\\n\\n**Problem:**\\nEmbeddings, semantic relations, and concept clusters are not saved. Loading a model requires expensive recomputation.\\n\\n**Currently Saved:**\\n- Layers (tokens, bigrams, documents)\\n- Document text\\n- Generic metadata\\n\\n**NOT Saved (must recompute):**\\n- `semantic_relations` - extracted IsA, PartOf, etc.\\n- `embeddings` - graph embeddings for all terms\\n- Concept clusters in Layer 2\\n\\n**Implementation Steps:**\\n1. Add `semantic_relations` to save state\\n2. Add `embeddings` to save state\\n3. Update `load_processor()` to restore these fields\\n4. Increment state version to `2.1`\\n5. Handle backward compatibility with v2.0 files\\n\\n**Files to Modify:**\\n- `cortical/persistence.py` - Update save/load (~30 lines)\\n- `cortical/processor.py` - Update save/load methods\\n\\n---\\n\\n## RAG Medium Priority\\n\\n### 13. Fix Remaining Type Annotation\\n\\n**File:** `cortical/embeddings.py`\\n**Line:** 26\\n**Status:** [x] Completed\\n\\n**Problem:**\\n```python\\n# Current (incorrect):\\n) -> Tuple[Dict[str, List[float]], Dict[str, any]]:\\n\\n# Should be:\\n) -> Tuple[Dict[str, List[float]], Dict[str, Any]]:\\n```\\n\\n**Implementation:** Single line fix, add `Any` to imports.\\n\\n---\\n\\n### 14. Optimize Spectral Embeddings Lookup\\n\\n**File:** `cortical/embeddings.py`\\n**Lines:** 151-156\\n**Status:** [x] Completed\\n\\n**Problem:**\\nSpectral embeddings use O(n) linear search instead of O(1) `get_by_id()`:\\n```python\\n# Current (slow):\\nfor t, c in layer.minicolumns.items():\\n    if c.id == neighbor_id:\\n        ...\\n\\n# Should use:\\nneighbor = layer.get_by_id(neighbor_id)\\n```\\n\\n---\\n\\n### 15. Add Incremental Document Indexing\\n\\n**File:** `cortical/processor.py`\\n**Status:** [x] Completed\\n\\n**Problem:**\\nAdding a document requires calling `compute_all()` which recomputes everything. For RAG systems with frequent updates, this is inefficient.\\n\\n**Solution Applied:**\\n1. Added staleness tracking with `_stale_computations` set and computation type constants\\n2. Added `add_document_incremental()` method with selectable recomputation levels:\\n   - `'none'`: Just add document, mark computations stale (fastest)\\n   - `'tfidf'`: Recompute TF-IDF only (good for search)\\n   - `'full'`: Run full `compute_all()` (most accurate)\\n3. Added `add_documents_batch()` for efficient batch additions with single recomputation\\n4. Added `recompute()` method with levels: `'stale'`, `'tfidf'`, `'full'`\\n5. Added helper methods: `is_stale()`, `get_stale_computations()`, `_mark_fresh()`, `_mark_all_stale()`\\n\\n**Files Modified:**\\n- `cortical/processor.py` - Added incremental indexing methods (~200 lines)\\n- `tests/test_processor.py` - Added 15 tests for incremental indexing\\n\\n**Usage Examples:**\\n```python\\n# Quick incremental update (TF-IDF only)\\nprocessor.add_document_incremental(\\\"new_doc\\\", \\\"content\\\", recompute='tfidf')\\n\\n# Batch add with deferred recomputation\\nprocessor.add_document_incremental(\\\"doc1\\\", \\\"content1\\\", recompute='none')\\nprocessor.add_document_incremental(\\\"doc2\\\", \\\"content2\\\", recompute='none')\\nprocessor.recompute(level='full')  # Single recomputation for batch\\n\\n# Efficient batch API\\ndocs = [(\\\"doc1\\\", \\\"content1\\\", {\\\"source\\\": \\\"web\\\"}), (\\\"doc2\\\", \\\"content2\\\", None)]\\nprocessor.add_documents_batch(docs, recompute='full')\\n```\\n\\n---\\n\\n## RAG Low Priority\\n\\n### 16. Document Magic Numbers in Gap Detection\\n\\n**File:** `cortical/gaps.py`\\n**Lines:** 62, 76, 99\\n**Status:** [x] Completed\\n\\n**Solution Applied:**\\nAdded documented constants at module level with detailed explanations:\\n- `ISOLATION_THRESHOLD = 0.02` - Documents below this avg cosine similarity are isolated\\n- `WELL_CONNECTED_THRESHOLD = 0.03` - Documents above this are well-integrated\\n- `WEAK_TOPIC_TFIDF_THRESHOLD = 0.005` - Terms above this TF-IDF are significant topics\\n- `BRIDGE_SIMILARITY_MIN = 0.005` and `BRIDGE_SIMILARITY_MAX = 0.03` - Range for bridging candidates\\n\\nEach constant includes documentation of typical ranges and usage guidance.\\n\\n---\\n\\n### 17. Add Multi-Stage Ranking Pipeline\\n\\n**Files:** `cortical/query.py`, `cortical/processor.py`\\n**Status:** [x] Completed\\n\\n**Problem:**\\nCurrent ranking is flat (Token TF-IDF → Document Score). Better RAG performance with staged ranking.\\n\\n**Solution Applied:**\\nImplemented a 4-stage ranking pipeline:\\n\\n1. **Stage 1 (Concepts):** Find relevant concepts from Layer 2 clusters, score by query term overlap\\n2. **Stage 2 (Documents):** Rank documents using combined concept + TF-IDF scores\\n3. **Stage 3 (Chunks):** Score passages within top documents using chunk-level TF-IDF\\n4. **Stage 4 (Rerank):** Combine all signals (chunk 50%, TF-IDF 30%, concept 20%) for final scoring\\n\\n**Files Modified:**\\n- `cortical/query.py` - Added `find_relevant_concepts()`, `multi_stage_rank()`, `multi_stage_rank_documents()` (~300 lines)\\n- `cortical/processor.py` - Added processor wrapper methods (~90 lines)\\n- `tests/test_processor.py` - Added 15 tests for multi-stage ranking\\n\\n**Usage Examples:**\\n```python\\n# Full 4-stage ranking (passages with stage breakdown)\\nresults = processor.multi_stage_rank(\\\"neural networks\\\", top_n=5, concept_boost=0.3)\\nfor passage, doc_id, start, end, score, stages in results:\\n    print(f\\\"[{doc_id}] Score: {score:.3f}\\\")\\n    print(f\\\"  Concept: {stages['concept_score']:.3f}\\\")\\n    print(f\\\"  Doc: {stages['doc_score']:.3f}\\\")\\n    print(f\\\"  Chunk: {stages['chunk_score']:.3f}\\\")\\n\\n# Document-level ranking (stages 1-2 only)\\nresults = processor.multi_stage_rank_documents(\\\"neural networks\\\", top_n=3)\\nfor doc_id, score, stages in results:\\n    print(f\\\"{doc_id}: {score:.3f} (concept: {stages['concept_score']:.3f})\\\")\\n```\\n\\n---\\n\\n### 18. Add Batch Query API\\n\\n**Files:** `cortical/query.py`, `cortical/processor.py`\\n**Status:** [x] Completed\\n\\n**Problem:**\\nNo efficient way to run multiple queries. Each query repeats tokenization and expansion.\\n\\n**Solution Applied:**\\n1. Added `find_documents_batch()` function to `query.py` with expansion caching\\n2. Added `find_passages_batch()` function to `query.py` with chunk pre-computation\\n3. Added corresponding methods to `CorticalTextProcessor`\\n4. Both functions share tokenization and expansion caches across queries\\n\\n**Files Modified:**\\n- `cortical/query.py` - Added batch query functions (~180 lines)\\n- `cortical/processor.py` - Added processor wrapper methods (~90 lines)\\n- `tests/test_processor.py` - Added 14 tests for batch query functionality\\n\\n**Usage Examples:**\\n```python\\n# Batch document search\\nqueries = [\\\"neural networks\\\", \\\"machine learning\\\", \\\"data processing\\\"]\\nresults = processor.find_documents_batch(queries, top_n=3)\\nfor query, docs in zip(queries, results):\\n    print(f\\\"{query}: {[doc_id for doc_id, _ in docs]}\\\")\\n\\n# Batch passage search (for RAG)\\nresults = processor.find_passages_batch(queries, top_n=5, chunk_size=512)\\nfor query, passages in zip(queries, results):\\n    print(f\\\"{query}: {len(passages)} passages found\\\")\\n```\\n\\n---\\n\\n---\\n\\n# ConceptNet-Enhanced PageRank\\n\\nThe following tasks implement a ConceptNet-like enhanced PageRank algorithm that leverages semantic relations, cross-layer connections, and typed edge weights for improved concept importance scoring.\\n\\n---\\n\\n## ConceptNet Critical Priority\\n\\n### 19. Build Cross-Layer Feedforward Connections\\n\\n**Files:** `cortical/analysis.py`, `cortical/processor.py`, `cortical/minicolumn.py`\\n**Status:** [x] Completed\\n\\n**Problem:**\\nLayers were isolated - concepts didn't connect back to their member tokens, and bigrams didn't link to component unigrams. This broke the hierarchical flow needed for cross-layer PageRank.\\n\\n**Solution Applied:**\\n1. Added `feedforward_connections: Dict[str, float]` to Minicolumn (weighted links to lower layer)\\n2. Added `feedback_connections: Dict[str, float]` to Minicolumn (weighted links to higher layer)\\n3. Added helper methods: `add_feedforward_connection()`, `add_feedback_connection()`\\n4. Updated bigram creation to link to component tokens with weight 1.0 per occurrence\\n5. Updated document processing to create bidirectional doc↔token connections\\n6. Updated concept creation to link to member tokens weighted by normalized PageRank\\n7. Updated `to_dict()`/`from_dict()` for persistence\\n\\n**Files Modified:**\\n- `cortical/minicolumn.py` - Added connection fields and helper methods (~50 lines)\\n- `cortical/processor.py` - Populate feedforward/feedback during document processing\\n- `cortical/analysis.py` - Updated `build_concept_clusters()` to create weighted links\\n- `tests/test_processor.py` - Added 12 tests for cross-layer connections\\n\\n**Connection Types:**\\n```python\\n# Bigram → Tokens (weight by occurrence count)\\nbigram.feedforward_connections[\\\"L0_neural\\\"] = 2.0  # seen twice\\n\\n# Token → Bigrams (feedback)\\ntoken.feedback_connections[\\\"L1_neural_networks\\\"] = 2.0\\n\\n# Document → Tokens (weight by term frequency)\\ndoc.feedforward_connections[\\\"L0_neural\\\"] = 3.0  # appears 3 times\\n\\n# Concept → Tokens (weight by normalized PageRank)\\nconcept.feedforward_connections[\\\"L0_neural\\\"] = 1.0  # highest PR\\nconcept.feedforward_connections[\\\"L0_networks\\\"] = 0.7  # lower PR\\n```\\n\\n---\\n\\n### 20. Add Concept-Level Lateral Connections\\n\\n**Files:** `cortical/analysis.py`, `cortical/processor.py`\\n**Status:** [x] Completed\\n\\n**Problem:**\\nLayer 2 (Concepts) had 0 lateral connections. Concept clusters should connect to each other based on shared documents and semantic overlap.\\n\\n**Solution Applied:**\\n1. Added `compute_concept_connections()` function to `analysis.py`\\n2. Connects concepts by Jaccard similarity of document sets\\n3. Optionally boosts weights using semantic relations between member tokens\\n4. Relation type weighting: IsA (1.5) > PartOf (1.3) > HasProperty (1.2) > RelatedTo (1.0)\\n5. Called from `compute_all()` after `build_concept_clusters()`\\n6. Added `compute_concept_connections()` method to processor with parameters\\n\\n**Files Modified:**\\n- `cortical/analysis.py` - Added `compute_concept_connections()` (~110 lines)\\n- `cortical/processor.py` - Added processor wrapper method, integrated into `compute_all()`\\n- `tests/test_processor.py` - Added 8 tests for concept connections\\n\\n**Usage:**\\n```python\\n# Automatic in compute_all()\\nprocessor.compute_all()  # Calls compute_concept_connections() automatically\\n\\n# Manual with options\\nstats = processor.compute_concept_connections(\\n    use_semantics=True,    # Boost weights with semantic relations\\n    min_shared_docs=1,     # Minimum shared documents\\n    min_jaccard=0.1        # Minimum Jaccard similarity\\n)\\n```\\n\\n---\\n\\n### 21. Add Bigram Lateral Connections\\n\\n**Files:** `cortical/analysis.py`, `cortical/processor.py`\\n**Status:** [x] Completed\\n\\n**Problem:**\\nLayer 1 (Bigrams) has 0 lateral connections. Bigrams should connect when they:\\n- Share a component term (\\\"neural_networks\\\" ↔ \\\"neural_processing\\\")\\n- Co-occur in the same documents\\n- Form chains (\\\"machine_learning\\\" ↔ \\\"learning_algorithms\\\")\\n\\n**Solution Applied:**\\n1. Added `compute_bigram_connections()` function to `analysis.py` (~140 lines)\\n2. Connects bigrams sharing left component (e.g., \\\"neural_networks\\\" ↔ \\\"neural_processing\\\")\\n3. Connects bigrams sharing right component (e.g., \\\"deep_learning\\\" ↔ \\\"machine_learning\\\")\\n4. Connects chain bigrams where right of one = left of other (\\\"machine_learning\\\" ↔ \\\"learning_algorithms\\\")\\n5. Adds document co-occurrence connections weighted by Jaccard similarity\\n6. Added configurable weights: `component_weight=0.5`, `chain_weight=0.7`, `cooccurrence_weight=0.3`\\n7. Added `compute_bigram_connections()` method to processor with full docstring\\n8. Integrated into `compute_all()` pipeline\\n9. Added `COMP_BIGRAM_CONNECTIONS` staleness tracking constant\\n10. Updated `recompute()` method to handle bigram connections\\n\\n**Files Modified:**\\n- `cortical/analysis.py` - Added `compute_bigram_connections()` (~140 lines)\\n- `cortical/processor.py` - Added wrapper method and integrated into `compute_all()`\\n- `tests/test_processor.py` - Added 11 tests for bigram connections\\n\\n**Usage:**\\n```python\\n# Automatic in compute_all()\\nprocessor.compute_all()  # Calls compute_bigram_connections() automatically\\n\\n# Manual with options\\nstats = processor.compute_bigram_connections(\\n    component_weight=0.5,  # Weight for shared component connections\\n    chain_weight=0.7,      # Weight for chain connections\\n    cooccurrence_weight=0.3,  # Weight for document co-occurrence\\n    verbose=True\\n)\\nprint(f\\\"Created {stats['connections_created']} bigram connections\\\")\\nprint(f\\\"  Component: {stats['component_connections']}\\\")\\nprint(f\\\"  Chain: {stats['chain_connections']}\\\")\\nprint(f\\\"  Co-occurrence: {stats['cooccurrence_connections']}\\\")\\n```\\n\\n---\\n\\n## ConceptNet High Priority\\n\\n### 22. Implement Relation-Weighted PageRank\\n\\n**Files:** `cortical/analysis.py`, `cortical/processor.py`\\n**Status:** [x] Completed\\n\\n**Problem:**\\nCurrent PageRank treats all `lateral_connections` equally. ConceptNet-style PageRank should weight edges by semantic relation type.\\n\\n**Solution Applied:**\\n1. Added `RELATION_WEIGHTS` constant to `analysis.py` with default weights:\\n   - IsA: 1.5, PartOf: 1.3, HasProperty: 1.2, SimilarTo: 1.4, RelatedTo: 1.0\\n   - Causes: 1.1, UsedFor: 1.0, CoOccurs: 0.8, Antonym: 0.3, DerivedFrom: 1.2\\n2. Created `compute_semantic_pagerank()` function (~120 lines):\\n   - Builds semantic relation lookup from (term1, term2) pairs\\n   - Applies relation-type multipliers to edge weights\\n   - Returns stats: pagerank scores, iterations_run, edges_with_relations\\n3. Added `compute_semantic_importance()` method to processor:\\n   - Falls back to standard PageRank if no semantic relations\\n   - Applies semantic PageRank to both token and bigram layers\\n   - Returns comprehensive statistics\\n4. Updated `compute_all()` with `pagerank_method` parameter:\\n   - 'standard': Traditional PageRank (default)\\n   - 'semantic': ConceptNet-style with relation weighting\\n   - Automatically extracts semantic relations if needed\\n\\n**Files Modified:**\\n- `cortical/analysis.py` - Added `RELATION_WEIGHTS` and `compute_semantic_pagerank()` (~130 lines)\\n- `cortical/processor.py` - Added `compute_semantic_importance()`, updated `compute_all()`\\n- `tests/test_processor.py` - Added 9 tests for semantic PageRank\\n\\n**Usage:**\\n```python\\n# Use semantic PageRank via compute_all\\nprocessor.compute_all(pagerank_method='semantic')\\n\\n# Or call directly\\nprocessor.extract_corpus_semantics()\\nstats = processor.compute_semantic_importance()\\nprint(f\\\"Found {stats['total_edges_with_relations']} semantic edges\\\")\\n\\n# Custom relation weights\\ncustom_weights = {'IsA': 2.0, 'CoOccurs': 0.5}\\nprocessor.compute_semantic_importance(relation_weights=custom_weights)\\n```\\n\\n---\\n\\n### 23. Implement Cross-Layer PageRank Propagation\\n\\n**Files:** `cortical/analysis.py`, `cortical/processor.py`\\n**Status:** [x] Completed\\n\\n**Problem:**\\nPageRank only flows within a single layer. Importance should propagate across layers:\\n- Important tokens boost their bigrams\\n- Important bigrams boost their concepts\\n- Important concepts boost their documents (and vice versa)\\n\\n**Solution Applied:**\\n1. Added `compute_hierarchical_pagerank()` function to `analysis.py` (~150 lines):\\n   - Computes local PageRank within each layer\\n   - Propagates scores up via feedback_connections (tokens → bigrams → concepts → documents)\\n   - Propagates scores down via feedforward_connections (documents → concepts → bigrams → tokens)\\n   - Normalizes PageRank within each layer after propagation\\n   - Converges when cross-layer changes are minimal\\n2. Added `compute_hierarchical_importance()` method to processor\\n3. Updated `compute_all()` with `pagerank_method='hierarchical'` option\\n4. Returns detailed statistics: iterations_run, converged, per-layer stats\\n\\n**Files Modified:**\\n- `cortical/analysis.py` - Added `compute_hierarchical_pagerank()` (~150 lines)\\n- `cortical/processor.py` - Added `compute_hierarchical_importance()`, updated `compute_all()`\\n- `tests/test_processor.py` - Added 9 tests for hierarchical PageRank\\n\\n**Usage:**\\n```python\\n# Use hierarchical PageRank via compute_all\\nprocessor.compute_all(pagerank_method='hierarchical')\\n\\n# Or call directly with custom parameters\\nstats = processor.compute_hierarchical_importance(\\n    layer_iterations=10,      # Iterations for intra-layer PageRank\\n    global_iterations=5,      # Iterations for cross-layer propagation\\n    cross_layer_damping=0.7   # Damping at layer boundaries\\n)\\nprint(f\\\"Converged: {stats['converged']} in {stats['iterations_run']} iterations\\\")\\nfor layer, info in stats['layer_stats'].items():\\n    print(f\\\"  {layer}: {info['nodes']} nodes, max PR={info['max_pagerank']:.4f}\\\")\\n```\\n\\n---\\n\\n### 24. Add Typed Edge Storage\\n\\n**Files:** `cortical/minicolumn.py`, `cortical/__init__.py`\\n**Status:** [x] Completed\\n\\n**Problem:**\\n`lateral_connections` only stores `{target_id: weight}`. ConceptNet-style graphs need edge metadata: relation type, confidence, source.\\n\\n**Solution Applied:**\\n1. Created `Edge` dataclass in `minicolumn.py` with:\\n   - `target_id`: Target minicolumn ID\\n   - `weight`: Connection strength (accumulates)\\n   - `relation_type`: Semantic type ('co_occurrence', 'IsA', 'PartOf', etc.)\\n   - `confidence`: Confidence score (0.0 to 1.0)\\n   - `source`: Origin ('corpus', 'semantic', 'inferred')\\n2. Added `typed_connections: Dict[str, Edge]` field to Minicolumn\\n3. Implemented `add_typed_connection()` with intelligent merging:\\n   - Weights accumulate\\n   - Specific relation types override 'co_occurrence'\\n   - Higher confidence is kept\\n   - Source priority: inferred > semantic > corpus\\n4. Added query methods:\\n   - `get_typed_connection(target_id)` - Get single edge\\n   - `get_connections_by_type(relation_type)` - Filter by relation\\n   - `get_connections_by_source(source)` - Filter by source\\n5. Updated `to_dict()` and `from_dict()` for persistence\\n6. Exported `Edge` class from package\\n\\n**Files Modified:**\\n- `cortical/minicolumn.py` - Added Edge dataclass and typed_connections\\n- `cortical/__init__.py` - Export Edge class\\n- `tests/test_layers.py` - Added 15 tests for Edge and typed connections\\n\\n**Usage:**\\n```python\\nfrom cortical import Minicolumn, Edge\\n\\ncol = Minicolumn(\\\"L0_test\\\", \\\"test\\\", 0)\\n\\n# Add typed connections\\ncol.add_typed_connection(\\\"L0_network\\\", 0.8, relation_type='RelatedTo')\\ncol.add_typed_connection(\\\"L0_brain\\\", 0.5, relation_type='IsA', source='semantic')\\n\\n# Query by type\\nis_a_edges = col.get_connections_by_type('IsA')\\nsemantic_edges = col.get_connections_by_source('semantic')\\n\\n# Get single edge\\nedge = col.get_typed_connection(\\\"L0_network\\\")\\nprint(f\\\"{edge.relation_type}: {edge.weight} ({edge.confidence})\\\")\\n```\\n\\n---\\n\\n## ConceptNet Medium Priority\\n\\n### 25. Implement Multi-Hop Semantic Inference\\n\\n**Files:** `cortical/query.py`, `cortical/processor.py`\\n**Status:** [x] Completed\\n\\n**Problem:**\\nQuery expansion only follows single-hop connections. ConceptNet enables multi-hop inference:\\n- \\\"dog\\\" → IsA → \\\"animal\\\" → HasProperty → \\\"living\\\"\\n- \\\"car\\\" → PartOf → \\\"engine\\\" → UsedFor → \\\"transportation\\\"\\n\\n**Solution Applied:**\\n1. Added `VALID_RELATION_CHAINS` constant to `query.py` defining valid relation chain patterns with validity scores:\\n   - Transitive hierarchies: IsA→IsA (1.0), PartOf→PartOf (1.0), IsA→HasProperty (0.9)\\n   - Association chains: RelatedTo→RelatedTo (0.6), SimilarTo→SimilarTo (0.7)\\n   - Causal chains: Causes→Causes (0.8), Causes→HasProperty (0.7)\\n   - Invalid chains: Antonym→IsA (0.1) - contradictory\\n2. Added `score_relation_path()` function to compute path validity scores\\n3. Added `expand_query_multihop()` function (~90 lines) implementing:\\n   - BFS-style expansion with hop tracking\\n   - Weight decay by hop distance: `weight *= decay_factor ** hop`\\n   - Path validity filtering with `min_path_score` threshold\\n   - Configurable parameters: `max_hops`, `max_expansions`, `decay_factor`, `min_path_score`\\n4. Added `expand_query_multihop()` method to processor with fallback to regular expansion\\n\\n**Files Modified:**\\n- `cortical/query.py` - Added `VALID_RELATION_CHAINS`, `score_relation_path()`, `expand_query_multihop()` (~150 lines)\\n- `cortical/processor.py` - Added processor wrapper method (~45 lines)\\n- `tests/test_processor.py` - Added 18 tests for multi-hop inference and path scoring\\n\\n**Usage:**\\n```python\\n# Extract semantic relations first\\nprocessor.extract_corpus_semantics()\\n\\n# Multi-hop expansion (finds 2-hop away terms)\\nexpanded = processor.expand_query_multihop(\\\"neural\\\", max_hops=2)\\n# Hop 1: networks (co-occur), learning (co-occur), brain (RelatedTo)\\n# Hop 2: deep (via learning), cortex (via brain), AI (via networks)\\n\\n# Custom parameters\\nexpanded = processor.expand_query_multihop(\\n    \\\"neural\\\",\\n    max_hops=3,           # Follow up to 3 hops\\n    decay_factor=0.6,     # Slower weight decay\\n    min_path_score=0.3,   # Filter low-validity paths\\n    max_expansions=20     # More expansion terms\\n)\\n```\\n\\n---\\n\\n### 26. Add Relation Path Scoring\\n\\n**Files:** `cortical/query.py`\\n**Status:** [x] Completed (implemented with Task 25)\\n\\n**Problem:**\\nNot all relation paths are equally valid for inference. Need to score paths by semantic coherence.\\n\\n**Valid Paths:**\\n- IsA → IsA (transitive hypernymy): \\\"poodle\\\" → \\\"dog\\\" → \\\"animal\\\" ✓\\n- PartOf → HasA (part inheritance): \\\"wheel\\\" → \\\"car\\\" → \\\"engine\\\" ✓\\n- RelatedTo → RelatedTo (association): loose but acceptable\\n\\n**Invalid Paths:**\\n- Antonym → IsA: contradictory\\n- Random oscillation: low confidence\\n\\n**Solution Applied (with Task 25):**\\n1. Added `VALID_RELATION_CHAINS` dict defining allowed transitions with validity scores:\\n   - Transitive: (IsA, IsA)=1.0, (PartOf, PartOf)=1.0\\n   - Property inheritance: (IsA, HasProperty)=0.9, (PartOf, HasProperty)=0.8\\n   - Association: (RelatedTo, RelatedTo)=0.6, (SimilarTo, SimilarTo)=0.7\\n   - Invalid: (Antonym, IsA)=0.1\\n2. Added `score_relation_path()` function that multiplies consecutive pair validities\\n3. Default validity score of 0.4 for unknown relation pairs\\n4. Integrated into `expand_query_multihop()` with `min_path_score` parameter\\n\\n**Files Modified:**\\n- `cortical/query.py` - Added constants and scoring function (~50 lines)\\n- `tests/test_processor.py` - Added 7 tests in `TestMultiHopPathScoring` class\\n\\n**Usage:**\\n```python\\nfrom cortical.query import score_relation_path, VALID_RELATION_CHAINS\\n\\n# Score a relation path\\nscore = score_relation_path(['IsA', 'IsA'])  # 1.0 (transitive)\\nscore = score_relation_path(['IsA', 'HasProperty'])  # 0.9 (property inheritance)\\nscore = score_relation_path(['Antonym', 'IsA'])  # 0.1 (contradictory)\\n\\n# Check valid chain patterns\\nprint(VALID_RELATION_CHAINS[('IsA', 'IsA')])  # 1.0\\n```\\n\\n---\\n\\n### 27. Implement Concept Inheritance\\n\\n**Files:** `cortical/semantics.py`, `cortical/processor.py`\\n**Status:** [x] Completed\\n\\n**Problem:**\\nIsA relations should enable property inheritance. If \\\"dog IsA animal\\\" and \\\"animal HasProperty living\\\", then \\\"dog\\\" should inherit \\\"living\\\".\\n\\n**Solution Applied:**\\n1. Added `build_isa_hierarchy()` function to extract parent-child relationships from IsA relations\\n2. Added `get_ancestors()` and `get_descendants()` functions for hierarchy traversal with depth tracking\\n3. Added `inherit_properties()` function that:\\n   - Extracts direct properties from HasProperty, HasA, CapableOf, AtLocation, UsedFor relations\\n   - Propagates properties down IsA chains with configurable decay factor\\n   - Returns mapping of term → {property: (weight, source_ancestor, depth)}\\n4. Added `compute_property_similarity()` for weighted Jaccard similarity based on shared properties\\n5. Added `apply_inheritance_to_connections()` to boost lateral connections for shared inherited properties\\n6. Added processor wrapper methods: `compute_property_inheritance()` and `compute_property_similarity()`\\n\\n**Files Modified:**\\n- `cortical/semantics.py` - Added 6 new functions (~280 lines)\\n- `cortical/processor.py` - Added 2 processor wrapper methods (~80 lines)\\n- `tests/test_semantics.py` - Added 23 tests across 7 new test classes\\n\\n**Usage:**\\n```python\\n# Compute property inheritance\\nprocessor.extract_corpus_semantics()\\nstats = processor.compute_property_inheritance(\\n    decay_factor=0.7,      # Weight decay per level\\n    max_depth=5,           # Maximum inheritance depth\\n    apply_to_connections=True,  # Boost lateral connections\\n    boost_factor=0.3       # Boost weight for shared properties\\n)\\n\\n# Check inherited properties for a term\\ninherited = stats['inherited']\\nif 'dog' in inherited:\\n    for prop, (weight, source, depth) in inherited['dog'].items():\\n        print(f\\\"  {prop}: {weight:.2f} (from {source}, depth {depth})\\\")\\n\\n# Compute similarity based on shared properties\\nsim = processor.compute_property_similarity(\\\"dog\\\", \\\"cat\\\")\\n```\\n\\n---\\n\\n## ConceptNet Low Priority\\n\\n### 28. Add Commonsense Relation Extraction\\n\\n**Files:** `cortical/semantics.py`, `cortical/processor.py`\\n**Status:** [x] Completed\\n\\n**Problem:**\\nCurrent relation extraction is limited to co-occurrence patterns. Could extract richer relations:\\n- \\\"X is a type of Y\\\" → IsA\\n- \\\"X contains Y\\\" → HasA\\n- \\\"X is used for Y\\\" → UsedFor\\n- \\\"X causes Y\\\" → Causes\\n\\n**Solution Applied:**\\n1. Added `RELATION_PATTERNS` constant with 30+ regex patterns covering:\\n   - IsA patterns: \\\"X is a type of Y\\\", \\\"X is a kind of Y\\\", \\\"X belongs to Y\\\"\\n   - HasA patterns: \\\"X has Y\\\", \\\"X contains Y\\\", \\\"X consists of Y\\\"\\n   - PartOf patterns: \\\"X is part of Y\\\", \\\"X is a component of Y\\\"\\n   - UsedFor patterns: \\\"X is used for Y\\\", \\\"X helps Y\\\", \\\"X enables Y\\\"\\n   - Causes patterns: \\\"X causes Y\\\", \\\"X leads to Y\\\", \\\"X produces Y\\\"\\n   - CapableOf patterns: \\\"X can Y\\\", \\\"X is able to Y\\\"\\n   - AtLocation patterns: \\\"X is found in Y\\\", \\\"X lives in Y\\\"\\n   - HasProperty patterns: \\\"X is Y\\\" (with context)\\n   - Antonym patterns: \\\"X is opposite of Y\\\"\\n   - DerivedFrom patterns: \\\"X comes from Y\\\"\\n   - DefinedBy patterns: \\\"X means Y\\\"\\n2. Added `extract_pattern_relations()` function with filtering for:\\n   - Invalid terms (not in corpus)\\n   - Stopwords\\n   - Self-relations\\n   - Duplicate relations\\n3. Added `get_pattern_statistics()` for relation type analysis\\n4. Updated `extract_corpus_semantics()` with `use_pattern_extraction` parameter\\n5. Added processor method `extract_pattern_relations()` for direct access\\n\\n**Files Modified:**\\n- `cortical/semantics.py` - Added `RELATION_PATTERNS`, `extract_pattern_relations()`, `get_pattern_statistics()` (~180 lines)\\n- `cortical/processor.py` - Updated `extract_corpus_semantics()`, added `extract_pattern_relations()` (~70 lines)\\n- `tests/test_semantics.py` - Added 16 tests for pattern extraction\\n\\n**Usage:**\\n```python\\n# Automatic pattern extraction during semantic extraction\\nprocessor.extract_corpus_semantics(\\n    use_pattern_extraction=True,    # Enabled by default\\n    min_pattern_confidence=0.6      # Minimum confidence threshold\\n)\\n\\n# Direct pattern extraction\\nrelations = processor.extract_pattern_relations(min_confidence=0.5)\\nfor t1, rel_type, t2, confidence in relations:\\n    print(f\\\"{t1} --{rel_type}--> {t2} ({confidence:.2f})\\\")\\n```\\n\\n---\\n\\n### 29. Visualize ConceptNet-Style Graph\\n\\n**Files:** `cortical/persistence.py`, `cortical/processor.py`\\n**Status:** [x] Completed\\n\\n**Problem:**\\nCurrent `export_graph_json()` doesn't distinguish edge types or layers. Need ConceptNet-style visualization export.\\n\\n**Solution Applied:**\\n1. Added `LAYER_COLORS` constant with color codes for each layer:\\n   - Tokens: Royal Blue (#4169E1)\\n   - Bigrams: Forest Green (#228B22)\\n   - Concepts: Dark Orange (#FF8C00)\\n   - Documents: Crimson (#DC143C)\\n2. Added `LAYER_NAMES` constant for display names\\n3. Added `export_conceptnet_json()` function (~200 lines) with:\\n   - Color-coded nodes by layer with layer_name\\n   - Typed edges with relation_type, confidence, and source_type\\n   - Cross-layer edges (feedforward/feedback)\\n   - Relation-based edge colors\\n   - D3.js/Cytoscape/Gephi-compatible format\\n4. Added `_get_relation_color()` helper with 16 relation type colors\\n5. Added `_count_edge_types()` and `_count_relation_types()` helpers\\n6. Added processor wrapper method `export_conceptnet_json()`\\n\\n**Files Modified:**\\n- `cortical/persistence.py` - Added constants and export function (~270 lines)\\n- `cortical/processor.py` - Added processor wrapper method (~50 lines)\\n- `tests/test_persistence.py` - Added 13 tests for ConceptNet export\\n\\n**Usage:**\\n```python\\n# Export ConceptNet-style graph\\nprocessor.extract_corpus_semantics(verbose=False)\\ngraph = processor.export_conceptnet_json(\\n    \\\"graph.json\\\",\\n    include_cross_layer=True,     # Include feedforward/feedback edges\\n    include_typed_edges=True,     # Include typed_connections\\n    min_weight=0.0,               # Minimum edge weight\\n    max_nodes_per_layer=100       # Limit nodes per layer\\n)\\n\\n# Open graph.json in D3.js, Cytoscape.js, or Gephi for visualization\\n```\\n\\n---\\n\\n### 30. Add Analogy Completion\\n\\n**Files:** `cortical/query.py`, `cortical/processor.py`\\n**Status:** [x] Completed\\n\\n**Problem:**\\nConceptNet enables analogy completion: \\\"king is to queen as man is to ?\\\" → \\\"woman\\\"\\nThis requires relation-aware vector arithmetic.\\n\\n**Solution Applied:**\\n1. Added `find_relation_between()` helper function to find semantic relations between two terms\\n2. Added `find_terms_with_relation()` helper to find terms connected by a specific relation type\\n3. Added `complete_analogy()` function (~120 lines) with three strategies:\\n   - **Relation matching**: Find a→b relation, apply to c\\n   - **Vector arithmetic**: Use embeddings for a - b + c ≈ d\\n   - **Pattern matching**: Use co-occurrence patterns as fallback\\n4. Added `complete_analogy_simple()` lightweight version using only co-occurrence patterns\\n5. Added processor wrapper methods: `complete_analogy()` and `complete_analogy_simple()`\\n\\n**Files Modified:**\\n- `cortical/query.py` - Added helper functions and analogy completion (~180 lines)\\n- `cortical/processor.py` - Added processor wrapper methods (~60 lines)\\n- `tests/test_processor.py` - Added 14 tests for analogy completion\\n\\n**Usage:**\\n```python\\n# Full analogy completion with multiple strategies\\nresults = processor.complete_analogy(\\\"king\\\", \\\"queen\\\", \\\"man\\\", top_n=5)\\nfor term, score, method in results:\\n    print(f\\\"  {term}: {score:.3f} ({method})\\\")\\n\\n# Simple version (co-occurrence only)\\nresults = processor.complete_analogy_simple(\\\"neural\\\", \\\"networks\\\", \\\"knowledge\\\")\\nfor term, score in results:\\n    print(f\\\"  {term}: {score:.3f}\\\")\\n\\n# Control which strategies to use\\nresults = processor.complete_analogy(\\n    \\\"neural\\\", \\\"networks\\\", \\\"knowledge\\\",\\n    use_embeddings=True,   # Enable vector arithmetic\\n    use_relations=True     # Enable relation matching\\n)\\n```\\n\\n---\\n\\n## Actionable Tasks (2025-12-10)\\n\\nThe following tasks were identified during comprehensive code review and are prioritized for implementation:\\n\\n---\\n\\n### 47. Dog-Food the System During Development\\n\\n**Files:** New `scripts/index_codebase.py`, `scripts/search_codebase.py`, `.claude/skills/`\\n**Status:** [x] Completed (2025-12-10)\\n**Priority:** High\\n\\n**Goal:**\\nUse the Cortical Text Processor to index and search its own codebase during development.\\n\\n**Solution Applied:**\\n1. Created `scripts/index_codebase.py`:\\n   - Indexes all 19 Python files in `cortical/` and `tests/`\\n   - Indexes 4 documentation files (CLAUDE.md, TASK_LIST.md, README.md, KNOWLEDGE_TRANSFER.md)\\n   - Saves indexed corpus to `corpus_dev.pkl` (23 documents, ~15,600 lines)\\n   - Computes semantic PageRank, TF-IDF, concepts, and semantic relations\\n\\n2. Created `scripts/search_codebase.py`:\\n   - Loads indexed corpus and performs semantic search\\n   - Returns file:line references for each result\\n   - Supports `--top N`, `--verbose`, `--expand`, `--interactive` options\\n   - Interactive mode with `/expand`, `/concepts`, `/stats`, `/quit` commands\\n\\n3. Created Claude Skills in `.claude/skills/`:\\n   - `codebase-search/SKILL.md` - Search skill for finding code patterns\\n   - `corpus-indexer/SKILL.md` - Indexing skill for updating corpus\\n\\n4. Updated `CLAUDE.md` with Dog-Fooding section documenting usage\\n\\n**Example Usage:**\\n```bash\\npython scripts/index_codebase.py\\npython scripts/search_codebase.py \\\"PageRank algorithm\\\" --top 3\\npython scripts/search_codebase.py \\\"bigram separator\\\" --expand\\npython scripts/search_codebase.py --interactive\\n```\\n\\n**Success Criteria:** All met\\n- Can find relevant code when searching for concepts\\n- Passages include accurate file:line references (e.g., `cortical/analysis.py:127`)\\n- System handles its own codebase without errors\\n- Identified usability issue: return value order in find_passages_for_query (fixed)\\n\\n---\\n\\n### 37. Create Dedicated Query Module Tests\\n\\n**File:** `tests/test_query.py` (new file)\\n**Status:** [x] Completed (2025-12-10)\\n**Priority:** High\\n\\n**Problem:**\\n`cortical/query.py` (1,503 lines, 20+ functions) has NO dedicated test file. Functions are tested only indirectly through `test_processor.py`.\\n\\n**Functions Needing Coverage:**\\n- `expand_query_multihop()` - Multi-hop inference with relation chains\\n- `score_relation_path()` - Relation path validation\\n- `get_expanded_query_terms()` - Helper for all expansion methods\\n- `find_relevant_concepts()` - Concept filtering for RAG\\n- `find_relation_between()` and `find_terms_with_relation()` - Relation discovery\\n- Chunking and batch operations\\n\\n**Deliverable:** Create `tests/test_query.py` with 30+ unit tests.\\n\\n**Solution Applied:**\\nCreated `tests/test_query.py` with 48 comprehensive tests covering:\\n- `TestScoreRelationPath` (4 tests) - Relation path validation\\n- `TestCreateChunks` (4 tests) - Text chunking\\n- `TestFindRelationBetween` (4 tests) - Relation discovery\\n- `TestFindTermsWithRelation` (4 tests) - Term relation lookup\\n- `TestExpandQuery` (4 tests) - Basic query expansion\\n- `TestExpandQueryMultihop` (4 tests) - Multi-hop expansion\\n- `TestGetExpandedQueryTerms` (3 tests) - Unified expansion helper\\n- `TestFindDocumentsForQuery` (4 tests) - Document retrieval\\n- `TestFindDocumentsBatch` (3 tests) - Batch document retrieval\\n- `TestFindPassagesForQuery` (2 tests) - Passage retrieval\\n- `TestFindRelevantConcepts` (2 tests) - Concept filtering\\n- `TestCompleteAnalogy` (3 tests) - Analogy completion\\n- `TestQueryWithSpreadingActivation` (2 tests) - Activation search\\n- `TestScoreChunk` (3 tests) - Chunk scoring\\n- `TestEdgeCases` (2 tests) - Edge case handling\\n\\nTest count increased from 340 to 388.\\n\\n---\\n\\n### 38. Add Input Validation to Public API\\n\\n**Files:** `cortical/processor.py`\\n**Status:** [x] Completed (2025-12-10)\\n**Priority:** High\\n\\n**Problem:**\\nPublic API methods silently accept invalid inputs, leading to confusing behavior.\\n\\n**Solution Applied:**\\nAdded input validation to 4 key public API methods:\\n\\n1. **`process_document()`** - Validates doc_id (non-empty string) and content (non-empty string)\\n2. **`find_documents_for_query()`** - Validates query_text (non-empty string) and top_n (positive int)\\n3. **`complete_analogy()`** - Validates all 3 terms (non-empty strings) and top_n (positive int)\\n4. **`add_documents_batch()`** - Validates documents list format, doc_id/content types, and recompute level\\n\\nAll methods now raise `ValueError` with descriptive messages for invalid input.\\n\\n**Tests Added:** 20 new tests in `TestInputValidation` class covering:\\n- Empty/None/non-string doc_id\\n- Empty/whitespace-only/non-string content\\n- Empty/whitespace-only query_text\\n- Invalid top_n values (0, negative)\\n- Invalid document batch formats\\n- Valid input acceptance\\n\\nTest count increased from 388 to 408.\\n\\n---\\n\\n### 39. Move Inline Imports to Module Top\\n\\n**Files:** `cortical/processor.py:161`, `cortical/semantics.py:493`\\n**Status:** [x] Completed (2025-12-10)\\n**Priority:** Low\\n\\n**Problem:**\\n`import copy` statements inside methods pollute namespaces and impact readability.\\n\\n**Solution Applied:**\\nMoved `import copy` to module-level imports in both files.\\n\\n---\\n\\n### 40. Add Parameter Range Validation\\n\\n**Files:** Multiple\\n**Status:** [x] Completed (2025-12-10)\\n**Priority:** Medium\\n\\n**Problem:**\\nNo validation for invalid parameter ranges.\\n\\n**Solution Applied:**\\nAdded validation to key functions:\\n- `compute_pagerank()`: damping must be in range (0, 1)\\n- `compute_semantic_pagerank()`: damping must be in range (0, 1)\\n- `compute_hierarchical_pagerank()`: damping and cross_layer_damping must be in range (0, 1)\\n- `retrofit_connections()`: alpha must be in range [0, 1]\\n- `retrofit_embeddings()`: alpha must be in range (0, 1]\\n- `create_chunks()`: chunk_size > 0, overlap >= 0, overlap < chunk_size\\n\\nAdded 9 new tests for parameter validation.\\n\\n---\\n\\n### 41. Create Configuration Dataclass\\n\\n**Files:** New `cortical/config.py`\\n**Status:** [ ] Not Started\\n**Priority:** Medium\\n\\n**Problem:**\\nMagic numbers scattered across modules with no central configuration:\\n- `gaps.py`: ISOLATION_THRESHOLD=0.02, WELL_CONNECTED_THRESHOLD=0.03\\n- `query.py`: VALID_RELATION_CHAINS (15 entries)\\n- `analysis.py`: damping=0.85, iterations=20, tolerance=1e-6\\n\\n**Solution:**\\n```python\\n@dataclass\\nclass CorticalConfig:\\n    # PageRank\\n    pagerank_damping: float = 0.85\\n    pagerank_iterations: int = 20\\n    pagerank_tolerance: float = 1e-6\\n\\n    # Clustering\\n    min_cluster_size: int = 3\\n    cluster_strictness: float = 1.0\\n\\n    # Gap detection\\n    isolation_threshold: float = 0.02\\n    well_connected_threshold: float = 0.03\\n```\\n\\n---\\n\\n### 42. Add Simple Query Language Support\\n\\n**File:** `cortical/query.py`\\n**Status:** [ ] Not Started\\n**Priority:** Low\\n\\n**Problem:**\\nOnly natural language queries supported. No structured filtering.\\n\\n**Solution:** Add minimal syntax:\\n- `\\\"term1 AND term2\\\"` - require both terms\\n- `\\\"term1 OR term2\\\"` - either term\\n- `\\\"-term1\\\"` - exclude term\\n- `\\\"term1\\\"` (quoted) - exact match\\n\\n---\\n\\n### 43. Optimize Chunk Scoring Performance\\n\\n**File:** `cortical/query.py:590-630`\\n**Status:** [x] Completed (2025-12-10)\\n**Priority:** Medium\\n\\n**Problem:**\\n`score_chunk()` tokenizes chunk text every call with no caching.\\n\\n**Solution Applied:**\\n1. Added `precompute_term_cols()` to cache minicolumn lookups for query terms\\n2. Added `score_chunk_fast()` for optimized scoring with pre-computed lookups\\n3. Updated `find_passages_for_query()` to use fast scoring\\n4. Updated `find_passages_batch()` to use fast scoring\\n\\nAdded 4 new tests for optimization functions.\\n\\n---\\n\\n### 44. Remove Deprecated feedforward_sources\\n\\n**Files:** `cortical/minicolumn.py:117`, `analysis.py:457`, `query.py:105`\\n**Status:** [ ] Not Started\\n**Priority:** Low\\n\\n**Problem:**\\n`feedforward_sources` is marked deprecated but still used in 4+ locations.\\n\\n**Solution:** Migrate all usages to `feedforward_connections` and remove deprecated attribute.\\n\\n---\\n\\n### 45. Add LRU Cache for Query Results\\n\\n**File:** `cortical/processor.py`\\n**Status:** [x] Completed (2025-12-10)\\n**Priority:** Medium\\n\\n**Problem:**\\nEvery query re-expands terms and rescores documents. Repeated queries (common in RAG loops) are slow.\\n\\n**Solution Applied:**\\n1. Added `_query_expansion_cache` dict and `_query_cache_max_size` to processor\\n2. Added `expand_query_cached()` method with cache lookup and LRU-style eviction\\n3. Added `clear_query_cache()` to manually invalidate cache\\n4. Added `set_query_cache_size()` to configure cache size\\n5. Auto-invalidate cache on `compute_all()` since corpus state changes\\n\\nAdded 8 new tests for cache functionality.\\n\\n---\\n\\n### 46. Standardize Return Types with Dataclasses\\n\\n**File:** `cortical/query.py`\\n**Status:** [ ] Not Started\\n**Priority:** Low\\n\\n**Problem:**\\nInconsistent return types across query functions:\\n- `find_documents_for_query()` → `List[Tuple[str, float]]`\\n- `find_passages_for_query()` → `List[Tuple[str, str, int, int, float]]`\\n- `complete_analogy()` → `List[Tuple[str, float, str]]`\\n\\n**Solution:**\\n```python\\n@dataclass\\nclass DocumentMatch:\\n    doc_id: str\\n    score: float\\n\\n@dataclass\\nclass PassageMatch:\\n    doc_id: str\\n    text: str\\n    start: int\\n    end: int\\n    score: float\\n```\\n\\n---\\n\\n## Summary\\n\\n| Priority | Task | Status | Category |\\n|----------|------|--------|----------|\\n| Critical | Fix TF-IDF per-doc calculation | ✅ Completed | Bug Fix |\\n| High | Add ID lookup optimization | ✅ Completed | Bug Fix |\\n| Medium | Fix type annotations (semantics.py) | ✅ Completed | Bug Fix |\\n| Medium | Remove unused import | ✅ Completed | Bug Fix |\\n| Medium | Add verbose parameter | ✅ Completed | Bug Fix |\\n| Low | Add test coverage | ✅ Completed | Bug Fix |\\n| **Critical** | **Implement chunk-level retrieval** | ✅ Completed | **RAG** |\\n| **Critical** | **Add document metadata support** | ✅ Completed | **RAG** |\\n| **High** | **Activate Layer 2 concepts** | ✅ Completed | **RAG** |\\n| **High** | **Integrate semantic relations** | ✅ Completed | **RAG** |\\n| **High** | **Persist full computed state** | ✅ Completed | **RAG** |\\n| Medium | Fix type annotation (embeddings.py) | ✅ Completed | Bug Fix |\\n| Medium | Optimize spectral embeddings | ✅ Completed | Performance |\\n| Medium | Add incremental indexing | ✅ Completed | RAG |\\n| Low | Document magic numbers | ✅ Completed | Documentation |\\n| Low | Multi-stage ranking pipeline | ✅ Completed | RAG |\\n| Low | Batch query API | ✅ Completed | RAG |\\n| **Critical** | **Build cross-layer feedforward connections** | ✅ Completed | **ConceptNet** |\\n| **Critical** | **Add concept-level lateral connections** | ✅ Completed | **ConceptNet** |\\n| **Critical** | **Add bigram lateral connections** | ✅ Completed | **ConceptNet** |\\n| **High** | **Implement relation-weighted PageRank** | ✅ Completed | **ConceptNet** |\\n| **High** | **Implement cross-layer PageRank propagation** | ✅ Completed | **ConceptNet** |\\n| **High** | **Add typed edge storage** | ✅ Completed | **ConceptNet** |\\n| Medium | Implement multi-hop semantic inference | ✅ Completed | ConceptNet |\\n| Medium | Add relation path scoring | ✅ Completed | ConceptNet |\\n| Medium | Implement concept inheritance | ✅ Completed | ConceptNet |\\n| Low | Add commonsense relation extraction | ✅ Completed | ConceptNet |\\n| Low | Visualize ConceptNet-style graph | ✅ Completed | ConceptNet |\\n| Low | Add analogy completion | ✅ Completed | ConceptNet |\\n\\n**Bug Fix Completion:** 8/8 tasks (100%)\\n**RAG Enhancement Completion:** 8/8 tasks (100%)\\n**ConceptNet Enhancement Completion:** 12/12 tasks (100%)\\n\\n---\\n\\n## Test Results\\n\\n```\\nRan 408 tests in 0.336s\\nOK\\n```\\n\\nAll tests passing as of 2025-12-10.\\n\\n---\\n\\n## Layer 2 Connection Improvements (2025-12-10)\\n\\n### Problem Statement\\n\\nLayer 2 (Concept Layer/V4) shows 0 connections when documents cover diverse topics because:\\n- Label propagation creates topic-specific clusters\\n- Concepts inherit only their members' documents\\n- Connection filter requires shared documents (Jaccard ≥ 0.1)\\n- No document overlap → no connections\\n\\n### Task L2-1: Add Configurable Connection Thresholds ✅ COMPLETED\\n**File:** `cortical/analysis.py` (lines 614-812)\\n\\n- [x] Add `min_shared_docs=0` option to allow connections without document overlap\\n- [x] Add `min_jaccard=0.0` option to disable Jaccard filtering\\n- [x] Expose these parameters in `CorticalTextProcessor.compute_concept_connections()`\\n- [x] Update docstrings to explain threshold behavior\\n- [x] Add tests for edge cases (zero thresholds, negative values)\\n\\n### Task L2-2: Connect Concepts via Semantic Relations ✅ COMPLETED\\n**File:** `cortical/analysis.py`\\n\\n- [x] Add new connection method that links concepts when their member tokens have semantic relations\\n- [x] For each concept pair, check if any (token1, relation, token2) exists in semantic_relations\\n- [x] Weight connections by number of semantic links between members\\n- [x] Make this work independently of document overlap\\n- [x] Add `use_member_semantics=True` parameter to `compute_concept_connections()`\\n- [x] Add tests verifying semantic-based connections\\n\\n### Task L2-3: Connect Concepts via Shared Vocabulary/Embeddings ✅ COMPLETED\\n**File:** `cortical/analysis.py`\\n\\n- [x] Add connection method based on embedding similarity between concept centroids\\n- [x] Compute concept centroid as average of member token embeddings\\n- [x] Connect concepts with cosine similarity above threshold\\n- [x] Add `use_embedding_similarity=True` and `embedding_threshold=0.3` parameters\\n- [x] Falls back gracefully if embeddings not computed\\n- [x] Add tests for embedding-based connections\\n\\n### Task L2-4: Improve Clustering to Reduce Topic Isolation ✅ COMPLETED\\n**File:** `cortical/analysis.py` (lines 482-616)\\n\\n- [x] Add `cluster_strictness` parameter to label propagation (0.0-1.0)\\n- [x] Lower strictness = more cross-topic token mixing in clusters\\n- [x] Add `bridge_weight` parameter for inter-document token bridging\\n- [x] Add tests for different strictness levels and bridging\\n\\n### Task L2-5: Integration and API Updates ✅ COMPLETED\\n**File:** `cortical/processor.py`\\n\\n- [x] Update `compute_all()` to accept connection strategy parameters\\n- [x] Add `connection_strategy` enum: 'document_overlap', 'semantic', 'embedding', 'hybrid'\\n- [x] 'hybrid' combines all three methods with configurable weights\\n- [x] Add documentation in CLAUDE.md\\n- [x] Add 6 new tests for compute_all strategies\\n\\n**Success Criteria:** ✅ ALL MET\\n- Layer 2 shows meaningful connections even with diverse document topics\\n- User can choose connection strategy based on their use case\\n- All existing tests continue to pass (337 tests)\\n- New tests cover the added functionality (17 new tests added)\\n\\n---\\n\\n## Code Quality Improvements (2025-12-10)\\n\\n### Query Expansion Helper Refactoring\\n\\n**File:** `cortical/query.py`\\n**Status:** [x] Completed\\n\\n**Problem:**\\nQuery expansion logic (expand + semantic merge) was duplicated in 6 functions:\\n- `find_documents_for_query()`\\n- `find_passages_for_query()`\\n- `find_documents_batch()`\\n- `find_passages_batch()`\\n- `multi_stage_rank()`\\n- `multi_stage_rank_documents()`\\n\\n**Solution Applied:**\\nAdded `get_expanded_query_terms()` helper function (~60 lines) that consolidates:\\n- Lateral connection expansion via `expand_query()`\\n- Semantic relation expansion via `expand_query_semantic()`\\n- Merging of expansion results with appropriate weighting\\n- Configurable parameters: `max_expansions`, `semantic_discount`\\n\\nAll 6 functions now use this helper, reducing code duplication by ~100 lines.\\n\\n---\\n\\n*Updated from code review on 2025-12-10*\\n\\n---\\n\\n## Critical Bug Fixes (2025-12-10)\\n\\nThe following critical bugs were identified during code review and must be fixed:\\n\\n### 34. Fix Bigram Separator Mismatch in Analogy Completion\\n\\n**File:** `cortical/query.py`\\n**Lines:** 1442-1468\\n**Status:** [x] Completed (2025-12-10)\\n**Priority:** Critical\\n\\n**Problem:**\\nThe `complete_analogy_simple()` function uses underscore separators for bigram lookup and parsing, but bigrams are stored with **space** separators (defined in `tokenizer.py:179`).\\n\\n**Affected Code:**\\n```python\\n# Line 1442-1443: WRONG - uses underscore\\nab_bigram = f\\\"{term_a}_{term_b}\\\"  # Creates \\\"neural_networks\\\"\\nba_bigram = f\\\"{term_b}_{term_a}\\\"\\n\\n# Line 1452: WRONG - splits by underscore\\nparts = bigram.split('_')\\n\\n# But bigrams are stored with spaces (tokenizer.py:179):\\n# ' '.join(tokens[i:i+n])  # Creates \\\"neural networks\\\"\\n```\\n\\n**Impact:**\\n- The bigram pattern matching strategy in `complete_analogy_simple()` is completely non-functional\\n- `ab_col` and `ba_col` will always be `None` because \\\"neural_networks\\\" doesn't exist in the corpus\\n- The `parts` split will never produce valid component extraction\\n\\n**Solution:**\\n```python\\n# Line 1442-1443: Should use space\\nab_bigram = f\\\"{term_a} {term_b}\\\"  # Correct: \\\"neural networks\\\"\\nba_bigram = f\\\"{term_b} {term_a}\\\"\\n\\n# Line 1452: Should split by space\\nparts = bigram.split(' ')\\n```\\n\\n**Files to Modify:**\\n- `cortical/query.py` - Fix separator in lines 1442, 1443, 1452\\n- `tests/test_processor.py` - Add tests for bigram-based analogy completion\\n\\n---\\n\\n### 35. Fix Bigram Separator Mismatch in Bigram Connections\\n\\n**File:** `cortical/analysis.py`\\n**Line:** 927\\n**Status:** [x] Completed (2025-12-10)\\n**Priority:** Critical\\n\\n**Problem:**\\nThe `compute_bigram_connections()` function splits bigram content by underscore, but bigrams are stored with **space** separators.\\n\\n**Affected Code:**\\n```python\\n# Line 927: WRONG - splits by underscore\\nfor bigram in bigrams:\\n    parts = bigram.content.split('_')\\n    if len(parts) == 2:\\n        left_index[parts[0]].append(bigram)\\n        right_index[parts[1]].append(bigram)\\n\\n# But bigrams are stored with spaces:\\n# \\\"neural networks\\\" not \\\"neural_networks\\\"\\n```\\n\\n**Impact:**\\n- `left_index` and `right_index` dictionaries are never populated\\n- Component-sharing connections (e.g., \\\"neural networks\\\" ↔ \\\"neural processing\\\") are never created\\n- Chain connections (e.g., \\\"machine learning\\\" ↔ \\\"learning algorithms\\\") are never created\\n- Only document co-occurrence connections work correctly\\n\\n**Solution:**\\n```python\\n# Line 927: Should split by space\\nparts = bigram.content.split(' ')\\n```\\n\\n**Verification:**\\nAfter fixing, the `compute_bigram_connections()` stats should show non-zero values for:\\n- `component_connections`\\n- `chain_connections`\\n\\nCurrently these are always 0 due to the bug.\\n\\n**Files to Modify:**\\n- `cortical/analysis.py` - Fix separator in line 927\\n- `tests/test_analysis.py` - Add tests verifying component/chain connections work\\n\\n---\\n\\n---\\n\\n## New Task Summary (2025-12-10)\\n\\n| # | Priority | Task | Status | Category |\\n|---|----------|------|--------|----------|\\n| 34 | **Critical** | Fix bigram separator in analogy completion | ✅ Completed | Bug Fix |\\n| 35 | **Critical** | Fix bigram separator in bigram connections | ✅ Completed | Bug Fix |\\n| 47 | **High** | Dog-food the system during development | ✅ Completed | Validation |\\n| 37 | **High** | Create dedicated query module tests | ✅ Completed | Testing |\\n| 38 | **High** | Add input validation to public API | ✅ Completed | Code Quality |\\n| 39 | Low | Move inline imports to module top | ✅ Completed | Code Quality |\\n| 40 | Medium | Add parameter range validation | ✅ Completed | Code Quality |\\n| 41 | Medium | Create configuration dataclass | [ ] Not Started | Architecture |\\n| 42 | Low | Add simple query language support | [ ] Not Started | Feature |\\n| 43 | Medium | Optimize chunk scoring performance | ✅ Completed | Performance |\\n| 44 | Low | Remove deprecated feedforward_sources | [ ] Not Started | Cleanup |\\n| 45 | Medium | Add LRU cache for query results | ✅ Completed | Performance |\\n| 46 | Low | Standardize return types with dataclasses | [ ] Not Started | API |\\n\\n**Completed:** 9/13 tasks\\n**High Priority Remaining:** 0 tasks\\n**Medium Priority Remaining:** 1 task (#41)\\n**Low Priority Remaining:** 3 tasks (#42, #44, #46)\\n\\n**Total Tests:** 593 (all passing)\\n\\n---\\n\\n### 57. Add Incremental Codebase Indexing\\n\\n**Files:** `scripts/index_codebase.py`, `cortical/processor.py`, `cortical/layers.py`, `tests/test_incremental_indexing.py`\\n**Status:** [x] Completed (2025-12-10)\\n**Priority:** High\\n\\n**Problem:**\\nThe codebase indexer had to rebuild the entire corpus on every run, even for small changes. This was slow and inefficient for iterative development.\\n\\n**Solution Applied:**\\n1. Added manifest file (`corpus_dev.manifest.json`) to track file modification times\\n2. Added `--incremental` flag to only re-index changed files\\n3. Added `--status` flag to show what would change without indexing\\n4. Added `--force` flag to force full rebuild\\n5. Added `remove_document()` and `remove_documents_batch()` methods to processor\\n6. Added `remove_minicolumn()` method to HierarchicalLayer\\n7. Added robust progress tracking with `ProgressTracker` class\\n8. Added phase timing and logging support (`--log FILE`)\\n9. Added timeout support (`--timeout N`)\\n10. Added fast mode (default) that skips slow bigram connections\\n\\n**Performance Fix:**\\nIdentified that `compute_bigram_connections()` has O(n²) complexity with large corpora (26,000+ bigrams), causing hangs. Fast mode skips this operation:\\n- Before: >10 minutes (hung)\\n- After: ~2.3 seconds\\n\\n**Files Modified:**\\n- `scripts/index_codebase.py` - Complete rewrite with incremental support (~840 lines)\\n- `cortical/processor.py` - Added `remove_document()`, `remove_documents_batch()` (~160 lines)\\n- `cortical/layers.py` - Added `remove_minicolumn()` (~20 lines)\\n- `tests/test_incremental_indexing.py` - 47 comprehensive tests\\n- `.claude/skills/corpus-indexer/SKILL.md` - Updated documentation\\n- `CLAUDE.md` - Updated Dog-Fooding section\\n\\n**Usage:**\\n```bash\\n# Fast incremental update (~1-2s)\\npython scripts/index_codebase.py --incremental\\n\\n# Check what would change\\npython scripts/index_codebase.py --status\\n\\n# Full rebuild with logging\\npython scripts/index_codebase.py --force --log index.log\\n\\n# With timeout safeguard\\npython scripts/index_codebase.py --timeout 60\\n```\\n\\n---\\n\\n## Intent-Based Code Search Enhancements\\n\\nThe following tasks enhance the system's ability to understand developer intent and retrieve code by meaning rather than exact keyword matching.\\n\\n---\\n\\n### 48. Add Code-Aware Tokenization\\n\\n**Files:** `cortical/tokenizer.py`, `tests/test_tokenizer.py`\\n**Status:** [x] Completed\\n**Priority:** High\\n\\n**Problem:**\\nCurrent tokenizer treats code like prose. It doesn't understand that `getUserCredentials`, `get_user_credentials`, and `fetch user credentials` are semantically equivalent.\\n\\n**Solution Applied:**\\n1. Added `split_identifier()` function to break camelCase, PascalCase, underscore_style, and CONSTANT_STYLE\\n2. Added `PROGRAMMING_KEYWORDS` constant for common code terms (function, class, def, get, set, etc.)\\n3. Added `split_identifiers` parameter to `Tokenizer.__init__()` and `tokenize()` method\\n4. Tokens include both original identifier and split components when enabled\\n5. Split parts don't duplicate already-seen tokens, preserving proper bigram extraction\\n\\n**Example:**\\n```python\\ntokenizer = Tokenizer(split_identifiers=True)\\ntokens = tokenizer.tokenize(\\\"getUserCredentials\\\")\\n# ['getusercredentials', 'get', 'user', 'credentials']\\n```\\n\\n**Tests Added:**\\n- 8 tests for `split_identifier()` function (camelCase, PascalCase, underscore_style, acronyms)\\n- 8 tests for code-aware tokenization (splitting, stop word filtering, min length, deduplication)\\n\\n---\\n\\n### 49. Add Synonym/Concept Mapping for Code Patterns\\n\\n**Files:** `cortical/code_concepts.py`, `cortical/query.py`, `cortical/processor.py`\\n**Status:** [x] Completed\\n**Priority:** High\\n\\n**Problem:**\\nThe system doesn't know that \\\"fetch\\\", \\\"get\\\", \\\"retrieve\\\", \\\"load\\\" are often interchangeable in code contexts, or that \\\"auth\\\", \\\"authentication\\\", \\\"credentials\\\", \\\"login\\\" form a concept cluster.\\n\\n**Solution Applied:**\\n1. Created `cortical/code_concepts.py` with 16 programming concept groups\\n2. Added `expand_code_concepts()` function for query expansion\\n3. Integrated with `expand_query()` via `use_code_concepts` parameter\\n4. Added `expand_query_for_code()` convenience method to processor\\n5. Added 33 tests in `tests/test_code_concepts.py`\\n\\n**Concept Groups Implemented:**\\n- retrieval, storage, deletion, auth, error, validation\\n- transform, network, database, async, config, logging\\n- testing, file, iteration, lifecycle, events\\n\\n---\\n\\n### 50. Add Intent-Based Query Understanding\\n\\n**Files:** `cortical/query.py`, `cortical/processor.py`, `tests/test_intent_query.py`\\n**Status:** [x] Completed\\n**Priority:** High\\n\\n**Problem:**\\nNatural language queries like \\\"where do we handle authentication?\\\" aren't decomposed into searchable intents.\\n\\n**Solution Applied:**\\n1. Added `parse_intent_query()` to extract action + subject + intent + expanded terms\\n2. Added `ParsedIntent` TypedDict for structured results\\n3. Added `QUESTION_INTENTS` mapping (where→location, how→implementation, what→definition, why→rationale, when→lifecycle)\\n4. Added `ACTION_VERBS` frozenset with 50+ common programming verbs\\n5. Added `search_by_intent()` for intent-aware document search\\n6. Added processor wrapper methods\\n7. Added 24 tests in `tests/test_intent_query.py`\\n\\n**Example:**\\n```python\\nparse_intent_query(\\\"where do we handle authentication?\\\")\\n# Returns: {\\n#   'action': 'handle',\\n#   'subject': 'authentication',\\n#   'intent': 'location',\\n#   'question_word': 'where',\\n#   'expanded_terms': ['handle', 'authentication', 'auth', 'login', ...]\\n# }\\n```\\n\\n---\\n\\n### 51. Add Fingerprint Export API\\n\\n**Files:** `cortical/fingerprint.py`, `cortical/processor.py`, `tests/test_fingerprint.py`\\n**Status:** [x] Completed\\n**Priority:** Medium\\n\\n**Problem:**\\nNo way to export or compare the semantic representation of code blocks.\\n\\n**Solution Applied:**\\n1. Created `cortical/fingerprint.py` with `SemanticFingerprint` TypedDict\\n2. Added `compute_fingerprint()` returning terms, concepts, bigrams, top_terms\\n3. Added `compare_fingerprints()` for cosine similarity scoring\\n4. Added `explain_fingerprint()` showing top contributing terms and concepts\\n5. Added `explain_similarity()` for human-readable explanations\\n6. Added processor methods: `get_fingerprint()`, `compare_fingerprints()`, `explain_fingerprint()`, `explain_similarity()`, `find_similar_texts()`\\n7. Added 24 tests in `tests/test_fingerprint.py`\\n\\n**Use Cases:**\\n- Compare similarity between functions\\n- Find duplicate/similar code blocks\\n- Explain why two code blocks are related\\n\\n---\\n\\n### 52. Optimize Query-to-Corpus Comparison\\n\\n**Files:** `cortical/query.py`, `cortical/processor.py`, `scripts/search_codebase.py`\\n**Status:** [x] Completed\\n**Priority:** Medium\\n\\n**Problem:**\\nEach query recomputes expansions and scores against all documents. For interactive use, this should be faster.\\n\\n**Solution Applied:**\\n1. Added `fast_find_documents()` using candidate pre-filtering\\n2. Added `build_document_index()` for pre-computed inverted index\\n3. Added `search_with_index()` for fastest cached search\\n4. Added processor wrappers: `fast_find_documents()`, `build_search_index()`, `search_with_index()`\\n5. Added `--fast` flag to search_codebase.py script\\n6. Added 20 tests in `tests/test_query_optimization.py`\\n\\n**Performance:**\\n- `fast_find_documents()`: ~2-3x faster than full search\\n- `search_with_index()`: Fastest when index is cached\\n\\n---\\n\\n## Intelligence Documentation\\n\\nThe following tasks create self-describing documentation that improves the system's ability to understand itself when indexed. This creates a feedback loop: better documentation → better semantic search → better AI understanding of the codebase.\\n\\n---\\n\\n### 53. Create Algorithm Intelligence Documentation\\n\\n**File:** New `docs/algorithms.md`\\n**Status:** [ ] In Progress\\n**Priority:** High\\n\\n**Goal:**\\nDocument the core IR algorithms in a way that helps semantic search understand what each algorithm does, when to use it, and how components relate.\\n\\n**Content:**\\n- PageRank explanation with use cases\\n- TF-IDF calculation and per-document vs global variants\\n- Label propagation for concept clustering\\n- Co-occurrence counting (\\\"Hebbian learning\\\" metaphor)\\n- Relation extraction patterns\\n- Query expansion strategies\\n\\n---\\n\\n### 54. Create Architecture Intelligence Documentation\\n\\n**File:** New `docs/architecture.md`\\n**Status:** [ ] Not Started\\n**Priority:** High\\n\\n**Goal:**\\nDocument the 4-layer architecture and data flow in searchable prose that helps answer \\\"where is X handled?\\\" and \\\"how does X work?\\\" queries.\\n\\n**Content:**\\n- Layer 0 (Tokens): Word-level processing\\n- Layer 1 (Bigrams): Phrase patterns\\n- Layer 2 (Concepts): Topic clusters\\n- Layer 3 (Documents): Full document representations\\n- Cross-layer connections (feedforward/feedback)\\n- Minicolumn data structure\\n\\n---\\n\\n### 55. Create Pattern Glossary\\n\\n**File:** New `docs/glossary.md`\\n**Status:** [ ] Not Started\\n**Priority:** Medium\\n\\n**Goal:**\\nDefine terminology used throughout the codebase so searches for concepts find relevant definitions.\\n\\n**Terms:**\\n- Minicolumn, Edge, HierarchicalLayer\\n- Lateral connections, typed connections\\n- Feedforward/feedback connections\\n- PageRank, TF-IDF, damping factor\\n- Semantic relations (IsA, PartOf, etc.)\\n- Query expansion, spreading activation\\n\\n---\\n\\n### 56. Create Usage Patterns Documentation\\n\\n**File:** New `docs/patterns.md`\\n**Status:** [ ] Not Started\\n**Priority:** Medium\\n\\n**Goal:**\\nDocument common usage patterns and code examples that help answer \\\"how do I...\\\" queries.\\n\\n**Patterns:**\\n- Basic document processing workflow\\n- RAG retrieval with passages\\n- Code search with intent parsing\\n- Fingerprint comparison for similarity\\n- Batch operations for performance\\n- Incremental updates\\n\\n---\\n\\n### 58. Git-Compatible Chunk-Based Indexing\\n\\n**Files:** `scripts/index_codebase.py`, `cortical/chunk_index.py` (new), `tests/test_chunk_indexing.py` (new)\\n**Status:** [ ] In Progress\\n**Priority:** High\\n\\n**Problem:**\\nThe current pkl-based index cannot be tracked in git (binary, merge conflicts). This prevents sharing indexed state across branches and team members, requiring full rebuilds.\\n\\n**Solution:**\\nImplement append-only, time-stamped JSON chunks that can be safely committed to git and merged without conflicts.\\n\\n**Architecture:**\\n```\\ncorpus_chunks/                        # Tracked in git\\n├── 2025-12-10_21-53-45_a1b2.json    # Session 1 changes\\n├── 2025-12-10_22-15-30_c3d4.json    # Session 2 changes\\n└── 2025-12-10_23-00-00_e5f6.json    # Session 3 changes\\n\\ncorpus_dev.pkl                        # NOT tracked (local cache)\\n```\\n\\n**Chunk Format:**\\n```json\\n{\\n  \\\"timestamp\\\": \\\"2025-12-10T21:53:45\\\",\\n  \\\"session_id\\\": \\\"a1b2c3d4\\\",\\n  \\\"branch\\\": \\\"feature-x\\\",\\n  \\\"operations\\\": [\\n    {\\\"op\\\": \\\"add\\\", \\\"doc_id\\\": \\\"docs/new.md\\\", \\\"content\\\": \\\"...\\\", \\\"mtime\\\": 1234567890},\\n    {\\\"op\\\": \\\"modify\\\", \\\"doc_id\\\": \\\"query.py\\\", \\\"content\\\": \\\"...\\\", \\\"mtime\\\": 1234567891},\\n    {\\\"op\\\": \\\"delete\\\", \\\"doc_id\\\": \\\"old.md\\\"}\\n  ]\\n}\\n```\\n\\n**Implementation Tasks:**\\n1. [ ] Create `ChunkWriter` class - save session changes as timestamped JSON\\n2. [ ] Create `ChunkLoader` class - combine chunks on startup (later timestamps win)\\n3. [ ] Add cache validator - check if pkl matches combined chunk hash\\n4. [ ] Add `--compact` command - merge old chunks into single file\\n5. [ ] Update CLI with `--use-chunks` flag\\n6. [ ] Handle deletions with tombstones\\n7. [ ] Add `.gitignore` entry for `corpus_dev.pkl` (keep chunks tracked)\\n8. [ ] Add comprehensive tests\\n\\n**Startup Flow:**\\n```\\n1. Load all chunk files (sorted by timestamp)\\n2. Replay operations → build document set\\n   - Later timestamps win for conflicts\\n   - Deletes remove documents\\n3. Check if pkl cache is valid (hash of combined docs)\\n   - Valid: load pkl (fast)\\n   - Invalid: recompute analysis (~2s)\\n```\\n\\n**Benefits:**\\n- No merge conflicts (unique timestamp+session names)\\n- Shared indexed state across team/branches\\n- Fast startup when cache valid\\n- Git-friendly (small JSON, append-only)\\n- Periodic compaction like `git gc`\\n\\n**Usage (planned):**\\n```bash\\n# Index with chunks (creates timestamped JSON)\\npython scripts/index_codebase.py --incremental --use-chunks\\n\\n# Compact old chunks\\npython scripts/index_codebase.py --compact --before 2025-12-01\\n\\n# Status including chunk info\\npython scripts/index_codebase.py --status --use-chunks\\n```\\n\\n---\\n\\n*Updated 2025-12-10*\\n\",",
        "      \"mtime\": 1765407597.6541517",
        "    },",
        "    {",
        "      \"op\": \"add\",",
        "      \"doc_id\": \"cortical/code_concepts.py\",",
        "      \"content\": \"\\\"\\\"\\\"\\nCode Concepts Module\\n====================\\n\\nProgramming concept groups for semantic code search.\\n\\nMaps common programming synonyms and related terms to enable\\nintent-based code retrieval. When a developer searches for \\\"get user\\\",\\nthe system can also find \\\"fetch user\\\", \\\"load user\\\", \\\"retrieve user\\\".\\n\\\"\\\"\\\"\\n\\nfrom typing import Dict, List, Set, FrozenSet\\n\\n\\n# Programming concept groups - terms that are often interchangeable in code\\nCODE_CONCEPT_GROUPS: Dict[str, FrozenSet[str]] = {\\n    # Data retrieval operations\\n    'retrieval': frozenset([\\n        'get', 'fetch', 'load', 'retrieve', 'read', 'query', 'find',\\n        'lookup', 'obtain', 'acquire', 'pull', 'select'\\n    ]),\\n\\n    # Data storage operations\\n    'storage': frozenset([\\n        'save', 'store', 'write', 'persist', 'cache', 'put', 'set',\\n        'insert', 'add', 'create', 'commit', 'push', 'update'\\n    ]),\\n\\n    # Deletion operations\\n    'deletion': frozenset([\\n        'delete', 'remove', 'drop', 'clear', 'destroy', 'purge',\\n        'erase', 'clean', 'reset', 'dispose', 'unset'\\n    ]),\\n\\n    # Authentication and security\\n    'auth': frozenset([\\n        'auth', 'authentication', 'login', 'logout', 'credentials',\\n        'token', 'session', 'password', 'user', 'permission', 'role',\\n        'access', 'authorize', 'verify', 'validate', 'identity'\\n    ]),\\n\\n    # Error handling\\n    'error': frozenset([\\n        'error', 'exception', 'fail', 'failure', 'catch', 'handle',\\n        'throw', 'raise', 'try', 'recover', 'retry', 'fallback',\\n        'invalid', 'warning', 'fault', 'crash'\\n    ]),\\n\\n    # Validation and checking\\n    'validation': frozenset([\\n        'validate', 'check', 'verify', 'assert', 'ensure', 'confirm',\\n        'test', 'inspect', 'examine', 'sanitize', 'filter', 'guard'\\n    ]),\\n\\n    # Transformation operations\\n    'transform': frozenset([\\n        'transform', 'convert', 'parse', 'format', 'serialize',\\n        'deserialize', 'encode', 'decode', 'map', 'reduce', 'filter',\\n        'normalize', 'process', 'translate', 'render'\\n    ]),\\n\\n    # Network and API\\n    'network': frozenset([\\n        'request', 'response', 'api', 'endpoint', 'http', 'rest',\\n        'client', 'server', 'socket', 'connection', 'send', 'receive',\\n        'url', 'route', 'handler', 'middleware'\\n    ]),\\n\\n    # Database operations\\n    'database': frozenset([\\n        'database', 'db', 'sql', 'query', 'table', 'record', 'row',\\n        'column', 'index', 'schema', 'migration', 'model', 'entity',\\n        'repository', 'orm', 'transaction'\\n    ]),\\n\\n    # Async and concurrency\\n    'async': frozenset([\\n        'async', 'await', 'promise', 'future', 'callback', 'thread',\\n        'concurrent', 'parallel', 'worker', 'queue', 'task', 'job',\\n        'schedule', 'spawn', 'sync', 'lock', 'mutex'\\n    ]),\\n\\n    # Configuration and settings\\n    'config': frozenset([\\n        'config', 'configuration', 'settings', 'options', 'preferences',\\n        'env', 'environment', 'property', 'parameter', 'argument',\\n        'flag', 'constant', 'default', 'override'\\n    ]),\\n\\n    # Logging and monitoring\\n    'logging': frozenset([\\n        'log', 'logger', 'logging', 'debug', 'info', 'warn', 'trace',\\n        'monitor', 'metric', 'telemetry', 'track', 'audit', 'record',\\n        'print', 'output', 'verbose'\\n    ]),\\n\\n    # Testing\\n    'testing': frozenset([\\n        'test', 'spec', 'mock', 'stub', 'fake', 'fixture', 'assert',\\n        'expect', 'verify', 'unit', 'integration', 'coverage', 'suite',\\n        'setup', 'teardown', 'before', 'after'\\n    ]),\\n\\n    # File operations\\n    'file': frozenset([\\n        'file', 'path', 'directory', 'folder', 'read', 'write', 'open',\\n        'close', 'stream', 'buffer', 'io', 'filesystem', 'upload',\\n        'download', 'copy', 'move', 'rename'\\n    ]),\\n\\n    # Iteration and collections\\n    'iteration': frozenset([\\n        'iterate', 'loop', 'each', 'map', 'filter', 'reduce', 'fold',\\n        'list', 'array', 'collection', 'set', 'dict', 'hash', 'tree',\\n        'queue', 'stack', 'sort', 'search', 'find'\\n    ]),\\n\\n    # Initialization and lifecycle\\n    'lifecycle': frozenset([\\n        'init', 'initialize', 'setup', 'start', 'stop', 'shutdown',\\n        'bootstrap', 'create', 'destroy', 'build', 'configure',\\n        'register', 'unregister', 'connect', 'disconnect', 'close'\\n    ]),\\n\\n    # Events and messaging\\n    'events': frozenset([\\n        'event', 'emit', 'listen', 'subscribe', 'publish', 'dispatch',\\n        'handler', 'callback', 'hook', 'trigger', 'notify', 'observe',\\n        'broadcast', 'signal', 'message', 'channel'\\n    ]),\\n}\\n\\n# Build reverse index: term -> list of concept groups it belongs to\\n_TERM_TO_CONCEPTS: Dict[str, List[str]] = {}\\nfor concept, terms in CODE_CONCEPT_GROUPS.items():\\n    for term in terms:\\n        if term not in _TERM_TO_CONCEPTS:\\n            _TERM_TO_CONCEPTS[term] = []\\n        _TERM_TO_CONCEPTS[term].append(concept)\\n\\n\\ndef get_related_terms(term: str, max_terms: int = 5) -> List[str]:\\n    \\\"\\\"\\\"\\n    Get programming terms related to the given term.\\n\\n    Args:\\n        term: A programming term (e.g., \\\"fetch\\\", \\\"authenticate\\\")\\n        max_terms: Maximum number of related terms to return\\n\\n    Returns:\\n        List of related terms, excluding the input term\\n\\n    Example:\\n        >>> get_related_terms(\\\"fetch\\\")\\n        ['get', 'load', 'retrieve', 'read', 'query']\\n    \\\"\\\"\\\"\\n    term_lower = term.lower()\\n    related: Set[str] = set()\\n\\n    # Find all concept groups this term belongs to\\n    concepts = _TERM_TO_CONCEPTS.get(term_lower, [])\\n\\n    for concept in concepts:\\n        terms = CODE_CONCEPT_GROUPS.get(concept, frozenset())\\n        related.update(terms)\\n\\n    # Remove the original term\\n    related.discard(term_lower)\\n\\n    # Return top terms sorted alphabetically for consistent results\\n    return sorted(related)[:max_terms]\\n\\n\\ndef expand_code_concepts(\\n    terms: List[str],\\n    max_expansions_per_term: int = 3,\\n    weight: float = 0.6\\n) -> Dict[str, float]:\\n    \\\"\\\"\\\"\\n    Expand a list of terms using code concept groups.\\n\\n    Args:\\n        terms: List of query terms to expand\\n        max_expansions_per_term: Max related terms to add per input term\\n        weight: Weight to assign to expanded terms (0.0-1.0)\\n\\n    Returns:\\n        Dict mapping expanded terms to weights\\n\\n    Example:\\n        >>> expand_code_concepts([\\\"fetch\\\", \\\"user\\\"])\\n        {'get': 0.6, 'load': 0.6, 'retrieve': 0.6, ...}\\n    \\\"\\\"\\\"\\n    expanded: Dict[str, float] = {}\\n    input_terms = set(t.lower() for t in terms)\\n\\n    for term in terms:\\n        related = get_related_terms(term, max_terms=max_expansions_per_term)\\n        for related_term in related:\\n            # Don't add terms that were in the original query\\n            if related_term not in input_terms:\\n                # Keep highest weight if term appears multiple times\\n                if related_term not in expanded or expanded[related_term] < weight:\\n                    expanded[related_term] = weight\\n\\n    return expanded\\n\\n\\ndef get_concept_group(term: str) -> List[str]:\\n    \\\"\\\"\\\"\\n    Get the concept group names a term belongs to.\\n\\n    Args:\\n        term: A programming term\\n\\n    Returns:\\n        List of concept group names\\n\\n    Example:\\n        >>> get_concept_group(\\\"fetch\\\")\\n        ['retrieval']\\n        >>> get_concept_group(\\\"validate\\\")\\n        ['validation', 'testing']\\n    \\\"\\\"\\\"\\n    return _TERM_TO_CONCEPTS.get(term.lower(), [])\\n\\n\\ndef list_concept_groups() -> List[str]:\\n    \\\"\\\"\\\"\\n    List all available concept group names.\\n\\n    Returns:\\n        Sorted list of concept group names\\n    \\\"\\\"\\\"\\n    return sorted(CODE_CONCEPT_GROUPS.keys())\\n\\n\\ndef get_group_terms(group_name: str) -> List[str]:\\n    \\\"\\\"\\\"\\n    Get all terms in a concept group.\\n\\n    Args:\\n        group_name: Name of the concept group\\n\\n    Returns:\\n        Sorted list of terms in the group, or empty list if group not found\\n    \\\"\\\"\\\"\\n    terms = CODE_CONCEPT_GROUPS.get(group_name, frozenset())\\n    return sorted(terms)\\n\",",
        "      \"mtime\": 1765392877.0",
        "    },",
        "    {",
        "      \"op\": \"add\",",
        "      \"doc_id\": \"tests/test_fingerprint.py\",",
        "      \"content\": \"\\\"\\\"\\\"\\nTests for fingerprint module.\\n\\nTests the semantic fingerprinting functionality for code comparison.\\n\\\"\\\"\\\"\\n\\nimport unittest\\nfrom cortical.fingerprint import (\\n    compute_fingerprint,\\n    compare_fingerprints,\\n    explain_fingerprint,\\n    explain_similarity,\\n    SemanticFingerprint,\\n)\\nfrom cortical.tokenizer import Tokenizer\\n\\n\\nclass TestComputeFingerprint(unittest.TestCase):\\n    \\\"\\\"\\\"Test the compute_fingerprint function.\\\"\\\"\\\"\\n\\n    def setUp(self):\\n        \\\"\\\"\\\"Set up test tokenizer.\\\"\\\"\\\"\\n        self.tokenizer = Tokenizer()\\n\\n    def test_basic_fingerprint(self):\\n        \\\"\\\"\\\"Test basic fingerprint computation.\\\"\\\"\\\"\\n        text = \\\"The function validates user input and handles errors.\\\"\\n        fp = compute_fingerprint(text, self.tokenizer)\\n\\n        self.assertIn('terms', fp)\\n        self.assertIn('concepts', fp)\\n        self.assertIn('bigrams', fp)\\n        self.assertIn('top_terms', fp)\\n        self.assertIn('term_count', fp)\\n        self.assertIn('raw_text_hash', fp)\\n\\n    def test_fingerprint_contains_terms(self):\\n        \\\"\\\"\\\"Test that fingerprint contains expected terms.\\\"\\\"\\\"\\n        text = \\\"fetch user data from database\\\"\\n        fp = compute_fingerprint(text, self.tokenizer)\\n\\n        self.assertIn('fetch', fp['terms'])\\n        self.assertIn('user', fp['terms'])\\n        self.assertIn('data', fp['terms'])\\n        self.assertIn('database', fp['terms'])\\n\\n    def test_fingerprint_concepts(self):\\n        \\\"\\\"\\\"Test that fingerprint captures concept membership.\\\"\\\"\\\"\\n        text = \\\"fetch data and save results\\\"\\n        fp = compute_fingerprint(text, self.tokenizer)\\n\\n        # 'fetch' is in retrieval group, 'save' is in storage group\\n        # (if code_concepts recognizes them)\\n        self.assertIsInstance(fp['concepts'], dict)\\n\\n    def test_fingerprint_bigrams(self):\\n        \\\"\\\"\\\"Test that fingerprint captures bigrams.\\\"\\\"\\\"\\n        text = \\\"neural networks process data efficiently\\\"\\n        fp = compute_fingerprint(text, self.tokenizer)\\n\\n        self.assertIn('bigrams', fp)\\n        self.assertIsInstance(fp['bigrams'], dict)\\n\\n    def test_fingerprint_top_terms_limit(self):\\n        \\\"\\\"\\\"Test that top_n limits top terms.\\\"\\\"\\\"\\n        text = \\\"word1 word2 word3 word4 word5 word6 word7 word8 word9 word10\\\"\\n        fp = compute_fingerprint(text, self.tokenizer, top_n=5)\\n\\n        self.assertLessEqual(len(fp['top_terms']), 5)\\n\\n    def test_empty_text_fingerprint(self):\\n        \\\"\\\"\\\"Test fingerprint of empty text.\\\"\\\"\\\"\\n        fp = compute_fingerprint(\\\"\\\", self.tokenizer)\\n\\n        self.assertEqual(fp['term_count'], 0)\\n        self.assertEqual(fp['terms'], {})\\n\\n    def test_fingerprint_term_weights_positive(self):\\n        \\\"\\\"\\\"Test that term weights are positive.\\\"\\\"\\\"\\n        text = \\\"validate user input data\\\"\\n        fp = compute_fingerprint(text, self.tokenizer)\\n\\n        for term, weight in fp['terms'].items():\\n            self.assertGreater(weight, 0)\\n\\n    def test_fingerprint_with_layers(self):\\n        \\\"\\\"\\\"Test fingerprint with corpus layers for TF-IDF.\\\"\\\"\\\"\\n        from cortical import CorticalTextProcessor\\n        processor = CorticalTextProcessor()\\n        processor.process_document(\\\"doc1\\\", \\\"test document content\\\")\\n        processor.compute_all()\\n\\n        # Compute fingerprint with layers\\n        fp = compute_fingerprint(\\n            \\\"test content\\\",\\n            processor.tokenizer,\\n            processor.layers\\n        )\\n\\n        self.assertIn('terms', fp)\\n        self.assertGreater(len(fp['terms']), 0)\\n\\n\\nclass TestCompareFingerprints(unittest.TestCase):\\n    \\\"\\\"\\\"Test the compare_fingerprints function.\\\"\\\"\\\"\\n\\n    def setUp(self):\\n        \\\"\\\"\\\"Set up test tokenizer.\\\"\\\"\\\"\\n        self.tokenizer = Tokenizer()\\n\\n    def test_identical_texts(self):\\n        \\\"\\\"\\\"Test comparing identical texts.\\\"\\\"\\\"\\n        text = \\\"validate user input data\\\"\\n        fp1 = compute_fingerprint(text, self.tokenizer)\\n        fp2 = compute_fingerprint(text, self.tokenizer)\\n\\n        result = compare_fingerprints(fp1, fp2)\\n\\n        self.assertTrue(result['identical'])\\n        self.assertEqual(result['overall_similarity'], 1.0)\\n\\n    def test_similar_texts(self):\\n        \\\"\\\"\\\"Test comparing similar texts.\\\"\\\"\\\"\\n        fp1 = compute_fingerprint(\\\"validate user input\\\", self.tokenizer)\\n        fp2 = compute_fingerprint(\\\"check user data\\\", self.tokenizer)\\n\\n        result = compare_fingerprints(fp1, fp2)\\n\\n        self.assertFalse(result['identical'])\\n        self.assertIn('user', result['shared_terms'])\\n        self.assertGreater(result['term_similarity'], 0)\\n\\n    def test_different_texts(self):\\n        \\\"\\\"\\\"Test comparing different texts.\\\"\\\"\\\"\\n        fp1 = compute_fingerprint(\\\"neural network training\\\", self.tokenizer)\\n        fp2 = compute_fingerprint(\\\"database query optimization\\\", self.tokenizer)\\n\\n        result = compare_fingerprints(fp1, fp2)\\n\\n        self.assertFalse(result['identical'])\\n        # Should have lower similarity\\n        self.assertLess(result['overall_similarity'], 0.5)\\n\\n    def test_comparison_contains_metrics(self):\\n        \\\"\\\"\\\"Test that comparison contains all expected metrics.\\\"\\\"\\\"\\n        fp1 = compute_fingerprint(\\\"text one\\\", self.tokenizer)\\n        fp2 = compute_fingerprint(\\\"text two\\\", self.tokenizer)\\n\\n        result = compare_fingerprints(fp1, fp2)\\n\\n        self.assertIn('identical', result)\\n        self.assertIn('term_similarity', result)\\n        self.assertIn('concept_similarity', result)\\n        self.assertIn('overall_similarity', result)\\n        self.assertIn('shared_terms', result)\\n\\n    def test_similarity_in_valid_range(self):\\n        \\\"\\\"\\\"Test that similarity scores are in [0, 1].\\\"\\\"\\\"\\n        fp1 = compute_fingerprint(\\\"fetch user data\\\", self.tokenizer)\\n        fp2 = compute_fingerprint(\\\"save user results\\\", self.tokenizer)\\n\\n        result = compare_fingerprints(fp1, fp2)\\n\\n        self.assertGreaterEqual(result['term_similarity'], 0)\\n        self.assertLessEqual(result['term_similarity'], 1)\\n        self.assertGreaterEqual(result['overall_similarity'], 0)\\n        self.assertLessEqual(result['overall_similarity'], 1)\\n\\n    def test_shared_terms_correct(self):\\n        \\\"\\\"\\\"Test that shared terms are correctly identified.\\\"\\\"\\\"\\n        fp1 = compute_fingerprint(\\\"user data validation\\\", self.tokenizer)\\n        fp2 = compute_fingerprint(\\\"user input checking\\\", self.tokenizer)\\n\\n        result = compare_fingerprints(fp1, fp2)\\n\\n        self.assertIn('user', result['shared_terms'])\\n\\n\\nclass TestExplainFingerprint(unittest.TestCase):\\n    \\\"\\\"\\\"Test the explain_fingerprint function.\\\"\\\"\\\"\\n\\n    def setUp(self):\\n        \\\"\\\"\\\"Set up test tokenizer.\\\"\\\"\\\"\\n        self.tokenizer = Tokenizer()\\n\\n    def test_explanation_structure(self):\\n        \\\"\\\"\\\"Test that explanation has expected structure.\\\"\\\"\\\"\\n        text = \\\"validate user input and handle errors\\\"\\n        fp = compute_fingerprint(text, self.tokenizer)\\n        explanation = explain_fingerprint(fp)\\n\\n        self.assertIn('summary', explanation)\\n        self.assertIn('top_terms', explanation)\\n        self.assertIn('top_concepts', explanation)\\n        self.assertIn('term_count', explanation)\\n\\n    def test_explanation_top_n_limit(self):\\n        \\\"\\\"\\\"Test that top_n limits items in explanation.\\\"\\\"\\\"\\n        text = \\\"word1 word2 word3 word4 word5 word6\\\"\\n        fp = compute_fingerprint(text, self.tokenizer)\\n        explanation = explain_fingerprint(fp, top_n=3)\\n\\n        self.assertLessEqual(len(explanation['top_terms']), 3)\\n\\n    def test_summary_is_string(self):\\n        \\\"\\\"\\\"Test that summary is a string.\\\"\\\"\\\"\\n        text = \\\"process data\\\"\\n        fp = compute_fingerprint(text, self.tokenizer)\\n        explanation = explain_fingerprint(fp)\\n\\n        self.assertIsInstance(explanation['summary'], str)\\n\\n\\nclass TestExplainSimilarity(unittest.TestCase):\\n    \\\"\\\"\\\"Test the explain_similarity function.\\\"\\\"\\\"\\n\\n    def setUp(self):\\n        \\\"\\\"\\\"Set up test tokenizer.\\\"\\\"\\\"\\n        self.tokenizer = Tokenizer()\\n\\n    def test_explanation_is_string(self):\\n        \\\"\\\"\\\"Test that similarity explanation is a string.\\\"\\\"\\\"\\n        fp1 = compute_fingerprint(\\\"fetch user data\\\", self.tokenizer)\\n        fp2 = compute_fingerprint(\\\"load user info\\\", self.tokenizer)\\n\\n        explanation = explain_similarity(fp1, fp2)\\n\\n        self.assertIsInstance(explanation, str)\\n        self.assertGreater(len(explanation), 0)\\n\\n    def test_identical_texts_explanation(self):\\n        \\\"\\\"\\\"Test explanation for identical texts.\\\"\\\"\\\"\\n        text = \\\"validate input\\\"\\n        fp1 = compute_fingerprint(text, self.tokenizer)\\n        fp2 = compute_fingerprint(text, self.tokenizer)\\n\\n        explanation = explain_similarity(fp1, fp2)\\n\\n        self.assertIn('identical', explanation.lower())\\n\\n\\nclass TestProcessorIntegration(unittest.TestCase):\\n    \\\"\\\"\\\"Test fingerprint integration with processor.\\\"\\\"\\\"\\n\\n    def setUp(self):\\n        \\\"\\\"\\\"Set up test processor.\\\"\\\"\\\"\\n        from cortical import CorticalTextProcessor\\n        self.processor = CorticalTextProcessor()\\n        self.processor.process_document(\\\"auth\\\", \\\"\\\"\\\"\\n            Authentication module handles user login and credentials.\\n            Validates tokens and manages sessions.\\n        \\\"\\\"\\\")\\n        self.processor.process_document(\\\"data\\\", \\\"\\\"\\\"\\n            Data processing module fetches and transforms data.\\n            Handles database queries and result formatting.\\n        \\\"\\\"\\\")\\n        self.processor.compute_all()\\n\\n    def test_processor_get_fingerprint(self):\\n        \\\"\\\"\\\"Test processor get_fingerprint method.\\\"\\\"\\\"\\n        fp = self.processor.get_fingerprint(\\\"validate user credentials\\\")\\n\\n        self.assertIn('terms', fp)\\n        self.assertIn('concepts', fp)\\n        self.assertGreater(fp['term_count'], 0)\\n\\n    def test_processor_compare_fingerprints(self):\\n        \\\"\\\"\\\"Test processor compare_fingerprints method.\\\"\\\"\\\"\\n        fp1 = self.processor.get_fingerprint(\\\"user authentication\\\")\\n        fp2 = self.processor.get_fingerprint(\\\"user validation\\\")\\n\\n        result = self.processor.compare_fingerprints(fp1, fp2)\\n\\n        self.assertIn('overall_similarity', result)\\n        self.assertIn('user', result['shared_terms'])\\n\\n    def test_processor_explain_fingerprint(self):\\n        \\\"\\\"\\\"Test processor explain_fingerprint method.\\\"\\\"\\\"\\n        fp = self.processor.get_fingerprint(\\\"fetch data from database\\\")\\n        explanation = self.processor.explain_fingerprint(fp)\\n\\n        self.assertIn('summary', explanation)\\n        self.assertIn('top_terms', explanation)\\n\\n    def test_processor_explain_similarity(self):\\n        \\\"\\\"\\\"Test processor explain_similarity method.\\\"\\\"\\\"\\n        fp1 = self.processor.get_fingerprint(\\\"fetch data\\\")\\n        fp2 = self.processor.get_fingerprint(\\\"load data\\\")\\n\\n        explanation = self.processor.explain_similarity(fp1, fp2)\\n\\n        self.assertIsInstance(explanation, str)\\n\\n    def test_processor_find_similar_texts(self):\\n        \\\"\\\"\\\"Test processor find_similar_texts method.\\\"\\\"\\\"\\n        candidates = [\\n            (\\\"auth_code\\\", \\\"validate user credentials and create session\\\"),\\n            (\\\"data_code\\\", \\\"fetch records from database and transform\\\"),\\n            (\\\"ui_code\\\", \\\"render button and handle click event\\\"),\\n        ]\\n\\n        results = self.processor.find_similar_texts(\\n            \\\"authenticate user login\\\",\\n            candidates,\\n            top_n=2\\n        )\\n\\n        self.assertLessEqual(len(results), 2)\\n        # Results should be sorted by similarity\\n        if len(results) >= 2:\\n            self.assertGreaterEqual(results[0][1], results[1][1])\\n\\n\\nif __name__ == '__main__':\\n    unittest.main()\\n\",",
        "      \"mtime\": 1765392877.0",
        "    },",
        "    {",
        "      \"op\": \"add\",",
        "      \"doc_id\": \"cortical/layers.py\",",
        "      \"content\": \"\\\"\\\"\\\"\\nLayers Module\\n=============\\n\\nDefines the hierarchical layer structure inspired by the visual cortex.\\n\\nThe neocortex processes information through a hierarchy of layers,\\neach extracting progressively more abstract features:\\n- V1: Edge detection (→ tokens)\\n- V2: Simple patterns (→ bigrams)\\n- V4: Complex shapes (→ concepts)\\n- IT: Object recognition (→ documents)\\n\\\"\\\"\\\"\\n\\nfrom enum import IntEnum\\nfrom typing import Dict, Optional, Iterator\\n\\nfrom .minicolumn import Minicolumn\\n\\n\\nclass CorticalLayer(IntEnum):\\n    \\\"\\\"\\\"\\n    Enumeration of cortical processing layers.\\n    \\n    Maps visual cortex layers to text processing hierarchy:\\n        TOKENS (0): Like V1 - basic feature extraction (words)\\n        BIGRAMS (1): Like V2 - simple patterns (word pairs)\\n        CONCEPTS (2): Like V4 - higher-level features (clusters)\\n        DOCUMENTS (3): Like IT - holistic recognition (full docs)\\n    \\\"\\\"\\\"\\n    TOKENS = 0      # Individual words (V1-like)\\n    BIGRAMS = 1     # Word pairs (V2-like)\\n    CONCEPTS = 2    # Concept clusters (V4-like)\\n    DOCUMENTS = 3   # Full documents (IT-like)\\n    \\n    @property\\n    def description(self) -> str:\\n        \\\"\\\"\\\"Human-readable description of this layer.\\\"\\\"\\\"\\n        descriptions = {\\n            0: \\\"Token layer - individual words (V1-like)\\\",\\n            1: \\\"Bigram layer - word pairs (V2-like)\\\",\\n            2: \\\"Concept layer - semantic clusters (V4-like)\\\",\\n            3: \\\"Document layer - full documents (IT-like)\\\"\\n        }\\n        return descriptions[self.value]\\n    \\n    @property\\n    def analogy(self) -> str:\\n        \\\"\\\"\\\"Visual cortex analogy for this layer.\\\"\\\"\\\"\\n        analogies = {\\n            0: \\\"V1-like: Edge/token detection\\\",\\n            1: \\\"V2-like: Feature/pattern detection\\\",\\n            2: \\\"V4-like: Shape/concept detection\\\",\\n            3: \\\"IT-like: Object/document recognition\\\"\\n        }\\n        return analogies[self.value]\\n\\n\\nclass HierarchicalLayer:\\n    \\\"\\\"\\\"\\n    A layer in the cortical hierarchy containing minicolumns.\\n    \\n    Each layer contains a collection of minicolumns and provides\\n    methods for managing them. Layers are organized hierarchically,\\n    with feedforward connections from lower to higher layers and\\n    lateral connections within each layer.\\n    \\n    Attributes:\\n        level: The layer number (0-3)\\n        minicolumns: Dictionary mapping content to Minicolumn objects\\n        _id_index: Secondary index mapping minicolumn IDs to content for O(1) lookups\\n\\n    Example:\\n        layer = HierarchicalLayer(CorticalLayer.TOKENS)\\n        col = layer.get_or_create_minicolumn(\\\"neural\\\")\\n        col.occurrence_count += 1\\n    \\\"\\\"\\\"\\n    \\n    def __init__(self, level: CorticalLayer):\\n        \\\"\\\"\\\"\\n        Initialize a hierarchical layer.\\n        \\n        Args:\\n            level: The CorticalLayer enum value for this layer\\n        \\\"\\\"\\\"\\n        self.level = level\\n        self.minicolumns: Dict[str, Minicolumn] = {}\\n        self._id_index: Dict[str, str] = {}  # Maps minicolumn ID to content for O(1) lookup\\n    \\n    def get_or_create_minicolumn(self, content: str) -> Minicolumn:\\n        \\\"\\\"\\\"\\n        Get existing minicolumn or create new one.\\n        \\n        This is the primary way to add content to a layer. If a\\n        minicolumn for this content already exists, return it.\\n        Otherwise, create a new one.\\n        \\n        Args:\\n            content: The content for this minicolumn\\n            \\n        Returns:\\n            The existing or newly created Minicolumn\\n        \\\"\\\"\\\"\\n        if content not in self.minicolumns:\\n            col_id = f\\\"L{self.level}_{content}\\\"\\n            self.minicolumns[content] = Minicolumn(col_id, content, self.level)\\n            self._id_index[col_id] = content  # Maintain ID index for O(1) lookup\\n        return self.minicolumns[content]\\n    \\n    def get_minicolumn(self, content: str) -> Optional[Minicolumn]:\\n        \\\"\\\"\\\"\\n        Get a minicolumn by content, or None if not found.\\n\\n        Args:\\n            content: The content to look up\\n\\n        Returns:\\n            The Minicolumn if found, None otherwise\\n        \\\"\\\"\\\"\\n        return self.minicolumns.get(content)\\n\\n    def get_by_id(self, col_id: str) -> Optional[Minicolumn]:\\n        \\\"\\\"\\\"\\n        Get a minicolumn by its ID in O(1) time.\\n\\n        This method uses a secondary index to avoid O(n) linear searches\\n        when looking up minicolumns by their ID rather than content.\\n\\n        Args:\\n            col_id: The minicolumn ID (e.g., \\\"L0_neural\\\")\\n\\n        Returns:\\n            The Minicolumn if found, None otherwise\\n        \\\"\\\"\\\"\\n        content = self._id_index.get(col_id)\\n        return self.minicolumns.get(content) if content else None\\n\\n    def remove_minicolumn(self, content: str) -> bool:\\n        \\\"\\\"\\\"\\n        Remove a minicolumn from this layer.\\n\\n        Args:\\n            content: The content key of the minicolumn to remove\\n\\n        Returns:\\n            True if the minicolumn was found and removed, False otherwise\\n        \\\"\\\"\\\"\\n        if content not in self.minicolumns:\\n            return False\\n\\n        col = self.minicolumns[content]\\n        # Remove from ID index\\n        if col.id in self._id_index:\\n            del self._id_index[col.id]\\n        # Remove from minicolumns dict\\n        del self.minicolumns[content]\\n        return True\\n\\n    def column_count(self) -> int:\\n        \\\"\\\"\\\"Return the number of minicolumns in this layer.\\\"\\\"\\\"\\n        return len(self.minicolumns)\\n    \\n    def total_connections(self) -> int:\\n        \\\"\\\"\\\"Return total number of lateral connections in this layer.\\\"\\\"\\\"\\n        return sum(col.connection_count() for col in self.minicolumns.values())\\n    \\n    def average_activation(self) -> float:\\n        \\\"\\\"\\\"Calculate average activation across all minicolumns.\\\"\\\"\\\"\\n        if not self.minicolumns:\\n            return 0.0\\n        return sum(col.activation for col in self.minicolumns.values()) / len(self.minicolumns)\\n    \\n    def activation_range(self) -> tuple:\\n        \\\"\\\"\\\"Return (min, max) activation values.\\\"\\\"\\\"\\n        if not self.minicolumns:\\n            return (0.0, 0.0)\\n        activations = [col.activation for col in self.minicolumns.values()]\\n        return (min(activations), max(activations))\\n    \\n    def sparsity(self) -> float:\\n        \\\"\\\"\\\"\\n        Calculate sparsity (fraction of columns with low activation).\\n        \\n        In biological neural networks, sparse representations are\\n        more efficient and allow for more distinct patterns.\\n        \\n        Returns:\\n            Fraction of columns with activation below threshold\\n        \\\"\\\"\\\"\\n        if not self.minicolumns:\\n            return 0.0\\n        threshold = 1.0  # Activation threshold\\n        low_activation = sum(1 for col in self.minicolumns.values() \\n                            if col.activation < threshold)\\n        return low_activation / len(self.minicolumns)\\n    \\n    def top_by_pagerank(self, n: int = 10) -> list:\\n        \\\"\\\"\\\"\\n        Get top minicolumns by PageRank score.\\n        \\n        Args:\\n            n: Number of results to return\\n            \\n        Returns:\\n            List of (content, pagerank) tuples\\n        \\\"\\\"\\\"\\n        sorted_cols = sorted(\\n            self.minicolumns.values(),\\n            key=lambda c: c.pagerank,\\n            reverse=True\\n        )\\n        return [(col.content, col.pagerank) for col in sorted_cols[:n]]\\n    \\n    def top_by_tfidf(self, n: int = 10) -> list:\\n        \\\"\\\"\\\"\\n        Get top minicolumns by TF-IDF score.\\n        \\n        Args:\\n            n: Number of results to return\\n            \\n        Returns:\\n            List of (content, tfidf) tuples\\n        \\\"\\\"\\\"\\n        sorted_cols = sorted(\\n            self.minicolumns.values(),\\n            key=lambda c: c.tfidf,\\n            reverse=True\\n        )\\n        return [(col.content, col.tfidf) for col in sorted_cols[:n]]\\n    \\n    def top_by_activation(self, n: int = 10) -> list:\\n        \\\"\\\"\\\"\\n        Get top minicolumns by activation level.\\n        \\n        Args:\\n            n: Number of results to return\\n            \\n        Returns:\\n            List of (content, activation) tuples\\n        \\\"\\\"\\\"\\n        sorted_cols = sorted(\\n            self.minicolumns.values(),\\n            key=lambda c: c.activation,\\n            reverse=True\\n        )\\n        return [(col.content, col.activation) for col in sorted_cols[:n]]\\n    \\n    def __iter__(self) -> Iterator[Minicolumn]:\\n        \\\"\\\"\\\"Iterate over minicolumns in this layer.\\\"\\\"\\\"\\n        return iter(self.minicolumns.values())\\n    \\n    def __len__(self) -> int:\\n        \\\"\\\"\\\"Return number of minicolumns.\\\"\\\"\\\"\\n        return len(self.minicolumns)\\n    \\n    def __contains__(self, content: str) -> bool:\\n        \\\"\\\"\\\"Check if content exists in this layer.\\\"\\\"\\\"\\n        return content in self.minicolumns\\n    \\n    def to_dict(self) -> Dict:\\n        \\\"\\\"\\\"\\n        Convert layer to dictionary for serialization.\\n        \\n        Returns:\\n            Dictionary representation of this layer\\n        \\\"\\\"\\\"\\n        return {\\n            'level': self.level,\\n            'minicolumns': {\\n                content: col.to_dict() \\n                for content, col in self.minicolumns.items()\\n            }\\n        }\\n    \\n    @classmethod\\n    def from_dict(cls, data: Dict) -> 'HierarchicalLayer':\\n        \\\"\\\"\\\"\\n        Create a layer from dictionary representation.\\n        \\n        Args:\\n            data: Dictionary with layer data\\n            \\n        Returns:\\n            New HierarchicalLayer instance\\n        \\\"\\\"\\\"\\n        layer = cls(CorticalLayer(data['level']))\\n        for content, col_data in data.get('minicolumns', {}).items():\\n            col = Minicolumn.from_dict(col_data)\\n            layer.minicolumns[content] = col\\n            layer._id_index[col.id] = content  # Rebuild ID index\\n        return layer\\n    \\n    def __repr__(self) -> str:\\n        return f\\\"HierarchicalLayer(level={self.level.name}, columns={len(self.minicolumns)})\\\"\\n\",",
        "      \"mtime\": 1765392997.0",
        "    },",
        "    {",
        "      \"op\": \"add\",",
        "      \"doc_id\": \"docs/algorithms.md\",",
        "      \"content\": \"# Core Algorithms\\n\\nThis document describes the information retrieval algorithms implemented in the Cortical Text Processor. These algorithms work together to build a semantic understanding of text corpora.\\n\\n## Overview\\n\\nThe system uses standard IR algorithms with a hierarchical, layered architecture:\\n\\n| Algorithm | Purpose | Primary File |\\n|-----------|---------|--------------|\\n| PageRank | Importance scoring | `analysis.py:22-95` |\\n| TF-IDF | Term weighting | `analysis.py:394-433` |\\n| Label Propagation | Concept clustering | `analysis.py:502-636` |\\n| Query Expansion | Semantic search | `query.py:55-176` |\\n| Relation Extraction | Knowledge building | `semantics.py:109-186` |\\n\\n---\\n\\n## PageRank - Importance Scoring\\n\\nPageRank measures term importance based on network structure. Terms connected to other important terms receive higher scores.\\n\\n### Standard PageRank\\n\\n**Location:** `analysis.py:22-95`\\n\\n**Algorithm:**\\n1. Initialize each term with equal PageRank: `1.0 / n`\\n2. Iterate until convergence:\\n   ```\\n   PR(i) = (1 - damping) / n + damping * Σ(PR(j) * weight(j→i) / outgoing(j))\\n   ```\\n3. Stop when max change < tolerance (1e-6) or after 20 iterations\\n\\n**Parameters:**\\n- `damping`: 0.85 (probability of following links vs random jump)\\n- `iterations`: 20 maximum\\n- `tolerance`: 1e-6 convergence threshold\\n\\n**Use Case:** Identify the most important terms in the corpus based on how they connect to other important terms.\\n\\n### Semantic PageRank\\n\\n**Location:** `analysis.py:113-235`\\n\\nEnhances PageRank by weighting edges according to semantic relation types.\\n\\n**Relation Weights:**\\n```\\nIsA: 1.5           (hypernym relationships are strong)\\nSimilarTo: 1.4     (similarity is important)\\nPartOf: 1.3        (part-whole relationships)\\nHasProperty: 1.2   (property associations)\\nDerivedFrom: 1.2   (morphological derivation)\\nCauses: 1.1        (causal relationships)\\nRelatedTo: 1.0     (general association - baseline)\\nUsedFor: 1.0       (functional relationships)\\nCoOccurs: 0.8      (basic co-occurrence - lower weight)\\nAntonym: 0.3       (opposing concepts - penalized)\\n```\\n\\n**Use Case:** When semantic relations have been extracted, use semantic PageRank for importance that respects relationship types.\\n\\n### Hierarchical PageRank\\n\\n**Location:** `analysis.py:238-391`\\n\\nPropagates importance across all 4 layers bidirectionally:\\n- Upward: tokens → bigrams → concepts → documents\\n- Downward: documents → concepts → bigrams → tokens\\n\\n**Algorithm:**\\n1. Compute local PageRank within each layer\\n2. Propagate scores upward via `feedback_connections`\\n3. Propagate scores downward via `feedforward_connections`\\n4. Normalize within each layer\\n5. Repeat until convergence\\n\\n**Parameters:**\\n- `layer_iterations`: 10 (within-layer iterations)\\n- `global_iterations`: 5 (cross-layer iterations)\\n- `cross_layer_damping`: 0.7 (damping at layer boundaries)\\n\\n**Use Case:** When you want importance to flow through the full hierarchy, enabling documents to boost their constituent terms and vice versa.\\n\\n---\\n\\n## TF-IDF - Term Weighting\\n\\n**Location:** `analysis.py:394-433`\\n\\nTF-IDF (Term Frequency - Inverse Document Frequency) measures how distinctive a term is to the corpus.\\n\\n**Formula:**\\n```\\nTF-IDF = TF × IDF\\nTF = log(1 + occurrence_count)\\nIDF = log(num_documents / document_frequency)\\n```\\n\\n**Two Variants:**\\n\\n1. **Global TF-IDF** (`col.tfidf`):\\n   - Uses total corpus occurrence count\\n   - Good for corpus-wide term importance\\n\\n2. **Per-Document TF-IDF** (`col.tfidf_per_doc[doc_id]`):\\n   - Uses occurrence count within specific document\\n   - Better for document-specific relevance scoring\\n\\n**Important:** Always use `tfidf_per_doc[doc_id]` for per-document scoring. The global `tfidf` field uses total occurrence count across all documents.\\n\\n---\\n\\n## Label Propagation - Concept Clustering\\n\\n**Location:** `analysis.py:502-636`\\n\\nLabel propagation is a semi-supervised community detection algorithm that clusters tokens into semantic concepts.\\n\\n**Algorithm:**\\n1. Each token starts with a unique label\\n2. Iterate up to 20 times:\\n   - Count neighbor labels weighted by connection strength\\n   - Adopt most common label if it exceeds change threshold\\n3. Group tokens by final label into clusters\\n4. Filter clusters smaller than `min_cluster_size`\\n\\n**Parameters:**\\n- `cluster_strictness` (0.0-1.0): Higher = more separate clusters\\n- `bridge_weight` (0.0-1.0): Synthetic connections between documents\\n- `min_cluster_size`: Minimum tokens per cluster (default 3)\\n\\n**Concept Creation:**\\nAfter clustering, each cluster becomes a concept in Layer 2:\\n- Named after top 3 members by PageRank: `\\\"neural/networks/learning\\\"`\\n- Connected bidirectionally to member tokens\\n- Aggregates member properties (documents, activation, pagerank)\\n\\n---\\n\\n## Query Expansion\\n\\n### Basic Expansion\\n\\n**Location:** `query.py:55-176`\\n\\nExpands query terms to find semantically related words.\\n\\n**Three Expansion Methods:**\\n\\n1. **Lateral Connections** - Direct word associations from co-occurrence\\n   - Score: `connection_weight × neighbor_pagerank × 0.6`\\n\\n2. **Concept Clusters** - Words from same semantic category\\n   - Score: `concept_pagerank × member_pagerank × 0.4`\\n\\n3. **Code Concepts** - Programming synonyms (optional)\\n   - Example: \\\"get\\\" → \\\"fetch\\\", \\\"load\\\", \\\"retrieve\\\"\\n   - Score: `0.6`\\n\\n### Multi-Hop Expansion\\n\\n**Location:** `query.py:407-531`\\n\\nFinds related terms through transitive relation chains.\\n\\n**Example Chains:**\\n- `\\\"dog\\\" → IsA → \\\"animal\\\" → HasProperty → \\\"living\\\"`\\n- `\\\"car\\\" → PartOf → \\\"engine\\\" → UsedFor → \\\"transportation\\\"`\\n\\n**Chain Validity Scoring:**\\nNot all relation chains are equally valid:\\n```\\n(IsA, IsA): 1.0           - Fully transitive hypernymy\\n(IsA, HasProperty): 0.9   - Property inheritance\\n(RelatedTo, RelatedTo): 0.6 - Weak association\\n(Antonym, Antonym): 0.3   - Double negation, unreliable\\n```\\n\\n**Parameters:**\\n- `max_hops`: Maximum chain depth (default 2)\\n- `decay_factor`: Weight decay per hop (default 0.5)\\n- `min_path_score`: Minimum chain validity (default 0.2)\\n\\n### Intent-Based Query Parsing\\n\\n**Location:** `query.py:179-284`\\n\\nParses natural language queries to extract intent.\\n\\n**Intent Types:**\\n- `\\\"where\\\"` → `location` (find file/function location)\\n- `\\\"how\\\"` → `implementation` (find implementation details)\\n- `\\\"what\\\"` → `definition` (find definitions)\\n- `\\\"why\\\"` → `rationale` (find explanations/comments)\\n- `\\\"when\\\"` → `lifecycle` (find lifecycle events)\\n\\n**Example:**\\n```\\nInput: \\\"where do we handle authentication?\\\"\\nOutput: ParsedIntent(\\n    action='handle',\\n    subject='authentication',\\n    intent='location',\\n    expanded_terms=['handle', 'manage', 'authentication', 'auth', ...]\\n)\\n```\\n\\n---\\n\\n## Relation Extraction\\n\\n### Pattern-Based Extraction\\n\\n**Location:** `semantics.py:109-186`\\n\\nExtracts semantic relations from text using regex patterns.\\n\\n**Relation Types:**\\n- **IsA**: \\\"dogs are animals\\\", \\\"a kind of\\\"\\n- **HasA**: \\\"dogs have ears\\\", \\\"contains\\\"\\n- **PartOf**: \\\"wheel is part of car\\\"\\n- **UsedFor**: \\\"hammer is used for nailing\\\"\\n- **Causes**: \\\"rain causes floods\\\"\\n- **CapableOf**: \\\"dog can bark\\\"\\n- **AtLocation**: \\\"found in\\\", \\\"lives in\\\"\\n- **HasProperty**: \\\"dog is loyal\\\"\\n- **Antonym**: \\\"big vs small\\\", \\\"opposite of\\\"\\n- **DerivedFrom**: \\\"comes from\\\"\\n\\nEach pattern has a confidence score (0.5-0.95) based on how reliable it is.\\n\\n### Co-occurrence Relations\\n\\n**Location:** `semantics.py:251-292`\\n\\nExtracts relations from statistical co-occurrence.\\n\\n**Algorithm:**\\n1. Count term pairs within sliding window (5 tokens)\\n2. Compute PMI (Pointwise Mutual Information):\\n   ```\\n   PMI = log((co-occurrence + 1) / (expected + 1))\\n   expected = (count_term1 × count_term2) / corpus_size\\n   ```\\n3. Create `CoOccurs` relations for high-PMI pairs\\n\\n### Similarity Relations\\n\\n**Location:** `semantics.py:294-363`\\n\\nFinds similar terms based on context vectors.\\n\\n**Algorithm:**\\n1. Build context vectors: what words appear near each term\\n2. Compute cosine similarity between context vectors\\n3. Create `SimilarTo` relations for pairs with similarity > 0.3\\n\\n---\\n\\n## Retrofitting\\n\\n**Location:** `semantics.py:378-476`\\n\\nAdjusts connection weights to align with semantic relations.\\n\\n**Algorithm:**\\n1. Store original lateral connection weights\\n2. Build semantic neighbor lookup\\n3. Iterate 10 times:\\n   - Blend original and semantic weights:\\n     ```\\n     new_weight = alpha × original + (1 - alpha) × semantic\\n     ```\\n   - Add new semantic connections that didn't exist\\n\\n**Parameter:**\\n- `alpha`: 0.3 (mostly semantic, some original)\\n\\n**Use Case:** If \\\"dog\\\" and \\\"cat\\\" aren't connected by co-occurrence but both have \\\"IsA animal\\\" relation, retrofitting strengthens their connection.\\n\\n---\\n\\n## Performance Optimizations\\n\\n| Optimization | Location | Benefit |\\n|--------------|----------|---------|\\n| O(1) ID lookups | `layer.get_by_id()` | Avoid O(n) iteration |\\n| Query cache | `expand_query_cached()` | Skip repeated expansions |\\n| Pre-computed lookups | `precompute_term_cols()` | Faster chunk scoring |\\n| Fast search | `fast_find_documents()` | 2-3x faster via candidate filtering |\\n| Inverted index | `build_document_index()` | Fastest repeated queries |\\n\\n---\\n\\n## Quick Reference\\n\\n**When to use which algorithm:**\\n\\n| Goal | Algorithm | Method |\\n|------|-----------|--------|\\n| Find important terms | PageRank | `compute_pagerank()` |\\n| Respect semantic relations | Semantic PageRank | `compute_semantic_importance()` |\\n| Cross-layer importance | Hierarchical PageRank | `compute_hierarchical_importance()` |\\n| Term distinctiveness | TF-IDF | `compute_tfidf()` |\\n| Group related terms | Label Propagation | `build_concept_clusters()` |\\n| Expand search queries | Query Expansion | `expand_query()` |\\n| Find distant relations | Multi-hop Expansion | `expand_query_multihop()` |\\n| Extract knowledge | Relation Extraction | `extract_corpus_semantics()` |\\n| Improve connections | Retrofitting | `retrofit_connections()` |\\n\",",
        "      \"mtime\": 1765392877.0",
        "    },",
        "    {",
        "      \"op\": \"add\",",
        "      \"doc_id\": \"tests/test_query_optimization.py\",",
        "      \"content\": \"\\\"\\\"\\\"\\nTests for query optimization functions.\\n\\nTests the fast search and indexing functionality for improved query performance.\\n\\\"\\\"\\\"\\n\\nimport unittest\\nfrom cortical.query import (\\n    fast_find_documents,\\n    build_document_index,\\n    search_with_index,\\n)\\nfrom cortical.tokenizer import Tokenizer\\nfrom cortical.layers import CorticalLayer\\n\\n\\nclass TestFastFindDocuments(unittest.TestCase):\\n    \\\"\\\"\\\"Test the fast_find_documents function.\\\"\\\"\\\"\\n\\n    def setUp(self):\\n        \\\"\\\"\\\"Set up test processor.\\\"\\\"\\\"\\n        from cortical import CorticalTextProcessor\\n        self.processor = CorticalTextProcessor()\\n        self.processor.process_document(\\\"auth\\\", \\\"\\\"\\\"\\n            Authentication module handles user login and credentials.\\n            Validates tokens and manages sessions securely.\\n        \\\"\\\"\\\")\\n        self.processor.process_document(\\\"data\\\", \\\"\\\"\\\"\\n            Data processing module fetches and transforms data.\\n            Handles database queries and result formatting.\\n        \\\"\\\"\\\")\\n        self.processor.process_document(\\\"validation\\\", \\\"\\\"\\\"\\n            Input validation module checks user input.\\n            Sanitizes and validates form data securely.\\n        \\\"\\\"\\\")\\n        self.processor.compute_all()\\n\\n    def test_fast_find_returns_results(self):\\n        \\\"\\\"\\\"Test that fast_find_documents returns results.\\\"\\\"\\\"\\n        results = fast_find_documents(\\n            \\\"authentication login\\\",\\n            self.processor.layers,\\n            self.processor.tokenizer\\n        )\\n        self.assertIsInstance(results, list)\\n        self.assertGreater(len(results), 0)\\n\\n    def test_fast_find_finds_relevant_doc(self):\\n        \\\"\\\"\\\"Test that fast_find_documents finds relevant document.\\\"\\\"\\\"\\n        results = fast_find_documents(\\n            \\\"authentication login\\\",\\n            self.processor.layers,\\n            self.processor.tokenizer\\n        )\\n        doc_ids = [r[0] for r in results]\\n        self.assertIn('auth', doc_ids)\\n\\n    def test_fast_find_respects_top_n(self):\\n        \\\"\\\"\\\"Test that fast_find_documents respects top_n.\\\"\\\"\\\"\\n        results = fast_find_documents(\\n            \\\"user data\\\",\\n            self.processor.layers,\\n            self.processor.tokenizer,\\n            top_n=2\\n        )\\n        self.assertLessEqual(len(results), 2)\\n\\n    def test_fast_find_empty_query(self):\\n        \\\"\\\"\\\"Test fast_find_documents with empty query.\\\"\\\"\\\"\\n        results = fast_find_documents(\\n            \\\"\\\",\\n            self.processor.layers,\\n            self.processor.tokenizer\\n        )\\n        self.assertEqual(results, [])\\n\\n    def test_fast_find_with_code_concepts(self):\\n        \\\"\\\"\\\"Test fast_find_documents with code concept expansion.\\\"\\\"\\\"\\n        # 'fetch' should expand to find 'data' doc which has 'fetches'\\n        results = fast_find_documents(\\n            \\\"fetch\\\",\\n            self.processor.layers,\\n            self.processor.tokenizer,\\n            use_code_concepts=True\\n        )\\n        # Should find data doc\\n        if results:\\n            doc_ids = [r[0] for r in results]\\n            self.assertIn('data', doc_ids)\\n\\n    def test_fast_find_without_code_concepts(self):\\n        \\\"\\\"\\\"Test fast_find_documents without code concept expansion.\\\"\\\"\\\"\\n        results = fast_find_documents(\\n            \\\"nonexistent term xyz\\\",\\n            self.processor.layers,\\n            self.processor.tokenizer,\\n            use_code_concepts=False\\n        )\\n        self.assertEqual(results, [])\\n\\n\\nclass TestBuildDocumentIndex(unittest.TestCase):\\n    \\\"\\\"\\\"Test the build_document_index function.\\\"\\\"\\\"\\n\\n    def setUp(self):\\n        \\\"\\\"\\\"Set up test processor.\\\"\\\"\\\"\\n        from cortical import CorticalTextProcessor\\n        self.processor = CorticalTextProcessor()\\n        self.processor.process_document(\\\"doc1\\\", \\\"neural network training data\\\")\\n        self.processor.process_document(\\\"doc2\\\", \\\"database query optimization\\\")\\n        self.processor.compute_all()\\n\\n    def test_build_index_returns_dict(self):\\n        \\\"\\\"\\\"Test that build_document_index returns a dict.\\\"\\\"\\\"\\n        index = build_document_index(self.processor.layers)\\n        self.assertIsInstance(index, dict)\\n\\n    def test_index_contains_terms(self):\\n        \\\"\\\"\\\"Test that index contains expected terms.\\\"\\\"\\\"\\n        index = build_document_index(self.processor.layers)\\n        self.assertIn('neural', index)\\n        self.assertIn('database', index)\\n\\n    def test_index_maps_to_docs(self):\\n        \\\"\\\"\\\"Test that index maps terms to documents.\\\"\\\"\\\"\\n        index = build_document_index(self.processor.layers)\\n\\n        # 'neural' should map to doc1\\n        self.assertIn('neural', index)\\n        self.assertIn('doc1', index['neural'])\\n\\n        # 'database' should map to doc2\\n        self.assertIn('database', index)\\n        self.assertIn('doc2', index['database'])\\n\\n    def test_index_values_are_scores(self):\\n        \\\"\\\"\\\"Test that index values are positive scores.\\\"\\\"\\\"\\n        index = build_document_index(self.processor.layers)\\n\\n        for term, doc_scores in index.items():\\n            for doc_id, score in doc_scores.items():\\n                self.assertGreater(score, 0)\\n\\n\\nclass TestSearchWithIndex(unittest.TestCase):\\n    \\\"\\\"\\\"Test the search_with_index function.\\\"\\\"\\\"\\n\\n    def setUp(self):\\n        \\\"\\\"\\\"Set up test processor and index.\\\"\\\"\\\"\\n        from cortical import CorticalTextProcessor\\n        self.processor = CorticalTextProcessor()\\n        self.processor.process_document(\\\"auth\\\", \\\"authentication login credentials\\\")\\n        self.processor.process_document(\\\"data\\\", \\\"database query optimization\\\")\\n        self.processor.process_document(\\\"network\\\", \\\"neural network training\\\")\\n        self.processor.compute_all()\\n        self.index = build_document_index(self.processor.layers)\\n\\n    def test_search_with_index_returns_results(self):\\n        \\\"\\\"\\\"Test that search_with_index returns results.\\\"\\\"\\\"\\n        results = search_with_index(\\n            \\\"authentication\\\",\\n            self.index,\\n            self.processor.tokenizer\\n        )\\n        self.assertIsInstance(results, list)\\n        self.assertGreater(len(results), 0)\\n\\n    def test_search_with_index_finds_relevant(self):\\n        \\\"\\\"\\\"Test that search_with_index finds relevant document.\\\"\\\"\\\"\\n        results = search_with_index(\\n            \\\"authentication login\\\",\\n            self.index,\\n            self.processor.tokenizer\\n        )\\n        doc_ids = [r[0] for r in results]\\n        self.assertIn('auth', doc_ids)\\n\\n    def test_search_with_index_respects_top_n(self):\\n        \\\"\\\"\\\"Test that search_with_index respects top_n.\\\"\\\"\\\"\\n        results = search_with_index(\\n            \\\"network\\\",\\n            self.index,\\n            self.processor.tokenizer,\\n            top_n=1\\n        )\\n        self.assertLessEqual(len(results), 1)\\n\\n    def test_search_with_index_empty_query(self):\\n        \\\"\\\"\\\"Test search_with_index with empty query.\\\"\\\"\\\"\\n        results = search_with_index(\\n            \\\"\\\",\\n            self.index,\\n            self.processor.tokenizer\\n        )\\n        self.assertEqual(results, [])\\n\\n    def test_search_with_index_no_matches(self):\\n        \\\"\\\"\\\"Test search_with_index with no matching terms.\\\"\\\"\\\"\\n        results = search_with_index(\\n            \\\"xyznonexistent\\\",\\n            self.index,\\n            self.processor.tokenizer\\n        )\\n        self.assertEqual(results, [])\\n\\n\\nclass TestProcessorIntegration(unittest.TestCase):\\n    \\\"\\\"\\\"Test query optimization integration with processor.\\\"\\\"\\\"\\n\\n    def setUp(self):\\n        \\\"\\\"\\\"Set up test processor.\\\"\\\"\\\"\\n        from cortical import CorticalTextProcessor\\n        self.processor = CorticalTextProcessor()\\n        self.processor.process_document(\\\"auth\\\", \\\"\\\"\\\"\\n            Authentication module handles user login and session management.\\n            Validates credentials and issues tokens.\\n        \\\"\\\"\\\")\\n        self.processor.process_document(\\\"data\\\", \\\"\\\"\\\"\\n            Data processing module fetches records from database.\\n            Transforms and validates data for export.\\n        \\\"\\\"\\\")\\n        self.processor.compute_all()\\n\\n    def test_processor_fast_find_documents(self):\\n        \\\"\\\"\\\"Test processor fast_find_documents method.\\\"\\\"\\\"\\n        results = self.processor.fast_find_documents(\\\"authentication\\\")\\n        self.assertIsInstance(results, list)\\n        if results:\\n            doc_ids = [r[0] for r in results]\\n            self.assertIn('auth', doc_ids)\\n\\n    def test_processor_build_search_index(self):\\n        \\\"\\\"\\\"Test processor build_search_index method.\\\"\\\"\\\"\\n        index = self.processor.build_search_index()\\n        self.assertIsInstance(index, dict)\\n        self.assertGreater(len(index), 0)\\n\\n    def test_processor_search_with_index(self):\\n        \\\"\\\"\\\"Test processor search_with_index method.\\\"\\\"\\\"\\n        index = self.processor.build_search_index()\\n        results = self.processor.search_with_index(\\\"database\\\", index)\\n        self.assertIsInstance(results, list)\\n        if results:\\n            doc_ids = [r[0] for r in results]\\n            self.assertIn('data', doc_ids)\\n\\n    def test_fast_vs_regular_same_results(self):\\n        \\\"\\\"\\\"Test that fast and regular search return similar results.\\\"\\\"\\\"\\n        query = \\\"authentication login\\\"\\n\\n        regular_results = self.processor.find_documents_for_query(query)\\n        fast_results = self.processor.fast_find_documents(query)\\n\\n        # Both should find 'auth' as top result\\n        if regular_results and fast_results:\\n            self.assertEqual(regular_results[0][0], fast_results[0][0])\\n\\n    def test_index_search_reusable(self):\\n        \\\"\\\"\\\"Test that built index can be reused for multiple queries.\\\"\\\"\\\"\\n        index = self.processor.build_search_index()\\n\\n        results1 = self.processor.search_with_index(\\\"authentication\\\", index)\\n        results2 = self.processor.search_with_index(\\\"database\\\", index)\\n\\n        # Should return different results for different queries\\n        if results1 and results2:\\n            self.assertNotEqual(results1[0][0], results2[0][0])\\n\\n\\nif __name__ == '__main__':\\n    unittest.main()\\n\",",
        "      \"mtime\": 1765392877.0",
        "    },",
        "    {",
        "      \"op\": \"add\",",
        "      \"doc_id\": \"cortical/fingerprint.py\",",
        "      \"content\": \"\\\"\\\"\\\"\\nFingerprint Module\\n==================\\n\\nSemantic fingerprinting for code comparison and similarity analysis.\\n\\nA fingerprint is an interpretable representation of a text's semantic\\ncontent, including term weights, concept memberships, and relations.\\nFingerprints can be compared to find similar code blocks or to explain\\nwhy two pieces of code are related.\\n\\\"\\\"\\\"\\n\\nfrom typing import Dict, List, Tuple, Optional, TypedDict, Any\\nfrom collections import defaultdict\\nimport math\\n\\nfrom .layers import CorticalLayer, HierarchicalLayer\\nfrom .tokenizer import Tokenizer\\nfrom .code_concepts import get_concept_group\\n\\n\\nclass SemanticFingerprint(TypedDict):\\n    \\\"\\\"\\\"Structured representation of a text's semantic fingerprint.\\\"\\\"\\\"\\n    terms: Dict[str, float]           # Term -> TF-IDF weight\\n    concepts: Dict[str, float]        # Concept group -> coverage score\\n    bigrams: Dict[str, float]         # Bigram -> weight\\n    top_terms: List[Tuple[str, float]]  # Top N terms by weight\\n    term_count: int                    # Total unique terms\\n    raw_text_hash: int                 # Hash of original text for identity check\\n\\n\\ndef compute_fingerprint(\\n    text: str,\\n    tokenizer: Tokenizer,\\n    layers: Optional[Dict[CorticalLayer, HierarchicalLayer]] = None,\\n    top_n: int = 20\\n) -> SemanticFingerprint:\\n    \\\"\\\"\\\"\\n    Compute the semantic fingerprint of a text.\\n\\n    The fingerprint captures the semantic essence of the text in an\\n    interpretable format that can be compared with other fingerprints.\\n\\n    Args:\\n        text: Input text to fingerprint\\n        tokenizer: Tokenizer instance\\n        layers: Optional corpus layers for TF-IDF weighting\\n        top_n: Number of top terms to include\\n\\n    Returns:\\n        SemanticFingerprint with terms, concepts, bigrams, and metadata\\n    \\\"\\\"\\\"\\n    # Tokenize\\n    tokens = tokenizer.tokenize(text)\\n    bigrams = tokenizer.extract_ngrams(tokens, n=2)\\n\\n    # Compute term frequencies\\n    term_freq: Dict[str, int] = defaultdict(int)\\n    for token in tokens:\\n        term_freq[token] += 1\\n\\n    # Compute bigram frequencies\\n    bigram_freq: Dict[str, int] = defaultdict(int)\\n    for bigram in bigrams:\\n        bigram_freq[bigram] += 1\\n\\n    # Normalize to TF weights (or use corpus TF-IDF if available)\\n    total_terms = len(tokens) if tokens else 1\\n    term_weights: Dict[str, float] = {}\\n\\n    for term, freq in term_freq.items():\\n        tf = freq / total_terms\\n\\n        # If we have corpus layers, use IDF weighting\\n        if layers:\\n            layer0 = layers.get(CorticalLayer.TOKENS)\\n            if layer0:\\n                col = layer0.get_minicolumn(term)\\n                if col and col.tfidf > 0:\\n                    # Use corpus TF-IDF as weight\\n                    term_weights[term] = tf * col.tfidf\\n                else:\\n                    term_weights[term] = tf\\n            else:\\n                term_weights[term] = tf\\n        else:\\n            term_weights[term] = tf\\n\\n    # Normalize bigram weights\\n    total_bigrams = len(bigrams) if bigrams else 1\\n    bigram_weights: Dict[str, float] = {}\\n    for bigram, freq in bigram_freq.items():\\n        bigram_weights[bigram] = freq / total_bigrams\\n\\n    # Compute concept coverage\\n    concept_scores: Dict[str, float] = defaultdict(float)\\n    for term, weight in term_weights.items():\\n        groups = get_concept_group(term)\\n        for group in groups:\\n            concept_scores[group] += weight\\n\\n    # Get top terms\\n    sorted_terms = sorted(term_weights.items(), key=lambda x: x[1], reverse=True)\\n    top_terms = sorted_terms[:top_n]\\n\\n    return SemanticFingerprint(\\n        terms=term_weights,\\n        concepts=dict(concept_scores),\\n        bigrams=bigram_weights,\\n        top_terms=top_terms,\\n        term_count=len(term_weights),\\n        raw_text_hash=hash(text)\\n    )\\n\\n\\ndef compare_fingerprints(\\n    fp1: SemanticFingerprint,\\n    fp2: SemanticFingerprint\\n) -> Dict[str, Any]:\\n    \\\"\\\"\\\"\\n    Compare two fingerprints and compute similarity metrics.\\n\\n    Args:\\n        fp1: First fingerprint\\n        fp2: Second fingerprint\\n\\n    Returns:\\n        Dict with similarity scores and shared terms\\n    \\\"\\\"\\\"\\n    # Check for identical text\\n    if fp1['raw_text_hash'] == fp2['raw_text_hash']:\\n        return {\\n            'identical': True,\\n            'term_similarity': 1.0,\\n            'concept_similarity': 1.0,\\n            'overall_similarity': 1.0,\\n            'shared_terms': list(fp1['terms'].keys()),\\n            'shared_concepts': list(fp1['concepts'].keys()),\\n        }\\n\\n    # Compute cosine similarity for terms\\n    term_sim = _cosine_similarity(fp1['terms'], fp2['terms'])\\n\\n    # Compute cosine similarity for concepts\\n    concept_sim = _cosine_similarity(fp1['concepts'], fp2['concepts'])\\n\\n    # Compute bigram similarity\\n    bigram_sim = _cosine_similarity(fp1['bigrams'], fp2['bigrams'])\\n\\n    # Find shared terms\\n    shared_terms = set(fp1['terms'].keys()) & set(fp2['terms'].keys())\\n\\n    # Find shared concepts\\n    shared_concepts = set(fp1['concepts'].keys()) & set(fp2['concepts'].keys())\\n\\n    # Compute overall similarity (weighted average)\\n    overall = 0.5 * term_sim + 0.3 * concept_sim + 0.2 * bigram_sim\\n\\n    return {\\n        'identical': False,\\n        'term_similarity': term_sim,\\n        'concept_similarity': concept_sim,\\n        'bigram_similarity': bigram_sim,\\n        'overall_similarity': overall,\\n        'shared_terms': sorted(shared_terms),\\n        'shared_concepts': sorted(shared_concepts),\\n        'unique_to_fp1': sorted(set(fp1['terms'].keys()) - shared_terms),\\n        'unique_to_fp2': sorted(set(fp2['terms'].keys()) - shared_terms),\\n    }\\n\\n\\ndef explain_fingerprint(\\n    fp: SemanticFingerprint,\\n    top_n: int = 10\\n) -> Dict[str, Any]:\\n    \\\"\\\"\\\"\\n    Generate a human-readable explanation of a fingerprint.\\n\\n    Args:\\n        fp: Fingerprint to explain\\n        top_n: Number of top items to include in explanation\\n\\n    Returns:\\n        Dict with explanation components\\n    \\\"\\\"\\\"\\n    # Get top terms\\n    top_terms = fp['top_terms'][:top_n]\\n\\n    # Get top concepts\\n    sorted_concepts = sorted(\\n        fp['concepts'].items(),\\n        key=lambda x: x[1],\\n        reverse=True\\n    )\\n    top_concepts = sorted_concepts[:top_n]\\n\\n    # Get top bigrams\\n    sorted_bigrams = sorted(\\n        fp['bigrams'].items(),\\n        key=lambda x: x[1],\\n        reverse=True\\n    )\\n    top_bigrams = sorted_bigrams[:top_n]\\n\\n    # Generate summary\\n    summary_parts = []\\n    if top_concepts:\\n        concept_names = [c[0] for c in top_concepts[:3]]\\n        summary_parts.append(f\\\"Concepts: {', '.join(concept_names)}\\\")\\n\\n    if top_terms:\\n        term_names = [t[0] for t in top_terms[:5]]\\n        summary_parts.append(f\\\"Key terms: {', '.join(term_names)}\\\")\\n\\n    return {\\n        'summary': ' | '.join(summary_parts) if summary_parts else 'No significant terms',\\n        'top_terms': top_terms,\\n        'top_concepts': top_concepts,\\n        'top_bigrams': top_bigrams,\\n        'term_count': fp['term_count'],\\n        'concept_coverage': len(fp['concepts']),\\n    }\\n\\n\\ndef explain_similarity(\\n    fp1: SemanticFingerprint,\\n    fp2: SemanticFingerprint,\\n    comparison: Optional[Dict[str, Any]] = None\\n) -> str:\\n    \\\"\\\"\\\"\\n    Generate a human-readable explanation of why two fingerprints are similar.\\n\\n    Args:\\n        fp1: First fingerprint\\n        fp2: Second fingerprint\\n        comparison: Optional pre-computed comparison result\\n\\n    Returns:\\n        Human-readable explanation string\\n    \\\"\\\"\\\"\\n    if comparison is None:\\n        comparison = compare_fingerprints(fp1, fp2)\\n\\n    if comparison['identical']:\\n        return \\\"These texts are identical.\\\"\\n\\n    lines = []\\n    similarity = comparison['overall_similarity']\\n\\n    if similarity > 0.8:\\n        lines.append(\\\"These texts are highly similar.\\\")\\n    elif similarity > 0.5:\\n        lines.append(\\\"These texts have moderate similarity.\\\")\\n    elif similarity > 0.2:\\n        lines.append(\\\"These texts have some common elements.\\\")\\n    else:\\n        lines.append(\\\"These texts are quite different.\\\")\\n\\n    # Explain shared concepts\\n    shared_concepts = comparison.get('shared_concepts', [])\\n    if shared_concepts:\\n        lines.append(f\\\"Shared concept domains: {', '.join(shared_concepts[:5])}\\\")\\n\\n    # Explain shared terms\\n    shared_terms = comparison.get('shared_terms', [])\\n    if shared_terms:\\n        # Get top shared terms by combined weight\\n        term_importance = []\\n        for term in shared_terms:\\n            weight = fp1['terms'].get(term, 0) + fp2['terms'].get(term, 0)\\n            term_importance.append((term, weight))\\n        term_importance.sort(key=lambda x: x[1], reverse=True)\\n        top_shared = [t[0] for t in term_importance[:5]]\\n        lines.append(f\\\"Key shared terms: {', '.join(top_shared)}\\\")\\n\\n    # Note differences\\n    unique1 = comparison.get('unique_to_fp1', [])\\n    unique2 = comparison.get('unique_to_fp2', [])\\n    if unique1 or unique2:\\n        lines.append(f\\\"First text has {len(unique1)} unique terms, second has {len(unique2)}.\\\")\\n\\n    return '\\\\n'.join(lines)\\n\\n\\ndef _cosine_similarity(vec1: Dict[str, float], vec2: Dict[str, float]) -> float:\\n    \\\"\\\"\\\"\\n    Compute cosine similarity between two sparse vectors.\\n\\n    Args:\\n        vec1: First vector as {dimension: value} dict\\n        vec2: Second vector as {dimension: value} dict\\n\\n    Returns:\\n        Cosine similarity in range [0, 1]\\n    \\\"\\\"\\\"\\n    if not vec1 or not vec2:\\n        return 0.0\\n\\n    # Find common dimensions\\n    common_keys = set(vec1.keys()) & set(vec2.keys())\\n\\n    if not common_keys:\\n        return 0.0\\n\\n    # Compute dot product\\n    dot_product = sum(vec1[k] * vec2[k] for k in common_keys)\\n\\n    # Compute magnitudes\\n    mag1 = math.sqrt(sum(v * v for v in vec1.values()))\\n    mag2 = math.sqrt(sum(v * v for v in vec2.values()))\\n\\n    if mag1 == 0 or mag2 == 0:\\n        return 0.0\\n\\n    return dot_product / (mag1 * mag2)\\n\",",
        "      \"mtime\": 1765392877.0",
        "    },",
        "    {",
        "      \"op\": \"add\",",
        "      \"doc_id\": \"KNOWLEDGE_TRANSFER.md\",",
        "      \"content\": \"# Knowledge Transfer Document: Cortical Text Processor\\n\\n**Document Version:** 1.0\\n**Date:** 2025-12-09\\n**Author:** Claude (Opus 4)\\n**Project Version:** 2.0.0\\n\\n---\\n\\n## Table of Contents\\n\\n1. [Executive Summary](#1-executive-summary)\\n2. [Project Context and Goals](#2-project-context-and-goals)\\n3. [Technical Architecture](#3-technical-architecture)\\n4. [Key Data Structures](#4-key-data-structures)\\n5. [Processing Pipeline](#5-processing-pipeline)\\n6. [Recent Development Work](#6-recent-development-work)\\n7. [Testing Strategy](#7-testing-strategy)\\n8. [Known Issues and Future Work](#8-known-issues-and-future-work)\\n9. [Development Workflow](#9-development-workflow)\\n10. [Quick Reference](#10-quick-reference)\\n\\n---\\n\\n## 1. Executive Summary\\n\\nThe Cortical Text Processor is a biologically-inspired NLP library that models text processing on the hierarchical structure of the human neocortex. It provides semantic analysis, document retrieval, and knowledge gap detection with **zero external dependencies**.\\n\\n### Key Statistics\\n\\n| Metric | Value |\\n|--------|-------|\\n| Library Version | 2.0.0 |\\n| Source Files | 11 Python modules |\\n| Lines of Code | ~4,300 (including tests) |\\n| Test Count | 109 tests |\\n| Test Coverage | Core functionality covered |\\n| Sample Documents | 44 documents |\\n| External Dependencies | None |\\n\\n### Project Health\\n\\n- All 109 unit tests passing\\n- 6 of 7 identified bugs fixed\\n- Demo produces 90.1% quality score\\n- Documentation complete (README, CLAUDE.md, CODE_REVIEW.md)\\n\\n---\\n\\n## 2. Project Context and Goals\\n\\n### Original Vision\\n\\nThe project implements a hierarchical text processing system inspired by neuroscience:\\n- **Visual Cortex Analogy**: Just as the visual cortex processes images through layers (V1→V2→V4→IT), this system processes text through hierarchical layers (Tokens→Bigrams→Concepts→Documents)\\n- **Cortical Columns**: The core data structure, `Minicolumn`, models cortical minicolumns that respond to specific features\\n- **Lateral Connections**: Words/concepts form associations through co-occurrence, similar to lateral inhibition and excitation in the brain\\n\\n### Design Principles\\n\\n1. **Zero Dependencies**: Pure Python stdlib only\\n2. **Educational Clarity**: Code explains biological analogies\\n3. **Modular Architecture**: Each module has single responsibility\\n4. **Self-Contained Semantics**: Derives meaning from corpus, not external knowledge bases\\n\\n---\\n\\n## 3. Technical Architecture\\n\\n### Module Dependency Graph\\n\\n```\\n                    ┌─────────────────┐\\n                    │   processor.py  │ (Orchestrator)\\n                    └────────┬────────┘\\n           ┌─────────────────┼─────────────────┐\\n           │                 │                 │\\n    ┌──────▼──────┐   ┌──────▼──────┐   ┌──────▼──────┐\\n    │ tokenizer.py │   │  layers.py  │   │ analysis.py │\\n    └─────────────┘   └──────┬──────┘   └─────────────┘\\n                             │\\n                      ┌──────▼──────┐\\n                      │ minicolumn.py│\\n                      └─────────────┘\\n                             │\\n    ┌────────────┬───────────┼───────────┬────────────┐\\n    │            │           │           │            │\\n┌───▼───┐   ┌────▼────┐ ┌────▼────┐ ┌────▼────┐ ┌─────▼─────┐\\n│query.py│   │semantics│ │embeddings│ │ gaps.py │ │persistence│\\n└───────┘   └─────────┘ └─────────┘ └─────────┘ └───────────┘\\n```\\n\\n### Layer Architecture\\n\\n| Layer | Type | Content | Purpose |\\n|-------|------|---------|---------|\\n| 0 | TOKENS | Individual words | Feature detection |\\n| 1 | BIGRAMS | Word pairs | Pattern recognition |\\n| 2 | CONCEPTS | Term clusters | Semantic grouping |\\n| 3 | DOCUMENTS | Full documents | Object recognition |\\n\\n### Core Classes\\n\\n1. **CorticalTextProcessor** (`processor.py:31`)\\n   - Main facade class\\n   - Coordinates all processing\\n   - Entry point for all operations\\n\\n2. **HierarchicalLayer** (`layers.py:54`)\\n   - Manages minicolumns at a hierarchy level\\n   - O(1) lookup via `_id_index` (recently added)\\n   - Statistics computation\\n\\n3. **Minicolumn** (`minicolumn.py:22`)\\n   - Core data structure\\n   - Uses `__slots__` for memory efficiency\\n   - Tracks: content, connections, scores, document associations\\n\\n---\\n\\n## 4. Key Data Structures\\n\\n### Minicolumn Fields\\n\\n```python\\nclass Minicolumn:\\n    __slots__ = [\\n        'content',              # str: The term/phrase\\n        'id',                   # str: Unique identifier\\n        'layer',                # CorticalLayer: Hierarchy level\\n        'activation',           # float: Current activation level\\n        'lateral_connections',  # Dict[str, float]: Term→Weight\\n        'feedforward_children', # List[str]: Lower-level IDs\\n        'feedback_parents',     # List[str]: Higher-level IDs\\n        'document_ids',         # Set[str]: Documents containing this term\\n        'doc_occurrence_counts',# Dict[str, int]: Per-doc term frequency (NEW)\\n        'metadata',             # Dict: Arbitrary metadata\\n        'pagerank',             # float: Importance score\\n        'tfidf',                # float: Global TF-IDF\\n        'tfidf_per_doc',        # Dict[str, float]: Per-document TF-IDF\\n        'cluster_id',           # Optional[int]: Cluster assignment\\n        'embedding',            # Optional[List[float]]: Vector representation\\n    ]\\n```\\n\\n### HierarchicalLayer Index\\n\\n```python\\nclass HierarchicalLayer:\\n    minicolumns: Dict[str, Minicolumn]  # content → Minicolumn\\n    _id_index: Dict[str, str]           # id → content (NEW: O(1) lookup)\\n\\n    def get_by_id(self, minicolumn_id: str) -> Optional[Minicolumn]:\\n        \\\"\\\"\\\"O(1) lookup by ID - added during bug fixes\\\"\\\"\\\"\\n```\\n\\n---\\n\\n## 5. Processing Pipeline\\n\\n### Standard Processing Flow\\n\\n```python\\n# 1. Initialize\\nprocessor = CorticalTextProcessor()\\n\\n# 2. Ingest Documents\\nprocessor.process_document(\\\"doc_id\\\", \\\"content...\\\")\\n# Creates: Tokens → Bigrams → Lateral connections\\n\\n# 3. Build Network (or use compute_all())\\nprocessor.propagate_activation()    # Spread activation through layers\\nprocessor.compute_importance()      # PageRank scoring\\nprocessor.compute_tfidf()          # TF-IDF weighting\\nprocessor.build_concept_clusters() # Semantic clustering\\nprocessor.compute_document_connections()  # Doc-doc similarity\\n\\n# 4. Enhance with Semantics (optional)\\nprocessor.extract_corpus_semantics()  # PMI-based relations\\nprocessor.retrofit_connections()      # Blend semantic weights\\n\\n# 5. Compute Embeddings (optional)\\nprocessor.compute_graph_embeddings(dimensions=32, method='adjacency')\\nprocessor.retrofit_embeddings()       # Improve with semantics\\n\\n# 6. Query\\nresults = processor.find_documents_for_query(\\\"search terms\\\")\\nexpanded = processor.expand_query(\\\"term\\\")\\nrelated = processor.find_related_documents(\\\"doc_id\\\")\\n\\n# 7. Analysis\\ngaps = processor.analyze_knowledge_gaps()\\nanomalies = processor.detect_anomalies(threshold=0.1)\\nhealth = processor.compute_corpus_health()\\n```\\n\\n### compute_all() Convenience Method\\n\\nThe `compute_all(verbose=True)` method runs steps 3-6 in optimal order:\\n1. propagate_activation()\\n2. compute_importance()\\n3. compute_tfidf()\\n4. extract_corpus_semantics()\\n5. retrofit_connections()\\n6. build_concept_clusters()\\n7. compute_document_connections()\\n8. compute_graph_embeddings()\\n9. retrofit_embeddings()\\n\\n---\\n\\n## 6. Recent Development Work\\n\\n### Code Review (2025-12-09)\\n\\nA comprehensive code review was performed, resulting in:\\n- CODE_REVIEW.md documenting findings\\n- TASK_LIST.md tracking required fixes\\n- CLAUDE.md project guide\\n\\n### Bug Fixes Applied\\n\\n| Issue | File | Fix |\\n|-------|------|-----|\\n| TF-IDF always 1 | analysis.py:131 | Added `doc_occurrence_counts` field |\\n| O(n) ID lookups | layers.py | Added `_id_index` + `get_by_id()` |\\n| Type error `any` | semantics.py | Changed to `Any` |\\n| Unused import | analysis.py | Removed `Counter` |\\n| Verbose ignored | persistence.py | Added verbose param |\\n\\n### Tests Added\\n\\n70 new tests across 5 test files:\\n\\n- **test_analysis.py**: 17 tests (PageRank, TF-IDF, clustering)\\n- **test_embeddings.py**: 15 tests (all embedding methods)\\n- **test_semantics.py**: 12 tests (PMI, retrofitting)\\n- **test_gaps.py**: 15 tests (gap detection, anomalies)\\n- **test_persistence.py**: 12 tests (save/load, export)\\n\\n### Sample Documents Added\\n\\n7 new domain documents for diverse corpus testing:\\n- financial_analysis.txt\\n- elliot_wave_theory.txt\\n- candlestick_patterns.txt\\n- data_structures.txt\\n- computational_theory.txt\\n- compilers.txt\\n- neocortex.txt\\n\\n---\\n\\n## 7. Testing Strategy\\n\\n### Running Tests\\n\\n```bash\\n# All tests\\npython -m unittest discover -s tests -v\\n\\n# Specific test file\\npython -m unittest tests.test_analysis -v\\n\\n# Single test\\npython -m unittest tests.test_analysis.TestPageRank.test_pagerank_convergence\\n```\\n\\n### Test Organization\\n\\n```\\ntests/\\n├── test_tokenizer.py    # Tokenization, stemming, stop words\\n├── test_processor.py    # Document processing, main workflow\\n├── test_layers.py       # Layer CRUD, statistics\\n├── test_analysis.py     # PageRank, TF-IDF, clustering\\n├── test_embeddings.py   # All embedding methods\\n├── test_semantics.py    # PMI, retrofitting\\n├── test_gaps.py         # Gap detection, anomalies\\n└── test_persistence.py  # Save/load, JSON export\\n```\\n\\n### Test Coverage Summary\\n\\n| Module | Status |\\n|--------|--------|\\n| tokenizer.py | Covered |\\n| processor.py | Covered |\\n| minicolumn.py | Covered |\\n| layers.py | Covered |\\n| analysis.py | Covered |\\n| semantics.py | Covered |\\n| embeddings.py | Covered |\\n| gaps.py | Covered |\\n| persistence.py | Covered |\\n| query.py | Partially covered (via processor tests) |\\n\\n---\\n\\n## 8. Known Issues and Future Work\\n\\n### Outstanding Issue\\n\\n**Magic Numbers in gaps.py**\\n\\n```python\\n# Line 62: Isolation threshold\\navg_sim < 0.02\\n\\n# Line 76: Weak topic threshold\\ntfidf > 0.005\\n\\n# Line 99: Bridge opportunity range\\n0.005 < sim < 0.03\\n```\\n\\n**Recommendation**: Make these configurable or document rationale.\\n\\n### Potential Enhancements\\n\\n1. **Performance**\\n   - Sparse similarity computation for large corpora\\n   - Streaming/incremental document processing\\n   - Progress callbacks for long operations\\n\\n2. **Features**\\n   - More embedding methods (word2vec-style)\\n   - Configurable threshold parameters\\n   - Document summarization improvements\\n\\n3. **Security**\\n   - JSON-based persistence as pickle alternative\\n   - Path validation for file operations\\n\\n---\\n\\n## 9. Development Workflow\\n\\n### Making Changes\\n\\n1. **Read existing code** before modifying\\n2. **Run tests** before and after changes\\n3. **Follow conventions**:\\n   - Type hints on all functions\\n   - Google-style docstrings\\n   - 100-char line limit\\n\\n### Adding New Features\\n\\n```python\\n# 1. Add to appropriate module\\ndef new_feature(self, param: str) -> List[str]:\\n    \\\"\\\"\\\"Short description.\\n\\n    Args:\\n        param: Description of parameter.\\n\\n    Returns:\\n        Description of return value.\\n    \\\"\\\"\\\"\\n    pass\\n\\n# 2. Export via processor.py if user-facing\\n\\n# 3. Add tests in tests/test_<module>.py\\n\\n# 4. Update CLAUDE.md if significant\\n```\\n\\n### Commit Message Format\\n\\n```\\n<type>: <description>\\n\\nTypes: Add, Fix, Update, Remove, Refactor\\nExample: Fix per-document TF-IDF calculation bug\\n```\\n\\n---\\n\\n## 10. Quick Reference\\n\\n### File Locations\\n\\n| Purpose | File |\\n|---------|------|\\n| Main entry point | cortical/processor.py |\\n| Core data structure | cortical/minicolumn.py |\\n| Layer management | cortical/layers.py |\\n| Text tokenization | cortical/tokenizer.py |\\n| PageRank/TF-IDF | cortical/analysis.py |\\n| Semantic extraction | cortical/semantics.py |\\n| Graph embeddings | cortical/embeddings.py |\\n| Search/retrieval | cortical/query.py |\\n| Gap detection | cortical/gaps.py |\\n| Save/load | cortical/persistence.py |\\n\\n### Common Operations\\n\\n```python\\n# Load saved model\\nprocessor = CorticalTextProcessor.load(\\\"model.pkl\\\")\\n\\n# Add documents\\nprocessor.process_document(\\\"id\\\", \\\"text\\\")\\nprocessor.process_documents_from_directory(\\\"./samples\\\")\\n\\n# Compute everything\\nprocessor.compute_all(verbose=True)\\n\\n# Search\\nresults = processor.find_documents_for_query(\\\"query\\\", top_n=5)\\n\\n# Expand query with synonyms\\nexpanded = processor.expand_query(\\\"term\\\", max_expansions=10)\\n\\n# Find similar terms\\nsimilar = processor.find_similar_by_embedding(\\\"term\\\", top_n=5)\\n\\n# Corpus health\\nhealth = processor.compute_corpus_health()\\ngaps = processor.analyze_knowledge_gaps()\\n\\n# Export for visualization\\nprocessor.export_graph_json(\\\"graph.json\\\", verbose=False)\\nprocessor.export_embeddings_json(\\\"embeddings.json\\\")\\n```\\n\\n### Key Metrics from Demo\\n\\n```\\nDocuments: 44\\nToken minicolumns: ~3,350\\nBigram minicolumns: ~6,530\\nLateral connections: ~38,200\\nQuality score: 90.1%\\n```\\n\\n---\\n\\n## Related Documentation\\n\\n- **README.md**: User-facing documentation and installation\\n- **CLAUDE.md**: Project guide for Claude Code\\n- **CODE_REVIEW.md**: Detailed code review findings\\n- **TASK_LIST.md**: Bug fix tracking and status\\n\\n---\\n\\n*Document prepared for knowledge transfer on 2025-12-09*\\n\",",
        "      \"mtime\": 1765375584.0",
        "    },",
        "    {",
        "      \"op\": \"add\",",
        "      \"doc_id\": \"docs/query-guide.md\",",
        "      \"content\": \"# Query Guide\\n\\nA comprehensive guide to formulating effective search queries and understanding how the query system works internally.\\n\\n---\\n\\n## Table of Contents\\n\\n1. [How Queries Work Internally](#how-queries-work-internally)\\n2. [Query Syntax and Patterns](#query-syntax-and-patterns)\\n3. [Understanding Query Expansion](#understanding-query-expansion)\\n4. [Single-Word vs Multi-Word Queries](#single-word-vs-multi-word-queries)\\n5. [Code Patterns vs Concept Searches](#code-patterns-vs-concept-searches)\\n6. [Intent-Based Queries](#intent-based-queries)\\n7. [Interpreting Relevance Scores](#interpreting-relevance-scores)\\n8. [When Queries Fail](#when-queries-fail)\\n9. [Advanced Techniques](#advanced-techniques)\\n\\n---\\n\\n## How Queries Work Internally\\n\\n### The Query Pipeline\\n\\nWhen you submit a query, the system performs a multi-stage pipeline:\\n\\n```\\nQuery Text\\n    |\\n[1. Tokenization] -> Split into words, remove stop words\\n    |\\n[2. Term Matching] -> Look up terms in token layer\\n    |\\n[3. Expansion] -> Add related terms via lateral connections\\n    |\\n[4. Document Scoring] -> TF-IDF weighting\\n    |\\n[5. Ranking] -> Sort by relevance score\\n    |\\nResults\\n```\\n\\n### Stage 1: Tokenization\\n\\nYour query is tokenized using the same rules as document processing:\\n\\n```python\\n# \\\"neural networks process data\\\" becomes:\\n[\\\"neural\\\", \\\"networks\\\", \\\"process\\\", \\\"data\\\"]\\n\\n# Stop words are removed: \\\"the\\\", \\\"a\\\", \\\"in\\\", \\\"of\\\", \\\"is\\\"\\n# Short words (< 3 characters) are removed\\n```\\n\\n**Key points:**\\n- Tokenization is **case-insensitive**\\n- Punctuation is removed\\n- Words shorter than 3 characters are filtered\\n\\n### Stage 2: Term Matching\\n\\nThe system looks up each query token in Layer 0:\\n\\n```\\nToken         Found?   Status\\n\\\"neural\\\"      YES      Exact match in corpus\\n\\\"networks\\\"    YES      Exact match in corpus\\n```\\n\\nIf a token doesn't exist, the system tries **word variants**:\\n- Stemmed versions\\n- Plural forms\\n- Common aliases\\n\\n### Stage 3: Expansion\\n\\nThe query is expanded using three methods:\\n\\n**Method A: Lateral Connections (Default)**\\n- Terms co-occurring with query terms\\n- Weights: connection strength x neighbor PageRank x 0.6\\n\\n**Method B: Concept Clustering**\\n- Terms in same semantic cluster\\n- Weights: concept PageRank x member PageRank x 0.4\\n\\n**Method C: Code Concepts (Optional)**\\n- Programming synonyms (get/fetch/load)\\n- Only enabled with `use_code_concepts=True`\\n\\n### Stage 4: Document Scoring\\n\\n```\\ndoc_score = sum(term_weight x tfidf_per_doc)\\n            for each term in expanded_query\\n\\nwhere:\\n  term_weight = original terms: 1.0\\n              = expanded terms: 0.3-0.8\\n```\\n\\n---\\n\\n## Query Syntax and Patterns\\n\\nThe system uses **simple, natural language-based syntax**. No special operators needed.\\n\\n### Basic Patterns\\n\\n| Pattern | Example | Effect |\\n|---------|---------|--------|\\n| **Single word** | `neural` | Search term and related concepts |\\n| **Multi-word** | `neural networks` | All words must match (AND logic) |\\n| **Question words** | `where authentication` | Intent-based search |\\n| **Action verbs** | `how validate input` | Parse action + subject |\\n\\n### What Doesn't Work\\n\\n```python\\n# NOT supported:\\n\\\"neural\\\" OR \\\"learning\\\"         # No boolean operators\\n\\\"neural*\\\"                      # No wildcards\\n\\\"exact phrase match\\\"           # No phrase searching\\n```\\n\\n### How Multi-Word Queries Work\\n\\nMulti-word queries use **AND logic** at document level:\\n\\n```\\nQuery: \\\"neural networks\\\"\\n\\nStep 1: Find docs with \\\"neural\\\"   -> [doc1, doc3, doc5]\\nStep 2: Find docs with \\\"networks\\\" -> [doc1, doc3, doc6]\\nStep 3: Intersection              -> [doc1, doc3]\\nStep 4: Rank by combined score\\n```\\n\\n---\\n\\n## Understanding Query Expansion\\n\\nQuery expansion is **the core secret** to finding relevant results even when your query doesn't exactly match.\\n\\n### How Expansion Works\\n\\nGiven query `\\\"fetch user\\\"`:\\n\\n```\\nOriginal Terms (weight 1.0):\\n  - fetch\\n  - user\\n\\nLateral Connection Expansion:\\n\\nNeighbors of \\\"fetch\\\":\\n  - get: 0.45\\n  - load: 0.42\\n  - data: 0.38\\n\\nNeighbors of \\\"user\\\":\\n  - profile: 0.52\\n  - account: 0.48\\n  - authenticate: 0.35\\n\\nFinal Query Terms:\\n{\\n  \\\"fetch\\\": 1.0,        # Original\\n  \\\"user\\\": 1.0,         # Original\\n  \\\"get\\\": 0.45,         # Expansion\\n  \\\"profile\\\": 0.52,     # Expansion\\n  ...\\n}\\n```\\n\\n### Controlling Expansion\\n\\n```python\\n# With lateral connections only\\nresults = processor.find_documents_for_query(\\n    \\\"neural networks\\\",\\n    use_expansion=True,\\n    use_semantic=False\\n)\\n\\n# No expansion (exact match)\\nresults = processor.find_documents_for_query(\\n    \\\"neural networks\\\",\\n    use_expansion=False\\n)\\n\\n# Code-specific expansion\\nresults = processor.expand_query_for_code(\\\"fetch user credentials\\\")\\n```\\n\\n### Debugging Expansion\\n\\n```python\\nexpanded = processor.expand_query(\\\"neural networks\\\", max_expansions=10)\\n\\nfor term, weight in sorted(expanded.items(), key=lambda x: -x[1]):\\n    print(f\\\"  {term}: {weight:.3f}\\\")\\n```\\n\\n---\\n\\n## Single-Word vs Multi-Word Queries\\n\\n### Single-Word Queries\\n\\n**Advantages:**\\n- Faster execution\\n- Broader matching\\n- Better for exploratory search\\n\\n**Disadvantages:**\\n- May return less relevant results if term is ambiguous\\n\\n```python\\nQuery: \\\"learning\\\"\\n# Finds all documents with \\\"learning\\\" and related terms\\n```\\n\\n### Multi-Word Queries\\n\\n**Advantages:**\\n- More specific results (AND logic)\\n- Provides disambiguation context\\n\\n**Disadvantages:**\\n- Harder to match (all terms must exist)\\n\\n```python\\nQuery: \\\"machine learning\\\"\\n# Returns only docs with BOTH terms\\n```\\n\\n### Strategy: Combining Both\\n\\n```python\\n# Broad search first\\nbroad = processor.find_documents_for_query(\\\"learning\\\", top_n=20)\\n\\n# Narrow with multi-word\\nnarrow = processor.find_documents_for_query(\\\"machine learning\\\", top_n=5)\\n\\n# Use narrow if available, fall back to broad\\nresults = narrow if narrow else broad\\n```\\n\\n---\\n\\n## Code Patterns vs Concept Searches\\n\\n### Concept Searches (General Text)\\n\\nBest for finding semantic topics:\\n\\n```python\\nprocessor.find_documents_for_query(\\\"authentication\\\")\\nprocessor.find_documents_for_query(\\\"neural networks\\\")\\n```\\n\\nUses:\\n- Lateral connections\\n- Concept clusters\\n- Natural language semantics\\n\\n### Code Pattern Searches\\n\\nBest for finding implementations:\\n\\n```python\\nprocessor.expand_query_for_code(\\\"get user credentials\\\")\\nprocessor.expand_query_for_code(\\\"validate input\\\")\\n```\\n\\nUses:\\n- Code concept groups (get/fetch/load)\\n- Programming keywords\\n- Identifier splitting\\n\\n### When to Use Each\\n\\n| Type | Use Case | Method |\\n|------|----------|--------|\\n| **Concept** | Find ideas, topics | `find_documents_for_query()` |\\n| **Code** | Find implementations | `expand_query_for_code()` |\\n| **Intent** | Find by action | `search_by_intent()` |\\n| **Passage** | Find specific text | `find_passages_for_query()` |\\n\\n---\\n\\n## Intent-Based Queries\\n\\nIntent-based queries use **natural language patterns** to understand what you're looking for.\\n\\n### Supported Question Words\\n\\n| Word | Intent | Example |\\n|------|--------|---------|\\n| **where** | location | \\\"where do we handle authentication?\\\" |\\n| **how** | implementation | \\\"how does validation work?\\\" |\\n| **what** | definition | \\\"what is a concept cluster?\\\" |\\n| **why** | rationale | \\\"why do we use PageRank?\\\" |\\n| **when** | lifecycle | \\\"when do we compute TF-IDF?\\\" |\\n\\n### How Intent Parsing Works\\n\\n```\\nQuery: \\\"where do we handle authentication?\\\"\\n\\nStep 1: Detect \\\"where\\\" -> intent = \\\"location\\\"\\nStep 2: Extract content words -> handle, authentication\\nStep 3: Identify action verb -> \\\"handle\\\"\\nStep 4: Identify subject -> \\\"authentication\\\"\\nStep 5: Build expanded terms\\nStep 6: Search with weighted terms\\n```\\n\\n### Using Intent Queries\\n\\n```python\\nresults = processor.search_by_intent(\\\"where do we validate input?\\\", top_n=5)\\n\\nparsed = processor.parse_intent_query(\\\"how does PageRank work?\\\")\\n# {\\n#   'action': 'work',\\n#   'subject': 'pagerank',\\n#   'intent': 'implementation',\\n#   'expanded_terms': ['work', 'pagerank', 'rank', ...]\\n# }\\n```\\n\\n---\\n\\n## Interpreting Relevance Scores\\n\\n### Score Meaning\\n\\n```\\nScore > 0.80   Very relevant - high confidence match\\nScore 0.50-0.80  Relevant - good match\\nScore 0.25-0.50  Somewhat relevant - weak connection\\nScore < 0.25   Marginally relevant\\n```\\n\\n### How Scores Are Calculated\\n\\n```python\\n# TF-IDF Score:\\ntf_idf = (term_count_in_doc / total_terms) x log(total_docs / docs_with_term)\\n\\n# Query Score:\\ndoc_score = sum(term_weight x term_tfidf_per_doc)\\n```\\n\\n### Factors Affecting Scores\\n\\n1. **Term Frequency (TF):** More occurrences = higher score\\n2. **Inverse Document Frequency (IDF):** Rarer terms = higher weight\\n3. **Query Term Weight:** Original (1.0) vs expansion (0.3-0.6)\\n4. **Concept overlap:** Documents in same cluster score higher\\n\\n---\\n\\n## When Queries Fail\\n\\n### Problem 1: No Results Found\\n\\n**Diagnosis:**\\n```python\\nlayer0 = processor.get_layer(CorticalLayer.TOKENS)\\nfor term in processor.tokenizer.tokenize(query):\\n    if not layer0.get_minicolumn(term):\\n        print(f\\\"{term}: NOT FOUND\\\")\\n```\\n\\n**Solutions:**\\n1. Try variant forms: `\\\"getUserData\\\"` -> `\\\"get user data\\\"`\\n2. Enable code splitting in tokenizer\\n3. Use related concepts instead\\n\\n### Problem 2: Wrong Documents Returned\\n\\n**Diagnosis:**\\n```python\\nexpanded = processor.expand_query(\\\"authentication\\\")\\n# Check for unexpected expansion terms\\n```\\n\\n**Solutions:**\\n1. Use multi-word queries for specificity\\n2. Disable expansion: `use_expansion=False`\\n3. Use intent-based search\\n\\n### Problem 3: Missing Relevant Documents\\n\\n**Solutions:**\\n1. Enable semantic expansion:\\n   ```python\\n   processor.extract_corpus_semantics()\\n   results = processor.find_documents_for_query(\\n       query,\\n       use_semantic=True\\n   )\\n   ```\\n\\n2. Use multi-hop expansion:\\n   ```python\\n   expanded = processor.expand_query_multihop(query, max_hops=2)\\n   ```\\n\\n### Problem 4: Slow Queries\\n\\n**Solutions:**\\n1. Use `fast_find_documents()`\\n2. Pre-build search index\\n3. Use narrower queries\\n\\n---\\n\\n## Advanced Techniques\\n\\n### Technique 1: Multi-Hop Expansion\\n\\n```python\\nprocessor.extract_corpus_semantics()\\n\\nexpanded = processor.expand_query_multihop(\\n    \\\"neural\\\",\\n    max_hops=2,\\n    max_expansions=15\\n)\\n\\n# Hop 0: neural\\n# Hop 1: networks, learning, brain\\n# Hop 2: deep (via learning), cortex (via brain)\\n```\\n\\n### Technique 2: Passage Retrieval (RAG)\\n\\n```python\\nresults = processor.find_passages_for_query(\\n    \\\"neural network training\\\",\\n    top_n=5,\\n    chunk_size=512,\\n    overlap=128\\n)\\n\\nfor passage, doc_id, start, end, score in results:\\n    print(f\\\"[{doc_id}:{start}-{end}] Score: {score:.3f}\\\")\\n    print(passage)\\n```\\n\\n### Technique 3: Multi-Stage Ranking\\n\\n```python\\nresults = processor.multi_stage_rank(\\n    \\\"neural networks\\\",\\n    top_n=5,\\n    concept_boost=0.3\\n)\\n\\nfor passage, doc_id, start, end, score, stages in results:\\n    print(f\\\"Concept: {stages['concept_score']:.3f}\\\")\\n    print(f\\\"Document: {stages['doc_score']:.3f}\\\")\\n    print(f\\\"Passage: {stages['chunk_score']:.3f}\\\")\\n```\\n\\n### Technique 4: Batch Queries\\n\\n```python\\nqueries = [\\\"neural networks\\\", \\\"machine learning\\\", \\\"deep learning\\\"]\\nresults = processor.find_documents_batch(queries, top_n=5)\\n# ~2-3x faster for multiple queries\\n```\\n\\n---\\n\\n## Quick Reference\\n\\n### Common Methods\\n\\n```python\\n# Basic search\\nprocessor.find_documents_for_query(query, top_n=5)\\n\\n# Fast search (large corpora)\\nprocessor.fast_find_documents(query, top_n=5)\\n\\n# Intent-based\\nprocessor.search_by_intent(\\\"where do we...?\\\", top_n=5)\\n\\n# Passages (RAG)\\nprocessor.find_passages_for_query(query, top_n=5, chunk_size=512)\\n\\n# Code-specific\\nprocessor.expand_query_for_code(query)\\n\\n# Multi-hop\\nprocessor.expand_query_multihop(query, max_hops=2)\\n\\n# Batch queries\\nprocessor.find_documents_batch(queries, top_n=5)\\n\\n# Debug expansion\\nprocessor.expand_query(query, max_expansions=10)\\n```\\n\\n### Query Tips\\n\\n1. **Start simple** - Single keywords first\\n2. **Add specificity** - Multi-word if needed\\n3. **Use intent words** - \\\"where\\\", \\\"how\\\", \\\"what\\\"\\n4. **Check expansion** - See what terms are added\\n5. **Trust the system** - Expansion finds related terms\\n\\n---\\n\\n*For practical recipes, see [Cookbook](cookbook.md). For Claude-specific usage, see [Claude Usage Guide](claude-usage.md).*\\n\",",
        "      \"mtime\": 1765402097.0",
        "    },",
        "    {",
        "      \"op\": \"add\",",
        "      \"doc_id\": \"cortical/processor.py\",",
        "      \"content\": \"\\\"\\\"\\\"\\nCortical Text Processor - Main processor class that orchestrates all components.\\n\\\"\\\"\\\"\\n\\nimport os\\nimport re\\nfrom typing import Dict, List, Tuple, Optional, Any\\nimport copy\\nfrom collections import defaultdict\\n\\nfrom .tokenizer import Tokenizer\\nfrom .minicolumn import Minicolumn\\nfrom .layers import CorticalLayer, HierarchicalLayer\\nfrom . import analysis\\nfrom . import semantics\\nfrom . import embeddings as emb_module\\nfrom . import query as query_module\\nfrom . import gaps as gaps_module\\nfrom . import persistence\\nfrom . import fingerprint as fp_module\\n\\n\\nclass CorticalTextProcessor:\\n    \\\"\\\"\\\"Neocortex-inspired text processing system.\\\"\\\"\\\"\\n\\n    # Computation types for tracking staleness\\n    COMP_TFIDF = 'tfidf'\\n    COMP_PAGERANK = 'pagerank'\\n    COMP_ACTIVATION = 'activation'\\n    COMP_DOC_CONNECTIONS = 'doc_connections'\\n    COMP_BIGRAM_CONNECTIONS = 'bigram_connections'\\n    COMP_CONCEPTS = 'concepts'\\n    COMP_EMBEDDINGS = 'embeddings'\\n    COMP_SEMANTICS = 'semantics'\\n\\n    def __init__(self, tokenizer: Optional[Tokenizer] = None):\\n        self.tokenizer = tokenizer or Tokenizer()\\n        self.layers: Dict[CorticalLayer, HierarchicalLayer] = {\\n            CorticalLayer.TOKENS: HierarchicalLayer(CorticalLayer.TOKENS),\\n            CorticalLayer.BIGRAMS: HierarchicalLayer(CorticalLayer.BIGRAMS),\\n            CorticalLayer.CONCEPTS: HierarchicalLayer(CorticalLayer.CONCEPTS),\\n            CorticalLayer.DOCUMENTS: HierarchicalLayer(CorticalLayer.DOCUMENTS),\\n        }\\n        self.documents: Dict[str, str] = {}\\n        self.document_metadata: Dict[str, Dict[str, Any]] = {}\\n        self.embeddings: Dict[str, List[float]] = {}\\n        self.semantic_relations: List[Tuple[str, str, str, float]] = []\\n        # Track which computations are stale and need recomputation\\n        self._stale_computations: set = set()\\n        # LRU cache for query expansion results\\n        self._query_expansion_cache: Dict[str, Dict[str, float]] = {}\\n        self._query_cache_max_size: int = 100\\n\\n    def process_document(\\n        self,\\n        doc_id: str,\\n        content: str,\\n        metadata: Optional[Dict[str, Any]] = None\\n    ) -> Dict[str, int]:\\n        \\\"\\\"\\\"\\n        Process a document and add it to the corpus.\\n\\n        Args:\\n            doc_id: Unique identifier for the document\\n            content: Document text content\\n            metadata: Optional metadata dict (source, timestamp, author, etc.)\\n\\n        Returns:\\n            Dict with processing statistics (tokens, bigrams, unique_tokens)\\n\\n        Raises:\\n            ValueError: If doc_id or content is empty or not a string\\n        \\\"\\\"\\\"\\n        # Input validation\\n        if not isinstance(doc_id, str) or not doc_id:\\n            raise ValueError(\\\"doc_id must be a non-empty string\\\")\\n        if not isinstance(content, str):\\n            raise ValueError(\\\"content must be a string\\\")\\n        if not content.strip():\\n            raise ValueError(\\\"content must not be empty or whitespace-only\\\")\\n\\n        self.documents[doc_id] = content\\n\\n        # Store metadata if provided\\n        if metadata:\\n            self.document_metadata[doc_id] = metadata.copy()\\n        elif doc_id not in self.document_metadata:\\n            self.document_metadata[doc_id] = {}\\n\\n        tokens = self.tokenizer.tokenize(content)\\n        bigrams = self.tokenizer.extract_ngrams(tokens, n=2)\\n        \\n        layer0 = self.layers[CorticalLayer.TOKENS]\\n        layer1 = self.layers[CorticalLayer.BIGRAMS]\\n        layer3 = self.layers[CorticalLayer.DOCUMENTS]\\n        \\n        doc_col = layer3.get_or_create_minicolumn(doc_id)\\n        doc_col.occurrence_count += 1\\n        \\n        for token in tokens:\\n            col = layer0.get_or_create_minicolumn(token)\\n            col.occurrence_count += 1\\n            col.document_ids.add(doc_id)\\n            col.activation += 1.0\\n            # Weighted feedforward: document → token (weight by occurrence count)\\n            doc_col.add_feedforward_connection(col.id, 1.0)\\n            # Weighted feedback: token → document (weight by occurrence count)\\n            col.add_feedback_connection(doc_col.id, 1.0)\\n            # Track per-document occurrence count for accurate TF-IDF\\n            col.doc_occurrence_counts[doc_id] = col.doc_occurrence_counts.get(doc_id, 0) + 1\\n        \\n        for i, token in enumerate(tokens):\\n            col = layer0.get_minicolumn(token)\\n            if col:\\n                for j in range(max(0, i-3), min(len(tokens), i+4)):\\n                    if i != j:\\n                        other = layer0.get_minicolumn(tokens[j])\\n                        if other:\\n                            col.add_lateral_connection(other.id, 1.0)\\n        \\n        for bigram in bigrams:\\n            col = layer1.get_or_create_minicolumn(bigram)\\n            col.occurrence_count += 1\\n            col.document_ids.add(doc_id)\\n            col.activation += 1.0\\n            for part in bigram.split():\\n                token_col = layer0.get_minicolumn(part)\\n                if token_col:\\n                    # Weighted feedforward: bigram → tokens (weight 1.0 per occurrence)\\n                    col.add_feedforward_connection(token_col.id, 1.0)\\n                    # Weighted feedback: token → bigram (weight 1.0 per occurrence)\\n                    token_col.add_feedback_connection(col.id, 1.0)\\n\\n        # Mark all computations as stale since document corpus changed\\n        self._mark_all_stale()\\n\\n        return {'tokens': len(tokens), 'bigrams': len(bigrams), 'unique_tokens': len(set(tokens))}\\n\\n    def set_document_metadata(self, doc_id: str, **kwargs) -> None:\\n        \\\"\\\"\\\"\\n        Set or update metadata for a document.\\n\\n        Args:\\n            doc_id: Document identifier\\n            **kwargs: Metadata key-value pairs to set\\n\\n        Example:\\n            >>> processor.set_document_metadata(\\\"doc1\\\",\\n            ...     source=\\\"https://example.com\\\",\\n            ...     author=\\\"John Doe\\\",\\n            ...     timestamp=\\\"2025-12-09\\\"\\n            ... )\\n        \\\"\\\"\\\"\\n        if doc_id not in self.document_metadata:\\n            self.document_metadata[doc_id] = {}\\n        self.document_metadata[doc_id].update(kwargs)\\n\\n    def get_document_metadata(self, doc_id: str) -> Dict[str, Any]:\\n        \\\"\\\"\\\"\\n        Get metadata for a document.\\n\\n        Args:\\n            doc_id: Document identifier\\n\\n        Returns:\\n            Metadata dict (empty dict if no metadata set)\\n        \\\"\\\"\\\"\\n        return self.document_metadata.get(doc_id, {})\\n\\n    def get_all_document_metadata(self) -> Dict[str, Dict[str, Any]]:\\n        \\\"\\\"\\\"\\n        Get metadata for all documents.\\n\\n        Returns:\\n            Dict mapping doc_id to metadata dict (deep copy)\\n        \\\"\\\"\\\"\\n        return copy.deepcopy(self.document_metadata)\\n\\n    def _mark_all_stale(self) -> None:\\n        \\\"\\\"\\\"Mark all computations as stale (needing recomputation).\\\"\\\"\\\"\\n        self._stale_computations = {\\n            self.COMP_TFIDF,\\n            self.COMP_PAGERANK,\\n            self.COMP_ACTIVATION,\\n            self.COMP_DOC_CONNECTIONS,\\n            self.COMP_BIGRAM_CONNECTIONS,\\n            self.COMP_CONCEPTS,\\n            self.COMP_EMBEDDINGS,\\n            self.COMP_SEMANTICS,\\n        }\\n\\n    def _mark_fresh(self, *computation_types: str) -> None:\\n        \\\"\\\"\\\"Mark specified computations as fresh (up-to-date).\\\"\\\"\\\"\\n        for comp in computation_types:\\n            self._stale_computations.discard(comp)\\n\\n    def is_stale(self, computation_type: str) -> bool:\\n        \\\"\\\"\\\"\\n        Check if a specific computation is stale.\\n\\n        Args:\\n            computation_type: One of COMP_TFIDF, COMP_PAGERANK, etc.\\n\\n        Returns:\\n            True if the computation needs to be run again\\n        \\\"\\\"\\\"\\n        return computation_type in self._stale_computations\\n\\n    def get_stale_computations(self) -> set:\\n        \\\"\\\"\\\"\\n        Get the set of computations that are currently stale.\\n\\n        Returns:\\n            Set of computation type strings that need recomputation\\n        \\\"\\\"\\\"\\n        return self._stale_computations.copy()\\n\\n    def add_document_incremental(\\n        self,\\n        doc_id: str,\\n        content: str,\\n        metadata: Optional[Dict[str, Any]] = None,\\n        recompute: str = 'tfidf'\\n    ) -> Dict[str, int]:\\n        \\\"\\\"\\\"\\n        Add a document with selective recomputation for efficiency.\\n\\n        Unlike process_document() + compute_all(), this method only recomputes\\n        what's necessary based on the recompute parameter. This is more efficient\\n        for RAG systems with frequent document updates.\\n\\n        Args:\\n            doc_id: Unique identifier for the document\\n            content: Document text content\\n            metadata: Optional metadata dict (source, timestamp, author, etc.)\\n            recompute: Level of recomputation to perform:\\n                - 'none': Just add document, mark all computations stale\\n                - 'tfidf': Recompute TF-IDF only (fast, updates term weights)\\n                - 'full': Run compute_all() (slowest, most accurate)\\n\\n        Returns:\\n            Dict with processing statistics (tokens, bigrams, unique_tokens)\\n\\n        Example:\\n            >>> # Quick update for search without full recomputation\\n            >>> processor.add_document_incremental(\\\"new_doc\\\", \\\"content\\\", recompute='tfidf')\\n            >>>\\n            >>> # Just queue document, recompute later in batch\\n            >>> processor.add_document_incremental(\\\"doc1\\\", \\\"content1\\\", recompute='none')\\n            >>> processor.add_document_incremental(\\\"doc2\\\", \\\"content2\\\", recompute='none')\\n            >>> processor.recompute(level='full')  # Batch recomputation\\n        \\\"\\\"\\\"\\n        stats = self.process_document(doc_id, content, metadata)\\n\\n        if recompute == 'tfidf':\\n            self.compute_tfidf(verbose=False)\\n            self._mark_fresh(self.COMP_TFIDF)\\n        elif recompute == 'full':\\n            self.compute_all(verbose=False)\\n            self._stale_computations.clear()\\n        # 'none' leaves all computations marked as stale\\n\\n        return stats\\n\\n    def add_documents_batch(\\n        self,\\n        documents: List[Tuple[str, str, Optional[Dict[str, Any]]]],\\n        recompute: str = 'full',\\n        verbose: bool = True\\n    ) -> Dict[str, Any]:\\n        \\\"\\\"\\\"\\n        Add multiple documents with a single recomputation.\\n\\n        More efficient than calling add_document_incremental() multiple times\\n        when adding many documents at once.\\n\\n        Args:\\n            documents: List of (doc_id, content, metadata) tuples.\\n                       metadata can be None for documents without metadata.\\n            recompute: Level of recomputation after all documents are added:\\n                - 'none': Just add documents, mark all computations stale\\n                - 'tfidf': Recompute TF-IDF only\\n                - 'full': Run compute_all()\\n            verbose: Print progress messages\\n\\n        Returns:\\n            Dict with batch statistics:\\n                - documents_added: Number of documents added\\n                - total_tokens: Total tokens across all documents\\n                - recomputation: Type of recomputation performed\\n\\n        Example:\\n            >>> docs = [\\n            ...     (\\\"doc1\\\", \\\"First document content\\\", {\\\"source\\\": \\\"web\\\"}),\\n            ...     (\\\"doc2\\\", \\\"Second document content\\\", None),\\n            ...     (\\\"doc3\\\", \\\"Third document content\\\", {\\\"author\\\": \\\"AI\\\"}),\\n            ... ]\\n            >>> processor.add_documents_batch(docs, recompute='full')\\n\\n        Raises:\\n            ValueError: If documents list is invalid or recompute level is unknown\\n        \\\"\\\"\\\"\\n        # Input validation\\n        if not isinstance(documents, list):\\n            raise ValueError(\\\"documents must be a list\\\")\\n        if not documents:\\n            raise ValueError(\\\"documents list must not be empty\\\")\\n\\n        valid_recompute = {'none', 'tfidf', 'full'}\\n        if recompute not in valid_recompute:\\n            raise ValueError(f\\\"recompute must be one of {valid_recompute}\\\")\\n\\n        for i, doc in enumerate(documents):\\n            if not isinstance(doc, (tuple, list)) or len(doc) < 2:\\n                raise ValueError(\\n                    f\\\"documents[{i}] must be a tuple of (doc_id, content) or \\\"\\n                    f\\\"(doc_id, content, metadata)\\\"\\n                )\\n            doc_id, content = doc[0], doc[1]\\n            if not isinstance(doc_id, str) or not doc_id:\\n                raise ValueError(f\\\"documents[{i}][0] (doc_id) must be a non-empty string\\\")\\n            if not isinstance(content, str):\\n                raise ValueError(f\\\"documents[{i}][1] (content) must be a string\\\")\\n\\n        total_tokens = 0\\n        total_bigrams = 0\\n\\n        if verbose:\\n            print(f\\\"Adding {len(documents)} documents...\\\")\\n\\n        for doc_id, content, metadata in documents:\\n            # Use process_document directly (not add_document_incremental)\\n            # to avoid per-document recomputation\\n            stats = self.process_document(doc_id, content, metadata)\\n            total_tokens += stats['tokens']\\n            total_bigrams += stats['bigrams']\\n\\n        if verbose:\\n            print(f\\\"Processed {total_tokens} tokens, {total_bigrams} bigrams\\\")\\n\\n        # Perform single recomputation for entire batch\\n        if recompute == 'tfidf':\\n            if verbose:\\n                print(\\\"Recomputing TF-IDF...\\\")\\n            self.compute_tfidf(verbose=False)\\n            self._mark_fresh(self.COMP_TFIDF)\\n        elif recompute == 'full':\\n            if verbose:\\n                print(\\\"Running full recomputation...\\\")\\n            self.compute_all(verbose=False)\\n            self._stale_computations.clear()\\n\\n        if verbose:\\n            print(\\\"Done.\\\")\\n\\n        return {\\n            'documents_added': len(documents),\\n            'total_tokens': total_tokens,\\n            'total_bigrams': total_bigrams,\\n            'recomputation': recompute\\n        }\\n\\n    def remove_document(self, doc_id: str, verbose: bool = False) -> Dict[str, Any]:\\n        \\\"\\\"\\\"\\n        Remove a document from the corpus.\\n\\n        Removes the document and cleans up all references to it in the layers:\\n        - Removes from documents dict and metadata\\n        - Removes document minicolumn from Layer 3\\n        - Removes doc_id from token and bigram document_ids sets\\n        - Decrements occurrence counts appropriately\\n        - Cleans up feedforward/feedback connections\\n\\n        Args:\\n            doc_id: Document identifier to remove\\n            verbose: Print progress messages\\n\\n        Returns:\\n            Dict with removal statistics:\\n                - found: Whether the document existed\\n                - tokens_affected: Number of tokens that referenced this document\\n                - bigrams_affected: Number of bigrams that referenced this document\\n\\n        Example:\\n            >>> processor.remove_document(\\\"old_doc\\\")\\n            {'found': True, 'tokens_affected': 42, 'bigrams_affected': 35}\\n        \\\"\\\"\\\"\\n        from .layers import CorticalLayer\\n\\n        if doc_id not in self.documents:\\n            return {'found': False, 'tokens_affected': 0, 'bigrams_affected': 0}\\n\\n        if verbose:\\n            print(f\\\"Removing document: {doc_id}\\\")\\n\\n        # Remove from documents and metadata\\n        del self.documents[doc_id]\\n        if doc_id in self.document_metadata:\\n            del self.document_metadata[doc_id]\\n\\n        # Remove document minicolumn from Layer 3\\n        layer3 = self.layers[CorticalLayer.DOCUMENTS]\\n        doc_col = layer3.get_minicolumn(doc_id)\\n        if doc_col:\\n            # Get tokens/bigrams that were connected to this document\\n            connected_ids = set(doc_col.feedforward_connections.keys())\\n            layer3.remove_minicolumn(doc_id)\\n\\n        # Clean up token references in Layer 0\\n        layer0 = self.layers[CorticalLayer.TOKENS]\\n        tokens_affected = 0\\n        for content, col in list(layer0.minicolumns.items()):\\n            if doc_id in col.document_ids:\\n                col.document_ids.discard(doc_id)\\n                tokens_affected += 1\\n\\n                # Decrement occurrence count by per-doc count\\n                if doc_id in col.doc_occurrence_counts:\\n                    col.occurrence_count -= col.doc_occurrence_counts[doc_id]\\n                    del col.doc_occurrence_counts[doc_id]\\n\\n                # Clean up feedback connections to document\\n                doc_col_id = f\\\"L3_{doc_id}\\\"\\n                if doc_col_id in col.feedback_connections:\\n                    del col.feedback_connections[doc_col_id]\\n\\n        # Clean up bigram references in Layer 1\\n        layer1 = self.layers[CorticalLayer.BIGRAMS]\\n        bigrams_affected = 0\\n        for content, col in list(layer1.minicolumns.items()):\\n            if doc_id in col.document_ids:\\n                col.document_ids.discard(doc_id)\\n                bigrams_affected += 1\\n\\n                # Decrement occurrence count (approximate since we don't track per-doc for bigrams)\\n                if doc_id in col.doc_occurrence_counts:\\n                    col.occurrence_count -= col.doc_occurrence_counts[doc_id]\\n                    del col.doc_occurrence_counts[doc_id]\\n\\n        # Mark all computations as stale\\n        self._mark_all_stale()\\n\\n        # Invalidate query cache since corpus changed\\n        if hasattr(self, '_query_expansion_cache'):\\n            self._query_expansion_cache.clear()\\n\\n        if verbose:\\n            print(f\\\"  Affected: {tokens_affected} tokens, {bigrams_affected} bigrams\\\")\\n\\n        return {\\n            'found': True,\\n            'tokens_affected': tokens_affected,\\n            'bigrams_affected': bigrams_affected\\n        }\\n\\n    def remove_documents_batch(\\n        self,\\n        doc_ids: List[str],\\n        recompute: str = 'none',\\n        verbose: bool = True\\n    ) -> Dict[str, Any]:\\n        \\\"\\\"\\\"\\n        Remove multiple documents efficiently with single recomputation.\\n\\n        Args:\\n            doc_ids: List of document identifiers to remove\\n            recompute: Level of recomputation after removal:\\n                - 'none': Just remove documents, mark computations stale\\n                - 'tfidf': Recompute TF-IDF only\\n                - 'full': Run full compute_all()\\n            verbose: Print progress messages\\n\\n        Returns:\\n            Dict with removal statistics:\\n                - documents_removed: Number of documents actually removed\\n                - documents_not_found: Number of doc_ids that didn't exist\\n                - total_tokens_affected: Total tokens affected\\n                - total_bigrams_affected: Total bigrams affected\\n\\n        Example:\\n            >>> processor.remove_documents_batch([\\\"old1\\\", \\\"old2\\\", \\\"old3\\\"])\\n        \\\"\\\"\\\"\\n        removed = 0\\n        not_found = 0\\n        total_tokens = 0\\n        total_bigrams = 0\\n\\n        if verbose:\\n            print(f\\\"Removing {len(doc_ids)} documents...\\\")\\n\\n        for doc_id in doc_ids:\\n            result = self.remove_document(doc_id, verbose=False)\\n            if result['found']:\\n                removed += 1\\n                total_tokens += result['tokens_affected']\\n                total_bigrams += result['bigrams_affected']\\n            else:\\n                not_found += 1\\n\\n        if verbose:\\n            print(f\\\"  Removed: {removed}, Not found: {not_found}\\\")\\n            print(f\\\"  Affected: {total_tokens} tokens, {total_bigrams} bigrams\\\")\\n\\n        # Perform recomputation\\n        if recompute == 'tfidf':\\n            if verbose:\\n                print(\\\"Recomputing TF-IDF...\\\")\\n            self.compute_tfidf(verbose=False)\\n            self._mark_fresh(self.COMP_TFIDF)\\n        elif recompute == 'full':\\n            if verbose:\\n                print(\\\"Running full recomputation...\\\")\\n            self.compute_all(verbose=False)\\n            self._stale_computations.clear()\\n\\n        return {\\n            'documents_removed': removed,\\n            'documents_not_found': not_found,\\n            'total_tokens_affected': total_tokens,\\n            'total_bigrams_affected': total_bigrams,\\n            'recomputation': recompute\\n        }\\n\\n    def recompute(\\n        self,\\n        level: str = 'stale',\\n        verbose: bool = True\\n    ) -> Dict[str, bool]:\\n        \\\"\\\"\\\"\\n        Recompute specified analysis levels.\\n\\n        Use this after adding documents with recompute='none' to batch\\n        the recomputation step.\\n\\n        Args:\\n            level: What to recompute:\\n                - 'stale': Only recompute what's marked as stale\\n                - 'tfidf': Only TF-IDF (marks others stale)\\n                - 'full': Run complete compute_all()\\n            verbose: Print progress messages\\n\\n        Returns:\\n            Dict indicating what was recomputed\\n\\n        Example:\\n            >>> # Add documents without recomputation\\n            >>> processor.add_document_incremental(\\\"doc1\\\", \\\"content\\\", recompute='none')\\n            >>> processor.add_document_incremental(\\\"doc2\\\", \\\"content\\\", recompute='none')\\n            >>> # Batch recompute\\n            >>> processor.recompute(level='full')\\n        \\\"\\\"\\\"\\n        recomputed = {}\\n\\n        if level == 'full':\\n            self.compute_all(verbose=verbose)\\n            self._stale_computations.clear()\\n            recomputed = {\\n                self.COMP_ACTIVATION: True,\\n                self.COMP_PAGERANK: True,\\n                self.COMP_TFIDF: True,\\n                self.COMP_DOC_CONNECTIONS: True,\\n                self.COMP_BIGRAM_CONNECTIONS: True,\\n                self.COMP_CONCEPTS: True,\\n            }\\n        elif level == 'tfidf':\\n            self.compute_tfidf(verbose=verbose)\\n            self._mark_fresh(self.COMP_TFIDF)\\n            recomputed[self.COMP_TFIDF] = True\\n        elif level == 'stale':\\n            # Recompute only what's stale, in dependency order\\n            if self.COMP_ACTIVATION in self._stale_computations:\\n                self.propagate_activation(verbose=verbose)\\n                self._mark_fresh(self.COMP_ACTIVATION)\\n                recomputed[self.COMP_ACTIVATION] = True\\n\\n            if self.COMP_PAGERANK in self._stale_computations:\\n                self.compute_importance(verbose=verbose)\\n                self._mark_fresh(self.COMP_PAGERANK)\\n                recomputed[self.COMP_PAGERANK] = True\\n\\n            if self.COMP_TFIDF in self._stale_computations:\\n                self.compute_tfidf(verbose=verbose)\\n                self._mark_fresh(self.COMP_TFIDF)\\n                recomputed[self.COMP_TFIDF] = True\\n\\n            if self.COMP_DOC_CONNECTIONS in self._stale_computations:\\n                self.compute_document_connections(verbose=verbose)\\n                self._mark_fresh(self.COMP_DOC_CONNECTIONS)\\n                recomputed[self.COMP_DOC_CONNECTIONS] = True\\n\\n            if self.COMP_BIGRAM_CONNECTIONS in self._stale_computations:\\n                self.compute_bigram_connections(verbose=verbose)\\n                self._mark_fresh(self.COMP_BIGRAM_CONNECTIONS)\\n                recomputed[self.COMP_BIGRAM_CONNECTIONS] = True\\n\\n            if self.COMP_CONCEPTS in self._stale_computations:\\n                self.build_concept_clusters(verbose=verbose)\\n                self._mark_fresh(self.COMP_CONCEPTS)\\n                recomputed[self.COMP_CONCEPTS] = True\\n\\n            if self.COMP_EMBEDDINGS in self._stale_computations:\\n                self.compute_graph_embeddings(verbose=verbose)\\n                self._mark_fresh(self.COMP_EMBEDDINGS)\\n                recomputed[self.COMP_EMBEDDINGS] = True\\n\\n            if self.COMP_SEMANTICS in self._stale_computations:\\n                self.extract_corpus_semantics(verbose=verbose)\\n                self._mark_fresh(self.COMP_SEMANTICS)\\n                recomputed[self.COMP_SEMANTICS] = True\\n\\n        return recomputed\\n\\n    def compute_all(\\n        self,\\n        verbose: bool = True,\\n        build_concepts: bool = True,\\n        pagerank_method: str = 'standard',\\n        connection_strategy: str = 'document_overlap',\\n        cluster_strictness: float = 1.0,\\n        bridge_weight: float = 0.0\\n    ) -> Dict[str, Any]:\\n        \\\"\\\"\\\"\\n        Run all computation steps.\\n\\n        Args:\\n            verbose: Print progress messages\\n            build_concepts: Build concept clusters in Layer 2 (default True)\\n                           This enables topic-based filtering and hierarchical search.\\n            pagerank_method: PageRank algorithm to use:\\n                - 'standard': Traditional PageRank using connection weights\\n                - 'semantic': ConceptNet-style PageRank with relation type weighting.\\n                              Requires semantic relations (extracts automatically if needed).\\n                - 'hierarchical': Cross-layer PageRank with importance propagation\\n                                  between layers (tokens ↔ bigrams ↔ concepts ↔ documents).\\n            connection_strategy: Strategy for connecting Layer 2 concepts:\\n                - 'document_overlap': Traditional Jaccard similarity (default)\\n                - 'semantic': Connect via semantic relations between members\\n                - 'embedding': Connect via embedding centroid similarity\\n                - 'hybrid': Combine all three strategies for maximum connectivity\\n            cluster_strictness: Controls clustering aggressiveness (0.0-1.0).\\n                Lower values create fewer, larger clusters with more connections.\\n            bridge_weight: Weight for inter-document token bridging (0.0-1.0).\\n                Higher values help bridge topic-isolated clusters.\\n\\n        Returns:\\n            Dict with computation statistics (concept_stats, etc.)\\n\\n        Example:\\n            >>> # Default behavior\\n            >>> processor.compute_all()\\n            >>>\\n            >>> # Maximum connectivity for diverse documents\\n            >>> processor.compute_all(\\n            ...     connection_strategy='hybrid',\\n            ...     cluster_strictness=0.5,\\n            ...     bridge_weight=0.3\\n            ... )\\n        \\\"\\\"\\\"\\n        stats: Dict[str, Any] = {}\\n\\n        if verbose:\\n            print(\\\"Computing activation propagation...\\\")\\n        self.propagate_activation(verbose=False)\\n\\n        if pagerank_method == 'semantic':\\n            # Extract semantic relations if not already done\\n            if not self.semantic_relations:\\n                if verbose:\\n                    print(\\\"Extracting semantic relations...\\\")\\n                self.extract_corpus_semantics(verbose=False)\\n            if verbose:\\n                print(\\\"Computing importance (Semantic PageRank)...\\\")\\n            self.compute_semantic_importance(verbose=False)\\n        elif pagerank_method == 'hierarchical':\\n            if verbose:\\n                print(\\\"Computing importance (Hierarchical PageRank)...\\\")\\n            self.compute_hierarchical_importance(verbose=False)\\n        else:\\n            if verbose:\\n                print(\\\"Computing importance (PageRank)...\\\")\\n            self.compute_importance(verbose=False)\\n        if verbose:\\n            print(\\\"Computing TF-IDF...\\\")\\n        self.compute_tfidf(verbose=False)\\n        if verbose:\\n            print(\\\"Computing document connections...\\\")\\n        self.compute_document_connections(verbose=False)\\n        if verbose:\\n            print(\\\"Computing bigram connections...\\\")\\n        self.compute_bigram_connections(verbose=False)\\n\\n        if build_concepts:\\n            if verbose:\\n                print(\\\"Building concept clusters...\\\")\\n            clusters = self.build_concept_clusters(\\n                cluster_strictness=cluster_strictness,\\n                bridge_weight=bridge_weight,\\n                verbose=False\\n            )\\n            stats['clusters_created'] = len(clusters)\\n\\n            # Determine connection parameters based on strategy\\n            use_member_semantics = connection_strategy in ('semantic', 'hybrid')\\n            use_embedding_similarity = connection_strategy in ('embedding', 'hybrid')\\n\\n            # For semantic/embedding strategies, extract/compute prerequisites\\n            if use_member_semantics and not self.semantic_relations:\\n                if verbose:\\n                    print(\\\"Extracting semantic relations...\\\")\\n                self.extract_corpus_semantics(verbose=False)\\n\\n            if use_embedding_similarity and not self.embeddings:\\n                if verbose:\\n                    print(\\\"Computing graph embeddings...\\\")\\n                self.compute_graph_embeddings(verbose=False)\\n\\n            # Set thresholds based on strategy\\n            if connection_strategy == 'hybrid':\\n                min_shared_docs = 0\\n                min_jaccard = 0.0\\n            elif connection_strategy in ('semantic', 'embedding'):\\n                min_shared_docs = 0\\n                min_jaccard = 0.0\\n            else:  # document_overlap\\n                min_shared_docs = 1\\n                min_jaccard = 0.1\\n\\n            if verbose:\\n                print(f\\\"Computing concept connections ({connection_strategy})...\\\")\\n            concept_stats = self.compute_concept_connections(\\n                use_member_semantics=use_member_semantics,\\n                use_embedding_similarity=use_embedding_similarity,\\n                min_shared_docs=min_shared_docs,\\n                min_jaccard=min_jaccard,\\n                verbose=False\\n            )\\n            stats['concept_connections'] = concept_stats\\n\\n        # Mark core computations as fresh\\n        fresh_comps = [\\n            self.COMP_ACTIVATION,\\n            self.COMP_PAGERANK,\\n            self.COMP_TFIDF,\\n            self.COMP_DOC_CONNECTIONS,\\n            self.COMP_BIGRAM_CONNECTIONS,\\n        ]\\n        if build_concepts:\\n            fresh_comps.append(self.COMP_CONCEPTS)\\n        self._mark_fresh(*fresh_comps)\\n\\n        # Invalidate query cache since corpus state changed\\n        self._query_expansion_cache.clear()\\n\\n        if verbose:\\n            print(\\\"Done.\\\")\\n\\n        return stats\\n    \\n    def propagate_activation(self, iterations: int = 3, decay: float = 0.8, verbose: bool = True) -> None:\\n        analysis.propagate_activation(self.layers, iterations, decay)\\n        if verbose: print(f\\\"Propagated activation ({iterations} iterations)\\\")\\n    \\n    def compute_importance(self, verbose: bool = True) -> None:\\n        for layer_enum in [CorticalLayer.TOKENS, CorticalLayer.BIGRAMS]:\\n            analysis.compute_pagerank(self.layers[layer_enum])\\n        if verbose: print(\\\"Computed PageRank importance\\\")\\n\\n    def compute_semantic_importance(\\n        self,\\n        relation_weights: Optional[Dict[str, float]] = None,\\n        verbose: bool = True\\n    ) -> Dict[str, Any]:\\n        \\\"\\\"\\\"\\n        Compute PageRank with semantic relation weighting.\\n\\n        Uses semantic relations to weight edges in the PageRank graph.\\n        Edges with stronger semantic relationships (e.g., IsA, PartOf) receive\\n        higher weights, affecting importance propagation.\\n\\n        Args:\\n            relation_weights: Optional custom relation type weights dict.\\n                Defaults to built-in weights (IsA: 1.5, PartOf: 1.3, etc.)\\n            verbose: Print progress messages\\n\\n        Returns:\\n            Dict with statistics:\\n            - total_edges_with_relations: Sum across layers\\n            - token_layer: Stats for token layer\\n            - bigram_layer: Stats for bigram layer\\n\\n        Example:\\n            >>> # Use default relation weights\\n            >>> stats = processor.compute_semantic_importance()\\n            >>> print(f\\\"Found {stats['total_edges_with_relations']} semantic edges\\\")\\n            >>>\\n            >>> # Custom weights\\n            >>> weights = {'IsA': 2.0, 'RelatedTo': 0.5}\\n            >>> processor.compute_semantic_importance(relation_weights=weights)\\n        \\\"\\\"\\\"\\n        if not self.semantic_relations:\\n            # Fall back to standard PageRank if no semantic relations\\n            self.compute_importance(verbose=verbose)\\n            return {\\n                'total_edges_with_relations': 0,\\n                'token_layer': {'edges_with_relations': 0},\\n                'bigram_layer': {'edges_with_relations': 0}\\n            }\\n\\n        total_edges = 0\\n        layer_stats = {}\\n\\n        for layer_enum in [CorticalLayer.TOKENS, CorticalLayer.BIGRAMS]:\\n            result = analysis.compute_semantic_pagerank(\\n                self.layers[layer_enum],\\n                self.semantic_relations,\\n                relation_weights=relation_weights\\n            )\\n            layer_name = 'token_layer' if layer_enum == CorticalLayer.TOKENS else 'bigram_layer'\\n            layer_stats[layer_name] = {\\n                'iterations_run': result['iterations_run'],\\n                'edges_with_relations': result['edges_with_relations']\\n            }\\n            total_edges += result['edges_with_relations']\\n\\n        if verbose:\\n            print(f\\\"Computed semantic PageRank ({total_edges} relation-weighted edges)\\\")\\n\\n        return {\\n            'total_edges_with_relations': total_edges,\\n            **layer_stats\\n        }\\n\\n    def compute_hierarchical_importance(\\n        self,\\n        layer_iterations: int = 10,\\n        global_iterations: int = 5,\\n        cross_layer_damping: float = 0.7,\\n        verbose: bool = True\\n    ) -> Dict[str, Any]:\\n        \\\"\\\"\\\"\\n        Compute PageRank with cross-layer propagation.\\n\\n        This hierarchical PageRank allows importance to flow between layers:\\n        - Upward: tokens → bigrams → concepts → documents\\n        - Downward: documents → concepts → bigrams → tokens\\n\\n        Important tokens boost their containing bigrams and concepts.\\n        Important documents boost their contained terms. This creates\\n        a more holistic importance score that considers the full hierarchy.\\n\\n        Args:\\n            layer_iterations: Max iterations for intra-layer PageRank (default 10)\\n            global_iterations: Max iterations for cross-layer propagation (default 5)\\n            cross_layer_damping: Damping factor at layer boundaries (default 0.7)\\n            verbose: Print progress messages\\n\\n        Returns:\\n            Dict with statistics:\\n            - iterations_run: Number of global iterations\\n            - converged: Whether the algorithm converged\\n            - layer_stats: Per-layer statistics (nodes, max/min/avg PageRank)\\n\\n        Example:\\n            >>> stats = processor.compute_hierarchical_importance()\\n            >>> print(f\\\"Converged: {stats['converged']}\\\")\\n            >>> for layer, info in stats['layer_stats'].items():\\n            ...     print(f\\\"{layer}: {info['nodes']} nodes, max PR={info['max_pagerank']:.4f}\\\")\\n        \\\"\\\"\\\"\\n        result = analysis.compute_hierarchical_pagerank(\\n            self.layers,\\n            layer_iterations=layer_iterations,\\n            global_iterations=global_iterations,\\n            cross_layer_damping=cross_layer_damping\\n        )\\n\\n        if verbose:\\n            status = \\\"converged\\\" if result['converged'] else \\\"did not converge\\\"\\n            print(f\\\"Computed hierarchical PageRank ({result['iterations_run']} iterations, {status})\\\")\\n\\n        return result\\n\\n    def compute_tfidf(self, verbose: bool = True) -> None:\\n        analysis.compute_tfidf(self.layers, self.documents)\\n        if verbose: print(\\\"Computed TF-IDF scores\\\")\\n    \\n    def compute_document_connections(self, min_shared_terms: int = 3, verbose: bool = True) -> None:\\n        analysis.compute_document_connections(self.layers, self.documents, min_shared_terms)\\n        if verbose: print(\\\"Computed document connections\\\")\\n\\n    def compute_bigram_connections(\\n        self,\\n        min_shared_docs: int = 1,\\n        component_weight: float = 0.5,\\n        chain_weight: float = 0.7,\\n        cooccurrence_weight: float = 0.3,\\n        verbose: bool = True\\n    ) -> Dict[str, Any]:\\n        \\\"\\\"\\\"\\n        Build lateral connections between bigrams based on shared components and co-occurrence.\\n\\n        Bigrams are connected when they:\\n        - Share a component term (\\\"neural_networks\\\" ↔ \\\"neural_processing\\\")\\n        - Form chains (\\\"machine_learning\\\" ↔ \\\"learning_algorithms\\\")\\n        - Co-occur in the same documents\\n\\n        Args:\\n            min_shared_docs: Minimum shared documents for co-occurrence connection\\n            component_weight: Weight for shared component connections (default 0.5)\\n            chain_weight: Weight for chain connections (default 0.7)\\n            cooccurrence_weight: Weight for document co-occurrence (default 0.3)\\n            verbose: Print progress messages\\n\\n        Returns:\\n            Statistics about connections created:\\n            - connections_created: Total bidirectional connections\\n            - component_connections: Connections from shared components\\n            - chain_connections: Connections from chains\\n            - cooccurrence_connections: Connections from document co-occurrence\\n\\n        Example:\\n            >>> stats = processor.compute_bigram_connections()\\n            >>> print(f\\\"Created {stats['connections_created']} bigram connections\\\")\\n            >>> print(f\\\"  Component: {stats['component_connections']}\\\")\\n            >>> print(f\\\"  Chain: {stats['chain_connections']}\\\")\\n            >>> print(f\\\"  Co-occurrence: {stats['cooccurrence_connections']}\\\")\\n        \\\"\\\"\\\"\\n        stats = analysis.compute_bigram_connections(\\n            self.layers,\\n            min_shared_docs=min_shared_docs,\\n            component_weight=component_weight,\\n            chain_weight=chain_weight,\\n            cooccurrence_weight=cooccurrence_weight\\n        )\\n        if verbose:\\n            print(f\\\"Created {stats['connections_created']} bigram connections \\\"\\n                  f\\\"(component: {stats['component_connections']}, \\\"\\n                  f\\\"chain: {stats['chain_connections']}, \\\"\\n                  f\\\"cooccur: {stats['cooccurrence_connections']})\\\")\\n        return stats\\n\\n    def build_concept_clusters(\\n        self,\\n        min_cluster_size: int = 3,\\n        cluster_strictness: float = 1.0,\\n        bridge_weight: float = 0.0,\\n        verbose: bool = True\\n    ) -> Dict[int, List[str]]:\\n        \\\"\\\"\\\"\\n        Build concept clusters from token layer using label propagation.\\n\\n        Args:\\n            min_cluster_size: Minimum tokens per cluster (default 3)\\n            cluster_strictness: Controls clustering aggressiveness (0.0-1.0).\\n                - 1.0 (default): Strict clustering, topics stay separate\\n                - 0.5: Moderate mixing, allows some cross-topic clustering\\n                - 0.0: Minimal clustering, most tokens group together\\n                Lower values create fewer, larger clusters with more connections.\\n            bridge_weight: Weight for synthetic inter-document connections (0.0-1.0).\\n                When > 0, adds weak connections between tokens from different\\n                documents, helping bridge topic-isolated clusters.\\n                - 0.0 (default): No bridging\\n                - 0.3: Light bridging\\n                - 0.7: Strong bridging\\n            verbose: Print progress messages\\n\\n        Returns:\\n            Dictionary mapping cluster_id to list of token contents\\n\\n        Example:\\n            >>> # Default strict clustering\\n            >>> clusters = processor.build_concept_clusters()\\n            >>>\\n            >>> # Looser clustering for more cross-topic connections\\n            >>> clusters = processor.build_concept_clusters(\\n            ...     cluster_strictness=0.5,\\n            ...     bridge_weight=0.3\\n            ... )\\n        \\\"\\\"\\\"\\n        clusters = analysis.cluster_by_label_propagation(\\n            self.layers[CorticalLayer.TOKENS],\\n            min_cluster_size=min_cluster_size,\\n            cluster_strictness=cluster_strictness,\\n            bridge_weight=bridge_weight\\n        )\\n        analysis.build_concept_clusters(self.layers, clusters)\\n        if verbose:\\n            print(f\\\"Built {len(clusters)} concept clusters\\\")\\n        return clusters\\n\\n    def compute_concept_connections(\\n        self,\\n        use_semantics: bool = True,\\n        min_shared_docs: int = 1,\\n        min_jaccard: float = 0.1,\\n        use_member_semantics: bool = False,\\n        use_embedding_similarity: bool = False,\\n        embedding_threshold: float = 0.3,\\n        verbose: bool = True\\n    ) -> Dict[str, Any]:\\n        \\\"\\\"\\\"\\n        Build lateral connections between concepts based on document overlap and semantics.\\n\\n        Multiple connection strategies can be combined:\\n        1. Document overlap (default): Jaccard similarity of document sets\\n        2. Semantic boost: Boost overlapping connections with semantic relations\\n        3. Member semantics: Connect via semantic relations even without doc overlap\\n        4. Embedding similarity: Connect via concept centroid similarity\\n\\n        Args:\\n            use_semantics: Use semantic relations to boost connection weights\\n            min_shared_docs: Minimum shared documents for connection (0 to disable)\\n            min_jaccard: Minimum Jaccard similarity threshold (0.0 to disable)\\n            use_member_semantics: Connect concepts via member token semantic relations,\\n                                  independent of document overlap\\n            use_embedding_similarity: Connect concepts via embedding centroid similarity\\n            embedding_threshold: Minimum cosine similarity for embedding connections\\n            verbose: Print progress messages\\n\\n        Returns:\\n            Statistics about connections created:\\n            - connections_created: Total connections\\n            - concepts: Number of concepts\\n            - doc_overlap_connections: Connections from document overlap\\n            - semantic_connections: Connections from member semantics\\n            - embedding_connections: Connections from embedding similarity\\n\\n        Example:\\n            >>> # Traditional document overlap only\\n            >>> stats = processor.compute_concept_connections()\\n            >>>\\n            >>> # Enable all connection strategies\\n            >>> processor.compute_graph_embeddings()\\n            >>> processor.extract_corpus_semantics()\\n            >>> stats = processor.compute_concept_connections(\\n            ...     use_member_semantics=True,\\n            ...     use_embedding_similarity=True,\\n            ...     min_shared_docs=0,\\n            ...     min_jaccard=0.0\\n            ... )\\n        \\\"\\\"\\\"\\n        semantic_rels = self.semantic_relations if use_semantics else None\\n        emb = self.embeddings if use_embedding_similarity else None\\n        stats = analysis.compute_concept_connections(\\n            self.layers,\\n            semantic_relations=semantic_rels,\\n            min_shared_docs=min_shared_docs,\\n            min_jaccard=min_jaccard,\\n            use_member_semantics=use_member_semantics,\\n            use_embedding_similarity=use_embedding_similarity,\\n            embedding_threshold=embedding_threshold,\\n            embeddings=emb\\n        )\\n        if verbose:\\n            parts = [f\\\"Created {stats['connections_created']} concept connections\\\"]\\n            if stats.get('doc_overlap_connections', 0) > 0:\\n                parts.append(f\\\"doc_overlap: {stats['doc_overlap_connections']}\\\")\\n            if stats.get('semantic_connections', 0) > 0:\\n                parts.append(f\\\"semantic: {stats['semantic_connections']}\\\")\\n            if stats.get('embedding_connections', 0) > 0:\\n                parts.append(f\\\"embedding: {stats['embedding_connections']}\\\")\\n            print(\\\", \\\".join(parts) if len(parts) > 1 else parts[0])\\n        return stats\\n\\n    def extract_corpus_semantics(\\n        self,\\n        use_pattern_extraction: bool = True,\\n        min_pattern_confidence: float = 0.6,\\n        verbose: bool = True\\n    ) -> int:\\n        \\\"\\\"\\\"\\n        Extract semantic relations from the corpus.\\n\\n        Combines co-occurrence analysis with pattern-based extraction to discover\\n        semantic relationships like IsA, HasA, UsedFor, Causes, etc.\\n\\n        Args:\\n            use_pattern_extraction: Extract relations from text patterns (e.g., \\\"X is a Y\\\")\\n            min_pattern_confidence: Minimum confidence for pattern-based relations\\n            verbose: Print progress messages\\n\\n        Returns:\\n            Number of relations extracted\\n\\n        Example:\\n            >>> count = processor.extract_corpus_semantics(verbose=False)\\n            >>> print(f\\\"Found {count} semantic relations\\\")\\n        \\\"\\\"\\\"\\n        self.semantic_relations = semantics.extract_corpus_semantics(\\n            self.layers,\\n            self.documents,\\n            self.tokenizer,\\n            use_pattern_extraction=use_pattern_extraction,\\n            min_pattern_confidence=min_pattern_confidence\\n        )\\n        if verbose:\\n            print(f\\\"Extracted {len(self.semantic_relations)} semantic relations\\\")\\n        return len(self.semantic_relations)\\n\\n    def extract_pattern_relations(\\n        self,\\n        min_confidence: float = 0.6,\\n        verbose: bool = True\\n    ) -> List[Tuple[str, str, str, float]]:\\n        \\\"\\\"\\\"\\n        Extract semantic relations using pattern matching only.\\n\\n        Uses regex patterns to identify commonsense relations from text patterns\\n        like \\\"X is a type of Y\\\" → IsA, \\\"X is used for Y\\\" → UsedFor, etc.\\n\\n        Args:\\n            min_confidence: Minimum confidence for extracted relations\\n            verbose: Print progress messages\\n\\n        Returns:\\n            List of (term1, relation_type, term2, confidence) tuples\\n\\n        Example:\\n            >>> relations = processor.extract_pattern_relations(verbose=False)\\n            >>> for t1, rel, t2, conf in relations[:5]:\\n            ...     print(f\\\"{t1} --{rel}--> {t2} ({conf:.2f})\\\")\\n        \\\"\\\"\\\"\\n        layer0 = self.get_layer(CorticalLayer.TOKENS)\\n        valid_terms = set(layer0.minicolumns.keys())\\n\\n        relations = semantics.extract_pattern_relations(\\n            self.documents,\\n            valid_terms,\\n            min_confidence=min_confidence\\n        )\\n\\n        if verbose:\\n            stats = semantics.get_pattern_statistics(relations)\\n            print(f\\\"Extracted {stats['total_relations']} pattern-based relations\\\")\\n            print(f\\\"  Types: {stats['relation_type_counts']}\\\")\\n\\n        return relations\\n    \\n    def retrofit_connections(self, iterations: int = 10, alpha: float = 0.3, verbose: bool = True) -> Dict:\\n        if not self.semantic_relations: self.extract_corpus_semantics(verbose=False)\\n        stats = semantics.retrofit_connections(self.layers, self.semantic_relations, iterations, alpha)\\n        if verbose: print(f\\\"Retrofitted {stats['tokens_affected']} tokens\\\")\\n        return stats\\n\\n    def compute_property_inheritance(\\n        self,\\n        decay_factor: float = 0.7,\\n        max_depth: int = 5,\\n        apply_to_connections: bool = True,\\n        boost_factor: float = 0.3,\\n        verbose: bool = True\\n    ) -> Dict[str, Any]:\\n        \\\"\\\"\\\"\\n        Compute property inheritance based on IsA hierarchy.\\n\\n        If \\\"dog IsA animal\\\" and \\\"animal HasProperty living\\\", then \\\"dog\\\" inherits\\n        \\\"living\\\" with a decayed weight. This enables similarity computation between\\n        terms that share inherited properties.\\n\\n        Args:\\n            decay_factor: Weight multiplier per inheritance level (default 0.7)\\n            max_depth: Maximum inheritance depth (default 5)\\n            apply_to_connections: Boost lateral connections for shared properties\\n            boost_factor: Weight boost for shared inherited properties\\n            verbose: Print progress messages\\n\\n        Returns:\\n            Dict with statistics:\\n            - terms_with_inheritance: Number of terms that inherited properties\\n            - total_properties_inherited: Total property inheritance relationships\\n            - connections_boosted: Connections boosted (if apply_to_connections=True)\\n            - inherited: The full inheritance mapping (for advanced use)\\n\\n        Example:\\n            >>> processor.extract_corpus_semantics()\\n            >>> stats = processor.compute_property_inheritance()\\n            >>> print(f\\\"{stats['terms_with_inheritance']} terms inherited properties\\\")\\n            >>>\\n            >>> # Check inherited properties for a term\\n            >>> inherited = stats['inherited']\\n            >>> if 'dog' in inherited:\\n            ...     for prop, (weight, source, depth) in inherited['dog'].items():\\n            ...         print(f\\\"  {prop}: {weight:.2f} (from {source}, depth {depth})\\\")\\n        \\\"\\\"\\\"\\n        if not self.semantic_relations:\\n            self.extract_corpus_semantics(verbose=False)\\n\\n        inherited = semantics.inherit_properties(\\n            self.semantic_relations,\\n            decay_factor=decay_factor,\\n            max_depth=max_depth\\n        )\\n\\n        total_props = sum(len(props) for props in inherited.values())\\n\\n        result = {\\n            'terms_with_inheritance': len(inherited),\\n            'total_properties_inherited': total_props,\\n            'inherited': inherited\\n        }\\n\\n        if apply_to_connections and inherited:\\n            conn_stats = semantics.apply_inheritance_to_connections(\\n                self.layers,\\n                inherited,\\n                boost_factor=boost_factor\\n            )\\n            result['connections_boosted'] = conn_stats['connections_boosted']\\n            result['total_boost'] = conn_stats['total_boost']\\n        else:\\n            result['connections_boosted'] = 0\\n            result['total_boost'] = 0.0\\n\\n        if verbose:\\n            print(f\\\"Computed property inheritance: {result['terms_with_inheritance']} terms, \\\"\\n                  f\\\"{total_props} properties, {result['connections_boosted']} connections boosted\\\")\\n\\n        return result\\n\\n    def compute_property_similarity(self, term1: str, term2: str) -> float:\\n        \\\"\\\"\\\"\\n        Compute similarity between terms based on shared properties (direct + inherited).\\n\\n        Requires that compute_property_inheritance() or extract_corpus_semantics()\\n        has been called first.\\n\\n        Args:\\n            term1: First term\\n            term2: Second term\\n\\n        Returns:\\n            Similarity score (0.0-1.0) based on Jaccard-like overlap of properties\\n\\n        Example:\\n            >>> processor.extract_corpus_semantics()\\n            >>> stats = processor.compute_property_inheritance()\\n            >>> sim = processor.compute_property_similarity(\\\"dog\\\", \\\"cat\\\")\\n            >>> # Both inherit \\\"living\\\" from \\\"animal\\\", so similarity > 0\\n        \\\"\\\"\\\"\\n        if not self.semantic_relations:\\n            return 0.0\\n\\n        # Compute inherited properties on the fly if needed\\n        inherited = semantics.inherit_properties(self.semantic_relations)\\n\\n        return semantics.compute_property_similarity(term1, term2, inherited)\\n    \\n    def compute_graph_embeddings(self, dimensions: int = 64, method: str = 'adjacency', verbose: bool = True) -> Dict:\\n        self.embeddings, stats = emb_module.compute_graph_embeddings(self.layers, dimensions, method)\\n        if verbose: print(f\\\"Computed {stats['terms_embedded']} embeddings ({method})\\\")\\n        return stats\\n    \\n    def retrofit_embeddings(self, iterations: int = 10, alpha: float = 0.4, verbose: bool = True) -> Dict:\\n        if not self.embeddings: self.compute_graph_embeddings(verbose=False)\\n        if not self.semantic_relations: self.extract_corpus_semantics(verbose=False)\\n        stats = semantics.retrofit_embeddings(self.embeddings, self.semantic_relations, iterations, alpha)\\n        if verbose: print(f\\\"Retrofitted embeddings (moved {stats['total_movement']:.2f} total)\\\")\\n        return stats\\n    \\n    def embedding_similarity(self, term1: str, term2: str) -> float:\\n        return emb_module.embedding_similarity(self.embeddings, term1, term2)\\n    \\n    def find_similar_by_embedding(self, term: str, top_n: int = 10) -> List[Tuple[str, float]]:\\n        return emb_module.find_similar_by_embedding(self.embeddings, term, top_n)\\n    \\n    def expand_query(\\n        self,\\n        query_text: str,\\n        max_expansions: int = 10,\\n        use_variants: bool = True,\\n        use_code_concepts: bool = False,\\n        verbose: bool = False\\n    ) -> Dict[str, float]:\\n        \\\"\\\"\\\"\\n        Expand a query using lateral connections and concept clusters.\\n\\n        Args:\\n            query_text: Original query string\\n            max_expansions: Maximum expansion terms to add\\n            use_variants: Try word variants when direct match fails\\n            use_code_concepts: Include programming synonym expansions\\n\\n        Returns:\\n            Dict mapping terms to weights\\n        \\\"\\\"\\\"\\n        return query_module.expand_query(\\n            query_text,\\n            self.layers,\\n            self.tokenizer,\\n            max_expansions=max_expansions,\\n            use_variants=use_variants,\\n            use_code_concepts=use_code_concepts\\n        )\\n\\n    def expand_query_for_code(self, query_text: str, max_expansions: int = 15) -> Dict[str, float]:\\n        \\\"\\\"\\\"\\n        Expand a query optimized for code search.\\n\\n        Enables code concept expansion to find programming synonyms\\n        (e.g., \\\"fetch\\\" also matches \\\"get\\\", \\\"load\\\", \\\"retrieve\\\").\\n\\n        Args:\\n            query_text: Original query string\\n            max_expansions: Maximum expansion terms to add\\n\\n        Returns:\\n            Dict mapping terms to weights\\n        \\\"\\\"\\\"\\n        return query_module.expand_query(\\n            query_text,\\n            self.layers,\\n            self.tokenizer,\\n            max_expansions=max_expansions,\\n            use_variants=True,\\n            use_code_concepts=True\\n        )\\n\\n    def expand_query_cached(\\n        self,\\n        query_text: str,\\n        max_expansions: int = 10,\\n        use_variants: bool = True,\\n        use_code_concepts: bool = False\\n    ) -> Dict[str, float]:\\n        \\\"\\\"\\\"\\n        Expand a query with caching for faster repeated lookups.\\n\\n        Uses an LRU-style cache to avoid recomputing expansion for\\n        frequently repeated queries. Useful in RAG loops where the\\n        same queries may be issued multiple times.\\n\\n        Args:\\n            query_text: Original query string\\n            max_expansions: Maximum expansion terms to add\\n            use_variants: Try word variants when direct match fails\\n            use_code_concepts: Include programming synonym expansions\\n\\n        Returns:\\n            Dict mapping terms to weights\\n        \\\"\\\"\\\"\\n        # Create cache key from parameters\\n        cache_key = f\\\"{query_text}|{max_expansions}|{use_variants}|{use_code_concepts}\\\"\\n\\n        # Check cache\\n        if cache_key in self._query_expansion_cache:\\n            return self._query_expansion_cache[cache_key].copy()\\n\\n        # Compute expansion\\n        result = query_module.expand_query(\\n            query_text,\\n            self.layers,\\n            self.tokenizer,\\n            max_expansions=max_expansions,\\n            use_variants=use_variants,\\n            use_code_concepts=use_code_concepts\\n        )\\n\\n        # Add to cache (with LRU eviction if at max size)\\n        if len(self._query_expansion_cache) >= self._query_cache_max_size:\\n            # Remove oldest entry (first key in dict - approximates LRU)\\n            oldest_key = next(iter(self._query_expansion_cache))\\n            del self._query_expansion_cache[oldest_key]\\n\\n        self._query_expansion_cache[cache_key] = result.copy()\\n        return result\\n\\n    def clear_query_cache(self) -> int:\\n        \\\"\\\"\\\"\\n        Clear the query expansion cache.\\n\\n        Should be called after modifying the corpus (adding documents,\\n        recomputing connections) to ensure fresh expansions.\\n\\n        Returns:\\n            Number of cache entries cleared\\n        \\\"\\\"\\\"\\n        count = len(self._query_expansion_cache)\\n        self._query_expansion_cache.clear()\\n        return count\\n\\n    def set_query_cache_size(self, max_size: int) -> None:\\n        \\\"\\\"\\\"\\n        Set the maximum size of the query expansion cache.\\n\\n        Args:\\n            max_size: Maximum number of queries to cache (must be > 0)\\n\\n        Raises:\\n            ValueError: If max_size <= 0\\n        \\\"\\\"\\\"\\n        if max_size <= 0:\\n            raise ValueError(f\\\"max_size must be positive, got {max_size}\\\")\\n        self._query_cache_max_size = max_size\\n\\n        # Trim cache if it exceeds new size\\n        while len(self._query_expansion_cache) > max_size:\\n            oldest_key = next(iter(self._query_expansion_cache))\\n            del self._query_expansion_cache[oldest_key]\\n\\n    def parse_intent_query(self, query_text: str) -> Dict:\\n        \\\"\\\"\\\"\\n        Parse a natural language query to extract intent and searchable terms.\\n\\n        Analyzes queries like \\\"where do we handle authentication?\\\" to identify:\\n        - Question word (where) -> intent type (location)\\n        - Action verb (handle) -> search for handling code\\n        - Subject (authentication) -> main topic with synonyms\\n\\n        Args:\\n            query_text: Natural language query string\\n\\n        Returns:\\n            Dict with 'action', 'subject', 'intent', 'question_word', 'expanded_terms'\\n        \\\"\\\"\\\"\\n        return query_module.parse_intent_query(query_text)\\n\\n    def search_by_intent(self, query_text: str, top_n: int = 5) -> List[Tuple[str, float, Dict]]:\\n        \\\"\\\"\\\"\\n        Search the corpus using intent-based query understanding.\\n\\n        Parses the query to understand intent, expands terms using code concepts,\\n        then searches with appropriate weighting based on intent type.\\n\\n        Args:\\n            query_text: Natural language query string\\n            top_n: Number of results to return\\n\\n        Returns:\\n            List of (doc_id, score, parsed_intent) tuples\\n        \\\"\\\"\\\"\\n        return query_module.search_by_intent(\\n            query_text,\\n            self.layers,\\n            self.tokenizer,\\n            top_n=top_n\\n        )\\n\\n    def expand_query_semantic(self, query_text: str, max_expansions: int = 10) -> Dict[str, float]:\\n        return query_module.expand_query_semantic(query_text, self.layers, self.tokenizer, self.semantic_relations, max_expansions)\\n\\n    def complete_analogy(\\n        self,\\n        term_a: str,\\n        term_b: str,\\n        term_c: str,\\n        top_n: int = 5,\\n        use_embeddings: bool = True,\\n        use_relations: bool = True\\n    ) -> List[Tuple[str, float, str]]:\\n        \\\"\\\"\\\"\\n        Complete an analogy: \\\"a is to b as c is to ?\\\"\\n\\n        Uses multiple strategies to find the best completion:\\n        1. Relation matching: Find what relation connects a→b, then find terms\\n           with the same relation from c\\n        2. Vector arithmetic: Use embeddings to compute d = c + (b - a)\\n        3. Pattern matching: Find terms that co-occur with c similarly to how\\n           b co-occurs with a\\n\\n        Args:\\n            term_a: First term of the known pair (e.g., \\\"king\\\")\\n            term_b: Second term of the known pair (e.g., \\\"queen\\\")\\n            term_c: First term of the query pair (e.g., \\\"man\\\")\\n            top_n: Number of candidates to return\\n            use_embeddings: Whether to use embedding-based completion\\n            use_relations: Whether to use relation-based completion\\n\\n        Returns:\\n            List of (candidate_term, confidence, method) tuples, where method\\n            describes which approach found this candidate ('relation:IsA',\\n            'embedding', 'pattern')\\n\\n        Example:\\n            >>> processor.extract_corpus_semantics()\\n            >>> processor.compute_graph_embeddings()\\n            >>> results = processor.complete_analogy(\\\"neural\\\", \\\"networks\\\", \\\"knowledge\\\")\\n            >>> for term, score, method in results:\\n            ...     print(f\\\"{term}: {score:.3f} ({method})\\\")\\n\\n        Raises:\\n            ValueError: If any term is empty or top_n is not positive\\n        \\\"\\\"\\\"\\n        # Input validation\\n        for name, term in [('term_a', term_a), ('term_b', term_b), ('term_c', term_c)]:\\n            if not isinstance(term, str) or not term.strip():\\n                raise ValueError(f\\\"{name} must be a non-empty string\\\")\\n        if not isinstance(top_n, int) or top_n < 1:\\n            raise ValueError(\\\"top_n must be a positive integer\\\")\\n\\n        if not self.semantic_relations:\\n            self.extract_corpus_semantics(verbose=False)\\n\\n        return query_module.complete_analogy(\\n            term_a, term_b, term_c,\\n            self.layers,\\n            self.semantic_relations,\\n            embeddings=self.embeddings,\\n            top_n=top_n,\\n            use_embeddings=use_embeddings,\\n            use_relations=use_relations\\n        )\\n\\n    def complete_analogy_simple(\\n        self,\\n        term_a: str,\\n        term_b: str,\\n        term_c: str,\\n        top_n: int = 5\\n    ) -> List[Tuple[str, float]]:\\n        \\\"\\\"\\\"\\n        Simplified analogy completion using only term relationships.\\n\\n        A lighter version that doesn't require embeddings. Uses bigram patterns\\n        and co-occurrence to find analogies.\\n\\n        Args:\\n            term_a: First term of the known pair\\n            term_b: Second term of the known pair\\n            term_c: First term of the query pair\\n            top_n: Number of candidates to return\\n\\n        Returns:\\n            List of (candidate_term, confidence) tuples\\n\\n        Example:\\n            >>> results = processor.complete_analogy_simple(\\\"neural\\\", \\\"networks\\\", \\\"knowledge\\\")\\n            >>> for term, score in results:\\n            ...     print(f\\\"{term}: {score:.3f}\\\")\\n        \\\"\\\"\\\"\\n        return query_module.complete_analogy_simple(\\n            term_a, term_b, term_c,\\n            self.layers,\\n            self.tokenizer,\\n            semantic_relations=self.semantic_relations,\\n            top_n=top_n\\n        )\\n\\n    def expand_query_multihop(\\n        self,\\n        query_text: str,\\n        max_hops: int = 2,\\n        max_expansions: int = 15,\\n        decay_factor: float = 0.5,\\n        min_path_score: float = 0.2\\n    ) -> Dict[str, float]:\\n        \\\"\\\"\\\"\\n        Expand query using multi-hop semantic inference.\\n\\n        Unlike single-hop expansion that only follows direct connections,\\n        this follows relation chains to discover semantically related terms\\n        through transitive relationships.\\n\\n        Example inference chains:\\n            \\\"dog\\\" → IsA → \\\"animal\\\" → HasProperty → \\\"living\\\"\\n            \\\"neural\\\" → RelatedTo → \\\"network\\\" → RelatedTo → \\\"deep\\\"\\n\\n        Args:\\n            query_text: Original query string\\n            max_hops: Maximum number of relation hops (default: 2)\\n            max_expansions: Maximum expansion terms to return\\n            decay_factor: Weight decay per hop (default: 0.5, so hop2 = 0.25)\\n            min_path_score: Minimum path validity score to include (default: 0.2)\\n\\n        Returns:\\n            Dict mapping terms to weights (original terms get weight 1.0,\\n            expansions get decayed weights based on hop distance and path validity)\\n\\n        Example:\\n            >>> # Extract semantic relations first\\n            >>> processor.extract_corpus_semantics()\\n            >>>\\n            >>> # Multi-hop expansion\\n            >>> expanded = processor.expand_query_multihop(\\\"neural\\\", max_hops=2)\\n            >>> for term, weight in sorted(expanded.items(), key=lambda x: -x[1]):\\n            ...     print(f\\\"{term}: {weight:.3f}\\\")\\n        \\\"\\\"\\\"\\n        if not self.semantic_relations:\\n            # Fall back to regular expansion if no semantic relations\\n            return self.expand_query(query_text, max_expansions=max_expansions)\\n\\n        return query_module.expand_query_multihop(\\n            query_text,\\n            self.layers,\\n            self.tokenizer,\\n            self.semantic_relations,\\n            max_hops=max_hops,\\n            max_expansions=max_expansions,\\n            decay_factor=decay_factor,\\n            min_path_score=min_path_score\\n        )\\n    \\n    def find_documents_for_query(\\n        self,\\n        query_text: str,\\n        top_n: int = 5,\\n        use_expansion: bool = True,\\n        use_semantic: bool = True\\n    ) -> List[Tuple[str, float]]:\\n        \\\"\\\"\\\"\\n        Find documents most relevant to a query.\\n\\n        Args:\\n            query_text: Search query\\n            top_n: Number of documents to return\\n            use_expansion: Whether to expand query terms using lateral connections\\n            use_semantic: Whether to use semantic relations for expansion (if available)\\n\\n        Returns:\\n            List of (doc_id, score) tuples ranked by relevance\\n\\n        Raises:\\n            ValueError: If query_text is empty or top_n is not positive\\n        \\\"\\\"\\\"\\n        # Input validation\\n        if not isinstance(query_text, str) or not query_text.strip():\\n            raise ValueError(\\\"query_text must be a non-empty string\\\")\\n        if not isinstance(top_n, int) or top_n < 1:\\n            raise ValueError(\\\"top_n must be a positive integer\\\")\\n\\n        return query_module.find_documents_for_query(\\n            query_text,\\n            self.layers,\\n            self.tokenizer,\\n            top_n=top_n,\\n            use_expansion=use_expansion,\\n            semantic_relations=self.semantic_relations if use_semantic else None,\\n            use_semantic=use_semantic\\n        )\\n\\n    def fast_find_documents(\\n        self,\\n        query_text: str,\\n        top_n: int = 5,\\n        candidate_multiplier: int = 3,\\n        use_code_concepts: bool = True\\n    ) -> List[Tuple[str, float]]:\\n        \\\"\\\"\\\"\\n        Fast document search using candidate filtering.\\n\\n        Optimizes search by:\\n        1. Using set intersection to find candidate documents\\n        2. Only scoring top candidates fully\\n        3. Using code concept expansion for better recall\\n\\n        ~2-3x faster than find_documents_for_query on large corpora.\\n\\n        Args:\\n            query_text: Search query\\n            top_n: Number of results to return\\n            candidate_multiplier: Multiplier for candidate set size\\n            use_code_concepts: Whether to use code concept expansion\\n\\n        Returns:\\n            List of (doc_id, score) tuples ranked by relevance\\n        \\\"\\\"\\\"\\n        return query_module.fast_find_documents(\\n            query_text,\\n            self.layers,\\n            self.tokenizer,\\n            top_n=top_n,\\n            candidate_multiplier=candidate_multiplier,\\n            use_code_concepts=use_code_concepts\\n        )\\n\\n    def build_search_index(self) -> Dict[str, Dict[str, float]]:\\n        \\\"\\\"\\\"\\n        Build an optimized inverted index for fast querying.\\n\\n        Pre-compute this once, then use search_with_index() for\\n        fastest possible search.\\n\\n        Returns:\\n            Dict mapping terms to {doc_id: tfidf_score} dicts\\n        \\\"\\\"\\\"\\n        return query_module.build_document_index(self.layers)\\n\\n    def search_with_index(\\n        self,\\n        query_text: str,\\n        index: Dict[str, Dict[str, float]],\\n        top_n: int = 5\\n    ) -> List[Tuple[str, float]]:\\n        \\\"\\\"\\\"\\n        Search using a pre-built inverted index.\\n\\n        This is the fastest search method. Build the index once with\\n        build_search_index(), then reuse for multiple queries.\\n\\n        Args:\\n            query_text: Search query\\n            index: Pre-built index from build_search_index()\\n            top_n: Number of results to return\\n\\n        Returns:\\n            List of (doc_id, score) tuples ranked by relevance\\n        \\\"\\\"\\\"\\n        return query_module.search_with_index(\\n            query_text,\\n            index,\\n            self.tokenizer,\\n            top_n=top_n\\n        )\\n\\n    def find_passages_for_query(\\n        self,\\n        query_text: str,\\n        top_n: int = 5,\\n        chunk_size: int = 512,\\n        overlap: int = 128,\\n        use_expansion: bool = True,\\n        doc_filter: Optional[List[str]] = None,\\n        use_semantic: bool = True\\n    ) -> List[Tuple[str, str, int, int, float]]:\\n        \\\"\\\"\\\"\\n        Find text passages most relevant to a query (for RAG systems).\\n\\n        Instead of returning just document IDs, this returns actual text passages\\n        with position information suitable for context windows and citations.\\n\\n        Args:\\n            query_text: Search query\\n            top_n: Number of passages to return\\n            chunk_size: Size of each chunk in characters (default 512)\\n            overlap: Overlap between chunks in characters (default 128)\\n            use_expansion: Whether to expand query terms\\n            doc_filter: Optional list of doc_ids to restrict search to\\n            use_semantic: Whether to use semantic relations for expansion (if available)\\n\\n        Returns:\\n            List of (passage_text, doc_id, start_char, end_char, score) tuples\\n            ranked by relevance\\n\\n        Example:\\n            >>> results = processor.find_passages_for_query(\\\"neural networks\\\")\\n            >>> for passage, doc_id, start, end, score in results:\\n            ...     print(f\\\"[{doc_id}:{start}-{end}] {passage[:50]}... (score: {score:.3f})\\\")\\n        \\\"\\\"\\\"\\n        return query_module.find_passages_for_query(\\n            query_text,\\n            self.layers,\\n            self.tokenizer,\\n            self.documents,\\n            top_n=top_n,\\n            chunk_size=chunk_size,\\n            overlap=overlap,\\n            use_expansion=use_expansion,\\n            doc_filter=doc_filter,\\n            semantic_relations=self.semantic_relations if use_semantic else None,\\n            use_semantic=use_semantic\\n        )\\n\\n    def find_documents_batch(\\n        self,\\n        queries: List[str],\\n        top_n: int = 5,\\n        use_expansion: bool = True,\\n        use_semantic: bool = True\\n    ) -> List[List[Tuple[str, float]]]:\\n        \\\"\\\"\\\"\\n        Find documents for multiple queries efficiently.\\n\\n        More efficient than calling find_documents_for_query() multiple times\\n        because it shares tokenization and expansion caching across queries.\\n\\n        Args:\\n            queries: List of search query strings\\n            top_n: Number of documents to return per query\\n            use_expansion: Whether to expand query terms using lateral connections\\n            use_semantic: Whether to use semantic relations for expansion\\n\\n        Returns:\\n            List of results, one per query. Each result is a list of (doc_id, score) tuples.\\n\\n        Example:\\n            >>> queries = [\\\"neural networks\\\", \\\"machine learning\\\", \\\"data processing\\\"]\\n            >>> results = processor.find_documents_batch(queries, top_n=3)\\n            >>> for query, docs in zip(queries, results):\\n            ...     print(f\\\"{query}: {[doc_id for doc_id, _ in docs]}\\\")\\n        \\\"\\\"\\\"\\n        return query_module.find_documents_batch(\\n            queries,\\n            self.layers,\\n            self.tokenizer,\\n            top_n=top_n,\\n            use_expansion=use_expansion,\\n            semantic_relations=self.semantic_relations if use_semantic else None,\\n            use_semantic=use_semantic\\n        )\\n\\n    def find_passages_batch(\\n        self,\\n        queries: List[str],\\n        top_n: int = 5,\\n        chunk_size: int = 512,\\n        overlap: int = 128,\\n        use_expansion: bool = True,\\n        doc_filter: Optional[List[str]] = None,\\n        use_semantic: bool = True\\n    ) -> List[List[Tuple[str, str, int, int, float]]]:\\n        \\\"\\\"\\\"\\n        Find passages for multiple queries efficiently.\\n\\n        More efficient than calling find_passages_for_query() multiple times\\n        because it shares chunk computation and expansion caching across queries.\\n\\n        Args:\\n            queries: List of search query strings\\n            top_n: Number of passages to return per query\\n            chunk_size: Size of each chunk in characters (default 512)\\n            overlap: Overlap between chunks in characters (default 128)\\n            use_expansion: Whether to expand query terms\\n            doc_filter: Optional list of doc_ids to restrict search to\\n            use_semantic: Whether to use semantic relations for expansion\\n\\n        Returns:\\n            List of results, one per query. Each result is a list of\\n            (passage_text, doc_id, start_char, end_char, score) tuples.\\n\\n        Example:\\n            >>> queries = [\\\"neural networks\\\", \\\"deep learning\\\"]\\n            >>> results = processor.find_passages_batch(queries)\\n            >>> for query, passages in zip(queries, results):\\n            ...     print(f\\\"{query}: {len(passages)} passages found\\\")\\n        \\\"\\\"\\\"\\n        return query_module.find_passages_batch(\\n            queries,\\n            self.layers,\\n            self.tokenizer,\\n            self.documents,\\n            top_n=top_n,\\n            chunk_size=chunk_size,\\n            overlap=overlap,\\n            use_expansion=use_expansion,\\n            doc_filter=doc_filter,\\n            semantic_relations=self.semantic_relations if use_semantic else None,\\n            use_semantic=use_semantic\\n        )\\n\\n    def multi_stage_rank(\\n        self,\\n        query_text: str,\\n        top_n: int = 5,\\n        chunk_size: int = 512,\\n        overlap: int = 128,\\n        concept_boost: float = 0.3,\\n        use_expansion: bool = True,\\n        use_semantic: bool = True\\n    ) -> List[Tuple[str, str, int, int, float, Dict[str, float]]]:\\n        \\\"\\\"\\\"\\n        Multi-stage ranking pipeline for improved RAG performance.\\n\\n        Uses a 4-stage pipeline combining concept, document, and chunk signals:\\n        1. Concepts: Filter by topic relevance using Layer 2 clusters\\n        2. Documents: Rank documents within relevant topics\\n        3. Chunks: Rank passages within top documents\\n        4. Rerank: Combine all signals for final scoring\\n\\n        Args:\\n            query_text: Search query\\n            top_n: Number of passages to return\\n            chunk_size: Size of each chunk in characters (default 512)\\n            overlap: Overlap between chunks in characters (default 128)\\n            concept_boost: Weight for concept relevance (0.0-1.0, default 0.3)\\n            use_expansion: Whether to expand query terms\\n            use_semantic: Whether to use semantic relations for expansion\\n\\n        Returns:\\n            List of (passage_text, doc_id, start_char, end_char, final_score, stage_scores)\\n            tuples. stage_scores contains: concept_score, doc_score, chunk_score, final_score\\n\\n        Example:\\n            >>> results = processor.multi_stage_rank(\\\"neural networks\\\", top_n=5)\\n            >>> for passage, doc_id, start, end, score, stages in results:\\n            ...     print(f\\\"[{doc_id}] Final: {score:.3f}, Concept: {stages['concept_score']:.3f}\\\")\\n        \\\"\\\"\\\"\\n        return query_module.multi_stage_rank(\\n            query_text,\\n            self.layers,\\n            self.tokenizer,\\n            self.documents,\\n            top_n=top_n,\\n            chunk_size=chunk_size,\\n            overlap=overlap,\\n            concept_boost=concept_boost,\\n            use_expansion=use_expansion,\\n            semantic_relations=self.semantic_relations if use_semantic else None,\\n            use_semantic=use_semantic\\n        )\\n\\n    def multi_stage_rank_documents(\\n        self,\\n        query_text: str,\\n        top_n: int = 5,\\n        concept_boost: float = 0.3,\\n        use_expansion: bool = True,\\n        use_semantic: bool = True\\n    ) -> List[Tuple[str, float, Dict[str, float]]]:\\n        \\\"\\\"\\\"\\n        Multi-stage ranking for documents (without chunk scoring).\\n\\n        Uses stages 1-2 of the pipeline for document-level ranking:\\n        1. Concepts: Filter by topic relevance\\n        2. Documents: Rank by combined concept + TF-IDF scores\\n\\n        Args:\\n            query_text: Search query\\n            top_n: Number of documents to return\\n            concept_boost: Weight for concept relevance (0.0-1.0, default 0.3)\\n            use_expansion: Whether to expand query terms\\n            use_semantic: Whether to use semantic relations\\n\\n        Returns:\\n            List of (doc_id, final_score, stage_scores) tuples.\\n            stage_scores contains: concept_score, tfidf_score, combined_score\\n\\n        Example:\\n            >>> results = processor.multi_stage_rank_documents(\\\"neural networks\\\")\\n            >>> for doc_id, score, stages in results:\\n            ...     print(f\\\"{doc_id}: {score:.3f} (concept: {stages['concept_score']:.3f})\\\")\\n        \\\"\\\"\\\"\\n        return query_module.multi_stage_rank_documents(\\n            query_text,\\n            self.layers,\\n            self.tokenizer,\\n            top_n=top_n,\\n            concept_boost=concept_boost,\\n            use_expansion=use_expansion,\\n            semantic_relations=self.semantic_relations if use_semantic else None,\\n            use_semantic=use_semantic\\n        )\\n\\n    def query_expanded(self, query_text: str, top_n: int = 10, max_expansions: int = 8) -> List[Tuple[str, float]]:\\n        return query_module.query_with_spreading_activation(query_text, self.layers, self.tokenizer, top_n, max_expansions)\\n    \\n    def find_related_documents(self, doc_id: str) -> List[Tuple[str, float]]:\\n        return query_module.find_related_documents(doc_id, self.layers)\\n    \\n    def analyze_knowledge_gaps(self) -> Dict:\\n        return gaps_module.analyze_knowledge_gaps(self.layers, self.documents)\\n    \\n    def detect_anomalies(self, threshold: float = 0.3) -> List[Dict]:\\n        return gaps_module.detect_anomalies(self.layers, self.documents, threshold)\\n    \\n    def get_layer(self, layer: CorticalLayer) -> HierarchicalLayer:\\n        return self.layers[layer]\\n    \\n    def get_document_signature(self, doc_id: str, n: int = 10) -> List[Tuple[str, float]]:\\n        layer0 = self.layers[CorticalLayer.TOKENS]\\n        terms = [(col.content, col.tfidf_per_doc.get(doc_id, col.tfidf)) \\n                 for col in layer0.minicolumns.values() if doc_id in col.document_ids]\\n        return sorted(terms, key=lambda x: x[1], reverse=True)[:n]\\n    \\n    def get_corpus_summary(self) -> Dict:\\n        return persistence.get_state_summary(self.layers, self.documents)\\n\\n    # Fingerprint methods for semantic comparison\\n    def get_fingerprint(self, text: str, top_n: int = 20) -> Dict:\\n        \\\"\\\"\\\"\\n        Compute the semantic fingerprint of a text.\\n\\n        The fingerprint captures the semantic essence of the text including\\n        term weights, concept memberships, and bigrams. Fingerprints can be\\n        compared to find similar code blocks.\\n\\n        Args:\\n            text: Input text to fingerprint\\n            top_n: Number of top terms to include\\n\\n        Returns:\\n            Dict with 'terms', 'concepts', 'bigrams', 'top_terms', 'term_count'\\n        \\\"\\\"\\\"\\n        return fp_module.compute_fingerprint(text, self.tokenizer, self.layers, top_n)\\n\\n    def compare_fingerprints(self, fp1: Dict, fp2: Dict) -> Dict:\\n        \\\"\\\"\\\"\\n        Compare two fingerprints and compute similarity metrics.\\n\\n        Args:\\n            fp1: First fingerprint from get_fingerprint()\\n            fp2: Second fingerprint from get_fingerprint()\\n\\n        Returns:\\n            Dict with similarity scores and shared terms\\n        \\\"\\\"\\\"\\n        return fp_module.compare_fingerprints(fp1, fp2)\\n\\n    def explain_fingerprint(self, fp: Dict, top_n: int = 10) -> Dict:\\n        \\\"\\\"\\\"\\n        Generate a human-readable explanation of a fingerprint.\\n\\n        Args:\\n            fp: Fingerprint from get_fingerprint()\\n            top_n: Number of top items to include\\n\\n        Returns:\\n            Dict with explanation components including summary\\n        \\\"\\\"\\\"\\n        return fp_module.explain_fingerprint(fp, top_n)\\n\\n    def explain_similarity(self, fp1: Dict, fp2: Dict) -> str:\\n        \\\"\\\"\\\"\\n        Generate a human-readable explanation of why two fingerprints are similar.\\n\\n        Args:\\n            fp1: First fingerprint\\n            fp2: Second fingerprint\\n\\n        Returns:\\n            Human-readable explanation string\\n        \\\"\\\"\\\"\\n        return fp_module.explain_similarity(fp1, fp2)\\n\\n    def find_similar_texts(\\n        self,\\n        text: str,\\n        candidates: List[Tuple[str, str]],\\n        top_n: int = 5\\n    ) -> List[Tuple[str, float, Dict]]:\\n        \\\"\\\"\\\"\\n        Find texts most similar to the given text.\\n\\n        Args:\\n            text: Query text to compare\\n            candidates: List of (id, text) tuples to search\\n            top_n: Number of results to return\\n\\n        Returns:\\n            List of (id, similarity_score, comparison) tuples sorted by similarity\\n        \\\"\\\"\\\"\\n        query_fp = self.get_fingerprint(text)\\n        results = []\\n\\n        for candidate_id, candidate_text in candidates:\\n            candidate_fp = self.get_fingerprint(candidate_text)\\n            comparison = self.compare_fingerprints(query_fp, candidate_fp)\\n            results.append((candidate_id, comparison['overall_similarity'], comparison))\\n\\n        # Sort by similarity descending\\n        results.sort(key=lambda x: x[1], reverse=True)\\n        return results[:top_n]\\n\\n    def save(self, filepath: str, verbose: bool = True) -> None:\\n        \\\"\\\"\\\"\\n        Save processor state to a file.\\n\\n        Saves all computed state including embeddings and semantic relations,\\n        so they don't need to be recomputed when loading.\\n        \\\"\\\"\\\"\\n        metadata = {\\n            'has_embeddings': bool(self.embeddings),\\n            'has_relations': bool(self.semantic_relations)\\n        }\\n        persistence.save_processor(\\n            filepath,\\n            self.layers,\\n            self.documents,\\n            self.document_metadata,\\n            self.embeddings,\\n            self.semantic_relations,\\n            metadata,\\n            verbose\\n        )\\n\\n    @classmethod\\n    def load(cls, filepath: str, verbose: bool = True) -> 'CorticalTextProcessor':\\n        \\\"\\\"\\\"\\n        Load processor state from a file.\\n\\n        Restores all computed state including embeddings and semantic relations.\\n        \\\"\\\"\\\"\\n        result = persistence.load_processor(filepath, verbose)\\n        layers, documents, document_metadata, embeddings, semantic_relations, metadata = result\\n        processor = cls()\\n        processor.layers = layers\\n        processor.documents = documents\\n        processor.document_metadata = document_metadata\\n        processor.embeddings = embeddings\\n        processor.semantic_relations = semantic_relations\\n        return processor\\n    \\n    def export_graph(self, filepath: str, layer: Optional[CorticalLayer] = None, max_nodes: int = 500) -> Dict:\\n        return persistence.export_graph_json(filepath, self.layers, layer, max_nodes=max_nodes)\\n\\n    def export_conceptnet_json(\\n        self,\\n        filepath: str,\\n        include_cross_layer: bool = True,\\n        include_typed_edges: bool = True,\\n        min_weight: float = 0.0,\\n        min_confidence: float = 0.0,\\n        max_nodes_per_layer: int = 100,\\n        verbose: bool = True\\n    ) -> Dict[str, Any]:\\n        \\\"\\\"\\\"\\n        Export ConceptNet-style graph for visualization.\\n\\n        Creates a rich graph format with:\\n        - Color-coded nodes by layer (tokens=blue, bigrams=green, concepts=orange, docs=red)\\n        - Typed edges with relation types and confidence scores\\n        - Cross-layer connections (feedforward/feedback)\\n        - D3.js/Cytoscape-compatible output\\n\\n        Args:\\n            filepath: Output file path (JSON)\\n            include_cross_layer: Include feedforward/feedback edges\\n            include_typed_edges: Include typed_connections with relation types\\n            min_weight: Minimum edge weight to include\\n            min_confidence: Minimum confidence for typed edges\\n            max_nodes_per_layer: Maximum nodes per layer (by PageRank)\\n            verbose: Print progress messages\\n\\n        Returns:\\n            The exported graph data\\n\\n        Example:\\n            >>> processor.extract_corpus_semantics(verbose=False)\\n            >>> graph = processor.export_conceptnet_json(\\\"graph.json\\\")\\n            >>> # Open graph.json in D3.js or Cytoscape for visualization\\n        \\\"\\\"\\\"\\n        return persistence.export_conceptnet_json(\\n            filepath,\\n            self.layers,\\n            semantic_relations=self.semantic_relations,\\n            include_cross_layer=include_cross_layer,\\n            include_typed_edges=include_typed_edges,\\n            min_weight=min_weight,\\n            min_confidence=min_confidence,\\n            max_nodes_per_layer=max_nodes_per_layer,\\n            verbose=verbose\\n        )\\n\\n    def summarize_document(self, doc_id: str, num_sentences: int = 3) -> str:\\n        if doc_id not in self.documents: return \\\"\\\"\\n        content = self.documents[doc_id]\\n        sentences = re.split(r'(?<=[.!?])\\\\s+', content)\\n        if len(sentences) <= num_sentences: return content\\n        \\n        layer0 = self.layers[CorticalLayer.TOKENS]\\n        scored = []\\n        for sent in sentences:\\n            tokens = self.tokenizer.tokenize(sent)\\n            score = sum(layer0.get_minicolumn(t).tfidf if layer0.get_minicolumn(t) else 0 for t in tokens)\\n            scored.append((sent, score))\\n        scored.sort(key=lambda x: x[1], reverse=True)\\n        top = [s for s, _ in scored[:num_sentences]]\\n        return ' '.join([s for s in sentences if s in top])\\n    \\n    def __repr__(self) -> str:\\n        stats = self.get_corpus_summary()\\n        return f\\\"CorticalTextProcessor(documents={stats['documents']}, columns={stats['total_columns']})\\\"\\n\",",
        "      \"mtime\": 1765392979.0",
        "    },",
        "    {",
        "      \"op\": \"add\",",
        "      \"doc_id\": \"tests/test_tokenizer.py\",",
        "      \"content\": \"\\\"\\\"\\\"Tests for the Tokenizer class.\\\"\\\"\\\"\\n\\nimport unittest\\nimport sys\\nsys.path.insert(0, '..')\\n\\nfrom cortical import Tokenizer\\n\\n\\nclass TestTokenizer(unittest.TestCase):\\n    \\\"\\\"\\\"Test the Tokenizer class.\\\"\\\"\\\"\\n    \\n    def setUp(self):\\n        self.tokenizer = Tokenizer()\\n    \\n    def test_basic_tokenization(self):\\n        \\\"\\\"\\\"Test basic word extraction.\\\"\\\"\\\"\\n        tokens = self.tokenizer.tokenize(\\\"Hello world\\\")\\n        self.assertEqual(tokens, [\\\"hello\\\", \\\"world\\\"])\\n    \\n    def test_stop_word_removal(self):\\n        \\\"\\\"\\\"Test that stop words are removed.\\\"\\\"\\\"\\n        tokens = self.tokenizer.tokenize(\\\"The quick brown fox\\\")\\n        self.assertNotIn(\\\"the\\\", tokens)\\n        self.assertIn(\\\"quick\\\", tokens)\\n        self.assertIn(\\\"brown\\\", tokens)\\n        self.assertIn(\\\"fox\\\", tokens)\\n    \\n    def test_minimum_length(self):\\n        \\\"\\\"\\\"Test minimum word length filtering.\\\"\\\"\\\"\\n        tokens = self.tokenizer.tokenize(\\\"I am a test of tokenization\\\")\\n        for token in tokens:\\n            self.assertGreaterEqual(len(token), 3)\\n    \\n    def test_lowercase(self):\\n        \\\"\\\"\\\"Test that tokens are lowercased.\\\"\\\"\\\"\\n        tokens = self.tokenizer.tokenize(\\\"NEURAL Networks PROCESSING\\\")\\n        self.assertEqual(tokens, [\\\"neural\\\", \\\"networks\\\", \\\"processing\\\"])\\n    \\n    def test_alphanumeric(self):\\n        \\\"\\\"\\\"Test handling of alphanumeric tokens.\\\"\\\"\\\"\\n        tokens = self.tokenizer.tokenize(\\\"word2vec and bert3 models\\\")\\n        self.assertIn(\\\"word2vec\\\", tokens)\\n        self.assertIn(\\\"bert3\\\", tokens)\\n    \\n    def test_extract_bigrams(self):\\n        \\\"\\\"\\\"Test bigram extraction.\\\"\\\"\\\"\\n        tokens = [\\\"neural\\\", \\\"network\\\", \\\"processing\\\"]\\n        bigrams = self.tokenizer.extract_ngrams(tokens, n=2)\\n        self.assertEqual(bigrams, [\\\"neural network\\\", \\\"network processing\\\"])\\n    \\n    def test_extract_trigrams(self):\\n        \\\"\\\"\\\"Test trigram extraction.\\\"\\\"\\\"\\n        tokens = [\\\"neural\\\", \\\"network\\\", \\\"information\\\", \\\"processing\\\"]\\n        trigrams = self.tokenizer.extract_ngrams(tokens, n=3)\\n        self.assertEqual(len(trigrams), 2)\\n\\n\\nclass TestTokenizerStemming(unittest.TestCase):\\n    \\\"\\\"\\\"Test tokenizer stemming and word variants.\\\"\\\"\\\"\\n    \\n    def setUp(self):\\n        self.tokenizer = Tokenizer()\\n    \\n    def test_stem_basic(self):\\n        \\\"\\\"\\\"Test basic stemming.\\\"\\\"\\\"\\n        self.assertEqual(self.tokenizer.stem(\\\"running\\\"), \\\"runn\\\")\\n        self.assertEqual(self.tokenizer.stem(\\\"processing\\\"), \\\"process\\\")\\n    \\n    def test_stem_preserves_short_words(self):\\n        \\\"\\\"\\\"Test that short words are not stemmed.\\\"\\\"\\\"\\n        self.assertEqual(self.tokenizer.stem(\\\"run\\\"), \\\"run\\\")\\n        self.assertEqual(self.tokenizer.stem(\\\"the\\\"), \\\"the\\\")\\n    \\n    def test_get_word_variants_basic(self):\\n        \\\"\\\"\\\"Test basic word variant generation.\\\"\\\"\\\"\\n        variants = self.tokenizer.get_word_variants(\\\"bread\\\")\\n        self.assertIn(\\\"bread\\\", variants)\\n        self.assertIn(\\\"sourdough\\\", variants)\\n    \\n    def test_get_word_variants_includes_plural(self):\\n        \\\"\\\"\\\"Test that variants include plural forms.\\\"\\\"\\\"\\n        variants = self.tokenizer.get_word_variants(\\\"network\\\")\\n        self.assertIn(\\\"network\\\", variants)\\n        self.assertIn(\\\"networks\\\", variants)\\n    \\n    def test_word_mappings_brain(self):\\n        \\\"\\\"\\\"Test brain-related word mappings.\\\"\\\"\\\"\\n        variants = self.tokenizer.get_word_variants(\\\"brain\\\")\\n        self.assertIn(\\\"neural\\\", variants)\\n        self.assertIn(\\\"cortical\\\", variants)\\n\\n\\nclass TestSplitIdentifier(unittest.TestCase):\\n    \\\"\\\"\\\"Test the split_identifier function.\\\"\\\"\\\"\\n\\n    def test_camel_case(self):\\n        \\\"\\\"\\\"Test splitting camelCase identifiers.\\\"\\\"\\\"\\n        from cortical.tokenizer import split_identifier\\n        self.assertEqual(split_identifier(\\\"getUserCredentials\\\"), [\\\"get\\\", \\\"user\\\", \\\"credentials\\\"])\\n        self.assertEqual(split_identifier(\\\"processData\\\"), [\\\"process\\\", \\\"data\\\"])\\n\\n    def test_pascal_case(self):\\n        \\\"\\\"\\\"Test splitting PascalCase identifiers.\\\"\\\"\\\"\\n        from cortical.tokenizer import split_identifier\\n        self.assertEqual(split_identifier(\\\"UserCredentials\\\"), [\\\"user\\\", \\\"credentials\\\"])\\n        self.assertEqual(split_identifier(\\\"DataProcessor\\\"), [\\\"data\\\", \\\"processor\\\"])\\n\\n    def test_underscore_style(self):\\n        \\\"\\\"\\\"Test splitting underscore_style identifiers.\\\"\\\"\\\"\\n        from cortical.tokenizer import split_identifier\\n        self.assertEqual(split_identifier(\\\"get_user_data\\\"), [\\\"get\\\", \\\"user\\\", \\\"data\\\"])\\n        self.assertEqual(split_identifier(\\\"process_http_request\\\"), [\\\"process\\\", \\\"http\\\", \\\"request\\\"])\\n\\n    def test_constant_style(self):\\n        \\\"\\\"\\\"Test splitting CONSTANT_STYLE identifiers.\\\"\\\"\\\"\\n        from cortical.tokenizer import split_identifier\\n        self.assertEqual(split_identifier(\\\"MAX_RETRY_COUNT\\\"), [\\\"max\\\", \\\"retry\\\", \\\"count\\\"])\\n\\n    def test_acronyms(self):\\n        \\\"\\\"\\\"Test handling of acronyms in identifiers.\\\"\\\"\\\"\\n        from cortical.tokenizer import split_identifier\\n        self.assertEqual(split_identifier(\\\"XMLParser\\\"), [\\\"xml\\\", \\\"parser\\\"])\\n        self.assertEqual(split_identifier(\\\"parseHTTPResponse\\\"), [\\\"parse\\\", \\\"http\\\", \\\"response\\\"])\\n        self.assertEqual(split_identifier(\\\"getURLString\\\"), [\\\"get\\\", \\\"url\\\", \\\"string\\\"])\\n\\n    def test_mixed_case_with_underscore(self):\\n        \\\"\\\"\\\"Test mixed camelCase and underscore_style.\\\"\\\"\\\"\\n        from cortical.tokenizer import split_identifier\\n        result = split_identifier(\\\"get_UserData\\\")\\n        self.assertIn(\\\"get\\\", result)\\n        self.assertIn(\\\"user\\\", result)\\n        self.assertIn(\\\"data\\\", result)\\n\\n    def test_single_word(self):\\n        \\\"\\\"\\\"Test single word identifiers.\\\"\\\"\\\"\\n        from cortical.tokenizer import split_identifier\\n        self.assertEqual(split_identifier(\\\"process\\\"), [\\\"process\\\"])\\n        self.assertEqual(split_identifier(\\\"data\\\"), [\\\"data\\\"])\\n\\n    def test_empty_string(self):\\n        \\\"\\\"\\\"Test empty string input.\\\"\\\"\\\"\\n        from cortical.tokenizer import split_identifier\\n        self.assertEqual(split_identifier(\\\"\\\"), [])\\n\\n\\nclass TestCodeAwareTokenization(unittest.TestCase):\\n    \\\"\\\"\\\"Test code-aware tokenization with identifier splitting.\\\"\\\"\\\"\\n\\n    def test_split_identifiers_disabled_by_default(self):\\n        \\\"\\\"\\\"Test that identifier splitting is disabled by default.\\\"\\\"\\\"\\n        tokenizer = Tokenizer()\\n        tokens = tokenizer.tokenize(\\\"getUserCredentials\\\")\\n        self.assertEqual(tokens, [\\\"getusercredentials\\\"])\\n\\n    def test_split_identifiers_enabled(self):\\n        \\\"\\\"\\\"Test tokenization with identifier splitting enabled.\\\"\\\"\\\"\\n        tokenizer = Tokenizer(split_identifiers=True)\\n        tokens = tokenizer.tokenize(\\\"getUserCredentials\\\")\\n        self.assertIn(\\\"getusercredentials\\\", tokens)\\n        self.assertIn(\\\"get\\\", tokens)\\n        self.assertIn(\\\"user\\\", tokens)\\n        self.assertIn(\\\"credentials\\\", tokens)\\n\\n    def test_split_identifiers_underscore_style(self):\\n        \\\"\\\"\\\"Test splitting underscore_style in tokenization.\\\"\\\"\\\"\\n        tokenizer = Tokenizer(split_identifiers=True)\\n        tokens = tokenizer.tokenize(\\\"process_user_data\\\")\\n        self.assertIn(\\\"process_user_data\\\", tokens)\\n        self.assertIn(\\\"process\\\", tokens)\\n        self.assertIn(\\\"user\\\", tokens)\\n        self.assertIn(\\\"data\\\", tokens)\\n\\n    def test_split_identifiers_preserves_context(self):\\n        \\\"\\\"\\\"Test that split tokens appear alongside regular tokens.\\\"\\\"\\\"\\n        tokenizer = Tokenizer(split_identifiers=True)\\n        tokens = tokenizer.tokenize(\\\"The getUserCredentials function returns data\\\")\\n        self.assertIn(\\\"getusercredentials\\\", tokens)\\n        self.assertIn(\\\"credentials\\\", tokens)\\n        self.assertIn(\\\"function\\\", tokens)\\n        self.assertIn(\\\"returns\\\", tokens)\\n        self.assertIn(\\\"data\\\", tokens)\\n\\n    def test_split_identifiers_override(self):\\n        \\\"\\\"\\\"Test overriding split_identifiers at call time.\\\"\\\"\\\"\\n        tokenizer = Tokenizer(split_identifiers=False)\\n        # Override to True\\n        tokens = tokenizer.tokenize(\\\"getUserData\\\", split_identifiers=True)\\n        self.assertIn(\\\"get\\\", tokens)\\n        self.assertIn(\\\"user\\\", tokens)\\n\\n    def test_no_duplicate_tokens(self):\\n        \\\"\\\"\\\"Test that split tokens don't create duplicates.\\\"\\\"\\\"\\n        tokenizer = Tokenizer(split_identifiers=True)\\n        tokens = tokenizer.tokenize(\\\"data process_data getData\\\")\\n        # 'data' should appear only once\\n        self.assertEqual(tokens.count(\\\"data\\\"), 1)\\n\\n    def test_stop_words_filtered_from_splits(self):\\n        \\\"\\\"\\\"Test that stop words in split parts are filtered.\\\"\\\"\\\"\\n        tokenizer = Tokenizer(split_identifiers=True)\\n        # 'the' is a stop word\\n        tokens = tokenizer.tokenize(\\\"getTheData\\\")\\n        self.assertNotIn(\\\"the\\\", tokens)\\n        self.assertIn(\\\"data\\\", tokens)\\n\\n    def test_min_length_applied_to_splits(self):\\n        \\\"\\\"\\\"Test that min_word_length applies to split parts.\\\"\\\"\\\"\\n        tokenizer = Tokenizer(split_identifiers=True, min_word_length=4)\\n        tokens = tokenizer.tokenize(\\\"getUserID\\\")\\n        # 'id' is too short (length 2)\\n        self.assertNotIn(\\\"id\\\", tokens)\\n        self.assertIn(\\\"user\\\", tokens)\\n\\n\\nif __name__ == \\\"__main__\\\":\\n    unittest.main(verbosity=2)\\n\",",
        "      \"mtime\": 1765392877.0",
        "    },",
        "    {",
        "      \"op\": \"add\",",
        "      \"doc_id\": \"tests/test_query.py\",",
        "      \"content\": \"\\\"\\\"\\\"\\nQuery Module Tests\\n==================\\n\\nComprehensive tests for cortical/query.py functions.\\n\\nTests cover:\\n- Query expansion (lateral, semantic, multihop)\\n- Relation path scoring\\n- Chunking and chunk scoring\\n- Batch operations\\n- Relation discovery functions\\n- Analogy completion\\n\\\"\\\"\\\"\\n\\nimport unittest\\nfrom typing import Dict, List, Tuple\\n\\nfrom cortical import CorticalTextProcessor\\nfrom cortical.layers import CorticalLayer, HierarchicalLayer\\nfrom cortical.tokenizer import Tokenizer\\nfrom cortical.query import (\\n    expand_query,\\n    expand_query_multihop,\\n    expand_query_semantic,\\n    get_expanded_query_terms,\\n    score_relation_path,\\n    create_chunks,\\n    score_chunk,\\n    find_documents_for_query,\\n    find_passages_for_query,\\n    find_documents_batch,\\n    find_passages_batch,\\n    find_relevant_concepts,\\n    find_relation_between,\\n    find_terms_with_relation,\\n    complete_analogy,\\n    complete_analogy_simple,\\n    query_with_spreading_activation,\\n    VALID_RELATION_CHAINS,\\n)\\n\\n\\nclass TestScoreRelationPath(unittest.TestCase):\\n    \\\"\\\"\\\"Test relation path scoring.\\\"\\\"\\\"\\n\\n    def test_empty_path(self):\\n        \\\"\\\"\\\"Empty path should return 1.0.\\\"\\\"\\\"\\n        self.assertEqual(score_relation_path([]), 1.0)\\n\\n    def test_single_relation(self):\\n        \\\"\\\"\\\"Single relation path should return 1.0.\\\"\\\"\\\"\\n        self.assertEqual(score_relation_path(['IsA']), 1.0)\\n        self.assertEqual(score_relation_path(['HasProperty']), 1.0)\\n\\n    def test_valid_chain(self):\\n        \\\"\\\"\\\"Valid chain should return high score.\\\"\\\"\\\"\\n        # IsA → HasProperty is typically valid\\n        score = score_relation_path(['IsA', 'HasProperty'])\\n        self.assertGreater(score, 0.5)\\n\\n    def test_long_path_degrades(self):\\n        \\\"\\\"\\\"Longer paths should have lower scores due to multiplication.\\\"\\\"\\\"\\n        score_2 = score_relation_path(['IsA', 'HasProperty'])\\n        score_3 = score_relation_path(['IsA', 'HasProperty', 'RelatedTo'])\\n        self.assertLessEqual(score_3, score_2)\\n\\n    def test_valid_relation_chains_constant(self):\\n        \\\"\\\"\\\"VALID_RELATION_CHAINS should be a non-empty dict.\\\"\\\"\\\"\\n        self.assertIsInstance(VALID_RELATION_CHAINS, dict)\\n        self.assertGreater(len(VALID_RELATION_CHAINS), 0)\\n\\n\\nclass TestCreateChunks(unittest.TestCase):\\n    \\\"\\\"\\\"Test text chunking.\\\"\\\"\\\"\\n\\n    def test_empty_text(self):\\n        \\\"\\\"\\\"Empty text should return empty list.\\\"\\\"\\\"\\n        self.assertEqual(create_chunks(\\\"\\\"), [])\\n\\n    def test_short_text(self):\\n        \\\"\\\"\\\"Text shorter than chunk_size should return single chunk.\\\"\\\"\\\"\\n        text = \\\"Short text.\\\"\\n        chunks = create_chunks(text, chunk_size=100, overlap=20)\\n        self.assertEqual(len(chunks), 1)\\n        self.assertEqual(chunks[0][0], text)\\n        self.assertEqual(chunks[0][1], 0)  # start\\n        self.assertEqual(chunks[0][2], len(text))  # end\\n\\n    def test_chunk_overlap(self):\\n        \\\"\\\"\\\"Chunks should overlap by specified amount.\\\"\\\"\\\"\\n        text = \\\"A\\\" * 100\\n        chunks = create_chunks(text, chunk_size=50, overlap=10)\\n        # With chunk_size=50 and overlap=10, stride=40\\n        # Chunks at: 0-50, 40-90, 80-100\\n        self.assertGreater(len(chunks), 1)\\n        # First chunk ends at 50, second starts at 40\\n        if len(chunks) >= 2:\\n            first_end = chunks[0][2]\\n            second_start = chunks[1][1]\\n            self.assertLess(second_start, first_end)  # Overlap exists\\n\\n    def test_chunk_boundaries(self):\\n        \\\"\\\"\\\"Chunk boundaries should be valid.\\\"\\\"\\\"\\n        text = \\\"Hello world, this is a test of chunking functionality.\\\"\\n        chunks = create_chunks(text, chunk_size=20, overlap=5)\\n\\n        for chunk_text, start, end in chunks:\\n            self.assertEqual(chunk_text, text[start:end])\\n            self.assertLessEqual(end, len(text))\\n\\n    def test_no_overlap(self):\\n        \\\"\\\"\\\"Chunks with zero overlap should not overlap.\\\"\\\"\\\"\\n        text = \\\"A\\\" * 100\\n        chunks = create_chunks(text, chunk_size=25, overlap=0)\\n        # Should have exactly 4 chunks\\n        self.assertEqual(len(chunks), 4)\\n        # Check no overlap\\n        for i in range(len(chunks) - 1):\\n            self.assertEqual(chunks[i][2], chunks[i + 1][1])\\n\\n    def test_invalid_chunk_size_zero(self):\\n        \\\"\\\"\\\"Chunk size of zero should raise ValueError.\\\"\\\"\\\"\\n        with self.assertRaises(ValueError) as ctx:\\n            create_chunks(\\\"test\\\", chunk_size=0)\\n        self.assertIn(\\\"chunk_size\\\", str(ctx.exception))\\n\\n    def test_invalid_chunk_size_negative(self):\\n        \\\"\\\"\\\"Negative chunk size should raise ValueError.\\\"\\\"\\\"\\n        with self.assertRaises(ValueError) as ctx:\\n            create_chunks(\\\"test\\\", chunk_size=-10)\\n        self.assertIn(\\\"chunk_size\\\", str(ctx.exception))\\n\\n    def test_invalid_overlap_negative(self):\\n        \\\"\\\"\\\"Negative overlap should raise ValueError.\\\"\\\"\\\"\\n        with self.assertRaises(ValueError) as ctx:\\n            create_chunks(\\\"test\\\", chunk_size=50, overlap=-5)\\n        self.assertIn(\\\"overlap\\\", str(ctx.exception))\\n\\n    def test_invalid_overlap_greater_than_chunk_size(self):\\n        \\\"\\\"\\\"Overlap >= chunk_size should raise ValueError.\\\"\\\"\\\"\\n        with self.assertRaises(ValueError) as ctx:\\n            create_chunks(\\\"test\\\", chunk_size=50, overlap=50)\\n        self.assertIn(\\\"overlap\\\", str(ctx.exception))\\n\\n        with self.assertRaises(ValueError):\\n            create_chunks(\\\"test\\\", chunk_size=50, overlap=100)\\n\\n\\nclass TestFindRelationBetween(unittest.TestCase):\\n    \\\"\\\"\\\"Test finding relations between terms.\\\"\\\"\\\"\\n\\n    def setUp(self):\\n        \\\"\\\"\\\"Set up sample relations.\\\"\\\"\\\"\\n        self.relations = [\\n            (\\\"dog\\\", \\\"IsA\\\", \\\"animal\\\", 1.0),\\n            (\\\"cat\\\", \\\"IsA\\\", \\\"animal\\\", 1.0),\\n            (\\\"dog\\\", \\\"HasProperty\\\", \\\"loyal\\\", 0.8),\\n            (\\\"neural\\\", \\\"RelatedTo\\\", \\\"networks\\\", 0.7),\\n            (\\\"machine\\\", \\\"RelatedTo\\\", \\\"learning\\\", 0.9),\\n        ]\\n\\n    def test_forward_relation(self):\\n        \\\"\\\"\\\"Find relation in forward direction.\\\"\\\"\\\"\\n        results = find_relation_between(\\\"dog\\\", \\\"animal\\\", self.relations)\\n        self.assertEqual(len(results), 1)\\n        self.assertEqual(results[0][0], \\\"IsA\\\")\\n        self.assertEqual(results[0][1], 1.0)\\n\\n    def test_reverse_relation(self):\\n        \\\"\\\"\\\"Find relation in reverse direction with penalty.\\\"\\\"\\\"\\n        results = find_relation_between(\\\"animal\\\", \\\"dog\\\", self.relations)\\n        self.assertEqual(len(results), 1)\\n        self.assertEqual(results[0][0], \\\"IsA\\\")\\n        self.assertLess(results[0][1], 1.0)  # Penalty applied\\n\\n    def test_no_relation(self):\\n        \\\"\\\"\\\"Return empty list when no relation exists.\\\"\\\"\\\"\\n        results = find_relation_between(\\\"dog\\\", \\\"neural\\\", self.relations)\\n        self.assertEqual(results, [])\\n\\n    def test_multiple_relations(self):\\n        \\\"\\\"\\\"Multiple relations between same terms.\\\"\\\"\\\"\\n        relations = [\\n            (\\\"dog\\\", \\\"IsA\\\", \\\"animal\\\", 1.0),\\n            (\\\"dog\\\", \\\"RelatedTo\\\", \\\"animal\\\", 0.5),\\n        ]\\n        results = find_relation_between(\\\"dog\\\", \\\"animal\\\", relations)\\n        self.assertEqual(len(results), 2)\\n        # Should be sorted by weight\\n        self.assertEqual(results[0][0], \\\"IsA\\\")\\n\\n\\nclass TestFindTermsWithRelation(unittest.TestCase):\\n    \\\"\\\"\\\"Test finding terms with specific relation.\\\"\\\"\\\"\\n\\n    def setUp(self):\\n        \\\"\\\"\\\"Set up sample relations.\\\"\\\"\\\"\\n        self.relations = [\\n            (\\\"dog\\\", \\\"IsA\\\", \\\"animal\\\", 1.0),\\n            (\\\"cat\\\", \\\"IsA\\\", \\\"animal\\\", 0.9),\\n            (\\\"bird\\\", \\\"IsA\\\", \\\"animal\\\", 0.8),\\n            (\\\"dog\\\", \\\"HasProperty\\\", \\\"loyal\\\", 0.8),\\n            (\\\"cat\\\", \\\"HasProperty\\\", \\\"independent\\\", 0.7),\\n        ]\\n\\n    def test_forward_direction(self):\\n        \\\"\\\"\\\"Find terms in forward direction (term → x).\\\"\\\"\\\"\\n        results = find_terms_with_relation(\\n            \\\"dog\\\", \\\"IsA\\\", self.relations, direction='forward'\\n        )\\n        self.assertEqual(len(results), 1)\\n        self.assertEqual(results[0][0], \\\"animal\\\")\\n\\n    def test_backward_direction(self):\\n        \\\"\\\"\\\"Find terms in backward direction (x → term).\\\"\\\"\\\"\\n        results = find_terms_with_relation(\\n            \\\"animal\\\", \\\"IsA\\\", self.relations, direction='backward'\\n        )\\n        self.assertEqual(len(results), 3)  # dog, cat, bird\\n        terms = [r[0] for r in results]\\n        self.assertIn(\\\"dog\\\", terms)\\n        self.assertIn(\\\"cat\\\", terms)\\n        self.assertIn(\\\"bird\\\", terms)\\n\\n    def test_no_matching_relation(self):\\n        \\\"\\\"\\\"Return empty when no matching relation type.\\\"\\\"\\\"\\n        results = find_terms_with_relation(\\n            \\\"dog\\\", \\\"PartOf\\\", self.relations, direction='forward'\\n        )\\n        self.assertEqual(results, [])\\n\\n    def test_results_sorted_by_weight(self):\\n        \\\"\\\"\\\"Results should be sorted by weight descending.\\\"\\\"\\\"\\n        results = find_terms_with_relation(\\n            \\\"animal\\\", \\\"IsA\\\", self.relations, direction='backward'\\n        )\\n        weights = [r[1] for r in results]\\n        self.assertEqual(weights, sorted(weights, reverse=True))\\n\\n\\nclass TestExpandQuery(unittest.TestCase):\\n    \\\"\\\"\\\"Test basic query expansion.\\\"\\\"\\\"\\n\\n    @classmethod\\n    def setUpClass(cls):\\n        \\\"\\\"\\\"Set up processor with documents.\\\"\\\"\\\"\\n        cls.processor = CorticalTextProcessor()\\n        cls.processor.process_document(\\n            \\\"doc1\\\",\\n            \\\"Neural networks are fundamental to machine learning. \\\"\\n            \\\"Deep learning uses neural architectures for complex tasks.\\\"\\n        )\\n        cls.processor.process_document(\\n            \\\"doc2\\\",\\n            \\\"Machine learning algorithms process data patterns. \\\"\\n            \\\"Neural models learn from training examples.\\\"\\n        )\\n        cls.processor.compute_all(verbose=False)\\n\\n    def test_expand_query_returns_dict(self):\\n        \\\"\\\"\\\"expand_query should return a dictionary.\\\"\\\"\\\"\\n        result = expand_query(\\n            \\\"neural networks\\\",\\n            self.processor.layers,\\n            self.processor.tokenizer\\n        )\\n        self.assertIsInstance(result, dict)\\n\\n    def test_expand_query_includes_original_terms(self):\\n        \\\"\\\"\\\"Expanded query should include original terms.\\\"\\\"\\\"\\n        result = expand_query(\\n            \\\"neural learning\\\",\\n            self.processor.layers,\\n            self.processor.tokenizer\\n        )\\n        # Original terms should have high weight\\n        self.assertIn(\\\"neural\\\", result)\\n        self.assertIn(\\\"learning\\\", result)\\n\\n    def test_expand_query_unknown_terms(self):\\n        \\\"\\\"\\\"Unknown terms should be handled gracefully.\\\"\\\"\\\"\\n        result = expand_query(\\n            \\\"xyznonexistent\\\",\\n            self.processor.layers,\\n            self.processor.tokenizer\\n        )\\n        self.assertIsInstance(result, dict)\\n\\n\\nclass TestExpandQueryMultihop(unittest.TestCase):\\n    \\\"\\\"\\\"Test multi-hop query expansion.\\\"\\\"\\\"\\n\\n    @classmethod\\n    def setUpClass(cls):\\n        \\\"\\\"\\\"Set up processor with semantic relations.\\\"\\\"\\\"\\n        cls.processor = CorticalTextProcessor()\\n        cls.processor.process_document(\\n            \\\"doc1\\\",\\n            \\\"Dogs are loyal animals. Cats are independent pets.\\\"\\n        )\\n        cls.processor.process_document(\\n            \\\"doc2\\\",\\n            \\\"Animals need food and water. Pets require care.\\\"\\n        )\\n        cls.processor.compute_all(verbose=False)\\n        cls.processor.extract_corpus_semantics(verbose=False)\\n\\n    def test_multihop_returns_dict(self):\\n        \\\"\\\"\\\"expand_query_multihop should return a dictionary.\\\"\\\"\\\"\\n        result = expand_query_multihop(\\n            \\\"dogs\\\",\\n            self.processor.layers,\\n            self.processor.tokenizer,\\n            self.processor.semantic_relations\\n        )\\n        self.assertIsInstance(result, dict)\\n\\n    def test_multihop_with_max_hops(self):\\n        \\\"\\\"\\\"max_hops should limit expansion depth.\\\"\\\"\\\"\\n        result_1 = expand_query_multihop(\\n            \\\"dogs\\\",\\n            self.processor.layers,\\n            self.processor.tokenizer,\\n            self.processor.semantic_relations,\\n            max_hops=1\\n        )\\n        result_2 = expand_query_multihop(\\n            \\\"dogs\\\",\\n            self.processor.layers,\\n            self.processor.tokenizer,\\n            self.processor.semantic_relations,\\n            max_hops=2\\n        )\\n        # More hops could mean more terms (or same if no valid chains)\\n        self.assertIsInstance(result_1, dict)\\n        self.assertIsInstance(result_2, dict)\\n\\n\\nclass TestGetExpandedQueryTerms(unittest.TestCase):\\n    \\\"\\\"\\\"Test the unified query expansion helper.\\\"\\\"\\\"\\n\\n    @classmethod\\n    def setUpClass(cls):\\n        \\\"\\\"\\\"Set up processor.\\\"\\\"\\\"\\n        cls.processor = CorticalTextProcessor()\\n        cls.processor.process_document(\\n            \\\"doc1\\\",\\n            \\\"Neural networks process information efficiently. \\\"\\n            \\\"Machine learning improves with data.\\\"\\n        )\\n        cls.processor.compute_all(verbose=False)\\n        cls.processor.extract_corpus_semantics(verbose=False)\\n\\n    def test_get_expanded_returns_dict(self):\\n        \\\"\\\"\\\"get_expanded_query_terms should return dict.\\\"\\\"\\\"\\n        result = get_expanded_query_terms(\\n            \\\"neural\\\",\\n            self.processor.layers,\\n            self.processor.tokenizer,\\n            self.processor.semantic_relations\\n        )\\n        self.assertIsInstance(result, dict)\\n\\n    def test_max_expansions_limits_results(self):\\n        \\\"\\\"\\\"max_expansions should limit number of terms.\\\"\\\"\\\"\\n        result = get_expanded_query_terms(\\n            \\\"neural networks machine\\\",\\n            self.processor.layers,\\n            self.processor.tokenizer,\\n            self.processor.semantic_relations,\\n            max_expansions=5\\n        )\\n        # Should have at most 5 expansion terms + original terms\\n        self.assertIsInstance(result, dict)\\n\\n    def test_semantic_discount_affects_weights(self):\\n        \\\"\\\"\\\"semantic_discount should reduce semantic expansion weights.\\\"\\\"\\\"\\n        result_high = get_expanded_query_terms(\\n            \\\"neural\\\",\\n            self.processor.layers,\\n            self.processor.tokenizer,\\n            self.processor.semantic_relations,\\n            semantic_discount=1.0\\n        )\\n        result_low = get_expanded_query_terms(\\n            \\\"neural\\\",\\n            self.processor.layers,\\n            self.processor.tokenizer,\\n            self.processor.semantic_relations,\\n            semantic_discount=0.1\\n        )\\n        self.assertIsInstance(result_high, dict)\\n        self.assertIsInstance(result_low, dict)\\n\\n\\nclass TestFindDocumentsForQuery(unittest.TestCase):\\n    \\\"\\\"\\\"Test document retrieval.\\\"\\\"\\\"\\n\\n    @classmethod\\n    def setUpClass(cls):\\n        \\\"\\\"\\\"Set up processor with documents.\\\"\\\"\\\"\\n        cls.processor = CorticalTextProcessor()\\n        cls.processor.process_document(\\n            \\\"neural_doc\\\",\\n            \\\"Neural networks are powerful models for pattern recognition. \\\"\\n            \\\"Deep learning architectures use multiple neural layers.\\\"\\n        )\\n        cls.processor.process_document(\\n            \\\"ml_doc\\\",\\n            \\\"Machine learning algorithms learn from data. \\\"\\n            \\\"Supervised learning requires labeled examples.\\\"\\n        )\\n        cls.processor.process_document(\\n            \\\"unrelated_doc\\\",\\n            \\\"The weather today is sunny with clear skies. \\\"\\n            \\\"Tomorrow expects rain in the afternoon.\\\"\\n        )\\n        cls.processor.compute_all(verbose=False)\\n\\n    def test_returns_list_of_tuples(self):\\n        \\\"\\\"\\\"Should return list of (doc_id, score) tuples.\\\"\\\"\\\"\\n        results = find_documents_for_query(\\n            \\\"neural networks\\\",\\n            self.processor.layers,\\n            self.processor.tokenizer\\n        )\\n        self.assertIsInstance(results, list)\\n        if results:\\n            self.assertEqual(len(results[0]), 2)\\n            self.assertIsInstance(results[0][0], str)\\n            self.assertIsInstance(results[0][1], float)\\n\\n    def test_relevant_docs_ranked_higher(self):\\n        \\\"\\\"\\\"Relevant documents should be ranked higher.\\\"\\\"\\\"\\n        results = find_documents_for_query(\\n            \\\"neural networks deep learning\\\",\\n            self.processor.layers,\\n            self.processor.tokenizer,\\n            top_n=3\\n        )\\n        if len(results) >= 2:\\n            doc_ids = [r[0] for r in results]\\n            # neural_doc should rank higher than unrelated_doc\\n            if \\\"neural_doc\\\" in doc_ids and \\\"unrelated_doc\\\" in doc_ids:\\n                self.assertLess(\\n                    doc_ids.index(\\\"neural_doc\\\"),\\n                    doc_ids.index(\\\"unrelated_doc\\\")\\n                )\\n\\n    def test_top_n_limits_results(self):\\n        \\\"\\\"\\\"top_n should limit number of results.\\\"\\\"\\\"\\n        results = find_documents_for_query(\\n            \\\"learning\\\",\\n            self.processor.layers,\\n            self.processor.tokenizer,\\n            top_n=1\\n        )\\n        self.assertLessEqual(len(results), 1)\\n\\n\\nclass TestFindDocumentsBatch(unittest.TestCase):\\n    \\\"\\\"\\\"Test batch document retrieval.\\\"\\\"\\\"\\n\\n    @classmethod\\n    def setUpClass(cls):\\n        \\\"\\\"\\\"Set up processor with documents.\\\"\\\"\\\"\\n        cls.processor = CorticalTextProcessor()\\n        cls.processor.process_document(\\\"doc1\\\", \\\"Neural networks learn patterns.\\\")\\n        cls.processor.process_document(\\\"doc2\\\", \\\"Machine learning uses algorithms.\\\")\\n        cls.processor.compute_all(verbose=False)\\n\\n    def test_batch_returns_list_of_lists(self):\\n        \\\"\\\"\\\"Should return list of result lists.\\\"\\\"\\\"\\n        queries = [\\\"neural\\\", \\\"machine\\\"]\\n        results = find_documents_batch(\\n            queries,\\n            self.processor.layers,\\n            self.processor.tokenizer\\n        )\\n        self.assertIsInstance(results, list)\\n        self.assertEqual(len(results), 2)\\n        for result_list in results:\\n            self.assertIsInstance(result_list, list)\\n\\n    def test_batch_empty_queries(self):\\n        \\\"\\\"\\\"Empty query list should return empty list.\\\"\\\"\\\"\\n        results = find_documents_batch(\\n            [],\\n            self.processor.layers,\\n            self.processor.tokenizer\\n        )\\n        self.assertEqual(results, [])\\n\\n\\nclass TestFindPassagesForQuery(unittest.TestCase):\\n    \\\"\\\"\\\"Test passage retrieval.\\\"\\\"\\\"\\n\\n    @classmethod\\n    def setUpClass(cls):\\n        \\\"\\\"\\\"Set up processor with documents.\\\"\\\"\\\"\\n        cls.processor = CorticalTextProcessor()\\n        cls.processor.process_document(\\n            \\\"doc1\\\",\\n            \\\"Neural networks are computational models. \\\"\\n            \\\"They process data through layers of neurons. \\\"\\n            \\\"Deep learning uses many layers for complex tasks.\\\"\\n        )\\n        cls.processor.compute_all(verbose=False)\\n\\n    def test_returns_list_of_tuples(self):\\n        \\\"\\\"\\\"Should return list with passage info.\\\"\\\"\\\"\\n        results = find_passages_for_query(\\n            \\\"neural networks\\\",\\n            self.processor.layers,\\n            self.processor.tokenizer,\\n            self.processor.documents\\n        )\\n        self.assertIsInstance(results, list)\\n        if results:\\n            # Should have (doc_id, passage_text, start, end, score)\\n            self.assertEqual(len(results[0]), 5)\\n\\n    def test_passage_contains_query_terms(self):\\n        \\\"\\\"\\\"Returned passages should be relevant to query.\\\"\\\"\\\"\\n        results = find_passages_for_query(\\n            \\\"neural networks\\\",\\n            self.processor.layers,\\n            self.processor.tokenizer,\\n            self.processor.documents,\\n            top_n=1\\n        )\\n        # Should return at least one passage\\n        self.assertIsInstance(results, list)\\n        # If results exist, check format is correct\\n        if results:\\n            doc_id, passage_text, start, end, score = results[0]\\n            self.assertIsInstance(doc_id, str)\\n            self.assertIsInstance(passage_text, str)\\n            self.assertIsInstance(score, float)\\n            self.assertGreater(len(passage_text), 0)\\n\\n\\nclass TestFindRelevantConcepts(unittest.TestCase):\\n    \\\"\\\"\\\"Test concept filtering for RAG.\\\"\\\"\\\"\\n\\n    @classmethod\\n    def setUpClass(cls):\\n        \\\"\\\"\\\"Set up processor with concepts.\\\"\\\"\\\"\\n        cls.processor = CorticalTextProcessor()\\n        cls.processor.process_document(\\n            \\\"doc1\\\",\\n            \\\"Neural networks process information. Machine learning improves results.\\\"\\n        )\\n        cls.processor.process_document(\\n            \\\"doc2\\\",\\n            \\\"Deep learning architectures use neural layers. Data processing is key.\\\"\\n        )\\n        cls.processor.compute_all(verbose=False)\\n\\n    def test_returns_list(self):\\n        \\\"\\\"\\\"Should return list of concept info.\\\"\\\"\\\"\\n        # find_relevant_concepts takes query_terms dict, not string\\n        query_terms = {\\\"neural\\\": 1.0, \\\"learning\\\": 0.8}\\n        result = find_relevant_concepts(\\n            query_terms,\\n            self.processor.layers\\n        )\\n        self.assertIsInstance(result, list)\\n\\n    def test_top_n_limits_results(self):\\n        \\\"\\\"\\\"top_n should limit results.\\\"\\\"\\\"\\n        query_terms = {\\\"neural\\\": 1.0, \\\"learning\\\": 0.8}\\n        result = find_relevant_concepts(\\n            query_terms,\\n            self.processor.layers,\\n            top_n=2\\n        )\\n        self.assertLessEqual(len(result), 2)\\n\\n\\nclass TestCompleteAnalogy(unittest.TestCase):\\n    \\\"\\\"\\\"Test analogy completion functions.\\\"\\\"\\\"\\n\\n    @classmethod\\n    def setUpClass(cls):\\n        \\\"\\\"\\\"Set up processor for analogy tests.\\\"\\\"\\\"\\n        cls.processor = CorticalTextProcessor()\\n        cls.processor.process_document(\\n            \\\"doc1\\\",\\n            \\\"Neural networks are like brain structures. \\\"\\n            \\\"Machine learning uses algorithms for patterns. \\\"\\n            \\\"Deep learning processes complex data.\\\"\\n        )\\n        cls.processor.process_document(\\n            \\\"doc2\\\",\\n            \\\"Data science analyzes information. \\\"\\n            \\\"Neural processing enables artificial intelligence.\\\"\\n        )\\n        cls.processor.compute_all(verbose=False)\\n        cls.processor.extract_corpus_semantics(verbose=False)\\n\\n    def test_complete_analogy_returns_list(self):\\n        \\\"\\\"\\\"complete_analogy should return list.\\\"\\\"\\\"\\n        results = complete_analogy(\\n            \\\"neural\\\", \\\"networks\\\", \\\"machine\\\",\\n            self.processor.layers,\\n            self.processor.semantic_relations\\n        )\\n        self.assertIsInstance(results, list)\\n\\n    def test_complete_analogy_excludes_input(self):\\n        \\\"\\\"\\\"Input terms should not appear in results.\\\"\\\"\\\"\\n        results = complete_analogy(\\n            \\\"neural\\\", \\\"networks\\\", \\\"machine\\\",\\n            self.processor.layers,\\n            self.processor.semantic_relations\\n        )\\n        result_terms = [r[0] for r in results]\\n        self.assertNotIn(\\\"neural\\\", result_terms)\\n        self.assertNotIn(\\\"networks\\\", result_terms)\\n        self.assertNotIn(\\\"machine\\\", result_terms)\\n\\n    def test_complete_analogy_simple_returns_list(self):\\n        \\\"\\\"\\\"complete_analogy_simple should return list.\\\"\\\"\\\"\\n        results = complete_analogy_simple(\\n            \\\"neural\\\", \\\"networks\\\", \\\"machine\\\",\\n            self.processor.layers,\\n            self.processor.tokenizer\\n        )\\n        self.assertIsInstance(results, list)\\n\\n    def test_complete_analogy_simple_format(self):\\n        \\\"\\\"\\\"Simple analogy results should be (term, score) tuples.\\\"\\\"\\\"\\n        results = complete_analogy_simple(\\n            \\\"neural\\\", \\\"networks\\\", \\\"learning\\\",\\n            self.processor.layers,\\n            self.processor.tokenizer,\\n            top_n=3\\n        )\\n        for result in results:\\n            self.assertEqual(len(result), 2)\\n            self.assertIsInstance(result[0], str)\\n            self.assertIsInstance(result[1], float)\\n\\n\\nclass TestQueryWithSpreadingActivation(unittest.TestCase):\\n    \\\"\\\"\\\"Test spreading activation search.\\\"\\\"\\\"\\n\\n    @classmethod\\n    def setUpClass(cls):\\n        \\\"\\\"\\\"Set up processor.\\\"\\\"\\\"\\n        cls.processor = CorticalTextProcessor()\\n        cls.processor.process_document(\\n            \\\"doc1\\\",\\n            \\\"Neural networks process signals. Deep learning improves accuracy.\\\"\\n        )\\n        cls.processor.compute_all(verbose=False)\\n\\n    def test_returns_list(self):\\n        \\\"\\\"\\\"Should return list of results.\\\"\\\"\\\"\\n        results = query_with_spreading_activation(\\n            \\\"neural\\\",\\n            self.processor.layers,\\n            self.processor.tokenizer\\n        )\\n        self.assertIsInstance(results, list)\\n\\n    def test_max_expansions_parameter(self):\\n        \\\"\\\"\\\"max_expansions parameter should be accepted.\\\"\\\"\\\"\\n        results = query_with_spreading_activation(\\n            \\\"neural\\\",\\n            self.processor.layers,\\n            self.processor.tokenizer,\\n            max_expansions=5\\n        )\\n        self.assertIsInstance(results, list)\\n\\n\\nclass TestScoreChunk(unittest.TestCase):\\n    \\\"\\\"\\\"Test chunk scoring.\\\"\\\"\\\"\\n\\n    @classmethod\\n    def setUpClass(cls):\\n        \\\"\\\"\\\"Set up processor.\\\"\\\"\\\"\\n        cls.processor = CorticalTextProcessor()\\n        cls.processor.process_document(\\n            \\\"doc1\\\",\\n            \\\"Neural networks are powerful tools for data analysis.\\\"\\n        )\\n        cls.processor.compute_all(verbose=False)\\n\\n    def test_score_chunk_returns_float(self):\\n        \\\"\\\"\\\"score_chunk should return a float.\\\"\\\"\\\"\\n        query_terms = {\\\"neural\\\": 1.0, \\\"networks\\\": 0.8}\\n        layer0 = self.processor.layers[CorticalLayer.TOKENS]\\n\\n        score = score_chunk(\\n            \\\"Neural networks process data\\\",\\n            query_terms,\\n            layer0,\\n            self.processor.tokenizer\\n        )\\n        self.assertIsInstance(score, float)\\n\\n    def test_relevant_chunk_higher_score(self):\\n        \\\"\\\"\\\"Chunks with query terms should score higher.\\\"\\\"\\\"\\n        query_terms = {\\\"neural\\\": 1.0, \\\"networks\\\": 0.8}\\n        layer0 = self.processor.layers[CorticalLayer.TOKENS]\\n\\n        relevant_score = score_chunk(\\n            \\\"Neural networks are amazing\\\",\\n            query_terms,\\n            layer0,\\n            self.processor.tokenizer\\n        )\\n        irrelevant_score = score_chunk(\\n            \\\"Weather is nice today\\\",\\n            query_terms,\\n            layer0,\\n            self.processor.tokenizer\\n        )\\n        self.assertGreaterEqual(relevant_score, irrelevant_score)\\n\\n    def test_empty_chunk(self):\\n        \\\"\\\"\\\"Empty chunk should return 0.\\\"\\\"\\\"\\n        query_terms = {\\\"neural\\\": 1.0}\\n        layer0 = self.processor.layers[CorticalLayer.TOKENS]\\n\\n        score = score_chunk(\\n            \\\"\\\",\\n            query_terms,\\n            layer0,\\n            self.processor.tokenizer\\n        )\\n        self.assertEqual(score, 0.0)\\n\\n\\nclass TestChunkScoringOptimization(unittest.TestCase):\\n    \\\"\\\"\\\"Test optimized chunk scoring functions.\\\"\\\"\\\"\\n\\n    @classmethod\\n    def setUpClass(cls):\\n        \\\"\\\"\\\"Set up processor.\\\"\\\"\\\"\\n        cls.processor = CorticalTextProcessor()\\n        cls.processor.process_document(\\n            \\\"doc1\\\",\\n            \\\"Neural networks are powerful tools for data analysis.\\\"\\n        )\\n        cls.processor.compute_all(verbose=False)\\n\\n    def test_precompute_term_cols_returns_dict(self):\\n        \\\"\\\"\\\"precompute_term_cols should return dict of Minicolumns.\\\"\\\"\\\"\\n        from cortical.query import precompute_term_cols\\n        query_terms = {\\\"neural\\\": 1.0, \\\"networks\\\": 0.8}\\n        layer0 = self.processor.layers[CorticalLayer.TOKENS]\\n\\n        term_cols = precompute_term_cols(query_terms, layer0)\\n\\n        self.assertIsInstance(term_cols, dict)\\n        self.assertIn(\\\"neural\\\", term_cols)\\n        self.assertIn(\\\"networks\\\", term_cols)\\n\\n    def test_precompute_term_cols_excludes_unknown(self):\\n        \\\"\\\"\\\"precompute_term_cols should exclude terms not in corpus.\\\"\\\"\\\"\\n        from cortical.query import precompute_term_cols\\n        query_terms = {\\\"neural\\\": 1.0, \\\"xyz_unknown\\\": 0.5}\\n        layer0 = self.processor.layers[CorticalLayer.TOKENS]\\n\\n        term_cols = precompute_term_cols(query_terms, layer0)\\n\\n        self.assertIn(\\\"neural\\\", term_cols)\\n        self.assertNotIn(\\\"xyz_unknown\\\", term_cols)\\n\\n    def test_score_chunk_fast_matches_regular(self):\\n        \\\"\\\"\\\"score_chunk_fast should produce same results as score_chunk.\\\"\\\"\\\"\\n        from cortical.query import precompute_term_cols, score_chunk_fast\\n        query_terms = {\\\"neural\\\": 1.0, \\\"networks\\\": 0.8}\\n        layer0 = self.processor.layers[CorticalLayer.TOKENS]\\n        chunk_text = \\\"Neural networks process data\\\"\\n\\n        # Regular score\\n        regular_score = score_chunk(\\n            chunk_text, query_terms, layer0, self.processor.tokenizer\\n        )\\n\\n        # Fast score\\n        term_cols = precompute_term_cols(query_terms, layer0)\\n        chunk_tokens = self.processor.tokenizer.tokenize(chunk_text)\\n        fast_score = score_chunk_fast(chunk_tokens, query_terms, term_cols)\\n\\n        self.assertAlmostEqual(regular_score, fast_score, places=6)\\n\\n    def test_score_chunk_fast_empty_tokens(self):\\n        \\\"\\\"\\\"score_chunk_fast should handle empty tokens list.\\\"\\\"\\\"\\n        from cortical.query import score_chunk_fast\\n        query_terms = {\\\"neural\\\": 1.0}\\n        term_cols = {}\\n\\n        score = score_chunk_fast([], query_terms, term_cols)\\n        self.assertEqual(score, 0.0)\\n\\n\\nclass TestEdgeCases(unittest.TestCase):\\n    \\\"\\\"\\\"Test edge cases and error handling.\\\"\\\"\\\"\\n\\n    @classmethod\\n    def setUpClass(cls):\\n        \\\"\\\"\\\"Set up empty and minimal processors.\\\"\\\"\\\"\\n        cls.empty_processor = CorticalTextProcessor()\\n\\n        cls.minimal_processor = CorticalTextProcessor()\\n        cls.minimal_processor.process_document(\\\"doc1\\\", \\\"Hello world\\\")\\n        cls.minimal_processor.compute_all(verbose=False)\\n\\n    def test_expand_query_empty_corpus(self):\\n        \\\"\\\"\\\"expand_query should handle empty corpus.\\\"\\\"\\\"\\n        result = expand_query(\\n            \\\"test query\\\",\\n            self.empty_processor.layers,\\n            self.empty_processor.tokenizer\\n        )\\n        self.assertIsInstance(result, dict)\\n\\n    def test_find_documents_empty_corpus(self):\\n        \\\"\\\"\\\"find_documents should handle empty corpus.\\\"\\\"\\\"\\n        result = find_documents_for_query(\\n            \\\"test\\\",\\n            self.empty_processor.layers,\\n            self.empty_processor.tokenizer\\n        )\\n        self.assertEqual(result, [])\\n\\n    def test_find_relation_empty_relations(self):\\n        \\\"\\\"\\\"find_relation_between should handle empty relations.\\\"\\\"\\\"\\n        result = find_relation_between(\\\"a\\\", \\\"b\\\", [])\\n        self.assertEqual(result, [])\\n\\n    def test_find_terms_empty_relations(self):\\n        \\\"\\\"\\\"find_terms_with_relation should handle empty relations.\\\"\\\"\\\"\\n        result = find_terms_with_relation(\\\"a\\\", \\\"IsA\\\", [])\\n        self.assertEqual(result, [])\\n\\n\\nif __name__ == '__main__':\\n    unittest.main()\\n\",",
        "      \"mtime\": 1765392877.0",
        "    },",
        "    {",
        "      \"op\": \"add\",",
        "      \"doc_id\": \"tests/test_intent_query.py\",",
        "      \"content\": \"\\\"\\\"\\\"\\nTests for intent-based query understanding.\\n\\nTests the parse_intent_query and search_by_intent functions\\nused for natural language code search.\\n\\\"\\\"\\\"\\n\\nimport unittest\\nfrom cortical.query import (\\n    parse_intent_query,\\n    search_by_intent,\\n    QUESTION_INTENTS,\\n    ACTION_VERBS,\\n    ParsedIntent,\\n)\\n\\n\\nclass TestParseIntentQuery(unittest.TestCase):\\n    \\\"\\\"\\\"Test the parse_intent_query function.\\\"\\\"\\\"\\n\\n    def test_where_query(self):\\n        \\\"\\\"\\\"Test parsing 'where' queries for location intent.\\\"\\\"\\\"\\n        result = parse_intent_query(\\\"where do we handle authentication?\\\")\\n        self.assertEqual(result['intent'], 'location')\\n        self.assertEqual(result['question_word'], 'where')\\n        self.assertEqual(result['action'], 'handle')\\n        self.assertEqual(result['subject'], 'authentication')\\n\\n    def test_how_query(self):\\n        \\\"\\\"\\\"Test parsing 'how' queries for implementation intent.\\\"\\\"\\\"\\n        result = parse_intent_query(\\\"how do we validate user input?\\\")\\n        self.assertEqual(result['intent'], 'implementation')\\n        self.assertEqual(result['question_word'], 'how')\\n        self.assertEqual(result['action'], 'validate')\\n        self.assertIn('user', [result['subject'], result['expanded_terms']])\\n\\n    def test_what_query(self):\\n        \\\"\\\"\\\"Test parsing 'what' queries for definition intent.\\\"\\\"\\\"\\n        result = parse_intent_query(\\\"what is the database schema?\\\")\\n        self.assertEqual(result['intent'], 'definition')\\n        self.assertEqual(result['question_word'], 'what')\\n\\n    def test_why_query(self):\\n        \\\"\\\"\\\"Test parsing 'why' queries for rationale intent.\\\"\\\"\\\"\\n        result = parse_intent_query(\\\"why do we cache this data?\\\")\\n        self.assertEqual(result['intent'], 'rationale')\\n        self.assertEqual(result['question_word'], 'why')\\n\\n    def test_when_query(self):\\n        \\\"\\\"\\\"Test parsing 'when' queries for lifecycle intent.\\\"\\\"\\\"\\n        result = parse_intent_query(\\\"when does initialization happen?\\\")\\n        self.assertEqual(result['intent'], 'lifecycle')\\n        self.assertEqual(result['question_word'], 'when')\\n\\n    def test_no_question_word(self):\\n        \\\"\\\"\\\"Test parsing queries without question words.\\\"\\\"\\\"\\n        result = parse_intent_query(\\\"fetch user data\\\")\\n        self.assertEqual(result['intent'], 'search')\\n        self.assertIsNone(result['question_word'])\\n        self.assertEqual(result['action'], 'fetch')\\n\\n    def test_empty_query(self):\\n        \\\"\\\"\\\"Test parsing empty query.\\\"\\\"\\\"\\n        result = parse_intent_query(\\\"\\\")\\n        self.assertEqual(result['intent'], 'search')\\n        self.assertIsNone(result['action'])\\n        self.assertIsNone(result['subject'])\\n        self.assertEqual(result['expanded_terms'], [])\\n\\n    def test_punctuation_handling(self):\\n        \\\"\\\"\\\"Test that punctuation is handled correctly.\\\"\\\"\\\"\\n        result = parse_intent_query(\\\"where is authentication???\\\")\\n        self.assertEqual(result['intent'], 'location')\\n        self.assertIn('authentication', result['expanded_terms'])\\n\\n    def test_expanded_terms_include_synonyms(self):\\n        \\\"\\\"\\\"Test that expanded terms include code concept synonyms.\\\"\\\"\\\"\\n        result = parse_intent_query(\\\"how to fetch data\\\")\\n        # 'fetch' should expand to include related terms\\n        self.assertIn('fetch', result['expanded_terms'])\\n        # Should have some related terms (from retrieval group)\\n        self.assertGreater(len(result['expanded_terms']), 1)\\n\\n    def test_action_verb_detection(self):\\n        \\\"\\\"\\\"Test detection of various action verbs.\\\"\\\"\\\"\\n        test_cases = [\\n            (\\\"validate input\\\", \\\"validate\\\"),\\n            (\\\"process request\\\", \\\"process\\\"),\\n            (\\\"save user data\\\", \\\"save\\\"),\\n            (\\\"delete old records\\\", \\\"delete\\\"),\\n            (\\\"transform response\\\", \\\"transform\\\"),\\n        ]\\n        for query, expected_action in test_cases:\\n            result = parse_intent_query(query)\\n            self.assertEqual(result['action'], expected_action,\\n                           f\\\"Failed for query: {query}\\\")\\n\\n    def test_subject_extraction(self):\\n        \\\"\\\"\\\"Test extraction of query subject.\\\"\\\"\\\"\\n        result = parse_intent_query(\\\"handle errors gracefully\\\")\\n        self.assertEqual(result['subject'], 'errors')\\n\\n    def test_filler_words_removed(self):\\n        \\\"\\\"\\\"Test that filler words don't become subject/action.\\\"\\\"\\\"\\n        result = parse_intent_query(\\\"do we have a database connection?\\\")\\n        self.assertNotEqual(result['subject'], 'do')\\n        self.assertNotEqual(result['subject'], 'we')\\n        self.assertNotEqual(result['subject'], 'have')\\n\\n\\nclass TestQuestionIntents(unittest.TestCase):\\n    \\\"\\\"\\\"Test the QUESTION_INTENTS mapping.\\\"\\\"\\\"\\n\\n    def test_all_question_words_mapped(self):\\n        \\\"\\\"\\\"Test that common question words are mapped.\\\"\\\"\\\"\\n        expected_words = ['where', 'how', 'what', 'why', 'when', 'which', 'who']\\n        for word in expected_words:\\n            self.assertIn(word, QUESTION_INTENTS)\\n\\n    def test_intent_types(self):\\n        \\\"\\\"\\\"Test that intent types are meaningful.\\\"\\\"\\\"\\n        self.assertEqual(QUESTION_INTENTS['where'], 'location')\\n        self.assertEqual(QUESTION_INTENTS['how'], 'implementation')\\n        self.assertEqual(QUESTION_INTENTS['what'], 'definition')\\n        self.assertEqual(QUESTION_INTENTS['why'], 'rationale')\\n\\n\\nclass TestActionVerbs(unittest.TestCase):\\n    \\\"\\\"\\\"Test the ACTION_VERBS set.\\\"\\\"\\\"\\n\\n    def test_common_verbs_included(self):\\n        \\\"\\\"\\\"Test that common programming action verbs are included.\\\"\\\"\\\"\\n        expected_verbs = [\\n            'handle', 'process', 'create', 'delete', 'update', 'fetch',\\n            'validate', 'parse', 'transform', 'authenticate', 'initialize'\\n        ]\\n        for verb in expected_verbs:\\n            self.assertIn(verb, ACTION_VERBS)\\n\\n    def test_is_frozenset(self):\\n        \\\"\\\"\\\"Test that ACTION_VERBS is immutable.\\\"\\\"\\\"\\n        self.assertIsInstance(ACTION_VERBS, frozenset)\\n\\n\\nclass TestSearchByIntent(unittest.TestCase):\\n    \\\"\\\"\\\"Test the search_by_intent function.\\\"\\\"\\\"\\n\\n    def setUp(self):\\n        \\\"\\\"\\\"Set up test processor.\\\"\\\"\\\"\\n        from cortical import CorticalTextProcessor\\n        self.processor = CorticalTextProcessor()\\n        self.processor.process_document(\\\"auth_handler\\\", \\\"\\\"\\\"\\n            Authentication handler module.\\n            This module handles user authentication and login.\\n            It validates credentials and creates sessions.\\n        \\\"\\\"\\\")\\n        self.processor.process_document(\\\"data_fetcher\\\", \\\"\\\"\\\"\\n            Data fetching utilities.\\n            Functions to fetch and retrieve data from external APIs.\\n            Handles HTTP requests and response parsing.\\n        \\\"\\\"\\\")\\n        self.processor.process_document(\\\"validator\\\", \\\"\\\"\\\"\\n            Input validation module.\\n            Validates and sanitizes user input.\\n            Checks for required fields and data types.\\n        \\\"\\\"\\\")\\n        self.processor.compute_all()\\n\\n    def test_search_returns_results(self):\\n        \\\"\\\"\\\"Test that search returns results.\\\"\\\"\\\"\\n        results = self.processor.search_by_intent(\\\"where do we handle authentication?\\\")\\n        self.assertIsInstance(results, list)\\n        # Should find auth_handler document\\n        if results:\\n            doc_ids = [r[0] for r in results]\\n            self.assertIn('auth_handler', doc_ids)\\n\\n    def test_search_returns_parsed_intent(self):\\n        \\\"\\\"\\\"Test that search returns parsed intent with results.\\\"\\\"\\\"\\n        results = self.processor.search_by_intent(\\\"how to validate input?\\\")\\n        if results:\\n            doc_id, score, parsed = results[0]\\n            self.assertIn('intent', parsed)\\n            self.assertIn('action', parsed)\\n            self.assertIn('expanded_terms', parsed)\\n\\n    def test_search_empty_query(self):\\n        \\\"\\\"\\\"Test search with empty query.\\\"\\\"\\\"\\n        results = self.processor.search_by_intent(\\\"\\\")\\n        self.assertEqual(results, [])\\n\\n    def test_search_top_n_limit(self):\\n        \\\"\\\"\\\"Test that top_n limits results.\\\"\\\"\\\"\\n        results = self.processor.search_by_intent(\\\"fetch data\\\", top_n=2)\\n        self.assertLessEqual(len(results), 2)\\n\\n    def test_processor_parse_intent_query(self):\\n        \\\"\\\"\\\"Test the processor wrapper for parse_intent_query.\\\"\\\"\\\"\\n        result = self.processor.parse_intent_query(\\\"where is the login function?\\\")\\n        self.assertEqual(result['intent'], 'location')\\n        # 'login' is detected as action verb, so 'function' becomes subject\\n        self.assertEqual(result['action'], 'login')\\n        self.assertEqual(result['subject'], 'function')\\n\\n\\nclass TestParsedIntentStructure(unittest.TestCase):\\n    \\\"\\\"\\\"Test the ParsedIntent TypedDict structure.\\\"\\\"\\\"\\n\\n    def test_all_keys_present(self):\\n        \\\"\\\"\\\"Test that all expected keys are in parsed result.\\\"\\\"\\\"\\n        result = parse_intent_query(\\\"where do we handle errors?\\\")\\n        expected_keys = ['action', 'subject', 'intent', 'question_word', 'expanded_terms']\\n        for key in expected_keys:\\n            self.assertIn(key, result)\\n\\n    def test_expanded_terms_is_list(self):\\n        \\\"\\\"\\\"Test that expanded_terms is a list.\\\"\\\"\\\"\\n        result = parse_intent_query(\\\"handle authentication\\\")\\n        self.assertIsInstance(result['expanded_terms'], list)\\n\\n    def test_no_duplicate_expanded_terms(self):\\n        \\\"\\\"\\\"Test that expanded_terms has no duplicates.\\\"\\\"\\\"\\n        result = parse_intent_query(\\\"handle handle authentication\\\")\\n        self.assertEqual(\\n            len(result['expanded_terms']),\\n            len(set(result['expanded_terms']))\\n        )\\n\\n\\nif __name__ == '__main__':\\n    unittest.main()\\n\",",
        "      \"mtime\": 1765392877.0",
        "    },",
        "    {",
        "      \"op\": \"add\",",
        "      \"doc_id\": \"tests/test_semantics.py\",",
        "      \"content\": \"\\\"\\\"\\\"Tests for the semantics module.\\\"\\\"\\\"\\n\\nimport unittest\\nimport sys\\nsys.path.insert(0, '..')\\n\\nfrom cortical import CorticalTextProcessor, CorticalLayer\\nfrom cortical.semantics import (\\n    extract_corpus_semantics,\\n    retrofit_connections,\\n    retrofit_embeddings,\\n    get_relation_type_weight,\\n    RELATION_WEIGHTS,\\n    RELATION_PATTERNS,\\n    build_isa_hierarchy,\\n    get_ancestors,\\n    get_descendants,\\n    inherit_properties,\\n    compute_property_similarity,\\n    apply_inheritance_to_connections,\\n    extract_pattern_relations,\\n    get_pattern_statistics\\n)\\nfrom cortical.embeddings import compute_graph_embeddings\\n\\n\\nclass TestSemantics(unittest.TestCase):\\n    \\\"\\\"\\\"Test the semantics module.\\\"\\\"\\\"\\n\\n    @classmethod\\n    def setUpClass(cls):\\n        \\\"\\\"\\\"Set up processor with sample data.\\\"\\\"\\\"\\n        cls.processor = CorticalTextProcessor()\\n        cls.processor.process_document(\\\"doc1\\\", \\\"\\\"\\\"\\n            Neural networks are a type of machine learning model.\\n            Deep learning uses neural networks for pattern recognition.\\n            Neural processing happens in the brain cortex.\\n        \\\"\\\"\\\")\\n        cls.processor.process_document(\\\"doc2\\\", \\\"\\\"\\\"\\n            Machine learning algorithms learn from data examples.\\n            Training models requires optimization techniques.\\n            Learning neural networks needs backpropagation.\\n        \\\"\\\"\\\")\\n        cls.processor.process_document(\\\"doc3\\\", \\\"\\\"\\\"\\n            The brain processes information through neurons.\\n            Cortical columns are like neural networks.\\n            Processing patterns requires learning.\\n        \\\"\\\"\\\")\\n        cls.processor.compute_all(verbose=False)\\n\\n    def test_extract_corpus_semantics(self):\\n        \\\"\\\"\\\"Test semantic relation extraction.\\\"\\\"\\\"\\n        relations = extract_corpus_semantics(\\n            self.processor.layers,\\n            self.processor.documents,\\n            self.processor.tokenizer\\n        )\\n        self.assertIsInstance(relations, list)\\n        # Should find some relations\\n        self.assertGreater(len(relations), 0)\\n\\n        # Check relation format\\n        for relation in relations:\\n            self.assertEqual(len(relation), 4)\\n            term1, rel_type, term2, weight = relation\\n            self.assertIsInstance(term1, str)\\n            self.assertIsInstance(rel_type, str)\\n            self.assertIsInstance(term2, str)\\n            self.assertIsInstance(weight, float)\\n\\n    def test_extract_corpus_semantics_cooccurs(self):\\n        \\\"\\\"\\\"Test that CoOccurs relations are found.\\\"\\\"\\\"\\n        relations = extract_corpus_semantics(\\n            self.processor.layers,\\n            self.processor.documents,\\n            self.processor.tokenizer\\n        )\\n        relation_types = set(r[1] for r in relations)\\n        self.assertIn('CoOccurs', relation_types)\\n\\n    def test_retrofit_connections(self):\\n        \\\"\\\"\\\"Test retrofitting lateral connections.\\\"\\\"\\\"\\n        relations = extract_corpus_semantics(\\n            self.processor.layers,\\n            self.processor.documents,\\n            self.processor.tokenizer\\n        )\\n\\n        stats = retrofit_connections(\\n            self.processor.layers,\\n            relations,\\n            iterations=5,\\n            alpha=0.3\\n        )\\n\\n        self.assertIsInstance(stats, dict)\\n        self.assertIn('iterations', stats)\\n        self.assertIn('alpha', stats)\\n        self.assertIn('tokens_affected', stats)\\n        self.assertIn('total_adjustment', stats)\\n        self.assertIn('relations_used', stats)\\n\\n        self.assertEqual(stats['iterations'], 5)\\n        self.assertEqual(stats['alpha'], 0.3)\\n\\n    def test_retrofit_connections_affects_weights(self):\\n        \\\"\\\"\\\"Test that retrofitting changes connection weights.\\\"\\\"\\\"\\n        # Create fresh processor\\n        processor = CorticalTextProcessor()\\n        processor.process_document(\\\"doc1\\\", \\\"neural networks learning deep\\\")\\n        processor.process_document(\\\"doc2\\\", \\\"neural learning patterns data\\\")\\n        processor.compute_all(verbose=False)\\n\\n        relations = extract_corpus_semantics(\\n            processor.layers,\\n            processor.documents,\\n            processor.tokenizer\\n        )\\n\\n        stats = retrofit_connections(\\n            processor.layers,\\n            relations,\\n            iterations=10,\\n            alpha=0.3\\n        )\\n\\n        # If there are relations, some adjustment should occur\\n        if stats['relations_used'] > 0:\\n            self.assertGreaterEqual(stats['tokens_affected'], 0)\\n\\n    def test_retrofit_embeddings(self):\\n        \\\"\\\"\\\"Test retrofitting embeddings.\\\"\\\"\\\"\\n        relations = extract_corpus_semantics(\\n            self.processor.layers,\\n            self.processor.documents,\\n            self.processor.tokenizer\\n        )\\n\\n        embeddings, _ = compute_graph_embeddings(\\n            self.processor.layers,\\n            dimensions=16,\\n            method='adjacency'\\n        )\\n\\n        stats = retrofit_embeddings(\\n            embeddings,\\n            relations,\\n            iterations=5,\\n            alpha=0.4\\n        )\\n\\n        self.assertIsInstance(stats, dict)\\n        self.assertIn('iterations', stats)\\n        self.assertIn('alpha', stats)\\n        self.assertIn('terms_retrofitted', stats)\\n        self.assertIn('total_movement', stats)\\n\\n        self.assertEqual(stats['iterations'], 5)\\n        self.assertEqual(stats['alpha'], 0.4)\\n\\n    def test_get_relation_type_weight(self):\\n        \\\"\\\"\\\"Test getting relation type weights.\\\"\\\"\\\"\\n        # Test known relation types\\n        self.assertEqual(get_relation_type_weight('IsA'), 1.5)\\n        self.assertEqual(get_relation_type_weight('SameAs'), 2.0)\\n        self.assertEqual(get_relation_type_weight('Antonym'), -0.5)\\n        self.assertEqual(get_relation_type_weight('RelatedTo'), 0.5)\\n\\n        # Test unknown relation type defaults to 0.5\\n        self.assertEqual(get_relation_type_weight('UnknownRelation'), 0.5)\\n\\n    def test_relation_weights_constant(self):\\n        \\\"\\\"\\\"Test that RELATION_WEIGHTS contains expected keys.\\\"\\\"\\\"\\n        expected_relations = ['IsA', 'PartOf', 'HasA', 'SameAs', 'RelatedTo', 'CoOccurs']\\n        for rel in expected_relations:\\n            self.assertIn(rel, RELATION_WEIGHTS)\\n\\n\\nclass TestSemanticsEmptyCorpus(unittest.TestCase):\\n    \\\"\\\"\\\"Test semantics with empty corpus.\\\"\\\"\\\"\\n\\n    def test_empty_corpus_semantics(self):\\n        \\\"\\\"\\\"Test semantic extraction on empty processor.\\\"\\\"\\\"\\n        processor = CorticalTextProcessor()\\n        relations = extract_corpus_semantics(\\n            processor.layers,\\n            processor.documents,\\n            processor.tokenizer\\n        )\\n        self.assertEqual(relations, [])\\n\\n    def test_retrofit_empty_relations(self):\\n        \\\"\\\"\\\"Test retrofitting with empty relations list.\\\"\\\"\\\"\\n        processor = CorticalTextProcessor()\\n        processor.process_document(\\\"doc1\\\", \\\"test content here\\\")\\n        processor.compute_all(verbose=False)\\n\\n        stats = retrofit_connections(\\n            processor.layers,\\n            [],  # Empty relations\\n            iterations=5,\\n            alpha=0.3\\n        )\\n\\n        self.assertEqual(stats['tokens_affected'], 0)\\n        self.assertEqual(stats['relations_used'], 0)\\n\\n\\nclass TestSemanticsWindowSize(unittest.TestCase):\\n    \\\"\\\"\\\"Test semantic extraction with different window sizes.\\\"\\\"\\\"\\n\\n    def test_larger_window_more_relations(self):\\n        \\\"\\\"\\\"Test that larger window finds more co-occurrences.\\\"\\\"\\\"\\n        processor = CorticalTextProcessor()\\n        processor.process_document(\\\"doc1\\\", \\\"\\\"\\\"\\n            word1 word2 word3 word4 word5 word6 word7 word8\\n        \\\"\\\"\\\")\\n        processor.compute_all(verbose=False)\\n\\n        relations_small = extract_corpus_semantics(\\n            processor.layers,\\n            processor.documents,\\n            processor.tokenizer,\\n            window_size=2\\n        )\\n\\n        relations_large = extract_corpus_semantics(\\n            processor.layers,\\n            processor.documents,\\n            processor.tokenizer,\\n            window_size=10\\n        )\\n\\n        # Larger window should find at least as many relations\\n        self.assertGreaterEqual(len(relations_large), len(relations_small))\\n\\n\\nclass TestIsAHierarchy(unittest.TestCase):\\n    \\\"\\\"\\\"Test IsA hierarchy building.\\\"\\\"\\\"\\n\\n    def test_build_isa_hierarchy_basic(self):\\n        \\\"\\\"\\\"Test building IsA hierarchy from relations.\\\"\\\"\\\"\\n        relations = [\\n            (\\\"dog\\\", \\\"IsA\\\", \\\"animal\\\", 1.0),\\n            (\\\"cat\\\", \\\"IsA\\\", \\\"animal\\\", 1.0),\\n            (\\\"animal\\\", \\\"IsA\\\", \\\"living_thing\\\", 1.0),\\n        ]\\n        parents, children = build_isa_hierarchy(relations)\\n\\n        self.assertIn(\\\"animal\\\", parents[\\\"dog\\\"])\\n        self.assertIn(\\\"animal\\\", parents[\\\"cat\\\"])\\n        self.assertIn(\\\"living_thing\\\", parents[\\\"animal\\\"])\\n        self.assertIn(\\\"dog\\\", children[\\\"animal\\\"])\\n        self.assertIn(\\\"cat\\\", children[\\\"animal\\\"])\\n        self.assertIn(\\\"animal\\\", children[\\\"living_thing\\\"])\\n\\n    def test_build_isa_hierarchy_empty(self):\\n        \\\"\\\"\\\"Test building hierarchy from empty relations.\\\"\\\"\\\"\\n        parents, children = build_isa_hierarchy([])\\n        self.assertEqual(parents, {})\\n        self.assertEqual(children, {})\\n\\n    def test_build_isa_hierarchy_non_isa_ignored(self):\\n        \\\"\\\"\\\"Test that non-IsA relations are ignored.\\\"\\\"\\\"\\n        relations = [\\n            (\\\"dog\\\", \\\"IsA\\\", \\\"animal\\\", 1.0),\\n            (\\\"dog\\\", \\\"HasProperty\\\", \\\"furry\\\", 0.9),\\n            (\\\"dog\\\", \\\"RelatedTo\\\", \\\"pet\\\", 0.8),\\n        ]\\n        parents, children = build_isa_hierarchy(relations)\\n\\n        # Only IsA relation should be captured\\n        self.assertEqual(len(parents), 1)\\n        self.assertIn(\\\"dog\\\", parents)\\n        self.assertEqual(parents[\\\"dog\\\"], {\\\"animal\\\"})\\n\\n\\nclass TestAncestorsDescendants(unittest.TestCase):\\n    \\\"\\\"\\\"Test ancestor and descendant traversal.\\\"\\\"\\\"\\n\\n    def setUp(self):\\n        \\\"\\\"\\\"Set up a simple hierarchy.\\\"\\\"\\\"\\n        relations = [\\n            (\\\"poodle\\\", \\\"IsA\\\", \\\"dog\\\", 1.0),\\n            (\\\"dog\\\", \\\"IsA\\\", \\\"canine\\\", 1.0),\\n            (\\\"canine\\\", \\\"IsA\\\", \\\"mammal\\\", 1.0),\\n            (\\\"mammal\\\", \\\"IsA\\\", \\\"animal\\\", 1.0),\\n            (\\\"cat\\\", \\\"IsA\\\", \\\"feline\\\", 1.0),\\n            (\\\"feline\\\", \\\"IsA\\\", \\\"mammal\\\", 1.0),\\n        ]\\n        self.parents, self.children = build_isa_hierarchy(relations)\\n\\n    def test_get_ancestors(self):\\n        \\\"\\\"\\\"Test getting ancestors of a term.\\\"\\\"\\\"\\n        ancestors = get_ancestors(\\\"poodle\\\", self.parents)\\n\\n        self.assertIn(\\\"dog\\\", ancestors)\\n        self.assertIn(\\\"canine\\\", ancestors)\\n        self.assertIn(\\\"mammal\\\", ancestors)\\n        self.assertIn(\\\"animal\\\", ancestors)\\n        self.assertEqual(ancestors[\\\"dog\\\"], 1)\\n        self.assertEqual(ancestors[\\\"canine\\\"], 2)\\n        self.assertEqual(ancestors[\\\"mammal\\\"], 3)\\n        self.assertEqual(ancestors[\\\"animal\\\"], 4)\\n\\n    def test_get_ancestors_direct_only(self):\\n        \\\"\\\"\\\"Test that max_depth limits ancestor traversal.\\\"\\\"\\\"\\n        ancestors = get_ancestors(\\\"poodle\\\", self.parents, max_depth=2)\\n\\n        self.assertIn(\\\"dog\\\", ancestors)\\n        self.assertIn(\\\"canine\\\", ancestors)\\n        self.assertNotIn(\\\"mammal\\\", ancestors)\\n\\n    def test_get_ancestors_no_parents(self):\\n        \\\"\\\"\\\"Test ancestors of a root term.\\\"\\\"\\\"\\n        ancestors = get_ancestors(\\\"animal\\\", self.parents)\\n        self.assertEqual(ancestors, {})\\n\\n    def test_get_descendants(self):\\n        \\\"\\\"\\\"Test getting descendants of a term.\\\"\\\"\\\"\\n        descendants = get_descendants(\\\"mammal\\\", self.children)\\n\\n        self.assertIn(\\\"canine\\\", descendants)\\n        self.assertIn(\\\"dog\\\", descendants)\\n        self.assertIn(\\\"poodle\\\", descendants)\\n        self.assertIn(\\\"feline\\\", descendants)\\n        self.assertIn(\\\"cat\\\", descendants)\\n\\n    def test_get_descendants_depth(self):\\n        \\\"\\\"\\\"Test descendant depths are correct.\\\"\\\"\\\"\\n        descendants = get_descendants(\\\"mammal\\\", self.children)\\n\\n        self.assertEqual(descendants[\\\"canine\\\"], 1)\\n        self.assertEqual(descendants[\\\"feline\\\"], 1)\\n        self.assertEqual(descendants[\\\"dog\\\"], 2)\\n        self.assertEqual(descendants[\\\"cat\\\"], 2)\\n        self.assertEqual(descendants[\\\"poodle\\\"], 3)\\n\\n\\nclass TestPropertyInheritance(unittest.TestCase):\\n    \\\"\\\"\\\"Test property inheritance through IsA hierarchy.\\\"\\\"\\\"\\n\\n    def test_inherit_properties_basic(self):\\n        \\\"\\\"\\\"Test basic property inheritance.\\\"\\\"\\\"\\n        relations = [\\n            (\\\"dog\\\", \\\"IsA\\\", \\\"animal\\\", 1.0),\\n            (\\\"animal\\\", \\\"HasProperty\\\", \\\"living\\\", 0.9),\\n            (\\\"animal\\\", \\\"HasProperty\\\", \\\"mortal\\\", 0.8),\\n        ]\\n        inherited = inherit_properties(relations)\\n\\n        self.assertIn(\\\"dog\\\", inherited)\\n        self.assertIn(\\\"living\\\", inherited[\\\"dog\\\"])\\n        self.assertIn(\\\"mortal\\\", inherited[\\\"dog\\\"])\\n\\n        # Check inherited weight is decayed\\n        living_weight, source, depth = inherited[\\\"dog\\\"][\\\"living\\\"]\\n        self.assertEqual(source, \\\"animal\\\")\\n        self.assertEqual(depth, 1)\\n        # Weight should be 0.9 * 0.7 (default decay) = 0.63\\n        self.assertAlmostEqual(living_weight, 0.63, places=2)\\n\\n    def test_inherit_properties_multi_level(self):\\n        \\\"\\\"\\\"Test property inheritance through multiple levels.\\\"\\\"\\\"\\n        relations = [\\n            (\\\"poodle\\\", \\\"IsA\\\", \\\"dog\\\", 1.0),\\n            (\\\"dog\\\", \\\"IsA\\\", \\\"animal\\\", 1.0),\\n            (\\\"animal\\\", \\\"HasProperty\\\", \\\"living\\\", 1.0),\\n        ]\\n        inherited = inherit_properties(relations, decay_factor=0.5)\\n\\n        # Poodle should inherit \\\"living\\\" through dog → animal\\n        self.assertIn(\\\"poodle\\\", inherited)\\n        self.assertIn(\\\"living\\\", inherited[\\\"poodle\\\"])\\n\\n        # Weight should be decayed twice: 1.0 * 0.5^2 = 0.25\\n        weight, source, depth = inherited[\\\"poodle\\\"][\\\"living\\\"]\\n        self.assertAlmostEqual(weight, 0.25, places=2)\\n        self.assertEqual(depth, 2)\\n\\n    def test_inherit_properties_empty(self):\\n        \\\"\\\"\\\"Test inheritance with no IsA relations.\\\"\\\"\\\"\\n        relations = [\\n            (\\\"dog\\\", \\\"RelatedTo\\\", \\\"pet\\\", 1.0),\\n            (\\\"dog\\\", \\\"HasProperty\\\", \\\"furry\\\", 0.9),\\n        ]\\n        inherited = inherit_properties(relations)\\n\\n        # No inheritance should occur (no IsA hierarchy)\\n        self.assertEqual(len(inherited), 0)\\n\\n    def test_inherit_properties_custom_decay(self):\\n        \\\"\\\"\\\"Test custom decay factor.\\\"\\\"\\\"\\n        relations = [\\n            (\\\"dog\\\", \\\"IsA\\\", \\\"animal\\\", 1.0),\\n            (\\\"animal\\\", \\\"HasProperty\\\", \\\"living\\\", 1.0),\\n        ]\\n\\n        inherited_slow = inherit_properties(relations, decay_factor=0.9)\\n        inherited_fast = inherit_properties(relations, decay_factor=0.3)\\n\\n        slow_weight, _, _ = inherited_slow[\\\"dog\\\"][\\\"living\\\"]\\n        fast_weight, _, _ = inherited_fast[\\\"dog\\\"][\\\"living\\\"]\\n\\n        # Slower decay should give higher weight\\n        self.assertGreater(slow_weight, fast_weight)\\n\\n    def test_inherit_properties_max_depth(self):\\n        \\\"\\\"\\\"Test max_depth limits inheritance.\\\"\\\"\\\"\\n        relations = [\\n            (\\\"a\\\", \\\"IsA\\\", \\\"b\\\", 1.0),\\n            (\\\"b\\\", \\\"IsA\\\", \\\"c\\\", 1.0),\\n            (\\\"c\\\", \\\"IsA\\\", \\\"d\\\", 1.0),\\n            (\\\"d\\\", \\\"HasProperty\\\", \\\"prop\\\", 1.0),\\n        ]\\n\\n        inherited = inherit_properties(relations, max_depth=2)\\n\\n        # 'c' is at depth 2, so it should inherit\\n        self.assertIn(\\\"c\\\", inherited)\\n        # 'a' would need depth 3 to reach 'd', so it shouldn't inherit\\n        self.assertNotIn(\\\"a\\\", inherited)\\n\\n\\nclass TestPropertySimilarity(unittest.TestCase):\\n    \\\"\\\"\\\"Test property-based similarity computation.\\\"\\\"\\\"\\n\\n    def test_compute_property_similarity_shared(self):\\n        \\\"\\\"\\\"Test similarity between terms with shared inherited properties.\\\"\\\"\\\"\\n        relations = [\\n            (\\\"dog\\\", \\\"IsA\\\", \\\"animal\\\", 1.0),\\n            (\\\"cat\\\", \\\"IsA\\\", \\\"animal\\\", 1.0),\\n            (\\\"animal\\\", \\\"HasProperty\\\", \\\"living\\\", 1.0),\\n            (\\\"animal\\\", \\\"HasProperty\\\", \\\"mortal\\\", 1.0),\\n        ]\\n        inherited = inherit_properties(relations)\\n\\n        sim = compute_property_similarity(\\\"dog\\\", \\\"cat\\\", inherited)\\n\\n        # Both inherit same properties, so similarity should be 1.0\\n        self.assertAlmostEqual(sim, 1.0, places=2)\\n\\n    def test_compute_property_similarity_disjoint(self):\\n        \\\"\\\"\\\"Test similarity between terms with no shared properties.\\\"\\\"\\\"\\n        relations = [\\n            (\\\"dog\\\", \\\"IsA\\\", \\\"animal\\\", 1.0),\\n            (\\\"car\\\", \\\"IsA\\\", \\\"vehicle\\\", 1.0),\\n            (\\\"animal\\\", \\\"HasProperty\\\", \\\"living\\\", 1.0),\\n            (\\\"vehicle\\\", \\\"HasProperty\\\", \\\"mechanical\\\", 1.0),\\n        ]\\n        inherited = inherit_properties(relations)\\n\\n        sim = compute_property_similarity(\\\"dog\\\", \\\"car\\\", inherited)\\n\\n        # No shared properties\\n        self.assertEqual(sim, 0.0)\\n\\n    def test_compute_property_similarity_partial(self):\\n        \\\"\\\"\\\"Test similarity with partial property overlap.\\\"\\\"\\\"\\n        relations = [\\n            (\\\"dog\\\", \\\"IsA\\\", \\\"pet\\\", 1.0),\\n            (\\\"cat\\\", \\\"IsA\\\", \\\"pet\\\", 1.0),\\n            (\\\"pet\\\", \\\"HasProperty\\\", \\\"domesticated\\\", 1.0),\\n            (\\\"dog\\\", \\\"IsA\\\", \\\"canine\\\", 1.0),\\n            (\\\"canine\\\", \\\"HasProperty\\\", \\\"pack_animal\\\", 1.0),\\n        ]\\n        inherited = inherit_properties(relations)\\n\\n        sim = compute_property_similarity(\\\"dog\\\", \\\"cat\\\", inherited)\\n\\n        # Partial overlap: both have \\\"domesticated\\\", only dog has \\\"pack_animal\\\"\\n        self.assertGreater(sim, 0.0)\\n        self.assertLess(sim, 1.0)\\n\\n    def test_compute_property_similarity_no_inheritance(self):\\n        \\\"\\\"\\\"Test similarity when terms have no inherited properties.\\\"\\\"\\\"\\n        inherited = {}\\n        sim = compute_property_similarity(\\\"unknown1\\\", \\\"unknown2\\\", inherited)\\n        self.assertEqual(sim, 0.0)\\n\\n\\nclass TestApplyInheritanceToConnections(unittest.TestCase):\\n    \\\"\\\"\\\"Test applying inheritance to lateral connections.\\\"\\\"\\\"\\n\\n    def test_apply_inheritance_to_connections(self):\\n        \\\"\\\"\\\"Test that inheritance boosts connections.\\\"\\\"\\\"\\n        processor = CorticalTextProcessor()\\n        processor.process_document(\\\"doc1\\\", \\\"The dog and cat are both animals.\\\")\\n        processor.compute_all(verbose=False)\\n\\n        relations = [\\n            (\\\"dog\\\", \\\"IsA\\\", \\\"animal\\\", 1.0),\\n            (\\\"cat\\\", \\\"IsA\\\", \\\"animal\\\", 1.0),\\n            (\\\"animal\\\", \\\"HasProperty\\\", \\\"living\\\", 1.0),\\n        ]\\n        inherited = inherit_properties(relations)\\n\\n        # Get initial connection weight between dog and cat\\n        layer0 = processor.get_layer(CorticalLayer.TOKENS)\\n        dog = layer0.get_minicolumn(\\\"dog\\\")\\n        cat = layer0.get_minicolumn(\\\"cat\\\")\\n\\n        if dog and cat:\\n            initial_weight = dog.lateral_connections.get(cat.id, 0)\\n\\n            stats = apply_inheritance_to_connections(\\n                processor.layers,\\n                inherited,\\n                boost_factor=0.5\\n            )\\n\\n            # Should have boosted at least one connection\\n            self.assertGreaterEqual(stats['connections_boosted'], 0)\\n\\n    def test_apply_inheritance_empty(self):\\n        \\\"\\\"\\\"Test applying empty inheritance.\\\"\\\"\\\"\\n        processor = CorticalTextProcessor()\\n        processor.process_document(\\\"doc1\\\", \\\"Test content.\\\")\\n        processor.compute_all(verbose=False)\\n\\n        stats = apply_inheritance_to_connections(\\n            processor.layers,\\n            {},  # Empty inheritance\\n            boost_factor=0.3\\n        )\\n\\n        self.assertEqual(stats['connections_boosted'], 0)\\n        self.assertEqual(stats['total_boost'], 0.0)\\n\\n\\nclass TestProcessorPropertyInheritance(unittest.TestCase):\\n    \\\"\\\"\\\"Test processor-level property inheritance methods.\\\"\\\"\\\"\\n\\n    @classmethod\\n    def setUpClass(cls):\\n        \\\"\\\"\\\"Set up processor with sample data containing IsA patterns.\\\"\\\"\\\"\\n        cls.processor = CorticalTextProcessor()\\n        # Documents with IsA patterns\\n        cls.processor.process_document(\\\"doc1\\\", \\\"\\\"\\\"\\n            A dog is a type of animal that barks.\\n            Dogs are loyal pets that live with humans.\\n            Animals are living creatures that need food.\\n        \\\"\\\"\\\")\\n        cls.processor.process_document(\\\"doc2\\\", \\\"\\\"\\\"\\n            Cats are animals that meow and purr.\\n            A cat is a popular pet in many homes.\\n            Pets are domesticated animals.\\n        \\\"\\\"\\\")\\n        cls.processor.process_document(\\\"doc3\\\", \\\"\\\"\\\"\\n            Cars are vehicles used for transportation.\\n            A vehicle is a machine that moves people.\\n            Machines are mechanical devices.\\n        \\\"\\\"\\\")\\n        cls.processor.compute_all(verbose=False)\\n\\n    def test_compute_property_inheritance_returns_stats(self):\\n        \\\"\\\"\\\"Test that compute_property_inheritance returns expected stats.\\\"\\\"\\\"\\n        stats = self.processor.compute_property_inheritance(\\n            apply_to_connections=False,\\n            verbose=False\\n        )\\n\\n        self.assertIn('terms_with_inheritance', stats)\\n        self.assertIn('total_properties_inherited', stats)\\n        self.assertIn('inherited', stats)\\n        self.assertIn('connections_boosted', stats)\\n\\n    def test_compute_property_inheritance_with_connections(self):\\n        \\\"\\\"\\\"Test inheritance applied to connections.\\\"\\\"\\\"\\n        stats = self.processor.compute_property_inheritance(\\n            apply_to_connections=True,\\n            boost_factor=0.3,\\n            verbose=False\\n        )\\n\\n        # Should have processed without error\\n        self.assertIsInstance(stats['connections_boosted'], int)\\n        self.assertIsInstance(stats['total_boost'], float)\\n\\n    def test_compute_property_similarity_method(self):\\n        \\\"\\\"\\\"Test processor compute_property_similarity method.\\\"\\\"\\\"\\n        self.processor.extract_corpus_semantics(verbose=False)\\n\\n        # Compute similarity (may be 0 if no shared properties in this corpus)\\n        sim = self.processor.compute_property_similarity(\\\"dog\\\", \\\"cat\\\")\\n        self.assertIsInstance(sim, float)\\n        self.assertGreaterEqual(sim, 0.0)\\n        self.assertLessEqual(sim, 1.0)\\n\\n    def test_compute_property_inheritance_no_relations(self):\\n        \\\"\\\"\\\"Test inheritance when no semantic relations extracted.\\\"\\\"\\\"\\n        processor = CorticalTextProcessor()\\n        processor.process_document(\\\"doc1\\\", \\\"Simple test content.\\\")\\n        processor.compute_all(verbose=False)\\n        # Don't extract semantics\\n\\n        # Should work without error (extracts semantics automatically)\\n        stats = processor.compute_property_inheritance(\\n            apply_to_connections=False,\\n            verbose=False\\n        )\\n        self.assertIn('terms_with_inheritance', stats)\\n\\n\\nclass TestPatternRelationExtraction(unittest.TestCase):\\n    \\\"\\\"\\\"Test pattern-based relation extraction.\\\"\\\"\\\"\\n\\n    def test_relation_patterns_defined(self):\\n        \\\"\\\"\\\"Test that RELATION_PATTERNS constant is defined.\\\"\\\"\\\"\\n        self.assertIsInstance(RELATION_PATTERNS, list)\\n        self.assertGreater(len(RELATION_PATTERNS), 0)\\n\\n        # Each pattern should be a tuple with 4 elements\\n        for pattern in RELATION_PATTERNS:\\n            self.assertEqual(len(pattern), 4)\\n            regex, rel_type, confidence, swap = pattern\\n            self.assertIsInstance(regex, str)\\n            self.assertIsInstance(rel_type, str)\\n            self.assertIsInstance(confidence, float)\\n            self.assertIsInstance(swap, bool)\\n\\n    def test_extract_isa_pattern(self):\\n        \\\"\\\"\\\"Test extraction of IsA relations from text patterns.\\\"\\\"\\\"\\n        docs = {\\n            \\\"doc1\\\": \\\"A dog is a type of animal. The cat is an animal too.\\\"\\n        }\\n        valid_terms = {\\\"dog\\\", \\\"animal\\\", \\\"cat\\\", \\\"type\\\"}\\n\\n        relations = extract_pattern_relations(docs, valid_terms)\\n\\n        # Should find at least some IsA relations\\n        isa_relations = [r for r in relations if r[1] == 'IsA']\\n        # Note: may or may not find depending on pattern specificity\\n        self.assertIsInstance(relations, list)\\n\\n    def test_extract_hasa_pattern(self):\\n        \\\"\\\"\\\"Test extraction of HasA relations from text patterns.\\\"\\\"\\\"\\n        docs = {\\n            \\\"doc1\\\": \\\"The car has an engine. A house contains rooms.\\\"\\n        }\\n        valid_terms = {\\\"car\\\", \\\"engine\\\", \\\"house\\\", \\\"rooms\\\"}\\n\\n        relations = extract_pattern_relations(docs, valid_terms, min_confidence=0.5)\\n\\n        # Check we got some relations\\n        self.assertIsInstance(relations, list)\\n\\n    def test_extract_usedfor_pattern(self):\\n        \\\"\\\"\\\"Test extraction of UsedFor relations from text patterns.\\\"\\\"\\\"\\n        docs = {\\n            \\\"doc1\\\": \\\"The hammer is used for construction. Tools are useful for building.\\\"\\n        }\\n        valid_terms = {\\\"hammer\\\", \\\"construction\\\", \\\"tools\\\", \\\"building\\\"}\\n\\n        relations = extract_pattern_relations(docs, valid_terms, min_confidence=0.5)\\n\\n        usedfor_relations = [r for r in relations if r[1] == 'UsedFor']\\n        # May find UsedFor relations\\n        self.assertIsInstance(usedfor_relations, list)\\n\\n    def test_extract_causes_pattern(self):\\n        \\\"\\\"\\\"Test extraction of Causes relations from text patterns.\\\"\\\"\\\"\\n        docs = {\\n            \\\"doc1\\\": \\\"Rain causes floods. The virus leads to illness.\\\"\\n        }\\n        valid_terms = {\\\"rain\\\", \\\"floods\\\", \\\"virus\\\", \\\"illness\\\"}\\n\\n        relations = extract_pattern_relations(docs, valid_terms, min_confidence=0.5)\\n\\n        causes_relations = [r for r in relations if r[1] == 'Causes']\\n        # Should find some causal relations\\n        self.assertIsInstance(causes_relations, list)\\n\\n    def test_min_confidence_filtering(self):\\n        \\\"\\\"\\\"Test that min_confidence filters low-confidence relations.\\\"\\\"\\\"\\n        docs = {\\n            \\\"doc1\\\": \\\"The dog is happy. A cat is a pet.\\\"\\n        }\\n        valid_terms = {\\\"dog\\\", \\\"happy\\\", \\\"cat\\\", \\\"pet\\\"}\\n\\n        # Low confidence threshold\\n        relations_low = extract_pattern_relations(docs, valid_terms, min_confidence=0.3)\\n\\n        # High confidence threshold\\n        relations_high = extract_pattern_relations(docs, valid_terms, min_confidence=0.9)\\n\\n        # Low threshold should find at least as many\\n        self.assertGreaterEqual(len(relations_low), len(relations_high))\\n\\n    def test_stopwords_filtered(self):\\n        \\\"\\\"\\\"Test that stopwords are filtered from extracted relations.\\\"\\\"\\\"\\n        docs = {\\n            \\\"doc1\\\": \\\"The is a the. A an is the a.\\\"\\n        }\\n        valid_terms = {\\\"the\\\", \\\"a\\\", \\\"an\\\", \\\"is\\\"}\\n\\n        relations = extract_pattern_relations(docs, valid_terms)\\n\\n        # Should not find relations between pure stopwords\\n        self.assertEqual(len(relations), 0)\\n\\n    def test_same_term_filtered(self):\\n        \\\"\\\"\\\"Test that relations between same terms are filtered.\\\"\\\"\\\"\\n        docs = {\\n            \\\"doc1\\\": \\\"The dog is a dog. Cat is cat.\\\"\\n        }\\n        valid_terms = {\\\"dog\\\", \\\"cat\\\"}\\n\\n        relations = extract_pattern_relations(docs, valid_terms)\\n\\n        # Should not find self-relations\\n        for t1, rel, t2, conf in relations:\\n            self.assertNotEqual(t1, t2)\\n\\n    def test_invalid_terms_filtered(self):\\n        \\\"\\\"\\\"Test that relations with terms not in corpus are filtered.\\\"\\\"\\\"\\n        docs = {\\n            \\\"doc1\\\": \\\"A unicorn is a mythical creature.\\\"\\n        }\\n        valid_terms = {\\\"creature\\\"}  # \\\"unicorn\\\" and \\\"mythical\\\" not valid\\n\\n        relations = extract_pattern_relations(docs, valid_terms)\\n\\n        # Should not find relations with invalid terms\\n        self.assertEqual(len(relations), 0)\\n\\n    def test_get_pattern_statistics(self):\\n        \\\"\\\"\\\"Test pattern statistics computation.\\\"\\\"\\\"\\n        relations = [\\n            (\\\"dog\\\", \\\"IsA\\\", \\\"animal\\\", 0.9),\\n            (\\\"cat\\\", \\\"IsA\\\", \\\"animal\\\", 0.9),\\n            (\\\"hammer\\\", \\\"UsedFor\\\", \\\"construction\\\", 0.8),\\n        ]\\n\\n        stats = get_pattern_statistics(relations)\\n\\n        self.assertEqual(stats['total_relations'], 3)\\n        self.assertEqual(stats['unique_types'], 2)\\n        self.assertEqual(stats['relation_type_counts']['IsA'], 2)\\n        self.assertEqual(stats['relation_type_counts']['UsedFor'], 1)\\n        self.assertAlmostEqual(stats['average_confidence_by_type']['IsA'], 0.9)\\n\\n    def test_empty_relations_statistics(self):\\n        \\\"\\\"\\\"Test statistics with empty relations.\\\"\\\"\\\"\\n        stats = get_pattern_statistics([])\\n\\n        self.assertEqual(stats['total_relations'], 0)\\n        self.assertEqual(stats['unique_types'], 0)\\n        self.assertEqual(stats['relation_type_counts'], {})\\n\\n\\nclass TestProcessorPatternExtraction(unittest.TestCase):\\n    \\\"\\\"\\\"Test processor-level pattern extraction methods.\\\"\\\"\\\"\\n\\n    @classmethod\\n    def setUpClass(cls):\\n        \\\"\\\"\\\"Set up processor with documents containing various patterns.\\\"\\\"\\\"\\n        cls.processor = CorticalTextProcessor()\\n        cls.processor.process_document(\\\"doc1\\\", \\\"\\\"\\\"\\n            A neural network is a type of machine learning model.\\n            Machine learning is used for pattern recognition.\\n            Deep learning enables complex feature extraction.\\n        \\\"\\\"\\\")\\n        cls.processor.process_document(\\\"doc2\\\", \\\"\\\"\\\"\\n            The brain contains neurons that process information.\\n            Neurons are connected by synapses.\\n            Processing causes activation patterns.\\n        \\\"\\\"\\\")\\n        cls.processor.process_document(\\\"doc3\\\", \\\"\\\"\\\"\\n            Algorithms are used for data processing.\\n            Data processing leads to insights.\\n            Insights help decision making.\\n        \\\"\\\"\\\")\\n        cls.processor.compute_all(verbose=False)\\n\\n    def test_extract_pattern_relations_returns_list(self):\\n        \\\"\\\"\\\"Test that extract_pattern_relations returns a list.\\\"\\\"\\\"\\n        relations = self.processor.extract_pattern_relations(verbose=False)\\n        self.assertIsInstance(relations, list)\\n\\n    def test_extract_pattern_relations_format(self):\\n        \\\"\\\"\\\"Test that extracted relations have correct format.\\\"\\\"\\\"\\n        relations = self.processor.extract_pattern_relations(verbose=False)\\n\\n        for relation in relations:\\n            self.assertEqual(len(relation), 4)\\n            t1, rel_type, t2, confidence = relation\\n            self.assertIsInstance(t1, str)\\n            self.assertIsInstance(rel_type, str)\\n            self.assertIsInstance(t2, str)\\n            self.assertIsInstance(confidence, float)\\n            self.assertGreater(confidence, 0)\\n            self.assertLessEqual(confidence, 1.0)\\n\\n    def test_extract_corpus_semantics_with_patterns(self):\\n        \\\"\\\"\\\"Test extract_corpus_semantics with pattern extraction enabled.\\\"\\\"\\\"\\n        count = self.processor.extract_corpus_semantics(\\n            use_pattern_extraction=True,\\n            verbose=False\\n        )\\n\\n        self.assertGreater(count, 0)\\n        self.assertGreater(len(self.processor.semantic_relations), 0)\\n\\n    def test_extract_corpus_semantics_without_patterns(self):\\n        \\\"\\\"\\\"Test extract_corpus_semantics without pattern extraction.\\\"\\\"\\\"\\n        processor = CorticalTextProcessor()\\n        processor.process_document(\\\"doc1\\\", \\\"Neural networks process information quickly.\\\")\\n        processor.compute_all(verbose=False)\\n\\n        count_with = processor.extract_corpus_semantics(\\n            use_pattern_extraction=True,\\n            verbose=False\\n        )\\n\\n        processor.semantic_relations = []\\n\\n        count_without = processor.extract_corpus_semantics(\\n            use_pattern_extraction=False,\\n            verbose=False\\n        )\\n\\n        # With patterns should find at least as many (usually more)\\n        # But depending on corpus, might be same\\n        self.assertGreaterEqual(count_with, 0)\\n        self.assertGreaterEqual(count_without, 0)\\n\\n    def test_custom_min_confidence(self):\\n        \\\"\\\"\\\"Test custom minimum confidence threshold.\\\"\\\"\\\"\\n        relations_low = self.processor.extract_pattern_relations(\\n            min_confidence=0.3,\\n            verbose=False\\n        )\\n\\n        relations_high = self.processor.extract_pattern_relations(\\n            min_confidence=0.9,\\n            verbose=False\\n        )\\n\\n        # Lower confidence should find at least as many\\n        self.assertGreaterEqual(len(relations_low), len(relations_high))\\n\\n\\nif __name__ == \\\"__main__\\\":\\n    unittest.main(verbosity=2)\\n\",",
        "      \"mtime\": 1765375584.0",
        "    },",
        "    {",
        "      \"op\": \"add\",",
        "      \"doc_id\": \"cortical/analysis.py\",",
        "      \"content\": \"\\\"\\\"\\\"\\nAnalysis Module\\n===============\\n\\nGraph analysis algorithms for the cortical network.\\n\\nContains implementations of:\\n- PageRank for importance scoring\\n- TF-IDF for term weighting\\n- Label propagation for clustering\\n- Activation propagation for information flow\\n\\\"\\\"\\\"\\n\\nimport math\\nfrom typing import Dict, List, Tuple, Set, Optional, Any\\nfrom collections import defaultdict\\n\\nfrom .layers import CorticalLayer, HierarchicalLayer\\nfrom .minicolumn import Minicolumn\\n\\n\\ndef compute_pagerank(\\n    layer: HierarchicalLayer,\\n    damping: float = 0.85,\\n    iterations: int = 20,\\n    tolerance: float = 1e-6\\n) -> Dict[str, float]:\\n    \\\"\\\"\\\"\\n    Compute PageRank scores for minicolumns in a layer.\\n\\n    PageRank measures importance based on connection structure.\\n    Highly connected columns that are connected to other important\\n    columns receive higher scores.\\n\\n    Args:\\n        layer: The layer to compute PageRank for\\n        damping: Damping factor (probability of following links)\\n        iterations: Maximum number of iterations\\n        tolerance: Convergence threshold\\n\\n    Returns:\\n        Dictionary mapping column IDs to PageRank scores\\n\\n    Raises:\\n        ValueError: If damping is not in range (0, 1)\\n    \\\"\\\"\\\"\\n    if not (0 < damping < 1):\\n        raise ValueError(f\\\"damping must be between 0 and 1, got {damping}\\\")\\n\\n    n = len(layer.minicolumns)\\n    if n == 0:\\n        return {}\\n\\n    # Initialize PageRank uniformly\\n    pagerank = {col.id: 1.0 / n for col in layer.minicolumns.values()}\\n\\n    # Build incoming links map\\n    incoming: Dict[str, List[Tuple[str, float]]] = defaultdict(list)\\n    outgoing_sum: Dict[str, float] = defaultdict(float)\\n\\n    for col in layer.minicolumns.values():\\n        for target_id, weight in col.lateral_connections.items():\\n            # Use O(1) lookup via get_by_id instead of O(n) linear search\\n            if layer.get_by_id(target_id) is not None:\\n                incoming[target_id].append((col.id, weight))\\n                outgoing_sum[col.id] += weight\\n\\n    # Iterate until convergence\\n    for iteration in range(iterations):\\n        new_pagerank = {}\\n        max_diff = 0.0\\n\\n        for col in layer.minicolumns.values():\\n            # Sum of weighted incoming PageRank\\n            incoming_sum = 0.0\\n            for source_id, weight in incoming[col.id]:\\n                if source_id in pagerank and outgoing_sum[source_id] > 0:\\n                    incoming_sum += pagerank[source_id] * weight / outgoing_sum[source_id]\\n\\n            # Apply damping\\n            new_rank = (1 - damping) / n + damping * incoming_sum\\n            new_pagerank[col.id] = new_rank\\n\\n            max_diff = max(max_diff, abs(new_rank - pagerank.get(col.id, 0)))\\n\\n        pagerank = new_pagerank\\n\\n        if max_diff < tolerance:\\n            break\\n\\n    # Update minicolumn pagerank values\\n    for col in layer.minicolumns.values():\\n        col.pagerank = pagerank.get(col.id, 1.0 / n)\\n\\n    return pagerank\\n\\n\\n# Default relation weights for semantic PageRank\\nRELATION_WEIGHTS = {\\n    'IsA': 1.5,           # Hypernym relationships are strong\\n    'PartOf': 1.3,        # Meronym relationships\\n    'HasProperty': 1.2,   # Property associations\\n    'RelatedTo': 1.0,     # Default co-occurrence\\n    'SimilarTo': 1.4,     # Similarity relationships\\n    'Causes': 1.1,        # Causal relationships\\n    'UsedFor': 1.0,       # Functional relationships\\n    'CoOccurs': 0.8,      # Basic co-occurrence\\n    'Antonym': 0.3,       # Opposing concepts (lower weight)\\n    'DerivedFrom': 1.2,   # Morphological derivation\\n}\\n\\n\\ndef compute_semantic_pagerank(\\n    layer: HierarchicalLayer,\\n    semantic_relations: List[Tuple[str, str, str, float]],\\n    relation_weights: Optional[Dict[str, float]] = None,\\n    damping: float = 0.85,\\n    iterations: int = 20,\\n    tolerance: float = 1e-6\\n) -> Dict[str, Any]:\\n    \\\"\\\"\\\"\\n    Compute PageRank with semantic relation type weighting.\\n\\n    This ConceptNet-style PageRank applies different multipliers based on\\n    the semantic relation type between nodes. For example, IsA relationships\\n    are weighted more heavily than simple co-occurrence.\\n\\n    Args:\\n        layer: The layer to compute PageRank for\\n        semantic_relations: List of (term1, relation, term2, weight) tuples\\n        relation_weights: Optional custom relation weights dict. If None, uses defaults.\\n        damping: Damping factor (probability of following links)\\n        iterations: Maximum number of iterations\\n        tolerance: Convergence threshold\\n\\n    Returns:\\n        Dict containing:\\n        - pagerank: Dict mapping column IDs to PageRank scores\\n        - iterations_run: Number of iterations until convergence\\n        - edges_with_relations: Number of edges that had semantic relation info\\n\\n    Example:\\n        >>> relations = [(\\\"neural\\\", \\\"RelatedTo\\\", \\\"networks\\\", 0.8)]\\n        >>> result = compute_semantic_pagerank(layer, relations)\\n        >>> print(f\\\"PageRank converged in {result['iterations_run']} iterations\\\")\\n\\n    Raises:\\n        ValueError: If damping is not in range (0, 1)\\n    \\\"\\\"\\\"\\n    if not (0 < damping < 1):\\n        raise ValueError(f\\\"damping must be between 0 and 1, got {damping}\\\")\\n\\n    n = len(layer.minicolumns)\\n    if n == 0:\\n        return {'pagerank': {}, 'iterations_run': 0, 'edges_with_relations': 0}\\n\\n    # Use default weights if not provided\\n    weights = relation_weights or RELATION_WEIGHTS\\n\\n    # Build semantic relation lookup: (term1, term2) -> (relation_type, weight)\\n    semantic_lookup: Dict[Tuple[str, str], Tuple[str, float]] = {}\\n    for t1, relation, t2, rel_weight in semantic_relations:\\n        # Store in both directions for undirected lookup\\n        semantic_lookup[(t1, t2)] = (relation, rel_weight)\\n        semantic_lookup[(t2, t1)] = (relation, rel_weight)\\n\\n    # Initialize PageRank uniformly\\n    pagerank = {col.id: 1.0 / n for col in layer.minicolumns.values()}\\n\\n    # Build incoming links map with relation-weighted edges\\n    incoming: Dict[str, List[Tuple[str, float]]] = defaultdict(list)\\n    outgoing_sum: Dict[str, float] = defaultdict(float)\\n    edges_with_relations = 0\\n\\n    # Build content -> id mapping for semantic lookup\\n    content_to_id: Dict[str, str] = {}\\n    for col in layer.minicolumns.values():\\n        content_to_id[col.content] = col.id\\n\\n    for col in layer.minicolumns.values():\\n        for target_id, base_weight in col.lateral_connections.items():\\n            target = layer.get_by_id(target_id)\\n            if target is None:\\n                continue\\n\\n            # Check if there's a semantic relation between these terms\\n            lookup_key = (col.content, target.content)\\n            if lookup_key in semantic_lookup:\\n                relation_type, rel_weight = semantic_lookup[lookup_key]\\n                # Apply relation type multiplier\\n                type_multiplier = weights.get(relation_type, 1.0)\\n                # Combined weight: base_weight * relation_weight * type_multiplier\\n                adjusted_weight = base_weight * rel_weight * type_multiplier\\n                edges_with_relations += 1\\n            else:\\n                # No semantic relation, use base weight\\n                adjusted_weight = base_weight\\n\\n            incoming[target_id].append((col.id, adjusted_weight))\\n            outgoing_sum[col.id] += adjusted_weight\\n\\n    # Iterate until convergence\\n    iterations_run = 0\\n    for iteration in range(iterations):\\n        iterations_run = iteration + 1\\n        new_pagerank = {}\\n        max_diff = 0.0\\n\\n        for col in layer.minicolumns.values():\\n            # Sum of weighted incoming PageRank\\n            incoming_sum = 0.0\\n            for source_id, weight in incoming[col.id]:\\n                if source_id in pagerank and outgoing_sum[source_id] > 0:\\n                    incoming_sum += pagerank[source_id] * weight / outgoing_sum[source_id]\\n\\n            # Apply damping\\n            new_rank = (1 - damping) / n + damping * incoming_sum\\n            new_pagerank[col.id] = new_rank\\n\\n            max_diff = max(max_diff, abs(new_rank - pagerank.get(col.id, 0)))\\n\\n        pagerank = new_pagerank\\n\\n        if max_diff < tolerance:\\n            break\\n\\n    # Update minicolumn pagerank values\\n    for col in layer.minicolumns.values():\\n        col.pagerank = pagerank.get(col.id, 1.0 / n)\\n\\n    return {\\n        'pagerank': pagerank,\\n        'iterations_run': iterations_run,\\n        'edges_with_relations': edges_with_relations\\n    }\\n\\n\\ndef compute_hierarchical_pagerank(\\n    layers: Dict[CorticalLayer, HierarchicalLayer],\\n    layer_iterations: int = 10,\\n    global_iterations: int = 5,\\n    damping: float = 0.85,\\n    cross_layer_damping: float = 0.7,\\n    tolerance: float = 1e-4\\n) -> Dict[str, Any]:\\n    \\\"\\\"\\\"\\n    Compute PageRank with cross-layer propagation.\\n\\n    This hierarchical PageRank allows importance to flow between layers:\\n    - Upward: tokens → bigrams → concepts → documents\\n    - Downward: documents → concepts → bigrams → tokens\\n\\n    The algorithm alternates between:\\n    1. Computing local PageRank within each layer\\n    2. Propagating scores up the hierarchy (via feedback_connections)\\n    3. Propagating scores down the hierarchy (via feedforward_connections)\\n\\n    Args:\\n        layers: Dictionary of all layers\\n        layer_iterations: Max iterations for intra-layer PageRank\\n        global_iterations: Max iterations for cross-layer propagation\\n        damping: Damping factor for intra-layer PageRank\\n        cross_layer_damping: Damping factor for cross-layer propagation (default 0.7)\\n        tolerance: Convergence threshold for global iterations\\n\\n    Returns:\\n        Dict containing:\\n        - iterations_run: Number of global iterations\\n        - converged: Whether the algorithm converged\\n        - layer_stats: Per-layer statistics\\n\\n    Example:\\n        >>> result = compute_hierarchical_pagerank(layers)\\n        >>> print(f\\\"Converged in {result['iterations_run']} iterations\\\")\\n\\n    Raises:\\n        ValueError: If damping or cross_layer_damping is not in range (0, 1)\\n    \\\"\\\"\\\"\\n    if not (0 < damping < 1):\\n        raise ValueError(f\\\"damping must be between 0 and 1, got {damping}\\\")\\n    if not (0 < cross_layer_damping < 1):\\n        raise ValueError(f\\\"cross_layer_damping must be between 0 and 1, got {cross_layer_damping}\\\")\\n\\n    # Define layer order for propagation\\n    layer_order = [\\n        CorticalLayer.TOKENS,\\n        CorticalLayer.BIGRAMS,\\n        CorticalLayer.CONCEPTS,\\n        CorticalLayer.DOCUMENTS\\n    ]\\n\\n    # Filter to only existing layers with minicolumns\\n    active_layers = [l for l in layer_order if l in layers and layers[l].column_count() > 0]\\n\\n    if not active_layers:\\n        return {'iterations_run': 0, 'converged': True, 'layer_stats': {}}\\n\\n    # Store previous PageRank values for convergence check\\n    prev_pageranks: Dict[CorticalLayer, Dict[str, float]] = {}\\n\\n    iterations_run = 0\\n    converged = False\\n\\n    for global_iter in range(global_iterations):\\n        iterations_run = global_iter + 1\\n        max_global_diff = 0.0\\n\\n        # Step 1: Compute local PageRank for each layer\\n        for layer_enum in active_layers:\\n            layer = layers[layer_enum]\\n            compute_pagerank(layer, damping=damping, iterations=layer_iterations, tolerance=1e-6)\\n\\n        # Step 2: Propagate up (tokens → bigrams → concepts → documents)\\n        for i in range(len(active_layers) - 1):\\n            lower_layer_enum = active_layers[i]\\n            upper_layer_enum = active_layers[i + 1]\\n            lower_layer = layers[lower_layer_enum]\\n            upper_layer = layers[upper_layer_enum]\\n\\n            # Propagate from lower to upper via feedback connections\\n            for col in lower_layer.minicolumns.values():\\n                if not col.feedback_connections:\\n                    continue\\n\\n                for target_id, weight in col.feedback_connections.items():\\n                    target = upper_layer.get_by_id(target_id)\\n                    if target:\\n                        # Boost upper layer node based on lower layer importance\\n                        boost = col.pagerank * weight * cross_layer_damping\\n                        target.pagerank += boost\\n\\n        # Step 3: Propagate down (documents → concepts → bigrams → tokens)\\n        for i in range(len(active_layers) - 1, 0, -1):\\n            upper_layer_enum = active_layers[i]\\n            lower_layer_enum = active_layers[i - 1]\\n            upper_layer = layers[upper_layer_enum]\\n            lower_layer = layers[lower_layer_enum]\\n\\n            # Propagate from upper to lower via feedforward connections\\n            for col in upper_layer.minicolumns.values():\\n                if not col.feedforward_connections:\\n                    continue\\n\\n                for target_id, weight in col.feedforward_connections.items():\\n                    target = lower_layer.get_by_id(target_id)\\n                    if target:\\n                        # Boost lower layer node based on upper layer importance\\n                        boost = col.pagerank * weight * cross_layer_damping\\n                        target.pagerank += boost\\n\\n        # Normalize PageRank within each layer\\n        for layer_enum in active_layers:\\n            layer = layers[layer_enum]\\n            total = sum(col.pagerank for col in layer.minicolumns.values())\\n            if total > 0:\\n                for col in layer.minicolumns.values():\\n                    col.pagerank /= total\\n\\n        # Check convergence\\n        for layer_enum in active_layers:\\n            layer = layers[layer_enum]\\n            current_pr = {col.id: col.pagerank for col in layer.minicolumns.values()}\\n\\n            if layer_enum in prev_pageranks:\\n                for col_id, pr in current_pr.items():\\n                    prev_pr = prev_pageranks[layer_enum].get(col_id, 0)\\n                    max_global_diff = max(max_global_diff, abs(pr - prev_pr))\\n\\n            prev_pageranks[layer_enum] = current_pr\\n\\n        if max_global_diff < tolerance and global_iter > 0:\\n            converged = True\\n            break\\n\\n    # Collect layer statistics\\n    layer_stats = {}\\n    for layer_enum in active_layers:\\n        layer = layers[layer_enum]\\n        pageranks = [col.pagerank for col in layer.minicolumns.values()]\\n        layer_stats[layer_enum.name] = {\\n            'nodes': len(pageranks),\\n            'max_pagerank': max(pageranks) if pageranks else 0,\\n            'min_pagerank': min(pageranks) if pageranks else 0,\\n            'avg_pagerank': sum(pageranks) / len(pageranks) if pageranks else 0\\n        }\\n\\n    return {\\n        'iterations_run': iterations_run,\\n        'converged': converged,\\n        'layer_stats': layer_stats\\n    }\\n\\n\\ndef compute_tfidf(\\n    layers: Dict[CorticalLayer, HierarchicalLayer],\\n    documents: Dict[str, str]\\n) -> None:\\n    \\\"\\\"\\\"\\n    Compute TF-IDF scores for tokens.\\n    \\n    TF-IDF (Term Frequency - Inverse Document Frequency) measures\\n    how distinctive a term is to the corpus. High TF-IDF terms are\\n    both frequent in their documents and rare across the corpus.\\n    \\n    Args:\\n        layers: Dictionary of layers (needs TOKENS layer)\\n        documents: Dictionary mapping doc_id to content\\n    \\\"\\\"\\\"\\n    layer0 = layers[CorticalLayer.TOKENS]\\n    num_docs = len(documents)\\n    \\n    if num_docs == 0:\\n        return\\n    \\n    for col in layer0.minicolumns.values():\\n        # Document frequency\\n        df = len(col.document_ids)\\n        \\n        if df > 0:\\n            # Inverse document frequency\\n            idf = math.log(num_docs / df)\\n            \\n            # Term frequency (normalized by occurrence count)\\n            tf = math.log1p(col.occurrence_count)\\n            \\n            # TF-IDF\\n            col.tfidf = tf * idf\\n            \\n            # Per-document TF-IDF using actual occurrence counts\\n            for doc_id in col.document_ids:\\n                # Get actual term frequency in this document\\n                doc_tf = col.doc_occurrence_counts.get(doc_id, 1)\\n                col.tfidf_per_doc[doc_id] = math.log1p(doc_tf) * idf\\n\\n\\ndef propagate_activation(\\n    layers: Dict[CorticalLayer, HierarchicalLayer],\\n    iterations: int = 3,\\n    decay: float = 0.8,\\n    lateral_weight: float = 0.3\\n) -> None:\\n    \\\"\\\"\\\"\\n    Propagate activation through the network.\\n    \\n    This simulates how information flows through cortical layers:\\n    - Activation spreads to connected columns (lateral)\\n    - Activation flows up the hierarchy (feedforward)\\n    - Activation decays over time\\n    \\n    Args:\\n        layers: Dictionary of all layers\\n        iterations: Number of propagation iterations\\n        decay: How much activation decays per iteration\\n        lateral_weight: Weight for lateral spreading\\n    \\\"\\\"\\\"\\n    for _ in range(iterations):\\n        # Store new activations\\n        new_activations: Dict[str, float] = {}\\n        \\n        # Process each layer\\n        for layer_enum in CorticalLayer:\\n            if layer_enum not in layers:\\n                continue\\n            layer = layers[layer_enum]\\n            \\n            for col in layer.minicolumns.values():\\n                # Start with decayed current activation\\n                new_act = col.activation * decay\\n                \\n                # Add lateral input using O(1) ID lookup\\n                for neighbor_id, weight in col.lateral_connections.items():\\n                    neighbor = layer.get_by_id(neighbor_id)\\n                    if neighbor:\\n                        new_act += neighbor.activation * weight * lateral_weight\\n                \\n                # Add feedforward input using O(1) ID lookup\\n                for source_id in col.feedforward_sources:\\n                    # Find source in lower layers\\n                    for lower_enum in CorticalLayer:\\n                        if lower_enum >= layer_enum:\\n                            break\\n                        if lower_enum not in layers:\\n                            continue\\n                        lower_layer = layers[lower_enum]\\n                        source = lower_layer.get_by_id(source_id)\\n                        if source:\\n                            new_act += source.activation * 0.5\\n                            break\\n                \\n                new_activations[col.id] = new_act\\n        \\n        # Apply new activations\\n        for layer_enum in CorticalLayer:\\n            if layer_enum not in layers:\\n                continue\\n            layer = layers[layer_enum]\\n            for col in layer.minicolumns.values():\\n                if col.id in new_activations:\\n                    col.activation = new_activations[col.id]\\n\\n\\ndef cluster_by_label_propagation(\\n    layer: HierarchicalLayer,\\n    min_cluster_size: int = 3,\\n    max_iterations: int = 20,\\n    cluster_strictness: float = 1.0,\\n    bridge_weight: float = 0.0\\n) -> Dict[int, List[str]]:\\n    \\\"\\\"\\\"\\n    Cluster minicolumns using label propagation.\\n\\n    Label propagation is a semi-supervised community detection\\n    algorithm. Each node adopts the most common label among its\\n    neighbors, causing labels to propagate through densely\\n    connected regions.\\n\\n    Args:\\n        layer: Layer to cluster\\n        min_cluster_size: Minimum nodes per cluster\\n        max_iterations: Maximum iterations\\n        cluster_strictness: Controls clustering aggressiveness (0.0-1.0).\\n            - 1.0 (default): Strict clustering, topics stay separate\\n            - 0.5: Moderate mixing, allows some cross-topic clustering\\n            - 0.0: Minimal clustering, most tokens group together\\n            Lower values create fewer, larger clusters.\\n        bridge_weight: Weight for synthetic inter-document connections (0.0-1.0).\\n            When > 0, adds weak connections between tokens that appear in\\n            different documents, helping bridge topic-isolated clusters.\\n            - 0.0 (default): No bridging\\n            - 0.3: Light bridging\\n            - 0.7: Strong bridging\\n\\n    Returns:\\n        Dictionary mapping cluster_id to list of column contents\\n    \\\"\\\"\\\"\\n    # Clamp parameters to valid range\\n    cluster_strictness = max(0.0, min(1.0, cluster_strictness))\\n    bridge_weight = max(0.0, min(1.0, bridge_weight))\\n\\n    # Initialize each node with unique label\\n    labels = {col.content: i for i, col in enumerate(layer.minicolumns.values())}\\n\\n    # Get column list for shuffling\\n    columns = list(layer.minicolumns.keys())\\n\\n    # Build augmented connection weights (includes optional bridging)\\n    augmented_connections: Dict[str, Dict[str, float]] = defaultdict(dict)\\n\\n    for content in columns:\\n        col = layer.minicolumns[content]\\n        for neighbor_id, weight in col.lateral_connections.items():\\n            neighbor = layer.get_by_id(neighbor_id)\\n            if neighbor:\\n                augmented_connections[content][neighbor.content] = weight\\n\\n    # Add synthetic bridge connections between documents if requested\\n    if bridge_weight > 0:\\n        # Group tokens by document\\n        doc_tokens: Dict[str, List[str]] = defaultdict(list)\\n        for content in columns:\\n            col = layer.minicolumns[content]\\n            for doc_id in col.document_ids:\\n                doc_tokens[doc_id].append(content)\\n\\n        # Create weak connections between tokens from different documents\\n        doc_ids = list(doc_tokens.keys())\\n        for i, doc1 in enumerate(doc_ids):\\n            for doc2 in doc_ids[i+1:]:\\n                tokens1 = doc_tokens[doc1]\\n                tokens2 = doc_tokens[doc2]\\n                # Connect a sample of tokens to avoid O(n²) explosion\\n                sample_size = min(5, len(tokens1), len(tokens2))\\n                for t1 in tokens1[:sample_size]:\\n                    for t2 in tokens2[:sample_size]:\\n                        if t1 != t2:\\n                            # Add weak bidirectional bridge\\n                            current = augmented_connections[t1].get(t2, 0)\\n                            augmented_connections[t1][t2] = current + bridge_weight * 0.5\\n                            current = augmented_connections[t2].get(t1, 0)\\n                            augmented_connections[t2][t1] = current + bridge_weight * 0.5\\n\\n    # Calculate label change threshold based on strictness\\n    # Higher strictness = requires stronger evidence to change label\\n    change_threshold = (1.0 - cluster_strictness) * 0.3\\n\\n    for iteration in range(max_iterations):\\n        changed = False\\n\\n        # Process in order (could shuffle for better results)\\n        for content in columns:\\n            # Count neighbor labels weighted by connection strength\\n            label_weights: Dict[int, float] = defaultdict(float)\\n\\n            for neighbor_content, weight in augmented_connections[content].items():\\n                if neighbor_content in labels:\\n                    label_weights[labels[neighbor_content]] += weight\\n\\n            # Apply strictness: current label gets a bonus based on strictness\\n            current_label = labels[content]\\n            if current_label in label_weights and cluster_strictness < 1.0:\\n                # Lower strictness = stronger bias toward current label\\n                label_weights[current_label] *= (1 + (1 - cluster_strictness) * 2)\\n\\n            # Adopt most common label\\n            if label_weights:\\n                best_label, best_weight = max(label_weights.items(), key=lambda x: x[1])\\n                current_weight = label_weights.get(current_label, 0)\\n\\n                # Only change if the improvement exceeds threshold\\n                if best_label != current_label:\\n                    if current_weight == 0 or (best_weight / max(current_weight, 0.001) - 1) > change_threshold:\\n                        labels[content] = best_label\\n                        changed = True\\n\\n        if not changed:\\n            break\\n\\n    # Build clusters\\n    clusters: Dict[int, List[str]] = defaultdict(list)\\n    for content, label in labels.items():\\n        clusters[label].append(content)\\n\\n    # Filter by minimum size\\n    filtered = {\\n        label: members\\n        for label, members in clusters.items()\\n        if len(members) >= min_cluster_size\\n    }\\n\\n    # Update cluster_id on minicolumns\\n    for label, members in filtered.items():\\n        for content in members:\\n            if content in layer.minicolumns:\\n                layer.minicolumns[content].cluster_id = label\\n\\n    return filtered\\n\\n\\ndef build_concept_clusters(\\n    layers: Dict[CorticalLayer, HierarchicalLayer],\\n    clusters: Dict[int, List[str]]\\n) -> None:\\n    \\\"\\\"\\\"\\n    Build concept layer from token clusters.\\n    \\n    Creates Layer 2 (Concepts) minicolumns from clustered tokens.\\n    Each concept is named after its most important members.\\n    \\n    Args:\\n        layers: Dictionary of all layers\\n        clusters: Cluster dictionary from label propagation\\n    \\\"\\\"\\\"\\n    layer0 = layers[CorticalLayer.TOKENS]\\n    layer2 = layers[CorticalLayer.CONCEPTS]\\n    \\n    for cluster_id, members in clusters.items():\\n        if len(members) < 2:\\n            continue\\n        \\n        # Get member columns and sort by PageRank\\n        member_cols = []\\n        for m in members:\\n            col = layer0.get_minicolumn(m)\\n            if col:\\n                member_cols.append(col)\\n        \\n        if not member_cols:\\n            continue\\n        \\n        member_cols.sort(key=lambda c: c.pagerank, reverse=True)\\n        \\n        # Name concept after top members\\n        top_names = [c.content for c in member_cols[:3]]\\n        concept_name = '/'.join(top_names)\\n        \\n        # Create concept minicolumn\\n        concept = layer2.get_or_create_minicolumn(concept_name)\\n        concept.cluster_id = cluster_id\\n        \\n        # Aggregate properties from members with weighted connections\\n        max_pagerank = max(c.pagerank for c in member_cols) if member_cols else 1.0\\n        for col in member_cols:\\n            concept.feedforward_sources.add(col.id)\\n            concept.document_ids.update(col.document_ids)\\n            concept.activation += col.activation * 0.5\\n            concept.occurrence_count += col.occurrence_count\\n            # Weighted feedforward: concept → token (weight by normalized PageRank)\\n            weight = col.pagerank / max_pagerank if max_pagerank > 0 else 1.0\\n            concept.add_feedforward_connection(col.id, weight)\\n            # Weighted feedback: token → concept (weight by normalized PageRank)\\n            col.add_feedback_connection(concept.id, weight)\\n\\n        # Set PageRank as average of members\\n        concept.pagerank = sum(c.pagerank for c in member_cols) / len(member_cols)\\n\\n\\ndef compute_concept_connections(\\n    layers: Dict[CorticalLayer, HierarchicalLayer],\\n    semantic_relations: List[Tuple[str, str, str, float]] = None,\\n    min_shared_docs: int = 1,\\n    min_jaccard: float = 0.1,\\n    use_member_semantics: bool = False,\\n    use_embedding_similarity: bool = False,\\n    embedding_threshold: float = 0.3,\\n    embeddings: Dict[str, List[float]] = None\\n) -> Dict[str, Any]:\\n    \\\"\\\"\\\"\\n    Build lateral connections between concepts in Layer 2.\\n\\n    Concepts are connected based on:\\n    1. Shared documents (Jaccard similarity of document sets)\\n    2. Semantic relations between member tokens (if provided)\\n    3. Semantic relations between members independent of docs (use_member_semantics)\\n    4. Embedding similarity of concept centroids (use_embedding_similarity)\\n\\n    Args:\\n        layers: Dictionary of all layers\\n        semantic_relations: Optional list of (term1, relation, term2, weight) tuples\\n        min_shared_docs: Minimum shared documents for connection (0 to disable filter)\\n        min_jaccard: Minimum Jaccard similarity threshold (0.0 to disable filter)\\n        use_member_semantics: Connect concepts via semantic relations between members,\\n                              even without document overlap\\n        use_embedding_similarity: Connect concepts via embedding similarity of centroids\\n        embedding_threshold: Minimum cosine similarity for embedding-based connections\\n        embeddings: Term embeddings dict (required if use_embedding_similarity=True)\\n\\n    Returns:\\n        Statistics about connections created\\n    \\\"\\\"\\\"\\n    layer0 = layers[CorticalLayer.TOKENS]\\n    layer2 = layers[CorticalLayer.CONCEPTS]\\n\\n    if layer2.column_count() == 0:\\n        return {\\n            'connections_created': 0,\\n            'concepts': 0,\\n            'doc_overlap_connections': 0,\\n            'semantic_connections': 0,\\n            'embedding_connections': 0\\n        }\\n\\n    concepts = list(layer2.minicolumns.values())\\n    connections_created = 0\\n    doc_overlap_connections = 0\\n    semantic_connections = 0\\n    embedding_connections = 0\\n\\n    # Build semantic relation lookup for faster access\\n    semantic_lookup: Dict[str, Dict[str, Tuple[str, float]]] = defaultdict(dict)\\n    if semantic_relations:\\n        for t1, relation, t2, weight in semantic_relations:\\n            # Store relation in both directions\\n            semantic_lookup[t1][t2] = (relation, weight)\\n            semantic_lookup[t2][t1] = (relation, weight)\\n\\n    # Relation type weights for scoring\\n    relation_weights = {\\n        'IsA': 1.5,\\n        'PartOf': 1.3,\\n        'HasProperty': 1.2,\\n        'RelatedTo': 1.0,\\n        'Antonym': 0.3,\\n    }\\n\\n    # Pre-compute member tokens for each concept (used by multiple strategies)\\n    concept_members: Dict[str, Set[str]] = {}\\n    for concept in concepts:\\n        members = set()\\n        for token_id in concept.feedforward_connections:\\n            token = layer0.get_by_id(token_id)\\n            if token:\\n                members.add(token.content)\\n        concept_members[concept.id] = members\\n\\n    # Pre-compute concept centroids if using embedding similarity\\n    concept_centroids: Dict[str, List[float]] = {}\\n    if use_embedding_similarity and embeddings:\\n        for concept in concepts:\\n            members = concept_members[concept.id]\\n            member_embeddings = [embeddings[m] for m in members if m in embeddings]\\n            if member_embeddings:\\n                dim = len(member_embeddings[0])\\n                centroid = [0.0] * dim\\n                for emb in member_embeddings:\\n                    for j, v in enumerate(emb):\\n                        centroid[j] += v\\n                for j in range(dim):\\n                    centroid[j] /= len(member_embeddings)\\n                concept_centroids[concept.id] = centroid\\n\\n    # Track which pairs have been connected to avoid duplicates\\n    connected_pairs: Set[Tuple[str, str]] = set()\\n\\n    def add_connection(c1: Minicolumn, c2: Minicolumn, weight: float) -> bool:\\n        \\\"\\\"\\\"Add bidirectional connection if not already connected.\\\"\\\"\\\"\\n        pair = tuple(sorted([c1.id, c2.id]))\\n        if pair in connected_pairs:\\n            # Already connected, strengthen existing connection\\n            c1.add_lateral_connection(c2.id, weight)\\n            c2.add_lateral_connection(c1.id, weight)\\n            return False\\n        connected_pairs.add(pair)\\n        c1.add_lateral_connection(c2.id, weight)\\n        c2.add_lateral_connection(c1.id, weight)\\n        return True\\n\\n    # Compare all concept pairs\\n    for i, concept1 in enumerate(concepts):\\n        docs1 = concept1.document_ids\\n        members1 = concept_members[concept1.id]\\n\\n        for concept2 in concepts[i+1:]:\\n            docs2 = concept2.document_ids\\n            members2 = concept_members[concept2.id]\\n\\n            # Strategy 1: Document overlap (traditional method)\\n            shared_docs = docs1 & docs2\\n            union_docs = docs1 | docs2\\n            jaccard = len(shared_docs) / len(union_docs) if union_docs else 0\\n\\n            passes_doc_filter = (\\n                len(shared_docs) >= min_shared_docs and jaccard >= min_jaccard\\n            )\\n\\n            if passes_doc_filter:\\n                # Base weight from document overlap\\n                weight = jaccard\\n\\n                # Add semantic relation bonus if available\\n                if semantic_relations:\\n                    semantic_bonus = 0.0\\n                    relation_count = 0\\n                    for m1 in members1:\\n                        if m1 in semantic_lookup:\\n                            for m2 in members2:\\n                                if m2 in semantic_lookup[m1]:\\n                                    relation, rel_weight = semantic_lookup[m1][m2]\\n                                    rel_multiplier = relation_weights.get(relation, 1.0)\\n                                    semantic_bonus += rel_weight * rel_multiplier\\n                                    relation_count += 1\\n\\n                    # Normalize and add semantic bonus (max 50% boost)\\n                    if relation_count > 0:\\n                        avg_semantic = semantic_bonus / relation_count\\n                        weight *= (1 + min(avg_semantic, 0.5))\\n\\n                if add_connection(concept1, concept2, weight):\\n                    connections_created += 1\\n                    doc_overlap_connections += 1\\n\\n            # Strategy 2: Member semantic relations (independent of document overlap)\\n            if use_member_semantics and semantic_relations and not passes_doc_filter:\\n                semantic_score = 0.0\\n                relation_count = 0\\n                for m1 in members1:\\n                    if m1 in semantic_lookup:\\n                        for m2 in members2:\\n                            if m2 in semantic_lookup[m1]:\\n                                relation, rel_weight = semantic_lookup[m1][m2]\\n                                rel_multiplier = relation_weights.get(relation, 1.0)\\n                                semantic_score += rel_weight * rel_multiplier\\n                                relation_count += 1\\n\\n                if relation_count > 0:\\n                    # Normalize by number of relations found\\n                    avg_score = semantic_score / relation_count\\n                    # Scale to reasonable weight range (0.1 - 0.8)\\n                    weight = min(0.1 + avg_score * 0.3, 0.8)\\n                    if add_connection(concept1, concept2, weight):\\n                        connections_created += 1\\n                        semantic_connections += 1\\n\\n            # Strategy 3: Embedding similarity (independent of document overlap)\\n            if use_embedding_similarity and embeddings and not passes_doc_filter:\\n                if concept1.id in concept_centroids and concept2.id in concept_centroids:\\n                    centroid1 = concept_centroids[concept1.id]\\n                    centroid2 = concept_centroids[concept2.id]\\n                    sim = cosine_similarity(\\n                        {str(i): v for i, v in enumerate(centroid1)},\\n                        {str(i): v for i, v in enumerate(centroid2)}\\n                    )\\n                    if sim >= embedding_threshold:\\n                        # Scale similarity to connection weight\\n                        weight = sim * 0.7  # Scale down slightly\\n                        if add_connection(concept1, concept2, weight):\\n                            connections_created += 1\\n                            embedding_connections += 1\\n\\n    return {\\n        'connections_created': connections_created,\\n        'concepts': len(concepts),\\n        'doc_overlap_connections': doc_overlap_connections,\\n        'semantic_connections': semantic_connections,\\n        'embedding_connections': embedding_connections\\n    }\\n\\n\\ndef compute_bigram_connections(\\n    layers: Dict[CorticalLayer, HierarchicalLayer],\\n    min_shared_docs: int = 1,\\n    component_weight: float = 0.5,\\n    chain_weight: float = 0.7,\\n    cooccurrence_weight: float = 0.3\\n) -> Dict[str, Any]:\\n    \\\"\\\"\\\"\\n    Build lateral connections between bigrams in Layer 1.\\n\\n    Bigrams are connected based on:\\n    1. Shared component terms (\\\"neural_networks\\\" ↔ \\\"neural_processing\\\")\\n    2. Document co-occurrence (appear in same documents)\\n    3. Chains (\\\"machine_learning\\\" ↔ \\\"learning_algorithms\\\" where right=left)\\n\\n    Args:\\n        layers: Dictionary of all layers\\n        min_shared_docs: Minimum shared documents for co-occurrence connection\\n        component_weight: Weight for shared component connections (default 0.5)\\n        chain_weight: Weight for chain connections (default 0.7)\\n        cooccurrence_weight: Weight for document co-occurrence (default 0.3)\\n\\n    Returns:\\n        Statistics about connections created:\\n        - connections_created: Total bidirectional connections\\n        - component_connections: Connections from shared components\\n        - chain_connections: Connections from chains\\n        - cooccurrence_connections: Connections from document co-occurrence\\n    \\\"\\\"\\\"\\n    layer1 = layers[CorticalLayer.BIGRAMS]\\n\\n    if layer1.column_count() == 0:\\n        return {\\n            'connections_created': 0,\\n            'bigrams': 0,\\n            'component_connections': 0,\\n            'chain_connections': 0,\\n            'cooccurrence_connections': 0\\n        }\\n\\n    bigrams = list(layer1.minicolumns.values())\\n\\n    # Build indexes for efficient lookup\\n    # left_component_index: {\\\"neural\\\": [bigram1, bigram2, ...]}\\n    # right_component_index: {\\\"networks\\\": [bigram1, bigram3, ...]}\\n    # Note: Bigrams use space separators (e.g., \\\"neural networks\\\")\\n    left_index: Dict[str, List[Minicolumn]] = defaultdict(list)\\n    right_index: Dict[str, List[Minicolumn]] = defaultdict(list)\\n\\n    for bigram in bigrams:\\n        parts = bigram.content.split(' ')\\n        if len(parts) == 2:\\n            left_index[parts[0]].append(bigram)\\n            right_index[parts[1]].append(bigram)\\n\\n    # Track connection types for statistics\\n    component_connections = 0\\n    chain_connections = 0\\n    cooccurrence_connections = 0\\n\\n    # Track which pairs we've already connected (avoid duplicates)\\n    connected_pairs: Set[Tuple[str, str]] = set()\\n\\n    def add_connection(b1: Minicolumn, b2: Minicolumn, weight: float, conn_type: str) -> bool:\\n        \\\"\\\"\\\"Add bidirectional connection if not already connected.\\\"\\\"\\\"\\n        nonlocal component_connections, chain_connections, cooccurrence_connections\\n\\n        pair = tuple(sorted([b1.id, b2.id]))\\n        if pair in connected_pairs:\\n            # Already connected, just strengthen the connection\\n            b1.add_lateral_connection(b2.id, weight)\\n            b2.add_lateral_connection(b1.id, weight)\\n            return False\\n\\n        connected_pairs.add(pair)\\n        b1.add_lateral_connection(b2.id, weight)\\n        b2.add_lateral_connection(b1.id, weight)\\n\\n        if conn_type == 'component':\\n            component_connections += 1\\n        elif conn_type == 'chain':\\n            chain_connections += 1\\n        elif conn_type == 'cooccurrence':\\n            cooccurrence_connections += 1\\n\\n        return True\\n\\n    # 1. Connect bigrams sharing a component\\n    # Left component matches: \\\"neural_networks\\\" ↔ \\\"neural_processing\\\"\\n    for component, bigram_list in left_index.items():\\n        for i, b1 in enumerate(bigram_list):\\n            for b2 in bigram_list[i+1:]:\\n                # Weight by component's PageRank importance (if available)\\n                weight = component_weight\\n                add_connection(b1, b2, weight, 'component')\\n\\n    # Right component matches: \\\"deep_learning\\\" ↔ \\\"machine_learning\\\"\\n    for component, bigram_list in right_index.items():\\n        for i, b1 in enumerate(bigram_list):\\n            for b2 in bigram_list[i+1:]:\\n                weight = component_weight\\n                add_connection(b1, b2, weight, 'component')\\n\\n    # 2. Connect chain bigrams (right of one = left of other)\\n    # \\\"machine_learning\\\" ↔ \\\"learning_algorithms\\\"\\n    for term in left_index:\\n        if term in right_index:\\n            # term appears as right component in some bigrams and left in others\\n            for b_left in right_index[term]:  # ends with term\\n                for b_right in left_index[term]:  # starts with term\\n                    if b_left.id != b_right.id:\\n                        add_connection(b_left, b_right, chain_weight, 'chain')\\n\\n    # 3. Connect bigrams that co-occur in the same documents\\n    # Use inverted index for O(d * b²) instead of O(n²) where d=docs, b=bigrams per doc\\n    doc_to_bigrams: Dict[str, List[Minicolumn]] = defaultdict(list)\\n    for bigram in bigrams:\\n        for doc_id in bigram.document_ids:\\n            doc_to_bigrams[doc_id].append(bigram)\\n\\n    # Track pairs we've already processed to avoid duplicate work\\n    cooccur_processed: Set[Tuple[str, str]] = set()\\n\\n    for doc_id, doc_bigrams in doc_to_bigrams.items():\\n        # Only compare bigrams within the same document\\n        for i, b1 in enumerate(doc_bigrams):\\n            docs1 = b1.document_ids\\n            for b2 in doc_bigrams[i+1:]:\\n                # Skip if already processed this pair\\n                pair_key = tuple(sorted([b1.id, b2.id]))\\n                if pair_key in cooccur_processed:\\n                    continue\\n                cooccur_processed.add(pair_key)\\n\\n                docs2 = b2.document_ids\\n                shared_docs = docs1 & docs2\\n                if len(shared_docs) >= min_shared_docs:\\n                    # Weight by Jaccard similarity of document sets\\n                    jaccard = len(shared_docs) / len(docs1 | docs2)\\n                    weight = cooccurrence_weight * jaccard\\n                    add_connection(b1, b2, weight, 'cooccurrence')\\n\\n    return {\\n        'connections_created': len(connected_pairs),\\n        'bigrams': len(bigrams),\\n        'component_connections': component_connections,\\n        'chain_connections': chain_connections,\\n        'cooccurrence_connections': cooccurrence_connections\\n    }\\n\\n\\ndef compute_document_connections(\\n    layers: Dict[CorticalLayer, HierarchicalLayer],\\n    documents: Dict[str, str],\\n    min_shared_terms: int = 3\\n) -> None:\\n    \\\"\\\"\\\"\\n    Build lateral connections between documents.\\n    \\n    Documents are connected based on shared vocabulary,\\n    weighted by TF-IDF scores of shared terms.\\n    \\n    Args:\\n        layers: Dictionary of all layers\\n        documents: Dictionary of documents\\n        min_shared_terms: Minimum shared terms for connection\\n    \\\"\\\"\\\"\\n    layer0 = layers[CorticalLayer.TOKENS]\\n    layer3 = layers[CorticalLayer.DOCUMENTS]\\n    \\n    doc_ids = list(documents.keys())\\n    \\n    for i, doc1 in enumerate(doc_ids):\\n        col1 = layer3.get_minicolumn(doc1)\\n        if not col1:\\n            col1 = layer3.get_or_create_minicolumn(doc1)\\n        \\n        for doc2 in doc_ids[i+1:]:\\n            col2 = layer3.get_minicolumn(doc2)\\n            if not col2:\\n                col2 = layer3.get_or_create_minicolumn(doc2)\\n            \\n            # Find shared terms\\n            shared_weight = 0.0\\n            shared_count = 0\\n            \\n            for token_col in layer0.minicolumns.values():\\n                if doc1 in token_col.document_ids and doc2 in token_col.document_ids:\\n                    # Weight by TF-IDF\\n                    weight = token_col.tfidf\\n                    shared_weight += weight\\n                    shared_count += 1\\n            \\n            if shared_count >= min_shared_terms:\\n                col1.add_lateral_connection(col2.id, shared_weight)\\n                col2.add_lateral_connection(col1.id, shared_weight)\\n\\n\\ndef cosine_similarity(vec1: Dict[str, float], vec2: Dict[str, float]) -> float:\\n    \\\"\\\"\\\"\\n    Compute cosine similarity between two sparse vectors.\\n    \\n    Args:\\n        vec1: First vector as dict of term -> weight\\n        vec2: Second vector as dict of term -> weight\\n        \\n    Returns:\\n        Cosine similarity between 0 and 1\\n    \\\"\\\"\\\"\\n    # Find common keys\\n    common = set(vec1.keys()) & set(vec2.keys())\\n    \\n    if not common:\\n        return 0.0\\n    \\n    # Compute dot product\\n    dot = sum(vec1[k] * vec2[k] for k in common)\\n    \\n    # Compute magnitudes\\n    mag1 = math.sqrt(sum(v * v for v in vec1.values()))\\n    mag2 = math.sqrt(sum(v * v for v in vec2.values()))\\n    \\n    if mag1 == 0 or mag2 == 0:\\n        return 0.0\\n    \\n    return dot / (mag1 * mag2)\\n\",",
        "      \"mtime\": 1765392877.0",
        "    },",
        "    {",
        "      \"op\": \"add\",",
        "      \"doc_id\": \"tests/test_incremental_indexing.py\",",
        "      \"content\": \"\\\"\\\"\\\"\\nTests for incremental indexing functionality.\\n\\nTests cover:\\n- remove_document() method in processor\\n- remove_minicolumn() method in layers\\n- Manifest file operations\\n- File change detection\\n\\\"\\\"\\\"\\n\\nimport json\\nimport os\\nimport sys\\nimport tempfile\\nimport unittest\\nfrom pathlib import Path\\nfrom unittest.mock import patch, MagicMock\\n\\n# Add parent directory to path for imports\\nsys.path.insert(0, str(Path(__file__).parent.parent))\\n\\nfrom cortical.processor import CorticalTextProcessor\\nfrom cortical.layers import CorticalLayer, HierarchicalLayer\\nfrom cortical.minicolumn import Minicolumn\\n\\n\\nclass TestRemoveDocument(unittest.TestCase):\\n    \\\"\\\"\\\"Tests for CorticalTextProcessor.remove_document()\\\"\\\"\\\"\\n\\n    def setUp(self):\\n        \\\"\\\"\\\"Set up a processor with test documents.\\\"\\\"\\\"\\n        self.processor = CorticalTextProcessor()\\n        self.processor.process_document(\\\"doc1\\\", \\\"Neural networks process information efficiently.\\\")\\n        self.processor.process_document(\\\"doc2\\\", \\\"Machine learning algorithms learn patterns.\\\")\\n        self.processor.process_document(\\\"doc3\\\", \\\"Neural machine translation uses deep learning.\\\")\\n        self.processor.compute_all(verbose=False)\\n\\n    def test_remove_document_basic(self):\\n        \\\"\\\"\\\"Test basic document removal.\\\"\\\"\\\"\\n        self.assertEqual(len(self.processor.documents), 3)\\n\\n        result = self.processor.remove_document(\\\"doc1\\\")\\n\\n        self.assertTrue(result['found'])\\n        self.assertEqual(len(self.processor.documents), 2)\\n        self.assertNotIn(\\\"doc1\\\", self.processor.documents)\\n\\n    def test_remove_document_not_found(self):\\n        \\\"\\\"\\\"Test removing a non-existent document.\\\"\\\"\\\"\\n        result = self.processor.remove_document(\\\"nonexistent\\\")\\n\\n        self.assertFalse(result['found'])\\n        self.assertEqual(result['tokens_affected'], 0)\\n        self.assertEqual(result['bigrams_affected'], 0)\\n\\n    def test_remove_document_cleans_token_document_ids(self):\\n        \\\"\\\"\\\"Test that document ID is removed from token document_ids sets.\\\"\\\"\\\"\\n        layer0 = self.processor.layers[CorticalLayer.TOKENS]\\n\\n        # neural appears in doc1 and doc3\\n        neural_col = layer0.get_minicolumn(\\\"neural\\\")\\n        self.assertIn(\\\"doc1\\\", neural_col.document_ids)\\n\\n        self.processor.remove_document(\\\"doc1\\\")\\n\\n        # neural should no longer reference doc1\\n        self.assertNotIn(\\\"doc1\\\", neural_col.document_ids)\\n        # But should still reference doc3\\n        self.assertIn(\\\"doc3\\\", neural_col.document_ids)\\n\\n    def test_remove_document_cleans_bigram_document_ids(self):\\n        \\\"\\\"\\\"Test that document ID is removed from bigram document_ids sets.\\\"\\\"\\\"\\n        layer1 = self.processor.layers[CorticalLayer.BIGRAMS]\\n\\n        # Find a bigram from doc1\\n        bigram_col = layer1.get_minicolumn(\\\"neural networks\\\")\\n        if bigram_col:\\n            self.assertIn(\\\"doc1\\\", bigram_col.document_ids)\\n            self.processor.remove_document(\\\"doc1\\\")\\n            self.assertNotIn(\\\"doc1\\\", bigram_col.document_ids)\\n\\n    def test_remove_document_removes_layer3_minicolumn(self):\\n        \\\"\\\"\\\"Test that the document minicolumn is removed from Layer 3.\\\"\\\"\\\"\\n        layer3 = self.processor.layers[CorticalLayer.DOCUMENTS]\\n\\n        self.assertIn(\\\"doc1\\\", layer3.minicolumns)\\n        self.processor.remove_document(\\\"doc1\\\")\\n        self.assertNotIn(\\\"doc1\\\", layer3.minicolumns)\\n\\n    def test_remove_document_removes_metadata(self):\\n        \\\"\\\"\\\"Test that document metadata is removed.\\\"\\\"\\\"\\n        self.processor.set_document_metadata(\\\"doc1\\\", source=\\\"test\\\")\\n        self.assertEqual(self.processor.get_document_metadata(\\\"doc1\\\"), {\\\"source\\\": \\\"test\\\"})\\n\\n        self.processor.remove_document(\\\"doc1\\\")\\n\\n        self.assertEqual(self.processor.get_document_metadata(\\\"doc1\\\"), {})\\n\\n    def test_remove_document_marks_stale(self):\\n        \\\"\\\"\\\"Test that removal marks computations as stale.\\\"\\\"\\\"\\n        # After compute_all, computations should not be stale\\n        self.assertFalse(self.processor.is_stale(self.processor.COMP_TFIDF))\\n\\n        self.processor.remove_document(\\\"doc1\\\")\\n\\n        # After removal, computations should be stale\\n        self.assertTrue(self.processor.is_stale(self.processor.COMP_TFIDF))\\n\\n    def test_remove_document_returns_affected_counts(self):\\n        \\\"\\\"\\\"Test that removal returns correct affected counts.\\\"\\\"\\\"\\n        result = self.processor.remove_document(\\\"doc1\\\")\\n\\n        self.assertTrue(result['found'])\\n        self.assertGreater(result['tokens_affected'], 0)\\n        self.assertGreater(result['bigrams_affected'], 0)\\n\\n    def test_remove_document_verbose(self):\\n        \\\"\\\"\\\"Test verbose mode prints output.\\\"\\\"\\\"\\n        with patch('builtins.print') as mock_print:\\n            self.processor.remove_document(\\\"doc1\\\", verbose=True)\\n            mock_print.assert_called()\\n\\n\\nclass TestRemoveDocumentsBatch(unittest.TestCase):\\n    \\\"\\\"\\\"Tests for CorticalTextProcessor.remove_documents_batch()\\\"\\\"\\\"\\n\\n    def setUp(self):\\n        \\\"\\\"\\\"Set up a processor with test documents.\\\"\\\"\\\"\\n        self.processor = CorticalTextProcessor()\\n        for i in range(5):\\n            self.processor.process_document(f\\\"doc{i}\\\", f\\\"Document {i} content here.\\\")\\n        self.processor.compute_all(verbose=False)\\n\\n    def test_remove_documents_batch_basic(self):\\n        \\\"\\\"\\\"Test removing multiple documents.\\\"\\\"\\\"\\n        result = self.processor.remove_documents_batch([\\\"doc0\\\", \\\"doc1\\\", \\\"doc2\\\"])\\n\\n        self.assertEqual(result['documents_removed'], 3)\\n        self.assertEqual(result['documents_not_found'], 0)\\n        self.assertEqual(len(self.processor.documents), 2)\\n\\n    def test_remove_documents_batch_with_missing(self):\\n        \\\"\\\"\\\"Test removing documents when some don't exist.\\\"\\\"\\\"\\n        result = self.processor.remove_documents_batch([\\\"doc0\\\", \\\"nonexistent\\\", \\\"doc1\\\"])\\n\\n        self.assertEqual(result['documents_removed'], 2)\\n        self.assertEqual(result['documents_not_found'], 1)\\n\\n    def test_remove_documents_batch_with_recompute_tfidf(self):\\n        \\\"\\\"\\\"Test batch removal with TF-IDF recomputation.\\\"\\\"\\\"\\n        result = self.processor.remove_documents_batch([\\\"doc0\\\"], recompute='tfidf')\\n\\n        self.assertEqual(result['recomputation'], 'tfidf')\\n        self.assertFalse(self.processor.is_stale(self.processor.COMP_TFIDF))\\n\\n    def test_remove_documents_batch_with_recompute_full(self):\\n        \\\"\\\"\\\"Test batch removal with full recomputation.\\\"\\\"\\\"\\n        result = self.processor.remove_documents_batch([\\\"doc0\\\"], recompute='full')\\n\\n        self.assertEqual(result['recomputation'], 'full')\\n        self.assertEqual(len(self.processor.get_stale_computations()), 0)\\n\\n\\nclass TestRemoveMinicolumn(unittest.TestCase):\\n    \\\"\\\"\\\"Tests for HierarchicalLayer.remove_minicolumn()\\\"\\\"\\\"\\n\\n    def setUp(self):\\n        \\\"\\\"\\\"Set up a test layer with minicolumns.\\\"\\\"\\\"\\n        self.layer = HierarchicalLayer(CorticalLayer.TOKENS)\\n        self.layer.get_or_create_minicolumn(\\\"test\\\")\\n        self.layer.get_or_create_minicolumn(\\\"neural\\\")\\n        self.layer.get_or_create_minicolumn(\\\"network\\\")\\n\\n    def test_remove_minicolumn_basic(self):\\n        \\\"\\\"\\\"Test basic minicolumn removal.\\\"\\\"\\\"\\n        self.assertEqual(self.layer.column_count(), 3)\\n\\n        result = self.layer.remove_minicolumn(\\\"test\\\")\\n\\n        self.assertTrue(result)\\n        self.assertEqual(self.layer.column_count(), 2)\\n        self.assertNotIn(\\\"test\\\", self.layer.minicolumns)\\n\\n    def test_remove_minicolumn_not_found(self):\\n        \\\"\\\"\\\"Test removing non-existent minicolumn.\\\"\\\"\\\"\\n        result = self.layer.remove_minicolumn(\\\"nonexistent\\\")\\n\\n        self.assertFalse(result)\\n        self.assertEqual(self.layer.column_count(), 3)\\n\\n    def test_remove_minicolumn_removes_from_id_index(self):\\n        \\\"\\\"\\\"Test that removal updates the ID index.\\\"\\\"\\\"\\n        col = self.layer.get_minicolumn(\\\"test\\\")\\n        col_id = col.id\\n\\n        self.assertIsNotNone(self.layer.get_by_id(col_id))\\n\\n        self.layer.remove_minicolumn(\\\"test\\\")\\n\\n        self.assertIsNone(self.layer.get_by_id(col_id))\\n\\n\\nclass TestManifestOperations(unittest.TestCase):\\n    \\\"\\\"\\\"Tests for manifest file operations in index_codebase.py\\\"\\\"\\\"\\n\\n    def setUp(self):\\n        \\\"\\\"\\\"Set up temporary directory for tests.\\\"\\\"\\\"\\n        self.temp_dir = tempfile.mkdtemp()\\n        self.manifest_path = Path(self.temp_dir) / \\\"test.manifest.json\\\"\\n\\n    def tearDown(self):\\n        \\\"\\\"\\\"Clean up temporary files.\\\"\\\"\\\"\\n        import shutil\\n        shutil.rmtree(self.temp_dir)\\n\\n    def test_save_manifest(self):\\n        \\\"\\\"\\\"Test saving a manifest file.\\\"\\\"\\\"\\n        # Import the functions from the script\\n        sys.path.insert(0, str(Path(__file__).parent.parent / 'scripts'))\\n        from index_codebase import save_manifest, load_manifest\\n\\n        files = {\\n            \\\"cortical/processor.py\\\": 1234567890.0,\\n            \\\"tests/test_processor.py\\\": 1234567891.0,\\n        }\\n        stats = {\\\"documents\\\": 2, \\\"tokens\\\": 100}\\n\\n        save_manifest(self.manifest_path, files, \\\"corpus.pkl\\\", stats)\\n\\n        self.assertTrue(self.manifest_path.exists())\\n\\n        # Verify content\\n        with open(self.manifest_path) as f:\\n            data = json.load(f)\\n\\n        self.assertEqual(data['version'], \\\"1.0\\\")\\n        self.assertEqual(data['corpus_path'], \\\"corpus.pkl\\\")\\n        self.assertEqual(len(data['files']), 2)\\n        self.assertEqual(data['stats']['documents'], 2)\\n\\n    def test_load_manifest_valid(self):\\n        \\\"\\\"\\\"Test loading a valid manifest file.\\\"\\\"\\\"\\n        sys.path.insert(0, str(Path(__file__).parent.parent / 'scripts'))\\n        from index_codebase import save_manifest, load_manifest\\n\\n        files = {\\\"test.py\\\": 1234567890.0}\\n        save_manifest(self.manifest_path, files, \\\"corpus.pkl\\\", {})\\n\\n        manifest = load_manifest(self.manifest_path)\\n\\n        self.assertIsNotNone(manifest)\\n        self.assertEqual(manifest['files'], files)\\n\\n    def test_load_manifest_not_exists(self):\\n        \\\"\\\"\\\"Test loading a non-existent manifest file.\\\"\\\"\\\"\\n        sys.path.insert(0, str(Path(__file__).parent.parent / 'scripts'))\\n        from index_codebase import load_manifest\\n\\n        manifest = load_manifest(Path(self.temp_dir) / \\\"nonexistent.json\\\")\\n\\n        self.assertIsNone(manifest)\\n\\n    def test_load_manifest_invalid_version(self):\\n        \\\"\\\"\\\"Test loading a manifest with wrong version.\\\"\\\"\\\"\\n        sys.path.insert(0, str(Path(__file__).parent.parent / 'scripts'))\\n        from index_codebase import load_manifest\\n\\n        # Write manifest with wrong version\\n        with open(self.manifest_path, 'w') as f:\\n            json.dump({\\\"version\\\": \\\"0.1\\\", \\\"files\\\": {}}, f)\\n\\n        manifest = load_manifest(self.manifest_path)\\n\\n        self.assertIsNone(manifest)\\n\\n\\nclass TestFileChangeDetection(unittest.TestCase):\\n    \\\"\\\"\\\"Tests for file change detection.\\\"\\\"\\\"\\n\\n    def setUp(self):\\n        \\\"\\\"\\\"Set up temporary directory with test files.\\\"\\\"\\\"\\n        self.temp_dir = tempfile.mkdtemp()\\n        self.base_path = Path(self.temp_dir)\\n\\n        # Create some test files\\n        (self.base_path / \\\"file1.py\\\").write_text(\\\"content1\\\")\\n        (self.base_path / \\\"file2.py\\\").write_text(\\\"content2\\\")\\n        (self.base_path / \\\"file3.py\\\").write_text(\\\"content3\\\")\\n\\n    def tearDown(self):\\n        \\\"\\\"\\\"Clean up temporary files.\\\"\\\"\\\"\\n        import shutil\\n        shutil.rmtree(self.temp_dir)\\n\\n    def test_get_file_changes_no_changes(self):\\n        \\\"\\\"\\\"Test detecting no changes.\\\"\\\"\\\"\\n        sys.path.insert(0, str(Path(__file__).parent.parent / 'scripts'))\\n        from index_codebase import get_file_changes, get_file_mtime\\n\\n        current_files = list(self.base_path.glob(\\\"*.py\\\"))\\n        manifest = {\\n            'files': {\\n                str(f.relative_to(self.base_path)): get_file_mtime(f)\\n                for f in current_files\\n            }\\n        }\\n\\n        added, modified, deleted = get_file_changes(manifest, current_files, self.base_path)\\n\\n        self.assertEqual(len(added), 0)\\n        self.assertEqual(len(modified), 0)\\n        self.assertEqual(len(deleted), 0)\\n\\n    def test_get_file_changes_added_file(self):\\n        \\\"\\\"\\\"Test detecting added files.\\\"\\\"\\\"\\n        sys.path.insert(0, str(Path(__file__).parent.parent / 'scripts'))\\n        from index_codebase import get_file_changes, get_file_mtime\\n\\n        # Create manifest without file3.py\\n        manifest = {\\n            'files': {\\n                \\\"file1.py\\\": get_file_mtime(self.base_path / \\\"file1.py\\\"),\\n                \\\"file2.py\\\": get_file_mtime(self.base_path / \\\"file2.py\\\"),\\n            }\\n        }\\n\\n        current_files = list(self.base_path.glob(\\\"*.py\\\"))\\n        added, modified, deleted = get_file_changes(manifest, current_files, self.base_path)\\n\\n        self.assertEqual(len(added), 1)\\n        self.assertEqual(added[0].name, \\\"file3.py\\\")\\n        self.assertEqual(len(modified), 0)\\n        self.assertEqual(len(deleted), 0)\\n\\n    def test_get_file_changes_deleted_file(self):\\n        \\\"\\\"\\\"Test detecting deleted files.\\\"\\\"\\\"\\n        sys.path.insert(0, str(Path(__file__).parent.parent / 'scripts'))\\n        from index_codebase import get_file_changes, get_file_mtime\\n\\n        # Create manifest with an extra file that doesn't exist\\n        manifest = {\\n            'files': {\\n                \\\"file1.py\\\": get_file_mtime(self.base_path / \\\"file1.py\\\"),\\n                \\\"file2.py\\\": get_file_mtime(self.base_path / \\\"file2.py\\\"),\\n                \\\"file3.py\\\": get_file_mtime(self.base_path / \\\"file3.py\\\"),\\n                \\\"deleted.py\\\": 1234567890.0,  # This file doesn't exist\\n            }\\n        }\\n\\n        current_files = list(self.base_path.glob(\\\"*.py\\\"))\\n        added, modified, deleted = get_file_changes(manifest, current_files, self.base_path)\\n\\n        self.assertEqual(len(added), 0)\\n        self.assertEqual(len(modified), 0)\\n        self.assertEqual(len(deleted), 1)\\n        self.assertIn(\\\"deleted.py\\\", deleted)\\n\\n    def test_get_file_changes_modified_file(self):\\n        \\\"\\\"\\\"Test detecting modified files.\\\"\\\"\\\"\\n        sys.path.insert(0, str(Path(__file__).parent.parent / 'scripts'))\\n        from index_codebase import get_file_changes, get_file_mtime\\n        import time\\n\\n        # Create manifest with old mtime\\n        manifest = {\\n            'files': {\\n                \\\"file1.py\\\": 0.0,  # Very old mtime\\n                \\\"file2.py\\\": get_file_mtime(self.base_path / \\\"file2.py\\\"),\\n                \\\"file3.py\\\": get_file_mtime(self.base_path / \\\"file3.py\\\"),\\n            }\\n        }\\n\\n        current_files = list(self.base_path.glob(\\\"*.py\\\"))\\n        added, modified, deleted = get_file_changes(manifest, current_files, self.base_path)\\n\\n        self.assertEqual(len(added), 0)\\n        self.assertEqual(len(modified), 1)\\n        self.assertEqual(modified[0].name, \\\"file1.py\\\")\\n        self.assertEqual(len(deleted), 0)\\n\\n\\nclass TestIncrementalIndexingIntegration(unittest.TestCase):\\n    \\\"\\\"\\\"Integration tests for incremental indexing workflow.\\\"\\\"\\\"\\n\\n    def test_add_remove_reindex_workflow(self):\\n        \\\"\\\"\\\"Test the full workflow of add, remove, and reindex.\\\"\\\"\\\"\\n        processor = CorticalTextProcessor()\\n\\n        # Initial indexing\\n        processor.process_document(\\\"doc1\\\", \\\"Neural networks are powerful.\\\")\\n        processor.process_document(\\\"doc2\\\", \\\"Machine learning is useful.\\\")\\n        processor.compute_all(verbose=False)\\n\\n        initial_doc_count = len(processor.documents)\\n        self.assertEqual(initial_doc_count, 2)\\n\\n        # Remove a document\\n        result = processor.remove_document(\\\"doc1\\\")\\n        self.assertTrue(result['found'])\\n        self.assertEqual(len(processor.documents), 1)\\n\\n        # Add a new document\\n        processor.process_document(\\\"doc3\\\", \\\"Deep learning advances rapidly.\\\")\\n\\n        # Recompute\\n        processor.compute_all(verbose=False)\\n\\n        # Verify final state\\n        self.assertEqual(len(processor.documents), 2)\\n        self.assertNotIn(\\\"doc1\\\", processor.documents)\\n        self.assertIn(\\\"doc2\\\", processor.documents)\\n        self.assertIn(\\\"doc3\\\", processor.documents)\\n\\n    def test_incremental_preserves_other_documents(self):\\n        \\\"\\\"\\\"Test that incremental updates don't affect unchanged documents.\\\"\\\"\\\"\\n        processor = CorticalTextProcessor()\\n\\n        processor.process_document(\\\"doc1\\\", \\\"The quick brown fox.\\\")\\n        processor.process_document(\\\"doc2\\\", \\\"Jumps over the lazy dog.\\\")\\n        processor.compute_all(verbose=False)\\n\\n        # Store original state of doc2\\n        layer0 = processor.layers[CorticalLayer.TOKENS]\\n        original_quick_docs = layer0.get_minicolumn(\\\"quick\\\").document_ids.copy()\\n\\n        # Remove doc1\\n        processor.remove_document(\\\"doc1\\\")\\n\\n        # doc2 tokens should still reference doc2\\n        lazy_col = layer0.get_minicolumn(\\\"lazy\\\")\\n        self.assertIn(\\\"doc2\\\", lazy_col.document_ids)\\n\\n\\nclass TestProgressTracker(unittest.TestCase):\\n    \\\"\\\"\\\"Tests for ProgressTracker class.\\\"\\\"\\\"\\n\\n    def setUp(self):\\n        \\\"\\\"\\\"Set up temporary directory for log files.\\\"\\\"\\\"\\n        self.temp_dir = tempfile.mkdtemp()\\n\\n    def tearDown(self):\\n        \\\"\\\"\\\"Clean up temporary files.\\\"\\\"\\\"\\n        import shutil\\n        shutil.rmtree(self.temp_dir)\\n\\n    def test_progress_tracker_init(self):\\n        \\\"\\\"\\\"Test ProgressTracker initialization.\\\"\\\"\\\"\\n        sys.path.insert(0, str(Path(__file__).parent.parent / 'scripts'))\\n        from index_codebase import ProgressTracker\\n\\n        tracker = ProgressTracker(quiet=True)\\n        self.assertIsNotNone(tracker.start_time)\\n        self.assertEqual(tracker.phases, {})\\n        self.assertIsNone(tracker.current_phase)\\n\\n    def test_progress_tracker_with_log_file(self):\\n        \\\"\\\"\\\"Test ProgressTracker with log file output.\\\"\\\"\\\"\\n        sys.path.insert(0, str(Path(__file__).parent.parent / 'scripts'))\\n        from index_codebase import ProgressTracker\\n\\n        log_path = os.path.join(self.temp_dir, \\\"test.log\\\")\\n        tracker = ProgressTracker(log_file=log_path, quiet=True)\\n        tracker.log(\\\"Test message\\\")\\n\\n        # Flush handlers\\n        for handler in tracker.logger.handlers:\\n            handler.flush()\\n\\n        self.assertTrue(os.path.exists(log_path))\\n        with open(log_path) as f:\\n            content = f.read()\\n        self.assertIn(\\\"Test message\\\", content)\\n\\n    def test_start_and_end_phase(self):\\n        \\\"\\\"\\\"Test phase tracking.\\\"\\\"\\\"\\n        sys.path.insert(0, str(Path(__file__).parent.parent / 'scripts'))\\n        from index_codebase import ProgressTracker\\n\\n        tracker = ProgressTracker(quiet=True)\\n\\n        tracker.start_phase(\\\"Test Phase\\\", total_items=10)\\n        self.assertEqual(tracker.current_phase, \\\"Test Phase\\\")\\n        self.assertIn(\\\"Test Phase\\\", tracker.phases)\\n        self.assertEqual(tracker.phases[\\\"Test Phase\\\"].status, \\\"running\\\")\\n\\n        tracker.end_phase(\\\"Test Phase\\\")\\n        self.assertEqual(tracker.phases[\\\"Test Phase\\\"].status, \\\"completed\\\")\\n        self.assertGreater(tracker.phases[\\\"Test Phase\\\"].duration, 0)\\n\\n    def test_update_progress(self):\\n        \\\"\\\"\\\"Test progress updates within a phase.\\\"\\\"\\\"\\n        sys.path.insert(0, str(Path(__file__).parent.parent / 'scripts'))\\n        from index_codebase import ProgressTracker\\n\\n        tracker = ProgressTracker(quiet=True)\\n        tracker.start_phase(\\\"Processing\\\", total_items=100)\\n\\n        tracker.update_progress(25, \\\"item_25\\\")\\n        self.assertEqual(tracker.phases[\\\"Processing\\\"].items_processed, 25)\\n        self.assertEqual(tracker.phases[\\\"Processing\\\"].progress_pct, 25.0)\\n\\n        tracker.update_progress(50, \\\"item_50\\\")\\n        self.assertEqual(tracker.phases[\\\"Processing\\\"].items_processed, 50)\\n        self.assertEqual(tracker.phases[\\\"Processing\\\"].progress_pct, 50.0)\\n\\n    def test_warn_and_error(self):\\n        \\\"\\\"\\\"Test warning and error tracking.\\\"\\\"\\\"\\n        sys.path.insert(0, str(Path(__file__).parent.parent / 'scripts'))\\n        from index_codebase import ProgressTracker\\n\\n        tracker = ProgressTracker(quiet=True)\\n\\n        tracker.warn(\\\"Test warning\\\")\\n        tracker.error(\\\"Test error\\\")\\n\\n        self.assertEqual(len(tracker.warnings), 1)\\n        self.assertEqual(len(tracker.errors), 1)\\n        self.assertIn(\\\"Test warning\\\", tracker.warnings)\\n        self.assertIn(\\\"Test error\\\", tracker.errors)\\n\\n    def test_get_summary(self):\\n        \\\"\\\"\\\"Test summary generation.\\\"\\\"\\\"\\n        sys.path.insert(0, str(Path(__file__).parent.parent / 'scripts'))\\n        from index_codebase import ProgressTracker\\n\\n        tracker = ProgressTracker(quiet=True)\\n        tracker.start_phase(\\\"Phase 1\\\", total_items=5)\\n        tracker.update_progress(5)\\n        tracker.end_phase(\\\"Phase 1\\\")\\n        tracker.warn(\\\"A warning\\\")\\n\\n        summary = tracker.get_summary()\\n\\n        self.assertIn(\\\"total_duration\\\", summary)\\n        self.assertIn(\\\"phases\\\", summary)\\n        self.assertIn(\\\"Phase 1\\\", summary[\\\"phases\\\"])\\n        self.assertEqual(summary[\\\"warnings\\\"], 1)\\n        self.assertEqual(summary[\\\"errors\\\"], 0)\\n\\n\\nclass TestPhaseStats(unittest.TestCase):\\n    \\\"\\\"\\\"Tests for PhaseStats dataclass.\\\"\\\"\\\"\\n\\n    def test_phase_stats_duration(self):\\n        \\\"\\\"\\\"Test duration calculation.\\\"\\\"\\\"\\n        sys.path.insert(0, str(Path(__file__).parent.parent / 'scripts'))\\n        from index_codebase import PhaseStats\\n        import time\\n\\n        phase = PhaseStats(name=\\\"test\\\", start_time=time.time())\\n        time.sleep(0.01)\\n        phase.end_time = time.time()\\n\\n        self.assertGreater(phase.duration, 0)\\n        self.assertLess(phase.duration, 1)\\n\\n    def test_phase_stats_progress_pct(self):\\n        \\\"\\\"\\\"Test progress percentage calculation.\\\"\\\"\\\"\\n        sys.path.insert(0, str(Path(__file__).parent.parent / 'scripts'))\\n        from index_codebase import PhaseStats\\n\\n        phase = PhaseStats(name=\\\"test\\\", items_total=100, items_processed=25)\\n        self.assertEqual(phase.progress_pct, 25.0)\\n\\n        phase.items_processed = 50\\n        self.assertEqual(phase.progress_pct, 50.0)\\n\\n        # Edge case: zero total\\n        phase.items_total = 0\\n        self.assertEqual(phase.progress_pct, 0.0)\\n\\n\\nclass TestTimeoutHandler(unittest.TestCase):\\n    \\\"\\\"\\\"Tests for timeout handling.\\\"\\\"\\\"\\n\\n    def test_timeout_handler_no_timeout(self):\\n        \\\"\\\"\\\"Test that timeout=0 means no timeout.\\\"\\\"\\\"\\n        sys.path.insert(0, str(Path(__file__).parent.parent / 'scripts'))\\n        from index_codebase import timeout_handler\\n\\n        # Should complete without issue\\n        with timeout_handler(0):\\n            result = 1 + 1\\n        self.assertEqual(result, 2)\\n\\n    def test_timeout_handler_completes_in_time(self):\\n        \\\"\\\"\\\"Test that operations completing in time succeed.\\\"\\\"\\\"\\n        sys.path.insert(0, str(Path(__file__).parent.parent / 'scripts'))\\n        from index_codebase import timeout_handler\\n\\n        with timeout_handler(5):\\n            result = sum(range(100))\\n        self.assertEqual(result, 4950)\\n\\n\\nclass TestIndexingFunctions(unittest.TestCase):\\n    \\\"\\\"\\\"Tests for indexing helper functions.\\\"\\\"\\\"\\n\\n    def setUp(self):\\n        \\\"\\\"\\\"Set up temporary directory with test files.\\\"\\\"\\\"\\n        self.temp_dir = tempfile.mkdtemp()\\n        self.base_path = Path(self.temp_dir)\\n\\n        # Create test file structure\\n        (self.base_path / \\\"cortical\\\").mkdir()\\n        (self.base_path / \\\"tests\\\").mkdir()\\n        (self.base_path / \\\"cortical\\\" / \\\"test.py\\\").write_text(\\\"# Test file\\\\nprint('hello')\\\")\\n        (self.base_path / \\\"tests\\\" / \\\"test_test.py\\\").write_text(\\\"# Test\\\\nimport unittest\\\")\\n        (self.base_path / \\\"CLAUDE.md\\\").write_text(\\\"# Documentation\\\")\\n\\n    def tearDown(self):\\n        \\\"\\\"\\\"Clean up temporary files.\\\"\\\"\\\"\\n        import shutil\\n        shutil.rmtree(self.temp_dir)\\n\\n    def test_get_python_files(self):\\n        \\\"\\\"\\\"Test Python file discovery.\\\"\\\"\\\"\\n        sys.path.insert(0, str(Path(__file__).parent.parent / 'scripts'))\\n        from index_codebase import get_python_files\\n\\n        files = get_python_files(self.base_path)\\n        file_names = [f.name for f in files]\\n\\n        self.assertIn(\\\"test.py\\\", file_names)\\n        self.assertIn(\\\"test_test.py\\\", file_names)\\n\\n    def test_get_doc_files(self):\\n        \\\"\\\"\\\"Test documentation file discovery.\\\"\\\"\\\"\\n        sys.path.insert(0, str(Path(__file__).parent.parent / 'scripts'))\\n        from index_codebase import get_doc_files\\n\\n        files = get_doc_files(self.base_path)\\n        file_names = [f.name for f in files]\\n\\n        self.assertIn(\\\"CLAUDE.md\\\", file_names)\\n\\n    def test_create_doc_id(self):\\n        \\\"\\\"\\\"Test document ID creation.\\\"\\\"\\\"\\n        sys.path.insert(0, str(Path(__file__).parent.parent / 'scripts'))\\n        from index_codebase import create_doc_id\\n\\n        file_path = self.base_path / \\\"cortical\\\" / \\\"test.py\\\"\\n        doc_id = create_doc_id(file_path, self.base_path)\\n\\n        self.assertEqual(doc_id, \\\"cortical/test.py\\\")\\n\\n    def test_get_file_mtime(self):\\n        \\\"\\\"\\\"Test file modification time retrieval.\\\"\\\"\\\"\\n        sys.path.insert(0, str(Path(__file__).parent.parent / 'scripts'))\\n        from index_codebase import get_file_mtime\\n\\n        file_path = self.base_path / \\\"CLAUDE.md\\\"\\n        mtime = get_file_mtime(file_path)\\n\\n        self.assertIsInstance(mtime, float)\\n        self.assertGreater(mtime, 0)\\n\\n    def test_index_file(self):\\n        \\\"\\\"\\\"Test single file indexing.\\\"\\\"\\\"\\n        sys.path.insert(0, str(Path(__file__).parent.parent / 'scripts'))\\n        from index_codebase import index_file\\n\\n        processor = CorticalTextProcessor()\\n        file_path = self.base_path / \\\"cortical\\\" / \\\"test.py\\\"\\n\\n        metadata = index_file(processor, file_path, self.base_path)\\n\\n        self.assertIsNotNone(metadata)\\n        self.assertEqual(metadata['relative_path'], \\\"cortical/test.py\\\")\\n        self.assertEqual(metadata['file_type'], \\\".py\\\")\\n        self.assertEqual(metadata['language'], \\\"python\\\")\\n        self.assertIn(\\\"cortical/test.py\\\", processor.documents)\\n\\n    def test_index_file_with_read_error(self):\\n        \\\"\\\"\\\"Test handling of unreadable files.\\\"\\\"\\\"\\n        sys.path.insert(0, str(Path(__file__).parent.parent / 'scripts'))\\n        from index_codebase import index_file, ProgressTracker\\n\\n        processor = CorticalTextProcessor()\\n        tracker = ProgressTracker(quiet=True)\\n        nonexistent = self.base_path / \\\"nonexistent.py\\\"\\n\\n        metadata = index_file(processor, nonexistent, self.base_path, tracker)\\n\\n        self.assertIsNone(metadata)\\n        self.assertEqual(len(tracker.warnings), 1)\\n\\n\\nclass TestFullIndexFunction(unittest.TestCase):\\n    \\\"\\\"\\\"Tests for full_index function.\\\"\\\"\\\"\\n\\n    def setUp(self):\\n        \\\"\\\"\\\"Set up temporary directory with test files.\\\"\\\"\\\"\\n        self.temp_dir = tempfile.mkdtemp()\\n        self.base_path = Path(self.temp_dir)\\n\\n        (self.base_path / \\\"file1.py\\\").write_text(\\\"# File 1\\\\nprint('a')\\\")\\n        (self.base_path / \\\"file2.py\\\").write_text(\\\"# File 2\\\\nprint('b')\\\")\\n\\n    def tearDown(self):\\n        \\\"\\\"\\\"Clean up temporary files.\\\"\\\"\\\"\\n        import shutil\\n        shutil.rmtree(self.temp_dir)\\n\\n    def test_full_index(self):\\n        \\\"\\\"\\\"Test full indexing of files.\\\"\\\"\\\"\\n        sys.path.insert(0, str(Path(__file__).parent.parent / 'scripts'))\\n        from index_codebase import full_index, ProgressTracker\\n\\n        processor = CorticalTextProcessor()\\n        tracker = ProgressTracker(quiet=True)\\n        all_files = list(self.base_path.glob(\\\"*.py\\\"))\\n\\n        indexed, total_lines, file_mtimes = full_index(\\n            processor, all_files, self.base_path, tracker\\n        )\\n\\n        self.assertEqual(indexed, 2)\\n        self.assertGreater(total_lines, 0)\\n        self.assertEqual(len(file_mtimes), 2)\\n        self.assertIn(\\\"Indexing files\\\", tracker.phases)\\n\\n\\nclass TestIncrementalIndexFunction(unittest.TestCase):\\n    \\\"\\\"\\\"Tests for incremental_index function.\\\"\\\"\\\"\\n\\n    def setUp(self):\\n        \\\"\\\"\\\"Set up temporary directory with test files.\\\"\\\"\\\"\\n        self.temp_dir = tempfile.mkdtemp()\\n        self.base_path = Path(self.temp_dir)\\n\\n        (self.base_path / \\\"existing.py\\\").write_text(\\\"# Existing\\\\nprint('x')\\\")\\n        (self.base_path / \\\"new.py\\\").write_text(\\\"# New\\\\nprint('y')\\\")\\n\\n    def tearDown(self):\\n        \\\"\\\"\\\"Clean up temporary files.\\\"\\\"\\\"\\n        import shutil\\n        shutil.rmtree(self.temp_dir)\\n\\n    def test_incremental_index_added_files(self):\\n        \\\"\\\"\\\"Test incremental indexing of added files.\\\"\\\"\\\"\\n        sys.path.insert(0, str(Path(__file__).parent.parent / 'scripts'))\\n        from index_codebase import incremental_index, ProgressTracker\\n\\n        processor = CorticalTextProcessor()\\n        tracker = ProgressTracker(quiet=True)\\n\\n        added = [self.base_path / \\\"new.py\\\"]\\n        modified = []\\n        deleted = []\\n\\n        added_count, modified_count, deleted_count, total_lines = incremental_index(\\n            processor, added, modified, deleted, self.base_path, tracker\\n        )\\n\\n        self.assertEqual(added_count, 1)\\n        self.assertEqual(modified_count, 0)\\n        self.assertEqual(deleted_count, 0)\\n        self.assertIn(\\\"new.py\\\", processor.documents)\\n\\n    def test_incremental_index_modified_files(self):\\n        \\\"\\\"\\\"Test incremental indexing of modified files.\\\"\\\"\\\"\\n        sys.path.insert(0, str(Path(__file__).parent.parent / 'scripts'))\\n        from index_codebase import incremental_index, index_file, ProgressTracker\\n\\n        processor = CorticalTextProcessor()\\n        tracker = ProgressTracker(quiet=True)\\n\\n        # First, index the existing file\\n        index_file(processor, self.base_path / \\\"existing.py\\\", self.base_path)\\n\\n        # Now modify it (in our test, just re-index as modified)\\n        added = []\\n        modified = [self.base_path / \\\"existing.py\\\"]\\n        deleted = []\\n\\n        added_count, modified_count, deleted_count, total_lines = incremental_index(\\n            processor, added, modified, deleted, self.base_path, tracker\\n        )\\n\\n        self.assertEqual(modified_count, 1)\\n\\n    def test_incremental_index_deleted_files(self):\\n        \\\"\\\"\\\"Test incremental indexing handles deleted files.\\\"\\\"\\\"\\n        sys.path.insert(0, str(Path(__file__).parent.parent / 'scripts'))\\n        from index_codebase import incremental_index, index_file, ProgressTracker\\n\\n        processor = CorticalTextProcessor()\\n        tracker = ProgressTracker(quiet=True)\\n\\n        # First, index a file\\n        index_file(processor, self.base_path / \\\"existing.py\\\", self.base_path)\\n        self.assertIn(\\\"existing.py\\\", processor.documents)\\n\\n        # Now mark it as deleted\\n        added = []\\n        modified = []\\n        deleted = [\\\"existing.py\\\"]\\n\\n        added_count, modified_count, deleted_count, total_lines = incremental_index(\\n            processor, added, modified, deleted, self.base_path, tracker\\n        )\\n\\n        self.assertEqual(deleted_count, 1)\\n        self.assertNotIn(\\\"existing.py\\\", processor.documents)\\n\\n\\nclass TestComputeAnalysis(unittest.TestCase):\\n    \\\"\\\"\\\"Tests for compute_analysis function.\\\"\\\"\\\"\\n\\n    def test_compute_analysis_fast_mode(self):\\n        \\\"\\\"\\\"Test fast mode analysis.\\\"\\\"\\\"\\n        sys.path.insert(0, str(Path(__file__).parent.parent / 'scripts'))\\n        from index_codebase import compute_analysis, ProgressTracker\\n\\n        processor = CorticalTextProcessor()\\n        processor.process_document(\\\"doc1\\\", \\\"Neural networks are powerful.\\\")\\n        processor.process_document(\\\"doc2\\\", \\\"Machine learning algorithms.\\\")\\n\\n        tracker = ProgressTracker(quiet=True)\\n        compute_analysis(processor, tracker, fast_mode=True)\\n\\n        self.assertIn(\\\"Computing analysis (fast mode)\\\", tracker.phases)\\n        # TF-IDF should be computed\\n        layer0 = processor.layers[CorticalLayer.TOKENS]\\n        neural_col = layer0.get_minicolumn(\\\"neural\\\")\\n        self.assertIsNotNone(neural_col)\\n        self.assertGreater(neural_col.tfidf, 0)\\n\\n\\nif __name__ == '__main__':\\n    unittest.main()\\n\",",
        "      \"mtime\": 1765399484.0",
        "    },",
        "    {",
        "      \"op\": \"add\",",
        "      \"doc_id\": \"cortical/semantics.py\",",
        "      \"content\": \"\\\"\\\"\\\"\\nSemantics Module\\n================\\n\\nCorpus-derived semantic relations and retrofitting.\\n\\nExtracts semantic relationships from co-occurrence patterns,\\nthen uses them to adjust connection weights (retrofitting).\\nThis is like building a \\\"poor man's ConceptNet\\\" from the corpus itself.\\n\\\"\\\"\\\"\\n\\nimport math\\nimport re\\nfrom typing import Any, Dict, List, Tuple, Set, Optional\\nimport copy\\nfrom collections import defaultdict\\n\\ntry:\\n    import numpy as np\\n    HAS_NUMPY = True\\nexcept ImportError:\\n    HAS_NUMPY = False\\n\\nfrom .layers import CorticalLayer, HierarchicalLayer\\nfrom .minicolumn import Minicolumn\\n\\n\\n# Relation type weights for retrofitting\\nRELATION_WEIGHTS = {\\n    'IsA': 1.5,\\n    'PartOf': 1.2,\\n    'HasA': 1.0,\\n    'UsedFor': 0.8,\\n    'CapableOf': 0.7,\\n    'AtLocation': 0.6,\\n    'Causes': 0.9,\\n    'HasProperty': 0.8,\\n    'SameAs': 2.0,\\n    'RelatedTo': 0.5,\\n    'Antonym': -0.5,\\n    'DerivedFrom': 1.0,\\n    'SimilarTo': 1.5,\\n    'CoOccurs': 0.6,\\n    'DefinedBy': 1.0,\\n}\\n\\n\\n# Commonsense relation patterns with confidence scores\\n# Format: (pattern_regex, relation_type, confidence, swap_order)\\n# swap_order: if True, the captured groups are in reverse order (t2, t1)\\nRELATION_PATTERNS = [\\n    # IsA patterns (hypernym/type relations)\\n    (r'(\\\\w+)\\\\s+(?:is|are)\\\\s+(?:a|an)\\\\s+(?:type\\\\s+of\\\\s+)?(\\\\w+)', 'IsA', 0.9, False),\\n    (r'(\\\\w+),?\\\\s+(?:a|an)\\\\s+(?:kind|type|form)\\\\s+of\\\\s+(\\\\w+)', 'IsA', 0.95, False),\\n    (r'(\\\\w+)\\\\s+(?:is|are)\\\\s+considered\\\\s+(?:a|an)?\\\\s*(\\\\w+)', 'IsA', 0.8, False),\\n    (r'(?:a|an)\\\\s+(\\\\w+)\\\\s+is\\\\s+(?:a|an)\\\\s+(\\\\w+)', 'IsA', 0.85, False),\\n    (r'(\\\\w+)\\\\s+(?:belongs?\\\\s+to|falls?\\\\s+under)\\\\s+(?:the\\\\s+)?(\\\\w+)', 'IsA', 0.8, False),\\n\\n    # HasA/Contains patterns (meronym relations)\\n    (r'(\\\\w+)\\\\s+(?:has|have|contains?|includes?)\\\\s+(?:a|an|the)?\\\\s*(\\\\w+)', 'HasA', 0.85, False),\\n    (r'(\\\\w+)\\\\s+(?:consists?\\\\s+of|comprises?|is\\\\s+made\\\\s+of)\\\\s+(\\\\w+)', 'HasA', 0.9, False),\\n    (r'(?:a|an|the)\\\\s+(\\\\w+)\\\\s+(?:with|having)\\\\s+(?:a|an|the)?\\\\s*(\\\\w+)', 'HasA', 0.75, False),\\n\\n    # PartOf patterns (part-whole relations)\\n    (r'(\\\\w+)\\\\s+(?:is|are)\\\\s+(?:a\\\\s+)?part\\\\s+of\\\\s+(?:a|an|the)?\\\\s*(\\\\w+)', 'PartOf', 0.95, False),\\n    (r'(\\\\w+)\\\\s+(?:is|are)\\\\s+(?:a\\\\s+)?component\\\\s+of\\\\s+(\\\\w+)', 'PartOf', 0.9, False),\\n    (r'(\\\\w+)\\\\s+(?:is|are)\\\\s+(?:in|within|inside)\\\\s+(?:a|an|the)?\\\\s*(\\\\w+)', 'PartOf', 0.7, False),\\n\\n    # UsedFor patterns (functional relations)\\n    (r'(\\\\w+)\\\\s+(?:is|are)\\\\s+used\\\\s+(?:for|to|in)\\\\s+(\\\\w+)', 'UsedFor', 0.9, False),\\n    (r'(\\\\w+)\\\\s+(?:helps?|enables?|allows?)\\\\s+(\\\\w+)', 'UsedFor', 0.75, False),\\n    (r'(?:use|using)\\\\s+(\\\\w+)\\\\s+(?:for|to)\\\\s+(\\\\w+)', 'UsedFor', 0.85, False),\\n    (r'(\\\\w+)\\\\s+(?:is|are)\\\\s+(?:useful|helpful)\\\\s+for\\\\s+(\\\\w+)', 'UsedFor', 0.8, False),\\n\\n    # Causes patterns (causal relations)\\n    (r'(\\\\w+)\\\\s+(?:causes?|leads?\\\\s+to|results?\\\\s+in)\\\\s+(\\\\w+)', 'Causes', 0.9, False),\\n    (r'(\\\\w+)\\\\s+(?:produces?|generates?|creates?)\\\\s+(\\\\w+)', 'Causes', 0.8, False),\\n    (r'(\\\\w+)\\\\s+(?:can\\\\s+)?(?:cause|lead\\\\s+to|result\\\\s+in)\\\\s+(\\\\w+)', 'Causes', 0.85, False),\\n    (r'(?:because\\\\s+of|due\\\\s+to)\\\\s+(\\\\w+),?\\\\s+(\\\\w+)', 'Causes', 0.7, True),  # Reversed order\\n\\n    # CapableOf patterns (ability relations)\\n    (r'(\\\\w+)\\\\s+(?:can|could|is\\\\s+able\\\\s+to)\\\\s+(\\\\w+)', 'CapableOf', 0.85, False),\\n    (r'(\\\\w+)\\\\s+(?:has\\\\s+the\\\\s+ability\\\\s+to|is\\\\s+capable\\\\s+of)\\\\s+(\\\\w+)', 'CapableOf', 0.9, False),\\n\\n    # AtLocation patterns (spatial relations)\\n    (r'(\\\\w+)\\\\s+(?:is|are)\\\\s+(?:found|located|situated)\\\\s+(?:in|at|on)\\\\s+(\\\\w+)', 'AtLocation', 0.9, False),\\n    (r'(\\\\w+)\\\\s+(?:lives?|exists?|occurs?)\\\\s+(?:in|at|on)\\\\s+(\\\\w+)', 'AtLocation', 0.85, False),\\n\\n    # HasProperty patterns (attribute relations)\\n    (r'(\\\\w+)\\\\s+(?:is|are)\\\\s+(\\\\w+)', 'HasProperty', 0.5, False),  # Very general, low confidence\\n    (r'(\\\\w+)\\\\s+(?:is|are)\\\\s+(?:typically|usually|often|generally)\\\\s+(\\\\w+)', 'HasProperty', 0.7, False),\\n    (r'(?:a|an)\\\\s+(\\\\w+)\\\\s+(\\\\w+)\\\\s+(?:is|are)', 'HasProperty', 0.6, True),  # \\\"a big dog\\\" → dog HasProperty big\\n\\n    # Antonym patterns (opposite relations)\\n    (r'(\\\\w+)\\\\s+(?:is|are)\\\\s+(?:the\\\\s+)?opposite\\\\s+of\\\\s+(\\\\w+)', 'Antonym', 0.95, False),\\n    (r'(\\\\w+)\\\\s+(?:vs\\\\.?|versus|or)\\\\s+(\\\\w+)', 'Antonym', 0.5, False),  # Lower confidence\\n    (r'(\\\\w+)\\\\s+(?:not|isn\\\\'t|aren\\\\'t)\\\\s+(\\\\w+)', 'Antonym', 0.6, False),\\n\\n    # DerivedFrom patterns (morphological/etymological relations)\\n    (r'(\\\\w+)\\\\s+(?:comes?\\\\s+from|is\\\\s+derived\\\\s+from|originates?\\\\s+from)\\\\s+(\\\\w+)', 'DerivedFrom', 0.9, False),\\n    (r'(\\\\w+)\\\\s+(?:is\\\\s+based\\\\s+on|stems?\\\\s+from)\\\\s+(\\\\w+)', 'DerivedFrom', 0.85, False),\\n\\n    # DefinedBy patterns (definitional relations)\\n    (r'(\\\\w+)\\\\s+(?:means?|refers?\\\\s+to|denotes?)\\\\s+(\\\\w+)', 'DefinedBy', 0.85, False),\\n    (r'(\\\\w+)\\\\s+(?:is\\\\s+defined\\\\s+as|is\\\\s+known\\\\s+as)\\\\s+(?:a|an|the)?\\\\s*(\\\\w+)', 'DefinedBy', 0.9, False),\\n]\\n\\n\\ndef extract_pattern_relations(\\n    documents: Dict[str, str],\\n    valid_terms: Set[str],\\n    min_confidence: float = 0.5\\n) -> List[Tuple[str, str, str, float]]:\\n    \\\"\\\"\\\"\\n    Extract semantic relations using pattern matching on document text.\\n\\n    Uses regex patterns to identify commonsense relations like IsA, HasA,\\n    UsedFor, Causes, etc. from natural language expressions.\\n\\n    Args:\\n        documents: Dictionary mapping doc_id to document content\\n        valid_terms: Set of terms that exist in the corpus (from layer0)\\n        min_confidence: Minimum confidence threshold for extracted relations\\n\\n    Returns:\\n        List of (term1, relation_type, term2, confidence) tuples\\n\\n    Example:\\n        >>> relations = extract_pattern_relations(docs, {\\\"dog\\\", \\\"animal\\\", \\\"pet\\\"})\\n        >>> # Finds relations like (\\\"dog\\\", \\\"IsA\\\", \\\"animal\\\", 0.9)\\n    \\\"\\\"\\\"\\n    relations: List[Tuple[str, str, str, float]] = []\\n    seen_relations: Set[Tuple[str, str, str]] = set()\\n\\n    for doc_id, content in documents.items():\\n        content_lower = content.lower()\\n\\n        for pattern, relation_type, confidence, swap_order in RELATION_PATTERNS:\\n            if confidence < min_confidence:\\n                continue\\n\\n            for match in re.finditer(pattern, content_lower):\\n                groups = match.groups()\\n                if len(groups) >= 2:\\n                    t1, t2 = groups[0], groups[1]\\n\\n                    if swap_order:\\n                        t1, t2 = t2, t1\\n\\n                    # Clean terms (remove leading/trailing non-alphanumeric)\\n                    t1 = t1.strip().lower()\\n                    t2 = t2.strip().lower()\\n\\n                    # Skip if terms are the same\\n                    if t1 == t2:\\n                        continue\\n\\n                    # Skip if terms don't exist in corpus\\n                    if t1 not in valid_terms or t2 not in valid_terms:\\n                        continue\\n\\n                    # Skip common stopwords that might slip through patterns\\n                    stopwords = {'the', 'a', 'an', 'is', 'are', 'was', 'were', 'be',\\n                                 'been', 'being', 'have', 'has', 'had', 'do', 'does',\\n                                 'did', 'will', 'would', 'could', 'should', 'may',\\n                                 'might', 'must', 'shall', 'can', 'this', 'that',\\n                                 'these', 'those', 'it', 'its', 'they', 'them',\\n                                 'their', 'we', 'us', 'our', 'you', 'your', 'i', 'me', 'my'}\\n                    if t1 in stopwords or t2 in stopwords:\\n                        continue\\n\\n                    # Create relation key to avoid duplicates\\n                    rel_key = (t1, relation_type, t2)\\n\\n                    # For symmetric relations, also check reverse\\n                    if relation_type in {'SimilarTo', 'Antonym', 'RelatedTo'}:\\n                        rev_key = (t2, relation_type, t1)\\n                        if rev_key in seen_relations:\\n                            continue\\n\\n                    if rel_key not in seen_relations:\\n                        seen_relations.add(rel_key)\\n                        relations.append((t1, relation_type, t2, confidence))\\n\\n    return relations\\n\\n\\ndef get_pattern_statistics(relations: List[Tuple[str, str, str, float]]) -> Dict[str, Any]:\\n    \\\"\\\"\\\"\\n    Get statistics about extracted pattern-based relations.\\n\\n    Args:\\n        relations: List of (term1, relation_type, term2, confidence) tuples\\n\\n    Returns:\\n        Dictionary with statistics about relation types and counts\\n    \\\"\\\"\\\"\\n    type_counts: Dict[str, int] = defaultdict(int)\\n    type_confidences: Dict[str, List[float]] = defaultdict(list)\\n\\n    for t1, rel_type, t2, conf in relations:\\n        type_counts[rel_type] += 1\\n        type_confidences[rel_type].append(conf)\\n\\n    # Compute average confidence per type\\n    avg_confidences = {\\n        rel_type: sum(confs) / len(confs)\\n        for rel_type, confs in type_confidences.items()\\n    }\\n\\n    return {\\n        'total_relations': len(relations),\\n        'relation_type_counts': dict(type_counts),\\n        'average_confidence_by_type': avg_confidences,\\n        'unique_types': len(type_counts)\\n    }\\n\\n\\ndef extract_corpus_semantics(\\n    layers: Dict[CorticalLayer, HierarchicalLayer],\\n    documents: Dict[str, str],\\n    tokenizer,\\n    window_size: int = 5,\\n    min_cooccurrence: int = 2,\\n    use_pattern_extraction: bool = True,\\n    min_pattern_confidence: float = 0.6\\n) -> List[Tuple[str, str, str, float]]:\\n    \\\"\\\"\\\"\\n    Extract semantic relations from corpus co-occurrence patterns.\\n\\n    Analyzes word co-occurrences to infer semantic relationships:\\n    - Words appearing together frequently → CoOccurs\\n    - Words appearing in similar contexts → SimilarTo\\n    - Pattern-based extraction → IsA, HasA, UsedFor, Causes, etc.\\n\\n    Args:\\n        layers: Dictionary of layers (needs TOKENS)\\n        documents: Dictionary of documents\\n        tokenizer: Tokenizer instance for processing text\\n        window_size: Co-occurrence window size\\n        min_cooccurrence: Minimum co-occurrences to form relation\\n        use_pattern_extraction: Whether to extract relations from text patterns\\n        min_pattern_confidence: Minimum confidence for pattern-based extraction\\n\\n    Returns:\\n        List of (term1, relation, term2, weight) tuples\\n    \\\"\\\"\\\"\\n    layer0 = layers[CorticalLayer.TOKENS]\\n    relations: List[Tuple[str, str, str, float]] = []\\n    \\n    # Track co-occurrences within window\\n    cooccurrence: Dict[Tuple[str, str], int] = defaultdict(int)\\n    \\n    # Track context vectors for similarity\\n    context_vectors: Dict[str, Dict[str, int]] = defaultdict(lambda: defaultdict(int))\\n    \\n    for doc_id, content in documents.items():\\n        tokens = tokenizer.tokenize(content)\\n        \\n        # Window-based co-occurrence\\n        for i, token in enumerate(tokens):\\n            window_start = max(0, i - window_size)\\n            window_end = min(len(tokens), i + window_size + 1)\\n            \\n            for j in range(window_start, window_end):\\n                if i != j:\\n                    other = tokens[j]\\n                    if token < other:  # Avoid duplicates\\n                        cooccurrence[(token, other)] += 1\\n                    else:\\n                        cooccurrence[(other, token)] += 1\\n                    \\n                    # Build context vector\\n                    context_vectors[token][other] += 1\\n    \\n    # Extract RelatedTo from co-occurrence\\n    # Compute total once outside the loop (was being computed per iteration!)\\n    total = sum(cooccurrence.values())\\n\\n    for (t1, t2), count in cooccurrence.items():\\n        if count >= min_cooccurrence:\\n            # Normalize by frequency\\n            col1 = layer0.get_minicolumn(t1)\\n            col2 = layer0.get_minicolumn(t2)\\n\\n            if col1 and col2:\\n                # PMI-like score\\n                expected = (col1.occurrence_count * col2.occurrence_count) / (total + 1)\\n                pmi = math.log((count + 1) / (expected + 1))\\n\\n                if pmi > 0:\\n                    relations.append((t1, 'CoOccurs', t2, min(pmi, 3.0)))\\n    \\n    # Extract SimilarTo from context similarity\\n    terms = list(context_vectors.keys())\\n    n_terms = len(terms)\\n\\n    if n_terms > 1 and HAS_NUMPY:\\n        # Fast path: use numpy vectorization\\n        # Build vocabulary of all context keys\\n        all_keys: Set[str] = set()\\n        for vec in context_vectors.values():\\n            all_keys.update(vec.keys())\\n        vocab = sorted(all_keys)\\n        key_to_idx = {k: i for i, k in enumerate(vocab)}\\n        n_vocab = len(vocab)\\n\\n        # Convert sparse vectors to dense numpy matrix\\n        matrix = np.zeros((n_terms, n_vocab), dtype=np.float32)\\n        for i, term in enumerate(terms):\\n            vec = context_vectors[term]\\n            for k, v in vec.items():\\n                matrix[i, key_to_idx[k]] = v\\n\\n        # Normalize rows for cosine similarity (dot product of normalized = cosine)\\n        norms = np.linalg.norm(matrix, axis=1, keepdims=True)\\n        norms[norms == 0] = 1  # Avoid division by zero\\n        matrix_norm = matrix / norms\\n\\n        # Compute all pairwise cosine similarities via matrix multiplication\\n        similarities = matrix_norm @ matrix_norm.T\\n\\n        # Count non-zero elements per row (for min common keys filter)\\n        nonzero_counts = (matrix > 0).astype(np.int32)\\n\\n        # Extract pairs with similarity > 0.3 and at least 3 common keys\\n        for i in range(n_terms):\\n            row_i = nonzero_counts[i]\\n            for j in range(i + 1, n_terms):\\n                if similarities[i, j] > 0.3:\\n                    common_count = np.sum(row_i & nonzero_counts[j])\\n                    if common_count >= 3:\\n                        relations.append((terms[i], 'SimilarTo', terms[j], float(similarities[i, j])))\\n\\n    elif n_terms > 1:\\n        # Fallback: pure Python implementation\\n        magnitudes: Dict[str, float] = {}\\n        for term in terms:\\n            vec = context_vectors[term]\\n            mag = math.sqrt(sum(v * v for v in vec.values()))\\n            magnitudes[term] = mag\\n\\n        key_sets: Dict[str, set] = {term: set(context_vectors[term].keys()) for term in terms}\\n\\n        for i, t1 in enumerate(terms):\\n            vec1 = context_vectors[t1]\\n            mag1 = magnitudes[t1]\\n            if mag1 == 0:\\n                continue\\n            keys1 = key_sets[t1]\\n\\n            for t2 in terms[i+1:]:\\n                mag2 = magnitudes[t2]\\n                if mag2 == 0:\\n                    continue\\n\\n                common = keys1 & key_sets[t2]\\n                if len(common) >= 3:\\n                    vec2 = context_vectors[t2]\\n                    dot = sum(vec1[k] * vec2[k] for k in common)\\n                    sim = dot / (mag1 * mag2)\\n                    if sim > 0.3:\\n                        relations.append((t1, 'SimilarTo', t2, sim))\\n\\n    # Extract commonsense relations from text patterns\\n    if use_pattern_extraction:\\n        valid_terms = set(layer0.minicolumns.keys())\\n        pattern_relations = extract_pattern_relations(\\n            documents,\\n            valid_terms,\\n            min_confidence=min_pattern_confidence\\n        )\\n        relations.extend(pattern_relations)\\n\\n    return relations\\n\\n\\ndef retrofit_connections(\\n    layers: Dict[CorticalLayer, HierarchicalLayer],\\n    semantic_relations: List[Tuple[str, str, str, float]],\\n    iterations: int = 10,\\n    alpha: float = 0.3\\n) -> Dict[str, Any]:\\n    \\\"\\\"\\\"\\n    Retrofit lateral connections using semantic relations.\\n    \\n    Adjusts connection weights by blending co-occurrence patterns\\n    with semantic relations. This is inspired by Faruqui et al.'s\\n    retrofitting algorithm for word vectors.\\n    \\n    Args:\\n        layers: Dictionary of layers\\n        semantic_relations: List of (term1, relation, term2, weight) tuples\\n        iterations: Number of retrofitting iterations\\n        alpha: Blend factor (0=all semantic, 1=all original)\\n\\n    Returns:\\n        Dictionary with retrofitting statistics\\n\\n    Raises:\\n        ValueError: If alpha is not in range [0, 1]\\n    \\\"\\\"\\\"\\n    if not (0 <= alpha <= 1):\\n        raise ValueError(f\\\"alpha must be between 0 and 1, got {alpha}\\\")\\n\\n    layer0 = layers[CorticalLayer.TOKENS]\\n\\n    # Store original weights\\n    original_weights: Dict[str, Dict[str, float]] = {}\\n    for col in layer0.minicolumns.values():\\n        original_weights[col.content] = dict(col.lateral_connections)\\n    \\n    # Build semantic neighbor lookup\\n    semantic_neighbors: Dict[str, List[Tuple[str, float]]] = defaultdict(list)\\n    \\n    for t1, relation, t2, weight in semantic_relations:\\n        relation_weight = RELATION_WEIGHTS.get(relation, 0.5)\\n        combined_weight = weight * relation_weight\\n        \\n        # Bidirectional\\n        semantic_neighbors[t1].append((t2, combined_weight))\\n        semantic_neighbors[t2].append((t1, combined_weight))\\n    \\n    # Iterative retrofitting\\n    tokens_affected = set()\\n    total_adjustment = 0.0\\n    \\n    for iteration in range(iterations):\\n        iteration_adjustment = 0.0\\n        \\n        for col in layer0.minicolumns.values():\\n            term = col.content\\n            \\n            if term not in semantic_neighbors:\\n                continue\\n            \\n            tokens_affected.add(term)\\n            \\n            # Get semantic target weights\\n            semantic_targets: Dict[str, float] = {}\\n            for neighbor, weight in semantic_neighbors[term]:\\n                neighbor_col = layer0.get_minicolumn(neighbor)\\n                if neighbor_col:\\n                    semantic_targets[neighbor_col.id] = weight\\n            \\n            if not semantic_targets:\\n                continue\\n            \\n            # Adjust each connection\\n            for target_id in list(col.lateral_connections.keys()):\\n                original = original_weights[term].get(target_id, 0)\\n                semantic = semantic_targets.get(target_id, 0)\\n                \\n                # Blend original and semantic\\n                new_weight = alpha * original + (1 - alpha) * semantic\\n                \\n                if new_weight > 0:\\n                    adjustment = abs(col.lateral_connections[target_id] - new_weight)\\n                    iteration_adjustment += adjustment\\n                    col.lateral_connections[target_id] = new_weight\\n            \\n            # Add new semantic connections\\n            for target_id, semantic_weight in semantic_targets.items():\\n                if target_id not in col.lateral_connections:\\n                    col.lateral_connections[target_id] = (1 - alpha) * semantic_weight\\n                    iteration_adjustment += (1 - alpha) * semantic_weight\\n        \\n        total_adjustment += iteration_adjustment\\n    \\n    return {\\n        'iterations': iterations,\\n        'alpha': alpha,\\n        'tokens_affected': len(tokens_affected),\\n        'total_adjustment': total_adjustment,\\n        'relations_used': len(semantic_relations)\\n    }\\n\\n\\ndef retrofit_embeddings(\\n    embeddings: Dict[str, List[float]],\\n    semantic_relations: List[Tuple[str, str, str, float]],\\n    iterations: int = 10,\\n    alpha: float = 0.4\\n) -> Dict[str, Any]:\\n    \\\"\\\"\\\"\\n    Retrofit embeddings using semantic relations.\\n    \\n    Like Faruqui et al.'s retrofitting, but for graph embeddings.\\n    Pulls semantically related terms closer in embedding space.\\n    \\n    Args:\\n        embeddings: Dictionary mapping terms to embedding vectors\\n        semantic_relations: List of (term1, relation, term2, weight) tuples\\n        iterations: Number of iterations\\n        alpha: Blend factor (higher = more original embedding)\\n\\n    Returns:\\n        Dictionary with retrofitting statistics\\n\\n    Raises:\\n        ValueError: If alpha is not in range (0, 1]\\n    \\\"\\\"\\\"\\n    if not (0 < alpha <= 1):\\n        raise ValueError(f\\\"alpha must be between 0 and 1 (exclusive of 0), got {alpha}\\\")\\n\\n    # Store original embeddings\\n    original = copy.deepcopy(embeddings)\\n    \\n    # Build neighbor lookup\\n    neighbors: Dict[str, List[Tuple[str, float]]] = defaultdict(list)\\n    \\n    for t1, relation, t2, weight in semantic_relations:\\n        if t1 in embeddings and t2 in embeddings:\\n            relation_weight = RELATION_WEIGHTS.get(relation, 0.5)\\n            combined = weight * relation_weight\\n            neighbors[t1].append((t2, combined))\\n            neighbors[t2].append((t1, combined))\\n    \\n    # Iterative retrofitting\\n    total_movement = 0.0\\n    terms_moved = set()\\n    \\n    for iteration in range(iterations):\\n        for term in list(embeddings.keys()):\\n            if term not in neighbors or not neighbors[term]:\\n                continue\\n            \\n            terms_moved.add(term)\\n            vec = embeddings[term]\\n            orig = original[term]\\n            \\n            # Compute semantic center (weighted average of neighbors)\\n            semantic_center = [0.0] * len(vec)\\n            total_weight = 0.0\\n            \\n            for neighbor, weight in neighbors[term]:\\n                if neighbor in embeddings:\\n                    neighbor_vec = embeddings[neighbor]\\n                    for i in range(len(vec)):\\n                        semantic_center[i] += neighbor_vec[i] * weight\\n                    total_weight += weight\\n            \\n            if total_weight > 0:\\n                for i in range(len(semantic_center)):\\n                    semantic_center[i] /= total_weight\\n                \\n                # Blend original with semantic center\\n                new_vec = []\\n                movement = 0.0\\n                \\n                for i in range(len(vec)):\\n                    new_val = alpha * orig[i] + (1 - alpha) * semantic_center[i]\\n                    movement += abs(new_val - vec[i])\\n                    new_vec.append(new_val)\\n                \\n                embeddings[term] = new_vec\\n                total_movement += movement\\n    \\n    return {\\n        'iterations': iterations,\\n        'alpha': alpha,\\n        'terms_retrofitted': len(terms_moved),\\n        'total_movement': total_movement\\n    }\\n\\n\\ndef get_relation_type_weight(relation_type: str) -> float:\\n    \\\"\\\"\\\"\\n    Get the weight for a relation type.\\n\\n    Args:\\n        relation_type: Type of semantic relation\\n\\n    Returns:\\n        Weight multiplier for this relation type\\n    \\\"\\\"\\\"\\n    return RELATION_WEIGHTS.get(relation_type, 0.5)\\n\\n\\ndef build_isa_hierarchy(\\n    semantic_relations: List[Tuple[str, str, str, float]]\\n) -> Tuple[Dict[str, Set[str]], Dict[str, Set[str]]]:\\n    \\\"\\\"\\\"\\n    Build IsA parent-child hierarchy from semantic relations.\\n\\n    Extracts all IsA relations and builds bidirectional parent-child mappings.\\n    For example, if \\\"dog IsA animal\\\", then:\\n    - parents[\\\"dog\\\"] = {\\\"animal\\\"}\\n    - children[\\\"animal\\\"] = {\\\"dog\\\"}\\n\\n    Args:\\n        semantic_relations: List of (term1, relation, term2, weight) tuples\\n\\n    Returns:\\n        Tuple of (parents, children) dicts:\\n        - parents: Maps term to set of parent terms (hypernyms)\\n        - children: Maps term to set of child terms (hyponyms)\\n\\n    Example:\\n        >>> relations = [(\\\"dog\\\", \\\"IsA\\\", \\\"animal\\\", 1.0), (\\\"cat\\\", \\\"IsA\\\", \\\"animal\\\", 1.0)]\\n        >>> parents, children = build_isa_hierarchy(relations)\\n        >>> print(parents[\\\"dog\\\"])  # {\\\"animal\\\"}\\n        >>> print(children[\\\"animal\\\"])  # {\\\"dog\\\", \\\"cat\\\"}\\n    \\\"\\\"\\\"\\n    parents: Dict[str, Set[str]] = defaultdict(set)\\n    children: Dict[str, Set[str]] = defaultdict(set)\\n\\n    for t1, relation, t2, weight in semantic_relations:\\n        if relation == 'IsA':\\n            # t1 IsA t2 means t2 is a parent (hypernym) of t1\\n            parents[t1].add(t2)\\n            children[t2].add(t1)\\n\\n    return dict(parents), dict(children)\\n\\n\\ndef get_ancestors(\\n    term: str,\\n    parents: Dict[str, Set[str]],\\n    max_depth: int = 10\\n) -> Dict[str, int]:\\n    \\\"\\\"\\\"\\n    Get all ancestors of a term with their depth in the hierarchy.\\n\\n    Performs BFS traversal up the IsA hierarchy to find all ancestors.\\n\\n    Args:\\n        term: Starting term\\n        parents: Parent mapping from build_isa_hierarchy()\\n        max_depth: Maximum depth to traverse (prevents infinite loops)\\n\\n    Returns:\\n        Dict mapping ancestor terms to their depth (1 = direct parent, 2 = grandparent, etc.)\\n\\n    Example:\\n        >>> # If dog IsA canine IsA animal\\n        >>> ancestors = get_ancestors(\\\"dog\\\", parents)\\n        >>> # ancestors = {\\\"canine\\\": 1, \\\"animal\\\": 2}\\n    \\\"\\\"\\\"\\n    ancestors: Dict[str, int] = {}\\n    frontier = [(p, 1) for p in parents.get(term, set())]\\n    visited = {term}\\n\\n    while frontier:\\n        current, depth = frontier.pop(0)\\n        if current in visited or depth > max_depth:\\n            continue\\n        visited.add(current)\\n        ancestors[current] = depth\\n\\n        # Add parents of current term\\n        for parent in parents.get(current, set()):\\n            if parent not in visited:\\n                frontier.append((parent, depth + 1))\\n\\n    return ancestors\\n\\n\\ndef get_descendants(\\n    term: str,\\n    children: Dict[str, Set[str]],\\n    max_depth: int = 10\\n) -> Dict[str, int]:\\n    \\\"\\\"\\\"\\n    Get all descendants of a term with their depth in the hierarchy.\\n\\n    Performs BFS traversal down the IsA hierarchy to find all descendants.\\n\\n    Args:\\n        term: Starting term\\n        children: Children mapping from build_isa_hierarchy()\\n        max_depth: Maximum depth to traverse (prevents infinite loops)\\n\\n    Returns:\\n        Dict mapping descendant terms to their depth (1 = direct child, 2 = grandchild, etc.)\\n    \\\"\\\"\\\"\\n    descendants: Dict[str, int] = {}\\n    frontier = [(c, 1) for c in children.get(term, set())]\\n    visited = {term}\\n\\n    while frontier:\\n        current, depth = frontier.pop(0)\\n        if current in visited or depth > max_depth:\\n            continue\\n        visited.add(current)\\n        descendants[current] = depth\\n\\n        # Add children of current term\\n        for child in children.get(current, set()):\\n            if child not in visited:\\n                frontier.append((child, depth + 1))\\n\\n    return descendants\\n\\n\\ndef inherit_properties(\\n    semantic_relations: List[Tuple[str, str, str, float]],\\n    decay_factor: float = 0.7,\\n    max_depth: int = 5\\n) -> Dict[str, Dict[str, Tuple[float, str, int]]]:\\n    \\\"\\\"\\\"\\n    Compute inherited properties for all terms based on IsA hierarchy.\\n\\n    If \\\"dog IsA animal\\\" and \\\"animal HasProperty living\\\", then \\\"dog\\\" inherits\\n    \\\"living\\\" with a decayed weight. Properties propagate down the IsA hierarchy\\n    with weight decaying at each level.\\n\\n    Args:\\n        semantic_relations: List of (term1, relation, term2, weight) tuples\\n        decay_factor: Weight multiplier per inheritance level (default 0.7)\\n        max_depth: Maximum inheritance depth (default 5)\\n\\n    Returns:\\n        Dict mapping terms to their inherited properties:\\n        {\\n            term: {\\n                property: (weight, source_ancestor, depth)\\n            }\\n        }\\n\\n    Example:\\n        >>> relations = [\\n        ...     (\\\"dog\\\", \\\"IsA\\\", \\\"animal\\\", 1.0),\\n        ...     (\\\"animal\\\", \\\"HasProperty\\\", \\\"living\\\", 0.9),\\n        ...     (\\\"animal\\\", \\\"HasProperty\\\", \\\"mortal\\\", 0.8),\\n        ... ]\\n        >>> inherited = inherit_properties(relations)\\n        >>> print(inherited[\\\"dog\\\"])\\n        >>> # {\\\"living\\\": (0.63, \\\"animal\\\", 1), \\\"mortal\\\": (0.56, \\\"animal\\\", 1)}\\n    \\\"\\\"\\\"\\n    # Build hierarchy\\n    parents, children = build_isa_hierarchy(semantic_relations)\\n\\n    # Extract direct properties for each term\\n    # Properties come from HasProperty, HasA, CapableOf, etc.\\n    property_relations = {'HasProperty', 'HasA', 'CapableOf', 'AtLocation', 'UsedFor'}\\n    direct_properties: Dict[str, Dict[str, float]] = defaultdict(dict)\\n\\n    for t1, relation, t2, weight in semantic_relations:\\n        if relation in property_relations:\\n            # t1 HasProperty t2 means t2 is a property of t1\\n            direct_properties[t1][t2] = max(direct_properties[t1].get(t2, 0), weight)\\n\\n    # Compute inherited properties for each term\\n    inherited: Dict[str, Dict[str, Tuple[float, str, int]]] = {}\\n\\n    # Get all terms that have parents (i.e., can inherit)\\n    all_terms = set(parents.keys())\\n    # Also include terms with direct properties (they might be ancestors)\\n    all_terms.update(direct_properties.keys())\\n\\n    for term in all_terms:\\n        term_inherited: Dict[str, Tuple[float, str, int]] = {}\\n\\n        # Get all ancestors and their depths\\n        ancestors = get_ancestors(term, parents, max_depth=max_depth)\\n\\n        # For each ancestor, inherit their properties\\n        for ancestor, depth in ancestors.items():\\n            if ancestor in direct_properties:\\n                # Compute decayed weight\\n                decay = decay_factor ** depth\\n                for prop, prop_weight in direct_properties[ancestor].items():\\n                    inherited_weight = prop_weight * decay\\n\\n                    # Keep the strongest inheritance path\\n                    if prop not in term_inherited or term_inherited[prop][0] < inherited_weight:\\n                        term_inherited[prop] = (inherited_weight, ancestor, depth)\\n\\n        if term_inherited:\\n            inherited[term] = term_inherited\\n\\n    return inherited\\n\\n\\ndef compute_property_similarity(\\n    term1: str,\\n    term2: str,\\n    inherited_properties: Dict[str, Dict[str, Tuple[float, str, int]]],\\n    direct_properties: Optional[Dict[str, Dict[str, float]]] = None\\n) -> float:\\n    \\\"\\\"\\\"\\n    Compute similarity between terms based on shared properties (direct + inherited).\\n\\n    Args:\\n        term1: First term\\n        term2: Second term\\n        inherited_properties: Output from inherit_properties()\\n        direct_properties: Optional dict of direct properties {term: {prop: weight}}\\n\\n    Returns:\\n        Similarity score based on Jaccard-like overlap of properties\\n\\n    Example:\\n        >>> sim = compute_property_similarity(\\\"dog\\\", \\\"cat\\\", inherited, direct)\\n        >>> # Both inherit \\\"living\\\" from \\\"animal\\\", so similarity > 0\\n    \\\"\\\"\\\"\\n    # Get all properties for each term\\n    props1: Dict[str, float] = {}\\n    props2: Dict[str, float] = {}\\n\\n    # Add inherited properties\\n    if term1 in inherited_properties:\\n        for prop, (weight, _, _) in inherited_properties[term1].items():\\n            props1[prop] = max(props1.get(prop, 0), weight)\\n\\n    if term2 in inherited_properties:\\n        for prop, (weight, _, _) in inherited_properties[term2].items():\\n            props2[prop] = max(props2.get(prop, 0), weight)\\n\\n    # Add direct properties if provided\\n    if direct_properties:\\n        if term1 in direct_properties:\\n            for prop, weight in direct_properties[term1].items():\\n                props1[prop] = max(props1.get(prop, 0), weight)\\n        if term2 in direct_properties:\\n            for prop, weight in direct_properties[term2].items():\\n                props2[prop] = max(props2.get(prop, 0), weight)\\n\\n    if not props1 or not props2:\\n        return 0.0\\n\\n    # Compute weighted Jaccard similarity\\n    common_props = set(props1.keys()) & set(props2.keys())\\n    all_props = set(props1.keys()) | set(props2.keys())\\n\\n    if not all_props:\\n        return 0.0\\n\\n    # Sum of minimum weights for common properties\\n    intersection_weight = sum(\\n        min(props1[p], props2[p]) for p in common_props\\n    )\\n\\n    # Sum of maximum weights for all properties\\n    union_weight = sum(\\n        max(props1.get(p, 0), props2.get(p, 0)) for p in all_props\\n    )\\n\\n    return intersection_weight / union_weight if union_weight > 0 else 0.0\\n\\n\\ndef apply_inheritance_to_connections(\\n    layers: Dict[CorticalLayer, HierarchicalLayer],\\n    inherited_properties: Dict[str, Dict[str, Tuple[float, str, int]]],\\n    boost_factor: float = 0.3\\n) -> Dict[str, Any]:\\n    \\\"\\\"\\\"\\n    Boost lateral connections between terms that share inherited properties.\\n\\n    Terms that share properties through inheritance should have stronger\\n    connections, even if they don't directly co-occur.\\n\\n    Args:\\n        layers: Dictionary of layers\\n        inherited_properties: Output from inherit_properties()\\n        boost_factor: Weight boost for shared properties (default 0.3)\\n\\n    Returns:\\n        Statistics about connections boosted\\n\\n    Example:\\n        >>> # \\\"dog\\\" and \\\"cat\\\" both inherit \\\"living\\\" from \\\"animal\\\"\\n        >>> # Their lateral connection gets boosted\\n        >>> stats = apply_inheritance_to_connections(layers, inherited)\\n    \\\"\\\"\\\"\\n    layer0 = layers[CorticalLayer.TOKENS]\\n    connections_boosted = 0\\n    total_boost = 0.0\\n\\n    # Get terms that have inherited properties\\n    terms_with_inheritance = set(inherited_properties.keys())\\n\\n    # For each pair of terms with inherited properties\\n    terms_list = list(terms_with_inheritance)\\n\\n    for i, term1 in enumerate(terms_list):\\n        col1 = layer0.get_minicolumn(term1)\\n        if not col1:\\n            continue\\n\\n        props1 = inherited_properties[term1]\\n\\n        for term2 in terms_list[i + 1:]:\\n            col2 = layer0.get_minicolumn(term2)\\n            if not col2:\\n                continue\\n\\n            props2 = inherited_properties[term2]\\n\\n            # Find shared inherited properties\\n            shared_props = set(props1.keys()) & set(props2.keys())\\n            if not shared_props:\\n                continue\\n\\n            # Compute boost based on shared properties\\n            boost = 0.0\\n            for prop in shared_props:\\n                w1, _, _ = props1[prop]\\n                w2, _, _ = props2[prop]\\n                # Average of the two inheritance weights\\n                boost += (w1 + w2) / 2 * boost_factor\\n\\n            if boost > 0:\\n                # Add boost to lateral connections\\n                col1.add_lateral_connection(col2.id, boost)\\n                col2.add_lateral_connection(col1.id, boost)\\n                connections_boosted += 1\\n                total_boost += boost\\n\\n    return {\\n        'connections_boosted': connections_boosted,\\n        'total_boost': total_boost,\\n        'terms_with_inheritance': len(terms_with_inheritance)\\n    }\\n\",",
        "      \"mtime\": 1765392877.0",
        "    },",
        "    {",
        "      \"op\": \"add\",",
        "      \"doc_id\": \"tests/test_analysis.py\",",
        "      \"content\": \"\\\"\\\"\\\"Tests for the analysis module.\\\"\\\"\\\"\\n\\nimport unittest\\nimport math\\nimport sys\\nsys.path.insert(0, '..')\\n\\nfrom cortical import CorticalTextProcessor, CorticalLayer, HierarchicalLayer\\nfrom cortical.analysis import (\\n    compute_pagerank,\\n    compute_tfidf,\\n    propagate_activation,\\n    cluster_by_label_propagation,\\n    build_concept_clusters,\\n    compute_document_connections,\\n    cosine_similarity\\n)\\n\\n\\nclass TestPageRank(unittest.TestCase):\\n    \\\"\\\"\\\"Test PageRank computation.\\\"\\\"\\\"\\n\\n    def test_pagerank_empty_layer(self):\\n        \\\"\\\"\\\"Test PageRank on empty layer.\\\"\\\"\\\"\\n        layer = HierarchicalLayer(CorticalLayer.TOKENS)\\n        result = compute_pagerank(layer)\\n        self.assertEqual(result, {})\\n\\n    def test_pagerank_single_node(self):\\n        \\\"\\\"\\\"Test PageRank with single node.\\\"\\\"\\\"\\n        layer = HierarchicalLayer(CorticalLayer.TOKENS)\\n        layer.get_or_create_minicolumn(\\\"test\\\")\\n        result = compute_pagerank(layer)\\n        self.assertEqual(len(result), 1)\\n        # With damping 0.85, single node gets (1-0.85)/1 = 0.15\\n        self.assertAlmostEqual(list(result.values())[0], 0.15, places=5)\\n\\n    def test_pagerank_multiple_nodes(self):\\n        \\\"\\\"\\\"Test PageRank with multiple connected nodes.\\\"\\\"\\\"\\n        processor = CorticalTextProcessor()\\n        processor.process_document(\\\"doc1\\\", \\\"neural networks learning deep\\\")\\n        processor.process_document(\\\"doc2\\\", \\\"neural learning patterns data\\\")\\n\\n        layer0 = processor.get_layer(CorticalLayer.TOKENS)\\n        result = compute_pagerank(layer0)\\n\\n        # All nodes should have positive PageRank\\n        for col in layer0.minicolumns.values():\\n            self.assertGreater(col.pagerank, 0)\\n\\n    def test_pagerank_convergence(self):\\n        \\\"\\\"\\\"Test that PageRank converges.\\\"\\\"\\\"\\n        processor = CorticalTextProcessor()\\n        processor.process_document(\\\"doc1\\\", \\\"word1 word2 word3 word4\\\")\\n\\n        layer0 = processor.get_layer(CorticalLayer.TOKENS)\\n        result = compute_pagerank(layer0, iterations=100)\\n\\n        # Sum should be approximately 1.0\\n        total = sum(result.values())\\n        self.assertAlmostEqual(total, 1.0, places=3)\\n\\n\\nclass TestTFIDF(unittest.TestCase):\\n    \\\"\\\"\\\"Test TF-IDF computation.\\\"\\\"\\\"\\n\\n    def test_tfidf_empty_corpus(self):\\n        \\\"\\\"\\\"Test TF-IDF on empty corpus.\\\"\\\"\\\"\\n        processor = CorticalTextProcessor()\\n        compute_tfidf(processor.layers, processor.documents)\\n        # Should not raise\\n\\n    def test_tfidf_single_document(self):\\n        \\\"\\\"\\\"Test TF-IDF with single document.\\\"\\\"\\\"\\n        processor = CorticalTextProcessor()\\n        processor.process_document(\\\"doc1\\\", \\\"word1 word2 word3\\\")\\n        compute_tfidf(processor.layers, processor.documents)\\n\\n        layer0 = processor.get_layer(CorticalLayer.TOKENS)\\n        # With single doc, IDF = log(1/1) = 0, so TF-IDF = 0\\n        for col in layer0.minicolumns.values():\\n            self.assertEqual(col.tfidf, 0.0)\\n\\n    def test_tfidf_multiple_documents(self):\\n        \\\"\\\"\\\"Test TF-IDF with multiple documents.\\\"\\\"\\\"\\n        processor = CorticalTextProcessor()\\n        processor.process_document(\\\"doc1\\\", \\\"neural networks learning\\\")\\n        processor.process_document(\\\"doc2\\\", \\\"machine learning algorithms\\\")\\n        processor.process_document(\\\"doc3\\\", \\\"database systems storage\\\")\\n        compute_tfidf(processor.layers, processor.documents)\\n\\n        layer0 = processor.get_layer(CorticalLayer.TOKENS)\\n\\n        # Terms unique to one doc should have higher TF-IDF\\n        unique_term = layer0.get_minicolumn(\\\"database\\\")\\n        common_term = layer0.get_minicolumn(\\\"learning\\\")\\n\\n        if unique_term and common_term:\\n            # database appears in 1 doc, learning in 2\\n            self.assertGreater(unique_term.tfidf, 0)\\n\\n    def test_tfidf_per_document(self):\\n        \\\"\\\"\\\"Test per-document TF-IDF.\\\"\\\"\\\"\\n        processor = CorticalTextProcessor()\\n        processor.process_document(\\\"doc1\\\", \\\"neural neural neural\\\")  # 3 occurrences\\n        processor.process_document(\\\"doc2\\\", \\\"neural learning\\\")  # 1 occurrence\\n        processor.process_document(\\\"doc3\\\", \\\"different content here\\\")  # No neural - needed for IDF > 0\\n        compute_tfidf(processor.layers, processor.documents)\\n\\n        layer0 = processor.get_layer(CorticalLayer.TOKENS)\\n        neural = layer0.get_minicolumn(\\\"neural\\\")\\n\\n        # Check per-document TF-IDF uses actual occurrence counts\\n        self.assertIn(\\\"doc1\\\", neural.tfidf_per_doc)\\n        self.assertIn(\\\"doc2\\\", neural.tfidf_per_doc)\\n        # doc1 has 3 occurrences, doc2 has 1\\n        # log1p(3) > log1p(1), so doc1 should have higher per-doc TF-IDF\\n        self.assertGreater(neural.tfidf_per_doc[\\\"doc1\\\"], neural.tfidf_per_doc[\\\"doc2\\\"])\\n\\n\\nclass TestActivationPropagation(unittest.TestCase):\\n    \\\"\\\"\\\"Test activation propagation.\\\"\\\"\\\"\\n\\n    def test_propagation_empty_layers(self):\\n        \\\"\\\"\\\"Test propagation on empty layers.\\\"\\\"\\\"\\n        processor = CorticalTextProcessor()\\n        propagate_activation(processor.layers)\\n        # Should not raise\\n\\n    def test_propagation_preserves_activation(self):\\n        \\\"\\\"\\\"Test that propagation modifies activations.\\\"\\\"\\\"\\n        processor = CorticalTextProcessor()\\n        processor.process_document(\\\"doc1\\\", \\\"neural networks learning\\\")\\n\\n        layer0 = processor.get_layer(CorticalLayer.TOKENS)\\n        initial_activations = {col.content: col.activation for col in layer0}\\n\\n        propagate_activation(processor.layers, iterations=3)\\n\\n        # Activations should have changed\\n        for col in layer0.minicolumns.values():\\n            # With decay, activation should decrease or stay same\\n            self.assertGreaterEqual(col.activation, 0)\\n\\n\\nclass TestLabelPropagation(unittest.TestCase):\\n    \\\"\\\"\\\"Test label propagation clustering.\\\"\\\"\\\"\\n\\n    def test_clustering_empty_layer(self):\\n        \\\"\\\"\\\"Test clustering on empty layer.\\\"\\\"\\\"\\n        layer = HierarchicalLayer(CorticalLayer.TOKENS)\\n        clusters = cluster_by_label_propagation(layer)\\n        self.assertEqual(clusters, {})\\n\\n    def test_clustering_returns_dict(self):\\n        \\\"\\\"\\\"Test that clustering returns dictionary.\\\"\\\"\\\"\\n        processor = CorticalTextProcessor()\\n        processor.process_document(\\\"doc1\\\", \\\"neural networks learning deep patterns\\\")\\n        processor.process_document(\\\"doc2\\\", \\\"neural learning patterns data\\\")\\n\\n        layer0 = processor.get_layer(CorticalLayer.TOKENS)\\n        clusters = cluster_by_label_propagation(layer0, min_cluster_size=2)\\n\\n        self.assertIsInstance(clusters, dict)\\n\\n    def test_clustering_min_size(self):\\n        \\\"\\\"\\\"Test that clusters respect minimum size.\\\"\\\"\\\"\\n        processor = CorticalTextProcessor()\\n        processor.process_document(\\\"doc1\\\", \\\"neural networks learning deep patterns\\\")\\n        processor.process_document(\\\"doc2\\\", \\\"neural learning patterns data\\\")\\n\\n        layer0 = processor.get_layer(CorticalLayer.TOKENS)\\n        clusters = cluster_by_label_propagation(layer0, min_cluster_size=3)\\n\\n        for members in clusters.values():\\n            self.assertGreaterEqual(len(members), 3)\\n\\n\\nclass TestConceptClusters(unittest.TestCase):\\n    \\\"\\\"\\\"Test concept cluster building.\\\"\\\"\\\"\\n\\n    def test_build_concept_clusters(self):\\n        \\\"\\\"\\\"Test building concept layer from clusters.\\\"\\\"\\\"\\n        processor = CorticalTextProcessor()\\n        processor.process_document(\\\"doc1\\\", \\\"neural networks learning deep\\\")\\n        processor.process_document(\\\"doc2\\\", \\\"neural learning patterns data\\\")\\n        processor.compute_importance(verbose=False)\\n\\n        layer0 = processor.get_layer(CorticalLayer.TOKENS)\\n        clusters = cluster_by_label_propagation(layer0, min_cluster_size=2)\\n        build_concept_clusters(processor.layers, clusters)\\n\\n        layer2 = processor.get_layer(CorticalLayer.CONCEPTS)\\n        # May or may not have concepts depending on cluster size\\n        self.assertIsInstance(layer2.minicolumns, dict)\\n\\n\\nclass TestDocumentConnections(unittest.TestCase):\\n    \\\"\\\"\\\"Test document connection computation.\\\"\\\"\\\"\\n\\n    def test_document_connections(self):\\n        \\\"\\\"\\\"Test building document connections.\\\"\\\"\\\"\\n        processor = CorticalTextProcessor()\\n        processor.process_document(\\\"doc1\\\", \\\"neural networks learning deep patterns\\\")\\n        processor.process_document(\\\"doc2\\\", \\\"neural learning patterns data\\\")\\n        processor.process_document(\\\"doc3\\\", \\\"completely different content here\\\")\\n        processor.compute_tfidf(verbose=False)\\n\\n        compute_document_connections(processor.layers, processor.documents, min_shared_terms=2)\\n\\n        layer3 = processor.get_layer(CorticalLayer.DOCUMENTS)\\n        doc1 = layer3.get_minicolumn(\\\"doc1\\\")\\n        doc2 = layer3.get_minicolumn(\\\"doc2\\\")\\n\\n        # doc1 and doc2 share terms, should be connected\\n        if doc1 and doc2:\\n            # Check if they have connections\\n            has_connection = len(doc1.lateral_connections) > 0 or len(doc2.lateral_connections) > 0\\n            self.assertTrue(has_connection)\\n\\n\\nclass TestCosineSimilarity(unittest.TestCase):\\n    \\\"\\\"\\\"Test cosine similarity function.\\\"\\\"\\\"\\n\\n    def test_cosine_identical_vectors(self):\\n        \\\"\\\"\\\"Test cosine similarity of identical vectors.\\\"\\\"\\\"\\n        vec = {'a': 1.0, 'b': 2.0, 'c': 3.0}\\n        sim = cosine_similarity(vec, vec)\\n        self.assertAlmostEqual(sim, 1.0, places=5)\\n\\n    def test_cosine_orthogonal_vectors(self):\\n        \\\"\\\"\\\"Test cosine similarity of non-overlapping vectors.\\\"\\\"\\\"\\n        vec1 = {'a': 1.0, 'b': 2.0}\\n        vec2 = {'c': 3.0, 'd': 4.0}\\n        sim = cosine_similarity(vec1, vec2)\\n        self.assertEqual(sim, 0.0)\\n\\n    def test_cosine_empty_vectors(self):\\n        \\\"\\\"\\\"Test cosine similarity with empty vectors.\\\"\\\"\\\"\\n        sim = cosine_similarity({}, {})\\n        self.assertEqual(sim, 0.0)\\n\\n    def test_cosine_partial_overlap(self):\\n        \\\"\\\"\\\"Test cosine similarity with partial overlap.\\\"\\\"\\\"\\n        vec1 = {'a': 1.0, 'b': 2.0, 'c': 3.0}\\n        vec2 = {'b': 2.0, 'c': 3.0, 'd': 4.0}\\n        sim = cosine_similarity(vec1, vec2)\\n        self.assertGreater(sim, 0.0)\\n        self.assertLess(sim, 1.0)\\n\\n    def test_cosine_zero_magnitude(self):\\n        \\\"\\\"\\\"Test cosine similarity with zero magnitude vector.\\\"\\\"\\\"\\n        vec1 = {'a': 0.0}\\n        vec2 = {'a': 1.0}\\n        sim = cosine_similarity(vec1, vec2)\\n        self.assertEqual(sim, 0.0)\\n\\n\\nclass TestGetByIdOptimization(unittest.TestCase):\\n    \\\"\\\"\\\"Test that get_by_id optimization works correctly.\\\"\\\"\\\"\\n\\n    def test_get_by_id_returns_correct_minicolumn(self):\\n        \\\"\\\"\\\"Test that get_by_id returns the correct minicolumn.\\\"\\\"\\\"\\n        layer = HierarchicalLayer(CorticalLayer.TOKENS)\\n        col1 = layer.get_or_create_minicolumn(\\\"neural\\\")\\n        col2 = layer.get_or_create_minicolumn(\\\"network\\\")\\n\\n        # Get by ID should return the same minicolumn\\n        retrieved = layer.get_by_id(col1.id)\\n        self.assertIs(retrieved, col1)\\n\\n        retrieved2 = layer.get_by_id(col2.id)\\n        self.assertIs(retrieved2, col2)\\n\\n    def test_get_by_id_returns_none_for_missing(self):\\n        \\\"\\\"\\\"Test that get_by_id returns None for missing ID.\\\"\\\"\\\"\\n        layer = HierarchicalLayer(CorticalLayer.TOKENS)\\n        layer.get_or_create_minicolumn(\\\"test\\\")\\n\\n        result = layer.get_by_id(\\\"nonexistent_id\\\")\\n        self.assertIsNone(result)\\n\\n\\nclass TestParameterValidation(unittest.TestCase):\\n    \\\"\\\"\\\"Test parameter validation in analysis functions.\\\"\\\"\\\"\\n\\n    def test_pagerank_invalid_damping_zero(self):\\n        \\\"\\\"\\\"Test PageRank rejects damping=0.\\\"\\\"\\\"\\n        layer = HierarchicalLayer(CorticalLayer.TOKENS)\\n        layer.get_or_create_minicolumn(\\\"test\\\")\\n        with self.assertRaises(ValueError) as ctx:\\n            compute_pagerank(layer, damping=0)\\n        self.assertIn(\\\"damping\\\", str(ctx.exception))\\n\\n    def test_pagerank_invalid_damping_one(self):\\n        \\\"\\\"\\\"Test PageRank rejects damping=1.\\\"\\\"\\\"\\n        layer = HierarchicalLayer(CorticalLayer.TOKENS)\\n        layer.get_or_create_minicolumn(\\\"test\\\")\\n        with self.assertRaises(ValueError) as ctx:\\n            compute_pagerank(layer, damping=1.0)\\n        self.assertIn(\\\"damping\\\", str(ctx.exception))\\n\\n    def test_pagerank_invalid_damping_negative(self):\\n        \\\"\\\"\\\"Test PageRank rejects negative damping.\\\"\\\"\\\"\\n        layer = HierarchicalLayer(CorticalLayer.TOKENS)\\n        layer.get_or_create_minicolumn(\\\"test\\\")\\n        with self.assertRaises(ValueError) as ctx:\\n            compute_pagerank(layer, damping=-0.5)\\n        self.assertIn(\\\"damping\\\", str(ctx.exception))\\n\\n    def test_pagerank_invalid_damping_greater_than_one(self):\\n        \\\"\\\"\\\"Test PageRank rejects damping > 1.\\\"\\\"\\\"\\n        layer = HierarchicalLayer(CorticalLayer.TOKENS)\\n        layer.get_or_create_minicolumn(\\\"test\\\")\\n        with self.assertRaises(ValueError) as ctx:\\n            compute_pagerank(layer, damping=1.5)\\n        self.assertIn(\\\"damping\\\", str(ctx.exception))\\n\\n    def test_pagerank_valid_damping(self):\\n        \\\"\\\"\\\"Test PageRank accepts valid damping values.\\\"\\\"\\\"\\n        layer = HierarchicalLayer(CorticalLayer.TOKENS)\\n        layer.get_or_create_minicolumn(\\\"test\\\")\\n        # Should not raise\\n        result = compute_pagerank(layer, damping=0.85)\\n        self.assertIsInstance(result, dict)\\n\\n\\nif __name__ == \\\"__main__\\\":\\n    unittest.main(verbosity=2)\\n\",",
        "      \"mtime\": 1765392877.0",
        "    },",
        "    {",
        "      \"op\": \"add\",",
        "      \"doc_id\": \"docs/claude-usage.md\",",
        "      \"content\": \"# Claude Usage Guide: Semantic Search System\\n\\nThis guide is written specifically for Claude (AI agents) to understand how to effectively use the Cortical Text Processor's semantic search system when working with this codebase.\\n\\n## Overview\\n\\nThe Cortical Text Processor can index and semantically search its own codebase, providing meaning-based retrieval instead of simple keyword matching. This guide explains how to use this capability strategically during development tasks.\\n\\n**Key principle:** The system finds code by understanding intent and concepts, not just exact keywords. \\\"Fetch\\\", \\\"get\\\", \\\"load\\\", and \\\"retrieve\\\" are treated as semantically similar.\\n\\n---\\n\\n## Table of Contents\\n\\n1. [When to Use Codebase-Search](#when-to-use-codebase-search)\\n2. [When to Use Direct File Reading](#when-to-use-direct-file-reading)\\n3. [Formulating Effective Search Queries](#formulating-effective-search-queries)\\n4. [Understanding Search Results](#understanding-search-results)\\n5. [When to Re-Index](#when-to-re-index)\\n6. [Handling No Results](#handling-no-results)\\n7. [Iterative Search Strategy](#iterative-search-strategy)\\n8. [Query Expansion Leverage](#query-expansion-leverage)\\n9. [System Limitations and Workarounds](#system-limitations-and-workarounds)\\n10. [Common Code Query Patterns](#common-code-query-patterns)\\n11. [Performance Considerations](#performance-considerations)\\n\\n---\\n\\n## When to Use Codebase-Search\\n\\nUse the **codebase-search** skill when you need to:\\n\\n### 1. Find implementations of concepts\\n```\\n\\\"How does PageRank algorithm work?\\\"\\n\\\"How is TF-IDF computed?\\\"\\n\\\"How are bigrams created?\\\"\\n```\\n\\nThe system will find relevant code passages even if your exact words don't match the implementation. For example, searching for \\\"importance scoring\\\" will find PageRank code.\\n\\n### 2. Locate functionality by intent\\n```\\n\\\"Where do we handle errors?\\\"\\n\\\"Where do we validate input?\\\"\\n\\\"Where do we tokenize text?\\\"\\n```\\n\\nIntent-based queries parse the natural language structure and find code implementing that action.\\n\\n### 3. Understand relationships between components\\n```\\n\\\"What connects to the tokenizer?\\\"\\n\\\"How do layers interact?\\\"\\n\\\"What uses layer 2 concepts?\\\"\\n```\\n\\nThe system understands component relationships through graph connections.\\n\\n### 4. Explore semantic concepts across the codebase\\n```\\n\\\"Neural network terminology\\\"\\n\\\"Graph algorithms\\\"\\n\\\"Performance optimization patterns\\\"\\n```\\n\\nQuery expansion automatically includes related terms, finding all discussions of a concept.\\n\\n### 5. When you need to understand code context\\nYou want to see how something is actually implemented, not just read the file directly. The search system gives you relevant passages in context.\\n\\n**Cost consideration:** Search is fast (~1 second for typical queries), so it's efficient for exploratory research.\\n\\n---\\n\\n## When to Use Direct File Reading\\n\\nUse **direct file reading** (Read tool) when you:\\n\\n### 1. Know the exact file location\\nIf you already know the file path (e.g., `cortical/processor.py`), reading directly is faster than searching.\\n\\n### 2. Need the complete file context\\nWhen you need to see the entire file structure, imports, and all methods in a class, reading the file is more efficient than multiple targeted searches.\\n\\n### 3. Are implementing a pattern you've already found\\nAfter a search tells you the file location, switch to direct reading to implement your changes.\\n\\n### 4. Need accurate line numbers for edits\\nWhile search provides file:line references, reading the file confirms the exact content at those lines.\\n\\n### 5. The concept is very common\\nIf the concept appears frequently (like \\\"process\\\" or \\\"handle\\\"), search may return many results. Direct reading is faster when you know where to look.\\n\\n**Workflow:** Search → Find file → Read file → Implement\\n\\n---\\n\\n## Formulating Effective Search Queries\\n\\n### Query Structure\\n\\nThe system parses queries into three components:\\n\\n1. **Question word** (optional): \\\"where\\\", \\\"how\\\", \\\"what\\\", \\\"why\\\" → affects intent\\n2. **Action verb** (optional): \\\"handle\\\", \\\"process\\\", \\\"create\\\", \\\"validate\\\" → narrows scope\\n3. **Subject**: The main concept you're searching for\\n\\nExamples:\\n- \\\"where do we validate input?\\\" → Intent: location, Action: validate, Subject: input\\n- \\\"how are bigrams created?\\\" → Intent: implementation, Action: create, Subject: bigrams\\n- \\\"PageRank algorithm\\\" → Intent: general, Subject: PageRank algorithm\\n\\n### Writing Effective Queries\\n\\n**✓ DO:** Use natural language as you would ask a colleague\\n```\\n\\\"How does query expansion find related terms?\\\"\\n\\\"Where do we compute document relevance?\\\"\\n\\\"What's the structure of a minicolumn?\\\"\\n```\\n\\n**✓ DO:** Include multiple related terms\\n```\\n\\\"PageRank importance scoring algorithm\\\" (better than just \\\"PageRank\\\")\\n\\\"TF-IDF term weighting relevance\\\" (better than just \\\"TF-IDF\\\")\\n```\\n\\n**✓ DO:** Use intent words\\n```\\n\\\"Find implementations of label propagation\\\"\\n\\\"Locate the tokenizer code\\\"\\n\\\"Show me how errors are handled\\\"\\n```\\n\\n**✗ DON'T:** Use only exact technical names without context\\n```\\n\\\"L0\\\" (too abstract - use \\\"token layer\\\" instead)\\n\\\"col\\\" (use \\\"minicolumn\\\" or \\\"column\\\")\\n```\\n\\n**✗ DON'T:** Use implementation details you're not sure about\\n```\\n\\\"Use lateral_connections\\\" (search for the concept instead: \\\"related terms\\\")\\n\\\"_id_index lookup\\\" (search for: \\\"ID lookup performance\\\")\\n```\\n\\n**✗ DON'T:** Search for very common words alone\\n```\\n\\\"the\\\" or \\\"and\\\" (these appear everywhere)\\n\\\"layer\\\" (almost every file mentions layers - add context: \\\"layer connections\\\")\\n```\\n\\n### Query Length\\n\\n- **Short queries (1-3 words):** Fast, but may return many results\\n  - Good for: \\\"PageRank\\\", \\\"stemming\\\", \\\"TF-IDF\\\"\\n  - Problem: High recall, may need filtering\\n\\n- **Medium queries (4-6 words):** Optimal for most cases\\n  - Good for: \\\"how bigrams are created\\\", \\\"Layer 0 token structure\\\"\\n  - Sweet spot for precision and recall\\n\\n- **Long queries (7+ words):** Very specific, low recall\\n  - Good for: Complete question phrases\\n  - Problem: May miss results if wording doesn't match docs\\n\\n**Best practice:** Start with 4-5 word queries; adjust based on results.\\n\\n---\\n\\n## Understanding Search Results\\n\\n### Result Format\\n\\nEach result shows:\\n\\n```\\n[N] cortical/processor.py:1265\\n    Score: 0.847\\n  - Passage text showing relevant code\\n  - Up to 5 lines displayed by default\\n```\\n\\n### Score Interpretation\\n\\nScores range from 0.0 to 1.0:\\n\\n| Score | Meaning | What to do |\\n|-------|---------|-----------|\\n| 0.9-1.0 | Excellent match | This is what you're looking for |\\n| 0.75-0.89 | Strong match | Very relevant, likely useful |\\n| 0.6-0.74 | Good match | Relevant but may need context |\\n| 0.45-0.59 | Weak match | May be tangentially related |\\n| <0.45 | Poor match | Likely noise, but sometimes useful |\\n\\n**Note:** Scores depend on query quality and corpus structure. A 0.75 for a common topic may be more relevant than a 0.95 for a niche query.\\n\\n### File:Line References\\n\\nThe format `filename:linenumber` tells you:\\n- Which file to examine\\n- Approximately where to look (line number may be off by ±10 lines due to chunking)\\n\\n**Action:** When you get a file:line reference:\\n1. Use Read tool on that file\\n2. Look around the suggested line (±5 lines on each side)\\n3. If not found, search again with different terms\\n\\n### Passage Text\\n\\nThe system shows relevant passages of code in context:\\n\\n- **In brief mode** (default): First 5 lines of the passage\\n- **In verbose mode** (`--verbose` flag): Up to 10 lines\\n\\n**Interpreting passages:**\\n- Look for function definitions, class declarations, and key logic\\n- Passages may be partial—read the full file for complete understanding\\n- Comments in passages are usually significant (the system ranks them highly)\\n\\n---\\n\\n## When to Re-Index\\n\\nThe semantic search uses a pre-built index (`corpus_dev.pkl`) created from your codebase. It's not real-time—it reflects the state when the index was last built.\\n\\n### Use corpus-indexer After:\\n\\n**1. You make code changes** (Most important)\\n```\\n- Add a new function\\n- Modify algorithm logic\\n- Change class structure\\n- Add new documentation\\n```\\n\\n**When:** Use `--incremental` flag for speed (1-2 seconds vs 2-3 seconds for full rebuild)\\n```python\\n# In your task: \\\"Use corpus-indexer with --incremental flag\\\"\\n```\\n\\n**2. You add new files**\\nThe indexer automatically detects new files in `cortical/`, `tests/`, and `docs/`.\\n\\n**When:** After adding `new_feature.py` or `test_new_feature.py`\\n\\n**3. Major refactoring**\\nIf you restructure multiple files, use `--force` flag to ensure clean rebuild.\\n\\n### When Index Staleness Matters\\n\\nSearch results won't reflect changes until re-indexing. This is fine for:\\n- Reading old code\\n- Understanding historical implementation\\n- Learning the architecture\\n\\nThis is problematic for:\\n- Verifying your own changes are searchable\\n- Finding newly added functionality\\n- Debugging code you just wrote\\n\\n### Index Staleness Detection\\n\\nBefore using search, check if the index is stale:\\n\\n```bash\\n# Check what would change\\npython scripts/index_codebase.py --status\\n```\\n\\nIf files changed since last index, results may be out of date.\\n\\n---\\n\\n## Handling No Results\\n\\nWhen a search returns no results, try these strategies in order:\\n\\n### Strategy 1: Broaden Your Query\\n\\n**Narrow query with no results:**\\n```\\n\\\"compute_semantic_pagerank with damping factor\\\"\\n```\\n\\n**Broadened version:**\\n```\\n\\\"PageRank algorithm\\\"\\n```\\n\\n**Action:** Remove specific implementation details and search for the concept.\\n\\n### Strategy 2: Use Synonym/Related Terms\\n\\n**Query with no results:**\\n```\\n\\\"fetch documents from corpus\\\"\\n```\\n\\n**Synonym version:**\\n```\\n\\\"retrieve documents relevance\\\"\\n```\\n\\n**Action:** Replace implementation-specific words with general synonyms.\\n\\n### Strategy 3: Search Different Layers\\n\\n**Technical terms not found:**\\n```\\n\\\"minicolumn lateral connection weight\\\"\\n```\\n\\n**Higher-level concept:**\\n```\\n\\\"related terms word associations\\\"\\n```\\n\\n**Action:** Describe the concept instead of the implementation.\\n\\n### Strategy 4: Check if Index Exists\\n\\n**Problem:** \\\"Error: Corpus file not found\\\"\\n\\n**Solution:**\\n```bash\\npython scripts/index_codebase.py\\n```\\n\\nThis creates `corpus_dev.pkl` (~2-3 seconds).\\n\\n### Strategy 5: Use Direct File Search\\n\\nIf semantic search fails, fall back to:\\n\\n1. **Grep search** for exact keywords:\\n   ```\\n   grep -r \\\"function_name\\\" cortical/\\n   ```\\n\\n2. **Direct file reading** if you know the likely file:\\n   ```\\n   Read cortical/analysis.py\\n   ```\\n\\n### Strategy 6: Check Query Expansion\\n\\nUse `--expand` flag to see what the system is actually searching for:\\n\\n```bash\\npython scripts/search_codebase.py \\\"your query\\\" --expand\\n```\\n\\nThis shows the expanded terms. If expansion is incorrect, try a different query.\\n\\n### Why No Results Happen\\n\\n1. **Concept doesn't exist in codebase** - You're asking for something that isn't implemented\\n2. **Different terminology** - The codebase uses different words than you're using\\n3. **Index is stale** - Recent changes haven't been indexed\\n4. **Query too specific** - You're combining terms that don't co-occur\\n5. **Implementation detail** - You're searching for internal variable names instead of the concept\\n\\n---\\n\\n## Iterative Search Strategy\\n\\nWhen researching a complex topic, use iterative searching:\\n\\n### Iteration 1: Broad Exploration\\n```\\nQuery: \\\"PageRank\\\"\\nGoal: Find where PageRank is implemented\\nAction: Choose the most relevant result file\\nResult: cortical/analysis.py:22\\n```\\n\\n### Iteration 2: Find Related Components\\n```\\nQuery: \\\"how does PageRank use connections\\\"\\nGoal: Understand what PageRank operates on\\nAction: Search results show \\\"lateral connections\\\" and \\\"weighted edges\\\"\\nResult: Learn that PageRank uses graph structure\\n```\\n\\n### Iteration 3: Understand Integration\\n```\\nQuery: \\\"where is PageRank computed in processor\\\"\\nGoal: Find where PageRank is called\\nAction: Results show processor.py lines that trigger compute_pagerank\\nResult: Understand when PageRank runs (after corpus changes)\\n```\\n\\n### Iteration 4: Deep Dive\\n```\\nQuery: \\\"PageRank damping factor convergence\\\"\\nGoal: Understand algorithm parameters\\nAction: Read the full analysis.py function\\nResult: Understand implementation details\\n```\\n\\n**Pattern:** Start broad → narrow down → deepen understanding → read full files\\n\\n---\\n\\n## Query Expansion Leverage\\n\\nThe system automatically expands queries using:\\n\\n1. **Lateral connections** - Terms frequently appearing together\\n2. **Concept clusters** - Semantic groupings\\n3. **Word variants** - Plurals, stems, related forms\\n4. **Code concepts** - Programming synonyms (get/fetch/load)\\n\\n### How to Leverage Expansion\\n\\n**1. Use umbrella terms**\\n\\nRather than searching for specific functions:\\n```\\n# Instead of: \\\"expand_query\\\"\\n# Search for: \\\"query expansion\\\"\\n```\\n\\nThe system will automatically find `expand_query`, `get_expanded_query_terms`, etc.\\n\\n**2. Use related terminology**\\n\\nExpansion finds connections:\\n```\\n\\\"authentication\\\" → also finds \\\"login\\\", \\\"credential\\\", \\\"token\\\", \\\"session\\\"\\n\\\"fetch\\\" → also finds \\\"get\\\", \\\"load\\\", \\\"retrieve\\\", \\\"access\\\"\\n```\\n\\n**3. Check what's actually being searched**\\n\\nUse `--expand` flag:\\n```bash\\npython scripts/search_codebase.py \\\"PageRank\\\" --expand\\n```\\n\\nOutput shows:\\n```\\npagerank: 1.000\\nimportance: 0.847\\nscore: 0.812\\nrank: 0.791\\n...\\n```\\n\\nThese are the actual terms being searched.\\n\\n**4. Add expansion hints to queries**\\n\\nIf expansion misses terms, add them explicitly:\\n```\\n# Instead of: \\\"PageRank\\\"\\n# Try: \\\"PageRank importance scoring algorithm\\\"\\n```\\n\\nNow expansion includes more related terms.\\n\\n### Expansion Limitations\\n\\nExpansion works well for:\\n- Common terms (appear in many documents)\\n- Concepts with multiple discussions\\n- Well-connected terms in the knowledge graph\\n\\nExpansion works poorly for:\\n- Rare specialized terms (appear in 1-2 documents)\\n- Very new features (not yet well-connected)\\n- Acronyms (expansion may not handle well)\\n\\n---\\n\\n## System Limitations and Workarounds\\n\\n### Limitation 1: Exact Matches Don't Always Score Highest\\n\\n**Problem:** When you search for a function name exactly, variations sometimes score higher.\\n\\n```\\nQuery: \\\"find_documents_for_query\\\"\\nTop result: \\\"fast_find_documents\\\" (unrelated function)\\n```\\n\\n**Reason:** The system ranks by relevance semantically, not by exact match.\\n\\n**Workaround:** Read the file you found or refine your query:\\n```\\n\\\"find_documents relevance scoring\\\"\\n```\\n\\n### Limitation 2: Code Structure Queries May Miss Abstract Concepts\\n\\n**Problem:** Searching for the structure of a data type:\\n```\\n\\\"what fields does Minicolumn have\\\"\\n```\\n\\nMay not find the class definition as well as you'd hope.\\n\\n**Reason:** The definition doesn't discuss relationships; it just declares fields.\\n\\n**Workaround:** Search for the concept instead:\\n```\\n\\\"minicolumn structure representation\\\"\\n```\\n\\nOr use direct file reading for data structure definitions:\\n```\\nRead cortical/minicolumn.py\\n```\\n\\n### Limitation 3: Semantic Similarity Can Be Too Broad\\n\\n**Problem:** Searching for common concepts returns too many results:\\n```\\nQuery: \\\"connection\\\"\\nResult: Returns all mentions of \\\"connections\\\" (hundreds)\\n```\\n\\n**Reason:** \\\"Connection\\\" is a core concept mentioned everywhere.\\n\\n**Workaround:** Be more specific:\\n```\\n\\\"lateral connections co-occurrence\\\"\\n\\\"feedforward connections hierarchy\\\"\\n```\\n\\n### Limitation 4: Fast Mode Only Returns Documents, Not Passages\\n\\n**Problem:** When using `--fast` flag, you only get file names, not specific passages.\\n\\n```bash\\npython scripts/search_codebase.py \\\"PageRank\\\" --fast\\n# Returns: cortical/analysis.py:1 (without specific passage)\\n```\\n\\n**Reason:** Fast mode skips passage extraction for speed (~2-3x faster).\\n\\n**Workaround:** Use without `--fast` for specific passages, or read the file directly after getting the filename.\\n\\n### Limitation 5: Index Doesn't Cover Git History\\n\\n**Problem:** You can't search for how code looked before changes.\\n\\n**Reason:** The index is built from current files only.\\n\\n**Workaround:** Use git history for temporal queries:\\n```bash\\ngit log -p cortical/query.py | grep \\\"function_name\\\"\\n```\\n\\n### Limitation 6: Documentation May Be Outdated\\n\\n**Problem:** Docs in the index reflect what was written, not necessarily what code actually does.\\n\\n```\\nQuery: \\\"how layer computation works\\\"\\nResult: May find outdated documentation\\n```\\n\\n**Reason:** Docs and code can drift.\\n\\n**Workaround:** Verify by reading the actual code after finding relevant documentation.\\n\\n### Limitation 7: Very New Code May Not Be Discoverable\\n\\n**Problem:** Code you just wrote won't be found until re-indexing.\\n\\n**Workaround:** Re-index with `--incremental` after writing code:\\n```bash\\npython scripts/index_codebase.py --incremental\\n```\\n\\n---\\n\\n## Common Code Query Patterns\\n\\n### Finding Algorithm Implementations\\n\\n**Goal:** Understand how a specific algorithm works\\n\\n```\\n\\\"PageRank importance scoring\\\"\\n\\\"TF-IDF term weighting\\\"\\n\\\"label propagation clustering\\\"\\n```\\n\\n**What to expect:** Functions implementing the algorithm, parameter documentation\\n\\n### Finding Bug Locations\\n\\n**Goal:** Locate where a bug might be\\n\\n```\\n\\\"bigram separator space\\\" (if debugging bigram issues)\\n\\\"layer ID index lookup\\\" (if debugging lookups)\\n\\\"tokenizer stemming\\\" (if debugging tokenization)\\n```\\n\\n**What to expect:** Code that handles the buggy component\\n\\n### Finding Integration Points\\n\\n**Goal:** Understand how components connect\\n\\n```\\n\\\"where PageRank results used\\\"\\n\\\"TF-IDF score returned\\\"\\n\\\"minicolumn connected to layer\\\"\\n```\\n\\n**What to expect:** Code that calls or uses the component\\n\\n### Finding Test Patterns\\n\\n**Goal:** Understand how to test a feature\\n\\n```\\n\\\"test PageRank computation\\\"\\n\\\"unittest layer structure\\\"\\n\\\"assert results valid\\\"\\n```\\n\\n**What to expect:** Test files showing testing patterns\\n\\n### Finding Performance Optimizations\\n\\n**Goal:** Understand efficiency strategies\\n\\n```\\n\\\"fast search document only\\\"\\n\\\"incremental indexing changes\\\"\\n\\\"O(1) ID lookup cache\\\"\\n```\\n\\n**What to expect:** Code with performance-related comments/optimization\\n\\n### Finding Data Structure Details\\n\\n**Goal:** Understand internal representations\\n\\n```\\n\\\"minicolumn connections fields\\\"\\n\\\"layer minicolumns dictionary\\\"\\n\\\"document ID format\\\"\\n```\\n\\n**What to expect:** Class definitions, docstrings explaining structure\\n\\n---\\n\\n## Performance Considerations\\n\\n### When to Use Each Search Method\\n\\n| Method | Speed | Use Case |\\n|--------|-------|----------|\\n| Normal search | 1-2s | Default, accurate passage extraction |\\n| Fast search (`--fast`) | 0.2-0.5s | Need just documents, not passages |\\n| Direct file read | <0.1s | Know exact file location |\\n| Interactive mode | 0.5-1s per query | Exploratory research sessions |\\n\\n### Batching Queries\\n\\nIf you have multiple searches, use interactive mode instead of multiple CLI calls:\\n\\n```bash\\npython scripts/search_codebase.py --interactive\\n# Then issue multiple queries in one session\\n# More efficient than multiple command calls\\n```\\n\\n### Caching Expansion\\n\\nIf you're searching for related terms repeatedly:\\n\\n```python\\n# In code, use:\\nprocessor.expand_query_cached(query)\\n```\\n\\nInstead of:\\n```python\\nprocessor.expand_query(query)\\n```\\n\\nThe cached version uses LRU cache for repeated queries.\\n\\n### Index Size Trade-offs\\n\\n**Fast mode (default):**\\n- Smaller index (~30MB)\\n- Faster indexing (2-3 seconds)\\n- Fast search (0.5-1s)\\n- No bigram connections, no concept analysis\\n\\n**Full analysis mode:**\\n- Larger index (~100+MB)\\n- Slow indexing (10+ minutes)\\n- More comprehensive results\\n- Use only when you need deep exploration\\n\\nFor normal development: **Use fast mode**. Use `--full-analysis` only for research sessions.\\n\\n---\\n\\n## Decision Tree: How to Find Code\\n\\n```\\nDo you know the exact file?\\n├─ YES: Use Read tool directly\\n└─ NO: Continue...\\n\\nDo you know what to search for?\\n├─ YES: Use codebase-search with query\\n└─ NO: Continue...\\n\\nIs it a well-known component?\\n├─ YES: Search for the component name\\n└─ NO: Continue...\\n\\nCan you describe what it does?\\n├─ YES: Search for the concept/behavior\\n└─ NO: Use grep or browse manually\\n\\nIs the search too slow?\\n├─ YES: Use --fast flag or break into narrower queries\\n└─ NO: Proceed normally\\n\\nDid you get results?\\n├─ YES: Pick the best match, read full file\\n└─ NO: Go to \\\"Handling No Results\\\" section\\n```\\n\\n---\\n\\n## Summary for Claude\\n\\nWhen working with this codebase:\\n\\n1. **Start with search, not reading** - The semantic search is fast and gives you context\\n2. **Use natural language queries** - Write queries as you would ask a colleague\\n3. **Trust the expansion** - The system automatically finds related terms\\n4. **Check scores, but don't over-interpret** - High scores are good, but context matters more\\n5. **Re-index after changes** - Always use `--incremental` after making code changes\\n6. **Fall back to direct reading** - Once you have a file:line reference, switch to Read\\n7. **Broaden when stuck** - If search returns nothing, remove implementation details and try again\\n8. **Use iterative refinement** - Start broad, then narrow based on what you learn\\n\\nThe semantic search system is designed to accelerate your understanding of the codebase by making it searchable by meaning, not just keywords. Use it as your primary tool for exploration and learning.\\n\\n---\\n\\n*Last updated: 2025-12-10*\\n*For the Cortical Text Processor codebase*\\n\",",
        "      \"mtime\": 1765401839.0",
        "    },",
        "    {",
        "      \"op\": \"add\",",
        "      \"doc_id\": \"cortical/minicolumn.py\",",
        "      \"content\": \"\\\"\\\"\\\"\\nMinicolumn Module\\n=================\\n\\nCore data structure representing a cortical minicolumn.\\n\\nIn the neocortex, minicolumns are vertical structures containing\\n~80-100 neurons that respond to similar features. This class models\\nthat concept for text processing.\\n\\\"\\\"\\\"\\n\\nfrom typing import Set, Dict, Optional, List\\nfrom dataclasses import dataclass, field, asdict\\n\\n\\n@dataclass\\nclass Edge:\\n    \\\"\\\"\\\"\\n    Typed edge with metadata for ConceptNet-style graph representation.\\n\\n    Stores not just the connection weight, but also relation type,\\n    confidence, and source information.\\n\\n    Attributes:\\n        target_id: ID of the target minicolumn\\n        weight: Connection strength (accumulated from multiple sources)\\n        relation_type: Semantic relation type ('co_occurrence', 'IsA', 'PartOf', etc.)\\n        confidence: Confidence score for this edge (0.0 to 1.0)\\n        source: Where this edge came from ('corpus', 'semantic', 'inferred')\\n\\n    Example:\\n        edge = Edge(\\\"L0_network\\\", 0.8, relation_type='RelatedTo', confidence=0.9)\\n    \\\"\\\"\\\"\\n    target_id: str\\n    weight: float = 1.0\\n    relation_type: str = 'co_occurrence'\\n    confidence: float = 1.0\\n    source: str = 'corpus'\\n\\n    def to_dict(self) -> Dict:\\n        \\\"\\\"\\\"Convert to dictionary for serialization.\\\"\\\"\\\"\\n        return asdict(self)\\n\\n    @classmethod\\n    def from_dict(cls, data: Dict) -> 'Edge':\\n        \\\"\\\"\\\"Create an Edge from dictionary representation.\\\"\\\"\\\"\\n        return cls(\\n            target_id=data['target_id'],\\n            weight=data.get('weight', 1.0),\\n            relation_type=data.get('relation_type', 'co_occurrence'),\\n            confidence=data.get('confidence', 1.0),\\n            source=data.get('source', 'corpus')\\n        )\\n\\n\\nclass Minicolumn:\\n    \\\"\\\"\\\"\\n    A minicolumn represents a single concept/feature at a given hierarchy level.\\n    \\n    In the biological neocortex, minicolumns are the fundamental processing\\n    units. Here, each minicolumn represents:\\n    - Layer 0: A single token/word\\n    - Layer 1: A bigram pattern\\n    - Layer 2: A concept cluster\\n    - Layer 3: A document\\n    \\n    Attributes:\\n        id: Unique identifier (e.g., \\\"L0_neural\\\")\\n        content: The actual content (word, bigram, doc_id)\\n        layer: Which layer this column belongs to\\n        activation: Current activation level (like neural firing rate)\\n        occurrence_count: How many times this has been observed\\n        document_ids: Which documents contain this content\\n        lateral_connections: Connections to other columns at same layer (simple weight dict)\\n        typed_connections: Typed edges with metadata (ConceptNet-style)\\n        feedforward_sources: IDs of columns that feed into this one (deprecated, use feedforward_connections)\\n        feedforward_connections: Weighted connections to lower layer columns\\n        feedback_connections: Weighted connections to higher layer columns\\n        tfidf: TF-IDF weight for this term\\n        tfidf_per_doc: Document-specific TF-IDF scores\\n        pagerank: Importance score from PageRank algorithm\\n        cluster_id: Which cluster this belongs to (for Layer 0)\\n        doc_occurrence_counts: Per-document occurrence counts for accurate TF-IDF\\n\\n    Example:\\n        col = Minicolumn(\\\"L0_neural\\\", \\\"neural\\\", 0)\\n        col.occurrence_count = 15\\n        col.add_lateral_connection(\\\"L0_network\\\", 0.8)\\n        col.add_typed_connection(\\\"L0_network\\\", 0.8, relation_type='RelatedTo')\\n    \\\"\\\"\\\"\\n\\n    __slots__ = [\\n        'id', 'content', 'layer', 'activation', 'occurrence_count',\\n        'document_ids', 'lateral_connections', 'typed_connections',\\n        'feedforward_sources', 'feedforward_connections', 'feedback_connections',\\n        'tfidf', 'tfidf_per_doc', 'pagerank', 'cluster_id',\\n        'doc_occurrence_counts'\\n    ]\\n    \\n    def __init__(self, id: str, content: str, layer: int):\\n        \\\"\\\"\\\"\\n        Initialize a minicolumn.\\n        \\n        Args:\\n            id: Unique identifier for this column\\n            content: The content this column represents\\n            layer: Layer number (0-3)\\n        \\\"\\\"\\\"\\n        self.id = id\\n        self.content = content\\n        self.layer = layer\\n        self.activation = 0.0\\n        self.occurrence_count = 0\\n        self.document_ids: Set[str] = set()\\n        self.lateral_connections: Dict[str, float] = {}\\n        self.typed_connections: Dict[str, Edge] = {}  # ConceptNet-style typed edges\\n        self.feedforward_sources: Set[str] = set()  # Deprecated: use feedforward_connections\\n        self.feedforward_connections: Dict[str, float] = {}  # Weighted links to lower layer\\n        self.feedback_connections: Dict[str, float] = {}  # Weighted links to higher layer\\n        self.tfidf = 0.0\\n        self.tfidf_per_doc: Dict[str, float] = {}\\n        self.pagerank = 1.0\\n        self.cluster_id: Optional[int] = None\\n        self.doc_occurrence_counts: Dict[str, int] = {}\\n    \\n    def add_lateral_connection(self, target_id: str, weight: float = 1.0) -> None:\\n        \\\"\\\"\\\"\\n        Add or strengthen a lateral connection to another column.\\n\\n        Lateral connections represent associations learned through\\n        co-occurrence (like Hebbian learning: \\\"neurons that fire together\\n        wire together\\\").\\n\\n        Args:\\n            target_id: ID of the target minicolumn\\n            weight: Connection strength to add\\n        \\\"\\\"\\\"\\n        self.lateral_connections[target_id] = (\\n            self.lateral_connections.get(target_id, 0) + weight\\n        )\\n\\n    def add_typed_connection(\\n        self,\\n        target_id: str,\\n        weight: float = 1.0,\\n        relation_type: str = 'co_occurrence',\\n        confidence: float = 1.0,\\n        source: str = 'corpus'\\n    ) -> None:\\n        \\\"\\\"\\\"\\n        Add or update a typed connection with metadata.\\n\\n        Typed connections store ConceptNet-style edge information including\\n        relation type, confidence, and source. If a connection to the target\\n        already exists, the weight is accumulated and metadata is updated.\\n\\n        Args:\\n            target_id: ID of the target minicolumn\\n            weight: Connection strength to add (accumulates with existing)\\n            relation_type: Semantic relation type ('co_occurrence', 'IsA', etc.)\\n            confidence: Confidence score for this edge (0.0 to 1.0)\\n            source: Where this edge came from ('corpus', 'semantic', 'inferred')\\n\\n        Example:\\n            col.add_typed_connection(\\\"L0_network\\\", 0.8, relation_type='RelatedTo')\\n            col.add_typed_connection(\\\"L0_brain\\\", 0.5, relation_type='IsA', source='semantic')\\n        \\\"\\\"\\\"\\n        if target_id in self.typed_connections:\\n            # Accumulate weight, keep most informative metadata\\n            existing = self.typed_connections[target_id]\\n            new_weight = existing.weight + weight\\n            # Prefer more specific relation types over 'co_occurrence'\\n            new_relation = relation_type if relation_type != 'co_occurrence' else existing.relation_type\\n            # Use higher confidence\\n            new_confidence = max(confidence, existing.confidence)\\n            # Prefer semantic/inferred over corpus\\n            source_priority = {'inferred': 3, 'semantic': 2, 'corpus': 1}\\n            new_source = source if source_priority.get(source, 0) > source_priority.get(existing.source, 0) else existing.source\\n            self.typed_connections[target_id] = Edge(\\n                target_id=target_id,\\n                weight=new_weight,\\n                relation_type=new_relation,\\n                confidence=new_confidence,\\n                source=new_source\\n            )\\n        else:\\n            self.typed_connections[target_id] = Edge(\\n                target_id=target_id,\\n                weight=weight,\\n                relation_type=relation_type,\\n                confidence=confidence,\\n                source=source\\n            )\\n\\n        # Also update simple lateral_connections for backward compatibility\\n        self.lateral_connections[target_id] = (\\n            self.lateral_connections.get(target_id, 0) + weight\\n        )\\n\\n    def get_typed_connection(self, target_id: str) -> Optional[Edge]:\\n        \\\"\\\"\\\"\\n        Get a typed connection by target ID.\\n\\n        Args:\\n            target_id: ID of the target minicolumn\\n\\n        Returns:\\n            Edge object if exists, None otherwise\\n        \\\"\\\"\\\"\\n        return self.typed_connections.get(target_id)\\n\\n    def get_connections_by_type(self, relation_type: str) -> List[Edge]:\\n        \\\"\\\"\\\"\\n        Get all typed connections with a specific relation type.\\n\\n        Args:\\n            relation_type: Relation type to filter by (e.g., 'IsA', 'PartOf')\\n\\n        Returns:\\n            List of Edge objects matching the relation type\\n        \\\"\\\"\\\"\\n        return [\\n            edge for edge in self.typed_connections.values()\\n            if edge.relation_type == relation_type\\n        ]\\n\\n    def get_connections_by_source(self, source: str) -> List[Edge]:\\n        \\\"\\\"\\\"\\n        Get all typed connections from a specific source.\\n\\n        Args:\\n            source: Source to filter by ('corpus', 'semantic', 'inferred')\\n\\n        Returns:\\n            List of Edge objects from the specified source\\n        \\\"\\\"\\\"\\n        return [\\n            edge for edge in self.typed_connections.values()\\n            if edge.source == source\\n        ]\\n\\n    def add_feedforward_connection(self, target_id: str, weight: float = 1.0) -> None:\\n        \\\"\\\"\\\"\\n        Add or strengthen a feedforward connection to a lower layer column.\\n\\n        Feedforward connections link higher-level representations to their\\n        component parts (e.g., bigram → tokens, concept → tokens).\\n\\n        Args:\\n            target_id: ID of the lower-layer minicolumn\\n            weight: Connection strength to add\\n        \\\"\\\"\\\"\\n        self.feedforward_connections[target_id] = (\\n            self.feedforward_connections.get(target_id, 0) + weight\\n        )\\n        # Also maintain legacy feedforward_sources for backward compatibility\\n        self.feedforward_sources.add(target_id)\\n\\n    def add_feedback_connection(self, target_id: str, weight: float = 1.0) -> None:\\n        \\\"\\\"\\\"\\n        Add or strengthen a feedback connection to a higher layer column.\\n\\n        Feedback connections link lower-level representations to the\\n        higher-level structures they participate in (e.g., token → bigrams).\\n\\n        Args:\\n            target_id: ID of the higher-layer minicolumn\\n            weight: Connection strength to add\\n        \\\"\\\"\\\"\\n        self.feedback_connections[target_id] = (\\n            self.feedback_connections.get(target_id, 0) + weight\\n        )\\n    \\n    def connection_count(self) -> int:\\n        \\\"\\\"\\\"Return the number of lateral connections.\\\"\\\"\\\"\\n        return len(self.lateral_connections)\\n    \\n    def top_connections(self, n: int = 5) -> list:\\n        \\\"\\\"\\\"\\n        Get the strongest lateral connections.\\n        \\n        Args:\\n            n: Number of connections to return\\n            \\n        Returns:\\n            List of (target_id, weight) tuples, sorted by weight\\n        \\\"\\\"\\\"\\n        sorted_conns = sorted(\\n            self.lateral_connections.items(),\\n            key=lambda x: x[1],\\n            reverse=True\\n        )\\n        return sorted_conns[:n]\\n    \\n    def to_dict(self) -> Dict:\\n        \\\"\\\"\\\"\\n        Convert to dictionary for serialization.\\n\\n        Returns:\\n            Dictionary representation of this minicolumn\\n        \\\"\\\"\\\"\\n        return {\\n            'id': self.id,\\n            'content': self.content,\\n            'layer': self.layer,\\n            'activation': self.activation,\\n            'occurrence_count': self.occurrence_count,\\n            'document_ids': list(self.document_ids),\\n            'lateral_connections': self.lateral_connections,\\n            'typed_connections': {\\n                target_id: edge.to_dict()\\n                for target_id, edge in self.typed_connections.items()\\n            },\\n            'feedforward_sources': list(self.feedforward_sources),\\n            'feedforward_connections': self.feedforward_connections,\\n            'feedback_connections': self.feedback_connections,\\n            'tfidf': self.tfidf,\\n            'tfidf_per_doc': self.tfidf_per_doc,\\n            'pagerank': self.pagerank,\\n            'cluster_id': self.cluster_id,\\n            'doc_occurrence_counts': self.doc_occurrence_counts\\n        }\\n    \\n    @classmethod\\n    def from_dict(cls, data: Dict) -> 'Minicolumn':\\n        \\\"\\\"\\\"\\n        Create a minicolumn from dictionary representation.\\n\\n        Args:\\n            data: Dictionary with minicolumn data\\n\\n        Returns:\\n            New Minicolumn instance\\n        \\\"\\\"\\\"\\n        col = cls(data['id'], data['content'], data['layer'])\\n        col.activation = data.get('activation', 0.0)\\n        col.occurrence_count = data.get('occurrence_count', 0)\\n        col.document_ids = set(data.get('document_ids', []))\\n        col.lateral_connections = data.get('lateral_connections', {})\\n        # Deserialize typed connections\\n        typed_conn_data = data.get('typed_connections', {})\\n        col.typed_connections = {\\n            target_id: Edge.from_dict(edge_data)\\n            for target_id, edge_data in typed_conn_data.items()\\n        }\\n        col.feedforward_sources = set(data.get('feedforward_sources', []))\\n        col.feedforward_connections = data.get('feedforward_connections', {})\\n        col.feedback_connections = data.get('feedback_connections', {})\\n        col.tfidf = data.get('tfidf', 0.0)\\n        col.tfidf_per_doc = data.get('tfidf_per_doc', {})\\n        col.pagerank = data.get('pagerank', 1.0)\\n        col.cluster_id = data.get('cluster_id')\\n        col.doc_occurrence_counts = data.get('doc_occurrence_counts', {})\\n        return col\\n    \\n    def __repr__(self) -> str:\\n        return f\\\"Minicolumn(id={self.id}, content={self.content}, layer={self.layer})\\\"\\n\",",
        "      \"mtime\": 1765375584.0",
        "    },",
        "    {",
        "      \"op\": \"add\",",
        "      \"doc_id\": \"cortical/query.py\",",
        "      \"content\": \"\\\"\\\"\\\"\\nQuery Module\\n============\\n\\nQuery expansion and search functionality.\\n\\nProvides methods for expanding queries using lateral connections,\\nconcept clusters, and word variants, then searching the corpus\\nusing TF-IDF and graph-based scoring.\\n\\\"\\\"\\\"\\n\\nfrom typing import Dict, List, Tuple, Optional, TypedDict\\nfrom collections import defaultdict\\nimport re\\n\\nfrom .layers import CorticalLayer, HierarchicalLayer\\nfrom .tokenizer import Tokenizer\\nfrom .code_concepts import expand_code_concepts, get_related_terms\\n\\n\\n# Intent types for query understanding\\nclass ParsedIntent(TypedDict):\\n    \\\"\\\"\\\"Structured representation of a parsed query intent.\\\"\\\"\\\"\\n    action: Optional[str]       # The verb/action (e.g., \\\"handle\\\", \\\"implement\\\")\\n    subject: Optional[str]      # The main subject (e.g., \\\"authentication\\\")\\n    intent: str                 # Query intent type (location, implementation, definition, etc.)\\n    question_word: Optional[str]  # Original question word if present\\n    expanded_terms: List[str]   # All searchable terms with synonyms\\n\\n\\n# Question word to intent mapping\\nQUESTION_INTENTS = {\\n    'where': 'location',      # Find location/file\\n    'how': 'implementation',  # Find implementation details\\n    'what': 'definition',     # Find definitions\\n    'why': 'rationale',       # Find comments/documentation explaining reasoning\\n    'when': 'lifecycle',      # Find when something happens (init, shutdown, etc.)\\n    'which': 'selection',     # Find choices/options\\n    'who': 'attribution',     # Find ownership/authorship (git blame territory)\\n}\\n\\n# Common action verbs in code queries\\nACTION_VERBS = frozenset([\\n    'handle', 'process', 'create', 'delete', 'update', 'fetch', 'get', 'set',\\n    'load', 'save', 'store', 'validate', 'check', 'parse', 'format', 'convert',\\n    'transform', 'render', 'display', 'show', 'hide', 'enable', 'disable',\\n    'start', 'stop', 'init', 'initialize', 'setup', 'configure', 'connect',\\n    'disconnect', 'send', 'receive', 'read', 'write', 'open', 'close',\\n    'authenticate', 'authorize', 'login', 'logout', 'register', 'subscribe',\\n    'publish', 'emit', 'listen', 'dispatch', 'trigger', 'call', 'invoke',\\n    'execute', 'run', 'build', 'compile', 'test', 'deploy', 'implement',\\n])\\n\\n\\ndef expand_query(\\n    query_text: str,\\n    layers: Dict[CorticalLayer, HierarchicalLayer],\\n    tokenizer: Tokenizer,\\n    max_expansions: int = 10,\\n    use_lateral: bool = True,\\n    use_concepts: bool = True,\\n    use_variants: bool = True,\\n    use_code_concepts: bool = False\\n) -> Dict[str, float]:\\n    \\\"\\\"\\\"\\n    Expand a query using lateral connections and concept clusters.\\n\\n    This mimics how the brain retrieves related memories when given a cue:\\n    - Lateral connections: direct word associations (like priming)\\n    - Concept clusters: semantic category membership\\n    - Word variants: stemming and synonym mapping\\n    - Code concepts: programming synonym groups (get/fetch/load)\\n\\n    Args:\\n        query_text: Original query string\\n        layers: Dictionary of layers\\n        tokenizer: Tokenizer instance\\n        max_expansions: Maximum number of expansion terms to add\\n        use_lateral: Include terms from lateral connections\\n        use_concepts: Include terms from concept clusters\\n        use_variants: Try word variants when direct match fails\\n        use_code_concepts: Include programming synonym expansions\\n\\n    Returns:\\n        Dict mapping terms to weights (original terms get weight 1.0)\\n    \\\"\\\"\\\"\\n    tokens = tokenizer.tokenize(query_text)\\n    layer0 = layers[CorticalLayer.TOKENS]\\n    layer2 = layers.get(CorticalLayer.CONCEPTS)\\n    \\n    # Start with original terms at full weight\\n    expanded: Dict[str, float] = {}\\n    unmatched_tokens = []\\n    \\n    for token in tokens:\\n        col = layer0.get_minicolumn(token)\\n        if col:\\n            expanded[token] = 1.0\\n        else:\\n            unmatched_tokens.append(token)\\n    \\n    # Try to match unmatched tokens using variants\\n    if use_variants and unmatched_tokens:\\n        for token in unmatched_tokens:\\n            variants = tokenizer.get_word_variants(token)\\n            for variant in variants:\\n                col = layer0.get_minicolumn(variant)\\n                if col and variant not in expanded:\\n                    expanded[variant] = 0.8\\n                    break\\n    \\n    if not expanded:\\n        return expanded\\n    \\n    candidate_expansions: Dict[str, float] = defaultdict(float)\\n    \\n    # Method 1: Lateral connections (direct associations)\\n    if use_lateral:\\n        for token in list(expanded.keys()):\\n            col = layer0.get_minicolumn(token)\\n            if col:\\n                sorted_neighbors = sorted(\\n                    col.lateral_connections.items(),\\n                    key=lambda x: x[1],\\n                    reverse=True\\n                )[:5]\\n                \\n                for neighbor_id, weight in sorted_neighbors:\\n                    # Use O(1) ID lookup instead of linear search\\n                    neighbor = layer0.get_by_id(neighbor_id)\\n                    if neighbor and neighbor.content not in expanded:\\n                        score = weight * neighbor.pagerank * 0.6\\n                        candidate_expansions[neighbor.content] = max(\\n                            candidate_expansions[neighbor.content], score\\n                        )\\n    \\n    # Method 2: Concept cluster membership\\n    if use_concepts and layer2 and layer2.column_count() > 0:\\n        for token in list(expanded.keys()):\\n            col = layer0.get_minicolumn(token)\\n            if col:\\n                for concept in layer2.minicolumns.values():\\n                    if col.id in concept.feedforward_sources:\\n                        for member_id in concept.feedforward_sources:\\n                            # Use O(1) ID lookup instead of linear search\\n                            member = layer0.get_by_id(member_id)\\n                            if member and member.content not in expanded:\\n                                score = concept.pagerank * member.pagerank * 0.4\\n                                candidate_expansions[member.content] = max(\\n                                    candidate_expansions[member.content], score\\n                                )\\n\\n    # Method 3: Code concept groups (programming synonyms)\\n    if use_code_concepts:\\n        code_expansions = expand_code_concepts(\\n            list(expanded.keys()),\\n            max_expansions_per_term=3,\\n            weight=0.6\\n        )\\n        for term, weight in code_expansions.items():\\n            if term not in expanded:\\n                candidate_expansions[term] = max(\\n                    candidate_expansions[term], weight\\n                )\\n\\n    # Select top expansions\\n    sorted_candidates = sorted(\\n        candidate_expansions.items(),\\n        key=lambda x: x[1],\\n        reverse=True\\n    )[:max_expansions]\\n    \\n    for term, score in sorted_candidates:\\n        expanded[term] = score\\n    \\n    return expanded\\n\\n\\ndef parse_intent_query(query_text: str) -> ParsedIntent:\\n    \\\"\\\"\\\"\\n    Parse a natural language query to extract intent and searchable terms.\\n\\n    Analyzes queries like \\\"where do we handle authentication?\\\" to identify:\\n    - Question word (where) -> intent type (location)\\n    - Action verb (handle) -> search for handling code\\n    - Subject (authentication) -> main topic with synonyms\\n\\n    Args:\\n        query_text: Natural language query string\\n\\n    Returns:\\n        ParsedIntent with action, subject, intent type, and expanded terms\\n\\n    Example:\\n        >>> parse_intent_query(\\\"where do we handle authentication?\\\")\\n        {\\n            'action': 'handle',\\n            'subject': 'authentication',\\n            'intent': 'location',\\n            'question_word': 'where',\\n            'expanded_terms': ['handle', 'authentication', 'auth', 'login', ...]\\n        }\\n    \\\"\\\"\\\"\\n    # Normalize query\\n    query_lower = query_text.lower().strip()\\n    query_lower = re.sub(r'[?!.,;:]', '', query_lower)  # Remove punctuation\\n    words = query_lower.split()\\n\\n    if not words:\\n        return ParsedIntent(\\n            action=None,\\n            subject=None,\\n            intent='search',\\n            question_word=None,\\n            expanded_terms=[]\\n        )\\n\\n    # Detect question word and intent\\n    question_word = None\\n    intent = 'search'  # Default intent\\n\\n    for word in words:\\n        if word in QUESTION_INTENTS:\\n            question_word = word\\n            intent = QUESTION_INTENTS[word]\\n            break\\n\\n    # Remove common filler words for parsing\\n    filler_words = {'do', 'we', 'i', 'you', 'the', 'a', 'an', 'is', 'are', 'was',\\n                    'were', 'can', 'could', 'should', 'would', 'does', 'did',\\n                    'have', 'has', 'had', 'be', 'been', 'being', 'will', 'to'}\\n    content_words = [w for w in words if w not in filler_words and w not in QUESTION_INTENTS]\\n\\n    # Find action verb\\n    action = None\\n    for word in content_words:\\n        if word in ACTION_VERBS:\\n            action = word\\n            break\\n\\n    # Find subject (first non-action content word, or last content word)\\n    subject = None\\n    for word in content_words:\\n        if word != action:\\n            subject = word\\n            break\\n    if not subject and content_words:\\n        subject = content_words[-1]\\n\\n    # Build expanded terms list\\n    expanded_terms = []\\n\\n    # Add action and its synonyms\\n    if action:\\n        expanded_terms.append(action)\\n        action_synonyms = get_related_terms(action, max_terms=5)\\n        expanded_terms.extend(action_synonyms)\\n\\n    # Add subject and its synonyms\\n    if subject:\\n        expanded_terms.append(subject)\\n        subject_synonyms = get_related_terms(subject, max_terms=5)\\n        expanded_terms.extend(subject_synonyms)\\n\\n    # Add remaining content words\\n    for word in content_words:\\n        if word not in expanded_terms:\\n            expanded_terms.append(word)\\n\\n    # Remove duplicates while preserving order\\n    seen = set()\\n    unique_terms = []\\n    for term in expanded_terms:\\n        if term not in seen:\\n            seen.add(term)\\n            unique_terms.append(term)\\n\\n    return ParsedIntent(\\n        action=action,\\n        subject=subject,\\n        intent=intent,\\n        question_word=question_word,\\n        expanded_terms=unique_terms\\n    )\\n\\n\\ndef search_by_intent(\\n    query_text: str,\\n    layers: Dict[CorticalLayer, HierarchicalLayer],\\n    tokenizer: Tokenizer,\\n    top_n: int = 5\\n) -> List[Tuple[str, float, ParsedIntent]]:\\n    \\\"\\\"\\\"\\n    Search the corpus using intent-based query understanding.\\n\\n    Parses the query to understand intent, expands terms using code concepts,\\n    then searches with appropriate weighting based on intent type.\\n\\n    Args:\\n        query_text: Natural language query string\\n        layers: Dictionary of layers\\n        tokenizer: Tokenizer instance\\n        top_n: Number of results to return\\n\\n    Returns:\\n        List of (doc_id, score, parsed_intent) tuples\\n\\n    Example:\\n        >>> search_by_intent(\\\"how do we validate user input?\\\", layers, tokenizer)\\n        [('validation.py', 0.85, {...}), ('forms.py', 0.72, {...}), ...]\\n    \\\"\\\"\\\"\\n    # Parse the query intent\\n    parsed = parse_intent_query(query_text)\\n\\n    if not parsed['expanded_terms']:\\n        return []\\n\\n    # Build weighted query from expanded terms\\n    layer0 = layers[CorticalLayer.TOKENS]\\n    layer3 = layers[CorticalLayer.DOCUMENTS]\\n\\n    # Score documents based on term matches\\n    doc_scores: Dict[str, float] = defaultdict(float)\\n\\n    for i, term in enumerate(parsed['expanded_terms']):\\n        # Earlier terms (action, subject) get higher weight\\n        term_weight = 1.0 / (1 + i * 0.2)\\n\\n        col = layer0.get_minicolumn(term)\\n        if col:\\n            for doc_id in col.document_ids:\\n                # Use TF-IDF if available\\n                tfidf = col.tfidf_per_doc.get(doc_id, col.tfidf)\\n                doc_scores[doc_id] += term_weight * tfidf\\n\\n    # Sort by score\\n    sorted_docs = sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)\\n\\n    # Return top results with parsed intent\\n    results = []\\n    for doc_id, score in sorted_docs[:top_n]:\\n        results.append((doc_id, score, parsed))\\n\\n    return results\\n\\n\\n# Valid relation chain patterns for multi-hop inference\\n# Key: (relation1, relation2) → validity score (0.0 = invalid, 1.0 = fully valid)\\nVALID_RELATION_CHAINS = {\\n    # Transitive hierarchies\\n    ('IsA', 'IsA'): 1.0,           # dog IsA animal IsA living_thing\\n    ('PartOf', 'PartOf'): 1.0,     # wheel PartOf car PartOf vehicle\\n    ('IsA', 'HasProperty'): 0.9,   # dog IsA animal HasProperty alive\\n    ('PartOf', 'HasProperty'): 0.8,  # wheel PartOf car HasProperty fast\\n\\n    # Association chains\\n    ('RelatedTo', 'RelatedTo'): 0.6,\\n    ('SimilarTo', 'SimilarTo'): 0.7,\\n    ('CoOccurs', 'CoOccurs'): 0.5,\\n    ('RelatedTo', 'IsA'): 0.7,\\n    ('RelatedTo', 'SimilarTo'): 0.7,\\n\\n    # Causal chains\\n    ('Causes', 'Causes'): 0.8,\\n    ('Causes', 'HasProperty'): 0.7,\\n\\n    # Derivation chains\\n    ('DerivedFrom', 'DerivedFrom'): 0.8,\\n    ('DerivedFrom', 'IsA'): 0.7,\\n\\n    # Usage chains\\n    ('UsedFor', 'UsedFor'): 0.6,\\n    ('UsedFor', 'RelatedTo'): 0.5,\\n\\n    # Antonym - generally invalid for chaining\\n    ('Antonym', 'Antonym'): 0.3,   # Double negation, weak\\n    ('Antonym', 'IsA'): 0.1,       # Contradictory\\n}\\n\\n\\ndef score_relation_path(path: List[str]) -> float:\\n    \\\"\\\"\\\"\\n    Score a relation path by its semantic coherence.\\n\\n    Args:\\n        path: List of relation types traversed (e.g., ['IsA', 'HasProperty'])\\n\\n    Returns:\\n        Score from 0.0 (invalid) to 1.0 (fully valid)\\n    \\\"\\\"\\\"\\n    if not path:\\n        return 1.0\\n    if len(path) == 1:\\n        return 1.0\\n\\n    # Compute score as product of consecutive pair validities\\n    total_score = 1.0\\n    for i in range(len(path) - 1):\\n        pair = (path[i], path[i + 1])\\n        # Check both orderings\\n        pair_score = VALID_RELATION_CHAINS.get(pair, 0.4)  # Default: moderate validity\\n        total_score *= pair_score\\n\\n    return total_score\\n\\n\\ndef expand_query_multihop(\\n    query_text: str,\\n    layers: Dict[CorticalLayer, HierarchicalLayer],\\n    tokenizer: Tokenizer,\\n    semantic_relations: List[Tuple[str, str, str, float]],\\n    max_hops: int = 2,\\n    max_expansions: int = 15,\\n    decay_factor: float = 0.5,\\n    min_path_score: float = 0.2\\n) -> Dict[str, float]:\\n    \\\"\\\"\\\"\\n    Expand query using multi-hop semantic inference.\\n\\n    Unlike single-hop expansion that only follows direct connections,\\n    this follows relation chains to discover semantically related terms\\n    through transitive relationships.\\n\\n    Example inference chains:\\n        \\\"dog\\\" → IsA → \\\"animal\\\" → HasProperty → \\\"living\\\"\\n        \\\"car\\\" → PartOf → \\\"engine\\\" → UsedFor → \\\"transportation\\\"\\n\\n    Args:\\n        query_text: Original query string\\n        layers: Dictionary of layers (needs TOKENS)\\n        tokenizer: Tokenizer instance\\n        semantic_relations: List of (term1, relation, term2, weight) tuples\\n        max_hops: Maximum number of relation hops (default: 2)\\n        max_expansions: Maximum expansion terms to return\\n        decay_factor: Weight decay per hop (default: 0.5, so hop2 = 0.25)\\n        min_path_score: Minimum path validity score to include (default: 0.2)\\n\\n    Returns:\\n        Dict mapping terms to weights (original terms get weight 1.0,\\n        expansions get decayed weights based on hop distance and path validity)\\n\\n    Example:\\n        >>> expanded = expand_query_multihop(\\\"neural\\\", layers, tokenizer, relations)\\n        >>> # Hop 1: networks (co-occur), learning (co-occur), brain (RelatedTo)\\n        >>> # Hop 2: deep (via learning), cortex (via brain), AI (via networks)\\n    \\\"\\\"\\\"\\n    tokens = tokenizer.tokenize(query_text)\\n    layer0 = layers[CorticalLayer.TOKENS]\\n\\n    # Start with original terms at full weight\\n    expanded: Dict[str, float] = {}\\n    for token in tokens:\\n        if layer0.get_minicolumn(token):\\n            expanded[token] = 1.0\\n\\n    if not expanded or not semantic_relations:\\n        return expanded\\n\\n    # Build bidirectional neighbor lookup with relation types\\n    # neighbors[term] = [(neighbor, relation_type, weight), ...]\\n    neighbors: Dict[str, List[Tuple[str, str, float]]] = defaultdict(list)\\n    for t1, relation, t2, weight in semantic_relations:\\n        neighbors[t1].append((t2, relation, weight))\\n        neighbors[t2].append((t1, relation, weight))\\n\\n    # Track expansions with their hop distance, weight, and relation path\\n    # (term, weight, hop, relation_path)\\n    candidates: Dict[str, Tuple[float, int, List[str]]] = {}\\n\\n    # BFS-style expansion with hop tracking\\n    # frontier: [(term, current_weight, hop_count, relation_path)]\\n    frontier: List[Tuple[str, float, int, List[str]]] = [\\n        (term, 1.0, 0, []) for term in expanded.keys()\\n    ]\\n\\n    visited_at_hop: Dict[str, int] = {term: 0 for term in expanded.keys()}\\n\\n    while frontier:\\n        current_term, current_weight, hop, path = frontier.pop(0)\\n\\n        if hop >= max_hops:\\n            continue\\n\\n        next_hop = hop + 1\\n\\n        for neighbor, relation, rel_weight in neighbors.get(current_term, []):\\n            # Skip if already in original query terms\\n            if neighbor in expanded:\\n                continue\\n\\n            # Check if term exists in corpus\\n            if not layer0.get_minicolumn(neighbor):\\n                continue\\n\\n            # Skip if we've visited this term at an earlier or equal hop\\n            if neighbor in visited_at_hop and visited_at_hop[neighbor] <= next_hop:\\n                continue\\n\\n            # Compute new path and its validity\\n            new_path = path + [relation]\\n            path_score = score_relation_path(new_path)\\n\\n            if path_score < min_path_score:\\n                continue\\n\\n            # Compute weight with decay and path validity\\n            # weight = base_weight * relation_weight * decay^hop * path_validity\\n            hop_decay = decay_factor ** next_hop\\n            new_weight = current_weight * rel_weight * hop_decay * path_score\\n\\n            # Update candidate if this path gives higher weight\\n            if neighbor not in candidates or candidates[neighbor][0] < new_weight:\\n                candidates[neighbor] = (new_weight, next_hop, new_path)\\n                visited_at_hop[neighbor] = next_hop\\n\\n                # Add to frontier for further expansion\\n                if next_hop < max_hops:\\n                    frontier.append((neighbor, new_weight, next_hop, new_path))\\n\\n    # Sort candidates by weight and take top expansions\\n    sorted_candidates = sorted(\\n        candidates.items(),\\n        key=lambda x: x[1][0],  # Sort by weight\\n        reverse=True\\n    )[:max_expansions]\\n\\n    # Add to expanded dict\\n    for term, (weight, hop, path) in sorted_candidates:\\n        expanded[term] = weight\\n\\n    return expanded\\n\\n\\ndef expand_query_semantic(\\n    query_text: str,\\n    layers: Dict[CorticalLayer, HierarchicalLayer],\\n    tokenizer: Tokenizer,\\n    semantic_relations: List[Tuple[str, str, str, float]],\\n    max_expansions: int = 10\\n) -> Dict[str, float]:\\n    \\\"\\\"\\\"\\n    Expand query using semantic relations extracted from corpus.\\n    \\n    Args:\\n        query_text: Original query\\n        layers: Dictionary of layers\\n        tokenizer: Tokenizer instance\\n        semantic_relations: List of (term1, relation, term2, weight) tuples\\n        max_expansions: Maximum expansions\\n        \\n    Returns:\\n        Dict mapping terms to weights\\n    \\\"\\\"\\\"\\n    tokens = tokenizer.tokenize(query_text)\\n    layer0 = layers[CorticalLayer.TOKENS]\\n    \\n    # Build semantic neighbor lookup\\n    neighbors: Dict[str, List[Tuple[str, float]]] = defaultdict(list)\\n    for t1, relation, t2, weight in semantic_relations:\\n        neighbors[t1].append((t2, weight))\\n        neighbors[t2].append((t1, weight))\\n    \\n    # Start with original terms\\n    expanded = {t: 1.0 for t in tokens if layer0.get_minicolumn(t)}\\n    \\n    if not expanded:\\n        return expanded\\n    \\n    # Add semantic neighbors\\n    candidates: Dict[str, float] = defaultdict(float)\\n    for token in list(expanded.keys()):\\n        for neighbor, weight in neighbors.get(token, []):\\n            if neighbor not in expanded and layer0.get_minicolumn(neighbor):\\n                candidates[neighbor] = max(candidates[neighbor], weight * 0.7)\\n    \\n    # Take top candidates\\n    sorted_candidates = sorted(candidates.items(), key=lambda x: x[1], reverse=True)\\n    for term, score in sorted_candidates[:max_expansions]:\\n        expanded[term] = score\\n    \\n    return expanded\\n\\n\\ndef get_expanded_query_terms(\\n    query_text: str,\\n    layers: Dict[CorticalLayer, HierarchicalLayer],\\n    tokenizer: Tokenizer,\\n    use_expansion: bool = True,\\n    semantic_relations: Optional[List[Tuple[str, str, str, float]]] = None,\\n    use_semantic: bool = True,\\n    max_expansions: int = 5,\\n    semantic_discount: float = 0.8\\n) -> Dict[str, float]:\\n    \\\"\\\"\\\"\\n    Get expanded query terms with optional semantic expansion.\\n\\n    This is a helper function that consolidates query expansion logic used\\n    by multiple search functions. It handles:\\n    - Lateral connection expansion via expand_query()\\n    - Semantic relation expansion via expand_query_semantic()\\n    - Merging of expansion results with appropriate weighting\\n\\n    Args:\\n        query_text: Original query string\\n        layers: Dictionary of layers\\n        tokenizer: Tokenizer instance\\n        use_expansion: Whether to expand query terms using lateral connections\\n        semantic_relations: Optional list of semantic relations for expansion\\n        use_semantic: Whether to use semantic relations for expansion\\n        max_expansions: Maximum expansion terms per method (default 5)\\n        semantic_discount: Weight multiplier for semantic expansions (default 0.8)\\n\\n    Returns:\\n        Dict mapping terms to weights (original terms get weight 1.0,\\n        expansions get lower weights based on connection strength)\\n\\n    Example:\\n        >>> terms = get_expanded_query_terms(\\\"neural networks\\\", layers, tokenizer)\\n        >>> # Returns: {'neural': 1.0, 'networks': 1.0, 'deep': 0.3, 'learning': 0.25, ...}\\n    \\\"\\\"\\\"\\n    if use_expansion:\\n        # Start with lateral connection expansion\\n        query_terms = expand_query(query_text, layers, tokenizer, max_expansions=max_expansions)\\n\\n        # Add semantic expansion if available\\n        if use_semantic and semantic_relations:\\n            semantic_terms = expand_query_semantic(\\n                query_text, layers, tokenizer, semantic_relations, max_expansions=max_expansions\\n            )\\n            # Merge semantic expansions (don't override stronger weights)\\n            for term, weight in semantic_terms.items():\\n                if term not in query_terms:\\n                    query_terms[term] = weight * semantic_discount\\n                else:\\n                    # Take the max weight\\n                    query_terms[term] = max(query_terms[term], weight * semantic_discount)\\n    else:\\n        tokens = tokenizer.tokenize(query_text)\\n        query_terms = {t: 1.0 for t in tokens}\\n\\n    return query_terms\\n\\n\\ndef find_documents_for_query(\\n    query_text: str,\\n    layers: Dict[CorticalLayer, HierarchicalLayer],\\n    tokenizer: Tokenizer,\\n    top_n: int = 5,\\n    use_expansion: bool = True,\\n    semantic_relations: Optional[List[Tuple[str, str, str, float]]] = None,\\n    use_semantic: bool = True\\n) -> List[Tuple[str, float]]:\\n    \\\"\\\"\\\"\\n    Find documents most relevant to a query using TF-IDF and optional expansion.\\n\\n    Args:\\n        query_text: Search query\\n        layers: Dictionary of layers\\n        tokenizer: Tokenizer instance\\n        top_n: Number of documents to return\\n        use_expansion: Whether to expand query terms using lateral connections\\n        semantic_relations: Optional list of semantic relations for expansion\\n        use_semantic: Whether to use semantic relations for expansion (if available)\\n\\n    Returns:\\n        List of (doc_id, score) tuples ranked by relevance\\n    \\\"\\\"\\\"\\n    layer0 = layers[CorticalLayer.TOKENS]\\n\\n    query_terms = get_expanded_query_terms(\\n        query_text, layers, tokenizer,\\n        use_expansion=use_expansion,\\n        semantic_relations=semantic_relations,\\n        use_semantic=use_semantic\\n    )\\n\\n    # Score each document\\n    doc_scores: Dict[str, float] = defaultdict(float)\\n\\n    for term, term_weight in query_terms.items():\\n        col = layer0.get_minicolumn(term)\\n        if col:\\n            for doc_id in col.document_ids:\\n                tfidf = col.tfidf_per_doc.get(doc_id, col.tfidf)\\n                doc_scores[doc_id] += tfidf * term_weight\\n\\n    sorted_docs = sorted(doc_scores.items(), key=lambda x: -x[1])\\n    return sorted_docs[:top_n]\\n\\n\\ndef fast_find_documents(\\n    query_text: str,\\n    layers: Dict[CorticalLayer, HierarchicalLayer],\\n    tokenizer: Tokenizer,\\n    top_n: int = 5,\\n    candidate_multiplier: int = 3,\\n    use_code_concepts: bool = True\\n) -> List[Tuple[str, float]]:\\n    \\\"\\\"\\\"\\n    Fast document search using candidate filtering.\\n\\n    Optimizes search by:\\n    1. Using set intersection to find candidate documents\\n    2. Only scoring top candidates fully\\n    3. Using code concept expansion for better recall\\n\\n    This is ~2-3x faster than full search on large corpora while\\n    maintaining similar result quality.\\n\\n    Args:\\n        query_text: Search query\\n        layers: Dictionary of layers\\n        tokenizer: Tokenizer instance\\n        top_n: Number of results to return\\n        candidate_multiplier: Multiplier for candidate set size\\n        use_code_concepts: Whether to use code concept expansion\\n\\n    Returns:\\n        List of (doc_id, score) tuples ranked by relevance\\n    \\\"\\\"\\\"\\n    layer0 = layers[CorticalLayer.TOKENS]\\n\\n    # Tokenize query\\n    tokens = tokenizer.tokenize(query_text)\\n    if not tokens:\\n        return []\\n\\n    # Phase 1: Find candidate documents (fast set operations)\\n    # Get documents containing ANY query term\\n    candidate_docs: Dict[str, int] = defaultdict(int)  # doc_id -> match count\\n\\n    for token in tokens:\\n        col = layer0.get_minicolumn(token)\\n        if col:\\n            for doc_id in col.document_ids:\\n                candidate_docs[doc_id] += 1\\n\\n    # If no candidates, try code concept expansion for recall\\n    if not candidate_docs and use_code_concepts:\\n        for token in tokens:\\n            related = get_related_terms(token, max_terms=3)\\n            for related_term in related:\\n                col = layer0.get_minicolumn(related_term)\\n                if col:\\n                    for doc_id in col.document_ids:\\n                        candidate_docs[doc_id] += 0.5  # Lower weight for expansion\\n\\n    if not candidate_docs:\\n        return []\\n\\n    # Rank candidates by match count first (fast pre-filter)\\n    sorted_candidates = sorted(\\n        candidate_docs.items(),\\n        key=lambda x: x[1],\\n        reverse=True\\n    )\\n\\n    # Take top N * multiplier candidates for full scoring\\n    max_candidates = top_n * candidate_multiplier\\n    top_candidates = sorted_candidates[:max_candidates]\\n\\n    # Phase 2: Full scoring only on top candidates\\n    doc_scores: Dict[str, float] = {}\\n\\n    for doc_id, match_count in top_candidates:\\n        score = 0.0\\n        for token in tokens:\\n            col = layer0.get_minicolumn(token)\\n            if col and doc_id in col.document_ids:\\n                tfidf = col.tfidf_per_doc.get(doc_id, col.tfidf)\\n                score += tfidf\\n\\n        # Boost by match coverage\\n        coverage_boost = match_count / len(tokens)\\n        doc_scores[doc_id] = score * (1 + 0.5 * coverage_boost)\\n\\n    # Return top results\\n    sorted_docs = sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)\\n    return sorted_docs[:top_n]\\n\\n\\ndef build_document_index(\\n    layers: Dict[CorticalLayer, HierarchicalLayer]\\n) -> Dict[str, Dict[str, float]]:\\n    \\\"\\\"\\\"\\n    Build an optimized inverted index for fast querying.\\n\\n    Creates a term -> {doc_id: score} mapping that can be used\\n    for fast set operations during search.\\n\\n    Args:\\n        layers: Dictionary of layers\\n\\n    Returns:\\n        Dict mapping terms to {doc_id: tfidf_score} dicts\\n    \\\"\\\"\\\"\\n    layer0 = layers.get(CorticalLayer.TOKENS)\\n    if not layer0:\\n        return {}\\n\\n    index: Dict[str, Dict[str, float]] = {}\\n\\n    for col in layer0.minicolumns.values():\\n        term = col.content\\n        term_index: Dict[str, float] = {}\\n\\n        for doc_id in col.document_ids:\\n            tfidf = col.tfidf_per_doc.get(doc_id, col.tfidf)\\n            term_index[doc_id] = tfidf\\n\\n        if term_index:\\n            index[term] = term_index\\n\\n    return index\\n\\n\\ndef search_with_index(\\n    query_text: str,\\n    index: Dict[str, Dict[str, float]],\\n    tokenizer: Tokenizer,\\n    top_n: int = 5\\n) -> List[Tuple[str, float]]:\\n    \\\"\\\"\\\"\\n    Search using a pre-built inverted index.\\n\\n    This is the fastest search method when the index is cached.\\n\\n    Args:\\n        query_text: Search query\\n        index: Pre-built index from build_document_index()\\n        tokenizer: Tokenizer instance\\n        top_n: Number of results to return\\n\\n    Returns:\\n        List of (doc_id, score) tuples ranked by relevance\\n    \\\"\\\"\\\"\\n    tokens = tokenizer.tokenize(query_text)\\n    if not tokens:\\n        return []\\n\\n    doc_scores: Dict[str, float] = defaultdict(float)\\n\\n    for token in tokens:\\n        if token in index:\\n            for doc_id, score in index[token].items():\\n                doc_scores[doc_id] += score\\n\\n    sorted_docs = sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)\\n    return sorted_docs[:top_n]\\n\\n\\ndef query_with_spreading_activation(\\n    query_text: str,\\n    layers: Dict[CorticalLayer, HierarchicalLayer],\\n    tokenizer: Tokenizer,\\n    top_n: int = 10,\\n    max_expansions: int = 8\\n) -> List[Tuple[str, float]]:\\n    \\\"\\\"\\\"\\n    Query with automatic expansion using spreading activation.\\n    \\n    This is like the brain's spreading activation during memory retrieval:\\n    a cue activates not just direct matches but semantically related concepts.\\n    \\n    Args:\\n        query_text: Search query\\n        layers: Dictionary of layers\\n        tokenizer: Tokenizer instance\\n        top_n: Number of results to return\\n        max_expansions: How many expansion terms to add\\n        \\n    Returns:\\n        List of (concept, score) tuples ranked by relevance\\n    \\\"\\\"\\\"\\n    expanded_terms = expand_query(\\n        query_text, layers, tokenizer,\\n        max_expansions=max_expansions\\n    )\\n    \\n    if not expanded_terms:\\n        return []\\n    \\n    layer0 = layers[CorticalLayer.TOKENS]\\n    activated: Dict[str, float] = {}\\n    \\n    # Activate based on expanded query\\n    for term, term_weight in expanded_terms.items():\\n        col = layer0.get_minicolumn(term)\\n        if col:\\n            # Direct activation\\n            score = col.pagerank * col.activation * term_weight\\n            activated[col.content] = activated.get(col.content, 0) + score\\n            \\n            # Spread to neighbors using O(1) ID lookup\\n            for neighbor_id, conn_weight in col.lateral_connections.items():\\n                neighbor = layer0.get_by_id(neighbor_id)\\n                if neighbor:\\n                    spread_score = neighbor.pagerank * conn_weight * term_weight * 0.3\\n                    activated[neighbor.content] = activated.get(neighbor.content, 0) + spread_score\\n    \\n    sorted_concepts = sorted(activated.items(), key=lambda x: -x[1])\\n    return sorted_concepts[:top_n]\\n\\n\\ndef find_related_documents(\\n    doc_id: str,\\n    layers: Dict[CorticalLayer, HierarchicalLayer]\\n) -> List[Tuple[str, float]]:\\n    \\\"\\\"\\\"\\n    Find documents related to a given document via lateral connections.\\n\\n    Args:\\n        doc_id: Source document ID\\n        layers: Dictionary of layers\\n\\n    Returns:\\n        List of (doc_id, weight) tuples for related documents\\n    \\\"\\\"\\\"\\n    layer3 = layers.get(CorticalLayer.DOCUMENTS)\\n    if not layer3:\\n        return []\\n\\n    col = layer3.get_minicolumn(doc_id)\\n    if not col:\\n        return []\\n\\n    related = []\\n    for neighbor_id, weight in col.lateral_connections.items():\\n        # Use O(1) ID lookup instead of linear search\\n        neighbor = layer3.get_by_id(neighbor_id)\\n        if neighbor:\\n            related.append((neighbor.content, weight))\\n\\n    return sorted(related, key=lambda x: -x[1])\\n\\n\\ndef create_chunks(\\n    text: str,\\n    chunk_size: int = 512,\\n    overlap: int = 128\\n) -> List[Tuple[str, int, int]]:\\n    \\\"\\\"\\\"\\n    Split text into overlapping chunks.\\n\\n    Args:\\n        text: Document text to chunk\\n        chunk_size: Target size of each chunk in characters\\n        overlap: Number of overlapping characters between chunks\\n\\n    Returns:\\n        List of (chunk_text, start_char, end_char) tuples\\n\\n    Raises:\\n        ValueError: If chunk_size <= 0 or overlap < 0 or overlap >= chunk_size\\n    \\\"\\\"\\\"\\n    if chunk_size <= 0:\\n        raise ValueError(f\\\"chunk_size must be positive, got {chunk_size}\\\")\\n    if overlap < 0:\\n        raise ValueError(f\\\"overlap must be non-negative, got {overlap}\\\")\\n    if overlap >= chunk_size:\\n        raise ValueError(f\\\"overlap must be less than chunk_size, got overlap={overlap}, chunk_size={chunk_size}\\\")\\n\\n    if not text:\\n        return []\\n\\n    chunks = []\\n    stride = max(1, chunk_size - overlap)\\n    text_len = len(text)\\n\\n    for start in range(0, text_len, stride):\\n        end = min(start + chunk_size, text_len)\\n        chunk = text[start:end]\\n        chunks.append((chunk, start, end))\\n\\n        if end >= text_len:\\n            break\\n\\n    return chunks\\n\\n\\ndef precompute_term_cols(\\n    query_terms: Dict[str, float],\\n    layer0: HierarchicalLayer\\n) -> Dict[str, 'Minicolumn']:\\n    \\\"\\\"\\\"\\n    Pre-compute minicolumn lookups for query terms.\\n\\n    This avoids repeated O(1) dictionary lookups for each chunk,\\n    enabling faster scoring when processing many chunks.\\n\\n    Args:\\n        query_terms: Dict mapping query terms to weights\\n        layer0: Token layer for lookups\\n\\n    Returns:\\n        Dict mapping term to Minicolumn (only for terms that exist in corpus)\\n    \\\"\\\"\\\"\\n    term_cols = {}\\n    for term in query_terms:\\n        col = layer0.get_minicolumn(term)\\n        if col:\\n            term_cols[term] = col\\n    return term_cols\\n\\n\\ndef score_chunk_fast(\\n    chunk_tokens: List[str],\\n    query_terms: Dict[str, float],\\n    term_cols: Dict[str, 'Minicolumn'],\\n    doc_id: Optional[str] = None\\n) -> float:\\n    \\\"\\\"\\\"\\n    Fast chunk scoring using pre-computed minicolumn lookups.\\n\\n    This is an optimized version of score_chunk that accepts pre-tokenized\\n    text and pre-computed minicolumn lookups. Use when scoring many chunks\\n    from the same document.\\n\\n    Args:\\n        chunk_tokens: Pre-tokenized chunk tokens\\n        query_terms: Dict mapping query terms to weights\\n        term_cols: Pre-computed term->Minicolumn mapping from precompute_term_cols()\\n        doc_id: Optional document ID for per-document TF-IDF\\n\\n    Returns:\\n        Relevance score for the chunk\\n    \\\"\\\"\\\"\\n    if not chunk_tokens:\\n        return 0.0\\n\\n    # Count token occurrences in chunk\\n    token_counts: Dict[str, int] = {}\\n    for token in chunk_tokens:\\n        token_counts[token] = token_counts.get(token, 0) + 1\\n\\n    score = 0.0\\n    for term, term_weight in query_terms.items():\\n        if term in token_counts and term in term_cols:\\n            col = term_cols[term]\\n            # Use per-document TF-IDF if available, otherwise global\\n            tfidf = col.tfidf_per_doc.get(doc_id, col.tfidf) if doc_id else col.tfidf\\n            # Weight by occurrence in chunk and query weight\\n            score += tfidf * token_counts[term] * term_weight\\n\\n    # Normalize by chunk length to avoid bias toward longer chunks\\n    return score / len(chunk_tokens)\\n\\n\\ndef score_chunk(\\n    chunk_text: str,\\n    query_terms: Dict[str, float],\\n    layer0: HierarchicalLayer,\\n    tokenizer: Tokenizer,\\n    doc_id: Optional[str] = None\\n) -> float:\\n    \\\"\\\"\\\"\\n    Score a chunk against query terms using TF-IDF.\\n\\n    Args:\\n        chunk_text: Text of the chunk\\n        query_terms: Dict mapping query terms to weights\\n        layer0: Token layer for TF-IDF lookups\\n        tokenizer: Tokenizer instance\\n        doc_id: Optional document ID for per-document TF-IDF\\n\\n    Returns:\\n        Relevance score for the chunk\\n    \\\"\\\"\\\"\\n    chunk_tokens = tokenizer.tokenize(chunk_text)\\n    if not chunk_tokens:\\n        return 0.0\\n\\n    # Count token occurrences in chunk\\n    token_counts: Dict[str, int] = {}\\n    for token in chunk_tokens:\\n        token_counts[token] = token_counts.get(token, 0) + 1\\n\\n    score = 0.0\\n    for term, term_weight in query_terms.items():\\n        if term in token_counts:\\n            col = layer0.get_minicolumn(term)\\n            if col:\\n                # Use per-document TF-IDF if available, otherwise global\\n                tfidf = col.tfidf_per_doc.get(doc_id, col.tfidf) if doc_id else col.tfidf\\n                # Weight by occurrence in chunk and query weight\\n                score += tfidf * token_counts[term] * term_weight\\n\\n    # Normalize by chunk length to avoid bias toward longer chunks\\n    return score / len(chunk_tokens) if chunk_tokens else 0.0\\n\\n\\ndef find_passages_for_query(\\n    query_text: str,\\n    layers: Dict[CorticalLayer, HierarchicalLayer],\\n    tokenizer: Tokenizer,\\n    documents: Dict[str, str],\\n    top_n: int = 5,\\n    chunk_size: int = 512,\\n    overlap: int = 128,\\n    use_expansion: bool = True,\\n    doc_filter: Optional[List[str]] = None,\\n    semantic_relations: Optional[List[Tuple[str, str, str, float]]] = None,\\n    use_semantic: bool = True\\n) -> List[Tuple[str, str, int, int, float]]:\\n    \\\"\\\"\\\"\\n    Find text passages most relevant to a query.\\n\\n    This is the key function for RAG systems - instead of returning document IDs,\\n    it returns actual text passages with position information for citations.\\n\\n    Args:\\n        query_text: Search query\\n        layers: Dictionary of layers\\n        tokenizer: Tokenizer instance\\n        documents: Dict mapping doc_id to document text\\n        top_n: Number of passages to return\\n        chunk_size: Size of each chunk in characters (default 512)\\n        overlap: Overlap between chunks in characters (default 128)\\n        use_expansion: Whether to expand query terms\\n        doc_filter: Optional list of doc_ids to restrict search to\\n        semantic_relations: Optional list of semantic relations for expansion\\n        use_semantic: Whether to use semantic relations for expansion (if available)\\n\\n    Returns:\\n        List of (passage_text, doc_id, start_char, end_char, score) tuples\\n        ranked by relevance\\n    \\\"\\\"\\\"\\n    layer0 = layers[CorticalLayer.TOKENS]\\n\\n    # Get expanded query terms\\n    query_terms = get_expanded_query_terms(\\n        query_text, layers, tokenizer,\\n        use_expansion=use_expansion,\\n        semantic_relations=semantic_relations,\\n        use_semantic=use_semantic\\n    )\\n\\n    if not query_terms:\\n        return []\\n\\n    # Pre-compute minicolumn lookups for query terms (optimization)\\n    term_cols = precompute_term_cols(query_terms, layer0)\\n\\n    # First, get candidate documents (more than we need, since we'll rank passages)\\n    doc_scores = find_documents_for_query(\\n        query_text, layers, tokenizer,\\n        top_n=min(len(documents), top_n * 3),\\n        use_expansion=use_expansion,\\n        semantic_relations=semantic_relations,\\n        use_semantic=use_semantic\\n    )\\n\\n    # Apply document filter if provided\\n    if doc_filter:\\n        doc_scores = [(doc_id, score) for doc_id, score in doc_scores if doc_id in doc_filter]\\n\\n    # Score passages within candidate documents\\n    passages: List[Tuple[str, str, int, int, float]] = []\\n\\n    for doc_id, doc_score in doc_scores:\\n        if doc_id not in documents:\\n            continue\\n\\n        text = documents[doc_id]\\n        chunks = create_chunks(text, chunk_size, overlap)\\n\\n        for chunk_text, start_char, end_char in chunks:\\n            # Use fast scoring with pre-computed lookups\\n            chunk_tokens = tokenizer.tokenize(chunk_text)\\n            chunk_score = score_chunk_fast(\\n                chunk_tokens, query_terms, term_cols, doc_id\\n            )\\n            # Combine chunk score with document score for final ranking\\n            combined_score = chunk_score * (1 + doc_score * 0.1)\\n\\n            passages.append((\\n                chunk_text,\\n                doc_id,\\n                start_char,\\n                end_char,\\n                combined_score\\n            ))\\n\\n    # Sort by score and return top passages\\n    passages.sort(key=lambda x: x[4], reverse=True)\\n    return passages[:top_n]\\n\\n\\ndef find_documents_batch(\\n    queries: List[str],\\n    layers: Dict[CorticalLayer, HierarchicalLayer],\\n    tokenizer: Tokenizer,\\n    top_n: int = 5,\\n    use_expansion: bool = True,\\n    semantic_relations: Optional[List[Tuple[str, str, str, float]]] = None,\\n    use_semantic: bool = True\\n) -> List[List[Tuple[str, float]]]:\\n    \\\"\\\"\\\"\\n    Find documents for multiple queries efficiently.\\n\\n    More efficient than calling find_documents_for_query() multiple times\\n    because it shares tokenization and expansion caching across queries.\\n\\n    Args:\\n        queries: List of search query strings\\n        layers: Dictionary of layers\\n        tokenizer: Tokenizer instance\\n        top_n: Number of documents to return per query\\n        use_expansion: Whether to expand query terms\\n        semantic_relations: Optional list of semantic relations for expansion\\n        use_semantic: Whether to use semantic relations for expansion\\n\\n    Returns:\\n        List of results, one per query. Each result is a list of (doc_id, score) tuples.\\n\\n    Example:\\n        >>> queries = [\\\"neural networks\\\", \\\"machine learning\\\", \\\"data processing\\\"]\\n        >>> results = find_documents_batch(queries, layers, tokenizer, top_n=3)\\n        >>> for query, docs in zip(queries, results):\\n        ...     print(f\\\"{query}: {[doc_id for doc_id, _ in docs]}\\\")\\n    \\\"\\\"\\\"\\n    layer0 = layers[CorticalLayer.TOKENS]\\n\\n    # Cache for expanded query terms to avoid redundant computation\\n    expansion_cache: Dict[str, Dict[str, float]] = {}\\n\\n    all_results: List[List[Tuple[str, float]]] = []\\n\\n    for query_text in queries:\\n        # Check cache first for expansion\\n        if query_text in expansion_cache:\\n            query_terms = expansion_cache[query_text]\\n        else:\\n            query_terms = get_expanded_query_terms(\\n                query_text, layers, tokenizer,\\n                use_expansion=use_expansion,\\n                semantic_relations=semantic_relations,\\n                use_semantic=use_semantic\\n            )\\n            expansion_cache[query_text] = query_terms\\n\\n        # Score documents\\n        doc_scores: Dict[str, float] = defaultdict(float)\\n        for term, term_weight in query_terms.items():\\n            col = layer0.get_minicolumn(term)\\n            if col:\\n                for doc_id in col.document_ids:\\n                    tfidf = col.tfidf_per_doc.get(doc_id, col.tfidf)\\n                    doc_scores[doc_id] += tfidf * term_weight\\n\\n        sorted_docs = sorted(doc_scores.items(), key=lambda x: -x[1])\\n        all_results.append(sorted_docs[:top_n])\\n\\n    return all_results\\n\\n\\ndef find_passages_batch(\\n    queries: List[str],\\n    layers: Dict[CorticalLayer, HierarchicalLayer],\\n    tokenizer: Tokenizer,\\n    documents: Dict[str, str],\\n    top_n: int = 5,\\n    chunk_size: int = 512,\\n    overlap: int = 128,\\n    use_expansion: bool = True,\\n    doc_filter: Optional[List[str]] = None,\\n    semantic_relations: Optional[List[Tuple[str, str, str, float]]] = None,\\n    use_semantic: bool = True\\n) -> List[List[Tuple[str, str, int, int, float]]]:\\n    \\\"\\\"\\\"\\n    Find passages for multiple queries efficiently.\\n\\n    More efficient than calling find_passages_for_query() multiple times\\n    because it shares chunk computation and expansion caching across queries.\\n\\n    Args:\\n        queries: List of search query strings\\n        layers: Dictionary of layers\\n        tokenizer: Tokenizer instance\\n        documents: Dict mapping doc_id to document text\\n        top_n: Number of passages to return per query\\n        chunk_size: Size of each chunk in characters\\n        overlap: Overlap between chunks in characters\\n        use_expansion: Whether to expand query terms\\n        doc_filter: Optional list of doc_ids to restrict search to\\n        semantic_relations: Optional list of semantic relations for expansion\\n        use_semantic: Whether to use semantic relations for expansion\\n\\n    Returns:\\n        List of results, one per query. Each result is a list of\\n        (passage_text, doc_id, start_char, end_char, score) tuples.\\n\\n    Example:\\n        >>> queries = [\\\"neural networks\\\", \\\"deep learning\\\"]\\n        >>> results = find_passages_batch(queries, layers, tokenizer, documents)\\n        >>> for query, passages in zip(queries, results):\\n        ...     print(f\\\"{query}: {len(passages)} passages found\\\")\\n    \\\"\\\"\\\"\\n    layer0 = layers[CorticalLayer.TOKENS]\\n\\n    # Pre-compute chunks for all documents to avoid redundant chunking\\n    doc_chunks_cache: Dict[str, List[Tuple[str, int, int]]] = {}\\n    for doc_id, text in documents.items():\\n        if doc_filter and doc_id not in doc_filter:\\n            continue\\n        doc_chunks_cache[doc_id] = create_chunks(text, chunk_size, overlap)\\n\\n    # Cache for expanded query terms\\n    expansion_cache: Dict[str, Dict[str, float]] = {}\\n\\n    all_results: List[List[Tuple[str, str, int, int, float]]] = []\\n\\n    for query_text in queries:\\n        # Get expanded query terms (with caching)\\n        if query_text in expansion_cache:\\n            query_terms = expansion_cache[query_text]\\n        else:\\n            query_terms = get_expanded_query_terms(\\n                query_text, layers, tokenizer,\\n                use_expansion=use_expansion,\\n                semantic_relations=semantic_relations,\\n                use_semantic=use_semantic\\n            )\\n            expansion_cache[query_text] = query_terms\\n\\n        if not query_terms:\\n            all_results.append([])\\n            continue\\n\\n        # Pre-compute minicolumn lookups for query terms (optimization)\\n        term_cols = precompute_term_cols(query_terms, layer0)\\n\\n        # Get candidate documents\\n        doc_scores = find_documents_for_query(\\n            query_text, layers, tokenizer,\\n            top_n=min(len(documents), top_n * 3),\\n            use_expansion=use_expansion,\\n            semantic_relations=semantic_relations,\\n            use_semantic=use_semantic\\n        )\\n\\n        # Apply document filter\\n        if doc_filter:\\n            doc_scores = [(doc_id, score) for doc_id, score in doc_scores if doc_id in doc_filter]\\n\\n        # Score passages using cached chunks and fast scoring\\n        passages: List[Tuple[str, str, int, int, float]] = []\\n\\n        for doc_id, doc_score in doc_scores:\\n            if doc_id not in doc_chunks_cache:\\n                continue\\n\\n            for chunk_text, start_char, end_char in doc_chunks_cache[doc_id]:\\n                # Use fast scoring with pre-computed lookups\\n                chunk_tokens = tokenizer.tokenize(chunk_text)\\n                chunk_score = score_chunk_fast(\\n                    chunk_tokens, query_terms, term_cols, doc_id\\n                )\\n                combined_score = chunk_score * (1 + doc_score * 0.1)\\n                passages.append((chunk_text, doc_id, start_char, end_char, combined_score))\\n\\n        passages.sort(key=lambda x: x[4], reverse=True)\\n        all_results.append(passages[:top_n])\\n\\n    return all_results\\n\\n\\ndef find_relevant_concepts(\\n    query_terms: Dict[str, float],\\n    layers: Dict[CorticalLayer, HierarchicalLayer],\\n    top_n: int = 5\\n) -> List[Tuple[str, float, set]]:\\n    \\\"\\\"\\\"\\n    Stage 1: Find concepts relevant to query terms.\\n\\n    Args:\\n        query_terms: Dict mapping query terms to weights\\n        layers: Dictionary of layers\\n        top_n: Maximum number of concepts to return\\n\\n    Returns:\\n        List of (concept_name, relevance_score, document_ids) tuples\\n    \\\"\\\"\\\"\\n    layer0 = layers[CorticalLayer.TOKENS]\\n    layer2 = layers.get(CorticalLayer.CONCEPTS)\\n\\n    if not layer2 or layer2.column_count() == 0:\\n        return []\\n\\n    concept_scores: Dict[str, float] = {}\\n    concept_docs: Dict[str, set] = {}\\n\\n    for term, weight in query_terms.items():\\n        col = layer0.get_minicolumn(term)\\n        if not col:\\n            continue\\n\\n        # Find concepts that contain this token\\n        for concept in layer2.minicolumns.values():\\n            if col.id in concept.feedforward_sources:\\n                # Score based on term weight, concept PageRank, and concept size\\n                score = weight * concept.pagerank * (1 + len(concept.feedforward_sources) * 0.01)\\n                concept_scores[concept.content] = concept_scores.get(concept.content, 0) + score\\n                if concept.content not in concept_docs:\\n                    concept_docs[concept.content] = set()\\n                concept_docs[concept.content].update(concept.document_ids)\\n\\n    # Sort by score and return top concepts\\n    sorted_concepts = sorted(concept_scores.items(), key=lambda x: -x[1])[:top_n]\\n    return [(name, score, concept_docs.get(name, set())) for name, score in sorted_concepts]\\n\\n\\ndef multi_stage_rank(\\n    query_text: str,\\n    layers: Dict[CorticalLayer, HierarchicalLayer],\\n    tokenizer: Tokenizer,\\n    documents: Dict[str, str],\\n    top_n: int = 5,\\n    chunk_size: int = 512,\\n    overlap: int = 128,\\n    concept_boost: float = 0.3,\\n    use_expansion: bool = True,\\n    semantic_relations: Optional[List[Tuple[str, str, str, float]]] = None,\\n    use_semantic: bool = True\\n) -> List[Tuple[str, str, int, int, float, Dict[str, float]]]:\\n    \\\"\\\"\\\"\\n    Multi-stage ranking pipeline for improved RAG performance.\\n\\n    Unlike flat ranking (TF-IDF → score), this uses a 4-stage pipeline:\\n    1. Concepts: Filter by topic relevance using Layer 2 clusters\\n    2. Documents: Rank documents within relevant topics\\n    3. Chunks: Rank passages within top documents\\n    4. Rerank: Combine all signals for final scoring\\n\\n    Args:\\n        query_text: Search query\\n        layers: Dictionary of layers\\n        tokenizer: Tokenizer instance\\n        documents: Dict mapping doc_id to document text\\n        top_n: Number of passages to return\\n        chunk_size: Size of each chunk in characters\\n        overlap: Overlap between chunks in characters\\n        concept_boost: Weight for concept relevance in final score (0.0-1.0)\\n        use_expansion: Whether to expand query terms\\n        semantic_relations: Optional list of semantic relations for expansion\\n        use_semantic: Whether to use semantic relations for expansion\\n\\n    Returns:\\n        List of (passage_text, doc_id, start_char, end_char, final_score, stage_scores) tuples.\\n        stage_scores dict contains: concept_score, doc_score, chunk_score, final_score\\n\\n    Example:\\n        >>> results = multi_stage_rank(query, layers, tokenizer, documents)\\n        >>> for passage, doc_id, start, end, score, stages in results:\\n        ...     print(f\\\"[{doc_id}] Score: {score:.3f}\\\")\\n        ...     print(f\\\"  Concept: {stages['concept_score']:.3f}\\\")\\n        ...     print(f\\\"  Doc: {stages['doc_score']:.3f}\\\")\\n        ...     print(f\\\"  Chunk: {stages['chunk_score']:.3f}\\\")\\n    \\\"\\\"\\\"\\n    layer0 = layers[CorticalLayer.TOKENS]\\n\\n    # Get expanded query terms\\n    query_terms = get_expanded_query_terms(\\n        query_text, layers, tokenizer,\\n        use_expansion=use_expansion,\\n        semantic_relations=semantic_relations,\\n        use_semantic=use_semantic\\n    )\\n\\n    if not query_terms:\\n        return []\\n\\n    # ========== STAGE 1: CONCEPTS ==========\\n    # Find relevant concepts to identify topic areas\\n    relevant_concepts = find_relevant_concepts(query_terms, layers, top_n=10)\\n\\n    # Build concept score per document\\n    doc_concept_scores: Dict[str, float] = defaultdict(float)\\n    if relevant_concepts:\\n        max_concept_score = max(score for _, score, _ in relevant_concepts) if relevant_concepts else 1.0\\n        for concept_name, concept_score, doc_ids in relevant_concepts:\\n            normalized_score = concept_score / max_concept_score if max_concept_score > 0 else 0\\n            for doc_id in doc_ids:\\n                doc_concept_scores[doc_id] = max(doc_concept_scores[doc_id], normalized_score)\\n\\n    # ========== STAGE 2: DOCUMENTS ==========\\n    # Score documents using TF-IDF (standard approach)\\n    doc_tfidf_scores: Dict[str, float] = defaultdict(float)\\n    for term, term_weight in query_terms.items():\\n        col = layer0.get_minicolumn(term)\\n        if col:\\n            for doc_id in col.document_ids:\\n                tfidf = col.tfidf_per_doc.get(doc_id, col.tfidf)\\n                doc_tfidf_scores[doc_id] += tfidf * term_weight\\n\\n    # Normalize TF-IDF scores\\n    max_tfidf = max(doc_tfidf_scores.values()) if doc_tfidf_scores else 1.0\\n    for doc_id in doc_tfidf_scores:\\n        doc_tfidf_scores[doc_id] /= max_tfidf if max_tfidf > 0 else 1.0\\n\\n    # Combine concept and TF-IDF scores for document ranking\\n    combined_doc_scores: Dict[str, float] = {}\\n    all_docs = set(doc_concept_scores.keys()) | set(doc_tfidf_scores.keys())\\n    for doc_id in all_docs:\\n        concept_score = doc_concept_scores.get(doc_id, 0.0)\\n        tfidf_score = doc_tfidf_scores.get(doc_id, 0.0)\\n        # Weighted combination\\n        combined_doc_scores[doc_id] = (\\n            (1 - concept_boost) * tfidf_score +\\n            concept_boost * concept_score\\n        )\\n\\n    # Get top documents for chunk scoring\\n    sorted_docs = sorted(combined_doc_scores.items(), key=lambda x: -x[1])\\n    top_docs = sorted_docs[:min(len(sorted_docs), top_n * 3)]\\n\\n    # ========== STAGE 3: CHUNKS ==========\\n    # Score passages within top documents\\n    passages: List[Tuple[str, str, int, int, float, Dict[str, float]]] = []\\n\\n    for doc_id, doc_score in top_docs:\\n        if doc_id not in documents:\\n            continue\\n\\n        text = documents[doc_id]\\n        chunks = create_chunks(text, chunk_size, overlap)\\n\\n        for chunk_text, start_char, end_char in chunks:\\n            chunk_score = score_chunk(chunk_text, query_terms, layer0, tokenizer, doc_id)\\n\\n            # ========== STAGE 4: RERANK ==========\\n            # Combine all signals for final score\\n            concept_score = doc_concept_scores.get(doc_id, 0.0)\\n            tfidf_score = doc_tfidf_scores.get(doc_id, 0.0)\\n\\n            # Normalize chunk score (avoid division by zero)\\n            normalized_chunk = chunk_score\\n\\n            # Final score combines:\\n            # - Chunk-level relevance (primary signal)\\n            # - Document-level TF-IDF (context signal)\\n            # - Concept relevance (topic signal)\\n            final_score = (\\n                0.5 * normalized_chunk +\\n                0.3 * tfidf_score +\\n                0.2 * concept_score\\n            ) * (1 + doc_score * 0.1)  # Slight boost from combined doc score\\n\\n            stage_scores = {\\n                'concept_score': concept_score,\\n                'doc_score': tfidf_score,\\n                'chunk_score': chunk_score,\\n                'combined_doc_score': doc_score,\\n                'final_score': final_score\\n            }\\n\\n            passages.append((\\n                chunk_text,\\n                doc_id,\\n                start_char,\\n                end_char,\\n                final_score,\\n                stage_scores\\n            ))\\n\\n    # Sort by final score and return top passages\\n    passages.sort(key=lambda x: x[4], reverse=True)\\n    return passages[:top_n]\\n\\n\\ndef multi_stage_rank_documents(\\n    query_text: str,\\n    layers: Dict[CorticalLayer, HierarchicalLayer],\\n    tokenizer: Tokenizer,\\n    top_n: int = 5,\\n    concept_boost: float = 0.3,\\n    use_expansion: bool = True,\\n    semantic_relations: Optional[List[Tuple[str, str, str, float]]] = None,\\n    use_semantic: bool = True\\n) -> List[Tuple[str, float, Dict[str, float]]]:\\n    \\\"\\\"\\\"\\n    Multi-stage ranking for documents (without chunk scoring).\\n\\n    Uses the first 2 stages of the pipeline:\\n    1. Concepts: Filter by topic relevance\\n    2. Documents: Rank by combined concept + TF-IDF scores\\n\\n    Args:\\n        query_text: Search query\\n        layers: Dictionary of layers\\n        tokenizer: Tokenizer instance\\n        top_n: Number of documents to return\\n        concept_boost: Weight for concept relevance (0.0-1.0)\\n        use_expansion: Whether to expand query terms\\n        semantic_relations: Optional list of semantic relations\\n        use_semantic: Whether to use semantic relations\\n\\n    Returns:\\n        List of (doc_id, final_score, stage_scores) tuples.\\n        stage_scores dict contains: concept_score, tfidf_score, combined_score\\n    \\\"\\\"\\\"\\n    layer0 = layers[CorticalLayer.TOKENS]\\n\\n    # Get expanded query terms\\n    query_terms = get_expanded_query_terms(\\n        query_text, layers, tokenizer,\\n        use_expansion=use_expansion,\\n        semantic_relations=semantic_relations,\\n        use_semantic=use_semantic\\n    )\\n\\n    if not query_terms:\\n        return []\\n\\n    # Stage 1: Concepts\\n    relevant_concepts = find_relevant_concepts(query_terms, layers, top_n=10)\\n\\n    doc_concept_scores: Dict[str, float] = defaultdict(float)\\n    if relevant_concepts:\\n        max_concept_score = max(score for _, score, _ in relevant_concepts) if relevant_concepts else 1.0\\n        for concept_name, concept_score, doc_ids in relevant_concepts:\\n            normalized_score = concept_score / max_concept_score if max_concept_score > 0 else 0\\n            for doc_id in doc_ids:\\n                doc_concept_scores[doc_id] = max(doc_concept_scores[doc_id], normalized_score)\\n\\n    # Stage 2: Documents\\n    doc_tfidf_scores: Dict[str, float] = defaultdict(float)\\n    for term, term_weight in query_terms.items():\\n        col = layer0.get_minicolumn(term)\\n        if col:\\n            for doc_id in col.document_ids:\\n                tfidf = col.tfidf_per_doc.get(doc_id, col.tfidf)\\n                doc_tfidf_scores[doc_id] += tfidf * term_weight\\n\\n    # Normalize TF-IDF\\n    max_tfidf = max(doc_tfidf_scores.values()) if doc_tfidf_scores else 1.0\\n    for doc_id in doc_tfidf_scores:\\n        doc_tfidf_scores[doc_id] /= max_tfidf if max_tfidf > 0 else 1.0\\n\\n    # Combine scores\\n    results: List[Tuple[str, float, Dict[str, float]]] = []\\n    all_docs = set(doc_concept_scores.keys()) | set(doc_tfidf_scores.keys())\\n\\n    for doc_id in all_docs:\\n        concept_score = doc_concept_scores.get(doc_id, 0.0)\\n        tfidf_score = doc_tfidf_scores.get(doc_id, 0.0)\\n        combined = (1 - concept_boost) * tfidf_score + concept_boost * concept_score\\n\\n        stage_scores = {\\n            'concept_score': concept_score,\\n            'tfidf_score': tfidf_score,\\n            'combined_score': combined\\n        }\\n        results.append((doc_id, combined, stage_scores))\\n\\n    results.sort(key=lambda x: x[1], reverse=True)\\n    return results[:top_n]\\n\\n\\ndef find_relation_between(\\n    term_a: str,\\n    term_b: str,\\n    semantic_relations: List[Tuple[str, str, str, float]]\\n) -> List[Tuple[str, float]]:\\n    \\\"\\\"\\\"\\n    Find semantic relations between two terms.\\n\\n    Args:\\n        term_a: Source term\\n        term_b: Target term\\n        semantic_relations: List of (t1, relation, t2, weight) tuples\\n\\n    Returns:\\n        List of (relation_type, weight) tuples\\n    \\\"\\\"\\\"\\n    relations = []\\n    for t1, rel_type, t2, weight in semantic_relations:\\n        if t1 == term_a and t2 == term_b:\\n            relations.append((rel_type, weight))\\n        elif t2 == term_a and t1 == term_b:\\n            # Reverse direction\\n            relations.append((rel_type, weight * 0.9))  # Slight penalty for reverse\\n\\n    return sorted(relations, key=lambda x: x[1], reverse=True)\\n\\n\\ndef find_terms_with_relation(\\n    term: str,\\n    relation_type: str,\\n    semantic_relations: List[Tuple[str, str, str, float]],\\n    direction: str = 'forward'\\n) -> List[Tuple[str, float]]:\\n    \\\"\\\"\\\"\\n    Find terms connected to a given term by a specific relation type.\\n\\n    Args:\\n        term: Source term\\n        relation_type: Type of relation to follow\\n        semantic_relations: List of (t1, relation, t2, weight) tuples\\n        direction: 'forward' (term→x) or 'backward' (x→term)\\n\\n    Returns:\\n        List of (target_term, weight) tuples\\n    \\\"\\\"\\\"\\n    results = []\\n    for t1, rel_type, t2, weight in semantic_relations:\\n        if rel_type != relation_type:\\n            continue\\n\\n        if direction == 'forward' and t1 == term:\\n            results.append((t2, weight))\\n        elif direction == 'backward' and t2 == term:\\n            results.append((t1, weight))\\n\\n    return sorted(results, key=lambda x: x[1], reverse=True)\\n\\n\\ndef complete_analogy(\\n    term_a: str,\\n    term_b: str,\\n    term_c: str,\\n    layers: Dict[CorticalLayer, HierarchicalLayer],\\n    semantic_relations: List[Tuple[str, str, str, float]],\\n    embeddings: Optional[Dict[str, List[float]]] = None,\\n    top_n: int = 5,\\n    use_embeddings: bool = True,\\n    use_relations: bool = True\\n) -> List[Tuple[str, float, str]]:\\n    \\\"\\\"\\\"\\n    Complete an analogy: \\\"a is to b as c is to ?\\\"\\n\\n    Uses multiple strategies to find the best completion:\\n    1. Relation matching: Find what relation connects a→b, then find terms with\\n       the same relation from c\\n    2. Vector arithmetic: Use embeddings to compute d = c + (b - a)\\n    3. Pattern matching: Find terms that co-occur with c similar to how b co-occurs with a\\n\\n    Example:\\n        \\\"neural\\\" is to \\\"networks\\\" as \\\"knowledge\\\" is to ?\\n        → \\\"graphs\\\" (both form compound technical terms with similar structure)\\n\\n    Args:\\n        term_a: First term of the known pair\\n        term_b: Second term of the known pair\\n        term_c: First term of the query pair\\n        layers: Dictionary of layers\\n        semantic_relations: List of (t1, relation, t2, weight) tuples\\n        embeddings: Optional graph embeddings for vector arithmetic\\n        top_n: Number of candidates to return\\n        use_embeddings: Whether to use embedding-based completion\\n        use_relations: Whether to use relation-based completion\\n\\n    Returns:\\n        List of (candidate_term, confidence, method) tuples, where method describes\\n        which approach found this candidate ('relation', 'embedding', 'pattern')\\n    \\\"\\\"\\\"\\n    layer0 = layers[CorticalLayer.TOKENS]\\n    candidates: Dict[str, Tuple[float, str]] = {}  # term → (score, method)\\n\\n    # Check that terms exist\\n    if not layer0.get_minicolumn(term_a) or not layer0.get_minicolumn(term_b):\\n        return []\\n    if not layer0.get_minicolumn(term_c):\\n        return []\\n\\n    # Strategy 1: Relation-based completion\\n    if use_relations and semantic_relations:\\n        # Find relation between a and b\\n        relations_ab = find_relation_between(term_a, term_b, semantic_relations)\\n\\n        for rel_type, rel_weight in relations_ab:\\n            # Find terms with same relation from c\\n            c_targets = find_terms_with_relation(\\n                term_c, rel_type, semantic_relations, direction='forward'\\n            )\\n\\n            for target, target_weight in c_targets:\\n                # Don't include the input terms\\n                if target in {term_a, term_b, term_c}:\\n                    continue\\n\\n                score = rel_weight * target_weight\\n                if target not in candidates or candidates[target][0] < score:\\n                    candidates[target] = (score, f'relation:{rel_type}')\\n\\n    # Strategy 2: Embedding-based completion (vector arithmetic)\\n    if use_embeddings and embeddings:\\n        if term_a in embeddings and term_b in embeddings and term_c in embeddings:\\n            vec_a = embeddings[term_a]\\n            vec_b = embeddings[term_b]\\n            vec_c = embeddings[term_c]\\n\\n            # d = c + (b - a)  (the analogy vector)\\n            vec_d = [\\n                c + (b - a)\\n                for a, b, c in zip(vec_a, vec_b, vec_c)\\n            ]\\n\\n            # Find nearest terms to vec_d\\n            best_matches = []\\n            for term, vec in embeddings.items():\\n                if term in {term_a, term_b, term_c}:\\n                    continue\\n\\n                # Cosine similarity\\n                dot = sum(d * v for d, v in zip(vec_d, vec))\\n                mag_d = sum(d * d for d in vec_d) ** 0.5\\n                mag_v = sum(v * v for v in vec) ** 0.5\\n\\n                if mag_d > 0 and mag_v > 0:\\n                    similarity = dot / (mag_d * mag_v)\\n                    best_matches.append((term, similarity))\\n\\n            # Sort by similarity and add to candidates\\n            best_matches.sort(key=lambda x: x[1], reverse=True)\\n            for term, sim in best_matches[:top_n * 2]:\\n                if sim > 0.5:  # Only include reasonably similar terms\\n                    if term not in candidates or candidates[term][0] < sim:\\n                        candidates[term] = (sim, 'embedding')\\n\\n    # Strategy 3: Pattern matching (co-occurrence structure)\\n    col_a = layer0.get_minicolumn(term_a)\\n    col_b = layer0.get_minicolumn(term_b)\\n    col_c = layer0.get_minicolumn(term_c)\\n\\n    if col_a and col_b and col_c:\\n        # Find terms that relate to c similarly to how b relates to a\\n        # I.e., if b co-occurs strongly with a, find terms that co-occur strongly with c\\n\\n        a_neighbors = set(col_a.lateral_connections.keys())\\n        c_neighbors = set(col_c.lateral_connections.keys())\\n\\n        # Look at c's neighbors that aren't a's neighbors (new context)\\n        for neighbor_id in c_neighbors:\\n            neighbor = layer0.get_by_id(neighbor_id)\\n            if not neighbor:\\n                continue\\n\\n            term = neighbor.content\\n            if term in {term_a, term_b, term_c}:\\n                continue\\n\\n            # Score based on how similar the neighbor's connection to c is\\n            # compared to b's connection to a\\n            c_weight = col_c.lateral_connections.get(neighbor_id, 0)\\n            b_to_a_weight = col_a.lateral_connections.get(col_b.id, 0)\\n\\n            if c_weight > 0 and b_to_a_weight > 0:\\n                # The term should have similar connection strength pattern\\n                score = min(c_weight, b_to_a_weight) * 0.5\\n                if score > 0.1:\\n                    if term not in candidates or candidates[term][0] < score:\\n                        candidates[term] = (score, 'pattern')\\n\\n    # Sort and return top candidates\\n    results = [\\n        (term, score, method)\\n        for term, (score, method) in candidates.items()\\n    ]\\n    results.sort(key=lambda x: x[1], reverse=True)\\n\\n    return results[:top_n]\\n\\n\\ndef complete_analogy_simple(\\n    term_a: str,\\n    term_b: str,\\n    term_c: str,\\n    layers: Dict[CorticalLayer, HierarchicalLayer],\\n    tokenizer: Tokenizer,\\n    semantic_relations: Optional[List[Tuple[str, str, str, float]]] = None,\\n    top_n: int = 5\\n) -> List[Tuple[str, float]]:\\n    \\\"\\\"\\\"\\n    Simplified analogy completion using only term relationships.\\n\\n    A lighter version of complete_analogy that doesn't require embeddings.\\n    Uses bigram patterns and co-occurrence to find analogies.\\n\\n    Example:\\n        \\\"neural\\\" is to \\\"networks\\\" as \\\"knowledge\\\" is to ?\\n        → Looks for terms that form similar bigrams with \\\"knowledge\\\"\\n\\n    Args:\\n        term_a: First term of the known pair\\n        term_b: Second term of the known pair\\n        term_c: First term of the query pair\\n        layers: Dictionary of layers\\n        tokenizer: Tokenizer instance\\n        semantic_relations: Optional semantic relations\\n        top_n: Number of candidates to return\\n\\n    Returns:\\n        List of (candidate_term, confidence) tuples\\n    \\\"\\\"\\\"\\n    layer0 = layers[CorticalLayer.TOKENS]\\n    layer1 = layers.get(CorticalLayer.BIGRAMS)\\n\\n    candidates: Dict[str, float] = {}\\n\\n    col_a = layer0.get_minicolumn(term_a)\\n    col_b = layer0.get_minicolumn(term_b)\\n    col_c = layer0.get_minicolumn(term_c)\\n\\n    if not col_a or not col_b or not col_c:\\n        return []\\n\\n    # Strategy 1: Bigram pattern matching\\n    if layer1:\\n        # Find bigrams containing \\\"a b\\\" pattern (bigrams use space separators)\\n        ab_bigram = f\\\"{term_a} {term_b}\\\"\\n        ba_bigram = f\\\"{term_b} {term_a}\\\"\\n\\n        ab_col = layer1.get_minicolumn(ab_bigram)\\n        ba_col = layer1.get_minicolumn(ba_bigram)\\n\\n        # If \\\"a b\\\" is a bigram, look for \\\"c ?\\\" bigrams\\n        if ab_col or ba_col:\\n            for bigram_col in layer1.minicolumns.values():\\n                bigram = bigram_col.content\\n                parts = bigram.split(' ')\\n                if len(parts) != 2:\\n                    continue\\n\\n                first, second = parts\\n\\n                # Look for bigrams starting with c\\n                if first == term_c and second not in {term_a, term_b, term_c}:\\n                    score = bigram_col.pagerank * 0.8\\n                    if second not in candidates or candidates[second] < score:\\n                        candidates[second] = score\\n\\n                # Look for bigrams ending with c\\n                if second == term_c and first not in {term_a, term_b, term_c}:\\n                    score = bigram_col.pagerank * 0.6\\n                    if first not in candidates or candidates[first] < score:\\n                        candidates[first] = score\\n\\n    # Strategy 2: Co-occurrence similarity\\n    # Find terms that co-occur with c like b co-occurs with a\\n    a_neighbors = col_a.lateral_connections\\n    c_neighbors = col_c.lateral_connections\\n\\n    for neighbor_id, c_weight in c_neighbors.items():\\n        neighbor = layer0.get_by_id(neighbor_id)\\n        if not neighbor:\\n            continue\\n\\n        term = neighbor.content\\n        if term in {term_a, term_b, term_c}:\\n            continue\\n\\n        # Check if this term has similar connection pattern\\n        score = c_weight * 0.3\\n        if score > 0.05:\\n            candidates[term] = candidates.get(term, 0) + score\\n\\n    # Strategy 3: Semantic relations (if available)\\n    if semantic_relations:\\n        relations_ab = find_relation_between(term_a, term_b, semantic_relations)\\n        for rel_type, rel_weight in relations_ab[:2]:  # Top 2 relations\\n            c_targets = find_terms_with_relation(\\n                term_c, rel_type, semantic_relations, direction='forward'\\n            )\\n            for target, target_weight in c_targets[:3]:  # Top 3 targets\\n                if target not in {term_a, term_b, term_c}:\\n                    score = rel_weight * target_weight\\n                    candidates[target] = candidates.get(target, 0) + score\\n\\n    # Sort and return\\n    results = sorted(candidates.items(), key=lambda x: x[1], reverse=True)\\n    return results[:top_n]\\n\",",
        "      \"mtime\": 1765392877.0",
        "    }",
        "  ]",
        "}"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "cortical/chunk_index.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "\"\"\"",
        "Chunk-based indexing for git-compatible corpus storage.",
        "",
        "This module provides append-only, time-stamped JSON chunks that can be",
        "safely committed to git without merge conflicts. Each indexing session",
        "creates a uniquely named chunk file containing document operations.",
        "",
        "Architecture:",
        "    corpus_chunks/                        # Tracked in git",
        "    ├── 2025-12-10_21-53-45_a1b2.json    # Session 1 changes",
        "    ├── 2025-12-10_22-15-30_c3d4.json    # Session 2 changes",
        "    └── 2025-12-10_23-00-00_e5f6.json    # Session 3 changes",
        "",
        "    corpus_dev.pkl                        # NOT tracked (local cache)",
        "",
        "Chunk Format:",
        "    {",
        "        \"version\": 1,",
        "        \"timestamp\": \"2025-12-10T21:53:45\",",
        "        \"session_id\": \"a1b2c3d4\",",
        "        \"branch\": \"main\",",
        "        \"operations\": [",
        "            {\"op\": \"add\", \"doc_id\": \"...\", \"content\": \"...\", \"mtime\": 123},",
        "            {\"op\": \"modify\", \"doc_id\": \"...\", \"content\": \"...\", \"mtime\": 124},",
        "            {\"op\": \"delete\", \"doc_id\": \"...\"}",
        "        ]",
        "    }",
        "\"\"\"",
        "",
        "import hashlib",
        "import json",
        "import os",
        "import subprocess",
        "import uuid",
        "from dataclasses import dataclass, field, asdict",
        "from datetime import datetime",
        "from pathlib import Path",
        "from typing import Dict, List, Optional, Tuple, Any",
        "",
        "",
        "# Chunk format version",
        "CHUNK_VERSION = 1",
        "",
        "",
        "@dataclass",
        "class ChunkOperation:",
        "    \"\"\"A single operation in a chunk (add, modify, or delete).\"\"\"",
        "    op: str  # 'add', 'modify', 'delete'",
        "    doc_id: str",
        "    content: Optional[str] = None  # None for delete operations",
        "    mtime: Optional[float] = None  # Modification time",
        "",
        "    def to_dict(self) -> Dict[str, Any]:",
        "        \"\"\"Convert to dictionary for JSON serialization.\"\"\"",
        "        d = {'op': self.op, 'doc_id': self.doc_id}",
        "        if self.content is not None:",
        "            d['content'] = self.content",
        "        if self.mtime is not None:",
        "            d['mtime'] = self.mtime",
        "        return d",
        "",
        "    @classmethod",
        "    def from_dict(cls, d: Dict[str, Any]) -> 'ChunkOperation':",
        "        \"\"\"Create from dictionary.\"\"\"",
        "        return cls(",
        "            op=d['op'],",
        "            doc_id=d['doc_id'],",
        "            content=d.get('content'),",
        "            mtime=d.get('mtime')",
        "        )",
        "",
        "",
        "@dataclass",
        "class Chunk:",
        "    \"\"\"A chunk containing operations from a single indexing session.\"\"\"",
        "    version: int",
        "    timestamp: str",
        "    session_id: str",
        "    branch: str",
        "    operations: List[ChunkOperation] = field(default_factory=list)",
        "",
        "    def to_dict(self) -> Dict[str, Any]:",
        "        \"\"\"Convert to dictionary for JSON serialization.\"\"\"",
        "        return {",
        "            'version': self.version,",
        "            'timestamp': self.timestamp,",
        "            'session_id': self.session_id,",
        "            'branch': self.branch,",
        "            'operations': [op.to_dict() for op in self.operations]",
        "        }",
        "",
        "    @classmethod",
        "    def from_dict(cls, d: Dict[str, Any]) -> 'Chunk':",
        "        \"\"\"Create from dictionary.\"\"\"",
        "        return cls(",
        "            version=d.get('version', 1),",
        "            timestamp=d['timestamp'],",
        "            session_id=d['session_id'],",
        "            branch=d.get('branch', 'unknown'),",
        "            operations=[ChunkOperation.from_dict(op) for op in d['operations']]",
        "        )",
        "",
        "    def get_filename(self) -> str:",
        "        \"\"\"Generate filename for this chunk.\"\"\"",
        "        # Format: YYYY-MM-DD_HH-MM-SS_sessionid.json",
        "        ts = self.timestamp.replace(':', '-').replace('T', '_')",
        "        short_id = self.session_id[:8]",
        "        return f\"{ts}_{short_id}.json\"",
        "",
        "",
        "class ChunkWriter:",
        "    \"\"\"",
        "    Writes indexing session changes to timestamped JSON chunks.",
        "",
        "    Usage:",
        "        writer = ChunkWriter(chunks_dir='corpus_chunks')",
        "        writer.add_document('doc1', 'content here', mtime=1234567890)",
        "        writer.modify_document('doc2', 'new content', mtime=1234567891)",
        "        writer.delete_document('doc3')",
        "        chunk_path = writer.save()",
        "    \"\"\"",
        "",
        "    def __init__(self, chunks_dir: str = 'corpus_chunks'):",
        "        self.chunks_dir = Path(chunks_dir)",
        "        self.session_id = uuid.uuid4().hex[:16]",
        "        self.timestamp = datetime.now().isoformat(timespec='seconds')",
        "        self.branch = self._get_git_branch()",
        "        self.operations: List[ChunkOperation] = []",
        "",
        "    def _get_git_branch(self) -> str:",
        "        \"\"\"Get current git branch name.\"\"\"",
        "        try:",
        "            result = subprocess.run(",
        "                ['git', 'rev-parse', '--abbrev-ref', 'HEAD'],",
        "                capture_output=True,",
        "                text=True,",
        "                timeout=5",
        "            )",
        "            if result.returncode == 0:",
        "                return result.stdout.strip()",
        "        except (subprocess.TimeoutExpired, FileNotFoundError):",
        "            pass",
        "        return 'unknown'",
        "",
        "    def add_document(self, doc_id: str, content: str, mtime: Optional[float] = None):",
        "        \"\"\"Record an add operation.\"\"\"",
        "        self.operations.append(ChunkOperation(",
        "            op='add',",
        "            doc_id=doc_id,",
        "            content=content,",
        "            mtime=mtime",
        "        ))",
        "",
        "    def modify_document(self, doc_id: str, content: str, mtime: Optional[float] = None):",
        "        \"\"\"Record a modify operation.\"\"\"",
        "        self.operations.append(ChunkOperation(",
        "            op='modify',",
        "            doc_id=doc_id,",
        "            content=content,",
        "            mtime=mtime",
        "        ))",
        "",
        "    def delete_document(self, doc_id: str):",
        "        \"\"\"Record a delete operation.\"\"\"",
        "        self.operations.append(ChunkOperation(",
        "            op='delete',",
        "            doc_id=doc_id",
        "        ))",
        "",
        "    def has_operations(self) -> bool:",
        "        \"\"\"Check if any operations were recorded.\"\"\"",
        "        return len(self.operations) > 0",
        "",
        "    def save(self) -> Optional[Path]:",
        "        \"\"\"",
        "        Save chunk to file.",
        "",
        "        Returns:",
        "            Path to saved chunk file, or None if no operations.",
        "        \"\"\"",
        "        if not self.operations:",
        "            return None",
        "",
        "        # Create chunks directory if needed",
        "        self.chunks_dir.mkdir(parents=True, exist_ok=True)",
        "",
        "        # Create chunk",
        "        chunk = Chunk(",
        "            version=CHUNK_VERSION,",
        "            timestamp=self.timestamp,",
        "            session_id=self.session_id,",
        "            branch=self.branch,",
        "            operations=self.operations",
        "        )",
        "",
        "        # Write to file",
        "        filepath = self.chunks_dir / chunk.get_filename()",
        "        with open(filepath, 'w', encoding='utf-8') as f:",
        "            json.dump(chunk.to_dict(), f, indent=2, ensure_ascii=False)",
        "",
        "        return filepath",
        "",
        "",
        "class ChunkLoader:",
        "    \"\"\"",
        "    Loads and combines chunks to rebuild document state.",
        "",
        "    Usage:",
        "        loader = ChunkLoader(chunks_dir='corpus_chunks')",
        "        documents = loader.load_all()  # Returns {doc_id: content}",
        "",
        "        # Check if cache is valid",
        "        if loader.is_cache_valid('corpus_dev.pkl'):",
        "            # Load from pkl",
        "        else:",
        "            # Rebuild from documents",
        "    \"\"\"",
        "",
        "    def __init__(self, chunks_dir: str = 'corpus_chunks'):",
        "        self.chunks_dir = Path(chunks_dir)",
        "        self._chunks: List[Chunk] = []",
        "        self._documents: Dict[str, str] = {}",
        "        self._mtimes: Dict[str, float] = {}",
        "        self._loaded = False",
        "",
        "    def get_chunk_files(self) -> List[Path]:",
        "        \"\"\"Get all chunk files sorted by timestamp.\"\"\"",
        "        if not self.chunks_dir.exists():",
        "            return []",
        "",
        "        files = list(self.chunks_dir.glob('*.json'))",
        "        # Sort by filename (which starts with timestamp)",
        "        return sorted(files, key=lambda p: p.name)",
        "",
        "    def load_chunk(self, filepath: Path) -> Chunk:",
        "        \"\"\"Load a single chunk file.\"\"\"",
        "        with open(filepath, 'r', encoding='utf-8') as f:",
        "            data = json.load(f)",
        "        return Chunk.from_dict(data)",
        "",
        "    def load_all(self) -> Dict[str, str]:",
        "        \"\"\"",
        "        Load all chunks and replay operations to get current document state.",
        "",
        "        Returns:",
        "            Dictionary mapping doc_id to content.",
        "        \"\"\"",
        "        if self._loaded:",
        "            return self._documents",
        "",
        "        self._chunks = []",
        "        self._documents = {}",
        "        self._mtimes = {}",
        "",
        "        for filepath in self.get_chunk_files():",
        "            chunk = self.load_chunk(filepath)",
        "            self._chunks.append(chunk)",
        "",
        "            # Replay operations",
        "            for op in chunk.operations:",
        "                if op.op == 'add':",
        "                    self._documents[op.doc_id] = op.content",
        "                    if op.mtime:",
        "                        self._mtimes[op.doc_id] = op.mtime",
        "                elif op.op == 'modify':",
        "                    self._documents[op.doc_id] = op.content",
        "                    if op.mtime:",
        "                        self._mtimes[op.doc_id] = op.mtime",
        "                elif op.op == 'delete':",
        "                    self._documents.pop(op.doc_id, None)",
        "                    self._mtimes.pop(op.doc_id, None)",
        "",
        "        self._loaded = True",
        "        return self._documents",
        "",
        "    def get_documents(self) -> Dict[str, str]:",
        "        \"\"\"Get loaded documents (calls load_all if needed).\"\"\"",
        "        if not self._loaded:",
        "            self.load_all()",
        "        return self._documents",
        "",
        "    def get_mtimes(self) -> Dict[str, float]:",
        "        \"\"\"Get document modification times.\"\"\"",
        "        if not self._loaded:",
        "            self.load_all()",
        "        return self._mtimes",
        "",
        "    def get_chunks(self) -> List[Chunk]:",
        "        \"\"\"Get loaded chunks.\"\"\"",
        "        if not self._loaded:",
        "            self.load_all()",
        "        return self._chunks",
        "",
        "    def compute_hash(self) -> str:",
        "        \"\"\"",
        "        Compute hash of current document state.",
        "",
        "        Used to check if pkl cache is still valid.",
        "        \"\"\"",
        "        if not self._loaded:",
        "            self.load_all()",
        "",
        "        # Hash based on sorted (doc_id, content) pairs",
        "        hasher = hashlib.sha256()",
        "        for doc_id in sorted(self._documents.keys()):",
        "            hasher.update(doc_id.encode('utf-8'))",
        "            hasher.update(self._documents[doc_id].encode('utf-8'))",
        "",
        "        return hasher.hexdigest()[:16]",
        "",
        "    def is_cache_valid(self, cache_path: str, cache_hash_path: Optional[str] = None) -> bool:",
        "        \"\"\"",
        "        Check if pkl cache is valid for current chunk state.",
        "",
        "        Args:",
        "            cache_path: Path to pkl cache file",
        "            cache_hash_path: Path to hash file (defaults to cache_path + '.hash')",
        "",
        "        Returns:",
        "            True if cache exists and hash matches",
        "        \"\"\"",
        "        cache_file = Path(cache_path)",
        "        if not cache_file.exists():",
        "            return False",
        "",
        "        hash_file = Path(cache_hash_path or f\"{cache_path}.hash\")",
        "        if not hash_file.exists():",
        "            return False",
        "",
        "        try:",
        "            with open(hash_file, 'r') as f:",
        "                stored_hash = f.read().strip()",
        "",
        "            current_hash = self.compute_hash()",
        "            return stored_hash == current_hash",
        "        except (IOError, OSError):",
        "            return False",
        "",
        "    def save_cache_hash(self, cache_path: str, cache_hash_path: Optional[str] = None):",
        "        \"\"\"Save current document hash for cache validation.\"\"\"",
        "        hash_file = Path(cache_hash_path or f\"{cache_path}.hash\")",
        "        current_hash = self.compute_hash()",
        "",
        "        with open(hash_file, 'w') as f:",
        "            f.write(current_hash)",
        "",
        "    def get_stats(self) -> Dict[str, Any]:",
        "        \"\"\"Get statistics about loaded chunks.\"\"\"",
        "        if not self._loaded:",
        "            self.load_all()",
        "",
        "        total_ops = sum(len(c.operations) for c in self._chunks)",
        "        add_ops = sum(",
        "            1 for c in self._chunks",
        "            for op in c.operations if op.op == 'add'",
        "        )",
        "        modify_ops = sum(",
        "            1 for c in self._chunks",
        "            for op in c.operations if op.op == 'modify'",
        "        )",
        "        delete_ops = sum(",
        "            1 for c in self._chunks",
        "            for op in c.operations if op.op == 'delete'",
        "        )",
        "",
        "        return {",
        "            'chunk_count': len(self._chunks),",
        "            'document_count': len(self._documents),",
        "            'total_operations': total_ops,",
        "            'add_operations': add_ops,",
        "            'modify_operations': modify_ops,",
        "            'delete_operations': delete_ops,",
        "            'hash': self.compute_hash()",
        "        }",
        "",
        "",
        "class ChunkCompactor:",
        "    \"\"\"",
        "    Compacts multiple chunk files into a single file.",
        "",
        "    Usage:",
        "        compactor = ChunkCompactor(chunks_dir='corpus_chunks')",
        "        compactor.compact(before='2025-12-01')  # Compact old chunks",
        "        compactor.compact()  # Compact all chunks into one",
        "    \"\"\"",
        "",
        "    def __init__(self, chunks_dir: str = 'corpus_chunks'):",
        "        self.chunks_dir = Path(chunks_dir)",
        "",
        "    def compact(",
        "        self,",
        "        before: Optional[str] = None,",
        "        keep_recent: int = 0,",
        "        dry_run: bool = False",
        "    ) -> Dict[str, Any]:",
        "        \"\"\"",
        "        Compact chunks into a single chunk.",
        "",
        "        Args:",
        "            before: Only compact chunks before this date (YYYY-MM-DD)",
        "            keep_recent: Keep this many recent chunks uncompacted",
        "            dry_run: If True, don't actually compact, just report what would happen",
        "",
        "        Returns:",
        "            Statistics about the compaction",
        "        \"\"\"",
        "        loader = ChunkLoader(str(self.chunks_dir))",
        "        chunk_files = loader.get_chunk_files()",
        "",
        "        if not chunk_files:",
        "            return {'status': 'no_chunks', 'compacted': 0}",
        "",
        "        # Filter chunks to compact",
        "        to_compact = []",
        "        to_keep = []",
        "",
        "        for filepath in chunk_files:",
        "            filename = filepath.name",
        "            # Extract date from filename (YYYY-MM-DD_HH-MM-SS_...)",
        "            file_date = filename[:10]",
        "",
        "            should_compact = True",
        "",
        "            if before:",
        "                should_compact = file_date < before",
        "",
        "            if should_compact:",
        "                to_compact.append(filepath)",
        "            else:",
        "                to_keep.append(filepath)",
        "",
        "        # Keep recent chunks if requested",
        "        if keep_recent > 0 and len(to_compact) > keep_recent:",
        "            # Move some from to_compact to to_keep",
        "            to_keep = to_compact[-keep_recent:] + to_keep",
        "            to_compact = to_compact[:-keep_recent]",
        "",
        "        if not to_compact:",
        "            return {'status': 'nothing_to_compact', 'compacted': 0}",
        "",
        "        if dry_run:",
        "            return {",
        "                'status': 'dry_run',",
        "                'would_compact': len(to_compact),",
        "                'would_keep': len(to_keep),",
        "                'files_to_compact': [str(f) for f in to_compact]",
        "            }",
        "",
        "        # Load and merge chunks to compact",
        "        documents = {}",
        "        mtimes = {}",
        "",
        "        for filepath in to_compact:",
        "            chunk = loader.load_chunk(filepath)",
        "            for op in chunk.operations:",
        "                if op.op in ('add', 'modify'):",
        "                    documents[op.doc_id] = op.content",
        "                    if op.mtime:",
        "                        mtimes[op.doc_id] = op.mtime",
        "                elif op.op == 'delete':",
        "                    documents.pop(op.doc_id, None)",
        "                    mtimes.pop(op.doc_id, None)",
        "",
        "        # Create compacted chunk with all remaining documents as 'add' operations",
        "        writer = ChunkWriter(str(self.chunks_dir))",
        "        writer.timestamp = datetime.now().isoformat(timespec='seconds')",
        "        writer.session_id = 'compacted_' + uuid.uuid4().hex[:8]",
        "",
        "        for doc_id, content in sorted(documents.items()):",
        "            writer.add_document(doc_id, content, mtimes.get(doc_id))",
        "",
        "        # Save compacted chunk",
        "        compacted_path = None",
        "        if writer.has_operations():",
        "            compacted_path = writer.save()",
        "",
        "        # Delete old chunk files",
        "        for filepath in to_compact:",
        "            filepath.unlink()",
        "",
        "        return {",
        "            'status': 'compacted',",
        "            'compacted': len(to_compact),",
        "            'kept': len(to_keep),",
        "            'documents': len(documents),",
        "            'compacted_file': str(compacted_path) if compacted_path else None",
        "        }",
        "",
        "",
        "def get_changes_from_manifest(",
        "    current_files: Dict[str, float],",
        "    manifest: Dict[str, float]",
        ") -> Tuple[List[str], List[str], List[str]]:",
        "    \"\"\"",
        "    Compare current files to manifest to find changes.",
        "",
        "    Args:",
        "        current_files: Dict mapping file paths to modification times",
        "        manifest: Dict mapping file paths to last indexed modification times",
        "",
        "    Returns:",
        "        Tuple of (added, modified, deleted) file lists",
        "    \"\"\"",
        "    current_set = set(current_files.keys())",
        "    manifest_set = set(manifest.keys())",
        "",
        "    added = list(current_set - manifest_set)",
        "    deleted = list(manifest_set - current_set)",
        "",
        "    # Check for modified files",
        "    modified = []",
        "    for filepath in current_set & manifest_set:",
        "        if current_files[filepath] > manifest[filepath]:",
        "            modified.append(filepath)",
        "",
        "    return added, modified, deleted"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "cortical/layers.py",
      "function": "class HierarchicalLayer:",
      "start_line": 128,
      "lines_added": [
        "    def remove_minicolumn(self, content: str) -> bool:",
        "        \"\"\"",
        "        Remove a minicolumn from this layer.",
        "",
        "        Args:",
        "            content: The content key of the minicolumn to remove",
        "",
        "        Returns:",
        "            True if the minicolumn was found and removed, False otherwise",
        "        \"\"\"",
        "        if content not in self.minicolumns:",
        "            return False",
        "",
        "        col = self.minicolumns[content]",
        "        # Remove from ID index",
        "        if col.id in self._id_index:",
        "            del self._id_index[col.id]",
        "        # Remove from minicolumns dict",
        "        del self.minicolumns[content]",
        "        return True",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "",
        "        Args:",
        "            col_id: The minicolumn ID (e.g., \"L0_neural\")",
        "",
        "        Returns:",
        "            The Minicolumn if found, None otherwise",
        "        \"\"\"",
        "        content = self._id_index.get(col_id)",
        "        return self.minicolumns.get(content) if content else None",
        ""
      ],
      "context_after": [
        "    def column_count(self) -> int:",
        "        \"\"\"Return the number of minicolumns in this layer.\"\"\"",
        "        return len(self.minicolumns)",
        "    ",
        "    def total_connections(self) -> int:",
        "        \"\"\"Return total number of lateral connections in this layer.\"\"\"",
        "        return sum(col.connection_count() for col in self.minicolumns.values())",
        "    ",
        "    def average_activation(self) -> float:",
        "        \"\"\"Calculate average activation across all minicolumns.\"\"\""
      ],
      "change_type": "add"
    },
    {
      "file": "cortical/processor.py",
      "function": "class CorticalTextProcessor:",
      "start_line": 353,
      "lines_added": [
        "    def remove_document(self, doc_id: str, verbose: bool = False) -> Dict[str, Any]:",
        "        \"\"\"",
        "        Remove a document from the corpus.",
        "",
        "        Removes the document and cleans up all references to it in the layers:",
        "        - Removes from documents dict and metadata",
        "        - Removes document minicolumn from Layer 3",
        "        - Removes doc_id from token and bigram document_ids sets",
        "        - Decrements occurrence counts appropriately",
        "        - Cleans up feedforward/feedback connections",
        "",
        "        Args:",
        "            doc_id: Document identifier to remove",
        "            verbose: Print progress messages",
        "",
        "        Returns:",
        "            Dict with removal statistics:",
        "                - found: Whether the document existed",
        "                - tokens_affected: Number of tokens that referenced this document",
        "                - bigrams_affected: Number of bigrams that referenced this document",
        "",
        "        Example:",
        "            >>> processor.remove_document(\"old_doc\")",
        "            {'found': True, 'tokens_affected': 42, 'bigrams_affected': 35}",
        "        \"\"\"",
        "        from .layers import CorticalLayer",
        "",
        "        if doc_id not in self.documents:",
        "            return {'found': False, 'tokens_affected': 0, 'bigrams_affected': 0}",
        "",
        "        if verbose:",
        "            print(f\"Removing document: {doc_id}\")",
        "",
        "        # Remove from documents and metadata",
        "        del self.documents[doc_id]",
        "        if doc_id in self.document_metadata:",
        "            del self.document_metadata[doc_id]",
        "",
        "        # Remove document minicolumn from Layer 3",
        "        layer3 = self.layers[CorticalLayer.DOCUMENTS]",
        "        doc_col = layer3.get_minicolumn(doc_id)",
        "        if doc_col:",
        "            # Get tokens/bigrams that were connected to this document",
        "            connected_ids = set(doc_col.feedforward_connections.keys())",
        "            layer3.remove_minicolumn(doc_id)",
        "",
        "        # Clean up token references in Layer 0",
        "        layer0 = self.layers[CorticalLayer.TOKENS]",
        "        tokens_affected = 0",
        "        for content, col in list(layer0.minicolumns.items()):",
        "            if doc_id in col.document_ids:",
        "                col.document_ids.discard(doc_id)",
        "                tokens_affected += 1",
        "",
        "                # Decrement occurrence count by per-doc count",
        "                if doc_id in col.doc_occurrence_counts:",
        "                    col.occurrence_count -= col.doc_occurrence_counts[doc_id]",
        "                    del col.doc_occurrence_counts[doc_id]",
        "",
        "                # Clean up feedback connections to document",
        "                doc_col_id = f\"L3_{doc_id}\"",
        "                if doc_col_id in col.feedback_connections:",
        "                    del col.feedback_connections[doc_col_id]",
        "",
        "        # Clean up bigram references in Layer 1",
        "        layer1 = self.layers[CorticalLayer.BIGRAMS]",
        "        bigrams_affected = 0",
        "        for content, col in list(layer1.minicolumns.items()):",
        "            if doc_id in col.document_ids:",
        "                col.document_ids.discard(doc_id)",
        "                bigrams_affected += 1",
        "",
        "                # Decrement occurrence count (approximate since we don't track per-doc for bigrams)",
        "                if doc_id in col.doc_occurrence_counts:",
        "                    col.occurrence_count -= col.doc_occurrence_counts[doc_id]",
        "                    del col.doc_occurrence_counts[doc_id]",
        "",
        "        # Mark all computations as stale",
        "        self._mark_all_stale()",
        "",
        "        # Invalidate query cache since corpus changed",
        "        if hasattr(self, '_query_expansion_cache'):",
        "            self._query_expansion_cache.clear()",
        "",
        "        if verbose:",
        "            print(f\"  Affected: {tokens_affected} tokens, {bigrams_affected} bigrams\")",
        "",
        "        return {",
        "            'found': True,",
        "            'tokens_affected': tokens_affected,",
        "            'bigrams_affected': bigrams_affected",
        "        }",
        "",
        "    def remove_documents_batch(",
        "        self,",
        "        doc_ids: List[str],",
        "        recompute: str = 'none',",
        "        verbose: bool = True",
        "    ) -> Dict[str, Any]:",
        "        \"\"\"",
        "        Remove multiple documents efficiently with single recomputation.",
        "",
        "        Args:",
        "            doc_ids: List of document identifiers to remove",
        "            recompute: Level of recomputation after removal:",
        "                - 'none': Just remove documents, mark computations stale",
        "                - 'tfidf': Recompute TF-IDF only",
        "                - 'full': Run full compute_all()",
        "            verbose: Print progress messages",
        "",
        "        Returns:",
        "            Dict with removal statistics:",
        "                - documents_removed: Number of documents actually removed",
        "                - documents_not_found: Number of doc_ids that didn't exist",
        "                - total_tokens_affected: Total tokens affected",
        "                - total_bigrams_affected: Total bigrams affected",
        "",
        "        Example:",
        "            >>> processor.remove_documents_batch([\"old1\", \"old2\", \"old3\"])",
        "        \"\"\"",
        "        removed = 0",
        "        not_found = 0",
        "        total_tokens = 0",
        "        total_bigrams = 0",
        "",
        "        if verbose:",
        "            print(f\"Removing {len(doc_ids)} documents...\")",
        "",
        "        for doc_id in doc_ids:",
        "            result = self.remove_document(doc_id, verbose=False)",
        "            if result['found']:",
        "                removed += 1",
        "                total_tokens += result['tokens_affected']",
        "                total_bigrams += result['bigrams_affected']",
        "            else:",
        "                not_found += 1",
        "",
        "        if verbose:",
        "            print(f\"  Removed: {removed}, Not found: {not_found}\")",
        "            print(f\"  Affected: {total_tokens} tokens, {total_bigrams} bigrams\")",
        "",
        "        # Perform recomputation",
        "        if recompute == 'tfidf':",
        "            if verbose:",
        "                print(\"Recomputing TF-IDF...\")",
        "            self.compute_tfidf(verbose=False)",
        "            self._mark_fresh(self.COMP_TFIDF)",
        "        elif recompute == 'full':",
        "            if verbose:",
        "                print(\"Running full recomputation...\")",
        "            self.compute_all(verbose=False)",
        "            self._stale_computations.clear()",
        "",
        "        return {",
        "            'documents_removed': removed,",
        "            'documents_not_found': not_found,",
        "            'total_tokens_affected': total_tokens,",
        "            'total_bigrams_affected': total_bigrams,",
        "            'recomputation': recompute",
        "        }",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "        if verbose:",
        "            print(\"Done.\")",
        "",
        "        return {",
        "            'documents_added': len(documents),",
        "            'total_tokens': total_tokens,",
        "            'total_bigrams': total_bigrams,",
        "            'recomputation': recompute",
        "        }",
        ""
      ],
      "context_after": [
        "    def recompute(",
        "        self,",
        "        level: str = 'stale',",
        "        verbose: bool = True",
        "    ) -> Dict[str, bool]:",
        "        \"\"\"",
        "        Recompute specified analysis levels.",
        "",
        "        Use this after adding documents with recompute='none' to batch",
        "        the recomputation step."
      ],
      "change_type": "add"
    },
    {
      "file": "docs/claude-usage.md",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "# Claude Usage Guide: Semantic Search System",
        "",
        "This guide is written specifically for Claude (AI agents) to understand how to effectively use the Cortical Text Processor's semantic search system when working with this codebase.",
        "",
        "## Overview",
        "",
        "The Cortical Text Processor can index and semantically search its own codebase, providing meaning-based retrieval instead of simple keyword matching. This guide explains how to use this capability strategically during development tasks.",
        "",
        "**Key principle:** The system finds code by understanding intent and concepts, not just exact keywords. \"Fetch\", \"get\", \"load\", and \"retrieve\" are treated as semantically similar.",
        "",
        "---",
        "",
        "## Table of Contents",
        "",
        "1. [When to Use Codebase-Search](#when-to-use-codebase-search)",
        "2. [When to Use Direct File Reading](#when-to-use-direct-file-reading)",
        "3. [Formulating Effective Search Queries](#formulating-effective-search-queries)",
        "4. [Understanding Search Results](#understanding-search-results)",
        "5. [When to Re-Index](#when-to-re-index)",
        "6. [Handling No Results](#handling-no-results)",
        "7. [Iterative Search Strategy](#iterative-search-strategy)",
        "8. [Query Expansion Leverage](#query-expansion-leverage)",
        "9. [System Limitations and Workarounds](#system-limitations-and-workarounds)",
        "10. [Common Code Query Patterns](#common-code-query-patterns)",
        "11. [Performance Considerations](#performance-considerations)",
        "",
        "---",
        "",
        "## When to Use Codebase-Search",
        "",
        "Use the **codebase-search** skill when you need to:",
        "",
        "### 1. Find implementations of concepts",
        "```",
        "\"How does PageRank algorithm work?\"",
        "\"How is TF-IDF computed?\"",
        "\"How are bigrams created?\"",
        "```",
        "",
        "The system will find relevant code passages even if your exact words don't match the implementation. For example, searching for \"importance scoring\" will find PageRank code.",
        "",
        "### 2. Locate functionality by intent",
        "```",
        "\"Where do we handle errors?\"",
        "\"Where do we validate input?\"",
        "\"Where do we tokenize text?\"",
        "```",
        "",
        "Intent-based queries parse the natural language structure and find code implementing that action.",
        "",
        "### 3. Understand relationships between components",
        "```",
        "\"What connects to the tokenizer?\"",
        "\"How do layers interact?\"",
        "\"What uses layer 2 concepts?\"",
        "```",
        "",
        "The system understands component relationships through graph connections.",
        "",
        "### 4. Explore semantic concepts across the codebase",
        "```",
        "\"Neural network terminology\"",
        "\"Graph algorithms\"",
        "\"Performance optimization patterns\"",
        "```",
        "",
        "Query expansion automatically includes related terms, finding all discussions of a concept.",
        "",
        "### 5. When you need to understand code context",
        "You want to see how something is actually implemented, not just read the file directly. The search system gives you relevant passages in context.",
        "",
        "**Cost consideration:** Search is fast (~1 second for typical queries), so it's efficient for exploratory research.",
        "",
        "---",
        "",
        "## When to Use Direct File Reading",
        "",
        "Use **direct file reading** (Read tool) when you:",
        "",
        "### 1. Know the exact file location",
        "If you already know the file path (e.g., `cortical/processor.py`), reading directly is faster than searching.",
        "",
        "### 2. Need the complete file context",
        "When you need to see the entire file structure, imports, and all methods in a class, reading the file is more efficient than multiple targeted searches.",
        "",
        "### 3. Are implementing a pattern you've already found",
        "After a search tells you the file location, switch to direct reading to implement your changes.",
        "",
        "### 4. Need accurate line numbers for edits",
        "While search provides file:line references, reading the file confirms the exact content at those lines.",
        "",
        "### 5. The concept is very common",
        "If the concept appears frequently (like \"process\" or \"handle\"), search may return many results. Direct reading is faster when you know where to look.",
        "",
        "**Workflow:** Search → Find file → Read file → Implement",
        "",
        "---",
        "",
        "## Formulating Effective Search Queries",
        "",
        "### Query Structure",
        "",
        "The system parses queries into three components:",
        "",
        "1. **Question word** (optional): \"where\", \"how\", \"what\", \"why\" → affects intent",
        "2. **Action verb** (optional): \"handle\", \"process\", \"create\", \"validate\" → narrows scope",
        "3. **Subject**: The main concept you're searching for",
        "",
        "Examples:",
        "- \"where do we validate input?\" → Intent: location, Action: validate, Subject: input",
        "- \"how are bigrams created?\" → Intent: implementation, Action: create, Subject: bigrams",
        "- \"PageRank algorithm\" → Intent: general, Subject: PageRank algorithm",
        "",
        "### Writing Effective Queries",
        "",
        "**✓ DO:** Use natural language as you would ask a colleague",
        "```",
        "\"How does query expansion find related terms?\"",
        "\"Where do we compute document relevance?\"",
        "\"What's the structure of a minicolumn?\"",
        "```",
        "",
        "**✓ DO:** Include multiple related terms",
        "```",
        "\"PageRank importance scoring algorithm\" (better than just \"PageRank\")",
        "\"TF-IDF term weighting relevance\" (better than just \"TF-IDF\")",
        "```",
        "",
        "**✓ DO:** Use intent words",
        "```",
        "\"Find implementations of label propagation\"",
        "\"Locate the tokenizer code\"",
        "\"Show me how errors are handled\"",
        "```",
        "",
        "**✗ DON'T:** Use only exact technical names without context",
        "```",
        "\"L0\" (too abstract - use \"token layer\" instead)",
        "\"col\" (use \"minicolumn\" or \"column\")",
        "```",
        "",
        "**✗ DON'T:** Use implementation details you're not sure about",
        "```",
        "\"Use lateral_connections\" (search for the concept instead: \"related terms\")",
        "\"_id_index lookup\" (search for: \"ID lookup performance\")",
        "```",
        "",
        "**✗ DON'T:** Search for very common words alone",
        "```",
        "\"the\" or \"and\" (these appear everywhere)",
        "\"layer\" (almost every file mentions layers - add context: \"layer connections\")",
        "```",
        "",
        "### Query Length",
        "",
        "- **Short queries (1-3 words):** Fast, but may return many results",
        "  - Good for: \"PageRank\", \"stemming\", \"TF-IDF\"",
        "  - Problem: High recall, may need filtering",
        "",
        "- **Medium queries (4-6 words):** Optimal for most cases",
        "  - Good for: \"how bigrams are created\", \"Layer 0 token structure\"",
        "  - Sweet spot for precision and recall",
        "",
        "- **Long queries (7+ words):** Very specific, low recall",
        "  - Good for: Complete question phrases",
        "  - Problem: May miss results if wording doesn't match docs",
        "",
        "**Best practice:** Start with 4-5 word queries; adjust based on results.",
        "",
        "---",
        "",
        "## Understanding Search Results",
        "",
        "### Result Format",
        "",
        "Each result shows:",
        "",
        "```",
        "[N] cortical/processor.py:1265",
        "    Score: 0.847",
        "  - Passage text showing relevant code",
        "  - Up to 5 lines displayed by default",
        "```",
        "",
        "### Score Interpretation",
        "",
        "Scores range from 0.0 to 1.0:",
        "",
        "| Score | Meaning | What to do |",
        "|-------|---------|-----------|",
        "| 0.9-1.0 | Excellent match | This is what you're looking for |",
        "| 0.75-0.89 | Strong match | Very relevant, likely useful |",
        "| 0.6-0.74 | Good match | Relevant but may need context |",
        "| 0.45-0.59 | Weak match | May be tangentially related |",
        "| <0.45 | Poor match | Likely noise, but sometimes useful |",
        "",
        "**Note:** Scores depend on query quality and corpus structure. A 0.75 for a common topic may be more relevant than a 0.95 for a niche query.",
        "",
        "### File:Line References",
        "",
        "The format `filename:linenumber` tells you:",
        "- Which file to examine",
        "- Approximately where to look (line number may be off by ±10 lines due to chunking)",
        "",
        "**Action:** When you get a file:line reference:",
        "1. Use Read tool on that file",
        "2. Look around the suggested line (±5 lines on each side)",
        "3. If not found, search again with different terms",
        "",
        "### Passage Text",
        "",
        "The system shows relevant passages of code in context:",
        "",
        "- **In brief mode** (default): First 5 lines of the passage",
        "- **In verbose mode** (`--verbose` flag): Up to 10 lines",
        "",
        "**Interpreting passages:**",
        "- Look for function definitions, class declarations, and key logic",
        "- Passages may be partial—read the full file for complete understanding",
        "- Comments in passages are usually significant (the system ranks them highly)",
        "",
        "---",
        "",
        "## When to Re-Index",
        "",
        "The semantic search uses a pre-built index (`corpus_dev.pkl`) created from your codebase. It's not real-time—it reflects the state when the index was last built.",
        "",
        "### Use corpus-indexer After:",
        "",
        "**1. You make code changes** (Most important)",
        "```",
        "- Add a new function",
        "- Modify algorithm logic",
        "- Change class structure",
        "- Add new documentation",
        "```",
        "",
        "**When:** Use `--incremental` flag for speed (1-2 seconds vs 2-3 seconds for full rebuild)",
        "```python",
        "# In your task: \"Use corpus-indexer with --incremental flag\"",
        "```",
        "",
        "**2. You add new files**",
        "The indexer automatically detects new files in `cortical/`, `tests/`, and `docs/`.",
        "",
        "**When:** After adding `new_feature.py` or `test_new_feature.py`",
        "",
        "**3. Major refactoring**",
        "If you restructure multiple files, use `--force` flag to ensure clean rebuild.",
        "",
        "### When Index Staleness Matters",
        "",
        "Search results won't reflect changes until re-indexing. This is fine for:",
        "- Reading old code",
        "- Understanding historical implementation",
        "- Learning the architecture",
        "",
        "This is problematic for:",
        "- Verifying your own changes are searchable",
        "- Finding newly added functionality",
        "- Debugging code you just wrote",
        "",
        "### Index Staleness Detection",
        "",
        "Before using search, check if the index is stale:",
        "",
        "```bash",
        "# Check what would change",
        "python scripts/index_codebase.py --status",
        "```",
        "",
        "If files changed since last index, results may be out of date.",
        "",
        "---",
        "",
        "## Handling No Results",
        "",
        "When a search returns no results, try these strategies in order:",
        "",
        "### Strategy 1: Broaden Your Query",
        "",
        "**Narrow query with no results:**",
        "```",
        "\"compute_semantic_pagerank with damping factor\"",
        "```",
        "",
        "**Broadened version:**",
        "```",
        "\"PageRank algorithm\"",
        "```",
        "",
        "**Action:** Remove specific implementation details and search for the concept.",
        "",
        "### Strategy 2: Use Synonym/Related Terms",
        "",
        "**Query with no results:**",
        "```",
        "\"fetch documents from corpus\"",
        "```",
        "",
        "**Synonym version:**",
        "```",
        "\"retrieve documents relevance\"",
        "```",
        "",
        "**Action:** Replace implementation-specific words with general synonyms.",
        "",
        "### Strategy 3: Search Different Layers",
        "",
        "**Technical terms not found:**",
        "```",
        "\"minicolumn lateral connection weight\"",
        "```",
        "",
        "**Higher-level concept:**",
        "```",
        "\"related terms word associations\"",
        "```",
        "",
        "**Action:** Describe the concept instead of the implementation.",
        "",
        "### Strategy 4: Check if Index Exists",
        "",
        "**Problem:** \"Error: Corpus file not found\"",
        "",
        "**Solution:**",
        "```bash",
        "python scripts/index_codebase.py",
        "```",
        "",
        "This creates `corpus_dev.pkl` (~2-3 seconds).",
        "",
        "### Strategy 5: Use Direct File Search",
        "",
        "If semantic search fails, fall back to:",
        "",
        "1. **Grep search** for exact keywords:",
        "   ```",
        "   grep -r \"function_name\" cortical/",
        "   ```",
        "",
        "2. **Direct file reading** if you know the likely file:",
        "   ```",
        "   Read cortical/analysis.py",
        "   ```",
        "",
        "### Strategy 6: Check Query Expansion",
        "",
        "Use `--expand` flag to see what the system is actually searching for:",
        "",
        "```bash",
        "python scripts/search_codebase.py \"your query\" --expand",
        "```",
        "",
        "This shows the expanded terms. If expansion is incorrect, try a different query.",
        "",
        "### Why No Results Happen",
        "",
        "1. **Concept doesn't exist in codebase** - You're asking for something that isn't implemented",
        "2. **Different terminology** - The codebase uses different words than you're using",
        "3. **Index is stale** - Recent changes haven't been indexed",
        "4. **Query too specific** - You're combining terms that don't co-occur",
        "5. **Implementation detail** - You're searching for internal variable names instead of the concept",
        "",
        "---",
        "",
        "## Iterative Search Strategy",
        "",
        "When researching a complex topic, use iterative searching:",
        "",
        "### Iteration 1: Broad Exploration",
        "```",
        "Query: \"PageRank\"",
        "Goal: Find where PageRank is implemented",
        "Action: Choose the most relevant result file",
        "Result: cortical/analysis.py:22",
        "```",
        "",
        "### Iteration 2: Find Related Components",
        "```",
        "Query: \"how does PageRank use connections\"",
        "Goal: Understand what PageRank operates on",
        "Action: Search results show \"lateral connections\" and \"weighted edges\"",
        "Result: Learn that PageRank uses graph structure",
        "```",
        "",
        "### Iteration 3: Understand Integration",
        "```",
        "Query: \"where is PageRank computed in processor\"",
        "Goal: Find where PageRank is called",
        "Action: Results show processor.py lines that trigger compute_pagerank",
        "Result: Understand when PageRank runs (after corpus changes)",
        "```",
        "",
        "### Iteration 4: Deep Dive",
        "```",
        "Query: \"PageRank damping factor convergence\"",
        "Goal: Understand algorithm parameters",
        "Action: Read the full analysis.py function",
        "Result: Understand implementation details",
        "```",
        "",
        "**Pattern:** Start broad → narrow down → deepen understanding → read full files",
        "",
        "---",
        "",
        "## Query Expansion Leverage",
        "",
        "The system automatically expands queries using:",
        "",
        "1. **Lateral connections** - Terms frequently appearing together",
        "2. **Concept clusters** - Semantic groupings",
        "3. **Word variants** - Plurals, stems, related forms",
        "4. **Code concepts** - Programming synonyms (get/fetch/load)",
        "",
        "### How to Leverage Expansion",
        "",
        "**1. Use umbrella terms**",
        "",
        "Rather than searching for specific functions:",
        "```",
        "# Instead of: \"expand_query\"",
        "# Search for: \"query expansion\"",
        "```",
        "",
        "The system will automatically find `expand_query`, `get_expanded_query_terms`, etc.",
        "",
        "**2. Use related terminology**",
        "",
        "Expansion finds connections:",
        "```",
        "\"authentication\" → also finds \"login\", \"credential\", \"token\", \"session\"",
        "\"fetch\" → also finds \"get\", \"load\", \"retrieve\", \"access\"",
        "```",
        "",
        "**3. Check what's actually being searched**",
        "",
        "Use `--expand` flag:",
        "```bash",
        "python scripts/search_codebase.py \"PageRank\" --expand",
        "```",
        "",
        "Output shows:",
        "```",
        "pagerank: 1.000",
        "importance: 0.847",
        "score: 0.812",
        "rank: 0.791",
        "...",
        "```",
        "",
        "These are the actual terms being searched.",
        "",
        "**4. Add expansion hints to queries**",
        "",
        "If expansion misses terms, add them explicitly:",
        "```",
        "# Instead of: \"PageRank\"",
        "# Try: \"PageRank importance scoring algorithm\"",
        "```",
        "",
        "Now expansion includes more related terms.",
        "",
        "### Expansion Limitations",
        "",
        "Expansion works well for:",
        "- Common terms (appear in many documents)",
        "- Concepts with multiple discussions",
        "- Well-connected terms in the knowledge graph",
        "",
        "Expansion works poorly for:",
        "- Rare specialized terms (appear in 1-2 documents)",
        "- Very new features (not yet well-connected)",
        "- Acronyms (expansion may not handle well)",
        "",
        "---",
        "",
        "## System Limitations and Workarounds",
        "",
        "### Limitation 1: Exact Matches Don't Always Score Highest",
        "",
        "**Problem:** When you search for a function name exactly, variations sometimes score higher.",
        "",
        "```",
        "Query: \"find_documents_for_query\"",
        "Top result: \"fast_find_documents\" (unrelated function)",
        "```",
        "",
        "**Reason:** The system ranks by relevance semantically, not by exact match.",
        "",
        "**Workaround:** Read the file you found or refine your query:",
        "```",
        "\"find_documents relevance scoring\"",
        "```",
        "",
        "### Limitation 2: Code Structure Queries May Miss Abstract Concepts",
        "",
        "**Problem:** Searching for the structure of a data type:",
        "```",
        "\"what fields does Minicolumn have\"",
        "```",
        "",
        "May not find the class definition as well as you'd hope.",
        "",
        "**Reason:** The definition doesn't discuss relationships; it just declares fields.",
        "",
        "**Workaround:** Search for the concept instead:",
        "```",
        "\"minicolumn structure representation\"",
        "```",
        "",
        "Or use direct file reading for data structure definitions:",
        "```",
        "Read cortical/minicolumn.py",
        "```",
        "",
        "### Limitation 3: Semantic Similarity Can Be Too Broad",
        "",
        "**Problem:** Searching for common concepts returns too many results:",
        "```",
        "Query: \"connection\"",
        "Result: Returns all mentions of \"connections\" (hundreds)",
        "```",
        "",
        "**Reason:** \"Connection\" is a core concept mentioned everywhere.",
        "",
        "**Workaround:** Be more specific:",
        "```",
        "\"lateral connections co-occurrence\"",
        "\"feedforward connections hierarchy\"",
        "```",
        "",
        "### Limitation 4: Fast Mode Only Returns Documents, Not Passages",
        "",
        "**Problem:** When using `--fast` flag, you only get file names, not specific passages.",
        "",
        "```bash",
        "python scripts/search_codebase.py \"PageRank\" --fast",
        "# Returns: cortical/analysis.py:1 (without specific passage)",
        "```",
        "",
        "**Reason:** Fast mode skips passage extraction for speed (~2-3x faster).",
        "",
        "**Workaround:** Use without `--fast` for specific passages, or read the file directly after getting the filename.",
        "",
        "### Limitation 5: Index Doesn't Cover Git History",
        "",
        "**Problem:** You can't search for how code looked before changes.",
        "",
        "**Reason:** The index is built from current files only.",
        "",
        "**Workaround:** Use git history for temporal queries:",
        "```bash",
        "git log -p cortical/query.py | grep \"function_name\"",
        "```",
        "",
        "### Limitation 6: Documentation May Be Outdated",
        "",
        "**Problem:** Docs in the index reflect what was written, not necessarily what code actually does.",
        "",
        "```",
        "Query: \"how layer computation works\"",
        "Result: May find outdated documentation",
        "```",
        "",
        "**Reason:** Docs and code can drift.",
        "",
        "**Workaround:** Verify by reading the actual code after finding relevant documentation.",
        "",
        "### Limitation 7: Very New Code May Not Be Discoverable",
        "",
        "**Problem:** Code you just wrote won't be found until re-indexing.",
        "",
        "**Workaround:** Re-index with `--incremental` after writing code:",
        "```bash",
        "python scripts/index_codebase.py --incremental",
        "```",
        "",
        "---",
        "",
        "## Common Code Query Patterns",
        "",
        "### Finding Algorithm Implementations",
        "",
        "**Goal:** Understand how a specific algorithm works",
        "",
        "```",
        "\"PageRank importance scoring\"",
        "\"TF-IDF term weighting\"",
        "\"label propagation clustering\"",
        "```",
        "",
        "**What to expect:** Functions implementing the algorithm, parameter documentation",
        "",
        "### Finding Bug Locations",
        "",
        "**Goal:** Locate where a bug might be",
        "",
        "```",
        "\"bigram separator space\" (if debugging bigram issues)",
        "\"layer ID index lookup\" (if debugging lookups)",
        "\"tokenizer stemming\" (if debugging tokenization)",
        "```",
        "",
        "**What to expect:** Code that handles the buggy component",
        "",
        "### Finding Integration Points",
        "",
        "**Goal:** Understand how components connect",
        "",
        "```",
        "\"where PageRank results used\"",
        "\"TF-IDF score returned\"",
        "\"minicolumn connected to layer\"",
        "```",
        "",
        "**What to expect:** Code that calls or uses the component",
        "",
        "### Finding Test Patterns",
        "",
        "**Goal:** Understand how to test a feature",
        "",
        "```",
        "\"test PageRank computation\"",
        "\"unittest layer structure\"",
        "\"assert results valid\"",
        "```",
        "",
        "**What to expect:** Test files showing testing patterns",
        "",
        "### Finding Performance Optimizations",
        "",
        "**Goal:** Understand efficiency strategies",
        "",
        "```",
        "\"fast search document only\"",
        "\"incremental indexing changes\"",
        "\"O(1) ID lookup cache\"",
        "```",
        "",
        "**What to expect:** Code with performance-related comments/optimization",
        "",
        "### Finding Data Structure Details",
        "",
        "**Goal:** Understand internal representations",
        "",
        "```",
        "\"minicolumn connections fields\"",
        "\"layer minicolumns dictionary\"",
        "\"document ID format\"",
        "```",
        "",
        "**What to expect:** Class definitions, docstrings explaining structure",
        "",
        "---",
        "",
        "## Performance Considerations",
        "",
        "### When to Use Each Search Method",
        "",
        "| Method | Speed | Use Case |",
        "|--------|-------|----------|",
        "| Normal search | 1-2s | Default, accurate passage extraction |",
        "| Fast search (`--fast`) | 0.2-0.5s | Need just documents, not passages |",
        "| Direct file read | <0.1s | Know exact file location |",
        "| Interactive mode | 0.5-1s per query | Exploratory research sessions |",
        "",
        "### Batching Queries",
        "",
        "If you have multiple searches, use interactive mode instead of multiple CLI calls:",
        "",
        "```bash",
        "python scripts/search_codebase.py --interactive",
        "# Then issue multiple queries in one session",
        "# More efficient than multiple command calls",
        "```",
        "",
        "### Caching Expansion",
        "",
        "If you're searching for related terms repeatedly:",
        "",
        "```python",
        "# In code, use:",
        "processor.expand_query_cached(query)",
        "```",
        "",
        "Instead of:",
        "```python",
        "processor.expand_query(query)",
        "```",
        "",
        "The cached version uses LRU cache for repeated queries.",
        "",
        "### Index Size Trade-offs",
        "",
        "**Fast mode (default):**",
        "- Smaller index (~30MB)",
        "- Faster indexing (2-3 seconds)",
        "- Fast search (0.5-1s)",
        "- No bigram connections, no concept analysis",
        "",
        "**Full analysis mode:**",
        "- Larger index (~100+MB)",
        "- Slow indexing (10+ minutes)",
        "- More comprehensive results",
        "- Use only when you need deep exploration",
        "",
        "For normal development: **Use fast mode**. Use `--full-analysis` only for research sessions.",
        "",
        "---",
        "",
        "## Decision Tree: How to Find Code",
        "",
        "```",
        "Do you know the exact file?",
        "├─ YES: Use Read tool directly",
        "└─ NO: Continue...",
        "",
        "Do you know what to search for?",
        "├─ YES: Use codebase-search with query",
        "└─ NO: Continue...",
        "",
        "Is it a well-known component?",
        "├─ YES: Search for the component name",
        "└─ NO: Continue...",
        "",
        "Can you describe what it does?",
        "├─ YES: Search for the concept/behavior",
        "└─ NO: Use grep or browse manually",
        "",
        "Is the search too slow?",
        "├─ YES: Use --fast flag or break into narrower queries",
        "└─ NO: Proceed normally",
        "",
        "Did you get results?",
        "├─ YES: Pick the best match, read full file",
        "└─ NO: Go to \"Handling No Results\" section",
        "```",
        "",
        "---",
        "",
        "## Summary for Claude",
        "",
        "When working with this codebase:",
        "",
        "1. **Start with search, not reading** - The semantic search is fast and gives you context",
        "2. **Use natural language queries** - Write queries as you would ask a colleague",
        "3. **Trust the expansion** - The system automatically finds related terms",
        "4. **Check scores, but don't over-interpret** - High scores are good, but context matters more",
        "5. **Re-index after changes** - Always use `--incremental` after making code changes",
        "6. **Fall back to direct reading** - Once you have a file:line reference, switch to Read",
        "7. **Broaden when stuck** - If search returns nothing, remove implementation details and try again",
        "8. **Use iterative refinement** - Start broad, then narrow based on what you learn",
        "",
        "The semantic search system is designed to accelerate your understanding of the codebase by making it searchable by meaning, not just keywords. Use it as your primary tool for exploration and learning.",
        "",
        "---",
        "",
        "*Last updated: 2025-12-10*",
        "*For the Cortical Text Processor codebase*"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "docs/cookbook.md",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "# Cortical Text Processor Cookbook",
        "",
        "A practical guide to common patterns and recipes for using the Cortical Text Processor effectively.",
        "",
        "---",
        "",
        "## Table of Contents",
        "",
        "1. [Document Processing Patterns](#document-processing-patterns)",
        "2. [Search Optimization Recipes](#search-optimization-recipes)",
        "3. [Corpus Maintenance Patterns](#corpus-maintenance-patterns)",
        "4. [Query Expansion Tuning](#query-expansion-tuning)",
        "5. [Clustering Configuration](#clustering-configuration)",
        "6. [Performance Optimization](#performance-optimization)",
        "7. [RAG Integration Patterns](#rag-integration-patterns)",
        "",
        "---",
        "",
        "## Document Processing Patterns",
        "",
        "### Recipe 1: Batch Processing (Recommended)",
        "",
        "**When to use:** Adding multiple documents at once (initial corpus loading, bulk imports).",
        "",
        "```python",
        "from cortical import CorticalTextProcessor",
        "",
        "processor = CorticalTextProcessor()",
        "",
        "# Prepare documents as list of (doc_id, content, metadata) tuples",
        "documents = [",
        "    (\"doc1\", \"Neural networks process information.\", {\"source\": \"book1\"}),",
        "    (\"doc2\", \"Deep learning enables pattern recognition.\", {\"source\": \"book1\"}),",
        "    (\"doc3\", \"Machine learning algorithms learn from data.\", {\"source\": \"book2\"}),",
        "]",
        "",
        "# Add all documents and recompute once",
        "stats = processor.add_documents_batch(",
        "    documents,",
        "    recompute='full',  # 'full', 'tfidf', or 'none'",
        "    verbose=True",
        ")",
        "",
        "print(f\"Added {stats['documents_added']} documents\")",
        "```",
        "",
        "**Expected outcome:**",
        "- Single recomputation pass instead of per-document recomputation",
        "- ~3-5x faster than calling `process_document()` in a loop",
        "",
        "**Recomputation options:**",
        "- `recompute='full'`: Slowest, most accurate (includes all graph algorithms)",
        "- `recompute='tfidf'`: Fast, good for search quality",
        "- `recompute='none'`: Fastest, but computations marked stale",
        "",
        "---",
        "",
        "### Recipe 2: Incremental Updates (Live Systems)",
        "",
        "**When to use:** Adding documents to an already-built corpus (RAG systems, streaming data).",
        "",
        "```python",
        "# Start with existing corpus",
        "processor = CorticalTextProcessor.load(\"corpus.pkl\")",
        "",
        "# Add new document without full recomputation",
        "processor.add_document_incremental(",
        "    \"new_doc\",",
        "    \"New document content.\",",
        "    metadata={\"timestamp\": \"2025-12-10\"},",
        "    recompute='tfidf'  # Only recompute TF-IDF for search quality",
        ")",
        "",
        "# Later: full recomputation when needed",
        "processor.recompute(level='full', verbose=True)",
        "```",
        "",
        "---",
        "",
        "### Recipe 3: Document Removal",
        "",
        "**When to use:** Delete outdated documents, remove duplicates.",
        "",
        "```python",
        "# Remove single document",
        "result = processor.remove_document(\"old_doc\", verbose=True)",
        "print(f\"Tokens affected: {result['tokens_affected']}\")",
        "",
        "# Remove multiple documents efficiently",
        "doc_ids_to_remove = [\"old_doc1\", \"old_doc2\", \"old_doc3\"]",
        "result = processor.remove_documents_batch(",
        "    doc_ids_to_remove,",
        "    recompute='tfidf',",
        "    verbose=True",
        ")",
        "```",
        "",
        "---",
        "",
        "## Search Optimization Recipes",
        "",
        "### Recipe 4: Choosing the Right Search Method",
        "",
        "**Decision tree:**",
        "",
        "```",
        "Searching repeatedly on same corpus?",
        "├─ YES → fast_find_documents() or build_search_index()",
        "└─ NO  → find_documents_for_query()",
        "",
        "Need text passages for RAG?",
        "├─ YES → find_passages_for_query()",
        "└─ NO  → find_documents_for_query()",
        "",
        "Large corpus (1000+ docs)?",
        "└─ YES → fast_find_documents() for ~2-3x speedup",
        "```",
        "",
        "---",
        "",
        "### Recipe 5: Fast Document Search",
        "",
        "**When to use:** Large corpora, need sub-100ms response time.",
        "",
        "```python",
        "# Fast search with candidate filtering",
        "results = processor.fast_find_documents(",
        "    \"neural networks\",",
        "    top_n=5,",
        "    candidate_multiplier=3,  # 5 * 3 = 15 candidates examined",
        "    use_code_concepts=True   # Enable for code search",
        ")",
        "",
        "for doc_id, score in results:",
        "    print(f\"{doc_id}: {score:.3f}\")",
        "```",
        "",
        "**Tuning `candidate_multiplier`:**",
        "- `1`: Aggressive (may miss relevant documents)",
        "- `3`: Balanced (recommended)",
        "- `5`: Conservative (slower but higher recall)",
        "",
        "---",
        "",
        "### Recipe 6: Pre-Built Search Index (Fastest)",
        "",
        "**When to use:** Repeated searching on stable corpus.",
        "",
        "```python",
        "# Build index once",
        "index = processor.build_search_index()",
        "",
        "# Use for fast searches",
        "queries = [\"neural networks\", \"machine learning\", \"deep learning\"]",
        "for query in queries:",
        "    results = processor.search_with_index(query, index, top_n=5)",
        "    print(f\"{query}: {len(results)} results\")",
        "```",
        "",
        "**Note:** Rebuild index after `add_documents_batch()` or `remove_document()`.",
        "",
        "---",
        "",
        "### Recipe 7: Passage Retrieval for RAG",
        "",
        "**When to use:** Building retrieval-augmented generation systems.",
        "",
        "```python",
        "results = processor.find_passages_for_query(",
        "    \"neural network training\",",
        "    top_n=5,",
        "    chunk_size=512,      # Characters per chunk",
        "    overlap=128,         # Overlap between chunks",
        "    use_expansion=True",
        ")",
        "",
        "# Results: (passage_text, doc_id, start_char, end_char, score)",
        "for passage, doc_id, start, end, score in results:",
        "    print(f\"[{doc_id}:{start}-{end}] Score: {score:.3f}\")",
        "    print(passage[:100] + \"...\")",
        "```",
        "",
        "**Chunk size tuning:**",
        "- `256`: Small, precise passages",
        "- `512`: Balanced (recommended)",
        "- `1024`: Large, more context",
        "",
        "---",
        "",
        "## Corpus Maintenance Patterns",
        "",
        "### Recipe 8: Detecting Stale Computations",
        "",
        "**When to use:** Understand what needs recomputation after changes.",
        "",
        "```python",
        "# Check what's stale",
        "stale = processor.get_stale_computations()",
        "print(f\"Stale: {stale}\")",
        "",
        "if 'tfidf' in stale:",
        "    print(\"TF-IDF scores are outdated - search quality affected\")",
        "    processor.compute_tfidf(verbose=True)",
        "",
        "if 'pagerank' in stale:",
        "    print(\"PageRank scores are outdated\")",
        "    processor.compute_importance(verbose=True)",
        "```",
        "",
        "---",
        "",
        "### Recipe 9: Save and Load Corpus",
        "",
        "**When to use:** Persist trained corpus for deployment.",
        "",
        "```python",
        "# Build and save",
        "processor = CorticalTextProcessor()",
        "processor.add_documents_batch(documents, recompute='full')",
        "processor.save(\"production_corpus.pkl\", verbose=True)",
        "",
        "# Load in production",
        "loaded = CorticalTextProcessor.load(\"production_corpus.pkl\")",
        "results = loaded.find_documents_for_query(\"query\")",
        "```",
        "",
        "---",
        "",
        "## Query Expansion Tuning",
        "",
        "### Recipe 10: Understanding Expansion",
        "",
        "```python",
        "# See what expansion adds",
        "expanded = processor.expand_query(\"neural\", max_expansions=10)",
        "",
        "print(\"Original term: neural\")",
        "print(\"Expanded with:\")",
        "for term, weight in sorted(expanded.items(), key=lambda x: -x[1]):",
        "    if term != \"neural\":",
        "        print(f\"  {term}: {weight:.3f}\")",
        "```",
        "",
        "**Expansion sources:**",
        "- **Lateral connections** (0.6x): Terms appearing near query term",
        "- **Concept membership** (0.4x): Terms in same semantic cluster",
        "- **Code concepts** (0.6x): Programming synonyms (get/fetch/load)",
        "",
        "---",
        "",
        "### Recipe 11: Tuning Expansion Parameters",
        "",
        "```python",
        "# Conservative expansion (higher precision)",
        "conservative = processor.expand_query(",
        "    \"neural networks\",",
        "    max_expansions=3,",
        "    use_variants=False",
        ")",
        "",
        "# Aggressive expansion (higher recall)",
        "aggressive = processor.expand_query(",
        "    \"neural networks\",",
        "    max_expansions=20,",
        "    use_variants=True,",
        "    use_code_concepts=True",
        ")",
        "",
        "# Balanced (recommended)",
        "balanced = processor.expand_query(",
        "    \"neural networks\",",
        "    max_expansions=10,",
        "    use_variants=True",
        ")",
        "```",
        "",
        "---",
        "",
        "### Recipe 12: Multi-Hop Expansion",
        "",
        "**When to use:** Discover distantly related terms through semantic relations.",
        "",
        "```python",
        "# Extract semantic relations first",
        "processor.extract_corpus_semantics()",
        "",
        "# Multi-hop expansion",
        "expanded = processor.expand_query_multihop(",
        "    \"neural\",",
        "    max_hops=2,         # Follow 2 relation hops",
        "    max_expansions=15,",
        "    decay_factor=0.5    # Weight decreases per hop",
        ")",
        "```",
        "",
        "---",
        "",
        "## Clustering Configuration",
        "",
        "### Recipe 13: Tuning Cluster Strictness",
        "",
        "```python",
        "# Strict clustering (more separate clusters)",
        "processor.compute_all(",
        "    build_concepts=True,",
        "    cluster_strictness=1.0,",
        "    bridge_weight=0.0",
        ")",
        "",
        "# Loose clustering (fewer, larger clusters)",
        "processor.compute_all(",
        "    build_concepts=True,",
        "    cluster_strictness=0.5,",
        "    bridge_weight=0.3",
        ")",
        "```",
        "",
        "**Strictness guide:**",
        "- `1.0`: Strict (more clusters, stronger topic separation)",
        "- `0.5`: Balanced (recommended)",
        "- `0.0`: Loose (fewer clusters, more topic mixing)",
        "",
        "**Bridge weight effects:**",
        "- `0.0`: No synthetic connections (isolated topics)",
        "- `0.1-0.3`: Light bridging (recommended)",
        "- `0.5+`: Strong bridging (may create spurious links)",
        "",
        "---",
        "",
        "## Performance Optimization",
        "",
        "### Recipe 14: Profiling Corpus Size",
        "",
        "```python",
        "summary = processor.get_corpus_summary()",
        "",
        "print(f\"Documents: {summary['documents']}\")",
        "print(f\"Total columns: {summary['total_columns']}\")",
        "print(f\"Layer breakdown:\")",
        "print(f\"  Tokens: {summary['layer_stats'].get(0, {}).get('minicolumns', 0)}\")",
        "print(f\"  Bigrams: {summary['layer_stats'].get(1, {}).get('minicolumns', 0)}\")",
        "",
        "# Optimization strategy",
        "if summary['documents'] < 100:",
        "    print(\"Small corpus: use standard methods\")",
        "elif summary['documents'] < 1000:",
        "    print(\"Medium corpus: consider fast_find_documents()\")",
        "else:",
        "    print(\"Large corpus: use search index\")",
        "```",
        "",
        "---",
        "",
        "### Recipe 15: Query Cache Management",
        "",
        "```python",
        "# Enable query caching",
        "processor.set_query_cache_size(100)",
        "",
        "# Cached expansion (instant for repeated queries)",
        "results1 = processor.expand_query_cached(\"neural networks\")",
        "results2 = processor.expand_query_cached(\"neural networks\")  # From cache",
        "",
        "# Clear cache when corpus changes",
        "processor.clear_query_cache()",
        "```",
        "",
        "---",
        "",
        "## RAG Integration Patterns",
        "",
        "### Recipe 16: Simple RAG Backend",
        "",
        "```python",
        "def rag_retrieve(processor, query: str, top_n: int = 5) -> str:",
        "    \"\"\"Retrieve context for RAG system.\"\"\"",
        "    passages = processor.find_passages_for_query(",
        "        query,",
        "        top_n=top_n,",
        "        chunk_size=512,",
        "        overlap=128",
        "    )",
        "",
        "    context = \"Context from knowledge base:\\n\\n\"",
        "    for passage, doc_id, _, _, score in passages:",
        "        context += f\"[{doc_id}] {passage}\\n\\n\"",
        "",
        "    return context",
        "",
        "# Use in RAG loop",
        "context = rag_retrieve(processor, \"How do neural networks learn?\")",
        "# Pass to LLM with question",
        "```",
        "",
        "---",
        "",
        "### Recipe 17: Multi-Stage RAG Ranking",
        "",
        "**When to use:** Maximum quality ranking combining multiple signals.",
        "",
        "```python",
        "results = processor.multi_stage_rank(",
        "    \"neural networks\",",
        "    top_n=5,",
        "    chunk_size=512,",
        "    concept_boost=0.3  # Weight for concept relevance",
        ")",
        "",
        "for passage, doc_id, start, end, score, stages in results:",
        "    print(f\"[{doc_id}] Final: {score:.3f}\")",
        "    print(f\"  Concept: {stages['concept_score']:.3f}\")",
        "    print(f\"  Document: {stages['doc_score']:.3f}\")",
        "    print(f\"  Passage: {stages['chunk_score']:.3f}\")",
        "```",
        "",
        "---",
        "",
        "## Quick Reference",
        "",
        "| Task | Best Method |",
        "|------|-------------|",
        "| Multiple documents | `add_documents_batch()` |",
        "| Incremental updates | `add_document_incremental()` |",
        "| Document removal | `remove_documents_batch()` |",
        "| General search | `find_documents_for_query()` |",
        "| Large corpus search | `fast_find_documents()` |",
        "| Repeated searches | `build_search_index()` |",
        "| RAG passages | `find_passages_for_query()` |",
        "| High-quality RAG | `multi_stage_rank()` |",
        "| Query debugging | `expand_query()` |",
        "| Intent search | `search_by_intent()` |",
        "",
        "---",
        "",
        "## Troubleshooting",
        "",
        "### No Results Found",
        "",
        "```python",
        "# Check if query terms exist",
        "layer0 = processor.get_layer(CorticalLayer.TOKENS)",
        "for term in processor.tokenizer.tokenize(query):",
        "    if not layer0.get_minicolumn(term):",
        "        print(f\"'{term}' not in corpus\")",
        "",
        "# Try with expansion",
        "results = processor.find_documents_for_query(query, use_expansion=True)",
        "```",
        "",
        "### Search is Slow",
        "",
        "```python",
        "# Use fast search",
        "results = processor.fast_find_documents(query, top_n=5)",
        "",
        "# Or build index",
        "index = processor.build_search_index()",
        "results = processor.search_with_index(query, index)",
        "```",
        "",
        "### Stale Results",
        "",
        "```python",
        "# Check and recompute",
        "stale = processor.get_stale_computations()",
        "if stale:",
        "    processor.recompute(level='full')",
        "```",
        "",
        "---",
        "",
        "*See also: [Query Guide](query-guide.md) for detailed query formulation, [Claude Usage Guide](claude-usage.md) for AI agent usage.*"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "docs/dogfooding.md",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "# Dog-Fooding Guide",
        "",
        "This guide explains how the Cortical Text Processor is used to build and improve itself - a practice known as \"dog-fooding.\" The system indexes its own codebase, enabling semantic search during development.",
        "",
        "---",
        "",
        "## Overview",
        "",
        "**Dog-fooding** means using your own product to develop it. The Cortical Text Processor can:",
        "",
        "1. **Index its own source code** - Build a searchable semantic model of the codebase",
        "2. **Search semantically** - Find relevant code by meaning, not just keywords",
        "3. **Update incrementally** - Keep the index current as code changes",
        "4. **Integrate with Claude** - Provide semantic search via Claude skills",
        "",
        "```",
        "┌─────────────────────────────────────────────────────────────────┐",
        "│                    Dog-Fooding Workflow                          │",
        "│                                                                  │",
        "│  ┌──────────┐    ┌──────────┐    ┌──────────┐    ┌──────────┐  │",
        "│  │  Index   │───▶│  Search  │───▶│ Develop  │───▶│ Re-index │  │",
        "│  │ Codebase │    │   Code   │    │   Code   │    │  Changes │  │",
        "│  └──────────┘    └──────────┘    └──────────┘    └────┬─────┘  │",
        "│       ▲                                               │        │",
        "│       └───────────────────────────────────────────────┘        │",
        "└─────────────────────────────────────────────────────────────────┘",
        "```",
        "",
        "---",
        "",
        "## Quick Start",
        "",
        "### 1. Index the Codebase",
        "",
        "```bash",
        "# First time: Full index (~2-3 seconds)",
        "python scripts/index_codebase.py",
        "",
        "# After changes: Incremental update (~1 second)",
        "python scripts/index_codebase.py --incremental",
        "```",
        "",
        "### 2. Search for Code",
        "",
        "```bash",
        "# Basic search",
        "python scripts/search_codebase.py \"PageRank algorithm\"",
        "",
        "# See query expansion",
        "python scripts/search_codebase.py \"bigram separator\" --expand",
        "",
        "# Interactive mode",
        "python scripts/search_codebase.py --interactive",
        "```",
        "",
        "### 3. Use Claude Skills",
        "",
        "When using Claude Code in this project:",
        "- **codebase-search**: Search for code patterns and implementations",
        "- **corpus-indexer**: Re-index after making changes",
        "",
        "---",
        "",
        "## The Indexing System",
        "",
        "### What Gets Indexed",
        "",
        "The indexer processes these files:",
        "",
        "| Category | Pattern | Purpose |",
        "|----------|---------|---------|",
        "| Source code | `cortical/*.py` | Core library implementation |",
        "| Tests | `tests/*.py` | Test cases and examples |",
        "| Documentation | `CLAUDE.md`, `README.md` | Project documentation |",
        "| Intelligence | `docs/*.md` | Architecture docs |",
        "| Task tracking | `TASK_LIST.md` | Development tasks |",
        "",
        "### Index Files Created",
        "",
        "| File | Purpose |",
        "|------|---------|",
        "| `corpus_dev.pkl` | Serialized processor state (searchable index) |",
        "| `corpus_dev.manifest.json` | File modification times for incremental updates |",
        "",
        "### Indexer Options",
        "",
        "```bash",
        "# Show what would be indexed without doing it",
        "python scripts/index_codebase.py --status",
        "",
        "# Force full rebuild even if nothing changed",
        "python scripts/index_codebase.py --force",
        "",
        "# See per-file progress",
        "python scripts/index_codebase.py --verbose",
        "",
        "# Log to file for debugging",
        "python scripts/index_codebase.py --log indexer.log",
        "",
        "# Set timeout (default 300s)",
        "python scripts/index_codebase.py --timeout 60",
        "",
        "# Full semantic analysis (slower, more accurate)",
        "python scripts/index_codebase.py --full-analysis",
        "```",
        "",
        "---",
        "",
        "## Incremental Indexing",
        "",
        "Incremental indexing is the key to efficient dog-fooding. Instead of rebuilding the entire index, it only processes changes.",
        "",
        "### How It Works",
        "",
        "```",
        "1. Load manifest (file modification times from last index)",
        "2. Scan current files",
        "3. Detect changes:",
        "   - ADDED: Files that didn't exist before",
        "   - MODIFIED: Files with newer modification times",
        "   - DELETED: Files in manifest but no longer exist",
        "4. Update only changed files:",
        "   - Remove deleted docs from index",
        "   - Re-index modified files (remove old, add new)",
        "   - Add new files",
        "5. Recompute analysis (PageRank, TF-IDF, etc.)",
        "6. Save updated index and manifest",
        "```",
        "",
        "### Performance",
        "",
        "| Operation | Time | Use Case |",
        "|-----------|------|----------|",
        "| No changes detected | ~0.1s | Check if re-index needed |",
        "| Few files changed | ~1-2s | Normal development |",
        "| Full rebuild (fast mode) | ~2-3s | After major refactoring |",
        "| Full rebuild (full analysis) | ~10+ min | Before deep exploration |",
        "",
        "### When to Re-index",
        "",
        "| Scenario | Command |",
        "|----------|---------|",
        "| After editing code | `--incremental` |",
        "| After adding new files | `--incremental` |",
        "| After deleting files | `--incremental` |",
        "| After major refactoring | `--force` |",
        "| Before deep code exploration | `--full-analysis` |",
        "| Search results seem stale | `--status` then decide |",
        "",
        "---",
        "",
        "## Search Capabilities",
        "",
        "### Basic Search",
        "",
        "```bash",
        "# Find code related to a concept",
        "python scripts/search_codebase.py \"query expansion\"",
        "",
        "# Output shows file:line references",
        "# cortical/query.py:55  [0.847]",
        "#   def get_expanded_query_terms(...)",
        "```",
        "",
        "### Query Expansion",
        "",
        "The search automatically expands queries with related terms:",
        "",
        "```bash",
        "python scripts/search_codebase.py \"PageRank\" --expand",
        "",
        "# Shows: pagerank → importance, score, rank, algorithm, weight, ...",
        "```",
        "",
        "### Interactive Mode",
        "",
        "For exploratory searching:",
        "",
        "```bash",
        "python scripts/search_codebase.py --interactive",
        "",
        "# Commands in interactive mode:",
        "# /expand <query>  - Show query expansion terms",
        "# /concepts        - List concept clusters",
        "# /stats           - Show corpus statistics",
        "# /quit            - Exit",
        "```",
        "",
        "### Search Options",
        "",
        "| Option | Description |",
        "|--------|-------------|",
        "| `--top N` | Return N results (default: 5) |",
        "| `--verbose` | Show full passage text |",
        "| `--expand` | Show query expansion terms |",
        "| `--fast` | Document-level search only (faster) |",
        "| `--interactive` | Interactive search mode |",
        "",
        "---",
        "",
        "## Claude Skills Integration",
        "",
        "### codebase-search Skill",
        "",
        "Use this skill to search the indexed codebase from Claude:",
        "",
        "```",
        "@claude: Use codebase-search to find how PageRank is implemented",
        "```",
        "",
        "The skill:",
        "1. Loads the pre-built corpus (`corpus_dev.pkl`)",
        "2. Executes semantic search",
        "3. Returns file:line references with relevant passages",
        "",
        "### corpus-indexer Skill",
        "",
        "Use this skill to re-index after making changes:",
        "",
        "```",
        "@claude: Use corpus-indexer to update the index",
        "",
        "# Or specifically:",
        "@claude: Use corpus-indexer with --incremental flag",
        "```",
        "",
        "The skill runs `scripts/index_codebase.py` with appropriate options.",
        "",
        "---",
        "",
        "## Development Workflow",
        "",
        "### Typical Development Cycle",
        "",
        "```bash",
        "# 1. Start by searching for relevant code",
        "python scripts/search_codebase.py \"feature I want to modify\"",
        "",
        "# 2. Make changes to the code",
        "# ... edit files ...",
        "",
        "# 3. Run tests",
        "python -m unittest discover -s tests -v",
        "",
        "# 4. Re-index to update search",
        "python scripts/index_codebase.py --incremental",
        "",
        "# 5. Verify changes are searchable",
        "python scripts/search_codebase.py \"my new function\"",
        "```",
        "",
        "### Adding a New Feature",
        "",
        "1. **Research existing code**",
        "   ```bash",
        "   python scripts/search_codebase.py \"related functionality\" --verbose",
        "   ```",
        "",
        "2. **Check the task list**",
        "   ```bash",
        "   python scripts/search_codebase.py \"TASK_LIST feature name\"",
        "   ```",
        "",
        "3. **Implement the feature**",
        "   - Follow patterns found in search results",
        "   - Add tests in `tests/`",
        "",
        "4. **Update the index**",
        "   ```bash",
        "   python scripts/index_codebase.py --incremental --verbose",
        "   ```",
        "",
        "5. **Verify searchability**",
        "   ```bash",
        "   python scripts/search_codebase.py \"new feature name\"",
        "   ```",
        "",
        "### Debugging with Search",
        "",
        "When debugging, use semantic search to find related code:",
        "",
        "```bash",
        "# Find error handling patterns",
        "python scripts/search_codebase.py \"handle error exception\"",
        "",
        "# Find similar implementations",
        "python scripts/search_codebase.py \"implementation pattern I'm looking at\"",
        "",
        "# Find test patterns",
        "python scripts/search_codebase.py \"test case for feature\"",
        "```",
        "",
        "---",
        "",
        "## Technical Details",
        "",
        "### Fast Mode vs Full Analysis",
        "",
        "**Fast Mode** (default):",
        "- Skips `compute_bigram_connections()` - O(n²) on large corpora",
        "- Computes: PageRank, TF-IDF, document connections",
        "- Time: ~2-3 seconds",
        "- Good for: Development, quick searches",
        "",
        "**Full Analysis Mode** (`--full-analysis`):",
        "- Runs complete `compute_all()` pipeline",
        "- Includes: Bigram connections, concept clusters, semantic relations",
        "- Time: ~10+ minutes for full codebase",
        "- Good for: Deep exploration, research sessions",
        "",
        "### Manifest File Format",
        "",
        "```json",
        "{",
        "  \"cortical/processor.py\": 1702234567.89,",
        "  \"tests/test_processor.py\": 1702234590.12,",
        "  ...",
        "}",
        "```",
        "",
        "Maps relative file paths to Unix modification timestamps.",
        "",
        "### Index Contents",
        "",
        "The `corpus_dev.pkl` file contains a serialized `CorticalTextProcessor` with:",
        "",
        "- **Layer 0 (TOKENS)**: ~6,000+ unique terms from source code",
        "- **Layer 1 (BIGRAMS)**: ~26,000+ word pairs",
        "- **Layer 2 (CONCEPTS)**: Semantic clusters (if full analysis)",
        "- **Layer 3 (DOCUMENTS)**: Each indexed file",
        "",
        "---",
        "",
        "## Troubleshooting",
        "",
        "### Index Taking Too Long",
        "",
        "**Symptom:** Indexer hangs at \"Computing analysis\"",
        "",
        "**Cause:** `compute_bigram_connections()` has O(n²) complexity",
        "",
        "**Solution:** Use fast mode (default) or add `--timeout`:",
        "```bash",
        "python scripts/index_codebase.py --timeout 60",
        "```",
        "",
        "### Search Results Seem Stale",
        "",
        "**Check index status:**",
        "```bash",
        "python scripts/search_codebase.py --status",
        "```",
        "",
        "**Force rebuild:**",
        "```bash",
        "python scripts/index_codebase.py --force",
        "```",
        "",
        "### \"No corpus found\" Error",
        "",
        "**Cause:** `corpus_dev.pkl` doesn't exist",
        "",
        "**Solution:** Run initial indexing:",
        "```bash",
        "python scripts/index_codebase.py",
        "```",
        "",
        "### Memory Issues with Large Corpus",
        "",
        "**Cause:** Full analysis mode creates many connections",
        "",
        "**Solution:** Use fast mode or limit file count",
        "",
        "### Index File Too Large",
        "",
        "**Cause:** Full analysis mode creates extensive connection data",
        "",
        "**Solution:** Use fast mode which produces smaller indices",
        "",
        "---",
        "",
        "## Best Practices",
        "",
        "### 1. Index Frequently",
        "",
        "Run `--incremental` after every significant code change:",
        "```bash",
        "python scripts/index_codebase.py --incremental",
        "```",
        "",
        "### 2. Use --status Before Decisions",
        "",
        "Check what would change before rebuilding:",
        "```bash",
        "python scripts/index_codebase.py --status",
        "```",
        "",
        "### 3. Log for Debugging",
        "",
        "When investigating issues, enable logging:",
        "```bash",
        "python scripts/index_codebase.py --verbose --log debug.log",
        "```",
        "",
        "### 4. Use Interactive Mode for Exploration",
        "",
        "When researching unfamiliar code:",
        "```bash",
        "python scripts/search_codebase.py --interactive",
        "```",
        "",
        "### 5. Trust the Expansion",
        "",
        "Let query expansion find related terms:",
        "```bash",
        "python scripts/search_codebase.py \"authentication\" --expand",
        "# May find: auth, login, credential, token, session, ...",
        "```",
        "",
        "### 6. Combine with Git",
        "",
        "Index before major refactoring to capture baseline:",
        "```bash",
        "git status",
        "python scripts/index_codebase.py --force --log pre-refactor.log",
        "```",
        "",
        "---",
        "",
        "## File Reference",
        "",
        "| File | Purpose |",
        "|------|---------|",
        "| `scripts/index_codebase.py` | Codebase indexer with incremental support |",
        "| `scripts/search_codebase.py` | Semantic search CLI |",
        "| `cortical/chunk_index.py` | Chunk-based indexing module |",
        "| `corpus_dev.pkl` | Serialized index (generated, gitignored) |",
        "| `corpus_dev.manifest.json` | File modification times (generated, gitignored) |",
        "| `corpus_chunks/` | Chunk files directory (git trackable) |",
        "| `.claude/skills/codebase-search/` | Claude search skill |",
        "| `.claude/skills/corpus-indexer/` | Claude indexer skill |",
        "",
        "---",
        "",
        "## Chunk-Based Indexing (Git-Compatible)",
        "",
        "The chunk-based indexing system stores index changes as append-only JSON chunks. This enables git tracking of index state without merge conflicts.",
        "",
        "### Why Chunks?",
        "",
        "| Traditional (PKL) | Chunk-Based |",
        "|-------------------|-------------|",
        "| Binary format | JSON text |",
        "| Merge conflicts | Append-only (no conflicts) |",
        "| Must rebuild to restore | Combine chunks on load |",
        "| Local-only | Git trackable |",
        "",
        "### Quick Start",
        "",
        "```bash",
        "# Index with chunk output",
        "python scripts/index_codebase.py --use-chunks",
        "",
        "# Check chunk status",
        "python scripts/index_codebase.py --use-chunks --status",
        "",
        "# Compact old chunks",
        "python scripts/index_codebase.py --compact --compact-keep 5",
        "```",
        "",
        "### How It Works",
        "",
        "```",
        "Session 1: Edit file A",
        "  → Creates: corpus_chunks/2025-12-10_09-00-00_abc123.json",
        "     {\"operations\": [{\"op\": \"modify\", \"doc_id\": \"A\", \"content\": \"...\"}]}",
        "",
        "Session 2: Edit file B",
        "  → Creates: corpus_chunks/2025-12-10_10-30-00_def456.json",
        "     {\"operations\": [{\"op\": \"modify\", \"doc_id\": \"B\", \"content\": \"...\"}]}",
        "",
        "Session 3: Load",
        "  → Combines all chunks chronologically",
        "  → Validates against PKL cache",
        "  → Rebuilds if needed",
        "```",
        "",
        "### Chunk Files",
        "",
        "Each chunk contains:",
        "",
        "```json",
        "{",
        "  \"version\": 1,",
        "  \"timestamp\": \"2025-12-10T09:00:00.123456\",",
        "  \"session_id\": \"abc123de\",",
        "  \"branch\": \"feature/my-branch\",",
        "  \"operations\": [",
        "    {\"op\": \"add\", \"doc_id\": \"file.py\", \"content\": \"...\", \"mtime\": 1702234567.89},",
        "    {\"op\": \"modify\", \"doc_id\": \"other.py\", \"content\": \"...\", \"mtime\": 1702234590.12},",
        "    {\"op\": \"delete\", \"doc_id\": \"removed.py\"}",
        "  ]",
        "}",
        "```",
        "",
        "### Cache Validation",
        "",
        "The system maintains a content hash to validate PKL cache against chunks:",
        "",
        "```bash",
        "# Show cache validation status",
        "python scripts/index_codebase.py --use-chunks --status",
        "",
        "# Output:",
        "# Chunk Statistics:",
        "#   Total chunks: 5",
        "#   Total operations: 47",
        "#   Content hash: a1b2c3d4e5f67890",
        "#   Cache valid: True",
        "```",
        "",
        "If the hash doesn't match, the PKL is rebuilt from chunks automatically.",
        "",
        "### Compaction",
        "",
        "Over time, chunks accumulate. Compaction merges them:",
        "",
        "```bash",
        "# Compact all chunks into one",
        "python scripts/index_codebase.py --compact",
        "",
        "# Keep last 5 chunks, compact the rest",
        "python scripts/index_codebase.py --compact --compact-keep 5",
        "",
        "# Compact only chunks before a date",
        "python scripts/index_codebase.py --compact --compact-before 2025-12-01",
        "",
        "# Dry run (show what would happen)",
        "python scripts/index_codebase.py --compact --dry-run",
        "```",
        "",
        "Compaction:",
        "1. Combines operations chronologically",
        "2. Removes redundant operations (e.g., add then delete)",
        "3. Creates single `compacted_<timestamp>.json`",
        "4. Deletes original chunk files",
        "",
        "### Git Workflow",
        "",
        "```bash",
        "# 1. Make code changes",
        "vim cortical/processor.py",
        "",
        "# 2. Index with chunks",
        "python scripts/index_codebase.py --use-chunks",
        "",
        "# 3. Commit both code and chunk",
        "git add cortical/processor.py corpus_chunks/",
        "git commit -m \"Add feature X\"",
        "",
        "# 4. Push - teammates get the chunk",
        "git push",
        "",
        "# 5. Teammate pulls and loads",
        "git pull",
        "python scripts/index_codebase.py --use-chunks  # Combines all chunks",
        "```",
        "",
        "### Chunk Options",
        "",
        "| Option | Description |",
        "|--------|-------------|",
        "| `--use-chunks` | Enable chunk-based indexing |",
        "| `--chunks-dir DIR` | Chunk directory (default: `corpus_chunks`) |",
        "| `--compact` | Run compaction |",
        "| `--compact-before DATE` | Only compact chunks before DATE (YYYY-MM-DD) |",
        "| `--compact-keep N` | Keep N most recent chunks |",
        "| `--dry-run` | Show what would happen without making changes |",
        "",
        "### Best Practices",
        "",
        "1. **Commit chunks with code changes** - Keep index in sync with code",
        "2. **Compact periodically** - Run `--compact --compact-keep 10` weekly",
        "3. **Use for team projects** - Git-tracked chunks enable shared index state",
        "4. **Keep PKL local** - The PKL cache should stay in `.gitignore`",
        "",
        "---",
        "",
        "## Summary",
        "",
        "Dog-fooding the Cortical Text Processor creates a virtuous cycle:",
        "",
        "1. **The system searches itself** - Find relevant code by meaning",
        "2. **Changes improve search** - Better algorithms help find code",
        "3. **Incremental updates are fast** - Stay productive during development",
        "4. **Claude integration automates** - Skills handle indexing and search",
        "",
        "This self-referential capability accelerates development by making the codebase semantically searchable while actively improving it.",
        "",
        "---",
        "",
        "*Updated 2025-12-10*"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "docs/query-guide.md",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "# Query Guide",
        "",
        "A comprehensive guide to formulating effective search queries and understanding how the query system works internally.",
        "",
        "---",
        "",
        "## Table of Contents",
        "",
        "1. [How Queries Work Internally](#how-queries-work-internally)",
        "2. [Query Syntax and Patterns](#query-syntax-and-patterns)",
        "3. [Understanding Query Expansion](#understanding-query-expansion)",
        "4. [Single-Word vs Multi-Word Queries](#single-word-vs-multi-word-queries)",
        "5. [Code Patterns vs Concept Searches](#code-patterns-vs-concept-searches)",
        "6. [Intent-Based Queries](#intent-based-queries)",
        "7. [Interpreting Relevance Scores](#interpreting-relevance-scores)",
        "8. [When Queries Fail](#when-queries-fail)",
        "9. [Advanced Techniques](#advanced-techniques)",
        "",
        "---",
        "",
        "## How Queries Work Internally",
        "",
        "### The Query Pipeline",
        "",
        "When you submit a query, the system performs a multi-stage pipeline:",
        "",
        "```",
        "Query Text",
        "    |",
        "[1. Tokenization] -> Split into words, remove stop words",
        "    |",
        "[2. Term Matching] -> Look up terms in token layer",
        "    |",
        "[3. Expansion] -> Add related terms via lateral connections",
        "    |",
        "[4. Document Scoring] -> TF-IDF weighting",
        "    |",
        "[5. Ranking] -> Sort by relevance score",
        "    |",
        "Results",
        "```",
        "",
        "### Stage 1: Tokenization",
        "",
        "Your query is tokenized using the same rules as document processing:",
        "",
        "```python",
        "# \"neural networks process data\" becomes:",
        "[\"neural\", \"networks\", \"process\", \"data\"]",
        "",
        "# Stop words are removed: \"the\", \"a\", \"in\", \"of\", \"is\"",
        "# Short words (< 3 characters) are removed",
        "```",
        "",
        "**Key points:**",
        "- Tokenization is **case-insensitive**",
        "- Punctuation is removed",
        "- Words shorter than 3 characters are filtered",
        "",
        "### Stage 2: Term Matching",
        "",
        "The system looks up each query token in Layer 0:",
        "",
        "```",
        "Token         Found?   Status",
        "\"neural\"      YES      Exact match in corpus",
        "\"networks\"    YES      Exact match in corpus",
        "```",
        "",
        "If a token doesn't exist, the system tries **word variants**:",
        "- Stemmed versions",
        "- Plural forms",
        "- Common aliases",
        "",
        "### Stage 3: Expansion",
        "",
        "The query is expanded using three methods:",
        "",
        "**Method A: Lateral Connections (Default)**",
        "- Terms co-occurring with query terms",
        "- Weights: connection strength x neighbor PageRank x 0.6",
        "",
        "**Method B: Concept Clustering**",
        "- Terms in same semantic cluster",
        "- Weights: concept PageRank x member PageRank x 0.4",
        "",
        "**Method C: Code Concepts (Optional)**",
        "- Programming synonyms (get/fetch/load)",
        "- Only enabled with `use_code_concepts=True`",
        "",
        "### Stage 4: Document Scoring",
        "",
        "```",
        "doc_score = sum(term_weight x tfidf_per_doc)",
        "            for each term in expanded_query",
        "",
        "where:",
        "  term_weight = original terms: 1.0",
        "              = expanded terms: 0.3-0.8",
        "```",
        "",
        "---",
        "",
        "## Query Syntax and Patterns",
        "",
        "The system uses **simple, natural language-based syntax**. No special operators needed.",
        "",
        "### Basic Patterns",
        "",
        "| Pattern | Example | Effect |",
        "|---------|---------|--------|",
        "| **Single word** | `neural` | Search term and related concepts |",
        "| **Multi-word** | `neural networks` | All words must match (AND logic) |",
        "| **Question words** | `where authentication` | Intent-based search |",
        "| **Action verbs** | `how validate input` | Parse action + subject |",
        "",
        "### What Doesn't Work",
        "",
        "```python",
        "# NOT supported:",
        "\"neural\" OR \"learning\"         # No boolean operators",
        "\"neural*\"                      # No wildcards",
        "\"exact phrase match\"           # No phrase searching",
        "```",
        "",
        "### How Multi-Word Queries Work",
        "",
        "Multi-word queries use **AND logic** at document level:",
        "",
        "```",
        "Query: \"neural networks\"",
        "",
        "Step 1: Find docs with \"neural\"   -> [doc1, doc3, doc5]",
        "Step 2: Find docs with \"networks\" -> [doc1, doc3, doc6]",
        "Step 3: Intersection              -> [doc1, doc3]",
        "Step 4: Rank by combined score",
        "```",
        "",
        "---",
        "",
        "## Understanding Query Expansion",
        "",
        "Query expansion is **the core secret** to finding relevant results even when your query doesn't exactly match.",
        "",
        "### How Expansion Works",
        "",
        "Given query `\"fetch user\"`:",
        "",
        "```",
        "Original Terms (weight 1.0):",
        "  - fetch",
        "  - user",
        "",
        "Lateral Connection Expansion:",
        "",
        "Neighbors of \"fetch\":",
        "  - get: 0.45",
        "  - load: 0.42",
        "  - data: 0.38",
        "",
        "Neighbors of \"user\":",
        "  - profile: 0.52",
        "  - account: 0.48",
        "  - authenticate: 0.35",
        "",
        "Final Query Terms:",
        "{",
        "  \"fetch\": 1.0,        # Original",
        "  \"user\": 1.0,         # Original",
        "  \"get\": 0.45,         # Expansion",
        "  \"profile\": 0.52,     # Expansion",
        "  ...",
        "}",
        "```",
        "",
        "### Controlling Expansion",
        "",
        "```python",
        "# With lateral connections only",
        "results = processor.find_documents_for_query(",
        "    \"neural networks\",",
        "    use_expansion=True,",
        "    use_semantic=False",
        ")",
        "",
        "# No expansion (exact match)",
        "results = processor.find_documents_for_query(",
        "    \"neural networks\",",
        "    use_expansion=False",
        ")",
        "",
        "# Code-specific expansion",
        "results = processor.expand_query_for_code(\"fetch user credentials\")",
        "```",
        "",
        "### Debugging Expansion",
        "",
        "```python",
        "expanded = processor.expand_query(\"neural networks\", max_expansions=10)",
        "",
        "for term, weight in sorted(expanded.items(), key=lambda x: -x[1]):",
        "    print(f\"  {term}: {weight:.3f}\")",
        "```",
        "",
        "---",
        "",
        "## Single-Word vs Multi-Word Queries",
        "",
        "### Single-Word Queries",
        "",
        "**Advantages:**",
        "- Faster execution",
        "- Broader matching",
        "- Better for exploratory search",
        "",
        "**Disadvantages:**",
        "- May return less relevant results if term is ambiguous",
        "",
        "```python",
        "Query: \"learning\"",
        "# Finds all documents with \"learning\" and related terms",
        "```",
        "",
        "### Multi-Word Queries",
        "",
        "**Advantages:**",
        "- More specific results (AND logic)",
        "- Provides disambiguation context",
        "",
        "**Disadvantages:**",
        "- Harder to match (all terms must exist)",
        "",
        "```python",
        "Query: \"machine learning\"",
        "# Returns only docs with BOTH terms",
        "```",
        "",
        "### Strategy: Combining Both",
        "",
        "```python",
        "# Broad search first",
        "broad = processor.find_documents_for_query(\"learning\", top_n=20)",
        "",
        "# Narrow with multi-word",
        "narrow = processor.find_documents_for_query(\"machine learning\", top_n=5)",
        "",
        "# Use narrow if available, fall back to broad",
        "results = narrow if narrow else broad",
        "```",
        "",
        "---",
        "",
        "## Code Patterns vs Concept Searches",
        "",
        "### Concept Searches (General Text)",
        "",
        "Best for finding semantic topics:",
        "",
        "```python",
        "processor.find_documents_for_query(\"authentication\")",
        "processor.find_documents_for_query(\"neural networks\")",
        "```",
        "",
        "Uses:",
        "- Lateral connections",
        "- Concept clusters",
        "- Natural language semantics",
        "",
        "### Code Pattern Searches",
        "",
        "Best for finding implementations:",
        "",
        "```python",
        "processor.expand_query_for_code(\"get user credentials\")",
        "processor.expand_query_for_code(\"validate input\")",
        "```",
        "",
        "Uses:",
        "- Code concept groups (get/fetch/load)",
        "- Programming keywords",
        "- Identifier splitting",
        "",
        "### When to Use Each",
        "",
        "| Type | Use Case | Method |",
        "|------|----------|--------|",
        "| **Concept** | Find ideas, topics | `find_documents_for_query()` |",
        "| **Code** | Find implementations | `expand_query_for_code()` |",
        "| **Intent** | Find by action | `search_by_intent()` |",
        "| **Passage** | Find specific text | `find_passages_for_query()` |",
        "",
        "---",
        "",
        "## Intent-Based Queries",
        "",
        "Intent-based queries use **natural language patterns** to understand what you're looking for.",
        "",
        "### Supported Question Words",
        "",
        "| Word | Intent | Example |",
        "|------|--------|---------|",
        "| **where** | location | \"where do we handle authentication?\" |",
        "| **how** | implementation | \"how does validation work?\" |",
        "| **what** | definition | \"what is a concept cluster?\" |",
        "| **why** | rationale | \"why do we use PageRank?\" |",
        "| **when** | lifecycle | \"when do we compute TF-IDF?\" |",
        "",
        "### How Intent Parsing Works",
        "",
        "```",
        "Query: \"where do we handle authentication?\"",
        "",
        "Step 1: Detect \"where\" -> intent = \"location\"",
        "Step 2: Extract content words -> handle, authentication",
        "Step 3: Identify action verb -> \"handle\"",
        "Step 4: Identify subject -> \"authentication\"",
        "Step 5: Build expanded terms",
        "Step 6: Search with weighted terms",
        "```",
        "",
        "### Using Intent Queries",
        "",
        "```python",
        "results = processor.search_by_intent(\"where do we validate input?\", top_n=5)",
        "",
        "parsed = processor.parse_intent_query(\"how does PageRank work?\")",
        "# {",
        "#   'action': 'work',",
        "#   'subject': 'pagerank',",
        "#   'intent': 'implementation',",
        "#   'expanded_terms': ['work', 'pagerank', 'rank', ...]",
        "# }",
        "```",
        "",
        "---",
        "",
        "## Interpreting Relevance Scores",
        "",
        "### Score Meaning",
        "",
        "```",
        "Score > 0.80   Very relevant - high confidence match",
        "Score 0.50-0.80  Relevant - good match",
        "Score 0.25-0.50  Somewhat relevant - weak connection",
        "Score < 0.25   Marginally relevant",
        "```",
        "",
        "### How Scores Are Calculated",
        "",
        "```python",
        "# TF-IDF Score:",
        "tf_idf = (term_count_in_doc / total_terms) x log(total_docs / docs_with_term)",
        "",
        "# Query Score:",
        "doc_score = sum(term_weight x term_tfidf_per_doc)",
        "```",
        "",
        "### Factors Affecting Scores",
        "",
        "1. **Term Frequency (TF):** More occurrences = higher score",
        "2. **Inverse Document Frequency (IDF):** Rarer terms = higher weight",
        "3. **Query Term Weight:** Original (1.0) vs expansion (0.3-0.6)",
        "4. **Concept overlap:** Documents in same cluster score higher",
        "",
        "---",
        "",
        "## When Queries Fail",
        "",
        "### Problem 1: No Results Found",
        "",
        "**Diagnosis:**",
        "```python",
        "layer0 = processor.get_layer(CorticalLayer.TOKENS)",
        "for term in processor.tokenizer.tokenize(query):",
        "    if not layer0.get_minicolumn(term):",
        "        print(f\"{term}: NOT FOUND\")",
        "```",
        "",
        "**Solutions:**",
        "1. Try variant forms: `\"getUserData\"` -> `\"get user data\"`",
        "2. Enable code splitting in tokenizer",
        "3. Use related concepts instead",
        "",
        "### Problem 2: Wrong Documents Returned",
        "",
        "**Diagnosis:**",
        "```python",
        "expanded = processor.expand_query(\"authentication\")",
        "# Check for unexpected expansion terms",
        "```",
        "",
        "**Solutions:**",
        "1. Use multi-word queries for specificity",
        "2. Disable expansion: `use_expansion=False`",
        "3. Use intent-based search",
        "",
        "### Problem 3: Missing Relevant Documents",
        "",
        "**Solutions:**",
        "1. Enable semantic expansion:",
        "   ```python",
        "   processor.extract_corpus_semantics()",
        "   results = processor.find_documents_for_query(",
        "       query,",
        "       use_semantic=True",
        "   )",
        "   ```",
        "",
        "2. Use multi-hop expansion:",
        "   ```python",
        "   expanded = processor.expand_query_multihop(query, max_hops=2)",
        "   ```",
        "",
        "### Problem 4: Slow Queries",
        "",
        "**Solutions:**",
        "1. Use `fast_find_documents()`",
        "2. Pre-build search index",
        "3. Use narrower queries",
        "",
        "---",
        "",
        "## Advanced Techniques",
        "",
        "### Technique 1: Multi-Hop Expansion",
        "",
        "```python",
        "processor.extract_corpus_semantics()",
        "",
        "expanded = processor.expand_query_multihop(",
        "    \"neural\",",
        "    max_hops=2,",
        "    max_expansions=15",
        ")",
        "",
        "# Hop 0: neural",
        "# Hop 1: networks, learning, brain",
        "# Hop 2: deep (via learning), cortex (via brain)",
        "```",
        "",
        "### Technique 2: Passage Retrieval (RAG)",
        "",
        "```python",
        "results = processor.find_passages_for_query(",
        "    \"neural network training\",",
        "    top_n=5,",
        "    chunk_size=512,",
        "    overlap=128",
        ")",
        "",
        "for passage, doc_id, start, end, score in results:",
        "    print(f\"[{doc_id}:{start}-{end}] Score: {score:.3f}\")",
        "    print(passage)",
        "```",
        "",
        "### Technique 3: Multi-Stage Ranking",
        "",
        "```python",
        "results = processor.multi_stage_rank(",
        "    \"neural networks\",",
        "    top_n=5,",
        "    concept_boost=0.3",
        ")",
        "",
        "for passage, doc_id, start, end, score, stages in results:",
        "    print(f\"Concept: {stages['concept_score']:.3f}\")",
        "    print(f\"Document: {stages['doc_score']:.3f}\")",
        "    print(f\"Passage: {stages['chunk_score']:.3f}\")",
        "```",
        "",
        "### Technique 4: Batch Queries",
        "",
        "```python",
        "queries = [\"neural networks\", \"machine learning\", \"deep learning\"]",
        "results = processor.find_documents_batch(queries, top_n=5)",
        "# ~2-3x faster for multiple queries",
        "```",
        "",
        "---",
        "",
        "## Quick Reference",
        "",
        "### Common Methods",
        "",
        "```python",
        "# Basic search",
        "processor.find_documents_for_query(query, top_n=5)",
        "",
        "# Fast search (large corpora)",
        "processor.fast_find_documents(query, top_n=5)",
        "",
        "# Intent-based",
        "processor.search_by_intent(\"where do we...?\", top_n=5)",
        "",
        "# Passages (RAG)",
        "processor.find_passages_for_query(query, top_n=5, chunk_size=512)",
        "",
        "# Code-specific",
        "processor.expand_query_for_code(query)",
        "",
        "# Multi-hop",
        "processor.expand_query_multihop(query, max_hops=2)",
        "",
        "# Batch queries",
        "processor.find_documents_batch(queries, top_n=5)",
        "",
        "# Debug expansion",
        "processor.expand_query(query, max_expansions=10)",
        "```",
        "",
        "### Query Tips",
        "",
        "1. **Start simple** - Single keywords first",
        "2. **Add specificity** - Multi-word if needed",
        "3. **Use intent words** - \"where\", \"how\", \"what\"",
        "4. **Check expansion** - See what terms are added",
        "5. **Trust the system** - Expansion finds related terms",
        "",
        "---",
        "",
        "*For practical recipes, see [Cookbook](cookbook.md). For Claude-specific usage, see [Claude Usage Guide](claude-usage.md).*"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "scripts/index_codebase.py",
      "function": null,
      "start_line": 1,
      "lines_added": [
        "Supports incremental indexing to only re-index changed files.",
        "",
        "    python scripts/index_codebase.py --incremental  # Only index changes",
        "    python scripts/index_codebase.py --status       # Show what would change",
        "    python scripts/index_codebase.py --force        # Force full rebuild",
        "    python scripts/index_codebase.py --log indexer.log  # Log to file",
        "import json",
        "import logging",
        "import signal",
        "import time",
        "from contextlib import contextmanager",
        "from dataclasses import dataclass, field",
        "from datetime import datetime",
        "from typing import Dict, List, Optional, Tuple, Any",
        "from cortical.chunk_index import (",
        "    ChunkWriter, ChunkLoader, ChunkCompactor,",
        "    get_changes_from_manifest as get_chunk_changes",
        ")",
        "",
        "",
        "# Manifest file version for compatibility checking",
        "MANIFEST_VERSION = \"1.0\"",
        "",
        "# Default timeout in seconds (0 = no timeout)",
        "DEFAULT_TIMEOUT = 300  # 5 minutes",
        "",
        "",
        "# =============================================================================",
        "# Progress Tracking System",
        "# =============================================================================",
        "",
        "@dataclass",
        "class PhaseStats:",
        "    \"\"\"Statistics for a single phase of indexing.\"\"\"",
        "    name: str",
        "    start_time: float = 0.0",
        "    end_time: float = 0.0",
        "    items_total: int = 0",
        "    items_processed: int = 0",
        "    status: str = \"pending\"  # pending, running, completed, failed",
        "",
        "    @property",
        "    def duration(self) -> float:",
        "        if self.end_time > 0:",
        "            return self.end_time - self.start_time",
        "        elif self.start_time > 0:",
        "            return time.time() - self.start_time",
        "        return 0.0",
        "",
        "    @property",
        "    def progress_pct(self) -> float:",
        "        if self.items_total == 0:",
        "            return 0.0",
        "        return (self.items_processed / self.items_total) * 100",
        "",
        "",
        "class ProgressTracker:",
        "    \"\"\"",
        "    Tracks progress through indexing phases with timing and logging.",
        "",
        "    Provides:",
        "    - Per-phase timing",
        "    - Per-file progress within phases",
        "    - Log file output",
        "    - Console progress updates",
        "    - Summary statistics",
        "    \"\"\"",
        "",
        "    def __init__(",
        "        self,",
        "        log_file: Optional[str] = None,",
        "        verbose: bool = False,",
        "        quiet: bool = False",
        "    ):",
        "        self.start_time = time.time()",
        "        self.phases: Dict[str, PhaseStats] = {}",
        "        self.current_phase: Optional[str] = None",
        "        self.verbose = verbose",
        "        self.quiet = quiet",
        "        self.warnings: List[str] = []",
        "        self.errors: List[str] = []",
        "",
        "        # Set up logging",
        "        self.logger = logging.getLogger(\"indexer\")",
        "        self.logger.setLevel(logging.DEBUG)",
        "",
        "        # Console handler (INFO level unless verbose)",
        "        if not quiet:",
        "            console_handler = logging.StreamHandler(sys.stdout)",
        "            console_handler.setLevel(logging.DEBUG if verbose else logging.INFO)",
        "            console_format = logging.Formatter('%(message)s')",
        "            console_handler.setFormatter(console_format)",
        "            self.logger.addHandler(console_handler)",
        "",
        "        # File handler (DEBUG level - captures everything)",
        "        if log_file:",
        "            file_handler = logging.FileHandler(log_file, mode='w')",
        "            file_handler.setLevel(logging.DEBUG)",
        "            file_format = logging.Formatter(",
        "                '%(asctime)s [%(levelname)s] %(message)s',",
        "                datefmt='%Y-%m-%d %H:%M:%S'",
        "            )",
        "            file_handler.setFormatter(file_format)",
        "            self.logger.addHandler(file_handler)",
        "            self.log_file = log_file",
        "        else:",
        "            self.log_file = None",
        "",
        "    def log(self, message: str, level: str = \"info\"):",
        "        \"\"\"Log a message at the specified level.\"\"\"",
        "        getattr(self.logger, level)(message)",
        "",
        "    def start_phase(self, name: str, total_items: int = 0):",
        "        \"\"\"Start a new phase of indexing.\"\"\"",
        "        self.current_phase = name",
        "        self.phases[name] = PhaseStats(",
        "            name=name,",
        "            start_time=time.time(),",
        "            items_total=total_items,",
        "            status=\"running\"",
        "        )",
        "        self.log(f\"\\n[PHASE] {name}\", \"info\")",
        "        if total_items > 0:",
        "            self.log(f\"  Items to process: {total_items}\", \"debug\")",
        "",
        "    def end_phase(self, name: Optional[str] = None, status: str = \"completed\"):",
        "        \"\"\"End a phase and record timing.\"\"\"",
        "        phase_name = name or self.current_phase",
        "        if phase_name and phase_name in self.phases:",
        "            phase = self.phases[phase_name]",
        "            phase.end_time = time.time()",
        "            phase.status = status",
        "            duration = phase.duration",
        "",
        "            status_symbol = \"✓\" if status == \"completed\" else \"✗\"",
        "            self.log(f\"  {status_symbol} {phase_name} completed in {duration:.2f}s\", \"info\")",
        "",
        "            if self.current_phase == phase_name:",
        "                self.current_phase = None",
        "",
        "    def update_progress(self, items_processed: int, item_name: Optional[str] = None):",
        "        \"\"\"Update progress within the current phase.\"\"\"",
        "        if self.current_phase and self.current_phase in self.phases:",
        "            phase = self.phases[self.current_phase]",
        "            phase.items_processed = items_processed",
        "",
        "            if phase.items_total > 0:",
        "                pct = phase.progress_pct",
        "                if item_name:",
        "                    self.log(",
        "                        f\"  [{items_processed}/{phase.items_total}] {pct:.0f}% - {item_name}\",",
        "                        \"debug\"",
        "                    )",
        "                # Show progress at 25%, 50%, 75% milestones",
        "                if items_processed in [",
        "                    phase.items_total // 4,",
        "                    phase.items_total // 2,",
        "                    (phase.items_total * 3) // 4",
        "                ]:",
        "                    self.log(f\"  Progress: {pct:.0f}% ({items_processed}/{phase.items_total})\", \"info\")",
        "",
        "    def warn(self, message: str):",
        "        \"\"\"Log a warning.\"\"\"",
        "        self.warnings.append(message)",
        "        self.log(f\"  WARNING: {message}\", \"warning\")",
        "",
        "    def error(self, message: str):",
        "        \"\"\"Log an error.\"\"\"",
        "        self.errors.append(message)",
        "        self.log(f\"  ERROR: {message}\", \"error\")",
        "",
        "    def get_summary(self) -> Dict[str, Any]:",
        "        \"\"\"Get a summary of all phases.\"\"\"",
        "        total_duration = time.time() - self.start_time",
        "        return {",
        "            \"total_duration\": total_duration,",
        "            \"phases\": {",
        "                name: {",
        "                    \"duration\": phase.duration,",
        "                    \"items_processed\": phase.items_processed,",
        "                    \"items_total\": phase.items_total,",
        "                    \"status\": phase.status",
        "                }",
        "                for name, phase in self.phases.items()",
        "            },",
        "            \"warnings\": len(self.warnings),",
        "            \"errors\": len(self.errors)",
        "        }",
        "",
        "    def print_summary(self):",
        "        \"\"\"Print a summary of the indexing run.\"\"\"",
        "        total_duration = time.time() - self.start_time",
        "",
        "        self.log(\"\\n\" + \"=\" * 50, \"info\")",
        "        self.log(\"INDEXING SUMMARY\", \"info\")",
        "        self.log(\"=\" * 50, \"info\")",
        "",
        "        self.log(f\"\\nTotal time: {total_duration:.2f}s\", \"info\")",
        "",
        "        self.log(\"\\nPhase breakdown:\", \"info\")",
        "        for name, phase in self.phases.items():",
        "            status_symbol = \"✓\" if phase.status == \"completed\" else \"✗\"",
        "            items_str = \"\"",
        "            if phase.items_total > 0:",
        "                items_str = f\" ({phase.items_processed}/{phase.items_total} items)\"",
        "            self.log(f\"  {status_symbol} {name}: {phase.duration:.2f}s{items_str}\", \"info\")",
        "",
        "        if self.warnings:",
        "            self.log(f\"\\nWarnings: {len(self.warnings)}\", \"warning\")",
        "            for w in self.warnings[:5]:",
        "                self.log(f\"  - {w}\", \"warning\")",
        "            if len(self.warnings) > 5:",
        "                self.log(f\"  ... and {len(self.warnings) - 5} more\", \"warning\")",
        "",
        "        if self.errors:",
        "            self.log(f\"\\nErrors: {len(self.errors)}\", \"error\")",
        "            for e in self.errors[:5]:",
        "                self.log(f\"  - {e}\", \"error\")",
        "",
        "        if self.log_file:",
        "            self.log(f\"\\nFull log written to: {self.log_file}\", \"info\")",
        "",
        "",
        "# =============================================================================",
        "# Timeout Handler",
        "# =============================================================================",
        "",
        "class TimeoutError(Exception):",
        "    \"\"\"Raised when indexing exceeds the timeout.\"\"\"",
        "    pass",
        "",
        "",
        "@contextmanager",
        "def timeout_handler(seconds: int, tracker: Optional[ProgressTracker] = None):",
        "    \"\"\"",
        "    Context manager for timeout handling.",
        "",
        "    Args:",
        "        seconds: Timeout in seconds (0 = no timeout)",
        "        tracker: Optional progress tracker for logging",
        "    \"\"\"",
        "    if seconds <= 0:",
        "        yield",
        "        return",
        "",
        "    def handler(signum, frame):",
        "        msg = f\"Indexing timed out after {seconds} seconds\"",
        "        if tracker:",
        "            tracker.error(msg)",
        "            tracker.print_summary()",
        "        raise TimeoutError(msg)",
        "",
        "    # Set the signal handler",
        "    old_handler = signal.signal(signal.SIGALRM, handler)",
        "    signal.alarm(seconds)",
        "",
        "    try:",
        "        yield",
        "    finally:",
        "        # Restore the old handler and cancel the alarm",
        "        signal.alarm(0)",
        "        signal.signal(signal.SIGALRM, old_handler)",
        "",
        "",
        "# =============================================================================",
        "# Manifest Operations",
        "# =============================================================================",
        "",
        "def get_manifest_path(corpus_path: Path) -> Path:",
        "    \"\"\"Get the manifest file path based on corpus path.\"\"\"",
        "    return corpus_path.with_suffix('.manifest.json')",
        "",
        "",
        "def load_manifest(",
        "    manifest_path: Path,",
        "    tracker: Optional[ProgressTracker] = None",
        ") -> Optional[Dict[str, Any]]:",
        "    \"\"\"",
        "    Load the manifest file if it exists.",
        "",
        "    Args:",
        "        manifest_path: Path to the manifest file",
        "        tracker: Optional progress tracker for logging",
        "",
        "    Returns:",
        "        Manifest dict if found and valid, None otherwise",
        "    \"\"\"",
        "    if not manifest_path.exists():",
        "        return None",
        "",
        "    try:",
        "        with open(manifest_path, 'r') as f:",
        "            manifest = json.load(f)",
        "",
        "        # Check version compatibility",
        "        if manifest.get('version') != MANIFEST_VERSION:",
        "            if tracker:",
        "                tracker.warn(f\"Manifest version mismatch (expected {MANIFEST_VERSION})\")",
        "            return None",
        "",
        "        return manifest",
        "    except (json.JSONDecodeError, IOError) as e:",
        "        if tracker:",
        "            tracker.warn(f\"Could not load manifest: {e}\")",
        "        return None",
        "",
        "",
        "def save_manifest(",
        "    manifest_path: Path,",
        "    files: Dict[str, float],",
        "    corpus_path: str,",
        "    stats: Dict[str, Any],",
        "    tracker: Optional[ProgressTracker] = None",
        ") -> None:",
        "    \"\"\"",
        "    Save the manifest file with current file state.",
        "",
        "    Args:",
        "        manifest_path: Path to save the manifest",
        "        files: Dict mapping file paths to modification times",
        "        corpus_path: Path to the corpus file",
        "        stats: Statistics about the indexed corpus",
        "        tracker: Optional progress tracker for logging",
        "    \"\"\"",
        "    manifest = {",
        "        'version': MANIFEST_VERSION,",
        "        'corpus_path': str(corpus_path),",
        "        'indexed_at': datetime.now().isoformat(),",
        "        'files': files,",
        "        'stats': stats,",
        "    }",
        "",
        "    with open(manifest_path, 'w') as f:",
        "        json.dump(manifest, f, indent=2)",
        "",
        "    if tracker:",
        "        tracker.log(f\"  Manifest saved to {manifest_path.name}\", \"debug\")",
        "",
        "",
        "# =============================================================================",
        "# File Operations",
        "# =============================================================================",
        "",
        "def get_file_mtime(file_path: Path) -> float:",
        "    \"\"\"Get the modification time of a file.\"\"\"",
        "    return file_path.stat().st_mtime",
        "",
        "",
        "def get_file_changes(",
        "    manifest: Dict[str, Any],",
        "    current_files: List[Path],",
        "    base_path: Path",
        ") -> Tuple[List[Path], List[Path], List[str]]:",
        "    \"\"\"",
        "    Compare current files to manifest and detect changes.",
        "",
        "    Args:",
        "        manifest: Previously saved manifest",
        "        current_files: List of current file paths",
        "        base_path: Base path for relative path calculation",
        "",
        "    Returns:",
        "        Tuple of (added_files, modified_files, deleted_doc_ids)",
        "    \"\"\"",
        "    old_files = manifest.get('files', {})",
        "",
        "    added = []",
        "    modified = []",
        "    deleted_ids = []",
        "",
        "    # Build set of current relative paths",
        "    current_rel_paths = {}",
        "    for file_path in current_files:",
        "        rel_path = str(file_path.relative_to(base_path))",
        "        current_rel_paths[rel_path] = file_path",
        "",
        "    # Check for added and modified files",
        "    for rel_path, file_path in current_rel_paths.items():",
        "        if rel_path not in old_files:",
        "            added.append(file_path)",
        "        else:",
        "            old_mtime = old_files[rel_path]",
        "            current_mtime = get_file_mtime(file_path)",
        "            if current_mtime > old_mtime:",
        "                modified.append(file_path)",
        "",
        "    # Check for deleted files",
        "    for rel_path in old_files:",
        "        if rel_path not in current_rel_paths:",
        "            deleted_ids.append(rel_path)  # doc_id is the relative path",
        "",
        "    return added, modified, deleted_ids"
      ],
      "lines_removed": [],
      "context_before": [
        "#!/usr/bin/env python3",
        "\"\"\"",
        "Index the Cortical Text Processor codebase for dog-fooding.",
        "",
        "This script indexes all Python files and documentation to enable",
        "semantic search over the codebase using the Cortical Text Processor itself.",
        ""
      ],
      "context_after": [
        "Usage:",
        "    python scripts/index_codebase.py [--output corpus_dev.pkl]",
        "\"\"\"",
        "",
        "import argparse",
        "import os",
        "import sys",
        "from pathlib import Path",
        "",
        "# Add parent directory to path for imports",
        "sys.path.insert(0, str(Path(__file__).parent.parent))",
        "",
        "from cortical.processor import CorticalTextProcessor",
        "",
        "",
        "def get_python_files(base_path: Path) -> list:",
        "    \"\"\"Get all Python files in cortical/ and tests/ directories.\"\"\"",
        "    files = []",
        "    for directory in ['cortical', 'tests']:",
        "        dir_path = base_path / directory",
        "        if dir_path.exists():",
        "            for py_file in dir_path.rglob('*.py'):",
        "                if not py_file.name.startswith('__'):"
      ],
      "change_type": "add"
    },
    {
      "file": "scripts/index_codebase.py",
      "function": "def get_doc_files(base_path: Path) -> list:",
      "start_line": 50,
      "lines_added": [
        "# =============================================================================",
        "# Indexing Operations",
        "# =============================================================================",
        "",
        "def index_file(",
        "    processor: CorticalTextProcessor,",
        "    file_path: Path,",
        "    base_path: Path,",
        "    tracker: Optional[ProgressTracker] = None",
        ") -> Optional[dict]:",
        "        if tracker:",
        "            tracker.warn(f\"Could not read {doc_id}: {e}\")",
        "        'mtime': get_file_mtime(file_path),",
        "def show_status(",
        "    added: List[Path],",
        "    modified: List[Path],",
        "    deleted: List[str],",
        "    base_path: Path,",
        "    tracker: ProgressTracker",
        ") -> None:",
        "    \"\"\"Display what would change without actually indexing.\"\"\"",
        "    tracker.log(\"\\n\" + \"=\" * 50)",
        "    tracker.log(\"STATUS: Changes detected (no indexing performed)\")",
        "    tracker.log(\"=\" * 50)",
        "",
        "    if not added and not modified and not deleted:",
        "        tracker.log(\"\\nNo changes detected. Corpus is up to date.\")",
        "        return",
        "",
        "    if added:",
        "        tracker.log(f\"\\n  Added ({len(added)} files):\")",
        "        for f in added[:10]:",
        "            tracker.log(f\"    + {create_doc_id(f, base_path)}\")",
        "        if len(added) > 10:",
        "            tracker.log(f\"    ... and {len(added) - 10} more\")",
        "",
        "    if modified:",
        "        tracker.log(f\"\\n  Modified ({len(modified)} files):\")",
        "        for f in modified[:10]:",
        "            tracker.log(f\"    ~ {create_doc_id(f, base_path)}\")",
        "        if len(modified) > 10:",
        "            tracker.log(f\"    ... and {len(modified) - 10} more\")",
        "",
        "    if deleted:",
        "        tracker.log(f\"\\n  Deleted ({len(deleted)} files):\")",
        "        for doc_id in deleted[:10]:",
        "            tracker.log(f\"    - {doc_id}\")",
        "        if len(deleted) > 10:",
        "            tracker.log(f\"    ... and {len(deleted) - 10} more\")",
        "",
        "    total = len(added) + len(modified) + len(deleted)",
        "    tracker.log(f\"\\nTotal: {total} files would be updated.\")",
        "    tracker.log(\"Run with --incremental to apply changes.\")",
        "",
        "",
        "def full_index(",
        "    processor: CorticalTextProcessor,",
        "    all_files: List[Path],",
        "    base_path: Path,",
        "    tracker: ProgressTracker",
        ") -> Tuple[int, int, Dict[str, float]]:",
        "    \"\"\"",
        "    Perform a full index of all files.",
        "",
        "    Returns:",
        "        Tuple of (indexed_count, total_lines, file_mtimes)",
        "    \"\"\"",
        "    tracker.start_phase(\"Indexing files\", len(all_files))",
        "",
        "    indexed = 0",
        "    total_lines = 0",
        "    file_mtimes = {}",
        "",
        "    for i, file_path in enumerate(all_files, 1):",
        "        doc_id = create_doc_id(file_path, base_path)",
        "        tracker.update_progress(i, doc_id)",
        "",
        "        metadata = index_file(processor, file_path, base_path, tracker)",
        "        if metadata:",
        "            indexed += 1",
        "            total_lines += metadata.get('line_count', 0)",
        "            file_mtimes[doc_id] = metadata.get('mtime', 0)",
        "",
        "    tracker.end_phase(\"Indexing files\")",
        "    tracker.log(f\"  Indexed {indexed} files ({total_lines:,} total lines)\")",
        "",
        "    return indexed, total_lines, file_mtimes",
        "",
        "",
        "def incremental_index(",
        "    processor: CorticalTextProcessor,",
        "    added: List[Path],",
        "    modified: List[Path],",
        "    deleted: List[str],",
        "    base_path: Path,",
        "    tracker: ProgressTracker",
        ") -> Tuple[int, int, int, int]:",
        "    \"\"\"",
        "    Perform an incremental index updating only changed files.",
        "",
        "    Returns:",
        "        Tuple of (added_count, modified_count, deleted_count, total_lines_updated)",
        "    \"\"\"",
        "    total_items = len(added) + len(modified) + len(deleted)",
        "    tracker.start_phase(\"Incremental update\", total_items)",
        "",
        "    added_count = 0",
        "    modified_count = 0",
        "    deleted_count = 0",
        "    total_lines = 0",
        "    processed = 0",
        "",
        "    # Remove deleted documents",
        "    if deleted:",
        "        tracker.log(f\"  Removing {len(deleted)} deleted files...\")",
        "        for doc_id in deleted:",
        "            result = processor.remove_document(doc_id, verbose=False)",
        "            if result['found']:",
        "                deleted_count += 1",
        "            processed += 1",
        "            tracker.update_progress(processed, f\"Deleted: {doc_id}\")",
        "",
        "    # Update modified documents (remove old, add new)",
        "    if modified:",
        "        tracker.log(f\"  Updating {len(modified)} modified files...\")",
        "        for file_path in modified:",
        "            doc_id = create_doc_id(file_path, base_path)",
        "            # Remove old version",
        "            processor.remove_document(doc_id, verbose=False)",
        "            # Add new version",
        "            metadata = index_file(processor, file_path, base_path, tracker)",
        "            if metadata:",
        "                modified_count += 1",
        "                total_lines += metadata.get('line_count', 0)",
        "            processed += 1",
        "            tracker.update_progress(processed, f\"Modified: {doc_id}\")",
        "",
        "    # Add new documents",
        "    if added:",
        "        tracker.log(f\"  Indexing {len(added)} new files...\")",
        "        for file_path in added:",
        "            doc_id = create_doc_id(file_path, base_path)",
        "            metadata = index_file(processor, file_path, base_path, tracker)",
        "            if metadata:",
        "                added_count += 1",
        "                total_lines += metadata.get('line_count', 0)",
        "            processed += 1",
        "            tracker.update_progress(processed, f\"Added: {doc_id}\")",
        "",
        "    tracker.end_phase(\"Incremental update\")",
        "    tracker.log(f\"  Added: {added_count}, Modified: {modified_count}, Deleted: {deleted_count}\")",
        "    tracker.log(f\"  Lines processed: {total_lines:,}\")",
        "",
        "    return added_count, modified_count, deleted_count, total_lines",
        "",
        "",
        "def compute_analysis(",
        "    processor: CorticalTextProcessor,",
        "    tracker: ProgressTracker,",
        "    fast_mode: bool = True",
        ") -> None:",
        "    \"\"\"",
        "    Run all analysis computations with progress tracking.",
        "",
        "    Args:",
        "        processor: The text processor",
        "        tracker: Progress tracker for logging",
        "        fast_mode: If True, use faster but simpler analysis (skips slow bigram connections).",
        "                   If False, use full semantic PageRank and hybrid connections.",
        "    \"\"\"",
        "    if fast_mode:",
        "        # Fast mode: Skip expensive operations",
        "        # - Use standard PageRank (not semantic/hierarchical)",
        "        # - Skip bigram connections (O(n²) on large corpora)",
        "        # - Skip concept cluster connections",
        "        # Completes in seconds for any size corpus",
        "        tracker.start_phase(\"Computing analysis (fast mode)\")",
        "",
        "        # Manual fast computation - skip compute_all() to avoid bigram connections",
        "        tracker.log(\"  Propagating activation...\", \"debug\")",
        "        processor.propagate_activation(verbose=False)",
        "",
        "        tracker.log(\"  Computing PageRank...\", \"debug\")",
        "        processor.compute_importance(verbose=False)",
        "",
        "        tracker.log(\"  Computing TF-IDF...\", \"debug\")",
        "        processor.compute_tfidf(verbose=False)",
        "",
        "        tracker.log(\"  Computing document connections...\", \"debug\")",
        "        processor.compute_document_connections(verbose=False)",
        "",
        "        # Skip bigram connections (too slow with large corpora)",
        "        # Skip concept clusters (not needed for basic search)",
        "",
        "        tracker.end_phase(\"Computing analysis (fast mode)\")",
        "    else:",
        "        # Full mode: semantic PageRank, hybrid connections",
        "        # More accurate but can take minutes for large codebases",
        "        tracker.start_phase(\"Computing analysis (full mode - may take several minutes)\")",
        "        processor.compute_all(",
        "            build_concepts=True,",
        "            pagerank_method='semantic',",
        "            connection_strategy='hybrid',",
        "            verbose=False",
        "        )",
        "        tracker.end_phase(\"Computing analysis (full mode - may take several minutes)\")",
        "",
        "        tracker.start_phase(\"Extracting semantic relations\")",
        "        processor.extract_corpus_semantics(",
        "            use_pattern_extraction=True,",
        "            verbose=False",
        "        )",
        "        tracker.end_phase(\"Extracting semantic relations\")",
        "",
        "",
        "# =============================================================================",
        "# Chunk-Based Indexing",
        "# =============================================================================",
        "",
        "def run_compaction(args, base_path: Path, tracker: ProgressTracker) -> None:",
        "    \"\"\"Run chunk compaction.\"\"\"",
        "    chunks_dir = base_path / args.chunks_dir",
        "",
        "    tracker.log(f\"\\nCompacting chunks in {chunks_dir}\")",
        "",
        "    compactor = ChunkCompactor(str(chunks_dir))",
        "",
        "    # First do a dry run to show what would happen",
        "    dry_result = compactor.compact(",
        "        before=args.compact_before,",
        "        keep_recent=args.compact_keep,",
        "        dry_run=True",
        "    )",
        "",
        "    if dry_result['status'] == 'no_chunks':",
        "        tracker.log(\"No chunks found to compact.\")",
        "        return",
        "",
        "    if dry_result['status'] == 'nothing_to_compact':",
        "        tracker.log(\"No chunks match compaction criteria.\")",
        "        return",
        "",
        "    tracker.log(f\"  Would compact {dry_result['would_compact']} chunks\")",
        "    tracker.log(f\"  Would keep {dry_result['would_keep']} chunks\")",
        "",
        "    # Actually compact",
        "    tracker.start_phase(\"Compacting chunks\")",
        "    result = compactor.compact(",
        "        before=args.compact_before,",
        "        keep_recent=args.compact_keep,",
        "        dry_run=False",
        "    )",
        "    tracker.end_phase(\"Compacting chunks\")",
        "",
        "    tracker.log(f\"\\nCompaction complete:\")",
        "    tracker.log(f\"  Chunks compacted: {result['compacted']}\")",
        "    tracker.log(f\"  Chunks kept: {result['kept']}\")",
        "    tracker.log(f\"  Documents in compacted chunk: {result['documents']}\")",
        "    if result.get('compacted_file'):",
        "        tracker.log(f\"  Output file: {result['compacted_file']}\")",
        "",
        "    tracker.print_summary()",
        "",
        "",
        "def index_with_chunks(",
        "    args,",
        "    base_path: Path,",
        "    output_path: Path,",
        "    tracker: ProgressTracker",
        ") -> None:",
        "    \"\"\"",
        "    Index using chunk-based storage.",
        "",
        "    This creates timestamped JSON chunks that can be committed to git.",
        "    \"\"\"",
        "    chunks_dir = base_path / args.chunks_dir",
        "",
        "    # Initialize chunk loader to get current state from existing chunks",
        "    loader = ChunkLoader(str(chunks_dir))",
        "    existing_docs = loader.get_documents()",
        "    existing_mtimes = loader.get_mtimes()",
        "",
        "    tracker.log(f\"\\nChunk-based indexing mode\")",
        "    tracker.log(f\"  Chunks directory: {chunks_dir}\")",
        "    tracker.log(f\"  Existing documents from chunks: {len(existing_docs)}\")",
        "",
        "    # Get files to index",
        "    tracker.start_phase(\"Discovering files\")",
        "    python_files = get_python_files(base_path)",
        "    doc_files = get_doc_files(base_path)",
        "    all_files = python_files + doc_files",
        "    tracker.end_phase(\"Discovering files\")",
        "",
        "    tracker.log(f\"\\nFound {len(python_files)} Python files and {len(doc_files)} documentation files\")",
        "",
        "    # Build current file state",
        "    current_files = {}",
        "    for file_path in all_files:",
        "        doc_id = create_doc_id(file_path, base_path)",
        "        current_files[doc_id] = get_file_mtime(file_path)",
        "",
        "    # Detect changes against chunk state",
        "    added, modified, deleted = get_chunk_changes(current_files, existing_mtimes)",
        "",
        "    tracker.log(f\"\\nChanges detected:\")",
        "    tracker.log(f\"  Added: {len(added)}, Modified: {len(modified)}, Deleted: {len(deleted)}\")",
        "",
        "    # Status mode",
        "    if args.status:",
        "        show_chunk_status(added, modified, deleted, loader, tracker)",
        "        return",
        "",
        "    # No changes",
        "    if not (added or modified or deleted):",
        "        tracker.log(\"\\nNo changes detected. Corpus is up to date.\")",
        "        return",
        "",
        "    # Create chunk writer for this session",
        "    writer = ChunkWriter(str(chunks_dir))",
        "",
        "    # Record operations",
        "    total_ops = len(added) + len(modified) + len(deleted)",
        "    tracker.start_phase(\"Recording chunk operations\", total_items=total_ops)",
        "    processed = 0",
        "",
        "    # Process added files",
        "    for doc_id in added:",
        "        file_path = base_path / doc_id",
        "        try:",
        "            content = file_path.read_text(encoding='utf-8')",
        "            mtime = get_file_mtime(file_path)",
        "            writer.add_document(doc_id, content, mtime)",
        "            processed += 1",
        "            tracker.update_progress(processed, f\"Added: {doc_id}\" if args.verbose else None)",
        "        except Exception as e:",
        "            tracker.warn(f\"Error reading {doc_id}: {e}\")",
        "",
        "    # Process modified files",
        "    for doc_id in modified:",
        "        file_path = base_path / doc_id",
        "        try:",
        "            content = file_path.read_text(encoding='utf-8')",
        "            mtime = get_file_mtime(file_path)",
        "            writer.modify_document(doc_id, content, mtime)",
        "            processed += 1",
        "            tracker.update_progress(processed, f\"Modified: {doc_id}\" if args.verbose else None)",
        "        except Exception as e:",
        "            tracker.warn(f\"Error reading {doc_id}: {e}\")",
        "",
        "    # Record deletions",
        "    for doc_id in deleted:",
        "        writer.delete_document(doc_id)",
        "        processed += 1",
        "        tracker.update_progress(processed, f\"Deleted: {doc_id}\" if args.verbose else None)",
        "",
        "    tracker.end_phase(\"Recording chunk operations\")",
        "",
        "    # Save chunk",
        "    tracker.start_phase(\"Saving chunk\")",
        "    chunk_path = writer.save()",
        "    if chunk_path:",
        "        tracker.log(f\"  Saved chunk: {chunk_path.name}\")",
        "    tracker.end_phase(\"Saving chunk\")",
        "",
        "    # Now rebuild processor from all chunks",
        "    tracker.start_phase(\"Loading documents from chunks\")",
        "    loader = ChunkLoader(str(chunks_dir))  # Reload to include new chunk",
        "    all_docs = loader.get_documents()",
        "    tracker.log(f\"  Total documents: {len(all_docs)}\")",
        "    tracker.end_phase(\"Loading documents from chunks\")",
        "",
        "    # Check if we can use cached pkl",
        "    cache_valid = loader.is_cache_valid(str(output_path))",
        "    if cache_valid and not (added or modified or deleted):",
        "        tracker.log(\"\\nCache is valid, loading from pkl...\")",
        "        processor = CorticalTextProcessor.load(str(output_path))",
        "    else:",
        "        # Build processor from documents",
        "        tracker.start_phase(\"Building processor from chunks\")",
        "        processor = CorticalTextProcessor()",
        "        documents = [(doc_id, content, None) for doc_id, content in all_docs.items()]",
        "        processor.add_documents_batch(documents, recompute='none', verbose=False)",
        "        tracker.log(f\"  Added {len(documents)} documents\")",
        "        tracker.end_phase(\"Building processor from chunks\")",
        "",
        "        # Compute analysis",
        "        fast_mode = not args.full_analysis",
        "        compute_analysis(processor, tracker, fast_mode=fast_mode)",
        "",
        "    # Print corpus statistics",
        "    tracker.log(\"\\nCorpus Statistics:\")",
        "    tracker.log(f\"  Documents: {len(processor.documents)}\")",
        "    tracker.log(f\"  Tokens (Layer 0): {processor.layers[0].column_count()}\")",
        "    tracker.log(f\"  Bigrams (Layer 1): {processor.layers[1].column_count()}\")",
        "",
        "    # Save processor",
        "    tracker.start_phase(\"Saving corpus\")",
        "    processor.save(str(output_path))",
        "    file_size = output_path.stat().st_size / 1024",
        "    tracker.log(f\"  Saved to {output_path.name} ({file_size:.1f} KB)\")",
        "    tracker.end_phase(\"Saving corpus\")",
        "",
        "    # Save cache hash",
        "    loader.save_cache_hash(str(output_path))",
        "    tracker.log(f\"  Cache hash saved\")",
        "",
        "    # Print chunk stats",
        "    stats = loader.get_stats()",
        "    tracker.log(f\"\\nChunk Statistics:\")",
        "    tracker.log(f\"  Total chunks: {stats['chunk_count']}\")",
        "    tracker.log(f\"  Total operations: {stats['total_operations']}\")",
        "    tracker.log(f\"  Content hash: {stats['hash']}\")",
        "",
        "    tracker.print_summary()",
        "    tracker.log(\"\\nDone! Use search_codebase.py to query the indexed corpus.\")",
        "",
        "",
        "def show_chunk_status(",
        "    added: List[str],",
        "    modified: List[str],",
        "    deleted: List[str],",
        "    loader: ChunkLoader,",
        "    tracker: ProgressTracker",
        ") -> None:",
        "    \"\"\"Show chunk status without indexing.\"\"\"",
        "    stats = loader.get_stats()",
        "",
        "    tracker.log(f\"\\n=== Chunk Status ===\")",
        "    tracker.log(f\"Chunks: {stats['chunk_count']}\")",
        "    tracker.log(f\"Documents: {stats['document_count']}\")",
        "    tracker.log(f\"Content hash: {stats['hash']}\")",
        "",
        "    if added:",
        "        tracker.log(f\"\\nFiles to add ({len(added)}):\")",
        "        for f in added[:10]:",
        "            tracker.log(f\"  + {f}\")",
        "        if len(added) > 10:",
        "            tracker.log(f\"  ... and {len(added) - 10} more\")",
        "",
        "    if modified:",
        "        tracker.log(f\"\\nFiles to modify ({len(modified)}):\")",
        "        for f in modified[:10]:",
        "            tracker.log(f\"  ~ {f}\")",
        "        if len(modified) > 10:",
        "            tracker.log(f\"  ... and {len(modified) - 10} more\")",
        "",
        "    if deleted:",
        "        tracker.log(f\"\\nFiles to delete ({len(deleted)}):\")",
        "        for f in deleted[:10]:",
        "            tracker.log(f\"  - {f}\")",
        "        if len(deleted) > 10:",
        "            tracker.log(f\"  ... and {len(deleted) - 10} more\")",
        "",
        "    if not (added or modified or deleted):",
        "        tracker.log(\"\\nNo changes detected.\")",
        "",
        "",
        "# =============================================================================",
        "# Main Entry Point",
        "# =============================================================================",
        "",
        "    parser = argparse.ArgumentParser(",
        "        description='Index the codebase for semantic search',",
        "        formatter_class=argparse.RawDescriptionHelpFormatter,",
        "        epilog=\"\"\"",
        "Examples:",
        "  python scripts/index_codebase.py                   # Full rebuild",
        "  python scripts/index_codebase.py --incremental    # Update changed files only",
        "  python scripts/index_codebase.py --status         # Show what would change",
        "  python scripts/index_codebase.py --force          # Force full rebuild",
        "  python scripts/index_codebase.py --log index.log  # Log to file",
        "  python scripts/index_codebase.py --timeout 60     # Timeout after 60s",
        "        \"\"\"",
        "    )",
        "    parser.add_argument('--incremental', '-i', action='store_true',",
        "                        help='Only index changed files (requires existing corpus)')",
        "    parser.add_argument('--force', '-f', action='store_true',",
        "                        help='Force full rebuild even if manifest exists')",
        "    parser.add_argument('--status', '-s', action='store_true',",
        "                        help='Show what would change without indexing')",
        "                        help='Show verbose output (per-file progress)')",
        "    parser.add_argument('--quiet', '-q', action='store_true',",
        "                        help='Suppress console output (still writes to log)')",
        "    parser.add_argument('--log', '-l', type=str, default=None,",
        "                        help='Log file path (writes detailed log)')",
        "    parser.add_argument('--timeout', '-t', type=int, default=DEFAULT_TIMEOUT,",
        "                        help=f'Timeout in seconds (0=none, default={DEFAULT_TIMEOUT})')",
        "    parser.add_argument('--full-analysis', action='store_true',",
        "                        help='Use full semantic analysis (slower but more accurate)')",
        "",
        "    # Chunk-based indexing options",
        "    parser.add_argument('--use-chunks', action='store_true',",
        "                        help='Use git-compatible chunk-based indexing')",
        "    parser.add_argument('--chunks-dir', default='corpus_chunks',",
        "                        help='Directory for chunk files (default: corpus_chunks)')",
        "    parser.add_argument('--compact', action='store_true',",
        "                        help='Compact old chunks into a single file')",
        "    parser.add_argument('--compact-before', type=str, default=None,",
        "                        help='Only compact chunks before this date (YYYY-MM-DD)')",
        "    parser.add_argument('--compact-keep', type=int, default=0,",
        "                        help='Keep this many recent chunks when compacting')",
        "",
        "    manifest_path = get_manifest_path(output_path)",
        "",
        "    # Set up log file path",
        "    log_path = None",
        "    if args.log:",
        "        log_path = args.log if os.path.isabs(args.log) else str(base_path / args.log)",
        "",
        "    # Initialize progress tracker",
        "    tracker = ProgressTracker(",
        "        log_file=log_path,",
        "        verbose=args.verbose,",
        "        quiet=args.quiet",
        "    )",
        "",
        "    tracker.log(\"Cortical Text Processor - Codebase Indexer\")",
        "    tracker.log(\"=\" * 50)",
        "    tracker.log(f\"Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")",
        "    if args.timeout > 0:",
        "        tracker.log(f\"Timeout: {args.timeout}s\")",
        "",
        "    # Handle compaction mode",
        "    if args.compact:",
        "        run_compaction(args, base_path, tracker)",
        "        return",
        "",
        "    try:",
        "        with timeout_handler(args.timeout, tracker):",
        "            if args.use_chunks:",
        "                index_with_chunks(args, base_path, output_path, tracker)",
        "            else:",
        "                run_indexer(args, base_path, output_path, manifest_path, tracker)",
        "    except TimeoutError:",
        "        tracker.log(\"\\nIndexing was terminated due to timeout.\", \"error\")",
        "        sys.exit(1)",
        "    except KeyboardInterrupt:",
        "        tracker.log(\"\\nIndexing was interrupted by user.\", \"warning\")",
        "        tracker.print_summary()",
        "        sys.exit(1)",
        "    except Exception as e:",
        "        tracker.error(f\"Unexpected error: {e}\")",
        "        tracker.print_summary()",
        "        raise",
        "def run_indexer(",
        "    args,",
        "    base_path: Path,",
        "    output_path: Path,",
        "    manifest_path: Path,",
        "    tracker: ProgressTracker",
        ") -> None:",
        "    \"\"\"Main indexing logic.\"\"\"",
        "    tracker.start_phase(\"Discovering files\")",
        "    tracker.end_phase(\"Discovering files\")",
        "    tracker.log(f\"\\nFound {len(python_files)} Python files and {len(doc_files)} documentation files\")",
        "    # Load existing manifest if doing incremental update",
        "    manifest = None",
        "    if (args.incremental or args.status) and not args.force:",
        "        if manifest_path.exists():",
        "            tracker.log(f\"\\nLoading manifest from {manifest_path.name}...\")",
        "            manifest = load_manifest(manifest_path, tracker)",
        "        else:",
        "            tracker.log(\"\\nNo manifest found - will perform full index\")",
        "    # Detect changes if we have a manifest",
        "    added, modified, deleted = [], [], []",
        "    if manifest:",
        "        added, modified, deleted = get_file_changes(manifest, all_files, base_path)",
        "        tracker.log(f\"\\nChanges detected:\")",
        "        tracker.log(f\"  Added: {len(added)}, Modified: {len(modified)}, Deleted: {len(deleted)}\")",
        "    # Status mode - just show what would change",
        "    if args.status:",
        "        show_status(added, modified, deleted, base_path, tracker)",
        "        return",
        "    # Determine if we need to do work",
        "    if manifest and not (added or modified or deleted):",
        "        tracker.log(\"\\nNo changes detected. Corpus is up to date.\")",
        "        tracker.log(\"Use --force to rebuild anyway.\")",
        "        return",
        "    # Initialize or load processor",
        "    if args.incremental and manifest and output_path.exists():",
        "        tracker.start_phase(\"Loading existing corpus\")",
        "        try:",
        "            processor = CorticalTextProcessor.load(str(output_path))",
        "            tracker.log(f\"  Loaded {len(processor.documents)} documents\")",
        "            tracker.end_phase(\"Loading existing corpus\")",
        "        except Exception as e:",
        "            tracker.warn(f\"Error loading corpus: {e}\")",
        "            tracker.log(\"  Falling back to full rebuild...\")",
        "            tracker.end_phase(\"Loading existing corpus\", status=\"failed\")",
        "            processor = CorticalTextProcessor()",
        "            added, modified, deleted = all_files, [], []",
        "    else:",
        "        processor = CorticalTextProcessor()",
        "        # Full index - treat all files as \"added\"",
        "        added = all_files",
        "        modified = []",
        "        deleted = []",
        "    # Perform indexing",
        "    if args.incremental and manifest:",
        "        incremental_index(processor, added, modified, deleted, base_path, tracker)",
        "    else:",
        "        full_index(processor, all_files, base_path, tracker)",
        "",
        "    # Compute analysis",
        "    fast_mode = not args.full_analysis",
        "    compute_analysis(processor, tracker, fast_mode=fast_mode)",
        "    # Print corpus statistics",
        "    tracker.log(\"\\nCorpus Statistics:\")",
        "    tracker.log(f\"  Documents: {len(processor.documents)}\")",
        "    tracker.log(f\"  Tokens (Layer 0): {processor.layers[0].column_count()}\")",
        "    tracker.log(f\"  Bigrams (Layer 1): {processor.layers[1].column_count()}\")",
        "    tracker.log(f\"  Concepts (Layer 2): {processor.layers[2].column_count()}\")",
        "    tracker.log(f\"  Semantic relations: {len(processor.semantic_relations)}\")",
        "    tracker.start_phase(\"Saving corpus\")",
        "    tracker.log(f\"  Saved to {output_path.name} ({file_size:.1f} KB)\")",
        "    tracker.end_phase(\"Saving corpus\")",
        "",
        "    # Build file_mtimes for manifest",
        "    file_mtimes = {}",
        "    for file_path in all_files:",
        "        rel_path = create_doc_id(file_path, base_path)",
        "        if rel_path in processor.documents:",
        "            file_mtimes[rel_path] = get_file_mtime(file_path)",
        "",
        "    # Save manifest",
        "    stats = {",
        "        'documents': len(processor.documents),",
        "        'tokens': processor.layers[0].column_count(),",
        "        'bigrams': processor.layers[1].column_count(),",
        "        'concepts': processor.layers[2].column_count(),",
        "        'semantic_relations': len(processor.semantic_relations),",
        "    }",
        "    save_manifest(manifest_path, file_mtimes, str(output_path), stats, tracker)",
        "",
        "    # Print summary",
        "    tracker.print_summary()",
        "    tracker.log(\"\\nDone! Use search_codebase.py to query the indexed corpus.\")"
      ],
      "lines_removed": [
        "def index_file(processor: CorticalTextProcessor, file_path: Path, base_path: Path) -> dict:",
        "        print(f\"  Warning: Could not read {doc_id}: {e}\")",
        "    parser = argparse.ArgumentParser(description='Index the codebase for semantic search')",
        "                        help='Show verbose output')",
        "    print(\"Cortical Text Processor - Codebase Indexer\")",
        "    print(\"=\" * 50)",
        "    # Initialize processor",
        "    processor = CorticalTextProcessor()",
        "    print(f\"\\nFound {len(python_files)} Python files and {len(doc_files)} documentation files\")",
        "    # Index all files",
        "    print(\"\\nIndexing files...\")",
        "    indexed = 0",
        "    total_lines = 0",
        "    for file_path in all_files:",
        "        if args.verbose:",
        "            print(f\"  Indexing: {create_doc_id(file_path, base_path)}\")",
        "        metadata = index_file(processor, file_path, base_path)",
        "        if metadata:",
        "            indexed += 1",
        "            total_lines += metadata.get('line_count', 0)",
        "    print(f\"  Indexed {indexed} files ({total_lines:,} total lines)\")",
        "    # Compute all analysis",
        "    print(\"\\nComputing analysis...\")",
        "    processor.compute_all(",
        "        build_concepts=True,",
        "        pagerank_method='semantic',",
        "        connection_strategy='hybrid',",
        "        verbose=args.verbose",
        "    )",
        "    # Extract semantic relations",
        "    print(\"Extracting semantic relations...\")",
        "    processor.extract_corpus_semantics(",
        "        use_pattern_extraction=True,",
        "        verbose=args.verbose",
        "    )",
        "    # Print statistics",
        "    print(\"\\nCorpus Statistics:\")",
        "    print(f\"  Documents: {len(processor.documents)}\")",
        "    print(f\"  Tokens (Layer 0): {processor.layers[0].column_count()}\")",
        "    print(f\"  Bigrams (Layer 1): {processor.layers[1].column_count()}\")",
        "    print(f\"  Concepts (Layer 2): {processor.layers[2].column_count()}\")",
        "    print(f\"  Semantic relations: {len(processor.semantic_relations)}\")",
        "    print(f\"\\nSaving corpus to {output_path}...\")",
        "",
        "    print(f\"  Saved ({file_size:.1f} KB)\")",
        "    print(\"\\nDone! Use search_codebase.py to query the indexed corpus.\")"
      ],
      "context_before": [
        "",
        "    return files",
        "",
        "",
        "def create_doc_id(file_path: Path, base_path: Path) -> str:",
        "    \"\"\"Create a document ID from file path.\"\"\"",
        "    rel_path = file_path.relative_to(base_path)",
        "    return str(rel_path)",
        "",
        ""
      ],
      "context_after": [
        "    \"\"\"Index a single file with line number metadata.\"\"\"",
        "    doc_id = create_doc_id(file_path, base_path)",
        "",
        "    try:",
        "        content = file_path.read_text(encoding='utf-8')",
        "    except Exception as e:",
        "        return None",
        "",
        "    # Create metadata with file info",
        "    metadata = {",
        "        'file_path': str(file_path),",
        "        'relative_path': doc_id,",
        "        'file_type': file_path.suffix,",
        "        'line_count': content.count('\\n') + 1,",
        "    }",
        "",
        "    # For Python files, extract additional metadata",
        "    if file_path.suffix == '.py':",
        "        metadata['language'] = 'python'",
        "        # Count functions and classes",
        "        metadata['function_count'] = content.count('\\ndef ')",
        "        metadata['class_count'] = content.count('\\nclass ')",
        "",
        "    processor.process_document(doc_id, content, metadata=metadata)",
        "    return metadata",
        "",
        "",
        "def main():",
        "    parser.add_argument('--output', '-o', default='corpus_dev.pkl',",
        "                        help='Output file path (default: corpus_dev.pkl)')",
        "    parser.add_argument('--verbose', '-v', action='store_true',",
        "    args = parser.parse_args()",
        "",
        "    base_path = Path(__file__).parent.parent",
        "    output_path = base_path / args.output",
        "",
        "",
        "",
        "    # Get files to index",
        "    python_files = get_python_files(base_path)",
        "    doc_files = get_doc_files(base_path)",
        "    all_files = python_files + doc_files",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "    # Save corpus",
        "    processor.save(str(output_path))",
        "    file_size = output_path.stat().st_size / 1024",
        "",
        "",
        "",
        "if __name__ == '__main__':",
        "    main()"
      ],
      "change_type": "modify"
    },
    {
      "file": "tests/test_chunk_indexing.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "\"\"\"Tests for chunk-based indexing.\"\"\"",
        "",
        "import json",
        "import os",
        "import sys",
        "import tempfile",
        "import unittest",
        "from pathlib import Path",
        "",
        "sys.path.insert(0, str(Path(__file__).parent.parent))",
        "",
        "from cortical.chunk_index import (",
        "    Chunk,",
        "    ChunkOperation,",
        "    ChunkWriter,",
        "    ChunkLoader,",
        "    ChunkCompactor,",
        "    CHUNK_VERSION,",
        "    get_changes_from_manifest,",
        ")",
        "",
        "",
        "class TestChunkOperation(unittest.TestCase):",
        "    \"\"\"Test ChunkOperation dataclass.\"\"\"",
        "",
        "    def test_add_operation(self):",
        "        \"\"\"Test creating an add operation.\"\"\"",
        "        op = ChunkOperation(op='add', doc_id='doc1', content='hello', mtime=123.0)",
        "        self.assertEqual(op.op, 'add')",
        "        self.assertEqual(op.doc_id, 'doc1')",
        "        self.assertEqual(op.content, 'hello')",
        "        self.assertEqual(op.mtime, 123.0)",
        "",
        "    def test_delete_operation(self):",
        "        \"\"\"Test creating a delete operation.\"\"\"",
        "        op = ChunkOperation(op='delete', doc_id='doc1')",
        "        self.assertEqual(op.op, 'delete')",
        "        self.assertEqual(op.doc_id, 'doc1')",
        "        self.assertIsNone(op.content)",
        "        self.assertIsNone(op.mtime)",
        "",
        "    def test_to_dict_add(self):",
        "        \"\"\"Test converting add operation to dict.\"\"\"",
        "        op = ChunkOperation(op='add', doc_id='doc1', content='hello', mtime=123.0)",
        "        d = op.to_dict()",
        "        self.assertEqual(d['op'], 'add')",
        "        self.assertEqual(d['doc_id'], 'doc1')",
        "        self.assertEqual(d['content'], 'hello')",
        "        self.assertEqual(d['mtime'], 123.0)",
        "",
        "    def test_to_dict_delete(self):",
        "        \"\"\"Test converting delete operation to dict (no content/mtime).\"\"\"",
        "        op = ChunkOperation(op='delete', doc_id='doc1')",
        "        d = op.to_dict()",
        "        self.assertEqual(d['op'], 'delete')",
        "        self.assertEqual(d['doc_id'], 'doc1')",
        "        self.assertNotIn('content', d)",
        "        self.assertNotIn('mtime', d)",
        "",
        "    def test_from_dict(self):",
        "        \"\"\"Test creating operation from dict.\"\"\"",
        "        d = {'op': 'add', 'doc_id': 'doc1', 'content': 'hello', 'mtime': 123.0}",
        "        op = ChunkOperation.from_dict(d)",
        "        self.assertEqual(op.op, 'add')",
        "        self.assertEqual(op.doc_id, 'doc1')",
        "        self.assertEqual(op.content, 'hello')",
        "        self.assertEqual(op.mtime, 123.0)",
        "",
        "",
        "class TestChunk(unittest.TestCase):",
        "    \"\"\"Test Chunk dataclass.\"\"\"",
        "",
        "    def test_chunk_creation(self):",
        "        \"\"\"Test creating a chunk.\"\"\"",
        "        chunk = Chunk(",
        "            version=1,",
        "            timestamp='2025-12-10T12:00:00',",
        "            session_id='abc123',",
        "            branch='main',",
        "            operations=[]",
        "        )",
        "        self.assertEqual(chunk.version, 1)",
        "        self.assertEqual(chunk.timestamp, '2025-12-10T12:00:00')",
        "        self.assertEqual(chunk.session_id, 'abc123')",
        "        self.assertEqual(chunk.branch, 'main')",
        "",
        "    def test_chunk_with_operations(self):",
        "        \"\"\"Test chunk with operations.\"\"\"",
        "        ops = [",
        "            ChunkOperation(op='add', doc_id='doc1', content='hello'),",
        "            ChunkOperation(op='delete', doc_id='doc2')",
        "        ]",
        "        chunk = Chunk(",
        "            version=1,",
        "            timestamp='2025-12-10T12:00:00',",
        "            session_id='abc123',",
        "            branch='main',",
        "            operations=ops",
        "        )",
        "        self.assertEqual(len(chunk.operations), 2)",
        "",
        "    def test_to_dict(self):",
        "        \"\"\"Test converting chunk to dict.\"\"\"",
        "        ops = [ChunkOperation(op='add', doc_id='doc1', content='hello')]",
        "        chunk = Chunk(",
        "            version=1,",
        "            timestamp='2025-12-10T12:00:00',",
        "            session_id='abc123',",
        "            branch='main',",
        "            operations=ops",
        "        )",
        "        d = chunk.to_dict()",
        "        self.assertEqual(d['version'], 1)",
        "        self.assertEqual(d['timestamp'], '2025-12-10T12:00:00')",
        "        self.assertEqual(len(d['operations']), 1)",
        "",
        "    def test_from_dict(self):",
        "        \"\"\"Test creating chunk from dict.\"\"\"",
        "        d = {",
        "            'version': 1,",
        "            'timestamp': '2025-12-10T12:00:00',",
        "            'session_id': 'abc123',",
        "            'branch': 'main',",
        "            'operations': [",
        "                {'op': 'add', 'doc_id': 'doc1', 'content': 'hello'}",
        "            ]",
        "        }",
        "        chunk = Chunk.from_dict(d)",
        "        self.assertEqual(chunk.version, 1)",
        "        self.assertEqual(len(chunk.operations), 1)",
        "        self.assertEqual(chunk.operations[0].doc_id, 'doc1')",
        "",
        "    def test_get_filename(self):",
        "        \"\"\"Test filename generation.\"\"\"",
        "        chunk = Chunk(",
        "            version=1,",
        "            timestamp='2025-12-10T12:00:00',",
        "            session_id='abc12345xyz',",
        "            branch='main',",
        "            operations=[]",
        "        )",
        "        filename = chunk.get_filename()",
        "        self.assertTrue(filename.endswith('.json'))",
        "        self.assertIn('2025-12-10', filename)",
        "        self.assertIn('abc12345', filename)",
        "",
        "",
        "class TestChunkWriter(unittest.TestCase):",
        "    \"\"\"Test ChunkWriter class.\"\"\"",
        "",
        "    def test_writer_creation(self):",
        "        \"\"\"Test creating a chunk writer.\"\"\"",
        "        with tempfile.TemporaryDirectory() as tmpdir:",
        "            writer = ChunkWriter(tmpdir)",
        "            self.assertEqual(len(writer.session_id), 16)",
        "            self.assertIsNotNone(writer.timestamp)",
        "",
        "    def test_add_document(self):",
        "        \"\"\"Test adding a document.\"\"\"",
        "        with tempfile.TemporaryDirectory() as tmpdir:",
        "            writer = ChunkWriter(tmpdir)",
        "            writer.add_document('doc1', 'content here', mtime=123.0)",
        "            self.assertEqual(len(writer.operations), 1)",
        "            self.assertEqual(writer.operations[0].op, 'add')",
        "",
        "    def test_modify_document(self):",
        "        \"\"\"Test modifying a document.\"\"\"",
        "        with tempfile.TemporaryDirectory() as tmpdir:",
        "            writer = ChunkWriter(tmpdir)",
        "            writer.modify_document('doc1', 'new content', mtime=456.0)",
        "            self.assertEqual(len(writer.operations), 1)",
        "            self.assertEqual(writer.operations[0].op, 'modify')",
        "",
        "    def test_delete_document(self):",
        "        \"\"\"Test deleting a document.\"\"\"",
        "        with tempfile.TemporaryDirectory() as tmpdir:",
        "            writer = ChunkWriter(tmpdir)",
        "            writer.delete_document('doc1')",
        "            self.assertEqual(len(writer.operations), 1)",
        "            self.assertEqual(writer.operations[0].op, 'delete')",
        "",
        "    def test_has_operations(self):",
        "        \"\"\"Test checking for operations.\"\"\"",
        "        with tempfile.TemporaryDirectory() as tmpdir:",
        "            writer = ChunkWriter(tmpdir)",
        "            self.assertFalse(writer.has_operations())",
        "            writer.add_document('doc1', 'content')",
        "            self.assertTrue(writer.has_operations())",
        "",
        "    def test_save_empty(self):",
        "        \"\"\"Test saving with no operations returns None.\"\"\"",
        "        with tempfile.TemporaryDirectory() as tmpdir:",
        "            writer = ChunkWriter(tmpdir)",
        "            result = writer.save()",
        "            self.assertIsNone(result)",
        "",
        "    def test_save_creates_file(self):",
        "        \"\"\"Test saving creates a JSON file.\"\"\"",
        "        with tempfile.TemporaryDirectory() as tmpdir:",
        "            writer = ChunkWriter(tmpdir)",
        "            writer.add_document('doc1', 'content here')",
        "            filepath = writer.save()",
        "",
        "            self.assertIsNotNone(filepath)",
        "            self.assertTrue(filepath.exists())",
        "            self.assertTrue(filepath.name.endswith('.json'))",
        "",
        "            # Verify contents",
        "            with open(filepath) as f:",
        "                data = json.load(f)",
        "            self.assertEqual(data['version'], CHUNK_VERSION)",
        "            self.assertEqual(len(data['operations']), 1)",
        "",
        "    def test_save_creates_directory(self):",
        "        \"\"\"Test saving creates the chunks directory if needed.\"\"\"",
        "        with tempfile.TemporaryDirectory() as tmpdir:",
        "            chunks_dir = os.path.join(tmpdir, 'new_chunks')",
        "            writer = ChunkWriter(chunks_dir)",
        "            writer.add_document('doc1', 'content')",
        "            filepath = writer.save()",
        "",
        "            self.assertTrue(os.path.exists(chunks_dir))",
        "            self.assertTrue(filepath.exists())",
        "",
        "",
        "class TestChunkLoader(unittest.TestCase):",
        "    \"\"\"Test ChunkLoader class.\"\"\"",
        "",
        "    def test_loader_empty_directory(self):",
        "        \"\"\"Test loading from empty directory.\"\"\"",
        "        with tempfile.TemporaryDirectory() as tmpdir:",
        "            loader = ChunkLoader(tmpdir)",
        "            docs = loader.load_all()",
        "            self.assertEqual(len(docs), 0)",
        "",
        "    def test_loader_nonexistent_directory(self):",
        "        \"\"\"Test loading from nonexistent directory.\"\"\"",
        "        loader = ChunkLoader('/nonexistent/path')",
        "        docs = loader.load_all()",
        "        self.assertEqual(len(docs), 0)",
        "",
        "    def test_load_single_chunk(self):",
        "        \"\"\"Test loading a single chunk.\"\"\"",
        "        with tempfile.TemporaryDirectory() as tmpdir:",
        "            # Create a chunk",
        "            writer = ChunkWriter(tmpdir)",
        "            writer.add_document('doc1', 'content1')",
        "            writer.add_document('doc2', 'content2')",
        "            writer.save()",
        "",
        "            # Load it",
        "            loader = ChunkLoader(tmpdir)",
        "            docs = loader.load_all()",
        "",
        "            self.assertEqual(len(docs), 2)",
        "            self.assertEqual(docs['doc1'], 'content1')",
        "            self.assertEqual(docs['doc2'], 'content2')",
        "",
        "    def test_load_multiple_chunks(self):",
        "        \"\"\"Test loading multiple chunks.\"\"\"",
        "        with tempfile.TemporaryDirectory() as tmpdir:",
        "            # Create first chunk",
        "            writer1 = ChunkWriter(tmpdir)",
        "            writer1.timestamp = '2025-12-10T10:00:00'",
        "            writer1.add_document('doc1', 'content1')",
        "            writer1.save()",
        "",
        "            # Create second chunk",
        "            writer2 = ChunkWriter(tmpdir)",
        "            writer2.timestamp = '2025-12-10T11:00:00'",
        "            writer2.add_document('doc2', 'content2')",
        "            writer2.save()",
        "",
        "            # Load both",
        "            loader = ChunkLoader(tmpdir)",
        "            docs = loader.load_all()",
        "",
        "            self.assertEqual(len(docs), 2)",
        "            self.assertIn('doc1', docs)",
        "            self.assertIn('doc2', docs)",
        "",
        "    def test_later_chunk_wins(self):",
        "        \"\"\"Test that later timestamps override earlier.\"\"\"",
        "        with tempfile.TemporaryDirectory() as tmpdir:",
        "            # Create first chunk",
        "            writer1 = ChunkWriter(tmpdir)",
        "            writer1.timestamp = '2025-12-10T10:00:00'",
        "            writer1.session_id = 'aaaa0000'",
        "            writer1.add_document('doc1', 'old content')",
        "            writer1.save()",
        "",
        "            # Create second chunk with modification",
        "            writer2 = ChunkWriter(tmpdir)",
        "            writer2.timestamp = '2025-12-10T11:00:00'",
        "            writer2.session_id = 'bbbb1111'",
        "            writer2.modify_document('doc1', 'new content')",
        "            writer2.save()",
        "",
        "            # Load - should have new content",
        "            loader = ChunkLoader(tmpdir)",
        "            docs = loader.load_all()",
        "",
        "            self.assertEqual(docs['doc1'], 'new content')",
        "",
        "    def test_delete_removes_document(self):",
        "        \"\"\"Test that delete operations remove documents.\"\"\"",
        "        with tempfile.TemporaryDirectory() as tmpdir:",
        "            # Create first chunk",
        "            writer1 = ChunkWriter(tmpdir)",
        "            writer1.timestamp = '2025-12-10T10:00:00'",
        "            writer1.session_id = 'aaaa0000'",
        "            writer1.add_document('doc1', 'content1')",
        "            writer1.add_document('doc2', 'content2')",
        "            writer1.save()",
        "",
        "            # Create second chunk with deletion",
        "            writer2 = ChunkWriter(tmpdir)",
        "            writer2.timestamp = '2025-12-10T11:00:00'",
        "            writer2.session_id = 'bbbb1111'",
        "            writer2.delete_document('doc1')",
        "            writer2.save()",
        "",
        "            # Load - doc1 should be gone",
        "            loader = ChunkLoader(tmpdir)",
        "            docs = loader.load_all()",
        "",
        "            self.assertEqual(len(docs), 1)",
        "            self.assertNotIn('doc1', docs)",
        "            self.assertIn('doc2', docs)",
        "",
        "    def test_get_mtimes(self):",
        "        \"\"\"Test getting modification times.\"\"\"",
        "        with tempfile.TemporaryDirectory() as tmpdir:",
        "            writer = ChunkWriter(tmpdir)",
        "            writer.add_document('doc1', 'content', mtime=123.0)",
        "            writer.add_document('doc2', 'content', mtime=456.0)",
        "            writer.save()",
        "",
        "            loader = ChunkLoader(tmpdir)",
        "            mtimes = loader.get_mtimes()",
        "",
        "            self.assertEqual(mtimes['doc1'], 123.0)",
        "            self.assertEqual(mtimes['doc2'], 456.0)",
        "",
        "    def test_compute_hash(self):",
        "        \"\"\"Test computing content hash.\"\"\"",
        "        with tempfile.TemporaryDirectory() as tmpdir:",
        "            writer = ChunkWriter(tmpdir)",
        "            writer.add_document('doc1', 'content1')",
        "            writer.add_document('doc2', 'content2')",
        "            writer.save()",
        "",
        "            loader = ChunkLoader(tmpdir)",
        "            hash1 = loader.compute_hash()",
        "",
        "            # Same content should have same hash",
        "            loader2 = ChunkLoader(tmpdir)",
        "            hash2 = loader2.compute_hash()",
        "",
        "            self.assertEqual(hash1, hash2)",
        "            self.assertEqual(len(hash1), 16)  # Truncated hash",
        "",
        "    def test_get_stats(self):",
        "        \"\"\"Test getting chunk statistics.\"\"\"",
        "        with tempfile.TemporaryDirectory() as tmpdir:",
        "            writer = ChunkWriter(tmpdir)",
        "            writer.add_document('doc1', 'content')",
        "            writer.modify_document('doc2', 'content')",
        "            writer.delete_document('doc3')",
        "            writer.save()",
        "",
        "            loader = ChunkLoader(tmpdir)",
        "            stats = loader.get_stats()",
        "",
        "            self.assertEqual(stats['chunk_count'], 1)",
        "            self.assertEqual(stats['document_count'], 2)  # doc1 and doc2",
        "            self.assertEqual(stats['total_operations'], 3)",
        "            self.assertEqual(stats['add_operations'], 1)",
        "            self.assertEqual(stats['modify_operations'], 1)",
        "            self.assertEqual(stats['delete_operations'], 1)",
        "",
        "    def test_cache_validation(self):",
        "        \"\"\"Test cache hash validation.\"\"\"",
        "        with tempfile.TemporaryDirectory() as tmpdir:",
        "            # Create chunk",
        "            writer = ChunkWriter(tmpdir)",
        "            writer.add_document('doc1', 'content')",
        "            writer.save()",
        "",
        "            # Create fake cache file",
        "            cache_path = os.path.join(tmpdir, 'cache.pkl')",
        "            with open(cache_path, 'w') as f:",
        "                f.write('fake cache')",
        "",
        "            # Load and save hash",
        "            loader = ChunkLoader(tmpdir)",
        "            loader.load_all()",
        "            loader.save_cache_hash(cache_path)",
        "",
        "            # Validate - should be valid",
        "            self.assertTrue(loader.is_cache_valid(cache_path))",
        "",
        "            # Add another chunk",
        "            writer2 = ChunkWriter(tmpdir)",
        "            writer2.add_document('doc2', 'content2')",
        "            writer2.save()",
        "",
        "            # Reload - hash should be different",
        "            loader2 = ChunkLoader(tmpdir)",
        "            self.assertFalse(loader2.is_cache_valid(cache_path))",
        "",
        "",
        "class TestChunkCompactor(unittest.TestCase):",
        "    \"\"\"Test ChunkCompactor class.\"\"\"",
        "",
        "    def test_compact_empty(self):",
        "        \"\"\"Test compacting empty directory.\"\"\"",
        "        with tempfile.TemporaryDirectory() as tmpdir:",
        "            compactor = ChunkCompactor(tmpdir)",
        "            result = compactor.compact()",
        "            self.assertEqual(result['status'], 'no_chunks')",
        "",
        "    def test_compact_all_chunks(self):",
        "        \"\"\"Test compacting all chunks into one.\"\"\"",
        "        with tempfile.TemporaryDirectory() as tmpdir:",
        "            # Create multiple chunks",
        "            for i in range(3):",
        "                writer = ChunkWriter(tmpdir)",
        "                writer.timestamp = f'2025-12-0{i+1}T10:00:00'",
        "                writer.session_id = f'session{i}'",
        "                writer.add_document(f'doc{i}', f'content{i}')",
        "                writer.save()",
        "",
        "            # Verify 3 chunks exist",
        "            self.assertEqual(len(list(Path(tmpdir).glob('*.json'))), 3)",
        "",
        "            # Compact",
        "            compactor = ChunkCompactor(tmpdir)",
        "            result = compactor.compact()",
        "",
        "            self.assertEqual(result['status'], 'compacted')",
        "            self.assertEqual(result['compacted'], 3)",
        "            self.assertEqual(result['documents'], 3)",
        "",
        "            # Should have 1 chunk now",
        "            self.assertEqual(len(list(Path(tmpdir).glob('*.json'))), 1)",
        "",
        "            # Documents should still be loadable",
        "            loader = ChunkLoader(tmpdir)",
        "            docs = loader.load_all()",
        "            self.assertEqual(len(docs), 3)",
        "",
        "    def test_compact_before_date(self):",
        "        \"\"\"Test compacting only chunks before a date.\"\"\"",
        "        with tempfile.TemporaryDirectory() as tmpdir:",
        "            # Create chunks on different dates",
        "            writer1 = ChunkWriter(tmpdir)",
        "            writer1.timestamp = '2025-12-01T10:00:00'",
        "            writer1.session_id = 'session1'",
        "            writer1.add_document('doc1', 'content1')",
        "            writer1.save()",
        "",
        "            writer2 = ChunkWriter(tmpdir)",
        "            writer2.timestamp = '2025-12-05T10:00:00'",
        "            writer2.session_id = 'session2'",
        "            writer2.add_document('doc2', 'content2')",
        "            writer2.save()",
        "",
        "            writer3 = ChunkWriter(tmpdir)",
        "            writer3.timestamp = '2025-12-10T10:00:00'",
        "            writer3.session_id = 'session3'",
        "            writer3.add_document('doc3', 'content3')",
        "            writer3.save()",
        "",
        "            # Compact only before 2025-12-08",
        "            compactor = ChunkCompactor(tmpdir)",
        "            result = compactor.compact(before='2025-12-08')",
        "",
        "            self.assertEqual(result['compacted'], 2)  # doc1 and doc2",
        "            self.assertEqual(result['kept'], 1)  # doc3",
        "",
        "    def test_compact_dry_run(self):",
        "        \"\"\"Test dry run doesn't modify files.\"\"\"",
        "        with tempfile.TemporaryDirectory() as tmpdir:",
        "            # Create chunks",
        "            for i in range(2):",
        "                writer = ChunkWriter(tmpdir)",
        "                writer.timestamp = f'2025-12-0{i+1}T10:00:00'",
        "                writer.session_id = f'session{i}'",
        "                writer.add_document(f'doc{i}', f'content{i}')",
        "                writer.save()",
        "",
        "            # Dry run",
        "            compactor = ChunkCompactor(tmpdir)",
        "            result = compactor.compact(dry_run=True)",
        "",
        "            self.assertEqual(result['status'], 'dry_run')",
        "            self.assertEqual(result['would_compact'], 2)",
        "",
        "            # Should still have 2 chunks",
        "            self.assertEqual(len(list(Path(tmpdir).glob('*.json'))), 2)",
        "",
        "",
        "class TestGetChangesFromManifest(unittest.TestCase):",
        "    \"\"\"Test change detection from manifest.\"\"\"",
        "",
        "    def test_no_changes(self):",
        "        \"\"\"Test when nothing changed.\"\"\"",
        "        current = {'doc1': 100.0, 'doc2': 200.0}",
        "        manifest = {'doc1': 100.0, 'doc2': 200.0}",
        "",
        "        added, modified, deleted = get_changes_from_manifest(current, manifest)",
        "",
        "        self.assertEqual(len(added), 0)",
        "        self.assertEqual(len(modified), 0)",
        "        self.assertEqual(len(deleted), 0)",
        "",
        "    def test_added_files(self):",
        "        \"\"\"Test detecting added files.\"\"\"",
        "        current = {'doc1': 100.0, 'doc2': 200.0, 'doc3': 300.0}",
        "        manifest = {'doc1': 100.0, 'doc2': 200.0}",
        "",
        "        added, modified, deleted = get_changes_from_manifest(current, manifest)",
        "",
        "        self.assertEqual(added, ['doc3'])",
        "        self.assertEqual(len(modified), 0)",
        "        self.assertEqual(len(deleted), 0)",
        "",
        "    def test_deleted_files(self):",
        "        \"\"\"Test detecting deleted files.\"\"\"",
        "        current = {'doc1': 100.0}",
        "        manifest = {'doc1': 100.0, 'doc2': 200.0}",
        "",
        "        added, modified, deleted = get_changes_from_manifest(current, manifest)",
        "",
        "        self.assertEqual(len(added), 0)",
        "        self.assertEqual(len(modified), 0)",
        "        self.assertEqual(deleted, ['doc2'])",
        "",
        "    def test_modified_files(self):",
        "        \"\"\"Test detecting modified files.\"\"\"",
        "        current = {'doc1': 150.0, 'doc2': 200.0}  # doc1 has newer mtime",
        "        manifest = {'doc1': 100.0, 'doc2': 200.0}",
        "",
        "        added, modified, deleted = get_changes_from_manifest(current, manifest)",
        "",
        "        self.assertEqual(len(added), 0)",
        "        self.assertEqual(modified, ['doc1'])",
        "        self.assertEqual(len(deleted), 0)",
        "",
        "    def test_all_change_types(self):",
        "        \"\"\"Test mix of all change types.\"\"\"",
        "        current = {'doc1': 150.0, 'doc3': 300.0}  # doc1 modified, doc3 added",
        "        manifest = {'doc1': 100.0, 'doc2': 200.0}  # doc2 deleted",
        "",
        "        added, modified, deleted = get_changes_from_manifest(current, manifest)",
        "",
        "        self.assertEqual(added, ['doc3'])",
        "        self.assertEqual(modified, ['doc1'])",
        "        self.assertEqual(deleted, ['doc2'])",
        "",
        "",
        "if __name__ == '__main__':",
        "    unittest.main(verbosity=2)"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "tests/test_incremental_indexing.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "\"\"\"",
        "Tests for incremental indexing functionality.",
        "",
        "Tests cover:",
        "- remove_document() method in processor",
        "- remove_minicolumn() method in layers",
        "- Manifest file operations",
        "- File change detection",
        "\"\"\"",
        "",
        "import json",
        "import os",
        "import sys",
        "import tempfile",
        "import unittest",
        "from pathlib import Path",
        "from unittest.mock import patch, MagicMock",
        "",
        "# Add parent directory to path for imports",
        "sys.path.insert(0, str(Path(__file__).parent.parent))",
        "",
        "from cortical.processor import CorticalTextProcessor",
        "from cortical.layers import CorticalLayer, HierarchicalLayer",
        "from cortical.minicolumn import Minicolumn",
        "",
        "",
        "class TestRemoveDocument(unittest.TestCase):",
        "    \"\"\"Tests for CorticalTextProcessor.remove_document()\"\"\"",
        "",
        "    def setUp(self):",
        "        \"\"\"Set up a processor with test documents.\"\"\"",
        "        self.processor = CorticalTextProcessor()",
        "        self.processor.process_document(\"doc1\", \"Neural networks process information efficiently.\")",
        "        self.processor.process_document(\"doc2\", \"Machine learning algorithms learn patterns.\")",
        "        self.processor.process_document(\"doc3\", \"Neural machine translation uses deep learning.\")",
        "        self.processor.compute_all(verbose=False)",
        "",
        "    def test_remove_document_basic(self):",
        "        \"\"\"Test basic document removal.\"\"\"",
        "        self.assertEqual(len(self.processor.documents), 3)",
        "",
        "        result = self.processor.remove_document(\"doc1\")",
        "",
        "        self.assertTrue(result['found'])",
        "        self.assertEqual(len(self.processor.documents), 2)",
        "        self.assertNotIn(\"doc1\", self.processor.documents)",
        "",
        "    def test_remove_document_not_found(self):",
        "        \"\"\"Test removing a non-existent document.\"\"\"",
        "        result = self.processor.remove_document(\"nonexistent\")",
        "",
        "        self.assertFalse(result['found'])",
        "        self.assertEqual(result['tokens_affected'], 0)",
        "        self.assertEqual(result['bigrams_affected'], 0)",
        "",
        "    def test_remove_document_cleans_token_document_ids(self):",
        "        \"\"\"Test that document ID is removed from token document_ids sets.\"\"\"",
        "        layer0 = self.processor.layers[CorticalLayer.TOKENS]",
        "",
        "        # neural appears in doc1 and doc3",
        "        neural_col = layer0.get_minicolumn(\"neural\")",
        "        self.assertIn(\"doc1\", neural_col.document_ids)",
        "",
        "        self.processor.remove_document(\"doc1\")",
        "",
        "        # neural should no longer reference doc1",
        "        self.assertNotIn(\"doc1\", neural_col.document_ids)",
        "        # But should still reference doc3",
        "        self.assertIn(\"doc3\", neural_col.document_ids)",
        "",
        "    def test_remove_document_cleans_bigram_document_ids(self):",
        "        \"\"\"Test that document ID is removed from bigram document_ids sets.\"\"\"",
        "        layer1 = self.processor.layers[CorticalLayer.BIGRAMS]",
        "",
        "        # Find a bigram from doc1",
        "        bigram_col = layer1.get_minicolumn(\"neural networks\")",
        "        if bigram_col:",
        "            self.assertIn(\"doc1\", bigram_col.document_ids)",
        "            self.processor.remove_document(\"doc1\")",
        "            self.assertNotIn(\"doc1\", bigram_col.document_ids)",
        "",
        "    def test_remove_document_removes_layer3_minicolumn(self):",
        "        \"\"\"Test that the document minicolumn is removed from Layer 3.\"\"\"",
        "        layer3 = self.processor.layers[CorticalLayer.DOCUMENTS]",
        "",
        "        self.assertIn(\"doc1\", layer3.minicolumns)",
        "        self.processor.remove_document(\"doc1\")",
        "        self.assertNotIn(\"doc1\", layer3.minicolumns)",
        "",
        "    def test_remove_document_removes_metadata(self):",
        "        \"\"\"Test that document metadata is removed.\"\"\"",
        "        self.processor.set_document_metadata(\"doc1\", source=\"test\")",
        "        self.assertEqual(self.processor.get_document_metadata(\"doc1\"), {\"source\": \"test\"})",
        "",
        "        self.processor.remove_document(\"doc1\")",
        "",
        "        self.assertEqual(self.processor.get_document_metadata(\"doc1\"), {})",
        "",
        "    def test_remove_document_marks_stale(self):",
        "        \"\"\"Test that removal marks computations as stale.\"\"\"",
        "        # After compute_all, computations should not be stale",
        "        self.assertFalse(self.processor.is_stale(self.processor.COMP_TFIDF))",
        "",
        "        self.processor.remove_document(\"doc1\")",
        "",
        "        # After removal, computations should be stale",
        "        self.assertTrue(self.processor.is_stale(self.processor.COMP_TFIDF))",
        "",
        "    def test_remove_document_returns_affected_counts(self):",
        "        \"\"\"Test that removal returns correct affected counts.\"\"\"",
        "        result = self.processor.remove_document(\"doc1\")",
        "",
        "        self.assertTrue(result['found'])",
        "        self.assertGreater(result['tokens_affected'], 0)",
        "        self.assertGreater(result['bigrams_affected'], 0)",
        "",
        "    def test_remove_document_verbose(self):",
        "        \"\"\"Test verbose mode prints output.\"\"\"",
        "        with patch('builtins.print') as mock_print:",
        "            self.processor.remove_document(\"doc1\", verbose=True)",
        "            mock_print.assert_called()",
        "",
        "",
        "class TestRemoveDocumentsBatch(unittest.TestCase):",
        "    \"\"\"Tests for CorticalTextProcessor.remove_documents_batch()\"\"\"",
        "",
        "    def setUp(self):",
        "        \"\"\"Set up a processor with test documents.\"\"\"",
        "        self.processor = CorticalTextProcessor()",
        "        for i in range(5):",
        "            self.processor.process_document(f\"doc{i}\", f\"Document {i} content here.\")",
        "        self.processor.compute_all(verbose=False)",
        "",
        "    def test_remove_documents_batch_basic(self):",
        "        \"\"\"Test removing multiple documents.\"\"\"",
        "        result = self.processor.remove_documents_batch([\"doc0\", \"doc1\", \"doc2\"])",
        "",
        "        self.assertEqual(result['documents_removed'], 3)",
        "        self.assertEqual(result['documents_not_found'], 0)",
        "        self.assertEqual(len(self.processor.documents), 2)",
        "",
        "    def test_remove_documents_batch_with_missing(self):",
        "        \"\"\"Test removing documents when some don't exist.\"\"\"",
        "        result = self.processor.remove_documents_batch([\"doc0\", \"nonexistent\", \"doc1\"])",
        "",
        "        self.assertEqual(result['documents_removed'], 2)",
        "        self.assertEqual(result['documents_not_found'], 1)",
        "",
        "    def test_remove_documents_batch_with_recompute_tfidf(self):",
        "        \"\"\"Test batch removal with TF-IDF recomputation.\"\"\"",
        "        result = self.processor.remove_documents_batch([\"doc0\"], recompute='tfidf')",
        "",
        "        self.assertEqual(result['recomputation'], 'tfidf')",
        "        self.assertFalse(self.processor.is_stale(self.processor.COMP_TFIDF))",
        "",
        "    def test_remove_documents_batch_with_recompute_full(self):",
        "        \"\"\"Test batch removal with full recomputation.\"\"\"",
        "        result = self.processor.remove_documents_batch([\"doc0\"], recompute='full')",
        "",
        "        self.assertEqual(result['recomputation'], 'full')",
        "        self.assertEqual(len(self.processor.get_stale_computations()), 0)",
        "",
        "",
        "class TestRemoveMinicolumn(unittest.TestCase):",
        "    \"\"\"Tests for HierarchicalLayer.remove_minicolumn()\"\"\"",
        "",
        "    def setUp(self):",
        "        \"\"\"Set up a test layer with minicolumns.\"\"\"",
        "        self.layer = HierarchicalLayer(CorticalLayer.TOKENS)",
        "        self.layer.get_or_create_minicolumn(\"test\")",
        "        self.layer.get_or_create_minicolumn(\"neural\")",
        "        self.layer.get_or_create_minicolumn(\"network\")",
        "",
        "    def test_remove_minicolumn_basic(self):",
        "        \"\"\"Test basic minicolumn removal.\"\"\"",
        "        self.assertEqual(self.layer.column_count(), 3)",
        "",
        "        result = self.layer.remove_minicolumn(\"test\")",
        "",
        "        self.assertTrue(result)",
        "        self.assertEqual(self.layer.column_count(), 2)",
        "        self.assertNotIn(\"test\", self.layer.minicolumns)",
        "",
        "    def test_remove_minicolumn_not_found(self):",
        "        \"\"\"Test removing non-existent minicolumn.\"\"\"",
        "        result = self.layer.remove_minicolumn(\"nonexistent\")",
        "",
        "        self.assertFalse(result)",
        "        self.assertEqual(self.layer.column_count(), 3)",
        "",
        "    def test_remove_minicolumn_removes_from_id_index(self):",
        "        \"\"\"Test that removal updates the ID index.\"\"\"",
        "        col = self.layer.get_minicolumn(\"test\")",
        "        col_id = col.id",
        "",
        "        self.assertIsNotNone(self.layer.get_by_id(col_id))",
        "",
        "        self.layer.remove_minicolumn(\"test\")",
        "",
        "        self.assertIsNone(self.layer.get_by_id(col_id))",
        "",
        "",
        "class TestManifestOperations(unittest.TestCase):",
        "    \"\"\"Tests for manifest file operations in index_codebase.py\"\"\"",
        "",
        "    def setUp(self):",
        "        \"\"\"Set up temporary directory for tests.\"\"\"",
        "        self.temp_dir = tempfile.mkdtemp()",
        "        self.manifest_path = Path(self.temp_dir) / \"test.manifest.json\"",
        "",
        "    def tearDown(self):",
        "        \"\"\"Clean up temporary files.\"\"\"",
        "        import shutil",
        "        shutil.rmtree(self.temp_dir)",
        "",
        "    def test_save_manifest(self):",
        "        \"\"\"Test saving a manifest file.\"\"\"",
        "        # Import the functions from the script",
        "        sys.path.insert(0, str(Path(__file__).parent.parent / 'scripts'))",
        "        from index_codebase import save_manifest, load_manifest",
        "",
        "        files = {",
        "            \"cortical/processor.py\": 1234567890.0,",
        "            \"tests/test_processor.py\": 1234567891.0,",
        "        }",
        "        stats = {\"documents\": 2, \"tokens\": 100}",
        "",
        "        save_manifest(self.manifest_path, files, \"corpus.pkl\", stats)",
        "",
        "        self.assertTrue(self.manifest_path.exists())",
        "",
        "        # Verify content",
        "        with open(self.manifest_path) as f:",
        "            data = json.load(f)",
        "",
        "        self.assertEqual(data['version'], \"1.0\")",
        "        self.assertEqual(data['corpus_path'], \"corpus.pkl\")",
        "        self.assertEqual(len(data['files']), 2)",
        "        self.assertEqual(data['stats']['documents'], 2)",
        "",
        "    def test_load_manifest_valid(self):",
        "        \"\"\"Test loading a valid manifest file.\"\"\"",
        "        sys.path.insert(0, str(Path(__file__).parent.parent / 'scripts'))",
        "        from index_codebase import save_manifest, load_manifest",
        "",
        "        files = {\"test.py\": 1234567890.0}",
        "        save_manifest(self.manifest_path, files, \"corpus.pkl\", {})",
        "",
        "        manifest = load_manifest(self.manifest_path)",
        "",
        "        self.assertIsNotNone(manifest)",
        "        self.assertEqual(manifest['files'], files)",
        "",
        "    def test_load_manifest_not_exists(self):",
        "        \"\"\"Test loading a non-existent manifest file.\"\"\"",
        "        sys.path.insert(0, str(Path(__file__).parent.parent / 'scripts'))",
        "        from index_codebase import load_manifest",
        "",
        "        manifest = load_manifest(Path(self.temp_dir) / \"nonexistent.json\")",
        "",
        "        self.assertIsNone(manifest)",
        "",
        "    def test_load_manifest_invalid_version(self):",
        "        \"\"\"Test loading a manifest with wrong version.\"\"\"",
        "        sys.path.insert(0, str(Path(__file__).parent.parent / 'scripts'))",
        "        from index_codebase import load_manifest",
        "",
        "        # Write manifest with wrong version",
        "        with open(self.manifest_path, 'w') as f:",
        "            json.dump({\"version\": \"0.1\", \"files\": {}}, f)",
        "",
        "        manifest = load_manifest(self.manifest_path)",
        "",
        "        self.assertIsNone(manifest)",
        "",
        "",
        "class TestFileChangeDetection(unittest.TestCase):",
        "    \"\"\"Tests for file change detection.\"\"\"",
        "",
        "    def setUp(self):",
        "        \"\"\"Set up temporary directory with test files.\"\"\"",
        "        self.temp_dir = tempfile.mkdtemp()",
        "        self.base_path = Path(self.temp_dir)",
        "",
        "        # Create some test files",
        "        (self.base_path / \"file1.py\").write_text(\"content1\")",
        "        (self.base_path / \"file2.py\").write_text(\"content2\")",
        "        (self.base_path / \"file3.py\").write_text(\"content3\")",
        "",
        "    def tearDown(self):",
        "        \"\"\"Clean up temporary files.\"\"\"",
        "        import shutil",
        "        shutil.rmtree(self.temp_dir)",
        "",
        "    def test_get_file_changes_no_changes(self):",
        "        \"\"\"Test detecting no changes.\"\"\"",
        "        sys.path.insert(0, str(Path(__file__).parent.parent / 'scripts'))",
        "        from index_codebase import get_file_changes, get_file_mtime",
        "",
        "        current_files = list(self.base_path.glob(\"*.py\"))",
        "        manifest = {",
        "            'files': {",
        "                str(f.relative_to(self.base_path)): get_file_mtime(f)",
        "                for f in current_files",
        "            }",
        "        }",
        "",
        "        added, modified, deleted = get_file_changes(manifest, current_files, self.base_path)",
        "",
        "        self.assertEqual(len(added), 0)",
        "        self.assertEqual(len(modified), 0)",
        "        self.assertEqual(len(deleted), 0)",
        "",
        "    def test_get_file_changes_added_file(self):",
        "        \"\"\"Test detecting added files.\"\"\"",
        "        sys.path.insert(0, str(Path(__file__).parent.parent / 'scripts'))",
        "        from index_codebase import get_file_changes, get_file_mtime",
        "",
        "        # Create manifest without file3.py",
        "        manifest = {",
        "            'files': {",
        "                \"file1.py\": get_file_mtime(self.base_path / \"file1.py\"),",
        "                \"file2.py\": get_file_mtime(self.base_path / \"file2.py\"),",
        "            }",
        "        }",
        "",
        "        current_files = list(self.base_path.glob(\"*.py\"))",
        "        added, modified, deleted = get_file_changes(manifest, current_files, self.base_path)",
        "",
        "        self.assertEqual(len(added), 1)",
        "        self.assertEqual(added[0].name, \"file3.py\")",
        "        self.assertEqual(len(modified), 0)",
        "        self.assertEqual(len(deleted), 0)",
        "",
        "    def test_get_file_changes_deleted_file(self):",
        "        \"\"\"Test detecting deleted files.\"\"\"",
        "        sys.path.insert(0, str(Path(__file__).parent.parent / 'scripts'))",
        "        from index_codebase import get_file_changes, get_file_mtime",
        "",
        "        # Create manifest with an extra file that doesn't exist",
        "        manifest = {",
        "            'files': {",
        "                \"file1.py\": get_file_mtime(self.base_path / \"file1.py\"),",
        "                \"file2.py\": get_file_mtime(self.base_path / \"file2.py\"),",
        "                \"file3.py\": get_file_mtime(self.base_path / \"file3.py\"),",
        "                \"deleted.py\": 1234567890.0,  # This file doesn't exist",
        "            }",
        "        }",
        "",
        "        current_files = list(self.base_path.glob(\"*.py\"))",
        "        added, modified, deleted = get_file_changes(manifest, current_files, self.base_path)",
        "",
        "        self.assertEqual(len(added), 0)",
        "        self.assertEqual(len(modified), 0)",
        "        self.assertEqual(len(deleted), 1)",
        "        self.assertIn(\"deleted.py\", deleted)",
        "",
        "    def test_get_file_changes_modified_file(self):",
        "        \"\"\"Test detecting modified files.\"\"\"",
        "        sys.path.insert(0, str(Path(__file__).parent.parent / 'scripts'))",
        "        from index_codebase import get_file_changes, get_file_mtime",
        "        import time",
        "",
        "        # Create manifest with old mtime",
        "        manifest = {",
        "            'files': {",
        "                \"file1.py\": 0.0,  # Very old mtime",
        "                \"file2.py\": get_file_mtime(self.base_path / \"file2.py\"),",
        "                \"file3.py\": get_file_mtime(self.base_path / \"file3.py\"),",
        "            }",
        "        }",
        "",
        "        current_files = list(self.base_path.glob(\"*.py\"))",
        "        added, modified, deleted = get_file_changes(manifest, current_files, self.base_path)",
        "",
        "        self.assertEqual(len(added), 0)",
        "        self.assertEqual(len(modified), 1)",
        "        self.assertEqual(modified[0].name, \"file1.py\")",
        "        self.assertEqual(len(deleted), 0)",
        "",
        "",
        "class TestIncrementalIndexingIntegration(unittest.TestCase):",
        "    \"\"\"Integration tests for incremental indexing workflow.\"\"\"",
        "",
        "    def test_add_remove_reindex_workflow(self):",
        "        \"\"\"Test the full workflow of add, remove, and reindex.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "",
        "        # Initial indexing",
        "        processor.process_document(\"doc1\", \"Neural networks are powerful.\")",
        "        processor.process_document(\"doc2\", \"Machine learning is useful.\")",
        "        processor.compute_all(verbose=False)",
        "",
        "        initial_doc_count = len(processor.documents)",
        "        self.assertEqual(initial_doc_count, 2)",
        "",
        "        # Remove a document",
        "        result = processor.remove_document(\"doc1\")",
        "        self.assertTrue(result['found'])",
        "        self.assertEqual(len(processor.documents), 1)",
        "",
        "        # Add a new document",
        "        processor.process_document(\"doc3\", \"Deep learning advances rapidly.\")",
        "",
        "        # Recompute",
        "        processor.compute_all(verbose=False)",
        "",
        "        # Verify final state",
        "        self.assertEqual(len(processor.documents), 2)",
        "        self.assertNotIn(\"doc1\", processor.documents)",
        "        self.assertIn(\"doc2\", processor.documents)",
        "        self.assertIn(\"doc3\", processor.documents)",
        "",
        "    def test_incremental_preserves_other_documents(self):",
        "        \"\"\"Test that incremental updates don't affect unchanged documents.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "",
        "        processor.process_document(\"doc1\", \"The quick brown fox.\")",
        "        processor.process_document(\"doc2\", \"Jumps over the lazy dog.\")",
        "        processor.compute_all(verbose=False)",
        "",
        "        # Store original state of doc2",
        "        layer0 = processor.layers[CorticalLayer.TOKENS]",
        "        original_quick_docs = layer0.get_minicolumn(\"quick\").document_ids.copy()",
        "",
        "        # Remove doc1",
        "        processor.remove_document(\"doc1\")",
        "",
        "        # doc2 tokens should still reference doc2",
        "        lazy_col = layer0.get_minicolumn(\"lazy\")",
        "        self.assertIn(\"doc2\", lazy_col.document_ids)",
        "",
        "",
        "class TestProgressTracker(unittest.TestCase):",
        "    \"\"\"Tests for ProgressTracker class.\"\"\"",
        "",
        "    def setUp(self):",
        "        \"\"\"Set up temporary directory for log files.\"\"\"",
        "        self.temp_dir = tempfile.mkdtemp()",
        "",
        "    def tearDown(self):",
        "        \"\"\"Clean up temporary files.\"\"\"",
        "        import shutil",
        "        shutil.rmtree(self.temp_dir)",
        "",
        "    def test_progress_tracker_init(self):",
        "        \"\"\"Test ProgressTracker initialization.\"\"\"",
        "        sys.path.insert(0, str(Path(__file__).parent.parent / 'scripts'))",
        "        from index_codebase import ProgressTracker",
        "",
        "        tracker = ProgressTracker(quiet=True)",
        "        self.assertIsNotNone(tracker.start_time)",
        "        self.assertEqual(tracker.phases, {})",
        "        self.assertIsNone(tracker.current_phase)",
        "",
        "    def test_progress_tracker_with_log_file(self):",
        "        \"\"\"Test ProgressTracker with log file output.\"\"\"",
        "        sys.path.insert(0, str(Path(__file__).parent.parent / 'scripts'))",
        "        from index_codebase import ProgressTracker",
        "",
        "        log_path = os.path.join(self.temp_dir, \"test.log\")",
        "        tracker = ProgressTracker(log_file=log_path, quiet=True)",
        "        tracker.log(\"Test message\")",
        "",
        "        # Flush handlers",
        "        for handler in tracker.logger.handlers:",
        "            handler.flush()",
        "",
        "        self.assertTrue(os.path.exists(log_path))",
        "        with open(log_path) as f:",
        "            content = f.read()",
        "        self.assertIn(\"Test message\", content)",
        "",
        "    def test_start_and_end_phase(self):",
        "        \"\"\"Test phase tracking.\"\"\"",
        "        sys.path.insert(0, str(Path(__file__).parent.parent / 'scripts'))",
        "        from index_codebase import ProgressTracker",
        "",
        "        tracker = ProgressTracker(quiet=True)",
        "",
        "        tracker.start_phase(\"Test Phase\", total_items=10)",
        "        self.assertEqual(tracker.current_phase, \"Test Phase\")",
        "        self.assertIn(\"Test Phase\", tracker.phases)",
        "        self.assertEqual(tracker.phases[\"Test Phase\"].status, \"running\")",
        "",
        "        tracker.end_phase(\"Test Phase\")",
        "        self.assertEqual(tracker.phases[\"Test Phase\"].status, \"completed\")",
        "        self.assertGreater(tracker.phases[\"Test Phase\"].duration, 0)",
        "",
        "    def test_update_progress(self):",
        "        \"\"\"Test progress updates within a phase.\"\"\"",
        "        sys.path.insert(0, str(Path(__file__).parent.parent / 'scripts'))",
        "        from index_codebase import ProgressTracker",
        "",
        "        tracker = ProgressTracker(quiet=True)",
        "        tracker.start_phase(\"Processing\", total_items=100)",
        "",
        "        tracker.update_progress(25, \"item_25\")",
        "        self.assertEqual(tracker.phases[\"Processing\"].items_processed, 25)",
        "        self.assertEqual(tracker.phases[\"Processing\"].progress_pct, 25.0)",
        "",
        "        tracker.update_progress(50, \"item_50\")",
        "        self.assertEqual(tracker.phases[\"Processing\"].items_processed, 50)",
        "        self.assertEqual(tracker.phases[\"Processing\"].progress_pct, 50.0)",
        "",
        "    def test_warn_and_error(self):",
        "        \"\"\"Test warning and error tracking.\"\"\"",
        "        sys.path.insert(0, str(Path(__file__).parent.parent / 'scripts'))",
        "        from index_codebase import ProgressTracker",
        "",
        "        tracker = ProgressTracker(quiet=True)",
        "",
        "        tracker.warn(\"Test warning\")",
        "        tracker.error(\"Test error\")",
        "",
        "        self.assertEqual(len(tracker.warnings), 1)",
        "        self.assertEqual(len(tracker.errors), 1)",
        "        self.assertIn(\"Test warning\", tracker.warnings)",
        "        self.assertIn(\"Test error\", tracker.errors)",
        "",
        "    def test_get_summary(self):",
        "        \"\"\"Test summary generation.\"\"\"",
        "        sys.path.insert(0, str(Path(__file__).parent.parent / 'scripts'))",
        "        from index_codebase import ProgressTracker",
        "",
        "        tracker = ProgressTracker(quiet=True)",
        "        tracker.start_phase(\"Phase 1\", total_items=5)",
        "        tracker.update_progress(5)",
        "        tracker.end_phase(\"Phase 1\")",
        "        tracker.warn(\"A warning\")",
        "",
        "        summary = tracker.get_summary()",
        "",
        "        self.assertIn(\"total_duration\", summary)",
        "        self.assertIn(\"phases\", summary)",
        "        self.assertIn(\"Phase 1\", summary[\"phases\"])",
        "        self.assertEqual(summary[\"warnings\"], 1)",
        "        self.assertEqual(summary[\"errors\"], 0)",
        "",
        "",
        "class TestPhaseStats(unittest.TestCase):",
        "    \"\"\"Tests for PhaseStats dataclass.\"\"\"",
        "",
        "    def test_phase_stats_duration(self):",
        "        \"\"\"Test duration calculation.\"\"\"",
        "        sys.path.insert(0, str(Path(__file__).parent.parent / 'scripts'))",
        "        from index_codebase import PhaseStats",
        "        import time",
        "",
        "        phase = PhaseStats(name=\"test\", start_time=time.time())",
        "        time.sleep(0.01)",
        "        phase.end_time = time.time()",
        "",
        "        self.assertGreater(phase.duration, 0)",
        "        self.assertLess(phase.duration, 1)",
        "",
        "    def test_phase_stats_progress_pct(self):",
        "        \"\"\"Test progress percentage calculation.\"\"\"",
        "        sys.path.insert(0, str(Path(__file__).parent.parent / 'scripts'))",
        "        from index_codebase import PhaseStats",
        "",
        "        phase = PhaseStats(name=\"test\", items_total=100, items_processed=25)",
        "        self.assertEqual(phase.progress_pct, 25.0)",
        "",
        "        phase.items_processed = 50",
        "        self.assertEqual(phase.progress_pct, 50.0)",
        "",
        "        # Edge case: zero total",
        "        phase.items_total = 0",
        "        self.assertEqual(phase.progress_pct, 0.0)",
        "",
        "",
        "class TestTimeoutHandler(unittest.TestCase):",
        "    \"\"\"Tests for timeout handling.\"\"\"",
        "",
        "    def test_timeout_handler_no_timeout(self):",
        "        \"\"\"Test that timeout=0 means no timeout.\"\"\"",
        "        sys.path.insert(0, str(Path(__file__).parent.parent / 'scripts'))",
        "        from index_codebase import timeout_handler",
        "",
        "        # Should complete without issue",
        "        with timeout_handler(0):",
        "            result = 1 + 1",
        "        self.assertEqual(result, 2)",
        "",
        "    def test_timeout_handler_completes_in_time(self):",
        "        \"\"\"Test that operations completing in time succeed.\"\"\"",
        "        sys.path.insert(0, str(Path(__file__).parent.parent / 'scripts'))",
        "        from index_codebase import timeout_handler",
        "",
        "        with timeout_handler(5):",
        "            result = sum(range(100))",
        "        self.assertEqual(result, 4950)",
        "",
        "",
        "class TestIndexingFunctions(unittest.TestCase):",
        "    \"\"\"Tests for indexing helper functions.\"\"\"",
        "",
        "    def setUp(self):",
        "        \"\"\"Set up temporary directory with test files.\"\"\"",
        "        self.temp_dir = tempfile.mkdtemp()",
        "        self.base_path = Path(self.temp_dir)",
        "",
        "        # Create test file structure",
        "        (self.base_path / \"cortical\").mkdir()",
        "        (self.base_path / \"tests\").mkdir()",
        "        (self.base_path / \"cortical\" / \"test.py\").write_text(\"# Test file\\nprint('hello')\")",
        "        (self.base_path / \"tests\" / \"test_test.py\").write_text(\"# Test\\nimport unittest\")",
        "        (self.base_path / \"CLAUDE.md\").write_text(\"# Documentation\")",
        "",
        "    def tearDown(self):",
        "        \"\"\"Clean up temporary files.\"\"\"",
        "        import shutil",
        "        shutil.rmtree(self.temp_dir)",
        "",
        "    def test_get_python_files(self):",
        "        \"\"\"Test Python file discovery.\"\"\"",
        "        sys.path.insert(0, str(Path(__file__).parent.parent / 'scripts'))",
        "        from index_codebase import get_python_files",
        "",
        "        files = get_python_files(self.base_path)",
        "        file_names = [f.name for f in files]",
        "",
        "        self.assertIn(\"test.py\", file_names)",
        "        self.assertIn(\"test_test.py\", file_names)",
        "",
        "    def test_get_doc_files(self):",
        "        \"\"\"Test documentation file discovery.\"\"\"",
        "        sys.path.insert(0, str(Path(__file__).parent.parent / 'scripts'))",
        "        from index_codebase import get_doc_files",
        "",
        "        files = get_doc_files(self.base_path)",
        "        file_names = [f.name for f in files]",
        "",
        "        self.assertIn(\"CLAUDE.md\", file_names)",
        "",
        "    def test_create_doc_id(self):",
        "        \"\"\"Test document ID creation.\"\"\"",
        "        sys.path.insert(0, str(Path(__file__).parent.parent / 'scripts'))",
        "        from index_codebase import create_doc_id",
        "",
        "        file_path = self.base_path / \"cortical\" / \"test.py\"",
        "        doc_id = create_doc_id(file_path, self.base_path)",
        "",
        "        self.assertEqual(doc_id, \"cortical/test.py\")",
        "",
        "    def test_get_file_mtime(self):",
        "        \"\"\"Test file modification time retrieval.\"\"\"",
        "        sys.path.insert(0, str(Path(__file__).parent.parent / 'scripts'))",
        "        from index_codebase import get_file_mtime",
        "",
        "        file_path = self.base_path / \"CLAUDE.md\"",
        "        mtime = get_file_mtime(file_path)",
        "",
        "        self.assertIsInstance(mtime, float)",
        "        self.assertGreater(mtime, 0)",
        "",
        "    def test_index_file(self):",
        "        \"\"\"Test single file indexing.\"\"\"",
        "        sys.path.insert(0, str(Path(__file__).parent.parent / 'scripts'))",
        "        from index_codebase import index_file",
        "",
        "        processor = CorticalTextProcessor()",
        "        file_path = self.base_path / \"cortical\" / \"test.py\"",
        "",
        "        metadata = index_file(processor, file_path, self.base_path)",
        "",
        "        self.assertIsNotNone(metadata)",
        "        self.assertEqual(metadata['relative_path'], \"cortical/test.py\")",
        "        self.assertEqual(metadata['file_type'], \".py\")",
        "        self.assertEqual(metadata['language'], \"python\")",
        "        self.assertIn(\"cortical/test.py\", processor.documents)",
        "",
        "    def test_index_file_with_read_error(self):",
        "        \"\"\"Test handling of unreadable files.\"\"\"",
        "        sys.path.insert(0, str(Path(__file__).parent.parent / 'scripts'))",
        "        from index_codebase import index_file, ProgressTracker",
        "",
        "        processor = CorticalTextProcessor()",
        "        tracker = ProgressTracker(quiet=True)",
        "        nonexistent = self.base_path / \"nonexistent.py\"",
        "",
        "        metadata = index_file(processor, nonexistent, self.base_path, tracker)",
        "",
        "        self.assertIsNone(metadata)",
        "        self.assertEqual(len(tracker.warnings), 1)",
        "",
        "",
        "class TestFullIndexFunction(unittest.TestCase):",
        "    \"\"\"Tests for full_index function.\"\"\"",
        "",
        "    def setUp(self):",
        "        \"\"\"Set up temporary directory with test files.\"\"\"",
        "        self.temp_dir = tempfile.mkdtemp()",
        "        self.base_path = Path(self.temp_dir)",
        "",
        "        (self.base_path / \"file1.py\").write_text(\"# File 1\\nprint('a')\")",
        "        (self.base_path / \"file2.py\").write_text(\"# File 2\\nprint('b')\")",
        "",
        "    def tearDown(self):",
        "        \"\"\"Clean up temporary files.\"\"\"",
        "        import shutil",
        "        shutil.rmtree(self.temp_dir)",
        "",
        "    def test_full_index(self):",
        "        \"\"\"Test full indexing of files.\"\"\"",
        "        sys.path.insert(0, str(Path(__file__).parent.parent / 'scripts'))",
        "        from index_codebase import full_index, ProgressTracker",
        "",
        "        processor = CorticalTextProcessor()",
        "        tracker = ProgressTracker(quiet=True)",
        "        all_files = list(self.base_path.glob(\"*.py\"))",
        "",
        "        indexed, total_lines, file_mtimes = full_index(",
        "            processor, all_files, self.base_path, tracker",
        "        )",
        "",
        "        self.assertEqual(indexed, 2)",
        "        self.assertGreater(total_lines, 0)",
        "        self.assertEqual(len(file_mtimes), 2)",
        "        self.assertIn(\"Indexing files\", tracker.phases)",
        "",
        "",
        "class TestIncrementalIndexFunction(unittest.TestCase):",
        "    \"\"\"Tests for incremental_index function.\"\"\"",
        "",
        "    def setUp(self):",
        "        \"\"\"Set up temporary directory with test files.\"\"\"",
        "        self.temp_dir = tempfile.mkdtemp()",
        "        self.base_path = Path(self.temp_dir)",
        "",
        "        (self.base_path / \"existing.py\").write_text(\"# Existing\\nprint('x')\")",
        "        (self.base_path / \"new.py\").write_text(\"# New\\nprint('y')\")",
        "",
        "    def tearDown(self):",
        "        \"\"\"Clean up temporary files.\"\"\"",
        "        import shutil",
        "        shutil.rmtree(self.temp_dir)",
        "",
        "    def test_incremental_index_added_files(self):",
        "        \"\"\"Test incremental indexing of added files.\"\"\"",
        "        sys.path.insert(0, str(Path(__file__).parent.parent / 'scripts'))",
        "        from index_codebase import incremental_index, ProgressTracker",
        "",
        "        processor = CorticalTextProcessor()",
        "        tracker = ProgressTracker(quiet=True)",
        "",
        "        added = [self.base_path / \"new.py\"]",
        "        modified = []",
        "        deleted = []",
        "",
        "        added_count, modified_count, deleted_count, total_lines = incremental_index(",
        "            processor, added, modified, deleted, self.base_path, tracker",
        "        )",
        "",
        "        self.assertEqual(added_count, 1)",
        "        self.assertEqual(modified_count, 0)",
        "        self.assertEqual(deleted_count, 0)",
        "        self.assertIn(\"new.py\", processor.documents)",
        "",
        "    def test_incremental_index_modified_files(self):",
        "        \"\"\"Test incremental indexing of modified files.\"\"\"",
        "        sys.path.insert(0, str(Path(__file__).parent.parent / 'scripts'))",
        "        from index_codebase import incremental_index, index_file, ProgressTracker",
        "",
        "        processor = CorticalTextProcessor()",
        "        tracker = ProgressTracker(quiet=True)",
        "",
        "        # First, index the existing file",
        "        index_file(processor, self.base_path / \"existing.py\", self.base_path)",
        "",
        "        # Now modify it (in our test, just re-index as modified)",
        "        added = []",
        "        modified = [self.base_path / \"existing.py\"]",
        "        deleted = []",
        "",
        "        added_count, modified_count, deleted_count, total_lines = incremental_index(",
        "            processor, added, modified, deleted, self.base_path, tracker",
        "        )",
        "",
        "        self.assertEqual(modified_count, 1)",
        "",
        "    def test_incremental_index_deleted_files(self):",
        "        \"\"\"Test incremental indexing handles deleted files.\"\"\"",
        "        sys.path.insert(0, str(Path(__file__).parent.parent / 'scripts'))",
        "        from index_codebase import incremental_index, index_file, ProgressTracker",
        "",
        "        processor = CorticalTextProcessor()",
        "        tracker = ProgressTracker(quiet=True)",
        "",
        "        # First, index a file",
        "        index_file(processor, self.base_path / \"existing.py\", self.base_path)",
        "        self.assertIn(\"existing.py\", processor.documents)",
        "",
        "        # Now mark it as deleted",
        "        added = []",
        "        modified = []",
        "        deleted = [\"existing.py\"]",
        "",
        "        added_count, modified_count, deleted_count, total_lines = incremental_index(",
        "            processor, added, modified, deleted, self.base_path, tracker",
        "        )",
        "",
        "        self.assertEqual(deleted_count, 1)",
        "        self.assertNotIn(\"existing.py\", processor.documents)",
        "",
        "",
        "class TestComputeAnalysis(unittest.TestCase):",
        "    \"\"\"Tests for compute_analysis function.\"\"\"",
        "",
        "    def test_compute_analysis_fast_mode(self):",
        "        \"\"\"Test fast mode analysis.\"\"\"",
        "        sys.path.insert(0, str(Path(__file__).parent.parent / 'scripts'))",
        "        from index_codebase import compute_analysis, ProgressTracker",
        "",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(\"doc1\", \"Neural networks are powerful.\")",
        "        processor.process_document(\"doc2\", \"Machine learning algorithms.\")",
        "",
        "        tracker = ProgressTracker(quiet=True)",
        "        compute_analysis(processor, tracker, fast_mode=True)",
        "",
        "        self.assertIn(\"Computing analysis (fast mode)\", tracker.phases)",
        "        # TF-IDF should be computed",
        "        layer0 = processor.layers[CorticalLayer.TOKENS]",
        "        neural_col = layer0.get_minicolumn(\"neural\")",
        "        self.assertIsNotNone(neural_col)",
        "        self.assertGreater(neural_col.tfidf, 0)",
        "",
        "",
        "if __name__ == '__main__':",
        "    unittest.main()"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    }
  ],
  "hour_of_day": 23,
  "day_of_week": "Wednesday",
  "seconds_since_last_commit": -397598,
  "is_merge": true,
  "is_initial": false,
  "parent_count": 2,
  "session_id": null,
  "related_chats": [],
  "ci_result": null,
  "reverted": false,
  "amended": false
}