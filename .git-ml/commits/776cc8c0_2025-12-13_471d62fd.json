{
  "hash": "776cc8c02e7e83d7962cc24a8a6cea980a616da8",
  "message": "Merge main into feature branch, resolve TASK_LIST.md conflicts",
  "author": "Claude",
  "timestamp": "2025-12-13 15:01:56 +0000",
  "branch": "claude/multi-index-design-DvifZ",
  "files_changed": [
    ".github/workflows/ci.yml",
    "TASK_LIST.md",
    "scripts/validate_task_list.py",
    "tests/test_behavioral.py",
    "tests/test_intent_query.py",
    "tests/test_query_optimization.py"
  ],
  "insertions": 169,
  "deletions": 1091,
  "hunks": [
    {
      "file": "workflows/ci.yml b/.github/workflows/ci.yml",
      "function": "jobs:",
      "start_line": 156,
      "lines_added": [
        "        # Includes tests/integration/ plus remaining legacy tests that provide unique coverage",
        "        # Note: 3 legacy tests removed 2025-12-13 (test_behavioral.py, test_intent_query.py,",
        "        #       test_query_optimization.py) - now covered by unit tests"
      ],
      "lines_removed": [
        "        # Includes tests/integration/ plus remaining legacy tests (not yet migrated)",
        "        # Note: 16 legacy tests removed 2025-12-13 - now covered by tests/unit/",
        "          tests/test_query_optimization.py \\",
        "          tests/test_intent_query.py \\"
      ],
      "context_before": [
        "",
        "    - name: Install test dependencies",
        "      run: |",
        "        python -m pip install --upgrade pip",
        "        pip install pytest coverage",
        "",
        "    - name: Run integration tests with coverage",
        "      run: |",
        "        echo \"=== Running Integration Tests ===\"",
        "        # Run integration tests - single pytest call is faster than multiple unittest discovers"
      ],
      "context_after": [
        "        coverage run --source=cortical -m pytest \\",
        "          tests/integration/ \\",
        "          tests/test_incremental_indexing.py \\",
        "          tests/test_edge_cases.py \\",
        "          tests/test_coverage_gaps.py \\",
        "          tests/test_analyze_louvain_resolution.py \\",
        "          tests/test_evaluate_cluster.py \\",
        "          tests/test_cli_wrapper.py \\",
        "          tests/test_search_codebase.py \\",
        "          tests/test_ask_codebase.py \\",
        "          tests/test_generate_ai_metadata.py \\",
        "          tests/test_showcase.py \\",
        "          -v --tb=short",
        "",
        "        coverage report --include=\"cortical/*\""
      ],
      "change_type": "modify"
    },
    {
      "file": "workflows/ci.yml b/.github/workflows/ci.yml",
      "function": "jobs:",
      "start_line": 236,
      "lines_added": [
        "        # Note: tests/test_behavioral.py removed 2025-12-13 - superseded by tests/behavioral/",
        "        python -m pytest tests/behavioral/ -v --tb=short"
      ],
      "lines_removed": [
        "        # Includes tests/behavioral/ plus legacy test_behavioral.py",
        "        python -m pytest tests/behavioral/ tests/test_behavioral.py -v --tb=short"
      ],
      "context_before": [
        "        python-version: '3.11'",
        "",
        "    - name: Install test dependencies",
        "      run: |",
        "        python -m pip install --upgrade pip",
        "        pip install pytest",
        "",
        "    - name: Run behavioral tests",
        "      run: |",
        "        echo \"=== Running Behavioral Tests ===\""
      ],
      "context_after": [
        "        echo \"‚úÖ Behavioral quality verified\"",
        "",
        "  # ==========================================================================",
        "  # Stage 6: Performance Tests (< 1 min, no coverage)",
        "  # Timing-based tests to catch performance regressions",
        "  # Runs in PARALLEL with unit/integration after smoke-tests pass",
        "  # ==========================================================================",
        "  performance-tests:",
        "    name: \"‚è±Ô∏è Performance Tests\"",
        "    runs-on: ubuntu-latest"
      ],
      "change_type": "modify"
    },
    {
      "file": "TASK_LIST.md",
      "function": null,
      "start_line": 1,
      "lines_added": [
        "**Pending Tasks:** 24",
        "**Completed Tasks:** 223 (see archive)",
        "",
        "**Legacy Test Cleanup:** ‚úÖ COMPLETE - All 8 tasks investigated (#198-205)",
        "- **KEEP (7 files, 506 tests):** Provide unique coverage not duplicated in unit tests",
        "  - #198 test_coverage_gaps.py (91 tests) - edge case coverage",
        "  - #199 test_cli_wrapper.py (96 tests) - CLI wrapper framework",
        "  - #200 test_edge_cases.py (53 tests) - robustness tests",
        "  - #201 test_incremental_indexing.py (47 tests) - script integration",
        "  - #205 Script tests: 6 files (132 tests) - scripts/ directory",
        "- **DELETED (3 files, 53 tests):** Covered by unit tests",
        "  - #202 test_intent_query.py - covered by tests/unit/test_query.py",
        "  - #203 test_behavioral.py - superseded by tests/behavioral/",
        "  - #204 test_query_optimization.py - covered by tests/unit/test_query_search.py"
      ],
      "lines_removed": [
        "**Pending Tasks:** 23",
        "**Completed Tasks:** 215 (see archive)",
        "",
        "**Legacy Test Cleanup:** 16 duplicated legacy tests removed, 13 remaining need investigation",
        "- See Tasks #198-205 for legacy test investigation"
      ],
      "context_before": [
        "# Task List: Cortical Text Processor",
        "",
        "Active backlog for the Cortical Text Processor project. Completed tasks are archived in [TASK_ARCHIVE.md](TASK_ARCHIVE.md).",
        "",
        "**Last Updated:** 2025-12-13"
      ],
      "context_after": [
        "",
        "**Unit Test Initiative:** ‚úÖ COMPLETE - 85% coverage from unit tests (1,729 tests)",
        "- 19 modules at 90%+ coverage",
        "- See [Coverage Baseline](#unit-test-coverage-baseline) for per-module status",
        "",
        "---",
        "",
        "## Active Backlog",
        "",
        "<!-- Machine-parseable format for automation -->"
      ],
      "change_type": "modify"
    },
    {
      "file": "TASK_LIST.md",
      "function": "Active backlog for the Cortical Text Processor project. Completed tasks are arch",
      "start_line": 28,
      "lines_added": [],
      "lines_removed": [
        "| 198 | Investigate legacy test_coverage_gaps.py (91 tests) | Testing | - | Medium |",
        "| 199 | Investigate legacy test_cli_wrapper.py (96 tests) | Testing | - | Medium |",
        "| 200 | Investigate legacy test_edge_cases.py (53 tests) | Testing | - | Small |",
        "| 201 | Investigate legacy test_incremental_indexing.py (47 tests) | Testing | - | Small |",
        "| 202 | Investigate legacy test_intent_query.py (24 tests) | Testing | - | Small |",
        "| 203 | Investigate legacy test_behavioral.py (9 tests) | Testing | - | Small |",
        "| 204 | Investigate legacy test_query_optimization.py (20 tests) | Testing | - | Small |",
        "| 205 | Investigate legacy script tests (6 files, 132 tests) | Testing | - | Medium |"
      ],
      "context_before": [
        "### üü° Medium (Do This Month)",
        "",
        "| # | Task | Category | Depends | Effort |",
        "|---|------|----------|---------|--------|",
        "| 133 | Implement WAL + snapshot persistence (fault-tolerant rebuild) | Arch | 132 | Large |",
        "| 134 | Implement protobuf serialization for corpus | Arch | 132 | Medium |",
        "| 135 | Implement chunked parallel processing for full-analysis | Arch | 132 | Large |",
        "| 95 | Split processor.py into modules | Arch | - | Large |",
        "| 99 | Add input validation to public methods | CodeQual | - | Medium |",
        "| 107 | Add Quick Context to tasks | TaskMgmt | - | Medium |"
      ],
      "context_after": [
        "",
        "### üü¢ Low (Backlog)",
        "",
        "| # | Task | Category | Depends | Effort |",
        "|---|------|----------|---------|--------|",
        "| 73 | Add \"Find Similar Code\" command | DevEx | - | Medium |",
        "| 74 | Add \"Explain This Code\" command | DevEx | - | Medium |",
        "| 75 | Add \"What Changed?\" semantic diff | DevEx | - | Large |",
        "| 76 | Add \"Suggest Related Files\" feature | DevEx | - | Medium |",
        "| 78 | Add code pattern detection | DevEx | - | Large |"
      ],
      "change_type": "delete"
    },
    {
      "file": "TASK_LIST.md",
      "function": "Active backlog for the Cortical Text Processor project. Completed tasks are arch",
      "start_line": 95,
      "lines_added": [
        "- #198-205 Legacy test investigation COMPLETE - 8 tasks, 10 files reviewed",
        "  - DELETED 3 duplicate files (53 tests): test_behavioral.py, test_intent_query.py, test_query_optimization.py",
        "  - KEPT 7 unique files (506 tests): test_coverage_gaps.py, test_cli_wrapper.py, test_edge_cases.py, test_incremental_indexing.py, + 6 script tests"
      ],
      "lines_removed": [],
      "context_before": [
        "<!-- Note: Task #87 was completed 2025-12-13, moved to archive -->",
        "",
        "---",
        "",
        "## Recently Completed",
        "",
        "All completed tasks are now archived in [TASK_ARCHIVE.md](TASK_ARCHIVE.md).",
        "",
        "**Latest completions (2025-12-13):**",
        "- #192 Deduplicate connections storage - typed_connections is now single source of truth, lateral_connections is cached property (15 tests)"
      ],
      "context_after": [
        "- #197 Task list validation in CI - Added validate-task-list job to workflow",
        "- #186 Simplified facade methods - quick_search(), rag_retrieve(), explore() (23 tests)",
        "- #196 Spectral embeddings warning - RuntimeWarning for large graphs (>5000 terms)",
        "- #193 Unify alpha validation - retrofit_embeddings() now accepts [0,1] consistently",
        "- #194 Layer validation - Added checks for invalid layer values (0-3) in persistence/layers",
        "- #195 Stopwords import - semantics.py now uses Tokenizer.DEFAULT_STOP_WORDS",
        "- #148 Performance test refactor - Moved to small synthetic corpus (25 docs)",
        "- #149 Performance test fix - Tests now use small_corpus.py fixtures",
        "- #182 Fluent API - FluentProcessor with method chaining (44 tests)",
        "- #183 Progress Feedback - ConsoleProgressReporter, callbacks (30 tests)"
      ],
      "change_type": "add"
    },
    {
      "file": "TASK_LIST.md",
      "function": "All completed tasks are now archived in [TASK_ARCHIVE.md](TASK_ARCHIVE.md).",
      "start_line": 139,
      "lines_added": [],
      "lines_removed": [
        "## Legacy Test Investigation Tasks",
        "",
        "These tasks were created during the test coverage review (2025-12-13). 16 duplicated legacy tests were removed, and 13 remaining tests need investigation to determine if they should be migrated to categorized test directories or kept as-is. Check git history for any previous migration work.",
        "",
        "### 198. Investigate test_coverage_gaps.py (91 tests)",
        "",
        "**Meta:** `status:pending` `priority:medium` `category:testing`",
        "**Files:** `tests/test_coverage_gaps.py`",
        "**Effort:** Medium",
        "",
        "**Problem:** 91 tests covering edge cases and coverage gaps. Investigate:",
        "1. Check git history for previous migration attempts",
        "2. Determine if tests should move to `tests/unit/` or `tests/regression/`",
        "3. Check for overlap with existing unit tests",
        "",
        "",
        "### 199. Investigate test_cli_wrapper.py (96 tests)",
        "",
        "**Meta:** `status:pending` `priority:medium` `category:testing`",
        "**Files:** `tests/test_cli_wrapper.py`",
        "**Effort:** Medium",
        "",
        "**Problem:** 96 tests for CLI wrapper. Investigate:",
        "1. Check git history for migration attempts",
        "2. Determine if these belong in `tests/integration/`",
        "3. Verify no unit test coverage exists",
        "",
        "",
        "### 200. Investigate test_edge_cases.py (53 tests)",
        "",
        "**Meta:** `status:pending` `priority:medium` `category:testing`",
        "**Files:** `tests/test_edge_cases.py`",
        "**Effort:** Small",
        "",
        "**Problem:** 53 tests for edge cases (Unicode, large docs, malformed inputs). Investigate:",
        "1. Check git history",
        "2. Consider moving to `tests/unit/` or creating `tests/robustness/`",
        "",
        "",
        "### 201. Investigate test_incremental_indexing.py (47 tests)",
        "",
        "**Meta:** `status:pending` `priority:medium` `category:testing`",
        "**Files:** `tests/test_incremental_indexing.py`",
        "**Effort:** Small",
        "",
        "**Problem:** 47 tests for incremental document operations. Investigate:",
        "1. Check git history",
        "2. Verify no overlap with `tests/unit/test_processor_core.py`",
        "3. Consider moving to `tests/integration/`",
        "",
        "",
        "### 202. Investigate test_intent_query.py (24 tests)",
        "",
        "**Meta:** `status:pending` `priority:medium` `category:testing`",
        "**Files:** `tests/test_intent_query.py`",
        "**Effort:** Small",
        "",
        "**Problem:** 24 tests for intent-based query parsing. Investigate:",
        "1. Check git history",
        "2. Check if `tests/unit/test_query.py` covers this",
        "3. Consider moving to `tests/unit/test_query_intent.py`",
        "",
        "",
        "### 203. Investigate test_behavioral.py (9 tests)",
        "",
        "**Meta:** `status:pending` `priority:medium` `category:testing`",
        "**Files:** `tests/test_behavioral.py`",
        "**Effort:** Small",
        "",
        "**Problem:** 9 behavioral/acceptance tests. Investigate:",
        "1. Check git history",
        "2. Move to `tests/behavioral/` if appropriate",
        "",
        "",
        "### 204. Investigate test_query_optimization.py (20 tests)",
        "",
        "**Meta:** `status:pending` `priority:medium` `category:testing`",
        "**Files:** `tests/test_query_optimization.py`",
        "**Effort:** Small",
        "",
        "**Problem:** 20 tests for query performance. Investigate:",
        "1. Check git history",
        "2. Consider moving to `tests/performance/`",
        "",
        "",
        "### 205. Investigate legacy script tests (6 files)",
        "",
        "**Meta:** `status:pending` `priority:medium` `category:testing`",
        "**Files:** `tests/test_analyze_louvain_resolution.py`, `tests/test_ask_codebase.py`, `tests/test_evaluate_cluster.py`, `tests/test_generate_ai_metadata.py`, `tests/test_search_codebase.py`, `tests/test_showcase.py`",
        "**Effort:** Medium",
        "",
        "**Problem:** 6 test files (132 tests total) for scripts and showcase. Investigate:",
        "1. Check git history for each",
        "2. Consider creating `tests/scripts/` directory",
        "3. Or moving to `tests/integration/`",
        "",
        ""
      ],
      "context_before": [
        "- `add_document(doc_id, content)` ‚Üí index document",
        "",
        "**Acceptance:**",
        "- [ ] Works in Claude Desktop",
        "- [ ] 5+ core tools implemented",
        "- [ ] Documentation for installation",
        "- [ ] Example MCP config file",
        "",
        "---",
        ""
      ],
      "context_after": [
        "## Unit Test Coverage Baseline",
        "",
        "‚úÖ **Unit test coverage as of 2025-12-13 (1,729 tests, 85% overall):**",
        "",
        "| Module | Coverage | Status | Task |",
        "|--------|----------|--------|------|",
        "| config.py | 100% | ‚úÖ | #168 |",
        "| minicolumn.py | 100% | ‚úÖ | #162 |",
        "| definitions.py | 100% | ‚úÖ | #173 |",
        "| tokenizer.py | 99% | ‚úÖ | #159 |"
      ],
      "change_type": "delete"
    },
    {
      "file": "TASK_LIST.md",
      "function": "These tasks were created during the test coverage review (2025-12-13). 16 duplic",
      "start_line": 280,
      "lines_added": [
        "| Arch | 6 | Architecture refactoring (#133, 134, 135, 95, 100, 101) |",
        "| Testing | 1 | Test coverage (#129) |"
      ],
      "lines_removed": [
        "| Arch | 5 | Architecture refactoring (#133, 134, 135, 95, 100, 101) |",
        "| Testing | 9 | Test coverage and legacy investigation (#129, 198-205) |"
      ],
      "context_before": [
        "| processor.py | 85% | üî∂ | #165-166 |",
        "",
        "**19 of 21 modules at 90%+ coverage**",
        "",
        "---",
        "",
        "## Category Index",
        "",
        "| Category | Pending | Description |",
        "|----------|---------|-------------|"
      ],
      "context_after": [
        "| CodeQual | 1 | Code quality improvements (#99) |",
        "| TaskMgmt | 3 | Task management system (#106, 107, 108) |",
        "| AINav | 2 | AI assistant navigation (#117, 118) |",
        "| DevEx | 7 | Developer experience, scripts (#73-80) |",
        "| Research | 2 | Research and analysis (#140, 131) |",
        "| Samples | 1 | Sample document improvements (#130) |",
        "| Integration | 1 | MCP Server (#184) |",
        "",
        "*Updated 2025-12-13 - Unit test initiative COMPLETE (85% coverage, 1,729 tests)*",
        "",
        "---"
      ],
      "change_type": "modify"
    },
    {
      "file": "scripts/validate_task_list.py",
      "function": "Checks for:",
      "start_line": 10,
      "lines_added": [
        "def find_completed_tasks_in_pending_section(task_list: str) -> list[tuple[str, str, str]]:",
        "    \"\"\"",
        "    Find tasks with completion markers in the Pending Task Details section.",
        "",
        "    Returns list of (task_number, task_title, marker_found) tuples.",
        "    \"\"\"",
        "    results = []",
        "",
        "    if \"## Pending Task Details\" not in task_list:",
        "        return results",
        "",
        "    pending_section = task_list.split(\"## Pending Task Details\")[1]",
        "",
        "    # Exclude coverage baseline table (which legitimately has ‚úÖ markers)",
        "    if \"## Unit Test Coverage Baseline\" in pending_section:",
        "        pending_section = pending_section.split(\"## Unit Test Coverage Baseline\")[0]",
        "    if \"## Category Index\" in pending_section:",
        "        pending_section = pending_section.split(\"## Category Index\")[0]",
        "",
        "    # Find task headers with completion markers",
        "    # Pattern: ### 123. Task Title ‚úÖ or ### 123. Task Title",
        "    task_pattern = r'### (\\d+)\\. ([^\\n]*)'",
        "    for match in re.finditer(task_pattern, pending_section):",
        "        task_num = match.group(1)",
        "        task_title = match.group(2).strip()",
        "",
        "        # Check for completion markers in the title",
        "        if '‚úÖ' in task_title:",
        "            results.append((task_num, task_title.replace('‚úÖ', '').strip(), '‚úÖ in title'))",
        "        elif '‚úì' in task_title:",
        "            results.append((task_num, task_title.replace('‚úì', '').strip(), '‚úì in title'))",
        "        else:",
        "            # Check for status:completed in the task's section",
        "            # Get the section content (until next ### or end)",
        "            start = match.end()",
        "            next_task = pending_section.find('### ', start)",
        "            section_end = next_task if next_task != -1 else len(pending_section)",
        "            section_content = pending_section[start:section_end]",
        "",
        "            if '`status:completed`' in section_content:",
        "                results.append((task_num, task_title, '`status:completed` in meta'))",
        "",
        "    return results",
        "",
        "",
        "def count_backlog_tasks(task_list: str) -> tuple[int, dict[str, list[str]]]:",
        "    \"\"\"",
        "    Count actual pending tasks in backlog tables.",
        "",
        "    Returns (total_count, {priority: [task_numbers]}).",
        "    \"\"\"",
        "    tasks_by_priority = {}",
        "    total = 0",
        "",
        "    # Match table rows in backlog sections",
        "    # Pattern: | 123 | Task name | Category | ... |",
        "    in_backlog = False",
        "    current_priority = None",
        "",
        "    for line in task_list.split('\\n'):",
        "        # Detect priority section headers",
        "        if '### üü† High' in line:",
        "            current_priority = 'High'",
        "            in_backlog = True",
        "            tasks_by_priority['High'] = []",
        "        elif '### üü° Medium' in line:",
        "            current_priority = 'Medium'",
        "            in_backlog = True",
        "            tasks_by_priority['Medium'] = []",
        "        elif '### üü¢ Low' in line:",
        "            current_priority = 'Low'",
        "            in_backlog = True",
        "            tasks_by_priority['Low'] = []",
        "        elif '### ‚è∏Ô∏è Deferred' in line or '### üîÆ Future' in line or '### üîÑ In Progress' in line:",
        "            in_backlog = False",
        "            current_priority = None",
        "        elif line.startswith('---'):",
        "            in_backlog = False",
        "            current_priority = None",
        "        elif in_backlog and current_priority and line.startswith('|'):",
        "            # Match task row: | 123 | ... |",
        "            match = re.match(r'\\| (\\d+) \\|', line)",
        "            if match:",
        "                task_num = match.group(1)",
        "                tasks_by_priority[current_priority].append(task_num)",
        "                total += 1",
        "",
        "    return total, tasks_by_priority",
        "",
        "",
        "    details = []  # Additional context for issues",
        "",
        "    claimed_pending = int(count_match.group(1)) if count_match else None",
        "    claimed_completed = int(completed_match.group(1)) if completed_match else None",
        "",
        "    if claimed_pending and claimed_pending > 100:",
        "        issues.append(f\"‚ö†Ô∏è  Pending count seems high: {claimed_pending} tasks\")",
        "        details.append(\"   Consider archiving completed tasks to TASK_ARCHIVE.md\")",
        "",
        "    if claimed_completed is not None and claimed_completed < 1:",
        "        issues.append(f\"‚ö†Ô∏è  Completed count seems low: {claimed_completed}\")",
        "        details.append(\"   Update the header count to match archived tasks\")",
        "",
        "    # Check 1b: Verify claimed pending count matches actual backlog",
        "    actual_pending, tasks_by_priority = count_backlog_tasks(task_list)",
        "    if claimed_pending and actual_pending != claimed_pending:",
        "        issues.append(f\"‚ö†Ô∏è  Pending task count mismatch: header says {claimed_pending}, actual is {actual_pending}\")",
        "        breakdown = \", \".join(f\"{p}: {len(t)}\" for p, t in tasks_by_priority.items() if t)",
        "        details.append(f\"   Breakdown: {breakdown}\")",
        "        details.append(f\"   Fix: Update header '**Pending Tasks:** {actual_pending}'\")",
        "        sorted_stale = sorted(stale, key=int)",
        "        issues.append(f\"‚ö†Ô∏è  {len(stale)} task(s) marked pending but already in archive: #{', #'.join(sorted_stale)}\")",
        "        details.append(\"   Fix: Remove these task details from Pending Task Details section\")",
        "        details.append(\"        (they're already archived in TASK_ARCHIVE.md)\")",
        "                        details.append(f\"   Fix: Update task #{task_num} status to completed and move to archive\")",
        "    # Check 4: Completed task markers in pending task details",
        "    completed_in_pending = find_completed_tasks_in_pending_section(task_list)",
        "    if completed_in_pending:",
        "        issues.append(f\"‚ö†Ô∏è  Found {len(completed_in_pending)} completed task(s) in Pending Task Details section:\")",
        "        for task_num, task_title, marker in completed_in_pending:",
        "            details.append(f\"      - #{task_num}: {task_title[:50]}{'...' if len(task_title) > 50 else ''} ({marker})\")",
        "        details.append(\"   Fix: Move completed task details to TASK_ARCHIVE.md\")",
        "        details.append(\"        or remove the section if summary exists elsewhere\")",
        "        issues.append(f\"‚ö†Ô∏è  TASK_LIST.md has {lines} lines (recommended: <500)\")",
        "        details.append(\"   Consider: Move completed task details to TASK_ARCHIVE.md\")",
        "        details.append(\"            Keep TASK_LIST.md focused on pending/active work\")",
        "        for i, issue in enumerate(issues):",
        "            # Print related details",
        "            for detail in details:",
        "                if detail.startswith(\"   \"):",
        "                    print(f\"  {detail}\")",
        "                    details.remove(detail)",
        "                else:",
        "                    break",
        "",
        "        # Print any remaining details",
        "        if details:",
        "            print(\"\\n  Additional context:\")",
        "            for detail in details:",
        "                print(f\"  {detail}\")",
        "",
        "        print(f\"\\n‚ùå {len(issues)} issue(s) found - see above for fix suggestions\")",
        "        if tasks_by_priority:",
        "            breakdown = \", \".join(f\"{p}: {len(t)}\" for p, t in tasks_by_priority.items() if t)",
        "            print(f\"   - Breakdown: {breakdown}\")"
      ],
      "lines_removed": [
        "    if count_match:",
        "        claimed_pending = int(count_match.group(1))",
        "        if claimed_pending > 100:",
        "            issues.append(f\"‚ö†Ô∏è  Pending count seems high: {claimed_pending} tasks\")",
        "    if completed_match:",
        "        claimed_completed = int(completed_match.group(1))",
        "        # Sanity check - completed should be positive",
        "        if claimed_completed < 1:",
        "            issues.append(f\"‚ö†Ô∏è  Completed count seems low: {claimed_completed}\")",
        "        issues.append(f\"‚ö†Ô∏è  Tasks marked pending but in archive: {', '.join(sorted(stale, key=int))}\")",
        "    # Check 4: Completed task markers (‚úÖ, ‚úì) in pending task details (not coverage tables)",
        "    if \"## Pending Task Details\" in task_list:",
        "        pending_section = task_list.split(\"## Pending Task Details\")[1]",
        "        # Exclude coverage baseline table (which legitimately has ‚úÖ markers)",
        "        if \"## Unit Test Coverage Baseline\" in pending_section:",
        "            pending_section = pending_section.split(\"## Unit Test Coverage Baseline\")[0]",
        "        if \"## Category Index\" in pending_section:",
        "            pending_section = pending_section.split(\"## Category Index\")[0]",
        "        # Look for completion markers in actual task descriptions",
        "        completed_markers = len(re.findall(r'### \\d+\\..*?‚úÖ|### \\d+\\..*?‚úì|`status:completed`', pending_section))",
        "        if completed_markers > 0:",
        "            issues.append(f\"‚ö†Ô∏è  Found {completed_markers} completed tasks in Pending Task Details section\")",
        "        issues.append(f\"‚ö†Ô∏è  TASK_LIST.md has {lines} lines - consider archiving completed task details\")",
        "        for issue in issues:",
        "        print(f\"\\n‚ùå {len(issues)} issue(s) found\")"
      ],
      "context_before": [
        "- Mismatched pending task counts",
        "- Stale \"In Progress\" tasks",
        "- Completed tasks with full details still in TASK_LIST.md",
        "\"\"\"",
        "",
        "import re",
        "import os",
        "from pathlib import Path",
        "",
        ""
      ],
      "context_after": [
        "def main():",
        "    task_list = Path(\"TASK_LIST.md\").read_text()",
        "    archive = Path(\"TASK_ARCHIVE.md\").read_text() if Path(\"TASK_ARCHIVE.md\").exists() else \"\"",
        "",
        "    issues = []",
        "",
        "    # Check 1: Header exists and has reasonable values",
        "    count_match = re.search(r'\\*\\*Pending Tasks:\\*\\* (\\d+)', task_list)",
        "    completed_match = re.search(r'\\*\\*Completed Tasks:\\*\\* (\\d+)', task_list)",
        "",
        "    # Check 2: Tasks with status:pending that are in archive",
        "    pending_tasks = re.findall(r'`status:pending`.*?#(\\d+)', task_list)",
        "    archived_tasks = re.findall(r'\\| (\\d+) \\|.*?\\| \\d{4}-\\d{2}-\\d{2} \\|', archive)",
        "    stale = set(pending_tasks) & set(archived_tasks)",
        "    if stale:",
        "",
        "    # Check 3: Unit test tasks still marked pending but tests exist",
        "    unit_test_dir = Path(\"tests/unit\")",
        "    if unit_test_dir.exists():",
        "        test_files = list(unit_test_dir.glob(\"test_*.py\"))",
        "        if len(test_files) > 15:  # Substantial unit test coverage exists",
        "            # Check if unit test tasks (#159-178) are still marked pending",
        "            for task_num in range(159, 179):",
        "                if f\"### {task_num}.\" in task_list and \"status:pending\" in task_list:",
        "                    pattern = rf'### {task_num}\\.[^\\n]*\\n\\n\\*\\*Meta:\\*\\* `status:pending`'",
        "                    if re.search(pattern, task_list):",
        "                        issues.append(f\"‚ö†Ô∏è  Task #{task_num} marked pending but tests/unit/ has {len(test_files)} test files\")",
        "                        break",
        "",
        "",
        "    # Check 5: Large file size warning",
        "    lines = task_list.count('\\n')",
        "    if lines > 500:",
        "",
        "    # Report",
        "    if issues:",
        "        print(\"üîç Task List Validation Issues Found:\\n\")",
        "            print(f\"  {issue}\")",
        "        return 1",
        "    else:",
        "        print(\"‚úÖ Task list looks healthy!\")",
        "        print(f\"   - {lines} lines\")",
        "        print(f\"   - Pending: {claimed_pending if count_match else 'unknown'}\")",
        "        print(f\"   - Completed: {claimed_completed if completed_match else 'unknown'}\")",
        "        return 0",
        "",
        "",
        "if __name__ == \"__main__\":",
        "    os.chdir(Path(__file__).parent.parent)",
        "    exit(main())"
      ],
      "change_type": "modify"
    },
    {
      "file": "tests/test_behavioral.py",
      "function": null,
      "start_line": 1,
      "lines_added": [],
      "lines_removed": [
        "\"\"\"",
        "Behavioral Tests for Core User Workflows",
        "=========================================",
        "",
        "These tests verify that the Cortical Text Processor delivers expected user",
        "outcomes. Unlike unit tests (function works correctly) or integration tests",
        "(components work together), behavioral tests verify \"the system feels right.\"",
        "",
        "These tests were created based on dog-fooding issues discovered during",
        "real usage testing (Tasks #141-145), ensuring that common user workflows",
        "produce sensible, timely results.",
        "",
        "Test Categories:",
        "- SearchBehavior: \"Search should feel relevant\"",
        "- PerformanceBehavior: \"System should feel responsive\"",
        "- QualityBehavior: \"Results should make sense\"",
        "- RobustnessBehavior: \"System should handle edge cases gracefully\"",
        "",
        "Task #146 Implementation",
        "\"\"\"",
        "",
        "import os",
        "import sys",
        "import time",
        "import unittest",
        "",
        "sys.path.insert(0, '..')",
        "",
        "from cortical import CorticalTextProcessor, CorticalLayer",
        "from cortical.tokenizer import Tokenizer",
        "",
        "",
        "# Module-level singleton for shared corpus (load once, reuse everywhere)",
        "_SHARED_PROCESSOR = None",
        "_SHARED_PROCESSOR_LOADED = False",
        "",
        "",
        "def get_shared_processor() -> CorticalTextProcessor:",
        "    \"\"\"",
        "    Get or create the shared processor with sample corpus.",
        "",
        "    This singleton ensures we only load the corpus once per test run,",
        "    dramatically reducing test time (from 5x compute_all to 1x).",
        "    \"\"\"",
        "    global _SHARED_PROCESSOR, _SHARED_PROCESSOR_LOADED",
        "",
        "    if _SHARED_PROCESSOR_LOADED:",
        "        return _SHARED_PROCESSOR",
        "",
        "    samples_dir = os.path.join(os.path.dirname(__file__), '..', 'samples')",
        "",
        "    tokenizer = Tokenizer(filter_code_noise=True)",
        "    processor = CorticalTextProcessor(tokenizer=tokenizer)",
        "",
        "    # Load all sample files",
        "    loaded = 0",
        "    for filename in os.listdir(samples_dir):",
        "        filepath = os.path.join(samples_dir, filename)",
        "        if os.path.isfile(filepath):",
        "            try:",
        "                with open(filepath, 'r', encoding='utf-8') as f:",
        "                    content = f.read()",
        "                doc_id = os.path.splitext(filename)[0]",
        "                processor.process_document(doc_id, content)",
        "                loaded += 1",
        "            except (IOError, UnicodeDecodeError):",
        "                continue",
        "",
        "    if loaded > 0:",
        "        processor.compute_all(verbose=False)",
        "",
        "    _SHARED_PROCESSOR = processor",
        "    _SHARED_PROCESSOR_LOADED = True",
        "    return processor",
        "",
        "",
        "class TestSearchBehavior(unittest.TestCase):",
        "    \"\"\"",
        "    Test that search feels relevant to users.",
        "",
        "    These tests verify that:",
        "    - Document names matching queries rank highly",
        "    - Query expansion improves recall without hurting precision",
        "    - Code searches prefer implementations over tests",
        "    \"\"\"",
        "",
        "    @classmethod",
        "    def setUpClass(cls):",
        "        \"\"\"Load corpus once for all search tests.\"\"\"",
        "        cls.processor = get_shared_processor()",
        "",
        "    def test_document_name_matches_rank_highly(self):",
        "        \"\"\"",
        "        Query matching document name should return that doc in top 2.",
        "",
        "        User expectation: If I search for \"distributed systems\" and there's",
        "        a document called \"distributed_systems\", it should be in my top results.",
        "",
        "        Regression test for Task #144 (doc_name_boost fix).",
        "        \"\"\"",
        "        # Test cases: (query, expected_doc_in_top_2)",
        "        test_cases = [",
        "            (\"distributed systems\", \"distributed_systems\"),",
        "            (\"quantum computing\", \"quantum_computing_basics\"),",
        "            (\"fermentation\", \"fermentation_science\"),",
        "            (\"pagerank\", \"pagerank_fundamentals\"),",
        "            (\"neural network optimization\", \"neural_network_optimization\"),",
        "        ]",
        "",
        "        for query, expected_doc in test_cases:",
        "            with self.subTest(query=query):",
        "                results = self.processor.find_documents_for_query(query, top_n=3)",
        "                top_3_docs = [doc_id for doc_id, score in results]",
        "",
        "                self.assertIn(",
        "                    expected_doc,",
        "                    top_3_docs,",
        "                    f\"Query '{query}' should return '{expected_doc}' in top 3, \"",
        "                    f\"got: {top_3_docs}\"",
        "                )",
        "",
        "    def test_query_expansion_improves_recall(self):",
        "        \"\"\"",
        "        Expanded queries should find more relevant docs than exact match.",
        "",
        "        User expectation: If I search for \"ML models\", I should get docs",
        "        about \"machine learning\" even if they don't use the abbreviation.",
        "        \"\"\"",
        "        # Search with expansion",
        "        expanded_results = self.processor.find_documents_for_query(",
        "            \"ML training\",",
        "            top_n=10",
        "        )",
        "        expanded_docs = {doc_id for doc_id, _ in expanded_results}",
        "",
        "        # Check that results include machine learning related docs",
        "        ml_related_docs = {",
        "            'comprehensive_machine_learning',",
        "            'deep_learning_revolution',",
        "            'neural_network_optimization',",
        "        }",
        "",
        "        # At least one ML doc should appear in results",
        "        found_ml_docs = expanded_docs & ml_related_docs",
        "        self.assertGreater(",
        "            len(found_ml_docs),",
        "            0,",
        "            f\"Query 'ML training' should find ML-related docs, got: {expanded_docs}\"",
        "        )",
        "",
        "    def test_code_search_finds_relevant_code_files(self):",
        "        \"\"\"",
        "        Code queries should find relevant code files in results.",
        "",
        "        User expectation: When searching for code concepts like \"data processor\",",
        "        relevant code files should appear in the top results.",
        "",
        "        Note: The test_file_penalty is applied in definition-focused searches",
        "        (find_definition_passages), not in general document search. See Task #128.",
        "        \"\"\"",
        "        results = self.processor.find_documents_for_query(",
        "            \"data processor\",",
        "            top_n=5",
        "        )",
        "",
        "        top_docs = [doc_id for doc_id, _ in results]",
        "",
        "        # Both data_processor and test_data_processor should appear in results",
        "        # (they're both relevant to \"data processor\" query)",
        "        data_processor_found = any(",
        "            'data_processor' in doc_id for doc_id in top_docs",
        "        )",
        "",
        "        self.assertTrue(",
        "            data_processor_found,",
        "            f\"Query 'data processor' should find data_processor files in results. \"",
        "            f\"Got: {top_docs}\"",
        "        )",
        "",
        "        # Test that results are not empty and have reasonable scores",
        "        self.assertGreater(",
        "            len(results),",
        "            0,",
        "            \"Code search should return results\"",
        "        )",
        "",
        "",
        "# NOTE: Performance tests have been moved to tests/performance/test_performance.py",
        "# which uses a small synthetic corpus for fast, reliable timing tests.",
        "# The old TestPerformanceBehavior class was removed to avoid slow test runs.",
        "",
        "",
        "class TestQualityBehavior(unittest.TestCase):",
        "    \"\"\"",
        "    Test that results make sense to users.",
        "",
        "    These tests verify that:",
        "    - Important terms identified by PageRank are meaningful, not noise",
        "    - Clustering produces coherent groups",
        "    - Embeddings capture semantic similarity",
        "    \"\"\"",
        "",
        "    @classmethod",
        "    def setUpClass(cls):",
        "        \"\"\"Load corpus once for all quality tests.\"\"\"",
        "        cls.processor = get_shared_processor()",
        "",
        "    def test_pagerank_surfaces_meaningful_terms(self):",
        "        \"\"\"",
        "        Top PageRank terms should be domain concepts, not noise.",
        "",
        "        User expectation: The most \"important\" terms should be meaningful",
        "        concepts like \"neural\", \"learning\", \"network\" - not Python syntax",
        "        like \"self\", \"def\", or test artifacts like \"assertequal\".",
        "",
        "        Regression test for Task #141 (filter_code_noise).",
        "        \"\"\"",
        "        layer0 = self.processor.get_layer(CorticalLayer.TOKENS)",
        "",
        "        # Get top 30 PageRank terms",
        "        pagerank_terms = sorted(",
        "            [(col.content, col.pagerank) for col in layer0],",
        "            key=lambda x: -x[1]",
        "        )[:30]",
        "        top_terms = [term for term, _ in pagerank_terms]",
        "",
        "        # Code noise that should NOT appear in top results",
        "        # (when filter_code_noise=True is working)",
        "        noise_tokens = {",
        "            'self', 'def', 'str', 'int', 'float', 'none', 'true', 'false',",
        "            'assertequal', 'asserttrue', 'assertfalse', 'mock',",
        "            'cls', 'args', 'kwargs', 'return', 'import', 'from',",
        "        }",
        "",
        "        # Check that no noise tokens appear in top 30",
        "        found_noise = [t for t in top_terms if t in noise_tokens]",
        "",
        "        self.assertEqual(",
        "            len(found_noise),",
        "            0,",
        "            f\"Top PageRank terms contain noise tokens: {found_noise}. \"",
        "            f\"Top 10 terms: {top_terms[:10]}. \"",
        "            \"Check that filter_code_noise=True is being applied.\"",
        "        )",
        "",
        "    def test_clustering_produces_coherent_groups(self):",
        "        \"\"\"",
        "        Clusters should have good community structure.",
        "",
        "        User expectation: The concept clusters should make sense -",
        "        related terms should be grouped together, and there shouldn't",
        "        be one mega-cluster containing everything.",
        "",
        "        Threshold: modularity > 0.2 (moderate community structure)",
        "        Note: Text corpora with many interconnected terms typically",
        "        achieve modularity 0.2-0.4. Values >0.3 indicate strong structure.",
        "        Based on Tasks #123-125 (Louvain clustering and quality metrics).",
        "        \"\"\"",
        "        from cortical.analysis import compute_clustering_quality",
        "",
        "        layer0 = self.processor.get_layer(CorticalLayer.TOKENS)",
        "        layer2 = self.processor.get_layer(CorticalLayer.CONCEPTS)",
        "",
        "        # Need cluster assignments",
        "        cluster_assignments = {}",
        "        for concept_col in layer2:",
        "            cluster_id = concept_col.content",
        "            # Get tokens in this cluster from feedforward connections",
        "            for token_id in concept_col.feedforward_connections:",
        "                token_col = layer0.get_by_id(token_id)",
        "                if token_col:",
        "                    cluster_assignments[token_col.content] = cluster_id",
        "",
        "        if len(cluster_assignments) > 0:",
        "            # compute_clustering_quality expects layers dict, not single layer",
        "            quality = compute_clustering_quality(self.processor.layers)",
        "",
        "            # Modularity > 0.2 indicates moderate community structure",
        "            # (consistent with tests/behavioral/test_behavioral.py)",
        "            self.assertGreater(",
        "                quality['modularity'],",
        "                0.2,",
        "                f\"Clustering modularity {quality['modularity']:.3f} is below \"",
        "                \"0.2 threshold for moderate community structure. \"",
        "                f\"Quality assessment: {quality['quality_assessment']}\"",
        "            )",
        "",
        "            # Should have multiple clusters (no single mega-cluster)",
        "            num_clusters = len(set(cluster_assignments.values()))",
        "            self.assertGreater(",
        "                num_clusters,",
        "                5,",
        "                f\"Only {num_clusters} clusters found. \"",
        "                \"Expected more diverse clustering for ~100 documents.\"",
        "            )",
        "",
        "    def test_embeddings_capture_semantic_similarity(self):",
        "        \"\"\"",
        "        Similar terms by embedding should be semantically related.",
        "",
        "        User expectation: If I look at what's similar to \"learning\",",
        "        I should see terms like \"neural\", \"training\", \"networks\" -",
        "        not random unrelated concepts.",
        "",
        "        Regression test for Task #145 (embedding quality).",
        "        \"\"\"",
        "        # Get embeddings using tfidf method (best for semantic similarity)",
        "        embeddings = self.processor.compute_graph_embeddings(",
        "            method='tfidf',",
        "            dimensions=64,",
        "            verbose=False",
        "        )",
        "",
        "        if 'learning' in embeddings and len(embeddings) > 10:",
        "            # Find similar terms to \"learning\"",
        "            from cortical.analysis import cosine_similarity",
        "",
        "            learning_emb = embeddings['learning']",
        "            similarities = []",
        "",
        "            for term, emb in embeddings.items():",
        "                if term != 'learning':",
        "                    sim = cosine_similarity(learning_emb, emb)",
        "                    similarities.append((term, sim))",
        "",
        "            # Get top 10 most similar",
        "            similarities.sort(key=lambda x: -x[1])",
        "            top_similar = [term for term, _ in similarities[:10]]",
        "",
        "            # Check that some expected related terms appear",
        "            expected_related = {",
        "                'neural', 'network', 'networks', 'training', 'model',",
        "                'models', 'deep', 'machine', 'data', 'algorithm'",
        "            }",
        "",
        "            found_related = [t for t in top_similar if t in expected_related]",
        "",
        "            self.assertGreater(",
        "                len(found_related),",
        "                2,",
        "                f\"Terms similar to 'learning' should include ML concepts. \"",
        "                f\"Found: {top_similar}. Expected some of: {expected_related}\"",
        "            )",
        "",
        "",
        "class TestRobustnessBehavior(unittest.TestCase):",
        "    \"\"\"",
        "    Test that the system handles edge cases gracefully.",
        "",
        "    These tests verify that:",
        "    - Empty or invalid queries don't crash the system",
        "    - Unknown terms are handled gracefully",
        "    - The system degrades gracefully rather than failing",
        "    \"\"\"",
        "",
        "    @classmethod",
        "    def setUpClass(cls):",
        "        \"\"\"Load corpus once for all robustness tests.\"\"\"",
        "        cls.processor = get_shared_processor()",
        "",
        "    def test_empty_query_raises_value_error(self):",
        "        \"\"\"",
        "        Empty queries should raise ValueError for explicit error handling.",
        "",
        "        User expectation: Empty queries are invalid input. The system should",
        "        fail explicitly with a clear error message rather than silently",
        "        returning empty results (which could mask bugs in calling code).",
        "",
        "        This is documented behavior - see processor.py find_documents_for_query()",
        "        \"\"\"",
        "        # Empty string should raise ValueError",
        "        with self.assertRaises(ValueError) as ctx:",
        "            self.processor.find_documents_for_query(\"\", top_n=5)",
        "        self.assertIn(\"non-empty\", str(ctx.exception).lower())",
        "",
        "        # Whitespace only should also raise ValueError",
        "        with self.assertRaises(ValueError) as ctx:",
        "            self.processor.find_documents_for_query(\"   \", top_n=5)",
        "        self.assertIn(\"non-empty\", str(ctx.exception).lower())",
        "",
        "    def test_unknown_terms_handled_gracefully(self):",
        "        \"\"\"",
        "        Queries with unknown terms should still return results.",
        "",
        "        User expectation: If I search for \"xyzzy123abc\" (nonsense),",
        "        the system should gracefully return empty results or best-effort",
        "        matches, not crash or hang.",
        "        \"\"\"",
        "        # Completely unknown term",
        "        results = self.processor.find_documents_for_query(",
        "            \"xyzzy123abc_unknown_term\",",
        "            top_n=5",
        "        )",
        "        # Should return empty list or partial matches, not crash",
        "        self.assertIsInstance(results, list)",
        "",
        "        # Mix of known and unknown terms",
        "        results = self.processor.find_documents_for_query(",
        "            \"neural xyzzy123abc_unknown\",",
        "            top_n=5",
        "        )",
        "        # Should still find neural-related docs",
        "        self.assertIsInstance(results, list)",
        "",
        "        # Query expansion should handle unknown terms",
        "        expanded = self.processor.expand_query(\"xyzzy123abc_unknown\", max_expansions=10)",
        "        # Should return dict (possibly empty), not crash",
        "        self.assertIsInstance(expanded, dict)",
        "",
        "    def test_special_characters_handled(self):",
        "        \"\"\"",
        "        Queries with special characters should be handled gracefully.",
        "",
        "        User expectation: Pasting code with special chars like",
        "        \"func() { return x; }\" shouldn't crash the search.",
        "        \"\"\"",
        "        special_queries = [",
        "            \"function() { return x; }\",",
        "            \"SELECT * FROM table WHERE id=1\",",
        "            \"@decorator def method(self):\",",
        "            \"http://example.com/path?query=value\",",
        "            \"{{template}} ${variable}\",",
        "        ]",
        "",
        "        for query in special_queries:",
        "            with self.subTest(query=query[:30]):  # Truncate for display",
        "                # Should not raise exceptions",
        "                try:",
        "                    results = self.processor.find_documents_for_query(query, top_n=5)",
        "                    self.assertIsInstance(results, list)",
        "                except Exception as e:",
        "                    self.fail(",
        "                        f\"Query '{query[:30]}...' raised exception: {type(e).__name__}: {e}\"",
        "                    )",
        "",
        "",
        "if __name__ == '__main__':",
        "    unittest.main()"
      ],
      "context_before": [],
      "context_after": [],
      "change_type": "delete"
    },
    {
      "file": "tests/test_intent_query.py",
      "function": null,
      "start_line": 1,
      "lines_added": [],
      "lines_removed": [
        "\"\"\"",
        "Tests for intent-based query understanding.",
        "",
        "Tests the parse_intent_query and search_by_intent functions",
        "used for natural language code search.",
        "\"\"\"",
        "",
        "import unittest",
        "from cortical.query import (",
        "    parse_intent_query,",
        "    search_by_intent,",
        "    QUESTION_INTENTS,",
        "    ACTION_VERBS,",
        "    ParsedIntent,",
        ")",
        "",
        "",
        "class TestParseIntentQuery(unittest.TestCase):",
        "    \"\"\"Test the parse_intent_query function.\"\"\"",
        "",
        "    def test_where_query(self):",
        "        \"\"\"Test parsing 'where' queries for location intent.\"\"\"",
        "        result = parse_intent_query(\"where do we handle authentication?\")",
        "        self.assertEqual(result['intent'], 'location')",
        "        self.assertEqual(result['question_word'], 'where')",
        "        self.assertEqual(result['action'], 'handle')",
        "        self.assertEqual(result['subject'], 'authentication')",
        "",
        "    def test_how_query(self):",
        "        \"\"\"Test parsing 'how' queries for implementation intent.\"\"\"",
        "        result = parse_intent_query(\"how do we validate user input?\")",
        "        self.assertEqual(result['intent'], 'implementation')",
        "        self.assertEqual(result['question_word'], 'how')",
        "        self.assertEqual(result['action'], 'validate')",
        "        self.assertIn('user', [result['subject'], result['expanded_terms']])",
        "",
        "    def test_what_query(self):",
        "        \"\"\"Test parsing 'what' queries for definition intent.\"\"\"",
        "        result = parse_intent_query(\"what is the database schema?\")",
        "        self.assertEqual(result['intent'], 'definition')",
        "        self.assertEqual(result['question_word'], 'what')",
        "",
        "    def test_why_query(self):",
        "        \"\"\"Test parsing 'why' queries for rationale intent.\"\"\"",
        "        result = parse_intent_query(\"why do we cache this data?\")",
        "        self.assertEqual(result['intent'], 'rationale')",
        "        self.assertEqual(result['question_word'], 'why')",
        "",
        "    def test_when_query(self):",
        "        \"\"\"Test parsing 'when' queries for lifecycle intent.\"\"\"",
        "        result = parse_intent_query(\"when does initialization happen?\")",
        "        self.assertEqual(result['intent'], 'lifecycle')",
        "        self.assertEqual(result['question_word'], 'when')",
        "",
        "    def test_no_question_word(self):",
        "        \"\"\"Test parsing queries without question words.\"\"\"",
        "        result = parse_intent_query(\"fetch user data\")",
        "        self.assertEqual(result['intent'], 'search')",
        "        self.assertIsNone(result['question_word'])",
        "        self.assertEqual(result['action'], 'fetch')",
        "",
        "    def test_empty_query(self):",
        "        \"\"\"Test parsing empty query.\"\"\"",
        "        result = parse_intent_query(\"\")",
        "        self.assertEqual(result['intent'], 'search')",
        "        self.assertIsNone(result['action'])",
        "        self.assertIsNone(result['subject'])",
        "        self.assertEqual(result['expanded_terms'], [])",
        "",
        "    def test_punctuation_handling(self):",
        "        \"\"\"Test that punctuation is handled correctly.\"\"\"",
        "        result = parse_intent_query(\"where is authentication???\")",
        "        self.assertEqual(result['intent'], 'location')",
        "        self.assertIn('authentication', result['expanded_terms'])",
        "",
        "    def test_expanded_terms_include_synonyms(self):",
        "        \"\"\"Test that expanded terms include code concept synonyms.\"\"\"",
        "        result = parse_intent_query(\"how to fetch data\")",
        "        # 'fetch' should expand to include related terms",
        "        self.assertIn('fetch', result['expanded_terms'])",
        "        # Should have some related terms (from retrieval group)",
        "        self.assertGreater(len(result['expanded_terms']), 1)",
        "",
        "    def test_action_verb_detection(self):",
        "        \"\"\"Test detection of various action verbs.\"\"\"",
        "        test_cases = [",
        "            (\"validate input\", \"validate\"),",
        "            (\"process request\", \"process\"),",
        "            (\"save user data\", \"save\"),",
        "            (\"delete old records\", \"delete\"),",
        "            (\"transform response\", \"transform\"),",
        "        ]",
        "        for query, expected_action in test_cases:",
        "            result = parse_intent_query(query)",
        "            self.assertEqual(result['action'], expected_action,",
        "                           f\"Failed for query: {query}\")",
        "",
        "    def test_subject_extraction(self):",
        "        \"\"\"Test extraction of query subject.\"\"\"",
        "        result = parse_intent_query(\"handle errors gracefully\")",
        "        self.assertEqual(result['subject'], 'errors')",
        "",
        "    def test_filler_words_removed(self):",
        "        \"\"\"Test that filler words don't become subject/action.\"\"\"",
        "        result = parse_intent_query(\"do we have a database connection?\")",
        "        self.assertNotEqual(result['subject'], 'do')",
        "        self.assertNotEqual(result['subject'], 'we')",
        "        self.assertNotEqual(result['subject'], 'have')",
        "",
        "",
        "class TestQuestionIntents(unittest.TestCase):",
        "    \"\"\"Test the QUESTION_INTENTS mapping.\"\"\"",
        "",
        "    def test_all_question_words_mapped(self):",
        "        \"\"\"Test that common question words are mapped.\"\"\"",
        "        expected_words = ['where', 'how', 'what', 'why', 'when', 'which', 'who']",
        "        for word in expected_words:",
        "            self.assertIn(word, QUESTION_INTENTS)",
        "",
        "    def test_intent_types(self):",
        "        \"\"\"Test that intent types are meaningful.\"\"\"",
        "        self.assertEqual(QUESTION_INTENTS['where'], 'location')",
        "        self.assertEqual(QUESTION_INTENTS['how'], 'implementation')",
        "        self.assertEqual(QUESTION_INTENTS['what'], 'definition')",
        "        self.assertEqual(QUESTION_INTENTS['why'], 'rationale')",
        "",
        "",
        "class TestActionVerbs(unittest.TestCase):",
        "    \"\"\"Test the ACTION_VERBS set.\"\"\"",
        "",
        "    def test_common_verbs_included(self):",
        "        \"\"\"Test that common programming action verbs are included.\"\"\"",
        "        expected_verbs = [",
        "            'handle', 'process', 'create', 'delete', 'update', 'fetch',",
        "            'validate', 'parse', 'transform', 'authenticate', 'initialize'",
        "        ]",
        "        for verb in expected_verbs:",
        "            self.assertIn(verb, ACTION_VERBS)",
        "",
        "    def test_is_frozenset(self):",
        "        \"\"\"Test that ACTION_VERBS is immutable.\"\"\"",
        "        self.assertIsInstance(ACTION_VERBS, frozenset)",
        "",
        "",
        "class TestSearchByIntent(unittest.TestCase):",
        "    \"\"\"Test the search_by_intent function.\"\"\"",
        "",
        "    def setUp(self):",
        "        \"\"\"Set up test processor.\"\"\"",
        "        from cortical import CorticalTextProcessor",
        "        self.processor = CorticalTextProcessor()",
        "        self.processor.process_document(\"auth_handler\", \"\"\"",
        "            Authentication handler module.",
        "            This module handles user authentication and login.",
        "            It validates credentials and creates sessions.",
        "        \"\"\")",
        "        self.processor.process_document(\"data_fetcher\", \"\"\"",
        "            Data fetching utilities.",
        "            Functions to fetch and retrieve data from external APIs.",
        "            Handles HTTP requests and response parsing.",
        "        \"\"\")",
        "        self.processor.process_document(\"validator\", \"\"\"",
        "            Input validation module.",
        "            Validates and sanitizes user input.",
        "            Checks for required fields and data types.",
        "        \"\"\")",
        "        self.processor.compute_all()",
        "",
        "    def test_search_returns_results(self):",
        "        \"\"\"Test that search returns results.\"\"\"",
        "        results = self.processor.search_by_intent(\"where do we handle authentication?\")",
        "        self.assertIsInstance(results, list)",
        "        # Should find auth_handler document",
        "        if results:",
        "            doc_ids = [r[0] for r in results]",
        "            self.assertIn('auth_handler', doc_ids)",
        "",
        "    def test_search_returns_parsed_intent(self):",
        "        \"\"\"Test that search returns parsed intent with results.\"\"\"",
        "        results = self.processor.search_by_intent(\"how to validate input?\")",
        "        if results:",
        "            doc_id, score, parsed = results[0]",
        "            self.assertIn('intent', parsed)",
        "            self.assertIn('action', parsed)",
        "            self.assertIn('expanded_terms', parsed)",
        "",
        "    def test_search_empty_query(self):",
        "        \"\"\"Test search with empty query.\"\"\"",
        "        results = self.processor.search_by_intent(\"\")",
        "        self.assertEqual(results, [])",
        "",
        "    def test_search_top_n_limit(self):",
        "        \"\"\"Test that top_n limits results.\"\"\"",
        "        results = self.processor.search_by_intent(\"fetch data\", top_n=2)",
        "        self.assertLessEqual(len(results), 2)",
        "",
        "    def test_processor_parse_intent_query(self):",
        "        \"\"\"Test the processor wrapper for parse_intent_query.\"\"\"",
        "        result = self.processor.parse_intent_query(\"where is the login function?\")",
        "        self.assertEqual(result['intent'], 'location')",
        "        # 'login' is detected as action verb, so 'function' becomes subject",
        "        self.assertEqual(result['action'], 'login')",
        "        self.assertEqual(result['subject'], 'function')",
        "",
        "",
        "class TestParsedIntentStructure(unittest.TestCase):",
        "    \"\"\"Test the ParsedIntent TypedDict structure.\"\"\"",
        "",
        "    def test_all_keys_present(self):",
        "        \"\"\"Test that all expected keys are in parsed result.\"\"\"",
        "        result = parse_intent_query(\"where do we handle errors?\")",
        "        expected_keys = ['action', 'subject', 'intent', 'question_word', 'expanded_terms']",
        "        for key in expected_keys:",
        "            self.assertIn(key, result)",
        "",
        "    def test_expanded_terms_is_list(self):",
        "        \"\"\"Test that expanded_terms is a list.\"\"\"",
        "        result = parse_intent_query(\"handle authentication\")",
        "        self.assertIsInstance(result['expanded_terms'], list)",
        "",
        "    def test_no_duplicate_expanded_terms(self):",
        "        \"\"\"Test that expanded_terms has no duplicates.\"\"\"",
        "        result = parse_intent_query(\"handle handle authentication\")",
        "        self.assertEqual(",
        "            len(result['expanded_terms']),",
        "            len(set(result['expanded_terms']))",
        "        )",
        "",
        "",
        "if __name__ == '__main__':",
        "    unittest.main()"
      ],
      "context_before": [],
      "context_after": [],
      "change_type": "delete"
    },
    {
      "file": "tests/test_query_optimization.py",
      "function": null,
      "start_line": 1,
      "lines_added": [],
      "lines_removed": [
        "\"\"\"",
        "Tests for query optimization functions.",
        "",
        "Tests the fast search and indexing functionality for improved query performance.",
        "\"\"\"",
        "",
        "import unittest",
        "from cortical.query import (",
        "    fast_find_documents,",
        "    build_document_index,",
        "    search_with_index,",
        ")",
        "from cortical.tokenizer import Tokenizer",
        "from cortical.layers import CorticalLayer",
        "",
        "",
        "class TestFastFindDocuments(unittest.TestCase):",
        "    \"\"\"Test the fast_find_documents function.\"\"\"",
        "",
        "    def setUp(self):",
        "        \"\"\"Set up test processor.\"\"\"",
        "        from cortical import CorticalTextProcessor",
        "        self.processor = CorticalTextProcessor()",
        "        self.processor.process_document(\"auth\", \"\"\"",
        "            Authentication module handles user login and credentials.",
        "            Validates tokens and manages sessions securely.",
        "        \"\"\")",
        "        self.processor.process_document(\"data\", \"\"\"",
        "            Data processing module fetches and transforms data.",
        "            Handles database queries and result formatting.",
        "        \"\"\")",
        "        self.processor.process_document(\"validation\", \"\"\"",
        "            Input validation module checks user input.",
        "            Sanitizes and validates form data securely.",
        "        \"\"\")",
        "        self.processor.compute_all()",
        "",
        "    def test_fast_find_returns_results(self):",
        "        \"\"\"Test that fast_find_documents returns results.\"\"\"",
        "        results = fast_find_documents(",
        "            \"authentication login\",",
        "            self.processor.layers,",
        "            self.processor.tokenizer",
        "        )",
        "        self.assertIsInstance(results, list)",
        "        self.assertGreater(len(results), 0)",
        "",
        "    def test_fast_find_finds_relevant_doc(self):",
        "        \"\"\"Test that fast_find_documents finds relevant document.\"\"\"",
        "        results = fast_find_documents(",
        "            \"authentication login\",",
        "            self.processor.layers,",
        "            self.processor.tokenizer",
        "        )",
        "        doc_ids = [r[0] for r in results]",
        "        self.assertIn('auth', doc_ids)",
        "",
        "    def test_fast_find_respects_top_n(self):",
        "        \"\"\"Test that fast_find_documents respects top_n.\"\"\"",
        "        results = fast_find_documents(",
        "            \"user data\",",
        "            self.processor.layers,",
        "            self.processor.tokenizer,",
        "            top_n=2",
        "        )",
        "        self.assertLessEqual(len(results), 2)",
        "",
        "    def test_fast_find_empty_query(self):",
        "        \"\"\"Test fast_find_documents with empty query.\"\"\"",
        "        results = fast_find_documents(",
        "            \"\",",
        "            self.processor.layers,",
        "            self.processor.tokenizer",
        "        )",
        "        self.assertEqual(results, [])",
        "",
        "    def test_fast_find_with_code_concepts(self):",
        "        \"\"\"Test fast_find_documents with code concept expansion.\"\"\"",
        "        # 'fetch' should expand to find 'data' doc which has 'fetches'",
        "        results = fast_find_documents(",
        "            \"fetch\",",
        "            self.processor.layers,",
        "            self.processor.tokenizer,",
        "            use_code_concepts=True",
        "        )",
        "        # Should find data doc",
        "        if results:",
        "            doc_ids = [r[0] for r in results]",
        "            self.assertIn('data', doc_ids)",
        "",
        "    def test_fast_find_without_code_concepts(self):",
        "        \"\"\"Test fast_find_documents without code concept expansion.\"\"\"",
        "        results = fast_find_documents(",
        "            \"nonexistent term xyz\",",
        "            self.processor.layers,",
        "            self.processor.tokenizer,",
        "            use_code_concepts=False",
        "        )",
        "        self.assertEqual(results, [])",
        "",
        "",
        "class TestBuildDocumentIndex(unittest.TestCase):",
        "    \"\"\"Test the build_document_index function.\"\"\"",
        "",
        "    def setUp(self):",
        "        \"\"\"Set up test processor.\"\"\"",
        "        from cortical import CorticalTextProcessor",
        "        self.processor = CorticalTextProcessor()",
        "        self.processor.process_document(\"doc1\", \"neural network training data\")",
        "        self.processor.process_document(\"doc2\", \"database query optimization\")",
        "        self.processor.compute_all()",
        "",
        "    def test_build_index_returns_dict(self):",
        "        \"\"\"Test that build_document_index returns a dict.\"\"\"",
        "        index = build_document_index(self.processor.layers)",
        "        self.assertIsInstance(index, dict)",
        "",
        "    def test_index_contains_terms(self):",
        "        \"\"\"Test that index contains expected terms.\"\"\"",
        "        index = build_document_index(self.processor.layers)",
        "        self.assertIn('neural', index)",
        "        self.assertIn('database', index)",
        "",
        "    def test_index_maps_to_docs(self):",
        "        \"\"\"Test that index maps terms to documents.\"\"\"",
        "        index = build_document_index(self.processor.layers)",
        "",
        "        # 'neural' should map to doc1",
        "        self.assertIn('neural', index)",
        "        self.assertIn('doc1', index['neural'])",
        "",
        "        # 'database' should map to doc2",
        "        self.assertIn('database', index)",
        "        self.assertIn('doc2', index['database'])",
        "",
        "    def test_index_values_are_scores(self):",
        "        \"\"\"Test that index values are positive scores.\"\"\"",
        "        index = build_document_index(self.processor.layers)",
        "",
        "        for term, doc_scores in index.items():",
        "            for doc_id, score in doc_scores.items():",
        "                self.assertGreater(score, 0)",
        "",
        "",
        "class TestSearchWithIndex(unittest.TestCase):",
        "    \"\"\"Test the search_with_index function.\"\"\"",
        "",
        "    def setUp(self):",
        "        \"\"\"Set up test processor and index.\"\"\"",
        "        from cortical import CorticalTextProcessor",
        "        self.processor = CorticalTextProcessor()",
        "        self.processor.process_document(\"auth\", \"authentication login credentials\")",
        "        self.processor.process_document(\"data\", \"database query optimization\")",
        "        self.processor.process_document(\"network\", \"neural network training\")",
        "        self.processor.compute_all()",
        "        self.index = build_document_index(self.processor.layers)",
        "",
        "    def test_search_with_index_returns_results(self):",
        "        \"\"\"Test that search_with_index returns results.\"\"\"",
        "        results = search_with_index(",
        "            \"authentication\",",
        "            self.index,",
        "            self.processor.tokenizer",
        "        )",
        "        self.assertIsInstance(results, list)",
        "        self.assertGreater(len(results), 0)",
        "",
        "    def test_search_with_index_finds_relevant(self):",
        "        \"\"\"Test that search_with_index finds relevant document.\"\"\"",
        "        results = search_with_index(",
        "            \"authentication login\",",
        "            self.index,",
        "            self.processor.tokenizer",
        "        )",
        "        doc_ids = [r[0] for r in results]",
        "        self.assertIn('auth', doc_ids)",
        "",
        "    def test_search_with_index_respects_top_n(self):",
        "        \"\"\"Test that search_with_index respects top_n.\"\"\"",
        "        results = search_with_index(",
        "            \"network\",",
        "            self.index,",
        "            self.processor.tokenizer,",
        "            top_n=1",
        "        )",
        "        self.assertLessEqual(len(results), 1)",
        "",
        "    def test_search_with_index_empty_query(self):",
        "        \"\"\"Test search_with_index with empty query.\"\"\"",
        "        results = search_with_index(",
        "            \"\",",
        "            self.index,",
        "            self.processor.tokenizer",
        "        )",
        "        self.assertEqual(results, [])",
        "",
        "    def test_search_with_index_no_matches(self):",
        "        \"\"\"Test search_with_index with no matching terms.\"\"\"",
        "        results = search_with_index(",
        "            \"xyznonexistent\",",
        "            self.index,",
        "            self.processor.tokenizer",
        "        )",
        "        self.assertEqual(results, [])",
        "",
        "",
        "class TestProcessorIntegration(unittest.TestCase):",
        "    \"\"\"Test query optimization integration with processor.\"\"\"",
        "",
        "    def setUp(self):",
        "        \"\"\"Set up test processor.\"\"\"",
        "        from cortical import CorticalTextProcessor",
        "        self.processor = CorticalTextProcessor()",
        "        self.processor.process_document(\"auth\", \"\"\"",
        "            Authentication module handles user login and session management.",
        "            Validates credentials and issues tokens.",
        "        \"\"\")",
        "        self.processor.process_document(\"data\", \"\"\"",
        "            Data processing module fetches records from database.",
        "            Transforms and validates data for export.",
        "        \"\"\")",
        "        self.processor.compute_all()",
        "",
        "    def test_processor_fast_find_documents(self):",
        "        \"\"\"Test processor fast_find_documents method.\"\"\"",
        "        results = self.processor.fast_find_documents(\"authentication\")",
        "        self.assertIsInstance(results, list)",
        "        if results:",
        "            doc_ids = [r[0] for r in results]",
        "            self.assertIn('auth', doc_ids)",
        "",
        "    def test_processor_build_search_index(self):",
        "        \"\"\"Test processor build_search_index method.\"\"\"",
        "        index = self.processor.build_search_index()",
        "        self.assertIsInstance(index, dict)",
        "        self.assertGreater(len(index), 0)",
        "",
        "    def test_processor_search_with_index(self):",
        "        \"\"\"Test processor search_with_index method.\"\"\"",
        "        index = self.processor.build_search_index()",
        "        results = self.processor.search_with_index(\"database\", index)",
        "        self.assertIsInstance(results, list)",
        "        if results:",
        "            doc_ids = [r[0] for r in results]",
        "            self.assertIn('data', doc_ids)",
        "",
        "    def test_fast_vs_regular_same_results(self):",
        "        \"\"\"Test that fast and regular search return similar results.\"\"\"",
        "        query = \"authentication login\"",
        "",
        "        regular_results = self.processor.find_documents_for_query(query)",
        "        fast_results = self.processor.fast_find_documents(query)",
        "",
        "        # Both should find 'auth' as top result",
        "        if regular_results and fast_results:",
        "            self.assertEqual(regular_results[0][0], fast_results[0][0])",
        "",
        "    def test_index_search_reusable(self):",
        "        \"\"\"Test that built index can be reused for multiple queries.\"\"\"",
        "        index = self.processor.build_search_index()",
        "",
        "        results1 = self.processor.search_with_index(\"authentication\", index)",
        "        results2 = self.processor.search_with_index(\"database\", index)",
        "",
        "        # Should return different results for different queries",
        "        if results1 and results2:",
        "            self.assertNotEqual(results1[0][0], results2[0][0])",
        "",
        "",
        "if __name__ == '__main__':",
        "    unittest.main()"
      ],
      "context_before": [],
      "context_after": [],
      "change_type": "delete"
    }
  ],
  "hour_of_day": 15,
  "day_of_week": "Saturday",
  "seconds_since_last_commit": -168172,
  "is_merge": true,
  "is_initial": false,
  "parent_count": 2,
  "session_id": null,
  "related_chats": [],
  "ci_result": null,
  "reverted": false,
  "amended": false
}