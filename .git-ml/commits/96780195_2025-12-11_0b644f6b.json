{
  "hash": "96780195ee81af8eb670ab9bb65e416eabc2deb0",
  "message": "Fix tokenizer to capture underscore-prefixed identifiers (task #81)",
  "author": "Claude",
  "timestamp": "2025-12-11 02:35:53 +0000",
  "branch": "claude/multi-index-design-DvifZ",
  "files_changed": [
    "TASK_LIST.md",
    "cortical/tokenizer.py",
    "tests/test_tokenizer.py"
  ],
  "insertions": 87,
  "deletions": 1,
  "hunks": [
    {
      "file": "TASK_LIST.md",
      "function": "Key files to understand:",
      "start_line": 2877,
      "lines_added": [
        "### 81. Fix Tokenizer Underscore-Prefixed Identifier Bug",
        "",
        "**File:** `cortical/tokenizer.py`",
        "**Line:** 265",
        "**Status:** [x] Completed",
        "**Priority:** High",
        "**Category:** Code Search",
        "",
        "**Problem:**",
        "The tokenizer regex `r'\\b[a-zA-Z][a-zA-Z0-9_]*\\b'` requires tokens to start with a letter, which causes Python dunder methods (`__init__`, `__slots__`, `__str__`) and private variables (`_id_index`, `_cache`) to be completely unsearchable in code search.",
        "",
        "**Found via dog-fooding:** Searching for `__slots__` returns zero results even though it exists in the codebase.",
        "",
        "**Root Cause:**",
        "```python",
        "# Line 265 - requires first char to be a letter",
        "raw_tokens = re.findall(r'\\b[a-zA-Z][a-zA-Z0-9_]*\\b', text)",
        "```",
        "",
        "**Solution:**",
        "1. Modify regex to capture underscore-prefixed identifiers:",
        "   ```python",
        "   raw_tokens = re.findall(r'\\b_*[a-zA-Z][a-zA-Z0-9_]*\\b', text)",
        "   ```",
        "2. Ensure dunder methods are preserved (not filtered as stop words)",
        "3. Add `'init'`, `'str'`, `'repr'` etc. to `PROGRAMMING_KEYWORDS` if not present",
        "4. Add tests for underscore-prefixed identifier tokenization",
        "",
        "**Impact:**",
        "- High for code search use case",
        "- Python-specific identifiers currently invisible to search",
        "- Affects private methods, dunder methods, internal variables",
        "",
        "**Files to Modify:**",
        "- `cortical/tokenizer.py` - Fix regex pattern",
        "- `tests/test_tokenizer.py` - Add tests for underscore identifiers",
        "",
        "---",
        "",
        "| 81 | High | Fix tokenizer underscore-prefixed identifiers | ✓ Done | Code Search |"
      ],
      "lines_removed": [],
      "context_before": [
        "  3. cortical/analysis.py - Graph algorithms",
        "",
        "[Press Enter to explore processor.py, or type a question]",
        "> How does PageRank work here?",
        "",
        "[Retrieves and explains relevant passages...]",
        "```",
        "",
        "---",
        ""
      ],
      "context_after": [
        "## Summary Table",
        "",
        "| # | Priority | Task | Status | Category |",
        "|---|----------|------|--------|----------|",
        "| 67 | Low | Fix O(n) lookup in showcase | ✓ Done | Showcase |",
        "| 68 | Medium | Add code-specific features to showcase | ✓ Done | Showcase |",
        "| 69 | Medium | Add passage-level search demo | ✓ Done | Showcase |",
        "| 70 | Low | Add performance timing to showcase | ✓ Done | Showcase |",
        "| 71 | High | Enable code-aware tokenization in index | ✓ Done | Code Index |",
        "| 72 | High | Use programming query expansion in search | ✓ Done | Code Index |",
        "| 73 | Medium | Add \"Find Similar Code\" command | | Code Index |",
        "| 74 | Medium | Add \"Explain This Code\" command | | Developer Experience |",
        "| 75 | Medium | Add \"What Changed?\" semantic diff | | Developer Experience |",
        "| 76 | Medium | Add \"Suggest Related Files\" feature | | Developer Experience |",
        "| 77 | High | Add interactive \"Ask the Codebase\" mode | ✓ Done | Developer Experience |",
        "| 78 | Low | Add code pattern detection | | Developer Experience |",
        "| 79 | Low | Add corpus health dashboard | | Developer Experience |",
        "| 80 | Low | Add \"Learning Mode\" for new contributors | | Developer Experience |",
        "",
        "---",
        "",
        "*Added 2025-12-11, completions updated 2025-12-11*"
      ],
      "change_type": "add"
    },
    {
      "file": "cortical/tokenizer.py",
      "function": "PROGRAMMING_KEYWORDS = frozenset({",
      "start_line": 18,
      "lines_added": [
        "    # Dunder method components (for __init__, __slots__, etc.)",
        "    'repr', 'slots', 'name', 'doc', 'call', 'iter', 'next', 'enter',",
        "    'exit', 'getitem', 'setitem', 'delitem', 'contains', 'hash', 'eq',"
      ],
      "lines_removed": [],
      "context_before": [
        "    'def', 'class', 'function', 'return', 'import', 'from', 'if', 'else',",
        "    'elif', 'for', 'while', 'try', 'except', 'finally', 'with', 'as',",
        "    'yield', 'async', 'await', 'lambda', 'pass', 'break', 'continue',",
        "    'raise', 'assert', 'global', 'nonlocal', 'del', 'true', 'false',",
        "    'none', 'null', 'void', 'int', 'str', 'float', 'bool', 'list',",
        "    'dict', 'set', 'tuple', 'self', 'cls', 'init', 'main', 'args',",
        "    'kwargs', 'super', 'property', 'staticmethod', 'classmethod',",
        "    'isinstance', 'hasattr', 'getattr', 'setattr', 'len', 'range',",
        "    'enumerate', 'zip', 'map', 'filter', 'print', 'open', 'read',",
        "    'write', 'close', 'append', 'extend', 'insert', 'remove', 'pop',"
      ],
      "context_after": [
        "    'const', 'let', 'var', 'public', 'private', 'protected', 'static',",
        "    'final', 'abstract', 'interface', 'implements', 'extends', 'new',",
        "    'this', 'constructor', 'module', 'export', 'require', 'package',",
        "    # Common identifier components that shouldn't be filtered",
        "    'get', 'set', 'add', 'put', 'has', 'can', 'run', 'max', 'min',",
        "})",
        "",
        "",
        "def split_identifier(identifier: str) -> List[str]:",
        "    \"\"\""
      ],
      "change_type": "add"
    },
    {
      "file": "cortical/tokenizer.py",
      "function": "class Tokenizer:",
      "start_line": 255,
      "lines_added": [
        "        # Also matches underscore-prefixed: __init__, _private, __slots__",
        "        raw_tokens = re.findall(r'\\b_*[a-zA-Z][a-zA-Z0-9_]*\\b', text)"
      ],
      "lines_removed": [
        "        raw_tokens = re.findall(r'\\b[a-zA-Z][a-zA-Z0-9_]*\\b', text)"
      ],
      "context_before": [
        "",
        "        Examples:",
        "            >>> t = Tokenizer(split_identifiers=True)",
        "            >>> t.tokenize(\"getUserCredentials fetches data\")",
        "            ['getusercredentials', 'get', 'user', 'credentials', 'fetches', 'data']",
        "        \"\"\"",
        "        should_split = split_identifiers if split_identifiers is not None else self.split_identifiers",
        "",
        "        # Extract potential identifiers (including camelCase with internal caps)",
        "        # Pattern matches: word2vec, getUserData, get_user_data, XMLParser"
      ],
      "context_after": [
        "",
        "        result = []",
        "        seen_splits = set()  # Only track splits to avoid duplicates from them",
        "",
        "        for token in raw_tokens:",
        "            token_lower = token.lower()",
        "",
        "            # Skip stop words and short words",
        "            if token_lower in self.stop_words or len(token_lower) < self.min_word_length:",
        "                continue"
      ],
      "change_type": "modify"
    },
    {
      "file": "tests/test_tokenizer.py",
      "function": "class TestCodeAwareTokenization(unittest.TestCase):",
      "start_line": 205,
      "lines_added": [
        "    def test_dunder_methods_tokenized(self):",
        "        \"\"\"Test that Python dunder methods are tokenized correctly.\"\"\"",
        "        tokenizer = Tokenizer()",
        "        # Dunder methods should be captured",
        "        tokens = tokenizer.tokenize(\"def __init__(self): pass\")",
        "        self.assertIn(\"__init__\", tokens)",
        "        self.assertIn(\"self\", tokens)",
        "",
        "        tokens = tokenizer.tokenize(\"__slots__ = ['x', 'y']\")",
        "        self.assertIn(\"__slots__\", tokens)",
        "",
        "        tokens = tokenizer.tokenize(\"def __str__(self): return ''\")",
        "        self.assertIn(\"__str__\", tokens)",
        "",
        "    def test_private_variables_tokenized(self):",
        "        \"\"\"Test that private variables (underscore-prefixed) are tokenized.\"\"\"",
        "        tokenizer = Tokenizer()",
        "        tokens = tokenizer.tokenize(\"self._id_index = {}\")",
        "        self.assertIn(\"self\", tokens)",
        "        self.assertIn(\"_id_index\", tokens)",
        "",
        "        tokens = tokenizer.tokenize(\"_private_cache = []\")",
        "        self.assertIn(\"_private_cache\", tokens)",
        "",
        "    def test_dunder_methods_with_split(self):",
        "        \"\"\"Test dunder methods with identifier splitting.\"\"\"",
        "        tokenizer = Tokenizer(split_identifiers=True)",
        "        tokens = tokenizer.tokenize(\"__init__\")",
        "        self.assertIn(\"__init__\", tokens)",
        "        self.assertIn(\"init\", tokens)",
        "",
        "        tokens = tokenizer.tokenize(\"__slots__\")",
        "        self.assertIn(\"__slots__\", tokens)",
        "        self.assertIn(\"slots\", tokens)",
        "",
        "    def test_private_vars_with_split(self):",
        "        \"\"\"Test private variables with identifier splitting.\"\"\"",
        "        tokenizer = Tokenizer(split_identifiers=True)",
        "        tokens = tokenizer.tokenize(\"_id_index\")",
        "        self.assertIn(\"_id_index\", tokens)",
        "        self.assertIn(\"index\", tokens)",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "        self.assertIn(\"data\", tokens)",
        "",
        "    def test_min_length_applied_to_splits(self):",
        "        \"\"\"Test that min_word_length applies to split parts.\"\"\"",
        "        tokenizer = Tokenizer(split_identifiers=True, min_word_length=4)",
        "        tokens = tokenizer.tokenize(\"getUserID\")",
        "        # 'id' is too short (length 2)",
        "        self.assertNotIn(\"id\", tokens)",
        "        self.assertIn(\"user\", tokens)",
        ""
      ],
      "context_after": [
        "",
        "if __name__ == \"__main__\":",
        "    unittest.main(verbosity=2)"
      ],
      "change_type": "add"
    }
  ],
  "hour_of_day": 2,
  "day_of_week": "Thursday",
  "seconds_since_last_commit": -385735,
  "is_merge": false,
  "is_initial": false,
  "parent_count": 1,
  "session_id": null,
  "related_chats": [],
  "ci_result": null,
  "reverted": false,
  "amended": false
}