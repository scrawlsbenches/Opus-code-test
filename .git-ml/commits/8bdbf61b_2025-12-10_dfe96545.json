{
  "hash": "8bdbf61b0633882589345095420131f77da654b5",
  "message": "Merge pull request #27 from scrawlsbenches/claude/complete-task-list-015j2QSwWkoXuGQUKJ7gzgoX",
  "author": "scrawlsbenches",
  "timestamp": "2025-12-10 20:20:46 -0500",
  "branch": "claude/multi-index-design-DvifZ",
  "files_changed": [
    ".claude/skills/corpus-indexer/SKILL.md",
    "CLAUDE.md",
    "TASK_LIST.md",
    "cortical/__init__.py",
    "cortical/chunk_index.py",
    "cortical/config.py",
    "docs/code-of-ethics.md",
    "docs/definition-of-done.md",
    "docs/dogfooding-checklist.md",
    "docs/patterns.md",
    "tests/test_chunk_indexing.py",
    "tests/test_config.py"
  ],
  "insertions": 2194,
  "deletions": 55,
  "hunks": [
    {
      "file": ".claude/skills/corpus-indexer/SKILL.md",
      "function": "This enables fast incremental updates by detecting only changed files.",
      "start_line": 92,
      "lines_added": [
        "- After indexing, **test search quality** - see `docs/dogfooding-checklist.md`",
        "# Check chunk status",
        "python scripts/index_codebase.py --status --use-chunks",
        "",
        "## Chunk Compaction",
        "",
        "Over time, chunk files accumulate. Use compaction to consolidate them (like `git gc`):",
        "",
        "**When to compact:**",
        "- After 10+ chunk files accumulate",
        "- When you see size warnings during save",
        "- Before merging branches with chunk histories",
        "- To clean up old deleted entries",
        "",
        "**Commands:**",
        "```bash",
        "# Compact all chunks into one",
        "python scripts/index_codebase.py --compact --use-chunks",
        "",
        "# Compact chunks before a date",
        "python scripts/index_codebase.py --compact --before 2025-12-01 --use-chunks",
        "```",
        "",
        "**What happens:**",
        "1. All chunks are read in timestamp order",
        "2. Operations are replayed (later timestamps win)",
        "3. A single compacted chunk is created",
        "4. Old chunk files are removed",
        "5. Cache is preserved if valid",
        "",
        "**Recommended frequency:**",
        "- Weekly for active development",
        "- Monthly for maintenance mode",
        "- Before major releases"
      ],
      "lines_removed": [
        "# Compact old chunks (reduces history size)",
        "python scripts/index_codebase.py --compact --before 2025-12-01"
      ],
      "context_before": [
        "| Incremental | ~1-2s | After small edits |",
        "| Full rebuild (fast) | ~2-3s | Default mode |",
        "| Full analysis | ~10+ min | Complete semantic analysis |",
        "",
        "## Maintenance",
        "",
        "- Use `--incremental` for quick updates during development",
        "- Use `--status` to check if re-indexing is needed",
        "- Use `--force` after major refactoring",
        "- Use `--full-analysis` before deep exploration sessions"
      ],
      "context_after": [
        "",
        "## Git-Compatible Chunk Storage",
        "",
        "For team collaboration, use `--use-chunks` to store changes as git-friendly JSON:",
        "",
        "```bash",
        "# Index with chunk storage",
        "python scripts/index_codebase.py --incremental --use-chunks",
        "",
        "```",
        "",
        "**Benefits:**",
        "- No merge conflicts (unique timestamp filenames)",
        "- Shared indexed state across branches",
        "- Fast startup when cache is valid",
        "",
        "**Files Created:**",
        "- `corpus_chunks/*.json` - Tracked in git (append-only changes)",
        "- `corpus_dev.pkl` - NOT tracked (local cache)",
        "- `corpus_dev.pkl.hash` - NOT tracked (cache validation)"
      ],
      "change_type": "modify"
    },
    {
      "file": "CLAUDE.md",
      "function": "The bigram separator mismatch bugs in `query.py:1442-1468` and `analysis.py:927`",
      "start_line": 114,
      "lines_added": [
        "4. **Dog-food the feature** - test with real usage (see [dogfooding-checklist.md](docs/dogfooding-checklist.md))",
        "5. **Document all findings** - add issues to TASK_LIST.md (see [code-of-ethics.md](docs/code-of-ethics.md))",
        "6. **Verify completion** - use [definition-of-done.md](docs/definition-of-done.md) checklist"
      ],
      "lines_removed": [],
      "context_before": [
        "",
        "1. **Run the full test suite**:",
        "   ```bash",
        "   python -m unittest discover -s tests -v",
        "   ```",
        "2. **Run the showcase** to verify integration:",
        "   ```bash",
        "   python showcase.py",
        "   ```",
        "3. **Check for regressions** in related functionality"
      ],
      "context_after": [
        "",
        "---",
        "",
        "## Testing Patterns",
        "",
        "Tests follow `unittest` conventions in `tests/` directory:",
        "",
        "```python",
        "class TestYourFeature(unittest.TestCase):",
        "    def setUp(self):"
      ],
      "change_type": "add"
    },
    {
      "file": "CLAUDE.md",
      "function": "corpus_dev.pkl                        # NOT tracked (local cache)",
      "start_line": 442,
      "lines_added": [
        "### Chunk Compaction",
        "",
        "Over time, chunk files accumulate. Use compaction to consolidate them, similar to `git gc`:",
        "",
        "**When to compact:**",
        "- After many indexing sessions (10+ chunk files)",
        "- When you see size warnings during indexing",
        "- Before merging branches with different chunk histories",
        "- To clean up deleted/modified document history",
        "",
        "**Compaction commands:**",
        "```bash",
        "# Compact all chunks into a single consolidated file",
        "python scripts/index_codebase.py --compact --use-chunks",
        "",
        "# Compact only chunks created before a specific date",
        "python scripts/index_codebase.py --compact --before 2025-12-01 --use-chunks",
        "",
        "# Check chunk status before compacting",
        "python scripts/index_codebase.py --status --use-chunks",
        "```",
        "",
        "**How compaction works:**",
        "1. Reads all chunk files (sorted by timestamp)",
        "2. Replays operations in order (later timestamps override)",
        "3. Creates a single compacted chunk with final state",
        "4. Removes old chunk files",
        "5. Preserves cache if still valid",
        "",
        "**Recommended frequency:**",
        "- Weekly for active development",
        "- Monthly for maintenance repositories",
        "- Before major releases",
        "",
        "- **Configuration**: `cortical/config.py` - `CorticalConfig` dataclass",
        "**Process Documentation:**",
        "- **Ethics**: `docs/code-of-ethics.md` - documentation, testing, and completion standards",
        "- **Dog-fooding**: `docs/dogfooding-checklist.md` - checklist for testing with real usage",
        "- **Definition of Done**: `docs/definition-of-done.md` - when is a task truly complete?",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "corpus_dev.pkl.hash                   # NOT tracked (cache validation)",
        "```",
        "",
        "**Benefits:**",
        "- No merge conflicts (unique timestamp+session filenames)",
        "- Shared indexed state across team/branches",
        "- Fast startup when cache is valid",
        "- Git-friendly (small JSON, append-only)",
        "- Periodic compaction like `git gc`",
        ""
      ],
      "context_after": [
        "---",
        "",
        "## File Quick Links",
        "",
        "- **Main API**: `cortical/processor.py` - `CorticalTextProcessor` class",
        "- **Graph algorithms**: `cortical/analysis.py` - PageRank, TF-IDF, clustering",
        "- **Search**: `cortical/query.py` - query expansion, document retrieval",
        "- **Data structures**: `cortical/minicolumn.py` - `Minicolumn`, `Edge`",
        "- **Tests**: `tests/test_processor.py` - most comprehensive test file",
        "- **Demo**: `showcase.py` - interactive demonstration",
        "",
        "---",
        "",
        "*Remember: Be skeptical, verify assumptions, and always run the tests.*"
      ],
      "change_type": "add"
    },
    {
      "file": "TASK_LIST.md",
      "function": "Added validation to key functions:",
      "start_line": 1264,
      "lines_added": [
        "**Status:** [x] Completed (2025-12-11)",
        "**Solution Applied:**",
        "1. Created `cortical/config.py` with `CorticalConfig` dataclass",
        "2. Centralized all magic numbers and defaults:",
        "   - PageRank: damping, iterations, tolerance",
        "   - Clustering: min_cluster_size, cluster_strictness",
        "   - Gap detection: isolation_threshold, well_connected_threshold, etc.",
        "   - Chunking: chunk_size, chunk_overlap",
        "   - Query expansion: max_query_expansions, semantic_expansion_discount",
        "   - Bigram connections: component_weight, chain_weight, cooccurrence_weight",
        "   - Concept connections: min_shared_docs, min_jaccard, embedding_threshold",
        "   - Multi-hop: max_hops, decay_factor, min_path_score",
        "   - Property inheritance: decay_factor, max_depth, boost_factor",
        "   - Relation weights dictionary",
        "3. Added validation in `__post_init__()` for all parameters",
        "4. Added `copy()`, `to_dict()`, and `from_dict()` methods",
        "5. Moved `VALID_RELATION_CHAINS` to config module",
        "6. Updated `__init__.py` to export new classes",
        "7. Created 29 tests in `tests/test_config.py`",
        "",
        "**Usage:**",
        "from cortical import CorticalTextProcessor, CorticalConfig",
        "",
        "# Custom configuration",
        "config = CorticalConfig(",
        "    pagerank_damping=0.9,",
        "    min_cluster_size=5,",
        "    isolation_threshold=0.03",
        ")",
        "processor = CorticalTextProcessor(config=config)"
      ],
      "lines_removed": [
        "**Status:** [ ] Not Started",
        "**Solution:**",
        "@dataclass",
        "class CorticalConfig:",
        "    # PageRank",
        "    pagerank_damping: float = 0.85",
        "    pagerank_iterations: int = 20",
        "    pagerank_tolerance: float = 1e-6",
        "",
        "    # Clustering",
        "    min_cluster_size: int = 3",
        "    cluster_strictness: float = 1.0",
        "",
        "    # Gap detection",
        "    isolation_threshold: float = 0.02",
        "    well_connected_threshold: float = 0.03"
      ],
      "context_before": [
        "- `retrofit_embeddings()`: alpha must be in range (0, 1]",
        "- `create_chunks()`: chunk_size > 0, overlap >= 0, overlap < chunk_size",
        "",
        "Added 9 new tests for parameter validation.",
        "",
        "---",
        "",
        "### 41. Create Configuration Dataclass",
        "",
        "**Files:** New `cortical/config.py`"
      ],
      "context_after": [
        "**Priority:** Medium",
        "",
        "**Problem:**",
        "Magic numbers scattered across modules with no central configuration:",
        "- `gaps.py`: ISOLATION_THRESHOLD=0.02, WELL_CONNECTED_THRESHOLD=0.03",
        "- `query.py`: VALID_RELATION_CHAINS (15 entries)",
        "- `analysis.py`: damping=0.85, iterations=20, tolerance=1e-6",
        "",
        "```python",
        "```",
        "",
        "---",
        "",
        "### 42. Add Simple Query Language Support",
        "",
        "**File:** `cortical/query.py`",
        "**Status:** [ ] Not Started",
        "**Priority:** Low",
        ""
      ],
      "change_type": "modify"
    },
    {
      "file": "TASK_LIST.md",
      "function": "Define terminology used throughout the codebase so searches for concepts find re",
      "start_line": 1918,
      "lines_added": [
        "**Status:** [x] Completed (2025-12-11)",
        "**Solution Applied:**",
        "Created `docs/patterns.md` with 15 usage patterns covering:",
        "",
        "1. **Code Search Patterns** (Patterns 1-4):",
        "   - Code-aware tokenization with identifier splitting",
        "   - Programming concept expansion",
        "   - Intent-based code search",
        "   - Combined code search",
        "",
        "2. **Fingerprint Comparison** (Patterns 5-8):",
        "   - Basic fingerprinting",
        "   - Explain similarity",
        "   - Find similar code blocks",
        "   - Code deduplication",
        "",
        "3. **Intent-Based Querying** (Patterns 9-10):",
        "   - Query intent detection",
        "   - Intent-aware search",
        "",
        "4. **Document Type Boosting** (Patterns 11-12):",
        "   - Boost documentation",
        "   - Search with type filtering",
        "",
        "5. **Configuration Patterns** (Patterns 13-15):",
        "   - Custom configuration",
        "   - Save and restore configuration",
        "   - Domain-specific configurations",
        "",
        "Note: Basic document processing, RAG retrieval, batch operations, and incremental",
        "updates are already covered in `docs/cookbook.md`."
      ],
      "lines_removed": [
        "**Status:** [ ] Not Started",
        "**Patterns:**",
        "- Basic document processing workflow",
        "- RAG retrieval with passages",
        "- Code search with intent parsing",
        "- Fingerprint comparison for similarity",
        "- Batch operations for performance",
        "- Incremental updates"
      ],
      "context_before": [
        "- Feedforward/feedback connections",
        "- PageRank, TF-IDF, damping factor",
        "- Semantic relations (IsA, PartOf, etc.)",
        "- Query expansion, spreading activation",
        "",
        "---",
        "",
        "### 56. Create Usage Patterns Documentation",
        "",
        "**File:** New `docs/patterns.md`"
      ],
      "context_after": [
        "**Priority:** Medium",
        "",
        "**Goal:**",
        "Document common usage patterns and code examples that help answer \"how do I...\" queries.",
        "",
        "",
        "---",
        "",
        "### 58. Git-Compatible Chunk-Based Indexing",
        "",
        "**Files:** `scripts/index_codebase.py`, `cortical/chunk_index.py` (new), `tests/test_chunk_indexing.py` (new)",
        "**Status:** [x] Completed (2025-12-10)",
        "**Priority:** High",
        "",
        "**Problem:**"
      ],
      "change_type": "modify"
    },
    {
      "file": "TASK_LIST.md",
      "function": "Added cross-platform timeout implementation:",
      "start_line": 2070,
      "lines_added": [
        "**Status:** [x] Completed (2025-12-11)",
        "**Solution Applied:**",
        "1. Added `DEFAULT_WARN_SIZE_KB = 1024` constant (1MB default threshold)",
        "2. Added `warn_size_kb` parameter to `ChunkWriter.save()` method",
        "3. Added warning emission when chunk file exceeds threshold",
        "4. Warning includes helpful message suggesting `--compact`",
        "5. Warning can be disabled by passing `warn_size_kb=0`",
        "6. Added 3 tests for warning functionality",
        "",
        "**Files Modified:**",
        "- `cortical/chunk_index.py` - Added warning logic",
        "- `tests/test_chunk_indexing.py` - Added 3 tests",
        "",
        "**Usage:**",
        "writer = ChunkWriter('corpus_chunks')",
        "writer.add_document('large_doc', 'x' * 2_000_000)",
        "",
        "# Default: warn if > 1MB",
        "writer.save()  # May emit warning",
        "",
        "# Custom threshold",
        "writer.save(warn_size_kb=500)  # Warn if > 500KB",
        "",
        "# Disable warning",
        "writer.save(warn_size_kb=0)  # Never warn",
        "**Status:** [x] Completed (2025-12-11)",
        "**Solution Applied:**",
        "Added comprehensive compaction documentation to both files covering:",
        "",
        "1. **When to compact:**",
        "   - After 10+ chunk files accumulate",
        "   - When size warnings appear",
        "   - Before merging branches",
        "   - To clean up deleted entries",
        "",
        "2. **How compaction works:**",
        "   - Reads chunks in timestamp order",
        "   - Replays operations (later timestamps win)",
        "   - Creates single compacted chunk",
        "   - Removes old chunk files",
        "   - Preserves valid cache",
        "",
        "3. **Example commands:**",
        "   - `--compact --use-chunks` for full compaction",
        "   - `--compact --before DATE --use-chunks` for date-based",
        "",
        "4. **Recommended frequency:**",
        "   - Weekly for active development",
        "   - Monthly for maintenance",
        "   - Before major releases",
        "",
        "**Files Modified:**",
        "- `CLAUDE.md` - Added \"Chunk Compaction\" section",
        "- `.claude/skills/corpus-indexer/SKILL.md` - Added compaction section"
      ],
      "lines_removed": [
        "**Status:** [ ] Not Started",
        "**Solution:**",
        "Add a warning when saving chunks that exceed a configurable threshold (e.g., 1MB):",
        "def save(self, warn_size_kb: int = 1024) -> Optional[Path]:",
        "    # ... save logic ...",
        "    if file_path.stat().st_size > warn_size_kb * 1024:",
        "        warnings.warn(f\"Chunk file exceeds {warn_size_kb}KB. Consider running --compact.\")",
        "**Status:** [ ] Not Started",
        "**Solution:**",
        "Add documentation explaining:",
        "- When to use `--compact`",
        "- How compaction works (merges chunks, preserves latest state)",
        "- Recommended compaction frequency",
        "- Example commands"
      ],
      "context_before": [
        "",
        "4. **Limitation documented**: Windows implementation cannot interrupt blocking I/O operations",
        "",
        "5. **Also addressed Task #59**: Renamed `TimeoutError` to `IndexingTimeoutError` to avoid shadowing the built-in",
        "",
        "---",
        "",
        "### 61. Add Chunk Size Warning for Large Chunks",
        "",
        "**Files:** `cortical/chunk_index.py`"
      ],
      "context_after": [
        "**Priority:** Low",
        "",
        "**Problem:**",
        "Large chunk files in git can bloat repository history. There's no warning when chunks exceed a reasonable size threshold.",
        "",
        "```python",
        "```",
        "",
        "---",
        "",
        "### 62. Add Chunk Compaction Documentation",
        "",
        "**Files:** `CLAUDE.md`, `.claude/skills/corpus-indexer/SKILL.md`",
        "**Priority:** Low",
        "",
        "**Problem:**",
        "The `--compact` feature is implemented but not documented in CLAUDE.md or the corpus-indexer skill.",
        "",
        "",
        "---",
        "",
        "## Dog-Fooding Findings (2025-12-10)",
        "",
        "The following issues were identified during a dog-fooding session reviewing the docs folder and testing search quality.",
        "",
        "---",
        "",
        "### 65. Add Document Metadata to Chunk-Based Indexing (Prerequisite)"
      ],
      "change_type": "modify"
    },
    {
      "file": "TASK_LIST.md",
      "function": "def get_doc_type(doc_id: str) -> str:",
      "start_line": 2236,
      "lines_added": [
        "### 66. Add Doc-Type Boosting to Passage-Level Search",
        "",
        "**Files:** `cortical/query.py`, `scripts/search_codebase.py`",
        "**Status:** [ ] Not Started",
        "**Priority:** Medium",
        "",
        "**Problem:**",
        "Document-level search correctly applies doc-type boosting (CLAUDE.md ranks #3 for \"chunk compaction\"), but passage-level search (`find_passages_for_query`) returns raw TF-IDF scores without boosting. This causes code snippets with keyword matches to rank higher than documentation passages for conceptual queries.",
        "",
        "**Evidence (2025-12-11 dog-fooding):**",
        "```",
        "# Document-level search correctly ranks docs:",
        "TASK_LIST.md: 9.837 (root_docs)",
        "CLAUDE.md: 8.146 (root_docs)  ← Documentation found",
        "",
        "# But passage search returns code first:",
        "[1] [CODE] cortical/chunk_index.py:291 Score: 2.549",
        "[2] [TEST] tests/test_chunk_indexing.py:77 Score: 2.157",
        "# CLAUDE.md \"Chunk Compaction\" section not in top 5",
        "```",
        "",
        "**Solution:**",
        "Propagate doc-type boost to passage scoring:",
        "1. After chunking documents, apply `get_doc_type_boost()` to passage scores",
        "2. For conceptual queries (`is_conceptual_query()`), multiply passage score by doc-type boost",
        "3. Re-rank passages after boosting",
        "",
        "**Implementation Sketch:**",
        "```python",
        "def find_passages_for_query(..., apply_doc_boost: bool = True):",
        "    # ... existing passage retrieval ...",
        "",
        "    if apply_doc_boost and is_conceptual_query(query_text):",
        "        boosted_passages = []",
        "        for passage, doc_id, start, end, score in passages:",
        "            boost = get_doc_type_boost(doc_id, doc_metadata)",
        "            boosted_passages.append((passage, doc_id, start, end, score * boost))",
        "        passages = sorted(boosted_passages, key=lambda x: -x[4])",
        "",
        "    return passages[:top_n]",
        "```",
        "",
        "---",
        "",
        "| 65 | High | Add document metadata to chunk indexing | [x] Completed | Infrastructure |",
        "| 63 | High | Improve search ranking for docs | [x] Completed | Search Quality |",
        "| 64 | Low | Add document type indicator | [x] Completed | UX |",
        "| 66 | Medium | Add doc-type boost to passage search | [ ] Not Started | Search Quality |",
        "**Dependency Chain:** #65 → #63 → #64 (all complete), #66 extends this work",
        "**Status Update (2025-12-11):**",
        "- Document-level search now correctly boosts documentation for conceptual queries",
        "- Passage-level search still needs boosting (#66) - docs found but code ranks higher",
        "| 59 | Low | Rename TimeoutError to avoid shadowing | [x] Completed | Code Quality |",
        "| 60 | Medium | Add Windows compatibility for timeout | [x] Completed | Compatibility |",
        "| 61 | Low | Add chunk size warning | [x] Completed | UX |",
        "| 62 | Low | Add chunk compaction documentation | [x] Completed | Documentation |",
        "",
        "**Test Results:** 691 tests passing (including 32 new tests)",
        "",
        "---",
        "## Actionable Tasks Summary (Updated 2025-12-11)",
        "",
        "| # | Priority | Task | Status | Category |",
        "|---|----------|------|--------|----------|",
        "| 41 | Medium | Create Configuration Dataclass | [x] Completed | Code Quality |",
        "| 56 | Medium | Create Usage Patterns Documentation | [x] Completed | Documentation |",
        "| 66 | Medium | Add doc-type boost to passage search | [ ] Not Started | Search Quality |",
        "| 42 | Low | Add Simple Query Language Support | [ ] Not Started | Feature |",
        "| 44 | Low | Remove Deprecated feedforward_sources | [ ] Not Started | Code Quality |",
        "| 46 | Low | Standardize Return Types with Dataclasses | [ ] Not Started | Code Quality |",
        "*Updated 2025-12-11*"
      ],
      "lines_removed": [
        "| 65 | High | Add document metadata to chunk indexing | [ ] Not Started | Infrastructure |",
        "| 63 | High | Improve search ranking for docs | [ ] Not Started | Search Quality |",
        "| 64 | Low | Add document type indicator | [ ] Not Started | UX |",
        "**Dependency Chain:** #65 → #63 → #64",
        "**Key Insight:** The dog-fooding loop isn't complete - documentation was indexed but doesn't surface well in search results. The \"better docs → better search\" feedback loop requires:",
        "1. **Infrastructure** (#65): Store document metadata (file type, headings) in chunks",
        "2. **Ranking** (#63): Use metadata to boost docs for conceptual queries",
        "3. **UX** (#64): Show document types in search results",
        "| 59 | Low | Rename TimeoutError to avoid shadowing | [ ] Not Started | Code Quality |",
        "| 60 | Medium | Add Windows compatibility for timeout | [ ] Not Started | Compatibility |",
        "| 61 | Low | Add chunk size warning | [ ] Not Started | UX |",
        "| 62 | Low | Add chunk compaction documentation | [ ] Not Started | Documentation |",
        "**Test Results:** 84 new tests, all passing",
        "*Updated 2025-12-10*"
      ],
      "context_before": [
        "    if doc_id.endswith('.md'):",
        "        return 'DOCS'",
        "    elif doc_id.startswith('tests/'):",
        "        return 'TEST'",
        "    else:",
        "        return 'CODE'",
        "```",
        "",
        "---",
        ""
      ],
      "context_after": [
        "## Dog-Fooding Summary",
        "",
        "| # | Priority | Task | Status | Category |",
        "|---|----------|------|--------|----------|",
        "",
        "",
        "",
        "---",
        "",
        "## Code Review Summary (PR #23)",
        "",
        "| # | Priority | Task | Status | Category |",
        "|---|----------|------|--------|----------|",
        "",
        "",
        "---",
        ""
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/__init__.py",
      "function": "Example:",
      "start_line": 11,
      "lines_added": [
        "from .config import CorticalConfig, get_default_config, VALID_RELATION_CHAINS",
        "    \"CorticalConfig\",",
        "    \"get_default_config\",",
        "    \"VALID_RELATION_CHAINS\","
      ],
      "lines_removed": [],
      "context_before": [
        "    processor = CorticalTextProcessor()",
        "    processor.process_document(\"doc1\", \"Neural networks process information...\")",
        "    processor.compute_all()",
        "    results = processor.find_documents_for_query(\"neural processing\")",
        "\"\"\"",
        "",
        "from .tokenizer import Tokenizer",
        "from .minicolumn import Minicolumn, Edge",
        "from .layers import CorticalLayer, HierarchicalLayer",
        "from .processor import CorticalTextProcessor"
      ],
      "context_after": [
        "",
        "__version__ = \"2.0.0\"",
        "__all__ = [",
        "    \"CorticalTextProcessor\",",
        "    \"CorticalLayer\",",
        "    \"HierarchicalLayer\",",
        "    \"Minicolumn\",",
        "    \"Edge\",",
        "    \"Tokenizer\",",
        "]"
      ],
      "change_type": "add"
    },
    {
      "file": "cortical/chunk_index.py",
      "function": "Chunk Format:",
      "start_line": 25,
      "lines_added": [
        "import warnings",
        "# Default size threshold for chunk size warnings (in KB)",
        "# Chunks larger than this may bloat git history",
        "DEFAULT_WARN_SIZE_KB = 1024  # 1MB",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "            {\"op\": \"delete\", \"doc_id\": \"...\"}",
        "        ]",
        "    }",
        "\"\"\"",
        "",
        "import hashlib",
        "import json",
        "import os",
        "import subprocess",
        "import uuid"
      ],
      "context_after": [
        "from dataclasses import dataclass, field, asdict",
        "from datetime import datetime",
        "from pathlib import Path",
        "from typing import Dict, List, Optional, Tuple, Any",
        "",
        "",
        "# Chunk format version",
        "CHUNK_VERSION = 1",
        "",
        "",
        "@dataclass",
        "class ChunkOperation:",
        "    \"\"\"A single operation in a chunk (add, modify, or delete).\"\"\"",
        "    op: str  # 'add', 'modify', 'delete'",
        "    doc_id: str",
        "    content: Optional[str] = None  # None for delete operations",
        "    mtime: Optional[float] = None  # Modification time",
        "    metadata: Optional[Dict[str, Any]] = None  # Document metadata (doc_type, headings, etc.)",
        ""
      ],
      "change_type": "add"
    },
    {
      "file": "cortical/chunk_index.py",
      "function": "class ChunkWriter:",
      "start_line": 182,
      "lines_added": [
        "    def save(self, warn_size_kb: int = DEFAULT_WARN_SIZE_KB) -> Optional[Path]:",
        "        Args:",
        "            warn_size_kb: Emit a warning if the saved chunk exceeds this size",
        "                in kilobytes. Set to 0 to disable warning. Default is 1024 KB (1MB).",
        ""
      ],
      "lines_removed": [
        "    def save(self) -> Optional[Path]:"
      ],
      "context_before": [
        "        \"\"\"Record a delete operation.\"\"\"",
        "        self.operations.append(ChunkOperation(",
        "            op='delete',",
        "            doc_id=doc_id",
        "        ))",
        "",
        "    def has_operations(self) -> bool:",
        "        \"\"\"Check if any operations were recorded.\"\"\"",
        "        return len(self.operations) > 0",
        ""
      ],
      "context_after": [
        "        \"\"\"",
        "        Save chunk to file.",
        "",
        "        Returns:",
        "            Path to saved chunk file, or None if no operations.",
        "        \"\"\"",
        "        if not self.operations:",
        "            return None",
        "",
        "        # Create chunks directory if needed",
        "        self.chunks_dir.mkdir(parents=True, exist_ok=True)",
        "",
        "        # Create chunk"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/chunk_index.py",
      "function": "class ChunkWriter:",
      "start_line": 209,
      "lines_added": [
        "        # Check file size and warn if too large",
        "        if warn_size_kb > 0:",
        "            file_size_bytes = filepath.stat().st_size",
        "            file_size_kb = file_size_bytes / 1024",
        "            if file_size_kb > warn_size_kb:",
        "                warnings.warn(",
        "                    f\"Chunk file '{filepath.name}' is {file_size_kb:.1f}KB \"",
        "                    f\"(exceeds {warn_size_kb}KB threshold). \"",
        "                    f\"Large chunks may bloat git history. \"",
        "                    f\"Consider running --compact to consolidate old chunks.\",",
        "                    UserWarning",
        "                )",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "            session_id=self.session_id,",
        "            branch=self.branch,",
        "            operations=self.operations",
        "        )",
        "",
        "        # Write to file",
        "        filepath = self.chunks_dir / chunk.get_filename()",
        "        with open(filepath, 'w', encoding='utf-8') as f:",
        "            json.dump(chunk.to_dict(), f, indent=2, ensure_ascii=False)",
        ""
      ],
      "context_after": [
        "        return filepath",
        "",
        "",
        "class ChunkLoader:",
        "    \"\"\"",
        "    Loads and combines chunks to rebuild document state.",
        "",
        "    Usage:",
        "        loader = ChunkLoader(chunks_dir='corpus_chunks')",
        "        documents = loader.load_all()  # Returns {doc_id: content}"
      ],
      "change_type": "add"
    },
    {
      "file": "cortical/config.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "\"\"\"",
        "Configuration Module",
        "====================",
        "",
        "Centralized configuration for the Cortical Text Processor.",
        "",
        "This module provides a dataclass-based configuration system that allows",
        "users to customize algorithm parameters, thresholds, and defaults without",
        "modifying the source code.",
        "",
        "Example:",
        "    from cortical import CorticalTextProcessor, CorticalConfig",
        "",
        "    # Use custom configuration",
        "    config = CorticalConfig(",
        "        pagerank_damping=0.9,",
        "        min_cluster_size=5,",
        "        isolation_threshold=0.03",
        "    )",
        "    processor = CorticalTextProcessor(config=config)",
        "",
        "    # Or modify defaults",
        "    config = CorticalConfig()",
        "    config.pagerank_iterations = 50",
        "    processor = CorticalTextProcessor(config=config)",
        "\"\"\"",
        "",
        "from dataclasses import dataclass, field",
        "from typing import Dict, Tuple, FrozenSet",
        "",
        "",
        "@dataclass",
        "class CorticalConfig:",
        "    \"\"\"",
        "    Configuration settings for the Cortical Text Processor.",
        "",
        "    All values have sensible defaults that work well for typical text corpora.",
        "    Adjust these based on your specific use case:",
        "    - Smaller corpora may need lower thresholds",
        "    - Specialized domains may need different relation weights",
        "    - Performance-critical applications may want fewer iterations",
        "",
        "    Attributes:",
        "        pagerank_damping: Damping factor for PageRank (0-1). Higher values",
        "            give more weight to link structure vs uniform distribution.",
        "        pagerank_iterations: Maximum PageRank iterations before stopping.",
        "        pagerank_tolerance: Convergence threshold for PageRank. Algorithm",
        "            stops when max change between iterations is below this value.",
        "",
        "        min_cluster_size: Minimum nodes required to form a concept cluster.",
        "            Smaller values create more fine-grained concepts.",
        "        cluster_strictness: Controls clustering aggressiveness (0.0-1.0).",
        "            Lower values allow more cross-topic mixing.",
        "",
        "        isolation_threshold: Documents below this average similarity are",
        "            considered isolated from the corpus.",
        "        well_connected_threshold: Documents above this average similarity",
        "            are considered well-integrated.",
        "        weak_topic_tfidf_threshold: Terms above this TF-IDF are considered",
        "            significant topics.",
        "        bridge_similarity_min: Minimum similarity for bridge opportunities.",
        "        bridge_similarity_max: Maximum similarity for bridge opportunities.",
        "",
        "        chunk_size: Default chunk size for passage retrieval (in characters).",
        "        chunk_overlap: Default overlap between chunks (in characters).",
        "",
        "        max_query_expansions: Maximum expansion terms to add to queries.",
        "        semantic_expansion_discount: Weight discount for semantic expansions",
        "            relative to lateral connection expansions.",
        "",
        "        cross_layer_damping: Damping at layer boundaries for hierarchical",
        "            PageRank propagation.",
        "",
        "        relation_weights: Weights for semantic relation types. Higher weights",
        "            increase influence of that relation type in algorithms.",
        "    \"\"\"",
        "",
        "    # PageRank settings",
        "    pagerank_damping: float = 0.85",
        "    pagerank_iterations: int = 20",
        "    pagerank_tolerance: float = 1e-6",
        "",
        "    # Clustering settings",
        "    min_cluster_size: int = 3",
        "    cluster_strictness: float = 1.0",
        "",
        "    # Gap detection thresholds",
        "    isolation_threshold: float = 0.02",
        "    well_connected_threshold: float = 0.03",
        "    weak_topic_tfidf_threshold: float = 0.005",
        "    bridge_similarity_min: float = 0.005",
        "    bridge_similarity_max: float = 0.03",
        "",
        "    # Chunking settings for RAG",
        "    chunk_size: int = 512",
        "    chunk_overlap: int = 128",
        "",
        "    # Query expansion settings",
        "    max_query_expansions: int = 10",
        "    semantic_expansion_discount: float = 0.7",
        "",
        "    # Cross-layer propagation",
        "    cross_layer_damping: float = 0.7",
        "",
        "    # Bigram connection weights",
        "    bigram_component_weight: float = 0.5",
        "    bigram_chain_weight: float = 0.7",
        "    bigram_cooccurrence_weight: float = 0.3",
        "",
        "    # Concept connection thresholds",
        "    concept_min_shared_docs: int = 1",
        "    concept_min_jaccard: float = 0.1",
        "    concept_embedding_threshold: float = 0.3",
        "",
        "    # Multi-hop expansion settings",
        "    multihop_max_hops: int = 2",
        "    multihop_decay_factor: float = 0.5",
        "    multihop_min_path_score: float = 0.3",
        "",
        "    # Property inheritance settings",
        "    inheritance_decay_factor: float = 0.7",
        "    inheritance_max_depth: int = 5",
        "    inheritance_boost_factor: float = 0.3",
        "",
        "    # Relation weights for semantic algorithms",
        "    relation_weights: Dict[str, float] = field(default_factory=lambda: {",
        "        'IsA': 1.5,",
        "        'PartOf': 1.2,",
        "        'HasA': 1.0,",
        "        'UsedFor': 0.8,",
        "        'CapableOf': 0.7,",
        "        'HasProperty': 1.1,",
        "        'SimilarTo': 1.3,",
        "        'RelatedTo': 1.0,",
        "        'Causes': 1.0,",
        "        'Antonym': 0.3,",
        "        'DerivedFrom': 1.1,",
        "        'AtLocation': 0.9,",
        "        'CoOccurs': 0.8,",
        "    })",
        "",
        "    def __post_init__(self):",
        "        \"\"\"Validate configuration values after initialization.\"\"\"",
        "        self._validate()",
        "",
        "    def _validate(self):",
        "        \"\"\"",
        "        Validate configuration values are within acceptable ranges.",
        "",
        "        Raises:",
        "            ValueError: If any configuration value is invalid.",
        "        \"\"\"",
        "        # PageRank validation",
        "        if not (0 < self.pagerank_damping < 1):",
        "            raise ValueError(",
        "                f\"pagerank_damping must be between 0 and 1, got {self.pagerank_damping}\"",
        "            )",
        "        if self.pagerank_iterations < 1:",
        "            raise ValueError(",
        "                f\"pagerank_iterations must be at least 1, got {self.pagerank_iterations}\"",
        "            )",
        "        if self.pagerank_tolerance <= 0:",
        "            raise ValueError(",
        "                f\"pagerank_tolerance must be positive, got {self.pagerank_tolerance}\"",
        "            )",
        "",
        "        # Clustering validation",
        "        if self.min_cluster_size < 1:",
        "            raise ValueError(",
        "                f\"min_cluster_size must be at least 1, got {self.min_cluster_size}\"",
        "            )",
        "        if not (0 <= self.cluster_strictness <= 1):",
        "            raise ValueError(",
        "                f\"cluster_strictness must be between 0 and 1, got {self.cluster_strictness}\"",
        "            )",
        "",
        "        # Threshold validation",
        "        if self.isolation_threshold < 0:",
        "            raise ValueError(",
        "                f\"isolation_threshold must be non-negative, got {self.isolation_threshold}\"",
        "            )",
        "        if self.well_connected_threshold < 0:",
        "            raise ValueError(",
        "                f\"well_connected_threshold must be non-negative, got {self.well_connected_threshold}\"",
        "            )",
        "        if self.weak_topic_tfidf_threshold < 0:",
        "            raise ValueError(",
        "                f\"weak_topic_tfidf_threshold must be non-negative, got {self.weak_topic_tfidf_threshold}\"",
        "            )",
        "",
        "        # Chunking validation",
        "        if self.chunk_size < 1:",
        "            raise ValueError(",
        "                f\"chunk_size must be at least 1, got {self.chunk_size}\"",
        "            )",
        "        if self.chunk_overlap < 0:",
        "            raise ValueError(",
        "                f\"chunk_overlap must be non-negative, got {self.chunk_overlap}\"",
        "            )",
        "        if self.chunk_overlap >= self.chunk_size:",
        "            raise ValueError(",
        "                f\"chunk_overlap ({self.chunk_overlap}) must be less than chunk_size ({self.chunk_size})\"",
        "            )",
        "",
        "        # Query expansion validation",
        "        if self.max_query_expansions < 0:",
        "            raise ValueError(",
        "                f\"max_query_expansions must be non-negative, got {self.max_query_expansions}\"",
        "            )",
        "        if not (0 <= self.semantic_expansion_discount <= 1):",
        "            raise ValueError(",
        "                f\"semantic_expansion_discount must be between 0 and 1, got {self.semantic_expansion_discount}\"",
        "            )",
        "",
        "        # Cross-layer damping validation",
        "        if not (0 < self.cross_layer_damping < 1):",
        "            raise ValueError(",
        "                f\"cross_layer_damping must be between 0 and 1, got {self.cross_layer_damping}\"",
        "            )",
        "",
        "    def copy(self) -> 'CorticalConfig':",
        "        \"\"\"",
        "        Create a copy of this configuration.",
        "",
        "        Returns:",
        "            A new CorticalConfig instance with the same values.",
        "        \"\"\"",
        "        return CorticalConfig(",
        "            pagerank_damping=self.pagerank_damping,",
        "            pagerank_iterations=self.pagerank_iterations,",
        "            pagerank_tolerance=self.pagerank_tolerance,",
        "            min_cluster_size=self.min_cluster_size,",
        "            cluster_strictness=self.cluster_strictness,",
        "            isolation_threshold=self.isolation_threshold,",
        "            well_connected_threshold=self.well_connected_threshold,",
        "            weak_topic_tfidf_threshold=self.weak_topic_tfidf_threshold,",
        "            bridge_similarity_min=self.bridge_similarity_min,",
        "            bridge_similarity_max=self.bridge_similarity_max,",
        "            chunk_size=self.chunk_size,",
        "            chunk_overlap=self.chunk_overlap,",
        "            max_query_expansions=self.max_query_expansions,",
        "            semantic_expansion_discount=self.semantic_expansion_discount,",
        "            cross_layer_damping=self.cross_layer_damping,",
        "            bigram_component_weight=self.bigram_component_weight,",
        "            bigram_chain_weight=self.bigram_chain_weight,",
        "            bigram_cooccurrence_weight=self.bigram_cooccurrence_weight,",
        "            concept_min_shared_docs=self.concept_min_shared_docs,",
        "            concept_min_jaccard=self.concept_min_jaccard,",
        "            concept_embedding_threshold=self.concept_embedding_threshold,",
        "            multihop_max_hops=self.multihop_max_hops,",
        "            multihop_decay_factor=self.multihop_decay_factor,",
        "            multihop_min_path_score=self.multihop_min_path_score,",
        "            inheritance_decay_factor=self.inheritance_decay_factor,",
        "            inheritance_max_depth=self.inheritance_max_depth,",
        "            inheritance_boost_factor=self.inheritance_boost_factor,",
        "            relation_weights=dict(self.relation_weights),",
        "        )",
        "",
        "    def to_dict(self) -> Dict:",
        "        \"\"\"",
        "        Convert configuration to a dictionary for serialization.",
        "",
        "        Returns:",
        "            Dictionary representation of the configuration.",
        "        \"\"\"",
        "        return {",
        "            'pagerank_damping': self.pagerank_damping,",
        "            'pagerank_iterations': self.pagerank_iterations,",
        "            'pagerank_tolerance': self.pagerank_tolerance,",
        "            'min_cluster_size': self.min_cluster_size,",
        "            'cluster_strictness': self.cluster_strictness,",
        "            'isolation_threshold': self.isolation_threshold,",
        "            'well_connected_threshold': self.well_connected_threshold,",
        "            'weak_topic_tfidf_threshold': self.weak_topic_tfidf_threshold,",
        "            'bridge_similarity_min': self.bridge_similarity_min,",
        "            'bridge_similarity_max': self.bridge_similarity_max,",
        "            'chunk_size': self.chunk_size,",
        "            'chunk_overlap': self.chunk_overlap,",
        "            'max_query_expansions': self.max_query_expansions,",
        "            'semantic_expansion_discount': self.semantic_expansion_discount,",
        "            'cross_layer_damping': self.cross_layer_damping,",
        "            'bigram_component_weight': self.bigram_component_weight,",
        "            'bigram_chain_weight': self.bigram_chain_weight,",
        "            'bigram_cooccurrence_weight': self.bigram_cooccurrence_weight,",
        "            'concept_min_shared_docs': self.concept_min_shared_docs,",
        "            'concept_min_jaccard': self.concept_min_jaccard,",
        "            'concept_embedding_threshold': self.concept_embedding_threshold,",
        "            'multihop_max_hops': self.multihop_max_hops,",
        "            'multihop_decay_factor': self.multihop_decay_factor,",
        "            'multihop_min_path_score': self.multihop_min_path_score,",
        "            'inheritance_decay_factor': self.inheritance_decay_factor,",
        "            'inheritance_max_depth': self.inheritance_max_depth,",
        "            'inheritance_boost_factor': self.inheritance_boost_factor,",
        "            'relation_weights': dict(self.relation_weights),",
        "        }",
        "",
        "    @classmethod",
        "    def from_dict(cls, data: Dict) -> 'CorticalConfig':",
        "        \"\"\"",
        "        Create configuration from a dictionary.",
        "",
        "        Args:",
        "            data: Dictionary with configuration values.",
        "",
        "        Returns:",
        "            CorticalConfig instance.",
        "        \"\"\"",
        "        return cls(**data)",
        "",
        "",
        "# Valid relation chains for multi-hop inference",
        "# Maps (relation1, relation2) -> validity score (0.0 to 1.0)",
        "# Higher scores indicate more semantically valid inference chains",
        "VALID_RELATION_CHAINS: Dict[Tuple[str, str], float] = {",
        "    # Transitive hierarchies",
        "    ('IsA', 'IsA'): 1.0,           # dog IsA animal IsA living_thing",
        "    ('PartOf', 'PartOf'): 1.0,     # wheel PartOf car PartOf vehicle",
        "    ('IsA', 'HasProperty'): 0.9,   # dog IsA animal HasProperty alive",
        "    ('PartOf', 'HasProperty'): 0.8,  # wheel PartOf car HasProperty fast",
        "",
        "    # Association chains",
        "    ('RelatedTo', 'RelatedTo'): 0.6,",
        "    ('SimilarTo', 'SimilarTo'): 0.7,",
        "    ('CoOccurs', 'CoOccurs'): 0.5,",
        "    ('RelatedTo', 'IsA'): 0.7,",
        "    ('RelatedTo', 'SimilarTo'): 0.7,",
        "",
        "    # Causal chains",
        "    ('Causes', 'Causes'): 0.8,",
        "    ('Causes', 'HasProperty'): 0.7,",
        "",
        "    # Derivation chains",
        "    ('DerivedFrom', 'DerivedFrom'): 0.8,",
        "    ('DerivedFrom', 'IsA'): 0.7,",
        "",
        "    # Invalid/contradictory chains (low scores)",
        "    ('Antonym', 'IsA'): 0.1,       # Contradictory: opposite → type",
        "    ('Antonym', 'Antonym'): 0.4,   # Double negation",
        "}",
        "",
        "# Default validity score for unlisted relation chain pairs",
        "DEFAULT_CHAIN_VALIDITY: float = 0.4",
        "",
        "",
        "def get_default_config() -> CorticalConfig:",
        "    \"\"\"",
        "    Get a new instance of the default configuration.",
        "",
        "    Returns:",
        "        CorticalConfig with default values.",
        "    \"\"\"",
        "    return CorticalConfig()"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "docs/code-of-ethics.md",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "# Code of Ethics - Cortical Text Processor Development",
        "",
        "## Preamble",
        "",
        "This project demands **scientific rigor** in all aspects of development. As computational engineers, we commit to the same standards applied in peer-reviewed research: reproducibility, transparency, and intellectual honesty. Code that \"works\" is not enough - we must understand *why* it works, document its limitations, and verify our claims with evidence.",
        "",
        "---",
        "",
        "## 1. Documentation Ethics",
        "",
        "### All findings must be documented, even if they seem minor",
        "",
        "Every observation during development carries information. What seems minor today may be critical context for future debugging or feature development.",
        "",
        "**Requirements:**",
        "- Document unexpected behavior immediately, not \"when time permits\"",
        "- Include reproduction steps, not just symptoms",
        "- Note what you tried, even if it didn't work",
        "- Add context about why the behavior matters",
        "",
        "### Issues discovered during testing MUST be added to TASK_LIST.md",
        "",
        "Testing is a discovery process. Issues found during dog-fooding are **not distractions** - they are the primary signal that our assumptions need refinement.",
        "",
        "**Requirements:**",
        "- Add tasks to `TASK_LIST.md` immediately upon discovery",
        "- Include severity/priority assessment",
        "- Reference the test case or usage scenario that revealed it",
        "- Link to related code locations with absolute paths",
        "",
        "**Example:**",
        "```markdown",
        "- [ ] **Task #X**: Fix passage-level search doc-type boosting",
        "  - **File**: `/home/user/Opus-code-test/cortical/query.py:find_passages_for_query`",
        "  - **Issue**: Document-level search applies doc-type boosting, but passage-level search does not",
        "  - **Discovered**: Dog-fooding test with code search queries",
        "  - **Priority**: Medium - reduces search quality for mixed-type corpora",
        "```",
        "",
        "### No \"it works well enough\" - if there's a limitation, document it",
        "",
        "Undocumented limitations are landmines for future developers. They waste time, create confusion, and erode trust in the codebase.",
        "",
        "**Requirements:**",
        "- Add limitations to docstrings for affected functions",
        "- Document known edge cases in `TASK_LIST.md` or module comments",
        "- Be specific: \"Doesn't support X when Y\" not \"Has limitations\"",
        "- Include workarounds if available, but track the underlying issue",
        "",
        "### Workarounds are not solutions - track the underlying issue",
        "",
        "A workaround is technical debt with interest. Document it as such.",
        "",
        "**Requirements:**",
        "- Add a comment explaining WHY the workaround exists",
        "- Create a task in `TASK_LIST.md` for the proper fix",
        "- Reference the task ID in the workaround comment",
        "- Never let a workaround become permanent through neglect",
        "",
        "---",
        "",
        "## 2. Testing Ethics",
        "",
        "### Always exercise new features with real usage (dog-fooding)",
        "",
        "Unit tests verify components. Dog-fooding verifies **value**. Both are required.",
        "",
        "**Requirements:**",
        "- Use new features in realistic scenarios, not toy examples",
        "- Test against the actual project codebase (we index ourselves for a reason)",
        "- Document the dog-fooding process and results",
        "- If a feature can't be dog-fooded meaningfully, question whether it should exist",
        "",
        "### Don't just run unit tests - verify the feature works end-to-end",
        "",
        "Passing tests are necessary but not sufficient. Integration and user experience matter.",
        "",
        "**Requirements:**",
        "- Run `showcase.py` after significant changes",
        "- Verify features work through the public API, not just internal functions",
        "- Test the entire pipeline: input → processing → output → interpretation",
        "- Consider: \"Would I trust this result in production?\"",
        "",
        "### Document unexpected behavior even if tests pass",
        "",
        "Tests encode our expectations. When reality differs from expectations, reality is teaching us something.",
        "",
        "**Requirements:**",
        "- Ask \"Why?\" when behavior surprises you, even if it's good",
        "- Document counterintuitive behavior in docstrings",
        "- Update tests to cover the unexpected case",
        "- Investigate whether the surprise indicates a deeper issue",
        "",
        "### Test edge cases and document limitations",
        "",
        "The difference between research code and production code is edge case handling.",
        "",
        "**Requirements:**",
        "- Test empty corpus, single document, massive corpus",
        "- Test malformed input, Unicode edge cases, pathological queries",
        "- Document what breaks and at what scale",
        "- Add \"Known Limitations\" sections to docstrings when appropriate",
        "",
        "---",
        "",
        "## 3. Completion Standards",
        "",
        "### A task isn't done until findings are documented",
        "",
        "\"Done\" has three components: implementation, testing, and documentation. All three are mandatory.",
        "",
        "**Definition of Done:**",
        "1. Feature implemented and tests pass",
        "2. Feature exercised with real usage (dog-fooding)",
        "3. Findings, limitations, and follow-up issues documented",
        "4. `TASK_LIST.md` updated with completion status and any new tasks",
        "",
        "### If testing reveals new issues, create follow-up tasks",
        "",
        "Testing expands our understanding. New knowledge creates new work - embrace it.",
        "",
        "**Requirements:**",
        "- Create follow-up tasks immediately, don't rely on memory",
        "- Link follow-up tasks to the parent task for context",
        "- Assess priority realistically (not everything is urgent)",
        "- Close the parent task only after follow-ups are tracked",
        "",
        "### Update summary tables when completing work",
        "",
        "Summary tables in `TASK_LIST.md` provide project health metrics. Keep them current.",
        "",
        "**Requirements:**",
        "- Mark tasks complete in both the detailed list AND summary tables",
        "- Update counts, statistics, and status overviews",
        "- Note completion date and any relevant metrics",
        "- Commit task list updates with the feature implementation",
        "",
        "### Leave the codebase better documented than you found it",
        "",
        "Every commit is an opportunity to improve clarity.",
        "",
        "**Requirements:**",
        "- If you struggled to understand code, improve its documentation",
        "- Add comments explaining the \"why\" behind non-obvious decisions",
        "- Update docstrings when behavior changes",
        "- Fix misleading comments immediately - they're worse than no comments",
        "",
        "---",
        "",
        "## 4. Scientific Rigor",
        "",
        "### Be skeptical of \"working\" results",
        "",
        "In science, reproducibility and understanding matter more than outcomes. Apply the same standard here.",
        "",
        "**Requirements:**",
        "- Question why a fix works, don't just celebrate that it does",
        "- Test the boundaries: when does it work? When does it fail?",
        "- Look for alternative explanations",
        "- Be especially skeptical of fixes that \"just work\" without clear causation",
        "",
        "### Verify claims with evidence",
        "",
        "Anecdotes are hypotheses. Measurements are evidence.",
        "",
        "**Requirements:**",
        "- Use quantitative metrics: execution time, memory usage, result quality",
        "- Provide reproduction steps for performance claims",
        "- Compare before/after with controlled tests",
        "- Document test methodology so others can verify",
        "",
        "### Document both successes AND limitations",
        "",
        "A complete scientific result includes what was learned, what worked, and what didn't.",
        "",
        "**Requirements:**",
        "- Note what approaches were tried and failed",
        "- Document performance characteristics (time/space complexity)",
        "- List known failure modes or edge cases",
        "- Be honest about scope: \"Solves X but not Y\"",
        "",
        "### Follow the evidence, not assumptions",
        "",
        "Our mental models are often wrong. The code and data don't lie.",
        "",
        "**Requirements:**",
        "- When behavior contradicts expectations, trust the behavior",
        "- Investigate discrepancies thoroughly before dismissing them",
        "- Update your understanding based on evidence",
        "- Document surprising findings - they're often the most valuable",
        "",
        "---",
        "",
        "## Enforcement",
        "",
        "This is not a bureaucratic exercise. These standards exist because:",
        "",
        "1. **We index our own codebase** - poor documentation directly impacts our tooling",
        "2. **We depend on our own library** - bugs and limitations affect our work",
        "3. **We are scientists** - rigor is not optional",
        "4. **We respect future developers** - including our future selves",
        "",
        "Violations aren't moral failures - they're opportunities to learn. When you notice a gap:",
        "",
        "1. Document it immediately",
        "2. Fix it if time permits",
        "3. Track it in `TASK_LIST.md` if not",
        "4. Improve processes to prevent recurrence",
        "",
        "---",
        "",
        "## Example: The Doc-Type Boosting Case Study",
        "",
        "**What happened:** Document-level search (`find_documents_for_query`) correctly applied doc-type boosting. Passage-level search (`find_passages_for_query`) did not, despite claiming to support the feature.",
        "",
        "**What we did wrong:**",
        "- Unit tests passed but didn't cover the integration path",
        "- Dog-fooding test existed but results weren't critically examined",
        "- The limitation wasn't documented in the docstring",
        "- No task was created when the gap was first noticed",
        "",
        "**What we should have done:**",
        "1. Add explicit test case for passage-level doc-type boosting",
        "2. Run dog-fooding test and examine actual score contributions",
        "3. Document in `find_passages_for_query` docstring: \"Note: Currently does not apply doc-type boosting (Task #XX)\"",
        "4. Create task in `TASK_LIST.md` immediately upon discovery",
        "5. Mark parent task as complete only after documenting this limitation",
        "",
        "**This is the standard.** Match it consistently, and the codebase will remain trustworthy.",
        "",
        "---",
        "",
        "*\"The first principle is that you must not fool yourself — and you are the easiest person to fool.\"* - Richard Feynman"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "docs/definition-of-done.md",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "# Definition of Done",
        "",
        "This document defines when a task is truly \"done\" versus merely \"code complete\". Following these criteria ensures that work is production-ready and all discoveries are properly documented.",
        "",
        "## Context",
        "",
        "During feature development, it's easy to focus solely on implementation and overlook critical steps like documentation, verification, and issue tracking. This document provides a checklist to prevent incomplete work from being marked as finished.",
        "",
        "**Example**: While implementing passage-level search features, we discovered a gap in passage-level boosting that could have been lost if not explicitly documented and added to TASK_LIST.md.",
        "",
        "---",
        "",
        "## Completion Criteria",
        "",
        "### 1. Code Complete (Necessary but Not Sufficient)",
        "",
        "- [ ] Implementation finished and functionally correct",
        "- [ ] Unit tests written and passing",
        "- [ ] No regressions in existing tests (full test suite passes)",
        "- [ ] Code follows project style guidelines",
        "- [ ] Type hints added to all public functions",
        "- [ ] No obvious performance issues introduced",
        "",
        "**Command to verify:**",
        "```bash",
        "python -m unittest discover -s tests -v",
        "```",
        "",
        "### 2. Documentation Complete",
        "",
        "- [ ] All public functions have Google-style docstrings",
        "  - Args section with types and descriptions",
        "  - Returns section with type and description",
        "  - Examples if the function is non-trivial",
        "- [ ] TASK_LIST.md updated with:",
        "  - Task marked as DONE",
        "  - Solution details added",
        "  - Implementation notes if applicable",
        "- [ ] New APIs documented in relevant files:",
        "  - Added to CLAUDE.md quick reference if user-facing",
        "  - Usage examples provided for complex features",
        "- [ ] PATTERNS.md updated if new patterns introduced",
        "",
        "**Files to check:**",
        "- Source code docstrings",
        "- `/home/user/Opus-code-test/TASK_LIST.md`",
        "- `/home/user/Opus-code-test/CLAUDE.md`",
        "- `/home/user/Opus-code-test/docs/PATTERNS.md`",
        "",
        "### 3. Verification Complete",
        "",
        "- [ ] Feature tested end-to-end (not just unit tests)",
        "  - Run showcase.py to verify integration",
        "  - Test with realistic data, not just toy examples",
        "- [ ] Dog-fooding performed when applicable:",
        "  - Use codebase search to find related code",
        "  - Test feature on the Cortical codebase itself",
        "  - Verify behavior matches expectations",
        "- [ ] Edge cases explored:",
        "  - Empty corpus",
        "  - Single document",
        "  - Large corpus (performance testing)",
        "  - Malformed input",
        "  - Boundary conditions",
        "- [ ] Limitations documented:",
        "  - Known issues noted in docstrings or TASK_LIST.md",
        "  - Performance characteristics documented",
        "  - Unsupported use cases called out",
        "",
        "**Commands to verify:**",
        "```bash",
        "python showcase.py",
        "python scripts/search_codebase.py \"your feature keywords\"",
        "```",
        "",
        "### 4. Issue Tracking Complete",
        "",
        "This is the step that is most often skipped but is critical for maintaining project knowledge.",
        "",
        "- [ ] All discovered issues added to TASK_LIST.md:",
        "  - New tasks created with clear descriptions",
        "  - Priority assigned (Critical/High/Medium/Low)",
        "  - Effort estimated (Small/Medium/Large)",
        "  - Dependencies noted",
        "- [ ] Summary tables updated:",
        "  - Task counts reflect new additions",
        "  - Status categories accurate",
        "  - No orphaned task numbers",
        "- [ ] Related tasks cross-referenced:",
        "  - \"See Task #X\" links added where relevant",
        "  - Dependencies noted in both directions",
        "- [ ] Future work captured:",
        "  - \"Nice to have\" features documented",
        "  - Performance optimization opportunities noted",
        "  - Potential extensions recorded",
        "",
        "**Example**: When implementing passage search, we found that passage-level boosting was missing. This became Task #66, properly categorized and linked to related search tasks.",
        "",
        "### 5. Truly Done",
        "",
        "All previous criteria met, plus:",
        "",
        "- [ ] Changes committed with descriptive message:",
        "  - Follows project commit message style",
        "  - References task numbers",
        "  - Explains the \"why\" not just the \"what\"",
        "- [ ] Commit includes all related files:",
        "  - Source code changes",
        "  - Test updates",
        "  - Documentation updates",
        "  - TASK_LIST.md changes",
        "- [ ] Changes pushed to remote branch",
        "- [ ] Ready for review/merge:",
        "  - No \"TODO\" comments left in code",
        "  - No commented-out debugging code",
        "  - No temporary files committed",
        "",
        "**Git commands:**",
        "```bash",
        "git status",
        "git diff",
        "git add <relevant files>",
        "git commit -m \"Implement Task #X: <description>\"",
        "git push origin <branch-name>",
        "```",
        "",
        "---",
        "",
        "## Quick Check",
        "",
        "Before marking a task as DONE, answer these questions:",
        "",
        "### Testing",
        "- [ ] Did I test this with real usage beyond unit tests?",
        "- [ ] Did I run the full test suite without failures?",
        "- [ ] Did I test edge cases (empty, single, large)?",
        "- [ ] Did I verify behavior in showcase.py or dog-fooding scripts?",
        "",
        "### Documentation",
        "- [ ] Did I document all findings, even unexpected ones?",
        "- [ ] Did I update TASK_LIST.md with solution details?",
        "- [ ] Do all new functions have complete docstrings?",
        "- [ ] Did I update user-facing documentation (CLAUDE.md)?",
        "",
        "### Issue Tracking",
        "- [ ] Did I create tasks for any issues found during implementation?",
        "- [ ] Did I create tasks for any limitations discovered?",
        "- [ ] Did I create tasks for related work that would improve this feature?",
        "- [ ] Are the summary tables in TASK_LIST.md current?",
        "",
        "### Completeness",
        "- [ ] Is the code committed with a descriptive message?",
        "- [ ] Are all related files included in the commit?",
        "- [ ] Is there any \"TODO\" or temporary code still present?",
        "- [ ] Would another developer understand this work from the documentation?",
        "",
        "**If any answer is \"no\", the task is not done.**",
        "",
        "---",
        "",
        "## Anti-Patterns to Avoid",
        "",
        "### The \"Quick Fix\" Trap",
        "**Symptom**: Implementing a feature and immediately marking it done without verification.",
        "",
        "**Problem**: Issues discovered later require context-switching and rework.",
        "",
        "**Solution**: Always run end-to-end tests and dog-fooding before marking done.",
        "",
        "### The \"It Works on My Machine\" Trap",
        "**Symptom**: Testing only the happy path with toy data.",
        "",
        "**Problem**: Edge cases fail in production or for other users.",
        "",
        "**Solution**: Test with realistic data, empty corpus, and boundary conditions.",
        "",
        "### The \"Lost Knowledge\" Trap",
        "**Symptom**: Discovering an issue during implementation but not documenting it.",
        "",
        "**Problem**: Issue gets forgotten and resurfaces later without context.",
        "",
        "**Solution**: Immediately add discovered issues to TASK_LIST.md, even if they're out of scope.",
        "",
        "### The \"Partial Commit\" Trap",
        "**Symptom**: Committing code changes but forgetting to commit documentation updates.",
        "",
        "**Problem**: Code and documentation fall out of sync.",
        "",
        "**Solution**: Use git status before committing to verify all related files are included.",
        "",
        "---",
        "",
        "## Template: Pre-Commit Checklist",
        "",
        "Copy this checklist into your task notes or PR description:",
        "",
        "```markdown",
        "## Definition of Done Checklist",
        "",
        "### Code Complete",
        "- [ ] Implementation finished",
        "- [ ] Unit tests passing",
        "- [ ] Full test suite passing",
        "- [ ] Type hints added",
        "",
        "### Documentation Complete",
        "- [ ] Docstrings added",
        "- [ ] TASK_LIST.md updated",
        "- [ ] User docs updated (if applicable)",
        "",
        "### Verification Complete",
        "- [ ] End-to-end testing done",
        "- [ ] showcase.py verified",
        "- [ ] Edge cases tested",
        "- [ ] Limitations documented",
        "",
        "### Issue Tracking Complete",
        "- [ ] New issues added to TASK_LIST.md",
        "- [ ] Summary tables updated",
        "- [ ] Dependencies noted",
        "",
        "### Truly Done",
        "- [ ] All files committed",
        "- [ ] Descriptive commit message",
        "- [ ] Ready for review",
        "```",
        "",
        "---",
        "",
        "## Process Flow",
        "",
        "```",
        "┌─────────────────┐",
        "│ Code Complete   │",
        "│ (Tests Pass)    │",
        "└────────┬────────┘",
        "         │",
        "         ▼",
        "┌─────────────────┐",
        "│ Documentation   │",
        "│ Complete        │",
        "└────────┬────────┘",
        "         │",
        "         ▼",
        "┌─────────────────┐",
        "│ Verification    │◄──── Found Issue? ────┐",
        "│ Complete        │                       │",
        "└────────┬────────┘                       │",
        "         │                                │",
        "         ▼                                │",
        "┌─────────────────┐                       │",
        "│ Issue Tracking  │───────────────────────┘",
        "│ Complete        │  Add to TASK_LIST.md",
        "└────────┬────────┘",
        "         │",
        "         ▼",
        "┌─────────────────┐",
        "│ Commit & Push   │",
        "│ (Truly Done)    │",
        "└─────────────────┘",
        "```",
        "",
        "---",
        "",
        "## Examples",
        "",
        "### Good: Task #59 (Add Metadata to Minicolumns)",
        "",
        "**What made it good:**",
        "- Code implemented with full type hints",
        "- Tests added and passing",
        "- Docstrings complete",
        "- TASK_LIST.md updated with solution details",
        "- Discovered prerequisite for Task #65 and documented it",
        "- Committed with clear message",
        "",
        "### Could Be Better: Initial Passage Search Implementation",
        "",
        "**What was missing:**",
        "- Almost forgot to document passage-level boosting gap",
        "- Didn't initially create Task #66 for the missing feature",
        "- Would have lost knowledge without explicit documentation",
        "",
        "**Lesson**: Always document issues found during implementation, even if they're out of scope for the current task.",
        "",
        "---",
        "",
        "## Summary",
        "",
        "**Code Complete ≠ Done**",
        "",
        "A task is only done when:",
        "1. Code works and tests pass",
        "2. Documentation is complete and accurate",
        "3. Verification confirms real-world usage works",
        "4. All discovered issues are tracked",
        "5. Changes are committed and pushed",
        "",
        "**When in doubt, ask:**",
        "- \"Would I be comfortable if someone else had to maintain this tomorrow?\"",
        "- \"Did I document everything I learned, including problems found?\"",
        "- \"Could someone else understand this work from the documentation alone?\"",
        "",
        "If the answer to any question is \"no\", keep working. Your future self (and your teammates) will thank you."
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "docs/dogfooding-checklist.md",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "# Dog-Fooding Checklist",
        "",
        "This checklist ensures we systematically test features using the Cortical Text Processor itself. The goal is to catch issues like the passage-level search doc-type boosting bug **before** they make it into production.",
        "",
        "## Context",
        "",
        "When implementing search features, it's easy to test individual components in isolation but miss integration issues. By actually using the system to index and search its own codebase, we catch problems that only appear in real-world usage.",
        "",
        "---",
        "",
        "## 1. Pre-Testing Setup",
        "",
        "- [ ] **Re-index the codebase after changes**",
        "  - Why: New code/docs won't appear in search results if the index is stale",
        "  - Command: `python scripts/index_codebase.py --incremental`",
        "",
        "- [ ] **Verify index completed successfully**",
        "  - Why: Partial failures can lead to inconsistent state",
        "  - Check: Look for \"✓ Indexing complete\" message, no exceptions",
        "",
        "- [ ] **Check document count matches expectations**",
        "  - Why: Missing or duplicate documents indicate indexing problems",
        "  - Command: `python scripts/search_codebase.py \"/stats\"` in interactive mode",
        "  - Verify: Count matches `find cortical tests -name \"*.py\" | wc -l` (or similar)",
        "",
        "---",
        "",
        "## 2. Search Quality Checks",
        "",
        "- [ ] **Test document-level search with known queries**",
        "  - Why: Baseline for comparing against passage-level search",
        "  - Example queries:",
        "    - `\"PageRank algorithm\"`",
        "    - `\"bigram separator\"`",
        "    - `\"compute TF-IDF\"`",
        "  - Verify: Relevant files appear in top 5 results",
        "",
        "- [ ] **Test passage-level search with same queries**",
        "  - Why: Passage-level should return focused context from same documents",
        "  - Command: Use `find_passages_for_query()` or `search_codebase.py` with passage mode",
        "  - Verify: Results point to correct files and line ranges",
        "",
        "- [ ] **Compare results - are they consistent?**",
        "  - Why: Document and passage results should be complementary, not contradictory",
        "  - Check: If `analysis.py` is #1 for doc search, it should appear in passage results too",
        "",
        "- [ ] **Test conceptual queries (\"what is X\")**",
        "  - Why: Should surface documentation and explanatory comments",
        "  - Example queries:",
        "    - `\"what is a minicolumn\"`",
        "    - `\"how does PageRank work\"`",
        "    - `\"concept clustering algorithm\"`",
        "  - Expected: `.md` files and docstrings rank highly",
        "",
        "- [ ] **Test implementation queries (\"where is X\")**",
        "  - Why: Should surface actual code implementations",
        "  - Example queries:",
        "    - `\"where is PageRank computed\"`",
        "    - `\"implementation of TF-IDF\"`",
        "    - `\"add document incremental\"`",
        "  - Expected: `.py` files with actual functions rank highly",
        "",
        "- [ ] **Verify doc-type boosting is working**",
        "  - Why: Catches the exact bug we found (passage search ignoring doc-type boosts)",
        "  - Test: For conceptual query, check if `.md` files are boosted",
        "  - Test: For implementation query, check if `.py` files are boosted",
        "  - Evidence: Compare scores with/without doc-type filter",
        "",
        "- [ ] **Check if documentation surfaces for conceptual queries**",
        "  - Why: Users asking \"what\" questions need docs, not raw code",
        "  - Query: `\"hierarchical layer structure\"`",
        "  - Expected: `CLAUDE.md` or relevant docs appear in top 3",
        "",
        "---",
        "",
        "## 3. New Feature Verification",
        "",
        "- [ ] **Search for terms from new code/docs**",
        "  - Why: Ensures new content is indexed and retrievable",
        "  - Action: Identify 2-3 unique terms from your new code",
        "  - Query: Search for those terms",
        "  - Verify: New file appears in results",
        "",
        "- [ ] **Verify new files appear in results**",
        "  - Why: New files might not be indexed if patterns are wrong",
        "  - Check: Search for filename or unique content",
        "  - Verify: File is in top results",
        "",
        "- [ ] **Test the specific feature end-to-end**",
        "  - Why: Unit tests may pass but integration fails",
        "  - Action: Use the exact workflow a user would follow",
        "  - Example: If you added intent parsing, run `search_by_intent()` on real corpus",
        "",
        "- [ ] **Try edge cases**",
        "  - Why: Edge cases reveal assumptions in the code",
        "  - Examples:",
        "    - Empty query",
        "    - Very long query (50+ words)",
        "    - Query with special characters",
        "    - Query matching zero documents",
        "    - Query matching all documents",
        "",
        "---",
        "",
        "## 4. Issue Discovery Protocol",
        "",
        "- [ ] **Document any unexpected behavior**",
        "  - Why: Memory is fallible; write it down immediately",
        "  - Format: Query → Expected → Actual → Why it matters",
        "",
        "- [ ] **Add new tasks to TASK_LIST.md immediately**",
        "  - Why: Issues discovered during testing are easy to forget",
        "  - Template:",
        "    ```markdown",
        "    ## Task #XX: Fix [brief description]",
        "    **Status**: Not Started",
        "    **Priority**: [High/Medium/Low]",
        "    **Created**: [date]",
        "",
        "    **Description**:",
        "    When testing [feature], discovered [issue].",
        "",
        "    Query: `[search query]`",
        "    Expected: [what should happen]",
        "    Actual: [what happened]",
        "",
        "    **Root Cause** (if known):",
        "    [explanation]",
        "",
        "    **Proposed Fix**:",
        "    [how to fix it]",
        "    ```",
        "",
        "- [ ] **Include evidence (query, results, expected vs actual)**",
        "  - Why: Makes debugging easier when you return to the task",
        "  - Save: Query strings, top 5 results, scores, file paths",
        "",
        "- [ ] **Update summary tables**",
        "  - Why: Keeps TASK_LIST.md organized and scannable",
        "  - Tables to update:",
        "    - Status summary (count by status)",
        "    - Priority breakdown",
        "    - Category summary",
        "",
        "---",
        "",
        "## 5. Final Verification",
        "",
        "- [ ] **All issues documented in TASK_LIST.md?**",
        "  - Why: Un-documented issues will be forgotten",
        "  - Check: Review your testing notes and ensure every issue has a task",
        "",
        "- [ ] **Summary tables updated?**",
        "  - Why: Tables provide quick overview of project health",
        "  - Verify: Counts match number of tasks in each section",
        "",
        "- [ ] **Changes committed and pushed?**",
        "  - Why: Sharing findings with team prevents duplicate work",
        "  - Check: `git status` shows clean working directory",
        "  - Verify: Latest commit includes test findings and new tasks",
        "",
        "---",
        "",
        "## Quick Example",
        "",
        "Here's what a complete dog-fooding session looks like:",
        "",
        "```bash",
        "# 1. Re-index",
        "python scripts/index_codebase.py --incremental",
        "",
        "# 2. Test known queries",
        "python scripts/search_codebase.py \"PageRank algorithm\" --verbose",
        "python scripts/search_codebase.py \"what is a minicolumn\" --verbose",
        "",
        "# 3. Test new feature",
        "python scripts/search_codebase.py \"my new function name\" --verbose",
        "",
        "# 4. Document issues",
        "# (Open TASK_LIST.md and add any problems found)",
        "",
        "# 5. Commit findings",
        "git add docs/ TASK_LIST.md",
        "git commit -m \"Add dog-fooding findings from feature X testing\"",
        "```",
        "",
        "---",
        "",
        "## Tips",
        "",
        "- **Test early, test often**: Don't wait until feature is \"done\" to dog-food",
        "- **Use interactive mode**: `python scripts/search_codebase.py --interactive` for rapid iteration",
        "- **Compare with grep**: If search misses obvious results, something is broken",
        "- **Think like a user**: What would someone actually search for?",
        "- **Document surprises**: Even if it's \"working as designed\", unexpected behavior may indicate UX issues",
        "",
        "---",
        "",
        "## Common Issues to Watch For",
        "",
        "| Symptom | Likely Cause |",
        "|---------|--------------|",
        "| New file not in results | Not re-indexed, or file pattern excluded |",
        "| Zero results for obvious query | Tokenization issue, or term not in corpus |",
        "| Wrong files ranked #1 | Scoring bug (TF-IDF, doc-type, etc.) |",
        "| Passage and doc results diverge | Passage search missing a boost/filter |",
        "| Docs don't surface for \"what is\" | Doc-type boosting not applied |",
        "| Code doesn't surface for \"where is\" | Same as above |",
        "",
        "---",
        "",
        "*Remember: The best way to ensure quality is to actually use what we build.*"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "docs/patterns.md",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "# Usage Patterns Guide",
        "",
        "Advanced usage patterns for the Cortical Text Processor, focusing on code-aware search, semantic fingerprinting, and intent-based querying.",
        "",
        "---",
        "",
        "## Table of Contents",
        "",
        "1. [Code Search Patterns](#code-search-patterns)",
        "2. [Fingerprint Comparison](#fingerprint-comparison)",
        "3. [Intent-Based Querying](#intent-based-querying)",
        "4. [Document Type Boosting](#document-type-boosting)",
        "5. [Configuration Patterns](#configuration-patterns)",
        "",
        "---",
        "",
        "## Code Search Patterns",
        "",
        "### Pattern 1: Code-Aware Tokenization",
        "",
        "Enable identifier splitting to search for code patterns:",
        "",
        "```python",
        "from cortical import CorticalTextProcessor, Tokenizer",
        "",
        "# Create processor with code-aware tokenizer",
        "tokenizer = Tokenizer(split_identifiers=True)",
        "processor = CorticalTextProcessor()",
        "",
        "# Process code with identifier splitting",
        "processor.process_document(",
        "    \"auth.py\",",
        "    \"\"\"",
        "    def getUserCredentials(userId):",
        "        return fetchUserFromDB(userId).credentials",
        "    \"\"\"",
        ")",
        "",
        "# Now searches for \"user\" will find \"getUserCredentials\"",
        "# because it was split into [\"get\", \"user\", \"credentials\"]",
        "```",
        "",
        "**What identifier splitting does:**",
        "- `getUserCredentials` → `[\"getusercredentials\", \"get\", \"user\", \"credentials\"]`",
        "- `fetch_user_from_db` → `[\"fetch\", \"user\", \"from\", \"db\"]`",
        "- `HTTPResponseCode` → `[\"httpresponsecode\", \"http\", \"response\", \"code\"]`",
        "",
        "---",
        "",
        "### Pattern 2: Programming Concept Expansion",
        "",
        "Expand queries with programming synonyms:",
        "",
        "```python",
        "# Search with code concept expansion",
        "results = processor.find_documents_for_query(",
        "    \"fetch user data\",",
        "    use_code_concepts=True",
        ")",
        "",
        "# Or use dedicated method",
        "results = processor.expand_query_for_code(\"fetch user data\")",
        "# Expands \"fetch\" to include: get, retrieve, load, read, query",
        "# Expands \"user\" to include: account, profile, member",
        "```",
        "",
        "**Built-in concept groups:**",
        "",
        "| Concept Group | Terms |",
        "|--------------|-------|",
        "| retrieval | get, fetch, retrieve, load, read, query |",
        "| storage | save, store, write, persist, cache, put |",
        "| deletion | delete, remove, drop, clear, purge, erase |",
        "| auth | auth, authenticate, authorize, login, signin |",
        "| error | error, exception, fail, invalid, wrong |",
        "| validation | validate, check, verify, assert, ensure |",
        "| transform | convert, transform, parse, serialize, encode |",
        "| async | async, await, promise, future, callback |",
        "",
        "---",
        "",
        "### Pattern 3: Intent-Based Code Search",
        "",
        "Search by developer intent rather than exact keywords:",
        "",
        "```python",
        "# Parse natural language query into structured intent",
        "parsed = processor.parse_intent_query(\"where do we handle authentication?\")",
        "print(parsed)",
        "# {",
        "#   'intent': 'location',        # What type of answer expected",
        "#   'action': 'handle',          # The action verb",
        "#   'subject': 'authentication', # What the action operates on",
        "#   'question_word': 'where',",
        "#   'expanded_terms': ['handle', 'authentication', 'auth', 'login', ...]",
        "# }",
        "",
        "# Search with intent understanding",
        "results = processor.search_by_intent(\"how do we validate user input?\")",
        "# Returns documents relevant to validation, input checking, assertions",
        "```",
        "",
        "**Supported intent types:**",
        "",
        "| Question Word | Intent Type | Typical Results |",
        "|--------------|-------------|-----------------|",
        "| where | location | File paths, module locations |",
        "| how | implementation | Code implementation details |",
        "| what | definition | Type definitions, interfaces |",
        "| why | rationale | Comments, design decisions |",
        "| when | lifecycle | Initialization, cleanup code |",
        "",
        "---",
        "",
        "### Pattern 4: Combined Code Search",
        "",
        "Combine multiple code search features:",
        "",
        "```python",
        "# Full code search with all features",
        "results = processor.find_documents_for_query(",
        "    \"authentication handler\",",
        "    use_expansion=True,           # Lateral connection expansion",
        "    use_code_concepts=True,       # Programming synonyms",
        "    use_semantic=True             # Semantic relations",
        ")",
        "",
        "# Or use intent search for natural language",
        "results = processor.search_by_intent(",
        "    \"where is the password validation logic?\"",
        ")",
        "```",
        "",
        "---",
        "",
        "## Fingerprint Comparison",
        "",
        "Semantic fingerprinting enables comparing the meaning of code blocks without embedding models.",
        "",
        "### Pattern 5: Basic Fingerprinting",
        "",
        "```python",
        "# Get semantic fingerprint of a code block",
        "code1 = \"\"\"",
        "def authenticate_user(username, password):",
        "    user = database.find_user(username)",
        "    if user and verify_password(password, user.hash):",
        "        return create_session(user)",
        "    return None",
        "\"\"\"",
        "",
        "code2 = \"\"\"",
        "def login(name, pwd):",
        "    account = db.get_account(name)",
        "    if account and check_pwd(pwd, account.password_hash):",
        "        return generate_token(account)",
        "    return None",
        "\"\"\"",
        "",
        "fp1 = processor.get_fingerprint(code1)",
        "fp2 = processor.get_fingerprint(code2)",
        "",
        "# Compare fingerprints",
        "comparison = processor.compare_fingerprints(fp1, fp2)",
        "print(f\"Similarity: {comparison['similarity']:.2%}\")",
        "# Output: Similarity: 78.5%",
        "```",
        "",
        "**Fingerprint contents:**",
        "```python",
        "{",
        "    'terms': {'user': 0.8, 'password': 0.7, 'authenticate': 0.6, ...},",
        "    'concepts': ['L2_authentication', 'L2_database_operations'],",
        "    'bigrams': ['find user', 'verify password', ...],",
        "    'top_terms': [('user', 0.8), ('password', 0.7), ...]",
        "}",
        "```",
        "",
        "---",
        "",
        "### Pattern 6: Explain Similarity",
        "",
        "Get human-readable explanation of why two code blocks are similar:",
        "",
        "```python",
        "explanation = processor.explain_similarity(fp1, fp2)",
        "print(explanation)",
        "# Output:",
        "# Shared terms: user (0.8), password (0.7), database (0.5)",
        "# Shared concepts: authentication (2), database_operations (1)",
        "# Both use patterns: user lookup, password verification",
        "",
        "# Or explain a single fingerprint",
        "fp_explanation = processor.explain_fingerprint(fp1)",
        "print(fp_explanation)",
        "# Output:",
        "# Main concepts: authentication, database access",
        "# Key terms: user (0.8), password (0.7), authenticate (0.6)",
        "# Notable bigrams: verify password, create session",
        "```",
        "",
        "---",
        "",
        "### Pattern 7: Find Similar Code Blocks",
        "",
        "Search for similar code across the corpus:",
        "",
        "```python",
        "# Find code blocks similar to a reference",
        "target_code = \"\"\"",
        "def validate_email(email):",
        "    pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'",
        "    return re.match(pattern, email) is not None",
        "\"\"\"",
        "",
        "similar = processor.find_similar_texts(",
        "    target_code,",
        "    top_n=5,",
        "    min_similarity=0.3",
        ")",
        "",
        "for doc_id, similarity in similar:",
        "    print(f\"{doc_id}: {similarity:.2%} similar\")",
        "```",
        "",
        "---",
        "",
        "### Pattern 8: Code Deduplication",
        "",
        "Use fingerprints to detect duplicate or near-duplicate code:",
        "",
        "```python",
        "def find_duplicates(processor, min_similarity=0.85):",
        "    \"\"\"Find potentially duplicate code blocks.\"\"\"",
        "    docs = processor.documents",
        "    fingerprints = {",
        "        doc_id: processor.get_fingerprint(content)",
        "        for doc_id, content in docs.items()",
        "    }",
        "",
        "    duplicates = []",
        "    doc_ids = list(fingerprints.keys())",
        "",
        "    for i, doc_id1 in enumerate(doc_ids):",
        "        for doc_id2 in doc_ids[i+1:]:",
        "            result = processor.compare_fingerprints(",
        "                fingerprints[doc_id1],",
        "                fingerprints[doc_id2]",
        "            )",
        "            if result['similarity'] >= min_similarity:",
        "                duplicates.append((doc_id1, doc_id2, result['similarity']))",
        "",
        "    return sorted(duplicates, key=lambda x: -x[2])",
        "",
        "# Find duplicates",
        "dupes = find_duplicates(processor, min_similarity=0.9)",
        "for doc1, doc2, sim in dupes:",
        "    print(f\"Possible duplicate: {doc1} <-> {doc2} ({sim:.1%})\")",
        "```",
        "",
        "---",
        "",
        "## Intent-Based Querying",
        "",
        "### Pattern 9: Query Intent Detection",
        "",
        "Let the system auto-detect query intent:",
        "",
        "```python",
        "# The system detects query type",
        "queries = [",
        "    \"what is PageRank\",           # Conceptual (wants definition)",
        "    \"where is PageRank computed\", # Implementation (wants location)",
        "    \"how does PageRank work\",     # Implementation (wants details)",
        "]",
        "",
        "for query in queries:",
        "    is_conceptual = processor.is_conceptual_query(query)",
        "    query_type = \"conceptual\" if is_conceptual else \"implementation\"",
        "    print(f\"{query} -> {query_type}\")",
        "```",
        "",
        "---",
        "",
        "### Pattern 10: Intent-Aware Search",
        "",
        "Search with intent understanding:",
        "",
        "```python",
        "# Conceptual query: boost documentation",
        "results = processor.search_by_intent(\"what is the 4-layer architecture?\")",
        "# Will prefer: docs/architecture.md over cortical/processor.py",
        "",
        "# Implementation query: boost code",
        "results = processor.search_by_intent(\"where is PageRank computed?\")",
        "# Will prefer: cortical/analysis.py over docs/algorithms.md",
        "```",
        "",
        "---",
        "",
        "## Document Type Boosting",
        "",
        "### Pattern 11: Boost Documentation",
        "",
        "When searching for concepts, boost documentation files:",
        "",
        "```python",
        "# Auto-detect intent and boost accordingly",
        "results = processor.find_documents_with_boost(",
        "    \"PageRank algorithm\",",
        "    auto_detect_intent=True,  # Auto-boost docs for conceptual queries",
        "    top_n=5",
        ")",
        "",
        "# Always prefer documentation",
        "results = processor.find_documents_with_boost(",
        "    \"PageRank algorithm\",",
        "    prefer_docs=True,         # Always boost docs",
        "    top_n=5",
        ")",
        "",
        "# Custom boost factors",
        "results = processor.find_documents_with_boost(",
        "    \"PageRank algorithm\",",
        "    custom_boosts={",
        "        'docs': 2.0,    # Double weight for docs/ folder",
        "        'root_docs': 1.5,  # 1.5x for root-level .md",
        "        'code': 1.0,    # Normal weight for code",
        "        'test': 0.5     # Half weight for tests",
        "    }",
        ")",
        "```",
        "",
        "---",
        "",
        "### Pattern 12: Search with Type Filtering",
        "",
        "Limit search to specific document types:",
        "",
        "```python",
        "# Search only in documentation",
        "doc_results = [",
        "    (doc_id, score)",
        "    for doc_id, score in processor.find_documents_for_query(\"PageRank\")",
        "    if doc_id.endswith('.md') or doc_id.startswith('docs/')",
        "]",
        "",
        "# Search only in code",
        "code_results = [",
        "    (doc_id, score)",
        "    for doc_id, score in processor.find_documents_for_query(\"PageRank\")",
        "    if doc_id.endswith('.py') and not doc_id.startswith('tests/')",
        "]",
        "```",
        "",
        "---",
        "",
        "## Configuration Patterns",
        "",
        "### Pattern 13: Custom Configuration",
        "",
        "Use custom configuration for specific use cases:",
        "",
        "```python",
        "from cortical import CorticalTextProcessor, CorticalConfig",
        "",
        "# High-precision configuration (less expansion, stricter clustering)",
        "precision_config = CorticalConfig(",
        "    max_query_expansions=5,",
        "    cluster_strictness=1.0,",
        "    pagerank_damping=0.85",
        ")",
        "",
        "# High-recall configuration (more expansion, looser clustering)",
        "recall_config = CorticalConfig(",
        "    max_query_expansions=20,",
        "    cluster_strictness=0.5,",
        "    semantic_expansion_discount=0.8",
        ")",
        "",
        "# Create processor with configuration",
        "processor = CorticalTextProcessor(config=precision_config)",
        "```",
        "",
        "---",
        "",
        "### Pattern 14: Save and Restore Configuration",
        "",
        "```python",
        "# Save configuration with corpus",
        "config = CorticalConfig(pagerank_damping=0.9, min_cluster_size=5)",
        "processor = CorticalTextProcessor(config=config)",
        "processor.add_documents_batch(docs, recompute='full')",
        "processor.save(\"corpus.pkl\")  # Config saved with corpus",
        "",
        "# Load and check configuration",
        "loaded = CorticalTextProcessor.load(\"corpus.pkl\")",
        "print(f\"Loaded config: {loaded.config.pagerank_damping}\")",
        "",
        "# Or export/import config separately",
        "config_dict = config.to_dict()",
        "# Save to JSON",
        "import json",
        "with open(\"config.json\", \"w\") as f:",
        "    json.dump(config_dict, f)",
        "",
        "# Restore",
        "with open(\"config.json\") as f:",
        "    restored_config = CorticalConfig.from_dict(json.load(f))",
        "```",
        "",
        "---",
        "",
        "### Pattern 15: Domain-Specific Configurations",
        "",
        "```python",
        "# Code search configuration",
        "code_config = CorticalConfig(",
        "    chunk_size=256,              # Smaller chunks for code",
        "    chunk_overlap=64,",
        "    max_query_expansions=15,     # More expansion for code synonyms",
        ")",
        "",
        "# Documentation search configuration",
        "docs_config = CorticalConfig(",
        "    chunk_size=1024,             # Larger chunks for prose",
        "    chunk_overlap=256,",
        "    max_query_expansions=8,",
        ")",
        "",
        "# RAG-optimized configuration",
        "rag_config = CorticalConfig(",
        "    chunk_size=512,",
        "    chunk_overlap=128,",
        "    pagerank_iterations=30,      # More iterations for stability",
        "    cluster_strictness=0.7,      # Balanced clustering",
        ")",
        "```",
        "",
        "---",
        "",
        "## Quick Reference",
        "",
        "| Pattern | Use Case | Key Method |",
        "|---------|----------|------------|",
        "| Code tokenization | Index code files | `Tokenizer(split_identifiers=True)` |",
        "| Code concepts | Expand with synonyms | `expand_query_for_code()` |",
        "| Intent search | Natural language | `search_by_intent()` |",
        "| Fingerprinting | Compare code blocks | `get_fingerprint()`, `compare_fingerprints()` |",
        "| Similarity search | Find duplicates | `find_similar_texts()` |",
        "| Doc boosting | Prefer documentation | `find_documents_with_boost()` |",
        "| Custom config | Tune behavior | `CorticalConfig()` |",
        "",
        "---",
        "",
        "*See also: [Cookbook](cookbook.md) for more recipes, [Query Guide](query-guide.md) for query details.*"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "tests/test_chunk_indexing.py",
      "function": "class TestChunkWriter(unittest.TestCase):",
      "start_line": 215,
      "lines_added": [
        "    def test_save_no_warning_small_chunk(self):",
        "        \"\"\"Test that small chunks don't trigger a warning.\"\"\"",
        "        import warnings",
        "        with tempfile.TemporaryDirectory() as tmpdir:",
        "            writer = ChunkWriter(tmpdir)",
        "            writer.add_document('doc1', 'small content')",
        "",
        "            # Should not warn",
        "            with warnings.catch_warnings(record=True) as w:",
        "                warnings.simplefilter(\"always\")",
        "                filepath = writer.save(warn_size_kb=100)  # 100KB threshold",
        "                # Small chunk should not trigger warning",
        "                self.assertEqual(len(w), 0)",
        "                self.assertIsNotNone(filepath)",
        "",
        "    def test_save_warning_large_chunk(self):",
        "        \"\"\"Test that large chunks trigger a warning.\"\"\"",
        "        import warnings",
        "        with tempfile.TemporaryDirectory() as tmpdir:",
        "            writer = ChunkWriter(tmpdir)",
        "            # Add large content (>1KB)",
        "            large_content = 'x' * 2000  # 2KB+ of content",
        "            writer.add_document('doc1', large_content)",
        "",
        "            with warnings.catch_warnings(record=True) as w:",
        "                warnings.simplefilter(\"always\")",
        "                filepath = writer.save(warn_size_kb=1)  # 1KB threshold",
        "                # Should trigger warning",
        "                self.assertEqual(len(w), 1)",
        "                self.assertIn('exceeds', str(w[0].message))",
        "                self.assertIn('compact', str(w[0].message).lower())",
        "                self.assertIsNotNone(filepath)",
        "",
        "    def test_save_warning_disabled(self):",
        "        \"\"\"Test that warning can be disabled with warn_size_kb=0.\"\"\"",
        "        import warnings",
        "        with tempfile.TemporaryDirectory() as tmpdir:",
        "            writer = ChunkWriter(tmpdir)",
        "            large_content = 'x' * 2000",
        "            writer.add_document('doc1', large_content)",
        "",
        "            with warnings.catch_warnings(record=True) as w:",
        "                warnings.simplefilter(\"always\")",
        "                filepath = writer.save(warn_size_kb=0)  # Disabled",
        "                # Should not warn even for large chunk",
        "                self.assertEqual(len(w), 0)",
        "                self.assertIsNotNone(filepath)",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "        \"\"\"Test saving creates the chunks directory if needed.\"\"\"",
        "        with tempfile.TemporaryDirectory() as tmpdir:",
        "            chunks_dir = os.path.join(tmpdir, 'new_chunks')",
        "            writer = ChunkWriter(chunks_dir)",
        "            writer.add_document('doc1', 'content')",
        "            filepath = writer.save()",
        "",
        "            self.assertTrue(os.path.exists(chunks_dir))",
        "            self.assertTrue(filepath.exists())",
        ""
      ],
      "context_after": [
        "",
        "class TestChunkLoader(unittest.TestCase):",
        "    \"\"\"Test ChunkLoader class.\"\"\"",
        "",
        "    def test_loader_empty_directory(self):",
        "        \"\"\"Test loading from empty directory.\"\"\"",
        "        with tempfile.TemporaryDirectory() as tmpdir:",
        "            loader = ChunkLoader(tmpdir)",
        "            docs = loader.load_all()",
        "            self.assertEqual(len(docs), 0)"
      ],
      "change_type": "add"
    },
    {
      "file": "tests/test_config.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "\"\"\"",
        "Tests for the configuration module.",
        "\"\"\"",
        "",
        "import unittest",
        "",
        "from cortical.config import (",
        "    CorticalConfig,",
        "    get_default_config,",
        "    VALID_RELATION_CHAINS,",
        "    DEFAULT_CHAIN_VALIDITY,",
        ")",
        "",
        "",
        "class TestCorticalConfig(unittest.TestCase):",
        "    \"\"\"Tests for CorticalConfig dataclass.\"\"\"",
        "",
        "    def test_default_values(self):",
        "        \"\"\"Test that default values are set correctly.\"\"\"",
        "        config = CorticalConfig()",
        "",
        "        # PageRank defaults",
        "        self.assertEqual(config.pagerank_damping, 0.85)",
        "        self.assertEqual(config.pagerank_iterations, 20)",
        "        self.assertEqual(config.pagerank_tolerance, 1e-6)",
        "",
        "        # Clustering defaults",
        "        self.assertEqual(config.min_cluster_size, 3)",
        "        self.assertEqual(config.cluster_strictness, 1.0)",
        "",
        "        # Chunking defaults",
        "        self.assertEqual(config.chunk_size, 512)",
        "        self.assertEqual(config.chunk_overlap, 128)",
        "",
        "    def test_custom_values(self):",
        "        \"\"\"Test creating config with custom values.\"\"\"",
        "        config = CorticalConfig(",
        "            pagerank_damping=0.9,",
        "            min_cluster_size=5,",
        "            chunk_size=1024",
        "        )",
        "",
        "        self.assertEqual(config.pagerank_damping, 0.9)",
        "        self.assertEqual(config.min_cluster_size, 5)",
        "        self.assertEqual(config.chunk_size, 1024)",
        "        # Other defaults still apply",
        "        self.assertEqual(config.pagerank_iterations, 20)",
        "",
        "    def test_relation_weights_default(self):",
        "        \"\"\"Test that relation weights have sensible defaults.\"\"\"",
        "        config = CorticalConfig()",
        "",
        "        self.assertIn('IsA', config.relation_weights)",
        "        self.assertIn('PartOf', config.relation_weights)",
        "        self.assertIn('RelatedTo', config.relation_weights)",
        "",
        "        # IsA should have high weight",
        "        self.assertGreater(config.relation_weights['IsA'], 1.0)",
        "        # Antonym should have low weight",
        "        self.assertLess(config.relation_weights['Antonym'], 1.0)",
        "",
        "",
        "class TestConfigValidation(unittest.TestCase):",
        "    \"\"\"Tests for configuration validation.\"\"\"",
        "",
        "    def test_invalid_pagerank_damping_too_high(self):",
        "        \"\"\"Test that damping > 1 raises ValueError.\"\"\"",
        "        with self.assertRaises(ValueError) as ctx:",
        "            CorticalConfig(pagerank_damping=1.5)",
        "        self.assertIn('pagerank_damping', str(ctx.exception))",
        "",
        "    def test_invalid_pagerank_damping_too_low(self):",
        "        \"\"\"Test that damping <= 0 raises ValueError.\"\"\"",
        "        with self.assertRaises(ValueError) as ctx:",
        "            CorticalConfig(pagerank_damping=0)",
        "        self.assertIn('pagerank_damping', str(ctx.exception))",
        "",
        "    def test_invalid_pagerank_damping_negative(self):",
        "        \"\"\"Test that negative damping raises ValueError.\"\"\"",
        "        with self.assertRaises(ValueError) as ctx:",
        "            CorticalConfig(pagerank_damping=-0.5)",
        "        self.assertIn('pagerank_damping', str(ctx.exception))",
        "",
        "    def test_invalid_pagerank_iterations(self):",
        "        \"\"\"Test that iterations < 1 raises ValueError.\"\"\"",
        "        with self.assertRaises(ValueError) as ctx:",
        "            CorticalConfig(pagerank_iterations=0)",
        "        self.assertIn('pagerank_iterations', str(ctx.exception))",
        "",
        "    def test_invalid_pagerank_tolerance(self):",
        "        \"\"\"Test that tolerance <= 0 raises ValueError.\"\"\"",
        "        with self.assertRaises(ValueError) as ctx:",
        "            CorticalConfig(pagerank_tolerance=0)",
        "        self.assertIn('pagerank_tolerance', str(ctx.exception))",
        "",
        "    def test_invalid_min_cluster_size(self):",
        "        \"\"\"Test that min_cluster_size < 1 raises ValueError.\"\"\"",
        "        with self.assertRaises(ValueError) as ctx:",
        "            CorticalConfig(min_cluster_size=0)",
        "        self.assertIn('min_cluster_size', str(ctx.exception))",
        "",
        "    def test_invalid_cluster_strictness_too_high(self):",
        "        \"\"\"Test that cluster_strictness > 1 raises ValueError.\"\"\"",
        "        with self.assertRaises(ValueError) as ctx:",
        "            CorticalConfig(cluster_strictness=1.5)",
        "        self.assertIn('cluster_strictness', str(ctx.exception))",
        "",
        "    def test_invalid_cluster_strictness_negative(self):",
        "        \"\"\"Test that cluster_strictness < 0 raises ValueError.\"\"\"",
        "        with self.assertRaises(ValueError) as ctx:",
        "            CorticalConfig(cluster_strictness=-0.1)",
        "        self.assertIn('cluster_strictness', str(ctx.exception))",
        "",
        "    def test_invalid_chunk_size(self):",
        "        \"\"\"Test that chunk_size < 1 raises ValueError.\"\"\"",
        "        with self.assertRaises(ValueError) as ctx:",
        "            CorticalConfig(chunk_size=0)",
        "        self.assertIn('chunk_size', str(ctx.exception))",
        "",
        "    def test_invalid_chunk_overlap_negative(self):",
        "        \"\"\"Test that negative chunk_overlap raises ValueError.\"\"\"",
        "        with self.assertRaises(ValueError) as ctx:",
        "            CorticalConfig(chunk_overlap=-1)",
        "        self.assertIn('chunk_overlap', str(ctx.exception))",
        "",
        "    def test_invalid_chunk_overlap_too_large(self):",
        "        \"\"\"Test that chunk_overlap >= chunk_size raises ValueError.\"\"\"",
        "        with self.assertRaises(ValueError) as ctx:",
        "            CorticalConfig(chunk_size=100, chunk_overlap=100)",
        "        self.assertIn('chunk_overlap', str(ctx.exception))",
        "",
        "    def test_invalid_cross_layer_damping(self):",
        "        \"\"\"Test that cross_layer_damping outside (0,1) raises ValueError.\"\"\"",
        "        with self.assertRaises(ValueError) as ctx:",
        "            CorticalConfig(cross_layer_damping=1.0)",
        "        self.assertIn('cross_layer_damping', str(ctx.exception))",
        "",
        "    def test_invalid_semantic_expansion_discount(self):",
        "        \"\"\"Test that semantic_expansion_discount outside [0,1] raises ValueError.\"\"\"",
        "        with self.assertRaises(ValueError) as ctx:",
        "            CorticalConfig(semantic_expansion_discount=1.5)",
        "        self.assertIn('semantic_expansion_discount', str(ctx.exception))",
        "",
        "    def test_valid_boundary_values(self):",
        "        \"\"\"Test that valid boundary values are accepted.\"\"\"",
        "        # Should not raise",
        "        config = CorticalConfig(",
        "            pagerank_damping=0.99,",
        "            cluster_strictness=0.0,",
        "            semantic_expansion_discount=0.0,",
        "            chunk_overlap=0",
        "        )",
        "        self.assertEqual(config.pagerank_damping, 0.99)",
        "        self.assertEqual(config.cluster_strictness, 0.0)",
        "",
        "",
        "class TestConfigCopy(unittest.TestCase):",
        "    \"\"\"Tests for configuration copying.\"\"\"",
        "",
        "    def test_copy_creates_new_instance(self):",
        "        \"\"\"Test that copy creates a new independent instance.\"\"\"",
        "        original = CorticalConfig(pagerank_damping=0.9)",
        "        copied = original.copy()",
        "",
        "        self.assertIsNot(original, copied)",
        "        self.assertEqual(original.pagerank_damping, copied.pagerank_damping)",
        "",
        "    def test_copy_is_independent(self):",
        "        \"\"\"Test that modifying copy doesn't affect original.\"\"\"",
        "        original = CorticalConfig()",
        "        copied = original.copy()",
        "",
        "        # Modify the copy's relation weights",
        "        copied.relation_weights['IsA'] = 999.0",
        "",
        "        # Original should be unchanged",
        "        self.assertNotEqual(original.relation_weights['IsA'], 999.0)",
        "",
        "    def test_copy_preserves_all_values(self):",
        "        \"\"\"Test that copy preserves all configuration values.\"\"\"",
        "        original = CorticalConfig(",
        "            pagerank_damping=0.9,",
        "            min_cluster_size=5,",
        "            chunk_size=1024,",
        "            isolation_threshold=0.05",
        "        )",
        "        copied = original.copy()",
        "",
        "        self.assertEqual(copied.pagerank_damping, 0.9)",
        "        self.assertEqual(copied.min_cluster_size, 5)",
        "        self.assertEqual(copied.chunk_size, 1024)",
        "        self.assertEqual(copied.isolation_threshold, 0.05)",
        "",
        "",
        "class TestConfigSerialization(unittest.TestCase):",
        "    \"\"\"Tests for configuration serialization.\"\"\"",
        "",
        "    def test_to_dict(self):",
        "        \"\"\"Test converting config to dictionary.\"\"\"",
        "        config = CorticalConfig(pagerank_damping=0.9)",
        "        data = config.to_dict()",
        "",
        "        self.assertIsInstance(data, dict)",
        "        self.assertEqual(data['pagerank_damping'], 0.9)",
        "        self.assertIn('relation_weights', data)",
        "",
        "    def test_from_dict(self):",
        "        \"\"\"Test creating config from dictionary.\"\"\"",
        "        data = {",
        "            'pagerank_damping': 0.9,",
        "            'min_cluster_size': 5,",
        "            'pagerank_iterations': 20,",
        "            'pagerank_tolerance': 1e-6,",
        "            'cluster_strictness': 1.0,",
        "            'isolation_threshold': 0.02,",
        "            'well_connected_threshold': 0.03,",
        "            'weak_topic_tfidf_threshold': 0.005,",
        "            'bridge_similarity_min': 0.005,",
        "            'bridge_similarity_max': 0.03,",
        "            'chunk_size': 512,",
        "            'chunk_overlap': 128,",
        "            'max_query_expansions': 10,",
        "            'semantic_expansion_discount': 0.7,",
        "            'cross_layer_damping': 0.7,",
        "            'bigram_component_weight': 0.5,",
        "            'bigram_chain_weight': 0.7,",
        "            'bigram_cooccurrence_weight': 0.3,",
        "            'concept_min_shared_docs': 1,",
        "            'concept_min_jaccard': 0.1,",
        "            'concept_embedding_threshold': 0.3,",
        "            'multihop_max_hops': 2,",
        "            'multihop_decay_factor': 0.5,",
        "            'multihop_min_path_score': 0.3,",
        "            'inheritance_decay_factor': 0.7,",
        "            'inheritance_max_depth': 5,",
        "            'inheritance_boost_factor': 0.3,",
        "            'relation_weights': {'IsA': 1.5, 'RelatedTo': 1.0}",
        "        }",
        "        config = CorticalConfig.from_dict(data)",
        "",
        "        self.assertEqual(config.pagerank_damping, 0.9)",
        "        self.assertEqual(config.min_cluster_size, 5)",
        "",
        "    def test_roundtrip(self):",
        "        \"\"\"Test that to_dict and from_dict are inverses.\"\"\"",
        "        original = CorticalConfig(",
        "            pagerank_damping=0.9,",
        "            min_cluster_size=5,",
        "            chunk_size=1024",
        "        )",
        "",
        "        data = original.to_dict()",
        "        restored = CorticalConfig.from_dict(data)",
        "",
        "        self.assertEqual(original.pagerank_damping, restored.pagerank_damping)",
        "        self.assertEqual(original.min_cluster_size, restored.min_cluster_size)",
        "        self.assertEqual(original.chunk_size, restored.chunk_size)",
        "",
        "",
        "class TestGetDefaultConfig(unittest.TestCase):",
        "    \"\"\"Tests for get_default_config function.\"\"\"",
        "",
        "    def test_returns_config_instance(self):",
        "        \"\"\"Test that get_default_config returns a CorticalConfig.\"\"\"",
        "        config = get_default_config()",
        "        self.assertIsInstance(config, CorticalConfig)",
        "",
        "    def test_returns_new_instance_each_time(self):",
        "        \"\"\"Test that get_default_config returns new instances.\"\"\"",
        "        config1 = get_default_config()",
        "        config2 = get_default_config()",
        "        self.assertIsNot(config1, config2)",
        "",
        "",
        "class TestValidRelationChains(unittest.TestCase):",
        "    \"\"\"Tests for VALID_RELATION_CHAINS constant.\"\"\"",
        "",
        "    def test_transitive_chains_high_score(self):",
        "        \"\"\"Test that transitive chains have high validity scores.\"\"\"",
        "        self.assertEqual(VALID_RELATION_CHAINS[('IsA', 'IsA')], 1.0)",
        "        self.assertEqual(VALID_RELATION_CHAINS[('PartOf', 'PartOf')], 1.0)",
        "",
        "    def test_contradictory_chains_low_score(self):",
        "        \"\"\"Test that contradictory chains have low validity scores.\"\"\"",
        "        self.assertLess(VALID_RELATION_CHAINS[('Antonym', 'IsA')], 0.5)",
        "",
        "    def test_association_chains_medium_score(self):",
        "        \"\"\"Test that association chains have medium validity scores.\"\"\"",
        "        score = VALID_RELATION_CHAINS[('RelatedTo', 'RelatedTo')]",
        "        self.assertGreater(score, 0.3)",
        "        self.assertLess(score, 0.8)",
        "",
        "    def test_default_chain_validity(self):",
        "        \"\"\"Test DEFAULT_CHAIN_VALIDITY value.\"\"\"",
        "        self.assertEqual(DEFAULT_CHAIN_VALIDITY, 0.4)",
        "",
        "",
        "if __name__ == '__main__':",
        "    unittest.main()"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    }
  ],
  "hour_of_day": 1,
  "day_of_week": "Thursday",
  "seconds_since_last_commit": -390242,
  "is_merge": true,
  "is_initial": false,
  "parent_count": 2,
  "session_id": null,
  "related_chats": [],
  "ci_result": null,
  "reverted": false,
  "amended": false
}