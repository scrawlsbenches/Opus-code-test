{
  "hash": "ab082ee0c78f4ce449d8966bb76b92e5e5508ca4",
  "message": "Add development ethics and process documentation",
  "author": "Claude",
  "timestamp": "2025-12-11 00:56:44 +0000",
  "branch": "claude/multi-index-design-DvifZ",
  "files_changed": [
    "docs/code-of-ethics.md",
    "docs/definition-of-done.md",
    "docs/dogfooding-checklist.md"
  ],
  "insertions": 749,
  "deletions": 0,
  "hunks": [
    {
      "file": "docs/code-of-ethics.md",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "# Code of Ethics - Cortical Text Processor Development",
        "",
        "## Preamble",
        "",
        "This project demands **scientific rigor** in all aspects of development. As computational engineers, we commit to the same standards applied in peer-reviewed research: reproducibility, transparency, and intellectual honesty. Code that \"works\" is not enough - we must understand *why* it works, document its limitations, and verify our claims with evidence.",
        "",
        "---",
        "",
        "## 1. Documentation Ethics",
        "",
        "### All findings must be documented, even if they seem minor",
        "",
        "Every observation during development carries information. What seems minor today may be critical context for future debugging or feature development.",
        "",
        "**Requirements:**",
        "- Document unexpected behavior immediately, not \"when time permits\"",
        "- Include reproduction steps, not just symptoms",
        "- Note what you tried, even if it didn't work",
        "- Add context about why the behavior matters",
        "",
        "### Issues discovered during testing MUST be added to TASK_LIST.md",
        "",
        "Testing is a discovery process. Issues found during dog-fooding are **not distractions** - they are the primary signal that our assumptions need refinement.",
        "",
        "**Requirements:**",
        "- Add tasks to `TASK_LIST.md` immediately upon discovery",
        "- Include severity/priority assessment",
        "- Reference the test case or usage scenario that revealed it",
        "- Link to related code locations with absolute paths",
        "",
        "**Example:**",
        "```markdown",
        "- [ ] **Task #X**: Fix passage-level search doc-type boosting",
        "  - **File**: `/home/user/Opus-code-test/cortical/query.py:find_passages_for_query`",
        "  - **Issue**: Document-level search applies doc-type boosting, but passage-level search does not",
        "  - **Discovered**: Dog-fooding test with code search queries",
        "  - **Priority**: Medium - reduces search quality for mixed-type corpora",
        "```",
        "",
        "### No \"it works well enough\" - if there's a limitation, document it",
        "",
        "Undocumented limitations are landmines for future developers. They waste time, create confusion, and erode trust in the codebase.",
        "",
        "**Requirements:**",
        "- Add limitations to docstrings for affected functions",
        "- Document known edge cases in `TASK_LIST.md` or module comments",
        "- Be specific: \"Doesn't support X when Y\" not \"Has limitations\"",
        "- Include workarounds if available, but track the underlying issue",
        "",
        "### Workarounds are not solutions - track the underlying issue",
        "",
        "A workaround is technical debt with interest. Document it as such.",
        "",
        "**Requirements:**",
        "- Add a comment explaining WHY the workaround exists",
        "- Create a task in `TASK_LIST.md` for the proper fix",
        "- Reference the task ID in the workaround comment",
        "- Never let a workaround become permanent through neglect",
        "",
        "---",
        "",
        "## 2. Testing Ethics",
        "",
        "### Always exercise new features with real usage (dog-fooding)",
        "",
        "Unit tests verify components. Dog-fooding verifies **value**. Both are required.",
        "",
        "**Requirements:**",
        "- Use new features in realistic scenarios, not toy examples",
        "- Test against the actual project codebase (we index ourselves for a reason)",
        "- Document the dog-fooding process and results",
        "- If a feature can't be dog-fooded meaningfully, question whether it should exist",
        "",
        "### Don't just run unit tests - verify the feature works end-to-end",
        "",
        "Passing tests are necessary but not sufficient. Integration and user experience matter.",
        "",
        "**Requirements:**",
        "- Run `showcase.py` after significant changes",
        "- Verify features work through the public API, not just internal functions",
        "- Test the entire pipeline: input → processing → output → interpretation",
        "- Consider: \"Would I trust this result in production?\"",
        "",
        "### Document unexpected behavior even if tests pass",
        "",
        "Tests encode our expectations. When reality differs from expectations, reality is teaching us something.",
        "",
        "**Requirements:**",
        "- Ask \"Why?\" when behavior surprises you, even if it's good",
        "- Document counterintuitive behavior in docstrings",
        "- Update tests to cover the unexpected case",
        "- Investigate whether the surprise indicates a deeper issue",
        "",
        "### Test edge cases and document limitations",
        "",
        "The difference between research code and production code is edge case handling.",
        "",
        "**Requirements:**",
        "- Test empty corpus, single document, massive corpus",
        "- Test malformed input, Unicode edge cases, pathological queries",
        "- Document what breaks and at what scale",
        "- Add \"Known Limitations\" sections to docstrings when appropriate",
        "",
        "---",
        "",
        "## 3. Completion Standards",
        "",
        "### A task isn't done until findings are documented",
        "",
        "\"Done\" has three components: implementation, testing, and documentation. All three are mandatory.",
        "",
        "**Definition of Done:**",
        "1. Feature implemented and tests pass",
        "2. Feature exercised with real usage (dog-fooding)",
        "3. Findings, limitations, and follow-up issues documented",
        "4. `TASK_LIST.md` updated with completion status and any new tasks",
        "",
        "### If testing reveals new issues, create follow-up tasks",
        "",
        "Testing expands our understanding. New knowledge creates new work - embrace it.",
        "",
        "**Requirements:**",
        "- Create follow-up tasks immediately, don't rely on memory",
        "- Link follow-up tasks to the parent task for context",
        "- Assess priority realistically (not everything is urgent)",
        "- Close the parent task only after follow-ups are tracked",
        "",
        "### Update summary tables when completing work",
        "",
        "Summary tables in `TASK_LIST.md` provide project health metrics. Keep them current.",
        "",
        "**Requirements:**",
        "- Mark tasks complete in both the detailed list AND summary tables",
        "- Update counts, statistics, and status overviews",
        "- Note completion date and any relevant metrics",
        "- Commit task list updates with the feature implementation",
        "",
        "### Leave the codebase better documented than you found it",
        "",
        "Every commit is an opportunity to improve clarity.",
        "",
        "**Requirements:**",
        "- If you struggled to understand code, improve its documentation",
        "- Add comments explaining the \"why\" behind non-obvious decisions",
        "- Update docstrings when behavior changes",
        "- Fix misleading comments immediately - they're worse than no comments",
        "",
        "---",
        "",
        "## 4. Scientific Rigor",
        "",
        "### Be skeptical of \"working\" results",
        "",
        "In science, reproducibility and understanding matter more than outcomes. Apply the same standard here.",
        "",
        "**Requirements:**",
        "- Question why a fix works, don't just celebrate that it does",
        "- Test the boundaries: when does it work? When does it fail?",
        "- Look for alternative explanations",
        "- Be especially skeptical of fixes that \"just work\" without clear causation",
        "",
        "### Verify claims with evidence",
        "",
        "Anecdotes are hypotheses. Measurements are evidence.",
        "",
        "**Requirements:**",
        "- Use quantitative metrics: execution time, memory usage, result quality",
        "- Provide reproduction steps for performance claims",
        "- Compare before/after with controlled tests",
        "- Document test methodology so others can verify",
        "",
        "### Document both successes AND limitations",
        "",
        "A complete scientific result includes what was learned, what worked, and what didn't.",
        "",
        "**Requirements:**",
        "- Note what approaches were tried and failed",
        "- Document performance characteristics (time/space complexity)",
        "- List known failure modes or edge cases",
        "- Be honest about scope: \"Solves X but not Y\"",
        "",
        "### Follow the evidence, not assumptions",
        "",
        "Our mental models are often wrong. The code and data don't lie.",
        "",
        "**Requirements:**",
        "- When behavior contradicts expectations, trust the behavior",
        "- Investigate discrepancies thoroughly before dismissing them",
        "- Update your understanding based on evidence",
        "- Document surprising findings - they're often the most valuable",
        "",
        "---",
        "",
        "## Enforcement",
        "",
        "This is not a bureaucratic exercise. These standards exist because:",
        "",
        "1. **We index our own codebase** - poor documentation directly impacts our tooling",
        "2. **We depend on our own library** - bugs and limitations affect our work",
        "3. **We are scientists** - rigor is not optional",
        "4. **We respect future developers** - including our future selves",
        "",
        "Violations aren't moral failures - they're opportunities to learn. When you notice a gap:",
        "",
        "1. Document it immediately",
        "2. Fix it if time permits",
        "3. Track it in `TASK_LIST.md` if not",
        "4. Improve processes to prevent recurrence",
        "",
        "---",
        "",
        "## Example: The Doc-Type Boosting Case Study",
        "",
        "**What happened:** Document-level search (`find_documents_for_query`) correctly applied doc-type boosting. Passage-level search (`find_passages_for_query`) did not, despite claiming to support the feature.",
        "",
        "**What we did wrong:**",
        "- Unit tests passed but didn't cover the integration path",
        "- Dog-fooding test existed but results weren't critically examined",
        "- The limitation wasn't documented in the docstring",
        "- No task was created when the gap was first noticed",
        "",
        "**What we should have done:**",
        "1. Add explicit test case for passage-level doc-type boosting",
        "2. Run dog-fooding test and examine actual score contributions",
        "3. Document in `find_passages_for_query` docstring: \"Note: Currently does not apply doc-type boosting (Task #XX)\"",
        "4. Create task in `TASK_LIST.md` immediately upon discovery",
        "5. Mark parent task as complete only after documenting this limitation",
        "",
        "**This is the standard.** Match it consistently, and the codebase will remain trustworthy.",
        "",
        "---",
        "",
        "*\"The first principle is that you must not fool yourself — and you are the easiest person to fool.\"* - Richard Feynman"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "docs/definition-of-done.md",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "# Definition of Done",
        "",
        "This document defines when a task is truly \"done\" versus merely \"code complete\". Following these criteria ensures that work is production-ready and all discoveries are properly documented.",
        "",
        "## Context",
        "",
        "During feature development, it's easy to focus solely on implementation and overlook critical steps like documentation, verification, and issue tracking. This document provides a checklist to prevent incomplete work from being marked as finished.",
        "",
        "**Example**: While implementing passage-level search features, we discovered a gap in passage-level boosting that could have been lost if not explicitly documented and added to TASK_LIST.md.",
        "",
        "---",
        "",
        "## Completion Criteria",
        "",
        "### 1. Code Complete (Necessary but Not Sufficient)",
        "",
        "- [ ] Implementation finished and functionally correct",
        "- [ ] Unit tests written and passing",
        "- [ ] No regressions in existing tests (full test suite passes)",
        "- [ ] Code follows project style guidelines",
        "- [ ] Type hints added to all public functions",
        "- [ ] No obvious performance issues introduced",
        "",
        "**Command to verify:**",
        "```bash",
        "python -m unittest discover -s tests -v",
        "```",
        "",
        "### 2. Documentation Complete",
        "",
        "- [ ] All public functions have Google-style docstrings",
        "  - Args section with types and descriptions",
        "  - Returns section with type and description",
        "  - Examples if the function is non-trivial",
        "- [ ] TASK_LIST.md updated with:",
        "  - Task marked as DONE",
        "  - Solution details added",
        "  - Implementation notes if applicable",
        "- [ ] New APIs documented in relevant files:",
        "  - Added to CLAUDE.md quick reference if user-facing",
        "  - Usage examples provided for complex features",
        "- [ ] PATTERNS.md updated if new patterns introduced",
        "",
        "**Files to check:**",
        "- Source code docstrings",
        "- `/home/user/Opus-code-test/TASK_LIST.md`",
        "- `/home/user/Opus-code-test/CLAUDE.md`",
        "- `/home/user/Opus-code-test/docs/PATTERNS.md`",
        "",
        "### 3. Verification Complete",
        "",
        "- [ ] Feature tested end-to-end (not just unit tests)",
        "  - Run showcase.py to verify integration",
        "  - Test with realistic data, not just toy examples",
        "- [ ] Dog-fooding performed when applicable:",
        "  - Use codebase search to find related code",
        "  - Test feature on the Cortical codebase itself",
        "  - Verify behavior matches expectations",
        "- [ ] Edge cases explored:",
        "  - Empty corpus",
        "  - Single document",
        "  - Large corpus (performance testing)",
        "  - Malformed input",
        "  - Boundary conditions",
        "- [ ] Limitations documented:",
        "  - Known issues noted in docstrings or TASK_LIST.md",
        "  - Performance characteristics documented",
        "  - Unsupported use cases called out",
        "",
        "**Commands to verify:**",
        "```bash",
        "python showcase.py",
        "python scripts/search_codebase.py \"your feature keywords\"",
        "```",
        "",
        "### 4. Issue Tracking Complete",
        "",
        "This is the step that is most often skipped but is critical for maintaining project knowledge.",
        "",
        "- [ ] All discovered issues added to TASK_LIST.md:",
        "  - New tasks created with clear descriptions",
        "  - Priority assigned (Critical/High/Medium/Low)",
        "  - Effort estimated (Small/Medium/Large)",
        "  - Dependencies noted",
        "- [ ] Summary tables updated:",
        "  - Task counts reflect new additions",
        "  - Status categories accurate",
        "  - No orphaned task numbers",
        "- [ ] Related tasks cross-referenced:",
        "  - \"See Task #X\" links added where relevant",
        "  - Dependencies noted in both directions",
        "- [ ] Future work captured:",
        "  - \"Nice to have\" features documented",
        "  - Performance optimization opportunities noted",
        "  - Potential extensions recorded",
        "",
        "**Example**: When implementing passage search, we found that passage-level boosting was missing. This became Task #66, properly categorized and linked to related search tasks.",
        "",
        "### 5. Truly Done",
        "",
        "All previous criteria met, plus:",
        "",
        "- [ ] Changes committed with descriptive message:",
        "  - Follows project commit message style",
        "  - References task numbers",
        "  - Explains the \"why\" not just the \"what\"",
        "- [ ] Commit includes all related files:",
        "  - Source code changes",
        "  - Test updates",
        "  - Documentation updates",
        "  - TASK_LIST.md changes",
        "- [ ] Changes pushed to remote branch",
        "- [ ] Ready for review/merge:",
        "  - No \"TODO\" comments left in code",
        "  - No commented-out debugging code",
        "  - No temporary files committed",
        "",
        "**Git commands:**",
        "```bash",
        "git status",
        "git diff",
        "git add <relevant files>",
        "git commit -m \"Implement Task #X: <description>\"",
        "git push origin <branch-name>",
        "```",
        "",
        "---",
        "",
        "## Quick Check",
        "",
        "Before marking a task as DONE, answer these questions:",
        "",
        "### Testing",
        "- [ ] Did I test this with real usage beyond unit tests?",
        "- [ ] Did I run the full test suite without failures?",
        "- [ ] Did I test edge cases (empty, single, large)?",
        "- [ ] Did I verify behavior in showcase.py or dog-fooding scripts?",
        "",
        "### Documentation",
        "- [ ] Did I document all findings, even unexpected ones?",
        "- [ ] Did I update TASK_LIST.md with solution details?",
        "- [ ] Do all new functions have complete docstrings?",
        "- [ ] Did I update user-facing documentation (CLAUDE.md)?",
        "",
        "### Issue Tracking",
        "- [ ] Did I create tasks for any issues found during implementation?",
        "- [ ] Did I create tasks for any limitations discovered?",
        "- [ ] Did I create tasks for related work that would improve this feature?",
        "- [ ] Are the summary tables in TASK_LIST.md current?",
        "",
        "### Completeness",
        "- [ ] Is the code committed with a descriptive message?",
        "- [ ] Are all related files included in the commit?",
        "- [ ] Is there any \"TODO\" or temporary code still present?",
        "- [ ] Would another developer understand this work from the documentation?",
        "",
        "**If any answer is \"no\", the task is not done.**",
        "",
        "---",
        "",
        "## Anti-Patterns to Avoid",
        "",
        "### The \"Quick Fix\" Trap",
        "**Symptom**: Implementing a feature and immediately marking it done without verification.",
        "",
        "**Problem**: Issues discovered later require context-switching and rework.",
        "",
        "**Solution**: Always run end-to-end tests and dog-fooding before marking done.",
        "",
        "### The \"It Works on My Machine\" Trap",
        "**Symptom**: Testing only the happy path with toy data.",
        "",
        "**Problem**: Edge cases fail in production or for other users.",
        "",
        "**Solution**: Test with realistic data, empty corpus, and boundary conditions.",
        "",
        "### The \"Lost Knowledge\" Trap",
        "**Symptom**: Discovering an issue during implementation but not documenting it.",
        "",
        "**Problem**: Issue gets forgotten and resurfaces later without context.",
        "",
        "**Solution**: Immediately add discovered issues to TASK_LIST.md, even if they're out of scope.",
        "",
        "### The \"Partial Commit\" Trap",
        "**Symptom**: Committing code changes but forgetting to commit documentation updates.",
        "",
        "**Problem**: Code and documentation fall out of sync.",
        "",
        "**Solution**: Use git status before committing to verify all related files are included.",
        "",
        "---",
        "",
        "## Template: Pre-Commit Checklist",
        "",
        "Copy this checklist into your task notes or PR description:",
        "",
        "```markdown",
        "## Definition of Done Checklist",
        "",
        "### Code Complete",
        "- [ ] Implementation finished",
        "- [ ] Unit tests passing",
        "- [ ] Full test suite passing",
        "- [ ] Type hints added",
        "",
        "### Documentation Complete",
        "- [ ] Docstrings added",
        "- [ ] TASK_LIST.md updated",
        "- [ ] User docs updated (if applicable)",
        "",
        "### Verification Complete",
        "- [ ] End-to-end testing done",
        "- [ ] showcase.py verified",
        "- [ ] Edge cases tested",
        "- [ ] Limitations documented",
        "",
        "### Issue Tracking Complete",
        "- [ ] New issues added to TASK_LIST.md",
        "- [ ] Summary tables updated",
        "- [ ] Dependencies noted",
        "",
        "### Truly Done",
        "- [ ] All files committed",
        "- [ ] Descriptive commit message",
        "- [ ] Ready for review",
        "```",
        "",
        "---",
        "",
        "## Process Flow",
        "",
        "```",
        "┌─────────────────┐",
        "│ Code Complete   │",
        "│ (Tests Pass)    │",
        "└────────┬────────┘",
        "         │",
        "         ▼",
        "┌─────────────────┐",
        "│ Documentation   │",
        "│ Complete        │",
        "└────────┬────────┘",
        "         │",
        "         ▼",
        "┌─────────────────┐",
        "│ Verification    │◄──── Found Issue? ────┐",
        "│ Complete        │                       │",
        "└────────┬────────┘                       │",
        "         │                                │",
        "         ▼                                │",
        "┌─────────────────┐                       │",
        "│ Issue Tracking  │───────────────────────┘",
        "│ Complete        │  Add to TASK_LIST.md",
        "└────────┬────────┘",
        "         │",
        "         ▼",
        "┌─────────────────┐",
        "│ Commit & Push   │",
        "│ (Truly Done)    │",
        "└─────────────────┘",
        "```",
        "",
        "---",
        "",
        "## Examples",
        "",
        "### Good: Task #59 (Add Metadata to Minicolumns)",
        "",
        "**What made it good:**",
        "- Code implemented with full type hints",
        "- Tests added and passing",
        "- Docstrings complete",
        "- TASK_LIST.md updated with solution details",
        "- Discovered prerequisite for Task #65 and documented it",
        "- Committed with clear message",
        "",
        "### Could Be Better: Initial Passage Search Implementation",
        "",
        "**What was missing:**",
        "- Almost forgot to document passage-level boosting gap",
        "- Didn't initially create Task #66 for the missing feature",
        "- Would have lost knowledge without explicit documentation",
        "",
        "**Lesson**: Always document issues found during implementation, even if they're out of scope for the current task.",
        "",
        "---",
        "",
        "## Summary",
        "",
        "**Code Complete ≠ Done**",
        "",
        "A task is only done when:",
        "1. Code works and tests pass",
        "2. Documentation is complete and accurate",
        "3. Verification confirms real-world usage works",
        "4. All discovered issues are tracked",
        "5. Changes are committed and pushed",
        "",
        "**When in doubt, ask:**",
        "- \"Would I be comfortable if someone else had to maintain this tomorrow?\"",
        "- \"Did I document everything I learned, including problems found?\"",
        "- \"Could someone else understand this work from the documentation alone?\"",
        "",
        "If the answer to any question is \"no\", keep working. Your future self (and your teammates) will thank you."
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "docs/dogfooding-checklist.md",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "# Dog-Fooding Checklist",
        "",
        "This checklist ensures we systematically test features using the Cortical Text Processor itself. The goal is to catch issues like the passage-level search doc-type boosting bug **before** they make it into production.",
        "",
        "## Context",
        "",
        "When implementing search features, it's easy to test individual components in isolation but miss integration issues. By actually using the system to index and search its own codebase, we catch problems that only appear in real-world usage.",
        "",
        "---",
        "",
        "## 1. Pre-Testing Setup",
        "",
        "- [ ] **Re-index the codebase after changes**",
        "  - Why: New code/docs won't appear in search results if the index is stale",
        "  - Command: `python scripts/index_codebase.py --incremental`",
        "",
        "- [ ] **Verify index completed successfully**",
        "  - Why: Partial failures can lead to inconsistent state",
        "  - Check: Look for \"✓ Indexing complete\" message, no exceptions",
        "",
        "- [ ] **Check document count matches expectations**",
        "  - Why: Missing or duplicate documents indicate indexing problems",
        "  - Command: `python scripts/search_codebase.py \"/stats\"` in interactive mode",
        "  - Verify: Count matches `find cortical tests -name \"*.py\" | wc -l` (or similar)",
        "",
        "---",
        "",
        "## 2. Search Quality Checks",
        "",
        "- [ ] **Test document-level search with known queries**",
        "  - Why: Baseline for comparing against passage-level search",
        "  - Example queries:",
        "    - `\"PageRank algorithm\"`",
        "    - `\"bigram separator\"`",
        "    - `\"compute TF-IDF\"`",
        "  - Verify: Relevant files appear in top 5 results",
        "",
        "- [ ] **Test passage-level search with same queries**",
        "  - Why: Passage-level should return focused context from same documents",
        "  - Command: Use `find_passages_for_query()` or `search_codebase.py` with passage mode",
        "  - Verify: Results point to correct files and line ranges",
        "",
        "- [ ] **Compare results - are they consistent?**",
        "  - Why: Document and passage results should be complementary, not contradictory",
        "  - Check: If `analysis.py` is #1 for doc search, it should appear in passage results too",
        "",
        "- [ ] **Test conceptual queries (\"what is X\")**",
        "  - Why: Should surface documentation and explanatory comments",
        "  - Example queries:",
        "    - `\"what is a minicolumn\"`",
        "    - `\"how does PageRank work\"`",
        "    - `\"concept clustering algorithm\"`",
        "  - Expected: `.md` files and docstrings rank highly",
        "",
        "- [ ] **Test implementation queries (\"where is X\")**",
        "  - Why: Should surface actual code implementations",
        "  - Example queries:",
        "    - `\"where is PageRank computed\"`",
        "    - `\"implementation of TF-IDF\"`",
        "    - `\"add document incremental\"`",
        "  - Expected: `.py` files with actual functions rank highly",
        "",
        "- [ ] **Verify doc-type boosting is working**",
        "  - Why: Catches the exact bug we found (passage search ignoring doc-type boosts)",
        "  - Test: For conceptual query, check if `.md` files are boosted",
        "  - Test: For implementation query, check if `.py` files are boosted",
        "  - Evidence: Compare scores with/without doc-type filter",
        "",
        "- [ ] **Check if documentation surfaces for conceptual queries**",
        "  - Why: Users asking \"what\" questions need docs, not raw code",
        "  - Query: `\"hierarchical layer structure\"`",
        "  - Expected: `CLAUDE.md` or relevant docs appear in top 3",
        "",
        "---",
        "",
        "## 3. New Feature Verification",
        "",
        "- [ ] **Search for terms from new code/docs**",
        "  - Why: Ensures new content is indexed and retrievable",
        "  - Action: Identify 2-3 unique terms from your new code",
        "  - Query: Search for those terms",
        "  - Verify: New file appears in results",
        "",
        "- [ ] **Verify new files appear in results**",
        "  - Why: New files might not be indexed if patterns are wrong",
        "  - Check: Search for filename or unique content",
        "  - Verify: File is in top results",
        "",
        "- [ ] **Test the specific feature end-to-end**",
        "  - Why: Unit tests may pass but integration fails",
        "  - Action: Use the exact workflow a user would follow",
        "  - Example: If you added intent parsing, run `search_by_intent()` on real corpus",
        "",
        "- [ ] **Try edge cases**",
        "  - Why: Edge cases reveal assumptions in the code",
        "  - Examples:",
        "    - Empty query",
        "    - Very long query (50+ words)",
        "    - Query with special characters",
        "    - Query matching zero documents",
        "    - Query matching all documents",
        "",
        "---",
        "",
        "## 4. Issue Discovery Protocol",
        "",
        "- [ ] **Document any unexpected behavior**",
        "  - Why: Memory is fallible; write it down immediately",
        "  - Format: Query → Expected → Actual → Why it matters",
        "",
        "- [ ] **Add new tasks to TASK_LIST.md immediately**",
        "  - Why: Issues discovered during testing are easy to forget",
        "  - Template:",
        "    ```markdown",
        "    ## Task #XX: Fix [brief description]",
        "    **Status**: Not Started",
        "    **Priority**: [High/Medium/Low]",
        "    **Created**: [date]",
        "",
        "    **Description**:",
        "    When testing [feature], discovered [issue].",
        "",
        "    Query: `[search query]`",
        "    Expected: [what should happen]",
        "    Actual: [what happened]",
        "",
        "    **Root Cause** (if known):",
        "    [explanation]",
        "",
        "    **Proposed Fix**:",
        "    [how to fix it]",
        "    ```",
        "",
        "- [ ] **Include evidence (query, results, expected vs actual)**",
        "  - Why: Makes debugging easier when you return to the task",
        "  - Save: Query strings, top 5 results, scores, file paths",
        "",
        "- [ ] **Update summary tables**",
        "  - Why: Keeps TASK_LIST.md organized and scannable",
        "  - Tables to update:",
        "    - Status summary (count by status)",
        "    - Priority breakdown",
        "    - Category summary",
        "",
        "---",
        "",
        "## 5. Final Verification",
        "",
        "- [ ] **All issues documented in TASK_LIST.md?**",
        "  - Why: Un-documented issues will be forgotten",
        "  - Check: Review your testing notes and ensure every issue has a task",
        "",
        "- [ ] **Summary tables updated?**",
        "  - Why: Tables provide quick overview of project health",
        "  - Verify: Counts match number of tasks in each section",
        "",
        "- [ ] **Changes committed and pushed?**",
        "  - Why: Sharing findings with team prevents duplicate work",
        "  - Check: `git status` shows clean working directory",
        "  - Verify: Latest commit includes test findings and new tasks",
        "",
        "---",
        "",
        "## Quick Example",
        "",
        "Here's what a complete dog-fooding session looks like:",
        "",
        "```bash",
        "# 1. Re-index",
        "python scripts/index_codebase.py --incremental",
        "",
        "# 2. Test known queries",
        "python scripts/search_codebase.py \"PageRank algorithm\" --verbose",
        "python scripts/search_codebase.py \"what is a minicolumn\" --verbose",
        "",
        "# 3. Test new feature",
        "python scripts/search_codebase.py \"my new function name\" --verbose",
        "",
        "# 4. Document issues",
        "# (Open TASK_LIST.md and add any problems found)",
        "",
        "# 5. Commit findings",
        "git add docs/ TASK_LIST.md",
        "git commit -m \"Add dog-fooding findings from feature X testing\"",
        "```",
        "",
        "---",
        "",
        "## Tips",
        "",
        "- **Test early, test often**: Don't wait until feature is \"done\" to dog-food",
        "- **Use interactive mode**: `python scripts/search_codebase.py --interactive` for rapid iteration",
        "- **Compare with grep**: If search misses obvious results, something is broken",
        "- **Think like a user**: What would someone actually search for?",
        "- **Document surprises**: Even if it's \"working as designed\", unexpected behavior may indicate UX issues",
        "",
        "---",
        "",
        "## Common Issues to Watch For",
        "",
        "| Symptom | Likely Cause |",
        "|---------|--------------|",
        "| New file not in results | Not re-indexed, or file pattern excluded |",
        "| Zero results for obvious query | Tokenization issue, or term not in corpus |",
        "| Wrong files ranked #1 | Scoring bug (TF-IDF, doc-type, etc.) |",
        "| Passage and doc results diverge | Passage search missing a boost/filter |",
        "| Docs don't surface for \"what is\" | Doc-type boosting not applied |",
        "| Code doesn't surface for \"where is\" | Same as above |",
        "",
        "---",
        "",
        "*Remember: The best way to ensure quality is to actually use what we build.*"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    }
  ],
  "hour_of_day": 0,
  "day_of_week": "Thursday",
  "seconds_since_last_commit": -391684,
  "is_merge": false,
  "is_initial": false,
  "parent_count": 1,
  "session_id": null,
  "related_chats": [],
  "ci_result": null,
  "reverted": false,
  "amended": false
}