{
  "hash": "1d9f52063dd5653e39366486b9622e1d291bf9e5",
  "message": "feat: Add schema validation for ML data integrity",
  "author": "Claude",
  "timestamp": "2025-12-15 05:04:29 +0000",
  "branch": "claude/multi-index-design-DvifZ",
  "files_changed": [
    "scripts/ml_data_collector.py"
  ],
  "insertions": 177,
  "deletions": 9,
  "hunks": [
    {
      "file": "scripts/ml_data_collector.py",
      "function": "import uuid",
      "start_line": 29,
      "lines_added": [
        "",
        "class SchemaValidationError(Exception):",
        "    \"\"\"Raised when data fails schema validation.\"\"\"",
        "    pass",
        "",
        "",
        "# Schema validation definitions",
        "COMMIT_SCHEMA = {",
        "    \"required\": [\"hash\", \"message\", \"author\", \"timestamp\", \"branch\", \"files_changed\",",
        "                 \"insertions\", \"deletions\", \"hunks\", \"hour_of_day\", \"day_of_week\"],",
        "    \"types\": {",
        "        \"hash\": str, \"message\": str, \"author\": str, \"timestamp\": str, \"branch\": str,",
        "        \"files_changed\": list, \"insertions\": int, \"deletions\": int, \"hunks\": list,",
        "        \"hour_of_day\": int, \"day_of_week\": str, \"is_merge\": bool, \"is_initial\": bool,",
        "        \"parent_count\": int, \"session_id\": (str, type(None)), \"related_chats\": list,",
        "    }",
        "}",
        "",
        "CHAT_SCHEMA = {",
        "    \"required\": [\"id\", \"timestamp\", \"session_id\", \"query\", \"response\",",
        "                 \"files_referenced\", \"files_modified\", \"tools_used\"],",
        "    \"types\": {",
        "        \"id\": str, \"timestamp\": str, \"session_id\": str, \"query\": str, \"response\": str,",
        "        \"files_referenced\": list, \"files_modified\": list, \"tools_used\": list,",
        "        \"query_tokens\": int, \"response_tokens\": int,",
        "    }",
        "}",
        "",
        "ACTION_SCHEMA = {",
        "    \"required\": [\"id\", \"timestamp\", \"session_id\", \"action_type\", \"target\"],",
        "    \"types\": {",
        "        \"id\": str, \"timestamp\": str, \"session_id\": str,",
        "        \"action_type\": str, \"target\": str, \"success\": bool,",
        "    }",
        "}",
        "",
        "",
        "def validate_schema(data: dict, schema: dict, data_type: str) -> List[str]:",
        "    \"\"\"Validate data against a schema. Returns list of errors (empty if valid).\"\"\"",
        "    errors = []",
        "    for field in schema[\"required\"]:",
        "        if field not in data:",
        "            errors.append(f\"{data_type}: missing required field '{field}'\")",
        "    for field, expected in schema[\"types\"].items():",
        "        if field in data and data[field] is not None:",
        "            if isinstance(expected, tuple):",
        "                if not isinstance(data[field], expected):",
        "                    errors.append(f\"{data_type}: field '{field}' has wrong type\")",
        "            elif not isinstance(data[field], expected):",
        "                errors.append(f\"{data_type}: field '{field}' has type {type(data[field]).__name__}, expected {expected.__name__}\")",
        "    return errors",
        "",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "from datetime import datetime",
        "from pathlib import Path",
        "from typing import Dict, List, Optional, Any",
        "from dataclasses import dataclass, asdict, field",
        "",
        "",
        "class GitCommandError(Exception):",
        "    \"\"\"Raised when a git command fails.\"\"\"",
        "    pass",
        ""
      ],
      "context_after": [
        "# ============================================================================",
        "# CONFIGURATION",
        "# ============================================================================",
        "",
        "ML_DATA_DIR = Path(\".git-ml\")",
        "COMMITS_DIR = ML_DATA_DIR / \"commits\"",
        "SESSIONS_DIR = ML_DATA_DIR / \"sessions\"",
        "CHATS_DIR = ML_DATA_DIR / \"chats\"",
        "ACTIONS_DIR = ML_DATA_DIR / \"actions\"",
        "",
        "# Training milestones",
        "MILESTONES = {",
        "    \"file_prediction\": {\"commits\": 500, \"sessions\": 100, \"chats\": 200},",
        "    \"commit_messages\": {\"commits\": 2000, \"sessions\": 500, \"chats\": 1000},",
        "    \"code_suggestions\": {\"commits\": 5000, \"sessions\": 2000, \"chats\": 5000},",
        "}",
        "",
        "# ============================================================================",
        "# DATA SCHEMAS",
        "# ============================================================================",
        "",
        "@dataclass",
        "class DiffHunk:",
        "    \"\"\"A single diff hunk from a commit.\"\"\"",
        "    file: str",
        "    function: Optional[str]  # Function/class containing the change",
        "    change_type: str  # add, modify, delete, rename"
      ],
      "change_type": "add"
    },
    {
      "file": "scripts/ml_data_collector.py",
      "function": "def atomic_write_json(filepath: Path, data: dict):",
      "start_line": 374,
      "lines_added": [
        "def save_commit_data(context: CommitContext, validate: bool = True):",
        "    \"\"\"Save commit context to disk atomically with validation.\"\"\"",
        "    data = asdict(context)",
        "",
        "    # Validate before writing",
        "    if validate:",
        "        errors = validate_schema(data, COMMIT_SCHEMA, \"commit\")",
        "        if errors:",
        "            raise SchemaValidationError(f\"Commit validation failed: {errors}\")",
        "",
        "    atomic_write_json(filepath, data)",
        "def save_chat_entry(entry: ChatEntry, validate: bool = True):",
        "    \"\"\"Save a chat entry to disk atomically with validation.\"\"\"",
        "    data = asdict(entry)",
        "",
        "    # Validate before writing",
        "    if validate:",
        "        errors = validate_schema(data, CHAT_SCHEMA, \"chat\")",
        "        if errors:",
        "            raise SchemaValidationError(f\"Chat validation failed: {errors}\")",
        "",
        "    atomic_write_json(filepath, data)"
      ],
      "lines_removed": [
        "def save_commit_data(context: CommitContext):",
        "    \"\"\"Save commit context to disk atomically.\"\"\"",
        "    atomic_write_json(filepath, asdict(context))",
        "def save_chat_entry(entry: ChatEntry):",
        "    \"\"\"Save a chat entry to disk atomically.\"\"\"",
        "    atomic_write_json(filepath, asdict(entry))"
      ],
      "context_before": [
        "            json.dump(data, f, indent=2, ensure_ascii=False)",
        "        # Atomic rename (on POSIX systems)",
        "        os.replace(temp_path, filepath)",
        "    except Exception:",
        "        # Clean up temp file on failure",
        "        if os.path.exists(temp_path):",
        "            os.unlink(temp_path)",
        "        raise",
        "",
        ""
      ],
      "context_after": [
        "    ensure_dirs()",
        "",
        "    # Use full hash + UUID suffix to prevent collisions",
        "    unique_id = uuid.uuid4().hex[:8]",
        "    filename = f\"{context.hash[:8]}_{context.timestamp[:10]}_{unique_id}.json\"",
        "    filepath = COMMITS_DIR / filename",
        "",
        "    print(f\"Saved commit data to {filepath}\")",
        "",
        "",
        "def generate_chat_id() -> str:",
        "    \"\"\"Generate unique chat entry ID.\"\"\"",
        "    timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")",
        "    suffix = hashlib.sha256(str(datetime.now().timestamp()).encode()).hexdigest()[:6]",
        "    return f\"chat-{timestamp}-{suffix}\"",
        "",
        "",
        "def generate_session_id() -> str:",
        "    \"\"\"Generate unique session ID.\"\"\"",
        "    return hashlib.sha256(str(datetime.now().timestamp()).encode()).hexdigest()[:8]",
        "",
        "",
        "    ensure_dirs()",
        "",
        "    # Organize by date",
        "    date_dir = CHATS_DIR / entry.timestamp[:10]",
        "    date_dir.mkdir(exist_ok=True)",
        "",
        "    filename = f\"{entry.id}.json\"",
        "    filepath = date_dir / filename",
        "",
        "    print(f\"Saved chat entry to {filepath}\")",
        "",
        "",
        "def log_chat(",
        "    query: str,",
        "    response: str,",
        "    session_id: Optional[str] = None,",
        "    files_referenced: Optional[List[str]] = None,",
        "    files_modified: Optional[List[str]] = None,",
        "    tools_used: Optional[List[str]] = None,"
      ],
      "change_type": "modify"
    },
    {
      "file": "scripts/ml_data_collector.py",
      "function": "def log_chat(",
      "start_line": 443,
      "lines_added": [
        "def save_action(entry: ActionEntry, validate: bool = True):",
        "    \"\"\"Save an action entry to disk atomically with validation.\"\"\"",
        "    data = asdict(entry)",
        "",
        "    # Validate before writing",
        "    if validate:",
        "        errors = validate_schema(data, ACTION_SCHEMA, \"action\")",
        "        if errors:",
        "            raise SchemaValidationError(f\"Action validation failed: {errors}\")",
        "",
        "    atomic_write_json(filepath, data)"
      ],
      "lines_removed": [
        "def save_action(entry: ActionEntry):",
        "    \"\"\"Save an action entry to disk atomically.\"\"\"",
        "    atomic_write_json(filepath, asdict(entry))"
      ],
      "context_before": [
        "        tools_used=tools_used or [],",
        "        user_feedback=user_feedback,",
        "        query_tokens=len(query.split()),  # Rough estimate",
        "        response_tokens=len(response.split()),",
        "    )",
        "",
        "    save_chat_entry(entry)",
        "    return entry",
        "",
        ""
      ],
      "context_after": [
        "    ensure_dirs()",
        "",
        "    date_dir = ACTIONS_DIR / entry.timestamp[:10]",
        "    date_dir.mkdir(exist_ok=True)",
        "",
        "    filename = f\"{entry.id}.json\"",
        "    filepath = date_dir / filename",
        "",
        "",
        "",
        "def log_action(",
        "    action_type: str,",
        "    target: str,",
        "    session_id: Optional[str] = None,",
        "    context: Optional[Dict] = None,",
        "    success: bool = True,",
        "    result_summary: Optional[str] = None,",
        ") -> ActionEntry:"
      ],
      "change_type": "modify"
    },
    {
      "file": "scripts/ml_data_collector.py",
      "function": "def main():",
      "start_line": 809,
      "lines_added": [
        "    elif command == \"validate\":",
        "        # Validate existing data against schemas",
        "        import argparse",
        "        parser = argparse.ArgumentParser()",
        "        parser.add_argument(\"--fix\", action=\"store_true\",",
        "                            help=\"Attempt to fix invalid entries\")",
        "        parser.add_argument(\"--verbose\", \"-v\", action=\"store_true\",",
        "                            help=\"Show all validation errors\")",
        "        args = parser.parse_args(sys.argv[2:])",
        "",
        "        print(\"\\n\" + \"=\" * 60)",
        "        print(\"VALIDATING ML DATA AGAINST SCHEMAS\")",
        "        print(\"=\" * 60)",
        "",
        "        all_errors = []",
        "",
        "        # Validate commits",
        "        if COMMITS_DIR.exists():",
        "            print(f\"\\nüìÅ Validating commits...\")",
        "            commit_files = list(COMMITS_DIR.glob(\"*.json\"))",
        "            commit_errors = 0",
        "            for f in commit_files:",
        "                try:",
        "                    with open(f, \"r\", encoding=\"utf-8\") as fp:",
        "                        data = json.load(fp)",
        "                    errors = validate_schema(data, COMMIT_SCHEMA, f\"commit:{f.name}\")",
        "                    if errors:",
        "                        commit_errors += 1",
        "                        all_errors.extend(errors)",
        "                        if args.verbose:",
        "                            for err in errors:",
        "                                print(f\"   ‚ùå {err}\")",
        "                except json.JSONDecodeError as e:",
        "                    commit_errors += 1",
        "                    all_errors.append(f\"commit:{f.name}: invalid JSON: {e}\")",
        "                    if args.verbose:",
        "                        print(f\"   ‚ùå {f.name}: invalid JSON\")",
        "            print(f\"   ‚úì {len(commit_files) - commit_errors}/{len(commit_files)} valid\")",
        "",
        "        # Validate chats",
        "        if CHATS_DIR.exists():",
        "            print(f\"\\nüìÅ Validating chats...\")",
        "            chat_files = list(CHATS_DIR.glob(\"**/*.json\"))",
        "            chat_errors = 0",
        "            for f in chat_files:",
        "                try:",
        "                    with open(f, \"r\", encoding=\"utf-8\") as fp:",
        "                        data = json.load(fp)",
        "                    errors = validate_schema(data, CHAT_SCHEMA, f\"chat:{f.name}\")",
        "                    if errors:",
        "                        chat_errors += 1",
        "                        all_errors.extend(errors)",
        "                        if args.verbose:",
        "                            for err in errors:",
        "                                print(f\"   ‚ùå {err}\")",
        "                except json.JSONDecodeError as e:",
        "                    chat_errors += 1",
        "                    all_errors.append(f\"chat:{f.name}: invalid JSON: {e}\")",
        "            print(f\"   ‚úì {len(chat_files) - chat_errors}/{len(chat_files)} valid\")",
        "",
        "        # Validate actions",
        "        if ACTIONS_DIR.exists():",
        "            print(f\"\\nüìÅ Validating actions...\")",
        "            action_files = list(ACTIONS_DIR.glob(\"**/*.json\"))",
        "            action_errors = 0",
        "            for f in action_files:",
        "                try:",
        "                    with open(f, \"r\", encoding=\"utf-8\") as fp:",
        "                        data = json.load(fp)",
        "                    errors = validate_schema(data, ACTION_SCHEMA, f\"action:{f.name}\")",
        "                    if errors:",
        "                        action_errors += 1",
        "                        all_errors.extend(errors)",
        "                        if args.verbose:",
        "                            for err in errors:",
        "                                print(f\"   ‚ùå {err}\")",
        "                except json.JSONDecodeError as e:",
        "                    action_errors += 1",
        "                    all_errors.append(f\"action:{f.name}: invalid JSON\")",
        "            print(f\"   ‚úì {len(action_files) - action_errors}/{len(action_files)} valid\")",
        "",
        "        # Summary",
        "        print(\"\\n\" + \"-\" * 60)",
        "        if all_errors:",
        "            print(f\"‚ö†Ô∏è  Found {len(all_errors)} validation errors\")",
        "            if not args.verbose:",
        "                print(\"   Run with --verbose to see details\")",
        "        else:",
        "            print(\"‚úÖ All data validated successfully!\")",
        "        print(\"=\" * 60 + \"\\n\")",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "",
        "    elif command == \"stats\":",
        "        print_stats()",
        "",
        "    elif command == \"estimate\":",
        "        estimate_project_size()",
        "",
        "    elif command == \"install-hooks\":",
        "        install_hooks()",
        ""
      ],
      "context_after": [
        "    else:",
        "        print(f\"Unknown command: {command}\")",
        "        print(__doc__)",
        "",
        "",
        "if __name__ == \"__main__\":",
        "    main()"
      ],
      "change_type": "add"
    }
  ],
  "hour_of_day": 5,
  "day_of_week": "Monday",
  "seconds_since_last_commit": -31219,
  "is_merge": false,
  "is_initial": false,
  "parent_count": 1,
  "session_id": null,
  "related_chats": [],
  "ci_result": null,
  "reverted": false,
  "amended": false
}