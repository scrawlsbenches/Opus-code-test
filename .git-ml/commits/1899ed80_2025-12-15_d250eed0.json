{
  "hash": "1899ed80d8b6ac23e650400531de7a83c2a3386d",
  "message": "test: Add unit tests for ML collector export, feedback, quality commands",
  "author": "Claude",
  "timestamp": "2025-12-15 11:28:43 +0000",
  "branch": "claude/multi-index-design-DvifZ",
  "files_changed": [
    "tests/unit/test_ml_export.py",
    "tests/unit/test_ml_feedback.py",
    "tests/unit/test_ml_quality.py"
  ],
  "insertions": 1842,
  "deletions": 0,
  "hunks": [
    {
      "file": "tests/unit/test_ml_export.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "#!/usr/bin/env python3",
        "\"\"\"",
        "Unit tests for ML data collector export functionality.",
        "",
        "Tests the export_data() function and related helpers for exporting",
        "collected ML data in training-ready formats (JSONL, CSV, HuggingFace).",
        "\"\"\"",
        "",
        "import csv",
        "import json",
        "import os",
        "import sys",
        "import tempfile",
        "import unittest",
        "from datetime import datetime",
        "from pathlib import Path",
        "",
        "# Add scripts to path",
        "sys.path.insert(0, str(Path(__file__).parent.parent.parent / \"scripts\"))",
        "",
        "import ml_data_collector as ml",
        "",
        "",
        "class TestSummarizeDiff(unittest.TestCase):",
        "    \"\"\"Tests for _summarize_diff helper function.\"\"\"",
        "",
        "    def test_empty_hunks(self):",
        "        \"\"\"Should return empty string for empty hunks list.\"\"\"",
        "        result = ml._summarize_diff([])",
        "        self.assertEqual(result, \"\")",
        "",
        "    def test_single_file_add_hunk(self):",
        "        \"\"\"Should summarize single add hunk.\"\"\"",
        "        hunks = [",
        "            {\"file\": \"test.py\", \"change_type\": \"add\"}",
        "        ]",
        "        result = ml._summarize_diff(hunks)",
        "        self.assertEqual(result, \"test.py: +1\")",
        "",
        "    def test_single_file_delete_hunk(self):",
        "        \"\"\"Should summarize single delete hunk.\"\"\"",
        "        hunks = [",
        "            {\"file\": \"test.py\", \"change_type\": \"delete\"}",
        "        ]",
        "        result = ml._summarize_diff(hunks)",
        "        self.assertEqual(result, \"test.py: -1\")",
        "",
        "    def test_single_file_modify_hunk(self):",
        "        \"\"\"Should summarize single modify hunk.\"\"\"",
        "        hunks = [",
        "            {\"file\": \"test.py\", \"change_type\": \"modify\"}",
        "        ]",
        "        result = ml._summarize_diff(hunks)",
        "        self.assertEqual(result, \"test.py: ~1\")",
        "",
        "    def test_single_file_multiple_changes(self):",
        "        \"\"\"Should summarize multiple changes to same file.\"\"\"",
        "        hunks = [",
        "            {\"file\": \"test.py\", \"change_type\": \"add\"},",
        "            {\"file\": \"test.py\", \"change_type\": \"modify\"},",
        "            {\"file\": \"test.py\", \"change_type\": \"modify\"},",
        "        ]",
        "        result = ml._summarize_diff(hunks)",
        "        self.assertEqual(result, \"test.py: +1 ~2\")",
        "",
        "    def test_multiple_files(self):",
        "        \"\"\"Should summarize changes across multiple files.\"\"\"",
        "        hunks = [",
        "            {\"file\": \"file1.py\", \"change_type\": \"add\"},",
        "            {\"file\": \"file2.py\", \"change_type\": \"delete\"},",
        "            {\"file\": \"file3.py\", \"change_type\": \"modify\"},",
        "        ]",
        "        result = ml._summarize_diff(hunks)",
        "        self.assertIn(\"file1.py: +1\", result)",
        "        self.assertIn(\"file2.py: -1\", result)",
        "        self.assertIn(\"file3.py: ~1\", result)",
        "        # Files separated by semicolons",
        "        self.assertEqual(result.count(\";\"), 2)",
        "",
        "    def test_missing_file_field(self):",
        "        \"\"\"Should handle hunks missing file field.\"\"\"",
        "        hunks = [",
        "            {\"change_type\": \"add\"},",
        "        ]",
        "        result = ml._summarize_diff(hunks)",
        "        self.assertIn(\"unknown\", result)",
        "",
        "    def test_missing_change_type_field(self):",
        "        \"\"\"Should default to modify when change_type missing.\"\"\"",
        "        hunks = [",
        "            {\"file\": \"test.py\"},",
        "        ]",
        "        result = ml._summarize_diff(hunks)",
        "        self.assertEqual(result, \"test.py: ~1\")",
        "",
        "    def test_limit_to_ten_files(self):",
        "        \"\"\"Should limit summary to first 10 files.\"\"\"",
        "        hunks = [",
        "            {\"file\": f\"file{i}.py\", \"change_type\": \"modify\"}",
        "            for i in range(20)",
        "        ]",
        "        result = ml._summarize_diff(hunks)",
        "        # Should only include first 10 files (9 semicolons)",
        "        self.assertEqual(result.count(\";\"), 9)",
        "",
        "",
        "class TestExportData(unittest.TestCase):",
        "    \"\"\"Tests for export_data() function.\"\"\"",
        "",
        "    def setUp(self):",
        "        \"\"\"Set up test environment with temporary directories.\"\"\"",
        "        self.test_dir = tempfile.TemporaryDirectory()",
        "        self.test_path = Path(self.test_dir.name)",
        "",
        "        # Temporarily override ML data directories",
        "        self.original_ml_data_dir = ml.ML_DATA_DIR",
        "        self.original_commits_dir = ml.COMMITS_DIR",
        "        self.original_chats_dir = ml.CHATS_DIR",
        "",
        "        ml.ML_DATA_DIR = self.test_path / \".git-ml\"",
        "        ml.COMMITS_DIR = ml.ML_DATA_DIR / \"commits\"",
        "        ml.CHATS_DIR = ml.ML_DATA_DIR / \"chats\"",
        "",
        "        # Create test directories",
        "        ml.ensure_dirs()",
        "",
        "    def tearDown(self):",
        "        \"\"\"Clean up test environment.\"\"\"",
        "        # Restore original directories",
        "        ml.ML_DATA_DIR = self.original_ml_data_dir",
        "        ml.COMMITS_DIR = self.original_commits_dir",
        "        ml.CHATS_DIR = self.original_chats_dir",
        "",
        "        # Clean up temp directory",
        "        self.test_dir.cleanup()",
        "",
        "    def test_export_empty_data_jsonl(self):",
        "        \"\"\"Should export empty JSONL file when no data exists.\"\"\"",
        "        output_path = self.test_path / \"export.jsonl\"",
        "        stats = ml.export_data(\"jsonl\", output_path)",
        "",
        "        # Check stats",
        "        self.assertEqual(stats[\"format\"], \"jsonl\")",
        "        self.assertEqual(stats[\"records\"], 0)",
        "        self.assertEqual(stats[\"commits\"], 0)",
        "        self.assertEqual(stats[\"chats\"], 0)",
        "        self.assertEqual(stats[\"output_path\"], str(output_path))",
        "",
        "        # Check file is empty",
        "        self.assertTrue(output_path.exists())",
        "        content = output_path.read_text()",
        "        self.assertEqual(content, \"\")",
        "",
        "    def test_export_empty_data_csv(self):",
        "        \"\"\"Should export CSV with headers only when no data exists.\"\"\"",
        "        output_path = self.test_path / \"export.csv\"",
        "        stats = ml.export_data(\"csv\", output_path)",
        "",
        "        # Check stats",
        "        self.assertEqual(stats[\"format\"], \"csv\")",
        "        self.assertEqual(stats[\"records\"], 0)",
        "",
        "        # Check file has headers only",
        "        self.assertTrue(output_path.exists())",
        "        with open(output_path, 'r', encoding='utf-8') as f:",
        "            reader = csv.DictReader(f)",
        "            headers = reader.fieldnames",
        "            self.assertIn('type', headers)",
        "            self.assertIn('timestamp', headers)",
        "            self.assertIn('input', headers)",
        "            self.assertIn('output', headers)",
        "            # Should have no data rows",
        "            rows = list(reader)",
        "            self.assertEqual(len(rows), 0)",
        "",
        "    def test_export_empty_data_huggingface(self):",
        "        \"\"\"Should export empty HuggingFace format when no data exists.\"\"\"",
        "        output_path = self.test_path / \"export.json\"",
        "        stats = ml.export_data(\"huggingface\", output_path)",
        "",
        "        # Check stats",
        "        self.assertEqual(stats[\"format\"], \"huggingface\")",
        "        self.assertEqual(stats[\"records\"], 0)",
        "",
        "        # Check file has correct structure",
        "        self.assertTrue(output_path.exists())",
        "        with open(output_path, 'r', encoding='utf-8') as f:",
        "            data = json.load(f)",
        "",
        "        # Should be dict of lists",
        "        self.assertIsInstance(data, dict)",
        "        self.assertIn('type', data)",
        "        self.assertIn('timestamp', data)",
        "        self.assertIn('input', data)",
        "        self.assertIn('output', data)",
        "        self.assertIn('session_id', data)",
        "        self.assertIn('files', data)",
        "        self.assertIn('tools_used', data)",
        "",
        "        # All lists should be empty",
        "        for key in data:",
        "            self.assertIsInstance(data[key], list)",
        "            self.assertEqual(len(data[key]), 0)",
        "",
        "    def test_export_with_commit_data_jsonl(self):",
        "        \"\"\"Should export commit data in JSONL format.\"\"\"",
        "        # Create mock commit",
        "        commit_data = {",
        "            \"hash\": \"abc123\",",
        "            \"message\": \"feat: Add feature\",",
        "            \"timestamp\": \"2025-12-15T10:00:00\",",
        "            \"files_changed\": [\"file1.py\", \"file2.py\"],",
        "            \"insertions\": 50,",
        "            \"deletions\": 10,",
        "            \"branch\": \"main\",",
        "            \"session_id\": \"sess1\",",
        "            \"hunks\": [",
        "                {\"file\": \"file1.py\", \"change_type\": \"add\"},",
        "                {\"file\": \"file2.py\", \"change_type\": \"modify\"},",
        "            ]",
        "        }",
        "",
        "        commit_file = ml.COMMITS_DIR / \"abc123_test.json\"",
        "        with open(commit_file, 'w', encoding='utf-8') as f:",
        "            json.dump(commit_data, f)",
        "",
        "        # Export",
        "        output_path = self.test_path / \"export.jsonl\"",
        "        stats = ml.export_data(\"jsonl\", output_path)",
        "",
        "        # Check stats",
        "        self.assertEqual(stats[\"records\"], 1)",
        "        self.assertEqual(stats[\"commits\"], 1)",
        "        self.assertEqual(stats[\"chats\"], 0)",
        "",
        "        # Check JSONL content",
        "        with open(output_path, 'r', encoding='utf-8') as f:",
        "            lines = f.readlines()",
        "",
        "        self.assertEqual(len(lines), 1)",
        "        record = json.loads(lines[0])",
        "",
        "        # Check record structure",
        "        self.assertEqual(record[\"type\"], \"commit\")",
        "        self.assertEqual(record[\"timestamp\"], \"2025-12-15T10:00:00\")",
        "        self.assertEqual(record[\"input\"], \"feat: Add feature\")",
        "        self.assertIn(\"file1.py\", record[\"output\"])  # Diff summary",
        "        self.assertEqual(record[\"context\"][\"session_id\"], \"sess1\")",
        "        self.assertEqual(record[\"context\"][\"files\"], [\"file1.py\", \"file2.py\"])",
        "        self.assertEqual(record[\"context\"][\"insertions\"], 50)",
        "        self.assertEqual(record[\"context\"][\"deletions\"], 10)",
        "",
        "    def test_export_with_chat_data_jsonl(self):",
        "        \"\"\"Should export chat data in JSONL format.\"\"\"",
        "        # Create mock chat",
        "        chat_data = {",
        "            \"id\": \"chat-001\",",
        "            \"timestamp\": \"2025-12-15T11:00:00\",",
        "            \"session_id\": \"sess1\",",
        "            \"query\": \"How do I fix the bug?\",",
        "            \"response\": \"You need to update line 42\",",
        "            \"files_referenced\": [\"bug.py\"],",
        "            \"files_modified\": [\"bug.py\", \"test.py\"],",
        "            \"tools_used\": [\"Read\", \"Edit\"],",
        "        }",
        "",
        "        chat_file = ml.CHATS_DIR / \"2025-12-15\" / \"chat-001.json\"",
        "        chat_file.parent.mkdir(parents=True, exist_ok=True)",
        "        with open(chat_file, 'w', encoding='utf-8') as f:",
        "            json.dump(chat_data, f)",
        "",
        "        # Export",
        "        output_path = self.test_path / \"export.jsonl\"",
        "        stats = ml.export_data(\"jsonl\", output_path)",
        "",
        "        # Check stats",
        "        self.assertEqual(stats[\"records\"], 1)",
        "        self.assertEqual(stats[\"commits\"], 0)",
        "        self.assertEqual(stats[\"chats\"], 1)",
        "",
        "        # Check JSONL content",
        "        with open(output_path, 'r', encoding='utf-8') as f:",
        "            lines = f.readlines()",
        "",
        "        self.assertEqual(len(lines), 1)",
        "        record = json.loads(lines[0])",
        "",
        "        # Check record structure",
        "        self.assertEqual(record[\"type\"], \"chat\")",
        "        self.assertEqual(record[\"timestamp\"], \"2025-12-15T11:00:00\")",
        "        self.assertEqual(record[\"input\"], \"How do I fix the bug?\")",
        "        self.assertEqual(record[\"output\"], \"You need to update line 42\")",
        "        self.assertEqual(record[\"context\"][\"session_id\"], \"sess1\")",
        "        self.assertIn(\"bug.py\", record[\"context\"][\"files\"])",
        "        self.assertIn(\"test.py\", record[\"context\"][\"files\"])",
        "        self.assertEqual(record[\"context\"][\"tools_used\"], [\"Read\", \"Edit\"])",
        "",
        "    def test_export_mixed_data_csv(self):",
        "        \"\"\"Should export both commits and chats in CSV format.\"\"\"",
        "        # Create commit",
        "        commit_data = {",
        "            \"hash\": \"abc123\",",
        "            \"message\": \"fix: Bug fix\",",
        "            \"timestamp\": \"2025-12-15T09:00:00\",",
        "            \"files_changed\": [\"fix.py\"],",
        "            \"insertions\": 5,",
        "            \"deletions\": 2,",
        "            \"branch\": \"main\",",
        "            \"session_id\": \"sess1\",",
        "            \"hunks\": [{\"file\": \"fix.py\", \"change_type\": \"modify\"}]",
        "        }",
        "        commit_file = ml.COMMITS_DIR / \"abc123_test.json\"",
        "        with open(commit_file, 'w', encoding='utf-8') as f:",
        "            json.dump(commit_data, f)",
        "",
        "        # Create chat",
        "        chat_data = {",
        "            \"timestamp\": \"2025-12-15T10:00:00\",",
        "            \"session_id\": \"sess1\",",
        "            \"query\": \"Test query\",",
        "            \"response\": \"Test response\",",
        "            \"files_referenced\": [\"test.py\"],",
        "            \"files_modified\": [],",
        "            \"tools_used\": [\"Read\"],",
        "        }",
        "        chat_file = ml.CHATS_DIR / \"chat-001.json\"",
        "        with open(chat_file, 'w', encoding='utf-8') as f:",
        "            json.dump(chat_data, f)",
        "",
        "        # Export",
        "        output_path = self.test_path / \"export.csv\"",
        "        stats = ml.export_data(\"csv\", output_path)",
        "",
        "        # Check stats",
        "        self.assertEqual(stats[\"records\"], 2)",
        "        self.assertEqual(stats[\"commits\"], 1)",
        "        self.assertEqual(stats[\"chats\"], 1)",
        "",
        "        # Check CSV content",
        "        with open(output_path, 'r', encoding='utf-8') as f:",
        "            reader = csv.DictReader(f)",
        "            rows = list(reader)",
        "",
        "        self.assertEqual(len(rows), 2)",
        "",
        "        # First row should be commit (sorted by timestamp)",
        "        self.assertEqual(rows[0][\"type\"], \"commit\")",
        "        self.assertEqual(rows[0][\"timestamp\"], \"2025-12-15T09:00:00\")",
        "        self.assertEqual(rows[0][\"input\"], \"fix: Bug fix\")",
        "        self.assertIn(\"fix.py\", rows[0][\"output\"])",
        "",
        "        # Second row should be chat",
        "        self.assertEqual(rows[1][\"type\"], \"chat\")",
        "        self.assertEqual(rows[1][\"timestamp\"], \"2025-12-15T10:00:00\")",
        "        self.assertEqual(rows[1][\"input\"], \"Test query\")",
        "        self.assertEqual(rows[1][\"output\"], \"Test response\")",
        "",
        "    def test_export_huggingface_format(self):",
        "        \"\"\"Should export in HuggingFace Dataset dict-of-lists format.\"\"\"",
        "        # Create test data",
        "        chat_data = {",
        "            \"timestamp\": \"2025-12-15T10:00:00\",",
        "            \"session_id\": \"sess1\",",
        "            \"query\": \"Query text\",",
        "            \"response\": \"Response text\",",
        "            \"files_referenced\": [\"file1.py\"],",
        "            \"files_modified\": [\"file2.py\"],",
        "            \"tools_used\": [\"Read\", \"Edit\"],",
        "        }",
        "        chat_file = ml.CHATS_DIR / \"chat-001.json\"",
        "        with open(chat_file, 'w', encoding='utf-8') as f:",
        "            json.dump(chat_data, f)",
        "",
        "        # Export",
        "        output_path = self.test_path / \"export.json\"",
        "        stats = ml.export_data(\"huggingface\", output_path)",
        "",
        "        # Check stats",
        "        self.assertEqual(stats[\"records\"], 1)",
        "",
        "        # Check HuggingFace format",
        "        with open(output_path, 'r', encoding='utf-8') as f:",
        "            data = json.load(f)",
        "",
        "        # Should be dict of lists",
        "        self.assertIsInstance(data, dict)",
        "",
        "        # Check all required fields",
        "        required_fields = ['type', 'timestamp', 'input', 'output',",
        "                          'session_id', 'files', 'tools_used']",
        "        for field in required_fields:",
        "            self.assertIn(field, data)",
        "            self.assertIsInstance(data[field], list)",
        "            self.assertEqual(len(data[field]), 1)",
        "",
        "        # Check values",
        "        self.assertEqual(data['type'][0], 'chat')",
        "        self.assertEqual(data['timestamp'][0], '2025-12-15T10:00:00')",
        "        self.assertEqual(data['input'][0], 'Query text')",
        "        self.assertEqual(data['output'][0], 'Response text')",
        "        self.assertEqual(data['session_id'][0], 'sess1')",
        "        self.assertIsInstance(data['files'][0], list)",
        "        self.assertIn('file1.py', data['files'][0])",
        "        self.assertIn('file2.py', data['files'][0])",
        "        self.assertEqual(data['tools_used'][0], ['Read', 'Edit'])",
        "",
        "    def test_export_huggingface_equal_lengths(self):",
        "        \"\"\"HuggingFace format should have equal-length lists.\"\"\"",
        "        # Create multiple records",
        "        for i in range(3):",
        "            chat_data = {",
        "                \"timestamp\": f\"2025-12-15T1{i}:00:00\",",
        "                \"session_id\": \"sess1\",",
        "                \"query\": f\"Query {i}\",",
        "                \"response\": f\"Response {i}\",",
        "                \"files_referenced\": [],",
        "                \"files_modified\": [],",
        "                \"tools_used\": [],",
        "            }",
        "            chat_file = ml.CHATS_DIR / f\"chat-00{i}.json\"",
        "            with open(chat_file, 'w', encoding='utf-8') as f:",
        "                json.dump(chat_data, f)",
        "",
        "        # Export",
        "        output_path = self.test_path / \"export.json\"",
        "        ml.export_data(\"huggingface\", output_path)",
        "",
        "        # Check all lists have same length",
        "        with open(output_path, 'r', encoding='utf-8') as f:",
        "            data = json.load(f)",
        "",
        "        lengths = {key: len(value) for key, value in data.items()}",
        "        unique_lengths = set(lengths.values())",
        "        self.assertEqual(len(unique_lengths), 1)  # All same length",
        "        self.assertEqual(list(unique_lengths)[0], 3)",
        "",
        "    def test_export_invalid_format(self):",
        "        \"\"\"Should raise ValueError for invalid format.\"\"\"",
        "        output_path = self.test_path / \"export.txt\"",
        "        with self.assertRaises(ValueError) as context:",
        "            ml.export_data(\"invalid_format\", output_path)",
        "",
        "        self.assertIn(\"Unknown format\", str(context.exception))",
        "",
        "    def test_export_creates_parent_directories(self):",
        "        \"\"\"Should create parent directories if they don't exist.\"\"\"",
        "        output_path = self.test_path / \"nested\" / \"path\" / \"export.jsonl\"",
        "        self.assertFalse(output_path.parent.exists())",
        "",
        "        ml.export_data(\"jsonl\", output_path)",
        "",
        "        self.assertTrue(output_path.parent.exists())",
        "        self.assertTrue(output_path.exists())",
        "",
        "    def test_export_csv_truncates_long_fields(self):",
        "        \"\"\"CSV export should truncate long input/output fields.\"\"\"",
        "        # Create chat with very long text",
        "        long_text = \"x\" * 2000",
        "        chat_data = {",
        "            \"timestamp\": \"2025-12-15T10:00:00\",",
        "            \"session_id\": \"sess1\",",
        "            \"query\": long_text,",
        "            \"response\": long_text,",
        "            \"files_referenced\": [],",
        "            \"files_modified\": [],",
        "            \"tools_used\": [],",
        "        }",
        "        chat_file = ml.CHATS_DIR / \"chat-001.json\"",
        "        with open(chat_file, 'w', encoding='utf-8') as f:",
        "            json.dump(chat_data, f)",
        "",
        "        # Export",
        "        output_path = self.test_path / \"export.csv\"",
        "        ml.export_data(\"csv\", output_path)",
        "",
        "        # Check truncation",
        "        with open(output_path, 'r', encoding='utf-8') as f:",
        "            reader = csv.DictReader(f)",
        "            row = next(reader)",
        "",
        "        # Should be truncated to 1000 chars",
        "        self.assertLessEqual(len(row['input']), 1000)",
        "        self.assertLessEqual(len(row['output']), 1000)",
        "",
        "    def test_export_csv_escapes_special_chars(self):",
        "        \"\"\"CSV export should properly escape special characters.\"\"\"",
        "        # Create chat with special chars",
        "        chat_data = {",
        "            \"timestamp\": \"2025-12-15T10:00:00\",",
        "            \"session_id\": \"sess1\",",
        "            \"query\": 'Text with \"quotes\" and, commas',",
        "            \"response\": \"Text with\\nnewlines\",",
        "            \"files_referenced\": [],",
        "            \"files_modified\": [],",
        "            \"tools_used\": [],",
        "        }",
        "        chat_file = ml.CHATS_DIR / \"chat-001.json\"",
        "        with open(chat_file, 'w', encoding='utf-8') as f:",
        "            json.dump(chat_data, f)",
        "",
        "        # Export",
        "        output_path = self.test_path / \"export.csv\"",
        "        ml.export_data(\"csv\", output_path)",
        "",
        "        # Should be able to read back without errors",
        "        with open(output_path, 'r', encoding='utf-8') as f:",
        "            reader = csv.DictReader(f)",
        "            row = next(reader)",
        "",
        "        self.assertIn('quotes', row['input'])",
        "        self.assertIn('newlines', row['output'])",
        "",
        "    def test_export_jsonl_valid_json_per_line(self):",
        "        \"\"\"JSONL export should have valid JSON on each line.\"\"\"",
        "        # Create multiple records",
        "        for i in range(3):",
        "            chat_data = {",
        "                \"timestamp\": f\"2025-12-15T1{i}:00:00\",",
        "                \"session_id\": \"sess1\",",
        "                \"query\": f\"Query {i}\",",
        "                \"response\": f\"Response {i}\",",
        "                \"files_referenced\": [],",
        "                \"files_modified\": [],",
        "                \"tools_used\": [],",
        "            }",
        "            chat_file = ml.CHATS_DIR / f\"chat-00{i}.json\"",
        "            with open(chat_file, 'w', encoding='utf-8') as f:",
        "                json.dump(chat_data, f)",
        "",
        "        # Export",
        "        output_path = self.test_path / \"export.jsonl\"",
        "        ml.export_data(\"jsonl\", output_path)",
        "",
        "        # Check each line is valid JSON",
        "        with open(output_path, 'r', encoding='utf-8') as f:",
        "            lines = f.readlines()",
        "",
        "        self.assertEqual(len(lines), 3)",
        "        for line in lines:",
        "            # Should not raise JSONDecodeError",
        "            record = json.loads(line)",
        "            self.assertIsInstance(record, dict)",
        "            self.assertIn('type', record)",
        "            self.assertIn('timestamp', record)",
        "",
        "    def test_export_sorts_by_timestamp(self):",
        "        \"\"\"Export should sort records by timestamp.\"\"\"",
        "        # Create records with different timestamps (out of order)",
        "        timestamps = [\"2025-12-15T12:00:00\", \"2025-12-15T09:00:00\", \"2025-12-15T15:00:00\"]",
        "",
        "        for i, ts in enumerate(timestamps):",
        "            chat_data = {",
        "                \"timestamp\": ts,",
        "                \"session_id\": \"sess1\",",
        "                \"query\": f\"Query {i}\",",
        "                \"response\": f\"Response {i}\",",
        "                \"files_referenced\": [],",
        "                \"files_modified\": [],",
        "                \"tools_used\": [],",
        "            }",
        "            chat_file = ml.CHATS_DIR / f\"chat-00{i}.json\"",
        "            with open(chat_file, 'w', encoding='utf-8') as f:",
        "                json.dump(chat_data, f)",
        "",
        "        # Export",
        "        output_path = self.test_path / \"export.jsonl\"",
        "        ml.export_data(\"jsonl\", output_path)",
        "",
        "        # Check order",
        "        with open(output_path, 'r', encoding='utf-8') as f:",
        "            records = [json.loads(line) for line in f.readlines()]",
        "",
        "        # Should be sorted",
        "        sorted_timestamps = sorted(timestamps)",
        "        for i, record in enumerate(records):",
        "            self.assertEqual(record['timestamp'], sorted_timestamps[i])",
        "",
        "",
        "if __name__ == \"__main__\":",
        "    unittest.main()"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "tests/unit/test_ml_feedback.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "#!/usr/bin/env python3",
        "\"\"\"",
        "Unit tests for ML data collector feedback functionality.",
        "",
        "Tests the add_chat_feedback() and list_chats_needing_feedback() functions,",
        "as well as schema validation for feedback data.",
        "\"\"\"",
        "",
        "import json",
        "import os",
        "import sys",
        "import tempfile",
        "import unittest",
        "from datetime import datetime",
        "from pathlib import Path",
        "from unittest.mock import patch, MagicMock",
        "",
        "# Add scripts to path",
        "sys.path.insert(0, str(Path(__file__).parent.parent.parent / \"scripts\"))",
        "",
        "import ml_data_collector as ml",
        "",
        "",
        "class TestAddChatFeedback(unittest.TestCase):",
        "    \"\"\"Test add_chat_feedback() function.\"\"\"",
        "",
        "    def setUp(self):",
        "        \"\"\"Set up test environment with temporary directories.\"\"\"",
        "        self.test_dir = tempfile.TemporaryDirectory()",
        "        self.test_path = Path(self.test_dir.name)",
        "",
        "        # Temporarily override ML data directories",
        "        self.original_ml_data_dir = ml.ML_DATA_DIR",
        "        self.original_chats_dir = ml.CHATS_DIR",
        "",
        "        ml.ML_DATA_DIR = self.test_path / \".git-ml\"",
        "        ml.CHATS_DIR = ml.ML_DATA_DIR / \"chats\"",
        "",
        "        # Create test directories",
        "        ml.ensure_dirs()",
        "",
        "    def tearDown(self):",
        "        \"\"\"Clean up test environment.\"\"\"",
        "        # Restore original directories",
        "        ml.ML_DATA_DIR = self.original_ml_data_dir",
        "        ml.CHATS_DIR = self.original_chats_dir",
        "",
        "        # Clean up temp directory",
        "        self.test_dir.cleanup()",
        "",
        "    def _create_test_chat(self, chat_id: str, user_feedback=None) -> Path:",
        "        \"\"\"Helper to create a test chat file.\"\"\"",
        "        # Create date directory",
        "        date_str = datetime.now().strftime(\"%Y-%m-%d\")",
        "        date_dir = ml.CHATS_DIR / date_str",
        "        date_dir.mkdir(parents=True, exist_ok=True)",
        "",
        "        # Create chat file",
        "        chat_data = {",
        "            \"id\": chat_id,",
        "            \"timestamp\": datetime.now().isoformat(),",
        "            \"session_id\": \"test-session\",",
        "            \"query\": \"Test query\",",
        "            \"response\": \"Test response\",",
        "            \"files_referenced\": [],",
        "            \"files_modified\": [],",
        "            \"tools_used\": [\"Read\"],",
        "        }",
        "",
        "        if user_feedback is not None:",
        "            chat_data[\"user_feedback\"] = user_feedback",
        "",
        "        chat_file = date_dir / f\"{chat_id}.json\"",
        "        with open(chat_file, 'w', encoding='utf-8') as f:",
        "            json.dump(chat_data, f)",
        "",
        "        return chat_file",
        "",
        "    def test_add_feedback_to_new_chat(self):",
        "        \"\"\"Test adding feedback to a chat without existing feedback.\"\"\"",
        "        chat_id = \"chat-test-001\"",
        "        self._create_test_chat(chat_id)",
        "",
        "        # Add feedback",
        "        result = ml.add_chat_feedback(chat_id, \"good\", \"Great response!\")",
        "",
        "        # Should succeed",
        "        self.assertTrue(result)",
        "",
        "        # Verify feedback was added",
        "        chat_file = ml.find_chat_file(chat_id)",
        "        with open(chat_file, 'r', encoding='utf-8') as f:",
        "            chat_data = json.load(f)",
        "",
        "        self.assertIn(\"user_feedback\", chat_data)",
        "        self.assertIsInstance(chat_data[\"user_feedback\"], dict)",
        "        self.assertEqual(chat_data[\"user_feedback\"][\"rating\"], \"good\")",
        "        self.assertEqual(chat_data[\"user_feedback\"][\"comment\"], \"Great response!\")",
        "        self.assertIn(\"timestamp\", chat_data[\"user_feedback\"])",
        "",
        "    def test_add_feedback_minimal(self):",
        "        \"\"\"Test adding feedback without comment.\"\"\"",
        "        chat_id = \"chat-test-002\"",
        "        self._create_test_chat(chat_id)",
        "",
        "        # Add feedback without comment",
        "        result = ml.add_chat_feedback(chat_id, \"neutral\")",
        "",
        "        # Should succeed",
        "        self.assertTrue(result)",
        "",
        "        # Verify feedback",
        "        chat_file = ml.find_chat_file(chat_id)",
        "        with open(chat_file, 'r', encoding='utf-8') as f:",
        "            chat_data = json.load(f)",
        "",
        "        self.assertEqual(chat_data[\"user_feedback\"][\"rating\"], \"neutral\")",
        "        self.assertIsNone(chat_data[\"user_feedback\"][\"comment\"])",
        "",
        "    def test_add_feedback_all_ratings(self):",
        "        \"\"\"Test all valid rating values.\"\"\"",
        "        valid_ratings = [\"good\", \"bad\", \"neutral\"]",
        "",
        "        for i, rating in enumerate(valid_ratings):",
        "            chat_id = f\"chat-test-rating-{i}\"",
        "            self._create_test_chat(chat_id)",
        "",
        "            result = ml.add_chat_feedback(chat_id, rating)",
        "            self.assertTrue(result)",
        "",
        "            # Verify rating",
        "            chat_file = ml.find_chat_file(chat_id)",
        "            with open(chat_file, 'r', encoding='utf-8') as f:",
        "                chat_data = json.load(f)",
        "",
        "            self.assertEqual(chat_data[\"user_feedback\"][\"rating\"], rating)",
        "",
        "    def test_add_feedback_invalid_chat_id(self):",
        "        \"\"\"Test adding feedback to non-existent chat.\"\"\"",
        "        result = ml.add_chat_feedback(\"nonexistent-chat-id\", \"good\")",
        "",
        "        # Should return False (chat not found)",
        "        self.assertFalse(result)",
        "",
        "    def test_add_feedback_invalid_rating(self):",
        "        \"\"\"Test adding feedback with invalid rating value.\"\"\"",
        "        chat_id = \"chat-test-003\"",
        "        self._create_test_chat(chat_id)",
        "",
        "        # Should raise ValueError",
        "        with self.assertRaises(ValueError) as ctx:",
        "            ml.add_chat_feedback(chat_id, \"excellent\")",
        "",
        "        self.assertIn(\"Invalid rating\", str(ctx.exception))",
        "        self.assertIn(\"excellent\", str(ctx.exception))",
        "",
        "    def test_add_feedback_overwrite_without_force(self):",
        "        \"\"\"Test that overwriting existing feedback without force fails.\"\"\"",
        "        chat_id = \"chat-test-004\"",
        "        existing_feedback = {",
        "            \"rating\": \"good\",",
        "            \"comment\": \"Original feedback\",",
        "            \"timestamp\": datetime.now().isoformat(),",
        "        }",
        "        self._create_test_chat(chat_id, user_feedback=existing_feedback)",
        "",
        "        # Try to update without force",
        "        result = ml.add_chat_feedback(chat_id, \"bad\", \"New feedback\")",
        "",
        "        # Should fail",
        "        self.assertFalse(result)",
        "",
        "        # Verify original feedback unchanged",
        "        chat_file = ml.find_chat_file(chat_id)",
        "        with open(chat_file, 'r', encoding='utf-8') as f:",
        "            chat_data = json.load(f)",
        "",
        "        self.assertEqual(chat_data[\"user_feedback\"][\"rating\"], \"good\")",
        "        self.assertEqual(chat_data[\"user_feedback\"][\"comment\"], \"Original feedback\")",
        "",
        "    def test_add_feedback_overwrite_with_force(self):",
        "        \"\"\"Test overwriting existing feedback with force=True.\"\"\"",
        "        chat_id = \"chat-test-005\"",
        "        existing_feedback = {",
        "            \"rating\": \"good\",",
        "            \"comment\": \"Original feedback\",",
        "            \"timestamp\": datetime.now().isoformat(),",
        "        }",
        "        self._create_test_chat(chat_id, user_feedback=existing_feedback)",
        "",
        "        # Update with force",
        "        result = ml.add_chat_feedback(chat_id, \"bad\", \"Updated feedback\", force=True)",
        "",
        "        # Should succeed",
        "        self.assertTrue(result)",
        "",
        "        # Verify feedback was updated",
        "        chat_file = ml.find_chat_file(chat_id)",
        "        with open(chat_file, 'r', encoding='utf-8') as f:",
        "            chat_data = json.load(f)",
        "",
        "        self.assertEqual(chat_data[\"user_feedback\"][\"rating\"], \"bad\")",
        "        self.assertEqual(chat_data[\"user_feedback\"][\"comment\"], \"Updated feedback\")",
        "",
        "    def test_add_feedback_upgrade_legacy_format(self):",
        "        \"\"\"Test upgrading from legacy string format to dict format.\"\"\"",
        "        chat_id = \"chat-test-006\"",
        "        # Create chat with legacy string feedback",
        "        self._create_test_chat(chat_id, user_feedback=\"good\")",
        "",
        "        # Try to add feedback (should allow upgrade from legacy format)",
        "        result = ml.add_chat_feedback(chat_id, \"neutral\", \"Upgraded feedback\")",
        "",
        "        # Should succeed (legacy format allows implicit upgrade)",
        "        self.assertTrue(result)",
        "",
        "        # Verify feedback is now in dict format",
        "        chat_file = ml.find_chat_file(chat_id)",
        "        with open(chat_file, 'r', encoding='utf-8') as f:",
        "            chat_data = json.load(f)",
        "",
        "        self.assertIsInstance(chat_data[\"user_feedback\"], dict)",
        "        self.assertEqual(chat_data[\"user_feedback\"][\"rating\"], \"neutral\")",
        "        self.assertEqual(chat_data[\"user_feedback\"][\"comment\"], \"Upgraded feedback\")",
        "",
        "",
        "class TestListChatsNeedingFeedback(unittest.TestCase):",
        "    \"\"\"Test list_chats_needing_feedback() function.\"\"\"",
        "",
        "    def setUp(self):",
        "        \"\"\"Set up test environment with temporary directories.\"\"\"",
        "        self.test_dir = tempfile.TemporaryDirectory()",
        "        self.test_path = Path(self.test_dir.name)",
        "",
        "        # Temporarily override ML data directories",
        "        self.original_ml_data_dir = ml.ML_DATA_DIR",
        "        self.original_chats_dir = ml.CHATS_DIR",
        "",
        "        ml.ML_DATA_DIR = self.test_path / \".git-ml\"",
        "        ml.CHATS_DIR = ml.ML_DATA_DIR / \"chats\"",
        "",
        "        # Create test directories",
        "        ml.ensure_dirs()",
        "",
        "    def tearDown(self):",
        "        \"\"\"Clean up test environment.\"\"\"",
        "        # Restore original directories",
        "        ml.ML_DATA_DIR = self.original_ml_data_dir",
        "        ml.CHATS_DIR = self.original_chats_dir",
        "",
        "        # Clean up temp directory",
        "        self.test_dir.cleanup()",
        "",
        "    def _create_test_chat(",
        "        self,",
        "        chat_id: str,",
        "        query: str = \"Test query\",",
        "        user_feedback=None,",
        "        date_str: str = None",
        "    ):",
        "        \"\"\"Helper to create a test chat file.\"\"\"",
        "        if date_str is None:",
        "            date_str = datetime.now().strftime(\"%Y-%m-%d\")",
        "",
        "        date_dir = ml.CHATS_DIR / date_str",
        "        date_dir.mkdir(parents=True, exist_ok=True)",
        "",
        "        chat_data = {",
        "            \"id\": chat_id,",
        "            \"timestamp\": datetime.now().isoformat(),",
        "            \"session_id\": \"test-session\",",
        "            \"query\": query,",
        "            \"response\": \"Test response\",",
        "            \"files_referenced\": [],",
        "            \"files_modified\": [],",
        "            \"tools_used\": [],",
        "        }",
        "",
        "        if user_feedback is not None:",
        "            chat_data[\"user_feedback\"] = user_feedback",
        "",
        "        chat_file = date_dir / f\"{chat_id}.json\"",
        "        with open(chat_file, 'w', encoding='utf-8') as f:",
        "            json.dump(chat_data, f)",
        "",
        "    def test_list_empty_directory(self):",
        "        \"\"\"Test listing when no chats exist.\"\"\"",
        "        result = ml.list_chats_needing_feedback()",
        "",
        "        # Should return empty list",
        "        self.assertEqual(result, [])",
        "",
        "    def test_list_chats_all_without_feedback(self):",
        "        \"\"\"Test listing when all chats lack feedback.\"\"\"",
        "        # Create 3 chats without feedback",
        "        for i in range(3):",
        "            self._create_test_chat(f\"chat-test-{i}\", f\"Query {i}\")",
        "",
        "        result = ml.list_chats_needing_feedback()",
        "",
        "        # Should return all 3 chats",
        "        self.assertEqual(len(result), 3)",
        "",
        "        # All should not have feedback",
        "        for chat in result:",
        "            self.assertFalse(chat[\"has_feedback\"])",
        "            self.assertIsNone(chat[\"feedback_rating\"])",
        "",
        "    def test_list_chats_all_with_feedback(self):",
        "        \"\"\"Test listing when all chats have feedback.\"\"\"",
        "        # Create chats with dict-format feedback",
        "        for i in range(3):",
        "            feedback = {",
        "                \"rating\": \"good\",",
        "                \"comment\": \"Test\",",
        "                \"timestamp\": datetime.now().isoformat(),",
        "            }",
        "            self._create_test_chat(f\"chat-test-{i}\", f\"Query {i}\", user_feedback=feedback)",
        "",
        "        result = ml.list_chats_needing_feedback()",
        "",
        "        # Should return all chats (function lists all chats, not just needing feedback)",
        "        self.assertEqual(len(result), 3)",
        "",
        "        # All should have feedback",
        "        for chat in result:",
        "            self.assertTrue(chat[\"has_feedback\"])",
        "            self.assertEqual(chat[\"feedback_rating\"], \"good\")",
        "",
        "    def test_list_chats_mixed_feedback(self):",
        "        \"\"\"Test listing with mix of chats with and without feedback.\"\"\"",
        "        # Create 2 chats without feedback",
        "        self._create_test_chat(\"chat-no-fb-1\", \"No feedback 1\")",
        "        self._create_test_chat(\"chat-no-fb-2\", \"No feedback 2\")",
        "",
        "        # Create 2 chats with feedback",
        "        feedback = {\"rating\": \"bad\", \"comment\": None, \"timestamp\": datetime.now().isoformat()}",
        "        self._create_test_chat(\"chat-with-fb-1\", \"Has feedback 1\", user_feedback=feedback)",
        "        self._create_test_chat(\"chat-with-fb-2\", \"Has feedback 2\", user_feedback=feedback)",
        "",
        "        result = ml.list_chats_needing_feedback()",
        "",
        "        # Should return all 4 chats",
        "        self.assertEqual(len(result), 4)",
        "",
        "        # Count feedback status",
        "        with_feedback = sum(1 for c in result if c[\"has_feedback\"])",
        "        without_feedback = sum(1 for c in result if not c[\"has_feedback\"])",
        "",
        "        self.assertEqual(with_feedback, 2)",
        "        self.assertEqual(without_feedback, 2)",
        "",
        "    def test_list_chats_limit_parameter(self):",
        "        \"\"\"Test that limit parameter works correctly.\"\"\"",
        "        # Create 10 chats",
        "        for i in range(10):",
        "            self._create_test_chat(f\"chat-test-{i:02d}\", f\"Query {i}\")",
        "",
        "        # Request only 5",
        "        result = ml.list_chats_needing_feedback(limit=5)",
        "",
        "        # Should return exactly 5",
        "        self.assertEqual(len(result), 5)",
        "",
        "        # Request 20 (more than exist)",
        "        result = ml.list_chats_needing_feedback(limit=20)",
        "",
        "        # Should return all 10",
        "        self.assertEqual(len(result), 10)",
        "",
        "    def test_list_chats_query_truncation(self):",
        "        \"\"\"Test that long queries are truncated.\"\"\"",
        "        long_query = \"A\" * 200  # 200 characters",
        "        self._create_test_chat(\"chat-test-long\", long_query)",
        "",
        "        result = ml.list_chats_needing_feedback()",
        "",
        "        # Query should be truncated to 100 chars",
        "        self.assertEqual(len(result[0][\"query\"]), 100)",
        "        self.assertEqual(result[0][\"query\"], \"A\" * 100)",
        "",
        "    def test_list_chats_legacy_string_feedback(self):",
        "        \"\"\"Test that legacy string format feedback is recognized.\"\"\"",
        "        # Create chat with legacy string feedback",
        "        self._create_test_chat(\"chat-legacy\", \"Legacy chat\", user_feedback=\"good\")",
        "",
        "        result = ml.list_chats_needing_feedback()",
        "",
        "        # Should recognize as having feedback",
        "        self.assertEqual(len(result), 1)",
        "        self.assertTrue(result[0][\"has_feedback\"])",
        "        self.assertEqual(result[0][\"feedback_rating\"], \"good\")",
        "",
        "    def test_list_chats_reverse_chronological(self):",
        "        \"\"\"Test that chats are returned in reverse chronological order (most recent first).\"\"\"",
        "        import time",
        "",
        "        # Create chats with slight time gaps",
        "        for i in range(3):",
        "            self._create_test_chat(f\"chat-test-{i}\", f\"Query {i}\")",
        "            time.sleep(0.01)  # Small delay to ensure different mtimes",
        "",
        "        result = ml.list_chats_needing_feedback()",
        "",
        "        # IDs should be in reverse order (most recent first)",
        "        # Note: This depends on file mtime, which should be newest first",
        "        self.assertEqual(len(result), 3)",
        "        # The most recently created should be first",
        "        self.assertEqual(result[0][\"id\"], \"chat-test-2\")",
        "",
        "    def test_list_chats_multiple_dates(self):",
        "        \"\"\"Test listing chats across multiple date directories.\"\"\"",
        "        # Create chats on different dates",
        "        self._create_test_chat(\"chat-2025-01\", \"Query 1\", date_str=\"2025-01-15\")",
        "        self._create_test_chat(\"chat-2025-02\", \"Query 2\", date_str=\"2025-02-15\")",
        "        self._create_test_chat(\"chat-2025-03\", \"Query 3\", date_str=\"2025-03-15\")",
        "",
        "        result = ml.list_chats_needing_feedback()",
        "",
        "        # Should find all chats across dates",
        "        self.assertEqual(len(result), 3)",
        "",
        "        # Should be sorted by date (most recent first)",
        "        chat_ids = [c[\"id\"] for c in result]",
        "        self.assertIn(\"chat-2025-01\", chat_ids)",
        "        self.assertIn(\"chat-2025-02\", chat_ids)",
        "        self.assertIn(\"chat-2025-03\", chat_ids)",
        "",
        "",
        "class TestFeedbackSchemaValidation(unittest.TestCase):",
        "    \"\"\"Test schema validation for feedback data.\"\"\"",
        "",
        "    def test_validate_chat_with_dict_feedback(self):",
        "        \"\"\"Test that chat with dict-format feedback validates.\"\"\"",
        "        chat_data = {",
        "            \"id\": \"test-chat\",",
        "            \"timestamp\": datetime.now().isoformat(),",
        "            \"session_id\": \"test-session\",",
        "            \"query\": \"Test query\",",
        "            \"response\": \"Test response\",",
        "            \"files_referenced\": [],",
        "            \"files_modified\": [],",
        "            \"tools_used\": [],",
        "            \"user_feedback\": {",
        "                \"rating\": \"good\",",
        "                \"comment\": \"Great!\",",
        "                \"timestamp\": datetime.now().isoformat(),",
        "            },",
        "        }",
        "",
        "        errors = ml.validate_schema(chat_data, ml.CHAT_SCHEMA, \"chat\")",
        "",
        "        # Should have no validation errors",
        "        self.assertEqual(errors, [])",
        "",
        "    def test_validate_chat_with_string_feedback(self):",
        "        \"\"\"Test that chat with legacy string-format feedback validates.\"\"\"",
        "        chat_data = {",
        "            \"id\": \"test-chat\",",
        "            \"timestamp\": datetime.now().isoformat(),",
        "            \"session_id\": \"test-session\",",
        "            \"query\": \"Test query\",",
        "            \"response\": \"Test response\",",
        "            \"files_referenced\": [],",
        "            \"files_modified\": [],",
        "            \"tools_used\": [],",
        "            \"user_feedback\": \"good\",  # Legacy string format",
        "        }",
        "",
        "        errors = ml.validate_schema(chat_data, ml.CHAT_SCHEMA, \"chat\")",
        "",
        "        # Should have no validation errors",
        "        self.assertEqual(errors, [])",
        "",
        "    def test_validate_chat_without_feedback(self):",
        "        \"\"\"Test that chat without feedback validates.\"\"\"",
        "        chat_data = {",
        "            \"id\": \"test-chat\",",
        "            \"timestamp\": datetime.now().isoformat(),",
        "            \"session_id\": \"test-session\",",
        "            \"query\": \"Test query\",",
        "            \"response\": \"Test response\",",
        "            \"files_referenced\": [],",
        "            \"files_modified\": [],",
        "            \"tools_used\": [],",
        "        }",
        "",
        "        errors = ml.validate_schema(chat_data, ml.CHAT_SCHEMA, \"chat\")",
        "",
        "        # Should have no validation errors",
        "        self.assertEqual(errors, [])",
        "",
        "    def test_validate_chat_with_none_feedback(self):",
        "        \"\"\"Test that chat with None feedback validates.\"\"\"",
        "        chat_data = {",
        "            \"id\": \"test-chat\",",
        "            \"timestamp\": datetime.now().isoformat(),",
        "            \"session_id\": \"test-session\",",
        "            \"query\": \"Test query\",",
        "            \"response\": \"Test response\",",
        "            \"files_referenced\": [],",
        "            \"files_modified\": [],",
        "            \"tools_used\": [],",
        "            \"user_feedback\": None,",
        "        }",
        "",
        "        errors = ml.validate_schema(chat_data, ml.CHAT_SCHEMA, \"chat\")",
        "",
        "        # Should have no validation errors",
        "        self.assertEqual(errors, [])",
        "",
        "    def test_validate_chat_with_invalid_feedback_type(self):",
        "        \"\"\"Test that chat with invalid feedback type fails validation.\"\"\"",
        "        chat_data = {",
        "            \"id\": \"test-chat\",",
        "            \"timestamp\": datetime.now().isoformat(),",
        "            \"session_id\": \"test-session\",",
        "            \"query\": \"Test query\",",
        "            \"response\": \"Test response\",",
        "            \"files_referenced\": [],",
        "            \"files_modified\": [],",
        "            \"tools_used\": [],",
        "            \"user_feedback\": 123,  # Invalid: should be dict, str, or None",
        "        }",
        "",
        "        errors = ml.validate_schema(chat_data, ml.CHAT_SCHEMA, \"chat\")",
        "",
        "        # Should have validation errors",
        "        self.assertGreater(len(errors), 0)",
        "        self.assertTrue(any(\"user_feedback\" in error for error in errors))",
        "",
        "",
        "if __name__ == \"__main__\":",
        "    unittest.main()"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "tests/unit/test_ml_quality.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "#!/usr/bin/env python3",
        "\"\"\"",
        "Tests for ML data collector quality-report functionality.",
        "\"\"\"",
        "",
        "import json",
        "import os",
        "import sys",
        "import tempfile",
        "import unittest",
        "from datetime import datetime",
        "from pathlib import Path",
        "from unittest.mock import patch, MagicMock",
        "",
        "# Add scripts to path",
        "sys.path.insert(0, str(Path(__file__).parent.parent.parent / \"scripts\"))",
        "",
        "import ml_data_collector as ml",
        "",
        "",
        "class TestDataQualityAnalysis(unittest.TestCase):",
        "    \"\"\"Test data quality analysis and reporting.\"\"\"",
        "",
        "    def setUp(self):",
        "        \"\"\"Set up test environment with temporary directories.\"\"\"",
        "        self.test_dir = tempfile.TemporaryDirectory()",
        "        self.test_path = Path(self.test_dir.name)",
        "",
        "        # Temporarily override ML data directories",
        "        self.original_ml_data_dir = ml.ML_DATA_DIR",
        "        self.original_commits_dir = ml.COMMITS_DIR",
        "        self.original_sessions_dir = ml.SESSIONS_DIR",
        "        self.original_chats_dir = ml.CHATS_DIR",
        "        self.original_actions_dir = ml.ACTIONS_DIR",
        "",
        "        ml.ML_DATA_DIR = self.test_path / \".git-ml\"",
        "        ml.COMMITS_DIR = ml.ML_DATA_DIR / \"commits\"",
        "        ml.SESSIONS_DIR = ml.ML_DATA_DIR / \"sessions\"",
        "        ml.CHATS_DIR = ml.ML_DATA_DIR / \"chats\"",
        "        ml.ACTIONS_DIR = ml.ML_DATA_DIR / \"actions\"",
        "",
        "        # Create test directories",
        "        ml.ensure_dirs()",
        "",
        "    def tearDown(self):",
        "        \"\"\"Clean up test environment.\"\"\"",
        "        # Restore original directories",
        "        ml.ML_DATA_DIR = self.original_ml_data_dir",
        "        ml.COMMITS_DIR = self.original_commits_dir",
        "        ml.SESSIONS_DIR = self.original_sessions_dir",
        "        ml.CHATS_DIR = self.original_chats_dir",
        "        ml.ACTIONS_DIR = self.original_actions_dir",
        "",
        "        # Clean up temp directory",
        "        self.test_dir.cleanup()",
        "",
        "    def test_analyze_empty_data(self):",
        "        \"\"\"Test quality analysis with no data.\"\"\"",
        "        result = ml.analyze_data_quality()",
        "",
        "        # Should return structure with zero counts",
        "        self.assertIn('completeness', result)",
        "        self.assertIn('diversity', result)",
        "        self.assertIn('anomalies', result)",
        "        self.assertIn('quality_score', result)",
        "",
        "        comp = result['completeness']",
        "        self.assertEqual(comp['chats_total'], 0)",
        "        self.assertEqual(comp['commits_total'], 0)",
        "        self.assertEqual(comp['sessions_total'], 0)",
        "",
        "        # Quality score should be low with no data",
        "        self.assertIsInstance(result['quality_score'], int)",
        "        self.assertLessEqual(result['quality_score'], 100)",
        "        self.assertGreaterEqual(result['quality_score'], 0)",
        "",
        "    def test_completeness_all_fields_present(self):",
        "        \"\"\"Test completeness calculation with all required fields.\"\"\"",
        "        # Create complete chat entry",
        "        chat_data = {",
        "            'id': 'chat-test-001',",
        "            'timestamp': datetime.now().isoformat(),",
        "            'session_id': 'session-001',",
        "            'query': 'Test query',",
        "            'response': 'Test response',",
        "            'files_referenced': ['/path/to/file.py'],",
        "            'files_modified': [],",
        "            'tools_used': ['Read'],",
        "            'query_tokens': 10,",
        "            'response_tokens': 20,",
        "            'user_feedback': None,",
        "        }",
        "",
        "        chat_file = ml.CHATS_DIR / \"chat-test-001.json\"",
        "        with open(chat_file, 'w', encoding='utf-8') as f:",
        "            json.dump(chat_data, f)",
        "",
        "        result = ml.analyze_data_quality()",
        "",
        "        comp = result['completeness']",
        "        self.assertEqual(comp['chats_total'], 1)",
        "        self.assertEqual(comp['chats_complete'], 1)",
        "        self.assertEqual(comp['chats_complete_pct'], 100.0)",
        "",
        "    def test_completeness_missing_fields(self):",
        "        \"\"\"Test completeness calculation with missing fields.\"\"\"",
        "        # Create incomplete chat entry (missing required fields)",
        "        incomplete_chat = {",
        "            'id': 'chat-test-002',",
        "            'timestamp': datetime.now().isoformat(),",
        "            'query': 'Test query',",
        "            # Missing: session_id, response, files_referenced, files_modified, tools_used",
        "        }",
        "",
        "        chat_file = ml.CHATS_DIR / \"chat-test-002.json\"",
        "        with open(chat_file, 'w', encoding='utf-8') as f:",
        "            json.dump(incomplete_chat, f)",
        "",
        "        result = ml.analyze_data_quality()",
        "",
        "        comp = result['completeness']",
        "        self.assertEqual(comp['chats_total'], 1)",
        "        self.assertEqual(comp['chats_complete'], 0)",
        "        self.assertEqual(comp['chats_complete_pct'], 0.0)",
        "",
        "    def test_commits_with_ci_results(self):",
        "        \"\"\"Test tracking commits with CI results.\"\"\"",
        "        # Commit with CI results",
        "        commit1 = {",
        "            'hash': 'abc123def456',",
        "            'message': 'Test commit',",
        "            'timestamp': datetime.now().isoformat(),",
        "            'files_changed': ['/path/to/file.py'],",
        "            'ci_result': {'status': 'pass', 'coverage': 89.5},",
        "        }",
        "",
        "        # Commit without CI results",
        "        commit2 = {",
        "            'hash': 'def456abc789',",
        "            'message': 'Another commit',",
        "            'timestamp': datetime.now().isoformat(),",
        "            'files_changed': ['/path/to/other.py'],",
        "        }",
        "",
        "        with open(ml.COMMITS_DIR / \"commit1.json\", 'w', encoding='utf-8') as f:",
        "            json.dump(commit1, f)",
        "        with open(ml.COMMITS_DIR / \"commit2.json\", 'w', encoding='utf-8') as f:",
        "            json.dump(commit2, f)",
        "",
        "        result = ml.analyze_data_quality()",
        "",
        "        comp = result['completeness']",
        "        self.assertEqual(comp['commits_total'], 2)",
        "        self.assertEqual(comp['commits_with_ci'], 1)",
        "        self.assertEqual(comp['commits_with_ci_pct'], 50.0)",
        "",
        "    def test_sessions_with_commits(self):",
        "        \"\"\"Test tracking sessions with linked commits.\"\"\"",
        "        # Create session",
        "        session_data = {",
        "            'id': 'session-001',",
        "            'start_time': datetime.now().isoformat(),",
        "            'chat_ids': ['chat-001'],",
        "        }",
        "",
        "        with open(ml.SESSIONS_DIR / \"session-001.json\", 'w', encoding='utf-8') as f:",
        "            json.dump(session_data, f)",
        "",
        "        # Create commit linked to session",
        "        commit_data = {",
        "            'hash': 'abc123',",
        "            'message': 'Test',",
        "            'timestamp': datetime.now().isoformat(),",
        "            'files_changed': ['test.py'],",
        "            'session_id': 'session-001',",
        "        }",
        "",
        "        with open(ml.COMMITS_DIR / \"commit.json\", 'w', encoding='utf-8') as f:",
        "            json.dump(commit_data, f)",
        "",
        "        result = ml.analyze_data_quality()",
        "",
        "        comp = result['completeness']",
        "        self.assertEqual(comp['sessions_total'], 1)",
        "        self.assertEqual(comp['sessions_with_commits'], 1)",
        "        self.assertEqual(comp['sessions_with_commits_pct'], 100.0)",
        "",
        "    def test_chats_with_feedback(self):",
        "        \"\"\"Test tracking chats with user feedback.\"\"\"",
        "        # Chat with feedback",
        "        chat1 = {",
        "            'id': 'chat-001',",
        "            'timestamp': datetime.now().isoformat(),",
        "            'session_id': 'session-001',",
        "            'query': 'Test',",
        "            'response': 'Response',",
        "            'files_referenced': [],",
        "            'files_modified': [],",
        "            'tools_used': [],",
        "            'user_feedback': {'rating': 5, 'comment': 'Great!'},",
        "        }",
        "",
        "        # Chat without feedback",
        "        chat2 = {",
        "            'id': 'chat-002',",
        "            'timestamp': datetime.now().isoformat(),",
        "            'session_id': 'session-001',",
        "            'query': 'Test',",
        "            'response': 'Response',",
        "            'files_referenced': [],",
        "            'files_modified': [],",
        "            'tools_used': [],",
        "        }",
        "",
        "        with open(ml.CHATS_DIR / \"chat1.json\", 'w', encoding='utf-8') as f:",
        "            json.dump(chat1, f)",
        "        with open(ml.CHATS_DIR / \"chat2.json\", 'w', encoding='utf-8') as f:",
        "            json.dump(chat2, f)",
        "",
        "        result = ml.analyze_data_quality()",
        "",
        "        comp = result['completeness']",
        "        self.assertEqual(comp['chats_total'], 2)",
        "        self.assertEqual(comp['chats_with_feedback'], 1)",
        "        self.assertEqual(comp['chats_with_feedback_pct'], 50.0)",
        "",
        "    def test_diversity_unique_files(self):",
        "        \"\"\"Test tracking unique files across chats and commits.\"\"\"",
        "        # Chat with files",
        "        chat = {",
        "            'id': 'chat-001',",
        "            'timestamp': datetime.now().isoformat(),",
        "            'session_id': 'session-001',",
        "            'query': 'Test',",
        "            'response': 'Response',",
        "            'files_referenced': ['/file1.py', '/file2.py'],",
        "            'files_modified': ['/file3.py'],",
        "            'tools_used': [],",
        "        }",
        "",
        "        # Commit with files (one overlapping)",
        "        commit = {",
        "            'hash': 'abc123',",
        "            'message': 'Test',",
        "            'timestamp': datetime.now().isoformat(),",
        "            'files_changed': ['/file3.py', '/file4.py'],",
        "        }",
        "",
        "        with open(ml.CHATS_DIR / \"chat.json\", 'w', encoding='utf-8') as f:",
        "            json.dump(chat, f)",
        "        with open(ml.COMMITS_DIR / \"commit.json\", 'w', encoding='utf-8') as f:",
        "            json.dump(commit, f)",
        "",
        "        result = ml.analyze_data_quality()",
        "",
        "        div = result['diversity']",
        "        # Should have 4 unique files: file1, file2, file3, file4",
        "        self.assertEqual(div['unique_files'], 4)",
        "",
        "    def test_diversity_unique_tools(self):",
        "        \"\"\"Test tracking unique tools and their usage counts.\"\"\"",
        "        chats = [",
        "            {",
        "                'id': f'chat-{i}',",
        "                'timestamp': datetime.now().isoformat(),",
        "                'session_id': 'session-001',",
        "                'query': 'Test',",
        "                'response': 'Response',",
        "                'files_referenced': [],",
        "                'files_modified': [],",
        "                'tools_used': ['Read', 'Edit'] if i % 2 == 0 else ['Bash', 'Read'],",
        "            }",
        "            for i in range(4)",
        "        ]",
        "",
        "        for chat in chats:",
        "            chat_file = ml.CHATS_DIR / f\"{chat['id']}.json\"",
        "            with open(chat_file, 'w', encoding='utf-8') as f:",
        "                json.dump(chat, f)",
        "",
        "        result = ml.analyze_data_quality()",
        "",
        "        div = result['diversity']",
        "        # Should have 3 unique tools: Read, Edit, Bash",
        "        self.assertEqual(div['unique_tools'], 3)",
        "",
        "        # Check tool distribution",
        "        tool_dist = div['tool_distribution']",
        "        self.assertEqual(tool_dist['Read'], 4)  # Used in all 4 chats",
        "        self.assertEqual(tool_dist['Edit'], 2)  # Used in 2 chats",
        "        self.assertEqual(tool_dist['Bash'], 2)  # Used in 2 chats",
        "",
        "    def test_diversity_query_response_lengths(self):",
        "        \"\"\"Test tracking query and response length statistics.\"\"\"",
        "        chats = [",
        "            {",
        "                'id': f'chat-{i}',",
        "                'timestamp': datetime.now().isoformat(),",
        "                'session_id': 'session-001',",
        "                'query': 'Q' * (10 * (i + 1)),  # 10, 20, 30, 40, 50",
        "                'response': 'R' * (100 * (i + 1)),  # 100, 200, 300, 400, 500",
        "                'files_referenced': [],",
        "                'files_modified': [],",
        "                'tools_used': [],",
        "            }",
        "            for i in range(5)",
        "        ]",
        "",
        "        for chat in chats:",
        "            chat_file = ml.CHATS_DIR / f\"{chat['id']}.json\"",
        "            with open(chat_file, 'w', encoding='utf-8') as f:",
        "                json.dump(chat, f)",
        "",
        "        result = ml.analyze_data_quality()",
        "",
        "        div = result['diversity']",
        "        self.assertEqual(div['query_length_min'], 10)",
        "        self.assertEqual(div['query_length_avg'], 30)  # (10+20+30+40+50)/5",
        "        self.assertEqual(div['query_length_max'], 50)",
        "        self.assertEqual(div['response_length_min'], 100)",
        "        self.assertEqual(div['response_length_avg'], 300)  # (100+200+300+400+500)/5",
        "        self.assertEqual(div['response_length_max'], 500)",
        "",
        "    def test_anomaly_empty_responses(self):",
        "        \"\"\"Test detection of empty responses.\"\"\"",
        "        chats = [",
        "            {",
        "                'id': 'chat-empty',",
        "                'timestamp': datetime.now().isoformat(),",
        "                'session_id': 'session-001',",
        "                'query': 'Test',",
        "                'response': '',  # Empty response",
        "                'files_referenced': [],",
        "                'files_modified': [],",
        "                'tools_used': [],",
        "            },",
        "            {",
        "                'id': 'chat-whitespace',",
        "                'timestamp': datetime.now().isoformat(),",
        "                'session_id': 'session-001',",
        "                'query': 'Test',",
        "                'response': '   ',  # Whitespace only",
        "                'files_referenced': [],",
        "                'files_modified': [],",
        "                'tools_used': [],",
        "            },",
        "            {",
        "                'id': 'chat-normal',",
        "                'timestamp': datetime.now().isoformat(),",
        "                'session_id': 'session-001',",
        "                'query': 'Test',",
        "                'response': 'Normal response',",
        "                'files_referenced': [],",
        "                'files_modified': [],",
        "                'tools_used': [],",
        "            },",
        "        ]",
        "",
        "        for chat in chats:",
        "            chat_file = ml.CHATS_DIR / f\"{chat['id']}.json\"",
        "            with open(chat_file, 'w', encoding='utf-8') as f:",
        "                json.dump(chat, f)",
        "",
        "        result = ml.analyze_data_quality()",
        "",
        "        anom = result['anomalies']",
        "        self.assertEqual(anom['empty_responses'], 2)",
        "",
        "    def test_anomaly_zero_file_commits(self):",
        "        \"\"\"Test detection of commits with no files changed.\"\"\"",
        "        commits = [",
        "            {",
        "                'hash': 'abc123',",
        "                'message': 'Empty commit',",
        "                'timestamp': datetime.now().isoformat(),",
        "                'files_changed': [],  # No files",
        "            },",
        "            {",
        "                'hash': 'def456',",
        "                'message': 'Normal commit',",
        "                'timestamp': datetime.now().isoformat(),",
        "                'files_changed': ['file.py'],",
        "            },",
        "        ]",
        "",
        "        for i, commit in enumerate(commits):",
        "            commit_file = ml.COMMITS_DIR / f\"commit{i}.json\"",
        "            with open(commit_file, 'w', encoding='utf-8') as f:",
        "                json.dump(commit, f)",
        "",
        "        result = ml.analyze_data_quality()",
        "",
        "        anom = result['anomalies']",
        "        self.assertEqual(anom['zero_file_commits'], 1)",
        "",
        "    def test_anomaly_empty_sessions(self):",
        "        \"\"\"Test detection of sessions with no chats.\"\"\"",
        "        sessions = [",
        "            {",
        "                'id': 'session-empty',",
        "                'start_time': datetime.now().isoformat(),",
        "                'chat_ids': [],  # No chats",
        "            },",
        "            {",
        "                'id': 'session-normal',",
        "                'start_time': datetime.now().isoformat(),",
        "                'chat_ids': ['chat-001'],",
        "            },",
        "        ]",
        "",
        "        for session in sessions:",
        "            session_file = ml.SESSIONS_DIR / f\"{session['id']}.json\"",
        "            with open(session_file, 'w', encoding='utf-8') as f:",
        "                json.dump(session, f)",
        "",
        "        result = ml.analyze_data_quality()",
        "",
        "        anom = result['anomalies']",
        "        self.assertEqual(anom['empty_sessions'], 1)",
        "",
        "    def test_anomaly_potential_duplicates(self):",
        "        \"\"\"Test detection of potential duplicate entries.\"\"\"",
        "        timestamp = datetime.now().isoformat()",
        "",
        "        # Two chats with same timestamp and query",
        "        chats = [",
        "            {",
        "                'id': 'chat-001',",
        "                'timestamp': timestamp,",
        "                'session_id': 'session-001',",
        "                'query': 'Identical query',",
        "                'response': 'Response 1',",
        "                'files_referenced': [],",
        "                'files_modified': [],",
        "                'tools_used': [],",
        "            },",
        "            {",
        "                'id': 'chat-002',",
        "                'timestamp': timestamp,",
        "                'session_id': 'session-001',",
        "                'query': 'Identical query',",
        "                'response': 'Response 2',",
        "                'files_referenced': [],",
        "                'files_modified': [],",
        "                'tools_used': [],",
        "            },",
        "        ]",
        "",
        "        for chat in chats:",
        "            chat_file = ml.CHATS_DIR / f\"{chat['id']}.json\"",
        "            with open(chat_file, 'w', encoding='utf-8') as f:",
        "                json.dump(chat, f)",
        "",
        "        result = ml.analyze_data_quality()",
        "",
        "        anom = result['anomalies']",
        "        # Second entry should be flagged as duplicate",
        "        self.assertEqual(anom['potential_duplicates'], 1)",
        "",
        "    def test_quality_score_perfect_data(self):",
        "        \"\"\"Test quality score calculation with perfect data.\"\"\"",
        "        # Create perfect data: complete chats, diverse tools/files, no anomalies",
        "",
        "        # Create 10 complete chats",
        "        for i in range(10):",
        "            chat = {",
        "                'id': f'chat-{i:03d}',",
        "                'timestamp': datetime.now().isoformat(),",
        "                'session_id': 'session-001',",
        "                'query': f'Query {i}',",
        "                'response': f'Response {i}',",
        "                'files_referenced': [f'/file{i}.py'],",
        "                'files_modified': [f'/modified{i}.py'],",
        "                'tools_used': ['Read', 'Edit', 'Grep', 'Bash', 'Write'][i % 5:i % 5 + 2],",
        "                'user_feedback': {'rating': 5},",
        "            }",
        "            chat_file = ml.CHATS_DIR / f\"{chat['id']}.json\"",
        "            with open(chat_file, 'w', encoding='utf-8') as f:",
        "                json.dump(chat, f)",
        "",
        "        # Create 5 commits with CI results",
        "        for i in range(5):",
        "            commit = {",
        "                'hash': f'commit{i:03d}',",
        "                'message': f'Commit {i}',",
        "                'timestamp': datetime.now().isoformat(),",
        "                'files_changed': [f'/commit_file{i}.py'] * (i + 1),",
        "                'ci_result': {'status': 'pass', 'coverage': 90},",
        "                'session_id': 'session-001',",
        "            }",
        "            commit_file = ml.COMMITS_DIR / f\"commit{i}.json\"",
        "            with open(commit_file, 'w', encoding='utf-8') as f:",
        "                json.dump(commit, f)",
        "",
        "        # Create session",
        "        session = {",
        "            'id': 'session-001',",
        "            'start_time': datetime.now().isoformat(),",
        "            'chat_ids': [f'chat-{i:03d}' for i in range(10)],",
        "        }",
        "        with open(ml.SESSIONS_DIR / \"session-001.json\", 'w', encoding='utf-8') as f:",
        "            json.dump(session, f)",
        "",
        "        result = ml.analyze_data_quality()",
        "",
        "        # With perfect data, quality score should be very high",
        "        self.assertGreater(result['quality_score'], 80)",
        "        self.assertLessEqual(result['quality_score'], 100)",
        "",
        "    def test_quality_score_low_with_anomalies(self):",
        "        \"\"\"Test that anomalies reduce quality score.\"\"\"",
        "        # Create data with many anomalies",
        "",
        "        # Empty responses",
        "        for i in range(10):",
        "            chat = {",
        "                'id': f'chat-{i:03d}',",
        "                'timestamp': datetime.now().isoformat(),",
        "                'session_id': 'session-001',",
        "                'query': f'Query {i}',",
        "                'response': '',  # Empty!",
        "                'files_referenced': [],",
        "                'files_modified': [],",
        "                'tools_used': [],",
        "            }",
        "            chat_file = ml.CHATS_DIR / f\"{chat['id']}.json\"",
        "            with open(chat_file, 'w', encoding='utf-8') as f:",
        "                json.dump(chat, f)",
        "",
        "        # Zero-file commits",
        "        for i in range(5):",
        "            commit = {",
        "                'hash': f'commit{i:03d}',",
        "                'message': f'Empty commit {i}',",
        "                'timestamp': datetime.now().isoformat(),",
        "                'files_changed': [],  # No files!",
        "            }",
        "            commit_file = ml.COMMITS_DIR / f\"commit{i}.json\"",
        "            with open(commit_file, 'w', encoding='utf-8') as f:",
        "                json.dump(commit, f)",
        "",
        "        # Empty sessions",
        "        for i in range(3):",
        "            session = {",
        "                'id': f'session-{i:03d}',",
        "                'start_time': datetime.now().isoformat(),",
        "                'chat_ids': [],  # No chats!",
        "            }",
        "            session_file = ml.SESSIONS_DIR / f\"session-{i:03d}.json\"",
        "            with open(session_file, 'w', encoding='utf-8') as f:",
        "                json.dump(session, f)",
        "",
        "        result = ml.analyze_data_quality()",
        "",
        "        # With many anomalies, quality score should be lower",
        "        # Not necessarily very low since some completeness metrics may still be ok",
        "        self.assertLess(result['quality_score'], 100)",
        "",
        "    def test_corrupted_json_files_skipped(self):",
        "        \"\"\"Test that corrupted JSON files are skipped gracefully.\"\"\"",
        "        # Create valid chat",
        "        valid_chat = {",
        "            'id': 'chat-valid',",
        "            'timestamp': datetime.now().isoformat(),",
        "            'session_id': 'session-001',",
        "            'query': 'Test',",
        "            'response': 'Response',",
        "            'files_referenced': [],",
        "            'files_modified': [],",
        "            'tools_used': [],",
        "        }",
        "        with open(ml.CHATS_DIR / \"valid.json\", 'w', encoding='utf-8') as f:",
        "            json.dump(valid_chat, f)",
        "",
        "        # Create corrupted JSON file",
        "        with open(ml.CHATS_DIR / \"corrupted.json\", 'w', encoding='utf-8') as f:",
        "            f.write(\"{ invalid json content }\")",
        "",
        "        # Should not raise an error",
        "        result = ml.analyze_data_quality()",
        "",
        "        # Should count only the valid chat",
        "        comp = result['completeness']",
        "        self.assertEqual(comp['chats_total'], 1)",
        "",
        "    def test_missing_directories_handled(self):",
        "        \"\"\"Test that missing directories are handled gracefully.\"\"\"",
        "        # Remove all data directories",
        "        import shutil",
        "        shutil.rmtree(ml.ML_DATA_DIR)",
        "",
        "        # Should not raise an error, just return empty metrics",
        "        result = ml.analyze_data_quality()",
        "",
        "        self.assertEqual(result['completeness']['chats_total'], 0)",
        "        self.assertEqual(result['completeness']['commits_total'], 0)",
        "        self.assertEqual(result['completeness']['sessions_total'], 0)",
        "",
        "    def test_mixed_valid_invalid_data(self):",
        "        \"\"\"Test analysis with mix of valid and invalid data.\"\"\"",
        "        # Valid complete chat",
        "        chat1 = {",
        "            'id': 'chat-001',",
        "            'timestamp': datetime.now().isoformat(),",
        "            'session_id': 'session-001',",
        "            'query': 'Test',",
        "            'response': 'Response',",
        "            'files_referenced': ['/file.py'],",
        "            'files_modified': [],",
        "            'tools_used': ['Read'],",
        "        }",
        "",
        "        # Invalid incomplete chat",
        "        chat2 = {",
        "            'id': 'chat-002',",
        "            'timestamp': datetime.now().isoformat(),",
        "            'query': 'Test',",
        "            # Missing required fields",
        "        }",
        "",
        "        # Valid commit with CI",
        "        commit1 = {",
        "            'hash': 'abc123',",
        "            'message': 'Test',",
        "            'timestamp': datetime.now().isoformat(),",
        "            'files_changed': ['file.py'],",
        "            'ci_result': {'status': 'pass'},",
        "        }",
        "",
        "        # Valid commit without CI",
        "        commit2 = {",
        "            'hash': 'def456',",
        "            'message': 'Test',",
        "            'timestamp': datetime.now().isoformat(),",
        "            'files_changed': ['other.py'],",
        "        }",
        "",
        "        with open(ml.CHATS_DIR / \"chat1.json\", 'w', encoding='utf-8') as f:",
        "            json.dump(chat1, f)",
        "        with open(ml.CHATS_DIR / \"chat2.json\", 'w', encoding='utf-8') as f:",
        "            json.dump(chat2, f)",
        "        with open(ml.COMMITS_DIR / \"commit1.json\", 'w', encoding='utf-8') as f:",
        "            json.dump(commit1, f)",
        "        with open(ml.COMMITS_DIR / \"commit2.json\", 'w', encoding='utf-8') as f:",
        "            json.dump(commit2, f)",
        "",
        "        result = ml.analyze_data_quality()",
        "",
        "        comp = result['completeness']",
        "        self.assertEqual(comp['chats_total'], 2)",
        "        self.assertEqual(comp['chats_complete'], 1)",
        "        self.assertEqual(comp['commits_total'], 2)",
        "        self.assertEqual(comp['commits_with_ci'], 1)",
        "",
        "    def test_quality_score_boundaries(self):",
        "        \"\"\"Test that quality score is always between 0 and 100.\"\"\"",
        "        # Test with various data scenarios",
        "        scenarios = [",
        "            {},  # Empty",
        "            # Will add data in loop",
        "        ]",
        "",
        "        # Scenario 1: Empty data",
        "        result = ml.analyze_data_quality()",
        "        self.assertGreaterEqual(result['quality_score'], 0)",
        "        self.assertLessEqual(result['quality_score'], 100)",
        "",
        "        # Scenario 2: Some data",
        "        chat = {",
        "            'id': 'chat-001',",
        "            'timestamp': datetime.now().isoformat(),",
        "            'session_id': 'session-001',",
        "            'query': 'Test',",
        "            'response': 'Response',",
        "            'files_referenced': [],",
        "            'files_modified': [],",
        "            'tools_used': [],",
        "        }",
        "        with open(ml.CHATS_DIR / \"chat.json\", 'w', encoding='utf-8') as f:",
        "            json.dump(chat, f)",
        "",
        "        result = ml.analyze_data_quality()",
        "        self.assertGreaterEqual(result['quality_score'], 0)",
        "        self.assertLessEqual(result['quality_score'], 100)",
        "",
        "    def test_zero_division_protection(self):",
        "        \"\"\"Test that analysis handles edge cases without division by zero.\"\"\"",
        "        # Empty data should not cause division by zero",
        "        result = ml.analyze_data_quality()",
        "",
        "        # All percentages should be valid numbers",
        "        comp = result['completeness']",
        "        for key in comp:",
        "            if '_pct' in key:",
        "                self.assertIsInstance(comp[key], (int, float))",
        "                self.assertGreaterEqual(comp[key], 0)",
        "",
        "        # Diversity stats should have valid averages",
        "        div = result['diversity']",
        "        self.assertGreaterEqual(div['query_length_avg'], 0)",
        "        self.assertGreaterEqual(div['response_length_avg'], 0)",
        "",
        "    def test_diversity_with_no_lengths(self):",
        "        \"\"\"Test diversity calculation when no query/response data exists.\"\"\"",
        "        # Create commits only (no chats)",
        "        commit = {",
        "            'hash': 'abc123',",
        "            'message': 'Test',",
        "            'timestamp': datetime.now().isoformat(),",
        "            'files_changed': ['file.py'],",
        "        }",
        "        with open(ml.COMMITS_DIR / \"commit.json\", 'w', encoding='utf-8') as f:",
        "            json.dump(commit, f)",
        "",
        "        result = ml.analyze_data_quality()",
        "",
        "        div = result['diversity']",
        "        # Should handle empty length lists gracefully",
        "        self.assertEqual(div['query_length_min'], 0)",
        "        self.assertEqual(div['query_length_avg'], 0)",
        "        self.assertEqual(div['query_length_max'], 0)",
        "        self.assertEqual(div['response_length_min'], 0)",
        "        self.assertEqual(div['response_length_avg'], 0)",
        "        self.assertEqual(div['response_length_max'], 0)",
        "",
        "",
        "if __name__ == \"__main__\":",
        "    unittest.main()"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    }
  ],
  "hour_of_day": 11,
  "day_of_week": "Monday",
  "seconds_since_last_commit": -8165,
  "is_merge": false,
  "is_initial": false,
  "parent_count": 1,
  "session_id": null,
  "related_chats": [],
  "ci_result": null,
  "reverted": false,
  "amended": false
}