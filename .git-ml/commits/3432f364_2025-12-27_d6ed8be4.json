{
  "hash": "3432f3641f2929b1c592850fce0c29978a55179b",
  "message": "fix(gitignore): Track ML training data (commits, chats, sessions, actions)",
  "author": "Claude",
  "timestamp": "2025-12-27 02:32:42 +0000",
  "branch": "claude/accept-handoff-ctrSI",
  "files_changed": [
    ".git-ml/commits/945cd340_2025-12-27_109bdefd.json",
    ".git-ml/commits/9f5413fa_2025-12-27_9579401d.json",
    ".git-ml/commits/HEAD_2025-12-27_039011da.json",
    ".git-ml/commits/b13044b8_2025-12-27_8455cd84.json",
    ".gitignore"
  ],
  "insertions": 1450,
  "deletions": 14,
  "hunks": [
    {
      "file": ".git-ml/commits/945cd340_2025-12-27_109bdefd.json",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "{",
        "  \"hash\": \"945cd340fe4033a8d96ca062de277409e9b09a07\",",
        "  \"message\": \"fix(benchmarks): Add safeguards to prevent accidental model overwrites\",",
        "  \"author\": \"Claude\",",
        "  \"timestamp\": \"2025-12-27 02:08:07 +0000\",",
        "  \"branch\": \"claude/accept-handoff-ctrSI\",",
        "  \"files_changed\": [",
        "    \"benchmarks/codebase_slm/train_augmented.py\",",
        "    \"benchmarks/codebase_slm/train_slm.py\"",
        "  ],",
        "  \"insertions\": 275,",
        "  \"deletions\": 23,",
        "  \"hunks\": [",
        "    {",
        "      \"file\": \"benchmarks/codebase_slm/train_augmented.py\",",
        "      \"function\": null,",
        "      \"start_line\": 1,",
        "      \"lines_added\": [",
        "        \"\",",
        "        \"IMPORTANT: This script saves models to disk. Use --dry-run to evaluate\",",
        "        \"without saving, or --output to specify a custom output path.\",",
        "        \"\",",
        "        \"Usage:\",",
        "        \"    # Dry run (evaluate only, no save)\",",
        "        \"    python -m benchmarks.codebase_slm.train_augmented --dry-run\",",
        "        \"\",",
        "        \"    # Save to custom path (recommended)\",",
        "        \"    python -m benchmarks.codebase_slm.train_augmented --output models/my_model.json\",",
        "        \"\",",
        "        \"    # Save to default path (creates backup first)\",",
        "        \"    python -m benchmarks.codebase_slm.train_augmented\",",
        "        \"\",",
        "        \"    # Force overwrite without backup\",",
        "        \"    python -m benchmarks.codebase_slm.train_augmented --force\",",
        "        \"import argparse\",",
        "        \"import hashlib\",",
        "        \"import shutil\",",
        "        \"from datetime import datetime\",",
        "        \"# Default output path\",",
        "        \"DEFAULT_MODEL_PATH = PROJECT_ROOT / \\\"benchmarks\\\" / \\\"codebase_slm\\\" / \\\"models\\\" / \\\"prism_augmented.json\\\"\",",
        "        \"BACKUP_DIR = PROJECT_ROOT / \\\"benchmarks\\\" / \\\"codebase_slm\\\" / \\\"models\\\" / \\\"backups\\\"\",",
        "        \"\",",
        "        \"        return [], None\",",
        "        \"    return lines, str(corpus_path)\",",
        "        \"        return [], None\"",
        "      ],",
        "      \"lines_removed\": [",
        "        \"        return []\",",
        "        \"    return lines\",",
        "        \"        return []\"",
        "      ],",
        "      \"context_before\": [",
        "        \"#!/usr/bin/env python3\",",
        "        \"\\\"\\\"\\\"\",",
        "        \"Train PRISM-SLM with augmented corpus and run benchmarks.\"",
        "      ],",
        "      \"context_after\": [",
        "        \"\\\"\\\"\\\"\",",
        "        \"\",",
        "        \"import json\",",
        "        \"import sys\",",
        "        \"from pathlib import Path\",",
        "        \"\",",
        "        \"PROJECT_ROOT = Path(__file__).parent.parent.parent\",",
        "        \"sys.path.insert(0, str(PROJECT_ROOT))\",",
        "        \"\",",
        "        \"from cortical.spark import NGramModel\",",
        "        \"\",",
        "        \"\",",
        "        \"def load_augmented_corpus():\",",
        "        \"    \\\"\\\"\\\"Load the augmented training corpus.\\\"\\\"\\\"\",",
        "        \"    corpus_path = PROJECT_ROOT / \\\"benchmarks\\\" / \\\"codebase_slm\\\" / \\\"data\\\" / \\\"augmented_corpus.txt\\\"\",",
        "        \"\",",
        "        \"    if not corpus_path.exists():\",",
        "        \"        print(\\\"ERROR: Augmented corpus not found. Run data_augmentation.py first.\\\")\",",
        "        \"\",",
        "        \"    with open(corpus_path) as f:\",",
        "        \"        lines = [line.strip() for line in f if line.strip()]\",",
        "        \"\",",
        "        \"    print(f\\\"Loaded {len(lines)} augmented training lines\\\")\",",
        "        \"\",",
        "        \"\",",
        "        \"def load_existing_patterns():\",",
        "        \"    \\\"\\\"\\\"Load existing training patterns.\\\"\\\"\\\"\",",
        "        \"    patterns_path = PROJECT_ROOT / \\\"benchmarks\\\" / \\\"codebase_slm\\\" / \\\"corpus\\\" / \\\"training_patterns.jsonl\\\"\",",
        "        \"\",",
        "        \"    if not patterns_path.exists():\",",
        "        \"        print(\\\"No existing patterns found\\\")\",",
        "        \"\",",
        "        \"    patterns = []\",",
        "        \"    with open(patterns_path) as f:\",",
        "        \"        for line in f:\",",
        "        \"            try:\",",
        "        \"                p = json.loads(line)\",",
        "        \"                # Format as training text\",",
        "        \"                ptype = p.get('pattern_type', '')\",",
        "        \"                input_text = p.get('input_text', '')\",",
        "        \"                target = p.get('target_text', '')\"",
        "      ],",
        "      \"change_type\": \"modify\"",
        "    },",
        "    {",
        "      \"file\": \"benchmarks/codebase_slm/train_augmented.py\",",
        "      \"function\": \"def load_existing_patterns():\",",
        "      \"start_line\": 49,",
        "      \"lines_added\": [",
        "        \"    return patterns, str(patterns_path)\"",
        "      ],",
        "      \"lines_removed\": [",
        "        \"    return patterns\"",
        "      ],",
        "      \"context_before\": [",
        "        \"                if ptype == 'qa':\",",
        "        \"                    text = f\\\"Q: {input_text} A: {target}\\\"\",",
        "        \"                else:\",",
        "        \"                    text = f\\\"{input_text} {target}\\\"\",",
        "        \"\",",
        "        \"                patterns.append(text)\",",
        "        \"            except:\",",
        "        \"                continue\",",
        "        \"\",",
        "        \"    print(f\\\"Loaded {len(patterns)} existing patterns\\\")\"",
        "      ],",
        "      \"context_after\": [",
        "        \"\",",
        "        \"\",",
        "        \"def train_model(corpus):\",",
        "        \"    \\\"\\\"\\\"Train NGramModel on combined corpus.\\\"\\\"\\\"\",",
        "        \"    print(f\\\"\\\\nTraining on {len(corpus)} patterns...\\\")\",",
        "        \"\",",
        "        \"    model = NGramModel(n=3)\",",
        "        \"    model.train(corpus)\",",
        "        \"\",",
        "        \"    print(f\\\"  Vocabulary size: {len(model.vocab)}\\\")\"",
        "      ],",
        "      \"change_type\": \"modify\"",
        "    },",
        "    {",
        "      \"file\": \"benchmarks/codebase_slm/train_augmented.py\",",
        "      \"function\": \"def test_model(model):\",",
        "      \"start_line\": 147,",
        "      \"lines_added\": [",
        "        \"def backup_existing_model(model_path: Path) -> Path | None:\",",
        "        \"    \\\"\\\"\\\"Create a backup of the existing model before overwriting.\\\"\\\"\\\"\",",
        "        \"    if not model_path.exists():\",",
        "        \"        return None\",",
        "        \"\",",
        "        \"    BACKUP_DIR.mkdir(parents=True, exist_ok=True)\",",
        "        \"\",",
        "        \"    # Create timestamped backup filename\",",
        "        \"    timestamp = datetime.now().strftime(\\\"%Y%m%d_%H%M%S\\\")\",",
        "        \"    backup_name = f\\\"prism_augmented_{timestamp}.json\\\"\",",
        "        \"    backup_path = BACKUP_DIR / backup_name\",",
        "        \"\",",
        "        \"    shutil.copy2(model_path, backup_path)\",",
        "        \"    print(f\\\"‚ö†Ô∏è  Backed up existing model to: {backup_path}\\\")\",",
        "        \"\",",
        "        \"    # Also keep only last 5 backups to avoid bloat\",",
        "        \"    backups = sorted(BACKUP_DIR.glob(\\\"prism_augmented_*.json\\\"))\",",
        "        \"    if len(backups) > 5:\",",
        "        \"        for old_backup in backups[:-5]:\",",
        "        \"            old_backup.unlink()\",",
        "        \"            print(f\\\"   Removed old backup: {old_backup.name}\\\")\",",
        "        \"\",",
        "        \"    return backup_path\",",
        "        \"\",",
        "        \"\",",
        "        \"def compute_corpus_hash(corpus: list) -> str:\",",
        "        \"    \\\"\\\"\\\"Compute a hash of the training corpus for provenance.\\\"\\\"\\\"\",",
        "        \"    content = \\\"\\\\n\\\".join(sorted(set(corpus)))  # Dedupe and sort for consistency\",",
        "        \"    return hashlib.sha256(content.encode()).hexdigest()[:16]\",",
        "        \"\",",
        "        \"\",",
        "        \"def save_model(model, path: Path, provenance: dict):\",",
        "        \"    \\\"\\\"\\\"Save trained model with provenance metadata.\\\"\\\"\\\"\",",
        "        \"        # Provenance metadata (new!)\",",
        "        \"        '_provenance': {\",",
        "        \"            'trained_at': datetime.now().isoformat(),\",",
        "        \"            'corpus_hash': provenance.get('corpus_hash', 'unknown'),\",",
        "        \"            'corpus_size': provenance.get('corpus_size', 0),\",",
        "        \"            'sources': provenance.get('sources', []),\",",
        "        \"            'script': 'train_augmented.py',\",",
        "        \"            'model_type': 'NGramModel',\",",
        "        \"        },\",",
        "        \"        # Model data\",",
        "        \"    path.parent.mkdir(parents=True, exist_ok=True)\",",
        "        \"\",",
        "        \"    print(f\\\"\\\\n‚úì Model saved to {path}\\\")\",",
        "        \"    print(f\\\"  Corpus hash: {provenance.get('corpus_hash', 'unknown')}\\\")\",",
        "        \"    print(f\\\"  Corpus size: {provenance.get('corpus_size', 0)} patterns\\\")\",",
        "        \"    parser = argparse.ArgumentParser(\",",
        "        \"        description='Train PRISM-SLM with augmented corpus',\",",
        "        \"        formatter_class=argparse.RawDescriptionHelpFormatter,\",",
        "        \"        epilog=\\\"\\\"\\\"\",",
        "        \"Examples:\",",
        "        \"  %(prog)s --dry-run              # Evaluate only, don't save\",",
        "        \"  %(prog)s --output my_model.json # Save to custom path\",",
        "        \"  %(prog)s                        # Save to default (with backup)\",",
        "        \"  %(prog)s --force                # Overwrite without backup\",",
        "        \"        \\\"\\\"\\\"\",",
        "        \"    )\",",
        "        \"    parser.add_argument('--output', '-o', type=str,\",",
        "        \"                        help='Output path for trained model (default: models/prism_augmented.json)')\",",
        "        \"    parser.add_argument('--dry-run', action='store_true',\",",
        "        \"                        help='Evaluate only, do not save model')\",",
        "        \"    parser.add_argument('--force', '-f', action='store_true',\",",
        "        \"                        help='Overwrite existing model without backup')\",",
        "        \"    parser.add_argument('--no-existing', action='store_true',\",",
        "        \"                        help='Train only on augmented corpus, skip existing patterns')\",",
        "        \"    args = parser.parse_args()\",",
        "        \"\",",
        "        \"    if args.dry_run:\",",
        "        \"        print(\\\"‚ö†Ô∏è  DRY RUN MODE - Model will NOT be saved\\\")\",",
        "        \"\",",
        "        \"    augmented, aug_source = load_augmented_corpus()\",",
        "        \"\",",
        "        \"    if args.no_existing:\",",
        "        \"        existing, exist_source = [], None\",",
        "        \"        print(\\\"Skipping existing patterns (--no-existing)\\\")\",",
        "        \"    else:\",",
        "        \"        existing, exist_source = load_existing_patterns()\",",
        "        \"    if not combined:\",",
        "        \"        print(\\\"ERROR: No training data available\\\")\",",
        "        \"        return 1\",",
        "        \"\",",
        "        \"    # Save (unless dry-run)\",",
        "        \"    if not args.dry_run:\",",
        "        \"        # Determine output path\",",
        "        \"        if args.output:\",",
        "        \"            model_path = Path(args.output)\",",
        "        \"            if not model_path.is_absolute():\",",
        "        \"                model_path = PROJECT_ROOT / \\\"benchmarks\\\" / \\\"codebase_slm\\\" / \\\"models\\\" / model_path\",",
        "        \"        else:\",",
        "        \"            model_path = DEFAULT_MODEL_PATH\",",
        "        \"\",",
        "        \"        # Backup existing model (unless --force)\",",
        "        \"        if model_path.exists() and not args.force:\",",
        "        \"            backup_existing_model(model_path)\",",
        "        \"        elif model_path.exists() and args.force:\",",
        "        \"            print(\\\"‚ö†Ô∏è  --force specified, skipping backup\\\")\",",
        "        \"\",",
        "        \"        # Build provenance\",",
        "        \"        sources = []\",",
        "        \"        if aug_source:\",",
        "        \"            sources.append(aug_source)\",",
        "        \"        if exist_source:\",",
        "        \"            sources.append(exist_source)\",",
        "        \"\",",
        "        \"        provenance = {\",",
        "        \"            'corpus_hash': compute_corpus_hash(combined),\",",
        "        \"            'corpus_size': len(combined),\",",
        "        \"            'sources': sources,\",",
        "        \"        }\",",
        "        \"\",",
        "        \"        save_model(model, model_path, provenance)\",",
        "        \"    else:\",",
        "        \"        print(\\\"\\\\n‚ö†Ô∏è  DRY RUN - Model was NOT saved\\\")\"",
        "      ],",
        "      \"lines_removed\": [",
        "        \"def save_model(model, path):\",",
        "        \"    \\\"\\\"\\\"Save trained model.\\\"\\\"\\\"\",",
        "        \"    print(f\\\"\\\\nModel saved to {path}\\\")\",",
        "        \"    augmented = load_augmented_corpus()\",",
        "        \"    existing = load_existing_patterns()\",",
        "        \"    # Save\",",
        "        \"    model_path = PROJECT_ROOT / \\\"benchmarks\\\" / \\\"codebase_slm\\\" / \\\"models\\\" / \\\"prism_augmented.json\\\"\",",
        "        \"    model_path.parent.mkdir(parents=True, exist_ok=True)\",",
        "        \"    save_model(model, model_path)\"",
        "      ],",
        "      \"context_before\": [",
        "        \"    for category, scores in results.items():\",",
        "        \"        avg = sum(scores) / len(scores) if scores else 0\",",
        "        \"        print(f\\\"  {category}: {avg:.0%}\\\")\",",
        "        \"\",",
        "        \"    overall = sum(sum(s) for s in results.values()) / sum(len(s) for s in results.values())\",",
        "        \"    print(f\\\"\\\\n  OVERALL: {overall:.0%}\\\")\",",
        "        \"\",",
        "        \"    return results\",",
        "        \"\",",
        "        \"\"",
        "      ],",
        "      \"context_after\": [",
        "        \"    model_data = {\",",
        "        \"        'vocab': list(model.vocab),\",",
        "        \"        'counts': {\",",
        "        \"            ' '.join(ctx): dict(counter)\",",
        "        \"            for ctx, counter in model.counts.items()\",",
        "        \"        },\",",
        "        \"        'context_totals': {\",",
        "        \"            ' '.join(ctx): total\",",
        "        \"            for ctx, total in model.context_totals.items()\",",
        "        \"        },\",",
        "        \"        'total_tokens': model.total_tokens,\",",
        "        \"        'total_documents': model.total_documents,\",",
        "        \"        'n': model.n,\",",
        "        \"    }\",",
        "        \"\",",
        "        \"    with open(path, 'w') as f:\",",
        "        \"        json.dump(model_data, f, indent=2)\",",
        "        \"\",",
        "        \"\",",
        "        \"\",",
        "        \"def main():\",",
        "        \"    print(\\\"=\\\" * 60)\",",
        "        \"    print(\\\"PRISM-SLM AUGMENTED TRAINING\\\")\",",
        "        \"    print(\\\"=\\\" * 60)\",",
        "        \"\",",
        "        \"    # Load corpora\",",
        "        \"\",",
        "        \"    # Combine (augmented has higher weight due to oversampling)\",",
        "        \"    combined = augmented + existing\",",
        "        \"    print(f\\\"\\\\nTotal training corpus: {len(combined)} patterns\\\")\",",
        "        \"\",",
        "        \"    # Train\",",
        "        \"    model = train_model(combined)\",",
        "        \"\",",
        "        \"    # Test\",",
        "        \"    results = test_model(model)\",",
        "        \"\",",
        "        \"\",",
        "        \"    # Compare with baseline\",",
        "        \"    print(\\\"\\\\n\\\" + \\\"=\\\" * 60)\",",
        "        \"    print(\\\"COMPARISON WITH BASELINE\\\")\",",
        "        \"    print(\\\"=\\\" * 60)\",",
        "        \"\",",
        "        \"    baseline_results = {\",",
        "        \"        'file_location': 0.875,\",",
        "        \"        'concept': 0.0,\",",
        "        \"        'how_to': 0.50,\"",
        "      ],",
        "      \"change_type\": \"modify\"",
        "    },",
        "    {",
        "      \"file\": \"benchmarks/codebase_slm/train_augmented.py\",",
        "      \"function\": \"def main():\",",
        "      \"start_line\": 217,",
        "      \"lines_added\": [",
        "        \"    return 0\",",
        "        \"\",",
        "        \"    sys.exit(main())\"",
        "      ],",
        "      \"lines_removed\": [",
        "        \"    main()\"",
        "      ],",
        "      \"context_before\": [",
        "        \"    print(\\\"\\\\n| Category      | Baseline | Augmented | Change |\\\")\",",
        "        \"    print(\\\"|---------------|----------|-----------|--------|\\\")\",",
        "        \"\",",
        "        \"    for category in ['concept', 'file_location']:\",",
        "        \"        baseline = baseline_results.get(category, 0)\",",
        "        \"        current = sum(results[category]) / len(results[category]) if results.get(category) else 0\",",
        "        \"        change = current - baseline\",",
        "        \"        change_str = f\\\"+{change:.0%}\\\" if change >= 0 else f\\\"{change:.0%}\\\"\",",
        "        \"        print(f\\\"| {category:13} | {baseline:.0%}      | {current:.0%}       | {change_str:6} |\\\")\",",
        "        \"\"",
        "      ],",
        "      \"context_after\": [",
        "        \"\",",
        "        \"if __name__ == \\\"__main__\\\":\"",
        "      ],",
        "      \"change_type\": \"modify\"",
        "    },",
        "    {",
        "      \"file\": \"benchmarks/codebase_slm/train_slm.py\",",
        "      \"function\": null,",
        "      \"start_line\": 1,",
        "      \"lines_added\": [",
        "        \"This script trains a domain-specific SLM that understands\",",
        "        \"IMPORTANT: Use --dry-run to evaluate without saving, or --output to\",",
        "        \"specify a custom output path.\",",
        "        \"\",",
        "        \"    # Quick training (sample corpus) - evaluate only\",",
        "        \"    python -m benchmarks.codebase_slm.train_slm --quick --dry-run\",",
        "        \"    # Full training with model save\",",
        "        \"    python -m benchmarks.codebase_slm.train_slm --full --output prism_full.json\",",
        "        \"    python -m benchmarks.codebase_slm.train_slm --quick --interactive\",",
        "        \"import hashlib\",",
        "        \"import shutil\",",
        "        \"from datetime import datetime\",",
        "        \"from typing import List, Dict, Any, Optional\",",
        "        \"PROJECT_ROOT = Path(__file__).parent.parent.parent\",",
        "        \"sys.path.insert(0, str(PROJECT_ROOT))\",",
        "        \"# Default paths\",",
        "        \"DEFAULT_MODEL_PATH = PROJECT_ROOT / \\\"benchmarks\\\" / \\\"codebase_slm\\\" / \\\"models\\\" / \\\"prism_slm.json\\\"\",",
        "        \"BACKUP_DIR = PROJECT_ROOT / \\\"benchmarks\\\" / \\\"codebase_slm\\\" / \\\"models\\\" / \\\"backups\\\"\",",
        "        \"\"",
        "      ],",
        "      \"lines_removed\": [",
        "        \"This script demonstrates training a domain-specific SLM that understands\",",
        "        \"    # Quick training (sample corpus)\",",
        "        \"    python -m benchmarks.codebase_slm.train_slm --quick\",",
        "        \"    # Full training\",",
        "        \"    python -m benchmarks.codebase_slm.train_slm --full\",",
        "        \"    python -m benchmarks.codebase_slm.train_slm --interactive\",",
        "        \"from typing import List, Dict, Any\",",
        "        \"sys.path.insert(0, str(Path(__file__).parent.parent.parent))\"",
        "      ],",
        "      \"context_before\": [",
        "        \"#!/usr/bin/env python3\",",
        "        \"\\\"\\\"\\\"\",",
        "        \"Train PRISM-SLM on the generated repository corpus.\",",
        "        \"\"",
        "      ],",
        "      \"context_after\": [",
        "        \"the repository's code, documentation, and structure.\",",
        "        \"\",",
        "        \"Usage:\",",
        "        \"\",",
        "        \"\",",
        "        \"    # Interactive mode after training\",",
        "        \"\\\"\\\"\\\"\",",
        "        \"\",",
        "        \"import argparse\",",
        "        \"import json\",",
        "        \"import sys\",",
        "        \"import time\",",
        "        \"from pathlib import Path\",",
        "        \"\",",
        "        \"# Add project root to path\",",
        "        \"\",",
        "        \"from cortical.reasoning.prism_slm import PRISMLanguageModel\",",
        "        \"\",",
        "        \"\",",
        "        \"def load_training_corpus(corpus_path: Path, limit: int = None) -> List[str]:\",",
        "        \"    \\\"\\\"\\\"Load training patterns from corpus file.\",",
        "        \"\",",
        "        \"    Uses Q: / A: format to create clear separation between\",",
        "        \"    questions and answers for n-gram learning.\",",
        "        \"    \\\"\\\"\\\"\",",
        "        \"    patterns = []\",",
        "        \"\",",
        "        \"    if not corpus_path.exists():\"",
        "      ],",
        "      \"change_type\": \"modify\"",
        "    },",
        "    {",
        "      \"file\": \"benchmarks/codebase_slm/train_slm.py\",",
        "      \"function\": \"def interactive_mode(model: PRISMLanguageModel):\",",
        "      \"start_line\": 147,",
        "      \"lines_added\": [",
        "        \"def backup_existing_model(model_path: Path) -> Optional[Path]:\",",
        "        \"    \\\"\\\"\\\"Create a backup of the existing model before overwriting.\\\"\\\"\\\"\",",
        "        \"    if not model_path.exists():\",",
        "        \"        return None\",",
        "        \"\",",
        "        \"    BACKUP_DIR.mkdir(parents=True, exist_ok=True)\",",
        "        \"\",",
        "        \"    # Create timestamped backup filename\",",
        "        \"    timestamp = datetime.now().strftime(\\\"%Y%m%d_%H%M%S\\\")\",",
        "        \"    backup_name = f\\\"{model_path.stem}_{timestamp}.json\\\"\",",
        "        \"    backup_path = BACKUP_DIR / backup_name\",",
        "        \"\",",
        "        \"    shutil.copy2(model_path, backup_path)\",",
        "        \"    print(f\\\"‚ö†Ô∏è  Backed up existing model to: {backup_path}\\\")\",",
        "        \"\",",
        "        \"    # Keep only last 5 backups per model type\",",
        "        \"    pattern = f\\\"{model_path.stem}_*.json\\\"\",",
        "        \"    backups = sorted(BACKUP_DIR.glob(pattern))\",",
        "        \"    if len(backups) > 5:\",",
        "        \"        for old_backup in backups[:-5]:\",",
        "        \"            old_backup.unlink()\",",
        "        \"            print(f\\\"   Removed old backup: {old_backup.name}\\\")\",",
        "        \"\",",
        "        \"    return backup_path\",",
        "        \"\",",
        "        \"\",",
        "        \"def compute_corpus_hash(patterns: List[str]) -> str:\",",
        "        \"    \\\"\\\"\\\"Compute a hash of the training corpus for provenance.\\\"\\\"\\\"\",",
        "        \"    content = \\\"\\\\n\\\".join(sorted(set(patterns)))\",",
        "        \"    return hashlib.sha256(content.encode()).hexdigest()[:16]\",",
        "        \"\",",
        "        \"\",",
        "        \"def save_model(model: PRISMLanguageModel, path: Path, provenance: dict):\",",
        "        \"    \\\"\\\"\\\"Save trained model with provenance metadata.\\\"\\\"\\\"\",",
        "        \"    # Get model internal state\",",
        "        \"    model_data = {\",",
        "        \"        # Provenance metadata\",",
        "        \"        '_provenance': {\",",
        "        \"            'trained_at': datetime.now().isoformat(),\",",
        "        \"            'corpus_hash': provenance.get('corpus_hash', 'unknown'),\",",
        "        \"            'corpus_size': provenance.get('corpus_size', 0),\",",
        "        \"            'corpus_path': provenance.get('corpus_path', 'unknown'),\",",
        "        \"            'script': 'train_slm.py',\",",
        "        \"            'model_type': 'PRISMLanguageModel',\",",
        "        \"            'context_size': model.context_size,\",",
        "        \"        },\",",
        "        \"        # Model data\",",
        "        \"        'vocab_size': model.vocab_size,\",",
        "        \"        'context_size': model.context_size,\",",
        "        \"        'transitions': {\",",
        "        \"            ' '.join(ctx): dict(trans)\",",
        "        \"            for ctx, trans in model.graph._transitions.items()\",",
        "        \"        },\",",
        "        \"    }\",",
        "        \"\",",
        "        \"    path.parent.mkdir(parents=True, exist_ok=True)\",",
        "        \"\",",
        "        \"    with open(path, 'w') as f:\",",
        "        \"        json.dump(model_data, f, indent=2)\",",
        "        \"\",",
        "        \"    print(f\\\"\\\\n‚úì Model saved to {path}\\\")\",",
        "        \"    print(f\\\"  Corpus hash: {provenance.get('corpus_hash', 'unknown')}\\\")\",",
        "        \"    print(f\\\"  Corpus size: {provenance.get('corpus_size', 0)} patterns\\\")\",",
        "        \"\",",
        "        \"\",",
        "        \"    parser = argparse.ArgumentParser(\",",
        "        \"        description='Train PRISM-SLM on repository corpus',\",",
        "        \"        formatter_class=argparse.RawDescriptionHelpFormatter,\",",
        "        \"        epilog=\\\"\\\"\\\"\",",
        "        \"Examples:\",",
        "        \"  %(prog)s --quick --dry-run           # Quick eval, no save\",",
        "        \"  %(prog)s --full --output my_model.json  # Full training, save to file\",",
        "        \"  %(prog)s --quick --interactive       # Quick train + interactive mode\",",
        "        \"        \\\"\\\"\\\"\",",
        "        \"    )\",",
        "        \"    parser.add_argument('--output', '-o', type=str,\",",
        "        \"                        help='Save trained model to this path')\",",
        "        \"    parser.add_argument('--dry-run', action='store_true',\",",
        "        \"                        help='Evaluate only, do not save model')\",",
        "        \"    parser.add_argument('--force', '-f', action='store_true',\",",
        "        \"                        help='Overwrite existing model without backup')\",",
        "        \"    if not corpus_path.is_absolute():\",",
        "        \"        corpus_path = PROJECT_ROOT / corpus_path\",",
        "        \"\",",
        "        \"    if args.dry_run:\",",
        "        \"        print(\\\"‚ö†Ô∏è  DRY RUN MODE - Model will NOT be saved\\\")\"",
        "      ],",
        "      \"lines_removed\": [",
        "        \"    parser = argparse.ArgumentParser(description='Train PRISM-SLM on repository corpus')\"",
        "      ],",
        "      \"context_before\": [",
        "        \"            print()\",",
        "        \"\",",
        "        \"        except KeyboardInterrupt:\",",
        "        \"            break\",",
        "        \"        except EOFError:\",",
        "        \"            break\",",
        "        \"\",",
        "        \"    print(\\\"\\\\nGoodbye!\\\")\",",
        "        \"\",",
        "        \"\"",
        "      ],",
        "      \"context_after\": [",
        "        \"def main():\",",
        "        \"    parser.add_argument('--quick', action='store_true', help='Quick training (1000 patterns)')\",",
        "        \"    parser.add_argument('--full', action='store_true', help='Full training (all patterns)')\",",
        "        \"    parser.add_argument('--interactive', action='store_true', help='Interactive mode after training')\",",
        "        \"    parser.add_argument('--corpus', type=str,\",",
        "        \"                        default='benchmarks/codebase_slm/corpus/training_patterns.jsonl',\",",
        "        \"                        help='Path to training corpus')\",",
        "        \"    parser.add_argument('--context-size', type=int, default=3, help='Context window size')\",",
        "        \"    args = parser.parse_args()\",",
        "        \"\",",
        "        \"    corpus_path = Path(args.corpus)\",",
        "        \"\",",
        "        \"    print(\\\"=\\\" * 60)\",",
        "        \"    print(\\\"Repository-Native SLM Training\\\")\",",
        "        \"    print(\\\"=\\\" * 60)\",",
        "        \"    print()\",",
        "        \"\",",
        "        \"    # Determine pattern limit\",",
        "        \"    limit = 1000 if args.quick else None\",",
        "        \"\",",
        "        \"    # Load corpus\",",
        "        \"    print(f\\\"Loading corpus from {corpus_path}...\\\")\",",
        "        \"    start = time.time()\",",
        "        \"    patterns = load_training_corpus(corpus_path, limit=limit)\",",
        "        \"\"",
        "      ],",
        "      \"change_type\": \"modify\"",
        "    },",
        "    {",
        "      \"file\": \"benchmarks/codebase_slm/train_slm.py\",",
        "      \"function\": \"def main():\",",
        "      \"start_line\": 237,",
        "      \"lines_added\": [",
        "        \"    # Save model if --output specified (and not --dry-run)\",",
        "        \"    if args.output and not args.dry_run:\",",
        "        \"        model_path = Path(args.output)\",",
        "        \"        if not model_path.is_absolute():\",",
        "        \"            model_path = PROJECT_ROOT / \\\"benchmarks\\\" / \\\"codebase_slm\\\" / \\\"models\\\" / model_path\",",
        "        \"\",",
        "        \"        # Backup existing model (unless --force)\",",
        "        \"        if model_path.exists() and not args.force:\",",
        "        \"            backup_existing_model(model_path)\",",
        "        \"        elif model_path.exists() and args.force:\",",
        "        \"            print(\\\"‚ö†Ô∏è  --force specified, skipping backup\\\")\",",
        "        \"\",",
        "        \"        provenance = {\",",
        "        \"            'corpus_hash': compute_corpus_hash(patterns),\",",
        "        \"            'corpus_size': len(patterns),\",",
        "        \"            'corpus_path': str(corpus_path),\",",
        "        \"        }\",",
        "        \"\",",
        "        \"        save_model(model, model_path, provenance)\",",
        "        \"    elif args.output and args.dry_run:\",",
        "        \"        print(\\\"\\\\n‚ö†Ô∏è  DRY RUN - Model was NOT saved (would save to: {})\\\".format(args.output))\",",
        "        \"    elif not args.output and not args.dry_run:\",",
        "        \"        print(\\\"\\\\nüí° Tip: Use --output <path> to save the trained model\\\")\",",
        "        \"\"",
        "      ],",
        "      \"lines_removed\": [],",
        "      \"context_before\": [",
        "        \"        print(f\\\"\\\\n[{q['category']}] {status}\\\")\",",
        "        \"        print(f\\\"  Prompt: {q['prompt']}\\\")\",",
        "        \"        print(f\\\"  Generated: {generated}\\\")\",",
        "        \"        print(f\\\"  Expected terms: {q['expected']}\\\")\",",
        "        \"        print(f\\\"  Match: {matches}/{len(expected_terms)} terms ({match_pct:.0f}%)\\\")\",",
        "        \"\",",
        "        \"    print(f\\\"\\\\n{'=' * 60}\\\")\",",
        "        \"    print(f\\\"RESULTS: {correct}/{len(test_queries)} queries matched (‚â•50% of terms)\\\")\",",
        "        \"    print(f\\\"{'=' * 60}\\\")\",",
        "        \"\"",
        "      ],",
        "      \"context_after\": [",
        "        \"    # Interactive mode\",",
        "        \"    if args.interactive:\",",
        "        \"        interactive_mode(model)\",",
        "        \"\",",
        "        \"    return 0\",",
        "        \"\",",
        "        \"\",",
        "        \"if __name__ == '__main__':\",",
        "        \"    sys.exit(main())\"",
        "      ],",
        "      \"change_type\": \"add\"",
        "    }",
        "  ],",
        "  \"hour_of_day\": 2,",
        "  \"day_of_week\": \"Saturday\",",
        "  \"seconds_since_last_commit\": 677,",
        "  \"is_merge\": false,",
        "  \"is_initial\": false,",
        "  \"parent_count\": 1,",
        "  \"session_id\": \"c7e7f25e\",",
        "  \"related_chats\": [",
        "    \"chat-20251227-015100-015a56\"",
        "  ],",
        "  \"ci_result\": null,",
        "  \"reverted\": false,",
        "  \"amended\": false",
        "}"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": ".git-ml/commits/9f5413fa_2025-12-27_9579401d.json",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "{",
        "  \"hash\": \"9f5413fae0460d2ad29b0058ca5c9204633dd947\",",
        "  \"message\": \"fix(gitignore): Add PRISM model backups directory to ignore list\",",
        "  \"author\": \"Claude\",",
        "  \"timestamp\": \"2025-12-27 02:29:03 +0000\",",
        "  \"branch\": \"claude/accept-handoff-ctrSI\",",
        "  \"files_changed\": [",
        "    \".gitignore\"",
        "  ],",
        "  \"insertions\": 4,",
        "  \"deletions\": 0,",
        "  \"hunks\": [",
        "    {",
        "      \"file\": \".gitignore\",",
        "      \"function\": \"corpus_dev/\",",
        "      \"start_line\": 154,",
        "      \"lines_added\": [",
        "        \"\",",
        "        \"# PRISM model backups (created by train_augmented.py safeguards)\",",
        "        \"# These are local recovery copies, not meant for git\",",
        "        \"benchmarks/codebase_slm/models/backups/\"",
        "      ],",
        "      \"lines_removed\": [],",
        "      \"context_before\": [",
        "        \"\",",
        "        \"# GoT migration artifacts\",",
        "        \".got-backup-*/\",",
        "        \".got-tx/\",",
        "        \".got/.got.lock\",",
        "        \".got/**/*.lock\",",
        "        \".spark_intelligence_model.json\",",
        "        \"\",",
        "        \"# Generated training corpus (regenerate with: python -m benchmarks.codebase_slm.generate_corpus --full)\",",
        "        \"benchmarks/codebase_slm/corpus/\"",
        "      ],",
        "      \"context_after\": [],",
        "      \"change_type\": \"add\"",
        "    }",
        "  ],",
        "  \"hour_of_day\": 2,",
        "  \"day_of_week\": \"Saturday\",",
        "  \"seconds_since_last_commit\": 484,",
        "  \"is_merge\": false,",
        "  \"is_initial\": false,",
        "  \"parent_count\": 1,",
        "  \"session_id\": \"c7e7f25e\",",
        "  \"related_chats\": [",
        "    \"chat-20251227-015100-015a56\"",
        "  ],",
        "  \"ci_result\": null,",
        "  \"reverted\": false,",
        "  \"amended\": false",
        "}"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": ".git-ml/commits/HEAD_2025-12-27_039011da.json",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "{",
        "  \"hash\": \"HEAD\",",
        "  \"message\": \"fix(benchmarks): Add safeguards to prevent accidental model overwrites\",",
        "  \"author\": \"Claude\",",
        "  \"timestamp\": \"2025-12-27 02:08:07 +0000\",",
        "  \"branch\": \"claude/accept-handoff-ctrSI\",",
        "  \"files_changed\": [",
        "    \"benchmarks/codebase_slm/train_augmented.py\",",
        "    \"benchmarks/codebase_slm/train_slm.py\"",
        "  ],",
        "  \"insertions\": 275,",
        "  \"deletions\": 23,",
        "  \"hunks\": [",
        "    {",
        "      \"file\": \"benchmarks/codebase_slm/train_augmented.py\",",
        "      \"function\": null,",
        "      \"start_line\": 1,",
        "      \"lines_added\": [",
        "        \"\",",
        "        \"IMPORTANT: This script saves models to disk. Use --dry-run to evaluate\",",
        "        \"without saving, or --output to specify a custom output path.\",",
        "        \"\",",
        "        \"Usage:\",",
        "        \"    # Dry run (evaluate only, no save)\",",
        "        \"    python -m benchmarks.codebase_slm.train_augmented --dry-run\",",
        "        \"\",",
        "        \"    # Save to custom path (recommended)\",",
        "        \"    python -m benchmarks.codebase_slm.train_augmented --output models/my_model.json\",",
        "        \"\",",
        "        \"    # Save to default path (creates backup first)\",",
        "        \"    python -m benchmarks.codebase_slm.train_augmented\",",
        "        \"\",",
        "        \"    # Force overwrite without backup\",",
        "        \"    python -m benchmarks.codebase_slm.train_augmented --force\",",
        "        \"import argparse\",",
        "        \"import hashlib\",",
        "        \"import shutil\",",
        "        \"from datetime import datetime\",",
        "        \"# Default output path\",",
        "        \"DEFAULT_MODEL_PATH = PROJECT_ROOT / \\\"benchmarks\\\" / \\\"codebase_slm\\\" / \\\"models\\\" / \\\"prism_augmented.json\\\"\",",
        "        \"BACKUP_DIR = PROJECT_ROOT / \\\"benchmarks\\\" / \\\"codebase_slm\\\" / \\\"models\\\" / \\\"backups\\\"\",",
        "        \"\",",
        "        \"        return [], None\",",
        "        \"    return lines, str(corpus_path)\",",
        "        \"        return [], None\"",
        "      ],",
        "      \"lines_removed\": [",
        "        \"        return []\",",
        "        \"    return lines\",",
        "        \"        return []\"",
        "      ],",
        "      \"context_before\": [",
        "        \"#!/usr/bin/env python3\",",
        "        \"\\\"\\\"\\\"\",",
        "        \"Train PRISM-SLM with augmented corpus and run benchmarks.\"",
        "      ],",
        "      \"context_after\": [",
        "        \"\\\"\\\"\\\"\",",
        "        \"\",",
        "        \"import json\",",
        "        \"import sys\",",
        "        \"from pathlib import Path\",",
        "        \"\",",
        "        \"PROJECT_ROOT = Path(__file__).parent.parent.parent\",",
        "        \"sys.path.insert(0, str(PROJECT_ROOT))\",",
        "        \"\",",
        "        \"from cortical.spark import NGramModel\",",
        "        \"\",",
        "        \"\",",
        "        \"def load_augmented_corpus():\",",
        "        \"    \\\"\\\"\\\"Load the augmented training corpus.\\\"\\\"\\\"\",",
        "        \"    corpus_path = PROJECT_ROOT / \\\"benchmarks\\\" / \\\"codebase_slm\\\" / \\\"data\\\" / \\\"augmented_corpus.txt\\\"\",",
        "        \"\",",
        "        \"    if not corpus_path.exists():\",",
        "        \"        print(\\\"ERROR: Augmented corpus not found. Run data_augmentation.py first.\\\")\",",
        "        \"\",",
        "        \"    with open(corpus_path) as f:\",",
        "        \"        lines = [line.strip() for line in f if line.strip()]\",",
        "        \"\",",
        "        \"    print(f\\\"Loaded {len(lines)} augmented training lines\\\")\",",
        "        \"\",",
        "        \"\",",
        "        \"def load_existing_patterns():\",",
        "        \"    \\\"\\\"\\\"Load existing training patterns.\\\"\\\"\\\"\",",
        "        \"    patterns_path = PROJECT_ROOT / \\\"benchmarks\\\" / \\\"codebase_slm\\\" / \\\"corpus\\\" / \\\"training_patterns.jsonl\\\"\",",
        "        \"\",",
        "        \"    if not patterns_path.exists():\",",
        "        \"        print(\\\"No existing patterns found\\\")\",",
        "        \"\",",
        "        \"    patterns = []\",",
        "        \"    with open(patterns_path) as f:\",",
        "        \"        for line in f:\",",
        "        \"            try:\",",
        "        \"                p = json.loads(line)\",",
        "        \"                # Format as training text\",",
        "        \"                ptype = p.get('pattern_type', '')\",",
        "        \"                input_text = p.get('input_text', '')\",",
        "        \"                target = p.get('target_text', '')\"",
        "      ],",
        "      \"change_type\": \"modify\"",
        "    },",
        "    {",
        "      \"file\": \"benchmarks/codebase_slm/train_augmented.py\",",
        "      \"function\": \"def load_existing_patterns():\",",
        "      \"start_line\": 49,",
        "      \"lines_added\": [",
        "        \"    return patterns, str(patterns_path)\"",
        "      ],",
        "      \"lines_removed\": [",
        "        \"    return patterns\"",
        "      ],",
        "      \"context_before\": [",
        "        \"                if ptype == 'qa':\",",
        "        \"                    text = f\\\"Q: {input_text} A: {target}\\\"\",",
        "        \"                else:\",",
        "        \"                    text = f\\\"{input_text} {target}\\\"\",",
        "        \"\",",
        "        \"                patterns.append(text)\",",
        "        \"            except:\",",
        "        \"                continue\",",
        "        \"\",",
        "        \"    print(f\\\"Loaded {len(patterns)} existing patterns\\\")\"",
        "      ],",
        "      \"context_after\": [",
        "        \"\",",
        "        \"\",",
        "        \"def train_model(corpus):\",",
        "        \"    \\\"\\\"\\\"Train NGramModel on combined corpus.\\\"\\\"\\\"\",",
        "        \"    print(f\\\"\\\\nTraining on {len(corpus)} patterns...\\\")\",",
        "        \"\",",
        "        \"    model = NGramModel(n=3)\",",
        "        \"    model.train(corpus)\",",
        "        \"\",",
        "        \"    print(f\\\"  Vocabulary size: {len(model.vocab)}\\\")\"",
        "      ],",
        "      \"change_type\": \"modify\"",
        "    },",
        "    {",
        "      \"file\": \"benchmarks/codebase_slm/train_augmented.py\",",
        "      \"function\": \"def test_model(model):\",",
        "      \"start_line\": 147,",
        "      \"lines_added\": [",
        "        \"def backup_existing_model(model_path: Path) -> Path | None:\",",
        "        \"    \\\"\\\"\\\"Create a backup of the existing model before overwriting.\\\"\\\"\\\"\",",
        "        \"    if not model_path.exists():\",",
        "        \"        return None\",",
        "        \"\",",
        "        \"    BACKUP_DIR.mkdir(parents=True, exist_ok=True)\",",
        "        \"\",",
        "        \"    # Create timestamped backup filename\",",
        "        \"    timestamp = datetime.now().strftime(\\\"%Y%m%d_%H%M%S\\\")\",",
        "        \"    backup_name = f\\\"prism_augmented_{timestamp}.json\\\"\",",
        "        \"    backup_path = BACKUP_DIR / backup_name\",",
        "        \"\",",
        "        \"    shutil.copy2(model_path, backup_path)\",",
        "        \"    print(f\\\"‚ö†Ô∏è  Backed up existing model to: {backup_path}\\\")\",",
        "        \"\",",
        "        \"    # Also keep only last 5 backups to avoid bloat\",",
        "        \"    backups = sorted(BACKUP_DIR.glob(\\\"prism_augmented_*.json\\\"))\",",
        "        \"    if len(backups) > 5:\",",
        "        \"        for old_backup in backups[:-5]:\",",
        "        \"            old_backup.unlink()\",",
        "        \"            print(f\\\"   Removed old backup: {old_backup.name}\\\")\",",
        "        \"\",",
        "        \"    return backup_path\",",
        "        \"\",",
        "        \"\",",
        "        \"def compute_corpus_hash(corpus: list) -> str:\",",
        "        \"    \\\"\\\"\\\"Compute a hash of the training corpus for provenance.\\\"\\\"\\\"\",",
        "        \"    content = \\\"\\\\n\\\".join(sorted(set(corpus)))  # Dedupe and sort for consistency\",",
        "        \"    return hashlib.sha256(content.encode()).hexdigest()[:16]\",",
        "        \"\",",
        "        \"\",",
        "        \"def save_model(model, path: Path, provenance: dict):\",",
        "        \"    \\\"\\\"\\\"Save trained model with provenance metadata.\\\"\\\"\\\"\",",
        "        \"        # Provenance metadata (new!)\",",
        "        \"        '_provenance': {\",",
        "        \"            'trained_at': datetime.now().isoformat(),\",",
        "        \"            'corpus_hash': provenance.get('corpus_hash', 'unknown'),\",",
        "        \"            'corpus_size': provenance.get('corpus_size', 0),\",",
        "        \"            'sources': provenance.get('sources', []),\",",
        "        \"            'script': 'train_augmented.py',\",",
        "        \"            'model_type': 'NGramModel',\",",
        "        \"        },\",",
        "        \"        # Model data\",",
        "        \"    path.parent.mkdir(parents=True, exist_ok=True)\",",
        "        \"\",",
        "        \"    print(f\\\"\\\\n‚úì Model saved to {path}\\\")\",",
        "        \"    print(f\\\"  Corpus hash: {provenance.get('corpus_hash', 'unknown')}\\\")\",",
        "        \"    print(f\\\"  Corpus size: {provenance.get('corpus_size', 0)} patterns\\\")\",",
        "        \"    parser = argparse.ArgumentParser(\",",
        "        \"        description='Train PRISM-SLM with augmented corpus',\",",
        "        \"        formatter_class=argparse.RawDescriptionHelpFormatter,\",",
        "        \"        epilog=\\\"\\\"\\\"\",",
        "        \"Examples:\",",
        "        \"  %(prog)s --dry-run              # Evaluate only, don't save\",",
        "        \"  %(prog)s --output my_model.json # Save to custom path\",",
        "        \"  %(prog)s                        # Save to default (with backup)\",",
        "        \"  %(prog)s --force                # Overwrite without backup\",",
        "        \"        \\\"\\\"\\\"\",",
        "        \"    )\",",
        "        \"    parser.add_argument('--output', '-o', type=str,\",",
        "        \"                        help='Output path for trained model (default: models/prism_augmented.json)')\",",
        "        \"    parser.add_argument('--dry-run', action='store_true',\",",
        "        \"                        help='Evaluate only, do not save model')\",",
        "        \"    parser.add_argument('--force', '-f', action='store_true',\",",
        "        \"                        help='Overwrite existing model without backup')\",",
        "        \"    parser.add_argument('--no-existing', action='store_true',\",",
        "        \"                        help='Train only on augmented corpus, skip existing patterns')\",",
        "        \"    args = parser.parse_args()\",",
        "        \"\",",
        "        \"    if args.dry_run:\",",
        "        \"        print(\\\"‚ö†Ô∏è  DRY RUN MODE - Model will NOT be saved\\\")\",",
        "        \"\",",
        "        \"    augmented, aug_source = load_augmented_corpus()\",",
        "        \"\",",
        "        \"    if args.no_existing:\",",
        "        \"        existing, exist_source = [], None\",",
        "        \"        print(\\\"Skipping existing patterns (--no-existing)\\\")\",",
        "        \"    else:\",",
        "        \"        existing, exist_source = load_existing_patterns()\",",
        "        \"    if not combined:\",",
        "        \"        print(\\\"ERROR: No training data available\\\")\",",
        "        \"        return 1\",",
        "        \"\",",
        "        \"    # Save (unless dry-run)\",",
        "        \"    if not args.dry_run:\",",
        "        \"        # Determine output path\",",
        "        \"        if args.output:\",",
        "        \"            model_path = Path(args.output)\",",
        "        \"            if not model_path.is_absolute():\",",
        "        \"                model_path = PROJECT_ROOT / \\\"benchmarks\\\" / \\\"codebase_slm\\\" / \\\"models\\\" / model_path\",",
        "        \"        else:\",",
        "        \"            model_path = DEFAULT_MODEL_PATH\",",
        "        \"\",",
        "        \"        # Backup existing model (unless --force)\",",
        "        \"        if model_path.exists() and not args.force:\",",
        "        \"            backup_existing_model(model_path)\",",
        "        \"        elif model_path.exists() and args.force:\",",
        "        \"            print(\\\"‚ö†Ô∏è  --force specified, skipping backup\\\")\",",
        "        \"\",",
        "        \"        # Build provenance\",",
        "        \"        sources = []\",",
        "        \"        if aug_source:\",",
        "        \"            sources.append(aug_source)\",",
        "        \"        if exist_source:\",",
        "        \"            sources.append(exist_source)\",",
        "        \"\",",
        "        \"        provenance = {\",",
        "        \"            'corpus_hash': compute_corpus_hash(combined),\",",
        "        \"            'corpus_size': len(combined),\",",
        "        \"            'sources': sources,\",",
        "        \"        }\",",
        "        \"\",",
        "        \"        save_model(model, model_path, provenance)\",",
        "        \"    else:\",",
        "        \"        print(\\\"\\\\n‚ö†Ô∏è  DRY RUN - Model was NOT saved\\\")\"",
        "      ],",
        "      \"lines_removed\": [",
        "        \"def save_model(model, path):\",",
        "        \"    \\\"\\\"\\\"Save trained model.\\\"\\\"\\\"\",",
        "        \"    print(f\\\"\\\\nModel saved to {path}\\\")\",",
        "        \"    augmented = load_augmented_corpus()\",",
        "        \"    existing = load_existing_patterns()\",",
        "        \"    # Save\",",
        "        \"    model_path = PROJECT_ROOT / \\\"benchmarks\\\" / \\\"codebase_slm\\\" / \\\"models\\\" / \\\"prism_augmented.json\\\"\",",
        "        \"    model_path.parent.mkdir(parents=True, exist_ok=True)\",",
        "        \"    save_model(model, model_path)\"",
        "      ],",
        "      \"context_before\": [",
        "        \"    for category, scores in results.items():\",",
        "        \"        avg = sum(scores) / len(scores) if scores else 0\",",
        "        \"        print(f\\\"  {category}: {avg:.0%}\\\")\",",
        "        \"\",",
        "        \"    overall = sum(sum(s) for s in results.values()) / sum(len(s) for s in results.values())\",",
        "        \"    print(f\\\"\\\\n  OVERALL: {overall:.0%}\\\")\",",
        "        \"\",",
        "        \"    return results\",",
        "        \"\",",
        "        \"\"",
        "      ],",
        "      \"context_after\": [",
        "        \"    model_data = {\",",
        "        \"        'vocab': list(model.vocab),\",",
        "        \"        'counts': {\",",
        "        \"            ' '.join(ctx): dict(counter)\",",
        "        \"            for ctx, counter in model.counts.items()\",",
        "        \"        },\",",
        "        \"        'context_totals': {\",",
        "        \"            ' '.join(ctx): total\",",
        "        \"            for ctx, total in model.context_totals.items()\",",
        "        \"        },\",",
        "        \"        'total_tokens': model.total_tokens,\",",
        "        \"        'total_documents': model.total_documents,\",",
        "        \"        'n': model.n,\",",
        "        \"    }\",",
        "        \"\",",
        "        \"    with open(path, 'w') as f:\",",
        "        \"        json.dump(model_data, f, indent=2)\",",
        "        \"\",",
        "        \"\",",
        "        \"\",",
        "        \"def main():\",",
        "        \"    print(\\\"=\\\" * 60)\",",
        "        \"    print(\\\"PRISM-SLM AUGMENTED TRAINING\\\")\",",
        "        \"    print(\\\"=\\\" * 60)\",",
        "        \"\",",
        "        \"    # Load corpora\",",
        "        \"\",",
        "        \"    # Combine (augmented has higher weight due to oversampling)\",",
        "        \"    combined = augmented + existing\",",
        "        \"    print(f\\\"\\\\nTotal training corpus: {len(combined)} patterns\\\")\",",
        "        \"\",",
        "        \"    # Train\",",
        "        \"    model = train_model(combined)\",",
        "        \"\",",
        "        \"    # Test\",",
        "        \"    results = test_model(model)\",",
        "        \"\",",
        "        \"\",",
        "        \"    # Compare with baseline\",",
        "        \"    print(\\\"\\\\n\\\" + \\\"=\\\" * 60)\",",
        "        \"    print(\\\"COMPARISON WITH BASELINE\\\")\",",
        "        \"    print(\\\"=\\\" * 60)\",",
        "        \"\",",
        "        \"    baseline_results = {\",",
        "        \"        'file_location': 0.875,\",",
        "        \"        'concept': 0.0,\",",
        "        \"        'how_to': 0.50,\"",
        "      ],",
        "      \"change_type\": \"modify\"",
        "    },",
        "    {",
        "      \"file\": \"benchmarks/codebase_slm/train_augmented.py\",",
        "      \"function\": \"def main():\",",
        "      \"start_line\": 217,",
        "      \"lines_added\": [",
        "        \"    return 0\",",
        "        \"\",",
        "        \"    sys.exit(main())\"",
        "      ],",
        "      \"lines_removed\": [",
        "        \"    main()\"",
        "      ],",
        "      \"context_before\": [",
        "        \"    print(\\\"\\\\n| Category      | Baseline | Augmented | Change |\\\")\",",
        "        \"    print(\\\"|---------------|----------|-----------|--------|\\\")\",",
        "        \"\",",
        "        \"    for category in ['concept', 'file_location']:\",",
        "        \"        baseline = baseline_results.get(category, 0)\",",
        "        \"        current = sum(results[category]) / len(results[category]) if results.get(category) else 0\",",
        "        \"        change = current - baseline\",",
        "        \"        change_str = f\\\"+{change:.0%}\\\" if change >= 0 else f\\\"{change:.0%}\\\"\",",
        "        \"        print(f\\\"| {category:13} | {baseline:.0%}      | {current:.0%}       | {change_str:6} |\\\")\",",
        "        \"\"",
        "      ],",
        "      \"context_after\": [",
        "        \"\",",
        "        \"if __name__ == \\\"__main__\\\":\"",
        "      ],",
        "      \"change_type\": \"modify\"",
        "    },",
        "    {",
        "      \"file\": \"benchmarks/codebase_slm/train_slm.py\",",
        "      \"function\": null,",
        "      \"start_line\": 1,",
        "      \"lines_added\": [",
        "        \"This script trains a domain-specific SLM that understands\",",
        "        \"IMPORTANT: Use --dry-run to evaluate without saving, or --output to\",",
        "        \"specify a custom output path.\",",
        "        \"\",",
        "        \"    # Quick training (sample corpus) - evaluate only\",",
        "        \"    python -m benchmarks.codebase_slm.train_slm --quick --dry-run\",",
        "        \"    # Full training with model save\",",
        "        \"    python -m benchmarks.codebase_slm.train_slm --full --output prism_full.json\",",
        "        \"    python -m benchmarks.codebase_slm.train_slm --quick --interactive\",",
        "        \"import hashlib\",",
        "        \"import shutil\",",
        "        \"from datetime import datetime\",",
        "        \"from typing import List, Dict, Any, Optional\",",
        "        \"PROJECT_ROOT = Path(__file__).parent.parent.parent\",",
        "        \"sys.path.insert(0, str(PROJECT_ROOT))\",",
        "        \"# Default paths\",",
        "        \"DEFAULT_MODEL_PATH = PROJECT_ROOT / \\\"benchmarks\\\" / \\\"codebase_slm\\\" / \\\"models\\\" / \\\"prism_slm.json\\\"\",",
        "        \"BACKUP_DIR = PROJECT_ROOT / \\\"benchmarks\\\" / \\\"codebase_slm\\\" / \\\"models\\\" / \\\"backups\\\"\",",
        "        \"\"",
        "      ],",
        "      \"lines_removed\": [",
        "        \"This script demonstrates training a domain-specific SLM that understands\",",
        "        \"    # Quick training (sample corpus)\",",
        "        \"    python -m benchmarks.codebase_slm.train_slm --quick\",",
        "        \"    # Full training\",",
        "        \"    python -m benchmarks.codebase_slm.train_slm --full\",",
        "        \"    python -m benchmarks.codebase_slm.train_slm --interactive\",",
        "        \"from typing import List, Dict, Any\",",
        "        \"sys.path.insert(0, str(Path(__file__).parent.parent.parent))\"",
        "      ],",
        "      \"context_before\": [",
        "        \"#!/usr/bin/env python3\",",
        "        \"\\\"\\\"\\\"\",",
        "        \"Train PRISM-SLM on the generated repository corpus.\",",
        "        \"\"",
        "      ],",
        "      \"context_after\": [",
        "        \"the repository's code, documentation, and structure.\",",
        "        \"\",",
        "        \"Usage:\",",
        "        \"\",",
        "        \"\",",
        "        \"    # Interactive mode after training\",",
        "        \"\\\"\\\"\\\"\",",
        "        \"\",",
        "        \"import argparse\",",
        "        \"import json\",",
        "        \"import sys\",",
        "        \"import time\",",
        "        \"from pathlib import Path\",",
        "        \"\",",
        "        \"# Add project root to path\",",
        "        \"\",",
        "        \"from cortical.reasoning.prism_slm import PRISMLanguageModel\",",
        "        \"\",",
        "        \"\",",
        "        \"def load_training_corpus(corpus_path: Path, limit: int = None) -> List[str]:\",",
        "        \"    \\\"\\\"\\\"Load training patterns from corpus file.\",",
        "        \"\",",
        "        \"    Uses Q: / A: format to create clear separation between\",",
        "        \"    questions and answers for n-gram learning.\",",
        "        \"    \\\"\\\"\\\"\",",
        "        \"    patterns = []\",",
        "        \"\",",
        "        \"    if not corpus_path.exists():\"",
        "      ],",
        "      \"change_type\": \"modify\"",
        "    },",
        "    {",
        "      \"file\": \"benchmarks/codebase_slm/train_slm.py\",",
        "      \"function\": \"def interactive_mode(model: PRISMLanguageModel):\",",
        "      \"start_line\": 147,",
        "      \"lines_added\": [",
        "        \"def backup_existing_model(model_path: Path) -> Optional[Path]:\",",
        "        \"    \\\"\\\"\\\"Create a backup of the existing model before overwriting.\\\"\\\"\\\"\",",
        "        \"    if not model_path.exists():\",",
        "        \"        return None\",",
        "        \"\",",
        "        \"    BACKUP_DIR.mkdir(parents=True, exist_ok=True)\",",
        "        \"\",",
        "        \"    # Create timestamped backup filename\",",
        "        \"    timestamp = datetime.now().strftime(\\\"%Y%m%d_%H%M%S\\\")\",",
        "        \"    backup_name = f\\\"{model_path.stem}_{timestamp}.json\\\"\",",
        "        \"    backup_path = BACKUP_DIR / backup_name\",",
        "        \"\",",
        "        \"    shutil.copy2(model_path, backup_path)\",",
        "        \"    print(f\\\"‚ö†Ô∏è  Backed up existing model to: {backup_path}\\\")\",",
        "        \"\",",
        "        \"    # Keep only last 5 backups per model type\",",
        "        \"    pattern = f\\\"{model_path.stem}_*.json\\\"\",",
        "        \"    backups = sorted(BACKUP_DIR.glob(pattern))\",",
        "        \"    if len(backups) > 5:\",",
        "        \"        for old_backup in backups[:-5]:\",",
        "        \"            old_backup.unlink()\",",
        "        \"            print(f\\\"   Removed old backup: {old_backup.name}\\\")\",",
        "        \"\",",
        "        \"    return backup_path\",",
        "        \"\",",
        "        \"\",",
        "        \"def compute_corpus_hash(patterns: List[str]) -> str:\",",
        "        \"    \\\"\\\"\\\"Compute a hash of the training corpus for provenance.\\\"\\\"\\\"\",",
        "        \"    content = \\\"\\\\n\\\".join(sorted(set(patterns)))\",",
        "        \"    return hashlib.sha256(content.encode()).hexdigest()[:16]\",",
        "        \"\",",
        "        \"\",",
        "        \"def save_model(model: PRISMLanguageModel, path: Path, provenance: dict):\",",
        "        \"    \\\"\\\"\\\"Save trained model with provenance metadata.\\\"\\\"\\\"\",",
        "        \"    # Get model internal state\",",
        "        \"    model_data = {\",",
        "        \"        # Provenance metadata\",",
        "        \"        '_provenance': {\",",
        "        \"            'trained_at': datetime.now().isoformat(),\",",
        "        \"            'corpus_hash': provenance.get('corpus_hash', 'unknown'),\",",
        "        \"            'corpus_size': provenance.get('corpus_size', 0),\",",
        "        \"            'corpus_path': provenance.get('corpus_path', 'unknown'),\",",
        "        \"            'script': 'train_slm.py',\",",
        "        \"            'model_type': 'PRISMLanguageModel',\",",
        "        \"            'context_size': model.context_size,\",",
        "        \"        },\",",
        "        \"        # Model data\",",
        "        \"        'vocab_size': model.vocab_size,\",",
        "        \"        'context_size': model.context_size,\",",
        "        \"        'transitions': {\",",
        "        \"            ' '.join(ctx): dict(trans)\",",
        "        \"            for ctx, trans in model.graph._transitions.items()\",",
        "        \"        },\",",
        "        \"    }\",",
        "        \"\",",
        "        \"    path.parent.mkdir(parents=True, exist_ok=True)\",",
        "        \"\",",
        "        \"    with open(path, 'w') as f:\",",
        "        \"        json.dump(model_data, f, indent=2)\",",
        "        \"\",",
        "        \"    print(f\\\"\\\\n‚úì Model saved to {path}\\\")\",",
        "        \"    print(f\\\"  Corpus hash: {provenance.get('corpus_hash', 'unknown')}\\\")\",",
        "        \"    print(f\\\"  Corpus size: {provenance.get('corpus_size', 0)} patterns\\\")\",",
        "        \"\",",
        "        \"\",",
        "        \"    parser = argparse.ArgumentParser(\",",
        "        \"        description='Train PRISM-SLM on repository corpus',\",",
        "        \"        formatter_class=argparse.RawDescriptionHelpFormatter,\",",
        "        \"        epilog=\\\"\\\"\\\"\",",
        "        \"Examples:\",",
        "        \"  %(prog)s --quick --dry-run           # Quick eval, no save\",",
        "        \"  %(prog)s --full --output my_model.json  # Full training, save to file\",",
        "        \"  %(prog)s --quick --interactive       # Quick train + interactive mode\",",
        "        \"        \\\"\\\"\\\"\",",
        "        \"    )\",",
        "        \"    parser.add_argument('--output', '-o', type=str,\",",
        "        \"                        help='Save trained model to this path')\",",
        "        \"    parser.add_argument('--dry-run', action='store_true',\",",
        "        \"                        help='Evaluate only, do not save model')\",",
        "        \"    parser.add_argument('--force', '-f', action='store_true',\",",
        "        \"                        help='Overwrite existing model without backup')\",",
        "        \"    if not corpus_path.is_absolute():\",",
        "        \"        corpus_path = PROJECT_ROOT / corpus_path\",",
        "        \"\",",
        "        \"    if args.dry_run:\",",
        "        \"        print(\\\"‚ö†Ô∏è  DRY RUN MODE - Model will NOT be saved\\\")\"",
        "      ],",
        "      \"lines_removed\": [",
        "        \"    parser = argparse.ArgumentParser(description='Train PRISM-SLM on repository corpus')\"",
        "      ],",
        "      \"context_before\": [",
        "        \"            print()\",",
        "        \"\",",
        "        \"        except KeyboardInterrupt:\",",
        "        \"            break\",",
        "        \"        except EOFError:\",",
        "        \"            break\",",
        "        \"\",",
        "        \"    print(\\\"\\\\nGoodbye!\\\")\",",
        "        \"\",",
        "        \"\"",
        "      ],",
        "      \"context_after\": [",
        "        \"def main():\",",
        "        \"    parser.add_argument('--quick', action='store_true', help='Quick training (1000 patterns)')\",",
        "        \"    parser.add_argument('--full', action='store_true', help='Full training (all patterns)')\",",
        "        \"    parser.add_argument('--interactive', action='store_true', help='Interactive mode after training')\",",
        "        \"    parser.add_argument('--corpus', type=str,\",",
        "        \"                        default='benchmarks/codebase_slm/corpus/training_patterns.jsonl',\",",
        "        \"                        help='Path to training corpus')\",",
        "        \"    parser.add_argument('--context-size', type=int, default=3, help='Context window size')\",",
        "        \"    args = parser.parse_args()\",",
        "        \"\",",
        "        \"    corpus_path = Path(args.corpus)\",",
        "        \"\",",
        "        \"    print(\\\"=\\\" * 60)\",",
        "        \"    print(\\\"Repository-Native SLM Training\\\")\",",
        "        \"    print(\\\"=\\\" * 60)\",",
        "        \"    print()\",",
        "        \"\",",
        "        \"    # Determine pattern limit\",",
        "        \"    limit = 1000 if args.quick else None\",",
        "        \"\",",
        "        \"    # Load corpus\",",
        "        \"    print(f\\\"Loading corpus from {corpus_path}...\\\")\",",
        "        \"    start = time.time()\",",
        "        \"    patterns = load_training_corpus(corpus_path, limit=limit)\",",
        "        \"\"",
        "      ],",
        "      \"change_type\": \"modify\"",
        "    },",
        "    {",
        "      \"file\": \"benchmarks/codebase_slm/train_slm.py\",",
        "      \"function\": \"def main():\",",
        "      \"start_line\": 237,",
        "      \"lines_added\": [",
        "        \"    # Save model if --output specified (and not --dry-run)\",",
        "        \"    if args.output and not args.dry_run:\",",
        "        \"        model_path = Path(args.output)\",",
        "        \"        if not model_path.is_absolute():\",",
        "        \"            model_path = PROJECT_ROOT / \\\"benchmarks\\\" / \\\"codebase_slm\\\" / \\\"models\\\" / model_path\",",
        "        \"\",",
        "        \"        # Backup existing model (unless --force)\",",
        "        \"        if model_path.exists() and not args.force:\",",
        "        \"            backup_existing_model(model_path)\",",
        "        \"        elif model_path.exists() and args.force:\",",
        "        \"            print(\\\"‚ö†Ô∏è  --force specified, skipping backup\\\")\",",
        "        \"\",",
        "        \"        provenance = {\",",
        "        \"            'corpus_hash': compute_corpus_hash(patterns),\",",
        "        \"            'corpus_size': len(patterns),\",",
        "        \"            'corpus_path': str(corpus_path),\",",
        "        \"        }\",",
        "        \"\",",
        "        \"        save_model(model, model_path, provenance)\",",
        "        \"    elif args.output and args.dry_run:\",",
        "        \"        print(\\\"\\\\n‚ö†Ô∏è  DRY RUN - Model was NOT saved (would save to: {})\\\".format(args.output))\",",
        "        \"    elif not args.output and not args.dry_run:\",",
        "        \"        print(\\\"\\\\nüí° Tip: Use --output <path> to save the trained model\\\")\",",
        "        \"\"",
        "      ],",
        "      \"lines_removed\": [],",
        "      \"context_before\": [",
        "        \"        print(f\\\"\\\\n[{q['category']}] {status}\\\")\",",
        "        \"        print(f\\\"  Prompt: {q['prompt']}\\\")\",",
        "        \"        print(f\\\"  Generated: {generated}\\\")\",",
        "        \"        print(f\\\"  Expected terms: {q['expected']}\\\")\",",
        "        \"        print(f\\\"  Match: {matches}/{len(expected_terms)} terms ({match_pct:.0f}%)\\\")\",",
        "        \"\",",
        "        \"    print(f\\\"\\\\n{'=' * 60}\\\")\",",
        "        \"    print(f\\\"RESULTS: {correct}/{len(test_queries)} queries matched (‚â•50% of terms)\\\")\",",
        "        \"    print(f\\\"{'=' * 60}\\\")\",",
        "        \"\"",
        "      ],",
        "      \"context_after\": [",
        "        \"    # Interactive mode\",",
        "        \"    if args.interactive:\",",
        "        \"        interactive_mode(model)\",",
        "        \"\",",
        "        \"    return 0\",",
        "        \"\",",
        "        \"\",",
        "        \"if __name__ == '__main__':\",",
        "        \"    sys.exit(main())\"",
        "      ],",
        "      \"change_type\": \"add\"",
        "    }",
        "  ],",
        "  \"hour_of_day\": 2,",
        "  \"day_of_week\": \"Saturday\",",
        "  \"seconds_since_last_commit\": 677,",
        "  \"is_merge\": false,",
        "  \"is_initial\": false,",
        "  \"parent_count\": 1,",
        "  \"session_id\": \"c7e7f25e\",",
        "  \"related_chats\": [",
        "    \"chat-20251227-015100-015a56\"",
        "  ],",
        "  \"ci_result\": null,",
        "  \"reverted\": false,",
        "  \"amended\": false",
        "}"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": ".git-ml/commits/b13044b8_2025-12-27_8455cd84.json",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "{",
        "  \"hash\": \"b13044b88739516a22f844715c88d956c4992dfe\",",
        "  \"message\": \"fix(ml): Configure Claude Code hooks and fix commit command\",",
        "  \"author\": \"Claude\",",
        "  \"timestamp\": \"2025-12-27 02:20:49 +0000\",",
        "  \"branch\": \"claude/accept-handoff-ctrSI\",",
        "  \"files_changed\": [",
        "    \".claude/settings.local.json\",",
        "    \"scripts/ml_data_collector.py\"",
        "  ],",
        "  \"insertions\": 24,",
        "  \"deletions\": 0,",
        "  \"hunks\": [",
        "    {",
        "      \"file\": \".claude/settings.local.json\",",
        "      \"function\": null,",
        "      \"start_line\": 1,",
        "      \"lines_added\": [",
        "        \"    \\\"SessionStart\\\": [\",",
        "        \"      {\",",
        "        \"        \\\"type\\\": \\\"command\\\",\",",
        "        \"        \\\"command\\\": \\\"bash scripts/ml-session-start-hook.sh\\\"\",",
        "        \"      }\",",
        "        \"    ],\",",
        "        \"    \\\"Stop\\\": [\",",
        "        \"      {\",",
        "        \"        \\\"type\\\": \\\"command\\\",\",",
        "        \"        \\\"command\\\": \\\"bash scripts/ml-session-capture-hook.sh\\\"\",",
        "        \"      }\",",
        "        \"    ]\"",
        "      ],",
        "      \"lines_removed\": [],",
        "      \"context_before\": [",
        "        \"{\",",
        "        \"  \\\"hooks\\\": {\"",
        "      ],",
        "      \"context_after\": [",
        "        \"  }\",",
        "        \"}\"",
        "      ],",
        "      \"change_type\": \"add\"",
        "    },",
        "    {",
        "      \"file\": \"scripts/ml_data_collector.py\",",
        "      \"function\": \"def main():\",",
        "      \"start_line\": 3391,",
        "      \"lines_added\": [",
        "        \"        if len(sys.argv) > 2 and sys.argv[2] in (\\\"-h\\\", \\\"--help\\\"):\",",
        "        \"            print(\\\"Usage: ml_data_collector.py commit [COMMIT_HASH]\\\")\",",
        "        \"            print()\",",
        "        \"            print(\\\"Collect rich context data for a commit.\\\")\",",
        "        \"            print()\",",
        "        \"            print(\\\"Arguments:\\\")\",",
        "        \"            print(\\\"  COMMIT_HASH   Git commit hash (default: HEAD)\\\")\",",
        "        \"            print()\",",
        "        \"            print(\\\"Examples:\\\")\",",
        "        \"            print(\\\"  ml_data_collector.py commit          # Collect HEAD commit\\\")\",",
        "        \"            print(\\\"  ml_data_collector.py commit abc123   # Collect specific commit\\\")\",",
        "        \"            return\"",
        "      ],",
        "      \"lines_removed\": [],",
        "      \"context_before\": [",
        "        \"    # Allow stats/estimate/validate/export/feedback/quality-report/orchestration even when collection is disabled\",",
        "        \"    read_only_commands = {\\\"stats\\\", \\\"estimate\\\", \\\"validate\\\", \\\"session\\\", \\\"export\\\", \\\"feedback\\\", \\\"quality-report\\\", \\\"orchestration\\\"}\",",
        "        \"\",",
        "        \"    # Check if collection is disabled (via ML_COLLECTION_ENABLED=0)\",",
        "        \"    if not ML_COLLECTION_ENABLED and command not in read_only_commands:\",",
        "        \"        # Silently exit for collection commands when disabled\",",
        "        \"        return\",",
        "        \"\",",
        "        \"    if command == \\\"commit\\\":\",",
        "        \"        # Collect data for current or specified commit\"",
        "      ],",
        "      \"context_after\": [",
        "        \"        commit_hash = sys.argv[2] if len(sys.argv) > 2 else None\",",
        "        \"        context = collect_commit_data(commit_hash)\",",
        "        \"        save_commit_data(context)\",",
        "        \"        # Also save lightweight version (trackable in git)\",",
        "        \"        lite_path = save_commit_lite(context)\",",
        "        \"        print(f\\\"Saved lightweight commit to {lite_path}\\\")\",",
        "        \"\",",
        "        \"    elif command == \\\"backfill\\\":\",",
        "        \"        # Backfill historical commits (no session linking for historical data)\",",
        "        \"        import argparse\"",
        "      ],",
        "      \"change_type\": \"add\"",
        "    }",
        "  ],",
        "  \"hour_of_day\": 2,",
        "  \"day_of_week\": \"Saturday\",",
        "  \"seconds_since_last_commit\": 762,",
        "  \"is_merge\": false,",
        "  \"is_initial\": false,",
        "  \"parent_count\": 1,",
        "  \"session_id\": \"c7e7f25e\",",
        "  \"related_chats\": [",
        "    \"chat-20251227-015100-015a56\"",
        "  ],",
        "  \"ci_result\": null,",
        "  \"reverted\": false,",
        "  \"amended\": false",
        "}"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": ".gitignore",
      "function": "TestResult.xml",
      "start_line": 89,
      "lines_added": [
        "# ML training data - TRACK EVERYTHING for model training",
        "# TRACKED IN GIT (training data - the whole point!):",
        "#   - .git-ml/commits/       - Full commit data with diffs (valuable context!)",
        "#   - .git-ml/chats/         - Chat transcripts (MOST VALUABLE training data!)",
        "#   - .git-ml/actions/       - Tool usage patterns",
        "#   - .git-ml/github/        - PR/Issue data",
        "# LOCAL ONLY (ephemeral/regeneratable):",
        "#   - .git-ml/models/        - Trained models (regenerate with train scripts)",
        "#   - .git-ml/cali/          - Local cache indices",
        "#   - .git-ml/predictions/   - Local prediction state",
        "#"
      ],
      "lines_removed": [
        "# ML training data - tiered sharing strategy",
        "# TRACKED IN GIT (small, shareable):",
        "#   - .git-ml/commits-lite/  - Legacy individual files (being migrated)",
        "# LOCAL ONLY (large or sensitive):",
        "#   - .git-ml/commits/       - Full commit data with diffs (too large)",
        "#   - .git-ml/chats/         - Chat transcripts (sensitive)",
        "#   - .git-ml/actions/       - Tool usage",
        "#   - .git-ml/github/        - PR/Issue data (contains discussions)",
        "#   - .git-ml/orchestration/ - Individual orchestration files (use tracked/orchestration.jsonl)",
        "# Full commit data with diffs is too large for GitHub",
        ".git-ml/commits/",
        ".git-ml/chats/",
        ".git-ml/sessions/",
        ".git-ml/actions/"
      ],
      "context_before": [
        "nunit-*.xml",
        "# Indexer progress files",
        ".index_progress.json",
        ".index_incremental_progress.json",
        "tasks/.current_session.json",
        "",
        "# Generated protobuf code (regenerated at runtime from schema.proto)",
        "cortical/proto/schema_pb2.py",
        "cortical/proto/__pycache__/",
        ""
      ],
      "context_after": [
        "#   - .git-ml/tracked/       - JSONL files (commits.jsonl, sessions.jsonl, orchestration.jsonl)",
        "#   - .git-ml/shared/        - Aggregated patterns",
        "#   - .git-ml/sessions/      - Full session data",
        "# Trained models are regeneratable (python scripts/ml_file_prediction.py train)",
        ".git-ml/models/",
        "# .git-ml/github/  # Uncomment to ignore PR/Issue data",
        "# .git-ml/orchestration/  # Uncomment to ignore individual orchestration JSON files",
        ".git-ml/current_session.json  # Session-specific data",
        "# .git-ml/contribution_consent.json  # Uncomment to ignore consent file",
        ".git-ml/current_session.json.lock",
        ".git-ml/.last_activity  # Session continuity tracking (local only)",
        ".git-ml/cali/  # CALI storage (local indices, objects, manifest - all regeneratable)",
        ".git-ml/predictions/  # Hubris prediction state (local, regeneratable)",
        "",
        "# Clean ignores for local ML data"
      ],
      "change_type": "modify"
    }
  ],
  "hour_of_day": 2,
  "day_of_week": "Saturday",
  "seconds_since_last_commit": 209,
  "is_merge": false,
  "is_initial": false,
  "parent_count": 1,
  "session_id": "c7e7f25e",
  "related_chats": [
    "chat-20251227-015100-015a56"
  ],
  "ci_result": null,
  "reverted": false,
  "amended": false
}