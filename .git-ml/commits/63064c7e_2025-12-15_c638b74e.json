{
  "hash": "63064c7ef0706740d884281ce73a493c46e8d5f5",
  "message": "docs: Add BM25/GB-BM25 documentation and tests",
  "author": "Claude",
  "timestamp": "2025-12-15 05:57:58 +0000",
  "branch": "claude/multi-index-design-DvifZ",
  "files_changed": [
    "CLAUDE.md",
    "tests/unit/test_query_search.py"
  ],
  "insertions": 191,
  "deletions": 0,
  "hunks": [
    {
      "file": "CLAUDE.md",
      "function": "def find_documents(",
      "start_line": 806,
      "lines_added": [
        "## Scoring Algorithms",
        "",
        "The processor supports multiple scoring algorithms for term weighting:",
        "",
        "### BM25 (Default)",
        "",
        "BM25 (Best Match 25) is the default scoring algorithm, optimized for code search:",
        "",
        "```python",
        "from cortical import CorticalTextProcessor",
        "from cortical.config import CorticalConfig",
        "",
        "# BM25 with default parameters (recommended)",
        "config = CorticalConfig(scoring_algorithm='bm25')",
        "",
        "# Tune BM25 parameters if needed",
        "config = CorticalConfig(",
        "    scoring_algorithm='bm25',",
        "    bm25_k1=1.2,  # Term frequency saturation (0.0-3.0, default 1.2)",
        "    bm25_b=0.75   # Length normalization (0.0-1.0, default 0.75)",
        ")",
        "processor = CorticalTextProcessor(config=config)",
        "```",
        "",
        "**Parameters:**",
        "- `bm25_k1`: Controls term frequency saturation. Higher values give more weight to term frequency.",
        "- `bm25_b`: Controls document length normalization. Set to 0.0 to disable length normalization.",
        "",
        "### TF-IDF (Legacy)",
        "",
        "Traditional TF-IDF scoring is still available:",
        "",
        "```python",
        "config = CorticalConfig(scoring_algorithm='tfidf')",
        "```",
        "",
        "### Graph-Boosted Search (GB-BM25)",
        "",
        "A hybrid search combining BM25 with graph signals:",
        "",
        "```python",
        "# Standard search (uses BM25 under the hood)",
        "results = processor.find_documents_for_query(\"query\")",
        "",
        "# Graph-boosted search (adds PageRank + proximity signals)",
        "results = processor.graph_boosted_search(",
        "    \"query\",",
        "    pagerank_weight=0.3,   # Weight for term importance (0-1)",
        "    proximity_weight=0.2   # Weight for connected terms (0-1)",
        ")",
        "```",
        "",
        "**GB-BM25 combines:**",
        "1. BM25 base score (term relevance)",
        "2. PageRank boost (important terms rank higher)",
        "3. Proximity boost (connected query terms boost documents)",
        "4. Coverage boost (documents matching more terms rank higher)",
        "",
        "---",
        "",
        "9. **Use `graph_boosted_search()`** for hybrid scoring with PageRank signals"
      ],
      "lines_removed": [],
      "context_before": [
        "        top_n: Number of results to return",
        "",
        "    Returns:",
        "        List of (doc_id, score) tuples sorted by relevance",
        "    \"\"\"",
        "    # Implementation",
        "```",
        "",
        "---",
        ""
      ],
      "context_after": [
        "## Performance Considerations",
        "",
        "1. **Use `get_by_id()` for ID lookups** - O(1) vs O(n) iteration",
        "2. **Batch document additions** with `add_documents_batch()` for bulk imports",
        "3. **Use incremental updates** with `add_document_incremental()` for live systems",
        "4. **Cache query expansions** when processing multiple similar queries",
        "5. **Pre-compute chunks** in `find_passages_batch()` to avoid redundant work",
        "6. **Use `fast_find_documents()`** for ~2-3x faster search on large corpora",
        "7. **Pre-build index** with `build_search_index()` for fastest repeated queries",
        "8. **Watch for O(n²) patterns** in loops over connections—use limits like `max_bigrams_per_term`",
        "",
        "---",
        "",
        "## Code Search Capabilities",
        "",
        "### Code-Aware Tokenization",
        "```python",
        "# Enable identifier splitting for code search",
        "tokenizer = Tokenizer(split_identifiers=True)",
        "tokens = tokenizer.tokenize(\"getUserCredentials\")"
      ],
      "change_type": "add"
    },
    {
      "file": "CLAUDE.md",
      "function": "python examples/observability_demo.py",
      "start_line": 986,
      "lines_added": [
        "| Hybrid search | `processor.graph_boosted_search(query)` |"
      ],
      "lines_removed": [],
      "context_before": [
        "---",
        "",
        "## Quick Reference",
        "",
        "| Task | Command/Method |",
        "|------|----------------|",
        "| Process document | `processor.process_document(id, text)` |",
        "| Build network | `processor.compute_all()` |",
        "| Search | `processor.find_documents_for_query(query)` |",
        "| Fast search | `processor.fast_find_documents(query)` |"
      ],
      "context_after": [
        "| Code search | `processor.expand_query_for_code(query)` |",
        "| Intent search | `processor.search_by_intent(\"where do we...\")` |",
        "| RAG passages | `processor.find_passages_for_query(query)` |",
        "| Fingerprint | `processor.get_fingerprint(text)` |",
        "| Compare | `processor.compare_fingerprints(fp1, fp2)` |",
        "| Save state | `processor.save(\"corpus.pkl\")` |",
        "| Load state | `processor = CorticalTextProcessor.load(\"corpus.pkl\")` |",
        "| Enable metrics | `processor = CorticalTextProcessor(enable_metrics=True)` |",
        "| Get metrics | `processor.get_metrics()` |",
        "| Metrics summary | `processor.get_metrics_summary()` |"
      ],
      "change_type": "add"
    },
    {
      "file": "tests/unit/test_query_search.py",
      "function": "These tests use mock layers and don't require a full processor.",
      "start_line": 18,
      "lines_added": [
        "    graph_boosted_search,"
      ],
      "lines_removed": [],
      "context_before": [
        "import pytest",
        "from unittest.mock import Mock",
        "",
        "from cortical.query.search import (",
        "    find_documents_for_query,",
        "    fast_find_documents,",
        "    build_document_index,",
        "    search_with_index,",
        "    query_with_spreading_activation,",
        "    find_related_documents,"
      ],
      "context_after": [
        ")",
        "from cortical.tokenizer import Tokenizer",
        "from tests.unit.mocks import (",
        "    MockMinicolumn,",
        "    MockHierarchicalLayer,",
        "    MockLayers,",
        "    LayerBuilder,",
        ")",
        "",
        ""
      ],
      "change_type": "add"
    },
    {
      "file": "tests/unit/test_query_search.py",
      "function": "class TestFindRelatedDocuments:",
      "start_line": 1041,
      "lines_added": [
        "",
        "",
        "# =============================================================================",
        "# GRAPH_BOOSTED_SEARCH TESTS",
        "# =============================================================================",
        "",
        "",
        "class TestGraphBoostedSearch:",
        "    \"\"\"Tests for graph_boosted_search hybrid scoring function.\"\"\"",
        "",
        "    def test_basic_search(self):",
        "        \"\"\"Basic search returns ranked documents.\"\"\"",
        "        # Create tokens with tfidf and pagerank",
        "        neural = MockMinicolumn(",
        "            content=\"neural\",",
        "            id=\"L0_neural\",",
        "            layer=MockLayers.TOKENS,",
        "            tfidf=1.0,",
        "            tfidf_per_doc={\"doc1\": 0.8, \"doc2\": 0.5},",
        "            document_ids={\"doc1\", \"doc2\"},",
        "            pagerank=0.3,",
        "            lateral_connections={}",
        "        )",
        "        networks = MockMinicolumn(",
        "            content=\"networks\",",
        "            id=\"L0_networks\",",
        "            layer=MockLayers.TOKENS,",
        "            tfidf=0.9,",
        "            tfidf_per_doc={\"doc1\": 0.7, \"doc3\": 0.4},",
        "            document_ids={\"doc1\", \"doc3\"},",
        "            pagerank=0.2,",
        "            lateral_connections={}",
        "        )",
        "",
        "        layers = MockLayers.empty()",
        "        layers[MockLayers.TOKENS] = MockHierarchicalLayer([neural, networks])",
        "        layers[MockLayers.DOCUMENTS] = MockHierarchicalLayer([])",
        "        tokenizer = Tokenizer()",
        "",
        "        results = graph_boosted_search(\"neural networks\", layers, tokenizer, top_n=3)",
        "",
        "        assert len(results) > 0",
        "        # doc1 should rank highest (has both terms)",
        "        assert results[0][0] == \"doc1\"",
        "",
        "    def test_empty_query(self):",
        "        \"\"\"Empty query returns empty results.\"\"\"",
        "        layers = MockLayers.single_term(\"term\", tfidf=1.0, doc_ids=[\"doc1\"])",
        "        tokenizer = Tokenizer()",
        "",
        "        results = graph_boosted_search(\"\", layers, tokenizer, top_n=5)",
        "        assert results == []",
        "",
        "    def test_no_matching_terms(self):",
        "        \"\"\"Query with no matching terms returns empty results.\"\"\"",
        "        layers = MockLayers.single_term(\"other\", tfidf=1.0, doc_ids=[\"doc1\"])",
        "        tokenizer = Tokenizer()",
        "",
        "        results = graph_boosted_search(\"nonexistent\", layers, tokenizer, top_n=5)",
        "        assert results == []",
        "",
        "    def test_pagerank_boost(self):",
        "        \"\"\"Documents with high-PageRank terms get boosted.\"\"\"",
        "        # High PageRank term",
        "        important = MockMinicolumn(",
        "            content=\"important\",",
        "            id=\"L0_important\",",
        "            layer=MockLayers.TOKENS,",
        "            tfidf=1.0,",
        "            tfidf_per_doc={\"doc1\": 1.0},",
        "            document_ids={\"doc1\"},",
        "            pagerank=0.9,  # High importance",
        "            lateral_connections={}",
        "        )",
        "        # Low PageRank term",
        "        common = MockMinicolumn(",
        "            content=\"common\",",
        "            id=\"L0_common\",",
        "            layer=MockLayers.TOKENS,",
        "            tfidf=1.0,",
        "            tfidf_per_doc={\"doc2\": 1.0},",
        "            document_ids={\"doc2\"},",
        "            pagerank=0.1,  # Low importance",
        "            lateral_connections={}",
        "        )",
        "",
        "        layers = MockLayers.empty()",
        "        layers[MockLayers.TOKENS] = MockHierarchicalLayer([important, common])",
        "        layers[MockLayers.DOCUMENTS] = MockHierarchicalLayer([])",
        "        tokenizer = Tokenizer()",
        "",
        "        # Search for both terms",
        "        results = graph_boosted_search(",
        "            \"important common\", layers, tokenizer, top_n=5,",
        "            pagerank_weight=0.5  # High PageRank influence",
        "        )",
        "",
        "        assert len(results) == 2",
        "        # doc1 should rank higher due to PageRank boost",
        "        assert results[0][0] == \"doc1\"",
        "",
        "    def test_respects_top_n(self):",
        "        \"\"\"Returns at most top_n results.\"\"\"",
        "        terms = []",
        "        for i in range(10):",
        "            terms.append(MockMinicolumn(",
        "                content=f\"term{i}\",",
        "                id=f\"L0_term{i}\",",
        "                layer=MockLayers.TOKENS,",
        "                tfidf=1.0,",
        "                tfidf_per_doc={f\"doc{i}\": 1.0},",
        "                document_ids={f\"doc{i}\"},",
        "                pagerank=0.1,",
        "                lateral_connections={}",
        "            ))",
        "",
        "        layers = MockLayers.empty()",
        "        layers[MockLayers.TOKENS] = MockHierarchicalLayer(terms)",
        "        layers[MockLayers.DOCUMENTS] = MockHierarchicalLayer([])",
        "        tokenizer = Tokenizer()",
        "",
        "        # Query that matches multiple docs",
        "        results = graph_boosted_search(",
        "            \" \".join(f\"term{i}\" for i in range(10)),",
        "            layers, tokenizer, top_n=3",
        "        )",
        "",
        "        assert len(results) <= 3"
      ],
      "lines_removed": [],
      "context_before": [
        "",
        "        layers = MockLayers.empty()",
        "        layer3 = MockHierarchicalLayer([doc1, doc2])",
        "        layers[MockLayers.DOCUMENTS] = layer3",
        "",
        "        result = find_related_documents(\"doc1\", layers)",
        "",
        "        # If this works, get_by_id was used successfully",
        "        assert len(result) == 1",
        "        assert result[0][0] == \"doc2\""
      ],
      "context_after": [],
      "change_type": "add"
    }
  ],
  "hour_of_day": 5,
  "day_of_week": "Monday",
  "seconds_since_last_commit": -28010,
  "is_merge": false,
  "is_initial": false,
  "parent_count": 1,
  "session_id": null,
  "related_chats": [],
  "ci_result": null,
  "reverted": false,
  "amended": false
}