{
  "hash": "ce8b1e20bf070711ee8322d844dc7a75b0619b86",
  "message": "Merge pull request #70 from scrawlsbenches/claude/setup-director-agent-01337m9RiUZqGyckcS4Q6tWR",
  "author": "scrawlsbenches",
  "timestamp": "2025-12-14 02:54:25 -0500",
  "branch": "claude/multi-index-design-DvifZ",
  "files_changed": [
    ".claude/prompts/director-agent-continuation.md",
    "cortical/minicolumn.py",
    "cortical/processor.py",
    "cortical/query/search.py",
    "scripts/session_context.py",
    "scripts/suggest_tasks.py",
    "scripts/task_utils.py",
    "tasks/2025-12-13_22-32-34_e233.json",
    "tasks/2025-12-13_22-42-20_6ac7.json",
    "tasks/2025-12-13_23-54-58_1a1d.json",
    "tasks/2025-12-14_00-21-46_8d66.json",
    "tasks/2025-12-14_01-53-45_7b60.json",
    "tests/integration/test_task_recovery.py",
    "tests/performance/test_doc_name_boost_perf.py",
    "tests/unit/test_compute_checkpointing.py",
    "tests/unit/test_processor_json_persistence.py",
    "tests/unit/test_session_context.py",
    "tests/unit/test_suggest_tasks.py",
    "tests/unit/test_task_retrospective.py"
  ],
  "insertions": 4797,
  "deletions": 109,
  "hunks": [
    {
      "file": ".claude/prompts/director-agent-continuation.md",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "# Director Agent Continuation Prompt",
        "",
        "You are resuming as the Director Agent for the Cortical Text Processor project.",
        "",
        "## Session Summary (2025-12-14)",
        "",
        "**Branch:** `claude/setup-director-agent-01337m9RiUZqGyckcS4Q6tWR`",
        "**Last Commit:** `78b3041` - feat: Complete 7 parallel tasks via Director Agent orchestration",
        "",
        "### Completed Work (7 tasks, ~4,700 lines added)",
        "",
        "| Task | Deliverable | Tests |",
        "|------|-------------|-------|",
        "| #206-P2 | `save_json()`, `load_json()`, `migrate_to_json()` in processor.py | 23 |",
        "| T-8d66-001 | `scripts/session_context.py` - agent handoff summaries | 23 |",
        "| T-8d66-003 | Checkpointing for `compute_all()` with resume support | 14 |",
        "| T-8d66-004 | `scripts/suggest_tasks.py` - auto-suggest follow-up tasks | 37 |",
        "| T-1a1d-001 | 2.78x search speedup via `name_tokens` caching | 4 |",
        "| T-e233-03 | Crash recovery tests (13 tests, 0 bugs found) | 13 |",
        "| T-6ac7-06 | Task retrospective metadata in `task_utils.py` | 20 |",
        "",
        "**All 134 new tests passing.**",
        "",
        "### Key Files Modified",
        "",
        "```",
        "cortical/processor.py     +250 lines (JSON persistence, checkpointing)",
        "cortical/minicolumn.py    +name_tokens field",
        "cortical/query/search.py  Uses cached name_tokens",
        "scripts/task_utils.py     +retrospective metadata",
        "scripts/session_context.py (new, 473 lines)",
        "scripts/suggest_tasks.py  (new, 588 lines)",
        "```",
        "",
        "### Workflow Learnings",
        "",
        "**What worked:**",
        "- Wave-based parallelization (5 agents in Wave 1, non-overlapping files)",
        "- Clear task decomposition with file scope boundaries",
        "- Dependency ordering (Wave 2 waited for Wave 1 on processor.py)",
        "",
        "**Areas to improve:**",
        "- Add quick review step between waves",
        "- Run smoke tests after each wave",
        "- Limit processor.py changes to one wave",
        "",
        "### Suggested Follow-Up Tasks",
        "",
        "| ID | Title | Priority | Rationale |",
        "|----|-------|----------|-----------|",
        "| T-new-001 | Add `clear_checkpoint()` method | Low | Checkpoints accumulate, need cleanup |",
        "| T-new-002 | Integration test for checkpoint + resume | Medium | Only unit tests, need e2e crash sim |",
        "| T-new-003 | Performance regression baseline | Medium | Hot path modified, no perf check done |",
        "",
        "### Remaining Backlog (from TASK_LIST.md)",
        "",
        "| # | Task | Priority |",
        "|---|------|----------|",
        "| 133 | WAL + snapshot persistence | Medium |",
        "| 134 | Protobuf serialization | Medium |",
        "| 135 | Chunked parallel compute_all() | Medium |",
        "| 95 | Split processor.py into modules | Medium |",
        "",
        "### Quick Commands",
        "",
        "```bash",
        "# Run all new tests",
        "python -m pytest tests/unit/test_processor_json_persistence.py tests/unit/test_session_context.py tests/unit/test_compute_checkpointing.py tests/unit/test_suggest_tasks.py tests/unit/test_task_retrospective.py tests/integration/test_task_recovery.py -v",
        "",
        "# Generate session context",
        "python scripts/session_context.py",
        "",
        "# Check for suggested tasks",
        "python scripts/suggest_tasks.py --all",
        "",
        "# Run smoke tests",
        "python -m pytest tests/smoke/ -v",
        "```",
        "",
        "### How to Continue",
        "",
        "1. Read `CLAUDE.md` for project conventions",
        "2. Run `python scripts/session_context.py` for latest status",
        "3. Check `tasks/*.json` for pending work",
        "4. Choose next task group and delegate to sub-agents",
        "",
        "---",
        "",
        "*Generated by Director Agent session 01337m9RiUZqGyckcS4Q6tWR*"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "cortical/minicolumn.py",
      "function": "class Minicolumn:",
      "start_line": 87,
      "lines_added": [
        "        'doc_occurrence_counts', 'name_tokens'"
      ],
      "lines_removed": [
        "        'doc_occurrence_counts'"
      ],
      "context_before": [
        "        col.occurrence_count = 15",
        "        col.add_lateral_connection(\"L0_network\", 0.8)",
        "        col.add_typed_connection(\"L0_network\", 0.8, relation_type='RelatedTo')",
        "    \"\"\"",
        "",
        "    __slots__ = [",
        "        'id', 'content', 'layer', 'activation', 'occurrence_count',",
        "        'document_ids', '_lateral_cache', '_lateral_cache_valid', 'typed_connections',",
        "        'feedforward_sources', 'feedforward_connections', 'feedback_connections',",
        "        'tfidf', 'tfidf_per_doc', 'pagerank', 'cluster_id',"
      ],
      "context_after": [
        "    ]",
        "    ",
        "    def __init__(self, id: str, content: str, layer: int):",
        "        \"\"\"",
        "        Initialize a minicolumn.",
        "        ",
        "        Args:",
        "            id: Unique identifier for this column",
        "            content: The content this column represents",
        "            layer: Layer number (0-3)"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/minicolumn.py",
      "function": "class Minicolumn:",
      "start_line": 116,
      "lines_added": [
        "        self.name_tokens: Optional[Set[str]] = None  # Cached tokenized name for document minicolumns"
      ],
      "lines_removed": [],
      "context_before": [
        "        self._lateral_cache_valid: bool = True  # Cache starts valid (empty matches empty)",
        "        self.typed_connections: Dict[str, Edge] = {}  # Single source of truth for connections",
        "        self.feedforward_sources: Set[str] = set()  # Deprecated: use feedforward_connections",
        "        self.feedforward_connections: Dict[str, float] = {}  # Weighted links to lower layer",
        "        self.feedback_connections: Dict[str, float] = {}  # Weighted links to higher layer",
        "        self.tfidf = 0.0",
        "        self.tfidf_per_doc: Dict[str, float] = {}",
        "        self.pagerank = 1.0",
        "        self.cluster_id: Optional[int] = None",
        "        self.doc_occurrence_counts: Dict[str, int] = {}"
      ],
      "context_after": [
        "",
        "    @property",
        "    def lateral_connections(self) -> Dict[str, float]:",
        "        \"\"\"",
        "        Get lateral connections as a simple weight dictionary.",
        "",
        "        This is a cached view derived from typed_connections. For backward",
        "        compatibility, this returns a dict mapping target_id to weight.",
        "        The cache is invalidated when connections are added/modified.",
        ""
      ],
      "change_type": "add"
    },
    {
      "file": "cortical/minicolumn.py",
      "function": "class Minicolumn:",
      "start_line": 425,
      "lines_added": [
        "        d = {",
        "        # Only include name_tokens if it's set (for document minicolumns)",
        "        if self.name_tokens is not None:",
        "            d['name_tokens'] = list(self.name_tokens)",
        "        return d"
      ],
      "lines_removed": [
        "        return {"
      ],
      "context_before": [
        "        )",
        "        return sorted_conns[:n]",
        "    ",
        "    def to_dict(self) -> Dict:",
        "        \"\"\"",
        "        Convert to dictionary for serialization.",
        "",
        "        Returns:",
        "            Dictionary representation of this minicolumn",
        "        \"\"\""
      ],
      "context_after": [
        "            'id': self.id,",
        "            'content': self.content,",
        "            'layer': self.layer,",
        "            'activation': self.activation,",
        "            'occurrence_count': self.occurrence_count,",
        "            'document_ids': list(self.document_ids),",
        "            'lateral_connections': self.lateral_connections,",
        "            'typed_connections': {",
        "                target_id: edge.to_dict()",
        "                for target_id, edge in self.typed_connections.items()",
        "            },",
        "            'feedforward_sources': list(self.feedforward_sources),",
        "            'feedforward_connections': self.feedforward_connections,",
        "            'feedback_connections': self.feedback_connections,",
        "            'tfidf': self.tfidf,",
        "            'tfidf_per_doc': self.tfidf_per_doc,",
        "            'pagerank': self.pagerank,",
        "            'cluster_id': self.cluster_id,",
        "            'doc_occurrence_counts': self.doc_occurrence_counts",
        "        }",
        "    ",
        "    @classmethod",
        "    def from_dict(cls, data: Dict) -> 'Minicolumn':",
        "        \"\"\"",
        "        Create a minicolumn from dictionary representation.",
        "",
        "        Handles backward compatibility: if typed_connections is present, use it.",
        "        If only lateral_connections is present (old format), convert it.",
        "",
        "        Args:"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/minicolumn.py",
      "function": "class Minicolumn:",
      "start_line": 490,
      "lines_added": [
        "        # Deserialize name_tokens if present (for document minicolumns)",
        "        col.name_tokens = set(data.get('name_tokens', [])) if data.get('name_tokens') else None"
      ],
      "lines_removed": [],
      "context_before": [
        "        # else: both empty, nothing to do (already initialized empty)",
        "",
        "        col.feedforward_sources = set(data.get('feedforward_sources', []))",
        "        col.feedforward_connections = data.get('feedforward_connections', {})",
        "        col.feedback_connections = data.get('feedback_connections', {})",
        "        col.tfidf = data.get('tfidf', 0.0)",
        "        col.tfidf_per_doc = data.get('tfidf_per_doc', {})",
        "        col.pagerank = data.get('pagerank', 1.0)",
        "        col.cluster_id = data.get('cluster_id')",
        "        col.doc_occurrence_counts = data.get('doc_occurrence_counts', {})"
      ],
      "context_after": [
        "        return col",
        "    ",
        "    def __repr__(self) -> str:",
        "        return f\"Minicolumn(id={self.id}, content={self.content}, layer={self.layer})\""
      ],
      "change_type": "add"
    },
    {
      "file": "cortical/processor.py",
      "function": null,
      "start_line": 1,
      "lines_added": [
        "import json",
        "from typing import Dict, List, Tuple, Optional, Any, Set",
        "from pathlib import Path",
        "from datetime import datetime",
        "from . import state_storage"
      ],
      "lines_removed": [
        "from typing import Dict, List, Tuple, Optional, Any"
      ],
      "context_before": [
        "\"\"\"",
        "Cortical Text Processor - Main processor class that orchestrates all components.",
        "\"\"\"",
        "",
        "import os",
        "import re",
        "import logging"
      ],
      "context_after": [
        "import copy",
        "from collections import defaultdict",
        "",
        "from .tokenizer import Tokenizer",
        "from .minicolumn import Minicolumn",
        "from .layers import CorticalLayer, HierarchicalLayer",
        "from .config import CorticalConfig",
        "from . import analysis",
        "from . import semantics",
        "from . import embeddings as emb_module",
        "from . import query as query_module",
        "from . import gaps as gaps_module",
        "from . import persistence",
        "from . import fingerprint as fp_module",
        "from .progress import (",
        "    ProgressReporter,",
        "    ConsoleProgressReporter,",
        "    CallbackProgressReporter,",
        "    SilentProgressReporter,",
        "    MultiPhaseProgress",
        ")",
        "",
        "logger = logging.getLogger(__name__)",
        ""
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/processor.py",
      "function": "class CorticalTextProcessor:",
      "start_line": 112,
      "lines_added": [
        "        # Cache tokenized document name for fast doc_name_boost in search",
        "        # This avoids re-tokenizing the doc_id on every query",
        "        doc_col.name_tokens = set(self.tokenizer.tokenize(doc_id.replace('_', ' ')))",
        ""
      ],
      "lines_removed": [
        "        "
      ],
      "context_before": [
        "",
        "        tokens = self.tokenizer.tokenize(content)",
        "        bigrams = self.tokenizer.extract_ngrams(tokens, n=2)",
        "        ",
        "        layer0 = self.layers[CorticalLayer.TOKENS]",
        "        layer1 = self.layers[CorticalLayer.BIGRAMS]",
        "        layer3 = self.layers[CorticalLayer.DOCUMENTS]",
        "        ",
        "        doc_col = layer3.get_or_create_minicolumn(doc_id)",
        "        doc_col.occurrence_count += 1"
      ],
      "context_after": [
        "        for token in tokens:",
        "            col = layer0.get_or_create_minicolumn(token)",
        "            col.occurrence_count += 1",
        "            col.document_ids.add(doc_id)",
        "            col.activation += 1.0",
        "            # Weighted feedforward: document → token (weight by occurrence count)",
        "            doc_col.add_feedforward_connection(col.id, 1.0)",
        "            # Weighted feedback: token → document (weight by occurrence count)",
        "            col.add_feedback_connection(doc_col.id, 1.0)",
        "            # Track per-document occurrence count for accurate TF-IDF"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/processor.py",
      "function": "class CorticalTextProcessor:",
      "start_line": 635,
      "lines_added": [
        "        show_progress: bool = False,",
        "        checkpoint_dir: Optional[str] = None,",
        "        resume: bool = False"
      ],
      "lines_removed": [
        "        show_progress: bool = False"
      ],
      "context_before": [
        "",
        "    def compute_all(",
        "        self,",
        "        verbose: bool = True,",
        "        build_concepts: bool = True,",
        "        pagerank_method: str = 'standard',",
        "        connection_strategy: str = 'document_overlap',",
        "        cluster_strictness: float = 1.0,",
        "        bridge_weight: float = 0.0,",
        "        progress_callback: Optional[ProgressReporter] = None,"
      ],
      "context_after": [
        "    ) -> Dict[str, Any]:",
        "        \"\"\"",
        "        Run all computation steps.",
        "",
        "        Args:",
        "            verbose: Print progress messages (deprecated, use show_progress)",
        "            build_concepts: Build concept clusters in Layer 2 (default True)",
        "                           This enables topic-based filtering and hierarchical search.",
        "            pagerank_method: PageRank algorithm to use:",
        "                - 'standard': Traditional PageRank using connection weights"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/processor.py",
      "function": "class CorticalTextProcessor:",
      "start_line": 661,
      "lines_added": [
        "            checkpoint_dir: Directory to save checkpoints after each phase (enables checkpointing).",
        "                If None (default), no checkpointing is performed.",
        "            resume: If True, resume from last checkpoint in checkpoint_dir.",
        "                Requires checkpoint_dir to be set."
      ],
      "lines_removed": [],
      "context_before": [
        "                - 'document_overlap': Traditional Jaccard similarity (default)",
        "                - 'semantic': Connect via semantic relations between members",
        "                - 'embedding': Connect via embedding centroid similarity",
        "                - 'hybrid': Combine all three strategies for maximum connectivity",
        "            cluster_strictness: Controls clustering aggressiveness (0.0-1.0).",
        "                Lower values create fewer, larger clusters with more connections.",
        "            bridge_weight: Weight for inter-document token bridging (0.0-1.0).",
        "                Higher values help bridge topic-isolated clusters.",
        "            progress_callback: Optional ProgressReporter for custom progress tracking",
        "            show_progress: Show progress bar on console (uses stderr)"
      ],
      "context_after": [
        "",
        "        Returns:",
        "            Dict with computation statistics (concept_stats, etc.)",
        "",
        "        Example:",
        "            >>> # Default behavior (silent)",
        "            >>> processor.compute_all()",
        "            >>>",
        "            >>> # With console progress bar",
        "            >>> processor.compute_all(show_progress=True)"
      ],
      "change_type": "add"
    },
    {
      "file": "cortical/processor.py",
      "function": "class CorticalTextProcessor:",
      "start_line": 685,
      "lines_added": [
        "            >>>",
        "            >>> # With checkpointing for long-running operations",
        "            >>> processor.compute_all(checkpoint_dir='checkpoints')",
        "            >>>",
        "            >>> # Resume from checkpoint after crash/timeout",
        "            >>> processor = CorticalTextProcessor.resume_from_checkpoint('checkpoints')",
        "            >>> processor.compute_all(checkpoint_dir='checkpoints', resume=True)",
        "        # Load checkpoint progress if resuming",
        "        completed_phases: Set[str] = set()",
        "        if resume and checkpoint_dir:",
        "            completed_phases = self._load_checkpoint_progress(checkpoint_dir)",
        "            if verbose and completed_phases:",
        "                logger.info(f\"Resuming from checkpoint with {len(completed_phases)} completed phases\")",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "            ...         lambda phase, pct, msg: print(f\"{phase}: {pct}%\")",
        "            ...     )",
        "            ... )",
        "            >>>",
        "            >>> # Maximum connectivity for diverse documents",
        "            >>> processor.compute_all(",
        "            ...     connection_strategy='hybrid',",
        "            ...     cluster_strictness=0.5,",
        "            ...     bridge_weight=0.3",
        "            ... )"
      ],
      "context_after": [
        "        \"\"\"",
        "        stats: Dict[str, Any] = {}",
        "",
        "        # Set up progress reporter",
        "        if progress_callback:",
        "            reporter = progress_callback",
        "        elif show_progress:",
        "            reporter = ConsoleProgressReporter()",
        "        else:",
        "            reporter = SilentProgressReporter()",
        "",
        "        # Define phase weights based on typical execution times",
        "        # These are estimates and may vary based on corpus size"
      ],
      "change_type": "add"
    },
    {
      "file": "cortical/processor.py",
      "function": "class CorticalTextProcessor:",
      "start_line": 719,
      "lines_added": [
        "        phase_name = \"activation_propagation\"",
        "        if phase_name in completed_phases:",
        "            if verbose:",
        "                logger.info(\"  Skipping activation propagation (already checkpointed)\")",
        "        else:",
        "            progress.start_phase(\"Activation propagation\")",
        "            if verbose:",
        "                logger.info(\"Computing activation propagation...\")",
        "            self.propagate_activation(verbose=False)",
        "            progress.update(100)",
        "            progress.complete_phase()",
        "",
        "            # Save checkpoint after phase",
        "            if checkpoint_dir:",
        "                self._save_checkpoint(checkpoint_dir, phase_name, verbose=verbose)",
        "        phase_name = f\"pagerank_{pagerank_method}\"",
        "        if phase_name in completed_phases:",
        "                logger.info(f\"  Skipping PageRank computation ({pagerank_method}) (already checkpointed)\")",
        "        else:",
        "            progress.start_phase(\"PageRank computation\")",
        "            if pagerank_method == 'semantic':",
        "                # Extract semantic relations if not already done",
        "                if not self.semantic_relations:",
        "                    if verbose:",
        "                        logger.info(\"Extracting semantic relations...\")",
        "                    progress.update(30, \"Extracting semantic relations\")",
        "                    self.extract_corpus_semantics(verbose=False)",
        "                if verbose:",
        "                    logger.info(\"Computing importance (Semantic PageRank)...\")",
        "                progress.update(70, \"Computing semantic PageRank\")",
        "                self.compute_semantic_importance(verbose=False)",
        "            elif pagerank_method == 'hierarchical':",
        "                if verbose:",
        "                    logger.info(\"Computing importance (Hierarchical PageRank)...\")",
        "                progress.update(50, \"Computing hierarchical PageRank\")",
        "                self.compute_hierarchical_importance(verbose=False)",
        "            else:",
        "                if verbose:",
        "                    logger.info(\"Computing importance (PageRank)...\")",
        "                progress.update(50, \"Computing PageRank\")",
        "                self.compute_importance(verbose=False)",
        "            progress.update(100)",
        "            progress.complete_phase()",
        "",
        "            # Save checkpoint after phase",
        "            if checkpoint_dir:",
        "                self._save_checkpoint(checkpoint_dir, phase_name, verbose=verbose)",
        "",
        "        # Phase 3: TF-IDF",
        "        phase_name = \"tfidf\"",
        "        if phase_name in completed_phases:",
        "                logger.info(\"  Skipping TF-IDF computation (already checkpointed)\")",
        "            progress.start_phase(\"TF-IDF computation\")",
        "                logger.info(\"Computing TF-IDF...\")",
        "            self.compute_tfidf(verbose=False)",
        "            progress.update(100)",
        "            progress.complete_phase()",
        "            # Save checkpoint after phase",
        "            if checkpoint_dir:",
        "                self._save_checkpoint(checkpoint_dir, phase_name, verbose=verbose)",
        "        phase_name = \"document_connections\"",
        "        if phase_name in completed_phases:",
        "            if verbose:",
        "                logger.info(\"  Skipping document connections (already checkpointed)\")",
        "        else:",
        "            progress.start_phase(\"Document connections\")",
        "            if verbose:",
        "                logger.info(\"Computing document connections...\")",
        "            self.compute_document_connections(verbose=False)",
        "            progress.update(100)",
        "            progress.complete_phase()",
        "            # Save checkpoint after phase",
        "            if checkpoint_dir:",
        "                self._save_checkpoint(checkpoint_dir, phase_name, verbose=verbose)",
        "        # Phase 5: Bigram connections",
        "        phase_name = \"bigram_connections\"",
        "        if phase_name in completed_phases:",
        "                logger.info(\"  Skipping bigram connections (already checkpointed)\")",
        "        else:",
        "            progress.start_phase(\"Bigram connections\")",
        "            if verbose:",
        "                logger.info(\"Computing bigram connections...\")",
        "            self.compute_bigram_connections(verbose=False)",
        "            # Save checkpoint after phase",
        "            if checkpoint_dir:",
        "                self._save_checkpoint(checkpoint_dir, phase_name, verbose=verbose)",
        "",
        "        if build_concepts:",
        "            # Phase 6: Concept clustering",
        "            phase_name = \"concept_clustering\"",
        "            if phase_name in completed_phases:",
        "                if verbose:",
        "                    logger.info(\"  Skipping concept clustering (already checkpointed)\")",
        "                # Load clusters count from previous run if available",
        "                stats['clusters_created'] = len([c for c in self.layers[CorticalLayer.CONCEPTS].minicolumns.values()])",
        "            else:",
        "                progress.start_phase(\"Concept clustering\")",
        "                if verbose:",
        "                    logger.info(\"Building concept clusters...\")",
        "                clusters = self.build_concept_clusters(",
        "                    cluster_strictness=cluster_strictness,",
        "                    bridge_weight=bridge_weight,",
        "                    verbose=False",
        "                )",
        "                stats['clusters_created'] = len(clusters)",
        "                progress.update(100)",
        "                progress.complete_phase()",
        "",
        "                # Save checkpoint after phase",
        "                if checkpoint_dir:",
        "                    self._save_checkpoint(checkpoint_dir, phase_name, verbose=verbose)",
        "",
        "                phase_name = \"semantic_extraction\"",
        "                if phase_name in completed_phases:",
        "                    if verbose:",
        "                        logger.info(\"  Skipping semantic extraction (already checkpointed)\")",
        "                else:",
        "                    progress.start_phase(\"Semantic extraction\")",
        "                    if verbose:",
        "                        logger.info(\"Extracting semantic relations...\")",
        "                    self.extract_corpus_semantics(verbose=False)",
        "                    progress.update(100)",
        "                    progress.complete_phase()",
        "",
        "                    # Save checkpoint after phase",
        "                    if checkpoint_dir:",
        "                        self._save_checkpoint(checkpoint_dir, phase_name, verbose=verbose)",
        "                phase_name = \"graph_embeddings\"",
        "                if phase_name in completed_phases:",
        "                    if verbose:",
        "                        logger.info(\"  Skipping graph embeddings (already checkpointed)\")",
        "                else:",
        "                    progress.start_phase(\"Graph embeddings\")",
        "                    if verbose:",
        "                        logger.info(\"Computing graph embeddings...\")",
        "                    self.compute_graph_embeddings(verbose=False)",
        "                    progress.update(100)",
        "                    progress.complete_phase()",
        "",
        "                    # Save checkpoint after phase",
        "                    if checkpoint_dir:",
        "                        self._save_checkpoint(checkpoint_dir, phase_name, verbose=verbose)",
        "            phase_name = f\"concept_connections_{connection_strategy}\"",
        "            if phase_name in completed_phases:",
        "                if verbose:",
        "                    logger.info(f\"  Skipping concept connections ({connection_strategy}) (already checkpointed)\")",
        "                # Try to get stats from existing connections",
        "                stats['concept_connections'] = {'strategy': connection_strategy}",
        "            else:",
        "                progress.start_phase(\"Concept connections\")",
        "                if verbose:",
        "                    logger.info(f\"Computing concept connections ({connection_strategy})...\")",
        "                concept_stats = self.compute_concept_connections(",
        "                    use_member_semantics=use_member_semantics,",
        "                    use_embedding_similarity=use_embedding_similarity,",
        "                    min_shared_docs=min_shared_docs,",
        "                    min_jaccard=min_jaccard,",
        "                    verbose=False",
        "                )",
        "                stats['concept_connections'] = concept_stats",
        "                progress.update(100)",
        "                progress.complete_phase()",
        "",
        "                # Save checkpoint after phase",
        "                if checkpoint_dir:",
        "                    self._save_checkpoint(checkpoint_dir, phase_name, verbose=verbose)",
        "",
        "    def _save_checkpoint(self, checkpoint_dir: str, completed_phase: str, verbose: bool = True) -> None:",
        "        \"\"\"",
        "        Save checkpoint after completing a phase.",
        "",
        "        Args:",
        "            checkpoint_dir: Directory to save checkpoint files",
        "            completed_phase: Name of the phase that was just completed",
        "            verbose: Print progress messages",
        "        \"\"\"",
        "        checkpoint_path = Path(checkpoint_dir)",
        "        checkpoint_path.mkdir(parents=True, exist_ok=True)",
        "",
        "        # Save current state using save_json",
        "        self.save_json(checkpoint_dir, force=True, verbose=False)",
        "",
        "        # Track completed phases in a separate progress file",
        "        progress_file = checkpoint_path / 'checkpoint_progress.json'",
        "        progress_data = {",
        "            'completed_phases': [],",
        "            'last_updated': datetime.now().isoformat()",
        "        }",
        "",
        "        # Load existing progress if it exists",
        "        if progress_file.exists():",
        "            try:",
        "                with open(progress_file, 'r', encoding='utf-8') as f:",
        "                    progress_data = json.load(f)",
        "            except (json.JSONDecodeError, IOError):",
        "                pass  # Use fresh progress data",
        "",
        "        # Add newly completed phase",
        "        if completed_phase not in progress_data['completed_phases']:",
        "            progress_data['completed_phases'].append(completed_phase)",
        "            progress_data['last_updated'] = datetime.now().isoformat()",
        "",
        "        # Write progress file atomically",
        "        temp_progress_file = progress_file.with_suffix('.json.tmp')",
        "        try:",
        "            with open(temp_progress_file, 'w', encoding='utf-8') as f:",
        "                json.dump(progress_data, f, indent=2, ensure_ascii=False)",
        "            temp_progress_file.replace(progress_file)",
        "        except Exception:",
        "            if temp_progress_file.exists():",
        "                temp_progress_file.unlink()",
        "            raise",
        "",
        "        if verbose:",
        "            logger.info(f\"  Checkpoint saved: {completed_phase}\")",
        "",
        "    def _load_checkpoint_progress(self, checkpoint_dir: str) -> Set[str]:",
        "        \"\"\"",
        "        Load completed phases from checkpoint directory.",
        "",
        "        Args:",
        "            checkpoint_dir: Directory containing checkpoint files",
        "",
        "        Returns:",
        "            Set of completed phase names",
        "        \"\"\"",
        "        progress_file = Path(checkpoint_dir) / 'checkpoint_progress.json'",
        "        if not progress_file.exists():",
        "            return set()",
        "",
        "        try:",
        "            with open(progress_file, 'r', encoding='utf-8') as f:",
        "                data = json.load(f)",
        "            return set(data.get('completed_phases', []))",
        "        except (json.JSONDecodeError, IOError) as e:",
        "            logger.warning(f\"Failed to load checkpoint progress: {e}\")",
        "            return set()",
        "",
        "    @classmethod",
        "    def resume_from_checkpoint(",
        "        cls,",
        "        checkpoint_dir: str,",
        "        config: Optional[CorticalConfig] = None,",
        "        verbose: bool = True",
        "    ) -> 'CorticalTextProcessor':",
        "        \"\"\"",
        "        Resume processing from a checkpoint directory.",
        "",
        "        This is a convenience method that loads the processor state from",
        "        a checkpoint directory created by compute_all() with checkpointing enabled.",
        "",
        "        Args:",
        "            checkpoint_dir: Directory containing checkpoint files",
        "            config: Optional configuration (default: uses CorticalConfig defaults)",
        "            verbose: Print progress messages",
        "",
        "        Returns:",
        "            Reconstructed CorticalTextProcessor instance ready to resume computation",
        "",
        "        Raises:",
        "            FileNotFoundError: If checkpoint directory doesn't exist",
        "",
        "        Example:",
        "            >>> # After a crash during compute_all()",
        "            >>> processor = CorticalTextProcessor.resume_from_checkpoint('checkpoints')",
        "            >>> # Continue from where it left off",
        "            >>> processor.compute_all(checkpoint_dir='checkpoints', resume=True)",
        "        \"\"\"",
        "        if verbose:",
        "            logger.info(f\"Resuming from checkpoint: {checkpoint_dir}\")",
        "",
        "        # Load the processor state from JSON",
        "        processor = cls.load_json(checkpoint_dir, config=config, verbose=verbose)",
        "",
        "        # Load and display progress",
        "        progress = processor._load_checkpoint_progress(checkpoint_dir)",
        "        if verbose and progress:",
        "            logger.info(f\"Found {len(progress)} completed phases: {', '.join(sorted(progress))}\")",
        "",
        "        return processor",
        ""
      ],
      "lines_removed": [
        "        progress.start_phase(\"Activation propagation\")",
        "        if verbose:",
        "            logger.info(\"Computing activation propagation...\")",
        "        self.propagate_activation(verbose=False)",
        "        progress.update(100)",
        "        progress.complete_phase()",
        "        progress.start_phase(\"PageRank computation\")",
        "        if pagerank_method == 'semantic':",
        "            # Extract semantic relations if not already done",
        "            if not self.semantic_relations:",
        "                if verbose:",
        "                    logger.info(\"Extracting semantic relations...\")",
        "                progress.update(30, \"Extracting semantic relations\")",
        "                self.extract_corpus_semantics(verbose=False)",
        "                logger.info(\"Computing importance (Semantic PageRank)...\")",
        "            progress.update(70, \"Computing semantic PageRank\")",
        "            self.compute_semantic_importance(verbose=False)",
        "        elif pagerank_method == 'hierarchical':",
        "                logger.info(\"Computing importance (Hierarchical PageRank)...\")",
        "            progress.update(50, \"Computing hierarchical PageRank\")",
        "            self.compute_hierarchical_importance(verbose=False)",
        "                logger.info(\"Computing importance (PageRank)...\")",
        "            progress.update(50, \"Computing PageRank\")",
        "            self.compute_importance(verbose=False)",
        "        progress.update(100)",
        "        progress.complete_phase()",
        "        # Phase 3: TF-IDF",
        "        progress.start_phase(\"TF-IDF computation\")",
        "        if verbose:",
        "            logger.info(\"Computing TF-IDF...\")",
        "        self.compute_tfidf(verbose=False)",
        "        progress.update(100)",
        "        progress.complete_phase()",
        "        progress.start_phase(\"Document connections\")",
        "        if verbose:",
        "            logger.info(\"Computing document connections...\")",
        "        self.compute_document_connections(verbose=False)",
        "        progress.update(100)",
        "        progress.complete_phase()",
        "        # Phase 5: Bigram connections",
        "        progress.start_phase(\"Bigram connections\")",
        "        if verbose:",
        "            logger.info(\"Computing bigram connections...\")",
        "        self.compute_bigram_connections(verbose=False)",
        "        progress.update(100)",
        "        progress.complete_phase()",
        "        if build_concepts:",
        "            # Phase 6: Concept clustering",
        "            progress.start_phase(\"Concept clustering\")",
        "                logger.info(\"Building concept clusters...\")",
        "            clusters = self.build_concept_clusters(",
        "                cluster_strictness=cluster_strictness,",
        "                bridge_weight=bridge_weight,",
        "                verbose=False",
        "            )",
        "            stats['clusters_created'] = len(clusters)",
        "                progress.start_phase(\"Semantic extraction\")",
        "                if verbose:",
        "                    logger.info(\"Extracting semantic relations...\")",
        "                self.extract_corpus_semantics(verbose=False)",
        "                progress.update(100)",
        "                progress.complete_phase()",
        "                progress.start_phase(\"Graph embeddings\")",
        "                if verbose:",
        "                    logger.info(\"Computing graph embeddings...\")",
        "                self.compute_graph_embeddings(verbose=False)",
        "                progress.update(100)",
        "                progress.complete_phase()",
        "            progress.start_phase(\"Concept connections\")",
        "            if verbose:",
        "                logger.info(f\"Computing concept connections ({connection_strategy})...\")",
        "            concept_stats = self.compute_concept_connections(",
        "                use_member_semantics=use_member_semantics,",
        "                use_embedding_similarity=use_embedding_similarity,",
        "                min_shared_docs=min_shared_docs,",
        "                min_jaccard=min_jaccard,",
        "                verbose=False",
        "            )",
        "            stats['concept_connections'] = concept_stats",
        "            progress.update(100)",
        "            progress.complete_phase()",
        "    "
      ],
      "context_before": [
        "            if connection_strategy in ('semantic', 'hybrid'):",
        "                phase_weights[\"Semantic extraction\"] = 10",
        "            if connection_strategy in ('embedding', 'hybrid'):",
        "                phase_weights[\"Graph embeddings\"] = 10",
        "            phase_weights[\"Concept connections\"] = 15",
        "",
        "        # Create multi-phase progress tracker",
        "        progress = MultiPhaseProgress(reporter, phase_weights)",
        "",
        "        # Phase 1: Activation propagation"
      ],
      "context_after": [
        "",
        "        # Phase 2: PageRank (varies by method)",
        "            if verbose:",
        "            if verbose:",
        "        else:",
        "            if verbose:",
        "",
        "",
        "        # Phase 4: Document connections",
        "",
        "",
        "            if verbose:",
        "            progress.update(100)",
        "            progress.complete_phase()",
        "",
        "            # Determine connection parameters based on strategy",
        "            use_member_semantics = connection_strategy in ('semantic', 'hybrid')",
        "            use_embedding_similarity = connection_strategy in ('embedding', 'hybrid')",
        "",
        "            # Phase 7: Semantic extraction (if needed)",
        "            if use_member_semantics and not self.semantic_relations:",
        "",
        "            # Phase 8: Graph embeddings (if needed)",
        "            if use_embedding_similarity and not self.embeddings:",
        "",
        "            # Set thresholds based on strategy",
        "            if connection_strategy == 'hybrid':",
        "                min_shared_docs = 0",
        "                min_jaccard = 0.0",
        "            elif connection_strategy in ('semantic', 'embedding'):",
        "                min_shared_docs = 0",
        "                min_jaccard = 0.0",
        "            else:  # document_overlap",
        "                min_shared_docs = 1",
        "                min_jaccard = 0.1",
        "",
        "            # Phase 9: Concept connections",
        "",
        "        # Mark core computations as fresh",
        "        fresh_comps = [",
        "            self.COMP_ACTIVATION,",
        "            self.COMP_PAGERANK,",
        "            self.COMP_TFIDF,",
        "            self.COMP_DOC_CONNECTIONS,",
        "            self.COMP_BIGRAM_CONNECTIONS,",
        "        ]",
        "        if build_concepts:",
        "            fresh_comps.append(self.COMP_CONCEPTS)",
        "        self._mark_fresh(*fresh_comps)",
        "",
        "        # Invalidate query cache since corpus state changed",
        "        self._query_expansion_cache.clear()",
        "",
        "        if verbose:",
        "            logger.info(\"Done.\")",
        "",
        "        return stats",
        "    def propagate_activation(self, iterations: int = 3, decay: float = 0.8, verbose: bool = True) -> None:",
        "        analysis.propagate_activation(self.layers, iterations, decay)",
        "        if verbose: logger.info(f\"Propagated activation ({iterations} iterations)\")",
        "",
        "    def compute_importance(self, verbose: bool = True) -> None:",
        "        for layer_enum in [CorticalLayer.TOKENS, CorticalLayer.BIGRAMS]:",
        "            analysis.compute_pagerank(self.layers[layer_enum])",
        "        if verbose: logger.info(\"Computed PageRank importance\")",
        "",
        "    def compute_semantic_importance("
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/processor.py",
      "function": "class CorticalTextProcessor:",
      "start_line": 2669,
      "lines_added": [
        "",
        "    def save_json(self, state_dir: str, force: bool = False, verbose: bool = True) -> Dict[str, bool]:",
        "        \"\"\"",
        "        Save processor state to git-friendly JSON format.",
        "",
        "        Instead of a single monolithic pickle file, creates a directory with:",
        "        - manifest.json: Version, checksums, staleness tracking",
        "        - documents.json: Document content and metadata",
        "        - layers/*.json: One file per layer (tokens, bigrams, concepts, documents)",
        "        - computed/*.json: Semantic relations and embeddings",
        "",
        "        Benefits over pickle:",
        "        - Git-friendly (can diff changes)",
        "        - No merge conflicts (incremental updates)",
        "        - Language/version independent (JSON)",
        "        - Only rewrites changed components",
        "",
        "        Args:",
        "            state_dir: Directory to write JSON state files",
        "            force: Force save even if unchanged (default: False)",
        "            verbose: Print progress messages (default: True)",
        "",
        "        Returns:",
        "            Dictionary mapping component names to whether they were written:",
        "            {'layer_0': True, 'layer_1': False, 'documents': True, ...}",
        "",
        "        Example:",
        "            >>> processor.save_json('corpus_state')",
        "            ✓ Saved state to corpus_state (3 files updated)",
        "",
        "            >>> # Incremental save only updates changed components",
        "            >>> processor.add_document(\"doc4\", \"New content\")",
        "            >>> results = processor.save_json('corpus_state')",
        "            >>> # Only documents.json and affected layers are rewritten",
        "        \"\"\"",
        "        writer = state_storage.StateWriter(state_dir)",
        "",
        "        # Prepare stale computations",
        "        stale = self._stale_computations if hasattr(self, '_stale_computations') else set()",
        "",
        "        return writer.save_all(",
        "            layers=self.layers,",
        "            documents=self.documents,",
        "            document_metadata=self.document_metadata,",
        "            embeddings=self.embeddings,",
        "            semantic_relations=self.semantic_relations,",
        "            stale_computations=stale,",
        "            force=force,",
        "            verbose=verbose",
        "        )",
        "",
        "    @classmethod",
        "    def load_json(cls, state_dir: str, config: Optional[CorticalConfig] = None, verbose: bool = True) -> 'CorticalTextProcessor':",
        "        \"\"\"",
        "        Load processor from git-friendly JSON format.",
        "",
        "        Reconstructs the processor from JSON state files created by save_json().",
        "        If config is not provided, uses defaults (config is not stored in JSON state).",
        "",
        "        Args:",
        "            state_dir: Directory containing JSON state files",
        "            config: Optional configuration (default: uses CorticalConfig defaults)",
        "            verbose: Print progress messages (default: True)",
        "",
        "        Returns:",
        "            Reconstructed CorticalTextProcessor instance",
        "",
        "        Raises:",
        "            FileNotFoundError: If state directory or required files don't exist",
        "            ValueError: If state format is invalid",
        "",
        "        Example:",
        "            >>> processor = CorticalTextProcessor.load_json('corpus_state')",
        "            Loading state from corpus_state",
        "              Loaded L0_tokens.json: 1523 minicolumns",
        "              Loaded L1_bigrams.json: 847 minicolumns",
        "              ...",
        "            ✓ Loaded state from corpus_state",
        "",
        "            >>> # Use custom config",
        "            >>> config = CorticalConfig(chunk_size=300)",
        "            >>> processor = CorticalTextProcessor.load_json('corpus_state', config=config)",
        "        \"\"\"",
        "        loader = state_storage.StateLoader(state_dir)",
        "",
        "        # Load all state components",
        "        layers, documents, metadata, embeddings, relations, manifest_data = loader.load_all(",
        "            validate=True,",
        "            verbose=verbose",
        "        )",
        "",
        "        # Create processor with provided or default config",
        "        processor = cls(config=config)",
        "",
        "        # Restore state",
        "        processor.layers = layers",
        "        processor.documents = documents",
        "        processor.document_metadata = metadata",
        "        processor.embeddings = embeddings",
        "        processor.semantic_relations = relations",
        "",
        "        # Restore staleness tracking",
        "        if 'stale_computations' in manifest_data:",
        "            processor._stale_computations = manifest_data['stale_computations']",
        "",
        "        return processor",
        "",
        "    def migrate_to_json(self, pkl_path: str, json_dir: str, verbose: bool = True) -> bool:",
        "        \"\"\"",
        "        Migrate existing pickle file to git-friendly JSON format.",
        "",
        "        This is a convenience method that loads a pickle file and saves it as JSON.",
        "        Useful for converting legacy .pkl files to the new JSON format.",
        "",
        "        Args:",
        "            pkl_path: Path to existing .pkl file",
        "            json_dir: Directory to write JSON state",
        "            verbose: Print progress messages (default: True)",
        "",
        "        Returns:",
        "            True if migration successful",
        "",
        "        Raises:",
        "            FileNotFoundError: If pkl file doesn't exist",
        "",
        "        Example:",
        "            >>> # Convert old pickle file to JSON",
        "            >>> processor = CorticalTextProcessor.load('corpus.pkl')",
        "            >>> processor.migrate_to_json('corpus.pkl', 'corpus_state')",
        "            Migrating corpus.pkl to corpus_state",
        "              Saved L0_tokens.json: 1523 minicolumns",
        "              Saved L1_bigrams.json: 847 minicolumns",
        "              ...",
        "            ✓ Migration complete: corpus.pkl → corpus_state",
        "",
        "            >>> # Or use the standalone function",
        "            >>> from cortical.state_storage import migrate_pkl_to_json",
        "            >>> migrate_pkl_to_json('corpus.pkl', 'corpus_state')",
        "        \"\"\"",
        "        return state_storage.migrate_pkl_to_json(pkl_path, json_dir, verbose=verbose)",
        ""
      ],
      "lines_removed": [
        "    "
      ],
      "context_before": [
        "                # Fall back to default config if restoration fails",
        "                config = None",
        "",
        "        processor = cls(config=config)",
        "        processor.layers = layers",
        "        processor.documents = documents",
        "        processor.document_metadata = document_metadata",
        "        processor.embeddings = embeddings",
        "        processor.semantic_relations = semantic_relations",
        "        return processor"
      ],
      "context_after": [
        "    def export_graph(self, filepath: str, layer: Optional[CorticalLayer] = None, max_nodes: int = 500) -> Dict:",
        "        return persistence.export_graph_json(filepath, self.layers, layer, max_nodes=max_nodes)",
        "",
        "    def export_conceptnet_json(",
        "        self,",
        "        filepath: str,",
        "        include_cross_layer: bool = True,",
        "        include_typed_edges: bool = True,",
        "        min_weight: float = 0.0,",
        "        min_confidence: float = 0.0,"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/query/search.py",
      "function": "def find_documents_for_query(",
      "start_line": 42,
      "lines_added": [
        "    layer3 = layers[CorticalLayer.DOCUMENTS]"
      ],
      "lines_removed": [],
      "context_before": [
        "        top_n: Number of documents to return",
        "        use_expansion: Whether to expand query terms using lateral connections",
        "        semantic_relations: Optional list of semantic relations for expansion",
        "        use_semantic: Whether to use semantic relations for expansion (if available)",
        "        doc_name_boost: Multiplier for documents whose name matches query terms (default 2.0)",
        "",
        "    Returns:",
        "        List of (doc_id, score) tuples ranked by relevance",
        "    \"\"\"",
        "    layer0 = layers[CorticalLayer.TOKENS]"
      ],
      "context_after": [
        "",
        "    query_terms = get_expanded_query_terms(",
        "        query_text, layers, tokenizer,",
        "        use_expansion=use_expansion,",
        "        semantic_relations=semantic_relations,",
        "        use_semantic=use_semantic",
        "    )",
        "",
        "    # Score each document",
        "    doc_scores: Dict[str, float] = defaultdict(float)"
      ],
      "change_type": "add"
    },
    {
      "file": "cortical/query/search.py",
      "function": "def find_documents_for_query(",
      "start_line": 70,
      "lines_added": [
        "            # Use cached tokenized name if available, otherwise tokenize on-the-fly",
        "            doc_col = layer3.get_by_id(f\"L3_{doc_id}\")",
        "            if doc_col and hasattr(doc_col, 'name_tokens') and doc_col.name_tokens is not None:",
        "                doc_name_tokens = doc_col.name_tokens",
        "            else:",
        "                # Fallback for old data without cached tokens (or mock objects in tests)",
        "                doc_name_tokens = set(tokenizer.tokenize(doc_id.replace('_', ' ')))"
      ],
      "lines_removed": [
        "            # Tokenize document ID (handle underscores as separators)",
        "            doc_name_tokens = set(tokenizer.tokenize(doc_id.replace('_', ' ')))"
      ],
      "context_before": [
        "    # Boost documents whose name matches query terms",
        "    if doc_name_boost > 1.0 and doc_scores:",
        "        query_tokens = set(tokenizer.tokenize(query_text))",
        "        max_score = max(doc_scores.values()) if doc_scores else 0.0",
        "",
        "        # First pass: identify exact and partial matches",
        "        exact_matches = []",
        "        partial_matches = []",
        "",
        "        for doc_id in doc_scores:"
      ],
      "context_after": [
        "            # Count how many query tokens appear in doc name",
        "            matches = len(query_tokens & doc_name_tokens)",
        "            if matches > 0:",
        "                match_ratio = matches / len(query_tokens) if query_tokens else 0",
        "",
        "                if match_ratio == 1.0:",
        "                    exact_matches.append(doc_id)",
        "                else:",
        "                    partial_matches.append((doc_id, match_ratio))",
        ""
      ],
      "change_type": "modify"
    },
    {
      "file": "scripts/session_context.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "#!/usr/bin/env python3",
        "\"\"\"",
        "Generate session context summaries for new agent sessions.",
        "",
        "This script provides a \"catch-up\" summary from task files and git history,",
        "reducing cold-start time for new agents joining the repository.",
        "",
        "Usage:",
        "    # Print context to stdout",
        "    python scripts/session_context.py",
        "",
        "    # Output JSON format",
        "    python scripts/session_context.py --json",
        "",
        "    # Custom time range",
        "    python scripts/session_context.py --days 14",
        "",
        "    # Save to file",
        "    python scripts/session_context.py --output context.md",
        "",
        "Example Output:",
        "    # Session Context (generated 2025-12-14 01:30:00)",
        "",
        "    ## Recent Work (last 5 sessions)",
        "    - [e233] completed: 2 tasks, pending: 1 tasks",
        "    - [2d89] completed: 1 tasks, pending: 0 tasks",
        "",
        "    ## Pending Tasks (by priority)",
        "    ### High",
        "    - T-xxxx-001: Task title",
        "",
        "    ## Recent Changes (last 7 days)",
        "    - cortical/: 5 files modified",
        "    - scripts/: 3 files added",
        "",
        "    ## Recent Commits",
        "    - abc1234: Fix validation bug (2 hours ago)",
        "\"\"\"",
        "",
        "import argparse",
        "import json",
        "import subprocess",
        "import sys",
        "from collections import defaultdict",
        "from datetime import datetime, timedelta",
        "from pathlib import Path",
        "from typing import Dict, List, Optional, Tuple, Any",
        "",
        "# Add parent directory to path for imports",
        "sys.path.insert(0, str(Path(__file__).parent))",
        "",
        "from task_utils import (",
        "    Task, TaskSession, load_all_tasks, DEFAULT_TASKS_DIR",
        ")",
        "",
        "",
        "class SessionContextGenerator:",
        "    \"\"\"Generate catch-up summaries from task files and git history.\"\"\"",
        "",
        "    def __init__(self, repo_path: str = '.'):",
        "        \"\"\"",
        "        Initialize the context generator.",
        "",
        "        Args:",
        "            repo_path: Path to git repository root",
        "        \"\"\"",
        "        self.repo_path = Path(repo_path)",
        "        self.tasks_dir = self.repo_path / DEFAULT_TASKS_DIR",
        "",
        "    def get_recent_sessions(self, n: int = 5) -> List[Dict[str, Any]]:",
        "        \"\"\"",
        "        Get N most recent task sessions from tasks/*.json files.",
        "",
        "        Args:",
        "            n: Number of recent sessions to retrieve",
        "",
        "        Returns:",
        "            List of session summaries with counts by status",
        "        \"\"\"",
        "        if not self.tasks_dir.exists():",
        "            return []",
        "",
        "        sessions = []",
        "        session_files = sorted(",
        "            self.tasks_dir.glob(\"*.json\"),",
        "            key=lambda p: p.stat().st_mtime,",
        "            reverse=True",
        "        )",
        "",
        "        for filepath in session_files[:n]:",
        "            try:",
        "                session = TaskSession.load(filepath)",
        "",
        "                # Count tasks by status",
        "                status_counts = defaultdict(int)",
        "                for task in session.tasks:",
        "                    status_counts[task.status] += 1",
        "",
        "                sessions.append({",
        "                    'session_id': session.session_id,",
        "                    'started_at': session.started_at,",
        "                    'filename': filepath.name,",
        "                    'total': len(session.tasks),",
        "                    'completed': status_counts.get('completed', 0),",
        "                    'in_progress': status_counts.get('in_progress', 0),",
        "                    'pending': status_counts.get('pending', 0),",
        "                    'deferred': status_counts.get('deferred', 0),",
        "                })",
        "            except (json.JSONDecodeError, KeyError) as e:",
        "                # Skip corrupted files",
        "                continue",
        "",
        "        return sessions",
        "",
        "    def get_pending_tasks(self) -> Dict[str, List[Task]]:",
        "        \"\"\"",
        "        Get all pending tasks grouped by priority.",
        "",
        "        Returns:",
        "            Dict of {priority: [tasks]} sorted within each priority",
        "        \"\"\"",
        "        all_tasks = load_all_tasks(str(self.tasks_dir))",
        "",
        "        # Group pending and in_progress tasks by priority",
        "        by_priority = {",
        "            'high': [],",
        "            'medium': [],",
        "            'low': []",
        "        }",
        "",
        "        for task in all_tasks:",
        "            if task.status in ('pending', 'in_progress'):",
        "                priority = task.priority if task.priority in by_priority else 'medium'",
        "                by_priority[priority].append(task)",
        "",
        "        # Sort each priority group by creation time",
        "        for priority in by_priority:",
        "            by_priority[priority].sort(key=lambda t: t.created_at)",
        "",
        "        return by_priority",
        "",
        "    def get_recent_commits(self, n: int = 10) -> List[Dict[str, str]]:",
        "        \"\"\"",
        "        Get N recent commits with metadata.",
        "",
        "        Args:",
        "            n: Number of commits to retrieve",
        "",
        "        Returns:",
        "            List of commit dicts with hash, time_ago, subject, files_changed",
        "        \"\"\"",
        "        try:",
        "            # Get commit info with relative time",
        "            result = subprocess.run(",
        "                ['git', 'log', f'--max-count={n}', '--format=%h|%ar|%s', '--no-merges'],",
        "                cwd=self.repo_path,",
        "                capture_output=True,",
        "                text=True,",
        "                check=True",
        "            )",
        "",
        "            commits = []",
        "            for line in result.stdout.strip().split('\\n'):",
        "                if not line:",
        "                    continue",
        "",
        "                parts = line.split('|', 2)",
        "                if len(parts) == 3:",
        "                    commit_hash, time_ago, subject = parts",
        "",
        "                    # Get files changed for this commit",
        "                    files_result = subprocess.run(",
        "                        ['git', 'diff-tree', '--no-commit-id', '--name-only', '-r', commit_hash],",
        "                        cwd=self.repo_path,",
        "                        capture_output=True,",
        "                        text=True,",
        "                        check=True",
        "                    )",
        "",
        "                    files = [f for f in files_result.stdout.strip().split('\\n') if f]",
        "",
        "                    commits.append({",
        "                        'hash': commit_hash,",
        "                        'time_ago': time_ago,",
        "                        'subject': subject,",
        "                        'files_changed': len(files),",
        "                        'files': files",
        "                    })",
        "",
        "            return commits",
        "",
        "        except (subprocess.CalledProcessError, FileNotFoundError):",
        "            # Git not available or not a git repo",
        "            return []",
        "",
        "    def get_recent_file_changes(self, days: int = 7) -> Dict[str, List[str]]:",
        "        \"\"\"",
        "        Get files changed in last N days grouped by directory.",
        "",
        "        Args:",
        "            days: Number of days to look back",
        "",
        "        Returns:",
        "            Dict of {directory: [filenames]} for files changed",
        "        \"\"\"",
        "        try:",
        "            since_date = datetime.now() - timedelta(days=days)",
        "            since_str = since_date.strftime('%Y-%m-%d')",
        "",
        "            result = subprocess.run(",
        "                ['git', 'log', f'--since={since_str}', '--name-only', '--format=', '--no-merges'],",
        "                cwd=self.repo_path,",
        "                capture_output=True,",
        "                text=True,",
        "                check=True",
        "            )",
        "",
        "            # Group by directory",
        "            by_dir = defaultdict(set)",
        "            for line in result.stdout.strip().split('\\n'):",
        "                if not line:",
        "                    continue",
        "",
        "                path = Path(line)",
        "                # Use first directory component or 'root' if in root",
        "                if len(path.parts) > 1:",
        "                    directory = path.parts[0]",
        "                else:",
        "                    directory = 'root'",
        "",
        "                by_dir[directory].add(line)",
        "",
        "            # Convert sets to sorted lists",
        "            return {dir: sorted(files) for dir, files in by_dir.items()}",
        "",
        "        except (subprocess.CalledProcessError, FileNotFoundError):",
        "            return {}",
        "",
        "    def _count_file_changes(self, changes: Dict[str, List[str]]) -> Dict[str, Dict[str, int]]:",
        "        \"\"\"",
        "        Count files by type (added/modified/deleted) per directory.",
        "",
        "        This requires comparing with previous state, which is complex.",
        "        For now, we just count total files changed per directory.",
        "",
        "        Args:",
        "            changes: Dict from get_recent_file_changes()",
        "",
        "        Returns:",
        "            Dict of {directory: {'total': count}}",
        "        \"\"\"",
        "        return {",
        "            directory: {'total': len(files)}",
        "            for directory, files in changes.items()",
        "        }",
        "",
        "    def generate_context(",
        "        self,",
        "        sessions: int = 5,",
        "        commits: int = 10,",
        "        days: int = 7",
        "    ) -> str:",
        "        \"\"\"",
        "        Generate full markdown context summary.",
        "",
        "        Args:",
        "            sessions: Number of recent sessions to include",
        "            commits: Number of recent commits to include",
        "            days: Number of days for file change analysis",
        "",
        "        Returns:",
        "            Markdown-formatted context string",
        "        \"\"\"",
        "        lines = []",
        "        now = datetime.now().strftime('%Y-%m-%d %H:%M:%S')",
        "",
        "        lines.append(f\"# Session Context (generated {now})\")",
        "        lines.append(\"\")",
        "",
        "        # Recent sessions",
        "        recent_sessions = self.get_recent_sessions(sessions)",
        "        if recent_sessions:",
        "            lines.append(f\"## Recent Work (last {len(recent_sessions)} sessions)\")",
        "            lines.append(\"\")",
        "            for session in recent_sessions:",
        "                started = datetime.fromisoformat(session['started_at']).strftime('%Y-%m-%d %H:%M')",
        "                lines.append(",
        "                    f\"- [{session['session_id']}] {started}: \"",
        "                    f\"{session['completed']} completed, \"",
        "                    f\"{session['in_progress']} in progress, \"",
        "                    f\"{session['pending']} pending\"",
        "                )",
        "            lines.append(\"\")",
        "        else:",
        "            lines.append(\"## Recent Work\")",
        "            lines.append(\"\")",
        "            lines.append(\"No task sessions found.\")",
        "            lines.append(\"\")",
        "",
        "        # Pending tasks by priority",
        "        pending_by_priority = self.get_pending_tasks()",
        "        total_pending = sum(len(tasks) for tasks in pending_by_priority.values())",
        "",
        "        if total_pending > 0:",
        "            lines.append(f\"## Pending Tasks ({total_pending} total)\")",
        "            lines.append(\"\")",
        "",
        "            for priority in ['high', 'medium', 'low']:",
        "                tasks = pending_by_priority.get(priority, [])",
        "                if tasks:",
        "                    lines.append(f\"### {priority.capitalize()} Priority\")",
        "                    lines.append(\"\")",
        "                    for task in tasks[:10]:  # Limit to 10 per priority",
        "                        status_marker = \"🔄\" if task.status == 'in_progress' else \"📋\"",
        "                        lines.append(f\"- {status_marker} **{task.id}**: {task.title}\")",
        "                        if task.description:",
        "                            # Show first 100 chars of description",
        "                            desc = task.description[:100]",
        "                            if len(task.description) > 100:",
        "                                desc += \"...\"",
        "                            lines.append(f\"  {desc}\")",
        "                    lines.append(\"\")",
        "        else:",
        "            lines.append(\"## Pending Tasks\")",
        "            lines.append(\"\")",
        "            lines.append(\"No pending tasks found.\")",
        "            lines.append(\"\")",
        "",
        "        # Recent file changes",
        "        file_changes = self.get_recent_file_changes(days)",
        "        if file_changes:",
        "            lines.append(f\"## Recent Changes (last {days} days)\")",
        "            lines.append(\"\")",
        "",
        "            # Sort directories by number of changes",
        "            sorted_dirs = sorted(",
        "                file_changes.items(),",
        "                key=lambda x: len(x[1]),",
        "                reverse=True",
        "            )",
        "",
        "            for directory, files in sorted_dirs[:10]:  # Top 10 directories",
        "                lines.append(f\"- **{directory}**/: {len(files)} files changed\")",
        "            lines.append(\"\")",
        "",
        "        # Recent commits",
        "        recent_commits = self.get_recent_commits(commits)",
        "        if recent_commits:",
        "            lines.append(f\"## Recent Commits (last {len(recent_commits)})\")",
        "            lines.append(\"\")",
        "            for commit in recent_commits:",
        "                lines.append(",
        "                    f\"- `{commit['hash']}`: {commit['subject']} \"",
        "                    f\"({commit['time_ago']}, {commit['files_changed']} files)\"",
        "                )",
        "            lines.append(\"\")",
        "",
        "        # Summary stats",
        "        lines.append(\"## Quick Stats\")",
        "        lines.append(\"\")",
        "        lines.append(f\"- Task sessions: {len(recent_sessions)}\")",
        "        lines.append(f\"- Pending tasks: {total_pending}\")",
        "        lines.append(f\"- Recent commits: {len(recent_commits)}\")",
        "        lines.append(f\"- Directories changed: {len(file_changes)}\")",
        "        lines.append(\"\")",
        "",
        "        return '\\n'.join(lines)",
        "",
        "    def generate_json(",
        "        self,",
        "        sessions: int = 5,",
        "        commits: int = 10,",
        "        days: int = 7",
        "    ) -> Dict[str, Any]:",
        "        \"\"\"",
        "        Generate context as JSON structure.",
        "",
        "        Args:",
        "            sessions: Number of recent sessions to include",
        "            commits: Number of recent commits to include",
        "            days: Number of days for file change analysis",
        "",
        "        Returns:",
        "            Dict with all context data",
        "        \"\"\"",
        "        return {",
        "            'generated_at': datetime.now().isoformat(),",
        "            'recent_sessions': self.get_recent_sessions(sessions),",
        "            'pending_tasks': {",
        "                priority: [t.to_dict() for t in tasks]",
        "                for priority, tasks in self.get_pending_tasks().items()",
        "            },",
        "            'recent_commits': self.get_recent_commits(commits),",
        "            'recent_file_changes': self.get_recent_file_changes(days),",
        "        }",
        "",
        "",
        "def main():",
        "    \"\"\"CLI entry point.\"\"\"",
        "    parser = argparse.ArgumentParser(",
        "        description=\"Generate session context for new agent sessions\",",
        "        formatter_class=argparse.RawDescriptionHelpFormatter,",
        "        epilog=\"\"\"",
        "Examples:",
        "  # Print context to stdout",
        "  python scripts/session_context.py",
        "",
        "  # Output as JSON",
        "  python scripts/session_context.py --json",
        "",
        "  # Analyze last 14 days",
        "  python scripts/session_context.py --days 14",
        "",
        "  # Save to file",
        "  python scripts/session_context.py --output context.md",
        "        \"\"\"",
        "    )",
        "",
        "    parser.add_argument(",
        "        '--json', action='store_true',",
        "        help='Output as JSON instead of markdown'",
        "    )",
        "    parser.add_argument(",
        "        '--days', type=int, default=7,",
        "        help='Number of days for file change analysis (default: 7)'",
        "    )",
        "    parser.add_argument(",
        "        '--sessions', type=int, default=5,",
        "        help='Number of recent sessions to include (default: 5)'",
        "    )",
        "    parser.add_argument(",
        "        '--commits', type=int, default=10,",
        "        help='Number of recent commits to include (default: 10)'",
        "    )",
        "    parser.add_argument(",
        "        '--output', '-o', type=str,",
        "        help='Output file path (default: stdout)'",
        "    )",
        "    parser.add_argument(",
        "        '--repo-path', default='.',",
        "        help='Path to git repository (default: current directory)'",
        "    )",
        "",
        "    args = parser.parse_args()",
        "",
        "    # Generate context",
        "    generator = SessionContextGenerator(args.repo_path)",
        "",
        "    if args.json:",
        "        context = generator.generate_json(",
        "            sessions=args.sessions,",
        "            commits=args.commits,",
        "            days=args.days",
        "        )",
        "        output = json.dumps(context, indent=2)",
        "    else:",
        "        output = generator.generate_context(",
        "            sessions=args.sessions,",
        "            commits=args.commits,",
        "            days=args.days",
        "        )",
        "",
        "    # Write to file or stdout",
        "    if args.output:",
        "        output_path = Path(args.output)",
        "        output_path.write_text(output)",
        "        print(f\"Context written to: {output_path}\")",
        "    else:",
        "        print(output)",
        "",
        "",
        "if __name__ == \"__main__\":",
        "    main()"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "scripts/suggest_tasks.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "#!/usr/bin/env python3",
        "\"\"\"",
        "Analyzes code changes and suggests follow-up tasks automatically.",
        "",
        "This script examines git changes (staged and/or unstaged) and applies",
        "pattern-based rules to detect when follow-up work is needed:",
        "- Missing docstrings on new methods",
        "- New tests that should be run",
        "- Performance-sensitive code changes requiring profiling",
        "- Validation logic changes requiring test verification",
        "- Documentation updates needed",
        "",
        "Usage:",
        "    python scripts/suggest_tasks.py              # Analyze staged changes",
        "    python scripts/suggest_tasks.py --all        # Include unstaged",
        "    python scripts/suggest_tasks.py --json       # JSON output",
        "    python scripts/suggest_tasks.py --create     # Create tasks in tasks/*.json",
        "",
        "Examples:",
        "    # Show suggestions for staged changes",
        "    python scripts/suggest_tasks.py",
        "",
        "    # Include unstaged changes and show detailed output",
        "    python scripts/suggest_tasks.py --all --verbose",
        "",
        "    # Create tasks from suggestions",
        "    python scripts/suggest_tasks.py --create",
        "\"\"\"",
        "",
        "import argparse",
        "import json",
        "import re",
        "import subprocess",
        "import sys",
        "from collections import defaultdict",
        "from pathlib import Path",
        "from typing import Dict, List, Optional, Set",
        "",
        "# Import task utilities if available",
        "try:",
        "    from task_utils import TaskSession",
        "    TASK_UTILS_AVAILABLE = True",
        "except ImportError:",
        "    TASK_UTILS_AVAILABLE = False",
        "",
        "",
        "class TaskSuggester:",
        "    \"\"\"Analyzes code changes and suggests follow-up tasks.\"\"\"",
        "",
        "    def __init__(self, repo_path: str = '.'):",
        "        \"\"\"",
        "        Initialize task suggester.",
        "",
        "        Args:",
        "            repo_path: Path to git repository (default: current directory)",
        "        \"\"\"",
        "        self.repo_path = Path(repo_path)",
        "        self._is_git_repo = self._check_git_repo()",
        "",
        "    def _check_git_repo(self) -> bool:",
        "        \"\"\"Check if the current directory is a git repository.\"\"\"",
        "        try:",
        "            result = subprocess.run(",
        "                ['git', 'rev-parse', '--git-dir'],",
        "                cwd=self.repo_path,",
        "                capture_output=True,",
        "                text=True,",
        "                timeout=5",
        "            )",
        "            return result.returncode == 0",
        "        except (subprocess.SubprocessError, FileNotFoundError):",
        "            return False",
        "",
        "    def get_staged_changes(self) -> Dict[str, List[str]]:",
        "        \"\"\"",
        "        Get staged file changes.",
        "",
        "        Returns:",
        "            Dict with keys 'added', 'modified', 'deleted' containing file paths",
        "",
        "        Raises:",
        "            RuntimeError: If not in a git repository",
        "        \"\"\"",
        "        if not self._is_git_repo:",
        "            raise RuntimeError(\"Not in a git repository\")",
        "",
        "        try:",
        "            result = subprocess.run(",
        "                ['git', 'diff', '--cached', '--name-status'],",
        "                cwd=self.repo_path,",
        "                capture_output=True,",
        "                text=True,",
        "                timeout=10",
        "            )",
        "",
        "            if result.returncode != 0:",
        "                return {'added': [], 'modified': [], 'deleted': []}",
        "",
        "            return self._parse_git_status(result.stdout)",
        "",
        "        except subprocess.SubprocessError as e:",
        "            raise RuntimeError(f\"Failed to get staged changes: {e}\")",
        "",
        "    def get_unstaged_changes(self) -> Dict[str, List[str]]:",
        "        \"\"\"",
        "        Get unstaged file changes.",
        "",
        "        Returns:",
        "            Dict with keys 'added', 'modified', 'deleted' containing file paths",
        "",
        "        Raises:",
        "            RuntimeError: If not in a git repository",
        "        \"\"\"",
        "        if not self._is_git_repo:",
        "            raise RuntimeError(\"Not in a git repository\")",
        "",
        "        try:",
        "            result = subprocess.run(",
        "                ['git', 'diff', '--name-status'],",
        "                cwd=self.repo_path,",
        "                capture_output=True,",
        "                text=True,",
        "                timeout=10",
        "            )",
        "",
        "            if result.returncode != 0:",
        "                return {'added': [], 'modified': [], 'deleted': []}",
        "",
        "            return self._parse_git_status(result.stdout)",
        "",
        "        except subprocess.SubprocessError as e:",
        "            raise RuntimeError(f\"Failed to get unstaged changes: {e}\")",
        "",
        "    def _parse_git_status(self, output: str) -> Dict[str, List[str]]:",
        "        \"\"\"",
        "        Parse git status output.",
        "",
        "        Args:",
        "            output: Output from git diff --name-status",
        "",
        "        Returns:",
        "            Dict with 'added', 'modified', 'deleted' lists",
        "        \"\"\"",
        "        changes = {'added': [], 'modified': [], 'deleted': []}",
        "",
        "        for line in output.strip().split('\\n'):",
        "            if not line:",
        "                continue",
        "",
        "            parts = line.split('\\t', 1)",
        "            if len(parts) != 2:",
        "                continue",
        "",
        "            status, filepath = parts",
        "            status = status.strip()",
        "",
        "            if status.startswith('A'):",
        "                changes['added'].append(filepath)",
        "            elif status.startswith('M'):",
        "                changes['modified'].append(filepath)",
        "            elif status.startswith('D'):",
        "                changes['deleted'].append(filepath)",
        "",
        "        return changes",
        "",
        "    def analyze_file(self, filepath: str) -> List[Dict]:",
        "        \"\"\"",
        "        Analyze a single file and return suggested tasks.",
        "",
        "        Args:",
        "            filepath: Path to file to analyze",
        "",
        "        Returns:",
        "            List of suggestion dicts with keys:",
        "                - title: Task title",
        "                - priority: high, medium, low",
        "                - category: Task category (docs, test, perf, etc.)",
        "                - reason: Why this task is suggested",
        "                - files: Files related to this task",
        "        \"\"\"",
        "        suggestions = []",
        "",
        "        # Skip deleted files",
        "        full_path = self.repo_path / filepath",
        "        if not full_path.exists():",
        "            return suggestions",
        "",
        "        # Only analyze Python files for now",
        "        if not filepath.endswith('.py'):",
        "            return suggestions",
        "",
        "        try:",
        "            content = full_path.read_text(encoding='utf-8')",
        "        except (IOError, UnicodeDecodeError):",
        "            return suggestions",
        "",
        "        # Rule 1: New public methods without docstrings",
        "        if self._has_undocumented_methods(content):",
        "            suggestions.append({",
        "                'title': f'Add docstrings to {filepath}',",
        "                'priority': 'low',",
        "                'category': 'docs',",
        "                'reason': 'New methods detected without docstrings',",
        "                'files': [filepath]",
        "            })",
        "",
        "        # Rule 2: New test file",
        "        if self._is_new_test_file(filepath, content):",
        "            suggestions.append({",
        "                'title': f'Run tests in {filepath}',",
        "                'priority': 'high',",
        "                'category': 'test',",
        "                'reason': 'New test file added - should verify tests pass',",
        "                'files': [filepath]",
        "            })",
        "",
        "        # Rule 3: Performance-sensitive code changes",
        "        if self._has_performance_code(filepath, content):",
        "            suggestions.append({",
        "                'title': f'Profile performance of {filepath}',",
        "                'priority': 'medium',",
        "                'category': 'perf',",
        "                'reason': 'Loop or performance-sensitive code detected',",
        "                'files': [filepath]",
        "            })",
        "",
        "        # Rule 4: Validation logic changes",
        "        if self._has_validation_logic(content):",
        "            suggestions.append({",
        "                'title': f'Verify validation tests for {filepath}',",
        "                'priority': 'medium',",
        "                'category': 'test',",
        "                'reason': 'Validation logic detected - ensure tests exist',",
        "                'files': [filepath]",
        "            })",
        "",
        "        # Rule 5: Missing type hints",
        "        if self._missing_type_hints(content):",
        "            suggestions.append({",
        "                'title': f'Add type hints to {filepath}',",
        "                'priority': 'low',",
        "                'category': 'codequal',",
        "                'reason': 'Functions without type hints detected',",
        "                'files': [filepath]",
        "            })",
        "",
        "        # Rule 6: Configuration changes",
        "        if self._is_config_change(filepath):",
        "            suggestions.append({",
        "                'title': f'Update documentation for config changes in {filepath}',",
        "                'priority': 'medium',",
        "                'category': 'docs',",
        "                'reason': 'Configuration file modified',",
        "                'files': [filepath]",
        "            })",
        "",
        "        # Rule 7: New public API methods",
        "        if self._has_new_public_api(filepath, content):",
        "            suggestions.append({",
        "                'title': f'Document new public API in {filepath}',",
        "                'priority': 'high',",
        "                'category': 'docs',",
        "                'reason': 'New public API methods detected',",
        "                'files': [filepath]",
        "            })",
        "",
        "        return suggestions",
        "",
        "    def _has_undocumented_methods(self, content: str) -> bool:",
        "        \"\"\"Check if file has methods without docstrings.\"\"\"",
        "        # Look for function definitions",
        "        func_pattern = re.compile(r'^\\s*(def|async def)\\s+(\\w+)\\s*\\(', re.MULTILINE)",
        "        docstring_pattern = re.compile(r'^\\s*\"\"\"', re.MULTILINE)",
        "",
        "        functions = func_pattern.findall(content)",
        "        docstrings = docstring_pattern.findall(content)",
        "",
        "        # Simple heuristic: if we have functions but fewer docstrings",
        "        return len(functions) > 0 and len(docstrings) < len(functions) * 0.5",
        "",
        "    def _is_new_test_file(self, filepath: str, content: str) -> bool:",
        "        \"\"\"Check if this is a new test file.\"\"\"",
        "        return 'test_' in filepath and 'def test_' in content",
        "",
        "    def _has_performance_code(self, filepath: str, content: str) -> bool:",
        "        \"\"\"Check if file contains performance-sensitive code.\"\"\"",
        "        # Only check core library files",
        "        if not filepath.startswith('cortical/'):",
        "            return False",
        "",
        "        # Look for loops and performance indicators",
        "        perf_indicators = [",
        "            r'\\bfor\\b.*\\bin\\b',  # for loops",
        "            r'\\bwhile\\b',  # while loops",
        "            r'O\\(n',  # Big-O notation",
        "            r'\\bslow\\b',  # Comments about slow code",
        "            r'\\bperformance\\b',  # Performance mentions",
        "        ]",
        "",
        "        return any(re.search(pattern, content, re.IGNORECASE) for pattern in perf_indicators)",
        "",
        "    def _has_validation_logic(self, content: str) -> bool:",
        "        \"\"\"Check if file contains validation logic.\"\"\"",
        "        validation_patterns = [",
        "            r'\\braise ValueError\\b',",
        "            r'\\braise TypeError\\b',",
        "            r'\\bassert\\b',",
        "            r'\\bif.*not\\b.*:.*raise\\b',",
        "        ]",
        "",
        "        return any(re.search(pattern, content) for pattern in validation_patterns)",
        "",
        "    def _missing_type_hints(self, content: str) -> bool:",
        "        \"\"\"Check if file has functions missing type hints.\"\"\"",
        "        # Look for function definitions without -> or : annotations",
        "        func_pattern = re.compile(r'^\\s*def\\s+(\\w+)\\s*\\([^)]*\\)\\s*:', re.MULTILINE)",
        "        typed_pattern = re.compile(r'^\\s*def\\s+(\\w+)\\s*\\([^)]*:\\s*\\w+', re.MULTILINE)",
        "",
        "        all_funcs = func_pattern.findall(content)",
        "        typed_funcs = typed_pattern.findall(content)",
        "",
        "        # If we have functions and less than 50% are typed",
        "        return len(all_funcs) > 0 and len(typed_funcs) < len(all_funcs) * 0.5",
        "",
        "    def _is_config_change(self, filepath: str) -> bool:",
        "        \"\"\"Check if this is a configuration file.\"\"\"",
        "        config_files = [",
        "            'config.py',",
        "            'settings.py',",
        "            '.yml',",
        "            '.yaml',",
        "            '.json',",
        "            '.toml',",
        "            'setup.py',",
        "            'pyproject.toml',",
        "        ]",
        "        return any(filepath.endswith(pattern) for pattern in config_files)",
        "",
        "    def _has_new_public_api(self, filepath: str, content: str) -> bool:",
        "        \"\"\"Check if file has new public API methods.\"\"\"",
        "        # Only check main library files",
        "        if not filepath.startswith('cortical/'):",
        "            return False",
        "",
        "        # Look for public class methods (not starting with _)",
        "        public_method_pattern = re.compile(",
        "            r'^\\s*def\\s+([a-z][a-z0-9_]*)\\s*\\(self',",
        "            re.MULTILINE",
        "        )",
        "        matches = public_method_pattern.findall(content)",
        "",
        "        # If we find public methods, suggest documentation",
        "        return len(matches) > 0",
        "",
        "    def suggest_from_changes(self, include_unstaged: bool = False) -> List[Dict]:",
        "        \"\"\"",
        "        Analyze all changes and return deduplicated suggestions.",
        "",
        "        Args:",
        "            include_unstaged: If True, analyze both staged and unstaged changes",
        "",
        "        Returns:",
        "            List of unique suggestion dicts",
        "",
        "        Raises:",
        "            RuntimeError: If not in a git repository",
        "        \"\"\"",
        "        all_suggestions = []",
        "",
        "        # Get changes",
        "        changes = self.get_staged_changes()",
        "        if include_unstaged:",
        "            unstaged = self.get_unstaged_changes()",
        "            for key in changes:",
        "                changes[key].extend(unstaged.get(key, []))",
        "",
        "        # Analyze each file",
        "        for filepath in changes.get('added', []) + changes.get('modified', []):",
        "            all_suggestions.extend(self.analyze_file(filepath))",
        "",
        "        # Deduplicate by title",
        "        seen_titles = set()",
        "        unique = []",
        "        for suggestion in all_suggestions:",
        "            if suggestion['title'] not in seen_titles:",
        "                seen_titles.add(suggestion['title'])",
        "                unique.append(suggestion)",
        "",
        "        # Sort by priority (high > medium > low)",
        "        priority_order = {'high': 0, 'medium': 1, 'low': 2}",
        "        unique.sort(key=lambda s: priority_order.get(s['priority'], 1))",
        "",
        "        # Limit to max 10 suggestions to avoid overwhelming",
        "        return unique[:10]",
        "",
        "    def format_suggestions(self, suggestions: List[Dict]) -> str:",
        "        \"\"\"",
        "        Format suggestions as markdown.",
        "",
        "        Args:",
        "            suggestions: List of suggestion dicts",
        "",
        "        Returns:",
        "            Markdown-formatted string",
        "        \"\"\"",
        "        if not suggestions:",
        "            return \"No suggestions at this time.\\n\"",
        "",
        "        lines = [",
        "            \"# Suggested Follow-up Tasks\",",
        "            \"\",",
        "            f\"**Found {len(suggestions)} suggestion(s)**\",",
        "            \"\",",
        "        ]",
        "",
        "        # Group by category",
        "        by_category = defaultdict(list)",
        "        for suggestion in suggestions:",
        "            by_category[suggestion['category']].append(suggestion)",
        "",
        "        category_icons = {",
        "            'docs': '📚',",
        "            'test': '🧪',",
        "            'perf': '⚡',",
        "            'codequal': '✨',",
        "        }",
        "",
        "        for category, items in sorted(by_category.items()):",
        "            icon = category_icons.get(category, '📋')",
        "            lines.append(f\"## {icon} {category.title()}\")",
        "            lines.append(\"\")",
        "",
        "            for item in items:",
        "                priority_badge = {",
        "                    'high': '🔴 HIGH',",
        "                    'medium': '🟡 MEDIUM',",
        "                    'low': '🟢 LOW'",
        "                }.get(item['priority'], item['priority'])",
        "",
        "                lines.append(f\"### {item['title']}\")",
        "                lines.append(f\"**Priority:** {priority_badge}\")",
        "                lines.append(f\"**Reason:** {item['reason']}\")",
        "                lines.append(f\"**Files:** {', '.join(item['files'])}\")",
        "                lines.append(\"\")",
        "",
        "        return '\\n'.join(lines)",
        "",
        "    def format_json(self, suggestions: List[Dict]) -> str:",
        "        \"\"\"",
        "        Format suggestions as JSON.",
        "",
        "        Args:",
        "            suggestions: List of suggestion dicts",
        "",
        "        Returns:",
        "            JSON-formatted string",
        "        \"\"\"",
        "        return json.dumps(suggestions, indent=2)",
        "",
        "    def create_tasks(self, suggestions: List[Dict], tasks_dir: str = 'tasks') -> int:",
        "        \"\"\"",
        "        Create tasks from suggestions using TaskSession.",
        "",
        "        Args:",
        "            suggestions: List of suggestion dicts",
        "            tasks_dir: Directory for task files",
        "",
        "        Returns:",
        "            Number of tasks created",
        "",
        "        Raises:",
        "            ImportError: If task_utils is not available",
        "        \"\"\"",
        "        if not TASK_UTILS_AVAILABLE:",
        "            raise ImportError(\"task_utils module not available\")",
        "",
        "        if not suggestions:",
        "            return 0",
        "",
        "        session = TaskSession(tasks_dir=tasks_dir)",
        "",
        "        for suggestion in suggestions:",
        "            session.create_task(",
        "                title=suggestion['title'],",
        "                priority=suggestion['priority'],",
        "                category=suggestion['category'],",
        "                description=suggestion['reason'],",
        "                context={'files': suggestion['files']}",
        "            )",
        "",
        "        session.save()",
        "        return len(suggestions)",
        "",
        "",
        "def main():",
        "    \"\"\"CLI entry point.\"\"\"",
        "    parser = argparse.ArgumentParser(",
        "        description='Analyze code changes and suggest follow-up tasks',",
        "        formatter_class=argparse.RawDescriptionHelpFormatter,",
        "        epilog=\"\"\"",
        "Examples:",
        "  %(prog)s                    # Analyze staged changes",
        "  %(prog)s --all              # Include unstaged changes",
        "  %(prog)s --json             # Output as JSON",
        "  %(prog)s --create           # Create tasks from suggestions",
        "  %(prog)s --all --verbose    # Detailed output with unstaged changes",
        "        \"\"\"",
        "    )",
        "",
        "    parser.add_argument(",
        "        '--all',",
        "        action='store_true',",
        "        help='Include unstaged changes (default: staged only)'",
        "    )",
        "    parser.add_argument(",
        "        '--json',",
        "        action='store_true',",
        "        help='Output as JSON instead of markdown'",
        "    )",
        "    parser.add_argument(",
        "        '--create',",
        "        action='store_true',",
        "        help='Create tasks in tasks/ directory'",
        "    )",
        "    parser.add_argument(",
        "        '--tasks-dir',",
        "        default='tasks',",
        "        help='Directory for task files (default: tasks/)'",
        "    )",
        "    parser.add_argument(",
        "        '--verbose', '-v',",
        "        action='store_true',",
        "        help='Verbose output'",
        "    )",
        "",
        "    args = parser.parse_args()",
        "",
        "    # Create suggester",
        "    try:",
        "        suggester = TaskSuggester()",
        "    except Exception as e:",
        "        print(f\"Error: {e}\", file=sys.stderr)",
        "        return 1",
        "",
        "    # Get suggestions",
        "    try:",
        "        suggestions = suggester.suggest_from_changes(include_unstaged=args.all)",
        "    except RuntimeError as e:",
        "        print(f\"Error: {e}\", file=sys.stderr)",
        "        return 1",
        "",
        "    # Output",
        "    if args.json:",
        "        print(suggester.format_json(suggestions))",
        "    else:",
        "        print(suggester.format_suggestions(suggestions))",
        "",
        "    # Create tasks if requested",
        "    if args.create:",
        "        if not TASK_UTILS_AVAILABLE:",
        "            print(\"Error: task_utils module not available\", file=sys.stderr)",
        "            return 1",
        "",
        "        try:",
        "            count = suggester.create_tasks(suggestions, args.tasks_dir)",
        "            print(f\"\\n✅ Created {count} task(s) in {args.tasks_dir}/\")",
        "        except Exception as e:",
        "            print(f\"Error creating tasks: {e}\", file=sys.stderr)",
        "            return 1",
        "",
        "    # Verbose summary",
        "    if args.verbose:",
        "        print(f\"\\n--- Summary ---\")",
        "        print(f\"Analyzed: {'staged + unstaged' if args.all else 'staged only'}\")",
        "        print(f\"Suggestions: {len(suggestions)}\")",
        "        if suggestions:",
        "            by_priority = defaultdict(int)",
        "            for s in suggestions:",
        "                by_priority[s['priority']] += 1",
        "            for priority in ['high', 'medium', 'low']:",
        "                if by_priority[priority] > 0:",
        "                    print(f\"  {priority}: {by_priority[priority]}\")",
        "",
        "    return 0",
        "",
        "",
        "if __name__ == '__main__':",
        "    sys.exit(main())"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "scripts/task_utils.py",
      "function": "class Task:",
      "start_line": 97,
      "lines_added": [
        "    retrospective: Optional[Dict[str, Any]] = None  # Captured on completion"
      ],
      "lines_removed": [],
      "context_before": [
        "    status: str = \"pending\"  # pending, in_progress, completed, deferred",
        "    priority: str = \"medium\"  # high, medium, low",
        "    category: str = \"general\"",
        "    description: str = \"\"",
        "    depends_on: List[str] = field(default_factory=list)",
        "    effort: str = \"medium\"  # small, medium, large",
        "    created_at: str = field(default_factory=lambda: datetime.now().isoformat())",
        "    updated_at: Optional[str] = None",
        "    completed_at: Optional[str] = None",
        "    context: Dict[str, Any] = field(default_factory=dict)"
      ],
      "context_after": [
        "",
        "    def to_dict(self) -> Dict[str, Any]:",
        "        \"\"\"Convert to dictionary for serialization.\"\"\"",
        "        return asdict(self)",
        "",
        "    @classmethod",
        "    def from_dict(cls, d: Dict[str, Any]) -> 'Task':",
        "        \"\"\"Create Task from dictionary.\"\"\"",
        "        return cls(**d)",
        ""
      ],
      "change_type": "add"
    },
    {
      "file": "scripts/task_utils.py",
      "function": "class TaskSession:",
      "start_line": 190,
      "lines_added": [
        "    def get_task(self, task_id: str) -> Optional[Task]:",
        "        \"\"\"",
        "        Get a task by ID from this session.",
        "",
        "        Args:",
        "            task_id: The task ID to find",
        "",
        "        Returns:",
        "            The Task object, or None if not found",
        "        \"\"\"",
        "        for task in self.tasks:",
        "            if task.id == task_id:",
        "                return task",
        "        return None",
        "",
        "    def _calculate_duration(self, start_time: str) -> int:",
        "        \"\"\"",
        "        Calculate duration in minutes from start time to now.",
        "",
        "        Args:",
        "            start_time: ISO format datetime string",
        "",
        "        Returns:",
        "            Duration in minutes (rounded)",
        "        \"\"\"",
        "        start = datetime.fromisoformat(start_time)",
        "        now = datetime.now()",
        "        delta = now - start",
        "        return round(delta.total_seconds() / 60)",
        "",
        "    def capture_retrospective(",
        "        self,",
        "        task_id: str,",
        "        files_touched: Optional[List[str]] = None,",
        "        tests_added: int = 0,",
        "        commits: Optional[List[str]] = None,",
        "        notes: Optional[str] = None",
        "    ) -> None:",
        "        \"\"\"",
        "        Capture retrospective data for a completed task.",
        "",
        "        This records metadata about what actually happened during task completion:",
        "        - Which files were modified",
        "        - How long it took",
        "        - How many tests were added",
        "        - Which commits were made",
        "        - Any completion notes",
        "",
        "        Args:",
        "            task_id: The task ID to add retrospective to",
        "            files_touched: List of file paths that were modified",
        "            tests_added: Number of test cases/functions added",
        "            commits: List of commit SHAs related to this task",
        "            notes: Optional free-form notes about completion",
        "",
        "        Raises:",
        "            ValueError: If task_id is not found in this session",
        "        \"\"\"",
        "        task = self.get_task(task_id)",
        "        if not task:",
        "            raise ValueError(f\"Task not found: {task_id}\")",
        "",
        "        # Calculate duration from creation to now",
        "        duration = self._calculate_duration(task.created_at)",
        "",
        "        task.retrospective = {",
        "            'files_touched': files_touched or [],",
        "            'duration_minutes': duration,",
        "            'tests_added': tests_added,",
        "            'commits': commits or [],",
        "            'notes': notes,",
        "            'captured_at': datetime.now().isoformat()",
        "        }",
        "",
        "    def get_retrospective_summary(self) -> Dict[str, Any]:",
        "        \"\"\"",
        "        Get aggregate statistics from all completed tasks with retrospective data.",
        "",
        "        Returns:",
        "            Dictionary with aggregate stats:",
        "            - total_completed: Number of tasks with retrospective data",
        "            - avg_duration_minutes: Average task duration",
        "            - total_duration_minutes: Total time spent on all tasks",
        "            - total_tests_added: Sum of all tests added",
        "            - most_touched_files: List of (file, count) tuples for top 10 files",
        "            - tasks_with_retrospective: List of task IDs that have retrospective data",
        "        \"\"\"",
        "        from collections import Counter",
        "",
        "        completed = [",
        "            t for t in self.tasks",
        "            if t.status == 'completed' and t.retrospective",
        "        ]",
        "",
        "        if not completed:",
        "            return {",
        "                'total_completed': 0,",
        "                'avg_duration_minutes': 0,",
        "                'total_duration_minutes': 0,",
        "                'total_tests_added': 0,",
        "                'most_touched_files': [],",
        "                'tasks_with_retrospective': []",
        "            }",
        "",
        "        # Calculate aggregates",
        "        durations = [t.retrospective['duration_minutes'] for t in completed]",
        "        total_duration = sum(durations)",
        "        avg_duration = total_duration / len(completed)",
        "",
        "        total_tests = sum(t.retrospective['tests_added'] for t in completed)",
        "",
        "        # Count file touches",
        "        all_files = []",
        "        for t in completed:",
        "            all_files.extend(t.retrospective['files_touched'])",
        "        file_counts = Counter(all_files)",
        "",
        "        return {",
        "            'total_completed': len(completed),",
        "            'avg_duration_minutes': round(avg_duration, 1),",
        "            'total_duration_minutes': total_duration,",
        "            'total_tests_added': total_tests,",
        "            'most_touched_files': file_counts.most_common(10),",
        "            'tasks_with_retrospective': [t.id for t in completed]",
        "        }",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "            priority=priority,",
        "            category=category,",
        "            description=description,",
        "            depends_on=depends_on or [],",
        "            effort=effort,",
        "            context=context or {}",
        "        )",
        "        self.tasks.append(task)",
        "        return task",
        ""
      ],
      "context_after": [
        "    def get_filename(self) -> str:",
        "        \"\"\"Get the session filename.\"\"\"",
        "        dt = datetime.fromisoformat(self.started_at)",
        "        timestamp = dt.strftime(\"%Y-%m-%d_%H-%M-%S\")",
        "        return f\"{timestamp}_{self.session_id}.json\"",
        "",
        "    def save(self, tasks_dir: Optional[str] = None) -> Path:",
        "        \"\"\"",
        "        Save session tasks to a JSON file atomically.",
        ""
      ],
      "change_type": "add"
    },
    {
      "file": "tasks/2025-12-13_22-32-34_e233.json",
      "function": null,
      "start_line": 34,
      "lines_added": [
        "      \"status\": \"completed\",",
        "      \"updated_at\": \"2025-12-14T01:55:00.000000\",",
        "      \"completed_at\": \"2025-12-14T01:55:00.000000\","
      ],
      "lines_removed": [
        "      \"status\": \"pending\",",
        "      \"updated_at\": null,",
        "      \"completed_at\": null,"
      ],
      "context_before": [
        "      ],",
        "      \"effort\": \"small\",",
        "      \"created_at\": \"2025-12-13T22:32:34.072592\",",
        "      \"updated_at\": \"2025-12-13T22:33:26.107132\",",
        "      \"completed_at\": \"2025-12-13T22:33:26.107132\",",
        "      \"context\": {}",
        "    },",
        "    {",
        "      \"id\": \"T-20251213-223234-e233-03\",",
        "      \"title\": \"Test task recovery from crash scenario\","
      ],
      "context_after": [
        "      \"priority\": \"medium\",",
        "      \"category\": \"test\",",
        "      \"description\": \"Verify tasks persist correctly if agent crashes mid-work\",",
        "      \"depends_on\": [],",
        "      \"effort\": \"medium\",",
        "      \"created_at\": \"2025-12-13T22:32:34.072623\",",
        "      \"context\": {}",
        "    }",
        "  ]",
        "}"
      ],
      "change_type": "modify"
    },
    {
      "file": "tasks/2025-12-13_22-42-20_6ac7.json",
      "function": null,
      "start_line": 94,
      "lines_added": [
        "      \"status\": \"completed\",",
        "      \"updated_at\": \"2025-12-14T01:55:00.000000\",",
        "      \"completed_at\": \"2025-12-14T01:55:00.000000\","
      ],
      "lines_removed": [
        "      \"status\": \"pending\",",
        "      \"updated_at\": null,",
        "      \"completed_at\": null,"
      ],
      "context_before": [
        "      \"context\": {",
        "        \"files\": [",
        "          \".github/workflows/ci.yml\",",
        "          \"scripts/create_tasks_from_ci.py\"",
        "        ]",
        "      }",
        "    },",
        "    {",
        "      \"id\": \"T-20251213-224220-6ac7-06\",",
        "      \"title\": \"Add task retrospective metadata capture\","
      ],
      "context_after": [
        "      \"priority\": \"medium\",",
        "      \"category\": \"feature\",",
        "      \"description\": \"Track files_touched, duration, tests_written for learning\",",
        "      \"depends_on\": [],",
        "      \"effort\": \"large\",",
        "      \"created_at\": \"2025-12-13T22:42:20.986983\",",
        "      \"context\": {}",
        "    }",
        "  ]",
        "}"
      ],
      "change_type": "modify"
    },
    {
      "file": "tasks/2025-12-13_23-54-58_1a1d.json",
      "function": null,
      "start_line": 1,
      "lines_added": [
        "      \"status\": \"completed\",",
        "      \"description\": \"Performance bottleneck identified: doc_name_boost re-tokenizes all document names on every search.\\n\\nRoot cause: Lines 70-103 in query/search.py tokenize every doc_id on every query.\\nImpact: 70% of search time on 2000-doc corpus (3.95ms overhead).\\n\\nRecommendation: Cache tokenized doc names in Minicolumn during process_document().\\nExpected benefit: 3-4x faster searches on large corpora.\\nEffort estimate: 2-4 hours.\\n\\nResult: Achieved 2.78x speedup via name_tokens caching.\",",
        "      \"updated_at\": \"2025-12-14T01:55:00.000000\",",
        "      \"completed_at\": \"2025-12-14T01:55:00.000000\","
      ],
      "lines_removed": [
        "      \"status\": \"pending\",",
        "      \"description\": \"Performance bottleneck identified: doc_name_boost re-tokenizes all document names on every search.\\n\\nRoot cause: Lines 70-103 in query/search.py tokenize every doc_id on every query.\\nImpact: 70% of search time on 2000-doc corpus (3.95ms overhead).\\n\\nRecommendation: Cache tokenized doc names in Minicolumn during process_document().\\nExpected benefit: 3-4x faster searches on large corpora.\\nEffort estimate: 2-4 hours.\",",
        "      \"updated_at\": null,",
        "      \"completed_at\": null,"
      ],
      "context_before": [
        "{",
        "  \"version\": 1,",
        "  \"session_id\": \"1a1d\",",
        "  \"started_at\": \"2025-12-13T23:54:58.530536\",",
        "  \"saved_at\": \"2025-12-13T23:54:58.531057\",",
        "  \"tasks\": [",
        "    {",
        "      \"id\": \"T-20251213-235458-1a1d-001\",",
        "      \"title\": \"Optimize doc_name_boost: cache tokenized document names\","
      ],
      "context_after": [
        "      \"priority\": \"medium\",",
        "      \"category\": \"perf\",",
        "      \"depends_on\": [],",
        "      \"effort\": \"medium\",",
        "      \"created_at\": \"2025-12-13T23:54:58.530954\",",
        "      \"context\": {}",
        "    }",
        "  ]",
        "}"
      ],
      "change_type": "modify"
    },
    {
      "file": "tasks/2025-12-14_00-21-46_8d66.json",
      "function": null,
      "start_line": 1,
      "lines_added": [
        "      \"status\": \"completed\",",
        "      \"updated_at\": \"2025-12-14T01:55:00.000000\",",
        "      \"completed_at\": \"2025-12-14T01:55:00.000000\","
      ],
      "lines_removed": [
        "      \"status\": \"pending\",",
        "      \"updated_at\": null,",
        "      \"completed_at\": null,"
      ],
      "context_before": [
        "{",
        "  \"version\": 1,",
        "  \"session_id\": \"8d66\",",
        "  \"started_at\": \"2025-12-14T00:21:46.447704\",",
        "  \"saved_at\": \"2025-12-14T00:27:34.846899\",",
        "  \"tasks\": [",
        "    {",
        "      \"id\": \"T-20251214-002146-8d66-001\",",
        "      \"title\": \"Add session context generator for agent handoff\","
      ],
      "context_after": [
        "      \"priority\": \"high\",",
        "      \"category\": \"agent-dx\",",
        "      \"description\": \"When a new agent session starts, generate a concise \\\"catch-up\\\" summary:\\n- What work was done in the last N sessions\\n- Current pending tasks sorted by priority\\n- Recent file changes with semantic summaries\\n- Key decisions made (from commit messages)\\n\\nThis reduces the 'cold start' problem where agents spend time re-understanding context.\\n\\nImplementation:\\n- scripts/session_context.py with SessionContextGenerator class\\n- Read from tasks/*.json for pending/completed work\\n- Use git log for recent changes\\n- Output: markdown summary suitable for agent consumption\",",
        "      \"depends_on\": [],",
        "      \"effort\": \"medium\",",
        "      \"created_at\": \"2025-12-14T00:21:46.447746\",",
        "      \"context\": {",
        "        \"files\": [",
        "          \"scripts/task_utils.py\",",
        "          \"scripts/consolidate_tasks.py\"",
        "        ],",
        "        \"patterns\": [",
        "          \"chunk_index.py for similar git-friendly storage\"",
        "        ]",
        "      }",
        "    },"
      ],
      "change_type": "modify"
    },
    {
      "file": "tasks/2025-12-14_00-21-46_8d66.json",
      "function": null,
      "start_line": 49,
      "lines_added": [
        "      \"status\": \"completed\",",
        "      \"updated_at\": \"2025-12-14T01:55:00.000000\",",
        "      \"completed_at\": \"2025-12-14T01:55:00.000000\",",
        "      \"status\": \"completed\",",
        "      \"updated_at\": \"2025-12-14T01:55:00.000000\",",
        "      \"completed_at\": \"2025-12-14T01:55:00.000000\","
      ],
      "lines_removed": [
        "      \"status\": \"pending\",",
        "      \"updated_at\": null,",
        "      \"completed_at\": null,",
        "      \"status\": \"pending\",",
        "      \"updated_at\": null,",
        "      \"completed_at\": null,"
      ],
      "context_before": [
        "          \"save_processor()\",",
        "          \"load_processor()\",",
        "          \"ChunkWriter\",",
        "          \"ChunkLoader\"",
        "        ]",
        "      }",
        "    },",
        "    {",
        "      \"id\": \"T-20251214-002146-8d66-003\",",
        "      \"title\": \"Add progress checkpointing for compute_all()\","
      ],
      "context_after": [
        "      \"priority\": \"medium\",",
        "      \"category\": \"agent-dx\",",
        "      \"description\": \"When compute_all() runs on large corpora, it can take 10+ minutes.\\nIf the agent session times out, all work is lost.\\n\\nSolution: Add checkpointing after each major phase:\\n1. After TF-IDF computation - save partial state\\n2. After bigram connections - save partial state  \\n3. After PageRank - save partial state\\n4. After concepts - save partial state\\n5. After semantics - final save\\n\\nBenefits:\\n- Resume from checkpoint if session times out\\n- Progress visibility (which phase are we on)\\n- Partial results usable even if later phases fail\\n\\nImplementation:\\n- Add checkpoint_dir parameter to compute_all()\\n- Save phase completions as JSON markers\\n- Add resume_from_checkpoint() method\",",
        "      \"depends_on\": [],",
        "      \"effort\": \"medium\",",
        "      \"created_at\": \"2025-12-14T00:21:46.447764\",",
        "      \"context\": {",
        "        \"files\": [",
        "          \"cortical/processor.py\"",
        "        ],",
        "        \"methods\": [",
        "          \"compute_all()\",",
        "          \"compute_tfidf()\",",
        "          \"compute_bigram_connections()\"",
        "        ]",
        "      }",
        "    },",
        "    {",
        "      \"id\": \"T-20251214-002146-8d66-004\",",
        "      \"title\": \"Auto-suggest tasks from code changes\",",
        "      \"priority\": \"low\",",
        "      \"category\": \"agent-dx\",",
        "      \"description\": \"When code is modified, automatically suggest follow-up tasks:\\n- Tests added? Suggest running test suite\\n- New public method? Suggest adding docstring\\n- Performance-sensitive code changed? Suggest profiling\\n- Validation logic changed? Suggest checking all related tests\\n\\nUses the semantic understanding from Cortical to identify patterns.\\n\\nImplementation:\\n- scripts/suggest_tasks.py\\n- Hook into git pre-commit or post-commit\\n- Output task suggestions in merge-friendly format\",",
        "      \"depends_on\": [],",
        "      \"effort\": \"medium\",",
        "      \"created_at\": \"2025-12-14T00:21:46.447781\",",
        "      \"context\": {",
        "        \"files\": [",
        "          \"scripts/task_utils.py\",",
        "          \"cortical/semantics.py\"",
        "        ],",
        "        \"patterns\": [",
        "          \"Could use search_codebase.py patterns\"",
        "        ]",
        "      }",
        "    }"
      ],
      "change_type": "modify"
    },
    {
      "file": "tasks/2025-12-14_01-53-45_7b60.json",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "{",
        "  \"version\": 1,",
        "  \"session_id\": \"7b60\",",
        "  \"started_at\": \"2025-12-14T01:53:45.787991\",",
        "  \"saved_at\": \"2025-12-14T01:53:45.788233\",",
        "  \"tasks\": [",
        "    {",
        "      \"id\": \"T-20251214-015345-7b60-001\",",
        "      \"title\": \"Add tests\",",
        "      \"status\": \"pending\",",
        "      \"priority\": \"high\",",
        "      \"category\": \"test\",",
        "      \"description\": \"New code added\",",
        "      \"depends_on\": [],",
        "      \"effort\": \"medium\",",
        "      \"created_at\": \"2025-12-14T01:53:45.788028\",",
        "      \"updated_at\": null,",
        "      \"completed_at\": null,",
        "      \"context\": {",
        "        \"files\": [",
        "          \"test.py\"",
        "        ]",
        "      },",
        "      \"retrospective\": null",
        "    },",
        "    {",
        "      \"id\": \"T-20251214-015345-7b60-002\",",
        "      \"title\": \"Add docs\",",
        "      \"status\": \"pending\",",
        "      \"priority\": \"low\",",
        "      \"category\": \"docs\",",
        "      \"description\": \"Missing docstrings\",",
        "      \"depends_on\": [],",
        "      \"effort\": \"medium\",",
        "      \"created_at\": \"2025-12-14T01:53:45.788037\",",
        "      \"updated_at\": null,",
        "      \"completed_at\": null,",
        "      \"context\": {",
        "        \"files\": [",
        "          \"module.py\"",
        "        ]",
        "      },",
        "      \"retrospective\": null",
        "    }",
        "  ]",
        "}"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "tests/integration/test_task_recovery.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "#!/usr/bin/env python3",
        "\"\"\"",
        "Integration tests for task persistence and crash recovery.",
        "",
        "Tests verify that the TaskSession atomic write pattern correctly",
        "handles various failure scenarios:",
        "- Interrupted saves",
        "- Concurrent sessions",
        "- Corrupted files",
        "- State persistence across reloads",
        "\"\"\"",
        "",
        "import pytest",
        "import json",
        "import tempfile",
        "import os",
        "from pathlib import Path",
        "from datetime import datetime",
        "",
        "# Import from scripts",
        "import sys",
        "sys.path.insert(0, str(Path(__file__).parent.parent.parent))",
        "from scripts.task_utils import TaskSession, Task, load_all_tasks",
        "",
        "",
        "class TestTaskRecovery:",
        "    \"\"\"Test task persistence survives crashes and failures.\"\"\"",
        "",
        "    def test_atomic_write_creates_temp_file(self, tmp_path):",
        "        \"\"\"Verify save() uses temp file before rename.\"\"\"",
        "        # Create session with a task",
        "        session = TaskSession(tasks_dir=str(tmp_path))",
        "        task = session.create_task(\"Test task for atomic write\")",
        "",
        "        # Track temp file creation (monkey-patch open to observe)",
        "        temp_file_seen = []",
        "        original_open = open",
        "",
        "        def tracking_open(path, *args, **kwargs):",
        "            if '.json.tmp' in str(path):",
        "                temp_file_seen.append(path)",
        "            return original_open(path, *args, **kwargs)",
        "",
        "        # Temporarily patch open",
        "        import builtins",
        "        builtins.open = tracking_open",
        "",
        "        try:",
        "            filepath = session.save()",
        "        finally:",
        "            # Restore original open",
        "            builtins.open = original_open",
        "",
        "        # Verify temp file was used",
        "        assert len(temp_file_seen) > 0, \"Temp file should be created during save\"",
        "",
        "        # Verify temp file doesn't persist after save",
        "        temp_path = filepath.with_suffix('.json.tmp')",
        "        assert not temp_path.exists(), \"Temp file should be cleaned up after successful save\"",
        "",
        "        # Verify final file exists and is valid",
        "        assert filepath.exists(), \"Final file should exist\"",
        "        with open(filepath) as f:",
        "            data = json.load(f)",
        "        assert data['version'] == 1",
        "        assert len(data['tasks']) == 1",
        "        assert data['tasks'][0]['title'] == \"Test task for atomic write\"",
        "",
        "    def test_recovery_after_interrupted_save(self, tmp_path):",
        "        \"\"\"Simulate crash during save, verify old state preserved.\"\"\"",
        "        # Create session with initial tasks",
        "        session = TaskSession(tasks_dir=str(tmp_path))",
        "        task1 = session.create_task(\"Original task 1\")",
        "        task2 = session.create_task(\"Original task 2\")",
        "",
        "        # Save successfully",
        "        filepath = session.save()",
        "        assert filepath.exists()",
        "",
        "        # Verify initial state",
        "        with open(filepath) as f:",
        "            original_data = json.load(f)",
        "        assert len(original_data['tasks']) == 2",
        "",
        "        # Simulate interrupted save by creating a partial temp file",
        "        temp_filepath = filepath.with_suffix('.json.tmp')",
        "        with open(temp_filepath, 'w') as f:",
        "            f.write('{\"version\": 1, \"incomplete\": ')  # Incomplete JSON",
        "",
        "        # Verify temp file exists (simulating crash mid-write)",
        "        assert temp_filepath.exists()",
        "",
        "        # Load session - should get last good state, not partial",
        "        loaded_session = TaskSession.load(filepath)",
        "",
        "        # Verify we got the original data, not the corrupted temp file",
        "        assert len(loaded_session.tasks) == 2",
        "        assert loaded_session.tasks[0].title == \"Original task 1\"",
        "        assert loaded_session.tasks[1].title == \"Original task 2\"",
        "",
        "        # Verify original file is still intact",
        "        with open(filepath) as f:",
        "            current_data = json.load(f)",
        "        assert current_data == original_data",
        "",
        "    def test_concurrent_sessions_no_conflict(self, tmp_path):",
        "        \"\"\"Two sessions can work in parallel without conflicts.\"\"\"",
        "        # Create two separate sessions",
        "        session1 = TaskSession(tasks_dir=str(tmp_path))",
        "        session2 = TaskSession(tasks_dir=str(tmp_path))",
        "",
        "        # Add tasks to both sessions",
        "        task1a = session1.create_task(\"Session 1 - Task A\", priority=\"high\")",
        "        task1b = session1.create_task(\"Session 1 - Task B\", priority=\"medium\")",
        "",
        "        task2a = session2.create_task(\"Session 2 - Task A\", priority=\"low\")",
        "        task2b = session2.create_task(\"Session 2 - Task B\", priority=\"high\")",
        "",
        "        # Save both sessions",
        "        file1 = session1.save()",
        "        file2 = session2.save()",
        "",
        "        # Verify both files exist",
        "        assert file1.exists(), \"Session 1 file should exist\"",
        "        assert file2.exists(), \"Session 2 file should exist\"",
        "        assert file1 != file2, \"Sessions should have different filenames\"",
        "",
        "        # Verify both files are valid",
        "        with open(file1) as f:",
        "            data1 = json.load(f)",
        "        assert len(data1['tasks']) == 2",
        "        assert data1['session_id'] == session1.session_id",
        "",
        "        with open(file2) as f:",
        "            data2 = json.load(f)",
        "        assert len(data2['tasks']) == 2",
        "        assert data2['session_id'] == session2.session_id",
        "",
        "        # Verify all tasks are loadable",
        "        all_tasks = load_all_tasks(str(tmp_path))",
        "        assert len(all_tasks) == 4, \"Should load all 4 tasks from both sessions\"",
        "",
        "        # Verify task IDs are unique",
        "        task_ids = [t.id for t in all_tasks]",
        "        assert len(task_ids) == len(set(task_ids)), \"All task IDs should be unique\"",
        "",
        "    def test_corrupted_json_handled_gracefully(self, tmp_path):",
        "        \"\"\"Corrupted task file doesn't crash loading.\"\"\"",
        "        # Create a valid session first",
        "        session1 = TaskSession(tasks_dir=str(tmp_path))",
        "        session1.create_task(\"Valid task\")",
        "        file1 = session1.save()",
        "",
        "        # Create a corrupted JSON file",
        "        corrupted_file = tmp_path / \"2025-12-13_10-00-00_corrupt.json\"",
        "        with open(corrupted_file, 'w') as f:",
        "            f.write('{\"version\": 1, \"tasks\": [INVALID JSON')",
        "",
        "        # Create another valid session",
        "        session2 = TaskSession(tasks_dir=str(tmp_path))",
        "        session2.create_task(\"Another valid task\")",
        "        file2 = session2.save()",
        "",
        "        # Load all tasks - should handle corruption gracefully",
        "        all_tasks = load_all_tasks(str(tmp_path))",
        "",
        "        # Should load only the 2 valid tasks, skipping corrupted file",
        "        assert len(all_tasks) == 2, \"Should load 2 tasks from valid files\"",
        "        assert all_tasks[0].title == \"Valid task\"",
        "        assert all_tasks[1].title == \"Another valid task\"",
        "",
        "    def test_task_state_persists_across_reload(self, tmp_path):",
        "        \"\"\"Task status changes persist across session reload.\"\"\"",
        "        # Create session with a pending task",
        "        session = TaskSession(tasks_dir=str(tmp_path))",
        "        task = session.create_task(\"Task with state changes\", priority=\"high\")",
        "        assert task.status == \"pending\"",
        "        assert task.completed_at is None",
        "",
        "        # Save and reload",
        "        filepath = session.save()",
        "        loaded_session1 = TaskSession.load(filepath)",
        "",
        "        # Verify task is still pending",
        "        assert len(loaded_session1.tasks) == 1",
        "        assert loaded_session1.tasks[0].status == \"pending\"",
        "        assert loaded_session1.tasks[0].priority == \"high\"",
        "",
        "        # Mark task as in progress",
        "        loaded_session1.tasks[0].mark_in_progress()",
        "        assert loaded_session1.tasks[0].status == \"in_progress\"",
        "        assert loaded_session1.tasks[0].updated_at is not None",
        "",
        "        # Save and reload again",
        "        loaded_session1.save(tasks_dir=str(tmp_path))",
        "        loaded_session2 = TaskSession.load(filepath)",
        "",
        "        # Verify status is still in_progress",
        "        assert loaded_session2.tasks[0].status == \"in_progress\"",
        "        assert loaded_session2.tasks[0].updated_at is not None",
        "",
        "        # Mark task as complete",
        "        loaded_session2.tasks[0].mark_complete()",
        "        assert loaded_session2.tasks[0].status == \"completed\"",
        "        assert loaded_session2.tasks[0].completed_at is not None",
        "",
        "        # Save and reload final time",
        "        loaded_session2.save(tasks_dir=str(tmp_path))",
        "        loaded_session3 = TaskSession.load(filepath)",
        "",
        "        # Verify status is still completed",
        "        assert loaded_session3.tasks[0].status == \"completed\"",
        "        assert loaded_session3.tasks[0].completed_at is not None",
        "",
        "        # Verify all metadata preserved",
        "        final_task = loaded_session3.tasks[0]",
        "        assert final_task.title == \"Task with state changes\"",
        "        assert final_task.priority == \"high\"",
        "        assert final_task.created_at == task.created_at",
        "",
        "    def test_partial_session_recovery(self, tmp_path):",
        "        \"\"\"If one task file is corrupt, others still load.\"\"\"",
        "        # Create multiple valid session files",
        "        sessions = []",
        "        for i in range(3):",
        "            session = TaskSession(tasks_dir=str(tmp_path))",
        "            session.create_task(f\"Valid task {i+1}\", priority=\"medium\")",
        "            filepath = session.save()",
        "            sessions.append((session, filepath))",
        "",
        "        # Corrupt the middle file",
        "        middle_file = sessions[1][1]",
        "        with open(middle_file, 'w') as f:",
        "            f.write('COMPLETELY INVALID JSON{{{')",
        "",
        "        # Load all tasks",
        "        all_tasks = load_all_tasks(str(tmp_path))",
        "",
        "        # Should load 2 valid tasks, skipping the corrupted one",
        "        assert len(all_tasks) == 2, \"Should load 2 tasks from valid files\"",
        "        assert all_tasks[0].title == \"Valid task 1\"",
        "        assert all_tasks[1].title == \"Valid task 3\"",
        "",
        "        # Verify corrupted file still exists (not deleted)",
        "        assert middle_file.exists(), \"Corrupted file should not be deleted\"",
        "",
        "    def test_temp_file_cleanup_on_write_failure(self, tmp_path):",
        "        \"\"\"Temp file is cleaned up if write fails.\"\"\"",
        "        session = TaskSession(tasks_dir=str(tmp_path))",
        "        session.create_task(\"Test task\")",
        "",
        "        filepath = tmp_path / session.get_filename()",
        "        temp_filepath = filepath.with_suffix('.json.tmp')",
        "",
        "        # Simulate write failure by making directory read-only after temp file creation",
        "        # This is hard to test reliably cross-platform, so we'll test the cleanup path",
        "        # by manually creating a temp file and verifying it gets cleaned up",
        "",
        "        # Create a pre-existing temp file",
        "        with open(temp_filepath, 'w') as f:",
        "            f.write('{\"old\": \"temp_file\"}')",
        "",
        "        assert temp_filepath.exists(), \"Pre-existing temp file should exist\"",
        "",
        "        # Normal save should clean up and replace it",
        "        session.save()",
        "",
        "        # After successful save, temp file should be gone",
        "        assert not temp_filepath.exists(), \"Temp file should be cleaned up after save\"",
        "        assert filepath.exists(), \"Final file should exist\"",
        "",
        "    def test_fsync_called_for_durability(self, tmp_path):",
        "        \"\"\"Verify that fsync is called to ensure data durability.\"\"\"",
        "        session = TaskSession(tasks_dir=str(tmp_path))",
        "        session.create_task(\"Durable task\")",
        "",
        "        # Track fsync calls",
        "        fsync_called = []",
        "        original_fsync = os.fsync",
        "",
        "        def tracking_fsync(fd):",
        "            fsync_called.append(fd)",
        "            return original_fsync(fd)",
        "",
        "        # Patch fsync",
        "        os.fsync = tracking_fsync",
        "",
        "        try:",
        "            filepath = session.save()",
        "        finally:",
        "            # Restore original fsync",
        "            os.fsync = original_fsync",
        "",
        "        # Verify fsync was called at least once",
        "        assert len(fsync_called) > 0, \"fsync should be called for data durability\"",
        "",
        "        # Verify file was saved correctly",
        "        assert filepath.exists()",
        "        with open(filepath) as f:",
        "            data = json.load(f)",
        "        assert len(data['tasks']) == 1",
        "",
        "    def test_task_id_uniqueness_within_session(self, tmp_path):",
        "        \"\"\"Task IDs are unique even when created in quick succession.\"\"\"",
        "        session = TaskSession(tasks_dir=str(tmp_path))",
        "",
        "        # Create many tasks quickly",
        "        task_ids = []",
        "        for i in range(100):",
        "            task = session.create_task(f\"Rapid task {i}\")",
        "            task_ids.append(task.id)",
        "",
        "        # Verify all IDs are unique",
        "        assert len(task_ids) == len(set(task_ids)), \"All task IDs should be unique\"",
        "",
        "        # Verify IDs follow expected format",
        "        for task_id in task_ids:",
        "            assert task_id.startswith('T-'), \"Task IDs should start with T-\"",
        "            parts = task_id.split('-')",
        "            assert len(parts) == 5, \"Task IDs should have 5 parts (T-DATE-TIME-SESSION-NUM)\"",
        "            assert parts[4].isdigit(), \"Last part should be task number\"",
        "",
        "    def test_load_with_missing_task_counter(self, tmp_path):",
        "        \"\"\"Loading a session without _task_counter field works.\"\"\"",
        "        # Create a session and save it",
        "        session = TaskSession(tasks_dir=str(tmp_path))",
        "        session.create_task(\"Task 1\")",
        "        session.create_task(\"Task 2\")",
        "        filepath = session.save()",
        "",
        "        # Load the session",
        "        loaded = TaskSession.load(filepath)",
        "",
        "        # Verify tasks loaded correctly",
        "        assert len(loaded.tasks) == 2",
        "        assert loaded.tasks[0].title == \"Task 1\"",
        "        assert loaded.tasks[1].title == \"Task 2\"",
        "",
        "        # The _task_counter is not persisted, so it should be 0 after load",
        "        # Creating new tasks should still work",
        "        new_task = loaded.create_task(\"Task 3\")",
        "        assert new_task.id is not None",
        "        assert len(loaded.tasks) == 3",
        "",
        "    def test_empty_tasks_directory(self, tmp_path):",
        "        \"\"\"Loading from empty directory returns empty list.\"\"\"",
        "        # Don't create any task files",
        "        empty_dir = tmp_path / \"empty_tasks\"",
        "",
        "        # Load from non-existent directory",
        "        tasks = load_all_tasks(str(empty_dir))",
        "        assert tasks == [], \"Should return empty list for non-existent directory\"",
        "",
        "        # Create directory but leave it empty",
        "        empty_dir.mkdir()",
        "        tasks = load_all_tasks(str(empty_dir))",
        "        assert tasks == [], \"Should return empty list for empty directory\"",
        "",
        "    def test_task_dependencies_persist(self, tmp_path):",
        "        \"\"\"Task dependencies are preserved across save/load.\"\"\"",
        "        session = TaskSession(tasks_dir=str(tmp_path))",
        "",
        "        # Create tasks with dependencies",
        "        task1 = session.create_task(\"Foundation task\")",
        "        task2 = session.create_task(",
        "            \"Dependent task\",",
        "            depends_on=[task1.id]",
        "        )",
        "        task3 = session.create_task(",
        "            \"Multi-dependency task\",",
        "            depends_on=[task1.id, task2.id]",
        "        )",
        "",
        "        # Save and reload",
        "        filepath = session.save()",
        "        loaded = TaskSession.load(filepath)",
        "",
        "        # Verify dependencies preserved",
        "        assert len(loaded.tasks) == 3",
        "        assert loaded.tasks[1].depends_on == [task1.id]",
        "        assert loaded.tasks[2].depends_on == [task1.id, task2.id]",
        "",
        "    def test_task_context_metadata_persists(self, tmp_path):",
        "        \"\"\"Task context metadata is preserved across save/load.\"\"\"",
        "        session = TaskSession(tasks_dir=str(tmp_path))",
        "",
        "        # Create task with rich context",
        "        task = session.create_task(",
        "            \"Task with context\",",
        "            context={",
        "                \"files\": [\"cortical/processor.py\", \"tests/test_processor.py\"],",
        "                \"methods\": [\"compute_all\", \"process_document\"],",
        "                \"line_numbers\": [100, 200, 300],",
        "                \"nested\": {\"key\": \"value\", \"count\": 42}",
        "            }",
        "        )",
        "",
        "        # Save and reload",
        "        filepath = session.save()",
        "        loaded = TaskSession.load(filepath)",
        "",
        "        # Verify context fully preserved",
        "        loaded_task = loaded.tasks[0]",
        "        assert loaded_task.context[\"files\"] == [\"cortical/processor.py\", \"tests/test_processor.py\"]",
        "        assert loaded_task.context[\"methods\"] == [\"compute_all\", \"process_document\"]",
        "        assert loaded_task.context[\"line_numbers\"] == [100, 200, 300]",
        "        assert loaded_task.context[\"nested\"][\"key\"] == \"value\"",
        "        assert loaded_task.context[\"nested\"][\"count\"] == 42"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "tests/performance/test_doc_name_boost_perf.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "\"\"\"",
        "Performance tests for doc_name_boost optimization (Task T-1a1d-001).",
        "",
        "Tests that caching tokenized document names provides significant speedup",
        "(target: 3-4x faster on large corpora).",
        "\"\"\"",
        "",
        "import pytest",
        "import time",
        "from cortical.processor import CorticalTextProcessor",
        "from cortical.layers import CorticalLayer",
        "",
        "",
        "class TestDocNameBoostPerformance:",
        "    \"\"\"",
        "    Performance tests for cached document name tokenization.",
        "",
        "    Verifies that caching tokenized doc_ids in name_tokens field",
        "    provides significant speedup for doc_name_boost in search queries.",
        "    \"\"\"",
        "",
        "    def test_cached_tokens_speedup(self):",
        "        \"\"\"",
        "        Test that cached name_tokens provide significant speedup.",
        "",
        "        Creates a 2000-document corpus and runs 100 search queries",
        "        to measure the performance improvement from caching.",
        "",
        "        Expected: At least 2x speedup (target: 3-4x).",
        "        \"\"\"",
        "        # Create processor with large corpus",
        "        processor = CorticalTextProcessor()",
        "",
        "        # Create 2000 documents with diverse names",
        "        num_docs = 2000",
        "        for i in range(num_docs):",
        "            doc_id = f\"document_{i}_analysis_test_data_file\"",
        "            content = f\"This is document {i} containing some test data and analysis content.\"",
        "            processor.process_document(doc_id, content)",
        "",
        "        # Compute TF-IDF for search",
        "        processor.compute_tfidf(verbose=False)",
        "",
        "        # Verify name_tokens are cached for document minicolumns",
        "        layer3 = processor.layers[CorticalLayer.DOCUMENTS]",
        "        doc_col = layer3.get_by_id(\"L3_document_0_analysis_test_data_file\")",
        "        assert doc_col is not None, \"Document minicolumn should exist\"",
        "        assert doc_col.name_tokens is not None, \"name_tokens should be cached\"",
        "        assert len(doc_col.name_tokens) > 0, \"name_tokens should contain tokens\"",
        "",
        "        # Expected tokens (tokenizer lowercases and stems)",
        "        expected_tokens = {'document', 'analysi', 'test', 'data', 'file'}  # stemmed versions",
        "        # Check that at least some expected tokens are present",
        "        assert any(tok in doc_col.name_tokens for tok in ['document', 'analysi', 'test']), \\",
        "            f\"name_tokens should contain expected tokens, got: {doc_col.name_tokens}\"",
        "",
        "        # Run 100 search queries with doc_name_boost",
        "        num_queries = 100",
        "        queries = [",
        "            \"analysis data\",",
        "            \"test file\",",
        "            \"document analysis\",",
        "            \"data file test\",",
        "            \"analysis test\"",
        "        ]",
        "",
        "        # Measure search time with cached tokens",
        "        start = time.time()",
        "        for i in range(num_queries):",
        "            query = queries[i % len(queries)]",
        "            results = processor.find_documents_for_query(",
        "                query,",
        "                top_n=10",
        "                # doc_name_boost=2.0 is the default in the underlying function",
        "            )",
        "            assert len(results) > 0, f\"Should find results for query: {query}\"",
        "        cached_time = time.time() - start",
        "",
        "        # Create a processor with old-style data (no cached tokens)",
        "        processor_uncached = CorticalTextProcessor()",
        "",
        "        # Add documents but clear name_tokens to simulate old data",
        "        for i in range(num_docs):",
        "            doc_id = f\"document_{i}_analysis_test_data_file\"",
        "            content = f\"This is document {i} containing some test data and analysis content.\"",
        "            processor_uncached.process_document(doc_id, content)",
        "",
        "        processor_uncached.compute_tfidf(verbose=False)",
        "",
        "        # Clear cached tokens to simulate old data format",
        "        layer3_uncached = processor_uncached.layers[CorticalLayer.DOCUMENTS]",
        "        for col in layer3_uncached.minicolumns.values():",
        "            col.name_tokens = None",
        "",
        "        # Measure search time without cached tokens",
        "        start = time.time()",
        "        for i in range(num_queries):",
        "            query = queries[i % len(queries)]",
        "            results = processor_uncached.find_documents_for_query(",
        "                query,",
        "                top_n=10",
        "            )",
        "            assert len(results) > 0, f\"Should find results for query: {query}\"",
        "        uncached_time = time.time() - start",
        "",
        "        # Calculate speedup",
        "        speedup = uncached_time / cached_time",
        "",
        "        # Report timing",
        "        print(f\"\\n=== doc_name_boost Performance ===\")",
        "        print(f\"Corpus size: {num_docs} documents\")",
        "        print(f\"Queries: {num_queries}\")",
        "        print(f\"Cached time: {cached_time:.3f}s ({cached_time/num_queries*1000:.2f}ms per query)\")",
        "        print(f\"Uncached time: {uncached_time:.3f}s ({uncached_time/num_queries*1000:.2f}ms per query)\")",
        "        print(f\"Speedup: {speedup:.2f}x\")",
        "",
        "        # Verify at least 2x speedup",
        "        # Note: Target is 3-4x, but we use 1.5x as minimum to account for test variance",
        "        assert speedup >= 1.5, \\",
        "            f\"Expected at least 1.5x speedup with cached tokens, got {speedup:.2f}x\"",
        "",
        "    def test_name_tokens_populated_on_creation(self):",
        "        \"\"\"",
        "        Test that name_tokens are populated when documents are added.",
        "",
        "        Verifies that the optimization is working as expected.",
        "        \"\"\"",
        "        processor = CorticalTextProcessor()",
        "",
        "        # Add a document",
        "        processor.process_document(",
        "            \"my_test_document_file\",",
        "            \"Some test content here.\"",
        "        )",
        "",
        "        # Get the document minicolumn",
        "        layer3 = processor.layers[CorticalLayer.DOCUMENTS]",
        "        doc_col = layer3.get_by_id(\"L3_my_test_document_file\")",
        "",
        "        # Verify name_tokens is set",
        "        assert doc_col is not None, \"Document minicolumn should exist\"",
        "        assert doc_col.name_tokens is not None, \"name_tokens should be populated\"",
        "",
        "        # Verify it contains expected tokens",
        "        # Tokenizer replaces underscores with spaces and stems",
        "        assert 'test' in doc_col.name_tokens or 'my' in doc_col.name_tokens, \\",
        "            f\"name_tokens should contain expected tokens, got: {doc_col.name_tokens}\"",
        "",
        "    def test_backward_compatibility(self):",
        "        \"\"\"",
        "        Test that search works with old data lacking name_tokens.",
        "",
        "        Ensures fallback tokenization works for backward compatibility.",
        "        \"\"\"",
        "        processor = CorticalTextProcessor()",
        "",
        "        # Add documents with content that matches queries",
        "        processor.process_document(\"test_doc_one\", \"Content about testing and documentation\")",
        "        processor.process_document(\"test_doc_two\", \"More content about testing\")",
        "        processor.compute_tfidf(verbose=False)",
        "",
        "        # Simulate old data by clearing name_tokens",
        "        layer3 = processor.layers[CorticalLayer.DOCUMENTS]",
        "        for col in layer3.minicolumns.values():",
        "            col.name_tokens = None",
        "",
        "        # Search should still work (using fallback)",
        "        results = processor.find_documents_for_query(",
        "            \"testing\",  # Query matches content",
        "            top_n=5",
        "        )",
        "",
        "        # Should find documents based on content",
        "        assert len(results) >= 1, \"Should find documents even without cached tokens\"",
        "        doc_ids = [doc_id for doc_id, _ in results]",
        "        # Verify at least one of our test docs is found",
        "        assert any(doc_id in [\"test_doc_one\", \"test_doc_two\"] for doc_id in doc_ids), \\",
        "            f\"Should find test documents, got: {doc_ids}\"",
        "",
        "    def test_incremental_add_caches_tokens(self):",
        "        \"\"\"",
        "        Test that add_document_incremental also caches name_tokens.",
        "",
        "        Ensures the optimization works for all document addition methods.",
        "        \"\"\"",
        "        processor = CorticalTextProcessor()",
        "",
        "        # Use incremental add",
        "        processor.add_document_incremental(",
        "            \"incremental_test_document\",",
        "            \"Test content for incremental add\",",
        "            recompute='tfidf'",
        "        )",
        "",
        "        # Verify name_tokens is cached",
        "        layer3 = processor.layers[CorticalLayer.DOCUMENTS]",
        "        doc_col = layer3.get_by_id(\"L3_incremental_test_document\")",
        "",
        "        assert doc_col is not None, \"Document should exist\"",
        "        assert doc_col.name_tokens is not None, \"name_tokens should be cached\"",
        "        assert len(doc_col.name_tokens) > 0, \"name_tokens should contain tokens\"",
        "",
        "",
        "if __name__ == '__main__':",
        "    pytest.main([__file__, '-v', '-s'])"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "tests/unit/test_compute_checkpointing.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "\"\"\"",
        "Unit tests for compute_all() checkpointing functionality.",
        "",
        "Tests checkpoint saving, loading, resuming, and full checkpoint/resume cycles.",
        "\"\"\"",
        "",
        "import json",
        "import shutil",
        "import tempfile",
        "import unittest",
        "from pathlib import Path",
        "",
        "from cortical.processor import CorticalTextProcessor",
        "from cortical.config import CorticalConfig",
        "",
        "",
        "class TestComputeCheckpointing(unittest.TestCase):",
        "    \"\"\"Test suite for compute_all() checkpointing functionality.\"\"\"",
        "",
        "    def setUp(self):",
        "        \"\"\"Create a processor with sample documents for testing.\"\"\"",
        "        self.processor = CorticalTextProcessor()",
        "",
        "        # Add some test documents",
        "        self.processor.process_document(",
        "            \"doc1\",",
        "            \"Neural networks are computational models inspired by biological neurons.\"",
        "        )",
        "        self.processor.process_document(",
        "            \"doc2\",",
        "            \"Machine learning algorithms can be trained on large datasets.\"",
        "        )",
        "        self.processor.process_document(",
        "            \"doc3\",",
        "            \"Deep learning uses multiple layers of neural networks.\"",
        "        )",
        "",
        "        # Create temporary directory for checkpoints",
        "        self.checkpoint_dir = tempfile.mkdtemp(prefix='checkpoint_test_')",
        "",
        "    def tearDown(self):",
        "        \"\"\"Clean up temporary checkpoint directory.\"\"\"",
        "        if Path(self.checkpoint_dir).exists():",
        "            shutil.rmtree(self.checkpoint_dir)",
        "",
        "    def test_checkpoint_creates_progress_file(self):",
        "        \"\"\"Test that checkpointing creates a progress file.\"\"\"",
        "        # Run compute_all with checkpointing",
        "        self.processor.compute_all(",
        "            checkpoint_dir=self.checkpoint_dir,",
        "            verbose=False,",
        "            build_concepts=False  # Simpler test",
        "        )",
        "",
        "        # Check that progress file exists",
        "        progress_file = Path(self.checkpoint_dir) / 'checkpoint_progress.json'",
        "        self.assertTrue(progress_file.exists(), \"Progress file should be created\")",
        "",
        "        # Check progress file content",
        "        with open(progress_file, 'r') as f:",
        "            progress_data = json.load(f)",
        "",
        "        self.assertIn('completed_phases', progress_data)",
        "        self.assertIn('last_updated', progress_data)",
        "        self.assertIsInstance(progress_data['completed_phases'], list)",
        "        self.assertGreater(len(progress_data['completed_phases']), 0,",
        "                          \"Should have completed at least one phase\")",
        "",
        "    def test_checkpoint_creates_state_files(self):",
        "        \"\"\"Test that checkpointing creates all required state files.\"\"\"",
        "        self.processor.compute_all(",
        "            checkpoint_dir=self.checkpoint_dir,",
        "            verbose=False,",
        "            build_concepts=False",
        "        )",
        "",
        "        checkpoint_path = Path(self.checkpoint_dir)",
        "",
        "        # Check for manifest and documents",
        "        self.assertTrue((checkpoint_path / 'manifest.json').exists())",
        "        self.assertTrue((checkpoint_path / 'documents.json').exists())",
        "",
        "        # Check for layer files",
        "        layers_dir = checkpoint_path / 'layers'",
        "        self.assertTrue(layers_dir.exists())",
        "        self.assertTrue((layers_dir / 'L0_tokens.json').exists())",
        "        self.assertTrue((layers_dir / 'L1_bigrams.json').exists())",
        "",
        "    def test_checkpoint_progress_accumulates(self):",
        "        \"\"\"Test that checkpoint progress accumulates across phases.\"\"\"",
        "        # Create a custom checkpoint test by manually calling _save_checkpoint",
        "        self.processor._save_checkpoint(self.checkpoint_dir, 'phase1', verbose=False)",
        "",
        "        progress = self.processor._load_checkpoint_progress(self.checkpoint_dir)",
        "        self.assertEqual(len(progress), 1)",
        "        self.assertIn('phase1', progress)",
        "",
        "        # Add another phase",
        "        self.processor._save_checkpoint(self.checkpoint_dir, 'phase2', verbose=False)",
        "",
        "        progress = self.processor._load_checkpoint_progress(self.checkpoint_dir)",
        "        self.assertEqual(len(progress), 2)",
        "        self.assertIn('phase1', progress)",
        "        self.assertIn('phase2', progress)",
        "",
        "    def test_resume_skips_completed_phases(self):",
        "        \"\"\"Test that resuming skips already completed phases.\"\"\"",
        "        # Run compute_all to completion with checkpointing",
        "        stats1 = self.processor.compute_all(",
        "            checkpoint_dir=self.checkpoint_dir,",
        "            verbose=False,",
        "            build_concepts=False",
        "        )",
        "",
        "        # Load checkpoint and resume (should skip all phases)",
        "        processor2 = CorticalTextProcessor.resume_from_checkpoint(",
        "            self.checkpoint_dir,",
        "            verbose=False",
        "        )",
        "",
        "        # Resume should complete quickly since all phases are done",
        "        stats2 = processor2.compute_all(",
        "            checkpoint_dir=self.checkpoint_dir,",
        "            resume=True,",
        "            verbose=False,",
        "            build_concepts=False",
        "        )",
        "",
        "        # Both should have similar stats structure",
        "        self.assertIsInstance(stats2, dict)",
        "",
        "    def test_checkpoint_state_matches_in_memory(self):",
        "        \"\"\"Test that checkpoint state matches the in-memory processor state.\"\"\"",
        "        # Compute all phases",
        "        self.processor.compute_all(",
        "            checkpoint_dir=self.checkpoint_dir,",
        "            verbose=False,",
        "            build_concepts=False",
        "        )",
        "",
        "        # Load from checkpoint",
        "        processor2 = CorticalTextProcessor.resume_from_checkpoint(",
        "            self.checkpoint_dir,",
        "            verbose=False",
        "        )",
        "",
        "        # Compare document counts",
        "        self.assertEqual(",
        "            len(self.processor.documents),",
        "            len(processor2.documents),",
        "            \"Document count should match\"",
        "        )",
        "",
        "        # Compare layer counts",
        "        from cortical.layers import CorticalLayer",
        "        for layer_enum in [CorticalLayer.TOKENS, CorticalLayer.BIGRAMS]:",
        "            count1 = len(self.processor.layers[layer_enum].minicolumns)",
        "            count2 = len(processor2.layers[layer_enum].minicolumns)",
        "            self.assertEqual(",
        "                count1, count2,",
        "                f\"Layer {layer_enum.name} minicolumn count should match\"",
        "            )",
        "",
        "    def test_no_checkpointing_when_dir_none(self):",
        "        \"\"\"Test that no checkpointing occurs when checkpoint_dir is None.\"\"\"",
        "        # Run without checkpointing",
        "        self.processor.compute_all(",
        "            checkpoint_dir=None,  # Default behavior",
        "            verbose=False,",
        "            build_concepts=False",
        "        )",
        "",
        "        # Progress file should not exist in any default location",
        "        progress_file = Path(self.checkpoint_dir) / 'checkpoint_progress.json'",
        "        self.assertFalse(progress_file.exists(),",
        "                        \"Progress file should not be created without checkpoint_dir\")",
        "",
        "    def test_resume_with_concepts(self):",
        "        \"\"\"Test resuming with concept building enabled.\"\"\"",
        "        # Run partial computation and stop",
        "        # We'll simulate this by running compute_all partially",
        "        self.processor.compute_all(",
        "            checkpoint_dir=self.checkpoint_dir,",
        "            verbose=False,",
        "            build_concepts=True  # Enable concept building",
        "        )",
        "",
        "        # Resume from checkpoint",
        "        processor2 = CorticalTextProcessor.resume_from_checkpoint(",
        "            self.checkpoint_dir,",
        "            verbose=False",
        "        )",
        "",
        "        # Complete the computation (should skip completed phases)",
        "        stats = processor2.compute_all(",
        "            checkpoint_dir=self.checkpoint_dir,",
        "            resume=True,",
        "            verbose=False,",
        "            build_concepts=True",
        "        )",
        "",
        "        self.assertIsInstance(stats, dict)",
        "        # If concepts were built, should have clusters_created",
        "        if 'clusters_created' in stats:",
        "            self.assertIsInstance(stats['clusters_created'], int)",
        "",
        "    def test_checkpoint_with_different_pagerank_methods(self):",
        "        \"\"\"Test checkpointing works with different PageRank methods.\"\"\"",
        "        for method in ['standard', 'semantic', 'hierarchical']:",
        "            with self.subTest(pagerank_method=method):",
        "                # Clean checkpoint dir for each test",
        "                if Path(self.checkpoint_dir).exists():",
        "                    shutil.rmtree(self.checkpoint_dir)",
        "                Path(self.checkpoint_dir).mkdir()",
        "",
        "                # Run with specific pagerank method",
        "                stats = self.processor.compute_all(",
        "                    checkpoint_dir=self.checkpoint_dir,",
        "                    verbose=False,",
        "                    build_concepts=False,",
        "                    pagerank_method=method",
        "                )",
        "",
        "                # Check that progress file contains the right phase",
        "                progress = self.processor._load_checkpoint_progress(self.checkpoint_dir)",
        "                expected_phase = f\"pagerank_{method}\"",
        "                self.assertIn(expected_phase, progress,",
        "                            f\"Should checkpoint {expected_phase}\")",
        "",
        "    def test_checkpoint_with_different_connection_strategies(self):",
        "        \"\"\"Test checkpointing works with different concept connection strategies.\"\"\"",
        "        for strategy in ['document_overlap', 'semantic', 'embedding', 'hybrid']:",
        "            with self.subTest(connection_strategy=strategy):",
        "                # Clean checkpoint dir for each test",
        "                if Path(self.checkpoint_dir).exists():",
        "                    shutil.rmtree(self.checkpoint_dir)",
        "                Path(self.checkpoint_dir).mkdir()",
        "",
        "                # Run with specific connection strategy",
        "                stats = self.processor.compute_all(",
        "                    checkpoint_dir=self.checkpoint_dir,",
        "                    verbose=False,",
        "                    build_concepts=True,",
        "                    connection_strategy=strategy",
        "                )",
        "",
        "                # Check that progress file contains the right phase",
        "                progress = self.processor._load_checkpoint_progress(self.checkpoint_dir)",
        "                expected_phase = f\"concept_connections_{strategy}\"",
        "                self.assertIn(expected_phase, progress,",
        "                            f\"Should checkpoint {expected_phase}\")",
        "",
        "    def test_load_checkpoint_progress_empty_dir(self):",
        "        \"\"\"Test loading checkpoint progress from empty directory.\"\"\"",
        "        # Create an empty directory",
        "        empty_dir = tempfile.mkdtemp(prefix='empty_checkpoint_')",
        "        try:",
        "            progress = self.processor._load_checkpoint_progress(empty_dir)",
        "            self.assertEqual(len(progress), 0, \"Empty dir should have no progress\")",
        "            self.assertIsInstance(progress, set)",
        "        finally:",
        "            shutil.rmtree(empty_dir)",
        "",
        "    def test_load_checkpoint_progress_corrupted_file(self):",
        "        \"\"\"Test loading checkpoint progress from corrupted file.\"\"\"",
        "        # Create a corrupted progress file",
        "        progress_file = Path(self.checkpoint_dir) / 'checkpoint_progress.json'",
        "        progress_file.parent.mkdir(parents=True, exist_ok=True)",
        "        with open(progress_file, 'w') as f:",
        "            f.write(\"{ invalid json }\")",
        "",
        "        # Should handle gracefully",
        "        progress = self.processor._load_checkpoint_progress(self.checkpoint_dir)",
        "        self.assertEqual(len(progress), 0, \"Corrupted file should return empty progress\")",
        "",
        "    def test_full_checkpoint_resume_cycle(self):",
        "        \"\"\"Test a full checkpoint and resume cycle.\"\"\"",
        "        # Step 1: Run compute_all with checkpointing",
        "        stats1 = self.processor.compute_all(",
        "            checkpoint_dir=self.checkpoint_dir,",
        "            verbose=False,",
        "            build_concepts=True,",
        "            pagerank_method='standard',",
        "            connection_strategy='document_overlap'",
        "        )",
        "",
        "        # Step 2: Verify checkpoint files exist",
        "        checkpoint_path = Path(self.checkpoint_dir)",
        "        self.assertTrue((checkpoint_path / 'checkpoint_progress.json').exists())",
        "        self.assertTrue((checkpoint_path / 'manifest.json').exists())",
        "",
        "        # Step 3: Load checkpoint progress",
        "        progress = self.processor._load_checkpoint_progress(self.checkpoint_dir)",
        "        self.assertGreater(len(progress), 0, \"Should have completed phases\")",
        "",
        "        # Step 4: Resume from checkpoint",
        "        processor2 = CorticalTextProcessor.resume_from_checkpoint(",
        "            self.checkpoint_dir,",
        "            verbose=False",
        "        )",
        "",
        "        # Step 5: Verify resumed processor state",
        "        self.assertEqual(len(processor2.documents), len(self.processor.documents))",
        "",
        "        # Step 6: Complete computation (should skip all phases)",
        "        stats2 = processor2.compute_all(",
        "            checkpoint_dir=self.checkpoint_dir,",
        "            resume=True,",
        "            verbose=False,",
        "            build_concepts=True,",
        "            pagerank_method='standard',",
        "            connection_strategy='document_overlap'",
        "        )",
        "",
        "        # Both runs should complete successfully",
        "        self.assertIsInstance(stats1, dict)",
        "        self.assertIsInstance(stats2, dict)",
        "",
        "    def test_resume_from_checkpoint_classmethod(self):",
        "        \"\"\"Test the resume_from_checkpoint class method.\"\"\"",
        "        # Create a checkpoint",
        "        self.processor.compute_all(",
        "            checkpoint_dir=self.checkpoint_dir,",
        "            verbose=False,",
        "            build_concepts=False",
        "        )",
        "",
        "        # Resume using class method",
        "        processor2 = CorticalTextProcessor.resume_from_checkpoint(",
        "            self.checkpoint_dir,",
        "            verbose=False",
        "        )",
        "",
        "        # Verify it's a valid processor",
        "        self.assertIsInstance(processor2, CorticalTextProcessor)",
        "        self.assertEqual(len(processor2.documents), 3)",
        "        self.assertGreater(len(processor2.layers), 0)",
        "",
        "    def test_checkpoint_atomicity(self):",
        "        \"\"\"Test that checkpoint saves are atomic.\"\"\"",
        "        # Save a checkpoint",
        "        self.processor._save_checkpoint(self.checkpoint_dir, 'test_phase', verbose=False)",
        "",
        "        # Verify no temporary files remain",
        "        checkpoint_path = Path(self.checkpoint_dir)",
        "        temp_files = list(checkpoint_path.glob('**/*.tmp'))",
        "        self.assertEqual(len(temp_files), 0, \"No temporary files should remain\")",
        "",
        "        # Verify progress file exists",
        "        progress_file = checkpoint_path / 'checkpoint_progress.json'",
        "        self.assertTrue(progress_file.exists())",
        "",
        "",
        "if __name__ == '__main__':",
        "    unittest.main()"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "tests/unit/test_processor_json_persistence.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "\"\"\"",
        "Unit tests for JSON persistence methods in CorticalTextProcessor.",
        "",
        "Tests the save_json(), load_json(), and migrate_to_json() methods.",
        "\"\"\"",
        "",
        "import json",
        "import os",
        "import shutil",
        "import tempfile",
        "import unittest",
        "from pathlib import Path",
        "",
        "from cortical.processor import CorticalTextProcessor",
        "from cortical.config import CorticalConfig",
        "from cortical.layers import CorticalLayer",
        "",
        "",
        "class TestProcessorJSONPersistence(unittest.TestCase):",
        "    \"\"\"Test JSON persistence methods.\"\"\"",
        "",
        "    def setUp(self):",
        "        \"\"\"Create test processor and temporary directories.\"\"\"",
        "        self.temp_dir = tempfile.mkdtemp()",
        "        self.json_dir = os.path.join(self.temp_dir, 'test_state')",
        "        self.pkl_path = os.path.join(self.temp_dir, 'test.pkl')",
        "",
        "        # Create processor with some test data",
        "        self.processor = CorticalTextProcessor()",
        "        self.processor.process_document(\"doc1\", \"Neural networks learn patterns from data.\")",
        "        self.processor.process_document(\"doc2\", \"Machine learning algorithms optimize parameters.\")",
        "        self.processor.compute_all()",
        "",
        "    def tearDown(self):",
        "        \"\"\"Clean up temporary files.\"\"\"",
        "        if os.path.exists(self.temp_dir):",
        "            shutil.rmtree(self.temp_dir)",
        "",
        "    def test_save_json_creates_directory_structure(self):",
        "        \"\"\"Test that save_json creates the correct directory structure.\"\"\"",
        "        results = self.processor.save_json(self.json_dir, verbose=False)",
        "",
        "        # Check that directory exists",
        "        self.assertTrue(os.path.exists(self.json_dir))",
        "",
        "        # Check that manifest exists",
        "        manifest_path = os.path.join(self.json_dir, 'manifest.json')",
        "        self.assertTrue(os.path.exists(manifest_path))",
        "",
        "        # Check that subdirectories exist",
        "        layers_dir = os.path.join(self.json_dir, 'layers')",
        "        computed_dir = os.path.join(self.json_dir, 'computed')",
        "        self.assertTrue(os.path.exists(layers_dir))",
        "        self.assertTrue(os.path.exists(computed_dir))",
        "",
        "        # Check that layer files exist",
        "        for level in range(4):",
        "            layer_file = os.path.join(layers_dir, f'L{level}_{[\"tokens\", \"bigrams\", \"concepts\", \"documents\"][level]}.json')",
        "            self.assertTrue(os.path.exists(layer_file), f\"Layer file {layer_file} should exist\")",
        "",
        "        # Check that documents file exists",
        "        docs_path = os.path.join(self.json_dir, 'documents.json')",
        "        self.assertTrue(os.path.exists(docs_path))",
        "",
        "    def test_save_json_returns_write_status(self):",
        "        \"\"\"Test that save_json returns correct write status.\"\"\"",
        "        # First save should write all files",
        "        results = self.processor.save_json(self.json_dir, verbose=False)",
        "",
        "        # Check that results contain expected keys",
        "        self.assertIn('layer_0', results)",
        "        self.assertIn('layer_1', results)",
        "        self.assertIn('layer_2', results)",
        "        self.assertIn('layer_3', results)",
        "        self.assertIn('documents', results)",
        "",
        "        # First save should write files",
        "        self.assertTrue(results['layer_0'])",
        "        self.assertTrue(results['documents'])",
        "",
        "    def test_save_json_incremental_only_updates_changed(self):",
        "        \"\"\"Test that incremental save only updates changed components.\"\"\"",
        "        # First save",
        "        self.processor.save_json(self.json_dir, verbose=False)",
        "",
        "        # Second save without changes should not rewrite files",
        "        results = self.processor.save_json(self.json_dir, verbose=False)",
        "",
        "        # No files should be written (unchanged)",
        "        self.assertFalse(results.get('layer_0', True))",
        "        self.assertFalse(results.get('documents', True))",
        "",
        "        # Add a new document",
        "        self.processor.add_document_incremental(\"doc3\", \"Deep learning uses neural networks.\", recompute='none')",
        "",
        "        # Third save should only update documents and affected layers",
        "        results = self.processor.save_json(self.json_dir, verbose=False)",
        "",
        "        # Documents should be rewritten",
        "        self.assertTrue(results.get('documents', False))",
        "",
        "    def test_save_json_force_overwrites_all(self):",
        "        \"\"\"Test that force=True always overwrites files.\"\"\"",
        "        # First save",
        "        self.processor.save_json(self.json_dir, verbose=False)",
        "",
        "        # Second save with force should rewrite all files",
        "        results = self.processor.save_json(self.json_dir, force=True, verbose=False)",
        "",
        "        # All files should be written",
        "        self.assertTrue(results['layer_0'])",
        "        self.assertTrue(results['documents'])",
        "",
        "    def test_load_json_restores_processor_state(self):",
        "        \"\"\"Test that load_json correctly restores processor state.\"\"\"",
        "        # Save processor",
        "        self.processor.save_json(self.json_dir, verbose=False)",
        "",
        "        # Load processor",
        "        loaded = CorticalTextProcessor.load_json(self.json_dir, verbose=False)",
        "",
        "        # Verify documents are restored",
        "        self.assertEqual(len(loaded.documents), 2)",
        "        self.assertIn(\"doc1\", loaded.documents)",
        "        self.assertIn(\"doc2\", loaded.documents)",
        "        self.assertEqual(loaded.documents[\"doc1\"], \"Neural networks learn patterns from data.\")",
        "",
        "        # Verify layers are restored",
        "        for layer_enum in [CorticalLayer.TOKENS, CorticalLayer.BIGRAMS, CorticalLayer.CONCEPTS, CorticalLayer.DOCUMENTS]:",
        "            self.assertIn(layer_enum, loaded.layers)",
        "            # Check that layers have content",
        "            layer = loaded.layers[layer_enum]",
        "            if layer_enum in [CorticalLayer.TOKENS, CorticalLayer.DOCUMENTS]:",
        "                self.assertGreater(len(layer.minicolumns), 0)",
        "",
        "    def test_load_json_with_custom_config(self):",
        "        \"\"\"Test that load_json accepts custom config.\"\"\"",
        "        # Save processor",
        "        self.processor.save_json(self.json_dir, verbose=False)",
        "",
        "        # Load with custom config",
        "        custom_config = CorticalConfig(chunk_size=300, chunk_overlap=75)",
        "        loaded = CorticalTextProcessor.load_json(self.json_dir, config=custom_config, verbose=False)",
        "",
        "        # Verify config was applied",
        "        self.assertEqual(loaded.config.chunk_size, 300)",
        "        self.assertEqual(loaded.config.chunk_overlap, 75)",
        "",
        "        # Verify data is still loaded correctly",
        "        self.assertEqual(len(loaded.documents), 2)",
        "",
        "    def test_load_json_restores_embeddings(self):",
        "        \"\"\"Test that load_json restores graph embeddings.\"\"\"",
        "        # Compute embeddings",
        "        self.processor.compute_graph_embeddings(verbose=False)",
        "",
        "        # Save processor",
        "        self.processor.save_json(self.json_dir, verbose=False)",
        "",
        "        # Load processor",
        "        loaded = CorticalTextProcessor.load_json(self.json_dir, verbose=False)",
        "",
        "        # Verify embeddings are restored",
        "        self.assertGreater(len(loaded.embeddings), 0)",
        "        # Check that some terms have embeddings",
        "        for term in ['neural', 'networks', 'learning']:",
        "            if term in self.processor.embeddings:",
        "                self.assertIn(term, loaded.embeddings)",
        "                # Check that embedding vectors match",
        "                self.assertEqual(",
        "                    loaded.embeddings[term],",
        "                    self.processor.embeddings[term]",
        "                )",
        "",
        "    def test_load_json_restores_semantic_relations(self):",
        "        \"\"\"Test that load_json restores semantic relations.\"\"\"",
        "        # Extract semantic relations",
        "        self.processor.extract_corpus_semantics(verbose=False)",
        "",
        "        # Save processor",
        "        self.processor.save_json(self.json_dir, verbose=False)",
        "",
        "        # Load processor",
        "        loaded = CorticalTextProcessor.load_json(self.json_dir, verbose=False)",
        "",
        "        # Verify semantic relations are restored",
        "        self.assertEqual(len(loaded.semantic_relations), len(self.processor.semantic_relations))",
        "",
        "        # Check that some relations match",
        "        if self.processor.semantic_relations:",
        "            # Relations are tuples of (term1, relation, term2, weight)",
        "            self.assertIn(self.processor.semantic_relations[0], loaded.semantic_relations)",
        "",
        "    def test_load_json_restores_staleness_tracking(self):",
        "        \"\"\"Test that load_json restores staleness tracking.\"\"\"",
        "        # Mark some computations as stale",
        "        self.processor._mark_all_stale()",
        "",
        "        # Save processor",
        "        self.processor.save_json(self.json_dir, verbose=False)",
        "",
        "        # Load processor",
        "        loaded = CorticalTextProcessor.load_json(self.json_dir, verbose=False)",
        "",
        "        # Verify staleness is restored",
        "        stale = loaded.get_stale_computations()",
        "        self.assertIsInstance(stale, set)",
        "        # At least some computations should be marked stale",
        "        self.assertTrue(len(stale) > 0)",
        "",
        "    def test_round_trip_preserves_all_data(self):",
        "        \"\"\"Test that save_json followed by load_json preserves all data.\"\"\"",
        "        # Compute all features",
        "        self.processor.compute_all(verbose=False)",
        "        self.processor.compute_graph_embeddings(verbose=False)",
        "        self.processor.extract_corpus_semantics(verbose=False)",
        "",
        "        # Save",
        "        self.processor.save_json(self.json_dir, verbose=False)",
        "",
        "        # Load",
        "        loaded = CorticalTextProcessor.load_json(self.json_dir, verbose=False)",
        "",
        "        # Verify all data is preserved",
        "        self.assertEqual(loaded.documents, self.processor.documents)",
        "        self.assertEqual(len(loaded.layers), len(self.processor.layers))",
        "",
        "        # Verify layer 0 minicolumns",
        "        layer0_orig = self.processor.layers[CorticalLayer.TOKENS]",
        "        layer0_loaded = loaded.layers[CorticalLayer.TOKENS]",
        "        self.assertEqual(len(layer0_orig.minicolumns), len(layer0_loaded.minicolumns))",
        "",
        "        # Verify a specific minicolumn's data",
        "        if layer0_orig.minicolumns:",
        "            sample_id = list(layer0_orig.minicolumns.keys())[0]",
        "            orig_col = layer0_orig.minicolumns[sample_id]",
        "            loaded_col = layer0_loaded.minicolumns[sample_id]",
        "",
        "            self.assertEqual(orig_col.id, loaded_col.id)",
        "            self.assertEqual(orig_col.content, loaded_col.content)",
        "            self.assertEqual(orig_col.document_ids, loaded_col.document_ids)",
        "            # PageRank and TF-IDF should be close (floating point comparison)",
        "            self.assertAlmostEqual(orig_col.pagerank, loaded_col.pagerank, places=6)",
        "",
        "    def test_migrate_to_json_converts_pickle(self):",
        "        \"\"\"Test that migrate_to_json successfully converts pickle to JSON.\"\"\"",
        "        # Save as pickle first",
        "        self.processor.save(self.pkl_path, verbose=False)",
        "",
        "        # Verify pickle file exists",
        "        self.assertTrue(os.path.exists(self.pkl_path))",
        "",
        "        # Migrate to JSON",
        "        result = self.processor.migrate_to_json(self.pkl_path, self.json_dir, verbose=False)",
        "",
        "        # Verify migration succeeded",
        "        self.assertTrue(result)",
        "",
        "        # Verify JSON directory was created",
        "        self.assertTrue(os.path.exists(self.json_dir))",
        "",
        "        # Verify manifest exists",
        "        manifest_path = os.path.join(self.json_dir, 'manifest.json')",
        "        self.assertTrue(os.path.exists(manifest_path))",
        "",
        "        # Load from JSON and verify data",
        "        loaded = CorticalTextProcessor.load_json(self.json_dir, verbose=False)",
        "        self.assertEqual(len(loaded.documents), 2)",
        "        self.assertIn(\"doc1\", loaded.documents)",
        "",
        "    def test_migrate_to_json_preserves_data(self):",
        "        \"\"\"Test that migration from pickle to JSON preserves all data.\"\"\"",
        "        # Compute all features",
        "        self.processor.compute_all(verbose=False)",
        "        self.processor.compute_graph_embeddings(verbose=False)",
        "        self.processor.extract_corpus_semantics(verbose=False)",
        "",
        "        # Save as pickle",
        "        self.processor.save(self.pkl_path, verbose=False)",
        "",
        "        # Load from pickle to verify original state",
        "        pkl_loaded = CorticalTextProcessor.load(self.pkl_path, verbose=False)",
        "",
        "        # Migrate to JSON",
        "        self.processor.migrate_to_json(self.pkl_path, self.json_dir, verbose=False)",
        "",
        "        # Load from JSON",
        "        json_loaded = CorticalTextProcessor.load_json(self.json_dir, verbose=False)",
        "",
        "        # Verify data matches",
        "        self.assertEqual(json_loaded.documents, pkl_loaded.documents)",
        "        self.assertEqual(len(json_loaded.layers), len(pkl_loaded.layers))",
        "        self.assertEqual(len(json_loaded.embeddings), len(pkl_loaded.embeddings))",
        "        self.assertEqual(len(json_loaded.semantic_relations), len(pkl_loaded.semantic_relations))",
        "",
        "    def test_save_json_with_empty_processor(self):",
        "        \"\"\"Test that save_json works with empty processor.\"\"\"",
        "        empty_processor = CorticalTextProcessor()",
        "",
        "        # Save empty processor",
        "        results = empty_processor.save_json(self.json_dir, verbose=False)",
        "",
        "        # Verify directory was created",
        "        self.assertTrue(os.path.exists(self.json_dir))",
        "",
        "        # Verify files were written",
        "        self.assertTrue(results.get('layer_0', False))",
        "",
        "        # Load and verify",
        "        loaded = CorticalTextProcessor.load_json(self.json_dir, verbose=False)",
        "        self.assertEqual(len(loaded.documents), 0)",
        "",
        "    def test_load_json_raises_on_missing_directory(self):",
        "        \"\"\"Test that load_json raises FileNotFoundError for missing directory.\"\"\"",
        "        missing_dir = os.path.join(self.temp_dir, 'nonexistent')",
        "",
        "        with self.assertRaises(FileNotFoundError):",
        "            CorticalTextProcessor.load_json(missing_dir, verbose=False)",
        "",
        "    def test_save_json_manifest_contains_correct_metadata(self):",
        "        \"\"\"Test that manifest.json contains correct metadata.\"\"\"",
        "        # Save processor",
        "        self.processor.save_json(self.json_dir, verbose=False)",
        "",
        "        # Read manifest",
        "        manifest_path = os.path.join(self.json_dir, 'manifest.json')",
        "        with open(manifest_path, 'r', encoding='utf-8') as f:",
        "            manifest = json.load(f)",
        "",
        "        # Verify manifest structure",
        "        self.assertIn('version', manifest)",
        "        self.assertIn('created_at', manifest)",
        "        self.assertIn('updated_at', manifest)",
        "        self.assertIn('checksums', manifest)",
        "        self.assertIn('document_count', manifest)",
        "        self.assertIn('layer_stats', manifest)",
        "",
        "        # Verify document count",
        "        self.assertEqual(manifest['document_count'], 2)",
        "",
        "        # Verify checksums exist for components",
        "        self.assertIn('layer_0', manifest['checksums'])",
        "        self.assertIn('documents', manifest['checksums'])",
        "",
        "    def test_save_json_documents_file_structure(self):",
        "        \"\"\"Test that documents.json has correct structure.\"\"\"",
        "        # Save processor",
        "        self.processor.save_json(self.json_dir, verbose=False)",
        "",
        "        # Read documents.json",
        "        docs_path = os.path.join(self.json_dir, 'documents.json')",
        "        with open(docs_path, 'r', encoding='utf-8') as f:",
        "            data = json.load(f)",
        "",
        "        # Verify structure",
        "        self.assertIn('documents', data)",
        "        self.assertIn('metadata', data)",
        "",
        "        # Verify content",
        "        self.assertEqual(len(data['documents']), 2)",
        "        self.assertIn('doc1', data['documents'])",
        "        self.assertEqual(data['documents']['doc1'], \"Neural networks learn patterns from data.\")",
        "",
        "    def test_save_json_layer_file_structure(self):",
        "        \"\"\"Test that layer JSON files have correct structure.\"\"\"",
        "        # Save processor",
        "        self.processor.save_json(self.json_dir, verbose=False)",
        "",
        "        # Read a layer file",
        "        layer_path = os.path.join(self.json_dir, 'layers', 'L0_tokens.json')",
        "        with open(layer_path, 'r', encoding='utf-8') as f:",
        "            data = json.load(f)",
        "",
        "        # Verify structure (should match HierarchicalLayer.to_dict())",
        "        self.assertIn('level', data)",
        "        self.assertIn('minicolumns', data)",
        "",
        "        # Verify level",
        "        self.assertEqual(data['level'], 0)",
        "",
        "        # Verify minicolumns is a dict",
        "        self.assertIsInstance(data['minicolumns'], dict)",
        "",
        "    def test_multiple_save_load_cycles(self):",
        "        \"\"\"Test that multiple save/load cycles work correctly.\"\"\"",
        "        # First cycle",
        "        self.processor.save_json(self.json_dir, verbose=False)",
        "        loaded1 = CorticalTextProcessor.load_json(self.json_dir, verbose=False)",
        "",
        "        # Add more data",
        "        loaded1.add_document_incremental(\"doc3\", \"Deep learning is powerful.\", recompute='none')",
        "",
        "        # Second cycle",
        "        loaded1.save_json(self.json_dir, verbose=False)",
        "        loaded2 = CorticalTextProcessor.load_json(self.json_dir, verbose=False)",
        "",
        "        # Verify all documents present",
        "        self.assertEqual(len(loaded2.documents), 3)",
        "        self.assertIn(\"doc1\", loaded2.documents)",
        "        self.assertIn(\"doc2\", loaded2.documents)",
        "        self.assertIn(\"doc3\", loaded2.documents)",
        "",
        "    def test_save_json_preserves_document_metadata(self):",
        "        \"\"\"Test that document metadata is preserved.\"\"\"",
        "        # Add metadata",
        "        self.processor.document_metadata['doc1'] = {'source': 'test', 'timestamp': '2025-12-14'}",
        "        self.processor.document_metadata['doc2'] = {'source': 'test2', 'timestamp': '2025-12-15'}",
        "",
        "        # Save",
        "        self.processor.save_json(self.json_dir, verbose=False)",
        "",
        "        # Load",
        "        loaded = CorticalTextProcessor.load_json(self.json_dir, verbose=False)",
        "",
        "        # Verify metadata preserved",
        "        self.assertEqual(loaded.document_metadata['doc1']['source'], 'test')",
        "        self.assertEqual(loaded.document_metadata['doc2']['timestamp'], '2025-12-15')",
        "",
        "",
        "class TestJSONPersistenceEdgeCases(unittest.TestCase):",
        "    \"\"\"Test edge cases for JSON persistence.\"\"\"",
        "",
        "    def setUp(self):",
        "        \"\"\"Create temporary directory.\"\"\"",
        "        self.temp_dir = tempfile.mkdtemp()",
        "        self.json_dir = os.path.join(self.temp_dir, 'test_state')",
        "",
        "    def tearDown(self):",
        "        \"\"\"Clean up temporary files.\"\"\"",
        "        if os.path.exists(self.temp_dir):",
        "            shutil.rmtree(self.temp_dir)",
        "",
        "    def test_save_json_with_no_semantic_relations(self):",
        "        \"\"\"Test saving when semantic_relations is empty.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(\"doc1\", \"Test content.\")",
        "",
        "        # Don't extract semantics",
        "        results = processor.save_json(self.json_dir, verbose=False)",
        "",
        "        # Should still succeed",
        "        self.assertTrue(os.path.exists(self.json_dir))",
        "",
        "        # Load and verify",
        "        loaded = CorticalTextProcessor.load_json(self.json_dir, verbose=False)",
        "        self.assertEqual(len(loaded.semantic_relations), 0)",
        "",
        "    def test_save_json_with_no_embeddings(self):",
        "        \"\"\"Test saving when embeddings is empty.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(\"doc1\", \"Test content.\")",
        "",
        "        # Don't compute embeddings",
        "        results = processor.save_json(self.json_dir, verbose=False)",
        "",
        "        # Should still succeed",
        "        self.assertTrue(os.path.exists(self.json_dir))",
        "",
        "        # Load and verify",
        "        loaded = CorticalTextProcessor.load_json(self.json_dir, verbose=False)",
        "        self.assertEqual(len(loaded.embeddings), 0)",
        "",
        "    def test_load_json_with_missing_computed_files(self):",
        "        \"\"\"Test loading when computed/ files are missing.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(\"doc1\", \"Test content.\")",
        "",
        "        # Save",
        "        processor.save_json(self.json_dir, verbose=False)",
        "",
        "        # Remove computed files",
        "        computed_dir = os.path.join(self.json_dir, 'computed')",
        "        if os.path.exists(computed_dir):",
        "            shutil.rmtree(computed_dir)",
        "",
        "        # Should still load successfully (computed data is optional)",
        "        loaded = CorticalTextProcessor.load_json(self.json_dir, verbose=False)",
        "        self.assertEqual(len(loaded.documents), 1)",
        "        self.assertEqual(len(loaded.embeddings), 0)",
        "        self.assertEqual(len(loaded.semantic_relations), 0)",
        "",
        "    def test_save_json_verbose_output(self):",
        "        \"\"\"Test that verbose=True produces log output.\"\"\"",
        "        import logging",
        "        from io import StringIO",
        "",
        "        # Capture log output",
        "        log_capture = StringIO()",
        "        handler = logging.StreamHandler(log_capture)",
        "        handler.setLevel(logging.INFO)",
        "        logger = logging.getLogger('cortical.state_storage')",
        "        logger.addHandler(handler)",
        "        logger.setLevel(logging.INFO)",
        "",
        "        try:",
        "            processor = CorticalTextProcessor()",
        "            processor.process_document(\"doc1\", \"Test content.\")",
        "",
        "            # Save with verbose=True",
        "            processor.save_json(self.json_dir, verbose=True)",
        "",
        "            # Check that log contains expected messages",
        "            log_output = log_capture.getvalue()",
        "            # Note: verbose logging goes through state_storage module",
        "            # The exact message depends on state_storage implementation",
        "",
        "        finally:",
        "            logger.removeHandler(handler)",
        "",
        "",
        "if __name__ == '__main__':",
        "    unittest.main()"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "tests/unit/test_session_context.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "#!/usr/bin/env python3",
        "\"\"\"",
        "Unit tests for session_context.py - session context generator.",
        "",
        "Tests cover:",
        "- Recent session retrieval",
        "- Pending task grouping",
        "- Git commit parsing",
        "- File change analysis",
        "- Markdown and JSON generation",
        "- Edge cases (missing directories, no git, corrupted files)",
        "\"\"\"",
        "",
        "import json",
        "import subprocess",
        "import tempfile",
        "import unittest",
        "from datetime import datetime, timedelta",
        "from pathlib import Path",
        "from unittest.mock import patch, MagicMock",
        "import sys",
        "",
        "# Add scripts to path",
        "sys.path.insert(0, str(Path(__file__).parent.parent.parent / 'scripts'))",
        "",
        "from session_context import SessionContextGenerator",
        "from task_utils import Task, TaskSession",
        "",
        "",
        "class TestSessionContextGenerator(unittest.TestCase):",
        "    \"\"\"Test suite for SessionContextGenerator class.\"\"\"",
        "",
        "    def setUp(self):",
        "        \"\"\"Create temporary directory for test files.\"\"\"",
        "        self.temp_dir = tempfile.TemporaryDirectory()",
        "        self.repo_path = Path(self.temp_dir.name)",
        "        self.tasks_dir = self.repo_path / 'tasks'",
        "        self.tasks_dir.mkdir()",
        "",
        "        self.generator = SessionContextGenerator(str(self.repo_path))",
        "",
        "    def tearDown(self):",
        "        \"\"\"Clean up temporary directory.\"\"\"",
        "        self.temp_dir.cleanup()",
        "",
        "    def _create_test_session(",
        "        self,",
        "        session_id: str,",
        "        num_tasks: int = 3,",
        "        started_at: str = None",
        "    ) -> Path:",
        "        \"\"\"",
        "        Helper to create a test session file.",
        "",
        "        Args:",
        "            session_id: Session identifier",
        "            num_tasks: Number of tasks to create",
        "            started_at: ISO timestamp (default: now)",
        "",
        "        Returns:",
        "            Path to created session file",
        "        \"\"\"",
        "        if started_at is None:",
        "            started_at = datetime.now().isoformat()",
        "",
        "        session = TaskSession(",
        "            session_id=session_id,",
        "            started_at=started_at,",
        "            tasks_dir=str(self.tasks_dir)",
        "        )",
        "",
        "        # Create tasks with different statuses",
        "        statuses = ['completed', 'in_progress', 'pending', 'deferred']",
        "        for i in range(num_tasks):",
        "            task = session.create_task(",
        "                title=f\"Task {i+1}\",",
        "                priority=['high', 'medium', 'low'][i % 3],",
        "                category=\"test\",",
        "                description=f\"Test task number {i+1}\"",
        "            )",
        "            task.status = statuses[i % len(statuses)]",
        "",
        "        return session.save()",
        "",
        "    def test_init(self):",
        "        \"\"\"Test SessionContextGenerator initialization.\"\"\"",
        "        self.assertEqual(self.generator.repo_path, self.repo_path)",
        "        self.assertEqual(self.generator.tasks_dir, self.tasks_dir)",
        "",
        "    def test_get_recent_sessions_empty(self):",
        "        \"\"\"Test getting recent sessions when no tasks exist.\"\"\"",
        "        sessions = self.generator.get_recent_sessions(5)",
        "        self.assertEqual(sessions, [])",
        "",
        "    def test_get_recent_sessions_missing_dir(self):",
        "        \"\"\"Test getting recent sessions when tasks directory doesn't exist.\"\"\"",
        "        # Remove tasks directory",
        "        self.tasks_dir.rmdir()",
        "",
        "        sessions = self.generator.get_recent_sessions(5)",
        "        self.assertEqual(sessions, [])",
        "",
        "    def test_get_recent_sessions_single(self):",
        "        \"\"\"Test getting a single recent session.\"\"\"",
        "        # Create one session",
        "        self._create_test_session('abc1', num_tasks=3)",
        "",
        "        sessions = self.generator.get_recent_sessions(5)",
        "",
        "        self.assertEqual(len(sessions), 1)",
        "        self.assertEqual(sessions[0]['session_id'], 'abc1')",
        "        self.assertEqual(sessions[0]['total'], 3)",
        "        # Check status counts (created with rotating statuses: completed, in_progress, pending)",
        "        self.assertEqual(sessions[0]['completed'], 1)",
        "",
        "    def test_get_recent_sessions_multiple(self):",
        "        \"\"\"Test getting multiple recent sessions.\"\"\"",
        "        # Create sessions with different timestamps",
        "        base_time = datetime.now()",
        "",
        "        for i in range(5):",
        "            timestamp = (base_time - timedelta(hours=i)).isoformat()",
        "            self._create_test_session(f'test{i}', num_tasks=2, started_at=timestamp)",
        "",
        "        sessions = self.generator.get_recent_sessions(3)",
        "",
        "        # Should get 3 most recent (sorted by file modification time, so last created is first)",
        "        self.assertEqual(len(sessions), 3)",
        "",
        "        # Most recent file (test4) should be first since it was created last",
        "        self.assertEqual(sessions[0]['session_id'], 'test4')",
        "",
        "    def test_get_recent_sessions_with_corrupted_file(self):",
        "        \"\"\"Test handling of corrupted session files.\"\"\"",
        "        # Create valid session",
        "        self._create_test_session('good', num_tasks=1)",
        "",
        "        # Create corrupted file",
        "        corrupted = self.tasks_dir / 'corrupted.json'",
        "        corrupted.write_text('{\"invalid json')",
        "",
        "        sessions = self.generator.get_recent_sessions(5)",
        "",
        "        # Should get only the valid session",
        "        self.assertEqual(len(sessions), 1)",
        "        self.assertEqual(sessions[0]['session_id'], 'good')",
        "",
        "    def test_get_pending_tasks_empty(self):",
        "        \"\"\"Test getting pending tasks when none exist.\"\"\"",
        "        pending = self.generator.get_pending_tasks()",
        "",
        "        self.assertIn('high', pending)",
        "        self.assertIn('medium', pending)",
        "        self.assertIn('low', pending)",
        "        self.assertEqual(len(pending['high']), 0)",
        "        self.assertEqual(len(pending['medium']), 0)",
        "        self.assertEqual(len(pending['low']), 0)",
        "",
        "    def test_get_pending_tasks_by_priority(self):",
        "        \"\"\"Test grouping pending tasks by priority.\"\"\"",
        "        session = TaskSession(session_id='test', tasks_dir=str(self.tasks_dir))",
        "",
        "        # Create tasks with different priorities and statuses",
        "        high_task = session.create_task(\"High priority\", priority='high')",
        "        high_task.status = 'pending'",
        "",
        "        med_task = session.create_task(\"Medium priority\", priority='medium')",
        "        med_task.status = 'in_progress'",
        "",
        "        low_task = session.create_task(\"Low priority\", priority='low')",
        "        low_task.status = 'pending'",
        "",
        "        completed = session.create_task(\"Completed\", priority='high')",
        "        completed.status = 'completed'",
        "",
        "        session.save()",
        "",
        "        pending = self.generator.get_pending_tasks()",
        "",
        "        # Should group by priority, exclude completed",
        "        self.assertEqual(len(pending['high']), 1)",
        "        self.assertEqual(len(pending['medium']), 1)",
        "        self.assertEqual(len(pending['low']), 1)",
        "",
        "        self.assertEqual(pending['high'][0].title, \"High priority\")",
        "        self.assertEqual(pending['medium'][0].title, \"Medium priority\")",
        "        self.assertEqual(pending['low'][0].title, \"Low priority\")",
        "",
        "    def test_get_pending_tasks_invalid_priority(self):",
        "        \"\"\"Test handling tasks with invalid priority values.\"\"\"",
        "        session = TaskSession(session_id='test', tasks_dir=str(self.tasks_dir))",
        "",
        "        task = session.create_task(\"Test task\", priority='invalid')",
        "        task.status = 'pending'",
        "        session.save()",
        "",
        "        pending = self.generator.get_pending_tasks()",
        "",
        "        # Should default to medium priority",
        "        self.assertEqual(len(pending['medium']), 1)",
        "",
        "    @patch('subprocess.run')",
        "    def test_get_recent_commits(self, mock_run):",
        "        \"\"\"Test getting recent commits from git.\"\"\"",
        "        # Mock git log output",
        "        mock_result = MagicMock()",
        "        mock_result.stdout = (",
        "            \"abc1234|2 hours ago|Fix validation bug\\n\"",
        "            \"def5678|1 day ago|Add new feature\\n\"",
        "        )",
        "        mock_result.returncode = 0",
        "",
        "        # Mock git diff-tree output for files",
        "        mock_files_result = MagicMock()",
        "        mock_files_result.stdout = \"file1.py\\nfile2.py\\n\"",
        "        mock_files_result.returncode = 0",
        "",
        "        mock_run.side_effect = [mock_result, mock_files_result, mock_result, mock_files_result]",
        "",
        "        commits = self.generator.get_recent_commits(2)",
        "",
        "        self.assertEqual(len(commits), 2)",
        "        self.assertEqual(commits[0]['hash'], 'abc1234')",
        "        self.assertEqual(commits[0]['time_ago'], '2 hours ago')",
        "        self.assertEqual(commits[0]['subject'], 'Fix validation bug')",
        "        self.assertEqual(commits[0]['files_changed'], 2)",
        "",
        "    @patch('subprocess.run')",
        "    def test_get_recent_commits_no_git(self, mock_run):",
        "        \"\"\"Test handling when git is not available.\"\"\"",
        "        mock_run.side_effect = FileNotFoundError()",
        "",
        "        commits = self.generator.get_recent_commits(10)",
        "",
        "        self.assertEqual(commits, [])",
        "",
        "    @patch('subprocess.run')",
        "    def test_get_recent_commits_git_error(self, mock_run):",
        "        \"\"\"Test handling git command errors.\"\"\"",
        "        mock_run.side_effect = subprocess.CalledProcessError(1, 'git')",
        "",
        "        commits = self.generator.get_recent_commits(10)",
        "",
        "        self.assertEqual(commits, [])",
        "",
        "    @patch('subprocess.run')",
        "    def test_get_recent_file_changes(self, mock_run):",
        "        \"\"\"Test getting recent file changes grouped by directory.\"\"\"",
        "        mock_result = MagicMock()",
        "        mock_result.stdout = (",
        "            \"cortical/processor.py\\n\"",
        "            \"cortical/analysis.py\\n\"",
        "            \"scripts/session_context.py\\n\"",
        "            \"tests/unit/test_session.py\\n\"",
        "            \"README.md\\n\"",
        "        )",
        "        mock_result.returncode = 0",
        "        mock_run.return_value = mock_result",
        "",
        "        changes = self.generator.get_recent_file_changes(7)",
        "",
        "        # Should group by top-level directory",
        "        self.assertIn('cortical', changes)",
        "        self.assertIn('scripts', changes)",
        "        self.assertIn('tests', changes)",
        "        self.assertIn('root', changes)",
        "",
        "        self.assertEqual(len(changes['cortical']), 2)",
        "        self.assertEqual(len(changes['scripts']), 1)",
        "        self.assertEqual(len(changes['root']), 1)",
        "",
        "    @patch('subprocess.run')",
        "    def test_get_recent_file_changes_no_git(self, mock_run):",
        "        \"\"\"Test handling when git is not available.\"\"\"",
        "        mock_run.side_effect = FileNotFoundError()",
        "",
        "        changes = self.generator.get_recent_file_changes(7)",
        "",
        "        self.assertEqual(changes, {})",
        "",
        "    def test_generate_context_empty_repo(self):",
        "        \"\"\"Test generating context for empty repository.\"\"\"",
        "        with patch.object(self.generator, 'get_recent_commits', return_value=[]):",
        "            with patch.object(self.generator, 'get_recent_file_changes', return_value={}):",
        "                context = self.generator.generate_context()",
        "",
        "                self.assertIn('Session Context', context)",
        "                self.assertIn('No task sessions found', context)",
        "                self.assertIn('No pending tasks found', context)",
        "",
        "    def test_generate_context_with_data(self):",
        "        \"\"\"Test generating context with sessions and tasks.\"\"\"",
        "        # Create test sessions",
        "        session = TaskSession(session_id='test1', tasks_dir=str(self.tasks_dir))",
        "",
        "        task1 = session.create_task(\"Pending high priority\", priority='high')",
        "        task1.status = 'pending'",
        "",
        "        task2 = session.create_task(\"In progress medium\", priority='medium')",
        "        task2.status = 'in_progress'",
        "",
        "        session.save()",
        "",
        "        # Mock git functions",
        "        mock_commits = [{",
        "            'hash': 'abc123',",
        "            'time_ago': '2 hours ago',",
        "            'subject': 'Test commit',",
        "            'files_changed': 3,",
        "            'files': ['file1.py', 'file2.py', 'file3.py']",
        "        }]",
        "",
        "        mock_changes = {",
        "            'cortical': ['cortical/file1.py', 'cortical/file2.py'],",
        "            'scripts': ['scripts/test.py']",
        "        }",
        "",
        "        with patch.object(self.generator, 'get_recent_commits', return_value=mock_commits):",
        "            with patch.object(self.generator, 'get_recent_file_changes', return_value=mock_changes):",
        "                context = self.generator.generate_context()",
        "",
        "                # Should contain all sections",
        "                self.assertIn('Session Context', context)",
        "                self.assertIn('Recent Work', context)",
        "                self.assertIn('Pending Tasks (2 total)', context)",
        "                self.assertIn('High Priority', context)",
        "                self.assertIn('Medium Priority', context)",
        "                self.assertIn('Recent Changes', context)",
        "                self.assertIn('Recent Commits', context)",
        "                self.assertIn('Quick Stats', context)",
        "",
        "                # Check task details",
        "                self.assertIn('Pending high priority', context)",
        "                self.assertIn('In progress medium', context)",
        "",
        "                # Check commit details",
        "                self.assertIn('abc123', context)",
        "                self.assertIn('Test commit', context)",
        "",
        "                # Check file changes",
        "                self.assertIn('cortical', context)",
        "                self.assertIn('scripts', context)",
        "",
        "    def test_generate_json_structure(self):",
        "        \"\"\"Test JSON output structure.\"\"\"",
        "        # Create minimal test data",
        "        self._create_test_session('json_test', num_tasks=1)",
        "",
        "        with patch.object(self.generator, 'get_recent_commits', return_value=[]):",
        "            with patch.object(self.generator, 'get_recent_file_changes', return_value={}):",
        "                json_output = self.generator.generate_json()",
        "",
        "                # Check structure",
        "                self.assertIn('generated_at', json_output)",
        "                self.assertIn('recent_sessions', json_output)",
        "                self.assertIn('pending_tasks', json_output)",
        "                self.assertIn('recent_commits', json_output)",
        "                self.assertIn('recent_file_changes', json_output)",
        "",
        "                # Check types",
        "                self.assertIsInstance(json_output['recent_sessions'], list)",
        "                self.assertIsInstance(json_output['pending_tasks'], dict)",
        "                self.assertIsInstance(json_output['recent_commits'], list)",
        "                self.assertIsInstance(json_output['recent_file_changes'], dict)",
        "",
        "    def test_generate_json_serializable(self):",
        "        \"\"\"Test that JSON output is fully serializable.\"\"\"",
        "        self._create_test_session('serial_test', num_tasks=2)",
        "",
        "        with patch.object(self.generator, 'get_recent_commits', return_value=[]):",
        "            with patch.object(self.generator, 'get_recent_file_changes', return_value={}):",
        "                json_output = self.generator.generate_json()",
        "",
        "                # Should not raise",
        "                serialized = json.dumps(json_output, indent=2)",
        "                self.assertIsInstance(serialized, str)",
        "",
        "                # Should be deserializable",
        "                deserialized = json.loads(serialized)",
        "                self.assertEqual(deserialized['generated_at'], json_output['generated_at'])",
        "",
        "    def test_pending_tasks_sorted_by_creation(self):",
        "        \"\"\"Test that pending tasks are sorted by creation time within priority.\"\"\"",
        "        session = TaskSession(session_id='sort_test', tasks_dir=str(self.tasks_dir))",
        "",
        "        # Create tasks in non-chronological order",
        "        base_time = datetime.now()",
        "",
        "        task1 = session.create_task(\"Second created\", priority='high')",
        "        task1.created_at = (base_time - timedelta(hours=1)).isoformat()",
        "        task1.status = 'pending'",
        "",
        "        task2 = session.create_task(\"First created\", priority='high')",
        "        task2.created_at = (base_time - timedelta(hours=2)).isoformat()",
        "        task2.status = 'pending'",
        "",
        "        task3 = session.create_task(\"Third created\", priority='high')",
        "        task3.created_at = base_time.isoformat()",
        "        task3.status = 'pending'",
        "",
        "        session.save()",
        "",
        "        pending = self.generator.get_pending_tasks()",
        "",
        "        # Should be sorted by creation time (oldest first)",
        "        self.assertEqual(len(pending['high']), 3)",
        "        self.assertEqual(pending['high'][0].title, \"First created\")",
        "        self.assertEqual(pending['high'][1].title, \"Second created\")",
        "        self.assertEqual(pending['high'][2].title, \"Third created\")",
        "",
        "    def test_context_limits_tasks_per_priority(self):",
        "        \"\"\"Test that context generation limits tasks shown per priority.\"\"\"",
        "        session = TaskSession(session_id='limit_test', tasks_dir=str(self.tasks_dir))",
        "",
        "        # Create 15 high priority tasks",
        "        for i in range(15):",
        "            task = session.create_task(f\"Task {i}\", priority='high')",
        "            task.status = 'pending'",
        "",
        "        session.save()",
        "",
        "        context = self.generator.generate_context()",
        "",
        "        # Should show max 10 per priority",
        "        task_lines = [line for line in context.split('\\n') if line.startswith('- 📋')]",
        "        self.assertLessEqual(len(task_lines), 10)",
        "",
        "    def test_context_truncates_long_descriptions(self):",
        "        \"\"\"Test that long task descriptions are truncated.\"\"\"",
        "        session = TaskSession(session_id='desc_test', tasks_dir=str(self.tasks_dir))",
        "",
        "        long_desc = \"A\" * 200  # 200 character description",
        "        task = session.create_task(\"Test task\", priority='high', description=long_desc)",
        "        task.status = 'pending'",
        "",
        "        session.save()",
        "",
        "        context = self.generator.generate_context()",
        "",
        "        # Description should be truncated to ~100 chars",
        "        self.assertIn('AAA...', context)  # Should have ellipsis",
        "        self.assertNotIn('A' * 150, context)  # Shouldn't have full length",
        "",
        "    def test_get_recent_commits_empty_output(self):",
        "        \"\"\"Test handling empty git log output.\"\"\"",
        "        with patch('subprocess.run') as mock_run:",
        "            mock_result = MagicMock()",
        "            mock_result.stdout = \"\"",
        "            mock_result.returncode = 0",
        "            mock_run.return_value = mock_result",
        "",
        "            commits = self.generator.get_recent_commits(10)",
        "",
        "            self.assertEqual(commits, [])",
        "",
        "",
        "class TestCLI(unittest.TestCase):",
        "    \"\"\"Test CLI argument parsing and output.\"\"\"",
        "",
        "    def setUp(self):",
        "        \"\"\"Set up test environment.\"\"\"",
        "        self.temp_dir = tempfile.TemporaryDirectory()",
        "        self.repo_path = Path(self.temp_dir.name)",
        "",
        "    def tearDown(self):",
        "        \"\"\"Clean up.\"\"\"",
        "        self.temp_dir.cleanup()",
        "",
        "    @patch('sys.argv', ['session_context.py', '--repo-path', '/tmp/test'])",
        "    @patch('session_context.SessionContextGenerator.generate_context')",
        "    def test_cli_default_markdown(self, mock_generate):",
        "        \"\"\"Test CLI with default markdown output.\"\"\"",
        "        mock_generate.return_value = \"# Test Context\"",
        "",
        "        # Import here to trigger CLI with mocked argv",
        "        import session_context",
        "        # Would need to mock print to fully test, but this validates imports work",
        "",
        "",
        "if __name__ == '__main__':",
        "    unittest.main()"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "tests/unit/test_suggest_tasks.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "#!/usr/bin/env python3",
        "\"\"\"",
        "Unit tests for scripts/suggest_tasks.py",
        "",
        "Tests the TaskSuggester class and its methods for analyzing",
        "code changes and suggesting follow-up tasks.",
        "\"\"\"",
        "",
        "import json",
        "import subprocess",
        "import sys",
        "import tempfile",
        "import unittest",
        "from pathlib import Path",
        "from unittest.mock import Mock, patch, MagicMock",
        "",
        "# Add scripts to path",
        "scripts_path = Path(__file__).parent.parent.parent / 'scripts'",
        "sys.path.insert(0, str(scripts_path))",
        "",
        "from suggest_tasks import TaskSuggester",
        "",
        "",
        "class TestTaskSuggester(unittest.TestCase):",
        "    \"\"\"Test TaskSuggester class.\"\"\"",
        "",
        "    def setUp(self):",
        "        \"\"\"Create a temporary directory for tests.\"\"\"",
        "        self.temp_dir = tempfile.mkdtemp()",
        "        self.temp_path = Path(self.temp_dir)",
        "",
        "    def tearDown(self):",
        "        \"\"\"Clean up temporary directory.\"\"\"",
        "        import shutil",
        "        shutil.rmtree(self.temp_dir, ignore_errors=True)",
        "",
        "    @patch('subprocess.run')",
        "    def test_check_git_repo_valid(self, mock_run):",
        "        \"\"\"Test _check_git_repo with valid git repository.\"\"\"",
        "        mock_run.return_value = Mock(returncode=0)",
        "        suggester = TaskSuggester(self.temp_dir)",
        "        self.assertTrue(suggester._is_git_repo)",
        "",
        "    @patch('subprocess.run')",
        "    def test_check_git_repo_invalid(self, mock_run):",
        "        \"\"\"Test _check_git_repo with non-git directory.\"\"\"",
        "        mock_run.return_value = Mock(returncode=1)",
        "        suggester = TaskSuggester(self.temp_dir)",
        "        self.assertFalse(suggester._is_git_repo)",
        "",
        "    @patch('subprocess.run')",
        "    def test_get_staged_changes(self, mock_run):",
        "        \"\"\"Test get_staged_changes parsing.\"\"\"",
        "        mock_run.side_effect = [",
        "            Mock(returncode=0),  # git rev-parse check",
        "            Mock(",
        "                returncode=0,",
        "                stdout='A\\tscripts/new_file.py\\nM\\tcortical/processor.py\\nD\\told_file.py\\n'",
        "            )",
        "        ]",
        "",
        "        suggester = TaskSuggester(self.temp_dir)",
        "        changes = suggester.get_staged_changes()",
        "",
        "        self.assertEqual(changes['added'], ['scripts/new_file.py'])",
        "        self.assertEqual(changes['modified'], ['cortical/processor.py'])",
        "        self.assertEqual(changes['deleted'], ['old_file.py'])",
        "",
        "    @patch('subprocess.run')",
        "    def test_get_unstaged_changes(self, mock_run):",
        "        \"\"\"Test get_unstaged_changes parsing.\"\"\"",
        "        mock_run.side_effect = [",
        "            Mock(returncode=0),  # git rev-parse check",
        "            Mock(",
        "                returncode=0,",
        "                stdout='M\\ttests/test_processor.py\\n'",
        "            )",
        "        ]",
        "",
        "        suggester = TaskSuggester(self.temp_dir)",
        "        changes = suggester.get_unstaged_changes()",
        "",
        "        self.assertEqual(changes['modified'], ['tests/test_processor.py'])",
        "        self.assertEqual(changes['added'], [])",
        "        self.assertEqual(changes['deleted'], [])",
        "",
        "    @patch('subprocess.run')",
        "    def test_get_staged_changes_not_git_repo(self, mock_run):",
        "        \"\"\"Test get_staged_changes raises error if not git repo.\"\"\"",
        "        mock_run.return_value = Mock(returncode=1)",
        "        suggester = TaskSuggester(self.temp_dir)",
        "",
        "        with self.assertRaises(RuntimeError):",
        "            suggester.get_staged_changes()",
        "",
        "    def test_parse_git_status(self):",
        "        \"\"\"Test _parse_git_status parsing logic.\"\"\"",
        "        suggester = TaskSuggester(self.temp_dir)",
        "",
        "        output = \"\"\"A\\tnew_file.py",
        "M\\tmodified_file.py",
        "D\\tdeleted_file.py",
        "A\\tanother_new.py\"\"\"",
        "",
        "        changes = suggester._parse_git_status(output)",
        "",
        "        self.assertEqual(len(changes['added']), 2)",
        "        self.assertEqual(len(changes['modified']), 1)",
        "        self.assertEqual(len(changes['deleted']), 1)",
        "        self.assertIn('new_file.py', changes['added'])",
        "        self.assertIn('modified_file.py', changes['modified'])",
        "",
        "    def test_parse_git_status_empty(self):",
        "        \"\"\"Test _parse_git_status with empty output.\"\"\"",
        "        suggester = TaskSuggester(self.temp_dir)",
        "        changes = suggester._parse_git_status('')",
        "",
        "        self.assertEqual(changes['added'], [])",
        "        self.assertEqual(changes['modified'], [])",
        "        self.assertEqual(changes['deleted'], [])",
        "",
        "    def test_has_undocumented_methods_true(self):",
        "        \"\"\"Test _has_undocumented_methods detects missing docstrings.\"\"\"",
        "        suggester = TaskSuggester(self.temp_dir)",
        "",
        "        content = \"\"\"",
        "def function_one():",
        "    pass",
        "",
        "def function_two():",
        "    pass",
        "",
        "def function_three():",
        "    pass",
        "\"\"\"",
        "        self.assertTrue(suggester._has_undocumented_methods(content))",
        "",
        "    def test_has_undocumented_methods_false(self):",
        "        \"\"\"Test _has_undocumented_methods with good documentation.\"\"\"",
        "        suggester = TaskSuggester(self.temp_dir)",
        "",
        "        content = \"\"\"",
        "def function_one():",
        "    \\\"\\\"\\\"Docstring here.\\\"\\\"\\\"",
        "    pass",
        "",
        "def function_two():",
        "    \\\"\\\"\\\"Another docstring.\\\"\\\"\\\"",
        "    pass",
        "\"\"\"",
        "        self.assertFalse(suggester._has_undocumented_methods(content))",
        "",
        "    def test_is_new_test_file_true(self):",
        "        \"\"\"Test _is_new_test_file detects test files.\"\"\"",
        "        suggester = TaskSuggester(self.temp_dir)",
        "",
        "        content = \"\"\"",
        "def test_something():",
        "    assert True",
        "\"\"\"",
        "        self.assertTrue(suggester._is_new_test_file('test_feature.py', content))",
        "",
        "    def test_is_new_test_file_false(self):",
        "        \"\"\"Test _is_new_test_file with non-test file.\"\"\"",
        "        suggester = TaskSuggester(self.temp_dir)",
        "",
        "        content = \"\"\"",
        "def process_data():",
        "    pass",
        "\"\"\"",
        "        self.assertFalse(suggester._is_new_test_file('processor.py', content))",
        "",
        "    def test_has_performance_code_true(self):",
        "        \"\"\"Test _has_performance_code detects loops.\"\"\"",
        "        suggester = TaskSuggester(self.temp_dir)",
        "",
        "        content = \"\"\"",
        "# This is O(n^2)",
        "for item in items:",
        "    for other in others:",
        "        process(item, other)",
        "\"\"\"",
        "        self.assertTrue(suggester._has_performance_code('cortical/analysis.py', content))",
        "",
        "    def test_has_performance_code_false(self):",
        "        \"\"\"Test _has_performance_code with simple code.\"\"\"",
        "        suggester = TaskSuggester(self.temp_dir)",
        "",
        "        content = \"\"\"",
        "def simple_function():",
        "    return True",
        "\"\"\"",
        "        self.assertFalse(suggester._has_performance_code('cortical/analysis.py', content))",
        "",
        "    def test_has_performance_code_non_cortical_false(self):",
        "        \"\"\"Test _has_performance_code ignores non-cortical files.\"\"\"",
        "        suggester = TaskSuggester(self.temp_dir)",
        "",
        "        content = \"\"\"",
        "for item in items:",
        "    process(item)",
        "\"\"\"",
        "        # Should be False because not in cortical/",
        "        self.assertFalse(suggester._has_performance_code('scripts/tool.py', content))",
        "",
        "    def test_has_validation_logic_true(self):",
        "        \"\"\"Test _has_validation_logic detects validation code.\"\"\"",
        "        suggester = TaskSuggester(self.temp_dir)",
        "",
        "        content = \"\"\"",
        "def validate(value):",
        "    if value < 0:",
        "        raise ValueError(\"Value must be positive\")",
        "    return value",
        "\"\"\"",
        "        self.assertTrue(suggester._has_validation_logic(content))",
        "",
        "    def test_has_validation_logic_false(self):",
        "        \"\"\"Test _has_validation_logic with no validation.\"\"\"",
        "        suggester = TaskSuggester(self.temp_dir)",
        "",
        "        content = \"\"\"",
        "def process(value):",
        "    return value * 2",
        "\"\"\"",
        "        self.assertFalse(suggester._has_validation_logic(content))",
        "",
        "    def test_missing_type_hints_true(self):",
        "        \"\"\"Test _missing_type_hints detects untyped functions.\"\"\"",
        "        suggester = TaskSuggester(self.temp_dir)",
        "",
        "        content = \"\"\"",
        "def function_one(x):",
        "    return x",
        "",
        "def function_two(y):",
        "    return y",
        "\"\"\"",
        "        self.assertTrue(suggester._missing_type_hints(content))",
        "",
        "    def test_missing_type_hints_false(self):",
        "        \"\"\"Test _missing_type_hints with typed functions.\"\"\"",
        "        suggester = TaskSuggester(self.temp_dir)",
        "",
        "        content = \"\"\"",
        "def function_one(x: int) -> int:",
        "    return x",
        "",
        "def function_two(y: str) -> str:",
        "    return y",
        "\"\"\"",
        "        self.assertFalse(suggester._missing_type_hints(content))",
        "",
        "    def test_is_config_change_true(self):",
        "        \"\"\"Test _is_config_change detects config files.\"\"\"",
        "        suggester = TaskSuggester(self.temp_dir)",
        "",
        "        self.assertTrue(suggester._is_config_change('cortical/config.py'))",
        "        self.assertTrue(suggester._is_config_change('.github/workflows/ci.yml'))",
        "        self.assertTrue(suggester._is_config_change('pyproject.toml'))",
        "",
        "    def test_is_config_change_false(self):",
        "        \"\"\"Test _is_config_change with regular files.\"\"\"",
        "        suggester = TaskSuggester(self.temp_dir)",
        "",
        "        self.assertFalse(suggester._is_config_change('cortical/processor.py'))",
        "        self.assertFalse(suggester._is_config_change('tests/test_processor.py'))",
        "",
        "    def test_has_new_public_api_true(self):",
        "        \"\"\"Test _has_new_public_api detects public methods.\"\"\"",
        "        suggester = TaskSuggester(self.temp_dir)",
        "",
        "        content = \"\"\"",
        "class Processor:",
        "    def process_data(self, data):",
        "        return data",
        "",
        "    def compute_result(self):",
        "        return 42",
        "\"\"\"",
        "        self.assertTrue(suggester._has_new_public_api('cortical/processor.py', content))",
        "",
        "    def test_has_new_public_api_false_private(self):",
        "        \"\"\"Test _has_new_public_api ignores private methods.\"\"\"",
        "        suggester = TaskSuggester(self.temp_dir)",
        "",
        "        content = \"\"\"",
        "class Processor:",
        "    def _internal_method(self):",
        "        pass",
        "",
        "    def __private_method(self):",
        "        pass",
        "\"\"\"",
        "        self.assertFalse(suggester._has_new_public_api('cortical/processor.py', content))",
        "",
        "    def test_has_new_public_api_false_non_cortical(self):",
        "        \"\"\"Test _has_new_public_api ignores non-cortical files.\"\"\"",
        "        suggester = TaskSuggester(self.temp_dir)",
        "",
        "        content = \"\"\"",
        "class Tool:",
        "    def run(self):",
        "        pass",
        "\"\"\"",
        "        self.assertFalse(suggester._has_new_public_api('scripts/tool.py', content))",
        "",
        "    @patch('subprocess.run')",
        "    def test_analyze_file_undocumented(self, mock_run):",
        "        \"\"\"Test analyze_file detects undocumented methods.\"\"\"",
        "        mock_run.return_value = Mock(returncode=0)",
        "",
        "        # Create test file",
        "        test_file = self.temp_path / 'test.py'",
        "        test_file.write_text(\"\"\"",
        "def function_one():",
        "    pass",
        "",
        "def function_two():",
        "    pass",
        "\"\"\")",
        "",
        "        suggester = TaskSuggester(self.temp_dir)",
        "        suggestions = suggester.analyze_file('test.py')",
        "",
        "        # Should suggest adding docstrings",
        "        self.assertTrue(any('docstring' in s['title'].lower() for s in suggestions))",
        "",
        "    @patch('subprocess.run')",
        "    def test_analyze_file_new_test(self, mock_run):",
        "        \"\"\"Test analyze_file detects new test files.\"\"\"",
        "        mock_run.return_value = Mock(returncode=0)",
        "",
        "        # Create test file",
        "        test_file = self.temp_path / 'test_feature.py'",
        "        test_file.write_text(\"\"\"",
        "def test_something():",
        "    assert True",
        "\"\"\")",
        "",
        "        suggester = TaskSuggester(self.temp_dir)",
        "        suggestions = suggester.analyze_file('test_feature.py')",
        "",
        "        # Should suggest running tests",
        "        self.assertTrue(any('run tests' in s['title'].lower() for s in suggestions))",
        "",
        "    @patch('subprocess.run')",
        "    def test_analyze_file_validation(self, mock_run):",
        "        \"\"\"Test analyze_file detects validation logic.\"\"\"",
        "        mock_run.return_value = Mock(returncode=0)",
        "",
        "        # Create file with validation",
        "        test_file = self.temp_path / 'validator.py'",
        "        test_file.write_text(\"\"\"",
        "def validate(value):",
        "    if value < 0:",
        "        raise ValueError(\"Invalid\")",
        "\"\"\")",
        "",
        "        suggester = TaskSuggester(self.temp_dir)",
        "        suggestions = suggester.analyze_file('validator.py')",
        "",
        "        # Should suggest verifying validation tests",
        "        self.assertTrue(any('validation' in s['title'].lower() for s in suggestions))",
        "",
        "    @patch('subprocess.run')",
        "    def test_analyze_file_nonexistent(self, mock_run):",
        "        \"\"\"Test analyze_file handles non-existent files.\"\"\"",
        "        mock_run.return_value = Mock(returncode=0)",
        "",
        "        suggester = TaskSuggester(self.temp_dir)",
        "        suggestions = suggester.analyze_file('nonexistent.py')",
        "",
        "        # Should return empty list",
        "        self.assertEqual(suggestions, [])",
        "",
        "    @patch('subprocess.run')",
        "    def test_analyze_file_non_python(self, mock_run):",
        "        \"\"\"Test analyze_file skips non-Python files.\"\"\"",
        "        mock_run.return_value = Mock(returncode=0)",
        "",
        "        # Create non-Python file",
        "        test_file = self.temp_path / 'README.md'",
        "        test_file.write_text('# Documentation')",
        "",
        "        suggester = TaskSuggester(self.temp_dir)",
        "        suggestions = suggester.analyze_file('README.md')",
        "",
        "        # Should return empty list",
        "        self.assertEqual(suggestions, [])",
        "",
        "    @patch('subprocess.run')",
        "    def test_suggest_from_changes_deduplication(self, mock_run):",
        "        \"\"\"Test suggest_from_changes deduplicates suggestions.\"\"\"",
        "        # Mock git repo check and changes",
        "        mock_run.side_effect = [",
        "            Mock(returncode=0),  # git rev-parse check",
        "            Mock(returncode=0, stdout='M\\ttest1.py\\nM\\ttest2.py\\n')  # staged changes",
        "        ]",
        "",
        "        # Create test files with similar issues",
        "        test1 = self.temp_path / 'test1.py'",
        "        test1.write_text('def f1(): pass\\ndef f2(): pass')",
        "",
        "        test2 = self.temp_path / 'test2.py'",
        "        test2.write_text('def f3(): pass\\ndef f4(): pass')",
        "",
        "        suggester = TaskSuggester(self.temp_dir)",
        "        suggestions = suggester.suggest_from_changes()",
        "",
        "        # Check that duplicate titles are removed",
        "        titles = [s['title'] for s in suggestions]",
        "        self.assertEqual(len(titles), len(set(titles)))",
        "",
        "    @patch('subprocess.run')",
        "    def test_suggest_from_changes_priority_ordering(self, mock_run):",
        "        \"\"\"Test suggest_from_changes sorts by priority.\"\"\"",
        "        # Mock git repo check and changes",
        "        mock_run.side_effect = [",
        "            Mock(returncode=0),  # git rev-parse check",
        "            Mock(returncode=0, stdout='M\\ttest_high.py\\n')",
        "        ]",
        "",
        "        # Create file that triggers high priority suggestion",
        "        high_file = self.temp_path / 'test_high.py'",
        "        high_file.write_text('def test_something(): pass')  # High priority: new test",
        "",
        "        suggester = TaskSuggester(self.temp_dir)",
        "        suggestions = suggester.suggest_from_changes()",
        "",
        "        # Should have at least one suggestion",
        "        self.assertGreater(len(suggestions), 0)",
        "",
        "        # Find the test-related suggestion",
        "        test_suggestions = [s for s in suggestions if 'test' in s['title'].lower()]",
        "        if test_suggestions:",
        "            # Test suggestions should be high priority",
        "            self.assertEqual(test_suggestions[0]['priority'], 'high')",
        "",
        "    @patch('subprocess.run')",
        "    def test_suggest_from_changes_limit(self, mock_run):",
        "        \"\"\"Test suggest_from_changes limits to 10 suggestions.\"\"\"",
        "        # Mock git repo check and many changes",
        "        files = '\\n'.join([f'M\\tfile{i}.py' for i in range(20)])",
        "        mock_run.side_effect = [",
        "            Mock(returncode=0),  # git rev-parse check",
        "            Mock(returncode=0, stdout=files)",
        "        ]",
        "",
        "        # Create many files",
        "        for i in range(20):",
        "            f = self.temp_path / f'file{i}.py'",
        "            f.write_text(f'def f{i}(): pass')",
        "",
        "        suggester = TaskSuggester(self.temp_dir)",
        "        suggestions = suggester.suggest_from_changes()",
        "",
        "        # Should be limited to 10",
        "        self.assertLessEqual(len(suggestions), 10)",
        "",
        "    def test_format_suggestions_empty(self):",
        "        \"\"\"Test format_suggestions with no suggestions.\"\"\"",
        "        suggester = TaskSuggester(self.temp_dir)",
        "        output = suggester.format_suggestions([])",
        "",
        "        self.assertIn('No suggestions', output)",
        "",
        "    def test_format_suggestions_markdown(self):",
        "        \"\"\"Test format_suggestions produces markdown.\"\"\"",
        "        suggester = TaskSuggester(self.temp_dir)",
        "",
        "        suggestions = [",
        "            {",
        "                'title': 'Add tests',",
        "                'priority': 'high',",
        "                'category': 'test',",
        "                'reason': 'New code added',",
        "                'files': ['test.py']",
        "            }",
        "        ]",
        "",
        "        output = suggester.format_suggestions(suggestions)",
        "",
        "        self.assertIn('# Suggested Follow-up Tasks', output)",
        "        self.assertIn('Add tests', output)",
        "        self.assertIn('high', output.lower())",
        "        self.assertIn('test', output.lower())",
        "",
        "    def test_format_json(self):",
        "        \"\"\"Test format_json produces valid JSON.\"\"\"",
        "        suggester = TaskSuggester(self.temp_dir)",
        "",
        "        suggestions = [",
        "            {",
        "                'title': 'Add tests',",
        "                'priority': 'high',",
        "                'category': 'test',",
        "                'reason': 'New code added',",
        "                'files': ['test.py']",
        "            }",
        "        ]",
        "",
        "        output = suggester.format_json(suggestions)",
        "",
        "        # Should be valid JSON",
        "        parsed = json.loads(output)",
        "        self.assertEqual(len(parsed), 1)",
        "        self.assertEqual(parsed[0]['title'], 'Add tests')",
        "",
        "    @patch('suggest_tasks.TaskSession')",
        "    def test_create_tasks(self, mock_session_class):",
        "        \"\"\"Test create_tasks creates tasks using TaskSession.\"\"\"",
        "        # Check if task_utils is actually available",
        "        import suggest_tasks",
        "        if not suggest_tasks.TASK_UTILS_AVAILABLE:",
        "            self.skipTest(\"task_utils not available\")",
        "",
        "        mock_session = Mock()",
        "        mock_session_class.return_value = mock_session",
        "",
        "        suggester = TaskSuggester(self.temp_dir)",
        "",
        "        suggestions = [",
        "            {",
        "                'title': 'Add tests',",
        "                'priority': 'high',",
        "                'category': 'test',",
        "                'reason': 'New code added',",
        "                'files': ['test.py']",
        "            },",
        "            {",
        "                'title': 'Add docs',",
        "                'priority': 'low',",
        "                'category': 'docs',",
        "                'reason': 'Missing docstrings',",
        "                'files': ['module.py']",
        "            }",
        "        ]",
        "",
        "        count = suggester.create_tasks(suggestions)",
        "",
        "        # Should create 2 tasks",
        "        self.assertEqual(count, 2)",
        "        self.assertEqual(mock_session.create_task.call_count, 2)",
        "        mock_session.save.assert_called_once()",
        "",
        "    def test_create_tasks_no_task_utils(self):",
        "        \"\"\"Test create_tasks raises error if task_utils not available.\"\"\"",
        "        import suggest_tasks",
        "        original_value = suggest_tasks.TASK_UTILS_AVAILABLE",
        "        try:",
        "            # Temporarily disable task_utils",
        "            suggest_tasks.TASK_UTILS_AVAILABLE = False",
        "",
        "            suggester = TaskSuggester(self.temp_dir)",
        "",
        "            with self.assertRaises(ImportError):",
        "                suggester.create_tasks([])",
        "        finally:",
        "            # Restore original value",
        "            suggest_tasks.TASK_UTILS_AVAILABLE = original_value",
        "",
        "",
        "class TestTaskSuggesterIntegration(unittest.TestCase):",
        "    \"\"\"Integration tests using real git repository.\"\"\"",
        "",
        "    def setUp(self):",
        "        \"\"\"Create temporary git repository.\"\"\"",
        "        import shutil",
        "        self.temp_dir = tempfile.mkdtemp()",
        "        self.temp_path = Path(self.temp_dir)",
        "",
        "        # Initialize git repo",
        "        subprocess.run(['git', 'init'], cwd=self.temp_dir, check=True,",
        "                      capture_output=True)",
        "        subprocess.run(['git', 'config', 'user.email', 'test@test.com'],",
        "                      cwd=self.temp_dir, check=True, capture_output=True)",
        "        subprocess.run(['git', 'config', 'user.name', 'Test User'],",
        "                      cwd=self.temp_dir, check=True, capture_output=True)",
        "",
        "    def tearDown(self):",
        "        \"\"\"Clean up temporary directory.\"\"\"",
        "        import shutil",
        "        shutil.rmtree(self.temp_dir, ignore_errors=True)",
        "",
        "    def test_real_git_workflow(self):",
        "        \"\"\"Test with real git staging workflow.\"\"\"",
        "        # Create and stage a file",
        "        test_file = self.temp_path / 'new_feature.py'",
        "        test_file.write_text(\"\"\"",
        "def new_function():",
        "    pass",
        "",
        "def another_function():",
        "    pass",
        "\"\"\")",
        "",
        "        subprocess.run(['git', 'add', 'new_feature.py'],",
        "                      cwd=self.temp_dir, check=True, capture_output=True)",
        "",
        "        # Analyze",
        "        suggester = TaskSuggester(self.temp_dir)",
        "        suggestions = suggester.suggest_from_changes()",
        "",
        "        # Should have suggestions",
        "        self.assertGreater(len(suggestions), 0)",
        "",
        "",
        "if __name__ == '__main__':",
        "    unittest.main()"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "tests/unit/test_task_retrospective.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "#!/usr/bin/env python3",
        "\"\"\"",
        "Tests for task retrospective metadata capture.",
        "",
        "This module tests the retrospective tracking features added to the task",
        "management system, which capture metadata about completed tasks such as",
        "files touched, duration, and tests added.",
        "\"\"\"",
        "",
        "import json",
        "import tempfile",
        "import time",
        "import unittest",
        "from datetime import datetime, timedelta",
        "from pathlib import Path",
        "",
        "from scripts.task_utils import Task, TaskSession",
        "",
        "",
        "class TestTaskRetrospective(unittest.TestCase):",
        "    \"\"\"Tests for retrospective metadata capture on tasks.\"\"\"",
        "",
        "    def setUp(self):",
        "        \"\"\"Set up test session.\"\"\"",
        "        self.session = TaskSession()",
        "",
        "    def test_task_has_retrospective_field(self):",
        "        \"\"\"Task dataclass includes retrospective field.\"\"\"",
        "        task = Task(",
        "            id=\"T-test-001\",",
        "            title=\"Test task\"",
        "        )",
        "        self.assertIsNone(task.retrospective)",
        "        self.assertTrue(hasattr(task, 'retrospective'))",
        "",
        "    def test_capture_retrospective_basic(self):",
        "        \"\"\"capture_retrospective stores data correctly.\"\"\"",
        "        task = self.session.create_task(\"Implement feature X\")",
        "        task_id = task.id",
        "",
        "        # Capture retrospective data",
        "        self.session.capture_retrospective(",
        "            task_id=task_id,",
        "            files_touched=['cortical/processor.py', 'tests/test_processor.py'],",
        "            tests_added=3,",
        "            commits=['abc1234', 'def5678'],",
        "            notes='Added new search algorithm'",
        "        )",
        "",
        "        # Verify data was stored",
        "        task = self.session.get_task(task_id)",
        "        self.assertIsNotNone(task.retrospective)",
        "        self.assertEqual(",
        "            task.retrospective['files_touched'],",
        "            ['cortical/processor.py', 'tests/test_processor.py']",
        "        )",
        "        self.assertEqual(task.retrospective['tests_added'], 3)",
        "        self.assertEqual(task.retrospective['commits'], ['abc1234', 'def5678'])",
        "        self.assertEqual(task.retrospective['notes'], 'Added new search algorithm')",
        "        self.assertIsInstance(task.retrospective['duration_minutes'], int)",
        "        self.assertIn('captured_at', task.retrospective)",
        "",
        "    def test_capture_retrospective_with_defaults(self):",
        "        \"\"\"capture_retrospective works with minimal parameters.\"\"\"",
        "        task = self.session.create_task(\"Simple task\")",
        "        task_id = task.id",
        "",
        "        # Capture with only task_id",
        "        self.session.capture_retrospective(task_id=task_id)",
        "",
        "        task = self.session.get_task(task_id)",
        "        self.assertIsNotNone(task.retrospective)",
        "        self.assertEqual(task.retrospective['files_touched'], [])",
        "        self.assertEqual(task.retrospective['tests_added'], 0)",
        "        self.assertEqual(task.retrospective['commits'], [])",
        "        self.assertIsNone(task.retrospective['notes'])",
        "        self.assertGreaterEqual(task.retrospective['duration_minutes'], 0)",
        "",
        "    def test_capture_retrospective_invalid_task_id(self):",
        "        \"\"\"capture_retrospective raises ValueError for unknown task.\"\"\"",
        "        with self.assertRaises(ValueError) as ctx:",
        "            self.session.capture_retrospective(task_id=\"T-nonexistent-001\")",
        "",
        "        self.assertIn(\"Task not found\", str(ctx.exception))",
        "",
        "    def test_duration_calculation(self):",
        "        \"\"\"Duration is calculated correctly from task creation.\"\"\"",
        "        task = self.session.create_task(\"Time test\")",
        "        task_id = task.id",
        "",
        "        # Simulate some time passing (small delay for testing)",
        "        time.sleep(0.1)",
        "",
        "        self.session.capture_retrospective(task_id=task_id)",
        "",
        "        task = self.session.get_task(task_id)",
        "        # Duration should be >= 0 minutes (rounds down for sub-minute tasks)",
        "        self.assertGreaterEqual(task.retrospective['duration_minutes'], 0)",
        "        self.assertIsInstance(task.retrospective['duration_minutes'], int)",
        "",
        "    def test_duration_calculation_edge_case(self):",
        "        \"\"\"Duration calculation with manually set old created_at.\"\"\"",
        "        # Create task with old timestamp",
        "        task = Task(",
        "            id=\"T-old-001\",",
        "            title=\"Old task\",",
        "            created_at=(datetime.now() - timedelta(hours=2, minutes=30)).isoformat()",
        "        )",
        "        self.session.tasks.append(task)",
        "",
        "        self.session.capture_retrospective(task_id=task.id)",
        "",
        "        task = self.session.get_task(task.id)",
        "        # Should be approximately 150 minutes (2.5 hours)",
        "        duration = task.retrospective['duration_minutes']",
        "        self.assertGreater(duration, 145)  # Allow some margin",
        "        self.assertLess(duration, 155)",
        "",
        "    def test_get_retrospective_summary_empty(self):",
        "        \"\"\"get_retrospective_summary returns correct defaults for empty session.\"\"\"",
        "        summary = self.session.get_retrospective_summary()",
        "",
        "        self.assertEqual(summary['total_completed'], 0)",
        "        self.assertEqual(summary['avg_duration_minutes'], 0)",
        "        self.assertEqual(summary['total_duration_minutes'], 0)",
        "        self.assertEqual(summary['total_tests_added'], 0)",
        "        self.assertEqual(summary['most_touched_files'], [])",
        "        self.assertEqual(summary['tasks_with_retrospective'], [])",
        "",
        "    def test_get_retrospective_summary_single_task(self):",
        "        \"\"\"get_retrospective_summary aggregates single task correctly.\"\"\"",
        "        task = self.session.create_task(\"Task 1\")",
        "        task.status = \"completed\"",
        "",
        "        self.session.capture_retrospective(",
        "            task_id=task.id,",
        "            files_touched=['file1.py', 'file2.py'],",
        "            tests_added=5,",
        "            commits=['abc123']",
        "        )",
        "",
        "        summary = self.session.get_retrospective_summary()",
        "",
        "        self.assertEqual(summary['total_completed'], 1)",
        "        self.assertGreaterEqual(summary['avg_duration_minutes'], 0)",
        "        self.assertEqual(summary['total_tests_added'], 5)",
        "        self.assertEqual(len(summary['most_touched_files']), 2)",
        "        self.assertEqual(summary['tasks_with_retrospective'], [task.id])",
        "",
        "    def test_get_retrospective_summary_multiple_tasks(self):",
        "        \"\"\"get_retrospective_summary aggregates multiple tasks correctly.\"\"\"",
        "        # Create and complete multiple tasks",
        "        task1 = self.session.create_task(\"Task 1\")",
        "        task1.status = \"completed\"",
        "        self.session.capture_retrospective(",
        "            task_id=task1.id,",
        "            files_touched=['file1.py', 'file2.py'],",
        "            tests_added=3",
        "        )",
        "",
        "        task2 = self.session.create_task(\"Task 2\")",
        "        task2.status = \"completed\"",
        "        self.session.capture_retrospective(",
        "            task_id=task2.id,",
        "            files_touched=['file1.py', 'file3.py'],",
        "            tests_added=7",
        "        )",
        "",
        "        task3 = self.session.create_task(\"Task 3\")",
        "        task3.status = \"completed\"",
        "        self.session.capture_retrospective(",
        "            task_id=task3.id,",
        "            files_touched=['file2.py'],",
        "            tests_added=2",
        "        )",
        "",
        "        summary = self.session.get_retrospective_summary()",
        "",
        "        self.assertEqual(summary['total_completed'], 3)",
        "        self.assertEqual(summary['total_tests_added'], 12)  # 3 + 7 + 2",
        "",
        "        # Check file counts",
        "        file_counts = dict(summary['most_touched_files'])",
        "        self.assertEqual(file_counts['file1.py'], 2)  # in task1 and task2",
        "        self.assertEqual(file_counts['file2.py'], 2)  # in task1 and task3",
        "        self.assertEqual(file_counts['file3.py'], 1)  # only in task2",
        "",
        "        self.assertEqual(len(summary['tasks_with_retrospective']), 3)",
        "",
        "    def test_get_retrospective_summary_ignores_incomplete_tasks(self):",
        "        \"\"\"get_retrospective_summary only includes completed tasks.\"\"\"",
        "        # Create pending task with retrospective (shouldn't happen in practice)",
        "        task1 = self.session.create_task(\"Pending task\")",
        "        task1.status = \"pending\"",
        "        self.session.capture_retrospective(task_id=task1.id, tests_added=5)",
        "",
        "        # Create completed task without retrospective",
        "        task2 = self.session.create_task(\"Completed without retro\")",
        "        task2.status = \"completed\"",
        "",
        "        # Create completed task with retrospective",
        "        task3 = self.session.create_task(\"Completed with retro\")",
        "        task3.status = \"completed\"",
        "        self.session.capture_retrospective(task_id=task3.id, tests_added=3)",
        "",
        "        summary = self.session.get_retrospective_summary()",
        "",
        "        # Only task3 should be counted",
        "        self.assertEqual(summary['total_completed'], 1)",
        "        self.assertEqual(summary['total_tests_added'], 3)",
        "        self.assertEqual(summary['tasks_with_retrospective'], [task3.id])",
        "",
        "    def test_retrospective_persists_across_save_load(self):",
        "        \"\"\"Retrospective data survives save/load cycle.\"\"\"",
        "        with tempfile.TemporaryDirectory() as tmpdir:",
        "            # Create task with retrospective",
        "            task = self.session.create_task(\"Persistent task\")",
        "            task.status = \"completed\"",
        "            self.session.capture_retrospective(",
        "                task_id=task.id,",
        "                files_touched=['file1.py', 'file2.py'],",
        "                tests_added=5,",
        "                commits=['abc123'],",
        "                notes='Test persistence'",
        "            )",
        "",
        "            # Save session",
        "            saved_path = self.session.save(tasks_dir=tmpdir)",
        "            self.assertTrue(saved_path.exists())",
        "",
        "            # Load session",
        "            loaded_session = TaskSession.load(saved_path)",
        "            loaded_task = loaded_session.get_task(task.id)",
        "",
        "            # Verify retrospective data",
        "            self.assertIsNotNone(loaded_task.retrospective)",
        "            self.assertEqual(",
        "                loaded_task.retrospective['files_touched'],",
        "                ['file1.py', 'file2.py']",
        "            )",
        "            self.assertEqual(loaded_task.retrospective['tests_added'], 5)",
        "            self.assertEqual(loaded_task.retrospective['commits'], ['abc123'])",
        "            self.assertEqual(loaded_task.retrospective['notes'], 'Test persistence')",
        "",
        "    def test_backward_compatibility_tasks_without_retrospective(self):",
        "        \"\"\"Tasks without retrospective field load correctly (backward compat).\"\"\"",
        "        with tempfile.TemporaryDirectory() as tmpdir:",
        "            # Manually create a task file without retrospective field",
        "            task_data = {",
        "                \"version\": 1,",
        "                \"session_id\": \"test\",",
        "                \"started_at\": datetime.now().isoformat(),",
        "                \"saved_at\": datetime.now().isoformat(),",
        "                \"tasks\": [",
        "                    {",
        "                        \"id\": \"T-old-001\",",
        "                        \"title\": \"Old task without retrospective\",",
        "                        \"status\": \"completed\",",
        "                        \"priority\": \"medium\",",
        "                        \"category\": \"general\",",
        "                        \"description\": \"\",",
        "                        \"depends_on\": [],",
        "                        \"effort\": \"medium\",",
        "                        \"created_at\": datetime.now().isoformat(),",
        "                        \"updated_at\": None,",
        "                        \"completed_at\": None,",
        "                        \"context\": {}",
        "                        # Note: no retrospective field",
        "                    }",
        "                ]",
        "            }",
        "",
        "            filepath = Path(tmpdir) / \"old_session.json\"",
        "            with open(filepath, 'w') as f:",
        "                json.dump(task_data, f, indent=2)",
        "",
        "            # Load and verify",
        "            loaded_session = TaskSession.load(filepath)",
        "            loaded_task = loaded_session.tasks[0]",
        "",
        "            self.assertEqual(loaded_task.id, \"T-old-001\")",
        "            self.assertIsNone(loaded_task.retrospective)  # Should default to None",
        "",
        "    def test_to_dict_includes_retrospective(self):",
        "        \"\"\"Task.to_dict() includes retrospective field.\"\"\"",
        "        task = Task(",
        "            id=\"T-test-001\",",
        "            title=\"Test task\"",
        "        )",
        "",
        "        # Without retrospective",
        "        task_dict = task.to_dict()",
        "        self.assertIn('retrospective', task_dict)",
        "        self.assertIsNone(task_dict['retrospective'])",
        "",
        "        # With retrospective",
        "        task.retrospective = {",
        "            'files_touched': ['file1.py'],",
        "            'duration_minutes': 30,",
        "            'tests_added': 2,",
        "            'commits': ['abc123'],",
        "            'notes': 'Done',",
        "            'captured_at': datetime.now().isoformat()",
        "        }",
        "        task_dict = task.to_dict()",
        "        self.assertIsNotNone(task_dict['retrospective'])",
        "        self.assertEqual(task_dict['retrospective']['tests_added'], 2)",
        "",
        "    def test_from_dict_handles_retrospective(self):",
        "        \"\"\"Task.from_dict() correctly reconstructs retrospective.\"\"\"",
        "        task_dict = {",
        "            'id': 'T-test-001',",
        "            'title': 'Test task',",
        "            'status': 'completed',",
        "            'priority': 'high',",
        "            'category': 'feature',",
        "            'description': 'Test description',",
        "            'depends_on': [],",
        "            'effort': 'medium',",
        "            'created_at': datetime.now().isoformat(),",
        "            'updated_at': None,",
        "            'completed_at': None,",
        "            'context': {},",
        "            'retrospective': {",
        "                'files_touched': ['file1.py', 'file2.py'],",
        "                'duration_minutes': 45,",
        "                'tests_added': 5,",
        "                'commits': ['abc123', 'def456'],",
        "                'notes': 'All done',",
        "                'captured_at': datetime.now().isoformat()",
        "            }",
        "        }",
        "",
        "        task = Task.from_dict(task_dict)",
        "",
        "        self.assertEqual(task.id, 'T-test-001')",
        "        self.assertIsNotNone(task.retrospective)",
        "        self.assertEqual(task.retrospective['files_touched'], ['file1.py', 'file2.py'])",
        "        self.assertEqual(task.retrospective['duration_minutes'], 45)",
        "        self.assertEqual(task.retrospective['tests_added'], 5)",
        "",
        "    def test_most_touched_files_ordering(self):",
        "        \"\"\"most_touched_files returns files in descending order by count.\"\"\"",
        "        # Create tasks touching files different amounts",
        "        for i in range(5):",
        "            task = self.session.create_task(f\"Task {i}\")",
        "            task.status = \"completed\"",
        "            # file1.py touched 5 times, file2.py 4 times, etc.",
        "            files = ['file1.py'] * (5 - i)",
        "            if i < 4:",
        "                files.append('file2.py')",
        "            if i < 3:",
        "                files.append('file3.py')",
        "            self.session.capture_retrospective(",
        "                task_id=task.id,",
        "                files_touched=files",
        "            )",
        "",
        "        summary = self.session.get_retrospective_summary()",
        "        most_touched = summary['most_touched_files']",
        "",
        "        # Should be ordered by count descending",
        "        self.assertGreater(len(most_touched), 0)",
        "        counts = [count for _, count in most_touched]",
        "        self.assertEqual(counts, sorted(counts, reverse=True))",
        "",
        "    def test_get_task_returns_none_for_unknown_id(self):",
        "        \"\"\"get_task returns None for unknown task ID.\"\"\"",
        "        result = self.session.get_task(\"T-nonexistent-001\")",
        "        self.assertIsNone(result)",
        "",
        "    def test_get_task_finds_existing_task(self):",
        "        \"\"\"get_task finds task by ID.\"\"\"",
        "        task = self.session.create_task(\"Findable task\")",
        "        found = self.session.get_task(task.id)",
        "        self.assertIsNotNone(found)",
        "        self.assertEqual(found.id, task.id)",
        "        self.assertEqual(found.title, \"Findable task\")",
        "",
        "",
        "class TestRetrospectiveDurationCalculation(unittest.TestCase):",
        "    \"\"\"Focused tests for duration calculation logic.\"\"\"",
        "",
        "    def test_calculate_duration_basic(self):",
        "        \"\"\"_calculate_duration computes correct minutes.\"\"\"",
        "        session = TaskSession()",
        "",
        "        # 1 hour ago",
        "        one_hour_ago = (datetime.now() - timedelta(hours=1)).isoformat()",
        "        duration = session._calculate_duration(one_hour_ago)",
        "        self.assertGreater(duration, 55)  # Allow some margin",
        "        self.assertLess(duration, 65)",
        "",
        "    def test_calculate_duration_rounds(self):",
        "        \"\"\"_calculate_duration rounds to nearest minute.\"\"\"",
        "        session = TaskSession()",
        "",
        "        # 90 seconds ago (1.5 minutes)",
        "        ninety_sec_ago = (datetime.now() - timedelta(seconds=90)).isoformat()",
        "        duration = session._calculate_duration(ninety_sec_ago)",
        "        # Should round to 2 minutes",
        "        self.assertIn(duration, [1, 2])  # Could be 1 or 2 depending on rounding",
        "",
        "    def test_calculate_duration_zero(self):",
        "        \"\"\"_calculate_duration handles immediate calculation.\"\"\"",
        "        session = TaskSession()",
        "",
        "        # Right now",
        "        now = datetime.now().isoformat()",
        "        duration = session._calculate_duration(now)",
        "        self.assertEqual(duration, 0)",
        "",
        "",
        "if __name__ == '__main__':",
        "    unittest.main()"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    }
  ],
  "hour_of_day": 7,
  "day_of_week": "Sunday",
  "seconds_since_last_commit": -107423,
  "is_merge": true,
  "is_initial": false,
  "parent_count": 2,
  "session_id": null,
  "related_chats": [],
  "ci_result": null,
  "reverted": false,
  "amended": false
}