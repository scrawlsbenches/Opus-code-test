{
  "hash": "a19009e3b04688f1a15c31a1c9ae85e97b323392",
  "message": "Merge pull request #86 from scrawlsbenches/claude/explore-tfidf-alternatives-FKYpq",
  "author": "scrawlsbenches",
  "timestamp": "2025-12-15 07:27:02 -0500",
  "branch": "claude/multi-index-design-DvifZ",
  "files_changed": [
    "CLAUDE.md",
    "benchmarks/BASELINE_SUMMARY.md",
    "benchmarks/after_bm25.json",
    "benchmarks/baseline_tfidf.json",
    "benchmarks/baseline_tfidf_real.json",
    "cortical/analysis.py",
    "cortical/config.py",
    "cortical/processor/compute.py",
    "cortical/processor/core.py",
    "cortical/processor/documents.py",
    "cortical/processor/persistence_api.py",
    "cortical/processor/query_api.py",
    "cortical/query/__init__.py",
    "cortical/query/search.py",
    "cortical/semantics.py",
    "docs/knowledge-transfer-bm25-optimization.md",
    "scripts/benchmark_scoring.py",
    "tasks/2025-12-15_05-23-36_ceac.json",
    "tests/test_edge_cases.py",
    "tests/unit/test_processor_core.py",
    "tests/unit/test_query_search.py"
  ],
  "insertions": 3184,
  "deletions": 75,
  "hunks": [
    {
      "file": "CLAUDE.md",
      "function": "def find_documents(",
      "start_line": 829,
      "lines_added": [
        "## Scoring Algorithms",
        "",
        "The processor supports multiple scoring algorithms for term weighting:",
        "",
        "### BM25 (Default)",
        "",
        "BM25 (Best Match 25) is the default scoring algorithm, optimized for code search:",
        "",
        "```python",
        "from cortical import CorticalTextProcessor",
        "from cortical.config import CorticalConfig",
        "",
        "# BM25 with default parameters (recommended)",
        "config = CorticalConfig(scoring_algorithm='bm25')",
        "",
        "# Tune BM25 parameters if needed",
        "config = CorticalConfig(",
        "    scoring_algorithm='bm25',",
        "    bm25_k1=1.2,  # Term frequency saturation (0.0-3.0, default 1.2)",
        "    bm25_b=0.75   # Length normalization (0.0-1.0, default 0.75)",
        ")",
        "processor = CorticalTextProcessor(config=config)",
        "```",
        "",
        "**Parameters:**",
        "- `bm25_k1`: Controls term frequency saturation. Higher values give more weight to term frequency.",
        "- `bm25_b`: Controls document length normalization. Set to 0.0 to disable length normalization.",
        "",
        "### TF-IDF (Legacy)",
        "",
        "Traditional TF-IDF scoring is still available:",
        "",
        "```python",
        "config = CorticalConfig(scoring_algorithm='tfidf')",
        "```",
        "",
        "### Graph-Boosted Search (GB-BM25)",
        "",
        "A hybrid search combining BM25 with graph signals:",
        "",
        "```python",
        "# Standard search (uses BM25 under the hood)",
        "results = processor.find_documents_for_query(\"query\")",
        "",
        "# Graph-boosted search (adds PageRank + proximity signals)",
        "results = processor.graph_boosted_search(",
        "    \"query\",",
        "    pagerank_weight=0.3,   # Weight for term importance (0-1)",
        "    proximity_weight=0.2   # Weight for connected terms (0-1)",
        ")",
        "```",
        "",
        "**GB-BM25 combines:**",
        "1. BM25 base score (term relevance)",
        "2. PageRank boost (important terms rank higher)",
        "3. Proximity boost (connected query terms boost documents)",
        "4. Coverage boost (documents matching more terms rank higher)",
        "",
        "---",
        "",
        "9. **Use `graph_boosted_search()`** for hybrid scoring with PageRank signals"
      ],
      "lines_removed": [],
      "context_before": [
        "        top_n: Number of results to return",
        "",
        "    Returns:",
        "        List of (doc_id, score) tuples sorted by relevance",
        "    \"\"\"",
        "    # Implementation",
        "```",
        "",
        "---",
        ""
      ],
      "context_after": [
        "## Performance Considerations",
        "",
        "1. **Use `get_by_id()` for ID lookups** - O(1) vs O(n) iteration",
        "2. **Batch document additions** with `add_documents_batch()` for bulk imports",
        "3. **Use incremental updates** with `add_document_incremental()` for live systems",
        "4. **Cache query expansions** when processing multiple similar queries",
        "5. **Pre-compute chunks** in `find_passages_batch()` to avoid redundant work",
        "6. **Use `fast_find_documents()`** for ~2-3x faster search on large corpora",
        "7. **Pre-build index** with `build_search_index()` for fastest repeated queries",
        "8. **Watch for O(n²) patterns** in loops over connections—use limits like `max_bigrams_per_term`",
        "",
        "---",
        "",
        "## Code Search Capabilities",
        "",
        "### Code-Aware Tokenization",
        "```python",
        "# Enable identifier splitting for code search",
        "tokenizer = Tokenizer(split_identifiers=True)",
        "tokens = tokenizer.tokenize(\"getUserCredentials\")"
      ],
      "change_type": "add"
    },
    {
      "file": "CLAUDE.md",
      "function": "python examples/observability_demo.py",
      "start_line": 1009,
      "lines_added": [
        "| Hybrid search | `processor.graph_boosted_search(query)` |"
      ],
      "lines_removed": [],
      "context_before": [
        "---",
        "",
        "## Quick Reference",
        "",
        "| Task | Command/Method |",
        "|------|----------------|",
        "| Process document | `processor.process_document(id, text)` |",
        "| Build network | `processor.compute_all()` |",
        "| Search | `processor.find_documents_for_query(query)` |",
        "| Fast search | `processor.fast_find_documents(query)` |"
      ],
      "context_after": [
        "| Code search | `processor.expand_query_for_code(query)` |",
        "| Intent search | `processor.search_by_intent(\"where do we...\")` |",
        "| RAG passages | `processor.find_passages_for_query(query)` |",
        "| Fingerprint | `processor.get_fingerprint(text)` |",
        "| Compare | `processor.compare_fingerprints(fp1, fp2)` |",
        "| Save state | `processor.save(\"corpus.pkl\")` |",
        "| Load state | `processor = CorticalTextProcessor.load(\"corpus.pkl\")` |",
        "| Enable metrics | `processor = CorticalTextProcessor(enable_metrics=True)` |",
        "| Get metrics | `processor.get_metrics()` |",
        "| Metrics summary | `processor.get_metrics_summary()` |"
      ],
      "change_type": "add"
    },
    {
      "file": "benchmarks/BASELINE_SUMMARY.md",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "# Scoring Algorithm Performance Comparison",
        "",
        "**Date:** 2025-12-15",
        "**Algorithms Compared:** TF-IDF vs BM25",
        "**Default Algorithm:** BM25 (as of this commit)",
        "",
        "## Executive Summary",
        "",
        "### TF-IDF vs BM25 Comparison",
        "",
        "| Metric | TF-IDF | BM25 | Change |",
        "|--------|--------|------|--------|",
        "| Score Computation (100 docs) | 0.72ms | 1.26ms | +75% |",
        "| Search Latency | 0.15ms | 0.15ms | +0.8% |",
        "| Mean P@3 | 0.75 | 0.75 | 0% |",
        "| Mean MRR | 0.78 | 0.78 | 0% |",
        "| Scaling Complexity | O(n) | O(n) | Same |",
        "",
        "### Key Findings",
        "",
        "1. **BM25 computation is ~60-90% slower** than TF-IDF due to length normalization overhead",
        "2. **Search latency is virtually identical** (< 1% difference)",
        "3. **Relevance metrics are the same** on the synthetic corpus (uniform document lengths)",
        "4. **Both algorithms scale linearly** with corpus size",
        "",
        "### Why Use BM25 Despite Slower Computation?",
        "",
        "1. **Term frequency saturation**: Prevents single repeated terms from dominating scores",
        "2. **Length normalization**: Fair comparison across documents of different sizes",
        "3. **Industry standard**: Used by Elasticsearch, Lucene, and most modern search engines",
        "4. **Real-world relevance**: Benefits appear with variable document lengths",
        "",
        "## Detailed Benchmarks",
        "",
        "### Score Computation Time",
        "",
        "| Corpus Size | TF-IDF (ms) | BM25 (ms) | Overhead |",
        "|-------------|-------------|-----------|----------|",
        "| 25 docs | 0.20 | 0.33 | +65% |",
        "| 50 docs | 0.38 | 0.62 | +63% |",
        "| 100 docs | 0.72 | 1.26 | +75% |",
        "| 200 docs | 1.42 | 2.73 | +92% |",
        "| **Real (150 docs)** | **16.3** | **25.6** | **+57%** |",
        "",
        "**Note:** The overhead comes from the length normalization calculation in BM25.",
        "",
        "### Search Query Latency",
        "",
        "Both algorithms have nearly identical search latency:",
        "",
        "| Algorithm | Mean Latency | Throughput |",
        "|-----------|--------------|------------|",
        "| TF-IDF | 0.15ms | 6,507 QPS |",
        "| BM25 | 0.15ms | 6,374 QPS |",
        "",
        "Search uses pre-computed scores, so the algorithm choice doesn't affect query time.",
        "",
        "### Search Relevance Quality",
        "",
        "On the synthetic corpus (uniform document lengths):",
        "",
        "| Metric | TF-IDF | BM25 |",
        "|--------|--------|------|",
        "| Mean P@1 | 0.75 | 0.75 |",
        "| Mean P@3 | 0.75 | 0.75 |",
        "| Mean MRR | 0.78 | 0.78 |",
        "| Term Recall | 0.80 | 0.80 |",
        "",
        "**Note:** Relevance is identical on synthetic corpus because documents have uniform lengths. BM25's benefits appear with variable document lengths.",
        "",
        "### Memory Footprint",
        "",
        "| Corpus Size | TF-IDF (KB) | BM25 (KB) |",
        "|-------------|-------------|-----------|",
        "| 100 docs | 193.7 | 193.8 |",
        "| 200 docs | 398.6 | 398.7 |",
        "",
        "Memory usage is essentially identical. The doc_lengths dictionary adds negligible overhead.",
        "",
        "### Scaling Behavior",
        "",
        "| Algorithm | Scaling Exponent | Complexity |",
        "|-----------|-----------------|------------|",
        "| TF-IDF | 0.94 | O(n) |",
        "| BM25 | 0.96 | O(n) |",
        "",
        "Both algorithms maintain linear scaling with corpus size.",
        "",
        "## Configuration",
        "",
        "BM25 is now the default. To switch algorithms:",
        "",
        "```python",
        "from cortical import CorticalTextProcessor",
        "from cortical.config import CorticalConfig",
        "",
        "# Use BM25 (default)",
        "processor = CorticalTextProcessor()",
        "",
        "# Use TF-IDF",
        "config = CorticalConfig(scoring_algorithm='tfidf')",
        "processor = CorticalTextProcessor(config=config)",
        "",
        "# Tune BM25 parameters",
        "config = CorticalConfig(",
        "    scoring_algorithm='bm25',",
        "    bm25_k1=1.5,  # Term frequency saturation (default: 1.2)",
        "    bm25_b=0.75   # Length normalization (default: 0.75)",
        ")",
        "```",
        "",
        "### BM25 Parameters",
        "",
        "| Parameter | Default | Range | Effect |",
        "|-----------|---------|-------|--------|",
        "| `bm25_k1` | 1.2 | 0-3 | Term frequency saturation. Higher = more weight to term frequency |",
        "| `bm25_b` | 0.75 | 0-1 | Length normalization. 0 = none, 1 = full |",
        "",
        "## Files",
        "",
        "- `baseline_tfidf.json` - TF-IDF benchmark results",
        "- `baseline_tfidf_real.json` - TF-IDF real corpus results",
        "- `after_bm25.json` - BM25 benchmark results",
        "",
        "## How to Run Benchmarks",
        "",
        "```bash",
        "# Run with current default algorithm (BM25)",
        "python scripts/benchmark_scoring.py --output benchmarks/current.json",
        "",
        "# Run with specific algorithm",
        "python scripts/benchmark_scoring.py --algorithm tfidf --output benchmarks/tfidf.json",
        "python scripts/benchmark_scoring.py --algorithm bm25 --output benchmarks/bm25.json",
        "",
        "# Compare two benchmark runs",
        "python scripts/benchmark_scoring.py --compare benchmarks/baseline_tfidf.json benchmarks/after_bm25.json",
        "```",
        "",
        "## Implementation Notes",
        "",
        "### What Changed",
        "",
        "1. **config.py**: Added `scoring_algorithm`, `bm25_k1`, `bm25_b` parameters",
        "2. **analysis.py**: Added `compute_bm25()` and `_bm25_core()` functions",
        "3. **processor/core.py**: Added `doc_lengths` and `avg_doc_length` tracking",
        "4. **processor/documents.py**: Track document lengths during processing",
        "5. **processor/compute.py**: `compute_tfidf()` now respects `scoring_algorithm` config",
        "6. **processor/persistence_api.py**: Save/restore document lengths",
        "",
        "### BM25 Formula",
        "",
        "```",
        "BM25(t, d) = IDF(t) × (tf(t,d) × (k1 + 1)) / (tf(t,d) + k1 × (1 - b + b × |d|/avgdl))",
        "",
        "Where:",
        "- IDF(t) = log((N - df + 0.5) / (df + 0.5) + 1)",
        "- tf(t,d) = term frequency in document",
        "- |d| = document length (in tokens)",
        "- avgdl = average document length",
        "- k1 = term frequency saturation parameter",
        "- b = length normalization parameter",
        "```",
        "",
        "### Backward Compatibility",
        "",
        "- Scores are stored in the same `col.tfidf` and `col.tfidf_per_doc` fields",
        "- All existing search functions work unchanged",
        "- Old pickle files are compatible (doc_lengths are recomputed on load)"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "benchmarks/after_bm25.json",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "{",
        "  \"version\": \"1.0\",",
        "  \"algorithm\": \"bm25\",",
        "  \"timestamp\": \"2025-12-15T04:45:14.336647\",",
        "  \"system_info\": {",
        "    \"python_version\": \"3.11.14\",",
        "    \"platform\": \"Linux-4.4.0-x86_64-with-glibc2.39\",",
        "    \"processor\": \"x86_64\"",
        "  },",
        "  \"results\": [",
        "    {",
        "      \"name\": \"compute_scores\",",
        "      \"algorithm\": \"bm25\",",
        "      \"timestamp\": \"2025-12-15T04:45:14.406581\",",
        "      \"corpus_size\": 25,",
        "      \"vocabulary_size\": 64,",
        "      \"metrics\": {",
        "        \"corpus_size\": 25,",
        "        \"vocabulary_size\": 64,",
        "        \"mean_time_ms\": 0.3291995999688879,",
        "        \"std_time_ms\": 0.020515947884730667,",
        "        \"min_time_ms\": 0.31208699999751843,",
        "        \"max_time_ms\": 0.3623689999585622,",
        "        \"time_per_doc_ms\": 0.013167983998755517,",
        "        \"time_per_term_us\": 5.143743749513874",
        "      }",
        "    },",
        "    {",
        "      \"name\": \"compute_scores\",",
        "      \"algorithm\": \"bm25\",",
        "      \"timestamp\": \"2025-12-15T04:45:14.507138\",",
        "      \"corpus_size\": 50,",
        "      \"vocabulary_size\": 64,",
        "      \"metrics\": {",
        "        \"corpus_size\": 50,",
        "        \"vocabulary_size\": 64,",
        "        \"mean_time_ms\": 0.6236895999791159,",
        "        \"std_time_ms\": 0.009425228295778567,",
        "        \"min_time_ms\": 0.6114489999617945,",
        "        \"max_time_ms\": 0.6329369999775736,",
        "        \"time_per_doc_ms\": 0.012473791999582318,",
        "        \"time_per_term_us\": 9.745149999673686",
        "      }",
        "    },",
        "    {",
        "      \"name\": \"compute_scores\",",
        "      \"algorithm\": \"bm25\",",
        "      \"timestamp\": \"2025-12-15T04:45:14.693444\",",
        "      \"corpus_size\": 100,",
        "      \"vocabulary_size\": 64,",
        "      \"metrics\": {",
        "        \"corpus_size\": 100,",
        "        \"vocabulary_size\": 64,",
        "        \"mean_time_ms\": 1.2571063999757826,",
        "        \"std_time_ms\": 0.04260392122155854,",
        "        \"min_time_ms\": 1.2077509999244285,",
        "        \"max_time_ms\": 1.3081880000527235,",
        "        \"time_per_doc_ms\": 0.012571063999757826,",
        "        \"time_per_term_us\": 19.642287499621602",
        "      }",
        "    },",
        "    {",
        "      \"name\": \"compute_scores\",",
        "      \"algorithm\": \"bm25\",",
        "      \"timestamp\": \"2025-12-15T04:45:15.054362\",",
        "      \"corpus_size\": 200,",
        "      \"vocabulary_size\": 64,",
        "      \"metrics\": {",
        "        \"corpus_size\": 200,",
        "        \"vocabulary_size\": 64,",
        "        \"mean_time_ms\": 2.7318268000044554,",
        "        \"std_time_ms\": 0.38535747660282266,",
        "        \"min_time_ms\": 2.4955750000117405,",
        "        \"max_time_ms\": 3.4158199999865246,",
        "        \"time_per_doc_ms\": 0.013659134000022277,",
        "        \"time_per_term_us\": 42.684793750069616",
        "      }",
        "    },",
        "    {",
        "      \"name\": \"search_latency\",",
        "      \"algorithm\": \"bm25\",",
        "      \"timestamp\": \"2025-12-15T04:45:16.118779\",",
        "      \"corpus_size\": 100,",
        "      \"vocabulary_size\": 64,",
        "      \"metrics\": {",
        "        \"corpus_size\": 100,",
        "        \"vocabulary_size\": 64,",
        "        \"num_queries\": 8,",
        "        \"mean_latency_ms\": 0.15497148750540646,",
        "        \"median_latency_ms\": 0.1494892500033984,",
        "        \"p95_latency_ms\": 0.18840760000102819,",
        "        \"max_latency_ms\": 0.18840760000102819,",
        "        \"min_latency_ms\": 0.14132020000943157,",
        "        \"throughput_qps\": 6452.799905951171",
        "      }",
        "    },",
        "    {",
        "      \"name\": \"search_relevance\",",
        "      \"algorithm\": \"bm25\",",
        "      \"timestamp\": \"2025-12-15T04:45:17.204662\",",
        "      \"corpus_size\": 100,",
        "      \"vocabulary_size\": 64,",
        "      \"metrics\": {",
        "        \"corpus_size\": 100,",
        "        \"num_queries\": 4,",
        "        \"mean_p@1\": 0.75,",
        "        \"mean_p@3\": 0.75,",
        "        \"mean_p@5\": 0.75,",
        "        \"mean_mrr\": 0.7777777777777778,",
        "        \"mean_term_recall\": 0.8,",
        "        \"per_query\": [",
        "          {",
        "            \"query\": \"neural network training\",",
        "            \"domain\": \"ml\",",
        "            \"p@1\": 0.0,",
        "            \"p@3\": 0.0,",
        "            \"p@5\": 0.0,",
        "            \"mrr\": 0.1111111111111111,",
        "            \"term_recall\": 0.8",
        "          },",
        "          {",
        "            \"query\": \"database query optimization\",",
        "            \"domain\": \"db\",",
        "            \"p@1\": 1.0,",
        "            \"p@3\": 1.0,",
        "            \"p@5\": 1.0,",
        "            \"mrr\": 1.0,",
        "            \"term_recall\": 0.8",
        "          },",
        "          {",
        "            \"query\": \"process memory management\",",
        "            \"domain\": \"sys\",",
        "            \"p@1\": 1.0,",
        "            \"p@3\": 1.0,",
        "            \"p@5\": 1.0,",
        "            \"mrr\": 1.0,",
        "            \"term_recall\": 0.8",
        "          },",
        "          {",
        "            \"query\": \"api authentication\",",
        "            \"domain\": \"web\",",
        "            \"p@1\": 1.0,",
        "            \"p@3\": 1.0,",
        "            \"p@5\": 1.0,",
        "            \"mrr\": 1.0,",
        "            \"term_recall\": 0.8",
        "          }",
        "        ]",
        "      }",
        "    },",
        "    {",
        "      \"name\": \"memory_footprint\",",
        "      \"algorithm\": \"bm25\",",
        "      \"timestamp\": \"2025-12-15T04:45:17.481222\",",
        "      \"corpus_size\": 25,",
        "      \"vocabulary_size\": 64,",
        "      \"metrics\": {",
        "        \"corpus_size\": 25,",
        "        \"vocabulary_size\": 64,",
        "        \"score_memory_bytes\": 53184,",
        "        \"score_memory_kb\": 51.9375,",
        "        \"total_tfidf_entries\": 995,",
        "        \"bytes_per_entry\": 53.451256281407034,",
        "        \"bytes_per_term\": 831.0",
        "      }",
        "    },",
        "    {",
        "      \"name\": \"memory_footprint\",",
        "      \"algorithm\": \"bm25\",",
        "      \"timestamp\": \"2025-12-15T04:45:17.967628\",",
        "      \"corpus_size\": 50,",
        "      \"vocabulary_size\": 64,",
        "      \"metrics\": {",
        "        \"corpus_size\": 50,",
        "        \"vocabulary_size\": 64,",
        "        \"score_memory_bytes\": 101568,",
        "        \"score_memory_kb\": 99.1875,",
        "        \"total_tfidf_entries\": 1988,",
        "        \"bytes_per_entry\": 51.09054325955734,",
        "        \"bytes_per_term\": 1587.0",
        "      }",
        "    },",
        "    {",
        "      \"name\": \"memory_footprint\",",
        "      \"algorithm\": \"bm25\",",
        "      \"timestamp\": \"2025-12-15T04:45:18.912447\",",
        "      \"corpus_size\": 100,",
        "      \"vocabulary_size\": 64,",
        "      \"metrics\": {",
        "        \"corpus_size\": 100,",
        "        \"vocabulary_size\": 64,",
        "        \"score_memory_bytes\": 198432,",
        "        \"score_memory_kb\": 193.78125,",
        "        \"total_tfidf_entries\": 4010,",
        "        \"bytes_per_entry\": 49.48428927680798,",
        "        \"bytes_per_term\": 3100.5",
        "      }",
        "    },",
        "    {",
        "      \"name\": \"memory_footprint\",",
        "      \"algorithm\": \"bm25\",",
        "      \"timestamp\": \"2025-12-15T04:45:20.819995\",",
        "      \"corpus_size\": 200,",
        "      \"vocabulary_size\": 64,",
        "      \"metrics\": {",
        "        \"corpus_size\": 200,",
        "        \"vocabulary_size\": 64,",
        "        \"score_memory_bytes\": 408312,",
        "        \"score_memory_kb\": 398.7421875,",
        "        \"total_tfidf_entries\": 8043,",
        "        \"bytes_per_entry\": 50.76613204028347,",
        "        \"bytes_per_term\": 6379.875",
        "      }",
        "    },",
        "    {",
        "      \"name\": \"scaling_behavior\",",
        "      \"algorithm\": \"bm25\",",
        "      \"timestamp\": \"2025-12-15T04:45:21.684457\",",
        "      \"corpus_size\": 200,",
        "      \"vocabulary_size\": 64,",
        "      \"metrics\": {",
        "        \"data_points\": [",
        "          {",
        "            \"n_docs\": 10,",
        "            \"vocab_size\": 64,",
        "            \"time_ms\": 0.14531600004374923",
        "          },",
        "          {",
        "            \"n_docs\": 25,",
        "            \"vocab_size\": 64,",
        "            \"time_ms\": 0.309473666637435",
        "          },",
        "          {",
        "            \"n_docs\": 50,",
        "            \"vocab_size\": 64,",
        "            \"time_ms\": 0.5840006666251915",
        "          },",
        "          {",
        "            \"n_docs\": 100,",
        "            \"vocab_size\": 64,",
        "            \"time_ms\": 1.267608999986199",
        "          },",
        "          {",
        "            \"n_docs\": 150,",
        "            \"vocab_size\": 64,",
        "            \"time_ms\": 1.775815666633207",
        "          },",
        "          {",
        "            \"n_docs\": 200,",
        "            \"vocab_size\": 64,",
        "            \"time_ms\": 2.5024700000055113",
        "          }",
        "        ],",
        "        \"scaling_exponent\": 0.9558648590683401,",
        "        \"estimated_complexity\": \"O(n)\"",
        "      }",
        "    },",
        "    {",
        "      \"name\": \"real_corpus\",",
        "      \"algorithm\": \"bm25\",",
        "      \"timestamp\": \"2025-12-15T04:45:26.225935\",",
        "      \"corpus_size\": 150,",
        "      \"vocabulary_size\": 11862,",
        "      \"metrics\": {",
        "        \"corpus_size\": 150,",
        "        \"vocabulary_size\": 11862,",
        "        \"compute_time_ms\": 25.580323666683096,",
        "        \"mean_query_time_ms\": 0.37774209999383856,",
        "        \"max_query_time_ms\": 0.5424007999863534,",
        "        \"queries_tested\": 8",
        "      }",
        "    }",
        "  ]",
        "}"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "benchmarks/baseline_tfidf.json",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "{",
        "  \"version\": \"1.0\",",
        "  \"algorithm\": \"tfidf\",",
        "  \"timestamp\": \"2025-12-15T04:17:28.406343\",",
        "  \"system_info\": {",
        "    \"python_version\": \"3.11.14\",",
        "    \"platform\": \"Linux-4.4.0-x86_64-with-glibc2.39\",",
        "    \"processor\": \"x86_64\"",
        "  },",
        "  \"results\": [",
        "    {",
        "      \"name\": \"compute_scores\",",
        "      \"algorithm\": \"tfidf\",",
        "      \"timestamp\": \"2025-12-15T04:17:28.472214\",",
        "      \"corpus_size\": 25,",
        "      \"vocabulary_size\": 64,",
        "      \"metrics\": {",
        "        \"corpus_size\": 25,",
        "        \"vocabulary_size\": 64,",
        "        \"mean_time_ms\": 0.20012960000030944,",
        "        \"std_time_ms\": 0.018232865339288584,",
        "        \"min_time_ms\": 0.18339399997557848,",
        "        \"max_time_ms\": 0.22034799997072696,",
        "        \"time_per_doc_ms\": 0.008005184000012378,",
        "        \"time_per_term_us\": 3.127025000004835",
        "      }",
        "    },",
        "    {",
        "      \"name\": \"compute_scores\",",
        "      \"algorithm\": \"tfidf\",",
        "      \"timestamp\": \"2025-12-15T04:17:28.565169\",",
        "      \"corpus_size\": 50,",
        "      \"vocabulary_size\": 64,",
        "      \"metrics\": {",
        "        \"corpus_size\": 50,",
        "        \"vocabulary_size\": 64,",
        "        \"mean_time_ms\": 0.38337980001870164,",
        "        \"std_time_ms\": 0.010113204743546902,",
        "        \"min_time_ms\": 0.3726560000245627,",
        "        \"max_time_ms\": 0.39955299996563554,",
        "        \"time_per_doc_ms\": 0.007667596000374033,",
        "        \"time_per_term_us\": 5.990309375292213",
        "      }",
        "    },",
        "    {",
        "      \"name\": \"compute_scores\",",
        "      \"algorithm\": \"tfidf\",",
        "      \"timestamp\": \"2025-12-15T04:17:28.736963\",",
        "      \"corpus_size\": 100,",
        "      \"vocabulary_size\": 64,",
        "      \"metrics\": {",
        "        \"corpus_size\": 100,",
        "        \"vocabulary_size\": 64,",
        "        \"mean_time_ms\": 0.7150193999791554,",
        "        \"std_time_ms\": 0.01533470549806313,",
        "        \"min_time_ms\": 0.6907419999606645,",
        "        \"max_time_ms\": 0.7290339999599382,",
        "        \"time_per_doc_ms\": 0.007150193999791554,",
        "        \"time_per_term_us\": 11.172178124674303",
        "      }",
        "    },",
        "    {",
        "      \"name\": \"compute_scores\",",
        "      \"algorithm\": \"tfidf\",",
        "      \"timestamp\": \"2025-12-15T04:17:29.081526\",",
        "      \"corpus_size\": 200,",
        "      \"vocabulary_size\": 64,",
        "      \"metrics\": {",
        "        \"corpus_size\": 200,",
        "        \"vocabulary_size\": 64,",
        "        \"mean_time_ms\": 1.4186690000087765,",
        "        \"std_time_ms\": 0.05244864629302831,",
        "        \"min_time_ms\": 1.360104000013962,",
        "        \"max_time_ms\": 1.486779000003935,",
        "        \"time_per_doc_ms\": 0.0070933450000438825,",
        "        \"time_per_term_us\": 22.166703125137133",
        "      }",
        "    },",
        "    {",
        "      \"name\": \"search_latency\",",
        "      \"algorithm\": \"tfidf\",",
        "      \"timestamp\": \"2025-12-15T04:17:30.094282\",",
        "      \"corpus_size\": 100,",
        "      \"vocabulary_size\": 64,",
        "      \"metrics\": {",
        "        \"corpus_size\": 100,",
        "        \"vocabulary_size\": 64,",
        "        \"num_queries\": 8,",
        "        \"mean_latency_ms\": 0.15369055000178378,",
        "        \"median_latency_ms\": 0.14959700000360954,",
        "        \"p95_latency_ms\": 0.17851069999892388,",
        "        \"max_latency_ms\": 0.17851069999892388,",
        "        \"min_latency_ms\": 0.14334240000266618,",
        "        \"throughput_qps\": 6506.580918530083",
        "      }",
        "    },",
        "    {",
        "      \"name\": \"search_relevance\",",
        "      \"algorithm\": \"tfidf\",",
        "      \"timestamp\": \"2025-12-15T04:17:31.080923\",",
        "      \"corpus_size\": 100,",
        "      \"vocabulary_size\": 64,",
        "      \"metrics\": {",
        "        \"corpus_size\": 100,",
        "        \"num_queries\": 4,",
        "        \"mean_p@1\": 0.75,",
        "        \"mean_p@3\": 0.75,",
        "        \"mean_p@5\": 0.75,",
        "        \"mean_mrr\": 0.7777777777777778,",
        "        \"mean_term_recall\": 0.8,",
        "        \"per_query\": [",
        "          {",
        "            \"query\": \"neural network training\",",
        "            \"domain\": \"ml\",",
        "            \"p@1\": 0.0,",
        "            \"p@3\": 0.0,",
        "            \"p@5\": 0.0,",
        "            \"mrr\": 0.1111111111111111,",
        "            \"term_recall\": 0.8",
        "          },",
        "          {",
        "            \"query\": \"database query optimization\",",
        "            \"domain\": \"db\",",
        "            \"p@1\": 1.0,",
        "            \"p@3\": 1.0,",
        "            \"p@5\": 1.0,",
        "            \"mrr\": 1.0,",
        "            \"term_recall\": 0.8",
        "          },",
        "          {",
        "            \"query\": \"process memory management\",",
        "            \"domain\": \"sys\",",
        "            \"p@1\": 1.0,",
        "            \"p@3\": 1.0,",
        "            \"p@5\": 1.0,",
        "            \"mrr\": 1.0,",
        "            \"term_recall\": 0.8",
        "          },",
        "          {",
        "            \"query\": \"api authentication\",",
        "            \"domain\": \"web\",",
        "            \"p@1\": 1.0,",
        "            \"p@3\": 1.0,",
        "            \"p@5\": 1.0,",
        "            \"mrr\": 1.0,",
        "            \"term_recall\": 0.8",
        "          }",
        "        ]",
        "      }",
        "    },",
        "    {",
        "      \"name\": \"memory_footprint\",",
        "      \"algorithm\": \"tfidf\",",
        "      \"timestamp\": \"2025-12-15T04:17:31.291935\",",
        "      \"corpus_size\": 25,",
        "      \"vocabulary_size\": 64,",
        "      \"metrics\": {",
        "        \"corpus_size\": 25,",
        "        \"vocabulary_size\": 64,",
        "        \"score_memory_bytes\": 53088,",
        "        \"score_memory_kb\": 51.84375,",
        "        \"total_tfidf_entries\": 995,",
        "        \"bytes_per_entry\": 53.35477386934674,",
        "        \"bytes_per_term\": 829.5",
        "      }",
        "    },",
        "    {",
        "      \"name\": \"memory_footprint\",",
        "      \"algorithm\": \"tfidf\",",
        "      \"timestamp\": \"2025-12-15T04:17:31.688317\",",
        "      \"corpus_size\": 50,",
        "      \"vocabulary_size\": 64,",
        "      \"metrics\": {",
        "        \"corpus_size\": 50,",
        "        \"vocabulary_size\": 64,",
        "        \"score_memory_bytes\": 101472,",
        "        \"score_memory_kb\": 99.09375,",
        "        \"total_tfidf_entries\": 1988,",
        "        \"bytes_per_entry\": 51.04225352112676,",
        "        \"bytes_per_term\": 1585.5",
        "      }",
        "    },",
        "    {",
        "      \"name\": \"memory_footprint\",",
        "      \"algorithm\": \"tfidf\",",
        "      \"timestamp\": \"2025-12-15T04:17:32.496817\",",
        "      \"corpus_size\": 100,",
        "      \"vocabulary_size\": 64,",
        "      \"metrics\": {",
        "        \"corpus_size\": 100,",
        "        \"vocabulary_size\": 64,",
        "        \"score_memory_bytes\": 198336,",
        "        \"score_memory_kb\": 193.6875,",
        "        \"total_tfidf_entries\": 4010,",
        "        \"bytes_per_entry\": 49.46034912718204,",
        "        \"bytes_per_term\": 3099.0",
        "      }",
        "    },",
        "    {",
        "      \"name\": \"memory_footprint\",",
        "      \"algorithm\": \"tfidf\",",
        "      \"timestamp\": \"2025-12-15T04:17:34.129549\",",
        "      \"corpus_size\": 200,",
        "      \"vocabulary_size\": 64,",
        "      \"metrics\": {",
        "        \"corpus_size\": 200,",
        "        \"vocabulary_size\": 64,",
        "        \"score_memory_bytes\": 408216,",
        "        \"score_memory_kb\": 398.6484375,",
        "        \"total_tfidf_entries\": 8043,",
        "        \"bytes_per_entry\": 50.75419619544946,",
        "        \"bytes_per_term\": 6378.375",
        "      }",
        "    },",
        "    {",
        "      \"name\": \"scaling_behavior\",",
        "      \"algorithm\": \"tfidf\",",
        "      \"timestamp\": \"2025-12-15T04:17:34.914271\",",
        "      \"corpus_size\": 200,",
        "      \"vocabulary_size\": 64,",
        "      \"metrics\": {",
        "        \"data_points\": [",
        "          {",
        "            \"n_docs\": 10,",
        "            \"vocab_size\": 64,",
        "            \"time_ms\": 0.0802899999901759",
        "          },",
        "          {",
        "            \"n_docs\": 25,",
        "            \"vocab_size\": 64,",
        "            \"time_ms\": 0.16422099997726036",
        "          },",
        "          {",
        "            \"n_docs\": 50,",
        "            \"vocab_size\": 64,",
        "            \"time_ms\": 0.3338203333100864",
        "          },",
        "          {",
        "            \"n_docs\": 100,",
        "            \"vocab_size\": 64,",
        "            \"time_ms\": 0.6225586666725272",
        "          },",
        "          {",
        "            \"n_docs\": 150,",
        "            \"vocab_size\": 64,",
        "            \"time_ms\": 0.986785666649818",
        "          },",
        "          {",
        "            \"n_docs\": 200,",
        "            \"vocab_size\": 64,",
        "            \"time_ms\": 1.2952903333219485",
        "          }",
        "        ],",
        "        \"scaling_exponent\": 0.9389157086150623,",
        "        \"estimated_complexity\": \"O(n)\"",
        "      }",
        "    }",
        "  ]",
        "}"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "benchmarks/baseline_tfidf_real.json",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "{",
        "  \"version\": \"1.0\",",
        "  \"algorithm\": \"tfidf\",",
        "  \"timestamp\": \"2025-12-15T04:18:12.435168\",",
        "  \"system_info\": {",
        "    \"python_version\": \"3.11.14\",",
        "    \"platform\": \"Linux-4.4.0-x86_64-with-glibc2.39\",",
        "    \"processor\": \"x86_64\"",
        "  },",
        "  \"results\": [",
        "    {",
        "      \"name\": \"real_corpus\",",
        "      \"algorithm\": \"tfidf\",",
        "      \"timestamp\": \"2025-12-15T04:18:16.392542\",",
        "      \"corpus_size\": 150,",
        "      \"vocabulary_size\": 11862,",
        "      \"metrics\": {",
        "        \"corpus_size\": 150,",
        "        \"vocabulary_size\": 11862,",
        "        \"compute_time_ms\": 16.29648966665324,",
        "        \"mean_query_time_ms\": 0.36824840000662107,",
        "        \"max_query_time_ms\": 0.5212204000144993,",
        "        \"queries_tested\": 8",
        "      }",
        "    }",
        "  ]",
        "}"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "cortical/analysis.py",
      "function": "def compute_tfidf(",
      "start_line": 916,
      "lines_added": [
        "def _bm25_core(",
        "    term_stats: Dict[str, Tuple[int, int, Dict[str, int]]],",
        "    num_docs: int,",
        "    doc_lengths: Dict[str, int],",
        "    avg_doc_length: float,",
        "    k1: float = 1.2,",
        "    b: float = 0.75",
        ") -> Dict[str, Tuple[float, Dict[str, float]]]:",
        "    \"\"\"",
        "    Pure BM25 calculation.",
        "",
        "    BM25 (Best Match 25) is a ranking function that improves on TF-IDF by:",
        "    - Term frequency saturation: diminishing returns for repeated terms",
        "    - Document length normalization: fair comparison across different lengths",
        "",
        "    This core function takes primitive types and can be unit tested without",
        "    needing HierarchicalLayer objects.",
        "",
        "    Args:",
        "        term_stats: Dictionary mapping term to (occurrence_count, doc_frequency, {doc_id: count})",
        "                   - occurrence_count: total times term appears in corpus",
        "                   - doc_frequency: number of documents containing term",
        "                   - doc_counts: per-document occurrence counts",
        "        num_docs: Total number of documents in corpus",
        "        doc_lengths: Dictionary mapping doc_id to document length (in tokens)",
        "        avg_doc_length: Average document length across corpus",
        "        k1: Term frequency saturation parameter (0.0-3.0, typical 1.2-2.0)",
        "            - Higher k1 = more weight to term frequency",
        "            - k1=0 = binary model (presence/absence only)",
        "        b: Length normalization parameter (0.0-1.0)",
        "            - b=0 = no length normalization",
        "            - b=1 = full length normalization",
        "",
        "    Returns:",
        "        Dictionary mapping term to (global_bm25, {doc_id: per_doc_bm25})",
        "",
        "    Example:",
        "        >>> stats = {",
        "        ...     \"rare\": (5, 1, {\"doc1\": 5}),",
        "        ...     \"common\": (100, 10, {\"doc1\": 10, \"doc2\": 10, ...})",
        "        ... }",
        "        >>> doc_lengths = {\"doc1\": 100, \"doc2\": 150}",
        "        >>> results = _bm25_core(stats, num_docs=10, doc_lengths=doc_lengths, avg_doc_length=125)",
        "        >>> assert results[\"rare\"][0] > results[\"common\"][0]",
        "    \"\"\"",
        "    if num_docs == 0 or avg_doc_length == 0:",
        "        return {}",
        "",
        "    results = {}",
        "",
        "    for term, (occurrence_count, doc_frequency, doc_counts) in term_stats.items():",
        "        if doc_frequency > 0:",
        "            # IDF component (same as TF-IDF but can use BM25 variant)",
        "            # Standard BM25 IDF: log((N - df + 0.5) / (df + 0.5))",
        "            # This can go negative for very common terms, so we use a floor",
        "            idf = math.log((num_docs - doc_frequency + 0.5) / (doc_frequency + 0.5) + 1)",
        "",
        "            # Global BM25 (using total occurrence count and average length)",
        "            # This is an approximation for global term importance",
        "            tf_global = occurrence_count / num_docs  # Average TF across docs",
        "            global_bm25 = idf * ((tf_global * (k1 + 1)) / (tf_global + k1))",
        "",
        "            # Per-document BM25",
        "            per_doc_bm25 = {}",
        "            for doc_id, tf in doc_counts.items():",
        "                doc_len = doc_lengths.get(doc_id, avg_doc_length)",
        "                # Length normalization factor",
        "                len_norm = 1 - b + b * (doc_len / avg_doc_length)",
        "                # BM25 score for this document",
        "                numerator = tf * (k1 + 1)",
        "                denominator = tf + k1 * len_norm",
        "                per_doc_bm25[doc_id] = idf * (numerator / denominator)",
        "",
        "            results[term] = (global_bm25, per_doc_bm25)",
        "        else:",
        "            results[term] = (0.0, {})",
        "",
        "    return results",
        "",
        "",
        "def compute_bm25(",
        "    layers: Dict[CorticalLayer, HierarchicalLayer],",
        "    documents: Dict[str, str],",
        "    doc_lengths: Dict[str, int],",
        "    avg_doc_length: float,",
        "    k1: float = 1.2,",
        "    b: float = 0.75",
        ") -> None:",
        "    \"\"\"",
        "    Compute BM25 scores for tokens.",
        "",
        "    BM25 (Best Match 25) is a ranking function that addresses TF-IDF limitations:",
        "    - Term frequency saturation: prevents domination by repeated terms",
        "    - Document length normalization: fair comparison across different lengths",
        "",
        "    This stores scores in the same fields as TF-IDF (tfidf, tfidf_per_doc)",
        "    for backward compatibility with existing search functions.",
        "",
        "    Args:",
        "        layers: Dictionary of layers (needs TOKENS layer)",
        "        documents: Dictionary mapping doc_id to content",
        "        doc_lengths: Dictionary mapping doc_id to token count",
        "        avg_doc_length: Average document length in tokens",
        "        k1: Term frequency saturation (0-3, default 1.2)",
        "        b: Length normalization (0-1, default 0.75)",
        "    \"\"\"",
        "    layer0 = layers[CorticalLayer.TOKENS]",
        "    num_docs = len(documents)",
        "",
        "    if num_docs == 0 or avg_doc_length == 0:",
        "        return",
        "",
        "    for col in layer0.minicolumns.values():",
        "        # Document frequency",
        "        df = len(col.document_ids)",
        "",
        "        if df > 0:",
        "            # IDF component",
        "            # BM25 IDF: log((N - df + 0.5) / (df + 0.5) + 1)",
        "            # The +1 ensures non-negative values for common terms",
        "            idf = math.log((num_docs - df + 0.5) / (df + 0.5) + 1)",
        "",
        "            # Global BM25 (approximation using average TF)",
        "            avg_tf = col.occurrence_count / num_docs",
        "            col.tfidf = idf * ((avg_tf * (k1 + 1)) / (avg_tf + k1))",
        "",
        "            # Per-document BM25",
        "            for doc_id in col.document_ids:",
        "                tf = col.doc_occurrence_counts.get(doc_id, 1)",
        "                doc_len = doc_lengths.get(doc_id, avg_doc_length)",
        "                # Length normalization factor",
        "                len_norm = 1 - b + b * (doc_len / avg_doc_length)",
        "                # BM25 score",
        "                numerator = tf * (k1 + 1)",
        "                denominator = tf + k1 * len_norm",
        "                col.tfidf_per_doc[doc_id] = idf * (numerator / denominator)",
        "",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "            # TF-IDF",
        "            col.tfidf = tf * idf",
        "            ",
        "            # Per-document TF-IDF using actual occurrence counts",
        "            for doc_id in col.document_ids:",
        "                # Get actual term frequency in this document",
        "                doc_tf = col.doc_occurrence_counts.get(doc_id, 1)",
        "                col.tfidf_per_doc[doc_id] = math.log1p(doc_tf) * idf",
        "",
        ""
      ],
      "context_after": [
        "def propagate_activation(",
        "    layers: Dict[CorticalLayer, HierarchicalLayer],",
        "    iterations: int = 3,",
        "    decay: float = 0.8,",
        "    lateral_weight: float = 0.3",
        ") -> None:",
        "    \"\"\"",
        "    Propagate activation through the network.",
        "    ",
        "    This simulates how information flows through cortical layers:"
      ],
      "change_type": "add"
    },
    {
      "file": "cortical/analysis.py",
      "function": "def compute_bigram_connections(",
      "start_line": 1874,
      "lines_added": [
        "    # OPTIMIZED: Use inverted index approach instead of O(n²) matrix multiplication",
        "    # Additional optimization: importance-based filtering and early termination",
        "    skipped_low_importance = 0",
        "    # Build inverted index: doc_id -> list of bigram minicolumns",
        "    # Sort by TF-IDF importance within each document for priority processing",
        "    doc_to_bigrams: Dict[str, List[Minicolumn]] = defaultdict(list)",
        "        for doc_id in bigram.document_ids:",
        "            doc_to_bigrams[doc_id].append(bigram)",
        "",
        "    # Compute importance threshold (median TF-IDF) for filtering",
        "    tfidf_values = [b.tfidf for b in bigrams if b.tfidf > 0]",
        "    importance_threshold = sorted(tfidf_values)[len(tfidf_values) // 4] if tfidf_values else 0",
        "",
        "    # Process each document's bigram pairs",
        "    for doc_id, doc_bigrams in doc_to_bigrams.items():",
        "        # Skip large documents to avoid O(n²) explosion",
        "        if len(doc_bigrams) > max_bigrams_per_doc:",
        "        # Filter to important bigrams only (reduces pairs quadratically)",
        "        important_bigrams = [b for b in doc_bigrams if b.tfidf >= importance_threshold]",
        "        if len(important_bigrams) < 2:",
        "            continue",
        "        # Sort by importance for priority connections",
        "        important_bigrams.sort(key=lambda b: b.tfidf, reverse=True)",
        "        # Connect pairs of important bigrams in this document",
        "        # Limit to top connections per bigram to avoid explosion",
        "        for i, b1 in enumerate(important_bigrams):",
        "            # Early termination if this bigram is at connection limit",
        "            if connection_counts[b1.id] >= max_connections_per_bigram:",
        "                continue",
        "            for b2 in important_bigrams[i+1:]:",
        "                if connection_counts[b2.id] >= max_connections_per_bigram:",
        "                    continue",
        "                # Fast path: they share at least this document",
        "                if len(shared_docs) < min_shared_docs:",
        "                    continue"
      ],
      "lines_removed": [
        "    # Use sparse matrix multiplication for efficient co-occurrence computation",
        "    # Build document-term matrix using sparse representation",
        "    # Create mappings: doc_id -> row index, bigram -> col index",
        "    doc_to_row: Dict[str, int] = {}",
        "    bigram_to_col: Dict[str, int] = {}",
        "",
        "    # Collect all documents first",
        "    all_docs: Set[str] = set()",
        "        all_docs.update(bigram.document_ids)",
        "",
        "    # Assign row indices to documents (excluding large docs)",
        "    row_idx = 0",
        "    for doc_id in sorted(all_docs):",
        "        # Count bigrams in this doc",
        "        doc_bigram_count = sum(1 for b in bigrams if doc_id in b.document_ids)",
        "        if doc_bigram_count > max_bigrams_per_doc:",
        "        doc_to_row[doc_id] = row_idx",
        "        row_idx += 1",
        "",
        "    # Assign column indices to bigrams",
        "    for col_idx, bigram in enumerate(bigrams):",
        "        bigram_to_col[bigram.id] = col_idx",
        "",
        "    # Build sparse document-term matrix",
        "    # Rows = documents, Cols = bigrams",
        "    # Entry [d, b] = 1 if bigram b appears in document d",
        "    if doc_to_row:  # Only if we have valid documents",
        "        doc_term_matrix = SparseMatrix(len(doc_to_row), len(bigrams))",
        "",
        "        for bigram in bigrams:",
        "            col_idx = bigram_to_col[bigram.id]",
        "            for doc_id in bigram.document_ids:",
        "                if doc_id in doc_to_row:  # Skip large docs",
        "                    row_idx = doc_to_row[doc_id]",
        "                    doc_term_matrix.set(row_idx, col_idx, 1.0)",
        "",
        "        # Compute bigram-bigram co-occurrence matrix: D^T * D",
        "        # Result[i, j] = number of shared documents between bigram i and bigram j",
        "        cooccur_matrix = doc_term_matrix.multiply_transpose()",
        "",
        "        # Create inverse mapping: col_idx -> bigram",
        "        col_to_bigram = {col_idx: bigram for bigram, col_idx in bigram_to_col.items()}",
        "",
        "        # Process co-occurrence matrix to create connections",
        "        for col1, col2, shared_count in cooccur_matrix.get_nonzero():",
        "            # Skip diagonal (bigram with itself)",
        "            if col1 == col2:",
        "                continue",
        "",
        "            # Skip if below threshold",
        "            if shared_count < min_shared_docs:",
        "                continue",
        "            # Get bigrams",
        "            bigram1_id = col_to_bigram[col1]",
        "            bigram2_id = col_to_bigram[col2]",
        "            # Find the actual minicolumn objects",
        "            b1 = layer1.get_by_id(bigram1_id)",
        "            b2 = layer1.get_by_id(bigram2_id)",
        "            if b1 and b2:",
        "                # Compute Jaccard similarity"
      ],
      "context_before": [
        "            # Skip overly common terms",
        "            if len(left_index[term]) > max_bigrams_per_term or len(right_index[term]) > max_bigrams_per_term:",
        "                continue",
        "            # term appears as right component in some bigrams and left in others",
        "            for b_left in right_index[term]:  # ends with term",
        "                for b_right in left_index[term]:  # starts with term",
        "                    if b_left.id != b_right.id:",
        "                        add_connection(b_left, b_right, chain_weight, 'chain')",
        "",
        "    # 3. Connect bigrams that co-occur in the same documents"
      ],
      "context_after": [
        "    skipped_large_docs = 0",
        "",
        "    for bigram in bigrams:",
        "            skipped_large_docs += 1",
        "            continue",
        "",
        "",
        "",
        "                docs1 = b1.document_ids",
        "                docs2 = b2.document_ids",
        "                shared_docs = docs1 & docs2",
        "                jaccard = len(shared_docs) / len(docs1 | docs2)",
        "                weight = cooccurrence_weight * jaccard",
        "                add_connection(b1, b2, weight, 'cooccurrence')",
        "",
        "    return {",
        "        'connections_created': len(connected_pairs),",
        "        'bigrams': len(bigrams),",
        "        'component_connections': component_connections,",
        "        'chain_connections': chain_connections,",
        "        'cooccurrence_connections': cooccurrence_connections,"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/config.py",
      "function": "class CorticalConfig:",
      "start_line": 98,
      "lines_added": [
        "    # Scoring algorithm settings",
        "    scoring_algorithm: str = 'bm25'  # 'tfidf' or 'bm25'",
        "    bm25_k1: float = 1.2  # Term frequency saturation parameter (0.0-3.0, typical 1.2-2.0)",
        "    bm25_b: float = 0.75  # Length normalization parameter (0.0-1.0)",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "    bridge_similarity_max: float = 0.03",
        "",
        "    # Chunking settings for RAG",
        "    chunk_size: int = 512",
        "    chunk_overlap: int = 128",
        "",
        "    # Query expansion settings",
        "    max_query_expansions: int = 10",
        "    semantic_expansion_discount: float = 0.7",
        ""
      ],
      "context_after": [
        "    # Cross-layer propagation",
        "    cross_layer_damping: float = 0.7",
        "",
        "    # Bigram connection weights",
        "    bigram_component_weight: float = 0.5",
        "    bigram_chain_weight: float = 0.7",
        "    bigram_cooccurrence_weight: float = 0.3",
        "",
        "    # Concept connection thresholds",
        "    concept_min_shared_docs: int = 1"
      ],
      "change_type": "add"
    },
    {
      "file": "cortical/config.py",
      "function": "class CorticalConfig:",
      "start_line": 232,
      "lines_added": [
        "        # BM25 validation",
        "        if self.scoring_algorithm not in ('tfidf', 'bm25'):",
        "            raise ValueError(",
        "                f\"scoring_algorithm must be 'tfidf' or 'bm25', got {self.scoring_algorithm}\"",
        "            )",
        "        if not (0 <= self.bm25_k1 <= 3):",
        "            raise ValueError(",
        "                f\"bm25_k1 must be between 0 and 3, got {self.bm25_k1}\"",
        "            )",
        "        if not (0 <= self.bm25_b <= 1):",
        "            raise ValueError(",
        "                f\"bm25_b must be between 0 and 1, got {self.bm25_b}\"",
        "            )",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "            raise ValueError(",
        "                f\"semantic_expansion_discount must be between 0 and 1, got {self.semantic_expansion_discount}\"",
        "            )",
        "",
        "        # Cross-layer damping validation",
        "        if not (0 < self.cross_layer_damping < 1):",
        "            raise ValueError(",
        "                f\"cross_layer_damping must be between 0 and 1, got {self.cross_layer_damping}\"",
        "            )",
        ""
      ],
      "context_after": [
        "    def copy(self) -> 'CorticalConfig':",
        "        \"\"\"",
        "        Create a copy of this configuration.",
        "",
        "        Returns:",
        "            A new CorticalConfig instance with the same values.",
        "        \"\"\"",
        "        return CorticalConfig(",
        "            pagerank_damping=self.pagerank_damping,",
        "            pagerank_iterations=self.pagerank_iterations,"
      ],
      "change_type": "add"
    },
    {
      "file": "cortical/config.py",
      "function": "class CorticalConfig:",
      "start_line": 255,
      "lines_added": [
        "            scoring_algorithm=self.scoring_algorithm,",
        "            bm25_k1=self.bm25_k1,",
        "            bm25_b=self.bm25_b,"
      ],
      "lines_removed": [],
      "context_before": [
        "            louvain_resolution=self.louvain_resolution,",
        "            isolation_threshold=self.isolation_threshold,",
        "            well_connected_threshold=self.well_connected_threshold,",
        "            weak_topic_tfidf_threshold=self.weak_topic_tfidf_threshold,",
        "            bridge_similarity_min=self.bridge_similarity_min,",
        "            bridge_similarity_max=self.bridge_similarity_max,",
        "            chunk_size=self.chunk_size,",
        "            chunk_overlap=self.chunk_overlap,",
        "            max_query_expansions=self.max_query_expansions,",
        "            semantic_expansion_discount=self.semantic_expansion_discount,"
      ],
      "context_after": [
        "            cross_layer_damping=self.cross_layer_damping,",
        "            bigram_component_weight=self.bigram_component_weight,",
        "            bigram_chain_weight=self.bigram_chain_weight,",
        "            bigram_cooccurrence_weight=self.bigram_cooccurrence_weight,",
        "            concept_min_shared_docs=self.concept_min_shared_docs,",
        "            concept_min_jaccard=self.concept_min_jaccard,",
        "            concept_embedding_threshold=self.concept_embedding_threshold,",
        "            multihop_max_hops=self.multihop_max_hops,",
        "            multihop_decay_factor=self.multihop_decay_factor,",
        "            multihop_min_path_score=self.multihop_min_path_score,"
      ],
      "change_type": "add"
    },
    {
      "file": "cortical/config.py",
      "function": "class CorticalConfig:",
      "start_line": 294,
      "lines_added": [
        "            'scoring_algorithm': self.scoring_algorithm,",
        "            'bm25_k1': self.bm25_k1,",
        "            'bm25_b': self.bm25_b,"
      ],
      "lines_removed": [],
      "context_before": [
        "            'louvain_resolution': self.louvain_resolution,",
        "            'isolation_threshold': self.isolation_threshold,",
        "            'well_connected_threshold': self.well_connected_threshold,",
        "            'weak_topic_tfidf_threshold': self.weak_topic_tfidf_threshold,",
        "            'bridge_similarity_min': self.bridge_similarity_min,",
        "            'bridge_similarity_max': self.bridge_similarity_max,",
        "            'chunk_size': self.chunk_size,",
        "            'chunk_overlap': self.chunk_overlap,",
        "            'max_query_expansions': self.max_query_expansions,",
        "            'semantic_expansion_discount': self.semantic_expansion_discount,"
      ],
      "context_after": [
        "            'cross_layer_damping': self.cross_layer_damping,",
        "            'bigram_component_weight': self.bigram_component_weight,",
        "            'bigram_chain_weight': self.bigram_chain_weight,",
        "            'bigram_cooccurrence_weight': self.bigram_cooccurrence_weight,",
        "            'concept_min_shared_docs': self.concept_min_shared_docs,",
        "            'concept_min_jaccard': self.concept_min_jaccard,",
        "            'concept_embedding_threshold': self.concept_embedding_threshold,",
        "            'multihop_max_hops': self.multihop_max_hops,",
        "            'multihop_decay_factor': self.multihop_decay_factor,",
        "            'multihop_min_path_score': self.multihop_min_path_score,"
      ],
      "change_type": "add"
    },
    {
      "file": "cortical/processor/compute.py",
      "function": "class ComputeMixin:",
      "start_line": 653,
      "lines_added": [
        "        \"\"\"",
        "        Compute document relevance scores using the configured algorithm.",
        "",
        "        Uses the scoring_algorithm from config ('tfidf' or 'bm25').",
        "        BM25 provides improved relevance through term frequency saturation",
        "        and document length normalization.",
        "",
        "        Args:",
        "            verbose: Print progress messages",
        "        \"\"\"",
        "        if self.config.scoring_algorithm == 'bm25':",
        "            analysis.compute_bm25(",
        "                self.layers,",
        "                self.documents,",
        "                self.doc_lengths,",
        "                self.avg_doc_length,",
        "                k1=self.config.bm25_k1,",
        "                b=self.config.bm25_b",
        "            )",
        "            if verbose:",
        "                logger.info(f\"Computed BM25 scores (k1={self.config.bm25_k1}, b={self.config.bm25_b})\")",
        "        else:",
        "            analysis.compute_tfidf(self.layers, self.documents)",
        "            if verbose:",
        "                logger.info(\"Computed TF-IDF scores\")",
        "",
        "    @timed(\"compute_bm25\")",
        "    def compute_bm25(",
        "        self,",
        "        k1: float = None,",
        "        b: float = None,",
        "        verbose: bool = True",
        "    ) -> None:",
        "        \"\"\"",
        "        Compute BM25 scores for document relevance ranking.",
        "",
        "        BM25 (Best Match 25) improves on TF-IDF by:",
        "        - Term frequency saturation: diminishing returns for repeated terms",
        "        - Document length normalization: fair comparison across lengths",
        "",
        "        Args:",
        "            k1: Term frequency saturation (0-3). Default from config (1.2)",
        "            b: Length normalization (0-1). Default from config (0.75)",
        "            verbose: Print progress messages",
        "        \"\"\"",
        "        k1 = k1 if k1 is not None else self.config.bm25_k1",
        "        b = b if b is not None else self.config.bm25_b",
        "",
        "        analysis.compute_bm25(",
        "            self.layers,",
        "            self.documents,",
        "            self.doc_lengths,",
        "            self.avg_doc_length,",
        "            k1=k1,",
        "            b=b",
        "        )",
        "            logger.info(f\"Computed BM25 scores (k1={k1}, b={b})\")"
      ],
      "lines_removed": [
        "        analysis.compute_tfidf(self.layers, self.documents)",
        "            logger.info(\"Computed TF-IDF scores\")"
      ],
      "context_before": [
        "        )",
        "",
        "        if verbose:",
        "            status = \"converged\" if result['converged'] else \"did not converge\"",
        "            logger.info(f\"Computed hierarchical PageRank ({result['iterations_run']} iterations, {status})\")",
        "",
        "        return result",
        "",
        "    @timed(\"compute_tfidf\")",
        "    def compute_tfidf(self, verbose: bool = True) -> None:"
      ],
      "context_after": [
        "        if verbose:",
        "",
        "    @timed(\"compute_document_connections\")",
        "    def compute_document_connections(self, min_shared_terms: int = 3, verbose: bool = True) -> None:",
        "        analysis.compute_document_connections(self.layers, self.documents, min_shared_terms)",
        "        if verbose:",
        "            logger.info(\"Computed document connections\")",
        "",
        "    @timed(\"compute_bigram_connections\")",
        "    def compute_bigram_connections(",
        "        self,"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/processor/core.py",
      "function": "class CoreMixin:",
      "start_line": 54,
      "lines_added": [
        "        # Document length tracking for BM25",
        "        self.doc_lengths: Dict[str, int] = {}  # doc_id -> token count",
        "        self.avg_doc_length: float = 0.0  # Average document length in tokens"
      ],
      "lines_removed": [],
      "context_before": [
        "        self.layers: Dict[CorticalLayer, HierarchicalLayer] = {",
        "            CorticalLayer.TOKENS: HierarchicalLayer(CorticalLayer.TOKENS),",
        "            CorticalLayer.BIGRAMS: HierarchicalLayer(CorticalLayer.BIGRAMS),",
        "            CorticalLayer.CONCEPTS: HierarchicalLayer(CorticalLayer.CONCEPTS),",
        "            CorticalLayer.DOCUMENTS: HierarchicalLayer(CorticalLayer.DOCUMENTS),",
        "        }",
        "        self.documents: Dict[str, str] = {}",
        "        self.document_metadata: Dict[str, Dict[str, Any]] = {}",
        "        self.embeddings: Dict[str, list] = {}",
        "        self.semantic_relations: list = []"
      ],
      "context_after": [
        "        # Track which computations are stale and need recomputation",
        "        self._stale_computations: set = set()",
        "        # LRU cache for query expansion results",
        "        self._query_expansion_cache: Dict[str, Dict[str, float]] = {}",
        "        self._query_cache_max_size: int = 100",
        "        # Observability: metrics collection",
        "        self._metrics = MetricsCollector(enabled=enable_metrics)",
        "",
        "    def _mark_all_stale(self) -> None:",
        "        \"\"\"Mark all computations as stale (needing recomputation).\"\"\""
      ],
      "change_type": "add"
    },
    {
      "file": "cortical/processor/documents.py",
      "function": "class DocumentsMixin:",
      "start_line": 55,
      "lines_added": [
        "        # Track document length for BM25",
        "        self.doc_lengths[doc_id] = len(tokens)",
        "        # Update average document length",
        "        if self.doc_lengths:",
        "            self.avg_doc_length = sum(self.doc_lengths.values()) / len(self.doc_lengths)",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "",
        "        # Store metadata if provided",
        "        if metadata:",
        "            self.document_metadata[doc_id] = metadata.copy()",
        "        elif doc_id not in self.document_metadata:",
        "            self.document_metadata[doc_id] = {}",
        "",
        "        tokens = self.tokenizer.tokenize(content)",
        "        bigrams = self.tokenizer.extract_ngrams(tokens, n=2)",
        ""
      ],
      "context_after": [
        "        layer0 = self.layers[CorticalLayer.TOKENS]",
        "        layer1 = self.layers[CorticalLayer.BIGRAMS]",
        "        layer3 = self.layers[CorticalLayer.DOCUMENTS]",
        "",
        "        doc_col = layer3.get_or_create_minicolumn(doc_id)",
        "        doc_col.occurrence_count += 1",
        "        # Cache tokenized document name for fast doc_name_boost in search",
        "        # This avoids re-tokenizing the doc_id on every query",
        "        doc_col.name_tokens = set(self.tokenizer.tokenize(doc_id.replace('_', ' ')))",
        ""
      ],
      "change_type": "add"
    },
    {
      "file": "cortical/processor/persistence_api.py",
      "function": "class PersistenceMixin:",
      "start_line": 42,
      "lines_added": [
        "            'config': self.config.to_dict(),",
        "            'doc_lengths': self.doc_lengths,",
        "            'avg_doc_length': self.avg_doc_length"
      ],
      "lines_removed": [
        "            'config': self.config.to_dict()"
      ],
      "context_before": [
        "",
        "        Args:",
        "            filepath: Path to save file",
        "            verbose: Print progress",
        "            signing_key: Optional HMAC key for signing pickle files (SEC-003).",
        "                If provided, creates a .sig file alongside the pickle file.",
        "        \"\"\"",
        "        metadata = {",
        "            'has_embeddings': bool(self.embeddings),",
        "            'has_relations': bool(self.semantic_relations),"
      ],
      "context_after": [
        "        }",
        "        persistence.save_processor(",
        "            filepath,",
        "            self.layers,",
        "            self.documents,",
        "            self.document_metadata,",
        "            self.embeddings,",
        "            self.semantic_relations,",
        "            metadata,",
        "            verbose,"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/processor/persistence_api.py",
      "function": "class PersistenceMixin:",
      "start_line": 95,
      "lines_added": [
        "",
        "        # Restore BM25 document length data if available",
        "        if metadata:",
        "            processor.doc_lengths = metadata.get('doc_lengths', {})",
        "            processor.avg_doc_length = metadata.get('avg_doc_length', 0.0)",
        "",
        "        # Recompute doc_lengths if not in metadata (backward compatibility)",
        "        if not processor.doc_lengths and processor.documents:",
        "            from ..tokenizer import Tokenizer",
        "            tokenizer = processor.tokenizer if hasattr(processor, 'tokenizer') else Tokenizer()",
        "            for doc_id, content in processor.documents.items():",
        "                tokens = tokenizer.tokenize(content)",
        "                processor.doc_lengths[doc_id] = len(tokens)",
        "            if processor.doc_lengths:",
        "                processor.avg_doc_length = sum(processor.doc_lengths.values()) / len(processor.doc_lengths)",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "                config = CorticalConfig.from_dict(metadata['config'])",
        "            except (KeyError, TypeError):",
        "                config = None",
        "",
        "        processor = cls(config=config)",
        "        processor.layers = layers",
        "        processor.documents = documents",
        "        processor.document_metadata = document_metadata",
        "        processor.embeddings = embeddings",
        "        processor.semantic_relations = semantic_relations"
      ],
      "context_after": [
        "        return processor",
        "",
        "    def save_json(self, state_dir: str, force: bool = False, verbose: bool = True) -> Dict[str, bool]:",
        "        \"\"\"",
        "        Save processor state to git-friendly JSON format.",
        "",
        "        Instead of a single monolithic pickle file, creates a directory with:",
        "        - manifest.json: Version, checksums, staleness tracking",
        "        - documents.json: Document content and metadata",
        "        - layers/*.json: One file per layer"
      ],
      "change_type": "add"
    },
    {
      "file": "cortical/processor/query_api.py",
      "function": "class QueryMixin:",
      "start_line": 380,
      "lines_added": [
        "    def graph_boosted_search(",
        "        self,",
        "        query_text: str,",
        "        top_n: int = 5,",
        "        pagerank_weight: float = 0.3,",
        "        proximity_weight: float = 0.2,",
        "        use_expansion: bool = True",
        "    ) -> List[Tuple[str, float]]:",
        "        \"\"\"",
        "        Graph-Boosted BM25 (GB-BM25): Hybrid scoring combining BM25 with graph signals.",
        "",
        "        This algorithm combines multiple signals for improved code search:",
        "        1. BM25/TF-IDF base score (term relevance)",
        "        2. PageRank boost (matched term importance)",
        "        3. Proximity boost (query terms connected in graph)",
        "        4. Coverage boost (documents with more unique query term matches)",
        "",
        "        Args:",
        "            query_text: Search query",
        "            top_n: Number of results to return",
        "            pagerank_weight: Weight for PageRank boost (0-1, default 0.3)",
        "            proximity_weight: Weight for term proximity boost (0-1, default 0.2)",
        "            use_expansion: Whether to use query expansion",
        "",
        "        Returns:",
        "            List of (doc_id, score) tuples ranked by combined relevance",
        "",
        "        Raises:",
        "            ValueError: If query_text is empty or parameters are invalid",
        "        \"\"\"",
        "        if not isinstance(query_text, str) or not query_text.strip():",
        "            raise ValueError(\"query_text must be a non-empty string\")",
        "        if not isinstance(top_n, int) or top_n < 1:",
        "            raise ValueError(\"top_n must be a positive integer\")",
        "        if not 0 <= pagerank_weight <= 1:",
        "            raise ValueError(\"pagerank_weight must be between 0 and 1\")",
        "        if not 0 <= proximity_weight <= 1:",
        "            raise ValueError(\"proximity_weight must be between 0 and 1\")",
        "",
        "        return query_module.graph_boosted_search(",
        "            query_text,",
        "            self.layers,",
        "            self.tokenizer,",
        "            top_n=top_n,",
        "            pagerank_weight=pagerank_weight,",
        "            proximity_weight=proximity_weight,",
        "            use_expansion=use_expansion,",
        "            semantic_relations=self.semantic_relations",
        "        )",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "",
        "        return query_module.fast_find_documents(",
        "            query_text,",
        "            self.layers,",
        "            self.tokenizer,",
        "            top_n=top_n,",
        "            candidate_multiplier=candidate_multiplier,",
        "            use_code_concepts=use_code_concepts",
        "        )",
        ""
      ],
      "context_after": [
        "    def quick_search(self, query: str, top_n: int = 5) -> List[str]:",
        "        \"\"\"",
        "        One-call document search with sensible defaults.",
        "",
        "        Args:",
        "            query: Search query string",
        "            top_n: Number of results to return (default 5)",
        "",
        "        Returns:",
        "            List of document IDs ranked by relevance"
      ],
      "change_type": "add"
    },
    {
      "file": "cortical/query/__init__.py",
      "function": "from .expansion import (",
      "start_line": 59,
      "lines_added": [
        "    graph_boosted_search,"
      ],
      "lines_removed": [],
      "context_before": [
        ")",
        "",
        "# Document search",
        "from .search import (",
        "    find_documents_for_query,",
        "    fast_find_documents,",
        "    build_document_index,",
        "    search_with_index,",
        "    query_with_spreading_activation,",
        "    find_related_documents,"
      ],
      "context_after": [
        ")",
        "",
        "# Document type boosting and ranking",
        "from .ranking import (",
        "    DOC_TYPE_BOOSTS,",
        "    CONCEPTUAL_KEYWORDS,",
        "    IMPLEMENTATION_KEYWORDS,",
        "    is_conceptual_query,",
        "    get_doc_type_boost,",
        "    apply_doc_type_boost,"
      ],
      "change_type": "add"
    },
    {
      "file": "cortical/query/__init__.py",
      "function": "__all__ = [",
      "start_line": 136,
      "lines_added": [
        "    'graph_boosted_search',"
      ],
      "lines_removed": [],
      "context_before": [
        "    'expand_query_semantic',",
        "    'expand_query_multihop',",
        "    'get_expanded_query_terms',",
        "    # Search",
        "    'find_documents_for_query',",
        "    'fast_find_documents',",
        "    'build_document_index',",
        "    'search_with_index',",
        "    'query_with_spreading_activation',",
        "    'find_related_documents',"
      ],
      "context_after": [
        "    # Ranking",
        "    'DOC_TYPE_BOOSTS',",
        "    'CONCEPTUAL_KEYWORDS',",
        "    'IMPLEMENTATION_KEYWORDS',",
        "    'is_conceptual_query',",
        "    'get_doc_type_boost',",
        "    'apply_doc_type_boost',",
        "    'find_documents_with_boost',",
        "    'find_relevant_concepts',",
        "    'multi_stage_rank',"
      ],
      "change_type": "add"
    },
    {
      "file": "cortical/query/search.py",
      "function": "def find_related_documents(",
      "start_line": 413,
      "lines_added": [
        "",
        "",
        "def graph_boosted_search(",
        "    query_text: str,",
        "    layers: Dict[CorticalLayer, HierarchicalLayer],",
        "    tokenizer: Tokenizer,",
        "    top_n: int = 5,",
        "    pagerank_weight: float = 0.3,",
        "    proximity_weight: float = 0.2,",
        "    use_expansion: bool = True,",
        "    semantic_relations: Optional[List[Tuple[str, str, str, float]]] = None",
        ") -> List[Tuple[str, float]]:",
        "    \"\"\"",
        "    Graph-Boosted BM25 (GB-BM25): Hybrid scoring combining BM25 with graph signals.",
        "",
        "    This creative algorithm combines multiple signals:",
        "    1. BM25/TF-IDF base score (term relevance)",
        "    2. PageRank boost (matched term importance)",
        "    3. Proximity boost (query terms connected in graph)",
        "    4. Coverage boost (documents with more unique query term matches)",
        "",
        "    This approach is designed for code search where:",
        "    - Important functions/classes should rank higher (PageRank)",
        "    - Related concepts should boost each other (graph connections)",
        "    - Comprehensive matches beat partial matches (coverage)",
        "",
        "    Args:",
        "        query_text: Search query",
        "        layers: Dictionary of layers",
        "        tokenizer: Tokenizer instance",
        "        top_n: Number of results to return",
        "        pagerank_weight: Weight for PageRank boost (0-1, default 0.3)",
        "        proximity_weight: Weight for term proximity boost (0-1, default 0.2)",
        "        use_expansion: Whether to use query expansion",
        "        semantic_relations: Optional semantic relations for expansion",
        "",
        "    Returns:",
        "        List of (doc_id, score) tuples ranked by combined relevance",
        "    \"\"\"",
        "    layer0 = layers[CorticalLayer.TOKENS]",
        "    layer3 = layers[CorticalLayer.DOCUMENTS]",
        "",
        "    # Get expanded query terms",
        "    query_terms = get_expanded_query_terms(",
        "        query_text, layers, tokenizer,",
        "        use_expansion=use_expansion,",
        "        semantic_relations=semantic_relations,",
        "        use_semantic=True",
        "    )",
        "",
        "    if not query_terms:",
        "        return []",
        "",
        "    # Phase 1: Compute base BM25/TF-IDF scores per document",
        "    doc_scores: Dict[str, float] = defaultdict(float)",
        "    doc_term_matches: Dict[str, set] = defaultdict(set)  # Track unique term matches",
        "    doc_pagerank_sum: Dict[str, float] = defaultdict(float)  # Sum of PageRank for matched terms",
        "",
        "    for term, term_weight in query_terms.items():",
        "        col = layer0.get_minicolumn(term)",
        "        if col:",
        "            # Get term's PageRank importance",
        "            term_pagerank = getattr(col, 'pagerank', 0.0) or 0.0",
        "",
        "            for doc_id in col.document_ids:",
        "                # Base BM25/TF-IDF score",
        "                tfidf = col.tfidf_per_doc.get(doc_id, col.tfidf)",
        "                doc_scores[doc_id] += tfidf * term_weight",
        "",
        "                # Track term match for coverage",
        "                doc_term_matches[doc_id].add(term)",
        "",
        "                # Accumulate PageRank boost",
        "                doc_pagerank_sum[doc_id] += term_pagerank * term_weight",
        "",
        "    if not doc_scores:",
        "        return []",
        "",
        "    # Phase 2: Compute proximity boost using lateral connections",
        "    # Boost documents where query terms are connected in the graph",
        "    proximity_scores: Dict[str, float] = defaultdict(float)",
        "",
        "    original_tokens = tokenizer.tokenize(query_text)",
        "    if len(original_tokens) > 1:",
        "        # Check if query terms have lateral connections to each other",
        "        for i, t1 in enumerate(original_tokens):",
        "            col1 = layer0.get_minicolumn(t1)",
        "            if not col1:",
        "                continue",
        "",
        "            for t2 in original_tokens[i+1:]:",
        "                col2 = layer0.get_minicolumn(t2)",
        "                if not col2:",
        "                    continue",
        "",
        "                # Check for connection between terms",
        "                conn_weight = col1.lateral_connections.get(col2.id, 0.0)",
        "                if conn_weight > 0:",
        "                    # Boost documents containing both terms",
        "                    shared_docs = col1.document_ids & col2.document_ids",
        "                    for doc_id in shared_docs:",
        "                        proximity_scores[doc_id] += conn_weight",
        "",
        "    # Phase 3: Combine all signals",
        "    max_base_score = max(doc_scores.values()) if doc_scores else 1.0",
        "    max_pagerank = max(doc_pagerank_sum.values()) if doc_pagerank_sum else 1.0",
        "    max_proximity = max(proximity_scores.values()) if proximity_scores else 1.0",
        "",
        "    final_scores: Dict[str, float] = {}",
        "    num_query_terms = len(set(original_tokens))",
        "",
        "    for doc_id, base_score in doc_scores.items():",
        "        # Normalize base score",
        "        norm_base = base_score / max_base_score if max_base_score > 0 else 0",
        "",
        "        # Normalize PageRank boost",
        "        pagerank_boost = doc_pagerank_sum.get(doc_id, 0.0)",
        "        norm_pagerank = pagerank_boost / max_pagerank if max_pagerank > 0 else 0",
        "",
        "        # Normalize proximity boost",
        "        prox_boost = proximity_scores.get(doc_id, 0.0)",
        "        norm_proximity = prox_boost / max_proximity if max_proximity > 0 else 0",
        "",
        "        # Coverage boost: reward documents matching more unique query terms",
        "        coverage = len(doc_term_matches.get(doc_id, set())) / num_query_terms if num_query_terms > 0 else 0",
        "",
        "        # Combine signals with weights",
        "        # Base score dominates, with boosts from graph signals",
        "        combined = (",
        "            (1 - pagerank_weight - proximity_weight) * norm_base +",
        "            pagerank_weight * norm_pagerank +",
        "            proximity_weight * norm_proximity",
        "        )",
        "",
        "        # Apply coverage multiplier (0.5 to 1.5 range)",
        "        coverage_mult = 0.5 + coverage",
        "",
        "        # Final score preserves relative magnitude",
        "        final_scores[doc_id] = combined * coverage_mult * max_base_score",
        "",
        "    sorted_docs = sorted(final_scores.items(), key=lambda x: -x[1])",
        "    return sorted_docs[:top_n]"
      ],
      "lines_removed": [],
      "context_before": [
        "        return []",
        "",
        "    related = []",
        "    for neighbor_id, weight in col.lateral_connections.items():",
        "        # Use O(1) ID lookup instead of linear search",
        "        neighbor = layer3.get_by_id(neighbor_id)",
        "        if neighbor:",
        "            related.append((neighbor.content, weight))",
        "",
        "    return sorted(related, key=lambda x: -x[1])"
      ],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "cortical/semantics.py",
      "function": "def extract_corpus_semantics(",
      "start_line": 309,
      "lines_added": [
        "        # Pre-filter terms by minimum context keys AND TF-IDF importance",
        "        tfidf_scores: Dict[str, float] = {}",
        "            # Get TF-IDF score for importance ranking",
        "            col = layer0.get_minicolumn(term)",
        "            tfidf_scores[term] = col.tfidf if col else 0.0",
        "        # Get filtered terms with enough context, sorted by importance",
        "        # OPTIMIZATION: Sort by TF-IDF importance and limit to top terms",
        "        # This focuses similarity computation on the most important terms",
        "        filtered_terms.sort(key=lambda t: tfidf_scores.get(t, 0), reverse=True)",
        "",
        "        # Limit to top N important terms to avoid O(n²) explosion",
        "        # sqrt(max_similarity_pairs) gives balanced coverage",
        "        max_terms = int(math.sqrt(max_similarity_pairs * 2)) if max_similarity_pairs > 0 else len(filtered_terms)",
        "        filtered_terms = filtered_terms[:max_terms]",
        ""
      ],
      "lines_removed": [
        "        # Pre-filter terms by minimum context keys",
        "        # Get filtered terms with enough context"
      ],
      "context_before": [
        "        for i in range(n_terms):",
        "            row_i = nonzero_counts[i]",
        "            for j in range(i + 1, n_terms):",
        "                if similarities[i, j] > 0.3:",
        "                    common_count = np.sum(row_i & nonzero_counts[j])",
        "                    if common_count >= 3:",
        "                        relations.append((terms[i], 'SimilarTo', terms[j], float(similarities[i, j])))",
        "",
        "    elif n_terms > 1:",
        "        # Fallback: pure Python implementation with optimizations"
      ],
      "context_after": [
        "        key_sets: Dict[str, set] = {}",
        "        magnitudes: Dict[str, float] = {}",
        "",
        "        for term in terms:",
        "            vec = context_vectors[term]",
        "            keys = set(vec.keys())",
        "            # Skip terms with too few context keys (can't meet min_context_keys threshold)",
        "            if len(keys) < min_context_keys:",
        "                continue",
        "            key_sets[term] = keys",
        "            mag = math.sqrt(sum(v * v for v in vec.values()))",
        "            magnitudes[term] = mag",
        "",
        "        filtered_terms = [t for t in terms if t in key_sets and magnitudes.get(t, 0) > 0]",
        "",
        "        # Track pairs checked for early termination",
        "        pairs_checked = 0",
        "",
        "        for i, t1 in enumerate(filtered_terms):",
        "            vec1 = context_vectors[t1]",
        "            mag1 = magnitudes[t1]",
        "            keys1 = key_sets[t1]",
        "",
        "            for t2 in filtered_terms[i+1:]:",
        "                # Check pair limit"
      ],
      "change_type": "modify"
    },
    {
      "file": "docs/knowledge-transfer-bm25-optimization.md",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "# Knowledge Transfer: BM25 Implementation & Performance Optimization",
        "",
        "**Date:** 2025-12-15",
        "**Author:** Claude (AI Assistant)",
        "**Branch:** `claude/explore-tfidf-alternatives-FKYpq`",
        "**Status:** Ready for PR",
        "",
        "---",
        "",
        "## Executive Summary",
        "",
        "This document captures the complete knowledge transfer for the BM25 scoring algorithm implementation and performance optimizations made to the Cortical Text Processor. The changes improve `compute_all()` performance by **34.5%** while adding a new hybrid search algorithm (GB-BM25).",
        "",
        "---",
        "",
        "## Table of Contents",
        "",
        "1. [Background & Motivation](#background--motivation)",
        "2. [BM25 Implementation](#bm25-implementation)",
        "3. [Performance Optimizations](#performance-optimizations)",
        "4. [Graph-Boosted Search (GB-BM25)](#graph-boosted-search-gb-bm25)",
        "5. [Code Locations](#code-locations)",
        "6. [Configuration Reference](#configuration-reference)",
        "7. [Testing Strategy](#testing-strategy)",
        "8. [Performance Benchmarks](#performance-benchmarks)",
        "9. [Trade-offs & Limitations](#trade-offs--limitations)",
        "10. [Future Work](#future-work)",
        "11. [Troubleshooting Guide](#troubleshooting-guide)",
        "",
        "---",
        "",
        "## Background & Motivation",
        "",
        "### Why Replace TF-IDF?",
        "",
        "TF-IDF has limitations for code search:",
        "",
        "1. **No term frequency saturation**: TF-IDF keeps increasing linearly with term frequency, over-weighting repeated terms",
        "2. **No document length normalization**: Long files get unfairly boosted",
        "3. **Single document IDF issue**: `log(N/df)` returns 0 when a term appears in only one document",
        "",
        "### Why BM25?",
        "",
        "BM25 (Best Match 25) addresses these issues:",
        "",
        "1. **Term frequency saturation** via `k1` parameter: Diminishing returns for repeated terms",
        "2. **Document length normalization** via `b` parameter: Adjusts scores based on document length",
        "3. **Better IDF formula**: `log((N - df + 0.5) / (df + 0.5) + 1)` never returns 0",
        "",
        "### Research Process",
        "",
        "Alternatives considered:",
        "- **BM25** (selected) - Best balance of effectiveness and simplicity",
        "- **BM25F** - Field-weighted variant, more complex, deferred for future",
        "- **Language Models with Dirichlet Smoothing** - More complex, less interpretable",
        "- **DFR (Divergence From Randomness)** - More parameters, harder to tune",
        "- **Pivoted Length Normalization** - Less widely adopted",
        "",
        "---",
        "",
        "## BM25 Implementation",
        "",
        "### Core Algorithm",
        "",
        "Located in `cortical/analysis.py`:",
        "",
        "```python",
        "def _bm25_core(",
        "    term_stats: Dict[str, Tuple[int, int, Dict[str, int]]],",
        "    num_docs: int,",
        "    doc_lengths: Dict[str, int],",
        "    avg_doc_length: float,",
        "    k1: float = 1.2,",
        "    b: float = 0.75",
        ") -> Dict[str, Tuple[float, Dict[str, float]]]:",
        "```",
        "",
        "**BM25 Formula:**",
        "",
        "```",
        "score(D, Q) = Σ IDF(qi) * (f(qi, D) * (k1 + 1)) / (f(qi, D) + k1 * (1 - b + b * |D|/avgdl))",
        "```",
        "",
        "Where:",
        "- `f(qi, D)` = frequency of term qi in document D",
        "- `|D|` = length of document D (in tokens)",
        "- `avgdl` = average document length across corpus",
        "- `k1` = term frequency saturation parameter (default 1.2)",
        "- `b` = length normalization parameter (default 0.75)",
        "",
        "**IDF Formula:**",
        "",
        "```",
        "IDF(qi) = log((N - n(qi) + 0.5) / (n(qi) + 0.5) + 1)",
        "```",
        "",
        "Where:",
        "- `N` = total number of documents",
        "- `n(qi)` = number of documents containing term qi",
        "",
        "### Document Length Tracking",
        "",
        "New fields added to processor (`cortical/processor/core.py`):",
        "",
        "```python",
        "self.doc_lengths: Dict[str, int] = {}  # doc_id -> token count",
        "self.avg_doc_length: float = 0.0       # Average across corpus",
        "```",
        "",
        "Updated in `process_document()` (`cortical/processor/documents.py`):",
        "",
        "```python",
        "self.doc_lengths[doc_id] = len(tokens)",
        "self.avg_doc_length = sum(self.doc_lengths.values()) / len(self.doc_lengths)",
        "```",
        "",
        "### Persistence",
        "",
        "Document lengths are saved/loaded with the processor state (`cortical/processor/persistence_api.py`):",
        "",
        "```python",
        "# Save",
        "metadata = {",
        "    'doc_lengths': self.doc_lengths,",
        "    'avg_doc_length': self.avg_doc_length,",
        "    ...",
        "}",
        "",
        "# Load (with backward compatibility)",
        "processor.doc_lengths = metadata.get('doc_lengths', {})",
        "processor.avg_doc_length = metadata.get('avg_doc_length', 0.0)",
        "```",
        "",
        "### Backward Compatibility",
        "",
        "- Old pickle files without `doc_lengths` are handled gracefully",
        "- `scoring_algorithm='tfidf'` still works for legacy behavior",
        "- Scores are stored in the same `col.tfidf` and `col.tfidf_per_doc` fields",
        "",
        "---",
        "",
        "## Performance Optimizations",
        "",
        "### Bottleneck Analysis",
        "",
        "Initial profiling revealed the real bottlenecks:",
        "",
        "| Phase | Before | After | Reduction |",
        "|-------|--------|-------|-----------|",
        "| `compute_bigram_connections` | 6,067ms | 3,055ms | 50% |",
        "| `extract_corpus_semantics` | 2,448ms | ~2,400ms | ~2% |",
        "| `compute_tfidf` (BM25) | 8.5ms | 6.8ms | 20% |",
        "| **Total `compute_all()`** | **7,546ms** | **4,946ms** | **34.5%** |",
        "",
        "**Key insight**: BM25 itself was already fast. The real bottlenecks were in bigram connections and semantic extraction.",
        "",
        "### Optimization 1: Inverted Index for Co-occurrence",
        "",
        "**Problem**: Original code used O(n²) sparse matrix multiplication.",
        "",
        "**Solution**: Replace with O(n) inverted index approach.",
        "",
        "**Before** (`cortical/analysis.py`):",
        "```python",
        "# O(n²) matrix multiplication",
        "doc_term_matrix = SparseMatrix(len(doc_to_row), len(bigrams))",
        "cooccur_matrix = doc_term_matrix.multiply_transpose()  # SLOW!",
        "```",
        "",
        "**After**:",
        "```python",
        "# O(n) inverted index",
        "doc_to_bigrams: Dict[str, List[Minicolumn]] = defaultdict(list)",
        "for bigram in bigrams:",
        "    for doc_id in bigram.document_ids:",
        "        doc_to_bigrams[doc_id].append(bigram)",
        "",
        "# Process each document's pairs directly",
        "for doc_id, doc_bigrams in doc_to_bigrams.items():",
        "    for i, b1 in enumerate(doc_bigrams):",
        "        for b2 in doc_bigrams[i+1:]:",
        "            # Connect pair",
        "```",
        "",
        "### Optimization 2: Importance-Based Filtering",
        "",
        "**Problem**: Too many low-value connections computed.",
        "",
        "**Solution**: Filter to important bigrams only (TF-IDF ≥ 25th percentile).",
        "",
        "```python",
        "# Compute importance threshold",
        "tfidf_values = [b.tfidf for b in bigrams if b.tfidf > 0]",
        "importance_threshold = sorted(tfidf_values)[len(tfidf_values) // 4]",
        "",
        "# Filter to important bigrams",
        "important_bigrams = [b for b in doc_bigrams if b.tfidf >= importance_threshold]",
        "```",
        "",
        "**Impact**: Reduces pair count quadratically (filtering 75% of bigrams reduces pairs by ~94%).",
        "",
        "### Optimization 3: Early Termination",
        "",
        "**Problem**: Bigrams at connection limit still being processed.",
        "",
        "**Solution**: Skip bigrams that have reached `max_connections_per_bigram`.",
        "",
        "```python",
        "for i, b1 in enumerate(important_bigrams):",
        "    if connection_counts[b1.id] >= max_connections_per_bigram:",
        "        continue  # Skip - already at limit",
        "```",
        "",
        "### Optimization 4: Semantic Similarity Term Limiting",
        "",
        "**Problem**: O(n²) similarity computation for all terms.",
        "",
        "**Solution**: Limit to top N terms by TF-IDF importance.",
        "",
        "```python",
        "# Sort by importance and limit",
        "filtered_terms.sort(key=lambda t: tfidf_scores.get(t, 0), reverse=True)",
        "max_terms = int(math.sqrt(max_similarity_pairs * 2))",
        "filtered_terms = filtered_terms[:max_terms]",
        "```",
        "",
        "---",
        "",
        "## Graph-Boosted Search (GB-BM25)",
        "",
        "### Concept",
        "",
        "A hybrid scoring algorithm combining BM25 with graph signals for improved code search.",
        "",
        "### Location",
        "",
        "`cortical/query/search.py:graph_boosted_search()`",
        "",
        "### Algorithm",
        "",
        "**Phase 1: Base BM25 Scoring**",
        "```python",
        "for term, term_weight in query_terms.items():",
        "    col = layer0.get_minicolumn(term)",
        "    if col:",
        "        for doc_id in col.document_ids:",
        "            tfidf = col.tfidf_per_doc.get(doc_id, col.tfidf)",
        "            doc_scores[doc_id] += tfidf * term_weight",
        "```",
        "",
        "**Phase 2: PageRank Boost**",
        "```python",
        "term_pagerank = getattr(col, 'pagerank', 0.0) or 0.0",
        "doc_pagerank_sum[doc_id] += term_pagerank * term_weight",
        "```",
        "",
        "**Phase 3: Proximity Boost**",
        "```python",
        "# Check if query terms are connected in the graph",
        "conn_weight = col1.lateral_connections.get(col2.id, 0.0)",
        "if conn_weight > 0:",
        "    for doc_id in col1.document_ids & col2.document_ids:",
        "        proximity_scores[doc_id] += conn_weight",
        "```",
        "",
        "**Phase 4: Coverage Boost**",
        "```python",
        "coverage = len(doc_term_matches[doc_id]) / num_query_terms",
        "coverage_mult = 0.5 + coverage  # Range: 0.5 to 1.5",
        "```",
        "",
        "**Final Score Combination**:",
        "```python",
        "combined = (",
        "    (1 - pagerank_weight - proximity_weight) * norm_base +",
        "    pagerank_weight * norm_pagerank +",
        "    proximity_weight * norm_proximity",
        ")",
        "final_score = combined * coverage_mult * max_base_score",
        "```",
        "",
        "### Usage",
        "",
        "```python",
        "# Basic usage",
        "results = processor.graph_boosted_search(\"query\")",
        "",
        "# With custom weights",
        "results = processor.graph_boosted_search(",
        "    \"query\",",
        "    pagerank_weight=0.3,   # Higher = more PageRank influence",
        "    proximity_weight=0.2   # Higher = more proximity influence",
        ")",
        "```",
        "",
        "### When to Use",
        "",
        "| Scenario | Recommended Method |",
        "|----------|-------------------|",
        "| General search | `find_documents_for_query()` |",
        "| Speed-critical | `fast_find_documents()` |",
        "| Code search with importance | `graph_boosted_search()` |",
        "| Repeated queries | `search_with_index()` |",
        "",
        "---",
        "",
        "## Code Locations",
        "",
        "### Core Implementation",
        "",
        "| File | Purpose |",
        "|------|---------|",
        "| `cortical/config.py` | BM25 parameters: `scoring_algorithm`, `bm25_k1`, `bm25_b` |",
        "| `cortical/analysis.py:_bm25_core()` | Pure BM25 algorithm |",
        "| `cortical/analysis.py:compute_bm25()` | BM25 wrapper for layers |",
        "| `cortical/analysis.py:compute_bigram_connections()` | Optimized connections |",
        "| `cortical/processor/core.py` | `doc_lengths`, `avg_doc_length` fields |",
        "| `cortical/processor/documents.py` | Document length tracking |",
        "| `cortical/processor/compute.py:compute_tfidf()` | Algorithm dispatch |",
        "| `cortical/processor/persistence_api.py` | Save/load doc_lengths |",
        "| `cortical/query/search.py:graph_boosted_search()` | GB-BM25 algorithm |",
        "| `cortical/processor/query_api.py` | Processor API wrapper |",
        "| `cortical/semantics.py` | Optimized similarity extraction |",
        "",
        "### Tests",
        "",
        "| File | Coverage |",
        "|------|----------|",
        "| `tests/unit/test_query_search.py` | `graph_boosted_search()` tests |",
        "| `tests/test_edge_cases.py` | Algorithm-aware scoring tests |",
        "",
        "### Documentation",
        "",
        "| File | Content |",
        "|------|---------|",
        "| `CLAUDE.md` | Scoring Algorithms section |",
        "| `docs/knowledge-transfer-bm25-optimization.md` | This document |",
        "| `benchmarks/BASELINE_SUMMARY.md` | Performance comparison |",
        "",
        "---",
        "",
        "## Configuration Reference",
        "",
        "### CorticalConfig Parameters",
        "",
        "```python",
        "from cortical.config import CorticalConfig",
        "",
        "config = CorticalConfig(",
        "    # Scoring algorithm selection",
        "    scoring_algorithm='bm25',  # 'bm25' (default) or 'tfidf'",
        "",
        "    # BM25 parameters",
        "    bm25_k1=1.2,  # Term frequency saturation (0.0-3.0)",
        "                  # Higher = more weight to term frequency",
        "                  # Lower = faster saturation",
        "",
        "    bm25_b=0.75,  # Length normalization (0.0-1.0)",
        "                  # 1.0 = full normalization",
        "                  # 0.0 = no normalization (treat all docs equally)",
        ")",
        "```",
        "",
        "### Recommended Settings",
        "",
        "| Use Case | k1 | b | Notes |",
        "|----------|-----|---|-------|",
        "| General code search | 1.2 | 0.75 | Default, balanced |",
        "| Short documents | 1.5 | 0.5 | Less length penalty |",
        "| Long documents | 1.0 | 0.9 | More length penalty |",
        "| Exact matching focus | 0.5 | 0.75 | Quick saturation |",
        "",
        "### Graph-Boosted Search Parameters",
        "",
        "```python",
        "results = processor.graph_boosted_search(",
        "    query,",
        "    top_n=5,                 # Number of results",
        "    pagerank_weight=0.3,     # 0-1, importance boost weight",
        "    proximity_weight=0.2,    # 0-1, connection boost weight",
        "    use_expansion=True       # Query expansion enabled",
        ")",
        "```",
        "",
        "---",
        "",
        "## Testing Strategy",
        "",
        "### Unit Tests Added",
        "",
        "1. **`test_basic_search`**: Verifies ranking with multiple terms",
        "2. **`test_empty_query`**: Empty query returns empty results",
        "3. **`test_no_matching_terms`**: Unknown terms return empty results",
        "4. **`test_pagerank_boost`**: High-PageRank terms boost documents",
        "5. **`test_respects_top_n`**: Result count limited correctly",
        "",
        "### Verification Commands",
        "",
        "```bash",
        "# Quick smoke test",
        "python -c \"",
        "from cortical import CorticalTextProcessor",
        "from cortical.config import CorticalConfig",
        "config = CorticalConfig(scoring_algorithm='bm25')",
        "p = CorticalTextProcessor(config=config)",
        "p.process_document('doc1', 'Test content')",
        "p.compute_all(verbose=False)",
        "results = p.graph_boosted_search('test')",
        "print(f'Results: {len(results)}')",
        "\"",
        "",
        "# Run unit tests (requires pytest)",
        "python -m pytest tests/unit/test_query_search.py -v -k \"graph_boosted\"",
        "",
        "# Run full test suite",
        "python -m unittest discover -s tests -v",
        "```",
        "",
        "---",
        "",
        "## Performance Benchmarks",
        "",
        "### Test Corpus",
        "",
        "- 43 Python files from `cortical/` directory",
        "- ~715KB total text",
        "- 4,238 unique tokens",
        "- 27,829 bigrams",
        "",
        "### Timing Results (5-run average)",
        "",
        "| Metric | Before | After | Change |",
        "|--------|--------|-------|--------|",
        "| `compute_all()` | 7,546ms | 4,946ms | -34.5% |",
        "| `compute_bigram_connections()` | 6,067ms | 3,055ms | -49.6% |",
        "| `compute_tfidf()` | 8.5ms | 6.8ms | -20.0% |",
        "| `compute_importance()` | 670ms | 626ms | -6.6% |",
        "",
        "### Search Latency",
        "",
        "| Method | Latency (avg) |",
        "|--------|---------------|",
        "| `fast_find_documents()` | 0.14ms |",
        "| `find_documents_for_query()` | 70ms* |",
        "| `graph_boosted_search()` | 74ms* |",
        "",
        "*Includes query expansion overhead",
        "",
        "### Memory Impact",
        "",
        "No significant memory increase. `doc_lengths` adds ~1KB per 1000 documents.",
        "",
        "---",
        "",
        "## Trade-offs & Limitations",
        "",
        "### What Was Traded Off",
        "",
        "1. **Importance Filtering**: Only bigrams with TF-IDF ≥ 25th percentile get co-occurrence connections",
        "   - **Impact**: Low - connections between unimportant terms rarely help search",
        "   - **Mitigation**: Component and chain connections still computed for all bigrams",
        "",
        "2. **Similarity Term Limit**: Only top ~447 terms (by TF-IDF) considered for SimilarTo relations",
        "   - **Impact**: Low - important terms still get similarity relations",
        "   - **Mitigation**: Configurable via `max_similarity_pairs` parameter",
        "",
        "### Known Limitations",
        "",
        "1. **No field weighting**: BM25F not implemented (function names vs body treated equally)",
        "2. **Static parameters**: k1 and b are fixed at index time",
        "3. **No query-time tuning**: Same parameters used for all queries",
        "",
        "### Edge Cases",
        "",
        "1. **Single document corpus**: BM25 IDF formula returns small positive value (unlike TF-IDF which returns 0)",
        "2. **Empty documents**: Handled gracefully, excluded from length average",
        "3. **Very long documents**: May be penalized heavily with default b=0.75",
        "",
        "---",
        "",
        "## Future Work",
        "",
        "### Potential Improvements",
        "",
        "1. **BM25F Implementation**: Field-weighted scoring for code (function names, docstrings, body)",
        "2. **Query-time Parameter Tuning**: Adjust k1/b based on query characteristics",
        "3. **Numpy Acceleration**: Use numpy for vectorized BM25 computation",
        "4. **PageRank Optimization**: Currently 626ms, could be improved with sparse matrix",
        "",
        "### Deferred Optimizations",
        "",
        "1. **Parallel bigram processing**: Use multiprocessing for large corpora",
        "2. **Incremental BM25 updates**: Avoid full recomputation on document add",
        "3. **LSH for similarity**: Locality Sensitive Hashing for O(n) similarity",
        "",
        "### Related Tasks Created",
        "",
        "- 16 code coverage improvement tasks in `tasks/` directory",
        "- See `tasks/2025-12-15_05-23-36_ceac.json`",
        "",
        "---",
        "",
        "## Troubleshooting Guide",
        "",
        "### Common Issues",
        "",
        "**Q: BM25 scores seem wrong for single-document corpus**",
        "A: This is expected. BM25's IDF formula returns positive values even for df=N. Use `scoring_algorithm='tfidf'` if you need traditional behavior.",
        "",
        "**Q: Semantic relations are empty**",
        "A: Ensure `compute_all()` completes successfully. Check that documents have sufficient co-occurring terms (minimum 2 co-occurrences by default).",
        "",
        "**Q: Search results changed after upgrade**",
        "A: BM25 is now the default. Set `scoring_algorithm='tfidf'` in config to restore old behavior.",
        "",
        "**Q: `doc_lengths` missing after load**",
        "A: Old pickle files don't have this field. The processor will recompute lengths on first `compute_tfidf()` call.",
        "",
        "### Debug Commands",
        "",
        "```python",
        "# Check scoring algorithm",
        "print(processor.config.scoring_algorithm)",
        "",
        "# Check document lengths",
        "print(f\"Doc lengths: {len(processor.doc_lengths)}\")",
        "print(f\"Avg length: {processor.avg_doc_length}\")",
        "",
        "# Check BM25 scores",
        "from cortical.layers import CorticalLayer",
        "layer0 = processor.layers[CorticalLayer.TOKENS]",
        "col = layer0.get_minicolumn(\"some_term\")",
        "print(f\"Global TF-IDF/BM25: {col.tfidf}\")",
        "print(f\"Per-doc scores: {col.tfidf_per_doc}\")",
        "",
        "# Check connections",
        "from cortical.layers import CorticalLayer",
        "layer1 = processor.layers[CorticalLayer.BIGRAMS]",
        "print(f\"Bigrams: {layer1.column_count()}\")",
        "total_conns = sum(len(c.lateral_connections) for c in layer1.minicolumns.values())",
        "print(f\"Total connections: {total_conns}\")",
        "```",
        "",
        "---",
        "",
        "## Appendix: Commit History",
        "",
        "| Commit | Message |",
        "|--------|---------|",
        "| `924ae02` | feat: Add comprehensive scoring algorithm benchmark suite |",
        "| `0a52858` | feat: Implement BM25 scoring algorithm as default |",
        "| `d0732b4` | chore: Add 16 code coverage improvement tasks |",
        "| `fcce0c2` | feat: Optimize compute_all and add Graph-Boosted search (GB-BM25) |",
        "| `63064c7` | docs: Add BM25/GB-BM25 documentation and tests |",
        "",
        "---",
        "",
        "*Document generated for knowledge transfer. For questions, refer to CLAUDE.md or the source code documentation.*"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "scripts/benchmark_scoring.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "#!/usr/bin/env python3",
        "\"\"\"",
        "Benchmark Scoring Algorithms (TF-IDF vs BM25)",
        "=============================================",
        "",
        "Comprehensive benchmark suite for comparing document scoring algorithms.",
        "Run this BEFORE and AFTER implementing BM25 to measure impact.",
        "",
        "Usage:",
        "    # Run full benchmark suite",
        "    python scripts/benchmark_scoring.py",
        "",
        "    # Run specific benchmark",
        "    python scripts/benchmark_scoring.py --benchmark compute",
        "    python scripts/benchmark_scoring.py --benchmark search",
        "    python scripts/benchmark_scoring.py --benchmark relevance",
        "",
        "    # Save results for comparison",
        "    python scripts/benchmark_scoring.py --output baseline_tfidf.json",
        "",
        "    # Compare two benchmark runs",
        "    python scripts/benchmark_scoring.py --compare baseline_tfidf.json after_bm25.json",
        "",
        "Benchmarks:",
        "    1. COMPUTE: Time to compute scores for varying corpus sizes",
        "    2. SEARCH: Query latency and throughput",
        "    3. RELEVANCE: Search quality using known-relevant queries",
        "    4. MEMORY: Memory footprint of score storage",
        "    5. SCALING: How performance changes with corpus growth",
        "\"\"\"",
        "",
        "import argparse",
        "import json",
        "import sys",
        "import time",
        "import gc",
        "import statistics",
        "from pathlib import Path",
        "from typing import Dict, List, Any, Tuple, Optional",
        "from dataclasses import dataclass, asdict",
        "from datetime import datetime",
        "",
        "# Add parent directory to path",
        "sys.path.insert(0, str(Path(__file__).parent.parent))",
        "",
        "from cortical.processor import CorticalTextProcessor",
        "from cortical.tokenizer import Tokenizer",
        "from cortical.layers import CorticalLayer",
        "",
        "",
        "# ============================================================================",
        "# BENCHMARK DATA STRUCTURES",
        "# ============================================================================",
        "",
        "@dataclass",
        "class BenchmarkResult:",
        "    \"\"\"Result from a single benchmark run.\"\"\"",
        "    name: str",
        "    algorithm: str  # 'tfidf' or 'bm25'",
        "    timestamp: str",
        "    corpus_size: int",
        "    vocabulary_size: int",
        "    metrics: Dict[str, Any]",
        "",
        "    def to_dict(self) -> dict:",
        "        return asdict(self)",
        "",
        "",
        "@dataclass",
        "class BenchmarkSuite:",
        "    \"\"\"Collection of benchmark results.\"\"\"",
        "    version: str = \"1.0\"",
        "    algorithm: str = \"tfidf\"  # Current algorithm being benchmarked",
        "    timestamp: str = \"\"",
        "    system_info: Dict[str, Any] = None",
        "    results: List[Dict] = None",
        "",
        "    def __post_init__(self):",
        "        if self.timestamp == \"\":",
        "            self.timestamp = datetime.now().isoformat()",
        "        if self.system_info is None:",
        "            self.system_info = get_system_info()",
        "        if self.results is None:",
        "            self.results = []",
        "",
        "    def add_result(self, result: BenchmarkResult):",
        "        self.results.append(result.to_dict())",
        "",
        "    def to_dict(self) -> dict:",
        "        return {",
        "            'version': self.version,",
        "            'algorithm': self.algorithm,",
        "            'timestamp': self.timestamp,",
        "            'system_info': self.system_info,",
        "            'results': self.results",
        "        }",
        "",
        "    def save(self, path: str):",
        "        with open(path, 'w') as f:",
        "            json.dump(self.to_dict(), f, indent=2)",
        "        print(f\"Results saved to {path}\")",
        "",
        "    @classmethod",
        "    def load(cls, path: str) -> 'BenchmarkSuite':",
        "        with open(path) as f:",
        "            data = json.load(f)",
        "        suite = cls(",
        "            version=data['version'],",
        "            algorithm=data['algorithm'],",
        "            timestamp=data['timestamp'],",
        "            system_info=data['system_info'],",
        "            results=data['results']",
        "        )",
        "        return suite",
        "",
        "",
        "def get_system_info() -> Dict[str, Any]:",
        "    \"\"\"Collect system information for benchmark context.\"\"\"",
        "    import platform",
        "    return {",
        "        'python_version': platform.python_version(),",
        "        'platform': platform.platform(),",
        "        'processor': platform.processor(),",
        "    }",
        "",
        "",
        "# ============================================================================",
        "# TEST CORPUS GENERATORS",
        "# ============================================================================",
        "",
        "def generate_synthetic_corpus(n_docs: int, avg_length: int = 200) -> Dict[str, str]:",
        "    \"\"\"",
        "    Generate synthetic documents for benchmarking.",
        "",
        "    Uses deterministic content for reproducible benchmarks.",
        "    \"\"\"",
        "    import hashlib",
        "",
        "    # Domain vocabulary for realistic term distributions",
        "    domains = {",
        "        'ml': ['neural', 'network', 'learning', 'model', 'training', 'gradient',",
        "               'loss', 'optimization', 'batch', 'epoch', 'layer', 'activation',",
        "               'weights', 'bias', 'backpropagation', 'forward', 'inference'],",
        "        'db': ['database', 'query', 'index', 'table', 'row', 'column', 'join',",
        "               'transaction', 'commit', 'rollback', 'lock', 'cache', 'buffer',",
        "               'storage', 'retrieval', 'schema', 'normalization'],",
        "        'sys': ['process', 'thread', 'memory', 'allocation', 'kernel', 'system',",
        "                'file', 'socket', 'network', 'protocol', 'buffer', 'stream',",
        "                'handler', 'callback', 'event', 'scheduler', 'queue'],",
        "        'web': ['request', 'response', 'server', 'client', 'http', 'api',",
        "                'endpoint', 'route', 'middleware', 'session', 'cookie',",
        "                'authentication', 'authorization', 'token', 'header'],",
        "    }",
        "",
        "    all_terms = []",
        "    for terms in domains.values():",
        "        all_terms.extend(terms)",
        "",
        "    docs = {}",
        "    for i in range(n_docs):",
        "        # Deterministic seed based on doc index",
        "        seed = int(hashlib.md5(f\"doc_{i}\".encode()).hexdigest()[:8], 16)",
        "",
        "        # Select primary domain for this doc",
        "        domain_idx = seed % len(domains)",
        "        domain = list(domains.keys())[domain_idx]",
        "        domain_terms = domains[domain]",
        "",
        "        # Generate document content",
        "        words = []",
        "        word_count = avg_length + (seed % 100) - 50  # Vary length",
        "",
        "        for j in range(word_count):",
        "            term_seed = (seed + j * 31) % 1000",
        "            if term_seed < 600:  # 60% domain terms",
        "                term_idx = (seed + j) % len(domain_terms)",
        "                words.append(domain_terms[term_idx])",
        "            else:  # 40% cross-domain terms",
        "                term_idx = (seed + j * 17) % len(all_terms)",
        "                words.append(all_terms[term_idx])",
        "",
        "        doc_id = f\"synthetic/doc_{i:04d}.txt\"",
        "        docs[doc_id] = ' '.join(words)",
        "",
        "    return docs",
        "",
        "",
        "def load_real_corpus() -> Optional[Dict[str, str]]:",
        "    \"\"\"Load real corpus if available.\"\"\"",
        "    corpus_path = Path(\"corpus_dev.pkl\")",
        "    if corpus_path.exists():",
        "        try:",
        "            processor = CorticalTextProcessor.load(str(corpus_path))",
        "            return processor.documents",
        "        except Exception as e:",
        "            print(f\"Warning: Could not load corpus: {e}\")",
        "    return None",
        "",
        "",
        "# ============================================================================",
        "# RELEVANCE TEST QUERIES",
        "# ============================================================================",
        "",
        "# Queries with expected relevant documents (for synthetic corpus)",
        "RELEVANCE_QUERIES = [",
        "    {",
        "        'query': 'neural network training',",
        "        'domain': 'ml',",
        "        'expected_terms': ['neural', 'network', 'training', 'learning', 'model'],",
        "    },",
        "    {",
        "        'query': 'database query optimization',",
        "        'domain': 'db',",
        "        'expected_terms': ['database', 'query', 'index', 'cache', 'retrieval'],",
        "    },",
        "    {",
        "        'query': 'process memory management',",
        "        'domain': 'sys',",
        "        'expected_terms': ['process', 'memory', 'allocation', 'buffer', 'kernel'],",
        "    },",
        "    {",
        "        'query': 'api authentication',",
        "        'domain': 'web',",
        "        'expected_terms': ['api', 'authentication', 'token', 'request', 'session'],",
        "    },",
        "]",
        "",
        "",
        "# ============================================================================",
        "# BENCHMARK IMPLEMENTATIONS",
        "# ============================================================================",
        "",
        "def benchmark_compute(suite: BenchmarkSuite, corpus_sizes: List[int] = None):",
        "    \"\"\"",
        "    Benchmark: Score computation time.",
        "",
        "    Measures time to compute TF-IDF/BM25 scores for varying corpus sizes.",
        "    \"\"\"",
        "    print(\"\\n\" + \"=\" * 60)",
        "    print(\"BENCHMARK: Score Computation Time\")",
        "    print(\"=\" * 60)",
        "",
        "    if corpus_sizes is None:",
        "        corpus_sizes = [25, 50, 100, 200]",
        "",
        "    tokenizer = Tokenizer(filter_code_noise=True)",
        "",
        "    for n_docs in corpus_sizes:",
        "        print(f\"\\nCorpus size: {n_docs} documents\")",
        "",
        "        # Generate corpus",
        "        docs = generate_synthetic_corpus(n_docs)",
        "",
        "        # Create processor and load documents",
        "        processor = CorticalTextProcessor(tokenizer=tokenizer)",
        "        for doc_id, content in docs.items():",
        "            processor.process_document(doc_id, content)",
        "",
        "        layer0 = processor.layers.get(CorticalLayer.TOKENS)",
        "        vocab_size = layer0.column_count() if layer0 else 0",
        "",
        "        # Warm up",
        "        processor.compute_tfidf(verbose=False)",
        "",
        "        # Benchmark TF-IDF computation (multiple runs for stability)",
        "        times = []",
        "        for _ in range(5):",
        "            # Reset TF-IDF scores",
        "            for col in layer0.minicolumns.values():",
        "                col.tfidf = 0.0",
        "                col.tfidf_per_doc = {}",
        "",
        "            gc.collect()",
        "            start = time.perf_counter()",
        "            processor.compute_tfidf(verbose=False)",
        "            elapsed = time.perf_counter() - start",
        "            times.append(elapsed)",
        "",
        "        metrics = {",
        "            'corpus_size': n_docs,",
        "            'vocabulary_size': vocab_size,",
        "            'mean_time_ms': statistics.mean(times) * 1000,",
        "            'std_time_ms': statistics.stdev(times) * 1000 if len(times) > 1 else 0,",
        "            'min_time_ms': min(times) * 1000,",
        "            'max_time_ms': max(times) * 1000,",
        "            'time_per_doc_ms': (statistics.mean(times) * 1000) / n_docs,",
        "            'time_per_term_us': (statistics.mean(times) * 1_000_000) / vocab_size if vocab_size else 0,",
        "        }",
        "",
        "        print(f\"  Vocabulary: {vocab_size} terms\")",
        "        print(f\"  Mean time: {metrics['mean_time_ms']:.2f}ms (+/- {metrics['std_time_ms']:.2f}ms)\")",
        "        print(f\"  Per-doc: {metrics['time_per_doc_ms']:.3f}ms\")",
        "        print(f\"  Per-term: {metrics['time_per_term_us']:.2f}us\")",
        "",
        "        result = BenchmarkResult(",
        "            name='compute_scores',",
        "            algorithm=suite.algorithm,",
        "            timestamp=datetime.now().isoformat(),",
        "            corpus_size=n_docs,",
        "            vocabulary_size=vocab_size,",
        "            metrics=metrics",
        "        )",
        "        suite.add_result(result)",
        "",
        "        del processor",
        "        gc.collect()",
        "",
        "",
        "def benchmark_search(suite: BenchmarkSuite, n_docs: int = 100):",
        "    \"\"\"",
        "    Benchmark: Search query latency and throughput.",
        "",
        "    Measures time to execute search queries using computed scores.",
        "    \"\"\"",
        "    print(\"\\n\" + \"=\" * 60)",
        "    print(\"BENCHMARK: Search Query Performance\")",
        "    print(\"=\" * 60)",
        "",
        "    # Setup corpus",
        "    tokenizer = Tokenizer(filter_code_noise=True)",
        "    processor = CorticalTextProcessor(tokenizer=tokenizer)",
        "",
        "    docs = generate_synthetic_corpus(n_docs)",
        "    for doc_id, content in docs.items():",
        "        processor.process_document(doc_id, content)",
        "    processor.compute_all(verbose=False)",
        "",
        "    layer0 = processor.layers.get(CorticalLayer.TOKENS)",
        "    vocab_size = layer0.column_count() if layer0 else 0",
        "",
        "    # Test queries",
        "    queries = [q['query'] for q in RELEVANCE_QUERIES]",
        "    queries.extend([",
        "        'learning optimization',",
        "        'cache buffer',",
        "        'request handler',",
        "        'network protocol',",
        "    ])",
        "",
        "    # Warm up",
        "    for q in queries[:2]:",
        "        processor.find_documents_for_query(q, top_n=5)",
        "",
        "    # Benchmark queries",
        "    print(f\"\\nCorpus: {n_docs} docs, {vocab_size} terms\")",
        "    print(f\"Running {len(queries)} queries...\")",
        "",
        "    query_times = []",
        "    for query in queries:",
        "        times = []",
        "        for _ in range(10):",
        "            start = time.perf_counter()",
        "            results = processor.find_documents_for_query(query, top_n=5)",
        "            elapsed = time.perf_counter() - start",
        "            times.append(elapsed)",
        "",
        "        avg_time = statistics.mean(times)",
        "        query_times.append(avg_time)",
        "",
        "    metrics = {",
        "        'corpus_size': n_docs,",
        "        'vocabulary_size': vocab_size,",
        "        'num_queries': len(queries),",
        "        'mean_latency_ms': statistics.mean(query_times) * 1000,",
        "        'median_latency_ms': statistics.median(query_times) * 1000,",
        "        'p95_latency_ms': sorted(query_times)[int(len(query_times) * 0.95)] * 1000,",
        "        'max_latency_ms': max(query_times) * 1000,",
        "        'min_latency_ms': min(query_times) * 1000,",
        "        'throughput_qps': 1.0 / statistics.mean(query_times) if query_times else 0,",
        "    }",
        "",
        "    print(f\"  Mean latency: {metrics['mean_latency_ms']:.2f}ms\")",
        "    print(f\"  P95 latency: {metrics['p95_latency_ms']:.2f}ms\")",
        "    print(f\"  Throughput: {metrics['throughput_qps']:.1f} queries/sec\")",
        "",
        "    result = BenchmarkResult(",
        "        name='search_latency',",
        "        algorithm=suite.algorithm,",
        "        timestamp=datetime.now().isoformat(),",
        "        corpus_size=n_docs,",
        "        vocabulary_size=vocab_size,",
        "        metrics=metrics",
        "    )",
        "    suite.add_result(result)",
        "",
        "",
        "def benchmark_relevance(suite: BenchmarkSuite, n_docs: int = 100):",
        "    \"\"\"",
        "    Benchmark: Search result relevance quality.",
        "",
        "    Measures how well the scoring algorithm ranks relevant documents.",
        "    Uses domain-based relevance (docs from same domain should rank higher).",
        "    \"\"\"",
        "    print(\"\\n\" + \"=\" * 60)",
        "    print(\"BENCHMARK: Search Relevance Quality\")",
        "    print(\"=\" * 60)",
        "",
        "    # Setup corpus with domain tracking",
        "    tokenizer = Tokenizer(filter_code_noise=True)",
        "    processor = CorticalTextProcessor(tokenizer=tokenizer)",
        "",
        "    docs = generate_synthetic_corpus(n_docs)",
        "    for doc_id, content in docs.items():",
        "        processor.process_document(doc_id, content)",
        "    processor.compute_all(verbose=False)",
        "",
        "    # Determine document domains (based on generation algorithm)",
        "    import hashlib",
        "    domains = ['ml', 'db', 'sys', 'web']",
        "    doc_domains = {}",
        "    for i in range(n_docs):",
        "        seed = int(hashlib.md5(f\"doc_{i}\".encode()).hexdigest()[:8], 16)",
        "        domain_idx = seed % len(domains)",
        "        doc_id = f\"synthetic/doc_{i:04d}.txt\"",
        "        doc_domains[doc_id] = domains[domain_idx]",
        "",
        "    # Run relevance tests",
        "    print(f\"\\nCorpus: {n_docs} docs\")",
        "",
        "    relevance_scores = []",
        "",
        "    for test in RELEVANCE_QUERIES:",
        "        query = test['query']",
        "        expected_domain = test['domain']",
        "        expected_terms = test['expected_terms']",
        "",
        "        # Get search results",
        "        results = processor.find_documents_for_query(query, top_n=10)",
        "",
        "        # Calculate precision@k (docs from expected domain in top k)",
        "        precisions = {}",
        "        for k in [1, 3, 5, 10]:",
        "            top_k = results[:k]",
        "            relevant = sum(1 for doc_id, _ in top_k",
        "                         if doc_domains.get(doc_id) == expected_domain)",
        "            precisions[f'p@{k}'] = relevant / k if k <= len(results) else 0",
        "",
        "        # Calculate MRR (mean reciprocal rank of first relevant doc)",
        "        mrr = 0.0",
        "        for rank, (doc_id, _) in enumerate(results, 1):",
        "            if doc_domains.get(doc_id) == expected_domain:",
        "                mrr = 1.0 / rank",
        "                break",
        "",
        "        # Check if expected terms appear in expanded query",
        "        expanded = processor.expand_query(query, max_expansions=10)",
        "        term_recall = sum(1 for t in expected_terms if t in expanded) / len(expected_terms)",
        "",
        "        relevance_scores.append({",
        "            'query': query,",
        "            'domain': expected_domain,",
        "            'p@1': precisions['p@1'],",
        "            'p@3': precisions['p@3'],",
        "            'p@5': precisions['p@5'],",
        "            'mrr': mrr,",
        "            'term_recall': term_recall,",
        "        })",
        "",
        "        print(f\"  '{query}': P@3={precisions['p@3']:.2f}, MRR={mrr:.2f}, TermRecall={term_recall:.2f}\")",
        "",
        "    # Aggregate metrics",
        "    metrics = {",
        "        'corpus_size': n_docs,",
        "        'num_queries': len(relevance_scores),",
        "        'mean_p@1': statistics.mean(r['p@1'] for r in relevance_scores),",
        "        'mean_p@3': statistics.mean(r['p@3'] for r in relevance_scores),",
        "        'mean_p@5': statistics.mean(r['p@5'] for r in relevance_scores),",
        "        'mean_mrr': statistics.mean(r['mrr'] for r in relevance_scores),",
        "        'mean_term_recall': statistics.mean(r['term_recall'] for r in relevance_scores),",
        "        'per_query': relevance_scores,",
        "    }",
        "",
        "    print(f\"\\n  AGGREGATE:\")",
        "    print(f\"    Mean P@3: {metrics['mean_p@3']:.3f}\")",
        "    print(f\"    Mean MRR: {metrics['mean_mrr']:.3f}\")",
        "    print(f\"    Mean Term Recall: {metrics['mean_term_recall']:.3f}\")",
        "",
        "    result = BenchmarkResult(",
        "        name='search_relevance',",
        "        algorithm=suite.algorithm,",
        "        timestamp=datetime.now().isoformat(),",
        "        corpus_size=n_docs,",
        "        vocabulary_size=processor.layers[CorticalLayer.TOKENS].column_count(),",
        "        metrics=metrics",
        "    )",
        "    suite.add_result(result)",
        "",
        "",
        "def benchmark_memory(suite: BenchmarkSuite, corpus_sizes: List[int] = None):",
        "    \"\"\"",
        "    Benchmark: Memory footprint of score storage.",
        "",
        "    Measures memory used by TF-IDF/BM25 data structures.",
        "    \"\"\"",
        "    print(\"\\n\" + \"=\" * 60)",
        "    print(\"BENCHMARK: Memory Footprint\")",
        "    print(\"=\" * 60)",
        "",
        "    if corpus_sizes is None:",
        "        corpus_sizes = [25, 50, 100, 200]",
        "",
        "    import tracemalloc",
        "",
        "    tokenizer = Tokenizer(filter_code_noise=True)",
        "",
        "    for n_docs in corpus_sizes:",
        "        print(f\"\\nCorpus size: {n_docs} documents\")",
        "",
        "        # Generate corpus",
        "        docs = generate_synthetic_corpus(n_docs)",
        "",
        "        gc.collect()",
        "        tracemalloc.start()",
        "",
        "        # Create processor and load documents",
        "        processor = CorticalTextProcessor(tokenizer=tokenizer)",
        "        for doc_id, content in docs.items():",
        "            processor.process_document(doc_id, content)",
        "",
        "        # Snapshot before computing scores",
        "        before_compute = tracemalloc.take_snapshot()",
        "",
        "        # Compute scores",
        "        processor.compute_tfidf(verbose=False)",
        "",
        "        # Snapshot after computing scores",
        "        after_compute = tracemalloc.take_snapshot()",
        "",
        "        tracemalloc.stop()",
        "",
        "        # Calculate memory difference",
        "        diff = after_compute.compare_to(before_compute, 'lineno')",
        "        score_memory = sum(stat.size_diff for stat in diff if stat.size_diff > 0)",
        "",
        "        layer0 = processor.layers.get(CorticalLayer.TOKENS)",
        "        vocab_size = layer0.column_count() if layer0 else 0",
        "",
        "        # Estimate per-doc TF-IDF dict size",
        "        total_tfidf_entries = sum(",
        "            len(col.tfidf_per_doc)",
        "            for col in layer0.minicolumns.values()",
        "        )",
        "",
        "        metrics = {",
        "            'corpus_size': n_docs,",
        "            'vocabulary_size': vocab_size,",
        "            'score_memory_bytes': score_memory,",
        "            'score_memory_kb': score_memory / 1024,",
        "            'total_tfidf_entries': total_tfidf_entries,",
        "            'bytes_per_entry': score_memory / total_tfidf_entries if total_tfidf_entries else 0,",
        "            'bytes_per_term': score_memory / vocab_size if vocab_size else 0,",
        "        }",
        "",
        "        print(f\"  Vocabulary: {vocab_size} terms\")",
        "        print(f\"  TF-IDF entries: {total_tfidf_entries}\")",
        "        print(f\"  Score memory: {metrics['score_memory_kb']:.1f} KB\")",
        "        print(f\"  Per entry: {metrics['bytes_per_entry']:.1f} bytes\")",
        "",
        "        result = BenchmarkResult(",
        "            name='memory_footprint',",
        "            algorithm=suite.algorithm,",
        "            timestamp=datetime.now().isoformat(),",
        "            corpus_size=n_docs,",
        "            vocabulary_size=vocab_size,",
        "            metrics=metrics",
        "        )",
        "        suite.add_result(result)",
        "",
        "        del processor",
        "        gc.collect()",
        "",
        "",
        "def benchmark_scaling(suite: BenchmarkSuite):",
        "    \"\"\"",
        "    Benchmark: Scaling behavior analysis.",
        "",
        "    Measures how compute time scales with corpus size to detect O(n^2) issues.",
        "    \"\"\"",
        "    print(\"\\n\" + \"=\" * 60)",
        "    print(\"BENCHMARK: Scaling Behavior\")",
        "    print(\"=\" * 60)",
        "",
        "    corpus_sizes = [10, 25, 50, 100, 150, 200]",
        "    tokenizer = Tokenizer(filter_code_noise=True)",
        "",
        "    timings = []",
        "",
        "    for n_docs in corpus_sizes:",
        "        docs = generate_synthetic_corpus(n_docs)",
        "",
        "        processor = CorticalTextProcessor(tokenizer=tokenizer)",
        "        for doc_id, content in docs.items():",
        "            processor.process_document(doc_id, content)",
        "",
        "        layer0 = processor.layers.get(CorticalLayer.TOKENS)",
        "        vocab_size = layer0.column_count() if layer0 else 0",
        "",
        "        # Time score computation",
        "        gc.collect()",
        "        times = []",
        "        for _ in range(3):",
        "            # Reset scores",
        "            for col in layer0.minicolumns.values():",
        "                col.tfidf = 0.0",
        "                col.tfidf_per_doc = {}",
        "",
        "            start = time.perf_counter()",
        "            processor.compute_tfidf(verbose=False)",
        "            elapsed = time.perf_counter() - start",
        "            times.append(elapsed)",
        "",
        "        avg_time = statistics.mean(times)",
        "        timings.append({",
        "            'n_docs': n_docs,",
        "            'vocab_size': vocab_size,",
        "            'time_ms': avg_time * 1000,",
        "        })",
        "",
        "        print(f\"  {n_docs} docs, {vocab_size} terms: {avg_time*1000:.2f}ms\")",
        "",
        "        del processor",
        "        gc.collect()",
        "",
        "    # Analyze scaling behavior",
        "    # Linear: time ~ n, Quadratic: time ~ n^2",
        "    # Fit log-log slope to estimate complexity",
        "    import math",
        "",
        "    if len(timings) >= 3:",
        "        log_n = [math.log(t['n_docs']) for t in timings]",
        "        log_t = [math.log(t['time_ms']) for t in timings]",
        "",
        "        # Simple linear regression on log-log plot",
        "        n = len(log_n)",
        "        sum_x = sum(log_n)",
        "        sum_y = sum(log_t)",
        "        sum_xy = sum(x*y for x, y in zip(log_n, log_t))",
        "        sum_xx = sum(x*x for x in log_n)",
        "",
        "        slope = (n * sum_xy - sum_x * sum_y) / (n * sum_xx - sum_x * sum_x)",
        "",
        "        complexity = \"O(n)\" if slope < 1.3 else \"O(n log n)\" if slope < 1.7 else \"O(n^2)\"",
        "",
        "        print(f\"\\n  Scaling exponent: {slope:.2f}\")",
        "        print(f\"  Estimated complexity: {complexity}\")",
        "    else:",
        "        slope = 0",
        "        complexity = \"Unknown (need more data points)\"",
        "",
        "    metrics = {",
        "        'data_points': timings,",
        "        'scaling_exponent': slope,",
        "        'estimated_complexity': complexity,",
        "    }",
        "",
        "    result = BenchmarkResult(",
        "        name='scaling_behavior',",
        "        algorithm=suite.algorithm,",
        "        timestamp=datetime.now().isoformat(),",
        "        corpus_size=max(t['n_docs'] for t in timings),",
        "        vocabulary_size=max(t['vocab_size'] for t in timings),",
        "        metrics=metrics",
        "    )",
        "    suite.add_result(result)",
        "",
        "",
        "def benchmark_real_corpus(suite: BenchmarkSuite):",
        "    \"\"\"",
        "    Benchmark: Performance on real corpus (if available).",
        "",
        "    Uses the actual indexed codebase for realistic measurements.",
        "    \"\"\"",
        "    print(\"\\n\" + \"=\" * 60)",
        "    print(\"BENCHMARK: Real Corpus Performance\")",
        "    print(\"=\" * 60)",
        "",
        "    # Try to load real corpus",
        "    corpus_path = Path(\"corpus_dev.pkl\")",
        "    if not corpus_path.exists():",
        "        print(\"  Skipping: corpus_dev.pkl not found\")",
        "        print(\"  Run: python scripts/index_codebase.py first\")",
        "        return",
        "",
        "    try:",
        "        processor = CorticalTextProcessor.load(str(corpus_path))",
        "    except Exception as e:",
        "        print(f\"  Skipping: Could not load corpus: {e}\")",
        "        return",
        "",
        "    n_docs = len(processor.documents)",
        "    layer0 = processor.layers.get(CorticalLayer.TOKENS)",
        "    vocab_size = layer0.column_count() if layer0 else 0",
        "",
        "    print(f\"  Corpus: {n_docs} documents, {vocab_size} terms\")",
        "",
        "    # Test queries relevant to this codebase",
        "    codebase_queries = [",
        "        \"pagerank algorithm\",",
        "        \"tfidf computation\",",
        "        \"lateral connections\",",
        "        \"query expansion\",",
        "        \"document search\",",
        "        \"minicolumn layer\",",
        "        \"semantic relations\",",
        "        \"louvain clustering\",",
        "    ]",
        "",
        "    # Benchmark TF-IDF computation",
        "    print(\"\\n  TF-IDF Computation:\")",
        "    times = []",
        "    for _ in range(3):",
        "        # Reset scores",
        "        for col in layer0.minicolumns.values():",
        "            col.tfidf = 0.0",
        "            col.tfidf_per_doc = {}",
        "",
        "        gc.collect()",
        "        start = time.perf_counter()",
        "        processor.compute_tfidf(verbose=False)",
        "        elapsed = time.perf_counter() - start",
        "        times.append(elapsed)",
        "",
        "    compute_time = statistics.mean(times)",
        "    print(f\"    Time: {compute_time*1000:.2f}ms\")",
        "",
        "    # Benchmark search queries",
        "    print(\"\\n  Search Queries:\")",
        "    query_times = []",
        "    for query in codebase_queries:",
        "        times = []",
        "        for _ in range(5):",
        "            start = time.perf_counter()",
        "            results = processor.find_documents_for_query(query, top_n=5)",
        "            elapsed = time.perf_counter() - start",
        "            times.append(elapsed)",
        "",
        "        avg_time = statistics.mean(times)",
        "        query_times.append(avg_time)",
        "        print(f\"    '{query}': {avg_time*1000:.2f}ms, {len(results)} results\")",
        "",
        "    metrics = {",
        "        'corpus_size': n_docs,",
        "        'vocabulary_size': vocab_size,",
        "        'compute_time_ms': compute_time * 1000,",
        "        'mean_query_time_ms': statistics.mean(query_times) * 1000,",
        "        'max_query_time_ms': max(query_times) * 1000,",
        "        'queries_tested': len(codebase_queries),",
        "    }",
        "",
        "    result = BenchmarkResult(",
        "        name='real_corpus',",
        "        algorithm=suite.algorithm,",
        "        timestamp=datetime.now().isoformat(),",
        "        corpus_size=n_docs,",
        "        vocabulary_size=vocab_size,",
        "        metrics=metrics",
        "    )",
        "    suite.add_result(result)",
        "",
        "",
        "# ============================================================================",
        "# COMPARISON TOOLS",
        "# ============================================================================",
        "",
        "def compare_results(before_path: str, after_path: str):",
        "    \"\"\"Compare two benchmark runs and report improvements/regressions.\"\"\"",
        "    print(\"\\n\" + \"=\" * 60)",
        "    print(\"BENCHMARK COMPARISON\")",
        "    print(\"=\" * 60)",
        "",
        "    before = BenchmarkSuite.load(before_path)",
        "    after = BenchmarkSuite.load(after_path)",
        "",
        "    print(f\"\\nBefore: {before.algorithm} ({before.timestamp})\")",
        "    print(f\"After:  {after.algorithm} ({after.timestamp})\")",
        "",
        "    # Group results by benchmark name",
        "    before_by_name = {}",
        "    for r in before.results:",
        "        key = (r['name'], r['corpus_size'])",
        "        before_by_name[key] = r",
        "",
        "    after_by_name = {}",
        "    for r in after.results:",
        "        key = (r['name'], r['corpus_size'])",
        "        after_by_name[key] = r",
        "",
        "    print(\"\\n\" + \"-\" * 60)",
        "    print(f\"{'Benchmark':<30} {'Before':>12} {'After':>12} {'Change':>12}\")",
        "    print(\"-\" * 60)",
        "",
        "    # Compare common benchmarks",
        "    for key in sorted(before_by_name.keys()):",
        "        if key not in after_by_name:",
        "            continue",
        "",
        "        b = before_by_name[key]['metrics']",
        "        a = after_by_name[key]['metrics']",
        "        name = f\"{key[0]} (n={key[1]})\"",
        "",
        "        # Compare key metrics",
        "        if 'mean_time_ms' in b:",
        "            b_val = b['mean_time_ms']",
        "            a_val = a['mean_time_ms']",
        "            change = ((a_val - b_val) / b_val) * 100 if b_val else 0",
        "            indicator = \"faster\" if change < 0 else \"SLOWER\"",
        "            print(f\"{name:<30} {b_val:>10.2f}ms {a_val:>10.2f}ms {change:>+10.1f}% {indicator}\")",
        "",
        "        if 'mean_latency_ms' in b:",
        "            b_val = b['mean_latency_ms']",
        "            a_val = a['mean_latency_ms']",
        "            change = ((a_val - b_val) / b_val) * 100 if b_val else 0",
        "            indicator = \"faster\" if change < 0 else \"SLOWER\"",
        "            print(f\"{name:<30} {b_val:>10.2f}ms {a_val:>10.2f}ms {change:>+10.1f}% {indicator}\")",
        "",
        "        if 'mean_p@3' in b:",
        "            b_val = b['mean_p@3']",
        "            a_val = a['mean_p@3']",
        "            change = ((a_val - b_val) / b_val) * 100 if b_val else 0",
        "            indicator = \"BETTER\" if change > 0 else \"worse\"",
        "            print(f\"{name:<30} {b_val:>10.3f}   {a_val:>10.3f}   {change:>+10.1f}% {indicator}\")",
        "",
        "    print(\"-\" * 60)",
        "",
        "",
        "# ============================================================================",
        "# MAIN",
        "# ============================================================================",
        "",
        "def run_all_benchmarks(output_path: Optional[str] = None, algorithm: str = None):",
        "    \"\"\"Run all benchmarks and optionally save results.\"\"\"",
        "    # Detect algorithm from config if not specified",
        "    if algorithm is None:",
        "        from cortical.config import CorticalConfig",
        "        config = CorticalConfig()",
        "        algorithm = config.scoring_algorithm",
        "",
        "    suite = BenchmarkSuite(algorithm=algorithm)",
        "",
        "    print(\"\\n\" + \"=\" * 60)",
        "    print(\"SCORING ALGORITHM BENCHMARK SUITE\")",
        "    print(\"=\" * 60)",
        "    print(f\"Algorithm: {suite.algorithm}\")",
        "    print(f\"Timestamp: {suite.timestamp}\")",
        "    print(f\"Python: {suite.system_info['python_version']}\")",
        "",
        "    # Run benchmarks",
        "    benchmark_compute(suite)",
        "    benchmark_search(suite)",
        "    benchmark_relevance(suite)",
        "    benchmark_memory(suite)",
        "    benchmark_scaling(suite)",
        "    benchmark_real_corpus(suite)",
        "",
        "    # Summary",
        "    print(\"\\n\" + \"=\" * 60)",
        "    print(\"BENCHMARK SUMMARY\")",
        "    print(\"=\" * 60)",
        "",
        "    for r in suite.results:",
        "        print(f\"\\n{r['name']} (n={r['corpus_size']}):\")",
        "        for key, value in r['metrics'].items():",
        "            if key == 'per_query' or key == 'data_points':",
        "                continue",
        "            if isinstance(value, float):",
        "                print(f\"  {key}: {value:.3f}\")",
        "            else:",
        "                print(f\"  {key}: {value}\")",
        "",
        "    # Save if requested",
        "    if output_path:",
        "        suite.save(output_path)",
        "",
        "    return suite",
        "",
        "",
        "def main():",
        "    parser = argparse.ArgumentParser(",
        "        description=\"Benchmark scoring algorithms (TF-IDF vs BM25)\",",
        "        formatter_class=argparse.RawDescriptionHelpFormatter,",
        "        epilog=__doc__",
        "    )",
        "    parser.add_argument('--benchmark', choices=['all', 'compute', 'search', 'relevance',",
        "                                                  'memory', 'scaling', 'real'],",
        "                       default='all', help='Benchmark to run')",
        "    parser.add_argument('--output', '-o', help='Save results to JSON file')",
        "    parser.add_argument('--compare', nargs=2, metavar=('BEFORE', 'AFTER'),",
        "                       help='Compare two benchmark result files')",
        "    parser.add_argument('--corpus-sizes', type=int, nargs='+', default=[25, 50, 100, 200],",
        "                       help='Corpus sizes to test')",
        "    parser.add_argument('--algorithm', choices=['tfidf', 'bm25'],",
        "                       help='Algorithm to benchmark (default: from config)')",
        "",
        "    args = parser.parse_args()",
        "",
        "    if args.compare:",
        "        compare_results(args.compare[0], args.compare[1])",
        "        return",
        "",
        "    # Detect algorithm from config if not specified",
        "    algorithm = args.algorithm",
        "    if algorithm is None:",
        "        from cortical.config import CorticalConfig",
        "        config = CorticalConfig()",
        "        algorithm = config.scoring_algorithm",
        "",
        "    if args.benchmark == 'all':",
        "        run_all_benchmarks(args.output, algorithm=algorithm)",
        "    else:",
        "        suite = BenchmarkSuite(algorithm=algorithm)",
        "",
        "        if args.benchmark == 'compute':",
        "            benchmark_compute(suite, args.corpus_sizes)",
        "        elif args.benchmark == 'search':",
        "            benchmark_search(suite)",
        "        elif args.benchmark == 'relevance':",
        "            benchmark_relevance(suite)",
        "        elif args.benchmark == 'memory':",
        "            benchmark_memory(suite)",
        "        elif args.benchmark == 'scaling':",
        "            benchmark_scaling(suite)",
        "        elif args.benchmark == 'real':",
        "            benchmark_real_corpus(suite)",
        "",
        "        if args.output:",
        "            suite.save(args.output)",
        "",
        "",
        "if __name__ == '__main__':",
        "    main()"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "tasks/2025-12-15_05-23-36_ceac.json",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "{",
        "  \"version\": 1,",
        "  \"session_id\": \"ceac\",",
        "  \"started_at\": \"2025-12-15T05:23:36.072062\",",
        "  \"saved_at\": \"2025-12-15T05:24:40.799225\",",
        "  \"tasks\": [",
        "    {",
        "      \"id\": \"T-20251215-052336-ceac-001\",",
        "      \"title\": \"Add tests for query/search.py to improve coverage from 26% to >80%\",",
        "      \"status\": \"pending\",",
        "      \"priority\": \"high\",",
        "      \"category\": \"testing\",",
        "      \"description\": \"\",",
        "      \"depends_on\": [],",
        "      \"effort\": \"medium\",",
        "      \"created_at\": \"2025-12-15T05:23:36.072891\",",
        "      \"updated_at\": null,",
        "      \"completed_at\": null,",
        "      \"context\": {},",
        "      \"retrospective\": null",
        "    },",
        "    {",
        "      \"id\": \"T-20251215-052339-ceac-002\",",
        "      \"title\": \"Add tests for query/ranking.py to improve coverage from 25% to >80%\",",
        "      \"status\": \"pending\",",
        "      \"priority\": \"high\",",
        "      \"category\": \"testing\",",
        "      \"description\": \"\",",
        "      \"depends_on\": [],",
        "      \"effort\": \"medium\",",
        "      \"created_at\": \"2025-12-15T05:23:39.292952\",",
        "      \"updated_at\": null,",
        "      \"completed_at\": null,",
        "      \"context\": {},",
        "      \"retrospective\": null",
        "    },",
        "    {",
        "      \"id\": \"T-20251215-052342-ceac-003\",",
        "      \"title\": \"Add tests for query/passages.py to improve coverage from 43% to >80%\",",
        "      \"status\": \"pending\",",
        "      \"priority\": \"medium\",",
        "      \"category\": \"testing\",",
        "      \"description\": \"\",",
        "      \"depends_on\": [],",
        "      \"effort\": \"medium\",",
        "      \"created_at\": \"2025-12-15T05:23:42.361308\",",
        "      \"updated_at\": null,",
        "      \"completed_at\": null,",
        "      \"context\": {},",
        "      \"retrospective\": null",
        "    },",
        "    {",
        "      \"id\": \"T-20251215-052345-ceac-004\",",
        "      \"title\": \"Add tests for query/definitions.py to improve coverage from 30% to >80%\",",
        "      \"status\": \"pending\",",
        "      \"priority\": \"medium\",",
        "      \"category\": \"testing\",",
        "      \"description\": \"\",",
        "      \"depends_on\": [],",
        "      \"effort\": \"medium\",",
        "      \"created_at\": \"2025-12-15T05:23:45.335215\",",
        "      \"updated_at\": null,",
        "      \"completed_at\": null,",
        "      \"context\": {},",
        "      \"retrospective\": null",
        "    },",
        "    {",
        "      \"id\": \"T-20251215-052354-ceac-005\",",
        "      \"title\": \"Add tests for query/chunking.py to improve coverage from 43% to >80%\",",
        "      \"status\": \"pending\",",
        "      \"priority\": \"medium\",",
        "      \"category\": \"testing\",",
        "      \"description\": \"\",",
        "      \"depends_on\": [],",
        "      \"effort\": \"medium\",",
        "      \"created_at\": \"2025-12-15T05:23:54.641886\",",
        "      \"updated_at\": null,",
        "      \"completed_at\": null,",
        "      \"context\": {},",
        "      \"retrospective\": null",
        "    },",
        "    {",
        "      \"id\": \"T-20251215-052357-ceac-006\",",
        "      \"title\": \"Add tests for gaps.py to improve coverage from 9% to >80%\",",
        "      \"status\": \"pending\",",
        "      \"priority\": \"medium\",",
        "      \"category\": \"testing\",",
        "      \"description\": \"\",",
        "      \"depends_on\": [],",
        "      \"effort\": \"medium\",",
        "      \"created_at\": \"2025-12-15T05:23:57.795144\",",
        "      \"updated_at\": null,",
        "      \"completed_at\": null,",
        "      \"context\": {},",
        "      \"retrospective\": null",
        "    },",
        "    {",
        "      \"id\": \"T-20251215-052400-ceac-007\",",
        "      \"title\": \"Add tests for embeddings.py to improve coverage from 31% to >80%\",",
        "      \"status\": \"pending\",",
        "      \"priority\": \"medium\",",
        "      \"category\": \"testing\",",
        "      \"description\": \"\",",
        "      \"depends_on\": [],",
        "      \"effort\": \"medium\",",
        "      \"created_at\": \"2025-12-15T05:24:00.910240\",",
        "      \"updated_at\": null,",
        "      \"completed_at\": null,",
        "      \"context\": {},",
        "      \"retrospective\": null",
        "    },",
        "    {",
        "      \"id\": \"T-20251215-052403-ceac-008\",",
        "      \"title\": \"Add tests for diff.py to improve coverage from 30% to >80%\",",
        "      \"status\": \"pending\",",
        "      \"priority\": \"low\",",
        "      \"category\": \"testing\",",
        "      \"description\": \"\",",
        "      \"depends_on\": [],",
        "      \"effort\": \"medium\",",
        "      \"created_at\": \"2025-12-15T05:24:03.990703\",",
        "      \"updated_at\": null,",
        "      \"completed_at\": null,",
        "      \"context\": {},",
        "      \"retrospective\": null",
        "    },",
        "    {",
        "      \"id\": \"T-20251215-052412-ceac-009\",",
        "      \"title\": \"Add tests for patterns.py to improve coverage from 32% to >80%\",",
        "      \"status\": \"pending\",",
        "      \"priority\": \"low\",",
        "      \"category\": \"testing\",",
        "      \"description\": \"\",",
        "      \"depends_on\": [],",
        "      \"effort\": \"medium\",",
        "      \"created_at\": \"2025-12-15T05:24:12.737164\",",
        "      \"updated_at\": null,",
        "      \"completed_at\": null,",
        "      \"context\": {},",
        "      \"retrospective\": null",
        "    },",
        "    {",
        "      \"id\": \"T-20251215-052415-ceac-010\",",
        "      \"title\": \"Add tests for fluent.py to improve coverage from 25% to >80%\",",
        "      \"status\": \"pending\",",
        "      \"priority\": \"low\",",
        "      \"category\": \"testing\",",
        "      \"description\": \"\",",
        "      \"depends_on\": [],",
        "      \"effort\": \"medium\",",
        "      \"created_at\": \"2025-12-15T05:24:15.646917\",",
        "      \"updated_at\": null,",
        "      \"completed_at\": null,",
        "      \"context\": {},",
        "      \"retrospective\": null",
        "    },",
        "    {",
        "      \"id\": \"T-20251215-052418-ceac-011\",",
        "      \"title\": \"Add tests for code_concepts.py to improve coverage from 54% to >80%\",",
        "      \"status\": \"pending\",",
        "      \"priority\": \"low\",",
        "      \"category\": \"testing\",",
        "      \"description\": \"\",",
        "      \"depends_on\": [],",
        "      \"effort\": \"medium\",",
        "      \"created_at\": \"2025-12-15T05:24:18.762807\",",
        "      \"updated_at\": null,",
        "      \"completed_at\": null,",
        "      \"context\": {},",
        "      \"retrospective\": null",
        "    },",
        "    {",
        "      \"id\": \"T-20251215-052421-ceac-012\",",
        "      \"title\": \"Add tests for query/expansion.py to improve coverage from 53% to >80%\",",
        "      \"status\": \"pending\",",
        "      \"priority\": \"medium\",",
        "      \"category\": \"testing\",",
        "      \"description\": \"\",",
        "      \"depends_on\": [],",
        "      \"effort\": \"medium\",",
        "      \"created_at\": \"2025-12-15T05:24:21.608203\",",
        "      \"updated_at\": null,",
        "      \"completed_at\": null,",
        "      \"context\": {},",
        "      \"retrospective\": null",
        "    },",
        "    {",
        "      \"id\": \"T-20251215-052431-ceac-013\",",
        "      \"title\": \"Add tests for analysis.py to improve coverage from 60% to >80%\",",
        "      \"status\": \"pending\",",
        "      \"priority\": \"high\",",
        "      \"category\": \"testing\",",
        "      \"description\": \"\",",
        "      \"depends_on\": [],",
        "      \"effort\": \"medium\",",
        "      \"created_at\": \"2025-12-15T05:24:31.651297\",",
        "      \"updated_at\": null,",
        "      \"completed_at\": null,",
        "      \"context\": {},",
        "      \"retrospective\": null",
        "    },",
        "    {",
        "      \"id\": \"T-20251215-052434-ceac-014\",",
        "      \"title\": \"Add tests for semantics.py to improve coverage from 60% to >80%\",",
        "      \"status\": \"pending\",",
        "      \"priority\": \"medium\",",
        "      \"category\": \"testing\",",
        "      \"description\": \"\",",
        "      \"depends_on\": [],",
        "      \"effort\": \"medium\",",
        "      \"created_at\": \"2025-12-15T05:24:34.754429\",",
        "      \"updated_at\": null,",
        "      \"completed_at\": null,",
        "      \"context\": {},",
        "      \"retrospective\": null",
        "    },",
        "    {",
        "      \"id\": \"T-20251215-052437-ceac-015\",",
        "      \"title\": \"Add tests for config.py to improve coverage from 67% to >80%\",",
        "      \"status\": \"pending\",",
        "      \"priority\": \"medium\",",
        "      \"category\": \"testing\",",
        "      \"description\": \"\",",
        "      \"depends_on\": [],",
        "      \"effort\": \"medium\",",
        "      \"created_at\": \"2025-12-15T05:24:37.732544\",",
        "      \"updated_at\": null,",
        "      \"completed_at\": null,",
        "      \"context\": {},",
        "      \"retrospective\": null",
        "    },",
        "    {",
        "      \"id\": \"T-20251215-052440-ceac-016\",",
        "      \"title\": \"Add tests for fingerprint.py to improve coverage from 65% to >80%\",",
        "      \"status\": \"pending\",",
        "      \"priority\": \"medium\",",
        "      \"category\": \"testing\",",
        "      \"description\": \"\",",
        "      \"depends_on\": [],",
        "      \"effort\": \"medium\",",
        "      \"created_at\": \"2025-12-15T05:24:40.799030\",",
        "      \"updated_at\": null,",
        "      \"completed_at\": null,",
        "      \"context\": {},",
        "      \"retrospective\": null",
        "    }",
        "  ]",
        "}"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "tests/test_edge_cases.py",
      "function": "class TestComputationEdgeCases(unittest.TestCase):",
      "start_line": 494,
      "lines_added": [
        "        \"\"\"Test scoring computation with single document.\"\"\"",
        "        from cortical.config import CorticalConfig",
        "",
        "        # Test with TF-IDF (IDF = log(1/1) = 0 for single doc)",
        "        config_tfidf = CorticalConfig(scoring_algorithm='tfidf')",
        "        processor_tfidf = CorticalTextProcessor(config=config_tfidf)",
        "        processor_tfidf.process_document(\"only_doc\", \"neural networks machine learning\")",
        "        processor_tfidf.compute_tfidf(verbose=False)",
        "        layer0 = processor_tfidf.get_layer(CorticalLayer.TOKENS)",
        "            # TF-IDF should be 0 for single document corpus (IDF = log(1/1) = 0)",
        "        # Test with BM25 (uses different IDF formula, won't be zero)",
        "        config_bm25 = CorticalConfig(scoring_algorithm='bm25')",
        "        processor_bm25 = CorticalTextProcessor(config=config_bm25)",
        "        processor_bm25.process_document(\"only_doc\", \"neural networks machine learning\")",
        "        processor_bm25.compute_tfidf(verbose=False)",
        "",
        "        layer0_bm25 = processor_bm25.get_layer(CorticalLayer.TOKENS)",
        "        for col in layer0_bm25:",
        "            # BM25 IDF = log((N-df+0.5)/(df+0.5)+1) > 0 even for single doc",
        "            self.assertGreater(col.tfidf, 0.0)",
        ""
      ],
      "lines_removed": [
        "        \"\"\"Test TF-IDF computation with single document.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(\"only_doc\", \"neural networks machine learning\")",
        "        processor.compute_tfidf(verbose=False)",
        "        # With single document, IDF should be 0 (log(1/1) = 0)",
        "        layer0 = processor.get_layer(CorticalLayer.TOKENS)",
        "            # TF-IDF should be 0 for single document corpus"
      ],
      "context_before": [
        "    def test_compute_all_on_empty_corpus(self):",
        "        \"\"\"Test compute_all on empty processor.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        # Should handle gracefully without crashing",
        "        try:",
        "            processor.compute_all(verbose=False)",
        "        except Exception as e:",
        "            self.fail(f\"compute_all on empty corpus raised {type(e).__name__}: {e}\")",
        "",
        "    def test_compute_tfidf_single_document(self):"
      ],
      "context_after": [
        "",
        "        for col in layer0:",
        "            self.assertEqual(col.tfidf, 0.0)",
        "",
        "    def test_compute_importance_on_disconnected_graph(self):",
        "        \"\"\"Test PageRank on graph with no connections.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        # Single word documents with no shared terms",
        "        processor.process_document(\"doc1\", \"aardvark\")",
        "        processor.process_document(\"doc2\", \"zeppelin\")",
        "",
        "        # compute_importance should handle disconnected components",
        "        try:",
        "            processor.compute_importance(verbose=False)"
      ],
      "change_type": "modify"
    },
    {
      "file": "tests/unit/test_processor_core.py",
      "function": "class TestComputeWrapperMethods(unittest.TestCase):",
      "start_line": 1300,
      "lines_added": [
        "    @patch('cortical.analysis.compute_bm25')",
        "    def test_compute_tfidf_calls_bm25_by_default(self, mock_bm25):",
        "        \"\"\"compute_tfidf delegates to BM25 by default (new default algorithm).\"\"\"",
        "        # BM25 is now the default algorithm",
        "        mock_bm25.assert_called_once()",
        "",
        "    @patch('cortical.analysis.compute_tfidf')",
        "    def test_compute_tfidf_calls_tfidf_when_configured(self, mock_tfidf):",
        "        \"\"\"compute_tfidf delegates to TF-IDF when explicitly configured.\"\"\"",
        "        from cortical.config import CorticalConfig",
        "        config = CorticalConfig(scoring_algorithm='tfidf')",
        "        processor = CorticalTextProcessor(config=config)",
        "        processor.process_document(\"doc1\", \"test content\")",
        "",
        "        processor.compute_tfidf(verbose=False)",
        ""
      ],
      "lines_removed": [
        "    @patch('cortical.analysis.compute_tfidf')",
        "    def test_compute_tfidf_calls_analysis(self, mock_tfidf):",
        "        \"\"\"compute_tfidf delegates to analysis module.\"\"\""
      ],
      "context_before": [
        "    def test_compute_importance_calls_analysis(self, mock_pagerank):",
        "        \"\"\"compute_importance delegates to analysis module.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(\"doc1\", \"test content\")",
        "",
        "        processor.compute_importance(verbose=False)",
        "",
        "        # Should call PageRank for tokens and bigrams",
        "        self.assertEqual(mock_pagerank.call_count, 2)",
        ""
      ],
      "context_after": [
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(\"doc1\", \"test content\")",
        "",
        "        processor.compute_tfidf(verbose=False)",
        "",
        "        mock_tfidf.assert_called_once()",
        "",
        "    @patch('cortical.analysis.compute_document_connections')",
        "    def test_compute_document_connections_calls_analysis(self, mock_doc_conn):",
        "        \"\"\"compute_document_connections delegates to analysis module.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(\"doc1\", \"test content\")",
        "",
        "        processor.compute_document_connections(min_shared_terms=5, verbose=False)",
        ""
      ],
      "change_type": "modify"
    },
    {
      "file": "tests/unit/test_query_search.py",
      "function": "These tests use mock layers and don't require a full processor.",
      "start_line": 18,
      "lines_added": [
        "    graph_boosted_search,"
      ],
      "lines_removed": [],
      "context_before": [
        "import pytest",
        "from unittest.mock import Mock",
        "",
        "from cortical.query.search import (",
        "    find_documents_for_query,",
        "    fast_find_documents,",
        "    build_document_index,",
        "    search_with_index,",
        "    query_with_spreading_activation,",
        "    find_related_documents,"
      ],
      "context_after": [
        ")",
        "from cortical.tokenizer import Tokenizer",
        "from tests.unit.mocks import (",
        "    MockMinicolumn,",
        "    MockHierarchicalLayer,",
        "    MockLayers,",
        "    LayerBuilder,",
        ")",
        "",
        ""
      ],
      "change_type": "add"
    },
    {
      "file": "tests/unit/test_query_search.py",
      "function": "class TestFindRelatedDocuments:",
      "start_line": 1041,
      "lines_added": [
        "",
        "",
        "# =============================================================================",
        "# GRAPH_BOOSTED_SEARCH TESTS",
        "# =============================================================================",
        "",
        "",
        "class TestGraphBoostedSearch:",
        "    \"\"\"Tests for graph_boosted_search hybrid scoring function.\"\"\"",
        "",
        "    def test_basic_search(self):",
        "        \"\"\"Basic search returns ranked documents.\"\"\"",
        "        # Create tokens with tfidf and pagerank",
        "        neural = MockMinicolumn(",
        "            content=\"neural\",",
        "            id=\"L0_neural\",",
        "            layer=MockLayers.TOKENS,",
        "            tfidf=1.0,",
        "            tfidf_per_doc={\"doc1\": 0.8, \"doc2\": 0.5},",
        "            document_ids={\"doc1\", \"doc2\"},",
        "            pagerank=0.3,",
        "            lateral_connections={}",
        "        )",
        "        networks = MockMinicolumn(",
        "            content=\"networks\",",
        "            id=\"L0_networks\",",
        "            layer=MockLayers.TOKENS,",
        "            tfidf=0.9,",
        "            tfidf_per_doc={\"doc1\": 0.7, \"doc3\": 0.4},",
        "            document_ids={\"doc1\", \"doc3\"},",
        "            pagerank=0.2,",
        "            lateral_connections={}",
        "        )",
        "",
        "        layers = MockLayers.empty()",
        "        layers[MockLayers.TOKENS] = MockHierarchicalLayer([neural, networks])",
        "        layers[MockLayers.DOCUMENTS] = MockHierarchicalLayer([])",
        "        tokenizer = Tokenizer()",
        "",
        "        results = graph_boosted_search(\"neural networks\", layers, tokenizer, top_n=3)",
        "",
        "        assert len(results) > 0",
        "        # doc1 should rank highest (has both terms)",
        "        assert results[0][0] == \"doc1\"",
        "",
        "    def test_empty_query(self):",
        "        \"\"\"Empty query returns empty results.\"\"\"",
        "        layers = MockLayers.single_term(\"term\", tfidf=1.0, doc_ids=[\"doc1\"])",
        "        tokenizer = Tokenizer()",
        "",
        "        results = graph_boosted_search(\"\", layers, tokenizer, top_n=5)",
        "        assert results == []",
        "",
        "    def test_no_matching_terms(self):",
        "        \"\"\"Query with no matching terms returns empty results.\"\"\"",
        "        layers = MockLayers.single_term(\"other\", tfidf=1.0, doc_ids=[\"doc1\"])",
        "        tokenizer = Tokenizer()",
        "",
        "        results = graph_boosted_search(\"nonexistent\", layers, tokenizer, top_n=5)",
        "        assert results == []",
        "",
        "    def test_pagerank_boost(self):",
        "        \"\"\"Documents with high-PageRank terms get boosted.\"\"\"",
        "        # High PageRank term (use \"significant\" instead of \"important\" which is a stop word)",
        "        significant = MockMinicolumn(",
        "            content=\"significant\",",
        "            id=\"L0_significant\",",
        "            layer=MockLayers.TOKENS,",
        "            tfidf=1.0,",
        "            tfidf_per_doc={\"doc1\": 1.0},",
        "            document_ids={\"doc1\"},",
        "            pagerank=0.9,  # High importance",
        "            lateral_connections={}",
        "        )",
        "        # Low PageRank term",
        "        common = MockMinicolumn(",
        "            content=\"common\",",
        "            id=\"L0_common\",",
        "            layer=MockLayers.TOKENS,",
        "            tfidf=1.0,",
        "            tfidf_per_doc={\"doc2\": 1.0},",
        "            document_ids={\"doc2\"},",
        "            pagerank=0.1,  # Low importance",
        "            lateral_connections={}",
        "        )",
        "",
        "        layers = MockLayers.empty()",
        "        layers[MockLayers.TOKENS] = MockHierarchicalLayer([significant, common])",
        "        layers[MockLayers.DOCUMENTS] = MockHierarchicalLayer([])",
        "        tokenizer = Tokenizer()",
        "",
        "        # Search for both terms (using \"significant\" instead of \"important\")",
        "        results = graph_boosted_search(",
        "            \"significant common\", layers, tokenizer, top_n=5,",
        "            pagerank_weight=0.5  # High PageRank influence",
        "        )",
        "",
        "        assert len(results) == 2",
        "        # doc1 should rank higher due to PageRank boost",
        "        assert results[0][0] == \"doc1\"",
        "",
        "    def test_respects_top_n(self):",
        "        \"\"\"Returns at most top_n results.\"\"\"",
        "        terms = []",
        "        for i in range(10):",
        "            terms.append(MockMinicolumn(",
        "                content=f\"term{i}\",",
        "                id=f\"L0_term{i}\",",
        "                layer=MockLayers.TOKENS,",
        "                tfidf=1.0,",
        "                tfidf_per_doc={f\"doc{i}\": 1.0},",
        "                document_ids={f\"doc{i}\"},",
        "                pagerank=0.1,",
        "                lateral_connections={}",
        "            ))",
        "",
        "        layers = MockLayers.empty()",
        "        layers[MockLayers.TOKENS] = MockHierarchicalLayer(terms)",
        "        layers[MockLayers.DOCUMENTS] = MockHierarchicalLayer([])",
        "        tokenizer = Tokenizer()",
        "",
        "        # Query that matches multiple docs",
        "        results = graph_boosted_search(",
        "            \" \".join(f\"term{i}\" for i in range(10)),",
        "            layers, tokenizer, top_n=3",
        "        )",
        "",
        "        assert len(results) <= 3"
      ],
      "lines_removed": [],
      "context_before": [
        "",
        "        layers = MockLayers.empty()",
        "        layer3 = MockHierarchicalLayer([doc1, doc2])",
        "        layers[MockLayers.DOCUMENTS] = layer3",
        "",
        "        result = find_related_documents(\"doc1\", layers)",
        "",
        "        # If this works, get_by_id was used successfully",
        "        assert len(result) == 1",
        "        assert result[0][0] == \"doc2\""
      ],
      "context_after": [],
      "change_type": "add"
    }
  ],
  "hour_of_day": 12,
  "day_of_week": "Monday",
  "seconds_since_last_commit": -4666,
  "is_merge": true,
  "is_initial": false,
  "parent_count": 2,
  "session_id": null,
  "related_chats": [],
  "ci_result": null,
  "reverted": false,
  "amended": false
}