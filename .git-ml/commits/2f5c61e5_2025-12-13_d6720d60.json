{
  "hash": "2f5c61e506a136c1ad8af2d7c652b107904708bd",
  "message": "Fix 3 critical bugs: atomic writes, path traversal, counter overflow",
  "author": "Claude",
  "timestamp": "2025-12-13 22:48:31 +0000",
  "branch": "claude/multi-index-design-DvifZ",
  "files_changed": [
    "scripts/consolidate_tasks.py",
    "scripts/task_utils.py",
    "tasks/2025-12-13_22-42-20_6ac7.json",
    "tests/integration/test_task_integration.py",
    "tests/unit/test_task_utils.py"
  ],
  "insertions": 178,
  "deletions": 22,
  "hunks": [
    {
      "file": "scripts/consolidate_tasks.py",
      "function": "def write_consolidated_file(",
      "start_line": 198,
      "lines_added": [
        "    # Atomic write: temp file then rename",
        "    temp_filepath = filepath.with_suffix('.json.tmp')",
        "    try:",
        "        with open(temp_filepath, 'w') as f:",
        "            json.dump(data, f, indent=2)",
        "            f.flush()",
        "            os.fsync(f.fileno())",
        "        temp_filepath.rename(filepath)",
        "    except Exception:",
        "        if temp_filepath.exists():",
        "            temp_filepath.unlink()",
        "        raise",
        "def _validate_archive_path(tasks_dir: Path, archive_path: Path) -> None:",
        "    \"\"\"",
        "    Validate that archive_path is within or under tasks_dir.",
        "",
        "    Raises:",
        "        ValueError: If archive_path escapes tasks_dir boundary",
        "    \"\"\"",
        "    # Resolve to absolute paths for comparison",
        "    tasks_resolved = tasks_dir.resolve()",
        "    archive_resolved = archive_path.resolve()",
        "",
        "    # Check that archive is within tasks directory",
        "    try:",
        "        archive_resolved.relative_to(tasks_resolved)",
        "    except ValueError:",
        "        raise ValueError(",
        "            f\"Archive path '{archive_path}' must be within tasks directory '{tasks_dir}'. \"",
        "            f\"Path traversal is not allowed for security reasons.\"",
        "        )",
        "",
        "",
        "        archive_dir: Where to move old files (default: tasks/archive/).",
        "                     Must be within tasks_dir for security.",
        "",
        "    Raises:",
        "        ValueError: If archive_dir attempts path traversal outside tasks_dir",
        "",
        "    # Default to subdirectory, validate if custom path provided",
        "    if archive_dir is None:",
        "        archive_path = dir_path / \"archive\"",
        "    else:",
        "        archive_path = Path(archive_dir)",
        "        # Security: validate path stays within tasks directory",
        "        _validate_archive_path(dir_path, archive_path)",
        ""
      ],
      "lines_removed": [
        "    with open(filepath, 'w') as f:",
        "        json.dump(data, f, indent=2)",
        "        archive_dir: Where to move old files (default: tasks/archive/)",
        "    archive_path = Path(archive_dir or (dir_path / \"archive\"))"
      ],
      "context_before": [
        "",
        "    data = {",
        "        \"version\": 1,",
        "        \"type\": \"consolidated\",",
        "        \"session_id\": sid,",
        "        \"created_at\": datetime.now().isoformat(),",
        "        \"task_count\": len(tasks),",
        "        \"tasks\": [t.to_dict() for t in tasks]",
        "    }",
        ""
      ],
      "context_after": [
        "",
        "    return filepath",
        "",
        "",
        "def archive_old_session_files(",
        "    tasks_dir: str,",
        "    archive_dir: Optional[str] = None,",
        "    keep_consolidated: bool = True",
        ") -> List[Path]:",
        "    \"\"\"",
        "    Move old session files to archive after consolidation.",
        "",
        "    Args:",
        "        tasks_dir: Directory containing task files",
        "        keep_consolidated: Don't archive consolidated_*.json files",
        "",
        "    Returns:",
        "        List of archived file paths",
        "    \"\"\"",
        "    dir_path = Path(tasks_dir)",
        "    archive_path.mkdir(parents=True, exist_ok=True)",
        "",
        "    archived = []",
        "    for filepath in dir_path.glob(\"*.json\"):",
        "        if keep_consolidated and filepath.name.startswith(\"consolidated_\"):",
        "            continue",
        "",
        "        dest = archive_path / filepath.name",
        "        shutil.move(str(filepath), str(dest))",
        "        archived.append(dest)"
      ],
      "change_type": "modify"
    },
    {
      "file": "scripts/task_utils.py",
      "function": "class TaskSession:",
      "start_line": 149,
      "lines_added": [
        "        # Format: T-YYYYMMDD-HHMMSS-SSSS-NNN where NNN is task number (supports 999 tasks)",
        "        return f\"T-{date_str}-{time_str}-{self.session_id}-{self._task_counter:03d}\""
      ],
      "lines_removed": [
        "        # Format: T-YYYYMMDD-HHMMSS-SSSS-NN where NN is task number",
        "        return f\"T-{date_str}-{time_str}-{self.session_id}-{self._task_counter:02d}\""
      ],
      "context_before": [
        "    def new_task_id(self) -> str:",
        "        \"\"\"Generate a new task ID with this session's suffix and counter.",
        "",
        "        The counter ensures unique IDs even when multiple tasks are created",
        "        within the same second.",
        "        \"\"\"",
        "        self._task_counter += 1",
        "        now = datetime.now()",
        "        date_str = now.strftime(\"%Y%m%d\")",
        "        time_str = now.strftime(\"%H%M%S\")"
      ],
      "context_after": [
        "",
        "    def create_task(",
        "        self,",
        "        title: str,",
        "        priority: str = \"medium\",",
        "        category: str = \"general\",",
        "        description: str = \"\",",
        "        depends_on: Optional[List[str]] = None,",
        "        effort: str = \"medium\",",
        "        context: Optional[Dict[str, Any]] = None"
      ],
      "change_type": "modify"
    },
    {
      "file": "scripts/task_utils.py",
      "function": "class TaskSession:",
      "start_line": 198,
      "lines_added": [
        "        Save session tasks to a JSON file atomically.",
        "",
        "        Uses write-to-temp-then-rename pattern to prevent data loss",
        "        if the process crashes during write.",
        "",
        "        Raises:",
        "            OSError: If write or rename fails",
        "        temp_filepath = filepath.with_suffix('.json.tmp')",
        "        try:",
        "            # Write to temp file first",
        "            with open(temp_filepath, 'w') as f:",
        "                json.dump(data, f, indent=2)",
        "                f.flush()",
        "                os.fsync(f.fileno())  # Ensure data is on disk",
        "",
        "            # Atomic rename (on POSIX systems)",
        "            temp_filepath.rename(filepath)",
        "        except Exception:",
        "            # Clean up temp file on failure",
        "            if temp_filepath.exists():",
        "                temp_filepath.unlink()",
        "            raise"
      ],
      "lines_removed": [
        "        Save session tasks to a JSON file.",
        "        with open(filepath, 'w') as f:",
        "            json.dump(data, f, indent=2)"
      ],
      "context_before": [
        "        return task",
        "",
        "    def get_filename(self) -> str:",
        "        \"\"\"Get the session filename.\"\"\"",
        "        dt = datetime.fromisoformat(self.started_at)",
        "        timestamp = dt.strftime(\"%Y-%m-%d_%H-%M-%S\")",
        "        return f\"{timestamp}_{self.session_id}.json\"",
        "",
        "    def save(self, tasks_dir: Optional[str] = None) -> Path:",
        "        \"\"\""
      ],
      "context_after": [
        "",
        "        Args:",
        "            tasks_dir: Directory for task files (default: tasks/)",
        "",
        "        Returns:",
        "            Path to the saved file",
        "        \"\"\"",
        "        dir_path = Path(tasks_dir or self.tasks_dir)",
        "        dir_path.mkdir(parents=True, exist_ok=True)",
        "",
        "        filepath = dir_path / self.get_filename()",
        "",
        "        data = {",
        "            \"version\": 1,",
        "            \"session_id\": self.session_id,",
        "            \"started_at\": self.started_at,",
        "            \"saved_at\": datetime.now().isoformat(),",
        "            \"tasks\": [t.to_dict() for t in self.tasks]",
        "        }",
        "",
        "",
        "        return filepath",
        "",
        "    @classmethod",
        "    def load(cls, filepath: Path) -> 'TaskSession':",
        "        \"\"\"Load a session from file.\"\"\"",
        "        with open(filepath) as f:",
        "            data = json.load(f)",
        "",
        "        session = cls("
      ],
      "change_type": "modify"
    },
    {
      "file": "tasks/2025-12-13_22-42-20_6ac7.json",
      "function": null,
      "start_line": 1,
      "lines_added": [
        "  \"saved_at\": \"2025-12-13T22:48:20.764101\",",
        "      \"status\": \"completed\",",
        "      \"updated_at\": \"2025-12-13T22:48:20.519513\",",
        "      \"completed_at\": \"2025-12-13T22:48:20.519513\",",
        "      \"status\": \"completed\",",
        "      \"updated_at\": \"2025-12-13T22:48:20.628391\",",
        "      \"completed_at\": \"2025-12-13T22:48:20.628391\",",
        "      \"status\": \"completed\",",
        "      \"updated_at\": \"2025-12-13T22:48:20.763992\",",
        "      \"completed_at\": \"2025-12-13T22:48:20.763992\","
      ],
      "lines_removed": [
        "  \"saved_at\": \"2025-12-13T22:42:20.987134\",",
        "      \"status\": \"pending\",",
        "      \"updated_at\": null,",
        "      \"completed_at\": null,",
        "      \"status\": \"pending\",",
        "      \"updated_at\": null,",
        "      \"completed_at\": null,",
        "      \"status\": \"pending\",",
        "      \"updated_at\": null,",
        "      \"completed_at\": null,"
      ],
      "context_before": [
        "{",
        "  \"version\": 1,",
        "  \"session_id\": \"6ac7\",",
        "  \"started_at\": \"2025-12-13T22:42:20.986896\","
      ],
      "context_after": [
        "  \"tasks\": [",
        "    {",
        "      \"id\": \"T-20251213-224220-6ac7-01\",",
        "      \"title\": \"Fix non-atomic file writes (data loss risk)\",",
        "      \"priority\": \"high\",",
        "      \"category\": \"bugfix\",",
        "      \"description\": \"TaskSession.save() should write to .tmp then atomic rename\",",
        "      \"depends_on\": [],",
        "      \"effort\": \"small\",",
        "      \"created_at\": \"2025-12-13T22:42:20.986937\",",
        "      \"context\": {",
        "        \"files\": [",
        "          \"scripts/task_utils.py\"",
        "        ],",
        "        \"line\": 229",
        "      }",
        "    },",
        "    {",
        "      \"id\": \"T-20251213-224220-6ac7-02\",",
        "      \"title\": \"Fix path traversal vulnerability in archive\",",
        "      \"priority\": \"high\",",
        "      \"category\": \"security\",",
        "      \"description\": \"archive_old_session_files() must validate paths stay within tasks/\",",
        "      \"depends_on\": [],",
        "      \"effort\": \"small\",",
        "      \"created_at\": \"2025-12-13T22:42:20.986947\",",
        "      \"context\": {",
        "        \"files\": [",
        "          \"scripts/consolidate_tasks.py\"",
        "        ],",
        "        \"line\": 214",
        "      }",
        "    },",
        "    {",
        "      \"id\": \"T-20251213-224220-6ac7-03\",",
        "      \"title\": \"Fix task counter overflow at 100 tasks\",",
        "      \"priority\": \"high\",",
        "      \"category\": \"bugfix\",",
        "      \"description\": \"Expand counter format from 02d to 03d or use base36\",",
        "      \"depends_on\": [],",
        "      \"effort\": \"small\",",
        "      \"created_at\": \"2025-12-13T22:42:20.986955\",",
        "      \"context\": {",
        "        \"files\": [",
        "          \"scripts/task_utils.py\"",
        "        ],",
        "        \"line\": 156",
        "      }",
        "    },",
        "    {",
        "      \"id\": \"T-20251213-224220-6ac7-04\",",
        "      \"title\": \"Add workflow templates (bugfix, feature, refactor)\","
      ],
      "change_type": "modify"
    },
    {
      "file": "tests/integration/test_task_integration.py",
      "function": "class TestFileSystemResilience(unittest.TestCase):",
      "start_line": 398,
      "lines_added": [
        "class TestSecurityAndRobustness(unittest.TestCase):",
        "    \"\"\"Test security fixes and robustness improvements.\"\"\"",
        "",
        "    def setUp(self):",
        "        \"\"\"Create temporary directory for task files.\"\"\"",
        "        self.temp_dir = tempfile.mkdtemp()",
        "",
        "    def tearDown(self):",
        "        \"\"\"Clean up temporary directory.\"\"\"",
        "        shutil.rmtree(self.temp_dir)",
        "",
        "    def test_path_traversal_rejected(self):",
        "        \"\"\"Archive should reject path traversal attempts.\"\"\"",
        "        # Create a session with tasks",
        "        session = TaskSession()",
        "        session.create_task(title=\"Task\")",
        "        session.save(self.temp_dir)",
        "",
        "        # Attempt path traversal with ..",
        "        evil_archive = str(Path(self.temp_dir) / \"..\" / \"evil_dir\")",
        "        with self.assertRaises(ValueError) as cm:",
        "            archive_old_session_files(self.temp_dir, archive_dir=evil_archive)",
        "        self.assertIn(\"path traversal\", str(cm.exception).lower())",
        "",
        "    def test_absolute_path_outside_tasks_rejected(self):",
        "        \"\"\"Archive should reject absolute paths outside tasks directory.\"\"\"",
        "        session = TaskSession()",
        "        session.create_task(title=\"Task\")",
        "        session.save(self.temp_dir)",
        "",
        "        # Attempt to use /tmp as archive (outside tasks_dir)",
        "        with self.assertRaises(ValueError) as cm:",
        "            archive_old_session_files(self.temp_dir, archive_dir=\"/tmp\")",
        "        self.assertIn(\"must be within\", str(cm.exception).lower())",
        "",
        "    def test_subdirectory_archive_allowed(self):",
        "        \"\"\"Archive within tasks directory should be allowed.\"\"\"",
        "        session = TaskSession()",
        "        session.create_task(title=\"Task\")",
        "        session.save(self.temp_dir)",
        "",
        "        # Subdirectory should work fine",
        "        sub_archive = str(Path(self.temp_dir) / \"deep\" / \"archive\")",
        "        archived = archive_old_session_files(self.temp_dir, archive_dir=sub_archive)",
        "        self.assertEqual(len(archived), 1)",
        "        self.assertTrue(Path(sub_archive).exists())",
        "",
        "    def test_counter_supports_100_plus_tasks(self):",
        "        \"\"\"Session should support 100+ tasks without format issues.\"\"\"",
        "        session = TaskSession()",
        "",
        "        # Create 150 tasks",
        "        for i in range(150):",
        "            task = session.create_task(title=f\"Task {i}\")",
        "",
        "        # All IDs should be unique and follow pattern",
        "        all_ids = [t.id for t in session.tasks]",
        "        self.assertEqual(len(set(all_ids)), 150)",
        "",
        "        # Check format consistency (3-digit counter)",
        "        for task in session.tasks:",
        "            parts = task.id.split(\"-\")",
        "            self.assertEqual(len(parts), 5)  # T-YYYYMMDD-HHMMSS-XXXX-NNN",
        "            counter = parts[-1]",
        "            self.assertEqual(len(counter), 3)  # Always 3 digits",
        "",
        "    def test_atomic_write_no_temp_file_left_on_success(self):",
        "        \"\"\"Successful save should not leave temp files.\"\"\"",
        "        session = TaskSession()",
        "        session.create_task(title=\"Task\")",
        "        filepath = session.save(self.temp_dir)",
        "",
        "        # Check no .tmp files exist",
        "        tmp_files = list(Path(self.temp_dir).glob(\"*.tmp\"))",
        "        self.assertEqual(len(tmp_files), 0)",
        "",
        "        # Main file should exist",
        "        self.assertTrue(filepath.exists())",
        "",
        "    def test_save_creates_valid_json(self):",
        "        \"\"\"Saved file should be valid JSON after atomic write.\"\"\"",
        "        session = TaskSession()",
        "        for i in range(10):",
        "            session.create_task(title=f\"Task {i}\")",
        "        filepath = session.save(self.temp_dir)",
        "",
        "        # Should load without error",
        "        with open(filepath) as f:",
        "            data = json.load(f)",
        "",
        "        self.assertEqual(len(data[\"tasks\"]), 10)",
        "        self.assertEqual(data[\"session_id\"], session.session_id)",
        "",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "        \"\"\"save() should create directory if it doesn't exist.\"\"\"",
        "        nested_dir = Path(self.temp_dir) / \"nested\" / \"deep\" / \"tasks\"",
        "        session = TaskSession()",
        "        session.create_task(title=\"Task\")",
        "        filepath = session.save(str(nested_dir))",
        "",
        "        self.assertTrue(filepath.exists())",
        "        self.assertTrue(nested_dir.exists())",
        "",
        ""
      ],
      "context_after": [
        "if __name__ == \"__main__\":",
        "    unittest.main()"
      ],
      "change_type": "add"
    },
    {
      "file": "tests/unit/test_task_utils.py",
      "function": "class TestTaskSession(unittest.TestCase):",
      "start_line": 145,
      "lines_added": [
        "        # Task counters should be different (3-digit format)",
        "        self.assertEqual(counter1, \"001\")",
        "        self.assertEqual(counter2, \"002\")"
      ],
      "lines_removed": [
        "        # Task counters should be different",
        "        self.assertEqual(counter1, \"01\")",
        "        self.assertEqual(counter2, \"02\")"
      ],
      "context_before": [
        "        # Session suffix is second to last part",
        "        parts1 = id1.split(\"-\")",
        "        parts2 = id2.split(\"-\")",
        "",
        "        session_suffix1 = parts1[-2]",
        "        session_suffix2 = parts2[-2]",
        "",
        "        self.assertEqual(session_suffix1, session_suffix2)",
        "        self.assertEqual(session_suffix1, session.session_id)",
        ""
      ],
      "context_after": [
        "        counter1 = parts1[-1]",
        "        counter2 = parts2[-1]",
        "        self.assertNotEqual(counter1, counter2)",
        "",
        "    def test_create_task(self):",
        "        \"\"\"create_task should add task to session.\"\"\"",
        "        session = TaskSession()",
        "        task = session.create_task(",
        "            title=\"Test task\",",
        "            priority=\"high\"",
        "        )",
        "",
        "        self.assertEqual(len(session.tasks), 1)"
      ],
      "change_type": "modify"
    }
  ],
  "hour_of_day": 22,
  "day_of_week": "Saturday",
  "seconds_since_last_commit": -140177,
  "is_merge": false,
  "is_initial": false,
  "parent_count": 1,
  "session_id": null,
  "related_chats": [],
  "ci_result": null,
  "reverted": false,
  "amended": false
}