{
  "hash": "9a0ede825e3a64605945a193335d6a31135b1281",
  "message": "Add intent-based query understanding (Task #50)",
  "author": "Claude",
  "timestamp": "2025-12-10 14:34:23 +0000",
  "branch": "claude/multi-index-design-DvifZ",
  "files_changed": [
    "TASK_LIST.md",
    "cortical/processor.py",
    "cortical/query.py",
    "tests/test_intent_query.py"
  ],
  "insertions": 486,
  "deletions": 13,
  "hunks": [
    {
      "file": "TASK_LIST.md",
      "function": "The system doesn't know that \"fetch\", \"get\", \"retrieve\", \"load\" are often interc",
      "start_line": 1717,
      "lines_added": [
        "**Files:** `cortical/query.py`, `cortical/processor.py`, `tests/test_intent_query.py`",
        "**Status:** [x] Completed",
        "**Solution Applied:**",
        "1. Added `parse_intent_query()` to extract action + subject + intent + expanded terms",
        "2. Added `ParsedIntent` TypedDict for structured results",
        "3. Added `QUESTION_INTENTS` mapping (where→location, how→implementation, what→definition, why→rationale, when→lifecycle)",
        "4. Added `ACTION_VERBS` frozenset with 50+ common programming verbs",
        "5. Added `search_by_intent()` for intent-aware document search",
        "6. Added processor wrapper methods",
        "7. Added 24 tests in `tests/test_intent_query.py`",
        "#   'question_word': 'where',",
        "#   'expanded_terms': ['handle', 'authentication', 'auth', 'login', ...]"
      ],
      "lines_removed": [
        "**Files:** `cortical/query.py`",
        "**Status:** [ ] Not Started",
        "**Solution:**",
        "1. Add `parse_intent_query()` to extract action + subject + context",
        "2. Map question words to search strategies:",
        "   - \"where\" → location/file search",
        "   - \"how\" → implementation search",
        "   - \"what\" → definition search",
        "   - \"why\" → comment/documentation search",
        "3. Weight results by intent match",
        "#   'expanded_terms': ['handle', 'auth', 'authentication', 'login', ...]"
      ],
      "context_before": [
        "",
        "**Concept Groups Implemented:**",
        "- retrieval, storage, deletion, auth, error, validation",
        "- transform, network, database, async, config, logging",
        "- testing, file, iteration, lifecycle, events",
        "",
        "---",
        "",
        "### 50. Add Intent-Based Query Understanding",
        ""
      ],
      "context_after": [
        "**Priority:** High",
        "",
        "**Problem:**",
        "Natural language queries like \"where do we handle authentication?\" aren't decomposed into searchable intents.",
        "",
        "",
        "**Example:**",
        "```python",
        "parse_intent_query(\"where do we handle authentication?\")",
        "# Returns: {",
        "#   'action': 'handle',",
        "#   'subject': 'authentication',",
        "#   'intent': 'location',",
        "# }",
        "```",
        "",
        "---",
        "",
        "### 51. Add Fingerprint Export API",
        "",
        "**Files:** `cortical/processor.py`, `cortical/embeddings.py`",
        "**Status:** [ ] Not Started",
        "**Priority:** Medium"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/processor.py",
      "function": "class CorticalTextProcessor:",
      "start_line": 1138,
      "lines_added": [
        "    def parse_intent_query(self, query_text: str) -> Dict:",
        "        \"\"\"",
        "        Parse a natural language query to extract intent and searchable terms.",
        "",
        "        Analyzes queries like \"where do we handle authentication?\" to identify:",
        "        - Question word (where) -> intent type (location)",
        "        - Action verb (handle) -> search for handling code",
        "        - Subject (authentication) -> main topic with synonyms",
        "",
        "        Args:",
        "            query_text: Natural language query string",
        "",
        "        Returns:",
        "            Dict with 'action', 'subject', 'intent', 'question_word', 'expanded_terms'",
        "        \"\"\"",
        "        return query_module.parse_intent_query(query_text)",
        "",
        "    def search_by_intent(self, query_text: str, top_n: int = 5) -> List[Tuple[str, float, Dict]]:",
        "        \"\"\"",
        "        Search the corpus using intent-based query understanding.",
        "",
        "        Parses the query to understand intent, expands terms using code concepts,",
        "        then searches with appropriate weighting based on intent type.",
        "",
        "        Args:",
        "            query_text: Natural language query string",
        "            top_n: Number of results to return",
        "",
        "        Returns:",
        "            List of (doc_id, score, parsed_intent) tuples",
        "        \"\"\"",
        "        return query_module.search_by_intent(",
        "            query_text,",
        "            self.layers,",
        "            self.tokenizer,",
        "            top_n=top_n",
        "        )",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "        \"\"\"",
        "        return query_module.expand_query(",
        "            query_text,",
        "            self.layers,",
        "            self.tokenizer,",
        "            max_expansions=max_expansions,",
        "            use_variants=True,",
        "            use_code_concepts=True",
        "        )",
        ""
      ],
      "context_after": [
        "    def expand_query_semantic(self, query_text: str, max_expansions: int = 10) -> Dict[str, float]:",
        "        return query_module.expand_query_semantic(query_text, self.layers, self.tokenizer, self.semantic_relations, max_expansions)",
        "",
        "    def complete_analogy(",
        "        self,",
        "        term_a: str,",
        "        term_b: str,",
        "        term_c: str,",
        "        top_n: int = 5,",
        "        use_embeddings: bool = True,"
      ],
      "change_type": "add"
    },
    {
      "file": "cortical/query.py",
      "function": null,
      "start_line": 2,
      "lines_added": [
        "from typing import Dict, List, Tuple, Optional, TypedDict",
        "import re",
        "from .code_concepts import expand_code_concepts, get_related_terms",
        "",
        "",
        "# Intent types for query understanding",
        "class ParsedIntent(TypedDict):",
        "    \"\"\"Structured representation of a parsed query intent.\"\"\"",
        "    action: Optional[str]       # The verb/action (e.g., \"handle\", \"implement\")",
        "    subject: Optional[str]      # The main subject (e.g., \"authentication\")",
        "    intent: str                 # Query intent type (location, implementation, definition, etc.)",
        "    question_word: Optional[str]  # Original question word if present",
        "    expanded_terms: List[str]   # All searchable terms with synonyms",
        "",
        "",
        "# Question word to intent mapping",
        "QUESTION_INTENTS = {",
        "    'where': 'location',      # Find location/file",
        "    'how': 'implementation',  # Find implementation details",
        "    'what': 'definition',     # Find definitions",
        "    'why': 'rationale',       # Find comments/documentation explaining reasoning",
        "    'when': 'lifecycle',      # Find when something happens (init, shutdown, etc.)",
        "    'which': 'selection',     # Find choices/options",
        "    'who': 'attribution',     # Find ownership/authorship (git blame territory)",
        "}",
        "",
        "# Common action verbs in code queries",
        "ACTION_VERBS = frozenset([",
        "    'handle', 'process', 'create', 'delete', 'update', 'fetch', 'get', 'set',",
        "    'load', 'save', 'store', 'validate', 'check', 'parse', 'format', 'convert',",
        "    'transform', 'render', 'display', 'show', 'hide', 'enable', 'disable',",
        "    'start', 'stop', 'init', 'initialize', 'setup', 'configure', 'connect',",
        "    'disconnect', 'send', 'receive', 'read', 'write', 'open', 'close',",
        "    'authenticate', 'authorize', 'login', 'logout', 'register', 'subscribe',",
        "    'publish', 'emit', 'listen', 'dispatch', 'trigger', 'call', 'invoke',",
        "    'execute', 'run', 'build', 'compile', 'test', 'deploy', 'implement',",
        "])"
      ],
      "lines_removed": [
        "from typing import Dict, List, Tuple, Optional",
        "from .code_concepts import expand_code_concepts"
      ],
      "context_before": [
        "Query Module",
        "============",
        "",
        "Query expansion and search functionality.",
        "",
        "Provides methods for expanding queries using lateral connections,",
        "concept clusters, and word variants, then searching the corpus",
        "using TF-IDF and graph-based scoring.",
        "\"\"\"",
        ""
      ],
      "context_after": [
        "from collections import defaultdict",
        "",
        "from .layers import CorticalLayer, HierarchicalLayer",
        "from .tokenizer import Tokenizer",
        "",
        "",
        "def expand_query(",
        "    query_text: str,",
        "    layers: Dict[CorticalLayer, HierarchicalLayer],",
        "    tokenizer: Tokenizer,",
        "    max_expansions: int = 10,",
        "    use_lateral: bool = True,",
        "    use_concepts: bool = True,",
        "    use_variants: bool = True,"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/query.py",
      "function": "def expand_query(",
      "start_line": 134,
      "lines_added": [
        "def parse_intent_query(query_text: str) -> ParsedIntent:",
        "    \"\"\"",
        "    Parse a natural language query to extract intent and searchable terms.",
        "",
        "    Analyzes queries like \"where do we handle authentication?\" to identify:",
        "    - Question word (where) -> intent type (location)",
        "    - Action verb (handle) -> search for handling code",
        "    - Subject (authentication) -> main topic with synonyms",
        "",
        "    Args:",
        "        query_text: Natural language query string",
        "",
        "    Returns:",
        "        ParsedIntent with action, subject, intent type, and expanded terms",
        "",
        "    Example:",
        "        >>> parse_intent_query(\"where do we handle authentication?\")",
        "        {",
        "            'action': 'handle',",
        "            'subject': 'authentication',",
        "            'intent': 'location',",
        "            'question_word': 'where',",
        "            'expanded_terms': ['handle', 'authentication', 'auth', 'login', ...]",
        "        }",
        "    \"\"\"",
        "    # Normalize query",
        "    query_lower = query_text.lower().strip()",
        "    query_lower = re.sub(r'[?!.,;:]', '', query_lower)  # Remove punctuation",
        "    words = query_lower.split()",
        "",
        "    if not words:",
        "        return ParsedIntent(",
        "            action=None,",
        "            subject=None,",
        "            intent='search',",
        "            question_word=None,",
        "            expanded_terms=[]",
        "        )",
        "",
        "    # Detect question word and intent",
        "    question_word = None",
        "    intent = 'search'  # Default intent",
        "",
        "    for word in words:",
        "        if word in QUESTION_INTENTS:",
        "            question_word = word",
        "            intent = QUESTION_INTENTS[word]",
        "            break",
        "",
        "    # Remove common filler words for parsing",
        "    filler_words = {'do', 'we', 'i', 'you', 'the', 'a', 'an', 'is', 'are', 'was',",
        "                    'were', 'can', 'could', 'should', 'would', 'does', 'did',",
        "                    'have', 'has', 'had', 'be', 'been', 'being', 'will', 'to'}",
        "    content_words = [w for w in words if w not in filler_words and w not in QUESTION_INTENTS]",
        "",
        "    # Find action verb",
        "    action = None",
        "    for word in content_words:",
        "        if word in ACTION_VERBS:",
        "            action = word",
        "            break",
        "",
        "    # Find subject (first non-action content word, or last content word)",
        "    subject = None",
        "    for word in content_words:",
        "        if word != action:",
        "            subject = word",
        "            break",
        "    if not subject and content_words:",
        "        subject = content_words[-1]",
        "",
        "    # Build expanded terms list",
        "    expanded_terms = []",
        "",
        "    # Add action and its synonyms",
        "    if action:",
        "        expanded_terms.append(action)",
        "        action_synonyms = get_related_terms(action, max_terms=5)",
        "        expanded_terms.extend(action_synonyms)",
        "",
        "    # Add subject and its synonyms",
        "    if subject:",
        "        expanded_terms.append(subject)",
        "        subject_synonyms = get_related_terms(subject, max_terms=5)",
        "        expanded_terms.extend(subject_synonyms)",
        "",
        "    # Add remaining content words",
        "    for word in content_words:",
        "        if word not in expanded_terms:",
        "            expanded_terms.append(word)",
        "",
        "    # Remove duplicates while preserving order",
        "    seen = set()",
        "    unique_terms = []",
        "    for term in expanded_terms:",
        "        if term not in seen:",
        "            seen.add(term)",
        "            unique_terms.append(term)",
        "",
        "    return ParsedIntent(",
        "        action=action,",
        "        subject=subject,",
        "        intent=intent,",
        "        question_word=question_word,",
        "        expanded_terms=unique_terms",
        "    )",
        "",
        "",
        "def search_by_intent(",
        "    query_text: str,",
        "    layers: Dict[CorticalLayer, HierarchicalLayer],",
        "    tokenizer: Tokenizer,",
        "    top_n: int = 5",
        ") -> List[Tuple[str, float, ParsedIntent]]:",
        "    \"\"\"",
        "    Search the corpus using intent-based query understanding.",
        "",
        "    Parses the query to understand intent, expands terms using code concepts,",
        "    then searches with appropriate weighting based on intent type.",
        "",
        "    Args:",
        "        query_text: Natural language query string",
        "        layers: Dictionary of layers",
        "        tokenizer: Tokenizer instance",
        "        top_n: Number of results to return",
        "",
        "    Returns:",
        "        List of (doc_id, score, parsed_intent) tuples",
        "",
        "    Example:",
        "        >>> search_by_intent(\"how do we validate user input?\", layers, tokenizer)",
        "        [('validation.py', 0.85, {...}), ('forms.py', 0.72, {...}), ...]",
        "    \"\"\"",
        "    # Parse the query intent",
        "    parsed = parse_intent_query(query_text)",
        "",
        "    if not parsed['expanded_terms']:",
        "        return []",
        "",
        "    # Build weighted query from expanded terms",
        "    layer0 = layers[CorticalLayer.TOKENS]",
        "    layer3 = layers[CorticalLayer.DOCUMENTS]",
        "",
        "    # Score documents based on term matches",
        "    doc_scores: Dict[str, float] = defaultdict(float)",
        "",
        "    for i, term in enumerate(parsed['expanded_terms']):",
        "        # Earlier terms (action, subject) get higher weight",
        "        term_weight = 1.0 / (1 + i * 0.2)",
        "",
        "        col = layer0.get_minicolumn(term)",
        "        if col:",
        "            for doc_id in col.document_ids:",
        "                # Use TF-IDF if available",
        "                tfidf = col.tfidf_per_doc.get(doc_id, col.tfidf)",
        "                doc_scores[doc_id] += term_weight * tfidf",
        "",
        "    # Sort by score",
        "    sorted_docs = sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)",
        "",
        "    # Return top results with parsed intent",
        "    results = []",
        "    for doc_id, score in sorted_docs[:top_n]:",
        "        results.append((doc_id, score, parsed))",
        "",
        "    return results",
        "",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "        key=lambda x: x[1],",
        "        reverse=True",
        "    )[:max_expansions]",
        "    ",
        "    for term, score in sorted_candidates:",
        "        expanded[term] = score",
        "    ",
        "    return expanded",
        "",
        ""
      ],
      "context_after": [
        "# Valid relation chain patterns for multi-hop inference",
        "# Key: (relation1, relation2) → validity score (0.0 = invalid, 1.0 = fully valid)",
        "VALID_RELATION_CHAINS = {",
        "    # Transitive hierarchies",
        "    ('IsA', 'IsA'): 1.0,           # dog IsA animal IsA living_thing",
        "    ('PartOf', 'PartOf'): 1.0,     # wheel PartOf car PartOf vehicle",
        "    ('IsA', 'HasProperty'): 0.9,   # dog IsA animal HasProperty alive",
        "    ('PartOf', 'HasProperty'): 0.8,  # wheel PartOf car HasProperty fast",
        "",
        "    # Association chains"
      ],
      "change_type": "add"
    },
    {
      "file": "tests/test_intent_query.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "\"\"\"",
        "Tests for intent-based query understanding.",
        "",
        "Tests the parse_intent_query and search_by_intent functions",
        "used for natural language code search.",
        "\"\"\"",
        "",
        "import unittest",
        "from cortical.query import (",
        "    parse_intent_query,",
        "    search_by_intent,",
        "    QUESTION_INTENTS,",
        "    ACTION_VERBS,",
        "    ParsedIntent,",
        ")",
        "",
        "",
        "class TestParseIntentQuery(unittest.TestCase):",
        "    \"\"\"Test the parse_intent_query function.\"\"\"",
        "",
        "    def test_where_query(self):",
        "        \"\"\"Test parsing 'where' queries for location intent.\"\"\"",
        "        result = parse_intent_query(\"where do we handle authentication?\")",
        "        self.assertEqual(result['intent'], 'location')",
        "        self.assertEqual(result['question_word'], 'where')",
        "        self.assertEqual(result['action'], 'handle')",
        "        self.assertEqual(result['subject'], 'authentication')",
        "",
        "    def test_how_query(self):",
        "        \"\"\"Test parsing 'how' queries for implementation intent.\"\"\"",
        "        result = parse_intent_query(\"how do we validate user input?\")",
        "        self.assertEqual(result['intent'], 'implementation')",
        "        self.assertEqual(result['question_word'], 'how')",
        "        self.assertEqual(result['action'], 'validate')",
        "        self.assertIn('user', [result['subject'], result['expanded_terms']])",
        "",
        "    def test_what_query(self):",
        "        \"\"\"Test parsing 'what' queries for definition intent.\"\"\"",
        "        result = parse_intent_query(\"what is the database schema?\")",
        "        self.assertEqual(result['intent'], 'definition')",
        "        self.assertEqual(result['question_word'], 'what')",
        "",
        "    def test_why_query(self):",
        "        \"\"\"Test parsing 'why' queries for rationale intent.\"\"\"",
        "        result = parse_intent_query(\"why do we cache this data?\")",
        "        self.assertEqual(result['intent'], 'rationale')",
        "        self.assertEqual(result['question_word'], 'why')",
        "",
        "    def test_when_query(self):",
        "        \"\"\"Test parsing 'when' queries for lifecycle intent.\"\"\"",
        "        result = parse_intent_query(\"when does initialization happen?\")",
        "        self.assertEqual(result['intent'], 'lifecycle')",
        "        self.assertEqual(result['question_word'], 'when')",
        "",
        "    def test_no_question_word(self):",
        "        \"\"\"Test parsing queries without question words.\"\"\"",
        "        result = parse_intent_query(\"fetch user data\")",
        "        self.assertEqual(result['intent'], 'search')",
        "        self.assertIsNone(result['question_word'])",
        "        self.assertEqual(result['action'], 'fetch')",
        "",
        "    def test_empty_query(self):",
        "        \"\"\"Test parsing empty query.\"\"\"",
        "        result = parse_intent_query(\"\")",
        "        self.assertEqual(result['intent'], 'search')",
        "        self.assertIsNone(result['action'])",
        "        self.assertIsNone(result['subject'])",
        "        self.assertEqual(result['expanded_terms'], [])",
        "",
        "    def test_punctuation_handling(self):",
        "        \"\"\"Test that punctuation is handled correctly.\"\"\"",
        "        result = parse_intent_query(\"where is authentication???\")",
        "        self.assertEqual(result['intent'], 'location')",
        "        self.assertIn('authentication', result['expanded_terms'])",
        "",
        "    def test_expanded_terms_include_synonyms(self):",
        "        \"\"\"Test that expanded terms include code concept synonyms.\"\"\"",
        "        result = parse_intent_query(\"how to fetch data\")",
        "        # 'fetch' should expand to include related terms",
        "        self.assertIn('fetch', result['expanded_terms'])",
        "        # Should have some related terms (from retrieval group)",
        "        self.assertGreater(len(result['expanded_terms']), 1)",
        "",
        "    def test_action_verb_detection(self):",
        "        \"\"\"Test detection of various action verbs.\"\"\"",
        "        test_cases = [",
        "            (\"validate input\", \"validate\"),",
        "            (\"process request\", \"process\"),",
        "            (\"save user data\", \"save\"),",
        "            (\"delete old records\", \"delete\"),",
        "            (\"transform response\", \"transform\"),",
        "        ]",
        "        for query, expected_action in test_cases:",
        "            result = parse_intent_query(query)",
        "            self.assertEqual(result['action'], expected_action,",
        "                           f\"Failed for query: {query}\")",
        "",
        "    def test_subject_extraction(self):",
        "        \"\"\"Test extraction of query subject.\"\"\"",
        "        result = parse_intent_query(\"handle errors gracefully\")",
        "        self.assertEqual(result['subject'], 'errors')",
        "",
        "    def test_filler_words_removed(self):",
        "        \"\"\"Test that filler words don't become subject/action.\"\"\"",
        "        result = parse_intent_query(\"do we have a database connection?\")",
        "        self.assertNotEqual(result['subject'], 'do')",
        "        self.assertNotEqual(result['subject'], 'we')",
        "        self.assertNotEqual(result['subject'], 'have')",
        "",
        "",
        "class TestQuestionIntents(unittest.TestCase):",
        "    \"\"\"Test the QUESTION_INTENTS mapping.\"\"\"",
        "",
        "    def test_all_question_words_mapped(self):",
        "        \"\"\"Test that common question words are mapped.\"\"\"",
        "        expected_words = ['where', 'how', 'what', 'why', 'when', 'which', 'who']",
        "        for word in expected_words:",
        "            self.assertIn(word, QUESTION_INTENTS)",
        "",
        "    def test_intent_types(self):",
        "        \"\"\"Test that intent types are meaningful.\"\"\"",
        "        self.assertEqual(QUESTION_INTENTS['where'], 'location')",
        "        self.assertEqual(QUESTION_INTENTS['how'], 'implementation')",
        "        self.assertEqual(QUESTION_INTENTS['what'], 'definition')",
        "        self.assertEqual(QUESTION_INTENTS['why'], 'rationale')",
        "",
        "",
        "class TestActionVerbs(unittest.TestCase):",
        "    \"\"\"Test the ACTION_VERBS set.\"\"\"",
        "",
        "    def test_common_verbs_included(self):",
        "        \"\"\"Test that common programming action verbs are included.\"\"\"",
        "        expected_verbs = [",
        "            'handle', 'process', 'create', 'delete', 'update', 'fetch',",
        "            'validate', 'parse', 'transform', 'authenticate', 'initialize'",
        "        ]",
        "        for verb in expected_verbs:",
        "            self.assertIn(verb, ACTION_VERBS)",
        "",
        "    def test_is_frozenset(self):",
        "        \"\"\"Test that ACTION_VERBS is immutable.\"\"\"",
        "        self.assertIsInstance(ACTION_VERBS, frozenset)",
        "",
        "",
        "class TestSearchByIntent(unittest.TestCase):",
        "    \"\"\"Test the search_by_intent function.\"\"\"",
        "",
        "    def setUp(self):",
        "        \"\"\"Set up test processor.\"\"\"",
        "        from cortical import CorticalTextProcessor",
        "        self.processor = CorticalTextProcessor()",
        "        self.processor.process_document(\"auth_handler\", \"\"\"",
        "            Authentication handler module.",
        "            This module handles user authentication and login.",
        "            It validates credentials and creates sessions.",
        "        \"\"\")",
        "        self.processor.process_document(\"data_fetcher\", \"\"\"",
        "            Data fetching utilities.",
        "            Functions to fetch and retrieve data from external APIs.",
        "            Handles HTTP requests and response parsing.",
        "        \"\"\")",
        "        self.processor.process_document(\"validator\", \"\"\"",
        "            Input validation module.",
        "            Validates and sanitizes user input.",
        "            Checks for required fields and data types.",
        "        \"\"\")",
        "        self.processor.compute_all()",
        "",
        "    def test_search_returns_results(self):",
        "        \"\"\"Test that search returns results.\"\"\"",
        "        results = self.processor.search_by_intent(\"where do we handle authentication?\")",
        "        self.assertIsInstance(results, list)",
        "        # Should find auth_handler document",
        "        if results:",
        "            doc_ids = [r[0] for r in results]",
        "            self.assertIn('auth_handler', doc_ids)",
        "",
        "    def test_search_returns_parsed_intent(self):",
        "        \"\"\"Test that search returns parsed intent with results.\"\"\"",
        "        results = self.processor.search_by_intent(\"how to validate input?\")",
        "        if results:",
        "            doc_id, score, parsed = results[0]",
        "            self.assertIn('intent', parsed)",
        "            self.assertIn('action', parsed)",
        "            self.assertIn('expanded_terms', parsed)",
        "",
        "    def test_search_empty_query(self):",
        "        \"\"\"Test search with empty query.\"\"\"",
        "        results = self.processor.search_by_intent(\"\")",
        "        self.assertEqual(results, [])",
        "",
        "    def test_search_top_n_limit(self):",
        "        \"\"\"Test that top_n limits results.\"\"\"",
        "        results = self.processor.search_by_intent(\"fetch data\", top_n=2)",
        "        self.assertLessEqual(len(results), 2)",
        "",
        "    def test_processor_parse_intent_query(self):",
        "        \"\"\"Test the processor wrapper for parse_intent_query.\"\"\"",
        "        result = self.processor.parse_intent_query(\"where is the login function?\")",
        "        self.assertEqual(result['intent'], 'location')",
        "        # 'login' is detected as action verb, so 'function' becomes subject",
        "        self.assertEqual(result['action'], 'login')",
        "        self.assertEqual(result['subject'], 'function')",
        "",
        "",
        "class TestParsedIntentStructure(unittest.TestCase):",
        "    \"\"\"Test the ParsedIntent TypedDict structure.\"\"\"",
        "",
        "    def test_all_keys_present(self):",
        "        \"\"\"Test that all expected keys are in parsed result.\"\"\"",
        "        result = parse_intent_query(\"where do we handle errors?\")",
        "        expected_keys = ['action', 'subject', 'intent', 'question_word', 'expanded_terms']",
        "        for key in expected_keys:",
        "            self.assertIn(key, result)",
        "",
        "    def test_expanded_terms_is_list(self):",
        "        \"\"\"Test that expanded_terms is a list.\"\"\"",
        "        result = parse_intent_query(\"handle authentication\")",
        "        self.assertIsInstance(result['expanded_terms'], list)",
        "",
        "    def test_no_duplicate_expanded_terms(self):",
        "        \"\"\"Test that expanded_terms has no duplicates.\"\"\"",
        "        result = parse_intent_query(\"handle handle authentication\")",
        "        self.assertEqual(",
        "            len(result['expanded_terms']),",
        "            len(set(result['expanded_terms']))",
        "        )",
        "",
        "",
        "if __name__ == '__main__':",
        "    unittest.main()"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    }
  ],
  "hour_of_day": 14,
  "day_of_week": "Wednesday",
  "seconds_since_last_commit": -429025,
  "is_merge": false,
  "is_initial": false,
  "parent_count": 1,
  "session_id": null,
  "related_chats": [],
  "ci_result": null,
  "reverted": false,
  "amended": false
}