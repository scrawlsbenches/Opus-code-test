{
  "hash": "18f45ef04e4755e8c2385bc840046a9b2d1a157b",
  "message": "Optimize semantic extraction and bigram connections (2x speedup)",
  "author": "Claude",
  "timestamp": "2025-12-10 01:32:06 +0000",
  "branch": "claude/multi-index-design-DvifZ",
  "files_changed": [
    "cortical/analysis.py",
    "cortical/semantics.py"
  ],
  "insertions": 51,
  "deletions": 26,
  "hunks": [
    {
      "file": "cortical/analysis.py",
      "function": "def compute_bigram_connections(",
      "start_line": 981,
      "lines_added": [
        "    # Use inverted index for O(d * b²) instead of O(n²) where d=docs, b=bigrams per doc",
        "    doc_to_bigrams: Dict[str, List[Minicolumn]] = defaultdict(list)",
        "    for bigram in bigrams:",
        "        for doc_id in bigram.document_ids:",
        "            doc_to_bigrams[doc_id].append(bigram)",
        "",
        "    # Track pairs we've already processed to avoid duplicate work",
        "    cooccur_processed: Set[Tuple[str, str]] = set()",
        "",
        "    for doc_id, doc_bigrams in doc_to_bigrams.items():",
        "        # Only compare bigrams within the same document",
        "        for i, b1 in enumerate(doc_bigrams):",
        "            docs1 = b1.document_ids",
        "            for b2 in doc_bigrams[i+1:]:",
        "                # Skip if already processed this pair",
        "                pair_key = tuple(sorted([b1.id, b2.id]))",
        "                if pair_key in cooccur_processed:",
        "                    continue",
        "                cooccur_processed.add(pair_key)",
        "",
        "                docs2 = b2.document_ids",
        "                shared_docs = docs1 & docs2",
        "                if len(shared_docs) >= min_shared_docs:",
        "                    # Weight by Jaccard similarity of document sets",
        "                    jaccard = len(shared_docs) / len(docs1 | docs2)",
        "                    weight = cooccurrence_weight * jaccard",
        "                    add_connection(b1, b2, weight, 'cooccurrence')"
      ],
      "lines_removed": [
        "    for i, b1 in enumerate(bigrams):",
        "        docs1 = b1.document_ids",
        "        if not docs1:",
        "            continue",
        "",
        "        for b2 in bigrams[i+1:]:",
        "            docs2 = b2.document_ids",
        "            if not docs2:",
        "                continue",
        "",
        "            shared_docs = docs1 & docs2",
        "            if len(shared_docs) >= min_shared_docs:",
        "                # Weight by Jaccard similarity of document sets",
        "                jaccard = len(shared_docs) / len(docs1 | docs2)",
        "                weight = cooccurrence_weight * jaccard",
        "                add_connection(b1, b2, weight, 'cooccurrence')"
      ],
      "context_before": [
        "    # \"machine_learning\" ↔ \"learning_algorithms\"",
        "    for term in left_index:",
        "        if term in right_index:",
        "            # term appears as right component in some bigrams and left in others",
        "            for b_left in right_index[term]:  # ends with term",
        "                for b_right in left_index[term]:  # starts with term",
        "                    if b_left.id != b_right.id:",
        "                        add_connection(b_left, b_right, chain_weight, 'chain')",
        "",
        "    # 3. Connect bigrams that co-occur in the same documents"
      ],
      "context_after": [
        "",
        "    return {",
        "        'connections_created': len(connected_pairs),",
        "        'bigrams': len(bigrams),",
        "        'component_connections': component_connections,",
        "        'chain_connections': chain_connections,",
        "        'cooccurrence_connections': cooccurrence_connections",
        "    }",
        "",
        ""
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/semantics.py",
      "function": "def extract_corpus_semantics(",
      "start_line": 277,
      "lines_added": [
        "",
        "    # Pre-compute magnitudes once per vector (O(n) instead of O(n²))",
        "    magnitudes: Dict[str, float] = {}",
        "    for term in terms:",
        "        vec = context_vectors[term]",
        "        mag = math.sqrt(sum(v * v for v in vec.values()))",
        "        magnitudes[term] = mag",
        "",
        "    # Pre-compute key sets for faster intersection",
        "    key_sets: Dict[str, set] = {term: set(context_vectors[term].keys()) for term in terms}",
        "",
        "        mag1 = magnitudes[t1]",
        "        if mag1 == 0:",
        "            continue",
        "        keys1 = key_sets[t1]",
        "            mag2 = magnitudes[t2]",
        "            if mag2 == 0:",
        "                continue",
        "            # Fast intersection using pre-computed sets",
        "            common = keys1 & key_sets[t2]",
        "                vec2 = context_vectors[t2]",
        "                sim = dot / (mag1 * mag2)",
        "                if sim > 0.3:",
        "                    relations.append((t1, 'SimilarTo', t2, sim))"
      ],
      "lines_removed": [
        "            vec2 = context_vectors[t2]",
        "            # Cosine similarity of context vectors",
        "            common = set(vec1.keys()) & set(vec2.keys())",
        "                mag1 = math.sqrt(sum(v*v for v in vec1.values()))",
        "                mag2 = math.sqrt(sum(v*v for v in vec2.values()))",
        "",
        "                if mag1 > 0 and mag2 > 0:",
        "                    sim = dot / (mag1 * mag2)",
        "                    if sim > 0.3:",
        "                        relations.append((t1, 'SimilarTo', t2, sim))"
      ],
      "context_before": [
        "                # PMI-like score",
        "                total = sum(cooccurrence.values())",
        "                expected = (col1.occurrence_count * col2.occurrence_count) / (total + 1)",
        "                pmi = math.log((count + 1) / (expected + 1))",
        "                ",
        "                if pmi > 0:",
        "                    relations.append((t1, 'CoOccurs', t2, min(pmi, 3.0)))",
        "    ",
        "    # Extract SimilarTo from context similarity",
        "    terms = list(context_vectors.keys())"
      ],
      "context_after": [
        "    for i, t1 in enumerate(terms):",
        "        vec1 = context_vectors[t1]",
        "",
        "        for t2 in terms[i+1:]:",
        "",
        "            if len(common) >= 3:",
        "                dot = sum(vec1[k] * vec2[k] for k in common)",
        "",
        "    # Extract commonsense relations from text patterns",
        "    if use_pattern_extraction:",
        "        valid_terms = set(layer0.minicolumns.keys())",
        "        pattern_relations = extract_pattern_relations(",
        "            documents,",
        "            valid_terms,",
        "            min_confidence=min_pattern_confidence",
        "        )",
        "        relations.extend(pattern_relations)"
      ],
      "change_type": "modify"
    }
  ],
  "hour_of_day": 1,
  "day_of_week": "Wednesday",
  "seconds_since_last_commit": -475962,
  "is_merge": false,
  "is_initial": false,
  "parent_count": 1,
  "session_id": null,
  "related_chats": [],
  "ci_result": null,
  "reverted": false,
  "amended": false
}