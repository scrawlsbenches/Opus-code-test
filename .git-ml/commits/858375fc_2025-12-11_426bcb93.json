{
  "hash": "858375fc2060501a2a8da37fb5c10a9ff7595952",
  "message": "Implement AI metadata generator script (Task #119)",
  "author": "Claude",
  "timestamp": "2025-12-11 12:57:11 +0000",
  "branch": "claude/multi-index-design-DvifZ",
  "files_changed": [
    ".gitignore",
    "scripts/generate_ai_metadata.py"
  ],
  "insertions": 597,
  "deletions": 0,
  "hunks": [
    {
      "file": ".gitignore",
      "function": "__pycache__/",
      "start_line": 9,
      "lines_added": [
        "# AI metadata files (generated, not tracked)",
        "*.ai_meta",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "dist/",
        "build/",
        ".pytest_cache/",
        "",
        "# Generated corpus files",
        "corpus_dev.pkl",
        "*.pkl",
        "*.manifest.json",
        "*.pkl.hash",
        ""
      ],
      "context_after": [
        "# Indexer logs",
        "index.log",
        "*.log",
        "",
        "# Coverage",
        ".coverage",
        ".coverage.*",
        "coverage.xml",
        "htmlcov/",
        ""
      ],
      "change_type": "add"
    },
    {
      "file": "scripts/generate_ai_metadata.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "#!/usr/bin/env python3",
        "\"\"\"",
        "AI Metadata Generator for Cortical Text Processor",
        "",
        "Generates .ai_meta companion files for Python source files, providing",
        "structured navigation aids for AI assistants without cluttering source code.",
        "",
        "Usage:",
        "    python scripts/generate_ai_metadata.py                    # Generate all",
        "    python scripts/generate_ai_metadata.py --incremental      # Only changed files",
        "    python scripts/generate_ai_metadata.py cortical/query.py  # Specific file",
        "    python scripts/generate_ai_metadata.py --clean            # Remove all .ai_meta",
        "",
        "Output:",
        "    For each .py file, creates a .py.ai_meta YAML file containing:",
        "    - File metadata (lines, generated timestamp)",
        "    - Logical sections with line ranges",
        "    - Function/method details (signatures, docstrings, line numbers)",
        "    - Cross-references (\"See Also\" suggestions)",
        "    - Complexity hints for expensive operations",
        "    - Import dependencies",
        "\"\"\"",
        "",
        "import ast",
        "import os",
        "import sys",
        "import hashlib",
        "import argparse",
        "from datetime import datetime",
        "from typing import Dict, List, Optional, Tuple, Any",
        "from pathlib import Path",
        "from collections import defaultdict",
        "",
        "",
        "# Patterns for identifying related functions",
        "RELATED_FUNCTION_PATTERNS = {",
        "    'find_documents': ['fast_find_documents', 'find_passages', 'search_by_intent', 'expand_query'],",
        "    'find_passages': ['find_documents', 'find_passages_batch', 'chunking'],",
        "    'expand_query': ['expand_query_semantic', 'expand_query_multihop', 'expand_query_cached'],",
        "    'compute_pagerank': ['compute_semantic_pagerank', 'compute_hierarchical_pagerank', 'compute_importance'],",
        "    'compute_tfidf': ['compute_pagerank', 'compute_importance'],",
        "    'process_document': ['add_document_incremental', 'add_documents_batch', 'remove_document'],",
        "    'save': ['load', 'export_graph', 'export_conceptnet_json'],",
        "    'load': ['save', 'from_dict'],",
        "    'get_fingerprint': ['compare_fingerprints', 'explain_fingerprint', 'explain_similarity'],",
        "}",
        "",
        "# Known expensive operations with complexity hints",
        "COMPLEXITY_HINTS = {",
        "    'compute_all': 'O(n²) where n = total minicolumns. ~27s for 95 docs.',",
        "    'compute_pagerank': 'O(iterations × edges). Usually 20 iterations.',",
        "    'compute_semantic_pagerank': 'O(iterations × edges × relation_types).',",
        "    'compute_hierarchical_pagerank': 'O(iterations × edges × layers).',",
        "    'build_concept_clusters': 'O(n × iterations) label propagation.',",
        "    'compute_bigram_connections': 'O(bigrams²) for co-occurrence. Can be slow.',",
        "    'find_passages_for_query': 'O(docs × chunks_per_doc). Use find_documents for speed.',",
        "    'extract_corpus_semantics': 'O(docs × patterns). Pattern matching on all text.',",
        "    'retrofit_connections': 'O(iterations × relations). Blends semantic relations.',",
        "}",
        "",
        "# Section detection keywords for grouping functions",
        "SECTION_KEYWORDS = {",
        "    'document': ['process_document', 'add_document', 'remove_document', 'get_document', 'set_document'],",
        "    'computation': ['compute_', '_mark_stale', '_mark_fresh', 'is_stale', 'recompute'],",
        "    'query': ['find_documents', 'find_passages', 'expand_query', 'search_', 'query_'],",
        "    'semantic': ['extract_', 'retrofit_', 'semantic', 'relation', 'inherit'],",
        "    'embedding': ['embedding', 'fingerprint', 'similarity'],",
        "    'persistence': ['save', 'load', 'export_', 'to_dict', 'from_dict'],",
        "    'analysis': ['pagerank', 'tfidf', 'cluster', 'propagate'],",
        "}",
        "",
        "",
        "class FunctionInfo:",
        "    \"\"\"Holds extracted information about a function/method.\"\"\"",
        "",
        "    def __init__(self, node: ast.FunctionDef, source_lines: List[str]):",
        "        self.name = node.name",
        "        self.line_start = node.lineno",
        "        self.line_end = node.end_lineno or node.lineno",
        "        self.is_private = node.name.startswith('_')",
        "        self.is_dunder = node.name.startswith('__') and node.name.endswith('__')",
        "",
        "        # Extract signature",
        "        self.signature = self._extract_signature(node)",
        "",
        "        # Extract docstring",
        "        self.docstring = ast.get_docstring(node) or \"\"",
        "        self.docstring_summary = self._get_docstring_summary()",
        "",
        "        # Extract decorators",
        "        self.decorators = [self._get_decorator_name(d) for d in node.decorator_list]",
        "",
        "        # Determine if it's a method (has 'self' or 'cls' first arg)",
        "        self.is_method = False",
        "        if node.args.args:",
        "            first_arg = node.args.args[0].arg",
        "            self.is_method = first_arg in ('self', 'cls')",
        "",
        "    def _extract_signature(self, node: ast.FunctionDef) -> str:",
        "        \"\"\"Extract function signature as string.\"\"\"",
        "        args = []",
        "",
        "        # Regular args",
        "        defaults_offset = len(node.args.args) - len(node.args.defaults)",
        "        for i, arg in enumerate(node.args.args):",
        "            arg_str = arg.arg",
        "            if arg.annotation:",
        "                arg_str += f\": {ast.unparse(arg.annotation)}\"",
        "",
        "            # Add default value if present",
        "            default_idx = i - defaults_offset",
        "            if default_idx >= 0 and default_idx < len(node.args.defaults):",
        "                default = node.args.defaults[default_idx]",
        "                arg_str += f\" = {ast.unparse(default)}\"",
        "",
        "            args.append(arg_str)",
        "",
        "        # *args",
        "        if node.args.vararg:",
        "            arg_str = f\"*{node.args.vararg.arg}\"",
        "            if node.args.vararg.annotation:",
        "                arg_str += f\": {ast.unparse(node.args.vararg.annotation)}\"",
        "            args.append(arg_str)",
        "",
        "        # **kwargs",
        "        if node.args.kwarg:",
        "            arg_str = f\"**{node.args.kwarg.arg}\"",
        "            if node.args.kwarg.annotation:",
        "                arg_str += f\": {ast.unparse(node.args.kwarg.annotation)}\"",
        "            args.append(arg_str)",
        "",
        "        sig = f\"({', '.join(args)})\"",
        "",
        "        # Return type",
        "        if node.returns:",
        "            sig += f\" -> {ast.unparse(node.returns)}\"",
        "",
        "        return sig",
        "",
        "    def _get_docstring_summary(self) -> str:",
        "        \"\"\"Get first line of docstring as summary.\"\"\"",
        "        if not self.docstring:",
        "            return \"\"",
        "        lines = self.docstring.strip().split('\\n')",
        "        return lines[0].strip() if lines else \"\"",
        "",
        "    def _get_decorator_name(self, decorator) -> str:",
        "        \"\"\"Extract decorator name.\"\"\"",
        "        if isinstance(decorator, ast.Name):",
        "            return decorator.id",
        "        elif isinstance(decorator, ast.Attribute):",
        "            return f\"{ast.unparse(decorator)}\"",
        "        elif isinstance(decorator, ast.Call):",
        "            return self._get_decorator_name(decorator.func)",
        "        return str(decorator)",
        "",
        "",
        "class ClassInfo:",
        "    \"\"\"Holds extracted information about a class.\"\"\"",
        "",
        "    def __init__(self, node: ast.ClassDef, source_lines: List[str]):",
        "        self.name = node.name",
        "        self.line_start = node.lineno",
        "        self.line_end = node.end_lineno or node.lineno",
        "        self.docstring = ast.get_docstring(node) or \"\"",
        "        self.docstring_summary = self._get_docstring_summary()",
        "",
        "        # Extract base classes",
        "        self.bases = [ast.unparse(base) for base in node.bases]",
        "",
        "        # Extract methods",
        "        self.methods: List[FunctionInfo] = []",
        "        for item in node.body:",
        "            if isinstance(item, ast.FunctionDef):",
        "                self.methods.append(FunctionInfo(item, source_lines))",
        "",
        "    def _get_docstring_summary(self) -> str:",
        "        \"\"\"Get first line of docstring as summary.\"\"\"",
        "        if not self.docstring:",
        "            return \"\"",
        "        lines = self.docstring.strip().split('\\n')",
        "        return lines[0].strip() if lines else \"\"",
        "",
        "",
        "class ModuleAnalyzer:",
        "    \"\"\"Analyzes a Python module and extracts metadata.\"\"\"",
        "",
        "    def __init__(self, filepath: str):",
        "        self.filepath = filepath",
        "        self.filename = os.path.basename(filepath)",
        "",
        "        with open(filepath, 'r', encoding='utf-8') as f:",
        "            self.source = f.read()",
        "",
        "        self.source_lines = self.source.split('\\n')",
        "        self.line_count = len(self.source_lines)",
        "",
        "        # Parse AST",
        "        self.tree = ast.parse(self.source, filename=filepath)",
        "",
        "        # Extract components",
        "        self.module_docstring = ast.get_docstring(self.tree) or \"\"",
        "        self.imports: List[str] = []",
        "        self.classes: List[ClassInfo] = []",
        "        self.functions: List[FunctionInfo] = []  # Module-level functions",
        "",
        "        self._extract_components()",
        "",
        "    def _extract_components(self):",
        "        \"\"\"Extract all components from AST.\"\"\"",
        "        for node in ast.iter_child_nodes(self.tree):",
        "            if isinstance(node, ast.Import):",
        "                for alias in node.names:",
        "                    self.imports.append(alias.name)",
        "            elif isinstance(node, ast.ImportFrom):",
        "                module = node.module or \"\"",
        "                for alias in node.names:",
        "                    self.imports.append(f\"{module}.{alias.name}\")",
        "            elif isinstance(node, ast.ClassDef):",
        "                self.classes.append(ClassInfo(node, self.source_lines))",
        "            elif isinstance(node, ast.FunctionDef):",
        "                self.functions.append(FunctionInfo(node, self.source_lines))",
        "",
        "    def get_all_functions(self) -> List[Tuple[str, FunctionInfo]]:",
        "        \"\"\"Get all functions including class methods.\"\"\"",
        "        result = []",
        "",
        "        # Module-level functions",
        "        for func in self.functions:",
        "            result.append((None, func))",
        "",
        "        # Class methods",
        "        for cls in self.classes:",
        "            for method in cls.methods:",
        "                result.append((cls.name, method))",
        "",
        "        return result",
        "",
        "    def detect_sections(self) -> List[Dict[str, Any]]:",
        "        \"\"\"Detect logical sections in the file.\"\"\"",
        "        sections = []",
        "        all_funcs = self.get_all_functions()",
        "",
        "        if not all_funcs:",
        "            return sections",
        "",
        "        # Group functions by detected section type",
        "        section_groups = defaultdict(list)",
        "        uncategorized = []",
        "",
        "        for cls_name, func in all_funcs:",
        "            if func.is_dunder or func.is_private:",
        "                continue",
        "",
        "            categorized = False",
        "            for section_name, keywords in SECTION_KEYWORDS.items():",
        "                for keyword in keywords:",
        "                    if keyword in func.name.lower():",
        "                        section_groups[section_name].append((cls_name, func))",
        "                        categorized = True",
        "                        break",
        "                if categorized:",
        "                    break",
        "",
        "            if not categorized:",
        "                uncategorized.append((cls_name, func))",
        "",
        "        # Build section info",
        "        for section_name, funcs in section_groups.items():",
        "            if not funcs:",
        "                continue",
        "",
        "            lines = [f.line_start for _, f in funcs]",
        "            sections.append({",
        "                'name': section_name.title().replace('_', ' '),",
        "                'lines': [min(lines), max(lines)],",
        "                'functions': [f.name for _, f in funcs],",
        "            })",
        "",
        "        if uncategorized:",
        "            lines = [f.line_start for _, f in uncategorized]",
        "            sections.append({",
        "                'name': 'Other',",
        "                'lines': [min(lines), max(lines)],",
        "                'functions': [f.name for _, f in uncategorized],",
        "            })",
        "",
        "        # Sort by line number",
        "        sections.sort(key=lambda s: s['lines'][0])",
        "",
        "        return sections",
        "",
        "    def find_related_functions(self, func_name: str) -> List[str]:",
        "        \"\"\"Find related functions based on naming patterns.\"\"\"",
        "        related = []",
        "",
        "        # Check predefined patterns",
        "        for pattern, suggestions in RELATED_FUNCTION_PATTERNS.items():",
        "            if pattern in func_name:",
        "                related.extend(suggestions)",
        "",
        "        # Find functions with similar prefixes/suffixes",
        "        all_func_names = [f.name for _, f in self.get_all_functions() if not f.is_private]",
        "",
        "        # Same prefix (e.g., find_documents, find_passages)",
        "        parts = func_name.split('_')",
        "        if len(parts) >= 2:",
        "            prefix = parts[0]",
        "            for name in all_func_names:",
        "                if name != func_name and name.startswith(prefix + '_'):",
        "                    if name not in related:",
        "                        related.append(name)",
        "",
        "        # Limit to top 5",
        "        return related[:5]",
        "",
        "    def get_complexity_hint(self, func_name: str) -> Optional[str]:",
        "        \"\"\"Get complexity hint if known.\"\"\"",
        "        return COMPLEXITY_HINTS.get(func_name)",
        "",
        "    def generate_metadata(self) -> Dict[str, Any]:",
        "        \"\"\"Generate complete metadata dictionary.\"\"\"",
        "        metadata = {",
        "            'file': self.filepath,",
        "            'filename': self.filename,",
        "            'lines': self.line_count,",
        "            'generated': datetime.now().isoformat(),",
        "            'module_doc': self.module_docstring[:200] + '...' if len(self.module_docstring) > 200 else self.module_docstring,",
        "        }",
        "",
        "        # Sections",
        "        metadata['sections'] = self.detect_sections()",
        "",
        "        # Classes",
        "        if self.classes:",
        "            metadata['classes'] = {}",
        "            for cls in self.classes:",
        "                metadata['classes'][cls.name] = {",
        "                    'line': cls.line_start,",
        "                    'lines': [cls.line_start, cls.line_end],",
        "                    'bases': cls.bases,",
        "                    'doc': cls.docstring_summary,",
        "                    'methods': [m.name for m in cls.methods if not m.is_private],",
        "                }",
        "",
        "        # Functions (detailed)",
        "        metadata['functions'] = {}",
        "        for cls_name, func in self.get_all_functions():",
        "            if func.is_dunder:",
        "                continue",
        "",
        "            full_name = f\"{cls_name}.{func.name}\" if cls_name else func.name",
        "",
        "            func_meta = {",
        "                'line': func.line_start,",
        "                'signature': func.signature,",
        "                'doc': func.docstring_summary,",
        "                'private': func.is_private,",
        "            }",
        "",
        "            # Add related functions",
        "            related = self.find_related_functions(func.name)",
        "            if related:",
        "                func_meta['see_also'] = related",
        "",
        "            # Add complexity hint",
        "            complexity = self.get_complexity_hint(func.name)",
        "            if complexity:",
        "                func_meta['complexity'] = complexity",
        "",
        "            # Add decorators if present",
        "            if func.decorators:",
        "                func_meta['decorators'] = func.decorators",
        "",
        "            metadata['functions'][full_name] = func_meta",
        "",
        "        # Dependencies",
        "        metadata['imports'] = self._categorize_imports()",
        "",
        "        return metadata",
        "",
        "    def _categorize_imports(self) -> Dict[str, List[str]]:",
        "        \"\"\"Categorize imports by type.\"\"\"",
        "        stdlib = []",
        "        local = []",
        "",
        "        for imp in self.imports:",
        "            if imp.startswith('.') or imp.startswith('cortical'):",
        "                local.append(imp)",
        "            else:",
        "                stdlib.append(imp)",
        "",
        "        return {",
        "            'stdlib': sorted(set(stdlib)),",
        "            'local': sorted(set(local)),",
        "        }",
        "",
        "",
        "def dict_to_yaml(data: Any, indent: int = 0) -> str:",
        "    \"\"\"Convert dictionary to YAML string (simple implementation, no external deps).\"\"\"",
        "    lines = []",
        "    prefix = '  ' * indent",
        "",
        "    if isinstance(data, dict):",
        "        for key, value in data.items():",
        "            if isinstance(value, (dict, list)) and value:",
        "                lines.append(f\"{prefix}{key}:\")",
        "                lines.append(dict_to_yaml(value, indent + 1))",
        "            elif isinstance(value, list) and not value:",
        "                lines.append(f\"{prefix}{key}: []\")",
        "            elif isinstance(value, dict) and not value:",
        "                lines.append(f\"{prefix}{key}: {{}}\")",
        "            elif isinstance(value, str):",
        "                # Handle multiline strings",
        "                if '\\n' in value:",
        "                    lines.append(f\"{prefix}{key}: |\")",
        "                    for line in value.split('\\n'):",
        "                        lines.append(f\"{prefix}  {line}\")",
        "                elif ':' in value or '#' in value or value.startswith('{') or value.startswith('['):",
        "                    lines.append(f'{prefix}{key}: \"{value}\"')",
        "                else:",
        "                    lines.append(f\"{prefix}{key}: {value}\")",
        "            elif isinstance(value, bool):",
        "                lines.append(f\"{prefix}{key}: {str(value).lower()}\")",
        "            elif value is None:",
        "                lines.append(f\"{prefix}{key}: null\")",
        "            else:",
        "                lines.append(f\"{prefix}{key}: {value}\")",
        "    elif isinstance(data, list):",
        "        for item in data:",
        "            if isinstance(item, (dict, list)):",
        "                lines.append(f\"{prefix}-\")",
        "                lines.append(dict_to_yaml(item, indent + 1))",
        "            elif isinstance(item, str):",
        "                if ':' in item or '#' in item:",
        "                    lines.append(f'{prefix}- \"{item}\"')",
        "                else:",
        "                    lines.append(f\"{prefix}- {item}\")",
        "            else:",
        "                lines.append(f\"{prefix}- {item}\")",
        "",
        "    return '\\n'.join(lines)",
        "",
        "",
        "def generate_metadata_for_file(filepath: str) -> str:",
        "    \"\"\"Generate metadata for a single file and return YAML string.\"\"\"",
        "    analyzer = ModuleAnalyzer(filepath)",
        "    metadata = analyzer.generate_metadata()",
        "",
        "    header = f\"# {os.path.basename(filepath)}.ai_meta\\n\"",
        "    header += \"# Auto-generated AI navigation metadata - do not edit manually\\n\"",
        "    header += f\"# Regenerate with: python scripts/generate_ai_metadata.py {filepath}\\n\\n\"",
        "",
        "    return header + dict_to_yaml(metadata)",
        "",
        "",
        "def get_file_hash(filepath: str) -> str:",
        "    \"\"\"Get MD5 hash of file contents.\"\"\"",
        "    with open(filepath, 'rb') as f:",
        "        return hashlib.md5(f.read()).hexdigest()",
        "",
        "",
        "def should_regenerate(py_file: str, meta_file: str) -> bool:",
        "    \"\"\"Check if metadata needs regeneration.\"\"\"",
        "    if not os.path.exists(meta_file):",
        "        return True",
        "",
        "    py_mtime = os.path.getmtime(py_file)",
        "    meta_mtime = os.path.getmtime(meta_file)",
        "",
        "    return py_mtime > meta_mtime",
        "",
        "",
        "def main():",
        "    parser = argparse.ArgumentParser(",
        "        description='Generate AI metadata files for Python source files',",
        "        formatter_class=argparse.RawDescriptionHelpFormatter,",
        "        epilog=__doc__",
        "    )",
        "    parser.add_argument(",
        "        'files', nargs='*',",
        "        help='Specific files to process (default: all cortical/*.py)'",
        "    )",
        "    parser.add_argument(",
        "        '--incremental', '-i', action='store_true',",
        "        help='Only regenerate for changed files'",
        "    )",
        "    parser.add_argument(",
        "        '--clean', '-c', action='store_true',",
        "        help='Remove all .ai_meta files'",
        "    )",
        "    parser.add_argument(",
        "        '--output-dir', '-o', type=str, default=None,",
        "        help='Output directory for .ai_meta files (default: same as source)'",
        "    )",
        "    parser.add_argument(",
        "        '--verbose', '-v', action='store_true',",
        "        help='Verbose output'",
        "    )",
        "",
        "    args = parser.parse_args()",
        "",
        "    # Find project root",
        "    script_dir = os.path.dirname(os.path.abspath(__file__))",
        "    project_root = os.path.dirname(script_dir)",
        "",
        "    # Handle clean",
        "    if args.clean:",
        "        count = 0",
        "        for root, dirs, files in os.walk(project_root):",
        "            for f in files:",
        "                if f.endswith('.ai_meta'):",
        "                    path = os.path.join(root, f)",
        "                    os.remove(path)",
        "                    count += 1",
        "                    if args.verbose:",
        "                        print(f\"Removed: {path}\")",
        "        print(f\"Removed {count} .ai_meta files\")",
        "        return",
        "",
        "    # Determine files to process",
        "    if args.files:",
        "        py_files = [os.path.abspath(f) for f in args.files if f.endswith('.py')]",
        "    else:",
        "        # Default: all cortical/*.py files",
        "        cortical_dir = os.path.join(project_root, 'cortical')",
        "        py_files = [",
        "            os.path.join(cortical_dir, f)",
        "            for f in os.listdir(cortical_dir)",
        "            if f.endswith('.py') and not f.startswith('__')",
        "        ]",
        "",
        "    if not py_files:",
        "        print(\"No Python files to process\")",
        "        return",
        "",
        "    # Process each file",
        "    generated = 0",
        "    skipped = 0",
        "",
        "    for py_file in sorted(py_files):",
        "        if not os.path.exists(py_file):",
        "            print(f\"Warning: {py_file} not found, skipping\")",
        "            continue",
        "",
        "        # Determine output path",
        "        if args.output_dir:",
        "            meta_file = os.path.join(",
        "                args.output_dir,",
        "                os.path.basename(py_file) + '.ai_meta'",
        "            )",
        "        else:",
        "            meta_file = py_file + '.ai_meta'",
        "",
        "        # Check if regeneration needed",
        "        if args.incremental and not should_regenerate(py_file, meta_file):",
        "            skipped += 1",
        "            if args.verbose:",
        "                print(f\"Skipped (unchanged): {os.path.basename(py_file)}\")",
        "            continue",
        "",
        "        try:",
        "            yaml_content = generate_metadata_for_file(py_file)",
        "",
        "            # Ensure output directory exists",
        "            os.makedirs(os.path.dirname(meta_file) or '.', exist_ok=True)",
        "",
        "            with open(meta_file, 'w', encoding='utf-8') as f:",
        "                f.write(yaml_content)",
        "",
        "            generated += 1",
        "            if args.verbose:",
        "                print(f\"Generated: {os.path.basename(meta_file)}\")",
        "",
        "        except Exception as e:",
        "            print(f\"Error processing {py_file}: {e}\")",
        "            if args.verbose:",
        "                import traceback",
        "                traceback.print_exc()",
        "",
        "    print(f\"\\nGenerated {generated} .ai_meta files\")",
        "    if skipped:",
        "        print(f\"Skipped {skipped} unchanged files\")",
        "",
        "    # Reminder about .gitignore",
        "    gitignore_path = os.path.join(project_root, '.gitignore')",
        "    if os.path.exists(gitignore_path):",
        "        with open(gitignore_path, 'r') as f:",
        "            if '*.ai_meta' not in f.read():",
        "                print(\"\\n⚠️  Remember to add '*.ai_meta' to .gitignore\")",
        "",
        "",
        "if __name__ == '__main__':",
        "    main()"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    }
  ],
  "hour_of_day": 12,
  "day_of_week": "Thursday",
  "seconds_since_last_commit": -348457,
  "is_merge": false,
  "is_initial": false,
  "parent_count": 1,
  "session_id": null,
  "related_chats": [],
  "ci_result": null,
  "reverted": false,
  "amended": false
}