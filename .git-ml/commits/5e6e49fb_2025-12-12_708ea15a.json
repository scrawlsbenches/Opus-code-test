{
  "hash": "5e6e49fba9c1df4b0013c7e69c65a41e35ad045c",
  "message": "Merge main: resolve TASK_LIST.md conflicts, keep all completed tasks",
  "author": "Claude",
  "timestamp": "2025-12-12 11:01:15 +0000",
  "branch": "claude/multi-index-design-DvifZ",
  "files_changed": [
    "TASK_LIST.md",
    "cortical/analysis.py",
    "cortical/layers.py",
    "cortical/minicolumn.py",
    "cortical/query/definitions.py",
    "cortical/query/expansion.py",
    "samples/bottleneck_diagnosis_procedures.txt",
    "samples/codebase_search_procedures.txt",
    "samples/computation_staleness_management.txt",
    "samples/concept_cluster_evaluation.txt",
    "samples/corpus_indexing_procedures.txt",
    "samples/corpus_maintenance_workflow.txt",
    "samples/document_relevance_tuning.txt",
    "samples/incremental_indexing_workflow.txt",
    "samples/knowledge_gap_analysis_process.txt",
    "samples/performance_profiling_process.txt",
    "samples/query_expansion_tuning_guide.txt",
    "samples/query_optimization_workflow.txt",
    "samples/rag_pipeline_integration.txt",
    "samples/search_quality_validation_process.txt",
    "samples/semantic_relation_extraction_procedures.txt",
    "samples/system_health_monitoring_workflow.txt",
    "tests/test_layers.py",
    "tests/test_query.py"
  ],
  "insertions": 1466,
  "deletions": 217,
  "hunks": [
    {
      "file": "TASK_LIST.md",
      "function": null,
      "start_line": 1,
      "lines_added": [
        "**Completed Tasks:** 138+ (see archive and Recently Completed)"
      ],
      "lines_removed": [
        "**Completed Tasks:** 98+ (see archive)"
      ],
      "context_before": [
        "# Task List: Cortical Text Processor",
        "",
        "Active backlog for the Cortical Text Processor project. Completed tasks are archived in [TASK_ARCHIVE.md](TASK_ARCHIVE.md).",
        "",
        "**Last Updated:** 2025-12-12",
        "**Pending Tasks:** 31"
      ],
      "context_after": [
        "",
        "---",
        "",
        "## Active Backlog",
        "",
        "<!-- Machine-parseable format for automation -->",
        "",
        "### üî¥ Critical (Do Now)",
        "",
        "*All critical tasks completed!*"
      ],
      "change_type": "modify"
    },
    {
      "file": "TASK_LIST.md",
      "function": "Active backlog for the Cortical Text Processor project. Completed tasks are arch",
      "start_line": 74,
      "lines_added": [
        "| 147 | Fix misleading hardcoded values | 2025-12-12 | 5 fixes: backwards param names, sparsity threshold, config constant, tolerance param, confidence semantics |"
      ],
      "lines_removed": [],
      "context_before": [
        "---",
        "",
        "## Recently Completed (Last 7 Days)",
        "",
        "| # | Task | Completed | Notes |",
        "|---|------|-----------|-------|",
        "| 138 | Use sparse matrix multiplication for bigram connections | 2025-12-12 | Zero-dep SparseMatrix class in analysis.py for O(n¬≤) ‚Üí O(n*k) improvement |",
        "| 98 | Replace print() with logging | 2025-12-12 | 52+ print statements ‚Üí logging.info(), all modules use getLogger(__name__) |",
        "| 102 | Add tests for edge cases | 2025-12-12 | 53 new tests in test_edge_cases.py: Unicode, large docs, malformed inputs |",
        "| 115 | Create component interaction diagram | 2025-12-12 | docs/architecture.md with ASCII + Mermaid diagrams, module dependencies |"
      ],
      "context_after": [
        "| 139 | Batch bigram connection updates to reduce dict overhead | 2025-12-12 | add_lateral_connections_batch() method in minicolumn.py |",
        "| 137 | Cap bigram connections to top-K per bigram | 2025-12-12 | max_connections_per_bigram parameter (default 50) in analysis.py |",
        "| 116 | Document return value semantics | 2025-12-12 | Edge cases, score ranges, None vs exceptions, default parameters |",
        "| 114 | Add type aliases for complex types | 2025-12-12 | cortical/types.py with 20+ aliases: DocumentScore, PassageResult, SemanticRelation, etc. |",
        "| 113 | Document staleness tracking system | 2025-12-12 | Comprehensive docs in CLAUDE.md: computation types, API, incremental updates |",
        "| 96 | Centralize duplicate constants | 2025-12-12 | cortical/constants.py with RELATION_WEIGHTS, DOC_TYPE_BOOSTS, query keywords |",
        "| 91 | Create docs/README.md index | 2025-12-12 | Navigation by audience, reading paths, categorized docs |",
        "| 92 | Add badges to README.md | 2025-12-12 | Python, License, Tests, Coverage, Zero Dependencies badges |",
        "| 93 | Update README with docs references | 2025-12-12 | Documentation section with table linking to docs/*.md |",
        "| 146 | Create behavioral tests for core user workflows | 2025-12-12 | 11 tests across 4 categories: Search, Performance, Quality, Robustness |"
      ],
      "change_type": "add"
    },
    {
      "file": "TASK_LIST.md",
      "function": "Layer 2: Concept Layer (V4)",
      "start_line": 478,
      "lines_added": [
        "### 91. Create docs/README.md Index ‚úÖ",
        "**Meta:** `status:completed` `priority:medium` `category:docs`",
        "**Files:** `docs/README.md`",
        "**Completed:** 2025-12-12",
        "**Solution Applied:** Created `docs/README.md` with navigation by audience (New Users, Developers, AI Agents), reading paths, and categorized documentation links.",
        "### 92. Add Badges to README.md ‚úÖ",
        "**Meta:** `status:completed` `priority:medium` `category:devex`",
        "**Completed:** 2025-12-12",
        "**Solution Applied:** Added 5 badges to README.md: Python 3.8+, MIT License, Tests (1121 passing), Coverage (>89%), Zero Dependencies.",
        "### 93. Update README with Documentation References ‚úÖ",
        "**Meta:** `status:completed` `priority:medium` `category:docs`",
        "**Completed:** 2025-12-12",
        "**Solution Applied:** Added \"Documentation\" section to README.md with table linking to all docs/*.md files including quickstart, architecture, algorithms, query-guide, cookbook, and glossary.",
        "",
        "### 94. Split query.py into Focused Modules ‚úÖ",
        "**Meta:** `status:completed` `priority:high` `category:architecture`",
        "**Files:** `cortical/query.py` ‚Üí `cortical/query/` package (8 modules)",
        "**Completed:** 2025-12-12",
        "**Problem:** Single 2,719-line file violated Single Responsibility Principle.",
        "**Solution Applied:**",
        "‚îú‚îÄ‚îÄ __init__.py       # Re-export public API (backward compatible)",
        "‚îú‚îÄ‚îÄ passages.py       # find_passages_for_query",
        "‚îú‚îÄ‚îÄ chunking.py       # Chunk-based text processing",
        "‚îú‚îÄ‚îÄ ranking.py        # multi_stage_rank",
        "‚îî‚îÄ‚îÄ analogy.py        # complete_analogy",
        "- [x] No file >500 lines",
        "- [x] All existing tests pass",
        "- [x] Backward-compatible imports from `cortical.query`",
        "### 96. Centralize Duplicate Constants ‚úÖ",
        "**Meta:** `status:completed` `priority:medium` `category:code-quality`",
        "**Files:** `cortical/constants.py`",
        "**Completed:** 2025-12-12",
        "**Solution Applied:** Created `cortical/constants.py` as single source of truth containing RELATION_WEIGHTS, DOC_TYPE_BOOSTS, query keywords, and other shared constants. Updated imports across modules.",
        "### 97. Integrate CorticalConfig into CorticalTextProcessor ‚úÖ",
        "**Meta:** `status:completed` `priority:high` `category:architecture`",
        "**Completed:** 2025-12-11",
        "**Problem:** Config exists but isn't used by processor.",
        "**Solution Applied:**",
        "- Added `config` parameter to `CorticalTextProcessor.__init__()`",
        "- Config stored on processor instance (`self.config`)",
        "- Methods use config defaults for parameters like `damping`, `resolution`, etc.",
        "- Config saved/loaded with processor state via persistence",
        "- Per-call override still supported",
        "- [x] Config parameter accepted",
        "- [x] Methods use config defaults",
        "- [x] Override still possible per-call"
      ],
      "lines_removed": [
        "### 91. Create docs/README.md Index",
        "**Meta:** `status:pending` `priority:medium` `category:docs`",
        "**Files:** `docs/README.md` (new)",
        "**Solution:** Create index with recommended reading order.",
        "### 92. Add Badges to README.md",
        "**Meta:** `status:pending` `priority:medium` `category:devex`",
        "**Solution:** Add CI, coverage, Python version, license badges.",
        "### 93. Update README with Documentation References",
        "**Meta:** `status:pending` `priority:medium` `category:docs`",
        "### 94. Split query.py into Focused Modules",
        "**Meta:** `status:pending` `priority:high` `category:architecture`",
        "**Files:** `cortical/query.py` (2,719 lines) ‚Üí `cortical/query/` package",
        "**Quick Context:**",
        "- Current: 2,719 lines, 7+ responsibilities",
        "- Key functions: `expand_query()` (L127), `find_documents_for_query()` (L450), `find_passages_for_query()` (L890)",
        "- Imports: `processor.py` (L15), all test files",
        "",
        "**Problem:** Violates Single Responsibility Principle.",
        "**Solution:**",
        "‚îú‚îÄ‚îÄ __init__.py       # Re-export public API",
        "‚îú‚îÄ‚îÄ passages.py       # find_passages_for_query, chunking",
        "‚îî‚îÄ‚îÄ ranking.py        # multi_stage_rank",
        "- [ ] No file >500 lines",
        "- [ ] All existing tests pass",
        "- [ ] Backward-compatible imports from `cortical.query`",
        "### 96. Centralize Duplicate Constants",
        "**Meta:** `status:pending` `priority:medium` `category:code-quality`",
        "**Files:** `cortical/constants.py` (new), `cortical/semantics.py`, `cortical/query.py`",
        "**Solution:** Create `cortical/constants.py` as single source of truth.",
        "### 97. Integrate CorticalConfig into CorticalTextProcessor",
        "**Meta:** `status:pending` `priority:high` `category:architecture`",
        "**Quick Context:**",
        "- `CorticalConfig` exists with 20+ parameters",
        "- `CorticalTextProcessor.__init__()` doesn't accept config",
        "- All parameters passed at call time, scattered",
        "",
        "**Problem:** Config exists but isn't used.",
        "**Solution:**",
        "```python",
        "def __init__(",
        "    self,",
        "    config: Optional[CorticalConfig] = None,",
        "    tokenizer: Optional[Tokenizer] = None",
        "):",
        "    self.config = config or CorticalConfig()",
        "```",
        "- [ ] Config parameter accepted",
        "- [ ] Methods use config defaults",
        "- [ ] Override still possible per-call"
      ],
      "context_before": [
        "  - Understanding results section",
        "  - Query expansion demo",
        "  - Passage retrieval for RAG",
        "  - Key concepts table",
        "  - Save/load instructions",
        "  - Common patterns (batch, incremental, metadata)",
        "  - Troubleshooting section",
        "",
        "---",
        ""
      ],
      "context_after": [
        "",
        "**Effort:** Small",
        "",
        "**Problem:** 11 docs files with no navigation.",
        "",
        "",
        "---",
        "",
        "",
        "**Files:** `README.md`",
        "**Effort:** Small",
        "",
        "**Problem:** No visual project health indicators.",
        "",
        "",
        "---",
        "",
        "",
        "**Files:** `README.md`",
        "**Effort:** Small",
        "**Depends:** 91",
        "",
        "**Problem:** README doesn't mention docs/ folder.",
        "",
        "---",
        "",
        "",
        "**Effort:** Large",
        "",
        "",
        "```",
        "cortical/query/",
        "‚îú‚îÄ‚îÄ expansion.py      # expand_query, expand_query_semantic",
        "‚îú‚îÄ‚îÄ search.py         # find_documents_for_query, fast_find_documents",
        "‚îú‚îÄ‚îÄ intent.py         # parse_intent_query, search_by_intent",
        "‚îú‚îÄ‚îÄ definitions.py    # is_definition_query, find_definition_passages",
        "```",
        "",
        "**Acceptance Criteria:**",
        "",
        "---",
        "",
        "### 95. Split processor.py into Focused Modules",
        "",
        "**Meta:** `status:pending` `priority:medium` `category:architecture`",
        "**Files:** `cortical/processor.py` (2,301 lines)",
        "**Effort:** Large",
        "**Depends:** 97",
        "",
        "**Problem:** God Object with 100+ methods.",
        "",
        "**Solution:** Extract DocumentManager, ComputationManager as internal classes.",
        "",
        "---",
        "",
        "",
        "**Effort:** Small",
        "",
        "**Problem:** `RELATION_WEIGHTS`, `DOC_TYPE_BOOSTS` defined in multiple places.",
        "",
        "",
        "---",
        "",
        "",
        "**Files:** `cortical/processor.py`, `cortical/config.py`",
        "**Effort:** Medium",
        "",
        "",
        "",
        "**Acceptance Criteria:**",
        "",
        "---",
        "",
        "### 98. Replace print() with Logging Module",
        "",
        "**Meta:** `status:pending` `priority:medium` `category:code-quality`",
        "**Files:** `cortical/processor.py`, other modules",
        "**Effort:** Medium",
        "",
        "**Problem:** Uses print() for progress - can't configure."
      ],
      "change_type": "modify"
    },
    {
      "file": "TASK_LIST.md",
      "function": "def expand_query(self, query: str, max_expansions: int = 10):",
      "start_line": 812,
      "lines_added": [
        "### 113. Document Staleness Tracking System ‚úÖ",
        "**Meta:** `status:completed` `priority:medium` `category:ai-nav`",
        "**Files:** `CLAUDE.md`",
        "**Completed:** 2025-12-12",
        "**Problem:** The staleness tracking system was powerful but not documented.",
        "**Solution Applied:** Added comprehensive \"Staleness Tracking System\" section to CLAUDE.md including:",
        "- Table of all `COMP_*` constants and what they track",
        "- How staleness works (all start stale, computed marks fresh)",
        "- API methods (`is_stale()`, `get_stale_computations()`)",
        "- Incremental update behavior",
        "- When to check staleness before reading computed values",
        "### 114. Add Type Aliases for Complex Types ‚úÖ",
        "**Meta:** `status:completed` `priority:medium` `category:ai-nav`",
        "**Files:** `cortical/types.py`",
        "**Completed:** 2025-12-12",
        "**Solution Applied:** Created `cortical/types.py` with 20+ type aliases organized by category:",
        "- **Score types:** `DocumentScore`, `TermScore`, `DocumentResults`, `TermResults`",
        "- **Passage types:** `PassageResult`, `PassageResults`, `PassageWithPosition`",
        "- **Semantic types:** `SemanticRelation`, `SemanticRelations`",
        "- **Graph types:** `ConnectionMap`, `LayerDict`, `ClusterAssignment`",
        "- **Expansion types:** `ExpansionTerms`, `IntentResult`",
        "- **Fingerprint types:** `Fingerprint`, `FingerprintComparison`",
        "All types include docstrings explaining their structure."
      ],
      "lines_removed": [
        "### 113. Document Staleness Tracking System",
        "**Meta:** `status:pending` `priority:medium` `category:ai-nav`",
        "**Files:** `CLAUDE.md` or `docs/staleness.md` (new)",
        "**Problem:** The staleness tracking system (`COMP_TFIDF`, `COMP_PAGERANK`, `is_stale()`, `_mark_all_stale()`) is powerful but not documented. AI assistants discover it through exploration.",
        "**Solution:** Add documentation explaining:",
        "- What staleness means and why it matters",
        "- List of all `COMP_*` constants and what they track",
        "- When staleness is automatically set (which methods call `_mark_all_stale()`)",
        "- How to check and resolve staleness",
        "- Example workflow showing stale ‚Üí recompute ‚Üí fresh",
        "### 114. Add Type Aliases for Complex Types",
        "**Meta:** `status:pending` `priority:medium` `category:ai-nav`",
        "**Files:** `cortical/types.py` (new), update imports in other modules",
        "**Solution:** Create type aliases:",
        "```python",
        "# cortical/types.py",
        "from typing import List, Tuple, Dict, Any",
        "",
        "# Query results",
        "DocumentScore = Tuple[str, float]  # (doc_id, score)",
        "DocumentResults = List[DocumentScore]",
        "",
        "PassageResult = Tuple[str, float, str]  # (doc_id, score, passage_text)",
        "PassageResults = List[PassageResult]",
        "IntentResult = Tuple[str, float, Dict[str, Any]]  # (doc_id, score, intent_info)",
        "IntentResults = List[IntentResult]",
        "",
        "# Graph types",
        "ConnectionMap = Dict[str, float]  # {target_id: weight}",
        "LayerDict = Dict[CorticalLayer, HierarchicalLayer]",
        "```"
      ],
      "context_before": [
        "",
        "**Target Functions:**",
        "- `expand_query()`, `expand_query_semantic()`, `expand_query_multihop()`",
        "- `find_documents_for_query()`, `find_passages_for_query()`",
        "- `parse_intent_query()`, `search_by_intent()`",
        "- `complete_analogy()`",
        "- `compute_pagerank()`, `compute_tfidf()`",
        "",
        "---",
        ""
      ],
      "context_after": [
        "",
        "**Effort:** Small",
        "",
        "",
        "",
        "---",
        "",
        "",
        "**Effort:** Small",
        "",
        "**Problem:** Complex return types like `List[Tuple[str, float, Dict[str, Any]]]` are hard to understand at a glance.",
        "",
        "",
        "",
        "---",
        "",
        "### 115. Create Component Interaction Diagram",
        "",
        "**Meta:** `status:pending` `priority:medium` `category:ai-nav`",
        "**Files:** `docs/architecture.md` or `CLAUDE.md`",
        "**Effort:** Medium",
        "",
        "**Problem:** Understanding how modules call each other requires tracing imports and function calls."
      ],
      "change_type": "modify"
    },
    {
      "file": "TASK_LIST.md",
      "function": "LayerDict = Dict[CorticalLayer, HierarchicalLayer]",
      "start_line": 888,
      "lines_added": [
        "### 116. Document Return Value Semantics ‚úÖ",
        "**Meta:** `status:completed` `priority:medium` `category:ai-nav`",
        "**Completed:** 2025-12-12",
        "**Problem:** Inconsistent understanding of what functions return in edge cases.",
        "**Solution Applied:** Added \"Return Value Semantics\" section to CLAUDE.md documenting:",
        "- **Edge Case Returns:** Table showing what each scenario returns (empty corpus ‚Üí `[]`, unknown doc_id ‚Üí `{}`, etc.)",
        "- **Score Ranges:** Relevance (unbounded), PageRank (0-1), TF-IDF (unbounded), Similarity (0-1), Confidence (0-1)",
        "- **Lookup Functions:** `None` for missing items vs `KeyError` for invalid structure",
        "- **Default Parameters:** Table of key defaults (`top_n=5`, `damping=0.85`, `resolution=1.0`, etc.)"
      ],
      "lines_removed": [
        "### 116. Document Return Value Semantics",
        "**Meta:** `status:pending` `priority:medium` `category:ai-nav`",
        "**Problem:** Inconsistent understanding of what functions return in edge cases (empty corpus, no matches, invalid input).",
        "",
        "**Solution:** Add section to CLAUDE.md documenting:",
        "",
        "| Scenario | Return | Example Functions |",
        "|----------|--------|-------------------|",
        "| Empty corpus | Empty list `[]` | `find_documents_for_query()` |",
        "| No matches | Empty list `[]` | `find_passages_for_query()` |",
        "| Invalid doc_id | `None` | `get_document_metadata()` |",
        "| Invalid layer | Raises `KeyError` | `get_layer()` |",
        "Also document:",
        "- When functions return `None` vs raise exceptions",
        "- Default values for optional parameters",
        "- Score ranges (0.0-1.0 vs unbounded)"
      ],
      "context_before": [
        "       ‚ñº",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê",
        "‚îÇ      layers.py  ‚Üí  minicolumn.py             ‚îÇ",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò",
        "```",
        "",
        "Include which module calls which, and data flow direction.",
        "",
        "---",
        ""
      ],
      "context_after": [
        "",
        "**Files:** `CLAUDE.md`",
        "**Effort:** Medium",
        "",
        "",
        "",
        "---",
        "",
        "### 117. Create Debugging Cookbook",
        "",
        "**Meta:** `status:pending` `priority:low` `category:ai-nav`",
        "**Files:** `docs/debugging.md` (new)",
        "**Effort:** Medium",
        "",
        "**Problem:** Common debugging scenarios require discovering patterns through trial and error."
      ],
      "change_type": "modify"
    },
    {
      "file": "TASK_LIST.md",
      "function": "Coverage Assessment: ADEQUATE [~]",
      "start_line": 1205,
      "lines_added": [
        "### 140. Analyze Customer Service Cluster Quality"
      ],
      "lines_removed": [
        "### 128. Analyze Customer Service Cluster Quality"
      ],
      "context_before": [
        "",
        "**Acceptance Criteria:**",
        "- [x] Script identifies document clusters by topic/keywords",
        "- [x] Computes cohesion and separation metrics",
        "- [x] Provides clear coverage assessment (adequate/needs expansion)",
        "- [x] Suggests specific expansion topics when coverage is low",
        "- [x] Works with existing corpus or standalone document set",
        "",
        "---",
        ""
      ],
      "context_after": [
        "",
        "**Meta:** `status:pending` `priority:low` `category:research`",
        "**Files:** Analysis output",
        "**Effort:** Small",
        "**Depends:** 127",
        "",
        "**Problem:** The customer service cluster was added but not deeply analyzed.",
        "",
        "**Tasks:**",
        "1. Run cluster evaluation script on customer service documents"
      ],
      "change_type": "modify"
    },
    {
      "file": "TASK_LIST.md",
      "function": "TF-IDF embeddings capture semantic similarity much better because terms appearin",
      "start_line": 1468,
      "lines_added": [
        "### 146. Create Behavioral Tests for Core User Workflows ‚úÖ",
        "",
        "**Meta:** `status:completed` `priority:high` `category:testing`",
        "**Files:** `tests/test_behavioral.py`",
        "**Effort:** Medium",
        "**Completed:** 2025-12-12",
        "",
        "**Purpose:** Created acceptance/behavioral tests that verify the system delivers expected",
        "user outcomes, catching the kinds of issues discovered during dog-fooding.",
        "",
        "**Solution Applied:** Created `tests/test_behavioral.py` with 11 tests across 4 categories:",
        "",
        "1. **TestSearchBehavior** (3 tests)",
        "   - `test_document_name_matches_rank_highly` - \"distributed systems\" ‚Üí distributed_systems in top 2",
        "   - `test_query_expansion_improves_recall` - Expanded queries find more relevant docs",
        "   - `test_code_search_finds_implementations_not_tests` - Real code preferred over test files",
        "",
        "2. **TestPerformanceBehavior** (2 tests)",
        "   - `test_compute_all_under_threshold` - Full analysis < 30s for 100+ docs",
        "   - `test_search_is_fast` - Single query < 100ms",
        "",
        "3. **TestQualityBehavior** (4 tests)",
        "   - `test_pagerank_surfaces_meaningful_terms` - No 'self', 'def' in top 20",
        "   - `test_clustering_produces_coherent_groups` - modularity > 0.3",
        "   - `test_tfidf_embeddings_capture_semantic_similarity` - 'learning' similar to 'neural'",
        "   - `test_no_mega_cluster` - No cluster > 25% of tokens",
        "",
        "4. **TestRobustnessBehavior** (2 tests)",
        "   - `test_empty_query_raises_error` - ValueError for empty queries",
        "   - `test_unknown_terms_handled_gracefully` - Nonsense queries return empty results",
        "",
        "**Acceptance Criteria:**",
        "- [x] TestSearchBehavior class with 3+ tests",
        "- [x] TestPerformanceBehavior class with 2+ tests",
        "- [x] TestQualityBehavior class with 3+ tests (4 implemented)",
        "- [x] TestRobustnessBehavior class with 2+ tests",
        "- [x] All tests pass on current codebase",
        "- [x] Tests catch regressions from Tasks #141-145 if reverted",
        "",
        "---",
        "",
        "### 147. Fix Misleading Hardcoded Values ‚úÖ",
        "**Meta:** `status:completed` `priority:high` `category:bugfix`",
        "**Files:** `cortical/query/definitions.py`, `cortical/layers.py`, `cortical/query/expansion.py`, `cortical/analysis.py`, `cortical/minicolumn.py`, `tests/test_layers.py`, `tests/test_query.py`",
        "**Completed:** 2025-12-12",
        "",
        "**Problem:** Several hardcoded values in the codebase were misleading - they made something appear to be true when it wasn't.",
        "",
        "**Issues Fixed:**",
        "",
        "1. **Backwards parameter names** (`definitions.py:309-310`):",
        "   - `test_file_boost_factor=0.5` sounded like a boost but actually reduced scores by 50%",
        "   - `test_file_penalty=0.7` was called \"penalty\" but was lighter than the \"boost\"",
        "   - Renamed to `test_with_definition_penalty` and `test_without_definition_penalty`",
        "",
        "2. **Useless sparsity threshold** (`layers.py:192`):",
        "   - Hardcoded `threshold=1.0` only counted terms with activation=0 (unused terms)",
        "   - Changed to use `threshold_fraction * average_activation` for meaningful sparsity",
        "",
        "3. **Hardcoded config value** (`expansion.py:77`):",
        "   - Hardcoded `0.4` instead of using `DEFAULT_CHAIN_VALIDITY` constant",
        "   - Now imports and uses the config constant",
        "",
        "4. **Tolerance parameter ignored** (`analysis.py:301`):",
        "   - `compute_hierarchical_pagerank()` ignored caller's `tolerance` parameter",
        "   - Was hardcoded to `1e-6` instead of using the function parameter",
        "5. **Confidence only increasing** (`minicolumn.py:189`):",
        "   - Used `max(old, new)` so confidence could only increase, never decrease",
        "   - Changed to weighted average so confidence can decrease with lower-confidence evidence",
        "- [x] All misleading parameter names clarified",
        "- [x] Sparsity threshold made meaningful",
        "- [x] Config constants used consistently",
        "- [x] Function parameters respected (not ignored)",
        "- [x] Confidence semantics corrected",
        "- [x] All 1133 tests pass",
        "| Perf | 1 | Performance improvements (#138) |",
        "| Arch | 6 | Architecture refactoring (#133, 134, 135, 95, 100, 101) |",
        "| CodeQual | 2 | Code quality improvements (#98, 99) |",
        "| Testing | 2 | Test coverage (#102, 129) |",
        "| TaskMgmt | 3 | Task management system (#107, 106, 108) |",
        "| AINav | 3 | AI assistant navigation (#115, 117, 118) |",
        "| DevEx | 7 | Developer experience, scripts (#73-80) |",
        "| Research | 2 | Research and analysis (#140, 131) |",
        "| Samples | 1 | Sample document improvements (#130) |",
        "| Showcase | 1 | In progress (#87) |",
        "*Updated 2025-12-12 - Fixed staleness issues, renumbered Task #128 ‚Üí #140*"
      ],
      "lines_removed": [
        "### 146. Create Behavioral Tests for Core User Workflows",
        "**Meta:** `status:pending` `priority:high` `category:testing`",
        "**Files:** `tests/test_behavioral.py` (new)",
        "**Purpose:** Create acceptance/behavioral tests that verify the system delivers expected",
        "user outcomes. Unlike unit tests (function works correctly) or integration tests",
        "(components work together), behavioral tests verify \"the system feels right to users.\"",
        "",
        "**Why This Matters:**",
        "Dog-fooding revealed issues that unit tests missed:",
        "- Search returning wrong documents for obvious queries",
        "- Python keywords polluting analysis results",
        "- Performance regressions making the system feel slow",
        "",
        "Behavioral tests would have caught these as regressions.",
        "",
        "**Test Categories to Implement:**",
        "",
        "1. **SearchBehavior** - \"Search should feel relevant\"",
        "   ```python",
        "   def test_document_name_matches_rank_highly(self):",
        "       \"\"\"Query matching document name should return that doc in top 2.\"\"\"",
        "       # \"distributed systems\" ‚Üí distributed_systems in top 2",
        "",
        "   def test_query_expansion_improves_recall(self):",
        "       \"\"\"Expanded queries should find more relevant docs than exact match.\"\"\"",
        "",
        "   def test_code_search_finds_implementations_not_tests(self):",
        "       \"\"\"Code queries should prefer real implementations over test mocks.\"\"\"",
        "   ```",
        "",
        "2. **PerformanceBehavior** - \"System should feel responsive\"",
        "   ```python",
        "   def test_compute_all_under_threshold(self):",
        "       \"\"\"Full analysis should complete within reasonable time.\"\"\"",
        "       # With 109 sample docs: < 20 seconds",
        "",
        "   def test_search_is_fast(self):",
        "       \"\"\"Single query should return quickly.\"\"\"",
        "       # < 100ms per query",
        "   ```",
        "",
        "3. **QualityBehavior** - \"Results should make sense\"",
        "   ```python",
        "   def test_pagerank_surfaces_meaningful_terms(self):",
        "       \"\"\"Top PageRank terms should be domain concepts, not noise.\"\"\"",
        "       # No 'self', 'def', 'assertequal' in top 20",
        "",
        "   def test_clustering_produces_coherent_groups(self):",
        "       \"\"\"Clusters should have good community structure.\"\"\"",
        "       # modularity > 0.3",
        "",
        "   def test_embeddings_capture_semantic_similarity(self):",
        "       \"\"\"Similar terms by embedding should be semantically related.\"\"\"",
        "       # 'learning' similar to 'neural', 'training', 'networks'",
        "   ```",
        "",
        "4. **RobustnessBehavior** - \"System should handle edge cases gracefully\"",
        "   ```python",
        "   def test_empty_query_returns_empty_results(self):",
        "       \"\"\"Empty queries should not crash or return garbage.\"\"\"",
        "",
        "   def test_unknown_terms_handled_gracefully(self):",
        "       \"\"\"Queries with unknown terms should still return results.\"\"\"",
        "   ```",
        "",
        "**Implementation Notes:**",
        "- Use `tests/test_behavioral.py` as a new test file",
        "- Load sample corpus once in `setUpClass` for performance",
        "- Use descriptive assertion messages explaining expected behavior",
        "- Document the \"why\" for each threshold/expectation",
        "- [ ] TestSearchBehavior class with 3+ tests",
        "- [ ] TestPerformanceBehavior class with 2+ tests",
        "- [ ] TestQualityBehavior class with 3+ tests",
        "- [ ] TestRobustnessBehavior class with 2+ tests",
        "- [ ] All tests pass on current codebase",
        "- [ ] Tests catch regressions from Tasks #141-145 if reverted",
        "| Quality | 5 | Quality issues from dog-fooding |",
        "| Perf | 4 | Performance improvements |",
        "| AINav | 6 | AI assistant navigation & usability |",
        "| DevEx | 8 | Developer experience (scripts, tools) |",
        "| Docs | 2 | Documentation improvements |",
        "| Arch | 4 | Architecture refactoring |",
        "| CodeQual | 3 | Code quality improvements |",
        "| Testing | 3 | Test coverage |",
        "| TaskMgmt | 2 | Task management system |",
        "| Research | 2 | Research and analysis tasks |",
        "| Samples | 1 | Sample document improvements |",
        "*Updated 2025-12-12 after dog-fooding showcase.py*"
      ],
      "context_before": [
        "in similar documents are usually semantically related.",
        "",
        "**Acceptance Criteria:**",
        "- [x] 'data' filtered out by CODE_NOISE_TOKENS (Task #141)",
        "- [x] High-frequency terms don't dominate (TF-IDF naturally down-weights)",
        "- [x] Existing good similarities preserved and improved",
        "- [x] New 'tfidf' method available for semantic similarity tasks",
        "",
        "---",
        ""
      ],
      "context_after": [
        "",
        "**Effort:** Medium",
        "",
        "",
        "**Acceptance Criteria:**",
        "",
        "---",
        "",
        "## Category Index",
        "",
        "| Category | Pending | Description |",
        "|----------|---------|-------------|",
        "| Deferred | 7 | Low priority or superseded |",
        "",
        "",
        "---",
        "",
        "## Notes",
        "",
        "- **Effort estimates:** Small (<1 hour), Medium (1-4 hours), Large (1+ days)",
        "- **Dependencies:** Complete dependent tasks first",
        "- **Quick Context:** Key info to start task without searching",
        "- **Archive:** Full history in [TASK_ARCHIVE.md](TASK_ARCHIVE.md)",
        ""
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/analysis.py",
      "function": "def compute_hierarchical_pagerank(",
      "start_line": 388,
      "lines_added": [
        "            compute_pagerank(layer, damping=damping, iterations=layer_iterations, tolerance=tolerance)"
      ],
      "lines_removed": [
        "            compute_pagerank(layer, damping=damping, iterations=layer_iterations, tolerance=1e-6)"
      ],
      "context_before": [
        "    iterations_run = 0",
        "    converged = False",
        "",
        "    for global_iter in range(global_iterations):",
        "        iterations_run = global_iter + 1",
        "        max_global_diff = 0.0",
        "",
        "        # Step 1: Compute local PageRank for each layer",
        "        for layer_enum in active_layers:",
        "            layer = layers[layer_enum]"
      ],
      "context_after": [
        "",
        "        # Step 2: Propagate up (tokens ‚Üí bigrams ‚Üí concepts ‚Üí documents)",
        "        for i in range(len(active_layers) - 1):",
        "            lower_layer_enum = active_layers[i]",
        "            upper_layer_enum = active_layers[i + 1]",
        "            lower_layer = layers[lower_layer_enum]",
        "            upper_layer = layers[upper_layer_enum]",
        "",
        "            # Propagate from lower to upper via feedback connections",
        "            for col in lower_layer.minicolumns.values():"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/layers.py",
      "function": "class HierarchicalLayer:",
      "start_line": 170,
      "lines_added": [
        "    def sparsity(self, threshold_fraction: float = 0.5) -> float:",
        "        Calculate sparsity (fraction of columns with below-average activation).",
        "",
        "        more efficient and allow for more distinct patterns. This measures",
        "        the fraction of columns activated below a threshold relative to",
        "        the average activation.",
        "",
        "        Args:",
        "            threshold_fraction: Fraction of average activation to use as threshold.",
        "                Columns with activation < (average * threshold_fraction) count as sparse.",
        "                Default 0.5 means columns below 50% of average activation.",
        "",
        "        avg_activation = self.average_activation()",
        "        if avg_activation == 0:",
        "            return 1.0  # All columns are sparse if no activation",
        "        threshold = avg_activation * threshold_fraction",
        "        low_activation = sum(1 for col in self.minicolumns.values()"
      ],
      "lines_removed": [
        "    def sparsity(self) -> float:",
        "        Calculate sparsity (fraction of columns with low activation).",
        "        ",
        "        more efficient and allow for more distinct patterns.",
        "        ",
        "        threshold = 1.0  # Activation threshold",
        "        low_activation = sum(1 for col in self.minicolumns.values() "
      ],
      "context_before": [
        "            return 0.0",
        "        return sum(col.activation for col in self.minicolumns.values()) / len(self.minicolumns)",
        "    ",
        "    def activation_range(self) -> tuple:",
        "        \"\"\"Return (min, max) activation values.\"\"\"",
        "        if not self.minicolumns:",
        "            return (0.0, 0.0)",
        "        activations = [col.activation for col in self.minicolumns.values()]",
        "        return (min(activations), max(activations))",
        "    "
      ],
      "context_after": [
        "        \"\"\"",
        "        In biological neural networks, sparse representations are",
        "        Returns:",
        "            Fraction of columns with activation below threshold",
        "        \"\"\"",
        "        if not self.minicolumns:",
        "            return 0.0",
        "                            if col.activation < threshold)",
        "        return low_activation / len(self.minicolumns)",
        "    ",
        "    def top_by_pagerank(self, n: int = 10) -> list:",
        "        \"\"\"",
        "        Get top minicolumns by PageRank score.",
        "        ",
        "        Args:",
        "            n: Number of results to return",
        "            "
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/minicolumn.py",
      "function": "class Minicolumn:",
      "start_line": 178,
      "lines_added": [
        "            # Weighted average of confidence (allows confidence to decrease with weaker evidence)",
        "            new_confidence = (existing.confidence * existing.weight + confidence * weight) / new_weight"
      ],
      "lines_removed": [
        "            # Use higher confidence",
        "            new_confidence = max(confidence, existing.confidence)"
      ],
      "context_before": [
        "        Example:",
        "            col.add_typed_connection(\"L0_network\", 0.8, relation_type='RelatedTo')",
        "            col.add_typed_connection(\"L0_brain\", 0.5, relation_type='IsA', source='semantic')",
        "        \"\"\"",
        "        if target_id in self.typed_connections:",
        "            # Accumulate weight, keep most informative metadata",
        "            existing = self.typed_connections[target_id]",
        "            new_weight = existing.weight + weight",
        "            # Prefer more specific relation types over 'co_occurrence'",
        "            new_relation = relation_type if relation_type != 'co_occurrence' else existing.relation_type"
      ],
      "context_after": [
        "            # Prefer semantic/inferred over corpus",
        "            source_priority = {'inferred': 3, 'semantic': 2, 'corpus': 1}",
        "            new_source = source if source_priority.get(source, 0) > source_priority.get(existing.source, 0) else existing.source",
        "            self.typed_connections[target_id] = Edge(",
        "                target_id=target_id,",
        "                weight=new_weight,",
        "                relation_type=new_relation,",
        "                confidence=new_confidence,",
        "                source=new_source",
        "            )"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/query/definitions.py",
      "function": "def is_test_file(doc_id: str) -> bool:",
      "start_line": 299,
      "lines_added": [
        "    test_with_definition_penalty: float = 0.5,",
        "    test_without_definition_penalty: float = 0.7",
        "    - Test files with the definition pattern get test_with_definition_penalty (default 0.5x)",
        "    - All other test files get test_without_definition_penalty (default 0.7x)",
        "        test_with_definition_penalty: Multiplier for test files that contain the definition",
        "            (default 0.5). Even test files with definitions are penalized vs source files.",
        "        test_without_definition_penalty: Multiplier for test files without the definition",
        "            (default 0.7). Set to 1.0 to disable test file penalty.",
        "                # Test file with definition: still penalized vs source files",
        "                boosted_docs.append((doc_id, score * test_with_definition_penalty))",
        "            boosted_docs.append((doc_id, score * test_without_definition_penalty))"
      ],
      "lines_removed": [
        "    test_file_boost_factor: float = 0.5,",
        "    test_file_penalty: float = 0.7",
        "    - Test files with the definition pattern get test_file_boost_factor (default 0.5x)",
        "    - All other test files get test_file_penalty (default 0.7x) to deprioritize them",
        "        test_file_boost_factor: Multiplier for test files with definition (default 0.5)",
        "        test_file_penalty: Multiplier for test files without definition (default 0.7)",
        "            Set to 1.0 to disable test file penalty.",
        "                # Test file with definition: apply reduced boost",
        "                boosted_docs.append((doc_id, score * test_file_boost_factor))",
        "            boosted_docs.append((doc_id, score * test_file_penalty))"
      ],
      "context_before": [
        "        return True",
        "",
        "    return False",
        "",
        "",
        "def boost_definition_documents(",
        "    doc_results: List[Tuple[str, float]],",
        "    query_text: str,",
        "    documents: Dict[str, str],",
        "    boost_factor: float = 2.0,"
      ],
      "context_after": [
        ") -> List[Tuple[str, float]]:",
        "    \"\"\"",
        "    Boost documents that contain the actual definition being searched for.",
        "",
        "    This helps ensure the source file containing a class/function definition",
        "    is included in the document candidates, even if test files mention the",
        "    identifier more frequently.",
        "",
        "    For definition queries:",
        "    - Source files with the definition pattern get boost_factor (default 2.0x)",
        "",
        "    Args:",
        "        doc_results: List of (doc_id, score) tuples",
        "        query_text: The original search query",
        "        documents: Dict mapping doc_id to document text",
        "        boost_factor: Multiplier for definition-containing source docs (default 2.0)",
        "",
        "    Returns:",
        "        Re-scored document results with definition boost applied",
        "    \"\"\"",
        "    definition_info = detect_definition_query(query_text)",
        "",
        "    if not definition_info['is_definition_query'] or not definition_info['pattern']:",
        "        return doc_results",
        "",
        "    pattern = re.compile(definition_info['pattern'], re.IGNORECASE)",
        "    boosted_docs = []",
        "",
        "    for doc_id, score in doc_results:",
        "        doc_text = documents.get(doc_id, '')",
        "        has_definition = pattern.search(doc_text)",
        "        is_test = is_test_file(doc_id)",
        "",
        "        if has_definition:",
        "            if is_test:",
        "            else:",
        "                # Source file with definition: apply full boost",
        "                boosted_docs.append((doc_id, score * boost_factor))",
        "        elif is_test:",
        "            # Test file without definition: apply penalty to deprioritize",
        "        else:",
        "            # Source file without definition: keep original score",
        "            boosted_docs.append((doc_id, score))",
        "",
        "    # Re-sort by boosted scores",
        "    boosted_docs.sort(key=lambda x: x[1], reverse=True)",
        "    return boosted_docs"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/query/expansion.py",
      "function": "This module provides:",
      "start_line": 11,
      "lines_added": [
        "from ..config import DEFAULT_CHAIN_VALIDITY"
      ],
      "lines_removed": [],
      "context_before": [
        "- Multi-hop inference through relation chains",
        "- Code concept expansion (programming synonyms)",
        "\"\"\"",
        "",
        "from typing import Dict, List, Tuple, Optional",
        "from collections import defaultdict",
        "",
        "from ..layers import CorticalLayer, HierarchicalLayer",
        "from ..tokenizer import Tokenizer, CODE_EXPANSION_STOP_WORDS",
        "from ..code_concepts import expand_code_concepts"
      ],
      "context_after": [
        "",
        "",
        "# Valid relation chain patterns for multi-hop inference",
        "# Key: (relation1, relation2) -> validity score (0.0 = invalid, 1.0 = fully valid)",
        "VALID_RELATION_CHAINS = {",
        "    # Transitive hierarchies",
        "    ('IsA', 'IsA'): 1.0,           # dog IsA animal IsA living_thing",
        "    ('PartOf', 'PartOf'): 1.0,     # wheel PartOf car PartOf vehicle",
        "    ('IsA', 'HasProperty'): 0.9,   # dog IsA animal HasProperty alive",
        "    ('PartOf', 'HasProperty'): 0.8,  # wheel PartOf car HasProperty fast"
      ],
      "change_type": "add"
    },
    {
      "file": "cortical/query/expansion.py",
      "function": "def score_relation_path(path: List[str]) -> float:",
      "start_line": 67,
      "lines_added": [
        "        pair_score = VALID_RELATION_CHAINS.get(pair, DEFAULT_CHAIN_VALIDITY)"
      ],
      "lines_removed": [
        "        pair_score = VALID_RELATION_CHAINS.get(pair, 0.4)  # Default: moderate validity"
      ],
      "context_before": [
        "    if not path:",
        "        return 1.0",
        "    if len(path) == 1:",
        "        return 1.0",
        "",
        "    # Compute score as product of consecutive pair validities",
        "    total_score = 1.0",
        "    for i in range(len(path) - 1):",
        "        pair = (path[i], path[i + 1])",
        "        # Check both orderings"
      ],
      "context_after": [
        "        total_score *= pair_score",
        "",
        "    return total_score",
        "",
        "",
        "def expand_query(",
        "    query_text: str,",
        "    layers: Dict[CorticalLayer, HierarchicalLayer],",
        "    tokenizer: Tokenizer,",
        "    max_expansions: int = 10,"
      ],
      "change_type": "modify"
    },
    {
      "file": "samples/bottleneck_diagnosis_procedures.txt",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "Bottleneck Diagnosis Procedures: Systematic Performance Problem Identification",
        "",
        "Performance bottleneck diagnosis requires systematic investigation rather than",
        "guesswork. These procedures establish methodical approaches to identifying",
        "and resolving computational constraints.",
        "",
        "The diagnosis imperative: intuition about performance often misleads.",
        "Developers frequently blame complex algorithms when simple operations cause",
        "problems. Always profile to identify actual bottlenecks before attempting",
        "optimization.",
        "",
        "Profiling workflow starts with measurement. Run the profiling script to measure",
        "time spent in each computation phase. Record baseline metrics: TF-IDF time,",
        "PageRank time, bigram connection time, semantic extraction time, and",
        "clustering time.",
        "",
        "Phase identification isolates the problem area. Compare phase durations.",
        "A phase consuming 80% of total time is the primary bottleneck. Focus",
        "investigation on the dominant phase rather than spreading effort across",
        "all operations.",
        "",
        "Common bottleneck patterns recur across codebases. O(n^2) complexity from",
        "pairwise operations is frequent: computing similarity between all document",
        "pairs, creating connections between all term pairs, or checking all possible",
        "matches.",
        "",
        "Term frequency analysis reveals hidden causes. Common terms like \"the\" or",
        "\"self\" appear in many documents and create disproportionate work. A single",
        "term appearing 1000 times creates millions of potential bigram connections.",
        "Filtering common terms resolves these hidden bottlenecks.",
        "",
        "Memory bottlenecks differ from CPU bottlenecks. CPU bottlenecks show high",
        "utilization during processing. Memory bottlenecks show swapping or allocation",
        "failures. Distinguish the constraint type before selecting solutions.",
        "",
        "Parameter tuning resolves many bottlenecks. Limits like max_bigrams_per_term,",
        "max_similarity_pairs, and min_context_keys control combinatorial explosion.",
        "Profile with different parameter values to find the optimal tradeoff between",
        "thoroughness and performance.",
        "",
        "Algorithmic optimization addresses fundamental inefficiency. If a phase has",
        "poor algorithmic complexity, parameter tuning provides only temporary relief.",
        "Algorithmic improvement (better data structures, smarter algorithms) provides",
        "sustainable solutions.",
        "",
        "Incremental processing avoids recomputation bottlenecks. Rather than",
        "recomputing everything after each change, incremental updates process only",
        "what changed. Incremental approaches turn O(n) operations into O(1) operations",
        "for single updates.",
        "",
        "Caching eliminates redundant computation bottlenecks. If the same calculation",
        "runs repeatedly with the same inputs, cache results. Query expansion, term",
        "lookups, and similarity scores are common caching opportunities.",
        "",
        "Batch processing resolves per-item overhead bottlenecks. Per-item overhead",
        "(function calls, memory allocation) compounds across many items. Batch",
        "processing amortizes overhead across items, improving throughput.",
        "",
        "Parallel processing addresses CPU-bound bottlenecks. If one CPU core limits",
        "throughput, distribute work across multiple cores. Identify parallelizable",
        "operations: independent document processing, independent query handling,",
        "independent similarity calculations.",
        "",
        "Database optimization resolves storage bottlenecks. If persistence operations",
        "are slow, optimize storage format. Binary formats read faster than text.",
        "Indexed access is faster than sequential scan. Compression reduces I/O time.",
        "",
        "Regression testing prevents bottleneck recurrence. After resolving a bottleneck,",
        "add performance tests that fail if the bottleneck returns. Automated testing",
        "catches regressions before they reach production.",
        "",
        "Documentation captures diagnosis findings. Record: what bottleneck was found,",
        "how it was identified, what solution was applied, and what results were",
        "achieved. Documentation prevents repeating diagnosis efforts and enables",
        "team learning.",
        "",
        "Continuous profiling catches new bottlenecks. As code evolves and data grows,",
        "new bottlenecks emerge. Regular profiling identifies problems before users",
        "notice degradation. Integrate profiling into development and deployment",
        "workflows.",
        ""
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "samples/codebase_search_procedures.txt",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "Codebase Search Procedures: Finding Code Patterns and Implementations Effectively",
        "",
        "Codebase search applies the cortical processor to source code repositories,",
        "enabling semantic code search that understands programming concepts. These",
        "procedures establish effective approaches to code-oriented retrieval.",
        "",
        "Code indexing preparation differs from text indexing. Enable split_identifiers",
        "in tokenization to break camelCase and snake_case identifiers into constituent",
        "words. \"getUserCredentials\" becomes \"get\", \"user\", \"credentials\", enabling",
        "matches against related terms. Without identifier splitting, compound names",
        "match only as exact strings.",
        "",
        "Programming concept expansion uses code_concepts. The code concepts module",
        "provides programming synonyms: \"get\" expands to \"fetch\", \"retrieve\", \"load\".",
        "Enable use_code_concepts in query expansion for code search. This expansion",
        "bridges vocabulary differences between searchers and code authors.",
        "",
        "Definition search locates class and function definitions. Query patterns like",
        "\"class DataProcessor\" or \"def calculate_statistics\" trigger definition search",
        "mode. The system identifies these as definition queries and prioritizes files",
        "containing the specified definitions.",
        "",
        "Implementation versus documentation queries require different handling. \"How",
        "does sorting work\" is conceptual: boost documentation. \"Sort function bubble\"",
        "is implementation: boost code files. Intent detection classifies queries and",
        "adjusts document type weights accordingly.",
        "",
        "Test file handling prevents test code from dominating results. Test files",
        "often import and reference production code, containing many matching terms.",
        "Apply test_file_penalty to reduce test file scores when searching for",
        "implementations. Users explicitly searching for tests can disable the penalty.",
        "",
        "Code-aware chunking respects semantic boundaries. Rather than splitting code",
        "at arbitrary character counts, split at class and function definitions.",
        "This produces passages that contain complete logical units rather than",
        "fragments. Code-aware chunks improve passage retrieval quality for code.",
        "",
        "Query formulation for code search differs from prose. Effective code queries",
        "combine: the action (get, set, calculate), the subject (user, data, result),",
        "and sometimes the context (database, API, cache). \"database connection pool\"",
        "finds pool implementations. \"validate user input\" finds input validation.",
        "",
        "Search refinement narrows results. If initial results are too broad, add",
        "specificity: file type filters, module constraints, or additional terms.",
        "If results miss relevant code, broaden: remove constraints, expand terms,",
        "or try synonyms. Iterative refinement converges on useful results.",
        "",
        "Cross-referencing finds related code. When finding one relevant function,",
        "examine what it imports, calls, and is called by. The document relationship",
        "graph shows code file connections. Follow connections to discover related",
        "implementations.",
        "",
        "Fingerprint comparison identifies similar code. Compute semantic fingerprints",
        "for code blocks and compare similarity. Similar fingerprints suggest code",
        "duplication or shared patterns. Use fingerprinting to find copy-paste code",
        "or implementations of similar logic.",
        "",
        "Change impact analysis uses search to find affected code. When modifying",
        "a function, search for its usages across the codebase. Results show files",
        "that may need updates. Search-based impact analysis complements IDE features",
        "when working across language boundaries.",
        "",
        "Documentation search within code finds comments and docstrings. Code files",
        "contain embedded documentation. Searching this documentation helps understand",
        "unfamiliar code. Index documentation alongside code for unified search.",
        "",
        "API surface discovery finds public interfaces. Search for function signatures,",
        "class definitions, and exported names to understand module APIs. Filter",
        "for public symbols to focus on intended interfaces rather than internal",
        "implementation details.",
        "",
        "Bug localization uses search to find relevant code. Given a bug report with",
        "symptoms, search for related terms: error messages, affected features, data",
        "types involved. Search-based localization narrows investigation scope.",
        "",
        "Code review search finds patterns to check. Search for potentially problematic",
        "patterns: SQL concatenation, unchecked inputs, deprecated APIs. Pattern-based",
        "search supports systematic code review for security and quality.",
        "",
        "Index maintenance for evolving codebases requires regular updates. Code",
        "changes frequently. Use incremental indexing to keep the search index current",
        "with repository changes. Integrate indexing with version control hooks for",
        "automatic updates.",
        ""
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "samples/computation_staleness_management.txt",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "Computation Staleness Management: Ensuring Fresh Data in Dynamic Corpora",
        "",
        "Staleness management addresses the fundamental tension between computation",
        "cost and data freshness. Understanding when values are stale and strategically",
        "managing recomputation enables efficient corpus operation.",
        "",
        "Staleness fundamentals: computed values like TF-IDF and PageRank derive from",
        "corpus content. When documents change, these values become stale and no longer",
        "accurately reflect the current corpus state. Staleness tracking identifies",
        "which computations need refresh.",
        "",
        "The staleness lifecycle follows document operations. Adding a document marks",
        "all computations stale because term frequencies and graph structure change.",
        "Computing a value marks it fresh. The cycle continues as corpus evolves.",
        "",
        "Computation dependencies create staleness chains. TF-IDF depends on document",
        "frequencies. PageRank depends on connection structure. Concept clusters depend",
        "on term similarities. Changes propagate staleness through dependent computations.",
        "",
        "Staleness flags are computation-specific. Each computation type (COMP_TFIDF,",
        "COMP_PAGERANK, COMP_CONCEPTS, etc.) has an independent staleness flag.",
        "Recomputing one does not automatically refresh others. Selective recomputation",
        "addresses specific staleness.",
        "",
        "Checking staleness uses the is_stale method. Before using computed values,",
        "check their freshness. Stale values may be acceptable for approximate operations",
        "but problematic for precise requirements. Application context determines",
        "acceptable staleness.",
        "",
        "Staleness tolerance varies by use case. Interactive search may tolerate",
        "slightly stale TF-IDF for faster response. Batch analysis may require",
        "completely fresh values for accuracy. Define staleness tolerance per operation.",
        "",
        "Deferred recomputation batches updates. Rather than recomputing after each",
        "document addition, defer until a batch completes or a threshold triggers.",
        "Deferred recomputation amortizes the cost across multiple updates.",
        "",
        "Incremental versus full recomputation trades accuracy for speed. Incremental",
        "TF-IDF updates efficiently handle single documents. Full recomputation",
        "recalculates everything from scratch. Choose based on staleness severity",
        "and performance requirements.",
        "",
        "Staleness-aware caching invalidates cached results when source computations",
        "become stale. If query results cache TF-IDF-based scores, invalidate those",
        "cache entries when TF-IDF staleness occurs. Cache invalidation maintains",
        "result consistency.",
        "",
        "Recomputation priority ordering addresses critical staleness first. TF-IDF",
        "typically has highest priority because queries depend on it directly.",
        "PageRank and clustering have lower priority for most applications.",
        "Order recomputation by operational impact.",
        "",
        "Staleness propagation analysis identifies downstream effects. When TF-IDF",
        "becomes stale, what operations are affected? When PageRank becomes stale,",
        "what features degrade? Understanding propagation guides refresh scheduling.",
        "",
        "Persistence preserves staleness state. Saving a corpus includes current",
        "staleness flags. Loading restores those flags. This enables consistent",
        "operation across process restarts without unnecessary recomputation.",
        "",
        "Monitoring staleness duration tracks how long values remain stale. Long",
        "staleness durations indicate insufficient recomputation scheduling.",
        "Short durations may indicate unnecessary computation overhead.",
        "",
        "Staleness budgets limit acceptable freshness gaps. Define maximum staleness",
        "duration per computation type. Alert when budgets exceed. Budgets ensure",
        "freshness without mandating constant recomputation.",
        "",
        "Testing staleness handling verifies correct behavior. Tests should cover:",
        "staleness after document addition, freshness after computation, persistence",
        "of staleness state, and correct staleness checking before value usage.",
        ""
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "samples/concept_cluster_evaluation.txt",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "Concept Cluster Evaluation: Assessing and Improving Semantic Groupings",
        "",
        "Concept clusters group related terms into semantic units representing topics",
        "or themes in the corpus. Evaluating cluster quality and improving cluster",
        "formation creates better organization for search and analysis.",
        "",
        "Cluster quality metrics quantify effectiveness. Modularity measures how well",
        "clusters separate: high modularity means terms connect more within clusters",
        "than between clusters. Silhouette score measures how well terms fit their",
        "assigned clusters: high silhouette means tight, well-separated clusters.",
        "Balance measures cluster size distribution: balanced clusters avoid singleton",
        "outliers and dominant mega-clusters.",
        "",
        "Louvain clustering algorithm determines cluster boundaries. The algorithm",
        "optimizes modularity by iteratively reassigning terms to clusters. Resolution",
        "parameter controls granularity: lower resolution creates fewer larger clusters,",
        "higher resolution creates more smaller clusters. Tune resolution based on",
        "corpus size and desired granularity.",
        "",
        "Cluster interpretation assigns meaning to groups. Examine top terms in each",
        "cluster by PageRank or frequency. Identify the unifying theme: a cluster",
        "containing \"neural\", \"network\", \"learning\", \"training\" likely represents",
        "machine learning. Meaningful clusters aid navigation and understanding.",
        "",
        "Problematic cluster patterns indicate issues. Singleton clusters (one term)",
        "suggest insufficient content for that topic. Mega-clusters (containing most",
        "terms) suggest resolution is too low or corpus lacks clear structure.",
        "Mixed-theme clusters suggest resolution is too high or terms are ambiguous.",
        "",
        "Resolution tuning optimizes cluster formation. Start with default resolution",
        "(1.0). If clusters are too coarse, increase resolution. If clusters fragment",
        "excessively, decrease resolution. Use cluster quality metrics to guide",
        "adjustment. Optimal resolution varies by corpus.",
        "",
        "Cluster stability analysis tests robustness. Run clustering multiple times",
        "with slight parameter variations. Stable clusters appear consistently.",
        "Unstable clusters change membership frequently. Stable clusters represent",
        "real structure; unstable clusters represent noise or ambiguity.",
        "",
        "Manual cluster refinement overrides automatic assignment. When a term clearly",
        "belongs to a different cluster than assigned, manual adjustment improves",
        "quality. Document manual refinements to reapply after re-clustering.",
        "Balance manual effort against the cost of imperfect automatic clustering.",
        "",
        "Cross-validation with external categories evaluates alignment. If ground-truth",
        "categories exist (document tags, manual classifications), compare cluster",
        "membership against categories. High overlap suggests clusters capture real",
        "structure. Low overlap suggests clustering or categories need adjustment.",
        "",
        "Cluster evolution tracking monitors changes over time. As corpus grows,",
        "clusters should evolve meaningfully. New topics should form new clusters.",
        "Expanding topics should enlarge existing clusters. Track cluster changes",
        "across indexing runs to verify sensible evolution.",
        "",
        "Hierarchical clustering provides multiple granularities. Rather than choosing",
        "one resolution, compute clusters at multiple resolutions. Coarse clusters",
        "provide high-level topics. Fine clusters provide detailed subtopics. Users",
        "navigate from general to specific based on their needs.",
        "",
        "Cluster-based navigation enables browsing. Present clusters as navigation",
        "categories. Users click into clusters to see member terms and related",
        "documents. Cluster browsing complements keyword search by supporting",
        "exploration without specific query terms.",
        "",
        "Integration with query expansion uses clusters. Expand queries to include",
        "terms from the same cluster. This expands semantically rather than just",
        "by co-occurrence. Cluster-based expansion improves recall for conceptual",
        "queries.",
        "",
        "Visualization of clusters aids evaluation. Display clusters as colored groups",
        "in a term graph. Well-separated visual clusters indicate good quality.",
        "Overlapping or fragmented visual clusters indicate problems. Visual evaluation",
        "complements quantitative metrics.",
        "",
        "Cluster documentation captures interpretation. Record the theme or topic",
        "each cluster represents. Document which clusters are reliable versus noisy.",
        "Interpretation documentation guides users of cluster-based features and",
        "aids future evaluation.",
        "",
        "Iterative improvement cycle refines clusters. Evaluate metrics, identify",
        "problems, adjust parameters, re-cluster, re-evaluate. Iteration continues",
        "until quality metrics satisfy requirements or further improvement becomes",
        "impractical. Document the improvement process for future reference.",
        ""
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "samples/corpus_indexing_procedures.txt",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "Corpus Indexing Procedures: Building and Maintaining Document Collections",
        "",
        "Corpus indexing transforms raw documents into searchable hierarchical representations.",
        "Following consistent procedures ensures reliable indexing, reproducible results,",
        "and efficient corpus maintenance over time.",
        "",
        "Document preparation standardizes content before indexing. Remove formatting artifacts,",
        "normalize whitespace, and ensure consistent encoding (UTF-8 preferred). Documents",
        "with inconsistent formatting produce noisy token distributions that degrade search",
        "quality. Clean documents yield cleaner connections and better retrieval.",
        "",
        "Document identification requires unique stable IDs. The doc_id parameter identifies",
        "documents throughout the system. Use meaningful identifiers that remain stable",
        "across re-indexing: filenames, URLs, or content hashes. Changing IDs breaks",
        "document references and connection tracking.",
        "",
        "Batch indexing processes multiple documents efficiently. The add_documents_batch",
        "method processes documents together, optimizing memory usage and computation.",
        "Process documents in batches of 50-100 for large corpora rather than one at a time.",
        "Batch processing reduces overhead from repeated computation setup.",
        "",
        "The processing pipeline flows through four layers. Layer 0 (tokens) extracts",
        "individual words. Layer 1 (bigrams) captures word pairs. Layer 2 (concepts) forms",
        "semantic clusters. Layer 3 (documents) stores full document representations.",
        "Each layer builds on previous layers, creating hierarchical structure.",
        "",
        "Compute phases follow processing. After adding documents, run compute_all to",
        "calculate derived values: TF-IDF scores, PageRank importance, bigram connections,",
        "document relationships, and concept clusters. These computations establish the",
        "network structure enabling search and analysis.",
        "",
        "Incremental indexing updates existing corpora without full recomputation. The",
        "add_document_incremental method adds single documents and updates TF-IDF efficiently.",
        "Use incremental indexing for live systems where documents arrive continuously.",
        "Full recomputation via compute_all remains necessary for PageRank and clustering.",
        "",
        "Staleness tracking monitors computation freshness. After adding documents, TF-IDF,",
        "PageRank, and other computed values become stale. Check staleness with is_stale",
        "method before relying on computed values. Recompute only stale computations to",
        "minimize unnecessary work.",
        "",
        "Persistence procedures save corpus state. The save method serializes all layers,",
        "connections, and computed values to disk. Save after compute_all completes to",
        "preserve computation results. Load restores full state including staleness tracking.",
        "Use descriptive filenames including corpus version and date.",
        "",
        "Corpus verification validates indexing completeness. After indexing, verify:",
        "document count matches expectations, token distribution appears reasonable, no",
        "documents have zero tokens, and concept clusters form meaningful groups. Automated",
        "verification catches indexing problems early.",
        "",
        "Re-indexing procedures handle corpus updates. When documents change significantly,",
        "consider full re-index rather than incremental updates. Re-indexing rebuilds all",
        "connections and recalculates all scores. Schedule re-indexing during low-usage",
        "periods for production systems.",
        "",
        "Index versioning tracks corpus evolution. Maintain version numbers or timestamps",
        "for indexed corpora. Document which version corresponds to which document set.",
        "Version tracking enables rollback when indexing problems occur and comparison",
        "across corpus versions.",
        "",
        "Performance monitoring during indexing tracks resource usage. Monitor memory",
        "consumption (large corpora may exceed available RAM), processing time per document,",
        "and total indexing duration. Set timeouts for each phase to catch runaway",
        "computations. Profile indexing to identify bottlenecks.",
        "",
        "Quality metrics evaluate indexed corpus health. Coverage score indicates how",
        "well documents connect. Connectivity score measures network density. Concept",
        "cluster quality metrics (modularity, silhouette) assess clustering effectiveness.",
        "Track these metrics across indexing runs to detect degradation.",
        ""
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "samples/corpus_maintenance_workflow.txt",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "Corpus Maintenance Workflow: Sustaining Healthy Document Collections Over Time",
        "",
        "Corpus maintenance encompasses the ongoing activities required to keep document",
        "collections healthy, performant, and relevant. This workflow establishes",
        "regular maintenance procedures for sustained corpus quality.",
        "",
        "Maintenance philosophy: corpora are living systems that evolve with content",
        "changes, usage patterns, and requirements. Neglected corpora accumulate",
        "problems that compound over time. Regular maintenance prevents degradation.",
        "",
        "Content freshness procedures ensure documents remain current. Review content",
        "age distributions. Identify stale documents that may contain outdated",
        "information. Update or remove obsolete content. Fresh content improves",
        "search relevance.",
        "",
        "Connection quality validation checks graph health. Count orphaned minicolumns",
        "(no connections), measure connection density, and identify weak clusters.",
        "Unhealthy connection patterns indicate corpus structure problems requiring",
        "investigation.",
        "",
        "Index freshness maintenance keeps computations current. Monitor staleness",
        "duration for each computation type. Schedule recomputation before staleness",
        "exceeds acceptable thresholds. Balance freshness against computation cost.",
        "",
        "Storage optimization reduces corpus footprint. Compact chunk files to",
        "consolidate incremental changes. Remove deleted document artifacts.",
        "Compress rarely-accessed historical data. Optimized storage improves",
        "load times and reduces costs.",
        "",
        "Parameter review ensures settings match current needs. As corpus grows,",
        "optimal parameters may change. Periodically review limits like max_bigrams_per_term,",
        "max_expansions, and similarity thresholds. Adjust parameters to maintain",
        "performance as scale increases.",
        "",
        "Coverage analysis identifies content gaps. Run gap analysis to find isolated",
        "documents and weak topics. Plan content additions to strengthen weak areas.",
        "Coverage maintenance keeps the corpus balanced and well-connected.",
        "",
        "Quality metric tracking monitors corpus health trends. Track metrics like",
        "coverage score, connectivity score, query success rate, and response latency.",
        "Trending analysis reveals gradual degradation before acute problems occur.",
        "",
        "Backup procedures protect against data loss. Regular backups capture corpus",
        "state. Test backup restoration periodically. Backup validation ensures",
        "recovery capability when needed.",
        "",
        "Version control for configuration tracks parameter changes. Store settings",
        "in version control. Review configuration history to understand how settings",
        "evolved. Configuration versioning enables rollback if changes cause problems.",
        "",
        "Dependency updates maintain system health. Update tokenization rules, stop",
        "word lists, and code concept mappings as language evolves. Outdated",
        "dependencies cause indexing drift from actual usage patterns.",
        "",
        "Performance baseline updates account for growth. As corpus grows, baseline",
        "expectations should adjust. A query that was fast on 100 documents may be",
        "slower on 10,000 documents. Update baselines to reflect current scale.",
        "",
        "User feedback integration guides maintenance priorities. Users notice",
        "problems that automated monitoring misses. Collect and analyze user feedback",
        "to identify maintenance needs. User-reported issues often indicate high-impact",
        "problems.",
        "",
        "Audit procedures verify maintenance completion. Document maintenance activities.",
        "Verify scheduled tasks actually ran. Audit logs ensure maintenance",
        "commitments translate to actual work.",
        "",
        "Runbook documentation enables consistent maintenance. Document step-by-step",
        "procedures for each maintenance task. Runbooks enable any trained operator",
        "to perform maintenance correctly.",
        "",
        "Maintenance scheduling distributes work appropriately. Heavy maintenance",
        "runs during off-peak hours. Light maintenance can run continuously.",
        "Scheduled maintenance prevents both neglect and excessive overhead.",
        ""
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "samples/document_relevance_tuning.txt",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "Document Relevance Tuning: Improving Search Result Quality Through Configuration",
        "",
        "Document relevance tuning adjusts search behavior to surface the most useful",
        "documents for user queries. Systematic tuning improves precision without sacrificing",
        "recall, creating better search experiences across different query types.",
        "",
        "Relevance scoring fundamentals establish the baseline. Documents score based on",
        "term matches weighted by TF-IDF values. Higher TF-IDF terms contribute more to",
        "scores. Understanding this mechanism enables targeted adjustments: boost distinctive",
        "terms, reduce common term influence, or adjust term weights.",
        "",
        "Document type boosting prioritizes certain content types. The doc_type_boost",
        "parameter applies multipliers based on document classification. Boost documentation",
        "for conceptual queries, boost code for implementation queries, penalize test files",
        "when searching for source implementations. Type-based boosting aligns results with",
        "likely user intent.",
        "",
        "Test file penalties prevent test code from dominating results. Test files often",
        "contain the same terms as source files but with less useful context. Apply a",
        "test_file_penalty (default 0.5) to reduce test file scores. This surfaces source",
        "implementations while keeping tests findable when explicitly sought.",
        "",
        "Definition boost prioritizes files containing class or function definitions.",
        "When queries match definition patterns (class Foo, def bar), apply definition_boost",
        "to files containing those definitions. This helps implementation queries find",
        "source code rather than usage examples.",
        "",
        "Query expansion parameters balance precision and recall. Lower max_expansions",
        "increases precision by restricting query breadth. Higher max_expansions increases",
        "recall by including more related terms. Tune expansion based on corpus characteristics:",
        "specialized corpora may need less expansion than general collections.",
        "",
        "TF-IDF weighting adjustments customize term importance. Global TF-IDF uses corpus-wide",
        "statistics. Per-document TF-IDF considers document-specific term frequency. Choose",
        "the appropriate measure based on query needs. Per-document TF-IDF often provides",
        "better relevance for specific document retrieval.",
        "",
        "PageRank integration adds network centrality to scoring. Terms with high PageRank",
        "connect to many other important terms. Incorporating PageRank boosts documents",
        "containing central concepts. This helps for exploratory queries where users seek",
        "authoritative coverage rather than specific details.",
        "",
        "Passage retrieval parameters affect chunk quality. Chunk size determines passage",
        "length: smaller chunks provide more precise excerpts, larger chunks provide more",
        "context. Chunk overlap ensures important content at boundaries is not missed.",
        "Tune these parameters based on downstream usage requirements.",
        "",
        "Result diversity controls prevent result clustering. Without diversity controls,",
        "top results may all cover the same narrow topic. Diversity parameters ensure",
        "results span different concept clusters when multiple relevant topics exist.",
        "Balance diversity against pure relevance based on use case.",
        "",
        "Feedback integration improves tuning over time. Collect implicit feedback through",
        "click patterns or explicit feedback through ratings. Documents consistently clicked",
        "or highly rated should rank higher. Feedback-based tuning adapts to actual user",
        "needs rather than assumed preferences.",
        "",
        "A/B testing validates tuning changes. Before deploying tuning adjustments, compare",
        "new parameters against baseline in controlled tests. Measure precision, recall,",
        "user satisfaction, and task completion. Deploy changes only when testing confirms",
        "improvement.",
        "",
        "Tuning profiles store configurations for different use cases. A code search profile",
        "may differ from a documentation search profile. Save tuning parameters as named",
        "profiles that can be selected based on query context or user preference.",
        "",
        "Monitoring detects tuning degradation. Track relevance metrics continuously.",
        "As corpus content changes, tuning may become less effective. Establish alerts",
        "when metrics drop below thresholds to trigger re-tuning cycles.",
        "",
        "Documentation captures tuning rationale. Record why each parameter was set to",
        "its current value. When revisiting tuning, understanding past decisions prevents",
        "repeating unsuccessful experiments and preserves institutional knowledge.",
        ""
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "samples/incremental_indexing_workflow.txt",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "Incremental Indexing Workflow: Maintaining Live Corpora with Continuous Updates",
        "",
        "Incremental indexing enables corpus updates without full recomputation, essential",
        "for systems where documents arrive continuously or change frequently. This workflow",
        "establishes procedures for efficient incremental maintenance.",
        "",
        "The incremental indexing advantage is efficiency. Full compute_all recalculates",
        "everything: TF-IDF, PageRank, bigram connections, concept clusters, and document",
        "relationships. Incremental updates recalculate only affected components, reducing",
        "update latency from seconds to milliseconds for single document additions.",
        "",
        "Document addition workflow uses add_document_incremental. This method processes",
        "the document through all layers and updates TF-IDF scores. The recompute parameter",
        "controls what gets recalculated: 'tfidf' updates only TF-IDF (fastest), 'all'",
        "triggers full recomputation (thorough), 'none' skips recomputation (deferred).",
        "",
        "Staleness tracking after incremental updates matters. After add_document_incremental,",
        "PageRank and concept clusters become stale. Check staleness before relying on",
        "these values. Deferred computation batches multiple updates before expensive",
        "recalculation.",
        "",
        "Batch incremental updates balance efficiency and freshness. Rather than recomputing",
        "after each document, batch updates and recompute periodically. Add 100 documents",
        "with recompute='none', then call compute_all once. This approach maintains near-",
        "real-time indexing with periodic full refresh.",
        "",
        "Document modification requires deletion and re-addition. The system does not",
        "support in-place document updates. To modify a document: remove the old version,",
        "add the new version, then recompute affected values. Track modifications separately",
        "from additions for accurate change monitoring.",
        "",
        "Document deletion procedures clean up connections. Removing a document requires",
        "removing its minicolumn from the document layer and updating connections in other",
        "layers. After deletion, TF-IDF scores change because document frequency changes.",
        "Recompute to reflect the updated corpus state.",
        "",
        "Concurrent access considerations apply to live systems. If multiple processes",
        "add documents simultaneously, coordinate access to prevent race conditions.",
        "Use locking or single-writer patterns to ensure consistent corpus state during",
        "updates.",
        "",
        "Persistence strategy for incremental systems balances durability and performance.",
        "Saving after every document addition ensures durability but creates I/O overhead.",
        "Save periodically (every N documents or every M minutes) and accept potential",
        "loss of recent updates. Configure based on durability requirements.",
        "",
        "Recovery procedures handle interrupted updates. If the system crashes during",
        "incremental indexing, the in-memory state is lost. On restart, load the last",
        "saved state and re-process documents added since the save. Track document",
        "addition timestamps to identify missed documents.",
        "",
        "Chunk-based persistence for team collaboration stores changes incrementally.",
        "Each indexing session creates a new chunk file with additions and modifications.",
        "Chunk files are git-friendly: small, append-only, and conflict-free. Periodic",
        "compaction consolidates chunks like git garbage collection.",
        "",
        "Monitoring incremental performance tracks update latency. Measure time per",
        "document addition, time per periodic recomputation, and queue depth if updates",
        "are buffered. Alert when latency exceeds thresholds indicating system stress.",
        "",
        "Cache invalidation coordinates with incremental updates. If query results are",
        "cached, invalidate affected cache entries when documents change. Track which",
        "documents affect which cached queries for precise invalidation rather than",
        "full cache flush.",
        "",
        "Testing incremental workflows verifies correctness. Compare results from",
        "incremental updates against full recomputation to verify equivalence. Any",
        "divergence indicates bugs in incremental logic. Automated regression tests",
        "catch incremental update problems early.",
        "",
        "Scalability planning anticipates growth. Incremental indexing maintains performance",
        "as corpora grow, but eventual limits exist. Plan for corpus size milestones",
        "and test performance at projected sizes before they occur.",
        "",
        "Operational runbooks document incremental procedures. Standard operating procedures",
        "for common scenarios: adding documents, handling failures, triggering recomputation,",
        "and performing maintenance. Runbooks ensure consistent operations across team",
        "members and shifts.",
        ""
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "samples/knowledge_gap_analysis_process.txt",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "Knowledge Gap Analysis Process: Identifying and Addressing Corpus Deficiencies",
        "",
        "Knowledge gap analysis systematically identifies areas where corpus coverage",
        "is thin, disconnected, or missing entirely. This process enables targeted",
        "content development to strengthen weak areas and improve overall search quality.",
        "",
        "Gap detection begins with coverage assessment. The knowledge gap analysis function",
        "computes a coverage score indicating how well topics interconnect. Low coverage",
        "suggests isolated documents that share few concepts with the rest of the corpus.",
        "High coverage indicates dense interconnection where topics relate meaningfully.",
        "",
        "Isolated document identification finds outliers. Documents with low average",
        "similarity to other documents may represent unique topics worth expanding or",
        "irrelevant content worth removing. Review isolated documents to determine whether",
        "they add value or introduce noise.",
        "",
        "Weak topic detection scans for terms appearing in few documents. Terms with high",
        "TF-IDF but low document frequency indicate thin coverage. These distinctive terms",
        "lack supporting documents that would establish context and enable semantic",
        "connections. Weak topics represent opportunities for content expansion.",
        "",
        "Connectivity scoring evaluates network density. The connectivity score measures",
        "how well documents link through shared terms and concepts. Low connectivity",
        "suggests fragmentation: document clusters that do not connect to each other.",
        "High connectivity enables broader query expansion and better cross-topic retrieval.",
        "",
        "The gap analysis workflow follows systematic steps. First, run gap analysis on",
        "the indexed corpus to generate metrics. Second, review isolated documents to",
        "understand what topics lack support. Third, identify weak topics needing more",
        "coverage. Fourth, prioritize gaps by impact on search quality.",
        "",
        "Prioritization criteria rank gaps by importance. Consider: frequency of queries",
        "hitting the gap area, business importance of the topic, effort required to create",
        "supporting content, and potential to improve related areas. Address high-impact,",
        "low-effort gaps first.",
        "",
        "Content planning addresses identified gaps. For each priority gap, determine what",
        "documents would strengthen coverage. Plan documents that share terms with existing",
        "content while adding new perspectives. Overlapping coverage creates the connections",
        "that enable semantic expansion.",
        "",
        "Coverage validation verifies gap closure. After adding documents, re-run gap",
        "analysis to confirm improvement. Coverage score should increase. Previously",
        "isolated documents should gain connections. Previously weak topics should appear",
        "in more documents. Quantitative verification prevents subjective assessment.",
        "",
        "Iterative refinement continues the cycle. Gap analysis is not a one-time activity",
        "but ongoing maintenance. As corpora grow, new gaps emerge. Regular gap analysis",
        "identifies emerging weaknesses before they impact search quality significantly.",
        "",
        "Query log analysis supplements automated detection. Review search logs for queries",
        "with poor results or no results. These failed queries reveal user needs that the",
        "corpus does not address. Map failed queries to content gaps to align expansion",
        "with actual usage patterns.",
        "",
        "Concept cluster analysis reveals structural gaps. Well-formed clusters indicate",
        "coherent topic coverage. Fragmented or singleton clusters suggest insufficient",
        "documents to establish topic coherence. Aim for moderate-sized clusters with",
        "clear thematic unity.",
        "",
        "Document relationship mapping visualizes coverage. Export document connections",
        "and visualize as a graph. Dense regions indicate strong coverage. Sparse regions",
        "or disconnected components indicate gaps. Visual inspection often reveals patterns",
        "that metrics alone miss.",
        "",
        "Semantic relation gaps identify missing relationship types. The semantics layer",
        "extracts relations between terms. Review relation types to identify missing",
        "categories. If the corpus covers \"is-a\" relations but lacks \"causes\" relations,",
        "targeted content could address this structural gap.",
        "",
        "Cross-referencing external sources identifies absolute gaps. Compare corpus",
        "topics against domain taxonomies, Wikipedia categories, or textbook tables of",
        "contents. Topics present in references but absent from corpus represent clear",
        "expansion targets.",
        ""
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "samples/performance_profiling_process.txt",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "Performance Profiling Process: Identifying and Resolving Computational Bottlenecks",
        "",
        "Performance profiling systematically identifies bottlenecks in processing and",
        "search operations. This process establishes methodical approaches to finding",
        "and fixing performance problems.",
        "",
        "The profiling imperative: measure before optimizing. Intuition about performance",
        "is often wrong. The obvious culprit frequently is not the actual bottleneck.",
        "Always profile to identify actual slow operations before attempting optimization.",
        "Data-driven optimization succeeds; guesswork wastes effort.",
        "",
        "Full analysis profiling uses the profiling script. Run profile_full_analysis.py",
        "to measure time for each compute phase: TF-IDF, PageRank, bigram connections,",
        "semantic extraction, and concept clustering. Results reveal which phases",
        "dominate processing time.",
        "",
        "Phase-specific bottleneck identification focuses effort. If bigram_connections",
        "takes 80% of total time, optimize bigram processing rather than PageRank.",
        "If semantic extraction dominates, optimize pattern matching. Focus on the",
        "actual bottleneck, not the presumed bottleneck.",
        "",
        "O(n-squared) complexity detection identifies scaling problems. Loops that",
        "iterate over all pairs of items create O(n^2) complexity. Common culprits:",
        "similarity computation between all document pairs, connection creation",
        "between all term pairs. These operations explode as corpus grows.",
        "",
        "Parameter limits control combinatorial explosion. Max_bigrams_per_term limits",
        "how many bigram connections each term creates. Max_similarity_pairs limits",
        "total similarity computations. These limits trade completeness for tractability.",
        "Tune limits based on acceptable processing time.",
        "",
        "Common term filtering reduces unnecessary work. Terms like \"the\", \"is\", \"self\"",
        "appear everywhere and create massive connection counts without semantic value.",
        "Stop word removal and minimum document frequency thresholds filter common",
        "terms before they create computational problems.",
        "",
        "Memory profiling identifies resource constraints. Large corpora may exceed",
        "available memory. Profile memory usage during processing to identify peaks.",
        "If memory is the constraint, process documents in batches or use streaming",
        "approaches that avoid materializing all data simultaneously.",
        "",
        "Query performance profiling measures search latency. Profile representative",
        "queries to understand typical and worst-case latency. Identify queries that",
        "are unexpectedly slow. Slow queries often involve expensive expansion or",
        "large result sets.",
        "",
        "Caching strategies accelerate repeated operations. Cache expanded query terms",
        "to avoid recomputation. Cache document scores for common queries. Cache",
        "intermediate results during batch processing. Profile cache hit rates to",
        "verify caching effectiveness.",
        "",
        "Index optimization improves lookup speed. The _id_index in HierarchicalLayer",
        "provides O(1) lookups by ID. Ensure all lookups use indexed access rather",
        "than linear scans. Profile to verify lookups use indices as expected.",
        "",
        "Batch versus streaming tradeoffs affect resource usage. Batch processing",
        "uses more memory but enables optimization across items. Streaming processing",
        "uses less memory but processes items independently. Choose based on resource",
        "constraints and processing requirements.",
        "",
        "Parallel processing opportunities exist in independent operations. Query",
        "expansion, document scoring, and passage chunking can parallelize across",
        "items. Identify parallelizable operations and evaluate if parallelization",
        "improves throughput without excessive overhead.",
        "",
        "Regression testing catches performance degradation. Establish performance",
        "baselines and test against them regularly. Alert when performance regresses",
        "beyond acceptable thresholds. Performance regression tests prevent gradual",
        "degradation from going unnoticed.",
        "",
        "Profiling documentation records findings. Document: which operations were",
        "profiled, what bottlenecks were found, what optimizations were applied, and",
        "what results were achieved. Documentation enables learning from optimization",
        "efforts and prevents repeating unsuccessful approaches.",
        "",
        "Continuous monitoring in production tracks real-world performance. Development",
        "profiling uses synthetic workloads. Production monitoring captures actual",
        "usage patterns. Combine both to understand performance across representative",
        "and real-world conditions.",
        "",
        "Optimization prioritization balances effort and impact. Fix bottlenecks that",
        "affect common operations before rare edge cases. Consider maintainability",
        "cost of optimizations: complex optimizations create technical debt. Simple",
        "optimizations that deliver meaningful improvement are preferable.",
        "",
        "Performance budgets establish acceptable limits. Define target latencies for",
        "processing and queries. Design and optimize to meet budgets rather than",
        "pursuing unnecessary micro-optimization. Budgets focus effort on meaningful",
        "improvements.",
        ""
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "samples/query_expansion_tuning_guide.txt",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "Query Expansion Tuning Guide: Optimizing Search Recall and Precision",
        "",
        "Query expansion transforms user queries by adding semantically related terms,",
        "improving recall for searches that would otherwise miss relevant documents.",
        "Tuning expansion balances recall improvement against precision degradation.",
        "",
        "Expansion fundamentals: the expand_query function adds terms based on lateral",
        "connections in the token layer. Terms that frequently co-occur in documents",
        "form connections. Expansion adds connected terms to broaden the search.",
        "",
        "The max_expansions parameter controls breadth. Default value is 10, adding",
        "up to 10 related terms per query term. Lower values increase precision",
        "(fewer false positives). Higher values increase recall (fewer missed documents).",
        "Tune based on corpus characteristics and user needs.",
        "",
        "Expansion weight distribution affects ranking. Original query terms receive",
        "weight 1.0. Expanded terms receive weights based on connection strength,",
        "typically decreasing from 0.5 down to near zero. Strong connections receive",
        "higher weights; weak connections receive lower weights.",
        "",
        "Precision versus recall tradeoff is fundamental. More expansion increases",
        "recall (finding more relevant documents) but may decrease precision (more",
        "irrelevant results). Different applications favor different tradeoffs:",
        "exploratory search favors recall; precise lookup favors precision.",
        "",
        "Code-aware expansion uses programming synonyms. Enable use_code_concepts",
        "for code search. This adds programming vocabulary equivalents: \"get\" expands",
        "to \"fetch\", \"retrieve\", \"load\". Code-aware expansion bridges vocabulary",
        "differences between searchers and code authors.",
        "",
        "Query intent affects optimal expansion. Conceptual queries like \"what is",
        "machine learning\" benefit from more expansion to find explanatory documents.",
        "Implementation queries like \"compute pagerank\" benefit from less expansion",
        "to find specific code.",
        "",
        "Corpus characteristics influence expansion settings. Dense corpora with",
        "many shared terms may need lower expansion to avoid overwhelming results.",
        "Sparse corpora with distinct vocabularies may need higher expansion to",
        "find any connections.",
        "",
        "Evaluation methodology measures expansion effectiveness. Use test query sets",
        "with known relevant documents. Measure precision (relevant results / total",
        "results) and recall (found relevant / total relevant). Tune parameters to",
        "optimize desired metric.",
        "",
        "Per-query tuning handles diverse query types. Rather than global settings,",
        "adjust expansion based on query characteristics. Short queries may need",
        "more expansion. Long queries may need less. Query length heuristics can",
        "automate adjustment.",
        "",
        "Feedback-based tuning learns from usage. Track which queries succeed and",
        "which fail. Analyze failures to identify under-expansion (missing relevant",
        "docs) or over-expansion (too many irrelevant results). Adjust settings",
        "based on observed patterns.",
        "",
        "Expansion quality depends on corpus quality. If corpus lacks good lateral",
        "connections, expansion cannot find related terms. Invest in corpus coverage",
        "and connection quality before expecting expansion to perform well.",
        "",
        "Stop word handling affects expansion paths. Stop words removed from indexing",
        "do not participate in expansion. Ensure stop word lists are appropriate",
        "for your domain. Technical terms that are common in general text may be",
        "important in specialized corpora.",
        "",
        "Bigram expansion versus token expansion offers different behavior. Token",
        "expansion finds related individual words. Bigram expansion finds related",
        "phrases. Combine both for comprehensive query broadening.",
        "",
        "Testing expansion changes requires controlled comparison. Before deploying",
        "new expansion settings, A/B test against current settings. Statistical",
        "significance ensures observed improvements are real rather than noise.",
        "",
        "Documentation of tuning decisions preserves rationale. Record why each",
        "setting was chosen. Future maintainers need context to understand and",
        "update tuning. Without documentation, optimal settings may be lost to",
        "staff turnover or system changes.",
        ""
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "samples/query_optimization_workflow.txt",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "Query Optimization Workflow: Improving Search Results Through Systematic Refinement",
        "",
        "Effective query optimization requires understanding how the cortical text processor",
        "expands and scores search terms. This workflow establishes a systematic approach",
        "to improving search results through iterative refinement and analysis.",
        "",
        "The initial query assessment examines raw query terms before expansion. Start by",
        "tokenizing the query to understand what tokens will match against the corpus.",
        "Stop words are removed, stemming normalizes variants, and the remaining tokens",
        "form the search basis. Understanding this transformation prevents surprises when",
        "results differ from expectations.",
        "",
        "Query expansion analysis reveals how the system broadens searches. The expand_query",
        "function adds semantically related terms based on lateral connections in the token",
        "layer. High-weight connections come from co-occurrence patterns: terms appearing",
        "frequently together in documents develop strong associations. Review the expanded",
        "terms to verify they align with search intent.",
        "",
        "The expansion weight distribution affects result ranking significantly. Original",
        "query terms receive weight 1.0 by default, while expanded terms receive decreasing",
        "weights based on connection strength. If expanded terms dominate results, consider",
        "reducing max_expansions or adjusting expansion weights. If results are too narrow,",
        "increase expansion to capture related concepts.",
        "",
        "Document scoring combines term weights with TF-IDF values. A document scores higher",
        "when it contains multiple query terms with high TF-IDF scores. Understanding this",
        "scoring mechanism helps diagnose why certain documents rank unexpectedly. Low TF-IDF",
        "terms (common words) contribute less to scores than distinctive terms.",
        "",
        "Iterative refinement follows a cycle: query, review results, analyze expansion,",
        "adjust parameters, repeat. Track which parameter changes improve relevance. Common",
        "adjustments include: changing max_expansions (default 10), adjusting top_n results,",
        "enabling code-aware expansion for programming content, or using intent detection",
        "for natural language queries.",
        "",
        "Query intent classification distinguishes conceptual from implementation queries.",
        "Conceptual queries seek explanations and definitions. Implementation queries seek",
        "code and technical details. The system detects intent through patterns like \"what is\"",
        "versus \"how to implement\". Matching query intent to document type improves relevance.",
        "",
        "Phrase queries leverage bigram matching. When searching for specific phrases like",
        "\"neural networks\", the bigram layer matches the phrase directly rather than",
        "individual tokens. This produces higher precision for known phrases while single",
        "tokens provide higher recall for exploratory searches.",
        "",
        "Performance monitoring tracks query latency and result quality. Average queries",
        "complete in under 100ms. Slow queries may indicate inefficient expansion or large",
        "result sets. Track latency alongside relevance to balance thoroughness with speed.",
        "",
        "Result diversity evaluation checks whether results span different topics or cluster",
        "around one area. The concept layer reveals topic distribution across results. If",
        "results lack diversity, consider boosting documents from underrepresented clusters",
        "or adjusting document connection weights.",
        "",
        "The optimization feedback loop captures improvements systematically. Document which",
        "queries benefited from which adjustments. Build a query tuning guide specific to",
        "your corpus. Different corpora require different optimal parameters based on their",
        "term distributions and document relationships.",
        "",
        "Baseline comparison validates improvements. Before optimizing, establish baseline",
        "metrics: precision at top-5, mean reciprocal rank, and query latency. Compare",
        "optimized results against baselines to verify actual improvement rather than",
        "subjective impression.",
        "",
        "Query logging enables analysis over time. Log queries, expansions, and result",
        "document IDs. Review logs to identify patterns: recurring queries suggest missing",
        "documents, consistently poor results indicate corpus gaps, and successful queries",
        "reveal effective patterns to replicate.",
        ""
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "samples/rag_pipeline_integration.txt",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "RAG Pipeline Integration: Connecting Passage Retrieval to Language Models",
        "",
        "Retrieval-augmented generation (RAG) combines document retrieval with language",
        "model generation. This integration process establishes reliable pipelines from",
        "corpus search to augmented responses.",
        "",
        "RAG architecture overview connects components. The cortical processor provides",
        "retrieval: given a query, find relevant passages. The language model provides",
        "generation: given context passages, produce a response. Integration coordinates",
        "these components to produce grounded, accurate answers.",
        "",
        "Passage retrieval configuration optimizes for RAG. Use find_passages_for_query",
        "with appropriate chunk_size and chunk_overlap. Chunk size balances context",
        "completeness against token limits. Overlap ensures content at boundaries is",
        "captured. Tune based on language model context window size.",
        "",
        "Query preprocessing prepares user questions. Natural language questions may",
        "need transformation for effective retrieval. Intent parsing identifies what",
        "the user seeks. Query rewriting converts questions to keyword-style queries.",
        "Both original and rewritten queries can be used for diverse retrieval.",
        "",
        "Passage selection determines what context to provide. Retrieve more passages",
        "than needed, then select the best subset. Selection criteria: relevance score,",
        "diversity of sources, recency if applicable, and fit within token budget.",
        "Over-retrieval followed by selection improves context quality.",
        "",
        "Context formatting structures passages for the language model. Include passage",
        "content, source document ID, and relevance indicators. Formatting affects how",
        "the model interprets and uses the context. Consistent formatting enables",
        "reliable citation and grounding.",
        "",
        "Prompt engineering integrates context effectively. Instruct the model to use",
        "retrieved passages, cite sources, and acknowledge when passages do not contain",
        "the answer. Prompt design affects response quality as much as retrieval quality.",
        "Test prompts systematically to optimize.",
        "",
        "Response grounding verification checks accuracy. Compare model responses against",
        "retrieved passages. Responses should be supported by passage content. Unsupported",
        "claims indicate hallucination. Implement verification checks in the pipeline",
        "to catch and flag ungrounded responses.",
        "",
        "Source citation enables verification. Include source document IDs in responses.",
        "Users can verify claims by consulting original documents. Citation improves",
        "trust and enables fact-checking. Design citation format for clarity and",
        "actionability.",
        "",
        "Fallback handling addresses retrieval failures. When no relevant passages exist,",
        "the pipeline needs a fallback strategy. Options: acknowledge the information",
        "gap, expand the search, or provide general responses without retrieval. Define",
        "fallback behavior explicitly rather than relying on implicit model behavior.",
        "",
        "Latency optimization keeps responses fast. Retrieval adds latency before",
        "generation begins. Pre-compute common queries, cache frequent retrievals,",
        "and optimize passage selection to minimize delay. Users expect responsive",
        "interactions; latency budgets constrain pipeline design.",
        "",
        "Quality monitoring tracks pipeline performance. Measure: retrieval relevance,",
        "response accuracy, user satisfaction, and latency. Establish baselines and",
        "alert thresholds. Continuous monitoring detects degradation before users",
        "notice significant quality drops.",
        "",
        "Feedback loop integration improves over time. Collect signals: which responses",
        "users accept, which they reject, what follow-up questions they ask. Use",
        "feedback to improve retrieval tuning, expand corpus content, and refine",
        "prompt engineering.",
        "",
        "Testing strategies validate pipeline correctness. Unit tests verify each",
        "component. Integration tests verify end-to-end flow. Evaluation tests measure",
        "quality on benchmark query sets. Regression tests catch quality degradation",
        "from changes. Comprehensive testing ensures reliable operation.",
        "",
        "Error handling provides graceful degradation. When retrieval fails, when the",
        "model times out, when responses are malformed: handle each failure mode",
        "explicitly. Users should receive helpful responses even when components fail.",
        "Design error handling as carefully as the happy path.",
        "",
        "Scalability planning anticipates load growth. RAG pipelines may need to handle",
        "many concurrent requests. Plan for horizontal scaling of retrieval, efficient",
        "use of model API rate limits, and caching strategies. Test at projected load",
        "before production deployment.",
        "",
        "Documentation captures pipeline design. Record: component configurations,",
        "integration points, prompt templates, error handling strategies, and",
        "operational procedures. Documentation enables team collaboration and",
        "facilitates troubleshooting during incidents.",
        ""
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "samples/search_quality_validation_process.txt",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "Search Quality Validation Process: Ensuring Reliable Retrieval Results",
        "",
        "Search quality validation verifies that retrieval operations return relevant,",
        "accurate results. This process establishes systematic approaches to measuring",
        "and maintaining search quality.",
        "",
        "Quality definition for search: relevant documents should rank highly, irrelevant",
        "documents should rank low or not appear. Quality metrics quantify how well",
        "actual rankings match ideal rankings.",
        "",
        "Test query development creates evaluation baselines. Collect representative",
        "queries from actual usage or domain experts. For each query, identify known",
        "relevant documents (ground truth). Test queries enable objective measurement.",
        "",
        "Precision measurement calculates result accuracy. Precision equals relevant",
        "results divided by total results. Precision at 5 (P@5) measures quality of",
        "top 5 results. High precision means few irrelevant results in the result set.",
        "",
        "Recall measurement calculates completeness. Recall equals found relevant",
        "documents divided by all relevant documents. High recall means few relevant",
        "documents are missed. Recall often trades off against precision.",
        "",
        "Mean reciprocal rank measures first-result quality. MRR averages 1/rank of",
        "the first relevant result across queries. High MRR means relevant documents",
        "appear near the top of results.",
        "",
        "Query expansion validation tests expansion effectiveness. Compare results",
        "with and without expansion. Good expansion improves recall without destroying",
        "precision. Poor expansion floods results with irrelevant documents.",
        "",
        "Parameter sensitivity analysis tests robustness. Vary parameters slightly",
        "and measure impact on quality. Robust configurations maintain quality across",
        "small parameter variations. Sensitive configurations may produce inconsistent",
        "results.",
        "",
        "Regression testing catches quality degradation. After code changes, corpus",
        "updates, or parameter adjustments, run the full test query suite. Quality",
        "should maintain or improve. Unexpected degradation requires investigation.",
        "",
        "A/B testing validates improvements. Before deploying changes, test on a",
        "subset of traffic. Compare quality metrics between control and treatment.",
        "Statistical significance ensures observed differences are real.",
        "",
        "User satisfaction correlation links metrics to outcomes. Do users prefer",
        "results with higher metrics? Validate that metric improvements translate",
        "to actual user satisfaction. Metrics that do not correlate with satisfaction",
        "are not useful targets.",
        "",
        "Edge case testing validates unusual queries. Test empty queries, very long",
        "queries, queries with special characters, and queries with no matches.",
        "Edge cases often reveal bugs that normal queries miss.",
        "",
        "Cross-validation prevents overfitting. If tuning parameters on test queries,",
        "use separate query sets for tuning and final evaluation. Overfitting to",
        "test queries produces artificially good metrics that do not generalize.",
        "",
        "Monitoring continuous quality tracks production performance. Test queries",
        "are samples; production queries are reality. Monitor actual query success",
        "rates, user behavior after searches, and explicit feedback.",
        "",
        "Failure analysis investigates quality problems. When quality drops, analyze",
        "which queries failed and why. Root cause analysis enables targeted fixes",
        "rather than guessing at solutions.",
        "",
        "Baseline comparison contextualizes metrics. Compare against previous versions,",
        "industry benchmarks, or alternative systems. Context helps interpret whether",
        "metrics are good, acceptable, or problematic.",
        "",
        "Documentation of quality standards sets expectations. Define acceptable",
        "precision, recall, and latency thresholds. Document why those thresholds",
        "were chosen. Standards provide clear targets for maintenance and improvement.",
        "",
        "Quality improvement workflow iterates on results. Measure, analyze, improve,",
        "remeasure. Continuous improvement maintains quality as requirements evolve",
        "and corpus changes over time.",
        ""
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "samples/semantic_relation_extraction_procedures.txt",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "Semantic Relation Extraction Procedures: Discovering Meaningful Term Relationships",
        "",
        "Semantic relation extraction identifies meaningful relationships between terms",
        "in the corpus: synonyms, hierarchies, causes, parts, and more. These procedures",
        "establish systematic approaches to extracting and utilizing semantic relations.",
        "",
        "Relation extraction fundamentals begin with pattern matching. The semantics module",
        "uses linguistic patterns to identify relationships. Patterns like \"X is a Y\"",
        "suggest is-a relations. \"X causes Y\" suggests causal relations. \"X contains Y\"",
        "suggests part-whole relations. Pattern-based extraction requires no training data.",
        "",
        "Corpus semantics extraction runs extract_corpus_semantics. This method scans",
        "documents for relation patterns and populates the semantic_relations list with",
        "discovered relationships. Each relation includes: term1, relation_type, term2,",
        "and confidence weight based on pattern match quality and frequency.",
        "",
        "Relation type coverage depends on corpus content. Different domains emphasize",
        "different relation types. Technical corpora may have many is-a relations (class",
        "hierarchies). Scientific corpora may have causal relations. Evaluate which relation",
        "types your corpus supports and expand patterns for missing important types.",
        "",
        "Confidence scoring ranks relation reliability. High-confidence relations appear",
        "in multiple documents with clear pattern matches. Low-confidence relations appear",
        "once or match ambiguous patterns. Filter by confidence when applications require",
        "reliable relations; include low-confidence relations for exploration.",
        "",
        "Validation procedures verify extracted relations. Sample relations at each",
        "confidence level and manually assess accuracy. Calculate precision: what fraction",
        "of extracted relations are actually correct? Track precision over time as corpus",
        "and patterns evolve.",
        "",
        "Relation type refinement improves extraction quality. Review extracted relations",
        "to identify false positives. Adjust patterns to reduce spurious matches. Add new",
        "patterns for commonly missed relations. Pattern tuning iteratively improves",
        "extraction quality.",
        "",
        "Semantic retrofitting incorporates relations into term representations. After",
        "extraction, retrofit embeddings to bring related terms closer in vector space.",
        "This improves query expansion: queries expand to semantically related terms",
        "rather than just co-occurring terms.",
        "",
        "Graph construction from relations enables reasoning. Build a directed graph",
        "where nodes are terms and edges are relations. Graph algorithms (path finding,",
        "centrality, clustering) reveal structure not visible in raw relations.",
        "Hierarchies emerge from is-a chains. Causal paths emerge from cause chains.",
        "",
        "Cross-document relation aggregation strengthens confidence. A relation appearing",
        "in five documents has higher confidence than one appearing once. Aggregate",
        "relations across documents and weight by document count. This surfaces consistent",
        "patterns while filtering noise.",
        "",
        "Relation visualization aids understanding. Export relations to graph visualization",
        "tools. Color edges by relation type. Size nodes by term frequency. Layout",
        "algorithms reveal clusters and hierarchies. Visual inspection identifies",
        "extraction problems and corpus structure.",
        "",
        "Integration with query expansion uses relations for smarter expansion. Rather",
        "than expanding only via co-occurrence, expand via semantic relations. A query",
        "for \"dog\" expands to \"canine\" via synonym relation. A query for \"car\" expands",
        "to \"vehicle\" via is-a relation. Relation-based expansion improves relevance.",
        "",
        "Relation-based search enables structured queries. Users can search for specific",
        "relationships: \"things that cause cancer\", \"parts of a cell\", \"types of machine",
        "learning\". Parse these queries to identify the requested relation type and",
        "filter results accordingly.",
        "",
        "Continuous extraction maintains freshness. As documents are added, new relations",
        "may emerge. Run extraction periodically or incrementally to capture evolving",
        "corpus semantics. Track relation additions over time to monitor corpus growth.",
        "",
        "Quality metrics evaluate extraction effectiveness. Track: total relations",
        "extracted, relations per relation type, average confidence, coverage of corpus",
        "terms. Metrics reveal extraction health and guide improvement efforts.",
        "",
        "Documentation captures extraction configuration. Record which patterns are used,",
        "confidence thresholds, and post-processing steps. Configuration documentation",
        "enables reproducibility and facilitates troubleshooting when extraction quality",
        "changes.",
        ""
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "samples/system_health_monitoring_workflow.txt",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "System Health Monitoring Workflow: Tracking Corpus and Computation Freshness",
        "",
        "Effective system health monitoring combines staleness tracking, performance",
        "profiling, and validation procedures to ensure reliable corpus operations.",
        "This workflow establishes comprehensive monitoring practices.",
        "",
        "Staleness monitoring forms the foundation of computation tracking. The processor",
        "maintains staleness flags for each computation type: TF-IDF, PageRank, bigram",
        "connections, concept clusters, and semantic relations. Monitoring these flags",
        "prevents using outdated values that could produce incorrect results.",
        "",
        "The staleness check workflow runs before critical operations. Before querying,",
        "check TF-IDF staleness. Before importance-based ranking, check PageRank staleness.",
        "Before concept-based analysis, check cluster staleness. Systematic checks ensure",
        "computations are fresh when needed.",
        "",
        "Automated staleness alerts notify operators when computations become stale.",
        "Configure thresholds: if TF-IDF has been stale for more than N documents,",
        "alert. If PageRank staleness exceeds M minutes, alert. Proactive alerting",
        "prevents silent degradation from accumulating staleness.",
        "",
        "Recompute scheduling balances freshness against performance. Full recompute",
        "via compute_all is expensive. Schedule recomputation during low-traffic",
        "periods. For real-time systems, incremental recompute maintains TF-IDF",
        "freshness while deferring expensive computations.",
        "",
        "Performance monitoring tracks processing and query latency. Establish baselines",
        "for indexing speed, query response time, and recompute duration. Alert when",
        "metrics exceed baseline thresholds. Degrading performance often indicates",
        "corpus growth outpacing system capacity.",
        "",
        "Bottleneck detection identifies performance constraints. Profile compute phases",
        "to identify which operations dominate processing time. Common bottlenecks",
        "include bigram connection creation, similarity computation, and concept",
        "clustering. Targeted optimization addresses actual bottlenecks.",
        "",
        "Resource monitoring tracks memory and CPU utilization. Large corpora consume",
        "significant memory. Complex queries consume CPU. Monitor resource usage",
        "patterns to anticipate capacity needs and prevent out-of-memory failures.",
        "",
        "Connection health validates graph structure. Count total connections, average",
        "connections per minicolumn, and isolated nodes. Healthy corpora show dense",
        "connection patterns. Sparse connections indicate indexing problems or",
        "inappropriate parameters.",
        "",
        "Coverage metrics track corpus completeness. The coverage score indicates how",
        "well documents interconnect. Monitor coverage over time. Declining coverage",
        "suggests new documents are not integrating well with existing content.",
        "",
        "Quality validation runs periodic checks. Sample queries with known-good results",
        "should continue producing expected output. Quality regression indicates",
        "problems with indexing, parameters, or corpus content changes.",
        "",
        "Logging captures operational history. Log document additions, computation runs,",
        "staleness transitions, and query patterns. Logs enable post-incident analysis",
        "and trend identification over time.",
        "",
        "Dashboard visualization presents monitoring data. Display staleness status,",
        "performance metrics, resource utilization, and quality indicators on a",
        "unified dashboard. Visual monitoring enables rapid problem identification.",
        "",
        "Incident response procedures handle monitoring alerts. Define escalation paths",
        "for different alert types. Document resolution procedures for common issues.",
        "Trained responders resolve incidents faster than ad-hoc troubleshooting.",
        "",
        "Maintenance windows allow scheduled operations. Heavy recomputation, corpus",
        "compaction, and backup procedures require dedicated time. Schedule maintenance",
        "to avoid impacting user-facing operations.",
        "",
        "Trend analysis identifies gradual changes. Compare metrics week-over-week",
        "and month-over-month. Gradual degradation is harder to notice than sudden",
        "failures. Trend analysis catches slow problems before they become critical.",
        ""
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "tests/test_layers.py",
      "function": "class TestTypedConnections(unittest.TestCase):",
      "start_line": 326,
      "lines_added": [
        "    def test_typed_connection_confidence_weighted_average(self):",
        "        \"\"\"Test that confidence uses weighted average (can increase or decrease).\"\"\"",
        "        # Weighted average: (0.7 * 0.5 + 0.9 * 0.3) / 0.8 = 0.775",
        "        self.assertAlmostEqual(edge.confidence, 0.775, places=5)",
        "",
        "    def test_typed_connection_confidence_can_decrease(self):",
        "        \"\"\"Test that confidence can decrease with lower-confidence evidence.\"\"\"",
        "        col = Minicolumn(\"L0_test\", \"test\", 0)",
        "        col.add_typed_connection(\"L0_other\", 1.0, confidence=0.9)  # High confidence",
        "        col.add_typed_connection(\"L0_other\", 1.0, confidence=0.3)  # Low confidence evidence",
        "",
        "        edge = col.typed_connections[\"L0_other\"]",
        "        # Weighted average: (0.9 * 1.0 + 0.3 * 1.0) / 2.0 = 0.6",
        "        self.assertAlmostEqual(edge.confidence, 0.6, places=5)"
      ],
      "lines_removed": [
        "    def test_typed_connection_confidence_max(self):",
        "        \"\"\"Test that confidence uses max value.\"\"\"",
        "        self.assertEqual(edge.confidence, 0.9)"
      ],
      "context_before": [
        "",
        "    def test_typed_connection_source_priority(self):",
        "        \"\"\"Test that semantic/inferred sources take priority over corpus.\"\"\"",
        "        col = Minicolumn(\"L0_test\", \"test\", 0)",
        "        col.add_typed_connection(\"L0_other\", 0.5, source='corpus')",
        "        col.add_typed_connection(\"L0_other\", 0.3, source='semantic')",
        "",
        "        edge = col.typed_connections[\"L0_other\"]",
        "        self.assertEqual(edge.source, 'semantic')",
        ""
      ],
      "context_after": [
        "        col = Minicolumn(\"L0_test\", \"test\", 0)",
        "        col.add_typed_connection(\"L0_other\", 0.5, confidence=0.7)",
        "        col.add_typed_connection(\"L0_other\", 0.3, confidence=0.9)",
        "",
        "        edge = col.typed_connections[\"L0_other\"]",
        "",
        "    def test_get_typed_connection(self):",
        "        \"\"\"Test retrieving a typed connection.\"\"\"",
        "        col = Minicolumn(\"L0_test\", \"test\", 0)",
        "        col.add_typed_connection(\"L0_other\", 0.5, relation_type='IsA')",
        "",
        "        edge = col.get_typed_connection(\"L0_other\")",
        "        self.assertIsNotNone(edge)",
        "        self.assertEqual(edge.relation_type, 'IsA')",
        ""
      ],
      "change_type": "modify"
    },
    {
      "file": "tests/test_query.py",
      "function": "class TestBoostDefinitionDocumentsTestFilePenalty(unittest.TestCase):",
      "start_line": 1869,
      "lines_added": [
        "            test_with_definition_penalty=0.5",
        "        \"\"\"Test that test_with_definition_penalty=1.0 disables the penalty.\"\"\"",
        "            test_with_definition_penalty=1.0  # No penalty",
        "        # Test file doesn't get full boost, it gets test_with_definition_penalty (1.0 here means no change)",
        "        # With test_with_definition_penalty=1.0, test file gets 10.0 * 1.0 = 10.0"
      ],
      "lines_removed": [
        "            test_file_boost_factor=0.5",
        "        \"\"\"Test that test_file_boost_factor=1.0 disables the penalty.\"\"\"",
        "            test_file_boost_factor=1.0  # No penalty",
        "        # Test file doesn't get full boost, it gets test_file_boost_factor (1.0 here means no change)",
        "        # Wait, if test_file_boost_factor=1.0, test file gets 10.0 * 1.0 = 10.0"
      ],
      "context_before": [
        "        documents = {",
        "            \"tests/test_analysis.py\": \"def compute_pagerank(layers, damping=0.85): pass  # mock\",",
        "            \"cortical/analysis.py\": \"def compute_pagerank(layers, damping=0.85):\\n    '''Real implementation'''\\n    result = do_stuff()\",",
        "        }",
        "",
        "        boosted = boost_definition_documents(",
        "            doc_results,",
        "            \"def compute_pagerank\",",
        "            documents,",
        "            boost_factor=2.0,"
      ],
      "context_after": [
        "        )",
        "",
        "        # Source file should be ranked first after boosting",
        "        self.assertEqual(boosted[0][0], \"cortical/analysis.py\")",
        "        # Source file gets 2.0x boost: 10.0 * 2.0 = 20.0",
        "        self.assertEqual(boosted[0][1], 20.0)",
        "        # Test file gets 0.5x penalty: 10.0 * 0.5 = 5.0",
        "        self.assertEqual(boosted[1][1], 5.0)",
        "",
        "    def test_test_file_penalty_can_be_disabled(self):",
        "        from cortical.query import boost_definition_documents",
        "",
        "        doc_results = [",
        "            (\"tests/test_module.py\", 10.0),",
        "            (\"src/module.py\", 10.0),",
        "        ]",
        "",
        "        documents = {",
        "            \"tests/test_module.py\": \"def my_func(): pass\",",
        "            \"src/module.py\": \"def my_func(): return 42\",",
        "        }",
        "",
        "        boosted = boost_definition_documents(",
        "            doc_results,",
        "            \"def my_func\",",
        "            documents,",
        "            boost_factor=2.0,",
        "        )",
        "",
        "        # Both should get the same boost when penalty is disabled",
        "        scores = {doc_id: score for doc_id, score in boosted}",
        "        # Source file gets 10.0 * 2.0 = 20.0",
        "        self.assertEqual(scores[\"src/module.py\"], 20.0)",
        "        self.assertEqual(scores[\"tests/test_module.py\"], 10.0)",
        "",
        "    def test_non_definition_query_unchanged(self):",
        "        \"\"\"Test that non-definition queries are not affected.\"\"\"",
        "        from cortical.query import boost_definition_documents",
        "",
        "        doc_results = [",
        "            (\"tests/test_query.py\", 10.0),"
      ],
      "change_type": "modify"
    },
    {
      "file": "tests/test_query.py",
      "function": "class TestBoostDefinitionDocumentsTestFilePenalty(unittest.TestCase):",
      "start_line": 1955,
      "lines_added": [
        "            test_with_definition_penalty=0.5,",
        "            test_without_definition_penalty=0.7"
      ],
      "lines_removed": [
        "            test_file_boost_factor=0.5,",
        "            test_file_penalty=0.7"
      ],
      "context_before": [
        "        documents = {",
        "            \"tests/test_processor.py\": \"from analysis import compute_pagerank; result = compute_pagerank()\",",
        "            \"cortical/analysis.py\": \"def compute_pagerank(layers, damping=0.85):\\n    return pagerank_impl()\",",
        "        }",
        "",
        "        boosted = boost_definition_documents(",
        "            doc_results,",
        "            \"def compute_pagerank\",",
        "            documents,",
        "            boost_factor=2.0,"
      ],
      "context_after": [
        "        )",
        "",
        "        # Source file with definition should now rank first",
        "        # Source file gets 2.0x: 80.0 * 2.0 = 160.0",
        "        # Test file without definition gets 0.7x penalty: 100.0 * 0.7 = 70.0",
        "        self.assertEqual(boosted[0][0], \"cortical/analysis.py\")",
        "        self.assertEqual(boosted[0][1], 160.0)",
        "        self.assertEqual(boosted[1][1], 70.0)"
      ],
      "change_type": "modify"
    }
  ],
  "hour_of_day": 11,
  "day_of_week": "Friday",
  "seconds_since_last_commit": -269013,
  "is_merge": true,
  "is_initial": false,
  "parent_count": 2,
  "session_id": null,
  "related_chats": [],
  "ci_result": null,
  "reverted": false,
  "amended": false
}