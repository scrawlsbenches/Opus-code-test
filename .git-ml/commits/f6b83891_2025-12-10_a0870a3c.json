{
  "hash": "f6b83891f80d3b263b36dbf49a158cf437711e18",
  "message": "Add relation-weighted PageRank (Task 22)",
  "author": "Claude",
  "timestamp": "2025-12-10 00:00:28 +0000",
  "branch": "claude/multi-index-design-DvifZ",
  "files_changed": [
    "TASK_LIST.md",
    "cortical/analysis.py",
    "cortical/processor.py",
    "tests/test_processor.py"
  ],
  "insertions": 437,
  "deletions": 44,
  "hunks": [
    {
      "file": "TASK_LIST.md",
      "function": "print(f\"  Component: {stats['component_connections']}\")",
      "start_line": 661,
      "lines_added": [
        "**Files:** `cortical/analysis.py`, `cortical/processor.py`",
        "**Status:** [x] Completed",
        "**Solution Applied:**",
        "1. Added `RELATION_WEIGHTS` constant to `analysis.py` with default weights:",
        "   - IsA: 1.5, PartOf: 1.3, HasProperty: 1.2, SimilarTo: 1.4, RelatedTo: 1.0",
        "   - Causes: 1.1, UsedFor: 1.0, CoOccurs: 0.8, Antonym: 0.3, DerivedFrom: 1.2",
        "2. Created `compute_semantic_pagerank()` function (~120 lines):",
        "   - Builds semantic relation lookup from (term1, term2) pairs",
        "   - Applies relation-type multipliers to edge weights",
        "   - Returns stats: pagerank scores, iterations_run, edges_with_relations",
        "3. Added `compute_semantic_importance()` method to processor:",
        "   - Falls back to standard PageRank if no semantic relations",
        "   - Applies semantic PageRank to both token and bigram layers",
        "   - Returns comprehensive statistics",
        "4. Updated `compute_all()` with `pagerank_method` parameter:",
        "   - 'standard': Traditional PageRank (default)",
        "   - 'semantic': ConceptNet-style with relation weighting",
        "   - Automatically extracts semantic relations if needed",
        "**Files Modified:**",
        "- `cortical/analysis.py` - Added `RELATION_WEIGHTS` and `compute_semantic_pagerank()` (~130 lines)",
        "- `cortical/processor.py` - Added `compute_semantic_importance()`, updated `compute_all()`",
        "- `tests/test_processor.py` - Added 9 tests for semantic PageRank",
        "",
        "**Usage:**",
        "# Use semantic PageRank via compute_all",
        "processor.compute_all(pagerank_method='semantic')",
        "# Or call directly",
        "processor.extract_corpus_semantics()",
        "stats = processor.compute_semantic_importance()",
        "print(f\"Found {stats['total_edges_with_relations']} semantic edges\")",
        "",
        "# Custom relation weights",
        "custom_weights = {'IsA': 2.0, 'CoOccurs': 0.5}",
        "processor.compute_semantic_importance(relation_weights=custom_weights)",
        "```"
      ],
      "lines_removed": [
        "**Files:** `cortical/analysis.py`",
        "**Status:** [ ] Pending",
        "**Current PageRank:**",
        "```python",
        "for target_id, weight in col.lateral_connections.items():",
        "    # All weights treated the same",
        "```",
        "**Enhanced PageRank:**",
        "RELATION_WEIGHTS = {",
        "    'IsA': 1.5,        # Hypernym relationships are strong",
        "    'PartOf': 1.3,     # Meronym relationships",
        "    'HasProperty': 1.2,",
        "    'RelatedTo': 1.0,  # Default co-occurrence",
        "    'Antonym': 0.5,    # Opposing concepts",
        "}",
        "```",
        "**Implementation Steps:**",
        "1. Add `relation_type: str` field to connection edges (or use separate dict)",
        "2. Create `compute_semantic_pagerank()` function",
        "3. Apply relation-type multipliers during propagation",
        "4. Use semantic relations from `extract_corpus_semantics()` to label edges",
        "5. Add `pagerank_method` parameter to `compute_all()`: 'standard' | 'semantic'"
      ],
      "context_before": [
        "print(f\"  Chain: {stats['chain_connections']}\")",
        "print(f\"  Co-occurrence: {stats['cooccurrence_connections']}\")",
        "```",
        "",
        "---",
        "",
        "## ConceptNet High Priority",
        "",
        "### 22. Implement Relation-Weighted PageRank",
        ""
      ],
      "context_after": [
        "",
        "**Problem:**",
        "Current PageRank treats all `lateral_connections` equally. ConceptNet-style PageRank should weight edges by semantic relation type.",
        "",
        "",
        "```python",
        "",
        "",
        "---",
        "",
        "### 23. Implement Cross-Layer PageRank Propagation",
        "",
        "**Files:** `cortical/analysis.py`",
        "**Status:** [ ] Pending",
        "",
        "**Problem:**",
        "PageRank only flows within a single layer. Importance should propagate across layers:"
      ],
      "change_type": "modify"
    },
    {
      "file": "TASK_LIST.md",
      "function": "Semantic bonus is capped at 50% boost (`min(avg_semantic, 0.5)`). This is a reas",
      "start_line": 964,
      "lines_added": [
        "| **High** | **Implement relation-weighted PageRank** | ✅ Completed | **ConceptNet** |",
        "**ConceptNet Enhancement Completion:** 4/12 tasks (33%)",
        "Ran 213 tests in 0.177s"
      ],
      "lines_removed": [
        "| **High** | **Implement relation-weighted PageRank** | ⏳ Pending | **ConceptNet** |",
        "**ConceptNet Enhancement Completion:** 3/12 tasks (25%)",
        "Ran 204 tests in 0.219s"
      ],
      "context_before": [
        "| **High** | **Persist full computed state** | ✅ Completed | **RAG** |",
        "| Medium | Fix type annotation (embeddings.py) | ✅ Completed | Bug Fix |",
        "| Medium | Optimize spectral embeddings | ✅ Completed | Performance |",
        "| Medium | Add incremental indexing | ✅ Completed | RAG |",
        "| Low | Document magic numbers | ⏳ Deferred | Documentation |",
        "| Low | Multi-stage ranking pipeline | ✅ Completed | RAG |",
        "| Low | Batch query API | ✅ Completed | RAG |",
        "| **Critical** | **Build cross-layer feedforward connections** | ✅ Completed | **ConceptNet** |",
        "| **Critical** | **Add concept-level lateral connections** | ✅ Completed | **ConceptNet** |",
        "| **Critical** | **Add bigram lateral connections** | ✅ Completed | **ConceptNet** |"
      ],
      "context_after": [
        "| **High** | **Implement cross-layer PageRank propagation** | ⏳ Pending | **ConceptNet** |",
        "| **High** | **Add typed edge storage** | ⏳ Pending | **ConceptNet** |",
        "| Medium | Implement multi-hop semantic inference | ⏳ Pending | ConceptNet |",
        "| Medium | Add relation path scoring | ⏳ Pending | ConceptNet |",
        "| Medium | Implement concept inheritance | ⏳ Pending | ConceptNet |",
        "| Low | Add commonsense relation extraction | ⏳ Pending | ConceptNet |",
        "| Low | Visualize ConceptNet-style graph | ⏳ Pending | ConceptNet |",
        "| Low | Add analogy completion | ⏳ Pending | ConceptNet |",
        "",
        "**Bug Fix Completion:** 7/7 tasks (100%)",
        "**RAG Enhancement Completion:** 8/8 tasks (100%)",
        "",
        "---",
        "",
        "## Test Results",
        "",
        "```",
        "OK",
        "```",
        "",
        "All tests passing as of 2025-12-09.",
        "",
        "---",
        "",
        "*Updated from code review on 2025-12-09*"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/analysis.py",
      "function": "from .minicolumn import Minicolumn",
      "start_line": 20,
      "lines_added": [
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "# Default relation weights for semantic PageRank",
        "RELATION_WEIGHTS = {",
        "    'IsA': 1.5,           # Hypernym relationships are strong",
        "    'PartOf': 1.3,        # Meronym relationships",
        "    'HasProperty': 1.2,   # Property associations",
        "    'RelatedTo': 1.0,     # Default co-occurrence",
        "    'SimilarTo': 1.4,     # Similarity relationships",
        "    'Causes': 1.1,        # Causal relationships",
        "    'UsedFor': 1.0,       # Functional relationships",
        "    'CoOccurs': 0.8,      # Basic co-occurrence",
        "    'Antonym': 0.3,       # Opposing concepts (lower weight)",
        "    'DerivedFrom': 1.2,   # Morphological derivation",
        "}",
        "",
        "",
        "def compute_semantic_pagerank(",
        "    layer: HierarchicalLayer,",
        "    semantic_relations: List[Tuple[str, str, str, float]],",
        "    relation_weights: Optional[Dict[str, float]] = None,",
        "    damping: float = 0.85,",
        "    iterations: int = 20,",
        "    tolerance: float = 1e-6",
        ") -> Dict[str, Any]:",
        "    \"\"\"",
        "    Compute PageRank with semantic relation type weighting.",
        "",
        "    This ConceptNet-style PageRank applies different multipliers based on",
        "    the semantic relation type between nodes. For example, IsA relationships",
        "    are weighted more heavily than simple co-occurrence.",
        "",
        "    Args:",
        "        layer: The layer to compute PageRank for",
        "        semantic_relations: List of (term1, relation, term2, weight) tuples",
        "        relation_weights: Optional custom relation weights dict. If None, uses defaults.",
        "        damping: Damping factor (probability of following links)",
        "        iterations: Maximum number of iterations",
        "        tolerance: Convergence threshold",
        "",
        "    Returns:",
        "        Dict containing:",
        "        - pagerank: Dict mapping column IDs to PageRank scores",
        "        - iterations_run: Number of iterations until convergence",
        "        - edges_with_relations: Number of edges that had semantic relation info",
        "",
        "    Example:",
        "        >>> relations = [(\"neural\", \"RelatedTo\", \"networks\", 0.8)]",
        "        >>> result = compute_semantic_pagerank(layer, relations)",
        "        >>> print(f\"PageRank converged in {result['iterations_run']} iterations\")",
        "    \"\"\"",
        "    n = len(layer.minicolumns)",
        "    if n == 0:",
        "        return {'pagerank': {}, 'iterations_run': 0, 'edges_with_relations': 0}",
        "",
        "    # Use default weights if not provided",
        "    weights = relation_weights or RELATION_WEIGHTS",
        "",
        "    # Build semantic relation lookup: (term1, term2) -> (relation_type, weight)",
        "    semantic_lookup: Dict[Tuple[str, str], Tuple[str, float]] = {}",
        "    for t1, relation, t2, rel_weight in semantic_relations:",
        "        # Store in both directions for undirected lookup",
        "        semantic_lookup[(t1, t2)] = (relation, rel_weight)",
        "        semantic_lookup[(t2, t1)] = (relation, rel_weight)",
        "",
        "    # Initialize PageRank uniformly",
        "    pagerank = {col.id: 1.0 / n for col in layer.minicolumns.values()}",
        "",
        "    # Build incoming links map with relation-weighted edges",
        "    incoming: Dict[str, List[Tuple[str, float]]] = defaultdict(list)",
        "    outgoing_sum: Dict[str, float] = defaultdict(float)",
        "    edges_with_relations = 0",
        "",
        "    # Build content -> id mapping for semantic lookup",
        "    content_to_id: Dict[str, str] = {}",
        "    for col in layer.minicolumns.values():",
        "        content_to_id[col.content] = col.id",
        "",
        "    for col in layer.minicolumns.values():",
        "        for target_id, base_weight in col.lateral_connections.items():",
        "            target = layer.get_by_id(target_id)",
        "            if target is None:",
        "                continue",
        "",
        "            # Check if there's a semantic relation between these terms",
        "            lookup_key = (col.content, target.content)",
        "            if lookup_key in semantic_lookup:",
        "                relation_type, rel_weight = semantic_lookup[lookup_key]",
        "                # Apply relation type multiplier",
        "                type_multiplier = weights.get(relation_type, 1.0)",
        "                # Combined weight: base_weight * relation_weight * type_multiplier",
        "                adjusted_weight = base_weight * rel_weight * type_multiplier",
        "                edges_with_relations += 1",
        "            else:",
        "                # No semantic relation, use base weight",
        "                adjusted_weight = base_weight",
        "",
        "            incoming[target_id].append((col.id, adjusted_weight))",
        "            outgoing_sum[col.id] += adjusted_weight",
        "",
        "    # Iterate until convergence",
        "    iterations_run = 0",
        "    for iteration in range(iterations):",
        "        iterations_run = iteration + 1",
        "        new_pagerank = {}",
        "        max_diff = 0.0",
        "",
        "        for col in layer.minicolumns.values():",
        "            # Sum of weighted incoming PageRank",
        "            incoming_sum = 0.0",
        "            for source_id, weight in incoming[col.id]:",
        "                if source_id in pagerank and outgoing_sum[source_id] > 0:",
        "                    incoming_sum += pagerank[source_id] * weight / outgoing_sum[source_id]",
        "",
        "            # Apply damping",
        "            new_rank = (1 - damping) / n + damping * incoming_sum",
        "            new_pagerank[col.id] = new_rank",
        "",
        "            max_diff = max(max_diff, abs(new_rank - pagerank.get(col.id, 0)))",
        "",
        "        pagerank = new_pagerank",
        "",
        "        if max_diff < tolerance:",
        "            break",
        "",
        "    # Update minicolumn pagerank values",
        "    for col in layer.minicolumns.values():",
        "        col.pagerank = pagerank.get(col.id, 1.0 / n)",
        "",
        "    return {",
        "        'pagerank': pagerank,",
        "        'iterations_run': iterations_run,",
        "        'edges_with_relations': edges_with_relations",
        "    }",
        "",
        ""
      ],
      "lines_removed": [
        "    ",
        "    ",
        "        ",
        "    ",
        "    ",
        "    ",
        "    ",
        "        ",
        "            ",
        "            ",
        "        ",
        "        ",
        "    ",
        "    "
      ],
      "context_before": [
        "",
        "",
        "def compute_pagerank(",
        "    layer: HierarchicalLayer,",
        "    damping: float = 0.85,",
        "    iterations: int = 20,",
        "    tolerance: float = 1e-6",
        ") -> Dict[str, float]:",
        "    \"\"\"",
        "    Compute PageRank scores for minicolumns in a layer."
      ],
      "context_after": [
        "    PageRank measures importance based on connection structure.",
        "    Highly connected columns that are connected to other important",
        "    columns receive higher scores.",
        "    Args:",
        "        layer: The layer to compute PageRank for",
        "        damping: Damping factor (probability of following links)",
        "        iterations: Maximum number of iterations",
        "        tolerance: Convergence threshold",
        "    Returns:",
        "        Dictionary mapping column IDs to PageRank scores",
        "    \"\"\"",
        "    n = len(layer.minicolumns)",
        "    if n == 0:",
        "        return {}",
        "    # Initialize PageRank uniformly",
        "    pagerank = {col.id: 1.0 / n for col in layer.minicolumns.values()}",
        "    # Build incoming links map",
        "    incoming: Dict[str, List[Tuple[str, float]]] = defaultdict(list)",
        "    outgoing_sum: Dict[str, float] = defaultdict(float)",
        "    for col in layer.minicolumns.values():",
        "        for target_id, weight in col.lateral_connections.items():",
        "            # Use O(1) lookup via get_by_id instead of O(n) linear search",
        "            if layer.get_by_id(target_id) is not None:",
        "                incoming[target_id].append((col.id, weight))",
        "                outgoing_sum[col.id] += weight",
        "    # Iterate until convergence",
        "    for iteration in range(iterations):",
        "        new_pagerank = {}",
        "        max_diff = 0.0",
        "        for col in layer.minicolumns.values():",
        "            # Sum of weighted incoming PageRank",
        "            incoming_sum = 0.0",
        "            for source_id, weight in incoming[col.id]:",
        "                if source_id in pagerank and outgoing_sum[source_id] > 0:",
        "                    incoming_sum += pagerank[source_id] * weight / outgoing_sum[source_id]",
        "            # Apply damping",
        "            new_rank = (1 - damping) / n + damping * incoming_sum",
        "            new_pagerank[col.id] = new_rank",
        "            max_diff = max(max_diff, abs(new_rank - pagerank.get(col.id, 0)))",
        "        pagerank = new_pagerank",
        "        if max_diff < tolerance:",
        "            break",
        "    # Update minicolumn pagerank values",
        "    for col in layer.minicolumns.values():",
        "        col.pagerank = pagerank.get(col.id, 1.0 / n)",
        "    return pagerank",
        "",
        "",
        "def compute_tfidf(",
        "    layers: Dict[CorticalLayer, HierarchicalLayer],",
        "    documents: Dict[str, str]",
        ") -> None:",
        "    \"\"\"",
        "    Compute TF-IDF scores for tokens.",
        "    ",
        "    TF-IDF (Term Frequency - Inverse Document Frequency) measures",
        "    how distinctive a term is to the corpus. High TF-IDF terms are",
        "    both frequent in their documents and rare across the corpus."
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/processor.py",
      "function": "class CorticalTextProcessor:",
      "start_line": 402,
      "lines_added": [
        "    def compute_all(",
        "        self,",
        "        verbose: bool = True,",
        "        build_concepts: bool = True,",
        "        pagerank_method: str = 'standard'",
        "    ) -> None:",
        "            pagerank_method: PageRank algorithm to use:",
        "                - 'standard': Traditional PageRank using connection weights",
        "                - 'semantic': ConceptNet-style PageRank with relation type weighting.",
        "                              Requires semantic relations (extracts automatically if needed).",
        "",
        "        if pagerank_method == 'semantic':",
        "            # Extract semantic relations if not already done",
        "            if not self.semantic_relations:",
        "                if verbose:",
        "                    print(\"Extracting semantic relations...\")",
        "                self.extract_corpus_semantics(verbose=False)",
        "            if verbose:",
        "                print(\"Computing importance (Semantic PageRank)...\")",
        "            self.compute_semantic_importance(verbose=False)",
        "        else:",
        "            if verbose:",
        "                print(\"Computing importance (PageRank)...\")",
        "            self.compute_importance(verbose=False)"
      ],
      "lines_removed": [
        "    def compute_all(self, verbose: bool = True, build_concepts: bool = True) -> None:",
        "        if verbose:",
        "            print(\"Computing importance (PageRank)...\")",
        "        self.compute_importance(verbose=False)"
      ],
      "context_before": [
        "                self._mark_fresh(self.COMP_EMBEDDINGS)",
        "                recomputed[self.COMP_EMBEDDINGS] = True",
        "",
        "            if self.COMP_SEMANTICS in self._stale_computations:",
        "                self.extract_corpus_semantics(verbose=verbose)",
        "                self._mark_fresh(self.COMP_SEMANTICS)",
        "                recomputed[self.COMP_SEMANTICS] = True",
        "",
        "        return recomputed",
        ""
      ],
      "context_after": [
        "        \"\"\"",
        "        Run all computation steps.",
        "",
        "        Args:",
        "            verbose: Print progress messages",
        "            build_concepts: Build concept clusters in Layer 2 (default True)",
        "                           This enables topic-based filtering and hierarchical search.",
        "        \"\"\"",
        "        if verbose:",
        "            print(\"Computing activation propagation...\")",
        "        self.propagate_activation(verbose=False)",
        "        if verbose:",
        "            print(\"Computing TF-IDF...\")",
        "        self.compute_tfidf(verbose=False)",
        "        if verbose:",
        "            print(\"Computing document connections...\")",
        "        self.compute_document_connections(verbose=False)",
        "        if verbose:",
        "            print(\"Computing bigram connections...\")",
        "        self.compute_bigram_connections(verbose=False)",
        "        if build_concepts:"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/processor.py",
      "function": "class CorticalTextProcessor:",
      "start_line": 455,
      "lines_added": [
        "",
        "    def compute_semantic_importance(",
        "        self,",
        "        relation_weights: Optional[Dict[str, float]] = None,",
        "        verbose: bool = True",
        "    ) -> Dict[str, Any]:",
        "        \"\"\"",
        "        Compute PageRank with semantic relation weighting.",
        "",
        "        Uses semantic relations to weight edges in the PageRank graph.",
        "        Edges with stronger semantic relationships (e.g., IsA, PartOf) receive",
        "        higher weights, affecting importance propagation.",
        "",
        "        Args:",
        "            relation_weights: Optional custom relation type weights dict.",
        "                Defaults to built-in weights (IsA: 1.5, PartOf: 1.3, etc.)",
        "            verbose: Print progress messages",
        "",
        "        Returns:",
        "            Dict with statistics:",
        "            - total_edges_with_relations: Sum across layers",
        "            - token_layer: Stats for token layer",
        "            - bigram_layer: Stats for bigram layer",
        "",
        "        Example:",
        "            >>> # Use default relation weights",
        "            >>> stats = processor.compute_semantic_importance()",
        "            >>> print(f\"Found {stats['total_edges_with_relations']} semantic edges\")",
        "            >>>",
        "            >>> # Custom weights",
        "            >>> weights = {'IsA': 2.0, 'RelatedTo': 0.5}",
        "            >>> processor.compute_semantic_importance(relation_weights=weights)",
        "        \"\"\"",
        "        if not self.semantic_relations:",
        "            # Fall back to standard PageRank if no semantic relations",
        "            self.compute_importance(verbose=verbose)",
        "            return {",
        "                'total_edges_with_relations': 0,",
        "                'token_layer': {'edges_with_relations': 0},",
        "                'bigram_layer': {'edges_with_relations': 0}",
        "            }",
        "",
        "        total_edges = 0",
        "        layer_stats = {}",
        "",
        "        for layer_enum in [CorticalLayer.TOKENS, CorticalLayer.BIGRAMS]:",
        "            result = analysis.compute_semantic_pagerank(",
        "                self.layers[layer_enum],",
        "                self.semantic_relations,",
        "                relation_weights=relation_weights",
        "            )",
        "            layer_name = 'token_layer' if layer_enum == CorticalLayer.TOKENS else 'bigram_layer'",
        "            layer_stats[layer_name] = {",
        "                'iterations_run': result['iterations_run'],",
        "                'edges_with_relations': result['edges_with_relations']",
        "            }",
        "            total_edges += result['edges_with_relations']",
        "",
        "        if verbose:",
        "            print(f\"Computed semantic PageRank ({total_edges} relation-weighted edges)\")",
        "",
        "        return {",
        "            'total_edges_with_relations': total_edges,",
        "            **layer_stats",
        "        }",
        ""
      ],
      "lines_removed": [
        "    "
      ],
      "context_before": [
        "            print(\"Done.\")",
        "    ",
        "    def propagate_activation(self, iterations: int = 3, decay: float = 0.8, verbose: bool = True) -> None:",
        "        analysis.propagate_activation(self.layers, iterations, decay)",
        "        if verbose: print(f\"Propagated activation ({iterations} iterations)\")",
        "    ",
        "    def compute_importance(self, verbose: bool = True) -> None:",
        "        for layer_enum in [CorticalLayer.TOKENS, CorticalLayer.BIGRAMS]:",
        "            analysis.compute_pagerank(self.layers[layer_enum])",
        "        if verbose: print(\"Computed PageRank importance\")"
      ],
      "context_after": [
        "    def compute_tfidf(self, verbose: bool = True) -> None:",
        "        analysis.compute_tfidf(self.layers, self.documents)",
        "        if verbose: print(\"Computed TF-IDF scores\")",
        "    ",
        "    def compute_document_connections(self, min_shared_terms: int = 3, verbose: bool = True) -> None:",
        "        analysis.compute_document_connections(self.layers, self.documents, min_shared_terms)",
        "        if verbose: print(\"Computed document connections\")",
        "",
        "    def compute_bigram_connections(",
        "        self,"
      ],
      "change_type": "modify"
    },
    {
      "file": "tests/test_processor.py",
      "function": "class TestBigramConnections(unittest.TestCase):",
      "start_line": 1327,
      "lines_added": [
        "class TestSemanticPageRank(unittest.TestCase):",
        "    \"\"\"Test semantic PageRank functionality.\"\"\"",
        "",
        "    @classmethod",
        "    def setUpClass(cls):",
        "        \"\"\"Set up processor with documents for semantic PageRank testing.\"\"\"",
        "        cls.processor = CorticalTextProcessor()",
        "        cls.processor.process_document(",
        "            \"doc1\",",
        "            \"Neural networks are a type of machine learning model. \"",
        "            \"Deep learning uses neural networks for complex tasks.\"",
        "        )",
        "        cls.processor.process_document(",
        "            \"doc2\",",
        "            \"Machine learning algorithms process data patterns. \"",
        "            \"Neural networks learn from examples.\"",
        "        )",
        "        cls.processor.process_document(",
        "            \"doc3\",",
        "            \"Deep learning is part of artificial intelligence. \"",
        "            \"Machine learning models improve with data.\"",
        "        )",
        "        # Extract semantic relations first",
        "        cls.processor.extract_corpus_semantics(verbose=False)",
        "",
        "    def test_compute_semantic_importance_returns_stats(self):",
        "        \"\"\"Test that compute_semantic_importance returns expected statistics.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(\"doc1\", \"Neural networks process data efficiently.\")",
        "        processor.extract_corpus_semantics(verbose=False)",
        "",
        "        stats = processor.compute_semantic_importance(verbose=False)",
        "",
        "        self.assertIn('total_edges_with_relations', stats)",
        "        self.assertIn('token_layer', stats)",
        "        self.assertIn('bigram_layer', stats)",
        "",
        "    def test_semantic_pagerank_with_relations(self):",
        "        \"\"\"Test that semantic PageRank uses relation weights.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(",
        "            \"doc1\",",
        "            \"Neural networks learn patterns. Neural systems process data.\"",
        "        )",
        "        processor.extract_corpus_semantics(verbose=False)",
        "",
        "        # Get initial PageRank with standard method",
        "        processor.compute_importance(verbose=False)",
        "        layer0 = processor.get_layer(CorticalLayer.TOKENS)",
        "        standard_pr = {col.content: col.pagerank for col in layer0.minicolumns.values()}",
        "",
        "        # Now compute with semantic method",
        "        stats = processor.compute_semantic_importance(verbose=False)",
        "",
        "        # PageRank values should be updated",
        "        semantic_pr = {col.content: col.pagerank for col in layer0.minicolumns.values()}",
        "",
        "        # Just verify it ran and produced valid PageRank values",
        "        for content, pr in semantic_pr.items():",
        "            self.assertGreater(pr, 0)",
        "",
        "    def test_semantic_pagerank_no_relations(self):",
        "        \"\"\"Test semantic PageRank falls back when no relations exist.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(\"doc1\", \"Hello world.\")",
        "        # Don't extract semantic relations",
        "",
        "        stats = processor.compute_semantic_importance(verbose=False)",
        "",
        "        self.assertEqual(stats['total_edges_with_relations'], 0)",
        "",
        "    def test_compute_all_with_semantic_pagerank(self):",
        "        \"\"\"Test compute_all with pagerank_method='semantic'.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(",
        "            \"doc1\",",
        "            \"Neural networks process information efficiently.\"",
        "        )",
        "",
        "        # Should work without errors",
        "        processor.compute_all(verbose=False, pagerank_method='semantic')",
        "",
        "        # Verify computations ran",
        "        self.assertFalse(processor.is_stale(processor.COMP_PAGERANK))",
        "        self.assertFalse(processor.is_stale(processor.COMP_TFIDF))",
        "",
        "    def test_compute_all_standard_pagerank(self):",
        "        \"\"\"Test compute_all with default pagerank_method='standard'.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(",
        "            \"doc1\",",
        "            \"Neural networks process information efficiently.\"",
        "        )",
        "",
        "        processor.compute_all(verbose=False, pagerank_method='standard')",
        "",
        "        self.assertFalse(processor.is_stale(processor.COMP_PAGERANK))",
        "",
        "    def test_custom_relation_weights(self):",
        "        \"\"\"Test semantic PageRank with custom relation weights.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(",
        "            \"doc1\",",
        "            \"Neural networks learn patterns. Machine learning improves.\"",
        "        )",
        "        processor.extract_corpus_semantics(verbose=False)",
        "",
        "        # Use custom weights",
        "        custom_weights = {",
        "            'CoOccurs': 2.0,  # Boost co-occurrence",
        "            'RelatedTo': 0.1,  # Reduce related",
        "        }",
        "",
        "        stats = processor.compute_semantic_importance(",
        "            relation_weights=custom_weights,",
        "            verbose=False",
        "        )",
        "",
        "        # Should run without errors",
        "        self.assertIsNotNone(stats)",
        "",
        "    def test_semantic_pagerank_empty_layer(self):",
        "        \"\"\"Test semantic PageRank handles empty layer gracefully.\"\"\"",
        "        from cortical.analysis import compute_semantic_pagerank",
        "        from cortical.layers import HierarchicalLayer, CorticalLayer",
        "",
        "        empty_layer = HierarchicalLayer(CorticalLayer.TOKENS)",
        "        relations = [(\"test\", \"RelatedTo\", \"example\", 0.5)]",
        "",
        "        result = compute_semantic_pagerank(empty_layer, relations)",
        "",
        "        self.assertEqual(result['pagerank'], {})",
        "        self.assertEqual(result['iterations_run'], 0)",
        "        self.assertEqual(result['edges_with_relations'], 0)",
        "",
        "    def test_semantic_pagerank_convergence(self):",
        "        \"\"\"Test that semantic PageRank converges.\"\"\"",
        "        from cortical.analysis import compute_semantic_pagerank",
        "",
        "        layer0 = self.processor.get_layer(CorticalLayer.TOKENS)",
        "",
        "        result = compute_semantic_pagerank(",
        "            layer0,",
        "            self.processor.semantic_relations,",
        "            iterations=100,",
        "            tolerance=1e-6",
        "        )",
        "",
        "        # Should converge in less than max iterations",
        "        self.assertLessEqual(result['iterations_run'], 100)",
        "",
        "    def test_relation_weights_applied(self):",
        "        \"\"\"Test that different relation types get different weights.\"\"\"",
        "        from cortical.analysis import RELATION_WEIGHTS",
        "",
        "        # Verify key relations have expected relative weights",
        "        self.assertGreater(RELATION_WEIGHTS['IsA'], RELATION_WEIGHTS['RelatedTo'])",
        "        self.assertGreater(RELATION_WEIGHTS['PartOf'], RELATION_WEIGHTS['CoOccurs'])",
        "        self.assertLess(RELATION_WEIGHTS['Antonym'], RELATION_WEIGHTS['RelatedTo'])",
        "",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "        layer1 = self.processor.get_layer(CorticalLayer.BIGRAMS)",
        "",
        "        # Find bigrams that could be connected by multiple reasons",
        "        # (shared component AND co-occurrence)",
        "        for bigram in layer1.minicolumns.values():",
        "            for target_id, weight in bigram.lateral_connections.items():",
        "                # Weights should be positive",
        "                self.assertGreater(weight, 0)",
        "",
        ""
      ],
      "context_after": [
        "if __name__ == \"__main__\":",
        "    unittest.main(verbosity=2)"
      ],
      "change_type": "add"
    }
  ],
  "hour_of_day": 0,
  "day_of_week": "Wednesday",
  "seconds_since_last_commit": -481460,
  "is_merge": false,
  "is_initial": false,
  "parent_count": 1,
  "session_id": null,
  "related_chats": [],
  "ci_result": null,
  "reverted": false,
  "amended": false
}