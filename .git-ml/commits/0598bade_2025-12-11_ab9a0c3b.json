{
  "hash": "0598bade86fa8d28daf11b06c353317b6cd82119",
  "message": "Implement Task #127: Create cluster coverage evaluation script",
  "author": "Claude",
  "timestamp": "2025-12-11 23:53:18 +0000",
  "branch": "claude/multi-index-design-DvifZ",
  "files_changed": [
    "TASK_LIST.md",
    "scripts/evaluate_cluster.py",
    "tests/test_evaluate_cluster.py"
  ],
  "insertions": 954,
  "deletions": 53,
  "hunks": [
    {
      "file": "TASK_LIST.md",
      "function": null,
      "start_line": 1,
      "lines_added": [
        "**Pending Tasks:** 35"
      ],
      "lines_removed": [
        "**Pending Tasks:** 36",
        "| 127 | Create cluster coverage evaluation script | DevEx | 125 | Medium |"
      ],
      "context_before": [
        "# Task List: Cortical Text Processor",
        "",
        "Active backlog for the Cortical Text Processor project. Completed tasks are archived in [TASK_ARCHIVE.md](TASK_ARCHIVE.md).",
        "",
        "**Last Updated:** 2025-12-11"
      ],
      "context_after": [
        "**Completed Tasks:** 90+ (see archive)",
        "",
        "---",
        "",
        "## Active Backlog",
        "",
        "<!-- Machine-parseable format for automation -->",
        "",
        "### ðŸ”´ Critical (Do Now)",
        "",
        "*All critical tasks completed!*",
        "",
        "### ðŸŸ  High (Do This Week)",
        "",
        "| # | Task | Category | Depends | Effort |",
        "|---|------|----------|---------|--------|",
        "| 94 | Split query.py into focused modules | Arch | - | Large |",
        "| 97 | Integrate CorticalConfig into processor | Arch | - | Medium |",
        "",
        "### ðŸŸ¡ Medium (Do This Month)",
        "",
        "| # | Task | Category | Depends | Effort |",
        "|---|------|----------|---------|--------|",
        "| 137 | Cap bigram connections to top-K per bigram | Perf | - | Small |",
        "| 138 | Use sparse matrix multiplication for bigram connections | Perf | - | Medium |",
        "| 139 | Batch bigram connection updates to reduce dict overhead | Perf | - | Small |",
        "| 133 | Implement WAL + snapshot persistence (fault-tolerant rebuild) | Arch | 132 | Large |",
        "| 134 | Implement protobuf serialization for corpus | Arch | 132 | Medium |"
      ],
      "change_type": "modify"
    },
    {
      "file": "TASK_LIST.md",
      "function": "Active backlog for the Cortical Text Processor project. Completed tasks are arch",
      "start_line": 87,
      "lines_added": [
        "| 127 | Create cluster coverage evaluation script | 2025-12-11 | scripts/evaluate_cluster.py with 24 tests |"
      ],
      "lines_removed": [],
      "context_before": [
        "| # | Task | Started | Notes |",
        "|---|------|---------|-------|",
        "| 87 | Add Python code samples and showcase | 2025-12-11 | samples/*.py created |",
        "",
        "---",
        "",
        "## Recently Completed (Last 7 Days)",
        "",
        "| # | Task | Completed | Notes |",
        "|---|------|-----------|-------|"
      ],
      "context_after": [
        "| 125 | Add clustering quality metrics (modularity, silhouette) | 2025-12-11 | compute_clustering_quality() in analysis.py, showcase display |",
        "| 124 | Add minimum cluster count regression tests | 2025-12-11 | 4 new tests: coherence, showcase count, mega-cluster, distribution |",
        "| 128 | Fix definition boost that favors test mocks over real implementations | 2025-12-11 | Added is_test_file() and test file penalty |",
        "| 132 | Profile full-analysis bottleneck (bigram, semantics O(nÂ²)) | 2025-12-11 | Created profile_full_analysis.py, fixed bottlenecks |",
        "| 136 | Optimize semantics O(nÂ²) similarity with early termination | 2025-12-11 | Added max_similarity_pairs, min_context_keys |",
        "| 126 | Investigate optimal Louvain resolution for sample corpus | 2025-12-11 | Research confirms default 1.0 is optimal |",
        "| 123 | Replace label propagation with Louvain community detection | 2025-12-11 | Implemented Louvain algorithm, 34 clusters for 92 docs |",
        "| 122 | Investigate Concept Layer & Embeddings regressions | 2025-12-11 | Fixed inverted strictness, improved embeddings |",
        "| 119 | Create AI metadata generator script | 2025-12-11 | scripts/generate_ai_metadata.py with tests |",
        "| 120 | Add AI metadata loader to Claude skills | 2025-12-11 | ai-metadata skill created |"
      ],
      "change_type": "add"
    },
    {
      "file": "TASK_LIST.md",
      "function": "ls cortical/*.ai_meta || python scripts/generate_ai_metadata.py",
      "start_line": 1120,
      "lines_added": [
        "### 127. Create Cluster Coverage Evaluation Script âœ…",
        "**Meta:** `status:completed` `priority:high` `category:devex`",
        "**Files:** `scripts/evaluate_cluster.py`, `tests/test_evaluate_cluster.py`",
        "**Completed:** 2025-12-11",
        "**Solution Applied:** Created `scripts/evaluate_cluster.py` with:",
        "**Usage:**",
        "```bash",
        "# Find and evaluate documents by topic search",
        "",
        "# Evaluate specific documents",
        "python scripts/evaluate_cluster.py --documents doc1,doc2,doc3",
        "",
        "# Find documents by keywords",
        "python scripts/evaluate_cluster.py --keywords customer,ticket,escalation --min-keywords 2",
        "",
        "# Show expansion suggestions",
        "python scripts/evaluate_cluster.py --topic \"machine learning\" --suggest --verbose",
        "**Features Implemented:**",
        "1. **Three cluster detection modes:**",
        "   - `--topic`: Semantic search for related documents",
        "   - `--documents`: Explicit document list",
        "   - `--keywords`: Documents containing specified terms",
        "",
        "   - Internal Cohesion: Weighted Jaccard similarity within cluster",
        "   - External Separation: 1 - similarity to outside documents",
        "   - Concept Coverage: Number of concepts covered",
        "   - Term Diversity: Unique terms / total occurrences",
        "   - Hub Document: Most centrally connected document",
        "",
        "3. **Coverage Assessment:**",
        "   - STRONG: High cohesion + separation + coverage",
        "   - ADEQUATE: Usable but could improve",
        "   - NEEDS EXPANSION: Low metrics",
        "",
        "4. **Expansion Suggestions:** Related terms not well-covered by cluster",
        "",
        "**Example Output:**",
        "Cluster Analysis: Keywords: customer, ticket, support (4 documents)",
        "===================================================================",
        "  * complaint_resolution",
        "  * customer_satisfaction_metrics",
        "  * customer_support_fundamentals (hub)",
        "  * ticket_escalation_procedures",
        "  Internal Cohesion:    0.08 (weak)",
        "  External Separation:  0.98 (good)",
        "  Concept Coverage:     35 concepts",
        "  Term Diversity:       0.80",
        "",
        "Coverage Assessment: ADEQUATE [~]",
        "  Cluster is usable but could improve: weak internal connectivity.",
        "**Test Results:**",
        "- 24 new tests in tests/test_evaluate_cluster.py",
        "- 1025 total tests pass",
        "",
        "- [x] Script identifies document clusters by topic/keywords",
        "- [x] Computes cohesion and separation metrics",
        "- [x] Provides clear coverage assessment (adequate/needs expansion)",
        "- [x] Suggests specific expansion topics when coverage is low",
        "- [x] Works with existing corpus or standalone document set"
      ],
      "lines_removed": [
        "### 127. Create Cluster Coverage Evaluation Script",
        "**Meta:** `status:pending` `priority:high` `category:devex`",
        "**Files:** `scripts/evaluate_cluster.py` (new)",
        "**Solution:** Create a script that evaluates cluster quality and coverage:",
        "```python",
        "# Usage examples:",
        "python scripts/evaluate_cluster.py --documents customer_support_fundamentals,complaint_resolution",
        "python scripts/evaluate_cluster.py --keywords customer,ticket,escalation",
        "**Features:**",
        "1. **Cluster Detection** - Identify documents that cluster together based on similarity",
        "   - Internal cohesion (avg similarity within cluster)",
        "   - External separation (avg similarity to non-cluster docs)",
        "   - Concept coverage (unique concepts captured)",
        "   - Term diversity (vocabulary richness)",
        "3. **Gap Analysis:**",
        "   - Missing subtopics (based on concept graph)",
        "   - Weak connections (low-weight edges)",
        "   - Suggested expansion topics",
        "4. **Recommendations:**",
        "   - \"Cluster is well-formed\" vs \"Needs more coverage\"",
        "   - Specific suggestions: \"Add documents about X, Y, Z\"",
        "",
        "**Output Example:**",
        "Cluster Analysis: Customer Service (6 documents)",
        "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”",
        "  â€¢ customer_support_fundamentals (hub)",
        "  â€¢ ticket_escalation_procedures",
        "  â€¢ customer_satisfaction_metrics",
        "  â€¢ complaint_resolution",
        "  â€¢ call_center_operations",
        "  â€¢ customer_retention_strategies",
        "  Internal Cohesion:    0.72 (good)",
        "  External Separation:  0.45 (moderate)",
        "  Concept Coverage:     23 concepts",
        "  Term Diversity:       0.68",
        "",
        "Coverage Assessment: ADEQUATE âœ“",
        "  The cluster forms a coherent topic group with good internal",
        "  connectivity. Documents share key concepts (customer, ticket,",
        "  escalation, resolution) while maintaining distinct subtopics.",
        "",
        "Potential Expansions (optional):",
        "  â€¢ CRM integration / helpdesk software",
        "  â€¢ Chat support / live chat best practices",
        "  â€¢ SLA management / service level agreements",
        "  â€¢ Customer journey mapping",
        "- [ ] Script identifies document clusters by topic/keywords",
        "- [ ] Computes cohesion and separation metrics",
        "- [ ] Provides clear coverage assessment (adequate/needs expansion)",
        "- [ ] Suggests specific expansion topics when coverage is low",
        "- [ ] Works with existing corpus or standalone document set"
      ],
      "context_before": [
        "- All 820 tests pass âœ…",
        "",
        "**Acceptance Criteria:**",
        "- [x] Root cause identified via git history",
        "- [x] Embedding similarities semantically meaningful",
        "- [x] Regression test added to prevent recurrence",
        "- [~] Concept clusters > 10: Not achievable due to highly connected corpus (correct behavior)",
        "",
        "---",
        ""
      ],
      "context_after": [
        "",
        "**Effort:** Medium",
        "**Depends:** 125",
        "",
        "**Problem:** When adding sample documents to create topic clusters (e.g., customer service), there's no automated way to determine if the cluster has sufficient coverage or needs more documents.",
        "",
        "",
        "python scripts/evaluate_cluster.py --topic \"customer service\"",
        "```",
        "",
        "2. **Coverage Metrics:**",
        "```",
        "Documents:",
        "",
        "Metrics:",
        "```",
        "",
        "**Acceptance Criteria:**",
        "",
        "---",
        "",
        "### 128. Analyze Customer Service Cluster Quality",
        "",
        "**Meta:** `status:pending` `priority:low` `category:research`",
        "**Files:** Analysis output",
        "**Effort:** Small",
        "**Depends:** 127",
        ""
      ],
      "change_type": "modify"
    },
    {
      "file": "scripts/evaluate_cluster.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "#!/usr/bin/env python3",
        "\"\"\"",
        "Cluster Coverage Evaluation Script",
        "===================================",
        "",
        "Task #127: Evaluate cluster quality and coverage for topic-based document groups.",
        "",
        "This script helps determine if a document cluster has sufficient coverage",
        "or needs more documents to form a coherent topic group.",
        "",
        "Usage:",
        "    # Find and evaluate documents matching a topic",
        "    python scripts/evaluate_cluster.py --topic \"customer service\"",
        "",
        "    # Evaluate specific documents",
        "    python scripts/evaluate_cluster.py --documents customer_support_fundamentals,complaint_resolution",
        "",
        "    # Find documents containing specific keywords",
        "    python scripts/evaluate_cluster.py --keywords customer,ticket,escalation",
        "",
        "    # Use existing corpus file",
        "    python scripts/evaluate_cluster.py --corpus corpus_dev.pkl --topic \"machine learning\"",
        "",
        "    # Verbose output with expansion suggestions",
        "    python scripts/evaluate_cluster.py --topic \"customer\" --verbose --suggest",
        "\"\"\"",
        "",
        "import os",
        "import sys",
        "import argparse",
        "from typing import Dict, List, Set, Tuple, Any, Optional",
        "from collections import defaultdict",
        "from pathlib import Path",
        "",
        "# Add project root to path",
        "sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))",
        "",
        "from cortical import CorticalTextProcessor, CorticalLayer",
        "",
        "",
        "def load_corpus(",
        "    processor: CorticalTextProcessor,",
        "    samples_dir: str = \"samples\"",
        ") -> int:",
        "    \"\"\"Load all sample documents into the processor.\"\"\"",
        "    loaded = 0",
        "    samples_path = Path(samples_dir)",
        "",
        "    if not samples_path.is_dir():",
        "        print(f\"Samples directory not found: {samples_dir}\")",
        "        return 0",
        "",
        "    for filepath in sorted(samples_path.glob(\"*.txt\")):",
        "        try:",
        "            content = filepath.read_text(encoding='utf-8')",
        "            doc_id = filepath.stem",
        "            processor.process_document(doc_id, content)",
        "            loaded += 1",
        "        except Exception as e:",
        "            print(f\"Error loading {filepath.name}: {e}\")",
        "",
        "    return loaded",
        "",
        "",
        "def find_documents_by_topic(",
        "    processor: CorticalTextProcessor,",
        "    topic: str,",
        "    threshold: float = 0.1",
        ") -> List[Tuple[str, float]]:",
        "    \"\"\"",
        "    Find documents related to a topic using semantic search.",
        "",
        "    Returns list of (doc_id, score) tuples.",
        "    \"\"\"",
        "    results = processor.find_documents_for_query(topic, top_n=50)",
        "    # Filter by threshold",
        "    return [(doc_id, score) for doc_id, score in results if score >= threshold]",
        "",
        "",
        "def find_documents_by_keywords(",
        "    processor: CorticalTextProcessor,",
        "    keywords: List[str],",
        "    min_keywords: int = 1",
        ") -> List[str]:",
        "    \"\"\"",
        "    Find documents containing at least min_keywords of the specified keywords.",
        "    \"\"\"",
        "    layer0 = processor.layers[CorticalLayer.TOKENS]",
        "    doc_keyword_counts: Dict[str, int] = defaultdict(int)",
        "",
        "    for keyword in keywords:",
        "        keyword_lower = keyword.lower()",
        "        col = layer0.get_minicolumn(keyword_lower)",
        "        if col:",
        "            for doc_id in col.document_ids:",
        "                doc_keyword_counts[doc_id] += 1",
        "",
        "    return [doc_id for doc_id, count in doc_keyword_counts.items()",
        "            if count >= min_keywords]",
        "",
        "",
        "def compute_document_similarity(",
        "    processor: CorticalTextProcessor,",
        "    doc1: str,",
        "    doc2: str",
        ") -> float:",
        "    \"\"\"",
        "    Compute similarity between two documents based on shared terms.",
        "    Uses Jaccard similarity of term sets weighted by TF-IDF.",
        "    \"\"\"",
        "    layer0 = processor.layers[CorticalLayer.TOKENS]",
        "",
        "    # Get terms for each document",
        "    terms1: Dict[str, float] = {}",
        "    terms2: Dict[str, float] = {}",
        "",
        "    for col in layer0.minicolumns.values():",
        "        if doc1 in col.document_ids:",
        "            terms1[col.content] = col.tfidf_per_doc.get(doc1, col.tfidf)",
        "        if doc2 in col.document_ids:",
        "            terms2[col.content] = col.tfidf_per_doc.get(doc2, col.tfidf)",
        "",
        "    if not terms1 or not terms2:",
        "        return 0.0",
        "",
        "    # Compute weighted Jaccard",
        "    common = set(terms1.keys()) & set(terms2.keys())",
        "    if not common:",
        "        return 0.0",
        "",
        "    # Sum of minimum weights / sum of maximum weights",
        "    min_sum = sum(min(terms1[t], terms2[t]) for t in common)",
        "    all_terms = set(terms1.keys()) | set(terms2.keys())",
        "    max_sum = sum(max(terms1.get(t, 0), terms2.get(t, 0)) for t in all_terms)",
        "",
        "    return min_sum / max_sum if max_sum > 0 else 0.0",
        "",
        "",
        "def compute_cluster_metrics(",
        "    processor: CorticalTextProcessor,",
        "    cluster_docs: List[str],",
        "    all_docs: Optional[List[str]] = None",
        ") -> Dict[str, Any]:",
        "    \"\"\"",
        "    Compute comprehensive metrics for a document cluster.",
        "",
        "    Returns:",
        "        Dictionary with cohesion, separation, coverage, and diversity metrics.",
        "    \"\"\"",
        "    if all_docs is None:",
        "        all_docs = list(processor.documents.keys())",
        "",
        "    outside_docs = [d for d in all_docs if d not in cluster_docs]",
        "",
        "    layer0 = processor.layers[CorticalLayer.TOKENS]",
        "    layer2 = processor.layers[CorticalLayer.CONCEPTS]",
        "",
        "    # 1. Internal Cohesion: average similarity within cluster",
        "    internal_similarities = []",
        "    for i, doc1 in enumerate(cluster_docs):",
        "        for doc2 in cluster_docs[i+1:]:",
        "            sim = compute_document_similarity(processor, doc1, doc2)",
        "            internal_similarities.append(sim)",
        "",
        "    cohesion = sum(internal_similarities) / len(internal_similarities) if internal_similarities else 0.0",
        "",
        "    # 2. External Separation: average similarity to outside documents",
        "    external_similarities = []",
        "    for cluster_doc in cluster_docs:",
        "        for outside_doc in outside_docs[:20]:  # Sample for efficiency",
        "            sim = compute_document_similarity(processor, cluster_doc, outside_doc)",
        "            external_similarities.append(sim)",
        "",
        "    separation = 1.0 - (sum(external_similarities) / len(external_similarities) if external_similarities else 0.0)",
        "",
        "    # 3. Concept Coverage: unique concepts captured by cluster",
        "    cluster_tokens: Set[str] = set()",
        "    cluster_concepts: Set[str] = set()",
        "",
        "    for doc_id in cluster_docs:",
        "        for col in layer0.minicolumns.values():",
        "            if doc_id in col.document_ids:",
        "                cluster_tokens.add(col.content)",
        "",
        "    # Find which concepts contain our tokens",
        "    for concept_col in layer2.minicolumns.values():",
        "        for token_id in concept_col.feedforward_connections:",
        "            token_col = layer0.get_by_id(token_id)",
        "            if token_col and token_col.content in cluster_tokens:",
        "                cluster_concepts.add(concept_col.content)",
        "                break",
        "",
        "    # 4. Term Diversity: vocabulary richness (unique terms / total occurrences)",
        "    total_term_occurrences = 0",
        "    for doc_id in cluster_docs:",
        "        for col in layer0.minicolumns.values():",
        "            if doc_id in col.document_ids:",
        "                total_term_occurrences += 1",
        "",
        "    diversity = len(cluster_tokens) / total_term_occurrences if total_term_occurrences > 0 else 0.0",
        "",
        "    # 5. Hub Document: document most connected to others in cluster",
        "    hub_doc = cluster_docs[0] if cluster_docs else None  # Default to first doc",
        "    if len(cluster_docs) > 1:",
        "        max_avg_sim = -1.0",
        "        for doc in cluster_docs:",
        "            avg_sim = sum(",
        "                compute_document_similarity(processor, doc, other)",
        "                for other in cluster_docs if other != doc",
        "            ) / (len(cluster_docs) - 1)",
        "            if avg_sim > max_avg_sim:",
        "                max_avg_sim = avg_sim",
        "                hub_doc = doc",
        "",
        "    # 6. Key Terms: most important terms in cluster by TF-IDF",
        "    term_scores: Dict[str, float] = defaultdict(float)",
        "    for col in layer0.minicolumns.values():",
        "        docs_in_cluster = col.document_ids & set(cluster_docs)",
        "        if docs_in_cluster:",
        "            # Average TF-IDF across cluster documents",
        "            for doc_id in docs_in_cluster:",
        "                term_scores[col.content] += col.tfidf_per_doc.get(doc_id, col.tfidf)",
        "            term_scores[col.content] /= len(docs_in_cluster)",
        "",
        "    key_terms = sorted(term_scores.items(), key=lambda x: -x[1])[:15]",
        "",
        "    return {",
        "        'cohesion': cohesion,",
        "        'separation': separation,",
        "        'concept_count': len(cluster_concepts),",
        "        'term_count': len(cluster_tokens),",
        "        'diversity': diversity,",
        "        'hub_document': hub_doc,",
        "        'key_terms': key_terms,",
        "        'cluster_tokens': cluster_tokens,",
        "        'cluster_concepts': cluster_concepts,",
        "    }",
        "",
        "",
        "def find_expansion_suggestions(",
        "    processor: CorticalTextProcessor,",
        "    cluster_tokens: Set[str],",
        "    cluster_docs: List[str],",
        "    max_suggestions: int = 5",
        ") -> List[Tuple[str, str]]:",
        "    \"\"\"",
        "    Find topics that could expand the cluster coverage.",
        "",
        "    Looks for:",
        "    1. Related terms that appear in few/no cluster documents",
        "    2. Concepts connected to cluster concepts but not well covered",
        "",
        "    Returns:",
        "        List of (suggestion, reason) tuples",
        "    \"\"\"",
        "    layer0 = processor.layers[CorticalLayer.TOKENS]",
        "    suggestions = []",
        "",
        "    # Find terms connected to cluster terms but not in cluster",
        "    related_terms: Dict[str, float] = defaultdict(float)",
        "",
        "    for token in list(cluster_tokens)[:50]:  # Sample for efficiency",
        "        col = layer0.get_minicolumn(token)",
        "        if col:",
        "            for neighbor_id, weight in col.lateral_connections.items():",
        "                neighbor = layer0.get_by_id(neighbor_id)",
        "                if neighbor and neighbor.content not in cluster_tokens:",
        "                    # Check if this term appears mostly outside our cluster",
        "                    docs_in_cluster = neighbor.document_ids & set(cluster_docs)",
        "                    docs_outside = neighbor.document_ids - set(cluster_docs)",
        "                    if len(docs_outside) > len(docs_in_cluster):",
        "                        related_terms[neighbor.content] += weight",
        "",
        "    # Sort by connection strength",
        "    top_related = sorted(related_terms.items(), key=lambda x: -x[1])[:max_suggestions * 2]",
        "",
        "    for term, weight in top_related:",
        "        if len(suggestions) >= max_suggestions:",
        "            break",
        "",
        "        col = layer0.get_minicolumn(term)",
        "        if col:",
        "            # Find what documents have this term",
        "            example_docs = list(col.document_ids - set(cluster_docs))[:2]",
        "            if example_docs:",
        "                reason = f\"Related to cluster terms, found in: {', '.join(example_docs[:2])}\"",
        "            else:",
        "                reason = f\"Strongly connected to cluster vocabulary\"",
        "            suggestions.append((term, reason))",
        "",
        "    return suggestions",
        "",
        "",
        "def assess_coverage(metrics: Dict[str, Any], num_docs: int) -> Tuple[str, str]:",
        "    \"\"\"",
        "    Assess cluster coverage quality and provide recommendation.",
        "",
        "    Returns:",
        "        (assessment_label, explanation)",
        "    \"\"\"",
        "    cohesion = metrics['cohesion']",
        "    separation = metrics['separation']",
        "    concept_count = metrics['concept_count']",
        "",
        "    # Scoring",
        "    score = 0",
        "    issues = []",
        "    strengths = []",
        "",
        "    # Cohesion assessment",
        "    if cohesion >= 0.3:",
        "        score += 2",
        "        strengths.append(\"strong internal connectivity\")",
        "    elif cohesion >= 0.15:",
        "        score += 1",
        "        strengths.append(\"moderate internal connectivity\")",
        "    else:",
        "        issues.append(\"weak internal connectivity\")",
        "",
        "    # Separation assessment",
        "    if separation >= 0.6:",
        "        score += 2",
        "        strengths.append(\"well-separated from other topics\")",
        "    elif separation >= 0.4:",
        "        score += 1",
        "    else:",
        "        issues.append(\"overlaps significantly with other topics\")",
        "",
        "    # Coverage assessment",
        "    if concept_count >= 5:",
        "        score += 1",
        "        strengths.append(f\"covers {concept_count} concept clusters\")",
        "    elif concept_count < 2:",
        "        issues.append(\"limited concept coverage\")",
        "",
        "    # Document count",
        "    if num_docs >= 5:",
        "        score += 1",
        "    elif num_docs < 3:",
        "        issues.append(\"too few documents\")",
        "",
        "    # Generate assessment",
        "    if score >= 5:",
        "        label = \"STRONG\"",
        "        explanation = f\"Cluster is well-formed with {', '.join(strengths)}.\"",
        "    elif score >= 3:",
        "        label = \"ADEQUATE\"",
        "        if issues:",
        "            explanation = f\"Cluster is usable but could improve: {', '.join(issues)}.\"",
        "        else:",
        "            explanation = f\"Cluster forms a coherent topic group with {', '.join(strengths)}.\"",
        "    else:",
        "        label = \"NEEDS EXPANSION\"",
        "        explanation = f\"Cluster needs more coverage: {', '.join(issues)}.\"",
        "",
        "    return label, explanation",
        "",
        "",
        "def print_cluster_analysis(",
        "    cluster_name: str,",
        "    cluster_docs: List[str],",
        "    metrics: Dict[str, Any],",
        "    suggestions: List[Tuple[str, str]],",
        "    verbose: bool = False",
        ") -> None:",
        "    \"\"\"Print formatted cluster analysis report.\"\"\"",
        "",
        "    assessment_label, assessment_explanation = assess_coverage(metrics, len(cluster_docs))",
        "",
        "    # Header",
        "    title = f\"Cluster Analysis: {cluster_name} ({len(cluster_docs)} documents)\"",
        "    print(f\"\\n{title}\")",
        "    print(\"=\" * len(title))",
        "",
        "    # Documents",
        "    print(\"\\nDocuments:\")",
        "    hub = metrics.get('hub_document')",
        "    for doc in sorted(cluster_docs):",
        "        marker = \" (hub)\" if doc == hub else \"\"",
        "        print(f\"  * {doc}{marker}\")",
        "",
        "    # Metrics",
        "    print(\"\\nMetrics:\")",
        "",
        "    cohesion = metrics['cohesion']",
        "    if cohesion >= 0.3:",
        "        cohesion_label = \"strong\"",
        "    elif cohesion >= 0.15:",
        "        cohesion_label = \"moderate\"",
        "    else:",
        "        cohesion_label = \"weak\"",
        "    print(f\"  Internal Cohesion:    {cohesion:.2f} ({cohesion_label})\")",
        "",
        "    separation = metrics['separation']",
        "    if separation >= 0.6:",
        "        sep_label = \"good\"",
        "    elif separation >= 0.4:",
        "        sep_label = \"moderate\"",
        "    else:",
        "        sep_label = \"low\"",
        "    print(f\"  External Separation:  {separation:.2f} ({sep_label})\")",
        "",
        "    print(f\"  Concept Coverage:     {metrics['concept_count']} concepts\")",
        "    print(f\"  Term Diversity:       {metrics['diversity']:.2f}\")",
        "    print(f\"  Unique Terms:         {metrics['term_count']}\")",
        "",
        "    # Assessment",
        "    if assessment_label == \"STRONG\":",
        "        symbol = \"[OK]\"",
        "    elif assessment_label == \"ADEQUATE\":",
        "        symbol = \"[~]\"",
        "    else:",
        "        symbol = \"[!]\"",
        "",
        "    print(f\"\\nCoverage Assessment: {assessment_label} {symbol}\")",
        "    print(f\"  {assessment_explanation}\")",
        "",
        "    # Key terms",
        "    if verbose:",
        "        print(\"\\nKey Terms (by TF-IDF):\")",
        "        for term, score in metrics['key_terms'][:10]:",
        "            print(f\"  - {term}: {score:.3f}\")",
        "",
        "    # Expansion suggestions",
        "    if suggestions:",
        "        print(\"\\nPotential Expansions:\")",
        "        for term, reason in suggestions:",
        "            print(f\"  * {term}\")",
        "            if verbose:",
        "                print(f\"    ({reason})\")",
        "",
        "",
        "def main():",
        "    parser = argparse.ArgumentParser(",
        "        description=\"Evaluate cluster quality and coverage for document groups\",",
        "        formatter_class=argparse.RawDescriptionHelpFormatter,",
        "        epilog=\"\"\"",
        "Examples:",
        "  %(prog)s --topic \"customer service\"",
        "  %(prog)s --documents customer_support_fundamentals,complaint_resolution",
        "  %(prog)s --keywords customer,ticket,escalation",
        "  %(prog)s --corpus corpus_dev.pkl --topic \"machine learning\" --verbose",
        "        \"\"\"",
        "    )",
        "",
        "    # Input options (mutually exclusive)",
        "    input_group = parser.add_mutually_exclusive_group(required=True)",
        "    input_group.add_argument(",
        "        \"--topic\", \"-t\",",
        "        help=\"Find documents by semantic topic search\"",
        "    )",
        "    input_group.add_argument(",
        "        \"--documents\", \"-d\",",
        "        help=\"Comma-separated list of document IDs to evaluate\"",
        "    )",
        "    input_group.add_argument(",
        "        \"--keywords\", \"-k\",",
        "        help=\"Comma-separated keywords to find documents containing them\"",
        "    )",
        "",
        "    # Corpus options",
        "    parser.add_argument(",
        "        \"--corpus\", \"-c\",",
        "        help=\"Path to saved corpus file (default: load from samples/)\"",
        "    )",
        "    parser.add_argument(",
        "        \"--samples-dir\",",
        "        default=\"samples\",",
        "        help=\"Directory containing sample documents (default: samples/)\"",
        "    )",
        "",
        "    # Output options",
        "    parser.add_argument(",
        "        \"--verbose\", \"-v\",",
        "        action=\"store_true\",",
        "        help=\"Show detailed output including key terms\"",
        "    )",
        "    parser.add_argument(",
        "        \"--suggest\", \"-s\",",
        "        action=\"store_true\",",
        "        help=\"Show expansion suggestions\"",
        "    )",
        "    parser.add_argument(",
        "        \"--threshold\",",
        "        type=float,",
        "        default=0.1,",
        "        help=\"Minimum score threshold for topic search (default: 0.1)\"",
        "    )",
        "    parser.add_argument(",
        "        \"--min-keywords\",",
        "        type=int,",
        "        default=1,",
        "        help=\"Minimum keywords required for document match (default: 1)\"",
        "    )",
        "",
        "    args = parser.parse_args()",
        "",
        "    # Load or create processor",
        "    if args.corpus and os.path.exists(args.corpus):",
        "        print(f\"Loading corpus from {args.corpus}...\")",
        "        processor = CorticalTextProcessor.load(args.corpus)",
        "        print(f\"Loaded {len(processor.documents)} documents\")",
        "    else:",
        "        print(f\"Loading documents from {args.samples_dir}/...\")",
        "        processor = CorticalTextProcessor()",
        "        num_loaded = load_corpus(processor, args.samples_dir)",
        "        if num_loaded == 0:",
        "            print(\"Error: No documents loaded\")",
        "            sys.exit(1)",
        "        print(f\"Loaded {num_loaded} documents, computing analysis...\")",
        "        processor.compute_all(verbose=False)",
        "",
        "    # Find cluster documents",
        "    cluster_name = \"\"",
        "    cluster_docs: List[str] = []",
        "",
        "    if args.topic:",
        "        cluster_name = args.topic",
        "        results = find_documents_by_topic(processor, args.topic, args.threshold)",
        "        cluster_docs = [doc_id for doc_id, _ in results]",
        "        if not cluster_docs:",
        "            print(f\"No documents found matching topic: {args.topic}\")",
        "            sys.exit(1)",
        "        print(f\"Found {len(cluster_docs)} documents matching topic '{args.topic}'\")",
        "",
        "    elif args.documents:",
        "        doc_ids = [d.strip() for d in args.documents.split(\",\")]",
        "        cluster_name = f\"Selected ({len(doc_ids)} docs)\"",
        "        # Validate document IDs",
        "        missing = [d for d in doc_ids if d not in processor.documents]",
        "        if missing:",
        "            print(f\"Warning: Documents not found: {', '.join(missing)}\")",
        "        cluster_docs = [d for d in doc_ids if d in processor.documents]",
        "        if not cluster_docs:",
        "            print(\"Error: None of the specified documents were found\")",
        "            sys.exit(1)",
        "",
        "    elif args.keywords:",
        "        keywords = [k.strip() for k in args.keywords.split(\",\")]",
        "        cluster_name = f\"Keywords: {', '.join(keywords[:3])}\"",
        "        cluster_docs = find_documents_by_keywords(processor, keywords, args.min_keywords)",
        "        if not cluster_docs:",
        "            print(f\"No documents found containing keywords: {', '.join(keywords)}\")",
        "            sys.exit(1)",
        "        print(f\"Found {len(cluster_docs)} documents with keywords\")",
        "",
        "    # Compute metrics",
        "    metrics = compute_cluster_metrics(processor, cluster_docs)",
        "",
        "    # Find expansion suggestions if requested",
        "    suggestions = []",
        "    if args.suggest:",
        "        suggestions = find_expansion_suggestions(",
        "            processor,",
        "            metrics['cluster_tokens'],",
        "            cluster_docs",
        "        )",
        "",
        "    # Print analysis",
        "    print_cluster_analysis(",
        "        cluster_name,",
        "        cluster_docs,",
        "        metrics,",
        "        suggestions,",
        "        verbose=args.verbose",
        "    )",
        "",
        "",
        "if __name__ == \"__main__\":",
        "    main()"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "tests/test_evaluate_cluster.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "\"\"\"",
        "Tests for scripts/evaluate_cluster.py - Cluster coverage evaluation utilities.",
        "\"\"\"",
        "",
        "import unittest",
        "import sys",
        "from pathlib import Path",
        "",
        "# Add parent and scripts directories to path",
        "sys.path.insert(0, str(Path(__file__).parent.parent))",
        "sys.path.insert(0, str(Path(__file__).parent.parent / 'scripts'))",
        "",
        "from cortical.processor import CorticalTextProcessor",
        "from cortical.layers import CorticalLayer",
        "from evaluate_cluster import (",
        "    find_documents_by_keywords,",
        "    compute_document_similarity,",
        "    compute_cluster_metrics,",
        "    find_expansion_suggestions,",
        "    assess_coverage,",
        ")",
        "",
        "",
        "class TestFindDocumentsByKeywords(unittest.TestCase):",
        "    \"\"\"Tests for keyword-based document finding.\"\"\"",
        "",
        "    def setUp(self):",
        "        \"\"\"Create a processor with test documents.\"\"\"",
        "        self.processor = CorticalTextProcessor()",
        "        self.processor.process_document(\"ml1\", \"Neural networks deep learning training models\")",
        "        self.processor.process_document(\"ml2\", \"Machine learning algorithms neural data\")",
        "        self.processor.process_document(\"cook1\", \"Bread baking flour yeast oven temperature\")",
        "        self.processor.process_document(\"cook2\", \"Italian pasta cooking tomato sauce\")",
        "        self.processor.compute_all(verbose=False)",
        "",
        "    def test_single_keyword_match(self):",
        "        \"\"\"Test finding documents with a single keyword.\"\"\"",
        "        docs = find_documents_by_keywords(self.processor, [\"neural\"])",
        "        self.assertIn(\"ml1\", docs)",
        "        self.assertIn(\"ml2\", docs)",
        "        self.assertNotIn(\"cook1\", docs)",
        "",
        "    def test_multiple_keywords_any(self):",
        "        \"\"\"Test finding documents with any of multiple keywords.\"\"\"",
        "        docs = find_documents_by_keywords(self.processor, [\"neural\", \"pasta\"], min_keywords=1)",
        "        self.assertIn(\"ml1\", docs)",
        "        self.assertIn(\"ml2\", docs)",
        "        self.assertIn(\"cook2\", docs)",
        "",
        "    def test_multiple_keywords_all(self):",
        "        \"\"\"Test finding documents with all keywords.\"\"\"",
        "        docs = find_documents_by_keywords(self.processor, [\"neural\", \"learning\"], min_keywords=2)",
        "        # ml1 has both \"neural\" and \"learning\"",
        "        self.assertIn(\"ml1\", docs)",
        "",
        "    def test_no_matches(self):",
        "        \"\"\"Test with keywords that don't match any documents.\"\"\"",
        "        docs = find_documents_by_keywords(self.processor, [\"quantum\", \"physics\"])",
        "        self.assertEqual(docs, [])",
        "",
        "",
        "class TestComputeDocumentSimilarity(unittest.TestCase):",
        "    \"\"\"Tests for document similarity computation.\"\"\"",
        "",
        "    def setUp(self):",
        "        \"\"\"Create a processor with test documents.\"\"\"",
        "        self.processor = CorticalTextProcessor()",
        "        self.processor.process_document(\"doc1\", \"Neural networks deep learning models\")",
        "        self.processor.process_document(\"doc2\", \"Neural networks machine learning algorithms\")",
        "        self.processor.process_document(\"doc3\", \"Bread baking flour yeast recipes\")",
        "        self.processor.compute_all(verbose=False)",
        "",
        "    def test_similar_documents(self):",
        "        \"\"\"Test similarity between related documents.\"\"\"",
        "        sim = compute_document_similarity(self.processor, \"doc1\", \"doc2\")",
        "        # Both are about neural networks/ML, should have positive similarity",
        "        self.assertGreater(sim, 0.0)",
        "",
        "    def test_dissimilar_documents(self):",
        "        \"\"\"Test similarity between unrelated documents.\"\"\"",
        "        sim_ml_cook = compute_document_similarity(self.processor, \"doc1\", \"doc3\")",
        "        sim_ml_ml = compute_document_similarity(self.processor, \"doc1\", \"doc2\")",
        "        # ML docs should be more similar to each other than to cooking",
        "        self.assertGreater(sim_ml_ml, sim_ml_cook)",
        "",
        "    def test_self_similarity(self):",
        "        \"\"\"Test similarity of a document with itself.\"\"\"",
        "        sim = compute_document_similarity(self.processor, \"doc1\", \"doc1\")",
        "        # Self-similarity should be 1.0 (or close to it)",
        "        self.assertGreaterEqual(sim, 0.9)",
        "",
        "    def test_nonexistent_document(self):",
        "        \"\"\"Test similarity with non-existent document.\"\"\"",
        "        sim = compute_document_similarity(self.processor, \"doc1\", \"nonexistent\")",
        "        self.assertEqual(sim, 0.0)",
        "",
        "",
        "class TestComputeClusterMetrics(unittest.TestCase):",
        "    \"\"\"Tests for cluster metrics computation.\"\"\"",
        "",
        "    def setUp(self):",
        "        \"\"\"Create a processor with diverse documents.\"\"\"",
        "        self.processor = CorticalTextProcessor()",
        "",
        "        # ML cluster",
        "        self.processor.process_document(\"ml1\", \"Neural networks deep learning training models backpropagation\")",
        "        self.processor.process_document(\"ml2\", \"Machine learning algorithms neural data classification\")",
        "        self.processor.process_document(\"ml3\", \"Deep learning convolutional networks image recognition\")",
        "",
        "        # Cooking cluster",
        "        self.processor.process_document(\"cook1\", \"Bread baking flour yeast oven temperature recipes\")",
        "        self.processor.process_document(\"cook2\", \"Italian pasta cooking tomato sauce ingredients\")",
        "        self.processor.process_document(\"cook3\", \"French cuisine cooking techniques sauces\")",
        "",
        "        self.processor.compute_all(verbose=False)",
        "",
        "    def test_metrics_returns_dict(self):",
        "        \"\"\"Test that metrics returns expected dictionary structure.\"\"\"",
        "        metrics = compute_cluster_metrics(self.processor, [\"ml1\", \"ml2\", \"ml3\"])",
        "",
        "        self.assertIn(\"cohesion\", metrics)",
        "        self.assertIn(\"separation\", metrics)",
        "        self.assertIn(\"concept_count\", metrics)",
        "        self.assertIn(\"term_count\", metrics)",
        "        self.assertIn(\"diversity\", metrics)",
        "        self.assertIn(\"hub_document\", metrics)",
        "        self.assertIn(\"key_terms\", metrics)",
        "",
        "    def test_cohesion_range(self):",
        "        \"\"\"Test that cohesion is in valid range.\"\"\"",
        "        metrics = compute_cluster_metrics(self.processor, [\"ml1\", \"ml2\", \"ml3\"])",
        "        self.assertGreaterEqual(metrics[\"cohesion\"], 0.0)",
        "        self.assertLessEqual(metrics[\"cohesion\"], 1.0)",
        "",
        "    def test_separation_range(self):",
        "        \"\"\"Test that separation is in valid range.\"\"\"",
        "        metrics = compute_cluster_metrics(self.processor, [\"ml1\", \"ml2\", \"ml3\"])",
        "        self.assertGreaterEqual(metrics[\"separation\"], 0.0)",
        "        self.assertLessEqual(metrics[\"separation\"], 1.0)",
        "",
        "    def test_diversity_range(self):",
        "        \"\"\"Test that diversity is in valid range.\"\"\"",
        "        metrics = compute_cluster_metrics(self.processor, [\"ml1\", \"ml2\", \"ml3\"])",
        "        self.assertGreaterEqual(metrics[\"diversity\"], 0.0)",
        "        self.assertLessEqual(metrics[\"diversity\"], 1.0)",
        "",
        "    def test_hub_document_in_cluster(self):",
        "        \"\"\"Test that hub document is one of the cluster documents.\"\"\"",
        "        cluster = [\"ml1\", \"ml2\", \"ml3\"]",
        "        metrics = compute_cluster_metrics(self.processor, cluster)",
        "        self.assertIn(metrics[\"hub_document\"], cluster)",
        "",
        "    def test_term_count_positive(self):",
        "        \"\"\"Test that term count is positive for non-empty cluster.\"\"\"",
        "        metrics = compute_cluster_metrics(self.processor, [\"ml1\", \"ml2\"])",
        "        self.assertGreater(metrics[\"term_count\"], 0)",
        "",
        "    def test_single_document_cluster(self):",
        "        \"\"\"Test metrics for single-document cluster.\"\"\"",
        "        metrics = compute_cluster_metrics(self.processor, [\"ml1\"])",
        "        # Single doc has no internal pairs, cohesion should be 0",
        "        self.assertEqual(metrics[\"cohesion\"], 0.0)",
        "        # Should still have valid hub",
        "        self.assertEqual(metrics[\"hub_document\"], \"ml1\")",
        "",
        "",
        "class TestAssessCoverage(unittest.TestCase):",
        "    \"\"\"Tests for coverage assessment logic.\"\"\"",
        "",
        "    def test_strong_coverage(self):",
        "        \"\"\"Test that high metrics yield STRONG assessment.\"\"\"",
        "        metrics = {",
        "            \"cohesion\": 0.4,",
        "            \"separation\": 0.7,",
        "            \"concept_count\": 10,",
        "        }",
        "        label, _ = assess_coverage(metrics, num_docs=6)",
        "        self.assertEqual(label, \"STRONG\")",
        "",
        "    def test_adequate_coverage(self):",
        "        \"\"\"Test that moderate metrics yield ADEQUATE assessment.\"\"\"",
        "        metrics = {",
        "            \"cohesion\": 0.2,",
        "            \"separation\": 0.5,",
        "            \"concept_count\": 5,",
        "        }",
        "        label, _ = assess_coverage(metrics, num_docs=4)",
        "        self.assertEqual(label, \"ADEQUATE\")",
        "",
        "    def test_needs_expansion(self):",
        "        \"\"\"Test that low metrics yield NEEDS EXPANSION assessment.\"\"\"",
        "        metrics = {",
        "            \"cohesion\": 0.05,",
        "            \"separation\": 0.3,",
        "            \"concept_count\": 1,",
        "        }",
        "        label, _ = assess_coverage(metrics, num_docs=2)",
        "        self.assertEqual(label, \"NEEDS EXPANSION\")",
        "",
        "    def test_explanation_provided(self):",
        "        \"\"\"Test that assessment includes explanation.\"\"\"",
        "        metrics = {",
        "            \"cohesion\": 0.3,",
        "            \"separation\": 0.6,",
        "            \"concept_count\": 5,",
        "        }",
        "        label, explanation = assess_coverage(metrics, num_docs=5)",
        "        self.assertIsInstance(explanation, str)",
        "        self.assertGreater(len(explanation), 0)",
        "",
        "",
        "class TestFindExpansionSuggestions(unittest.TestCase):",
        "    \"\"\"Tests for expansion suggestion generation.\"\"\"",
        "",
        "    def setUp(self):",
        "        \"\"\"Create a processor with test documents.\"\"\"",
        "        self.processor = CorticalTextProcessor()",
        "        self.processor.process_document(\"ml1\", \"Neural networks deep learning training\")",
        "        self.processor.process_document(\"ml2\", \"Machine learning algorithms data\")",
        "        self.processor.process_document(\"other1\", \"Cooking recipes baking bread\")",
        "        self.processor.process_document(\"other2\", \"Legal contract law agreements\")",
        "        self.processor.compute_all(verbose=False)",
        "",
        "    def test_suggestions_returns_list(self):",
        "        \"\"\"Test that suggestions returns a list of tuples.\"\"\"",
        "        cluster_docs = [\"ml1\", \"ml2\"]",
        "        layer0 = self.processor.layers[CorticalLayer.TOKENS]",
        "        cluster_tokens = set()",
        "        for doc_id in cluster_docs:",
        "            for col in layer0.minicolumns.values():",
        "                if doc_id in col.document_ids:",
        "                    cluster_tokens.add(col.content)",
        "",
        "        suggestions = find_expansion_suggestions(",
        "            self.processor, cluster_tokens, cluster_docs, max_suggestions=3",
        "        )",
        "        self.assertIsInstance(suggestions, list)",
        "",
        "    def test_suggestions_format(self):",
        "        \"\"\"Test that each suggestion is a (term, reason) tuple.\"\"\"",
        "        cluster_docs = [\"ml1\", \"ml2\"]",
        "        layer0 = self.processor.layers[CorticalLayer.TOKENS]",
        "        cluster_tokens = set()",
        "        for doc_id in cluster_docs:",
        "            for col in layer0.minicolumns.values():",
        "                if doc_id in col.document_ids:",
        "                    cluster_tokens.add(col.content)",
        "",
        "        suggestions = find_expansion_suggestions(",
        "            self.processor, cluster_tokens, cluster_docs, max_suggestions=3",
        "        )",
        "",
        "        for suggestion in suggestions:",
        "            self.assertEqual(len(suggestion), 2)",
        "            self.assertIsInstance(suggestion[0], str)  # term",
        "            self.assertIsInstance(suggestion[1], str)  # reason",
        "",
        "    def test_max_suggestions_respected(self):",
        "        \"\"\"Test that max_suggestions limit is respected.\"\"\"",
        "        cluster_docs = [\"ml1\"]",
        "        layer0 = self.processor.layers[CorticalLayer.TOKENS]",
        "        cluster_tokens = set()",
        "        for doc_id in cluster_docs:",
        "            for col in layer0.minicolumns.values():",
        "                if doc_id in col.document_ids:",
        "                    cluster_tokens.add(col.content)",
        "",
        "        suggestions = find_expansion_suggestions(",
        "            self.processor, cluster_tokens, cluster_docs, max_suggestions=2",
        "        )",
        "        self.assertLessEqual(len(suggestions), 2)",
        "",
        "",
        "class TestIntegration(unittest.TestCase):",
        "    \"\"\"Integration tests using the full workflow.\"\"\"",
        "",
        "    @classmethod",
        "    def setUpClass(cls):",
        "        \"\"\"Load showcase corpus for integration tests.\"\"\"",
        "        cls.processor = CorticalTextProcessor()",
        "        samples_dir = Path(__file__).parent.parent / 'samples'",
        "",
        "        if not samples_dir.exists():",
        "            cls.skip_tests = True",
        "            return",
        "",
        "        txt_files = list(samples_dir.glob('*.txt'))[:20]  # Use subset for speed",
        "        if len(txt_files) < 5:",
        "            cls.skip_tests = True",
        "            return",
        "",
        "        cls.skip_tests = False",
        "        for f in txt_files:",
        "            cls.processor.process_document(f.stem, f.read_text())",
        "",
        "        cls.processor.compute_all(verbose=False)",
        "",
        "    def setUp(self):",
        "        if getattr(self.__class__, 'skip_tests', False):",
        "            self.skipTest(\"Sample corpus not available\")",
        "",
        "    def test_keyword_search_finds_documents(self):",
        "        \"\"\"Test that keyword search finds relevant documents.\"\"\"",
        "        docs = find_documents_by_keywords(self.processor, [\"neural\", \"network\"])",
        "        # Should find at least one ML-related document",
        "        self.assertGreater(len(docs), 0)",
        "",
        "    def test_full_metrics_workflow(self):",
        "        \"\"\"Test the full metrics computation workflow.\"\"\"",
        "        docs = find_documents_by_keywords(self.processor, [\"learning\"], min_keywords=1)",
        "        if len(docs) < 2:",
        "            self.skipTest(\"Not enough matching documents\")",
        "",
        "        metrics = compute_cluster_metrics(self.processor, docs[:5])",
        "",
        "        # All metrics should be valid",
        "        self.assertGreaterEqual(metrics[\"cohesion\"], 0.0)",
        "        self.assertGreaterEqual(metrics[\"separation\"], 0.0)",
        "        self.assertGreater(metrics[\"term_count\"], 0)",
        "",
        "",
        "if __name__ == \"__main__\":",
        "    unittest.main()"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    }
  ],
  "hour_of_day": 23,
  "day_of_week": "Thursday",
  "seconds_since_last_commit": -309090,
  "is_merge": false,
  "is_initial": false,
  "parent_count": 1,
  "session_id": null,
  "related_chats": [],
  "ci_result": null,
  "reverted": false,
  "amended": false
}