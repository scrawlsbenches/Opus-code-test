{
  "hash": "cc5767736afe6a477e861f8e9afd43f0e61a5fbd",
  "message": "Add cross-layer PageRank propagation (Task 23)",
  "author": "Claude",
  "timestamp": "2025-12-10 00:03:39 +0000",
  "branch": "claude/multi-index-design-DvifZ",
  "files_changed": [
    "TASK_LIST.md",
    "cortical/analysis.py",
    "cortical/processor.py",
    "tests/test_processor.py"
  ],
  "insertions": 392,
  "deletions": 19,
  "hunks": [
    {
      "file": "TASK_LIST.md",
      "function": "print(f\"Found {stats['total_edges_with_relations']} semantic edges\")",
      "start_line": 708,
      "lines_added": [
        "**Files:** `cortical/analysis.py`, `cortical/processor.py`",
        "**Status:** [x] Completed",
        "**Solution Applied:**",
        "1. Added `compute_hierarchical_pagerank()` function to `analysis.py` (~150 lines):",
        "   - Computes local PageRank within each layer",
        "   - Propagates scores up via feedback_connections (tokens → bigrams → concepts → documents)",
        "   - Propagates scores down via feedforward_connections (documents → concepts → bigrams → tokens)",
        "   - Normalizes PageRank within each layer after propagation",
        "   - Converges when cross-layer changes are minimal",
        "2. Added `compute_hierarchical_importance()` method to processor",
        "3. Updated `compute_all()` with `pagerank_method='hierarchical'` option",
        "4. Returns detailed statistics: iterations_run, converged, per-layer stats",
        "**Files Modified:**",
        "- `cortical/analysis.py` - Added `compute_hierarchical_pagerank()` (~150 lines)",
        "- `cortical/processor.py` - Added `compute_hierarchical_importance()`, updated `compute_all()`",
        "- `tests/test_processor.py` - Added 9 tests for hierarchical PageRank",
        "",
        "**Usage:**",
        "```python",
        "# Use hierarchical PageRank via compute_all",
        "processor.compute_all(pagerank_method='hierarchical')",
        "",
        "# Or call directly with custom parameters",
        "stats = processor.compute_hierarchical_importance(",
        "    layer_iterations=10,      # Iterations for intra-layer PageRank",
        "    global_iterations=5,      # Iterations for cross-layer propagation",
        "    cross_layer_damping=0.7   # Damping at layer boundaries",
        ")",
        "print(f\"Converged: {stats['converged']} in {stats['iterations_run']} iterations\")",
        "for layer, info in stats['layer_stats'].items():",
        "    print(f\"  {layer}: {info['nodes']} nodes, max PR={info['max_pagerank']:.4f}\")"
      ],
      "lines_removed": [
        "**Files:** `cortical/analysis.py`",
        "**Status:** [ ] Pending",
        "**Implementation Steps:**",
        "1. Add `compute_hierarchical_pagerank()` function",
        "2. Iterate: compute layer-local PageRank, then propagate to adjacent layers",
        "3. Use `feedforward_connections` and `feedback_connections` for cross-layer flow",
        "4. Apply damping factor at layer boundaries (e.g., 0.7)",
        "5. Converge when cross-layer changes are minimal",
        "**Algorithm:**",
        "```",
        "for iteration in range(max_iterations):",
        "    for layer in [TOKENS, BIGRAMS, CONCEPTS, DOCUMENTS]:",
        "        compute_local_pagerank(layer)",
        "    propagate_up(TOKENS → BIGRAMS → CONCEPTS → DOCUMENTS)",
        "    propagate_down(DOCUMENTS → CONCEPTS → BIGRAMS → TOKENS)",
        "    if converged: break"
      ],
      "context_before": [
        "",
        "# Custom relation weights",
        "custom_weights = {'IsA': 2.0, 'CoOccurs': 0.5}",
        "processor.compute_semantic_importance(relation_weights=custom_weights)",
        "```",
        "",
        "---",
        "",
        "### 23. Implement Cross-Layer PageRank Propagation",
        ""
      ],
      "context_after": [
        "",
        "**Problem:**",
        "PageRank only flows within a single layer. Importance should propagate across layers:",
        "- Important tokens boost their bigrams",
        "- Important bigrams boost their concepts",
        "- Important concepts boost their documents (and vice versa)",
        "",
        "",
        "```",
        "",
        "---",
        "",
        "### 24. Add Typed Edge Storage",
        "",
        "**Files:** `cortical/minicolumn.py`, `cortical/analysis.py`",
        "**Status:** [ ] Pending",
        "",
        "**Problem:**"
      ],
      "change_type": "modify"
    },
    {
      "file": "TASK_LIST.md",
      "function": "Semantic bonus is capped at 50% boost (`min(avg_semantic, 0.5)`). This is a reas",
      "start_line": 978,
      "lines_added": [
        "| **High** | **Implement cross-layer PageRank propagation** | ✅ Completed | **ConceptNet** |",
        "**ConceptNet Enhancement Completion:** 5/12 tasks (42%)",
        "Ran 222 tests in 0.183s"
      ],
      "lines_removed": [
        "| **High** | **Implement cross-layer PageRank propagation** | ⏳ Pending | **ConceptNet** |",
        "**ConceptNet Enhancement Completion:** 4/12 tasks (33%)",
        "Ran 213 tests in 0.177s"
      ],
      "context_before": [
        "| Medium | Fix type annotation (embeddings.py) | ✅ Completed | Bug Fix |",
        "| Medium | Optimize spectral embeddings | ✅ Completed | Performance |",
        "| Medium | Add incremental indexing | ✅ Completed | RAG |",
        "| Low | Document magic numbers | ⏳ Deferred | Documentation |",
        "| Low | Multi-stage ranking pipeline | ✅ Completed | RAG |",
        "| Low | Batch query API | ✅ Completed | RAG |",
        "| **Critical** | **Build cross-layer feedforward connections** | ✅ Completed | **ConceptNet** |",
        "| **Critical** | **Add concept-level lateral connections** | ✅ Completed | **ConceptNet** |",
        "| **Critical** | **Add bigram lateral connections** | ✅ Completed | **ConceptNet** |",
        "| **High** | **Implement relation-weighted PageRank** | ✅ Completed | **ConceptNet** |"
      ],
      "context_after": [
        "| **High** | **Add typed edge storage** | ⏳ Pending | **ConceptNet** |",
        "| Medium | Implement multi-hop semantic inference | ⏳ Pending | ConceptNet |",
        "| Medium | Add relation path scoring | ⏳ Pending | ConceptNet |",
        "| Medium | Implement concept inheritance | ⏳ Pending | ConceptNet |",
        "| Low | Add commonsense relation extraction | ⏳ Pending | ConceptNet |",
        "| Low | Visualize ConceptNet-style graph | ⏳ Pending | ConceptNet |",
        "| Low | Add analogy completion | ⏳ Pending | ConceptNet |",
        "",
        "**Bug Fix Completion:** 7/7 tasks (100%)",
        "**RAG Enhancement Completion:** 8/8 tasks (100%)",
        "",
        "---",
        "",
        "## Test Results",
        "",
        "```",
        "OK",
        "```",
        "",
        "All tests passing as of 2025-12-09.",
        "",
        "---",
        "",
        "*Updated from code review on 2025-12-09*"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/analysis.py",
      "function": "def compute_semantic_pagerank(",
      "start_line": 216,
      "lines_added": [
        "def compute_hierarchical_pagerank(",
        "    layers: Dict[CorticalLayer, HierarchicalLayer],",
        "    layer_iterations: int = 10,",
        "    global_iterations: int = 5,",
        "    damping: float = 0.85,",
        "    cross_layer_damping: float = 0.7,",
        "    tolerance: float = 1e-4",
        ") -> Dict[str, Any]:",
        "    \"\"\"",
        "    Compute PageRank with cross-layer propagation.",
        "",
        "    This hierarchical PageRank allows importance to flow between layers:",
        "    - Upward: tokens → bigrams → concepts → documents",
        "    - Downward: documents → concepts → bigrams → tokens",
        "",
        "    The algorithm alternates between:",
        "    1. Computing local PageRank within each layer",
        "    2. Propagating scores up the hierarchy (via feedback_connections)",
        "    3. Propagating scores down the hierarchy (via feedforward_connections)",
        "",
        "    Args:",
        "        layers: Dictionary of all layers",
        "        layer_iterations: Max iterations for intra-layer PageRank",
        "        global_iterations: Max iterations for cross-layer propagation",
        "        damping: Damping factor for intra-layer PageRank",
        "        cross_layer_damping: Damping factor for cross-layer propagation (default 0.7)",
        "        tolerance: Convergence threshold for global iterations",
        "",
        "    Returns:",
        "        Dict containing:",
        "        - iterations_run: Number of global iterations",
        "        - converged: Whether the algorithm converged",
        "        - layer_stats: Per-layer statistics",
        "",
        "    Example:",
        "        >>> result = compute_hierarchical_pagerank(layers)",
        "        >>> print(f\"Converged in {result['iterations_run']} iterations\")",
        "    \"\"\"",
        "    # Define layer order for propagation",
        "    layer_order = [",
        "        CorticalLayer.TOKENS,",
        "        CorticalLayer.BIGRAMS,",
        "        CorticalLayer.CONCEPTS,",
        "        CorticalLayer.DOCUMENTS",
        "    ]",
        "",
        "    # Filter to only existing layers with minicolumns",
        "    active_layers = [l for l in layer_order if l in layers and layers[l].column_count() > 0]",
        "",
        "    if not active_layers:",
        "        return {'iterations_run': 0, 'converged': True, 'layer_stats': {}}",
        "",
        "    # Store previous PageRank values for convergence check",
        "    prev_pageranks: Dict[CorticalLayer, Dict[str, float]] = {}",
        "",
        "    iterations_run = 0",
        "    converged = False",
        "",
        "    for global_iter in range(global_iterations):",
        "        iterations_run = global_iter + 1",
        "        max_global_diff = 0.0",
        "",
        "        # Step 1: Compute local PageRank for each layer",
        "        for layer_enum in active_layers:",
        "            layer = layers[layer_enum]",
        "            compute_pagerank(layer, damping=damping, iterations=layer_iterations, tolerance=1e-6)",
        "",
        "        # Step 2: Propagate up (tokens → bigrams → concepts → documents)",
        "        for i in range(len(active_layers) - 1):",
        "            lower_layer_enum = active_layers[i]",
        "            upper_layer_enum = active_layers[i + 1]",
        "            lower_layer = layers[lower_layer_enum]",
        "            upper_layer = layers[upper_layer_enum]",
        "",
        "            # Propagate from lower to upper via feedback connections",
        "            for col in lower_layer.minicolumns.values():",
        "                if not col.feedback_connections:",
        "                    continue",
        "",
        "                for target_id, weight in col.feedback_connections.items():",
        "                    target = upper_layer.get_by_id(target_id)",
        "                    if target:",
        "                        # Boost upper layer node based on lower layer importance",
        "                        boost = col.pagerank * weight * cross_layer_damping",
        "                        target.pagerank += boost",
        "",
        "        # Step 3: Propagate down (documents → concepts → bigrams → tokens)",
        "        for i in range(len(active_layers) - 1, 0, -1):",
        "            upper_layer_enum = active_layers[i]",
        "            lower_layer_enum = active_layers[i - 1]",
        "            upper_layer = layers[upper_layer_enum]",
        "            lower_layer = layers[lower_layer_enum]",
        "",
        "            # Propagate from upper to lower via feedforward connections",
        "            for col in upper_layer.minicolumns.values():",
        "                if not col.feedforward_connections:",
        "                    continue",
        "",
        "                for target_id, weight in col.feedforward_connections.items():",
        "                    target = lower_layer.get_by_id(target_id)",
        "                    if target:",
        "                        # Boost lower layer node based on upper layer importance",
        "                        boost = col.pagerank * weight * cross_layer_damping",
        "                        target.pagerank += boost",
        "",
        "        # Normalize PageRank within each layer",
        "        for layer_enum in active_layers:",
        "            layer = layers[layer_enum]",
        "            total = sum(col.pagerank for col in layer.minicolumns.values())",
        "            if total > 0:",
        "                for col in layer.minicolumns.values():",
        "                    col.pagerank /= total",
        "",
        "        # Check convergence",
        "        for layer_enum in active_layers:",
        "            layer = layers[layer_enum]",
        "            current_pr = {col.id: col.pagerank for col in layer.minicolumns.values()}",
        "",
        "            if layer_enum in prev_pageranks:",
        "                for col_id, pr in current_pr.items():",
        "                    prev_pr = prev_pageranks[layer_enum].get(col_id, 0)",
        "                    max_global_diff = max(max_global_diff, abs(pr - prev_pr))",
        "",
        "            prev_pageranks[layer_enum] = current_pr",
        "",
        "        if max_global_diff < tolerance and global_iter > 0:",
        "            converged = True",
        "            break",
        "",
        "    # Collect layer statistics",
        "    layer_stats = {}",
        "    for layer_enum in active_layers:",
        "        layer = layers[layer_enum]",
        "        pageranks = [col.pagerank for col in layer.minicolumns.values()]",
        "        layer_stats[layer_enum.name] = {",
        "            'nodes': len(pageranks),",
        "            'max_pagerank': max(pageranks) if pageranks else 0,",
        "            'min_pagerank': min(pageranks) if pageranks else 0,",
        "            'avg_pagerank': sum(pageranks) / len(pageranks) if pageranks else 0",
        "        }",
        "",
        "    return {",
        "        'iterations_run': iterations_run,",
        "        'converged': converged,",
        "        'layer_stats': layer_stats",
        "    }",
        "",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "    for col in layer.minicolumns.values():",
        "        col.pagerank = pagerank.get(col.id, 1.0 / n)",
        "",
        "    return {",
        "        'pagerank': pagerank,",
        "        'iterations_run': iterations_run,",
        "        'edges_with_relations': edges_with_relations",
        "    }",
        "",
        ""
      ],
      "context_after": [
        "def compute_tfidf(",
        "    layers: Dict[CorticalLayer, HierarchicalLayer],",
        "    documents: Dict[str, str]",
        ") -> None:",
        "    \"\"\"",
        "    Compute TF-IDF scores for tokens.",
        "    ",
        "    TF-IDF (Term Frequency - Inverse Document Frequency) measures",
        "    how distinctive a term is to the corpus. High TF-IDF terms are",
        "    both frequent in their documents and rare across the corpus."
      ],
      "change_type": "add"
    },
    {
      "file": "cortical/processor.py",
      "function": "class CorticalTextProcessor:",
      "start_line": 419,
      "lines_added": [
        "                - 'hierarchical': Cross-layer PageRank with importance propagation",
        "                                  between layers (tokens ↔ bigrams ↔ concepts ↔ documents).",
        "        elif pagerank_method == 'hierarchical':",
        "            if verbose:",
        "                print(\"Computing importance (Hierarchical PageRank)...\")",
        "            self.compute_hierarchical_importance(verbose=False)"
      ],
      "lines_removed": [],
      "context_before": [
        "        Run all computation steps.",
        "",
        "        Args:",
        "            verbose: Print progress messages",
        "            build_concepts: Build concept clusters in Layer 2 (default True)",
        "                           This enables topic-based filtering and hierarchical search.",
        "            pagerank_method: PageRank algorithm to use:",
        "                - 'standard': Traditional PageRank using connection weights",
        "                - 'semantic': ConceptNet-style PageRank with relation type weighting.",
        "                              Requires semantic relations (extracts automatically if needed)."
      ],
      "context_after": [
        "        \"\"\"",
        "        if verbose:",
        "            print(\"Computing activation propagation...\")",
        "        self.propagate_activation(verbose=False)",
        "",
        "        if pagerank_method == 'semantic':",
        "            # Extract semantic relations if not already done",
        "            if not self.semantic_relations:",
        "                if verbose:",
        "                    print(\"Extracting semantic relations...\")",
        "                self.extract_corpus_semantics(verbose=False)",
        "            if verbose:",
        "                print(\"Computing importance (Semantic PageRank)...\")",
        "            self.compute_semantic_importance(verbose=False)",
        "        else:",
        "            if verbose:",
        "                print(\"Computing importance (PageRank)...\")",
        "            self.compute_importance(verbose=False)",
        "        if verbose:",
        "            print(\"Computing TF-IDF...\")",
        "        self.compute_tfidf(verbose=False)",
        "        if verbose:",
        "            print(\"Computing document connections...\")",
        "        self.compute_document_connections(verbose=False)"
      ],
      "change_type": "add"
    },
    {
      "file": "cortical/processor.py",
      "function": "class CorticalTextProcessor:",
      "start_line": 541,
      "lines_added": [
        "    def compute_hierarchical_importance(",
        "        self,",
        "        layer_iterations: int = 10,",
        "        global_iterations: int = 5,",
        "        cross_layer_damping: float = 0.7,",
        "        verbose: bool = True",
        "    ) -> Dict[str, Any]:",
        "        \"\"\"",
        "        Compute PageRank with cross-layer propagation.",
        "",
        "        This hierarchical PageRank allows importance to flow between layers:",
        "        - Upward: tokens → bigrams → concepts → documents",
        "        - Downward: documents → concepts → bigrams → tokens",
        "",
        "        Important tokens boost their containing bigrams and concepts.",
        "        Important documents boost their contained terms. This creates",
        "        a more holistic importance score that considers the full hierarchy.",
        "",
        "        Args:",
        "            layer_iterations: Max iterations for intra-layer PageRank (default 10)",
        "            global_iterations: Max iterations for cross-layer propagation (default 5)",
        "            cross_layer_damping: Damping factor at layer boundaries (default 0.7)",
        "            verbose: Print progress messages",
        "",
        "        Returns:",
        "            Dict with statistics:",
        "            - iterations_run: Number of global iterations",
        "            - converged: Whether the algorithm converged",
        "            - layer_stats: Per-layer statistics (nodes, max/min/avg PageRank)",
        "",
        "        Example:",
        "            >>> stats = processor.compute_hierarchical_importance()",
        "            >>> print(f\"Converged: {stats['converged']}\")",
        "            >>> for layer, info in stats['layer_stats'].items():",
        "            ...     print(f\"{layer}: {info['nodes']} nodes, max PR={info['max_pagerank']:.4f}\")",
        "        \"\"\"",
        "        result = analysis.compute_hierarchical_pagerank(",
        "            self.layers,",
        "            layer_iterations=layer_iterations,",
        "            global_iterations=global_iterations,",
        "            cross_layer_damping=cross_layer_damping",
        "        )",
        "",
        "        if verbose:",
        "            status = \"converged\" if result['converged'] else \"did not converge\"",
        "            print(f\"Computed hierarchical PageRank ({result['iterations_run']} iterations, {status})\")",
        "",
        "        return result",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "            total_edges += result['edges_with_relations']",
        "",
        "        if verbose:",
        "            print(f\"Computed semantic PageRank ({total_edges} relation-weighted edges)\")",
        "",
        "        return {",
        "            'total_edges_with_relations': total_edges,",
        "            **layer_stats",
        "        }",
        ""
      ],
      "context_after": [
        "    def compute_tfidf(self, verbose: bool = True) -> None:",
        "        analysis.compute_tfidf(self.layers, self.documents)",
        "        if verbose: print(\"Computed TF-IDF scores\")",
        "    ",
        "    def compute_document_connections(self, min_shared_terms: int = 3, verbose: bool = True) -> None:",
        "        analysis.compute_document_connections(self.layers, self.documents, min_shared_terms)",
        "        if verbose: print(\"Computed document connections\")",
        "",
        "    def compute_bigram_connections(",
        "        self,"
      ],
      "change_type": "add"
    },
    {
      "file": "tests/test_processor.py",
      "function": "class TestSemanticPageRank(unittest.TestCase):",
      "start_line": 1488,
      "lines_added": [
        "class TestHierarchicalPageRank(unittest.TestCase):",
        "    \"\"\"Test hierarchical (cross-layer) PageRank functionality.\"\"\"",
        "",
        "    @classmethod",
        "    def setUpClass(cls):",
        "        \"\"\"Set up processor with documents for hierarchical PageRank testing.\"\"\"",
        "        cls.processor = CorticalTextProcessor()",
        "        cls.processor.process_document(",
        "            \"doc1\",",
        "            \"Neural networks are powerful machine learning models. \"",
        "            \"Deep learning uses neural networks for complex tasks.\"",
        "        )",
        "        cls.processor.process_document(",
        "            \"doc2\",",
        "            \"Machine learning algorithms process data patterns. \"",
        "            \"Neural networks learn from examples effectively.\"",
        "        )",
        "        cls.processor.process_document(",
        "            \"doc3\",",
        "            \"Deep learning is part of artificial intelligence. \"",
        "            \"Machine learning models improve with more data.\"",
        "        )",
        "        cls.processor.compute_all(verbose=False, build_concepts=True)",
        "",
        "    def test_compute_hierarchical_importance_returns_stats(self):",
        "        \"\"\"Test that compute_hierarchical_importance returns expected statistics.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(\"doc1\", \"Neural networks process data efficiently.\")",
        "        processor.compute_all(verbose=False)",
        "",
        "        stats = processor.compute_hierarchical_importance(verbose=False)",
        "",
        "        self.assertIn('iterations_run', stats)",
        "        self.assertIn('converged', stats)",
        "        self.assertIn('layer_stats', stats)",
        "",
        "    def test_hierarchical_pagerank_layer_stats(self):",
        "        \"\"\"Test that layer stats contain expected fields.\"\"\"",
        "        stats = self.processor.compute_hierarchical_importance(verbose=False)",
        "",
        "        for layer_name, layer_info in stats['layer_stats'].items():",
        "            self.assertIn('nodes', layer_info)",
        "            self.assertIn('max_pagerank', layer_info)",
        "            self.assertIn('min_pagerank', layer_info)",
        "            self.assertIn('avg_pagerank', layer_info)",
        "",
        "    def test_hierarchical_pagerank_convergence(self):",
        "        \"\"\"Test that hierarchical PageRank converges.\"\"\"",
        "        stats = self.processor.compute_hierarchical_importance(",
        "            global_iterations=10,",
        "            verbose=False",
        "        )",
        "",
        "        # Should run at least one iteration",
        "        self.assertGreaterEqual(stats['iterations_run'], 1)",
        "",
        "    def test_hierarchical_pagerank_affects_scores(self):",
        "        \"\"\"Test that hierarchical PageRank updates scores across layers.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(",
        "            \"doc1\",",
        "            \"Neural networks process information. Machine learning improves.\"",
        "        )",
        "        processor.compute_all(verbose=False, build_concepts=True)",
        "",
        "        # Get scores before hierarchical",
        "        layer0 = processor.get_layer(CorticalLayer.TOKENS)",
        "        before_scores = {col.content: col.pagerank for col in layer0.minicolumns.values()}",
        "",
        "        # Run hierarchical PageRank",
        "        processor.compute_hierarchical_importance(verbose=False)",
        "",
        "        # Scores should be updated (normalized to sum to 1)",
        "        after_scores = {col.content: col.pagerank for col in layer0.minicolumns.values()}",
        "",
        "        # Verify scores are valid probabilities",
        "        total = sum(after_scores.values())",
        "        self.assertAlmostEqual(total, 1.0, places=5)",
        "",
        "    def test_compute_all_with_hierarchical_pagerank(self):",
        "        \"\"\"Test compute_all with pagerank_method='hierarchical'.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(",
        "            \"doc1\",",
        "            \"Neural networks process information efficiently.\"",
        "        )",
        "",
        "        # Should work without errors",
        "        processor.compute_all(verbose=False, pagerank_method='hierarchical')",
        "",
        "        # Verify computations ran",
        "        self.assertFalse(processor.is_stale(processor.COMP_PAGERANK))",
        "        self.assertFalse(processor.is_stale(processor.COMP_TFIDF))",
        "",
        "    def test_hierarchical_empty_layers(self):",
        "        \"\"\"Test hierarchical PageRank handles empty layers gracefully.\"\"\"",
        "        from cortical.analysis import compute_hierarchical_pagerank",
        "        from cortical.layers import HierarchicalLayer, CorticalLayer",
        "",
        "        # Create empty layers dict",
        "        layers = {",
        "            CorticalLayer.TOKENS: HierarchicalLayer(CorticalLayer.TOKENS),",
        "            CorticalLayer.BIGRAMS: HierarchicalLayer(CorticalLayer.BIGRAMS),",
        "        }",
        "",
        "        result = compute_hierarchical_pagerank(layers)",
        "",
        "        self.assertEqual(result['iterations_run'], 0)",
        "        self.assertTrue(result['converged'])",
        "        self.assertEqual(result['layer_stats'], {})",
        "",
        "    def test_cross_layer_damping(self):",
        "        \"\"\"Test that cross-layer damping parameter affects propagation.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(",
        "            \"doc1\",",
        "            \"Neural networks learn from data patterns.\"",
        "        )",
        "        processor.compute_all(verbose=False, build_concepts=True)",
        "",
        "        # Run with different damping values",
        "        stats_low = processor.compute_hierarchical_importance(",
        "            cross_layer_damping=0.3,",
        "            verbose=False",
        "        )",
        "        stats_high = processor.compute_hierarchical_importance(",
        "            cross_layer_damping=0.9,",
        "            verbose=False",
        "        )",
        "",
        "        # Both should produce valid results",
        "        self.assertIsNotNone(stats_low)",
        "        self.assertIsNotNone(stats_high)",
        "",
        "    def test_hierarchical_with_concepts(self):",
        "        \"\"\"Test hierarchical PageRank includes concept layer.\"\"\"",
        "        stats = self.processor.compute_hierarchical_importance(verbose=False)",
        "",
        "        # Should include CONCEPTS layer if it has nodes",
        "        layer2 = self.processor.get_layer(CorticalLayer.CONCEPTS)",
        "        if layer2.column_count() > 0:",
        "            self.assertIn('CONCEPTS', stats['layer_stats'])",
        "",
        "    def test_feedforward_feedback_connections_used(self):",
        "        \"\"\"Test that cross-layer connections are used in propagation.\"\"\"",
        "        # Verify that tokens have feedback connections (to bigrams)",
        "        layer0 = self.processor.get_layer(CorticalLayer.TOKENS)",
        "",
        "        has_feedback = any(",
        "            col.feedback_connections",
        "            for col in layer0.minicolumns.values()",
        "        )",
        "        self.assertTrue(has_feedback, \"Tokens should have feedback connections to bigrams\")",
        "",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "    def test_relation_weights_applied(self):",
        "        \"\"\"Test that different relation types get different weights.\"\"\"",
        "        from cortical.analysis import RELATION_WEIGHTS",
        "",
        "        # Verify key relations have expected relative weights",
        "        self.assertGreater(RELATION_WEIGHTS['IsA'], RELATION_WEIGHTS['RelatedTo'])",
        "        self.assertGreater(RELATION_WEIGHTS['PartOf'], RELATION_WEIGHTS['CoOccurs'])",
        "        self.assertLess(RELATION_WEIGHTS['Antonym'], RELATION_WEIGHTS['RelatedTo'])",
        "",
        ""
      ],
      "context_after": [
        "if __name__ == \"__main__\":",
        "    unittest.main(verbosity=2)"
      ],
      "change_type": "add"
    }
  ],
  "hour_of_day": 0,
  "day_of_week": "Wednesday",
  "seconds_since_last_commit": -481269,
  "is_merge": false,
  "is_initial": false,
  "parent_count": 1,
  "session_id": null,
  "related_chats": [],
  "ci_result": null,
  "reverted": false,
  "amended": false
}