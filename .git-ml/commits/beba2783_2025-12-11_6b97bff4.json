{
  "hash": "beba27835dac427ae34f309a497e062d92c3b44a",
  "message": "Merge pull request #30 from scrawlsbenches/claude/showcase-and-tasks-019ttzM4aCE59rpdc8Snfq2Q",
  "author": "scrawlsbenches",
  "timestamp": "2025-12-11 05:24:20 -0500",
  "branch": "claude/multi-index-design-DvifZ",
  "files_changed": [
    "TASK_LIST.md",
    "cortical/processor.py",
    "cortical/query.py",
    "samples/data_processor.py",
    "samples/search_engine.py",
    "samples/test_data_processor.py",
    "showcase.py",
    "tests/test_query.py"
  ],
  "insertions": 2079,
  "deletions": 87,
  "hunks": [
    {
      "file": "TASK_LIST.md",
      "function": "def get_doc_type(doc_id: str) -> str:",
      "start_line": 2315,
      "lines_added": [
        "**Files:** `cortical/query.py`, `cortical/processor.py`, `tests/test_query.py`",
        "**Status:** [x] Completed (2025-12-11)",
        "**Solution Applied:**",
        "1. Added `apply_doc_boost` parameter to `find_passages_for_query()` (default True)",
        "2. Added `auto_detect_intent` parameter to auto-boost docs for conceptual queries (default True)",
        "3. Added `prefer_docs` parameter to always boost documentation (default False)",
        "4. Added `custom_boosts` parameter for custom boost factors",
        "5. Passage scores are multiplied by doc-type boost factor when appropriate",
        "6. Definition passages also receive doc-type boost",
        "7. Added processor wrappers with same parameters",
        "**Files Modified:**",
        "- `cortical/query.py` - Extended find_passages_for_query with boost parameters",
        "- `cortical/processor.py` - Updated processor wrapper",
        "- `tests/test_query.py` - Added 6 new tests",
        "**Usage:**",
        "# Auto-detect conceptual queries and boost docs (default)",
        "results = processor.find_passages_for_query(\"what is PageRank algorithm\")",
        "",
        "# Force docs preference",
        "results = processor.find_passages_for_query(\"PageRank\", prefer_docs=True)",
        "# Disable boosting (raw TF-IDF)",
        "results = processor.find_passages_for_query(\"PageRank\", apply_doc_boost=False)",
        "# Custom boost factors",
        "results = processor.find_passages_for_query(",
        "    \"query\",",
        "    custom_boosts={'docs': 2.0, 'code': 0.8, 'test': 0.5}",
        ")",
        "| 66 | Medium | Add doc-type boost to passage search | [x] Completed | Search Quality |"
      ],
      "lines_removed": [
        "**Files:** `cortical/query.py`, `scripts/search_codebase.py`",
        "**Status:** [ ] Not Started",
        "**Evidence (2025-12-11 dog-fooding):**",
        "```",
        "# Document-level search correctly ranks docs:",
        "TASK_LIST.md: 9.837 (root_docs)",
        "CLAUDE.md: 8.146 (root_docs)  ← Documentation found",
        "",
        "# But passage search returns code first:",
        "[1] [CODE] cortical/chunk_index.py:291 Score: 2.549",
        "[2] [TEST] tests/test_chunk_indexing.py:77 Score: 2.157",
        "# CLAUDE.md \"Chunk Compaction\" section not in top 5",
        "```",
        "**Solution:**",
        "Propagate doc-type boost to passage scoring:",
        "1. After chunking documents, apply `get_doc_type_boost()` to passage scores",
        "2. For conceptual queries (`is_conceptual_query()`), multiply passage score by doc-type boost",
        "3. Re-rank passages after boosting",
        "**Implementation Sketch:**",
        "def find_passages_for_query(..., apply_doc_boost: bool = True):",
        "    # ... existing passage retrieval ...",
        "    if apply_doc_boost and is_conceptual_query(query_text):",
        "        boosted_passages = []",
        "        for passage, doc_id, start, end, score in passages:",
        "            boost = get_doc_type_boost(doc_id, doc_metadata)",
        "            boosted_passages.append((passage, doc_id, start, end, score * boost))",
        "        passages = sorted(boosted_passages, key=lambda x: -x[4])",
        "    return passages[:top_n]",
        "| 66 | Medium | Add doc-type boost to passage search | [ ] Not Started | Search Quality |"
      ],
      "context_before": [
        "    elif doc_id.startswith('tests/'):",
        "        return 'TEST'",
        "    else:",
        "        return 'CODE'",
        "```",
        "",
        "---",
        "",
        "### 66. Add Doc-Type Boosting to Passage-Level Search",
        ""
      ],
      "context_after": [
        "**Priority:** Medium",
        "",
        "**Problem:**",
        "Document-level search correctly applies doc-type boosting (CLAUDE.md ranks #3 for \"chunk compaction\"), but passage-level search (`find_passages_for_query`) returns raw TF-IDF scores without boosting. This causes code snippets with keyword matches to rank higher than documentation passages for conceptual queries.",
        "",
        "",
        "",
        "```python",
        "",
        "",
        "```",
        "",
        "---",
        "",
        "## Dog-Fooding Summary",
        "",
        "| # | Priority | Task | Status | Category |",
        "|---|----------|------|--------|----------|",
        "| 65 | High | Add document metadata to chunk indexing | [x] Completed | Infrastructure |",
        "| 63 | High | Improve search ranking for docs | [x] Completed | Search Quality |",
        "| 64 | Low | Add document type indicator | [x] Completed | UX |",
        "",
        "**Dependency Chain:** #65 → #63 → #64 (all complete), #66 extends this work",
        "",
        "**Status Update (2025-12-11):**",
        "- Document-level search now correctly boosts documentation for conceptual queries",
        "- Passage-level search still needs boosting (#66) - docs found but code ranks higher",
        "",
        "---",
        "",
        "## Code Review Summary (PR #23)"
      ],
      "change_type": "modify"
    },
    {
      "file": "TASK_LIST.md",
      "function": "def find_passages_for_query(..., apply_doc_boost: bool = True):",
      "start_line": 2393,
      "lines_added": [
        "| 66 | Medium | Add doc-type boost to passage search | [x] Completed | Search Quality |"
      ],
      "lines_removed": [
        "| 66 | Medium | Add doc-type boost to passage search | [ ] Not Started | Search Quality |"
      ],
      "context_before": [
        "**Test Results:** 691 tests passing (including 32 new tests)",
        "",
        "---",
        "",
        "## Actionable Tasks Summary (Updated 2025-12-11)",
        "",
        "| # | Priority | Task | Status | Category |",
        "|---|----------|------|--------|----------|",
        "| 41 | Medium | Create Configuration Dataclass | [x] Completed | Code Quality |",
        "| 56 | Medium | Create Usage Patterns Documentation | [x] Completed | Documentation |"
      ],
      "context_after": [
        "| 42 | Low | Add Simple Query Language Support | [ ] Not Started | Feature |",
        "| 44 | Low | Remove Deprecated feedforward_sources | [ ] Not Started | Code Quality |",
        "| 46 | Low | Standardize Return Types with Dataclasses | [ ] Not Started | Code Quality |",
        "",
        "---",
        "",
        "*Updated 2025-12-11*",
        "",
        "---",
        ""
      ],
      "change_type": "modify"
    },
    {
      "file": "TASK_LIST.md",
      "function": "raw_tokens = re.findall(r'\\b[a-zA-Z][a-zA-Z0-9_]*\\b', text)",
      "start_line": 2918,
      "lines_added": [
        "**Files:** `cortical/query.py`, `cortical/processor.py`, `tests/test_query.py`",
        "**Status:** [x] Completed (2025-12-11)",
        "**Solution Applied:**",
        "1. Added `is_definition_query()` function to detect definition queries (class/def/function/method patterns)",
        "2. Added `find_definition_in_text()` to search for definition patterns in source code",
        "3. Added `find_definition_passages()` to extract high-scoring passages from definitions",
        "4. Updated `find_passages_for_query()` with `use_definition_search` parameter (default True)",
        "5. Definition passages are injected with high boost score (5.0) before regular passages",
        "6. Test files receive a penalty (0.6x) so source files rank higher",
        "7. Added processor wrapper methods: `is_definition_query()`, `find_definition_passages()`",
        "**Files Modified:**",
        "- `cortical/query.py` - Added definition search functions (~160 lines)",
        "- `cortical/processor.py` - Added processor wrappers (~65 lines)",
        "- `tests/test_query.py` - Added 19 new tests",
        "",
        "**Usage:**",
        "# Definition search is enabled by default",
        "results = processor.find_passages_for_query(\"class Minicolumn\")",
        "# Returns passage containing actual class definition first",
        "",
        "# Disable if needed",
        "results = processor.find_passages_for_query(\"class Minicolumn\", use_definition_search=False)",
        "",
        "# Check if query is a definition query",
        "is_def, def_type, name = processor.is_definition_query(\"def compute_pagerank\")",
        "# (True, 'function', 'compute_pagerank')",
        "**Files:** `cortical/query.py`",
        "**Status:** [x] Completed (2025-12-11) - Addressed by Tasks #84 and #66",
        "**Solution Applied:**",
        "This is now addressed by two mechanisms:",
        "",
        "1. **Definition search (Task #84)** applies 0.6x penalty to test files for definition queries like \"class Minicolumn\" and \"def compute_pagerank\"",
        "2. **Doc-type boost (Task #66)** applies 0.8x boost to test files via `DOC_TYPE_BOOSTS`. This is applied for:",
        "   - Conceptual queries (auto-detected when `auto_detect_intent=True`)",
        "   - When `prefer_docs=True` is set",
        "   - Custom boosts via `custom_boosts` parameter",
        "",
        "**Test file detection:**",
        "```python",
        "is_test = (doc_id.startswith('tests/') or '_test' in doc_id or 'test_' in doc_id)",
        "```",
        "",
        "**Result:**",
        "For \"class Minicolumn\" queries, the actual class definition in `minicolumn.py` now ranks higher than test mentions in `test_layers.py`",
        "**Files:** `cortical/query.py`, `cortical/processor.py`, `tests/test_query.py`",
        "**Status:** [x] Completed (2025-12-11)",
        "**Solution Applied:**",
        "1. Added `find_code_boundaries()` to detect class/function/decorator boundaries",
        "2. Added `create_code_aware_chunks()` to split at semantic boundaries",
        "3. Added `is_code_file()` to detect code files by extension",
        "4. Added `use_code_aware_chunks` parameter to `find_passages_for_query()` (default True)",
        "5. Code files automatically use semantic chunking, other files use fixed chunking",
        "",
        "**Files Modified:**",
        "- `cortical/query.py` - Added code-aware chunking functions (~150 lines)",
        "- `cortical/processor.py` - Updated processor wrapper",
        "- `tests/test_query.py` - Added 15 new tests",
        "",
        "**Supported boundaries:**",
        "- Class definitions (`class Foo:`)",
        "- Function/method definitions (`def bar():`, `async def baz():`)",
        "- Decorators (`@decorator`)",
        "- Comment separators (`# ---` or `# ===`)",
        "- Blank line sequences",
        "**Usage:**",
        "# Enabled by default for code files",
        "results = processor.find_passages_for_query(\"class Minicolumn\")",
        "",
        "# Disable if needed",
        "results = processor.find_passages_for_query(\"class Minicolumn\", use_code_aware_chunks=False)",
        "### 87. Add Python Code Samples and Update Showcase for Code Search",
        "",
        "**Files:** `showcase.py`, `samples/data_processor.py`, `samples/search_engine.py`, `samples/test_data_processor.py`",
        "**Status:** [ ] In Progress (2025-12-11)",
        "**Priority:** Medium",
        "**Category:** Showcase",
        "",
        "**Problem:**",
        "The showcase.py demonstrates code search features but the `samples/` directory only contains prose text files (`.txt`). The new code search features from Tasks #84, #66, and #86 need actual Python code to demonstrate properly:",
        "- Definition search needs actual `class` and `def` statements to find",
        "- Doc-type boosting needs test files vs source files to show scoring differences",
        "- Code-aware chunking needs code files to show semantic boundary detection",
        "",
        "**Solution Applied:**",
        "1. Added Python code samples to `samples/`:",
        "   - `data_processor.py` - DataRecord class, DataProcessor class, utility functions",
        "   - `search_engine.py` - SearchIndex class, QueryParser class, BM25 scoring",
        "   - `test_data_processor.py` - Unit tests (for test file penalty demonstration)",
        "",
        "2. Updated `showcase.py`:",
        "   - Load `.py` files alongside `.txt` files",
        "   - Add definition search demo (find \"class DataProcessor\", \"def calculate_statistics\")",
        "   - Add doc-type boosting demo (compare scores with/without boost)",
        "   - Add code-aware chunking demo (compare regular vs semantic chunks)",
        "",
        "**Files Modified:**",
        "- `showcase.py` - Load .py files, add new feature demos",
        "- `samples/data_processor.py` - New sample (570 words)",
        "- `samples/search_engine.py` - New sample (819 words)",
        "- `samples/test_data_processor.py` - New sample (611 words)",
        "",
        "---",
        ""
      ],
      "lines_removed": [
        "**Files:** `cortical/query.py`, `scripts/search_codebase.py`",
        "**Status:** [ ] Not Started",
        "**Solution:**",
        "1. For definition queries, directly search source files for the definition pattern",
        "2. Create a synthetic high-scoring passage from the definition location",
        "3. Inject this passage into results before final ranking",
        "4. Consider using regex-based chunk extraction around definition sites",
        "**Example:**",
        "def find_definition_passage(doc_text: str, pattern: str, context_chars: int = 500):",
        "    \"\"\"Extract passage around a definition pattern match.\"\"\"",
        "    match = re.search(pattern, doc_text)",
        "    if match:",
        "        start = max(0, match.start() - 50)",
        "        end = min(len(doc_text), match.end() + context_chars)",
        "        return doc_text[start:end], start, end",
        "    return None",
        "**Files:** `cortical/query.py`, `scripts/search_codebase.py`",
        "**Status:** [ ] Not Started",
        "**Solution Options:**",
        "1. Add file path-based scoring penalty for test files when query intent is \"implementation\"",
        "2. Detect test files (tests/, *_test.py, test_*.py) and apply negative boost",
        "3. Add user preference for \"prefer source over tests\" in search",
        "4. Use query intent detection to auto-adjust (implementation queries → penalize tests)",
        "**Considerations:**",
        "- Should be configurable (sometimes users want to find tests)",
        "- Could integrate with existing `get_doc_type_boost()` function",
        "- Balance: tests are valuable for understanding usage patterns",
        "**Files:** `cortical/query.py`",
        "**Status:** [ ] Not Started",
        "**Solution:**",
        "1. Detect code structure boundaries (class, def, blank lines)",
        "2. Adjust chunk boundaries to align with semantic units",
        "3. For Python: use indentation changes as boundary hints",
        "4. Consider AST-based chunking for supported languages",
        "**Example improvement:**",
        "def create_code_aware_chunks(text: str, target_size: int = 500):",
        "    \"\"\"Create chunks aligned to code structure boundaries.\"\"\"",
        "    # Find all class/def boundaries",
        "    boundaries = [m.start() for m in re.finditer(r'^(?:class |def )', text, re.M)]",
        "    # Create chunks that start at boundaries",
        "    ..."
      ],
      "context_before": [
        "- Affects private methods, dunder methods, internal variables",
        "",
        "**Files to Modify:**",
        "- `cortical/tokenizer.py` - Fix regex pattern",
        "- `tests/test_tokenizer.py` - Add tests for underscore identifiers",
        "",
        "---",
        "",
        "### 84. Add Direct Definition Pattern Search for Code Search",
        ""
      ],
      "context_after": [
        "**Priority:** High",
        "**Category:** Code Search",
        "",
        "**Problem:**",
        "When searching for \"class Minicolumn\", the passage containing the actual class definition (`class Minicolumn:`) scores low because TF-IDF favors chunks with more query term matches. The definition chunk is mostly docstring text with few matching terms.",
        "",
        "**Found via dog-fooding:** Even with document-level boosting, the actual class definition often doesn't appear in top results.",
        "",
        "",
        "```python",
        "```",
        "",
        "---",
        "",
        "### 85. Improve Test File vs Source File Ranking",
        "",
        "**Priority:** Medium",
        "**Category:** Code Search",
        "",
        "**Problem:**",
        "Test files often rank higher than source files because they mention class/function names more frequently (in test method names, assertions, etc.). This is counterintuitive for users looking for implementations.",
        "",
        "**Found via dog-fooding:** Searching \"class Minicolumn\" returns test_layers.py results before minicolumn.py results.",
        "",
        "",
        "",
        "---",
        "",
        "### 86. Add Semantic Chunk Boundaries for Code",
        "",
        "**Priority:** Medium",
        "**Category:** Code Search",
        "",
        "**Problem:**",
        "Current chunking uses fixed character boundaries which can split code mid-function or mid-class. This creates passages that lack context and score poorly.",
        "",
        "**Found via dog-fooding:** The chunk containing `class Minicolumn:` starts mid-way through the previous function's code.",
        "",
        "",
        "```python",
        "```",
        "",
        "---",
        "",
        "## Summary Table",
        "",
        "| # | Priority | Task | Status | Category |",
        "|---|----------|------|--------|----------|",
        "| 67 | Low | Fix O(n) lookup in showcase | ✓ Done | Showcase |",
        "| 68 | Medium | Add code-specific features to showcase | ✓ Done | Showcase |",
        "| 69 | Medium | Add passage-level search demo | ✓ Done | Showcase |",
        "| 70 | Low | Add performance timing to showcase | ✓ Done | Showcase |",
        "| 71 | High | Enable code-aware tokenization in index | ✓ Done | Code Index |",
        "| 72 | High | Use programming query expansion in search | ✓ Done | Code Index |"
      ],
      "change_type": "modify"
    },
    {
      "file": "TASK_LIST.md",
      "function": "def create_code_aware_chunks(text: str, target_size: int = 500):",
      "start_line": 3024,
      "lines_added": [
        "| 84 | High | Add direct definition pattern search | ✓ Done | Code Search |",
        "| 85 | Medium | Improve test file vs source file ranking | ✓ Done | Code Search |",
        "| 86 | Medium | Add semantic chunk boundaries for code | ✓ Done | Code Search |",
        "| 87 | Medium | Add Python code samples and update showcase | In Progress | Showcase |"
      ],
      "lines_removed": [
        "| 84 | High | Add direct definition pattern search | | Code Search |",
        "| 85 | Medium | Improve test file vs source file ranking | | Code Search |",
        "| 86 | Medium | Add semantic chunk boundaries for code | | Code Search |"
      ],
      "context_before": [
        "| 74 | Medium | Add \"Explain This Code\" command | | Developer Experience |",
        "| 75 | Medium | Add \"What Changed?\" semantic diff | | Developer Experience |",
        "| 76 | Medium | Add \"Suggest Related Files\" feature | | Developer Experience |",
        "| 77 | High | Add interactive \"Ask the Codebase\" mode | ✓ Done | Developer Experience |",
        "| 78 | Low | Add code pattern detection | | Developer Experience |",
        "| 79 | Low | Add corpus health dashboard | | Developer Experience |",
        "| 80 | Low | Add \"Learning Mode\" for new contributors | | Developer Experience |",
        "| 81 | High | Fix tokenizer underscore-prefixed identifiers | ✓ Done | Code Search |",
        "| 82 | High | Add code stop words filter for query expansion | ✓ Done | Code Search |",
        "| 83 | Medium | Add definition-aware boosting for class/def queries | ✓ Done | Code Search |"
      ],
      "context_after": [
        "",
        "---",
        "",
        "*Added 2025-12-11, completions updated 2025-12-11*"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/processor.py",
      "function": "class CorticalTextProcessor:",
      "start_line": 1765,
      "lines_added": [
        "        use_semantic: bool = True,",
        "        use_definition_search: bool = True,",
        "        definition_boost: float = 5.0,",
        "        apply_doc_boost: bool = True,",
        "        auto_detect_intent: bool = True,",
        "        prefer_docs: bool = False,",
        "        custom_boosts: Optional[Dict[str, float]] = None,",
        "        use_code_aware_chunks: bool = True",
        "        For definition queries (e.g., \"class Minicolumn\", \"def compute_pagerank\"),",
        "        the function will directly search for definition patterns and inject those",
        "        results with a high score, ensuring actual definitions appear in top results.",
        "",
        "        For conceptual queries (e.g., \"what is PageRank\", \"explain architecture\"),",
        "        documentation passages are boosted when auto_detect_intent=True.",
        "",
        "        For code files (.py, .js, etc.), semantic chunk boundaries are used to",
        "        align chunks with class/function definitions rather than fixed positions.",
        "",
        "            use_definition_search: Whether to search for definition patterns (default True)",
        "            definition_boost: Score boost for definition matches (default 5.0)",
        "            apply_doc_boost: Whether to apply document-type boosting (default True)",
        "            auto_detect_intent: Auto-detect conceptual queries and boost docs (default True)",
        "            prefer_docs: Always boost documentation regardless of query type (default False)",
        "            custom_boosts: Optional custom boost factors for doc types",
        "            use_code_aware_chunks: Use semantic boundaries for code files (default True)",
        "            >>> # For conceptual queries, docs are auto-boosted",
        "            >>> results = processor.find_passages_for_query(\"what is PageRank\")",
        "            use_semantic=use_semantic,",
        "            use_definition_search=use_definition_search,",
        "            definition_boost=definition_boost,",
        "            apply_doc_boost=apply_doc_boost,",
        "            doc_metadata=self.document_metadata,",
        "            auto_detect_intent=auto_detect_intent,",
        "            prefer_docs=prefer_docs,",
        "            custom_boosts=custom_boosts,",
        "            use_code_aware_chunks=use_code_aware_chunks",
        "        )",
        "",
        "    def is_definition_query(self, query_text: str) -> Tuple[bool, Optional[str], Optional[str]]:",
        "        \"\"\"",
        "        Detect if a query is looking for a code definition.",
        "",
        "        Recognizes patterns like:",
        "        - \"class Minicolumn\"",
        "        - \"def compute_pagerank\"",
        "        - \"function tokenize\"",
        "        - \"method process_document\"",
        "",
        "        Args:",
        "            query_text: The search query",
        "",
        "        Returns:",
        "            Tuple of (is_definition, definition_type, identifier_name)",
        "            If not a definition query, returns (False, None, None)",
        "",
        "        Example:",
        "            >>> is_def, def_type, name = processor.is_definition_query(\"class Minicolumn\")",
        "            >>> print(f\"Definition query: {is_def}, type: {def_type}, name: {name}\")",
        "            Definition query: True, type: class, name: Minicolumn",
        "        \"\"\"",
        "        return query_module.is_definition_query(query_text)",
        "",
        "    def find_definition_passages(",
        "        self,",
        "        query_text: str,",
        "        context_chars: int = 500,",
        "        boost: float = 5.0",
        "    ) -> List[Tuple[str, str, int, int, float]]:",
        "        \"\"\"",
        "        Find definition passages for a definition query.",
        "",
        "        If the query is looking for a class/function/method definition,",
        "        directly search source files for the definition and return",
        "        high-scoring passages.",
        "",
        "        Args:",
        "            query_text: Search query (e.g., \"class Minicolumn\", \"def compute_pagerank\")",
        "            context_chars: Characters of context to include after definition",
        "            boost: Score boost for definition matches",
        "",
        "        Returns:",
        "            List of (passage_text, doc_id, start_char, end_char, score) tuples.",
        "            Returns empty list if query is not a definition query.",
        "",
        "        Example:",
        "            >>> results = processor.find_definition_passages(\"class Minicolumn\")",
        "            >>> for passage, doc_id, start, end, score in results:",
        "            ...     print(f\"[{doc_id}] Score: {score:.2f}\")",
        "        \"\"\"",
        "        return query_module.find_definition_passages(",
        "            query_text, self.documents, context_chars, boost"
      ],
      "lines_removed": [
        "        use_semantic: bool = True",
        "            >>> results = processor.find_passages_for_query(\"neural networks\")",
        "            use_semantic=use_semantic"
      ],
      "context_before": [
        "        )",
        "",
        "    def find_passages_for_query(",
        "        self,",
        "        query_text: str,",
        "        top_n: int = 5,",
        "        chunk_size: int = 512,",
        "        overlap: int = 128,",
        "        use_expansion: bool = True,",
        "        doc_filter: Optional[List[str]] = None,"
      ],
      "context_after": [
        "    ) -> List[Tuple[str, str, int, int, float]]:",
        "        \"\"\"",
        "        Find text passages most relevant to a query (for RAG systems).",
        "",
        "        Instead of returning just document IDs, this returns actual text passages",
        "        with position information suitable for context windows and citations.",
        "",
        "        Args:",
        "            query_text: Search query",
        "            top_n: Number of passages to return",
        "            chunk_size: Size of each chunk in characters (default 512)",
        "            overlap: Overlap between chunks in characters (default 128)",
        "            use_expansion: Whether to expand query terms",
        "            doc_filter: Optional list of doc_ids to restrict search to",
        "            use_semantic: Whether to use semantic relations for expansion (if available)",
        "",
        "        Returns:",
        "            List of (passage_text, doc_id, start_char, end_char, score) tuples",
        "            ranked by relevance",
        "",
        "        Example:",
        "            >>> for passage, doc_id, start, end, score in results:",
        "            ...     print(f\"[{doc_id}:{start}-{end}] {passage[:50]}... (score: {score:.3f})\")",
        "        \"\"\"",
        "        return query_module.find_passages_for_query(",
        "            query_text,",
        "            self.layers,",
        "            self.tokenizer,",
        "            self.documents,",
        "            top_n=top_n,",
        "            chunk_size=chunk_size,",
        "            overlap=overlap,",
        "            use_expansion=use_expansion,",
        "            doc_filter=doc_filter,",
        "            semantic_relations=self.semantic_relations if use_semantic else None,",
        "        )",
        "",
        "    def find_documents_batch(",
        "        self,",
        "        queries: List[str],",
        "        top_n: int = 5,",
        "        use_expansion: bool = True,",
        "        use_semantic: bool = True",
        "    ) -> List[List[Tuple[str, float]]]:",
        "        \"\"\""
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/query.py",
      "function": "CONCEPTUAL_KEYWORDS = frozenset([",
      "start_line": 74,
      "lines_added": [
        "# =============================================================================",
        "# Definition Pattern Search",
        "# =============================================================================",
        "",
        "# Patterns for detecting definition queries",
        "DEFINITION_QUERY_PATTERNS = [",
        "    # \"class Foo\" or \"class foo\"",
        "    (r'\\bclass\\s+(\\w+)', 'class'),",
        "    # \"def bar\" or \"function bar\"",
        "    (r'\\bdef\\s+(\\w+)', 'function'),",
        "    (r'\\bfunction\\s+(\\w+)', 'function'),",
        "    # \"method baz\"",
        "    (r'\\bmethod\\s+(\\w+)', 'method'),",
        "]",
        "",
        "# Regex patterns for finding definitions in source code",
        "# Format: (pattern_template, definition_type)",
        "# pattern_template uses {name} placeholder for the identifier",
        "DEFINITION_SOURCE_PATTERNS = {",
        "    'python_class': r'^class\\s+{name}\\b[^:]*:',",
        "    'python_function': r'^def\\s+{name}\\s*\\(',",
        "    'python_method': r'^\\s+def\\s+{name}\\s*\\(',",
        "    'javascript_function': r'^function\\s+{name}\\s*\\(',",
        "    'javascript_class': r'^class\\s+{name}\\b',",
        "    'javascript_const_fn': r'^const\\s+{name}\\s*=\\s*(?:async\\s*)?\\(',",
        "}",
        "",
        "# Default boost for definition matches",
        "DEFINITION_BOOST = 5.0",
        "",
        "",
        "def is_definition_query(query_text: str) -> Tuple[bool, Optional[str], Optional[str]]:",
        "    \"\"\"",
        "    Detect if a query is looking for a code definition.",
        "",
        "    Recognizes patterns like:",
        "    - \"class Minicolumn\"",
        "    - \"def compute_pagerank\"",
        "    - \"function tokenize\"",
        "    - \"method process_document\"",
        "",
        "    Args:",
        "        query_text: The search query",
        "",
        "    Returns:",
        "        Tuple of (is_definition, definition_type, identifier_name)",
        "        If not a definition query, returns (False, None, None)",
        "    \"\"\"",
        "    query_lower = query_text.strip()",
        "",
        "    for pattern, def_type in DEFINITION_QUERY_PATTERNS:",
        "        match = re.search(pattern, query_lower, re.IGNORECASE)",
        "        if match:",
        "            identifier = match.group(1)",
        "            return (True, def_type, identifier)",
        "",
        "    return (False, None, None)",
        "",
        "",
        "def find_definition_in_text(",
        "    text: str,",
        "    identifier: str,",
        "    def_type: str,",
        "    context_chars: int = 500",
        ") -> Optional[Tuple[str, int, int]]:",
        "    \"\"\"",
        "    Find a definition in source text and extract surrounding context.",
        "",
        "    Args:",
        "        text: Source code text to search",
        "        identifier: Name of the class/function/method to find",
        "        def_type: Type of definition ('class', 'function', 'method')",
        "        context_chars: Number of characters of context to include after the definition",
        "",
        "    Returns:",
        "        Tuple of (passage_text, start_char, end_char) or None if not found",
        "    \"\"\"",
        "    # Build patterns to try based on definition type",
        "    patterns_to_try = []",
        "",
        "    if def_type == 'class':",
        "        patterns_to_try = [",
        "            DEFINITION_SOURCE_PATTERNS['python_class'],",
        "            DEFINITION_SOURCE_PATTERNS['javascript_class'],",
        "        ]",
        "    elif def_type in ('function', 'method'):",
        "        patterns_to_try = [",
        "            DEFINITION_SOURCE_PATTERNS['python_function'],",
        "            DEFINITION_SOURCE_PATTERNS['python_method'],",
        "            DEFINITION_SOURCE_PATTERNS['javascript_function'],",
        "            DEFINITION_SOURCE_PATTERNS['javascript_const_fn'],",
        "        ]",
        "",
        "    # Try each pattern",
        "    for pattern_template in patterns_to_try:",
        "        pattern = pattern_template.format(name=re.escape(identifier))",
        "        match = re.search(pattern, text, re.MULTILINE | re.IGNORECASE)",
        "        if match:",
        "            # Extract context around the definition",
        "            start = max(0, match.start() - 50)  # Small lead-in for context",
        "            end = min(len(text), match.end() + context_chars)",
        "",
        "            # Try to extend to next blank line or class/function boundary",
        "            remaining = text[match.end():end]",
        "            # Look for a good boundary (blank line followed by non-indented text)",
        "            boundary_match = re.search(r'\\n\\n(?=[^\\s])', remaining)",
        "            if boundary_match:",
        "                end = match.end() + boundary_match.end()",
        "",
        "            passage = text[start:end]",
        "            return (passage, start, end)",
        "",
        "    return None",
        "",
        "",
        "def find_definition_passages(",
        "    query_text: str,",
        "    documents: Dict[str, str],",
        "    context_chars: int = 500,",
        "    boost: float = DEFINITION_BOOST",
        ") -> List[Tuple[str, str, int, int, float]]:",
        "    \"\"\"",
        "    Find definition passages for a definition query.",
        "",
        "    If the query is looking for a class/function/method definition,",
        "    directly search source files for the definition and return",
        "    high-scoring passages.",
        "",
        "    Args:",
        "        query_text: Search query (e.g., \"class Minicolumn\", \"def compute_pagerank\")",
        "        documents: Dict mapping doc_id to document text",
        "        context_chars: Characters of context to include after definition",
        "        boost: Score boost for definition matches",
        "",
        "    Returns:",
        "        List of (passage_text, doc_id, start_char, end_char, score) tuples.",
        "        Returns empty list if query is not a definition query.",
        "    \"\"\"",
        "    is_def, def_type, identifier = is_definition_query(query_text)",
        "",
        "    if not is_def or not identifier:",
        "        return []",
        "",
        "    results = []",
        "",
        "    # Search all documents for the definition",
        "    for doc_id, text in documents.items():",
        "        # Prefer source files over test files for definitions",
        "        is_test = doc_id.startswith('tests/') or '_test' in doc_id or 'test_' in doc_id",
        "",
        "        result = find_definition_in_text(text, identifier, def_type, context_chars)",
        "        if result:",
        "            passage, start, end = result",
        "            # Apply boost, with penalty for test files",
        "            score = boost * (0.6 if is_test else 1.0)",
        "            results.append((passage, doc_id, start, end, score))",
        "",
        "    # Sort by score (highest first)",
        "    results.sort(key=lambda x: -x[4])",
        "    return results",
        "",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "])",
        "",
        "# Keywords that suggest an implementation query (should prefer code)",
        "IMPLEMENTATION_KEYWORDS = frozenset([",
        "    'where', 'implement', 'code', 'function', 'class', 'method', 'variable',",
        "    'line', 'file', 'bug', 'fix', 'error', 'exception', 'call', 'invoke',",
        "    'compute', 'calculate', 'return', 'parameter', 'argument',",
        "])",
        "",
        ""
      ],
      "context_after": [
        "def is_conceptual_query(query_text: str) -> bool:",
        "    \"\"\"",
        "    Determine if a query is conceptual (should boost documentation).",
        "",
        "    Conceptual queries ask about concepts, architecture, design, or",
        "    explanations rather than specific code locations.",
        "",
        "    Args:",
        "        query_text: The search query",
        ""
      ],
      "change_type": "add"
    },
    {
      "file": "cortical/query.py",
      "function": "def create_chunks(",
      "start_line": 1309,
      "lines_added": [
        "# Pattern to detect code structure boundaries",
        "CODE_BOUNDARY_PATTERN = re.compile(",
        "    r'^(?:'",
        "    r'class\\s+\\w+|'          # Class definitions",
        "    r'def\\s+\\w+|'            # Function definitions",
        "    r'async\\s+def\\s+\\w+|'    # Async function definitions",
        "    r'@\\w+|'                 # Decorators",
        "    r'#\\s*[-=]{3,}|'         # Comment separators (# --- or # ===)",
        "    r'\"\"\"[^\"]*\"\"\"|'          # Module/class docstrings (simple)",
        "    r\"'''[^']*'''\"           # Module/class docstrings (simple, single quotes)",
        "    r')',",
        "    re.MULTILINE",
        ")",
        "",
        "",
        "def find_code_boundaries(text: str) -> List[int]:",
        "    \"\"\"",
        "    Find semantic boundaries in code (class/function definitions, decorators).",
        "",
        "    Args:",
        "        text: Source code text",
        "",
        "    Returns:",
        "        Sorted list of character positions where semantic units begin",
        "    \"\"\"",
        "    boundaries = set([0])  # Always include start",
        "",
        "    # Find class/def boundaries",
        "    for match in CODE_BOUNDARY_PATTERN.finditer(text):",
        "        # Find the start of the line containing this match",
        "        line_start = text.rfind('\\n', 0, match.start()) + 1",
        "        boundaries.add(line_start)",
        "",
        "    # Also add positions after blank line sequences (natural section breaks)",
        "    blank_line_pattern = re.compile(r'\\n\\n+')",
        "    for match in blank_line_pattern.finditer(text):",
        "        boundaries.add(match.end())",
        "",
        "    return sorted(boundaries)",
        "",
        "",
        "def create_code_aware_chunks(",
        "    text: str,",
        "    target_size: int = 512,",
        "    min_size: int = 100,",
        "    max_size: int = 1024",
        ") -> List[Tuple[str, int, int]]:",
        "    \"\"\"",
        "    Create chunks aligned to code structure boundaries.",
        "",
        "    Unlike fixed-size chunking, this function tries to split code at",
        "    natural boundaries (class definitions, function definitions, blank lines)",
        "    to preserve semantic context within each chunk.",
        "",
        "    Args:",
        "        text: Source code text to chunk",
        "        target_size: Target chunk size in characters (default 512)",
        "        min_size: Minimum chunk size - won't create chunks smaller than this (default 100)",
        "        max_size: Maximum chunk size - will split even mid-code if exceeded (default 1024)",
        "",
        "    Returns:",
        "        List of (chunk_text, start_char, end_char) tuples",
        "",
        "    Example:",
        "        >>> text = '''",
        "        ... class Foo:",
        "        ...     def bar(self):",
        "        ...         pass",
        "        ...",
        "        ... class Baz:",
        "        ...     def qux(self):",
        "        ...         pass",
        "        ... '''",
        "        >>> chunks = create_code_aware_chunks(text, target_size=100)",
        "        >>> # Chunks will be aligned to class/function boundaries",
        "    \"\"\"",
        "    if not text:",
        "        return []",
        "",
        "    if len(text) <= target_size:",
        "        return [(text, 0, len(text))]",
        "",
        "    boundaries = find_code_boundaries(text)",
        "    boundaries.append(len(text))  # Add end of text",
        "",
        "    chunks = []",
        "    chunk_start = 0",
        "    i = 1",
        "",
        "    while chunk_start < len(text):",
        "        # Find the next boundary that would exceed target_size",
        "        best_end = chunk_start + max_size  # Default to max_size if no boundary found",
        "",
        "        # Look for a boundary between target_size and max_size",
        "        for j in range(i, len(boundaries)):",
        "            boundary = boundaries[j]",
        "            chunk_len = boundary - chunk_start",
        "",
        "            if chunk_len >= target_size:",
        "                if chunk_len <= max_size:",
        "                    # Good boundary within range",
        "                    best_end = boundary",
        "                    i = j + 1",
        "                    break",
        "                else:",
        "                    # Boundary too far, use previous one or force split",
        "                    if j > i:",
        "                        prev_boundary = boundaries[j - 1]",
        "                        prev_len = prev_boundary - chunk_start",
        "                        if prev_len >= min_size:",
        "                            best_end = prev_boundary",
        "                            i = j",
        "                            break",
        "                    # Force split at max_size",
        "                    best_end = chunk_start + max_size",
        "                    # Find the next boundary after our split point",
        "                    for k in range(i, len(boundaries)):",
        "                        if boundaries[k] > best_end:",
        "                            i = k",
        "                            break",
        "                    break",
        "        else:",
        "            # Reached end of boundaries, use text end",
        "            best_end = len(text)",
        "            i = len(boundaries)",
        "",
        "        # Ensure we don't exceed max_size",
        "        if best_end - chunk_start > max_size:",
        "            best_end = chunk_start + max_size",
        "",
        "        # Create chunk if non-empty",
        "        chunk_text = text[chunk_start:best_end]",
        "        if chunk_text.strip():",
        "            chunks.append((chunk_text, chunk_start, best_end))",
        "",
        "        chunk_start = best_end",
        "",
        "    return chunks",
        "",
        "",
        "def is_code_file(doc_id: str) -> bool:",
        "    \"\"\"",
        "    Determine if a document is a code file based on its path/extension.",
        "",
        "    Args:",
        "        doc_id: Document identifier (typically a file path)",
        "",
        "    Returns:",
        "        True if the document appears to be a code file",
        "    \"\"\"",
        "    code_extensions = {",
        "        '.py', '.js', '.ts', '.jsx', '.tsx', '.java', '.c', '.cpp', '.h',",
        "        '.go', '.rs', '.rb', '.php', '.swift', '.kt', '.scala', '.cs'",
        "    }",
        "    for ext in code_extensions:",
        "        if doc_id.endswith(ext):",
        "            return True",
        "    return False",
        "",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "        end = min(start + chunk_size, text_len)",
        "        chunk = text[start:end]",
        "        chunks.append((chunk, start, end))",
        "",
        "        if end >= text_len:",
        "            break",
        "",
        "    return chunks",
        "",
        ""
      ],
      "context_after": [
        "def precompute_term_cols(",
        "    query_terms: Dict[str, float],",
        "    layer0: HierarchicalLayer",
        ") -> Dict[str, 'Minicolumn']:",
        "    \"\"\"",
        "    Pre-compute minicolumn lookups for query terms.",
        "",
        "    This avoids repeated O(1) dictionary lookups for each chunk,",
        "    enabling faster scoring when processing many chunks.",
        ""
      ],
      "change_type": "add"
    },
    {
      "file": "cortical/query.py",
      "function": "def find_passages_for_query(",
      "start_line": 1431,
      "lines_added": [
        "    use_semantic: bool = True,",
        "    use_definition_search: bool = True,",
        "    definition_boost: float = DEFINITION_BOOST,",
        "    apply_doc_boost: bool = True,",
        "    doc_metadata: Optional[Dict[str, Dict[str, Any]]] = None,",
        "    auto_detect_intent: bool = True,",
        "    prefer_docs: bool = False,",
        "    custom_boosts: Optional[Dict[str, float]] = None,",
        "    use_code_aware_chunks: bool = True",
        "    For definition queries (e.g., \"class Minicolumn\", \"def compute_pagerank\"),",
        "    this function will directly search for the definition pattern and inject",
        "    those results with a high score, ensuring definitions appear in top results.",
        "",
        "    For conceptual queries (e.g., \"what is PageRank\", \"explain architecture\"),",
        "    documentation passages are boosted to appear higher in results when",
        "    auto_detect_intent=True.",
        "",
        "    For code files, semantic chunk boundaries can be used to align chunks",
        "    with class/function definitions rather than fixed character positions.",
        "",
        "        use_definition_search: Whether to search for definition patterns (default True)",
        "        definition_boost: Score boost for definition matches (default 5.0)",
        "        apply_doc_boost: Whether to apply document-type boosting (default True)",
        "        doc_metadata: Optional metadata dict {doc_id: {doc_type: ..., ...}}",
        "        auto_detect_intent: Auto-detect conceptual queries and boost docs (default True)",
        "        prefer_docs: Always boost documentation regardless of query type (default False)",
        "        custom_boosts: Optional custom boost factors for doc types",
        "        use_code_aware_chunks: Use semantic boundaries for code files (default True)",
        "    # Determine if we should apply doc-type boosting",
        "    should_boost = apply_doc_boost and (",
        "        prefer_docs or (auto_detect_intent and is_conceptual_query(query_text))",
        "    )",
        "",
        "    # Check for definition query and find definition passages",
        "    definition_passages: List[Tuple[str, str, int, int, float]] = []",
        "    if use_definition_search:",
        "        docs_to_search = documents",
        "        if doc_filter:",
        "            docs_to_search = {k: v for k, v in documents.items() if k in doc_filter}",
        "        definition_passages = find_definition_passages(",
        "            query_text, docs_to_search, chunk_size, definition_boost",
        "        )",
        "",
        "    if not query_terms and not definition_passages:",
        "    # If we only have definition results, apply boosting and return",
        "    if not query_terms:",
        "        if should_boost:",
        "            definition_passages = [",
        "                (p[0], p[1], p[2], p[3], p[4] * get_doc_type_boost(p[1], doc_metadata, custom_boosts))",
        "                for p in definition_passages",
        "            ]",
        "            definition_passages.sort(key=lambda x: -x[4])",
        "        return definition_passages[:top_n]",
        ""
      ],
      "lines_removed": [
        "    use_semantic: bool = True",
        "    if not query_terms:"
      ],
      "context_before": [
        "    query_text: str,",
        "    layers: Dict[CorticalLayer, HierarchicalLayer],",
        "    tokenizer: Tokenizer,",
        "    documents: Dict[str, str],",
        "    top_n: int = 5,",
        "    chunk_size: int = 512,",
        "    overlap: int = 128,",
        "    use_expansion: bool = True,",
        "    doc_filter: Optional[List[str]] = None,",
        "    semantic_relations: Optional[List[Tuple[str, str, str, float]]] = None,"
      ],
      "context_after": [
        ") -> List[Tuple[str, str, int, int, float]]:",
        "    \"\"\"",
        "    Find text passages most relevant to a query.",
        "",
        "    This is the key function for RAG systems - instead of returning document IDs,",
        "    it returns actual text passages with position information for citations.",
        "",
        "    Args:",
        "        query_text: Search query",
        "        layers: Dictionary of layers",
        "        tokenizer: Tokenizer instance",
        "        documents: Dict mapping doc_id to document text",
        "        top_n: Number of passages to return",
        "        chunk_size: Size of each chunk in characters (default 512)",
        "        overlap: Overlap between chunks in characters (default 128)",
        "        use_expansion: Whether to expand query terms",
        "        doc_filter: Optional list of doc_ids to restrict search to",
        "        semantic_relations: Optional list of semantic relations for expansion",
        "        use_semantic: Whether to use semantic relations for expansion (if available)",
        "",
        "    Returns:",
        "        List of (passage_text, doc_id, start_char, end_char, score) tuples",
        "        ranked by relevance",
        "    \"\"\"",
        "    layer0 = layers[CorticalLayer.TOKENS]",
        "",
        "    # Get expanded query terms",
        "    query_terms = get_expanded_query_terms(",
        "        query_text, layers, tokenizer,",
        "        use_expansion=use_expansion,",
        "        semantic_relations=semantic_relations,",
        "        use_semantic=use_semantic",
        "    )",
        "",
        "        return []",
        "",
        "    # Pre-compute minicolumn lookups for query terms (optimization)",
        "    term_cols = precompute_term_cols(query_terms, layer0)",
        "",
        "    # Get candidate documents",
        "    if doc_filter:",
        "        # Use provided filter directly as candidates (caller may have pre-boosted)",
        "        # Assign dummy scores since we'll re-score passages anyway",
        "        doc_scores = [(doc_id, 1.0) for doc_id in doc_filter if doc_id in documents]",
        "    else:",
        "        # No filter - get candidates via document search"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/query.py",
      "function": "def find_passages_for_query(",
      "start_line": 1490,
      "lines_added": [
        "    # Track definition passage locations to avoid duplicates",
        "    def_locations = {(p[1], p[2], p[3]) for p in definition_passages}",
        "",
        "",
        "        # Use code-aware chunking for code files if enabled",
        "        if use_code_aware_chunks and is_code_file(doc_id):",
        "            chunks = create_code_aware_chunks(",
        "                text,",
        "                target_size=chunk_size,",
        "                min_size=max(50, chunk_size // 4),",
        "                max_size=chunk_size * 2",
        "            )",
        "        else:",
        "            chunks = create_chunks(text, chunk_size, overlap)",
        "",
        "        # Pre-compute doc-type boost for this document",
        "        doc_type_boost = get_doc_type_boost(doc_id, doc_metadata, custom_boosts) if should_boost else 1.0",
        "            # Skip if this overlaps with a definition passage",
        "            if (doc_id, start_char, end_char) in def_locations:",
        "                continue",
        "",
        "            # Apply document-type boost",
        "            combined_score *= doc_type_boost",
        "",
        "    # Apply doc-type boost to definition passages too",
        "    if should_boost:",
        "        definition_passages = [",
        "            (p[0], p[1], p[2], p[3], p[4] * get_doc_type_boost(p[1], doc_metadata, custom_boosts))",
        "            for p in definition_passages",
        "        ]",
        "",
        "    # Combine definition passages with regular passages",
        "    all_passages = definition_passages + passages",
        "",
        "    all_passages.sort(key=lambda x: x[4], reverse=True)",
        "    return all_passages[:top_n]"
      ],
      "lines_removed": [
        "        chunks = create_chunks(text, chunk_size, overlap)",
        "    passages.sort(key=lambda x: x[4], reverse=True)",
        "    return passages[:top_n]"
      ],
      "context_before": [
        "            query_text, layers, tokenizer,",
        "            top_n=min(len(documents), top_n * 3),",
        "            use_expansion=use_expansion,",
        "            semantic_relations=semantic_relations,",
        "            use_semantic=use_semantic",
        "        )",
        "",
        "    # Score passages within candidate documents",
        "    passages: List[Tuple[str, str, int, int, float]] = []",
        ""
      ],
      "context_after": [
        "    for doc_id, doc_score in doc_scores:",
        "        if doc_id not in documents:",
        "            continue",
        "",
        "        text = documents[doc_id]",
        "",
        "        for chunk_text, start_char, end_char in chunks:",
        "            # Use fast scoring with pre-computed lookups",
        "            chunk_tokens = tokenizer.tokenize(chunk_text)",
        "            chunk_score = score_chunk_fast(",
        "                chunk_tokens, query_terms, term_cols, doc_id",
        "            )",
        "            # Combine chunk score with document score for final ranking",
        "            combined_score = chunk_score * (1 + doc_score * 0.1)",
        "",
        "            passages.append((",
        "                chunk_text,",
        "                doc_id,",
        "                start_char,",
        "                end_char,",
        "                combined_score",
        "            ))",
        "",
        "    # Sort by score and return top passages",
        "",
        "",
        "def find_documents_batch(",
        "    queries: List[str],",
        "    layers: Dict[CorticalLayer, HierarchicalLayer],",
        "    tokenizer: Tokenizer,",
        "    top_n: int = 5,",
        "    use_expansion: bool = True,",
        "    semantic_relations: Optional[List[Tuple[str, str, str, float]]] = None,",
        "    use_semantic: bool = True"
      ],
      "change_type": "modify"
    },
    {
      "file": "samples/data_processor.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "\"\"\"",
        "Data Processor Module - Sample code for demonstrating code search features.",
        "",
        "This module provides utilities for processing and transforming data records.",
        "\"\"\"",
        "",
        "from typing import Dict, List, Optional, Any",
        "from dataclasses import dataclass",
        "from collections import defaultdict",
        "",
        "",
        "@dataclass",
        "class DataRecord:",
        "    \"\"\"Represents a single data record with metadata.",
        "",
        "    Attributes:",
        "        id: Unique identifier for the record",
        "        content: The main content of the record",
        "        metadata: Optional dictionary of metadata fields",
        "        tags: List of tags associated with the record",
        "    \"\"\"",
        "    id: str",
        "    content: str",
        "    metadata: Optional[Dict[str, Any]] = None",
        "    tags: List[str] = None",
        "",
        "    def __post_init__(self):",
        "        if self.tags is None:",
        "            self.tags = []",
        "        if self.metadata is None:",
        "            self.metadata = {}",
        "",
        "",
        "class DataProcessor:",
        "    \"\"\"Main processor for handling data records.",
        "",
        "    The DataProcessor provides methods for filtering, transforming,",
        "    and aggregating data records efficiently.",
        "",
        "    Example:",
        "        processor = DataProcessor()",
        "        processor.add_record(DataRecord(\"1\", \"Hello world\"))",
        "        results = processor.filter_by_content(\"hello\")",
        "    \"\"\"",
        "",
        "    def __init__(self):",
        "        \"\"\"Initialize the data processor with empty storage.\"\"\"",
        "        self._records: Dict[str, DataRecord] = {}",
        "        self._tag_index: Dict[str, List[str]] = defaultdict(list)",
        "",
        "    def add_record(self, record: DataRecord) -> None:",
        "        \"\"\"Add a record to the processor.",
        "",
        "        Args:",
        "            record: The DataRecord to add",
        "        \"\"\"",
        "        self._records[record.id] = record",
        "        for tag in record.tags:",
        "            self._tag_index[tag].append(record.id)",
        "",
        "    def get_record(self, record_id: str) -> Optional[DataRecord]:",
        "        \"\"\"Retrieve a record by its ID.",
        "",
        "        Args:",
        "            record_id: The unique identifier of the record",
        "",
        "        Returns:",
        "            The DataRecord if found, None otherwise",
        "        \"\"\"",
        "        return self._records.get(record_id)",
        "",
        "    def filter_by_content(self, query: str) -> List[DataRecord]:",
        "        \"\"\"Filter records by content matching.",
        "",
        "        Args:",
        "            query: Search string to match against content",
        "",
        "        Returns:",
        "            List of matching DataRecord objects",
        "        \"\"\"",
        "        query_lower = query.lower()",
        "        return [",
        "            record for record in self._records.values()",
        "            if query_lower in record.content.lower()",
        "        ]",
        "",
        "    def filter_by_tag(self, tag: str) -> List[DataRecord]:",
        "        \"\"\"Filter records by tag.",
        "",
        "        Args:",
        "            tag: The tag to filter by",
        "",
        "        Returns:",
        "            List of DataRecord objects with the specified tag",
        "        \"\"\"",
        "        record_ids = self._tag_index.get(tag, [])",
        "        return [self._records[rid] for rid in record_ids if rid in self._records]",
        "",
        "    def transform_content(self, transformer_func) -> List[DataRecord]:",
        "        \"\"\"Apply a transformation function to all record contents.",
        "",
        "        Args:",
        "            transformer_func: Callable that takes content string and returns transformed string",
        "",
        "        Returns:",
        "            List of new DataRecord objects with transformed content",
        "        \"\"\"",
        "        results = []",
        "        for record in self._records.values():",
        "            new_content = transformer_func(record.content)",
        "            new_record = DataRecord(",
        "                id=record.id,",
        "                content=new_content,",
        "                metadata=record.metadata.copy(),",
        "                tags=record.tags.copy()",
        "            )",
        "            results.append(new_record)",
        "        return results",
        "",
        "    def aggregate_by_tag(self) -> Dict[str, int]:",
        "        \"\"\"Count records per tag.",
        "",
        "        Returns:",
        "            Dictionary mapping tag names to record counts",
        "        \"\"\"",
        "        return {tag: len(ids) for tag, ids in self._tag_index.items()}",
        "",
        "    def clear(self) -> None:",
        "        \"\"\"Remove all records from the processor.\"\"\"",
        "        self._records.clear()",
        "        self._tag_index.clear()",
        "",
        "",
        "def calculate_statistics(records: List[DataRecord]) -> Dict[str, Any]:",
        "    \"\"\"Calculate statistics for a list of records.",
        "",
        "    Args:",
        "        records: List of DataRecord objects to analyze",
        "",
        "    Returns:",
        "        Dictionary containing:",
        "            - count: Number of records",
        "            - avg_content_length: Average content length",
        "            - unique_tags: Set of all unique tags",
        "            - records_with_metadata: Count of records with non-empty metadata",
        "    \"\"\"",
        "    if not records:",
        "        return {",
        "            'count': 0,",
        "            'avg_content_length': 0,",
        "            'unique_tags': set(),",
        "            'records_with_metadata': 0",
        "        }",
        "",
        "    total_length = sum(len(r.content) for r in records)",
        "    all_tags = set()",
        "    metadata_count = 0",
        "",
        "    for record in records:",
        "        all_tags.update(record.tags)",
        "        if record.metadata:",
        "            metadata_count += 1",
        "",
        "    return {",
        "        'count': len(records),",
        "        'avg_content_length': total_length / len(records),",
        "        'unique_tags': all_tags,",
        "        'records_with_metadata': metadata_count",
        "    }",
        "",
        "",
        "def merge_records(records: List[DataRecord], separator: str = '\\n') -> DataRecord:",
        "    \"\"\"Merge multiple records into a single record.",
        "",
        "    Args:",
        "        records: List of DataRecord objects to merge",
        "        separator: String to use between merged contents",
        "",
        "    Returns:",
        "        A new DataRecord with combined content, merged metadata, and all tags",
        "    \"\"\"",
        "    if not records:",
        "        return DataRecord(id='merged', content='')",
        "",
        "    merged_content = separator.join(r.content for r in records)",
        "    merged_metadata = {}",
        "    merged_tags = []",
        "",
        "    for record in records:",
        "        merged_metadata.update(record.metadata)",
        "        merged_tags.extend(record.tags)",
        "",
        "    return DataRecord(",
        "        id='merged_' + '_'.join(r.id for r in records[:3]),",
        "        content=merged_content,",
        "        metadata=merged_metadata,",
        "        tags=list(set(merged_tags))",
        "    )"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "samples/search_engine.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "\"\"\"",
        "Search Engine Module - Sample code for demonstrating code search features.",
        "",
        "This module implements a simple search engine with indexing and ranking.",
        "\"\"\"",
        "",
        "from typing import Dict, List, Tuple, Set, Optional",
        "from collections import defaultdict",
        "import math",
        "",
        "",
        "class SearchIndex:",
        "    \"\"\"Inverted index for text search.",
        "",
        "    The SearchIndex maintains an inverted index mapping terms to documents,",
        "    enabling fast full-text search with TF-IDF ranking.",
        "",
        "    Example:",
        "        index = SearchIndex()",
        "        index.add_document(\"doc1\", \"hello world\")",
        "        results = index.search(\"hello\")",
        "    \"\"\"",
        "",
        "    def __init__(self):",
        "        \"\"\"Initialize empty search index.\"\"\"",
        "        self._inverted_index: Dict[str, Set[str]] = defaultdict(set)",
        "        self._documents: Dict[str, str] = {}",
        "        self._term_frequencies: Dict[str, Dict[str, int]] = defaultdict(lambda: defaultdict(int))",
        "        self._document_lengths: Dict[str, int] = {}",
        "",
        "    def add_document(self, doc_id: str, content: str) -> None:",
        "        \"\"\"Add a document to the index.",
        "",
        "        Args:",
        "            doc_id: Unique document identifier",
        "            content: Text content to index",
        "        \"\"\"",
        "        self._documents[doc_id] = content",
        "        tokens = self._tokenize(content)",
        "        self._document_lengths[doc_id] = len(tokens)",
        "",
        "        for token in tokens:",
        "            self._inverted_index[token].add(doc_id)",
        "            self._term_frequencies[doc_id][token] += 1",
        "",
        "    def remove_document(self, doc_id: str) -> bool:",
        "        \"\"\"Remove a document from the index.",
        "",
        "        Args:",
        "            doc_id: The document ID to remove",
        "",
        "        Returns:",
        "            True if document was removed, False if not found",
        "        \"\"\"",
        "        if doc_id not in self._documents:",
        "            return False",
        "",
        "        # Remove from inverted index",
        "        for term, doc_ids in self._inverted_index.items():",
        "            doc_ids.discard(doc_id)",
        "",
        "        # Clean up empty term entries",
        "        empty_terms = [t for t, ids in self._inverted_index.items() if not ids]",
        "        for term in empty_terms:",
        "            del self._inverted_index[term]",
        "",
        "        del self._documents[doc_id]",
        "        del self._term_frequencies[doc_id]",
        "        del self._document_lengths[doc_id]",
        "        return True",
        "",
        "    def search(self, query: str, top_k: int = 10) -> List[Tuple[str, float]]:",
        "        \"\"\"Search for documents matching the query.",
        "",
        "        Args:",
        "            query: Search query string",
        "            top_k: Maximum number of results to return",
        "",
        "        Returns:",
        "            List of (doc_id, score) tuples sorted by relevance",
        "        \"\"\"",
        "        query_tokens = self._tokenize(query)",
        "        if not query_tokens:",
        "            return []",
        "",
        "        scores = defaultdict(float)",
        "",
        "        for token in query_tokens:",
        "            if token not in self._inverted_index:",
        "                continue",
        "",
        "            idf = self._compute_idf(token)",
        "            for doc_id in self._inverted_index[token]:",
        "                tf = self._compute_tf(token, doc_id)",
        "                scores[doc_id] += tf * idf",
        "",
        "        # Sort by score descending",
        "        ranked = sorted(scores.items(), key=lambda x: x[1], reverse=True)",
        "        return ranked[:top_k]",
        "",
        "    def get_document(self, doc_id: str) -> Optional[str]:",
        "        \"\"\"Retrieve document content by ID.",
        "",
        "        Args:",
        "            doc_id: The document identifier",
        "",
        "        Returns:",
        "            Document content string if found, None otherwise",
        "        \"\"\"",
        "        return self._documents.get(doc_id)",
        "",
        "    def _tokenize(self, text: str) -> List[str]:",
        "        \"\"\"Tokenize text into lowercase terms.",
        "",
        "        Args:",
        "            text: Input text to tokenize",
        "",
        "        Returns:",
        "            List of lowercase tokens",
        "        \"\"\"",
        "        # Simple whitespace tokenization",
        "        return [t.lower().strip('.,!?;:()[]{}') for t in text.split() if t.strip()]",
        "",
        "    def _compute_tf(self, term: str, doc_id: str) -> float:",
        "        \"\"\"Compute term frequency for a term in a document.",
        "",
        "        Args:",
        "            term: The term to compute TF for",
        "            doc_id: The document identifier",
        "",
        "        Returns:",
        "            Normalized term frequency",
        "        \"\"\"",
        "        raw_tf = self._term_frequencies[doc_id][term]",
        "        doc_length = self._document_lengths[doc_id]",
        "        return raw_tf / doc_length if doc_length > 0 else 0",
        "",
        "    def _compute_idf(self, term: str) -> float:",
        "        \"\"\"Compute inverse document frequency for a term.",
        "",
        "        Args:",
        "            term: The term to compute IDF for",
        "",
        "        Returns:",
        "            IDF value using log scaling",
        "        \"\"\"",
        "        n_docs = len(self._documents)",
        "        doc_freq = len(self._inverted_index.get(term, set()))",
        "        if doc_freq == 0:",
        "            return 0",
        "        return math.log(n_docs / doc_freq)",
        "",
        "",
        "class QueryParser:",
        "    \"\"\"Parser for search queries with advanced syntax.",
        "",
        "    Supports:",
        "        - Simple terms: hello world",
        "        - Phrase queries: \"hello world\"",
        "        - Required terms: +important",
        "        - Excluded terms: -spam",
        "    \"\"\"",
        "",
        "    def __init__(self):",
        "        \"\"\"Initialize the query parser.\"\"\"",
        "        self._operators = {'+', '-', '\"'}",
        "",
        "    def parse(self, query: str) -> Dict[str, List[str]]:",
        "        \"\"\"Parse a query string into components.",
        "",
        "        Args:",
        "            query: The query string to parse",
        "",
        "        Returns:",
        "            Dictionary with keys:",
        "                - required: Terms that must appear",
        "                - excluded: Terms that must not appear",
        "                - optional: Regular search terms",
        "                - phrases: Exact phrase matches",
        "        \"\"\"",
        "        result = {",
        "            'required': [],",
        "            'excluded': [],",
        "            'optional': [],",
        "            'phrases': []",
        "        }",
        "",
        "        i = 0",
        "        tokens = query.split()",
        "",
        "        while i < len(tokens):",
        "            token = tokens[i]",
        "",
        "            if token.startswith('+'):",
        "                result['required'].append(token[1:].lower())",
        "            elif token.startswith('-'):",
        "                result['excluded'].append(token[1:].lower())",
        "            elif token.startswith('\"'):",
        "                # Handle phrase - collect until closing quote",
        "                phrase_tokens = [token[1:]]",
        "                i += 1",
        "                while i < len(tokens) and not tokens[i].endswith('\"'):",
        "                    phrase_tokens.append(tokens[i])",
        "                    i += 1",
        "                if i < len(tokens):",
        "                    phrase_tokens.append(tokens[i][:-1])",
        "                result['phrases'].append(' '.join(phrase_tokens).lower())",
        "            else:",
        "                result['optional'].append(token.lower())",
        "",
        "            i += 1",
        "",
        "        return result",
        "",
        "",
        "def compute_bm25_score(",
        "    term: str,",
        "    doc_id: str,",
        "    index: SearchIndex,",
        "    k1: float = 1.5,",
        "    b: float = 0.75",
        ") -> float:",
        "    \"\"\"Compute BM25 score for a term in a document.",
        "",
        "    BM25 is a ranking function used by search engines to estimate",
        "    the relevance of documents to a given search query.",
        "",
        "    Args:",
        "        term: The search term",
        "        doc_id: The document identifier",
        "        index: The SearchIndex to use",
        "        k1: Term frequency saturation parameter",
        "        b: Length normalization parameter",
        "",
        "    Returns:",
        "        BM25 score for the term-document pair",
        "    \"\"\"",
        "    tf = index._term_frequencies[doc_id].get(term, 0)",
        "    doc_length = index._document_lengths[doc_id]",
        "    avg_doc_length = sum(index._document_lengths.values()) / len(index._document_lengths)",
        "",
        "    idf = index._compute_idf(term)",
        "",
        "    numerator = tf * (k1 + 1)",
        "    denominator = tf + k1 * (1 - b + b * doc_length / avg_doc_length)",
        "",
        "    return idf * numerator / denominator",
        "",
        "",
        "def highlight_matches(text: str, query_terms: List[str], marker: str = '**') -> str:",
        "    \"\"\"Highlight query term matches in text.",
        "",
        "    Args:",
        "        text: The text to highlight",
        "        query_terms: List of terms to highlight",
        "        marker: String to use for highlighting (wraps matches)",
        "",
        "    Returns:",
        "        Text with query terms wrapped in markers",
        "    \"\"\"",
        "    result = text",
        "    for term in query_terms:",
        "        # Case-insensitive replacement",
        "        import re",
        "        pattern = re.compile(re.escape(term), re.IGNORECASE)",
        "        result = pattern.sub(f'{marker}{term}{marker}', result)",
        "    return result"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "samples/test_data_processor.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "\"\"\"",
        "Unit tests for the DataProcessor module.",
        "",
        "This test file demonstrates various testing patterns for the data processing",
        "functionality including fixtures, edge cases, and integration tests.",
        "\"\"\"",
        "",
        "import unittest",
        "from typing import List",
        "",
        "# Note: In a real project, this would import from the actual module",
        "# from data_processor import DataProcessor, DataRecord, calculate_statistics",
        "",
        "",
        "class MockDataRecord:",
        "    \"\"\"Mock DataRecord for testing purposes.\"\"\"",
        "    def __init__(self, id: str, content: str, metadata=None, tags=None):",
        "        self.id = id",
        "        self.content = content",
        "        self.metadata = metadata or {}",
        "        self.tags = tags or []",
        "",
        "",
        "class TestDataProcessorBasics(unittest.TestCase):",
        "    \"\"\"Test basic DataProcessor functionality.\"\"\"",
        "",
        "    def setUp(self):",
        "        \"\"\"Set up test fixtures.\"\"\"",
        "        self.processor = MockDataProcessor()",
        "        self.sample_record = MockDataRecord(",
        "            id=\"test1\",",
        "            content=\"Hello world test content\",",
        "            tags=[\"test\", \"sample\"]",
        "        )",
        "",
        "    def test_add_record(self):",
        "        \"\"\"Test adding a single record.\"\"\"",
        "        self.processor.add_record(self.sample_record)",
        "        result = self.processor.get_record(\"test1\")",
        "        self.assertIsNotNone(result)",
        "        self.assertEqual(result.content, \"Hello world test content\")",
        "",
        "    def test_add_multiple_records(self):",
        "        \"\"\"Test adding multiple records.\"\"\"",
        "        records = [",
        "            MockDataRecord(\"r1\", \"First record\"),",
        "            MockDataRecord(\"r2\", \"Second record\"),",
        "            MockDataRecord(\"r3\", \"Third record\"),",
        "        ]",
        "        for record in records:",
        "            self.processor.add_record(record)",
        "",
        "        self.assertEqual(len(self.processor._records), 3)",
        "",
        "    def test_get_nonexistent_record(self):",
        "        \"\"\"Test retrieving a record that doesn't exist.\"\"\"",
        "        result = self.processor.get_record(\"nonexistent\")",
        "        self.assertIsNone(result)",
        "",
        "",
        "class TestDataProcessorFiltering(unittest.TestCase):",
        "    \"\"\"Test filtering functionality.\"\"\"",
        "",
        "    def setUp(self):",
        "        \"\"\"Set up test data with various records.\"\"\"",
        "        self.processor = MockDataProcessor()",
        "        self.processor.add_record(MockDataRecord(\"1\", \"Python programming\"))",
        "        self.processor.add_record(MockDataRecord(\"2\", \"JavaScript development\"))",
        "        self.processor.add_record(MockDataRecord(\"3\", \"Python web development\"))",
        "        self.processor.add_record(MockDataRecord(\"4\", \"Database design\", tags=[\"db\"]))",
        "",
        "    def test_filter_by_content_single_match(self):",
        "        \"\"\"Test content filter with single match.\"\"\"",
        "        results = self.processor.filter_by_content(\"JavaScript\")",
        "        self.assertEqual(len(results), 1)",
        "        self.assertEqual(results[0].id, \"2\")",
        "",
        "    def test_filter_by_content_multiple_matches(self):",
        "        \"\"\"Test content filter with multiple matches.\"\"\"",
        "        results = self.processor.filter_by_content(\"Python\")",
        "        self.assertEqual(len(results), 2)",
        "",
        "    def test_filter_by_content_case_insensitive(self):",
        "        \"\"\"Test that content filtering is case insensitive.\"\"\"",
        "        results = self.processor.filter_by_content(\"python\")",
        "        self.assertEqual(len(results), 2)",
        "",
        "    def test_filter_by_content_no_match(self):",
        "        \"\"\"Test content filter with no matches.\"\"\"",
        "        results = self.processor.filter_by_content(\"Ruby\")",
        "        self.assertEqual(len(results), 0)",
        "",
        "    def test_filter_by_tag(self):",
        "        \"\"\"Test filtering by tag.\"\"\"",
        "        results = self.processor.filter_by_tag(\"db\")",
        "        self.assertEqual(len(results), 1)",
        "        self.assertEqual(results[0].id, \"4\")",
        "",
        "",
        "class TestDataProcessorTransformation(unittest.TestCase):",
        "    \"\"\"Test transformation functionality.\"\"\"",
        "",
        "    def setUp(self):",
        "        \"\"\"Set up test processor.\"\"\"",
        "        self.processor = MockDataProcessor()",
        "        self.processor.add_record(MockDataRecord(\"1\", \"hello world\"))",
        "        self.processor.add_record(MockDataRecord(\"2\", \"test content\"))",
        "",
        "    def test_transform_uppercase(self):",
        "        \"\"\"Test transforming content to uppercase.\"\"\"",
        "        results = self.processor.transform_content(str.upper)",
        "        self.assertEqual(results[0].content, \"HELLO WORLD\")",
        "",
        "    def test_transform_preserves_metadata(self):",
        "        \"\"\"Test that transformation preserves record metadata.\"\"\"",
        "        self.processor._records[\"1\"].metadata = {\"key\": \"value\"}",
        "        results = self.processor.transform_content(str.upper)",
        "        self.assertEqual(results[0].metadata, {\"key\": \"value\"})",
        "",
        "",
        "class TestStatisticsCalculation(unittest.TestCase):",
        "    \"\"\"Test statistics calculation functions.\"\"\"",
        "",
        "    def test_empty_records(self):",
        "        \"\"\"Test statistics with empty record list.\"\"\"",
        "        stats = mock_calculate_statistics([])",
        "        self.assertEqual(stats['count'], 0)",
        "        self.assertEqual(stats['avg_content_length'], 0)",
        "",
        "    def test_single_record(self):",
        "        \"\"\"Test statistics with single record.\"\"\"",
        "        records = [MockDataRecord(\"1\", \"Hello\")]",
        "        stats = mock_calculate_statistics(records)",
        "        self.assertEqual(stats['count'], 1)",
        "        self.assertEqual(stats['avg_content_length'], 5)",
        "",
        "    def test_multiple_records(self):",
        "        \"\"\"Test statistics with multiple records.\"\"\"",
        "        records = [",
        "            MockDataRecord(\"1\", \"Hi\", tags=[\"a\"]),",
        "            MockDataRecord(\"2\", \"Hello\", tags=[\"a\", \"b\"]),",
        "            MockDataRecord(\"3\", \"Goodbye\", tags=[\"c\"]),",
        "        ]",
        "        stats = mock_calculate_statistics(records)",
        "        self.assertEqual(stats['count'], 3)",
        "        self.assertEqual(stats['unique_tags'], {\"a\", \"b\", \"c\"})",
        "",
        "",
        "class TestEdgeCases(unittest.TestCase):",
        "    \"\"\"Test edge cases and boundary conditions.\"\"\"",
        "",
        "    def test_empty_content(self):",
        "        \"\"\"Test record with empty content.\"\"\"",
        "        processor = MockDataProcessor()",
        "        record = MockDataRecord(\"empty\", \"\")",
        "        processor.add_record(record)",
        "        self.assertEqual(processor.get_record(\"empty\").content, \"\")",
        "",
        "    def test_special_characters(self):",
        "        \"\"\"Test content with special characters.\"\"\"",
        "        processor = MockDataProcessor()",
        "        record = MockDataRecord(\"special\", \"Hello! @#$% World?\")",
        "        processor.add_record(record)",
        "        results = processor.filter_by_content(\"@#$%\")",
        "        self.assertEqual(len(results), 1)",
        "",
        "    def test_unicode_content(self):",
        "        \"\"\"Test content with unicode characters.\"\"\"",
        "        processor = MockDataProcessor()",
        "        record = MockDataRecord(\"unicode\", \"Hello 世界 🌍\")",
        "        processor.add_record(record)",
        "        results = processor.filter_by_content(\"世界\")",
        "        self.assertEqual(len(results), 1)",
        "",
        "",
        "# Mock implementations for testing",
        "class MockDataProcessor:",
        "    \"\"\"Mock implementation of DataProcessor for testing.\"\"\"",
        "",
        "    def __init__(self):",
        "        self._records = {}",
        "        self._tag_index = {}",
        "",
        "    def add_record(self, record):",
        "        self._records[record.id] = record",
        "        for tag in record.tags:",
        "            if tag not in self._tag_index:",
        "                self._tag_index[tag] = []",
        "            self._tag_index[tag].append(record.id)",
        "",
        "    def get_record(self, record_id):",
        "        return self._records.get(record_id)",
        "",
        "    def filter_by_content(self, query):",
        "        query_lower = query.lower()",
        "        return [r for r in self._records.values() if query_lower in r.content.lower()]",
        "",
        "    def filter_by_tag(self, tag):",
        "        ids = self._tag_index.get(tag, [])",
        "        return [self._records[i] for i in ids if i in self._records]",
        "",
        "    def transform_content(self, func):",
        "        results = []",
        "        for r in self._records.values():",
        "            new_record = MockDataRecord(r.id, func(r.content), r.metadata.copy(), r.tags.copy())",
        "            results.append(new_record)",
        "        return results",
        "",
        "",
        "def mock_calculate_statistics(records: List) -> dict:",
        "    \"\"\"Mock implementation of calculate_statistics.\"\"\"",
        "    if not records:",
        "        return {'count': 0, 'avg_content_length': 0, 'unique_tags': set(), 'records_with_metadata': 0}",
        "",
        "    total_length = sum(len(r.content) for r in records)",
        "    all_tags = set()",
        "    for r in records:",
        "        all_tags.update(r.tags)",
        "",
        "    return {",
        "        'count': len(records),",
        "        'avg_content_length': total_length / len(records),",
        "        'unique_tags': all_tags,",
        "        'records_with_metadata': sum(1 for r in records if r.metadata)",
        "    }",
        "",
        "",
        "if __name__ == '__main__':",
        "    unittest.main()"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "showcase.py",
      "function": "class CorticalShowcase:",
      "start_line": 107,
      "lines_added": [
        "        # Load both .txt and .py files",
        "        py_files = sorted([f for f in os.listdir(self.samples_dir) if f.endswith('.py')])",
        "        all_files = txt_files + py_files",
        "        if not all_files:",
        "        for filename in all_files:",
        "            # Handle both .txt and .py extensions",
        "            doc_id = filename.replace('.txt', '').replace('.py', '')"
      ],
      "lines_removed": [
        "        if not txt_files:",
        "        for filename in txt_files:",
        "            doc_id = filename.replace('.txt', '')"
      ],
      "context_before": [
        "        print_header(\"DOCUMENT INGESTION\", \"═\")",
        "",
        "        print(f\"Loading documents from: {self.samples_dir}\")",
        "        print(\"Processing through cortical hierarchy...\")",
        "        print(\"(Like visual information flowing V1 → V2 → V4 → IT)\\n\")",
        "",
        "        if not os.path.exists(self.samples_dir):",
        "            print(f\"  ❌ Directory not found: {self.samples_dir}\")",
        "            return False",
        ""
      ],
      "context_after": [
        "        txt_files = sorted([f for f in os.listdir(self.samples_dir) if f.endswith('.txt')])",
        "",
        "            return False",
        "",
        "        # Time document loading",
        "        self.timer.start('document_loading')",
        "            filepath = os.path.join(self.samples_dir, filename)",
        "            with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:",
        "                content = f.read()",
        "",
        "            self.processor.process_document(doc_id, content)",
        "            word_count = len(content.split())",
        "            self.loaded_files.append((doc_id, word_count))",
        "            print(f\"  📄 {doc_id:30} ({word_count:3} words)\")",
        "        load_time = self.timer.stop()",
        "",
        "        # Run all computations with hybrid strategy for better Layer 2 connectivity",
        "        print(\"\\nComputing cortical representations...\")",
        "        self.timer.start('compute_all')",
        "        self.processor.compute_all("
      ],
      "change_type": "modify"
    },
    {
      "file": "showcase.py",
      "function": "class CorticalShowcase:",
      "start_line": 425,
      "lines_added": [
        "        # 2. Definition search (NEW - Task #84)",
        "        print_subheader(\"\\n🔎 Definition Search\")",
        "        print(\"    Find class/function definitions directly in code:\\n\")",
        "",
        "        definition_queries = [\"class DataProcessor\", \"def calculate_statistics\", \"class SearchIndex\"]",
        "",
        "        for query in definition_queries:",
        "            is_def, def_type, identifier = self.processor.is_definition_query(query)",
        "            print(f\"    Query: \\\"{query}\\\"\")",
        "            print(f\"      Is definition query: {is_def} ({def_type} '{identifier}')\" if is_def else f\"      Is definition query: {is_def}\")",
        "",
        "            if is_def:",
        "                passages = self.processor.find_definition_passages(query)",
        "                if passages:",
        "                    text, doc_id, start, end, score = passages[0]",
        "                    # Show first line of the definition",
        "                    first_line = text.strip().split('\\n')[0][:60]",
        "                    print(f\"      Found in: {doc_id}\")",
        "                    print(f\"      Match: {first_line}...\")",
        "                else:",
        "                    print(f\"      (No definition found in corpus)\")",
        "            print()",
        "",
        "        # 3. Doc-type boosting (NEW - Task #66)",
        "        print_subheader(\"📊 Doc-Type Boosting\")",
        "        print(\"    Apply different weights to docs, code, and test files:\\n\")",
        "",
        "        query = \"filter data records\"",
        "        print(f\"    Query: \\\"{query}\\\"\\n\")",
        "",
        "        # Without boosting",
        "        results_normal = self.processor.find_passages_for_query(",
        "            query, top_n=3, apply_doc_boost=False",
        "        )",
        "",
        "        # With boosting (prefer docs over tests)",
        "        results_boosted = self.processor.find_passages_for_query(",
        "            query, top_n=3, apply_doc_boost=True, prefer_docs=True",
        "        )",
        "",
        "        print(\"    Without doc-type boost:\")",
        "        for text, doc_id, start, end, score in results_normal[:3]:",
        "            is_test = 'test' in doc_id.lower()",
        "            marker = \"🧪\" if is_test else \"📄\"",
        "            print(f\"      {marker} {doc_id}: {score:.3f}\")",
        "",
        "        print(\"\\n    With doc-type boost (prefer docs, penalize tests):\")",
        "        for text, doc_id, start, end, score in results_boosted[:3]:",
        "            is_test = 'test' in doc_id.lower()",
        "            marker = \"🧪\" if is_test else \"📄\"",
        "            print(f\"      {marker} {doc_id}: {score:.3f}\")",
        "",
        "        print(\"\\n    💡 Test files receive a 0.5x penalty to surface source files first.\")",
        "",
        "        # 4. Code-aware chunking (NEW - Task #86)",
        "        print_subheader(\"\\n✂️  Code-Aware Chunking\")",
        "        print(\"    Split code at semantic boundaries (class/function defs):\\n\")",
        "",
        "        # Find a Python file",
        "        code_doc_id = None",
        "        for doc_id, _ in self.loaded_files:",
        "            if doc_id in ['data_processor', 'search_engine']:",
        "                code_doc_id = doc_id",
        "                break",
        "",
        "        if code_doc_id:",
        "            content = self.processor.documents.get(code_doc_id, \"\")",
        "",
        "            # Regular chunking",
        "            from cortical.query import create_chunks, create_code_aware_chunks",
        "            regular_chunks = create_chunks(content, chunk_size=300, overlap=50)",
        "",
        "            # Code-aware chunking",
        "            code_chunks = create_code_aware_chunks(content, max_size=300)",
        "",
        "            print(f\"    File: {code_doc_id}\")",
        "            print(f\"    Regular chunks: {len(regular_chunks)} (fixed 300-char boundaries)\")",
        "            print(f\"    Code-aware chunks: {len(code_chunks)} (semantic boundaries)\\n\")",
        "",
        "            print(\"    Code-aware chunk boundaries:\")",
        "            for i, (chunk_text, start, end) in enumerate(code_chunks[:4]):",
        "                first_line = chunk_text.strip().split('\\n')[0][:50]",
        "                print(f\"      [{i+1}] {first_line}...\")",
        "        else:",
        "            print(\"    (No Python files in corpus)\")",
        "",
        "        # 5. Code-aware query expansion"
      ],
      "lines_removed": [
        "        # 2. Code-aware query expansion"
      ],
      "context_before": [
        "",
        "        for query, expected_conceptual in test_queries:",
        "            is_conceptual = self.processor.is_conceptual_query(query)",
        "            intent = \"conceptual\" if is_conceptual else \"implementation\"",
        "            marker = \"📖\" if is_conceptual else \"💻\"",
        "            print(f\"    {marker} \\\"{query}\\\" → {intent}\")",
        "",
        "        print(\"\\n    💡 Use case: Boost documentation for conceptual queries,\")",
        "        print(\"                 boost code files for implementation queries.\")",
        ""
      ],
      "context_after": [
        "        print_subheader(\"\\n🔧 Code-Aware Query Expansion\")",
        "        print(\"    Programming synonyms expand queries for better code search:\\n\")",
        "",
        "        code_queries = [\"fetch data\", \"get results\", \"process input\"]",
        "",
        "        for query in code_queries:",
        "            # Regular expansion",
        "            regular = self.processor.expand_query(query, max_expansions=5)",
        "            # Code-aware expansion",
        "            code_exp = self.processor.expand_query_for_code(query, max_expansions=8)"
      ],
      "change_type": "modify"
    },
    {
      "file": "showcase.py",
      "function": "class CorticalShowcase:",
      "start_line": 450,
      "lines_added": [
        "        # 6. Semantic fingerprinting"
      ],
      "lines_removed": [
        "        # 3. Semantic fingerprinting"
      ],
      "context_before": [
        "            new_terms = code_terms - regular_terms",
        "",
        "            print(f\"    Query: \\\"{query}\\\"\")",
        "            if new_terms:",
        "                new_list = sorted(new_terms, key=lambda t: -code_exp.get(t, 0))[:4]",
        "                print(f\"      + Code terms: {', '.join(new_list)}\")",
        "            else:",
        "                print(f\"      (corpus lacks programming synonyms for this query)\")",
        "            print()",
        ""
      ],
      "context_after": [
        "        print_subheader(\"🔍 Semantic Fingerprinting\")",
        "        print(\"    Compare text similarity using semantic fingerprints:\\n\")",
        "",
        "        # Get two related documents",
        "        if len(self.loaded_files) >= 2:",
        "            doc1_id = \"neural_pagerank\" if \"neural_pagerank\" in self.processor.documents else self.loaded_files[0][0]",
        "            doc2_id = \"pagerank_fundamentals\" if \"pagerank_fundamentals\" in self.processor.documents else self.loaded_files[1][0]",
        "",
        "            doc1_content = self.processor.documents.get(doc1_id, \"\")[:500]",
        "            doc2_content = self.processor.documents.get(doc2_id, \"\")[:500]"
      ],
      "change_type": "modify"
    },
    {
      "file": "tests/test_query.py",
      "function": "from cortical.query import (",
      "start_line": 31,
      "lines_added": [
        "    is_definition_query,",
        "    find_definition_in_text,",
        "    find_definition_passages,",
        "    DEFINITION_QUERY_PATTERNS,",
        "    DEFINITION_SOURCE_PATTERNS,",
        "    DEFINITION_BOOST,",
        "    find_code_boundaries,",
        "    create_code_aware_chunks,",
        "    is_code_file,"
      ],
      "lines_removed": [],
      "context_before": [
        "    find_passages_for_query,",
        "    find_documents_batch,",
        "    find_passages_batch,",
        "    find_relevant_concepts,",
        "    find_relation_between,",
        "    find_terms_with_relation,",
        "    complete_analogy,",
        "    complete_analogy_simple,",
        "    query_with_spreading_activation,",
        "    VALID_RELATION_CHAINS,"
      ],
      "context_after": [
        ")",
        "",
        "",
        "class TestScoreRelationPath(unittest.TestCase):",
        "    \"\"\"Test relation path scoring.\"\"\"",
        "",
        "    def test_empty_path(self):",
        "        \"\"\"Empty path should return 1.0.\"\"\"",
        "        self.assertEqual(score_relation_path([]), 1.0)",
        ""
      ],
      "change_type": "add"
    },
    {
      "file": "tests/test_query.py",
      "function": "class TestDocTypeBoostIntegration(unittest.TestCase):",
      "start_line": 1026,
      "lines_added": [
        "class TestDefinitionPatternSearch(unittest.TestCase):",
        "    \"\"\"Test definition pattern search functionality.\"\"\"",
        "",
        "    def test_is_definition_query_class(self):",
        "        \"\"\"Detect class definition queries.\"\"\"",
        "        is_def, def_type, name = is_definition_query(\"class Minicolumn\")",
        "        self.assertTrue(is_def)",
        "        self.assertEqual(def_type, 'class')",
        "        self.assertEqual(name, 'Minicolumn')",
        "",
        "    def test_is_definition_query_def(self):",
        "        \"\"\"Detect function definition queries.\"\"\"",
        "        is_def, def_type, name = is_definition_query(\"def compute_pagerank\")",
        "        self.assertTrue(is_def)",
        "        self.assertEqual(def_type, 'function')",
        "        self.assertEqual(name, 'compute_pagerank')",
        "",
        "    def test_is_definition_query_function(self):",
        "        \"\"\"Detect function keyword queries.\"\"\"",
        "        is_def, def_type, name = is_definition_query(\"function tokenize\")",
        "        self.assertTrue(is_def)",
        "        self.assertEqual(def_type, 'function')",
        "        self.assertEqual(name, 'tokenize')",
        "",
        "    def test_is_definition_query_method(self):",
        "        \"\"\"Detect method definition queries.\"\"\"",
        "        is_def, def_type, name = is_definition_query(\"method process_document\")",
        "        self.assertTrue(is_def)",
        "        self.assertEqual(def_type, 'method')",
        "        self.assertEqual(name, 'process_document')",
        "",
        "    def test_is_definition_query_not_definition(self):",
        "        \"\"\"Non-definition queries should return False.\"\"\"",
        "        is_def, def_type, name = is_definition_query(\"neural networks\")",
        "        self.assertFalse(is_def)",
        "        self.assertIsNone(def_type)",
        "        self.assertIsNone(name)",
        "",
        "    def test_is_definition_query_case_insensitive(self):",
        "        \"\"\"Definition detection should be case insensitive.\"\"\"",
        "        is_def, def_type, name = is_definition_query(\"CLASS MyClass\")",
        "        self.assertTrue(is_def)",
        "        self.assertEqual(def_type, 'class')",
        "        self.assertEqual(name, 'MyClass')",
        "",
        "    def test_find_definition_in_text_python_class(self):",
        "        \"\"\"Find Python class definitions.\"\"\"",
        "        text = '''",
        "import os",
        "",
        "class MyProcessor:",
        "    \"\"\"A processor class.\"\"\"",
        "",
        "    def __init__(self):",
        "        pass",
        "'''",
        "        result = find_definition_in_text(text, 'MyProcessor', 'class')",
        "        self.assertIsNotNone(result)",
        "        passage, start, end = result",
        "        self.assertIn('class MyProcessor:', passage)",
        "",
        "    def test_find_definition_in_text_python_function(self):",
        "        \"\"\"Find Python function definitions.\"\"\"",
        "        text = '''",
        "def compute_score(items, weights):",
        "    \"\"\"Compute weighted score.\"\"\"",
        "    total = sum(i * w for i, w in zip(items, weights))",
        "    return total / len(items)",
        "'''",
        "        result = find_definition_in_text(text, 'compute_score', 'function')",
        "        self.assertIsNotNone(result)",
        "        passage, start, end = result",
        "        self.assertIn('def compute_score(', passage)",
        "",
        "    def test_find_definition_in_text_not_found(self):",
        "        \"\"\"Return None when definition not found.\"\"\"",
        "        text = 'def other_function(): pass'",
        "        result = find_definition_in_text(text, 'nonexistent', 'function')",
        "        self.assertIsNone(result)",
        "",
        "    def test_find_definition_in_text_method(self):",
        "        \"\"\"Find method definitions (indented def).\"\"\"",
        "        text = '''",
        "class MyClass:",
        "    def my_method(self, arg):",
        "        return arg * 2",
        "'''",
        "        result = find_definition_in_text(text, 'my_method', 'method')",
        "        self.assertIsNotNone(result)",
        "        passage, start, end = result",
        "        self.assertIn('def my_method(', passage)",
        "",
        "    def test_find_definition_passages_basic(self):",
        "        \"\"\"Find definition passages from documents.\"\"\"",
        "        documents = {",
        "            'module.py': '''",
        "class TestClass:",
        "    \"\"\"A test class for demonstration.\"\"\"",
        "",
        "    def process(self):",
        "        pass",
        "''',",
        "            'other.py': 'def helper(): pass',",
        "        }",
        "        results = find_definition_passages(\"class TestClass\", documents)",
        "        self.assertTrue(len(results) > 0)",
        "        passage, doc_id, start, end, score = results[0]",
        "        self.assertEqual(doc_id, 'module.py')",
        "        self.assertIn('class TestClass:', passage)",
        "        self.assertEqual(score, DEFINITION_BOOST)  # No test file penalty",
        "",
        "    def test_find_definition_passages_test_file_penalty(self):",
        "        \"\"\"Test files should have lower score.\"\"\"",
        "        documents = {",
        "            'src/module.py': 'class MyClass: pass',",
        "            'tests/test_module.py': 'class MyClass: pass',",
        "        }",
        "        results = find_definition_passages(\"class MyClass\", documents)",
        "        self.assertEqual(len(results), 2)",
        "",
        "        # Sort by score descending",
        "        results.sort(key=lambda x: -x[4])",
        "",
        "        # Source file should rank higher",
        "        self.assertEqual(results[0][1], 'src/module.py')",
        "        self.assertEqual(results[1][1], 'tests/test_module.py')",
        "        self.assertGreater(results[0][4], results[1][4])",
        "",
        "    def test_find_definition_passages_not_definition_query(self):",
        "        \"\"\"Non-definition queries return empty list.\"\"\"",
        "        documents = {'test.py': 'class Foo: pass'}",
        "        results = find_definition_passages(\"neural networks\", documents)",
        "        self.assertEqual(results, [])",
        "",
        "",
        "class TestDefinitionSearchIntegration(unittest.TestCase):",
        "    \"\"\"Integration tests for definition search in passage retrieval.\"\"\"",
        "",
        "    def setUp(self):",
        "        \"\"\"Set up processor with code documents.\"\"\"",
        "        self.processor = CorticalTextProcessor()",
        "",
        "        # Add a code document with class and function definitions",
        "        self.processor.process_document('cortical/minicolumn.py', '''",
        "\"\"\"Minicolumn module for cortical processing.\"\"\"",
        "",
        "from dataclasses import dataclass",
        "from typing import Dict, List, Optional",
        "",
        "class Minicolumn:",
        "    \"\"\"",
        "    Core data structure representing a minicolumn in the cortical model.",
        "",
        "    A minicolumn stores information about a single concept at a specific",
        "    layer in the hierarchy.",
        "",
        "    Attributes:",
        "        id: Unique identifier",
        "        content: The text content (word, bigram, etc.)",
        "        layer: Which layer this belongs to (0-3)",
        "    \"\"\"",
        "",
        "    def __init__(self, id: str, content: str, layer: int):",
        "        self.id = id",
        "        self.content = content",
        "        self.layer = layer",
        "        self.lateral_connections: Dict[str, float] = {}",
        "        self.pagerank: float = 0.0",
        "        self.tfidf: float = 0.0",
        "",
        "    def add_connection(self, target_id: str, weight: float = 1.0):",
        "        \"\"\"Add a lateral connection to another minicolumn.\"\"\"",
        "        if target_id in self.lateral_connections:",
        "            self.lateral_connections[target_id] += weight",
        "        else:",
        "            self.lateral_connections[target_id] = weight",
        "''')",
        "",
        "        self.processor.process_document('tests/test_minicolumn.py', '''",
        "\"\"\"Tests for minicolumn module.\"\"\"",
        "",
        "import unittest",
        "from cortical.minicolumn import Minicolumn",
        "",
        "class TestMinicolumn(unittest.TestCase):",
        "    \"\"\"Test Minicolumn class.\"\"\"",
        "",
        "    def test_init(self):",
        "        col = Minicolumn(\"L0_test\", \"test\", 0)",
        "        self.assertEqual(col.id, \"L0_test\")",
        "        self.assertEqual(col.content, \"test\")",
        "",
        "    def test_add_connection(self):",
        "        col = Minicolumn(\"L0_a\", \"a\", 0)",
        "        col.add_connection(\"L0_b\", 0.5)",
        "        self.assertIn(\"L0_b\", col.lateral_connections)",
        "''')",
        "",
        "        self.processor.compute_all()",
        "",
        "    def test_definition_search_finds_class(self):",
        "        \"\"\"Definition search should find actual class definition.\"\"\"",
        "        results = self.processor.find_passages_for_query(",
        "            \"class Minicolumn\",",
        "            top_n=5,",
        "            use_definition_search=True",
        "        )",
        "",
        "        self.assertTrue(len(results) > 0)",
        "",
        "        # First result should be from the source file, not the test",
        "        passage, doc_id, start, end, score = results[0]",
        "        self.assertEqual(doc_id, 'cortical/minicolumn.py')",
        "        self.assertIn('class Minicolumn', passage)",
        "",
        "    def test_definition_search_finds_method(self):",
        "        \"\"\"Definition search should find method definitions.\"\"\"",
        "        results = self.processor.find_passages_for_query(",
        "            \"def add_connection\",",
        "            top_n=5,",
        "            use_definition_search=True",
        "        )",
        "",
        "        self.assertTrue(len(results) > 0)",
        "        passage, doc_id, start, end, score = results[0]",
        "        self.assertIn('def add_connection', passage)",
        "",
        "    def test_definition_search_disabled(self):",
        "        \"\"\"When disabled, definition search should not run.\"\"\"",
        "        # With a definition query but definition search disabled",
        "        results_disabled = self.processor.find_passages_for_query(",
        "            \"class Minicolumn\",",
        "            top_n=5,",
        "            use_definition_search=False",
        "        )",
        "",
        "        results_enabled = self.processor.find_passages_for_query(",
        "            \"class Minicolumn\",",
        "            top_n=5,",
        "            use_definition_search=True",
        "        )",
        "",
        "        # Enabled should have higher score for definition",
        "        if results_disabled and results_enabled:",
        "            # Definition search should boost the actual definition",
        "            self.assertGreaterEqual(results_enabled[0][4], results_disabled[0][4])",
        "",
        "    def test_processor_has_definition_methods(self):",
        "        \"\"\"Processor should have definition search methods.\"\"\"",
        "        self.assertTrue(hasattr(self.processor, 'is_definition_query'))",
        "        self.assertTrue(hasattr(self.processor, 'find_definition_passages'))",
        "",
        "    def test_is_definition_query_via_processor(self):",
        "        \"\"\"Test is_definition_query via processor wrapper.\"\"\"",
        "        is_def, def_type, name = self.processor.is_definition_query(\"class Minicolumn\")",
        "        self.assertTrue(is_def)",
        "        self.assertEqual(def_type, 'class')",
        "        self.assertEqual(name, 'Minicolumn')",
        "",
        "    def test_find_definition_passages_via_processor(self):",
        "        \"\"\"Test find_definition_passages via processor wrapper.\"\"\"",
        "        results = self.processor.find_definition_passages(\"class Minicolumn\")",
        "        self.assertTrue(len(results) > 0)",
        "        passage, doc_id, start, end, score = results[0]",
        "        self.assertIn('class Minicolumn', passage)",
        "",
        "",
        "class TestPassageDocTypeBoost(unittest.TestCase):",
        "    \"\"\"Test doc-type boosting for passage-level search.\"\"\"",
        "",
        "    def setUp(self):",
        "        \"\"\"Set up processor with code and documentation.\"\"\"",
        "        self.processor = CorticalTextProcessor()",
        "",
        "        # Add a code file",
        "        self.processor.process_document('cortical/analysis.py', '''",
        "\"\"\"Analysis module for computing PageRank and TF-IDF.\"\"\"",
        "",
        "def compute_pagerank(layers, damping=0.85):",
        "    \"\"\"Compute PageRank scores for all minicolumns.",
        "",
        "    PageRank is an iterative algorithm that assigns importance scores",
        "    to nodes based on the structure of incoming links.",
        "",
        "    Args:",
        "        layers: Dictionary of hierarchical layers",
        "        damping: Damping factor (default 0.85)",
        "",
        "    Returns:",
        "        Dict mapping node IDs to PageRank scores",
        "    \"\"\"",
        "    # Implementation details...",
        "    pass",
        "''', metadata={'doc_type': 'code'})",
        "",
        "        # Add a documentation file",
        "        self.processor.process_document('docs/algorithms.md', '''",
        "# PageRank Algorithm",
        "",
        "PageRank is the foundational algorithm that revolutionized web search.",
        "It computes importance scores for nodes in a graph by iteratively",
        "propagating scores through connections.",
        "",
        "## How PageRank Works",
        "",
        "1. Initialize all nodes with equal score",
        "2. Iteratively update scores based on incoming links",
        "3. Apply damping factor to prevent score accumulation",
        "4. Converge when changes are below tolerance",
        "",
        "The damping factor (typically 0.85) represents the probability that",
        "a random walker continues following links rather than jumping randomly.",
        "''', metadata={'doc_type': 'docs'})",
        "",
        "        # Add a test file",
        "        self.processor.process_document('tests/test_analysis.py', '''",
        "\"\"\"Tests for analysis module.\"\"\"",
        "",
        "import unittest",
        "",
        "class TestPageRank(unittest.TestCase):",
        "    def test_compute_pagerank(self):",
        "        \"\"\"Test PageRank computation.\"\"\"",
        "        result = compute_pagerank(self.layers)",
        "        self.assertIsInstance(result, dict)",
        "",
        "    def test_pagerank_damping(self):",
        "        \"\"\"Test PageRank with custom damping.\"\"\"",
        "        result = compute_pagerank(self.layers, damping=0.9)",
        "        self.assertGreater(len(result), 0)",
        "''', metadata={'doc_type': 'test'})",
        "",
        "        self.processor.compute_all()",
        "",
        "    def test_conceptual_query_boosts_docs(self):",
        "        \"\"\"Conceptual queries should boost documentation passages.\"\"\"",
        "        # Conceptual query - should boost docs",
        "        results = self.processor.find_passages_for_query(",
        "            \"what is PageRank algorithm\",",
        "            top_n=5,",
        "            auto_detect_intent=True,",
        "            apply_doc_boost=True",
        "        )",
        "",
        "        self.assertTrue(len(results) > 0)",
        "",
        "        # Check that docs/ folder file appears in results with boost",
        "        doc_ids = [r[1] for r in results]",
        "        # With boosting, docs should be prioritized for conceptual queries",
        "        self.assertIn('docs/algorithms.md', doc_ids)",
        "",
        "    def test_prefer_docs_always_boosts(self):",
        "        \"\"\"prefer_docs=True should always boost documentation.\"\"\"",
        "        # Implementation query that would normally prefer code",
        "        results = self.processor.find_passages_for_query(",
        "            \"compute pagerank\",",
        "            top_n=5,",
        "            prefer_docs=True,",
        "            apply_doc_boost=True",
        "        )",
        "",
        "        self.assertTrue(len(results) > 0)",
        "        # Results should include docs even for implementation query",
        "",
        "    def test_disable_doc_boost(self):",
        "        \"\"\"apply_doc_boost=False should use raw scores.\"\"\"",
        "        # Same query with and without boost",
        "        results_no_boost = self.processor.find_passages_for_query(",
        "            \"explain PageRank algorithm\",",
        "            top_n=5,",
        "            apply_doc_boost=False",
        "        )",
        "",
        "        results_with_boost = self.processor.find_passages_for_query(",
        "            \"explain PageRank algorithm\",",
        "            top_n=5,",
        "            apply_doc_boost=True,",
        "            auto_detect_intent=True",
        "        )",
        "",
        "        # Both should return results",
        "        self.assertTrue(len(results_no_boost) > 0)",
        "        self.assertTrue(len(results_with_boost) > 0)",
        "",
        "        # With boosting, if doc is found, it might have higher score",
        "        # (depends on corpus content and scores)",
        "",
        "    def test_implementation_query_no_boost(self):",
        "        \"\"\"Implementation queries should not boost docs when auto_detect_intent=True.\"\"\"",
        "        # Implementation query",
        "        results = self.processor.find_passages_for_query(",
        "            \"compute pagerank function code\",",
        "            top_n=5,",
        "            auto_detect_intent=True,",
        "            apply_doc_boost=True",
        "        )",
        "",
        "        self.assertTrue(len(results) > 0)",
        "        # Implementation queries shouldn't trigger doc boost",
        "",
        "    def test_custom_boosts(self):",
        "        \"\"\"Custom boost factors should be applied.\"\"\"",
        "        custom = {'docs': 3.0, 'code': 0.5, 'test': 0.3}",
        "",
        "        results = self.processor.find_passages_for_query(",
        "            \"what is PageRank\",",
        "            top_n=5,",
        "            prefer_docs=True,",
        "            custom_boosts=custom",
        "        )",
        "",
        "        self.assertTrue(len(results) > 0)",
        "",
        "",
        "class TestPassageDocTypeBoostIntegration(unittest.TestCase):",
        "    \"\"\"Integration tests for doc-type boost in passage search.\"\"\"",
        "",
        "    def test_find_passages_has_boost_params(self):",
        "        \"\"\"find_passages_for_query should accept boost parameters.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document('test.py', 'def foo(): pass')",
        "        processor.compute_all()",
        "",
        "        # Should not raise",
        "        results = processor.find_passages_for_query(",
        "            \"foo\",",
        "            apply_doc_boost=True,",
        "            auto_detect_intent=True,",
        "            prefer_docs=False,",
        "            custom_boosts={'code': 1.0}",
        "        )",
        "        # Results may be empty for simple doc but params should work",
        "",
        "",
        "class TestCodeAwareChunking(unittest.TestCase):",
        "    \"\"\"Test code-aware chunking functions.\"\"\"",
        "",
        "    def test_is_code_file_python(self):",
        "        \"\"\"Python files should be detected as code.\"\"\"",
        "        self.assertTrue(is_code_file('module.py'))",
        "        self.assertTrue(is_code_file('path/to/file.py'))",
        "",
        "    def test_is_code_file_javascript(self):",
        "        \"\"\"JavaScript files should be detected as code.\"\"\"",
        "        self.assertTrue(is_code_file('app.js'))",
        "        self.assertTrue(is_code_file('component.tsx'))",
        "",
        "    def test_is_code_file_markdown(self):",
        "        \"\"\"Markdown files should not be detected as code.\"\"\"",
        "        self.assertFalse(is_code_file('README.md'))",
        "        self.assertFalse(is_code_file('docs/guide.md'))",
        "",
        "    def test_is_code_file_other(self):",
        "        \"\"\"Other extensions should not be detected as code.\"\"\"",
        "        self.assertFalse(is_code_file('data.json'))",
        "        self.assertFalse(is_code_file('config.yaml'))",
        "",
        "    def test_find_code_boundaries_class(self):",
        "        \"\"\"Should find class definition boundaries.\"\"\"",
        "        code = '''",
        "import os",
        "",
        "class MyClass:",
        "    def method(self):",
        "        pass",
        "'''",
        "        boundaries = find_code_boundaries(code)",
        "        self.assertIn(0, boundaries)  # Start",
        "        # Should find the class line",
        "        class_line_start = code.find('class MyClass')",
        "        line_start = code.rfind('\\n', 0, class_line_start) + 1",
        "        self.assertIn(line_start, boundaries)",
        "",
        "    def test_find_code_boundaries_function(self):",
        "        \"\"\"Should find function definition boundaries.\"\"\"",
        "        code = '''def foo():",
        "    pass",
        "",
        "def bar():",
        "    pass",
        "'''",
        "        boundaries = find_code_boundaries(code)",
        "        # Should find both function boundaries",
        "        self.assertGreater(len(boundaries), 1)",
        "",
        "    def test_find_code_boundaries_blank_lines(self):",
        "        \"\"\"Should find blank line boundaries.\"\"\"",
        "        code = '''first section",
        "",
        "second section",
        "",
        "third section",
        "'''",
        "        boundaries = find_code_boundaries(code)",
        "        # Should include positions after blank lines",
        "        self.assertGreater(len(boundaries), 1)",
        "",
        "    def test_create_code_aware_chunks_empty(self):",
        "        \"\"\"Empty text should return empty list.\"\"\"",
        "        chunks = create_code_aware_chunks('')",
        "        self.assertEqual(chunks, [])",
        "",
        "    def test_create_code_aware_chunks_small_text(self):",
        "        \"\"\"Text smaller than target should return single chunk.\"\"\"",
        "        code = 'def foo(): pass'",
        "        chunks = create_code_aware_chunks(code, target_size=100)",
        "        self.assertEqual(len(chunks), 1)",
        "        self.assertEqual(chunks[0][0], code)",
        "",
        "    def test_create_code_aware_chunks_splits_at_boundaries(self):",
        "        \"\"\"Should split at class/function boundaries.\"\"\"",
        "        # Create code long enough to require multiple chunks",
        "        code = '''class FirstClass:",
        "    \"\"\"First class docstring with enough text to make this substantial.\"\"\"",
        "",
        "    def __init__(self):",
        "        self.value = 0",
        "        self.data = {}",
        "        self.cache = []",
        "",
        "    def method1(self):",
        "        \"\"\"A method that does something important.\"\"\"",
        "        result = self.value * 2",
        "        return result",
        "",
        "",
        "class SecondClass:",
        "    \"\"\"Second class docstring with substantial documentation text here.\"\"\"",
        "",
        "    def __init__(self):",
        "        self.items = []",
        "        self.count = 0",
        "",
        "    def method2(self):",
        "        \"\"\"Another method with documentation.\"\"\"",
        "        for item in self.items:",
        "            self.count += item",
        "        return self.count",
        "'''",
        "        # With target_size=200, this ~600 char code should split into multiple chunks",
        "        chunks = create_code_aware_chunks(code, target_size=200, min_size=50, max_size=400)",
        "        self.assertGreater(len(chunks), 1)",
        "",
        "        # Check that chunks start at sensible boundaries",
        "        for chunk_text, start, end in chunks:",
        "            # Chunks should not be empty",
        "            self.assertTrue(chunk_text.strip())",
        "",
        "    def test_create_code_aware_chunks_respects_max_size(self):",
        "        \"\"\"Should not exceed max_size.\"\"\"",
        "        # Create code with a very long function",
        "        long_function = 'def long_func():\\n' + '    x = 1\\n' * 100",
        "        chunks = create_code_aware_chunks(long_function, target_size=200, max_size=400)",
        "",
        "        for chunk_text, start, end in chunks:",
        "            self.assertLessEqual(len(chunk_text), 400)",
        "",
        "    def test_create_code_aware_chunks_no_whitespace_only(self):",
        "        \"\"\"Should not return whitespace-only chunks.\"\"\"",
        "        code = '''class A:",
        "    pass",
        "",
        "",
        "",
        "class B:",
        "    pass",
        "'''",
        "        chunks = create_code_aware_chunks(code, target_size=50, min_size=10)",
        "        for chunk_text, start, end in chunks:",
        "            self.assertTrue(chunk_text.strip())",
        "",
        "",
        "class TestCodeAwareChunkingIntegration(unittest.TestCase):",
        "    \"\"\"Integration tests for code-aware chunking in passage search.\"\"\"",
        "",
        "    def setUp(self):",
        "        \"\"\"Set up processor with code documents.\"\"\"",
        "        self.processor = CorticalTextProcessor()",
        "",
        "        # Add a code file with multiple classes/functions",
        "        self.processor.process_document('cortical/example.py', '''",
        "\"\"\"Example module with multiple classes and functions.\"\"\"",
        "",
        "import os",
        "from typing import Dict, List",
        "",
        "class FirstProcessor:",
        "    \"\"\"First processor class for demonstration.\"\"\"",
        "",
        "    def __init__(self):",
        "        self.data = {}",
        "",
        "    def process(self, item):",
        "        \"\"\"Process a single item.\"\"\"",
        "        return item * 2",
        "",
        "",
        "class SecondProcessor:",
        "    \"\"\"Second processor class for demonstration.\"\"\"",
        "",
        "    def __init__(self):",
        "        self.cache = []",
        "",
        "    def process_batch(self, items):",
        "        \"\"\"Process multiple items at once.\"\"\"",
        "        return [x * 3 for x in items]",
        "",
        "",
        "def utility_function(x, y):",
        "    \"\"\"A utility function outside classes.\"\"\"",
        "    return x + y",
        "''')",
        "",
        "        self.processor.compute_all()",
        "",
        "    def test_code_aware_chunks_enabled_by_default(self):",
        "        \"\"\"Code-aware chunking should be enabled by default.\"\"\"",
        "        results = self.processor.find_passages_for_query(",
        "            \"SecondProcessor\",",
        "            top_n=5",
        "        )",
        "        self.assertTrue(len(results) > 0)",
        "",
        "    def test_code_aware_chunks_can_be_disabled(self):",
        "        \"\"\"Should be able to disable code-aware chunking.\"\"\"",
        "        results = self.processor.find_passages_for_query(",
        "            \"SecondProcessor\",",
        "            top_n=5,",
        "            use_code_aware_chunks=False",
        "        )",
        "        # Should still return results, just with fixed chunking",
        "        self.assertTrue(len(results) >= 0)",
        "",
        "    def test_processor_has_code_chunk_param(self):",
        "        \"\"\"Processor should accept use_code_aware_chunks parameter.\"\"\"",
        "        # Should not raise",
        "        results = self.processor.find_passages_for_query(",
        "            \"utility\",",
        "            use_code_aware_chunks=True",
        "        )",
        "",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "        )",
        "",
        "        self.assertTrue(len(results) > 0)",
        "",
        "    def test_processor_wrapper_exists(self):",
        "        \"\"\"Processor should have find_documents_with_boost method.\"\"\"",
        "        self.assertTrue(hasattr(self.processor, 'find_documents_with_boost'))",
        "        self.assertTrue(hasattr(self.processor, 'is_conceptual_query'))",
        "",
        ""
      ],
      "context_after": [
        "if __name__ == '__main__':",
        "    unittest.main()"
      ],
      "change_type": "add"
    }
  ],
  "hour_of_day": 10,
  "day_of_week": "Thursday",
  "seconds_since_last_commit": -357628,
  "is_merge": true,
  "is_initial": false,
  "parent_count": 2,
  "session_id": null,
  "related_chats": [],
  "ci_result": null,
  "reverted": false,
  "amended": false
}