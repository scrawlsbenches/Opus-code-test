{
  "hash": "2e53f77d5c4cf97c8dafbace72239db30b39da6a",
  "message": "Fix bigram connection O(nÂ²) explosion with term/doc filtering",
  "author": "Claude",
  "timestamp": "2025-12-11 22:17:38 +0000",
  "branch": "claude/multi-index-design-DvifZ",
  "files_changed": [
    "TASK_LIST.md",
    "cortical/analysis.py",
    "cortical/processor.py"
  ],
  "insertions": 60,
  "deletions": 6,
  "hunks": [
    {
      "file": "TASK_LIST.md",
      "function": null,
      "start_line": 1,
      "lines_added": [
        "**Pending Tasks:** 41"
      ],
      "lines_removed": [
        "**Pending Tasks:** 37"
      ],
      "context_before": [
        "# Task List: Cortical Text Processor",
        "",
        "Active backlog for the Cortical Text Processor project. Completed tasks are archived in [TASK_ARCHIVE.md](TASK_ARCHIVE.md).",
        "",
        "**Last Updated:** 2025-12-11"
      ],
      "context_after": [
        "**Completed Tasks:** 88+ (see archive)",
        "",
        "---",
        "",
        "## Active Backlog",
        "",
        "<!-- Machine-parseable format for automation -->",
        "",
        "### ðŸ”´ Critical (Do Now)",
        ""
      ],
      "change_type": "modify"
    },
    {
      "file": "TASK_LIST.md",
      "function": "Active backlog for the Cortical Text Processor project. Completed tasks are arch",
      "start_line": 26,
      "lines_added": [
        "| 136 | Optimize semantics O(nÂ²) similarity - add sampling/early-exit | Perf | - | Medium |",
        "| 137 | Cap bigram connections to top-K per bigram | Perf | - | Small |",
        "| 138 | Use sparse matrix multiplication for bigram connections | Perf | - | Medium |",
        "| 139 | Batch bigram connection updates to reduce dict overhead | Perf | - | Small |"
      ],
      "lines_removed": [],
      "context_before": [
        "| # | Task | Category | Depends | Effort |",
        "|---|------|----------|---------|--------|",
        "| 94 | Split query.py into focused modules | Arch | - | Large |",
        "| 97 | Integrate CorticalConfig into processor | Arch | - | Medium |",
        "| 127 | Create cluster coverage evaluation script | DevEx | 125 | Medium |",
        "",
        "### ðŸŸ¡ Medium (Do This Month)",
        "",
        "| # | Task | Category | Depends | Effort |",
        "|---|------|----------|---------|--------|"
      ],
      "context_after": [
        "| 133 | Implement WAL + snapshot persistence (fault-tolerant rebuild) | Arch | 132 | Large |",
        "| 134 | Implement protobuf serialization for corpus | Arch | 132 | Medium |",
        "| 135 | Implement chunked parallel processing for full-analysis | Arch | 132 | Large |",
        "| 91 | Create docs/README.md index | Docs | - | Small |",
        "| 92 | Add badges to README.md | DevEx | - | Small |",
        "| 93 | Update README with docs references | Docs | 91 | Small |",
        "| 95 | Split processor.py into modules | Arch | 97 | Large |",
        "| 96 | Centralize duplicate constants | CodeQual | - | Small |",
        "| 98 | Replace print() with logging | CodeQual | - | Medium |",
        "| 99 | Add input validation to public methods | CodeQual | - | Medium |"
      ],
      "change_type": "add"
    },
    {
      "file": "cortical/analysis.py",
      "function": "def compute_concept_connections(",
      "start_line": 1211,
      "lines_added": [
        "    cooccurrence_weight: float = 0.3,",
        "    max_bigrams_per_term: int = 100,",
        "    max_bigrams_per_doc: int = 500",
        "        max_bigrams_per_term: Skip terms appearing in more than this many bigrams",
        "            to avoid O(nÂ²) explosion from common terms like \"self\", \"return\" (default 100)",
        "        max_bigrams_per_doc: Skip documents with more than this many bigrams for",
        "            co-occurrence connections to avoid O(nÂ²) explosion (default 500)",
        "        - skipped_common_terms: Number of terms skipped due to max_bigrams_per_term",
        "        - skipped_large_docs: Number of docs skipped due to max_bigrams_per_doc",
        "            'cooccurrence_connections': 0,",
        "            'skipped_common_terms': 0,",
        "            'skipped_large_docs': 0"
      ],
      "lines_removed": [
        "    cooccurrence_weight: float = 0.3",
        "            'cooccurrence_connections': 0"
      ],
      "context_before": [
        "        'semantic_connections': semantic_connections,",
        "        'embedding_connections': embedding_connections",
        "    }",
        "",
        "",
        "def compute_bigram_connections(",
        "    layers: Dict[CorticalLayer, HierarchicalLayer],",
        "    min_shared_docs: int = 1,",
        "    component_weight: float = 0.5,",
        "    chain_weight: float = 0.7,"
      ],
      "context_after": [
        ") -> Dict[str, Any]:",
        "    \"\"\"",
        "    Build lateral connections between bigrams in Layer 1.",
        "",
        "    Bigrams are connected based on:",
        "    1. Shared component terms (\"neural_networks\" â†” \"neural_processing\")",
        "    2. Document co-occurrence (appear in same documents)",
        "    3. Chains (\"machine_learning\" â†” \"learning_algorithms\" where right=left)",
        "",
        "    Args:",
        "        layers: Dictionary of all layers",
        "        min_shared_docs: Minimum shared documents for co-occurrence connection",
        "        component_weight: Weight for shared component connections (default 0.5)",
        "        chain_weight: Weight for chain connections (default 0.7)",
        "        cooccurrence_weight: Weight for document co-occurrence (default 0.3)",
        "",
        "    Returns:",
        "        Statistics about connections created:",
        "        - connections_created: Total bidirectional connections",
        "        - component_connections: Connections from shared components",
        "        - chain_connections: Connections from chains",
        "        - cooccurrence_connections: Connections from document co-occurrence",
        "    \"\"\"",
        "    layer1 = layers[CorticalLayer.BIGRAMS]",
        "",
        "    if layer1.column_count() == 0:",
        "        return {",
        "            'connections_created': 0,",
        "            'bigrams': 0,",
        "            'component_connections': 0,",
        "            'chain_connections': 0,",
        "        }",
        "",
        "    bigrams = list(layer1.minicolumns.values())",
        "",
        "    # Build indexes for efficient lookup",
        "    # left_component_index: {\"neural\": [bigram1, bigram2, ...]}",
        "    # right_component_index: {\"networks\": [bigram1, bigram3, ...]}",
        "    # Note: Bigrams use space separators (e.g., \"neural networks\")",
        "    left_index: Dict[str, List[Minicolumn]] = defaultdict(list)",
        "    right_index: Dict[str, List[Minicolumn]] = defaultdict(list)"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/analysis.py",
      "function": "def compute_bigram_connections(",
      "start_line": 1293,
      "lines_added": [
        "    # Track skipped common terms for statistics",
        "    skipped_common_terms = 0",
        "",
        "        # Skip overly common terms to avoid O(nÂ²) explosion",
        "        if len(bigram_list) > max_bigrams_per_term:",
        "            skipped_common_terms += 1",
        "            continue",
        "        # Skip overly common terms to avoid O(nÂ²) explosion",
        "        if len(bigram_list) > max_bigrams_per_term:",
        "            skipped_common_terms += 1",
        "            continue",
        "            # Skip overly common terms",
        "            if len(left_index[term]) > max_bigrams_per_term or len(right_index[term]) > max_bigrams_per_term:",
        "                continue",
        "    skipped_large_docs = 0",
        "        # Skip documents with too many bigrams to avoid O(nÂ²) explosion",
        "        if len(doc_bigrams) > max_bigrams_per_doc:",
        "            skipped_large_docs += 1",
        "            continue",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "",
        "        if conn_type == 'component':",
        "            component_connections += 1",
        "        elif conn_type == 'chain':",
        "            chain_connections += 1",
        "        elif conn_type == 'cooccurrence':",
        "            cooccurrence_connections += 1",
        "",
        "        return True",
        ""
      ],
      "context_after": [
        "    # 1. Connect bigrams sharing a component",
        "    # Left component matches: \"neural_networks\" â†” \"neural_processing\"",
        "    for component, bigram_list in left_index.items():",
        "        for i, b1 in enumerate(bigram_list):",
        "            for b2 in bigram_list[i+1:]:",
        "                # Weight by component's PageRank importance (if available)",
        "                weight = component_weight",
        "                add_connection(b1, b2, weight, 'component')",
        "",
        "    # Right component matches: \"deep_learning\" â†” \"machine_learning\"",
        "    for component, bigram_list in right_index.items():",
        "        for i, b1 in enumerate(bigram_list):",
        "            for b2 in bigram_list[i+1:]:",
        "                weight = component_weight",
        "                add_connection(b1, b2, weight, 'component')",
        "",
        "    # 2. Connect chain bigrams (right of one = left of other)",
        "    # \"machine_learning\" â†” \"learning_algorithms\"",
        "    for term in left_index:",
        "        if term in right_index:",
        "            # term appears as right component in some bigrams and left in others",
        "            for b_left in right_index[term]:  # ends with term",
        "                for b_right in left_index[term]:  # starts with term",
        "                    if b_left.id != b_right.id:",
        "                        add_connection(b_left, b_right, chain_weight, 'chain')",
        "",
        "    # 3. Connect bigrams that co-occur in the same documents",
        "    # Use inverted index for O(d * bÂ²) instead of O(nÂ²) where d=docs, b=bigrams per doc",
        "    doc_to_bigrams: Dict[str, List[Minicolumn]] = defaultdict(list)",
        "    for bigram in bigrams:",
        "        for doc_id in bigram.document_ids:",
        "            doc_to_bigrams[doc_id].append(bigram)",
        "",
        "    # Track pairs we've already processed to avoid duplicate work",
        "    cooccur_processed: Set[Tuple[str, str]] = set()",
        "",
        "    for doc_id, doc_bigrams in doc_to_bigrams.items():",
        "        # Only compare bigrams within the same document",
        "        for i, b1 in enumerate(doc_bigrams):",
        "            docs1 = b1.document_ids",
        "            for b2 in doc_bigrams[i+1:]:",
        "                # Skip if already processed this pair",
        "                pair_key = tuple(sorted([b1.id, b2.id]))",
        "                if pair_key in cooccur_processed:",
        "                    continue",
        "                cooccur_processed.add(pair_key)",
        ""
      ],
      "change_type": "add"
    },
    {
      "file": "cortical/analysis.py",
      "function": "def compute_bigram_connections(",
      "start_line": 1353,
      "lines_added": [
        "        'cooccurrence_connections': cooccurrence_connections,",
        "        'skipped_common_terms': skipped_common_terms,",
        "        'skipped_large_docs': skipped_large_docs"
      ],
      "lines_removed": [
        "        'cooccurrence_connections': cooccurrence_connections"
      ],
      "context_before": [
        "                    # Weight by Jaccard similarity of document sets",
        "                    jaccard = len(shared_docs) / len(docs1 | docs2)",
        "                    weight = cooccurrence_weight * jaccard",
        "                    add_connection(b1, b2, weight, 'cooccurrence')",
        "",
        "    return {",
        "        'connections_created': len(connected_pairs),",
        "        'bigrams': len(bigrams),",
        "        'component_connections': component_connections,",
        "        'chain_connections': chain_connections,"
      ],
      "context_after": [
        "    }",
        "",
        "",
        "def compute_document_connections(",
        "    layers: Dict[CorticalLayer, HierarchicalLayer],",
        "    documents: Dict[str, str],",
        "    min_shared_terms: int = 3",
        ") -> None:",
        "    \"\"\"",
        "    Build lateral connections between documents."
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/processor.py",
      "function": "class CorticalTextProcessor:",
      "start_line": 886,
      "lines_added": [
        "        max_bigrams_per_term: int = 100,",
        "        max_bigrams_per_doc: int = 500,",
        "            max_bigrams_per_term: Skip terms appearing in more than this many bigrams",
        "                to avoid O(nÂ²) explosion from common terms like \"self\", \"return\" (default 100)",
        "            max_bigrams_per_doc: Skip documents with more than this many bigrams for",
        "                co-occurrence connections to avoid O(nÂ²) explosion (default 500)",
        "            - skipped_common_terms: Number of terms skipped due to max_bigrams_per_term",
        "            - skipped_large_docs: Number of docs skipped due to max_bigrams_per_doc",
        "            cooccurrence_weight=cooccurrence_weight,",
        "            max_bigrams_per_term=max_bigrams_per_term,",
        "            max_bigrams_per_doc=max_bigrams_per_doc",
        "            skipped_terms = stats.get('skipped_common_terms', 0)",
        "            skipped_docs = stats.get('skipped_large_docs', 0)",
        "            skip_parts = []",
        "            if skipped_terms:",
        "                skip_parts.append(f\"{skipped_terms} common terms\")",
        "            if skipped_docs:",
        "                skip_parts.append(f\"{skipped_docs} large docs\")",
        "            skip_msg = f\", skipped {', '.join(skip_parts)}\" if skip_parts else \"\"",
        "                  f\"cooccur: {stats['cooccurrence_connections']}{skip_msg})\")"
      ],
      "lines_removed": [
        "            cooccurrence_weight=cooccurrence_weight",
        "                  f\"cooccur: {stats['cooccurrence_connections']})\")"
      ],
      "context_before": [
        "    def compute_document_connections(self, min_shared_terms: int = 3, verbose: bool = True) -> None:",
        "        analysis.compute_document_connections(self.layers, self.documents, min_shared_terms)",
        "        if verbose: print(\"Computed document connections\")",
        "",
        "    def compute_bigram_connections(",
        "        self,",
        "        min_shared_docs: int = 1,",
        "        component_weight: float = 0.5,",
        "        chain_weight: float = 0.7,",
        "        cooccurrence_weight: float = 0.3,"
      ],
      "context_after": [
        "        verbose: bool = True",
        "    ) -> Dict[str, Any]:",
        "        \"\"\"",
        "        Build lateral connections between bigrams based on shared components and co-occurrence.",
        "",
        "        Bigrams are connected when they:",
        "        - Share a component term (\"neural_networks\" â†” \"neural_processing\")",
        "        - Form chains (\"machine_learning\" â†” \"learning_algorithms\")",
        "        - Co-occur in the same documents",
        "",
        "        Args:",
        "            min_shared_docs: Minimum shared documents for co-occurrence connection",
        "            component_weight: Weight for shared component connections (default 0.5)",
        "            chain_weight: Weight for chain connections (default 0.7)",
        "            cooccurrence_weight: Weight for document co-occurrence (default 0.3)",
        "            verbose: Print progress messages",
        "",
        "        Returns:",
        "            Statistics about connections created:",
        "            - connections_created: Total bidirectional connections",
        "            - component_connections: Connections from shared components",
        "            - chain_connections: Connections from chains",
        "            - cooccurrence_connections: Connections from document co-occurrence",
        "",
        "        Example:",
        "            >>> stats = processor.compute_bigram_connections()",
        "            >>> print(f\"Created {stats['connections_created']} bigram connections\")",
        "            >>> print(f\"  Component: {stats['component_connections']}\")",
        "            >>> print(f\"  Chain: {stats['chain_connections']}\")",
        "            >>> print(f\"  Co-occurrence: {stats['cooccurrence_connections']}\")",
        "        \"\"\"",
        "        stats = analysis.compute_bigram_connections(",
        "            self.layers,",
        "            min_shared_docs=min_shared_docs,",
        "            component_weight=component_weight,",
        "            chain_weight=chain_weight,",
        "        )",
        "        if verbose:",
        "            print(f\"Created {stats['connections_created']} bigram connections \"",
        "                  f\"(component: {stats['component_connections']}, \"",
        "                  f\"chain: {stats['chain_connections']}, \"",
        "        return stats",
        "",
        "    def build_concept_clusters(",
        "        self,",
        "        min_cluster_size: int = 3,",
        "        clustering_method: str = 'louvain',",
        "        cluster_strictness: float = 1.0,",
        "        bridge_weight: float = 0.0,",
        "        resolution: float = 1.0,",
        "        verbose: bool = True"
      ],
      "change_type": "modify"
    }
  ],
  "hour_of_day": 22,
  "day_of_week": "Thursday",
  "seconds_since_last_commit": -314830,
  "is_merge": false,
  "is_initial": false,
  "parent_count": 1,
  "session_id": null,
  "related_chats": [],
  "ci_result": null,
  "reverted": false,
  "amended": false
}