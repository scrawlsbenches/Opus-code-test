{
  "hash": "090910fc26603aeee5b851fbeafd133a9faae70d",
  "message": "refactor: Split processor.py into modular processor/ package (LEGACY-095)",
  "author": "Claude",
  "timestamp": "2025-12-14 14:26:00 +0000",
  "branch": "claude/multi-index-design-DvifZ",
  "files_changed": [
    "CLAUDE.md",
    "cortical/__init__.py",
    "cortical/processor/__init__.py",
    "cortical/processor/compute.py",
    "cortical/processor/core.py",
    "cortical/processor/documents.py",
    "cortical/processor/introspection.py",
    "cortical/processor/persistence_api.py",
    "cortical/processor/query_api.py"
  ],
  "insertions": 2834,
  "deletions": 4,
  "hunks": [
    {
      "file": "CLAUDE.md",
      "function": "cat cortical/query.py.ai_meta | head -100    # Get overview",
      "start_line": 122,
      "lines_added": [
        "├── processor/        # Main orchestrator package - START HERE",
        "│   │                 # CorticalTextProcessor is the public API (composed from mixins)",
        "│   ├── __init__.py   # Re-exports CorticalTextProcessor class",
        "│   ├── core.py       # Initialization, staleness tracking, layer management (~100 lines)",
        "│   ├── documents.py  # Document processing, add/remove, metadata (~450 lines)",
        "│   ├── compute.py    # compute_all, PageRank, TF-IDF, clustering (~750 lines)",
        "│   ├── query_api.py  # Search, expansion, retrieval methods (~550 lines)",
        "│   ├── introspection.py  # State inspection, fingerprints, summaries (~200 lines)",
        "│   └── persistence_api.py # Save/load/export methods (~200 lines)"
      ],
      "lines_removed": [
        "├── processor.py      # Main orchestrator (2,301 lines) - START HERE",
        "│                     # CorticalTextProcessor is the public API"
      ],
      "context_before": [
        "python scripts/search_codebase.py \"expand query\"  # Find specific code",
        "# Then read specific line ranges as needed",
        "```",
        "",
        "---",
        "",
        "## Architecture Map",
        "",
        "```",
        "cortical/"
      ],
      "context_after": [
        "├── query/            # Search, retrieval, query expansion (split into 8 modules)",
        "│   ├── __init__.py   # Re-exports public API",
        "│   ├── expansion.py  # Query expansion",
        "│   ├── search.py     # Document search",
        "│   ├── passages.py   # Passage retrieval",
        "│   ├── chunking.py   # Text chunking",
        "│   ├── intent.py     # Intent-based queries",
        "│   ├── definitions.py # Definition search",
        "│   ├── ranking.py    # Multi-stage ranking",
        "│   └── analogy.py    # Analogy completion"
      ],
      "change_type": "modify"
    },
    {
      "file": "CLAUDE.md",
      "function": "cortical/",
      "start_line": 160,
      "lines_added": [
        "| Add/modify public API | `processor/` package - methods split into focused mixins |",
        "| Modify document processing | `processor/documents.py` - add/remove documents |",
        "| Modify compute methods | `processor/compute.py` - PageRank, TF-IDF, clustering |",
        "| Add query features | `processor/query_api.py` - search, expansion, retrieval |",
        "| Add introspection | `processor/introspection.py` - fingerprints, gaps, summaries |",
        "| Modify persistence | `processor/persistence_api.py` - save/load/export |",
        "| Implement search/retrieval | `query/` - all search functions (8 modules) |"
      ],
      "lines_removed": [
        "| Add/modify public API | `processor.py` - wrapper methods call other modules |",
        "| Implement search/retrieval | `query.py` - all search functions |"
      ],
      "context_before": [
        "**For detailed architecture documentation**, see [docs/architecture.md](docs/architecture.md), which includes:",
        "- Complete module dependency graphs (ASCII + Mermaid)",
        "- Component interaction patterns",
        "- Data flow diagrams",
        "- Layer hierarchy details",
        "",
        "### Module Purpose Quick Reference",
        "",
        "| If you need to... | Look in... |",
        "|-------------------|------------|"
      ],
      "context_after": [
        "| Add graph algorithms | `analysis.py` - PageRank, TF-IDF, clustering |",
        "| Add semantic relations | `semantics.py` - pattern extraction, retrofitting |",
        "| Modify data structures | `minicolumn.py` - Minicolumn, Edge classes |",
        "| Change layer behavior | `layers.py` - HierarchicalLayer class |",
        "| Adjust tokenization | `tokenizer.py` - stemming, stop words, ngrams |",
        "| Change configuration | `config.py` - CorticalConfig dataclass |",
        "| Modify persistence | `persistence.py` - save/load, export formats |",
        "| Add code search features | `code_concepts.py` - programming synonyms |",
        "| Modify embeddings | `embeddings.py` - graph embedding methods |",
        "| Change gap detection | `gaps.py` - knowledge gap analysis |"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/__init__.py",
      "function": "Example:",
      "start_line": 10,
      "lines_added": [
        "# Import from processor package (split from monolithic processor.py)"
      ],
      "lines_removed": [],
      "context_before": [
        "    ",
        "    processor = CorticalTextProcessor()",
        "    processor.process_document(\"doc1\", \"Neural networks process information...\")",
        "    processor.compute_all()",
        "    results = processor.find_documents_for_query(\"neural processing\")",
        "\"\"\"",
        "",
        "from .tokenizer import Tokenizer",
        "from .minicolumn import Minicolumn, Edge",
        "from .layers import CorticalLayer, HierarchicalLayer"
      ],
      "context_after": [
        "from .processor import CorticalTextProcessor",
        "from .config import CorticalConfig, get_default_config, VALID_RELATION_CHAINS",
        "from .fluent import FluentProcessor",
        "from .progress import (",
        "    ProgressReporter,",
        "    ConsoleProgressReporter,",
        "    CallbackProgressReporter,",
        "    SilentProgressReporter,",
        "    MultiPhaseProgress,",
        ")"
      ],
      "change_type": "add"
    },
    {
      "file": "cortical/processor/__init__.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "\"\"\"",
        "Cortical Text Processor - Main processor package.",
        "",
        "This package splits the monolithic processor.py into focused modules:",
        "- core.py: Initialization, staleness tracking, layer management",
        "- documents.py: Document processing, adding, removing, metadata",
        "- compute.py: Analysis computations, clustering, embeddings, semantics",
        "- query_api.py: Search, expansion, retrieval methods",
        "- introspection.py: State inspection, fingerprints, gaps, summaries",
        "- persistence_api.py: Save, load, export methods",
        "",
        "The CorticalTextProcessor class is composed from mixins in each module,",
        "maintaining full backwards compatibility with the original API.",
        "\"\"\"",
        "",
        "from .core import CoreMixin",
        "from .documents import DocumentsMixin",
        "from .compute import ComputeMixin",
        "from .query_api import QueryMixin",
        "from .introspection import IntrospectionMixin",
        "from .persistence_api import PersistenceMixin",
        "",
        "",
        "class CorticalTextProcessor(",
        "    CoreMixin,",
        "    DocumentsMixin,",
        "    ComputeMixin,",
        "    QueryMixin,",
        "    IntrospectionMixin,",
        "    PersistenceMixin",
        "):",
        "    \"\"\"",
        "    Neocortex-inspired text processing system.",
        "",
        "    This class provides a complete API for:",
        "    - Document processing and management",
        "    - TF-IDF, PageRank, and graph analysis",
        "    - Semantic relation extraction",
        "    - Query expansion and document search",
        "    - Passage retrieval for RAG systems",
        "    - State persistence and export",
        "",
        "    Example:",
        "        >>> from cortical import CorticalTextProcessor",
        "        >>> processor = CorticalTextProcessor()",
        "        >>> processor.process_document(\"doc1\", \"Neural networks process data.\")",
        "        >>> processor.compute_all()",
        "        >>> results = processor.find_documents_for_query(\"neural\")",
        "        >>> processor.save(\"corpus.pkl\")",
        "",
        "    The processor is composed from focused mixins:",
        "    - CoreMixin: Initialization, staleness tracking",
        "    - DocumentsMixin: Document add/remove operations",
        "    - ComputeMixin: Analysis and computation methods",
        "    - QueryMixin: Search and retrieval methods",
        "    - IntrospectionMixin: State inspection and comparison",
        "    - PersistenceMixin: Save/load operations",
        "    \"\"\"",
        "    pass",
        "",
        "",
        "# Re-export for backwards compatibility",
        "__all__ = ['CorticalTextProcessor']"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "cortical/processor/compute.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "\"\"\"",
        "Compute methods: analysis, clustering, embeddings, semantic extraction.",
        "",
        "This module contains all methods that perform computational analysis on the corpus,",
        "including PageRank, TF-IDF, clustering, and checkpointing.",
        "\"\"\"",
        "",
        "import json",
        "import logging",
        "from datetime import datetime",
        "from pathlib import Path",
        "from typing import Dict, List, Tuple, Optional, Any, Set",
        "",
        "from ..layers import CorticalLayer",
        "from .. import analysis",
        "from .. import semantics",
        "from .. import embeddings as emb_module",
        "from ..progress import (",
        "    ProgressReporter,",
        "    ConsoleProgressReporter,",
        "    SilentProgressReporter,",
        "    MultiPhaseProgress",
        ")",
        "",
        "logger = logging.getLogger(__name__)",
        "",
        "",
        "class ComputeMixin:",
        "    \"\"\"",
        "    Mixin providing computation functionality.",
        "",
        "    Requires CoreMixin to be present (provides layers, documents, tokenizer,",
        "    config, COMP_*, _mark_all_stale, _mark_fresh, _stale_computations).",
        "    \"\"\"",
        "",
        "    def recompute(",
        "        self,",
        "        level: str = 'stale',",
        "        verbose: bool = True",
        "    ) -> Dict[str, bool]:",
        "        \"\"\"",
        "        Recompute specified analysis levels.",
        "",
        "        Use this after adding documents with recompute='none' to batch",
        "        the recomputation step.",
        "",
        "        Args:",
        "            level: What to recompute:",
        "                - 'stale': Only recompute what's marked as stale",
        "                - 'tfidf': Only TF-IDF (marks others stale)",
        "                - 'full': Run complete compute_all()",
        "            verbose: Print progress messages",
        "",
        "        Returns:",
        "            Dict indicating what was recomputed",
        "",
        "        Example:",
        "            >>> # Add documents without recomputation",
        "            >>> processor.add_document_incremental(\"doc1\", \"content\", recompute='none')",
        "            >>> processor.add_document_incremental(\"doc2\", \"content\", recompute='none')",
        "            >>> # Batch recompute",
        "            >>> processor.recompute(level='full')",
        "        \"\"\"",
        "        recomputed = {}",
        "",
        "        if level == 'full':",
        "            self.compute_all(verbose=verbose)",
        "            self._stale_computations.clear()",
        "            recomputed = {",
        "                self.COMP_ACTIVATION: True,",
        "                self.COMP_PAGERANK: True,",
        "                self.COMP_TFIDF: True,",
        "                self.COMP_DOC_CONNECTIONS: True,",
        "                self.COMP_BIGRAM_CONNECTIONS: True,",
        "                self.COMP_CONCEPTS: True,",
        "            }",
        "        elif level == 'tfidf':",
        "            self.compute_tfidf(verbose=verbose)",
        "            self._mark_fresh(self.COMP_TFIDF)",
        "            recomputed[self.COMP_TFIDF] = True",
        "        elif level == 'stale':",
        "            # Recompute only what's stale, in dependency order",
        "            if self.COMP_ACTIVATION in self._stale_computations:",
        "                self.propagate_activation(verbose=verbose)",
        "                self._mark_fresh(self.COMP_ACTIVATION)",
        "                recomputed[self.COMP_ACTIVATION] = True",
        "",
        "            if self.COMP_PAGERANK in self._stale_computations:",
        "                self.compute_importance(verbose=verbose)",
        "                self._mark_fresh(self.COMP_PAGERANK)",
        "                recomputed[self.COMP_PAGERANK] = True",
        "",
        "            if self.COMP_TFIDF in self._stale_computations:",
        "                self.compute_tfidf(verbose=verbose)",
        "                self._mark_fresh(self.COMP_TFIDF)",
        "                recomputed[self.COMP_TFIDF] = True",
        "",
        "            if self.COMP_DOC_CONNECTIONS in self._stale_computations:",
        "                self.compute_document_connections(verbose=verbose)",
        "                self._mark_fresh(self.COMP_DOC_CONNECTIONS)",
        "                recomputed[self.COMP_DOC_CONNECTIONS] = True",
        "",
        "            if self.COMP_BIGRAM_CONNECTIONS in self._stale_computations:",
        "                self.compute_bigram_connections(verbose=verbose)",
        "                self._mark_fresh(self.COMP_BIGRAM_CONNECTIONS)",
        "                recomputed[self.COMP_BIGRAM_CONNECTIONS] = True",
        "",
        "            if self.COMP_CONCEPTS in self._stale_computations:",
        "                self.build_concept_clusters(verbose=verbose)",
        "                self._mark_fresh(self.COMP_CONCEPTS)",
        "                recomputed[self.COMP_CONCEPTS] = True",
        "",
        "            if self.COMP_EMBEDDINGS in self._stale_computations:",
        "                self.compute_graph_embeddings(verbose=verbose)",
        "                self._mark_fresh(self.COMP_EMBEDDINGS)",
        "                recomputed[self.COMP_EMBEDDINGS] = True",
        "",
        "            if self.COMP_SEMANTICS in self._stale_computations:",
        "                self.extract_corpus_semantics(verbose=verbose)",
        "                self._mark_fresh(self.COMP_SEMANTICS)",
        "                recomputed[self.COMP_SEMANTICS] = True",
        "",
        "        return recomputed",
        "",
        "    def compute_all(",
        "        self,",
        "        verbose: bool = True,",
        "        build_concepts: bool = True,",
        "        pagerank_method: str = 'standard',",
        "        connection_strategy: str = 'document_overlap',",
        "        cluster_strictness: float = 1.0,",
        "        bridge_weight: float = 0.0,",
        "        progress_callback: Optional[ProgressReporter] = None,",
        "        show_progress: bool = False,",
        "        checkpoint_dir: Optional[str] = None,",
        "        resume: bool = False",
        "    ) -> Dict[str, Any]:",
        "        \"\"\"",
        "        Run all computation steps.",
        "",
        "        Args:",
        "            verbose: Print progress messages (deprecated, use show_progress)",
        "            build_concepts: Build concept clusters in Layer 2 (default True)",
        "                           This enables topic-based filtering and hierarchical search.",
        "            pagerank_method: PageRank algorithm to use:",
        "                - 'standard': Traditional PageRank using connection weights",
        "                - 'semantic': ConceptNet-style PageRank with relation type weighting.",
        "                              Requires semantic relations (extracts automatically if needed).",
        "                - 'hierarchical': Cross-layer PageRank with importance propagation",
        "                                  between layers (tokens <-> bigrams <-> concepts <-> documents).",
        "            connection_strategy: Strategy for connecting Layer 2 concepts:",
        "                - 'document_overlap': Traditional Jaccard similarity (default)",
        "                - 'semantic': Connect via semantic relations between members",
        "                - 'embedding': Connect via embedding centroid similarity",
        "                - 'hybrid': Combine all three strategies for maximum connectivity",
        "            cluster_strictness: Controls clustering aggressiveness (0.0-1.0).",
        "                Lower values create fewer, larger clusters with more connections.",
        "            bridge_weight: Weight for inter-document token bridging (0.0-1.0).",
        "                Higher values help bridge topic-isolated clusters.",
        "            progress_callback: Optional ProgressReporter for custom progress tracking",
        "            show_progress: Show progress bar on console (uses stderr)",
        "            checkpoint_dir: Directory to save checkpoints after each phase (enables checkpointing).",
        "                If None (default), no checkpointing is performed.",
        "            resume: If True, resume from last checkpoint in checkpoint_dir.",
        "                Requires checkpoint_dir to be set.",
        "",
        "        Returns:",
        "            Dict with computation statistics (concept_stats, etc.)",
        "",
        "        Example:",
        "            >>> # Default behavior (silent)",
        "            >>> processor.compute_all()",
        "            >>>",
        "            >>> # With console progress bar",
        "            >>> processor.compute_all(show_progress=True)",
        "            >>>",
        "            >>> # With checkpointing for long-running operations",
        "            >>> processor.compute_all(checkpoint_dir='checkpoints')",
        "            >>>",
        "            >>> # Resume from checkpoint after crash/timeout",
        "            >>> processor = CorticalTextProcessor.resume_from_checkpoint('checkpoints')",
        "            >>> processor.compute_all(checkpoint_dir='checkpoints', resume=True)",
        "        \"\"\"",
        "        stats: Dict[str, Any] = {}",
        "",
        "        # Load checkpoint progress if resuming",
        "        completed_phases: Set[str] = set()",
        "        if resume and checkpoint_dir:",
        "            completed_phases = self._load_checkpoint_progress(checkpoint_dir)",
        "            if verbose and completed_phases:",
        "                logger.info(f\"Resuming from checkpoint with {len(completed_phases)} completed phases\")",
        "",
        "        # Set up progress reporter",
        "        if progress_callback:",
        "            reporter = progress_callback",
        "        elif show_progress:",
        "            reporter = ConsoleProgressReporter()",
        "        else:",
        "            reporter = SilentProgressReporter()",
        "",
        "        # Define phase weights based on typical execution times",
        "        phase_weights = {",
        "            \"Activation propagation\": 5,",
        "            \"PageRank computation\": 10,",
        "            \"TF-IDF computation\": 15,",
        "            \"Document connections\": 10,",
        "            \"Bigram connections\": 30,",
        "        }",
        "",
        "        # Add concept-related phases if building concepts",
        "        if build_concepts:",
        "            phase_weights[\"Concept clustering\"] = 15",
        "            if connection_strategy in ('semantic', 'hybrid'):",
        "                phase_weights[\"Semantic extraction\"] = 10",
        "            if connection_strategy in ('embedding', 'hybrid'):",
        "                phase_weights[\"Graph embeddings\"] = 10",
        "            phase_weights[\"Concept connections\"] = 15",
        "",
        "        # Create multi-phase progress tracker",
        "        progress = MultiPhaseProgress(reporter, phase_weights)",
        "",
        "        # Phase 1: Activation propagation",
        "        phase_name = \"activation_propagation\"",
        "        if phase_name in completed_phases:",
        "            if verbose:",
        "                logger.info(\"  Skipping activation propagation (already checkpointed)\")",
        "        else:",
        "            progress.start_phase(\"Activation propagation\")",
        "            if verbose:",
        "                logger.info(\"Computing activation propagation...\")",
        "            self.propagate_activation(verbose=False)",
        "            progress.update(100)",
        "            progress.complete_phase()",
        "",
        "            if checkpoint_dir:",
        "                self._save_checkpoint(checkpoint_dir, phase_name, verbose=verbose)",
        "",
        "        # Phase 2: PageRank (varies by method)",
        "        phase_name = f\"pagerank_{pagerank_method}\"",
        "        if phase_name in completed_phases:",
        "            if verbose:",
        "                logger.info(f\"  Skipping PageRank computation ({pagerank_method}) (already checkpointed)\")",
        "        else:",
        "            progress.start_phase(\"PageRank computation\")",
        "            if pagerank_method == 'semantic':",
        "                if not self.semantic_relations:",
        "                    if verbose:",
        "                        logger.info(\"Extracting semantic relations...\")",
        "                    progress.update(30, \"Extracting semantic relations\")",
        "                    self.extract_corpus_semantics(verbose=False)",
        "                if verbose:",
        "                    logger.info(\"Computing importance (Semantic PageRank)...\")",
        "                progress.update(70, \"Computing semantic PageRank\")",
        "                self.compute_semantic_importance(verbose=False)",
        "            elif pagerank_method == 'hierarchical':",
        "                if verbose:",
        "                    logger.info(\"Computing importance (Hierarchical PageRank)...\")",
        "                progress.update(50, \"Computing hierarchical PageRank\")",
        "                self.compute_hierarchical_importance(verbose=False)",
        "            else:",
        "                if verbose:",
        "                    logger.info(\"Computing importance (PageRank)...\")",
        "                progress.update(50, \"Computing PageRank\")",
        "                self.compute_importance(verbose=False)",
        "            progress.update(100)",
        "            progress.complete_phase()",
        "",
        "            if checkpoint_dir:",
        "                self._save_checkpoint(checkpoint_dir, phase_name, verbose=verbose)",
        "",
        "        # Phase 3: TF-IDF",
        "        phase_name = \"tfidf\"",
        "        if phase_name in completed_phases:",
        "            if verbose:",
        "                logger.info(\"  Skipping TF-IDF computation (already checkpointed)\")",
        "        else:",
        "            progress.start_phase(\"TF-IDF computation\")",
        "            if verbose:",
        "                logger.info(\"Computing TF-IDF...\")",
        "            self.compute_tfidf(verbose=False)",
        "            progress.update(100)",
        "            progress.complete_phase()",
        "",
        "            if checkpoint_dir:",
        "                self._save_checkpoint(checkpoint_dir, phase_name, verbose=verbose)",
        "",
        "        # Phase 4: Document connections",
        "        phase_name = \"document_connections\"",
        "        if phase_name in completed_phases:",
        "            if verbose:",
        "                logger.info(\"  Skipping document connections (already checkpointed)\")",
        "        else:",
        "            progress.start_phase(\"Document connections\")",
        "            if verbose:",
        "                logger.info(\"Computing document connections...\")",
        "            self.compute_document_connections(verbose=False)",
        "            progress.update(100)",
        "            progress.complete_phase()",
        "",
        "            if checkpoint_dir:",
        "                self._save_checkpoint(checkpoint_dir, phase_name, verbose=verbose)",
        "",
        "        # Phase 5: Bigram connections",
        "        phase_name = \"bigram_connections\"",
        "        if phase_name in completed_phases:",
        "            if verbose:",
        "                logger.info(\"  Skipping bigram connections (already checkpointed)\")",
        "        else:",
        "            progress.start_phase(\"Bigram connections\")",
        "            if verbose:",
        "                logger.info(\"Computing bigram connections...\")",
        "            self.compute_bigram_connections(verbose=False)",
        "            progress.update(100)",
        "            progress.complete_phase()",
        "",
        "            if checkpoint_dir:",
        "                self._save_checkpoint(checkpoint_dir, phase_name, verbose=verbose)",
        "",
        "        if build_concepts:",
        "            # Phase 6: Concept clustering",
        "            phase_name = \"concept_clustering\"",
        "            if phase_name in completed_phases:",
        "                if verbose:",
        "                    logger.info(\"  Skipping concept clustering (already checkpointed)\")",
        "                stats['clusters_created'] = len([c for c in self.layers[CorticalLayer.CONCEPTS].minicolumns.values()])",
        "            else:",
        "                progress.start_phase(\"Concept clustering\")",
        "                if verbose:",
        "                    logger.info(\"Building concept clusters...\")",
        "                clusters = self.build_concept_clusters(",
        "                    cluster_strictness=cluster_strictness,",
        "                    bridge_weight=bridge_weight,",
        "                    verbose=False",
        "                )",
        "                stats['clusters_created'] = len(clusters)",
        "                progress.update(100)",
        "                progress.complete_phase()",
        "",
        "                if checkpoint_dir:",
        "                    self._save_checkpoint(checkpoint_dir, phase_name, verbose=verbose)",
        "",
        "            # Determine connection parameters based on strategy",
        "            use_member_semantics = connection_strategy in ('semantic', 'hybrid')",
        "            use_embedding_similarity = connection_strategy in ('embedding', 'hybrid')",
        "",
        "            # Phase 7: Semantic extraction (if needed)",
        "            if use_member_semantics and not self.semantic_relations:",
        "                phase_name = \"semantic_extraction\"",
        "                if phase_name in completed_phases:",
        "                    if verbose:",
        "                        logger.info(\"  Skipping semantic extraction (already checkpointed)\")",
        "                else:",
        "                    progress.start_phase(\"Semantic extraction\")",
        "                    if verbose:",
        "                        logger.info(\"Extracting semantic relations...\")",
        "                    self.extract_corpus_semantics(verbose=False)",
        "                    progress.update(100)",
        "                    progress.complete_phase()",
        "",
        "                    if checkpoint_dir:",
        "                        self._save_checkpoint(checkpoint_dir, phase_name, verbose=verbose)",
        "",
        "            # Phase 8: Graph embeddings (if needed)",
        "            if use_embedding_similarity and not self.embeddings:",
        "                phase_name = \"graph_embeddings\"",
        "                if phase_name in completed_phases:",
        "                    if verbose:",
        "                        logger.info(\"  Skipping graph embeddings (already checkpointed)\")",
        "                else:",
        "                    progress.start_phase(\"Graph embeddings\")",
        "                    if verbose:",
        "                        logger.info(\"Computing graph embeddings...\")",
        "                    self.compute_graph_embeddings(verbose=False)",
        "                    progress.update(100)",
        "                    progress.complete_phase()",
        "",
        "                    if checkpoint_dir:",
        "                        self._save_checkpoint(checkpoint_dir, phase_name, verbose=verbose)",
        "",
        "            # Set thresholds based on strategy",
        "            if connection_strategy == 'hybrid':",
        "                min_shared_docs = 0",
        "                min_jaccard = 0.0",
        "            elif connection_strategy in ('semantic', 'embedding'):",
        "                min_shared_docs = 0",
        "                min_jaccard = 0.0",
        "            else:  # document_overlap",
        "                min_shared_docs = 1",
        "                min_jaccard = 0.1",
        "",
        "            # Phase 9: Concept connections",
        "            phase_name = f\"concept_connections_{connection_strategy}\"",
        "            if phase_name in completed_phases:",
        "                if verbose:",
        "                    logger.info(f\"  Skipping concept connections ({connection_strategy}) (already checkpointed)\")",
        "                stats['concept_connections'] = {'strategy': connection_strategy}",
        "            else:",
        "                progress.start_phase(\"Concept connections\")",
        "                if verbose:",
        "                    logger.info(f\"Computing concept connections ({connection_strategy})...\")",
        "                concept_stats = self.compute_concept_connections(",
        "                    use_member_semantics=use_member_semantics,",
        "                    use_embedding_similarity=use_embedding_similarity,",
        "                    min_shared_docs=min_shared_docs,",
        "                    min_jaccard=min_jaccard,",
        "                    verbose=False",
        "                )",
        "                stats['concept_connections'] = concept_stats",
        "                progress.update(100)",
        "                progress.complete_phase()",
        "",
        "                if checkpoint_dir:",
        "                    self._save_checkpoint(checkpoint_dir, phase_name, verbose=verbose)",
        "",
        "        # Mark core computations as fresh",
        "        fresh_comps = [",
        "            self.COMP_ACTIVATION,",
        "            self.COMP_PAGERANK,",
        "            self.COMP_TFIDF,",
        "            self.COMP_DOC_CONNECTIONS,",
        "            self.COMP_BIGRAM_CONNECTIONS,",
        "        ]",
        "        if build_concepts:",
        "            fresh_comps.append(self.COMP_CONCEPTS)",
        "        self._mark_fresh(*fresh_comps)",
        "",
        "        # Invalidate query cache since corpus state changed",
        "        self._query_expansion_cache.clear()",
        "",
        "        if verbose:",
        "            logger.info(\"Done.\")",
        "",
        "        return stats",
        "",
        "    def _save_checkpoint(self, checkpoint_dir: str, completed_phase: str, verbose: bool = True) -> None:",
        "        \"\"\"",
        "        Save checkpoint after completing a phase.",
        "",
        "        Args:",
        "            checkpoint_dir: Directory to save checkpoint files",
        "            completed_phase: Name of the phase that was just completed",
        "            verbose: Print progress messages",
        "        \"\"\"",
        "        checkpoint_path = Path(checkpoint_dir)",
        "        checkpoint_path.mkdir(parents=True, exist_ok=True)",
        "",
        "        # Save current state using save_json",
        "        self.save_json(checkpoint_dir, force=True, verbose=False)",
        "",
        "        # Track completed phases in a separate progress file",
        "        progress_file = checkpoint_path / 'checkpoint_progress.json'",
        "        progress_data = {",
        "            'completed_phases': [],",
        "            'last_updated': datetime.now().isoformat()",
        "        }",
        "",
        "        # Load existing progress if it exists",
        "        if progress_file.exists():",
        "            try:",
        "                with open(progress_file, 'r', encoding='utf-8') as f:",
        "                    progress_data = json.load(f)",
        "            except (json.JSONDecodeError, IOError):",
        "                pass  # Use fresh progress data",
        "",
        "        # Add newly completed phase",
        "        if completed_phase not in progress_data['completed_phases']:",
        "            progress_data['completed_phases'].append(completed_phase)",
        "            progress_data['last_updated'] = datetime.now().isoformat()",
        "",
        "        # Write progress file atomically",
        "        temp_progress_file = progress_file.with_suffix('.json.tmp')",
        "        try:",
        "            with open(temp_progress_file, 'w', encoding='utf-8') as f:",
        "                json.dump(progress_data, f, indent=2, ensure_ascii=False)",
        "            temp_progress_file.replace(progress_file)",
        "        except Exception:",
        "            if temp_progress_file.exists():",
        "                temp_progress_file.unlink()",
        "            raise",
        "",
        "        if verbose:",
        "            logger.info(f\"  Checkpoint saved: {completed_phase}\")",
        "",
        "    def _load_checkpoint_progress(self, checkpoint_dir: str) -> Set[str]:",
        "        \"\"\"",
        "        Load completed phases from checkpoint directory.",
        "",
        "        Args:",
        "            checkpoint_dir: Directory containing checkpoint files",
        "",
        "        Returns:",
        "            Set of completed phase names",
        "        \"\"\"",
        "        progress_file = Path(checkpoint_dir) / 'checkpoint_progress.json'",
        "        if not progress_file.exists():",
        "            return set()",
        "",
        "        try:",
        "            with open(progress_file, 'r', encoding='utf-8') as f:",
        "                data = json.load(f)",
        "            return set(data.get('completed_phases', []))",
        "        except (json.JSONDecodeError, IOError) as e:",
        "            logger.warning(f\"Failed to load checkpoint progress: {e}\")",
        "            return set()",
        "",
        "    @classmethod",
        "    def resume_from_checkpoint(",
        "        cls,",
        "        checkpoint_dir: str,",
        "        config: Optional['CorticalConfig'] = None,",
        "        verbose: bool = True",
        "    ) -> 'CorticalTextProcessor':",
        "        \"\"\"",
        "        Resume processing from a checkpoint directory.",
        "",
        "        This is a convenience method that loads the processor state from",
        "        a checkpoint directory created by compute_all() with checkpointing enabled.",
        "",
        "        Args:",
        "            checkpoint_dir: Directory containing checkpoint files",
        "            config: Optional configuration (default: uses CorticalConfig defaults)",
        "            verbose: Print progress messages",
        "",
        "        Returns:",
        "            Reconstructed CorticalTextProcessor instance ready to resume computation",
        "",
        "        Raises:",
        "            FileNotFoundError: If checkpoint directory doesn't exist",
        "",
        "        Example:",
        "            >>> # After a crash during compute_all()",
        "            >>> processor = CorticalTextProcessor.resume_from_checkpoint('checkpoints')",
        "            >>> # Continue from where it left off",
        "            >>> processor.compute_all(checkpoint_dir='checkpoints', resume=True)",
        "        \"\"\"",
        "        if verbose:",
        "            logger.info(f\"Resuming from checkpoint: {checkpoint_dir}\")",
        "",
        "        # Load the processor state from JSON",
        "        processor = cls.load_json(checkpoint_dir, config=config, verbose=verbose)",
        "",
        "        # Load and display progress",
        "        progress = processor._load_checkpoint_progress(checkpoint_dir)",
        "        if verbose and progress:",
        "            logger.info(f\"Found {len(progress)} completed phases: {', '.join(sorted(progress))}\")",
        "",
        "        return processor",
        "",
        "    def propagate_activation(self, iterations: int = 3, decay: float = 0.8, verbose: bool = True) -> None:",
        "        analysis.propagate_activation(self.layers, iterations, decay)",
        "        if verbose:",
        "            logger.info(f\"Propagated activation ({iterations} iterations)\")",
        "",
        "    def compute_importance(self, verbose: bool = True) -> None:",
        "        for layer_enum in [CorticalLayer.TOKENS, CorticalLayer.BIGRAMS]:",
        "            analysis.compute_pagerank(self.layers[layer_enum])",
        "        if verbose:",
        "            logger.info(\"Computed PageRank importance\")",
        "",
        "    def compute_semantic_importance(",
        "        self,",
        "        relation_weights: Optional[Dict[str, float]] = None,",
        "        verbose: bool = True",
        "    ) -> Dict[str, Any]:",
        "        \"\"\"",
        "        Compute PageRank with semantic relation weighting.",
        "",
        "        Uses semantic relations to weight edges in the PageRank graph.",
        "        Edges with stronger semantic relationships (e.g., IsA, PartOf) receive",
        "        higher weights, affecting importance propagation.",
        "",
        "        Args:",
        "            relation_weights: Optional custom relation type weights dict.",
        "                Defaults to built-in weights (IsA: 1.5, PartOf: 1.3, etc.)",
        "            verbose: Print progress messages",
        "",
        "        Returns:",
        "            Dict with statistics:",
        "            - total_edges_with_relations: Sum across layers",
        "            - token_layer: Stats for token layer",
        "            - bigram_layer: Stats for bigram layer",
        "        \"\"\"",
        "        if not self.semantic_relations:",
        "            self.compute_importance(verbose=verbose)",
        "            return {",
        "                'total_edges_with_relations': 0,",
        "                'token_layer': {'edges_with_relations': 0},",
        "                'bigram_layer': {'edges_with_relations': 0}",
        "            }",
        "",
        "        total_edges = 0",
        "        layer_stats = {}",
        "",
        "        for layer_enum in [CorticalLayer.TOKENS, CorticalLayer.BIGRAMS]:",
        "            result = analysis.compute_semantic_pagerank(",
        "                self.layers[layer_enum],",
        "                self.semantic_relations,",
        "                relation_weights=relation_weights",
        "            )",
        "            layer_name = 'token_layer' if layer_enum == CorticalLayer.TOKENS else 'bigram_layer'",
        "            layer_stats[layer_name] = {",
        "                'iterations_run': result['iterations_run'],",
        "                'edges_with_relations': result['edges_with_relations']",
        "            }",
        "            total_edges += result['edges_with_relations']",
        "",
        "        if verbose:",
        "            logger.info(f\"Computed semantic PageRank ({total_edges} relation-weighted edges)\")",
        "",
        "        return {",
        "            'total_edges_with_relations': total_edges,",
        "            **layer_stats",
        "        }",
        "",
        "    def compute_hierarchical_importance(",
        "        self,",
        "        layer_iterations: int = 10,",
        "        global_iterations: int = 5,",
        "        cross_layer_damping: Optional[float] = None,",
        "        verbose: bool = True",
        "    ) -> Dict[str, Any]:",
        "        \"\"\"",
        "        Compute PageRank with cross-layer propagation.",
        "",
        "        This hierarchical PageRank allows importance to flow between layers:",
        "        - Upward: tokens -> bigrams -> concepts -> documents",
        "        - Downward: documents -> concepts -> bigrams -> tokens",
        "",
        "        Args:",
        "            layer_iterations: Max iterations for intra-layer PageRank (default 10)",
        "            global_iterations: Max iterations for cross-layer propagation (default 5)",
        "            cross_layer_damping: Damping factor at layer boundaries (default from config)",
        "            verbose: Print progress messages",
        "",
        "        Returns:",
        "            Dict with statistics:",
        "            - iterations_run: Number of global iterations",
        "            - converged: Whether the algorithm converged",
        "            - layer_stats: Per-layer statistics",
        "        \"\"\"",
        "        if cross_layer_damping is None:",
        "            cross_layer_damping = self.config.cross_layer_damping",
        "",
        "        result = analysis.compute_hierarchical_pagerank(",
        "            self.layers,",
        "            layer_iterations=layer_iterations,",
        "            global_iterations=global_iterations,",
        "            cross_layer_damping=cross_layer_damping",
        "        )",
        "",
        "        if verbose:",
        "            status = \"converged\" if result['converged'] else \"did not converge\"",
        "            logger.info(f\"Computed hierarchical PageRank ({result['iterations_run']} iterations, {status})\")",
        "",
        "        return result",
        "",
        "    def compute_tfidf(self, verbose: bool = True) -> None:",
        "        analysis.compute_tfidf(self.layers, self.documents)",
        "        if verbose:",
        "            logger.info(\"Computed TF-IDF scores\")",
        "",
        "    def compute_document_connections(self, min_shared_terms: int = 3, verbose: bool = True) -> None:",
        "        analysis.compute_document_connections(self.layers, self.documents, min_shared_terms)",
        "        if verbose:",
        "            logger.info(\"Computed document connections\")",
        "",
        "    def compute_bigram_connections(",
        "        self,",
        "        min_shared_docs: int = 1,",
        "        component_weight: float = 0.5,",
        "        chain_weight: float = 0.7,",
        "        cooccurrence_weight: float = 0.3,",
        "        max_bigrams_per_term: int = 100,",
        "        max_bigrams_per_doc: int = 500,",
        "        max_connections_per_bigram: int = 50,",
        "        verbose: bool = True",
        "    ) -> Dict[str, Any]:",
        "        \"\"\"",
        "        Build lateral connections between bigrams based on shared components and co-occurrence.",
        "",
        "        Args:",
        "            min_shared_docs: Minimum shared documents for co-occurrence connection",
        "            component_weight: Weight for shared component connections (default 0.5)",
        "            chain_weight: Weight for chain connections (default 0.7)",
        "            cooccurrence_weight: Weight for document co-occurrence (default 0.3)",
        "            max_bigrams_per_term: Skip terms appearing in more than this many bigrams",
        "            max_bigrams_per_doc: Skip documents with more than this many bigrams",
        "            max_connections_per_bigram: Maximum lateral connections per bigram",
        "            verbose: Print progress messages",
        "",
        "        Returns:",
        "            Statistics about connections created",
        "        \"\"\"",
        "        stats = analysis.compute_bigram_connections(",
        "            self.layers,",
        "            min_shared_docs=min_shared_docs,",
        "            component_weight=component_weight,",
        "            chain_weight=chain_weight,",
        "            cooccurrence_weight=cooccurrence_weight,",
        "            max_bigrams_per_term=max_bigrams_per_term,",
        "            max_bigrams_per_doc=max_bigrams_per_doc,",
        "            max_connections_per_bigram=max_connections_per_bigram",
        "        )",
        "        if verbose:",
        "            skipped_terms = stats.get('skipped_common_terms', 0)",
        "            skipped_docs = stats.get('skipped_large_docs', 0)",
        "            skipped_conns = stats.get('skipped_max_connections', 0)",
        "            skip_parts = []",
        "            if skipped_terms:",
        "                skip_parts.append(f\"{skipped_terms} common terms\")",
        "            if skipped_docs:",
        "                skip_parts.append(f\"{skipped_docs} large docs\")",
        "            if skipped_conns:",
        "                skip_parts.append(f\"{skipped_conns} over-limit\")",
        "            skip_msg = f\", skipped {', '.join(skip_parts)}\" if skip_parts else \"\"",
        "            logger.info(f\"Created {stats['connections_created']} bigram connections \"",
        "                        f\"(component: {stats['component_connections']}, \"",
        "                        f\"chain: {stats['chain_connections']}, \"",
        "                        f\"cooccur: {stats['cooccurrence_connections']}{skip_msg})\")",
        "        return stats",
        "",
        "    def build_concept_clusters(",
        "        self,",
        "        min_cluster_size: Optional[int] = None,",
        "        clustering_method: str = 'louvain',",
        "        cluster_strictness: Optional[float] = None,",
        "        bridge_weight: float = 0.0,",
        "        resolution: Optional[float] = None,",
        "        verbose: bool = True",
        "    ) -> Dict[int, List[str]]:",
        "        \"\"\"",
        "        Build concept clusters from token layer.",
        "",
        "        Args:",
        "            min_cluster_size: Minimum tokens per cluster (default from config)",
        "            clustering_method: Algorithm to use ('louvain' or 'label_propagation')",
        "            cluster_strictness: For label_propagation only (0.0-1.0)",
        "            bridge_weight: For label_propagation only (0.0-1.0)",
        "            resolution: For louvain only (default from config)",
        "            verbose: Print progress messages",
        "",
        "        Returns:",
        "            Dictionary mapping cluster_id to list of token contents",
        "        \"\"\"",
        "        if min_cluster_size is None:",
        "            min_cluster_size = self.config.min_cluster_size",
        "        if cluster_strictness is None:",
        "            cluster_strictness = self.config.cluster_strictness",
        "        if resolution is None:",
        "            resolution = self.config.louvain_resolution",
        "",
        "        if clustering_method == 'louvain':",
        "            clusters = analysis.cluster_by_louvain(",
        "                self.layers[CorticalLayer.TOKENS],",
        "                min_cluster_size=min_cluster_size,",
        "                resolution=resolution",
        "            )",
        "        elif clustering_method == 'label_propagation':",
        "            clusters = analysis.cluster_by_label_propagation(",
        "                self.layers[CorticalLayer.TOKENS],",
        "                min_cluster_size=min_cluster_size,",
        "                cluster_strictness=cluster_strictness,",
        "                bridge_weight=bridge_weight",
        "            )",
        "        else:",
        "            raise ValueError(",
        "                f\"Unknown clustering_method: {clustering_method}. \"",
        "                f\"Use 'louvain' or 'label_propagation'.\"",
        "            )",
        "",
        "        analysis.build_concept_clusters(self.layers, clusters)",
        "        if verbose:",
        "            logger.info(f\"Built {len(clusters)} concept clusters using {clustering_method}\")",
        "        return clusters",
        "",
        "    def compute_clustering_quality(self, sample_size: int = 500) -> Dict[str, Any]:",
        "        \"\"\"",
        "        Compute clustering quality metrics for the concept layer.",
        "",
        "        Args:",
        "            sample_size: Max tokens to sample for silhouette calculation",
        "",
        "        Returns:",
        "            Dictionary with modularity, silhouette, balance, num_clusters, quality_assessment",
        "        \"\"\"",
        "        return analysis.compute_clustering_quality(self.layers, sample_size)",
        "",
        "    def compute_concept_connections(",
        "        self,",
        "        use_semantics: bool = True,",
        "        min_shared_docs: int = 1,",
        "        min_jaccard: float = 0.1,",
        "        use_member_semantics: bool = False,",
        "        use_embedding_similarity: bool = False,",
        "        embedding_threshold: float = 0.3,",
        "        verbose: bool = True",
        "    ) -> Dict[str, Any]:",
        "        \"\"\"",
        "        Build lateral connections between concepts based on document overlap and semantics.",
        "",
        "        Args:",
        "            use_semantics: Use semantic relations to boost connection weights",
        "            min_shared_docs: Minimum shared documents for connection",
        "            min_jaccard: Minimum Jaccard similarity threshold",
        "            use_member_semantics: Connect concepts via member token semantic relations",
        "            use_embedding_similarity: Connect concepts via embedding centroid similarity",
        "            embedding_threshold: Minimum cosine similarity for embedding connections",
        "            verbose: Print progress messages",
        "",
        "        Returns:",
        "            Statistics about connections created",
        "        \"\"\"",
        "        semantic_rels = self.semantic_relations if use_semantics else None",
        "        emb = self.embeddings if use_embedding_similarity else None",
        "        stats = analysis.compute_concept_connections(",
        "            self.layers,",
        "            semantic_relations=semantic_rels,",
        "            min_shared_docs=min_shared_docs,",
        "            min_jaccard=min_jaccard,",
        "            use_member_semantics=use_member_semantics,",
        "            use_embedding_similarity=use_embedding_similarity,",
        "            embedding_threshold=embedding_threshold,",
        "            embeddings=emb",
        "        )",
        "        if verbose:",
        "            parts = [f\"Created {stats['connections_created']} concept connections\"]",
        "            if stats.get('doc_overlap_connections', 0) > 0:",
        "                parts.append(f\"doc_overlap: {stats['doc_overlap_connections']}\")",
        "            if stats.get('semantic_connections', 0) > 0:",
        "                parts.append(f\"semantic: {stats['semantic_connections']}\")",
        "            if stats.get('embedding_connections', 0) > 0:",
        "                parts.append(f\"embedding: {stats['embedding_connections']}\")",
        "            logger.info(\", \".join(parts) if len(parts) > 1 else parts[0])",
        "        return stats",
        "",
        "    def extract_corpus_semantics(",
        "        self,",
        "        use_pattern_extraction: bool = True,",
        "        min_pattern_confidence: float = 0.6,",
        "        max_similarity_pairs: int = 100000,",
        "        min_context_keys: int = 3,",
        "        verbose: bool = True",
        "    ) -> int:",
        "        \"\"\"",
        "        Extract semantic relations from the corpus.",
        "",
        "        Args:",
        "            use_pattern_extraction: Extract relations from text patterns",
        "            min_pattern_confidence: Minimum confidence for pattern-based relations",
        "            max_similarity_pairs: Maximum pairs to check for SimilarTo relations",
        "            min_context_keys: Minimum context keys for SimilarTo consideration",
        "            verbose: Print progress messages",
        "",
        "        Returns:",
        "            Number of relations extracted",
        "        \"\"\"",
        "        self.semantic_relations = semantics.extract_corpus_semantics(",
        "            self.layers,",
        "            self.documents,",
        "            self.tokenizer,",
        "            use_pattern_extraction=use_pattern_extraction,",
        "            min_pattern_confidence=min_pattern_confidence,",
        "            max_similarity_pairs=max_similarity_pairs,",
        "            min_context_keys=min_context_keys",
        "        )",
        "        if verbose:",
        "            logger.info(f\"Extracted {len(self.semantic_relations)} semantic relations\")",
        "        return len(self.semantic_relations)",
        "",
        "    def extract_pattern_relations(",
        "        self,",
        "        min_confidence: float = 0.6,",
        "        verbose: bool = True",
        "    ) -> List[Tuple[str, str, str, float]]:",
        "        \"\"\"",
        "        Extract semantic relations using pattern matching only.",
        "",
        "        Args:",
        "            min_confidence: Minimum confidence for extracted relations",
        "            verbose: Print progress messages",
        "",
        "        Returns:",
        "            List of (term1, relation_type, term2, confidence) tuples",
        "        \"\"\"",
        "        layer0 = self.get_layer(CorticalLayer.TOKENS)",
        "        valid_terms = set(layer0.minicolumns.keys())",
        "",
        "        relations = semantics.extract_pattern_relations(",
        "            self.documents,",
        "            valid_terms,",
        "            min_confidence=min_confidence",
        "        )",
        "",
        "        if verbose:",
        "            stats = semantics.get_pattern_statistics(relations)",
        "            logger.info(f\"Extracted {stats['total_relations']} pattern-based relations\")",
        "            logger.info(f\"  Types: {stats['relation_type_counts']}\")",
        "",
        "        return relations",
        "",
        "    def retrofit_connections(self, iterations: int = 10, alpha: float = 0.3, verbose: bool = True) -> Dict:",
        "        if not self.semantic_relations:",
        "            self.extract_corpus_semantics(verbose=False)",
        "        stats = semantics.retrofit_connections(self.layers, self.semantic_relations, iterations, alpha)",
        "        if verbose:",
        "            logger.info(f\"Retrofitted {stats['tokens_affected']} tokens\")",
        "        return stats",
        "",
        "    def compute_property_inheritance(",
        "        self,",
        "        decay_factor: float = 0.7,",
        "        max_depth: int = 5,",
        "        apply_to_connections: bool = True,",
        "        boost_factor: float = 0.3,",
        "        verbose: bool = True",
        "    ) -> Dict[str, Any]:",
        "        \"\"\"",
        "        Compute property inheritance based on IsA hierarchy.",
        "",
        "        Args:",
        "            decay_factor: Weight multiplier per inheritance level (default 0.7)",
        "            max_depth: Maximum inheritance depth (default 5)",
        "            apply_to_connections: Boost lateral connections for shared properties",
        "            boost_factor: Weight boost for shared inherited properties",
        "            verbose: Print progress messages",
        "",
        "        Returns:",
        "            Dict with statistics about inheritance computation",
        "        \"\"\"",
        "        if not self.semantic_relations:",
        "            self.extract_corpus_semantics(verbose=False)",
        "",
        "        inherited = semantics.inherit_properties(",
        "            self.semantic_relations,",
        "            decay_factor=decay_factor,",
        "            max_depth=max_depth",
        "        )",
        "",
        "        total_props = sum(len(props) for props in inherited.values())",
        "",
        "        result = {",
        "            'terms_with_inheritance': len(inherited),",
        "            'total_properties_inherited': total_props,",
        "            'inherited': inherited",
        "        }",
        "",
        "        if apply_to_connections and inherited:",
        "            conn_stats = semantics.apply_inheritance_to_connections(",
        "                self.layers,",
        "                inherited,",
        "                boost_factor=boost_factor",
        "            )",
        "            result['connections_boosted'] = conn_stats['connections_boosted']",
        "            result['total_boost'] = conn_stats['total_boost']",
        "        else:",
        "            result['connections_boosted'] = 0",
        "            result['total_boost'] = 0.0",
        "",
        "        if verbose:",
        "            logger.info(f\"Computed property inheritance: {result['terms_with_inheritance']} terms, \"",
        "                        f\"{total_props} properties, {result['connections_boosted']} connections boosted\")",
        "",
        "        return result",
        "",
        "    def compute_property_similarity(self, term1: str, term2: str) -> float:",
        "        \"\"\"",
        "        Compute similarity between terms based on shared properties.",
        "",
        "        Args:",
        "            term1: First term",
        "            term2: Second term",
        "",
        "        Returns:",
        "            Similarity score (0.0-1.0) based on property overlap",
        "        \"\"\"",
        "        if not self.semantic_relations:",
        "            return 0.0",
        "",
        "        inherited = semantics.inherit_properties(self.semantic_relations)",
        "        return semantics.compute_property_similarity(term1, term2, inherited)",
        "",
        "    def compute_graph_embeddings(",
        "        self,",
        "        dimensions: int = 64,",
        "        method: str = 'fast',",
        "        max_terms: Optional[int] = None,",
        "        verbose: bool = True",
        "    ) -> Dict:",
        "        \"\"\"",
        "        Compute graph embeddings for tokens.",
        "",
        "        Args:",
        "            dimensions: Number of embedding dimensions (default 64)",
        "            method: Embedding method ('tfidf', 'fast', 'adjacency', 'random_walk', 'spectral')",
        "            max_terms: Maximum number of terms to embed (by PageRank)",
        "            verbose: Print progress messages",
        "",
        "        Returns:",
        "            Statistics dict with method, dimensions, terms_embedded",
        "        \"\"\"",
        "        token_count = self.layers[CorticalLayer.TOKENS].column_count()",
        "        if max_terms is None:",
        "            if token_count < 2000:",
        "                max_terms = None",
        "            elif token_count < 5000:",
        "                max_terms = 1500",
        "            else:",
        "                max_terms = 1000",
        "",
        "        self.embeddings, stats = emb_module.compute_graph_embeddings(",
        "            self.layers, dimensions, method, max_terms",
        "        )",
        "        if verbose:",
        "            sampled = stats.get('sampled', False)",
        "            sample_info = f\", sampled top {max_terms}\" if sampled else \"\"",
        "            logger.info(f\"Computed {stats['terms_embedded']} embeddings ({method}{sample_info})\")",
        "        return stats",
        "",
        "    def retrofit_embeddings(self, iterations: int = 10, alpha: float = 0.4, verbose: bool = True) -> Dict:",
        "        if not self.embeddings:",
        "            self.compute_graph_embeddings(verbose=False)",
        "        if not self.semantic_relations:",
        "            self.extract_corpus_semantics(verbose=False)",
        "        stats = semantics.retrofit_embeddings(self.embeddings, self.semantic_relations, iterations, alpha)",
        "        if verbose:",
        "            logger.info(f\"Retrofitted embeddings (moved {stats['total_movement']:.2f} total)\")",
        "        return stats",
        "",
        "    def embedding_similarity(self, term1: str, term2: str) -> float:",
        "        return emb_module.embedding_similarity(self.embeddings, term1, term2)",
        "",
        "    def find_similar_by_embedding(self, term: str, top_n: int = 10) -> List[Tuple[str, float]]:",
        "        return emb_module.find_similar_by_embedding(self.embeddings, term, top_n)"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "cortical/processor/core.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "\"\"\"",
        "Core processor functionality: initialization, staleness tracking, and layer management.",
        "",
        "This module contains the base class definition and core infrastructure that all",
        "other processor mixins depend on.",
        "\"\"\"",
        "",
        "import logging",
        "from typing import Dict, Optional, Any",
        "",
        "from ..tokenizer import Tokenizer",
        "from ..minicolumn import Minicolumn",
        "from ..layers import CorticalLayer, HierarchicalLayer",
        "from ..config import CorticalConfig",
        "",
        "logger = logging.getLogger(__name__)",
        "",
        "",
        "class CoreMixin:",
        "    \"\"\"",
        "    Core mixin providing initialization and staleness tracking.",
        "",
        "    This mixin defines the fundamental attributes and methods that all other",
        "    processor functionality depends on.",
        "    \"\"\"",
        "",
        "    # Computation types for tracking staleness",
        "    COMP_TFIDF = 'tfidf'",
        "    COMP_PAGERANK = 'pagerank'",
        "    COMP_ACTIVATION = 'activation'",
        "    COMP_DOC_CONNECTIONS = 'doc_connections'",
        "    COMP_BIGRAM_CONNECTIONS = 'bigram_connections'",
        "    COMP_CONCEPTS = 'concepts'",
        "    COMP_EMBEDDINGS = 'embeddings'",
        "    COMP_SEMANTICS = 'semantics'",
        "",
        "    def __init__(",
        "        self,",
        "        tokenizer: Optional[Tokenizer] = None,",
        "        config: Optional[CorticalConfig] = None",
        "    ):",
        "        \"\"\"",
        "        Initialize the Cortical Text Processor.",
        "",
        "        Args:",
        "            tokenizer: Optional custom tokenizer. Defaults to standard Tokenizer.",
        "            config: Optional configuration. Defaults to CorticalConfig with defaults.",
        "        \"\"\"",
        "        self.tokenizer = tokenizer or Tokenizer()",
        "        self.config = config or CorticalConfig()",
        "        self.layers: Dict[CorticalLayer, HierarchicalLayer] = {",
        "            CorticalLayer.TOKENS: HierarchicalLayer(CorticalLayer.TOKENS),",
        "            CorticalLayer.BIGRAMS: HierarchicalLayer(CorticalLayer.BIGRAMS),",
        "            CorticalLayer.CONCEPTS: HierarchicalLayer(CorticalLayer.CONCEPTS),",
        "            CorticalLayer.DOCUMENTS: HierarchicalLayer(CorticalLayer.DOCUMENTS),",
        "        }",
        "        self.documents: Dict[str, str] = {}",
        "        self.document_metadata: Dict[str, Dict[str, Any]] = {}",
        "        self.embeddings: Dict[str, list] = {}",
        "        self.semantic_relations: list = []",
        "        # Track which computations are stale and need recomputation",
        "        self._stale_computations: set = set()",
        "        # LRU cache for query expansion results",
        "        self._query_expansion_cache: Dict[str, Dict[str, float]] = {}",
        "        self._query_cache_max_size: int = 100",
        "",
        "    def _mark_all_stale(self) -> None:",
        "        \"\"\"Mark all computations as stale (needing recomputation).\"\"\"",
        "        self._stale_computations = {",
        "            self.COMP_TFIDF,",
        "            self.COMP_PAGERANK,",
        "            self.COMP_ACTIVATION,",
        "            self.COMP_DOC_CONNECTIONS,",
        "            self.COMP_BIGRAM_CONNECTIONS,",
        "            self.COMP_CONCEPTS,",
        "            self.COMP_EMBEDDINGS,",
        "            self.COMP_SEMANTICS,",
        "        }",
        "",
        "    def _mark_fresh(self, *computation_types: str) -> None:",
        "        \"\"\"Mark specified computations as fresh (up-to-date).\"\"\"",
        "        for comp in computation_types:",
        "            self._stale_computations.discard(comp)",
        "",
        "    def is_stale(self, computation_type: str) -> bool:",
        "        \"\"\"",
        "        Check if a specific computation is stale.",
        "",
        "        Args:",
        "            computation_type: One of COMP_TFIDF, COMP_PAGERANK, etc.",
        "",
        "        Returns:",
        "            True if the computation needs to be run again",
        "        \"\"\"",
        "        return computation_type in self._stale_computations",
        "",
        "    def get_stale_computations(self) -> set:",
        "        \"\"\"",
        "        Get the set of computations that are currently stale.",
        "",
        "        Returns:",
        "            Set of computation type strings that need recomputation",
        "        \"\"\"",
        "        return self._stale_computations.copy()",
        "",
        "    def get_layer(self, layer: CorticalLayer) -> HierarchicalLayer:",
        "        \"\"\"Get a specific layer by enum.\"\"\"",
        "        return self.layers[layer]"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "cortical/processor/documents.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "\"\"\"",
        "Document management: processing, adding, removing, and metadata handling.",
        "",
        "This module contains all methods related to managing documents in the corpus.",
        "\"\"\"",
        "",
        "import copy",
        "import logging",
        "from typing import Dict, List, Tuple, Optional, Any",
        "",
        "from ..layers import CorticalLayer",
        "",
        "logger = logging.getLogger(__name__)",
        "",
        "",
        "class DocumentsMixin:",
        "    \"\"\"",
        "    Mixin providing document management functionality.",
        "",
        "    Requires CoreMixin to be present (provides tokenizer, layers, documents,",
        "    document_metadata, _mark_all_stale, _query_expansion_cache).",
        "    \"\"\"",
        "",
        "    def process_document(",
        "        self,",
        "        doc_id: str,",
        "        content: str,",
        "        metadata: Optional[Dict[str, Any]] = None",
        "    ) -> Dict[str, int]:",
        "        \"\"\"",
        "        Process a document and add it to the corpus.",
        "",
        "        Args:",
        "            doc_id: Unique identifier for the document",
        "            content: Document text content",
        "            metadata: Optional metadata dict (source, timestamp, author, etc.)",
        "",
        "        Returns:",
        "            Dict with processing statistics (tokens, bigrams, unique_tokens)",
        "",
        "        Raises:",
        "            ValueError: If doc_id or content is empty or not a string",
        "        \"\"\"",
        "        # Input validation",
        "        if not isinstance(doc_id, str) or not doc_id:",
        "            raise ValueError(\"doc_id must be a non-empty string\")",
        "        if not isinstance(content, str):",
        "            raise ValueError(\"content must be a string\")",
        "        if not content.strip():",
        "            raise ValueError(\"content must not be empty or whitespace-only\")",
        "",
        "        self.documents[doc_id] = content",
        "",
        "        # Store metadata if provided",
        "        if metadata:",
        "            self.document_metadata[doc_id] = metadata.copy()",
        "        elif doc_id not in self.document_metadata:",
        "            self.document_metadata[doc_id] = {}",
        "",
        "        tokens = self.tokenizer.tokenize(content)",
        "        bigrams = self.tokenizer.extract_ngrams(tokens, n=2)",
        "",
        "        layer0 = self.layers[CorticalLayer.TOKENS]",
        "        layer1 = self.layers[CorticalLayer.BIGRAMS]",
        "        layer3 = self.layers[CorticalLayer.DOCUMENTS]",
        "",
        "        doc_col = layer3.get_or_create_minicolumn(doc_id)",
        "        doc_col.occurrence_count += 1",
        "        # Cache tokenized document name for fast doc_name_boost in search",
        "        # This avoids re-tokenizing the doc_id on every query",
        "        doc_col.name_tokens = set(self.tokenizer.tokenize(doc_id.replace('_', ' ')))",
        "",
        "        for token in tokens:",
        "            col = layer0.get_or_create_minicolumn(token)",
        "            col.occurrence_count += 1",
        "            col.document_ids.add(doc_id)",
        "            col.activation += 1.0",
        "            # Weighted feedforward: document -> token (weight by occurrence count)",
        "            doc_col.add_feedforward_connection(col.id, 1.0)",
        "            # Weighted feedback: token -> document (weight by occurrence count)",
        "            col.add_feedback_connection(doc_col.id, 1.0)",
        "            # Track per-document occurrence count for accurate TF-IDF",
        "            col.doc_occurrence_counts[doc_id] = col.doc_occurrence_counts.get(doc_id, 0) + 1",
        "",
        "        for i, token in enumerate(tokens):",
        "            col = layer0.get_minicolumn(token)",
        "            if col:",
        "                for j in range(max(0, i - 3), min(len(tokens), i + 4)):",
        "                    if i != j:",
        "                        other = layer0.get_minicolumn(tokens[j])",
        "                        if other:",
        "                            col.add_lateral_connection(other.id, 1.0)",
        "",
        "        for bigram in bigrams:",
        "            col = layer1.get_or_create_minicolumn(bigram)",
        "            col.occurrence_count += 1",
        "            col.document_ids.add(doc_id)",
        "            col.activation += 1.0",
        "            for part in bigram.split():",
        "                token_col = layer0.get_minicolumn(part)",
        "                if token_col:",
        "                    # Weighted feedforward: bigram -> tokens (weight 1.0 per occurrence)",
        "                    col.add_feedforward_connection(token_col.id, 1.0)",
        "                    # Weighted feedback: token -> bigram (weight 1.0 per occurrence)",
        "                    token_col.add_feedback_connection(col.id, 1.0)",
        "",
        "        # Mark all computations as stale since document corpus changed",
        "        self._mark_all_stale()",
        "",
        "        return {'tokens': len(tokens), 'bigrams': len(bigrams), 'unique_tokens': len(set(tokens))}",
        "",
        "    def set_document_metadata(self, doc_id: str, **kwargs) -> None:",
        "        \"\"\"",
        "        Set or update metadata for a document.",
        "",
        "        Args:",
        "            doc_id: Document identifier",
        "            **kwargs: Metadata key-value pairs to set",
        "",
        "        Example:",
        "            >>> processor.set_document_metadata(\"doc1\",",
        "            ...     source=\"https://example.com\",",
        "            ...     author=\"John Doe\",",
        "            ...     timestamp=\"2025-12-09\"",
        "            ... )",
        "        \"\"\"",
        "        if doc_id not in self.document_metadata:",
        "            self.document_metadata[doc_id] = {}",
        "        self.document_metadata[doc_id].update(kwargs)",
        "",
        "    def get_document_metadata(self, doc_id: str) -> Dict[str, Any]:",
        "        \"\"\"",
        "        Get metadata for a document.",
        "",
        "        Args:",
        "            doc_id: Document identifier",
        "",
        "        Returns:",
        "            Metadata dict (empty dict if no metadata set)",
        "        \"\"\"",
        "        return self.document_metadata.get(doc_id, {})",
        "",
        "    def get_all_document_metadata(self) -> Dict[str, Dict[str, Any]]:",
        "        \"\"\"",
        "        Get metadata for all documents.",
        "",
        "        Returns:",
        "            Dict mapping doc_id to metadata dict (deep copy)",
        "        \"\"\"",
        "        return copy.deepcopy(self.document_metadata)",
        "",
        "    def add_document_incremental(",
        "        self,",
        "        doc_id: str,",
        "        content: str,",
        "        metadata: Optional[Dict[str, Any]] = None,",
        "        recompute: str = 'tfidf'",
        "    ) -> Dict[str, int]:",
        "        \"\"\"",
        "        Add a document with selective recomputation for efficiency.",
        "",
        "        Unlike process_document() + compute_all(), this method only recomputes",
        "        what's necessary based on the recompute parameter. This is more efficient",
        "        for RAG systems with frequent document updates.",
        "",
        "        Args:",
        "            doc_id: Unique identifier for the document",
        "            content: Document text content",
        "            metadata: Optional metadata dict (source, timestamp, author, etc.)",
        "            recompute: Level of recomputation to perform:",
        "                - 'none': Just add document, mark all computations stale",
        "                - 'tfidf': Recompute TF-IDF only (fast, updates term weights)",
        "                - 'full': Run compute_all() (slowest, most accurate)",
        "",
        "        Returns:",
        "            Dict with processing statistics (tokens, bigrams, unique_tokens)",
        "",
        "        Example:",
        "            >>> # Quick update for search without full recomputation",
        "            >>> processor.add_document_incremental(\"new_doc\", \"content\", recompute='tfidf')",
        "            >>>",
        "            >>> # Just queue document, recompute later in batch",
        "            >>> processor.add_document_incremental(\"doc1\", \"content1\", recompute='none')",
        "            >>> processor.add_document_incremental(\"doc2\", \"content2\", recompute='none')",
        "            >>> processor.recompute(level='full')  # Batch recomputation",
        "        \"\"\"",
        "        stats = self.process_document(doc_id, content, metadata)",
        "",
        "        if recompute == 'tfidf':",
        "            self.compute_tfidf(verbose=False)",
        "            self._mark_fresh(self.COMP_TFIDF)",
        "        elif recompute == 'full':",
        "            self.compute_all(verbose=False)",
        "            self._stale_computations.clear()",
        "        # 'none' leaves all computations marked as stale",
        "",
        "        return stats",
        "",
        "    def add_documents_batch(",
        "        self,",
        "        documents: List[Tuple[str, str, Optional[Dict[str, Any]]]],",
        "        recompute: str = 'full',",
        "        verbose: bool = True",
        "    ) -> Dict[str, Any]:",
        "        \"\"\"",
        "        Add multiple documents with a single recomputation.",
        "",
        "        More efficient than calling add_document_incremental() multiple times",
        "        when adding many documents at once.",
        "",
        "        Args:",
        "            documents: List of (doc_id, content, metadata) tuples.",
        "                       metadata can be None for documents without metadata.",
        "            recompute: Level of recomputation after all documents are added:",
        "                - 'none': Just add documents, mark all computations stale",
        "                - 'tfidf': Recompute TF-IDF only",
        "                - 'full': Run compute_all()",
        "            verbose: Print progress messages",
        "",
        "        Returns:",
        "            Dict with batch statistics:",
        "                - documents_added: Number of documents added",
        "                - total_tokens: Total tokens across all documents",
        "                - recomputation: Type of recomputation performed",
        "",
        "        Example:",
        "            >>> docs = [",
        "            ...     (\"doc1\", \"First document content\", {\"source\": \"web\"}),",
        "            ...     (\"doc2\", \"Second document content\", None),",
        "            ...     (\"doc3\", \"Third document content\", {\"author\": \"AI\"}),",
        "            ... ]",
        "            >>> processor.add_documents_batch(docs, recompute='full')",
        "",
        "        Raises:",
        "            ValueError: If documents list is invalid or recompute level is unknown",
        "        \"\"\"",
        "        # Input validation",
        "        if not isinstance(documents, list):",
        "            raise ValueError(\"documents must be a list\")",
        "        if not documents:",
        "            raise ValueError(\"documents list must not be empty\")",
        "",
        "        valid_recompute = {'none', 'tfidf', 'full'}",
        "        if recompute not in valid_recompute:",
        "            raise ValueError(f\"recompute must be one of {valid_recompute}\")",
        "",
        "        for i, doc in enumerate(documents):",
        "            if not isinstance(doc, (tuple, list)) or len(doc) < 2:",
        "                raise ValueError(",
        "                    f\"documents[{i}] must be a tuple of (doc_id, content) or \"",
        "                    f\"(doc_id, content, metadata)\"",
        "                )",
        "            doc_id, content = doc[0], doc[1]",
        "            if not isinstance(doc_id, str) or not doc_id:",
        "                raise ValueError(f\"documents[{i}][0] (doc_id) must be a non-empty string\")",
        "            if not isinstance(content, str):",
        "                raise ValueError(f\"documents[{i}][1] (content) must be a string\")",
        "",
        "        total_tokens = 0",
        "        total_bigrams = 0",
        "",
        "        if verbose:",
        "            logger.info(f\"Adding {len(documents)} documents...\")",
        "",
        "        for doc_id, content, metadata in documents:",
        "            # Use process_document directly (not add_document_incremental)",
        "            # to avoid per-document recomputation",
        "            stats = self.process_document(doc_id, content, metadata)",
        "            total_tokens += stats['tokens']",
        "            total_bigrams += stats['bigrams']",
        "",
        "        if verbose:",
        "            logger.info(f\"Processed {total_tokens} tokens, {total_bigrams} bigrams\")",
        "",
        "        # Perform single recomputation for entire batch",
        "        if recompute == 'tfidf':",
        "            if verbose:",
        "                logger.info(\"Recomputing TF-IDF...\")",
        "            self.compute_tfidf(verbose=False)",
        "            self._mark_fresh(self.COMP_TFIDF)",
        "        elif recompute == 'full':",
        "            if verbose:",
        "                logger.info(\"Running full recomputation...\")",
        "            self.compute_all(verbose=False)",
        "            self._stale_computations.clear()",
        "",
        "        if verbose:",
        "            logger.info(\"Done.\")",
        "",
        "        return {",
        "            'documents_added': len(documents),",
        "            'total_tokens': total_tokens,",
        "            'total_bigrams': total_bigrams,",
        "            'recomputation': recompute",
        "        }",
        "",
        "    def remove_document(self, doc_id: str, verbose: bool = False) -> Dict[str, Any]:",
        "        \"\"\"",
        "        Remove a document from the corpus.",
        "",
        "        Removes the document and cleans up all references to it in the layers:",
        "        - Removes from documents dict and metadata",
        "        - Removes document minicolumn from Layer 3",
        "        - Removes doc_id from token and bigram document_ids sets",
        "        - Decrements occurrence counts appropriately",
        "        - Cleans up feedforward/feedback connections",
        "",
        "        Args:",
        "            doc_id: Document identifier to remove",
        "            verbose: Print progress messages",
        "",
        "        Returns:",
        "            Dict with removal statistics:",
        "                - found: Whether the document existed",
        "                - tokens_affected: Number of tokens that referenced this document",
        "                - bigrams_affected: Number of bigrams that referenced this document",
        "",
        "        Example:",
        "            >>> processor.remove_document(\"old_doc\")",
        "            {'found': True, 'tokens_affected': 42, 'bigrams_affected': 35}",
        "        \"\"\"",
        "        if doc_id not in self.documents:",
        "            return {'found': False, 'tokens_affected': 0, 'bigrams_affected': 0}",
        "",
        "        if verbose:",
        "            logger.info(f\"Removing document: {doc_id}\")",
        "",
        "        # Remove from documents and metadata",
        "        del self.documents[doc_id]",
        "        if doc_id in self.document_metadata:",
        "            del self.document_metadata[doc_id]",
        "",
        "        # Remove document minicolumn from Layer 3",
        "        layer3 = self.layers[CorticalLayer.DOCUMENTS]",
        "        doc_col = layer3.get_minicolumn(doc_id)",
        "        if doc_col:",
        "            # Get tokens/bigrams that were connected to this document",
        "            connected_ids = set(doc_col.feedforward_connections.keys())",
        "            layer3.remove_minicolumn(doc_id)",
        "",
        "        # Clean up token references in Layer 0",
        "        layer0 = self.layers[CorticalLayer.TOKENS]",
        "        tokens_affected = 0",
        "        for content, col in list(layer0.minicolumns.items()):",
        "            if doc_id in col.document_ids:",
        "                col.document_ids.discard(doc_id)",
        "                tokens_affected += 1",
        "",
        "                # Decrement occurrence count by per-doc count",
        "                if doc_id in col.doc_occurrence_counts:",
        "                    col.occurrence_count -= col.doc_occurrence_counts[doc_id]",
        "                    del col.doc_occurrence_counts[doc_id]",
        "",
        "                # Clean up feedback connections to document",
        "                doc_col_id = f\"L3_{doc_id}\"",
        "                if doc_col_id in col.feedback_connections:",
        "                    del col.feedback_connections[doc_col_id]",
        "",
        "        # Clean up bigram references in Layer 1",
        "        layer1 = self.layers[CorticalLayer.BIGRAMS]",
        "        bigrams_affected = 0",
        "        for content, col in list(layer1.minicolumns.items()):",
        "            if doc_id in col.document_ids:",
        "                col.document_ids.discard(doc_id)",
        "                bigrams_affected += 1",
        "",
        "                # Decrement occurrence count (approximate since we don't track per-doc for bigrams)",
        "                if doc_id in col.doc_occurrence_counts:",
        "                    col.occurrence_count -= col.doc_occurrence_counts[doc_id]",
        "                    del col.doc_occurrence_counts[doc_id]",
        "",
        "        # Mark all computations as stale",
        "        self._mark_all_stale()",
        "",
        "        # Invalidate query cache since corpus changed",
        "        if hasattr(self, '_query_expansion_cache'):",
        "            self._query_expansion_cache.clear()",
        "",
        "        if verbose:",
        "            logger.info(f\"  Affected: {tokens_affected} tokens, {bigrams_affected} bigrams\")",
        "",
        "        return {",
        "            'found': True,",
        "            'tokens_affected': tokens_affected,",
        "            'bigrams_affected': bigrams_affected",
        "        }",
        "",
        "    def remove_documents_batch(",
        "        self,",
        "        doc_ids: List[str],",
        "        recompute: str = 'none',",
        "        verbose: bool = True",
        "    ) -> Dict[str, Any]:",
        "        \"\"\"",
        "        Remove multiple documents efficiently with single recomputation.",
        "",
        "        Args:",
        "            doc_ids: List of document identifiers to remove",
        "            recompute: Level of recomputation after removal:",
        "                - 'none': Just remove documents, mark computations stale",
        "                - 'tfidf': Recompute TF-IDF only",
        "                - 'full': Run full compute_all()",
        "            verbose: Print progress messages",
        "",
        "        Returns:",
        "            Dict with removal statistics:",
        "                - documents_removed: Number of documents actually removed",
        "                - documents_not_found: Number of doc_ids that didn't exist",
        "                - total_tokens_affected: Total tokens affected",
        "                - total_bigrams_affected: Total bigrams affected",
        "",
        "        Example:",
        "            >>> processor.remove_documents_batch([\"old1\", \"old2\", \"old3\"])",
        "        \"\"\"",
        "        removed = 0",
        "        not_found = 0",
        "        total_tokens = 0",
        "        total_bigrams = 0",
        "",
        "        if verbose:",
        "            logger.info(f\"Removing {len(doc_ids)} documents...\")",
        "",
        "        for doc_id in doc_ids:",
        "            result = self.remove_document(doc_id, verbose=False)",
        "            if result['found']:",
        "                removed += 1",
        "                total_tokens += result['tokens_affected']",
        "                total_bigrams += result['bigrams_affected']",
        "            else:",
        "                not_found += 1",
        "",
        "        if verbose:",
        "            logger.info(f\"  Removed: {removed}, Not found: {not_found}\")",
        "            logger.info(f\"  Affected: {total_tokens} tokens, {total_bigrams} bigrams\")",
        "",
        "        # Perform recomputation",
        "        if recompute == 'tfidf':",
        "            if verbose:",
        "                logger.info(\"Recomputing TF-IDF...\")",
        "            self.compute_tfidf(verbose=False)",
        "            self._mark_fresh(self.COMP_TFIDF)",
        "        elif recompute == 'full':",
        "            if verbose:",
        "                logger.info(\"Running full recomputation...\")",
        "            self.compute_all(verbose=False)",
        "            self._stale_computations.clear()",
        "",
        "        return {",
        "            'documents_removed': removed,",
        "            'documents_not_found': not_found,",
        "            'total_tokens_affected': total_tokens,",
        "            'total_bigrams_affected': total_bigrams,",
        "            'recomputation': recompute",
        "        }"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "cortical/processor/introspection.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "\"\"\"",
        "Introspection: state inspection, fingerprints, gaps, and summaries.",
        "",
        "This module contains methods for examining the processor state and",
        "comparing texts/documents.",
        "\"\"\"",
        "",
        "import re",
        "import logging",
        "from typing import Dict, List, Tuple, Optional, Any, TYPE_CHECKING",
        "",
        "from ..layers import CorticalLayer",
        "from .. import gaps as gaps_module",
        "from .. import fingerprint as fp_module",
        "from .. import persistence",
        "",
        "if TYPE_CHECKING:",
        "    from . import CorticalTextProcessor",
        "",
        "logger = logging.getLogger(__name__)",
        "",
        "",
        "class IntrospectionMixin:",
        "    \"\"\"",
        "    Mixin providing introspection functionality.",
        "",
        "    Requires CoreMixin to be present (provides layers, documents, tokenizer).",
        "    \"\"\"",
        "",
        "    def get_document_signature(self, doc_id: str, n: int = 10) -> List[Tuple[str, float]]:",
        "        \"\"\"Get the top-n TF-IDF terms for a document.\"\"\"",
        "        layer0 = self.layers[CorticalLayer.TOKENS]",
        "        terms = [(col.content, col.tfidf_per_doc.get(doc_id, col.tfidf))",
        "                 for col in layer0.minicolumns.values() if doc_id in col.document_ids]",
        "        return sorted(terms, key=lambda x: x[1], reverse=True)[:n]",
        "",
        "    def get_corpus_summary(self) -> Dict:",
        "        \"\"\"Get summary statistics about the corpus.\"\"\"",
        "        return persistence.get_state_summary(self.layers, self.documents)",
        "",
        "    def analyze_knowledge_gaps(self) -> Dict:",
        "        \"\"\"Analyze the corpus for knowledge gaps.\"\"\"",
        "        return gaps_module.analyze_knowledge_gaps(self.layers, self.documents)",
        "",
        "    def detect_anomalies(self, threshold: float = 0.3) -> List[Dict]:",
        "        \"\"\"Detect anomalous patterns in the corpus.\"\"\"",
        "        return gaps_module.detect_anomalies(self.layers, self.documents, threshold)",
        "",
        "    # Fingerprint methods for semantic comparison",
        "    def get_fingerprint(self, text: str, top_n: int = 20) -> Dict:",
        "        \"\"\"",
        "        Compute the semantic fingerprint of a text.",
        "",
        "        The fingerprint captures the semantic essence of the text including",
        "        term weights, concept memberships, and bigrams.",
        "",
        "        Args:",
        "            text: Input text to fingerprint",
        "            top_n: Number of top terms to include",
        "",
        "        Returns:",
        "            Dict with 'terms', 'concepts', 'bigrams', 'top_terms', 'term_count'",
        "        \"\"\"",
        "        return fp_module.compute_fingerprint(text, self.tokenizer, self.layers, top_n)",
        "",
        "    def compare_fingerprints(self, fp1: Dict, fp2: Dict) -> Dict:",
        "        \"\"\"",
        "        Compare two fingerprints and compute similarity metrics.",
        "",
        "        Args:",
        "            fp1: First fingerprint from get_fingerprint()",
        "            fp2: Second fingerprint from get_fingerprint()",
        "",
        "        Returns:",
        "            Dict with similarity scores and shared terms",
        "        \"\"\"",
        "        return fp_module.compare_fingerprints(fp1, fp2)",
        "",
        "    def explain_fingerprint(self, fp: Dict, top_n: int = 10) -> Dict:",
        "        \"\"\"",
        "        Generate a human-readable explanation of a fingerprint.",
        "",
        "        Args:",
        "            fp: Fingerprint from get_fingerprint()",
        "            top_n: Number of top items to include",
        "",
        "        Returns:",
        "            Dict with explanation components including summary",
        "        \"\"\"",
        "        return fp_module.explain_fingerprint(fp, top_n)",
        "",
        "    def explain_similarity(self, fp1: Dict, fp2: Dict) -> str:",
        "        \"\"\"",
        "        Generate a human-readable explanation of fingerprint similarity.",
        "",
        "        Args:",
        "            fp1: First fingerprint",
        "            fp2: Second fingerprint",
        "",
        "        Returns:",
        "            Human-readable explanation string",
        "        \"\"\"",
        "        return fp_module.explain_similarity(fp1, fp2)",
        "",
        "    def find_similar_texts(",
        "        self,",
        "        text: str,",
        "        candidates: List[Tuple[str, str]],",
        "        top_n: int = 5",
        "    ) -> List[Tuple[str, float, Dict]]:",
        "        \"\"\"",
        "        Find texts most similar to the given text.",
        "",
        "        Args:",
        "            text: Query text to compare",
        "            candidates: List of (id, text) tuples to search",
        "            top_n: Number of results to return",
        "",
        "        Returns:",
        "            List of (id, similarity_score, comparison) tuples sorted by similarity",
        "        \"\"\"",
        "        query_fp = self.get_fingerprint(text)",
        "        results = []",
        "",
        "        for candidate_id, candidate_text in candidates:",
        "            candidate_fp = self.get_fingerprint(candidate_text)",
        "            comparison = self.compare_fingerprints(query_fp, candidate_fp)",
        "            results.append((candidate_id, comparison['overall_similarity'], comparison))",
        "",
        "        results.sort(key=lambda x: x[1], reverse=True)",
        "        return results[:top_n]",
        "",
        "    # Semantic Diff methods",
        "    def compare_with(",
        "        self,",
        "        other: 'CorticalTextProcessor',",
        "        top_movers: int = 20,",
        "        min_pagerank_delta: float = 0.0001",
        "    ) -> 'diff_module.SemanticDiff':",
        "        \"\"\"",
        "        Compare this processor state with another to find semantic differences.",
        "",
        "        Args:",
        "            other: Another CorticalTextProcessor to compare with",
        "            top_movers: Number of top importance changes to track",
        "            min_pagerank_delta: Minimum PageRank change to consider significant",
        "",
        "        Returns:",
        "            SemanticDiff object with all detected changes",
        "        \"\"\"",
        "        from .. import diff as diff_module",
        "        return diff_module.compare_processors(",
        "            old_processor=other,",
        "            new_processor=self,",
        "            top_movers=top_movers,",
        "            min_pagerank_delta=min_pagerank_delta",
        "        )",
        "",
        "    def compare_documents(self, doc_id_1: str, doc_id_2: str) -> Dict:",
        "        \"\"\"",
        "        Compare two documents within this corpus.",
        "",
        "        Args:",
        "            doc_id_1: ID of first document",
        "            doc_id_2: ID of second document",
        "",
        "        Returns:",
        "            Dict with comparison results",
        "        \"\"\"",
        "        from .. import diff as diff_module",
        "        return diff_module.compare_documents(self, doc_id_1, doc_id_2)",
        "",
        "    def what_changed(self, old_content: str, new_content: str) -> Dict:",
        "        \"\"\"",
        "        Compare two text contents to show what changed semantically.",
        "",
        "        Args:",
        "            old_content: The \"before\" text",
        "            new_content: The \"after\" text",
        "",
        "        Returns:",
        "            Dict with semantic diff results",
        "        \"\"\"",
        "        from .. import diff as diff_module",
        "        return diff_module.what_changed(self, old_content, new_content)",
        "",
        "    def summarize_document(self, doc_id: str, num_sentences: int = 3) -> str:",
        "        \"\"\"",
        "        Generate a summary of a document using extractive summarization.",
        "",
        "        Args:",
        "            doc_id: Document identifier",
        "            num_sentences: Number of sentences to include",
        "",
        "        Returns:",
        "            Summary string (empty if document not found)",
        "        \"\"\"",
        "        if doc_id not in self.documents:",
        "            return \"\"",
        "        content = self.documents[doc_id]",
        "        sentences = re.split(r'(?<=[.!?])\\s+', content)",
        "        if len(sentences) <= num_sentences:",
        "            return content",
        "",
        "        layer0 = self.layers[CorticalLayer.TOKENS]",
        "        scored = []",
        "        for sent in sentences:",
        "            tokens = self.tokenizer.tokenize(sent)",
        "            score = sum(layer0.get_minicolumn(t).tfidf if layer0.get_minicolumn(t) else 0 for t in tokens)",
        "            scored.append((sent, score))",
        "        scored.sort(key=lambda x: x[1], reverse=True)",
        "        top = [s for s, _ in scored[:num_sentences]]",
        "        return ' '.join([s for s in sentences if s in top])",
        "",
        "    def __repr__(self) -> str:",
        "        stats = self.get_corpus_summary()",
        "        return f\"CorticalTextProcessor(documents={stats['documents']}, columns={stats['total_columns']})\""
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "cortical/processor/persistence_api.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "\"\"\"",
        "Persistence API: save, load, export, and migration methods.",
        "",
        "This module contains all methods related to saving and loading processor state.",
        "\"\"\"",
        "",
        "import logging",
        "from typing import Dict, Optional, Any, TYPE_CHECKING",
        "",
        "from ..layers import CorticalLayer",
        "from ..config import CorticalConfig",
        "from .. import persistence",
        "from .. import state_storage",
        "",
        "if TYPE_CHECKING:",
        "    from . import CorticalTextProcessor",
        "",
        "logger = logging.getLogger(__name__)",
        "",
        "",
        "class PersistenceMixin:",
        "    \"\"\"",
        "    Mixin providing persistence functionality.",
        "",
        "    Requires CoreMixin to be present (provides layers, documents, document_metadata,",
        "    embeddings, semantic_relations, config, _stale_computations).",
        "    \"\"\"",
        "",
        "    def save(",
        "        self,",
        "        filepath: str,",
        "        verbose: bool = True,",
        "        signing_key: Optional[bytes] = None",
        "    ) -> None:",
        "        \"\"\"",
        "        Save processor state to a file.",
        "",
        "        Saves all computed state including embeddings, semantic relations,",
        "        and configuration, so they don't need to be recomputed when loading.",
        "",
        "        Args:",
        "            filepath: Path to save file",
        "            verbose: Print progress",
        "            signing_key: Optional HMAC key for signing pickle files (SEC-003).",
        "                If provided, creates a .sig file alongside the pickle file.",
        "        \"\"\"",
        "        metadata = {",
        "            'has_embeddings': bool(self.embeddings),",
        "            'has_relations': bool(self.semantic_relations),",
        "            'config': self.config.to_dict()",
        "        }",
        "        persistence.save_processor(",
        "            filepath,",
        "            self.layers,",
        "            self.documents,",
        "            self.document_metadata,",
        "            self.embeddings,",
        "            self.semantic_relations,",
        "            metadata,",
        "            verbose,",
        "            signing_key=signing_key",
        "        )",
        "",
        "    @classmethod",
        "    def load(",
        "        cls,",
        "        filepath: str,",
        "        verbose: bool = True,",
        "        verify_key: Optional[bytes] = None",
        "    ) -> 'CorticalTextProcessor':",
        "        \"\"\"",
        "        Load processor state from a file.",
        "",
        "        Restores all computed state including embeddings, semantic relations,",
        "        and configuration.",
        "",
        "        Args:",
        "            filepath: Path to saved file",
        "            verbose: Print progress",
        "            verify_key: Optional HMAC key for verifying pickle file signatures (SEC-003).",
        "",
        "        Raises:",
        "            SignatureVerificationError: If verify_key is provided and verification fails",
        "            FileNotFoundError: If verify_key is provided but no .sig file exists",
        "        \"\"\"",
        "        result = persistence.load_processor(filepath, verbose, verify_key=verify_key)",
        "        layers, documents, document_metadata, embeddings, semantic_relations, metadata = result",
        "",
        "        # Restore config if available",
        "        config = None",
        "        if metadata and 'config' in metadata:",
        "            try:",
        "                config = CorticalConfig.from_dict(metadata['config'])",
        "            except (KeyError, TypeError):",
        "                config = None",
        "",
        "        processor = cls(config=config)",
        "        processor.layers = layers",
        "        processor.documents = documents",
        "        processor.document_metadata = document_metadata",
        "        processor.embeddings = embeddings",
        "        processor.semantic_relations = semantic_relations",
        "        return processor",
        "",
        "    def save_json(self, state_dir: str, force: bool = False, verbose: bool = True) -> Dict[str, bool]:",
        "        \"\"\"",
        "        Save processor state to git-friendly JSON format.",
        "",
        "        Instead of a single monolithic pickle file, creates a directory with:",
        "        - manifest.json: Version, checksums, staleness tracking",
        "        - documents.json: Document content and metadata",
        "        - layers/*.json: One file per layer",
        "        - computed/*.json: Semantic relations and embeddings",
        "",
        "        Args:",
        "            state_dir: Directory to write JSON state files",
        "            force: Force save even if unchanged (default: False)",
        "            verbose: Print progress messages (default: True)",
        "",
        "        Returns:",
        "            Dictionary mapping component names to whether they were written",
        "        \"\"\"",
        "        writer = state_storage.StateWriter(state_dir)",
        "",
        "        stale = self._stale_computations if hasattr(self, '_stale_computations') else set()",
        "",
        "        return writer.save_all(",
        "            layers=self.layers,",
        "            documents=self.documents,",
        "            document_metadata=self.document_metadata,",
        "            embeddings=self.embeddings,",
        "            semantic_relations=self.semantic_relations,",
        "            stale_computations=stale,",
        "            force=force,",
        "            verbose=verbose",
        "        )",
        "",
        "    @classmethod",
        "    def load_json(",
        "        cls,",
        "        state_dir: str,",
        "        config: Optional[CorticalConfig] = None,",
        "        verbose: bool = True",
        "    ) -> 'CorticalTextProcessor':",
        "        \"\"\"",
        "        Load processor from git-friendly JSON format.",
        "",
        "        Args:",
        "            state_dir: Directory containing JSON state files",
        "            config: Optional configuration (default: uses CorticalConfig defaults)",
        "            verbose: Print progress messages (default: True)",
        "",
        "        Returns:",
        "            Reconstructed CorticalTextProcessor instance",
        "",
        "        Raises:",
        "            FileNotFoundError: If state directory or required files don't exist",
        "            ValueError: If state format is invalid",
        "        \"\"\"",
        "        loader = state_storage.StateLoader(state_dir)",
        "",
        "        layers, documents, metadata, embeddings, relations, manifest_data = loader.load_all(",
        "            validate=True,",
        "            verbose=verbose",
        "        )",
        "",
        "        processor = cls(config=config)",
        "        processor.layers = layers",
        "        processor.documents = documents",
        "        processor.document_metadata = metadata",
        "        processor.embeddings = embeddings",
        "        processor.semantic_relations = relations",
        "",
        "        if 'stale_computations' in manifest_data:",
        "            processor._stale_computations = manifest_data['stale_computations']",
        "",
        "        return processor",
        "",
        "    def migrate_to_json(self, pkl_path: str, json_dir: str, verbose: bool = True) -> bool:",
        "        \"\"\"",
        "        Migrate existing pickle file to git-friendly JSON format.",
        "",
        "        Args:",
        "            pkl_path: Path to existing .pkl file",
        "            json_dir: Directory to write JSON state",
        "            verbose: Print progress messages (default: True)",
        "",
        "        Returns:",
        "            True if migration successful",
        "",
        "        Raises:",
        "            FileNotFoundError: If pkl file doesn't exist",
        "        \"\"\"",
        "        return state_storage.migrate_pkl_to_json(pkl_path, json_dir, verbose=verbose)",
        "",
        "    def export_graph(",
        "        self,",
        "        filepath: str,",
        "        layer: Optional[CorticalLayer] = None,",
        "        max_nodes: int = 500",
        "    ) -> Dict:",
        "        \"\"\"Export graph to JSON for visualization.\"\"\"",
        "        return persistence.export_graph_json(filepath, self.layers, layer, max_nodes=max_nodes)",
        "",
        "    def export_conceptnet_json(",
        "        self,",
        "        filepath: str,",
        "        include_cross_layer: bool = True,",
        "        include_typed_edges: bool = True,",
        "        min_weight: float = 0.0,",
        "        min_confidence: float = 0.0,",
        "        max_nodes_per_layer: int = 100,",
        "        verbose: bool = True",
        "    ) -> Dict[str, Any]:",
        "        \"\"\"",
        "        Export ConceptNet-style graph for visualization.",
        "",
        "        Creates a rich graph format with color-coded nodes by layer",
        "        and typed edges with relation types and confidence scores.",
        "",
        "        Args:",
        "            filepath: Output file path (JSON)",
        "            include_cross_layer: Include feedforward/feedback edges",
        "            include_typed_edges: Include typed_connections with relation types",
        "            min_weight: Minimum edge weight to include",
        "            min_confidence: Minimum confidence for typed edges",
        "            max_nodes_per_layer: Maximum nodes per layer (by PageRank)",
        "            verbose: Print progress messages",
        "",
        "        Returns:",
        "            The exported graph data",
        "        \"\"\"",
        "        return persistence.export_conceptnet_json(",
        "            filepath,",
        "            self.layers,",
        "            semantic_relations=self.semantic_relations,",
        "            include_cross_layer=include_cross_layer,",
        "            include_typed_edges=include_typed_edges,",
        "            min_weight=min_weight,",
        "            min_confidence=min_confidence,",
        "            max_nodes_per_layer=max_nodes_per_layer,",
        "            verbose=verbose",
        "        )"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "cortical/processor/query_api.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "\"\"\"",
        "Query API: search, expansion, and retrieval methods.",
        "",
        "This module contains all query-related methods that delegate to the query module.",
        "\"\"\"",
        "",
        "import logging",
        "from typing import Dict, List, Tuple, Optional, Any",
        "",
        "from .. import query as query_module",
        "",
        "logger = logging.getLogger(__name__)",
        "",
        "",
        "class QueryMixin:",
        "    \"\"\"",
        "    Mixin providing query functionality.",
        "",
        "    Requires CoreMixin to be present (provides layers, documents, tokenizer,",
        "    config, semantic_relations, embeddings, _query_expansion_cache, _query_cache_max_size).",
        "    \"\"\"",
        "",
        "    def expand_query(",
        "        self,",
        "        query_text: str,",
        "        max_expansions: Optional[int] = None,",
        "        use_variants: bool = True,",
        "        use_code_concepts: bool = False,",
        "        filter_code_stop_words: bool = False,",
        "        verbose: bool = False",
        "    ) -> Dict[str, float]:",
        "        \"\"\"",
        "        Expand a query using lateral connections and concept clusters.",
        "",
        "        Args:",
        "            query_text: Original query string",
        "            max_expansions: Maximum expansion terms to add (default from config)",
        "            use_variants: Try word variants when direct match fails",
        "            use_code_concepts: Include programming synonym expansions",
        "            filter_code_stop_words: Filter ubiquitous code tokens (self, cls, etc.)",
        "",
        "        Returns:",
        "            Dict mapping terms to weights",
        "",
        "        Raises:",
        "            ValueError: If query_text is empty or max_expansions is negative",
        "        \"\"\"",
        "        if not isinstance(query_text, str) or not query_text.strip():",
        "            raise ValueError(\"query_text must be a non-empty string\")",
        "        if max_expansions is None:",
        "            max_expansions = self.config.max_query_expansions",
        "        if not isinstance(max_expansions, int) or max_expansions < 0:",
        "            raise ValueError(\"max_expansions must be a non-negative integer\")",
        "",
        "        return query_module.expand_query(",
        "            query_text,",
        "            self.layers,",
        "            self.tokenizer,",
        "            max_expansions=max_expansions,",
        "            use_variants=use_variants,",
        "            use_code_concepts=use_code_concepts,",
        "            filter_code_stop_words=filter_code_stop_words",
        "        )",
        "",
        "    def expand_query_for_code(self, query_text: str, max_expansions: Optional[int] = None) -> Dict[str, float]:",
        "        \"\"\"",
        "        Expand a query optimized for code search.",
        "",
        "        Args:",
        "            query_text: Original query string",
        "            max_expansions: Maximum expansion terms to add (default from config + 5)",
        "",
        "        Returns:",
        "            Dict mapping terms to weights",
        "        \"\"\"",
        "        if max_expansions is None:",
        "            max_expansions = self.config.max_query_expansions + 5",
        "",
        "        return query_module.expand_query(",
        "            query_text,",
        "            self.layers,",
        "            self.tokenizer,",
        "            max_expansions=max_expansions,",
        "            use_variants=True,",
        "            use_code_concepts=True,",
        "            filter_code_stop_words=True",
        "        )",
        "",
        "    def expand_query_cached(",
        "        self,",
        "        query_text: str,",
        "        max_expansions: Optional[int] = None,",
        "        use_variants: bool = True,",
        "        use_code_concepts: bool = False",
        "    ) -> Dict[str, float]:",
        "        \"\"\"",
        "        Expand a query with caching for faster repeated lookups.",
        "",
        "        Args:",
        "            query_text: Original query string",
        "            max_expansions: Maximum expansion terms to add (default from config)",
        "            use_variants: Try word variants when direct match fails",
        "            use_code_concepts: Include programming synonym expansions",
        "",
        "        Returns:",
        "            Dict mapping terms to weights",
        "        \"\"\"",
        "        if max_expansions is None:",
        "            max_expansions = self.config.max_query_expansions",
        "",
        "        cache_key = f\"{query_text}|{max_expansions}|{use_variants}|{use_code_concepts}\"",
        "",
        "        if cache_key in self._query_expansion_cache:",
        "            return self._query_expansion_cache[cache_key].copy()",
        "",
        "        result = query_module.expand_query(",
        "            query_text,",
        "            self.layers,",
        "            self.tokenizer,",
        "            max_expansions=max_expansions,",
        "            use_variants=use_variants,",
        "            use_code_concepts=use_code_concepts",
        "        )",
        "",
        "        if len(self._query_expansion_cache) >= self._query_cache_max_size:",
        "            oldest_key = next(iter(self._query_expansion_cache))",
        "            del self._query_expansion_cache[oldest_key]",
        "",
        "        self._query_expansion_cache[cache_key] = result.copy()",
        "        return result",
        "",
        "    def clear_query_cache(self) -> int:",
        "        \"\"\"",
        "        Clear the query expansion cache.",
        "",
        "        Returns:",
        "            Number of cache entries cleared",
        "        \"\"\"",
        "        count = len(self._query_expansion_cache)",
        "        self._query_expansion_cache.clear()",
        "        return count",
        "",
        "    def set_query_cache_size(self, max_size: int) -> None:",
        "        \"\"\"",
        "        Set the maximum size of the query expansion cache.",
        "",
        "        Args:",
        "            max_size: Maximum number of queries to cache (must be > 0)",
        "",
        "        Raises:",
        "            ValueError: If max_size <= 0",
        "        \"\"\"",
        "        if max_size <= 0:",
        "            raise ValueError(f\"max_size must be positive, got {max_size}\")",
        "        self._query_cache_max_size = max_size",
        "",
        "        while len(self._query_expansion_cache) > max_size:",
        "            oldest_key = next(iter(self._query_expansion_cache))",
        "            del self._query_expansion_cache[oldest_key]",
        "",
        "    def parse_intent_query(self, query_text: str) -> Dict:",
        "        \"\"\"",
        "        Parse a natural language query to extract intent and searchable terms.",
        "",
        "        Args:",
        "            query_text: Natural language query string",
        "",
        "        Returns:",
        "            Dict with 'action', 'subject', 'intent', 'question_word', 'expanded_terms'",
        "        \"\"\"",
        "        return query_module.parse_intent_query(query_text)",
        "",
        "    def search_by_intent(self, query_text: str, top_n: int = 5) -> List[Tuple[str, float, Dict]]:",
        "        \"\"\"",
        "        Search the corpus using intent-based query understanding.",
        "",
        "        Args:",
        "            query_text: Natural language query string",
        "            top_n: Number of results to return",
        "",
        "        Returns:",
        "            List of (doc_id, score, parsed_intent) tuples",
        "        \"\"\"",
        "        return query_module.search_by_intent(",
        "            query_text,",
        "            self.layers,",
        "            self.tokenizer,",
        "            top_n=top_n",
        "        )",
        "",
        "    def expand_query_semantic(self, query_text: str, max_expansions: int = 10) -> Dict[str, float]:",
        "        return query_module.expand_query_semantic(",
        "            query_text, self.layers, self.tokenizer, self.semantic_relations, max_expansions",
        "        )",
        "",
        "    def complete_analogy(",
        "        self,",
        "        term_a: str,",
        "        term_b: str,",
        "        term_c: str,",
        "        top_n: int = 5,",
        "        use_embeddings: bool = True,",
        "        use_relations: bool = True",
        "    ) -> List[Tuple[str, float, str]]:",
        "        \"\"\"",
        "        Complete an analogy: \"a is to b as c is to ?\"",
        "",
        "        Args:",
        "            term_a: First term of the known pair",
        "            term_b: Second term of the known pair",
        "            term_c: First term of the query pair",
        "            top_n: Number of candidates to return",
        "            use_embeddings: Whether to use embedding-based completion",
        "            use_relations: Whether to use relation-based completion",
        "",
        "        Returns:",
        "            List of (candidate_term, confidence, method) tuples",
        "",
        "        Raises:",
        "            ValueError: If any term is empty or top_n is not positive",
        "        \"\"\"",
        "        for name, term in [('term_a', term_a), ('term_b', term_b), ('term_c', term_c)]:",
        "            if not isinstance(term, str) or not term.strip():",
        "                raise ValueError(f\"{name} must be a non-empty string\")",
        "        if not isinstance(top_n, int) or top_n < 1:",
        "            raise ValueError(\"top_n must be a positive integer\")",
        "",
        "        if not self.semantic_relations:",
        "            self.extract_corpus_semantics(verbose=False)",
        "",
        "        return query_module.complete_analogy(",
        "            term_a, term_b, term_c,",
        "            self.layers,",
        "            self.semantic_relations,",
        "            embeddings=self.embeddings,",
        "            top_n=top_n,",
        "            use_embeddings=use_embeddings,",
        "            use_relations=use_relations",
        "        )",
        "",
        "    def complete_analogy_simple(",
        "        self,",
        "        term_a: str,",
        "        term_b: str,",
        "        term_c: str,",
        "        top_n: int = 5",
        "    ) -> List[Tuple[str, float]]:",
        "        \"\"\"",
        "        Simplified analogy completion using only term relationships.",
        "",
        "        Args:",
        "            term_a: First term of the known pair",
        "            term_b: Second term of the known pair",
        "            term_c: First term of the query pair",
        "            top_n: Number of candidates to return",
        "",
        "        Returns:",
        "            List of (candidate_term, confidence) tuples",
        "        \"\"\"",
        "        return query_module.complete_analogy_simple(",
        "            term_a, term_b, term_c,",
        "            self.layers,",
        "            self.tokenizer,",
        "            semantic_relations=self.semantic_relations,",
        "            top_n=top_n",
        "        )",
        "",
        "    def expand_query_multihop(",
        "        self,",
        "        query_text: str,",
        "        max_hops: int = 2,",
        "        max_expansions: int = 15,",
        "        decay_factor: float = 0.5,",
        "        min_path_score: float = 0.2",
        "    ) -> Dict[str, float]:",
        "        \"\"\"",
        "        Expand query using multi-hop semantic inference.",
        "",
        "        Args:",
        "            query_text: Original query string",
        "            max_hops: Maximum number of relation hops (default: 2)",
        "            max_expansions: Maximum expansion terms to return",
        "            decay_factor: Weight decay per hop (default: 0.5)",
        "            min_path_score: Minimum path validity score (default: 0.2)",
        "",
        "        Returns:",
        "            Dict mapping terms to weights",
        "        \"\"\"",
        "        if not self.semantic_relations:",
        "            return self.expand_query(query_text, max_expansions=max_expansions)",
        "",
        "        return query_module.expand_query_multihop(",
        "            query_text,",
        "            self.layers,",
        "            self.tokenizer,",
        "            self.semantic_relations,",
        "            max_hops=max_hops,",
        "            max_expansions=max_expansions,",
        "            decay_factor=decay_factor,",
        "            min_path_score=min_path_score",
        "        )",
        "",
        "    def find_documents_for_query(",
        "        self,",
        "        query_text: str,",
        "        top_n: int = 5,",
        "        use_expansion: bool = True,",
        "        use_semantic: bool = True",
        "    ) -> List[Tuple[str, float]]:",
        "        \"\"\"",
        "        Find documents most relevant to a query.",
        "",
        "        Args:",
        "            query_text: Search query",
        "            top_n: Number of documents to return",
        "            use_expansion: Whether to expand query terms",
        "            use_semantic: Whether to use semantic relations for expansion",
        "",
        "        Returns:",
        "            List of (doc_id, score) tuples ranked by relevance",
        "",
        "        Raises:",
        "            ValueError: If query_text is empty or top_n is not positive",
        "        \"\"\"",
        "        if not isinstance(query_text, str) or not query_text.strip():",
        "            raise ValueError(\"query_text must be a non-empty string\")",
        "        if not isinstance(top_n, int) or top_n < 1:",
        "            raise ValueError(\"top_n must be a positive integer\")",
        "",
        "        return query_module.find_documents_for_query(",
        "            query_text,",
        "            self.layers,",
        "            self.tokenizer,",
        "            top_n=top_n,",
        "            use_expansion=use_expansion,",
        "            semantic_relations=self.semantic_relations if use_semantic else None,",
        "            use_semantic=use_semantic",
        "        )",
        "",
        "    def fast_find_documents(",
        "        self,",
        "        query_text: str,",
        "        top_n: int = 5,",
        "        candidate_multiplier: int = 3,",
        "        use_code_concepts: bool = True",
        "    ) -> List[Tuple[str, float]]:",
        "        \"\"\"",
        "        Fast document search using candidate filtering.",
        "",
        "        Args:",
        "            query_text: Search query",
        "            top_n: Number of results to return",
        "            candidate_multiplier: Multiplier for candidate set size",
        "            use_code_concepts: Whether to use code concept expansion",
        "",
        "        Returns:",
        "            List of (doc_id, score) tuples ranked by relevance",
        "",
        "        Raises:",
        "            ValueError: If query_text is empty or top_n is not positive",
        "        \"\"\"",
        "        if not isinstance(query_text, str) or not query_text.strip():",
        "            raise ValueError(\"query_text must be a non-empty string\")",
        "        if not isinstance(top_n, int) or top_n < 1:",
        "            raise ValueError(\"top_n must be a positive integer\")",
        "        if not isinstance(candidate_multiplier, int) or candidate_multiplier < 1:",
        "            raise ValueError(\"candidate_multiplier must be a positive integer\")",
        "",
        "        return query_module.fast_find_documents(",
        "            query_text,",
        "            self.layers,",
        "            self.tokenizer,",
        "            top_n=top_n,",
        "            candidate_multiplier=candidate_multiplier,",
        "            use_code_concepts=use_code_concepts",
        "        )",
        "",
        "    def quick_search(self, query: str, top_n: int = 5) -> List[str]:",
        "        \"\"\"",
        "        One-call document search with sensible defaults.",
        "",
        "        Args:",
        "            query: Search query string",
        "            top_n: Number of results to return (default 5)",
        "",
        "        Returns:",
        "            List of document IDs ranked by relevance",
        "        \"\"\"",
        "        results = self.find_documents_for_query(query, top_n=top_n)",
        "        return [doc_id for doc_id, _score in results]",
        "",
        "    def rag_retrieve(",
        "        self,",
        "        query: str,",
        "        top_n: int = 3,",
        "        max_chars_per_passage: int = 500",
        "    ) -> List[Dict[str, Any]]:",
        "        \"\"\"",
        "        Retrieve passages optimized for RAG.",
        "",
        "        Args:",
        "            query: Search query string",
        "            top_n: Number of passages to return (default 3)",
        "            max_chars_per_passage: Maximum characters per passage (default 500)",
        "",
        "        Returns:",
        "            List of passage dictionaries with text, doc_id, start, end, score",
        "        \"\"\"",
        "        results = self.find_passages_for_query(",
        "            query,",
        "            top_n=top_n,",
        "            chunk_size=max_chars_per_passage",
        "        )",
        "        return [",
        "            {",
        "                'text': text,",
        "                'doc_id': doc_id,",
        "                'start': start,",
        "                'end': end,",
        "                'score': score",
        "            }",
        "            for text, doc_id, start, end, score in results",
        "        ]",
        "",
        "    def explore(self, query: str, top_n: int = 5) -> Dict[str, Any]:",
        "        \"\"\"",
        "        Search with query expansion visibility.",
        "",
        "        Args:",
        "            query: Search query string",
        "            top_n: Number of results to return (default 5)",
        "",
        "        Returns:",
        "            Dictionary with results, expansion, original_terms",
        "        \"\"\"",
        "        expansion = self.expand_query(query)",
        "        results = self.find_documents_for_query(query, top_n=top_n)",
        "        original_terms = list(self.tokenizer.tokenize(query))",
        "",
        "        return {",
        "            'results': results,",
        "            'expansion': expansion,",
        "            'original_terms': original_terms",
        "        }",
        "",
        "    def find_documents_with_boost(",
        "        self,",
        "        query_text: str,",
        "        top_n: int = 5,",
        "        auto_detect_intent: bool = True,",
        "        prefer_docs: bool = False,",
        "        custom_boosts: Optional[Dict[str, float]] = None,",
        "        use_expansion: bool = True,",
        "        use_semantic: bool = True",
        "    ) -> List[Tuple[str, float]]:",
        "        \"\"\"",
        "        Find documents with optional document-type boosting.",
        "",
        "        Args:",
        "            query_text: Search query",
        "            top_n: Number of results to return",
        "            auto_detect_intent: Auto-boost docs for conceptual queries",
        "            prefer_docs: Always boost documentation",
        "            custom_boosts: Optional custom boost factors per doc_type",
        "            use_expansion: Whether to expand query terms",
        "            use_semantic: Whether to use semantic relations",
        "",
        "        Returns:",
        "            List of (doc_id, score) tuples ranked by relevance",
        "        \"\"\"",
        "        return query_module.find_documents_with_boost(",
        "            query_text,",
        "            self.layers,",
        "            self.tokenizer,",
        "            top_n=top_n,",
        "            doc_metadata=self.document_metadata,",
        "            auto_detect_intent=auto_detect_intent,",
        "            prefer_docs=prefer_docs,",
        "            custom_boosts=custom_boosts,",
        "            use_expansion=use_expansion,",
        "            semantic_relations=self.semantic_relations if use_semantic else None,",
        "            use_semantic=use_semantic",
        "        )",
        "",
        "    def is_conceptual_query(self, query_text: str) -> bool:",
        "        \"\"\"Check if a query appears to be conceptual.\"\"\"",
        "        return query_module.is_conceptual_query(query_text)",
        "",
        "    def build_search_index(self) -> Dict[str, Dict[str, float]]:",
        "        \"\"\"Build an optimized inverted index for fast querying.\"\"\"",
        "        return query_module.build_document_index(self.layers)",
        "",
        "    def search_with_index(",
        "        self,",
        "        query_text: str,",
        "        index: Dict[str, Dict[str, float]],",
        "        top_n: int = 5",
        "    ) -> List[Tuple[str, float]]:",
        "        \"\"\"Search using a pre-built inverted index.\"\"\"",
        "        return query_module.search_with_index(",
        "            query_text,",
        "            index,",
        "            self.tokenizer,",
        "            top_n=top_n",
        "        )",
        "",
        "    def find_passages_for_query(",
        "        self,",
        "        query_text: str,",
        "        top_n: int = 5,",
        "        chunk_size: Optional[int] = None,",
        "        overlap: Optional[int] = None,",
        "        use_expansion: bool = True,",
        "        doc_filter: Optional[List[str]] = None,",
        "        use_semantic: bool = True,",
        "        use_definition_search: bool = True,",
        "        definition_boost: float = 5.0,",
        "        apply_doc_boost: bool = True,",
        "        auto_detect_intent: bool = True,",
        "        prefer_docs: bool = False,",
        "        custom_boosts: Optional[Dict[str, float]] = None,",
        "        use_code_aware_chunks: bool = True",
        "    ) -> List[Tuple[str, str, int, int, float]]:",
        "        \"\"\"",
        "        Find text passages most relevant to a query (for RAG systems).",
        "",
        "        Args:",
        "            query_text: Search query",
        "            top_n: Number of passages to return",
        "            chunk_size: Size of each chunk in characters (default from config)",
        "            overlap: Overlap between chunks in characters (default from config)",
        "            use_expansion: Whether to expand query terms",
        "            doc_filter: Optional list of doc_ids to restrict search to",
        "            use_semantic: Whether to use semantic relations for expansion",
        "            use_definition_search: Whether to search for definition patterns",
        "            definition_boost: Score boost for definition matches",
        "            apply_doc_boost: Whether to apply document-type boosting",
        "            auto_detect_intent: Auto-detect conceptual queries and boost docs",
        "            prefer_docs: Always boost documentation",
        "            custom_boosts: Optional custom boost factors for doc types",
        "            use_code_aware_chunks: Use semantic boundaries for code files",
        "",
        "        Returns:",
        "            List of (passage_text, doc_id, start_char, end_char, score) tuples",
        "",
        "        Raises:",
        "            ValueError: If query_text is empty or parameters are invalid",
        "        \"\"\"",
        "        if not isinstance(query_text, str) or not query_text.strip():",
        "            raise ValueError(\"query_text must be a non-empty string\")",
        "        if not isinstance(top_n, int) or top_n < 1:",
        "            raise ValueError(\"top_n must be a positive integer\")",
        "",
        "        if chunk_size is None:",
        "            chunk_size = self.config.chunk_size",
        "        else:",
        "            if not isinstance(chunk_size, int) or chunk_size < 1:",
        "                raise ValueError(\"chunk_size must be a positive integer\")",
        "",
        "        if overlap is None:",
        "            overlap = self.config.chunk_overlap",
        "        else:",
        "            if not isinstance(overlap, int) or overlap < 0:",
        "                raise ValueError(\"overlap must be a non-negative integer\")",
        "            if overlap >= chunk_size:",
        "                raise ValueError(f\"overlap ({overlap}) must be less than chunk_size ({chunk_size})\")",
        "",
        "        return query_module.find_passages_for_query(",
        "            query_text,",
        "            self.layers,",
        "            self.tokenizer,",
        "            self.documents,",
        "            top_n=top_n,",
        "            chunk_size=chunk_size,",
        "            overlap=overlap,",
        "            use_expansion=use_expansion,",
        "            doc_filter=doc_filter,",
        "            semantic_relations=self.semantic_relations if use_semantic else None,",
        "            use_semantic=use_semantic,",
        "            use_definition_search=use_definition_search,",
        "            definition_boost=definition_boost,",
        "            apply_doc_boost=apply_doc_boost,",
        "            doc_metadata=self.document_metadata,",
        "            auto_detect_intent=auto_detect_intent,",
        "            prefer_docs=prefer_docs,",
        "            custom_boosts=custom_boosts,",
        "            use_code_aware_chunks=use_code_aware_chunks",
        "        )",
        "",
        "    def is_definition_query(self, query_text: str) -> Tuple[bool, Optional[str], Optional[str]]:",
        "        \"\"\"Detect if a query is looking for a code definition.\"\"\"",
        "        return query_module.is_definition_query(query_text)",
        "",
        "    def find_definition_passages(",
        "        self,",
        "        query_text: str,",
        "        context_chars: int = 500,",
        "        boost: float = 5.0",
        "    ) -> List[Tuple[str, str, int, int, float]]:",
        "        \"\"\"Find definition passages for a definition query.\"\"\"",
        "        return query_module.find_definition_passages(",
        "            query_text, self.documents, context_chars, boost",
        "        )",
        "",
        "    def find_documents_batch(",
        "        self,",
        "        queries: List[str],",
        "        top_n: int = 5,",
        "        use_expansion: bool = True,",
        "        use_semantic: bool = True",
        "    ) -> List[List[Tuple[str, float]]]:",
        "        \"\"\"Find documents for multiple queries efficiently.\"\"\"",
        "        return query_module.find_documents_batch(",
        "            queries,",
        "            self.layers,",
        "            self.tokenizer,",
        "            top_n=top_n,",
        "            use_expansion=use_expansion,",
        "            semantic_relations=self.semantic_relations if use_semantic else None,",
        "            use_semantic=use_semantic",
        "        )",
        "",
        "    def find_passages_batch(",
        "        self,",
        "        queries: List[str],",
        "        top_n: int = 5,",
        "        chunk_size: int = 512,",
        "        overlap: int = 128,",
        "        use_expansion: bool = True,",
        "        doc_filter: Optional[List[str]] = None,",
        "        use_semantic: bool = True",
        "    ) -> List[List[Tuple[str, str, int, int, float]]]:",
        "        \"\"\"Find passages for multiple queries efficiently.\"\"\"",
        "        return query_module.find_passages_batch(",
        "            queries,",
        "            self.layers,",
        "            self.tokenizer,",
        "            self.documents,",
        "            top_n=top_n,",
        "            chunk_size=chunk_size,",
        "            overlap=overlap,",
        "            use_expansion=use_expansion,",
        "            doc_filter=doc_filter,",
        "            semantic_relations=self.semantic_relations if use_semantic else None,",
        "            use_semantic=use_semantic",
        "        )",
        "",
        "    def multi_stage_rank(",
        "        self,",
        "        query_text: str,",
        "        top_n: int = 5,",
        "        chunk_size: int = 512,",
        "        overlap: int = 128,",
        "        concept_boost: float = 0.3,",
        "        use_expansion: bool = True,",
        "        use_semantic: bool = True",
        "    ) -> List[Tuple[str, str, int, int, float, Dict[str, float]]]:",
        "        \"\"\"Multi-stage ranking pipeline for improved RAG performance.\"\"\"",
        "        return query_module.multi_stage_rank(",
        "            query_text,",
        "            self.layers,",
        "            self.tokenizer,",
        "            self.documents,",
        "            top_n=top_n,",
        "            chunk_size=chunk_size,",
        "            overlap=overlap,",
        "            concept_boost=concept_boost,",
        "            use_expansion=use_expansion,",
        "            semantic_relations=self.semantic_relations if use_semantic else None,",
        "            use_semantic=use_semantic",
        "        )",
        "",
        "    def multi_stage_rank_documents(",
        "        self,",
        "        query_text: str,",
        "        top_n: int = 5,",
        "        concept_boost: float = 0.3,",
        "        use_expansion: bool = True,",
        "        use_semantic: bool = True",
        "    ) -> List[Tuple[str, float, Dict[str, float]]]:",
        "        \"\"\"Multi-stage ranking for documents (without chunk scoring).\"\"\"",
        "        return query_module.multi_stage_rank_documents(",
        "            query_text,",
        "            self.layers,",
        "            self.tokenizer,",
        "            top_n=top_n,",
        "            concept_boost=concept_boost,",
        "            use_expansion=use_expansion,",
        "            semantic_relations=self.semantic_relations if use_semantic else None,",
        "            use_semantic=use_semantic",
        "        )",
        "",
        "    def query_expanded(self, query_text: str, top_n: int = 10, max_expansions: int = 8) -> List[Tuple[str, float]]:",
        "        return query_module.query_with_spreading_activation(",
        "            query_text, self.layers, self.tokenizer, top_n, max_expansions",
        "        )",
        "",
        "    def find_related_documents(self, doc_id: str) -> List[Tuple[str, float]]:",
        "        return query_module.find_related_documents(doc_id, self.layers)"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    }
  ],
  "hour_of_day": 14,
  "day_of_week": "Sunday",
  "seconds_since_last_commit": -83928,
  "is_merge": false,
  "is_initial": false,
  "parent_count": 1,
  "session_id": null,
  "related_chats": [],
  "ci_result": null,
  "reverted": false,
  "amended": false
}