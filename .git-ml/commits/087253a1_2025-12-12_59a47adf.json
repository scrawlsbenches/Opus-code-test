{
  "hash": "087253a167f76d52ff4d852a6dec4f229e9373c9",
  "message": "Merge pull request #47 from scrawlsbenches/claude/fix-hardcoded-values-013fphbkXkvegXoqXuLvhiyQ",
  "author": "scrawlsbenches",
  "timestamp": "2025-12-12 05:49:44 -0500",
  "branch": "claude/multi-index-design-DvifZ",
  "files_changed": [
    "TASK_LIST.md",
    "cortical/analysis.py",
    "cortical/layers.py",
    "cortical/minicolumn.py",
    "cortical/query/definitions.py",
    "cortical/query/expansion.py",
    "tests/test_layers.py",
    "tests/test_query.py"
  ],
  "insertions": 98,
  "deletions": 32,
  "hunks": [
    {
      "file": "TASK_LIST.md",
      "function": null,
      "start_line": 1,
      "lines_added": [
        "**Completed Tasks:** 134+ (see archive and Recently Completed)"
      ],
      "lines_removed": [
        "**Completed Tasks:** 133+ (see archive and Recently Completed)"
      ],
      "context_before": [
        "# Task List: Cortical Text Processor",
        "",
        "Active backlog for the Cortical Text Processor project. Completed tasks are archived in [TASK_ARCHIVE.md](TASK_ARCHIVE.md).",
        "",
        "**Last Updated:** 2025-12-12",
        "**Pending Tasks:** 35"
      ],
      "context_after": [
        "",
        "---",
        "",
        "## Active Backlog",
        "",
        "<!-- Machine-parseable format for automation -->",
        "",
        "### ðŸ”´ Critical (Do Now)",
        "",
        "*All critical tasks completed!*"
      ],
      "change_type": "modify"
    },
    {
      "file": "TASK_LIST.md",
      "function": "Active backlog for the Cortical Text Processor project. Completed tasks are arch",
      "start_line": 74,
      "lines_added": [
        "| 147 | Fix misleading hardcoded values | 2025-12-12 | 5 fixes: backwards param names, sparsity threshold, config constant, tolerance param, confidence semantics |"
      ],
      "lines_removed": [],
      "context_before": [
        "| # | Task | Started | Notes |",
        "|---|------|---------|-------|",
        "| 87 | Add Python code samples and showcase | 2025-12-11 | samples/*.py created |",
        "",
        "---",
        "",
        "## Recently Completed (Last 7 Days)",
        "",
        "| # | Task | Completed | Notes |",
        "|---|------|-----------|-------|"
      ],
      "context_after": [
        "| 139 | Batch bigram connection updates to reduce dict overhead | 2025-12-12 | add_lateral_connections_batch() method in minicolumn.py |",
        "| 137 | Cap bigram connections to top-K per bigram | 2025-12-12 | max_connections_per_bigram parameter (default 50) in analysis.py |",
        "| 116 | Document return value semantics | 2025-12-12 | Edge cases, score ranges, None vs exceptions, default parameters |",
        "| 114 | Add type aliases for complex types | 2025-12-12 | cortical/types.py with 20+ aliases: DocumentScore, PassageResult, SemanticRelation, etc. |",
        "| 113 | Document staleness tracking system | 2025-12-12 | Comprehensive docs in CLAUDE.md: computation types, API, incremental updates |",
        "| 96 | Centralize duplicate constants | 2025-12-12 | cortical/constants.py with RELATION_WEIGHTS, DOC_TYPE_BOOSTS, query keywords |",
        "| 91 | Create docs/README.md index | 2025-12-12 | Navigation by audience, reading paths, categorized docs |",
        "| 92 | Add badges to README.md | 2025-12-12 | Python, License, Tests, Coverage, Zero Dependencies badges |",
        "| 93 | Update README with docs references | 2025-12-12 | Documentation section with table linking to docs/*.md |",
        "| 146 | Create behavioral tests for core user workflows | 2025-12-12 | 11 tests across 4 categories: Search, Performance, Quality, Robustness |"
      ],
      "change_type": "add"
    },
    {
      "file": "TASK_LIST.md",
      "function": "user outcomes, catching the kinds of issues discovered during dog-fooding.",
      "start_line": 1491,
      "lines_added": [
        "### 147. Fix Misleading Hardcoded Values âœ…",
        "",
        "**Meta:** `status:completed` `priority:high` `category:bugfix`",
        "**Files:** `cortical/query/definitions.py`, `cortical/layers.py`, `cortical/query/expansion.py`, `cortical/analysis.py`, `cortical/minicolumn.py`, `tests/test_layers.py`, `tests/test_query.py`",
        "**Effort:** Medium",
        "**Completed:** 2025-12-12",
        "",
        "**Problem:** Several hardcoded values in the codebase were misleading - they made something appear to be true when it wasn't.",
        "",
        "**Issues Fixed:**",
        "",
        "1. **Backwards parameter names** (`definitions.py:309-310`):",
        "   - `test_file_boost_factor=0.5` sounded like a boost but actually reduced scores by 50%",
        "   - `test_file_penalty=0.7` was called \"penalty\" but was lighter than the \"boost\"",
        "   - Renamed to `test_with_definition_penalty` and `test_without_definition_penalty`",
        "",
        "2. **Useless sparsity threshold** (`layers.py:192`):",
        "   - Hardcoded `threshold=1.0` only counted terms with activation=0 (unused terms)",
        "   - Changed to use `threshold_fraction * average_activation` for meaningful sparsity",
        "",
        "3. **Hardcoded config value** (`expansion.py:77`):",
        "   - Hardcoded `0.4` instead of using `DEFAULT_CHAIN_VALIDITY` constant",
        "   - Now imports and uses the config constant",
        "",
        "4. **Tolerance parameter ignored** (`analysis.py:301`):",
        "   - `compute_hierarchical_pagerank()` ignored caller's `tolerance` parameter",
        "   - Was hardcoded to `1e-6` instead of using the function parameter",
        "",
        "5. **Confidence only increasing** (`minicolumn.py:189`):",
        "   - Used `max(old, new)` so confidence could only increase, never decrease",
        "   - Changed to weighted average so confidence can decrease with lower-confidence evidence",
        "",
        "**Acceptance Criteria:**",
        "- [x] All misleading parameter names clarified",
        "- [x] Sparsity threshold made meaningful",
        "- [x] Config constants used consistently",
        "- [x] Function parameters respected (not ignored)",
        "- [x] Confidence semantics corrected",
        "- [x] All 1133 tests pass",
        "",
        "---",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "**Acceptance Criteria:**",
        "- [x] TestSearchBehavior class with 3+ tests",
        "- [x] TestPerformanceBehavior class with 2+ tests",
        "- [x] TestQualityBehavior class with 3+ tests (4 implemented)",
        "- [x] TestRobustnessBehavior class with 2+ tests",
        "- [x] All tests pass on current codebase",
        "- [x] Tests catch regressions from Tasks #141-145 if reverted",
        "",
        "---",
        ""
      ],
      "context_after": [
        "## Category Index",
        "",
        "| Category | Pending | Description |",
        "|----------|---------|-------------|",
        "| Perf | 1 | Performance improvements (#138) |",
        "| Arch | 6 | Architecture refactoring (#133, 134, 135, 95, 100, 101) |",
        "| CodeQual | 2 | Code quality improvements (#98, 99) |",
        "| Testing | 2 | Test coverage (#102, 129) |",
        "| TaskMgmt | 3 | Task management system (#107, 106, 108) |",
        "| AINav | 3 | AI assistant navigation (#115, 117, 118) |"
      ],
      "change_type": "add"
    },
    {
      "file": "cortical/analysis.py",
      "function": "def compute_hierarchical_pagerank(",
      "start_line": 291,
      "lines_added": [
        "            compute_pagerank(layer, damping=damping, iterations=layer_iterations, tolerance=tolerance)"
      ],
      "lines_removed": [
        "            compute_pagerank(layer, damping=damping, iterations=layer_iterations, tolerance=1e-6)"
      ],
      "context_before": [
        "    iterations_run = 0",
        "    converged = False",
        "",
        "    for global_iter in range(global_iterations):",
        "        iterations_run = global_iter + 1",
        "        max_global_diff = 0.0",
        "",
        "        # Step 1: Compute local PageRank for each layer",
        "        for layer_enum in active_layers:",
        "            layer = layers[layer_enum]"
      ],
      "context_after": [
        "",
        "        # Step 2: Propagate up (tokens â†’ bigrams â†’ concepts â†’ documents)",
        "        for i in range(len(active_layers) - 1):",
        "            lower_layer_enum = active_layers[i]",
        "            upper_layer_enum = active_layers[i + 1]",
        "            lower_layer = layers[lower_layer_enum]",
        "            upper_layer = layers[upper_layer_enum]",
        "",
        "            # Propagate from lower to upper via feedback connections",
        "            for col in lower_layer.minicolumns.values():"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/layers.py",
      "function": "class HierarchicalLayer:",
      "start_line": 170,
      "lines_added": [
        "    def sparsity(self, threshold_fraction: float = 0.5) -> float:",
        "        Calculate sparsity (fraction of columns with below-average activation).",
        "",
        "        more efficient and allow for more distinct patterns. This measures",
        "        the fraction of columns activated below a threshold relative to",
        "        the average activation.",
        "",
        "        Args:",
        "            threshold_fraction: Fraction of average activation to use as threshold.",
        "                Columns with activation < (average * threshold_fraction) count as sparse.",
        "                Default 0.5 means columns below 50% of average activation.",
        "",
        "        avg_activation = self.average_activation()",
        "        if avg_activation == 0:",
        "            return 1.0  # All columns are sparse if no activation",
        "        threshold = avg_activation * threshold_fraction",
        "        low_activation = sum(1 for col in self.minicolumns.values()"
      ],
      "lines_removed": [
        "    def sparsity(self) -> float:",
        "        Calculate sparsity (fraction of columns with low activation).",
        "        ",
        "        more efficient and allow for more distinct patterns.",
        "        ",
        "        threshold = 1.0  # Activation threshold",
        "        low_activation = sum(1 for col in self.minicolumns.values() "
      ],
      "context_before": [
        "            return 0.0",
        "        return sum(col.activation for col in self.minicolumns.values()) / len(self.minicolumns)",
        "    ",
        "    def activation_range(self) -> tuple:",
        "        \"\"\"Return (min, max) activation values.\"\"\"",
        "        if not self.minicolumns:",
        "            return (0.0, 0.0)",
        "        activations = [col.activation for col in self.minicolumns.values()]",
        "        return (min(activations), max(activations))",
        "    "
      ],
      "context_after": [
        "        \"\"\"",
        "        In biological neural networks, sparse representations are",
        "        Returns:",
        "            Fraction of columns with activation below threshold",
        "        \"\"\"",
        "        if not self.minicolumns:",
        "            return 0.0",
        "                            if col.activation < threshold)",
        "        return low_activation / len(self.minicolumns)",
        "    ",
        "    def top_by_pagerank(self, n: int = 10) -> list:",
        "        \"\"\"",
        "        Get top minicolumns by PageRank score.",
        "        ",
        "        Args:",
        "            n: Number of results to return",
        "            "
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/minicolumn.py",
      "function": "class Minicolumn:",
      "start_line": 178,
      "lines_added": [
        "            # Weighted average of confidence (allows confidence to decrease with weaker evidence)",
        "            new_confidence = (existing.confidence * existing.weight + confidence * weight) / new_weight"
      ],
      "lines_removed": [
        "            # Use higher confidence",
        "            new_confidence = max(confidence, existing.confidence)"
      ],
      "context_before": [
        "        Example:",
        "            col.add_typed_connection(\"L0_network\", 0.8, relation_type='RelatedTo')",
        "            col.add_typed_connection(\"L0_brain\", 0.5, relation_type='IsA', source='semantic')",
        "        \"\"\"",
        "        if target_id in self.typed_connections:",
        "            # Accumulate weight, keep most informative metadata",
        "            existing = self.typed_connections[target_id]",
        "            new_weight = existing.weight + weight",
        "            # Prefer more specific relation types over 'co_occurrence'",
        "            new_relation = relation_type if relation_type != 'co_occurrence' else existing.relation_type"
      ],
      "context_after": [
        "            # Prefer semantic/inferred over corpus",
        "            source_priority = {'inferred': 3, 'semantic': 2, 'corpus': 1}",
        "            new_source = source if source_priority.get(source, 0) > source_priority.get(existing.source, 0) else existing.source",
        "            self.typed_connections[target_id] = Edge(",
        "                target_id=target_id,",
        "                weight=new_weight,",
        "                relation_type=new_relation,",
        "                confidence=new_confidence,",
        "                source=new_source",
        "            )"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/query/definitions.py",
      "function": "def is_test_file(doc_id: str) -> bool:",
      "start_line": 299,
      "lines_added": [
        "    test_with_definition_penalty: float = 0.5,",
        "    test_without_definition_penalty: float = 0.7",
        "    - Test files with the definition pattern get test_with_definition_penalty (default 0.5x)",
        "    - All other test files get test_without_definition_penalty (default 0.7x)",
        "        test_with_definition_penalty: Multiplier for test files that contain the definition",
        "            (default 0.5). Even test files with definitions are penalized vs source files.",
        "        test_without_definition_penalty: Multiplier for test files without the definition",
        "            (default 0.7). Set to 1.0 to disable test file penalty.",
        "                # Test file with definition: still penalized vs source files",
        "                boosted_docs.append((doc_id, score * test_with_definition_penalty))",
        "            boosted_docs.append((doc_id, score * test_without_definition_penalty))"
      ],
      "lines_removed": [
        "    test_file_boost_factor: float = 0.5,",
        "    test_file_penalty: float = 0.7",
        "    - Test files with the definition pattern get test_file_boost_factor (default 0.5x)",
        "    - All other test files get test_file_penalty (default 0.7x) to deprioritize them",
        "        test_file_boost_factor: Multiplier for test files with definition (default 0.5)",
        "        test_file_penalty: Multiplier for test files without definition (default 0.7)",
        "            Set to 1.0 to disable test file penalty.",
        "                # Test file with definition: apply reduced boost",
        "                boosted_docs.append((doc_id, score * test_file_boost_factor))",
        "            boosted_docs.append((doc_id, score * test_file_penalty))"
      ],
      "context_before": [
        "        return True",
        "",
        "    return False",
        "",
        "",
        "def boost_definition_documents(",
        "    doc_results: List[Tuple[str, float]],",
        "    query_text: str,",
        "    documents: Dict[str, str],",
        "    boost_factor: float = 2.0,"
      ],
      "context_after": [
        ") -> List[Tuple[str, float]]:",
        "    \"\"\"",
        "    Boost documents that contain the actual definition being searched for.",
        "",
        "    This helps ensure the source file containing a class/function definition",
        "    is included in the document candidates, even if test files mention the",
        "    identifier more frequently.",
        "",
        "    For definition queries:",
        "    - Source files with the definition pattern get boost_factor (default 2.0x)",
        "",
        "    Args:",
        "        doc_results: List of (doc_id, score) tuples",
        "        query_text: The original search query",
        "        documents: Dict mapping doc_id to document text",
        "        boost_factor: Multiplier for definition-containing source docs (default 2.0)",
        "",
        "    Returns:",
        "        Re-scored document results with definition boost applied",
        "    \"\"\"",
        "    definition_info = detect_definition_query(query_text)",
        "",
        "    if not definition_info['is_definition_query'] or not definition_info['pattern']:",
        "        return doc_results",
        "",
        "    pattern = re.compile(definition_info['pattern'], re.IGNORECASE)",
        "    boosted_docs = []",
        "",
        "    for doc_id, score in doc_results:",
        "        doc_text = documents.get(doc_id, '')",
        "        has_definition = pattern.search(doc_text)",
        "        is_test = is_test_file(doc_id)",
        "",
        "        if has_definition:",
        "            if is_test:",
        "            else:",
        "                # Source file with definition: apply full boost",
        "                boosted_docs.append((doc_id, score * boost_factor))",
        "        elif is_test:",
        "            # Test file without definition: apply penalty to deprioritize",
        "        else:",
        "            # Source file without definition: keep original score",
        "            boosted_docs.append((doc_id, score))",
        "",
        "    # Re-sort by boosted scores",
        "    boosted_docs.sort(key=lambda x: x[1], reverse=True)",
        "    return boosted_docs"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/query/expansion.py",
      "function": "This module provides:",
      "start_line": 11,
      "lines_added": [
        "from ..config import DEFAULT_CHAIN_VALIDITY"
      ],
      "lines_removed": [],
      "context_before": [
        "- Multi-hop inference through relation chains",
        "- Code concept expansion (programming synonyms)",
        "\"\"\"",
        "",
        "from typing import Dict, List, Tuple, Optional",
        "from collections import defaultdict",
        "",
        "from ..layers import CorticalLayer, HierarchicalLayer",
        "from ..tokenizer import Tokenizer, CODE_EXPANSION_STOP_WORDS",
        "from ..code_concepts import expand_code_concepts"
      ],
      "context_after": [
        "",
        "",
        "# Valid relation chain patterns for multi-hop inference",
        "# Key: (relation1, relation2) -> validity score (0.0 = invalid, 1.0 = fully valid)",
        "VALID_RELATION_CHAINS = {",
        "    # Transitive hierarchies",
        "    ('IsA', 'IsA'): 1.0,           # dog IsA animal IsA living_thing",
        "    ('PartOf', 'PartOf'): 1.0,     # wheel PartOf car PartOf vehicle",
        "    ('IsA', 'HasProperty'): 0.9,   # dog IsA animal HasProperty alive",
        "    ('PartOf', 'HasProperty'): 0.8,  # wheel PartOf car HasProperty fast"
      ],
      "change_type": "add"
    },
    {
      "file": "cortical/query/expansion.py",
      "function": "def score_relation_path(path: List[str]) -> float:",
      "start_line": 67,
      "lines_added": [
        "        pair_score = VALID_RELATION_CHAINS.get(pair, DEFAULT_CHAIN_VALIDITY)"
      ],
      "lines_removed": [
        "        pair_score = VALID_RELATION_CHAINS.get(pair, 0.4)  # Default: moderate validity"
      ],
      "context_before": [
        "    if not path:",
        "        return 1.0",
        "    if len(path) == 1:",
        "        return 1.0",
        "",
        "    # Compute score as product of consecutive pair validities",
        "    total_score = 1.0",
        "    for i in range(len(path) - 1):",
        "        pair = (path[i], path[i + 1])",
        "        # Check both orderings"
      ],
      "context_after": [
        "        total_score *= pair_score",
        "",
        "    return total_score",
        "",
        "",
        "def expand_query(",
        "    query_text: str,",
        "    layers: Dict[CorticalLayer, HierarchicalLayer],",
        "    tokenizer: Tokenizer,",
        "    max_expansions: int = 10,"
      ],
      "change_type": "modify"
    },
    {
      "file": "tests/test_layers.py",
      "function": "class TestTypedConnections(unittest.TestCase):",
      "start_line": 326,
      "lines_added": [
        "    def test_typed_connection_confidence_weighted_average(self):",
        "        \"\"\"Test that confidence uses weighted average (can increase or decrease).\"\"\"",
        "        # Weighted average: (0.7 * 0.5 + 0.9 * 0.3) / 0.8 = 0.775",
        "        self.assertAlmostEqual(edge.confidence, 0.775, places=5)",
        "",
        "    def test_typed_connection_confidence_can_decrease(self):",
        "        \"\"\"Test that confidence can decrease with lower-confidence evidence.\"\"\"",
        "        col = Minicolumn(\"L0_test\", \"test\", 0)",
        "        col.add_typed_connection(\"L0_other\", 1.0, confidence=0.9)  # High confidence",
        "        col.add_typed_connection(\"L0_other\", 1.0, confidence=0.3)  # Low confidence evidence",
        "",
        "        edge = col.typed_connections[\"L0_other\"]",
        "        # Weighted average: (0.9 * 1.0 + 0.3 * 1.0) / 2.0 = 0.6",
        "        self.assertAlmostEqual(edge.confidence, 0.6, places=5)"
      ],
      "lines_removed": [
        "    def test_typed_connection_confidence_max(self):",
        "        \"\"\"Test that confidence uses max value.\"\"\"",
        "        self.assertEqual(edge.confidence, 0.9)"
      ],
      "context_before": [
        "",
        "    def test_typed_connection_source_priority(self):",
        "        \"\"\"Test that semantic/inferred sources take priority over corpus.\"\"\"",
        "        col = Minicolumn(\"L0_test\", \"test\", 0)",
        "        col.add_typed_connection(\"L0_other\", 0.5, source='corpus')",
        "        col.add_typed_connection(\"L0_other\", 0.3, source='semantic')",
        "",
        "        edge = col.typed_connections[\"L0_other\"]",
        "        self.assertEqual(edge.source, 'semantic')",
        ""
      ],
      "context_after": [
        "        col = Minicolumn(\"L0_test\", \"test\", 0)",
        "        col.add_typed_connection(\"L0_other\", 0.5, confidence=0.7)",
        "        col.add_typed_connection(\"L0_other\", 0.3, confidence=0.9)",
        "",
        "        edge = col.typed_connections[\"L0_other\"]",
        "",
        "    def test_get_typed_connection(self):",
        "        \"\"\"Test retrieving a typed connection.\"\"\"",
        "        col = Minicolumn(\"L0_test\", \"test\", 0)",
        "        col.add_typed_connection(\"L0_other\", 0.5, relation_type='IsA')",
        "",
        "        edge = col.get_typed_connection(\"L0_other\")",
        "        self.assertIsNotNone(edge)",
        "        self.assertEqual(edge.relation_type, 'IsA')",
        ""
      ],
      "change_type": "modify"
    },
    {
      "file": "tests/test_query.py",
      "function": "class TestBoostDefinitionDocumentsTestFilePenalty(unittest.TestCase):",
      "start_line": 1869,
      "lines_added": [
        "            test_with_definition_penalty=0.5",
        "        \"\"\"Test that test_with_definition_penalty=1.0 disables the penalty.\"\"\"",
        "            test_with_definition_penalty=1.0  # No penalty",
        "        # Test file doesn't get full boost, it gets test_with_definition_penalty (1.0 here means no change)",
        "        # With test_with_definition_penalty=1.0, test file gets 10.0 * 1.0 = 10.0"
      ],
      "lines_removed": [
        "            test_file_boost_factor=0.5",
        "        \"\"\"Test that test_file_boost_factor=1.0 disables the penalty.\"\"\"",
        "            test_file_boost_factor=1.0  # No penalty",
        "        # Test file doesn't get full boost, it gets test_file_boost_factor (1.0 here means no change)",
        "        # Wait, if test_file_boost_factor=1.0, test file gets 10.0 * 1.0 = 10.0"
      ],
      "context_before": [
        "        documents = {",
        "            \"tests/test_analysis.py\": \"def compute_pagerank(layers, damping=0.85): pass  # mock\",",
        "            \"cortical/analysis.py\": \"def compute_pagerank(layers, damping=0.85):\\n    '''Real implementation'''\\n    result = do_stuff()\",",
        "        }",
        "",
        "        boosted = boost_definition_documents(",
        "            doc_results,",
        "            \"def compute_pagerank\",",
        "            documents,",
        "            boost_factor=2.0,"
      ],
      "context_after": [
        "        )",
        "",
        "        # Source file should be ranked first after boosting",
        "        self.assertEqual(boosted[0][0], \"cortical/analysis.py\")",
        "        # Source file gets 2.0x boost: 10.0 * 2.0 = 20.0",
        "        self.assertEqual(boosted[0][1], 20.0)",
        "        # Test file gets 0.5x penalty: 10.0 * 0.5 = 5.0",
        "        self.assertEqual(boosted[1][1], 5.0)",
        "",
        "    def test_test_file_penalty_can_be_disabled(self):",
        "        from cortical.query import boost_definition_documents",
        "",
        "        doc_results = [",
        "            (\"tests/test_module.py\", 10.0),",
        "            (\"src/module.py\", 10.0),",
        "        ]",
        "",
        "        documents = {",
        "            \"tests/test_module.py\": \"def my_func(): pass\",",
        "            \"src/module.py\": \"def my_func(): return 42\",",
        "        }",
        "",
        "        boosted = boost_definition_documents(",
        "            doc_results,",
        "            \"def my_func\",",
        "            documents,",
        "            boost_factor=2.0,",
        "        )",
        "",
        "        # Both should get the same boost when penalty is disabled",
        "        scores = {doc_id: score for doc_id, score in boosted}",
        "        # Source file gets 10.0 * 2.0 = 20.0",
        "        self.assertEqual(scores[\"src/module.py\"], 20.0)",
        "        self.assertEqual(scores[\"tests/test_module.py\"], 10.0)",
        "",
        "    def test_non_definition_query_unchanged(self):",
        "        \"\"\"Test that non-definition queries are not affected.\"\"\"",
        "        from cortical.query import boost_definition_documents",
        "",
        "        doc_results = [",
        "            (\"tests/test_query.py\", 10.0),"
      ],
      "change_type": "modify"
    },
    {
      "file": "tests/test_query.py",
      "function": "class TestBoostDefinitionDocumentsTestFilePenalty(unittest.TestCase):",
      "start_line": 1955,
      "lines_added": [
        "            test_with_definition_penalty=0.5,",
        "            test_without_definition_penalty=0.7"
      ],
      "lines_removed": [
        "            test_file_boost_factor=0.5,",
        "            test_file_penalty=0.7"
      ],
      "context_before": [
        "        documents = {",
        "            \"tests/test_processor.py\": \"from analysis import compute_pagerank; result = compute_pagerank()\",",
        "            \"cortical/analysis.py\": \"def compute_pagerank(layers, damping=0.85):\\n    return pagerank_impl()\",",
        "        }",
        "",
        "        boosted = boost_definition_documents(",
        "            doc_results,",
        "            \"def compute_pagerank\",",
        "            documents,",
        "            boost_factor=2.0,"
      ],
      "context_after": [
        "        )",
        "",
        "        # Source file with definition should now rank first",
        "        # Source file gets 2.0x: 80.0 * 2.0 = 160.0",
        "        # Test file without definition gets 0.7x penalty: 100.0 * 0.7 = 70.0",
        "        self.assertEqual(boosted[0][0], \"cortical/analysis.py\")",
        "        self.assertEqual(boosted[0][1], 160.0)",
        "        self.assertEqual(boosted[1][1], 70.0)"
      ],
      "change_type": "modify"
    }
  ],
  "hour_of_day": 10,
  "day_of_week": "Friday",
  "seconds_since_last_commit": -269704,
  "is_merge": true,
  "is_initial": false,
  "parent_count": 2,
  "session_id": null,
  "related_chats": [],
  "ci_result": null,
  "reverted": false,
  "amended": false
}