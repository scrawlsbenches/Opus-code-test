{
  "hash": "256e8426fe6765310f80a52d24b6e1472ddcc1d9",
  "message": "Add incremental batch mode for full analysis",
  "author": "Claude",
  "timestamp": "2025-12-11 18:53:59 +0000",
  "branch": "claude/multi-index-design-DvifZ",
  "files_changed": [
    "scripts/index_codebase.py"
  ],
  "insertions": 472,
  "deletions": 8,
  "hunks": [
    {
      "file": "scripts/index_codebase.py",
      "function": "semantic search over the codebase using the Cortical Text Processor itself.",
      "start_line": 7,
      "lines_added": [
        "Background Full Analysis (for long-running environments):",
        "Incremental Full Analysis (resumable, for short-lived processes):",
        "    python scripts/index_codebase.py --full-analysis --batch  # Process one batch",
        "    python scripts/index_codebase.py --full-analysis --batch  # Continue (run again)",
        "    python scripts/index_codebase.py --full-analysis --batch --batch-size 10  # Smaller batches",
        "    python scripts/index_codebase.py --full-analysis --batch --status  # Check progress",
        "",
        "can take several minutes. Two modes are available:",
        "",
        "1. Background mode (default): Spawns a background process. Best for environments",
        "   that support long-running processes. Progress file: .index_progress.json",
        "2. Batch mode (--batch): Processes files in batches, saving after each batch.",
        "   Run the command multiple times to complete. Best for environments with",
        "   process timeouts. Progress file: .index_incremental_progress.json"
      ],
      "lines_removed": [
        "Background Full Analysis:",
        "can take several minutes. By default it runs in a background process, allowing",
        "you to continue working. Run the same command again to check progress.",
        "Progress is tracked in .index_progress.json and can be monitored with:",
        "    cat .index_progress.json"
      ],
      "context_before": [
        "",
        "Supports incremental indexing to only re-index changed files.",
        "",
        "Usage:",
        "    python scripts/index_codebase.py [--output corpus_dev.pkl]",
        "    python scripts/index_codebase.py --incremental  # Only index changes",
        "    python scripts/index_codebase.py --status       # Show what would change",
        "    python scripts/index_codebase.py --force        # Force full rebuild",
        "    python scripts/index_codebase.py --log indexer.log  # Log to file",
        ""
      ],
      "context_after": [
        "    python scripts/index_codebase.py --full-analysis  # Start in background",
        "    python scripts/index_codebase.py --full-analysis  # Check progress (run again)",
        "    python scripts/index_codebase.py --full-analysis --foreground  # Run synchronously",
        "",
        "The --full-analysis flag runs semantic PageRank and hybrid connections, which",
        "",
        "\"\"\"",
        "",
        "import argparse",
        "import json",
        "import logging",
        "import os",
        "import platform",
        "import signal",
        "import sys",
        "import threading"
      ],
      "change_type": "modify"
    },
    {
      "file": "scripts/index_codebase.py",
      "function": "def start_background_analysis(",
      "start_line": 757,
      "lines_added": [
        "# =============================================================================",
        "# Incremental Full Analysis (Resumable Batch Mode)",
        "# =============================================================================",
        "",
        "DEFAULT_BATCH_SIZE = 20",
        "",
        "# Analysis phases for incremental mode",
        "PHASE_INDEXING = \"indexing\"",
        "PHASE_FAST_ANALYSIS = \"fast_analysis\"",
        "PHASE_FULL_ANALYSIS = \"full_analysis\"",
        "PHASE_SEMANTIC_RELATIONS = \"semantic_relations\"",
        "PHASE_COMPLETED = \"completed\"",
        "",
        "",
        "@dataclass",
        "class IncrementalProgress:",
        "    \"\"\"Progress state for incremental full-analysis runs.\"\"\"",
        "    status: str = \"not_started\"  # not_started, in_progress, completed",
        "    started_at: Optional[str] = None",
        "    last_updated: Optional[str] = None",
        "    completed_at: Optional[str] = None",
        "",
        "    # File tracking",
        "    total_files: int = 0",
        "    files_indexed: List[str] = field(default_factory=list)",
        "    files_pending: List[str] = field(default_factory=list)",
        "",
        "    # Phase tracking",
        "    current_phase: str = PHASE_INDEXING",
        "    phases_completed: List[str] = field(default_factory=list)",
        "",
        "    # Stats",
        "    batch_count: int = 0",
        "    total_lines: int = 0",
        "    error: Optional[str] = None",
        "    output_path: Optional[str] = None",
        "",
        "    def to_dict(self) -> Dict[str, Any]:",
        "        return {",
        "            'status': self.status,",
        "            'started_at': self.started_at,",
        "            'last_updated': self.last_updated,",
        "            'completed_at': self.completed_at,",
        "            'total_files': self.total_files,",
        "            'files_indexed': self.files_indexed,",
        "            'files_pending': self.files_pending,",
        "            'current_phase': self.current_phase,",
        "            'phases_completed': self.phases_completed,",
        "            'batch_count': self.batch_count,",
        "            'total_lines': self.total_lines,",
        "            'error': self.error,",
        "            'output_path': self.output_path,",
        "        }",
        "",
        "    @classmethod",
        "    def from_dict(cls, data: Dict[str, Any]) -> 'IncrementalProgress':",
        "        return cls(",
        "            status=data.get('status', 'not_started'),",
        "            started_at=data.get('started_at'),",
        "            last_updated=data.get('last_updated'),",
        "            completed_at=data.get('completed_at'),",
        "            total_files=data.get('total_files', 0),",
        "            files_indexed=data.get('files_indexed', []),",
        "            files_pending=data.get('files_pending', []),",
        "            current_phase=data.get('current_phase', PHASE_INDEXING),",
        "            phases_completed=data.get('phases_completed', []),",
        "            batch_count=data.get('batch_count', 0),",
        "            total_lines=data.get('total_lines', 0),",
        "            error=data.get('error'),",
        "            output_path=data.get('output_path'),",
        "        )",
        "",
        "    @property",
        "    def progress_percent(self) -> float:",
        "        \"\"\"Calculate overall progress percentage.\"\"\"",
        "        # Phases: indexing (60%), fast_analysis (10%), full_analysis (20%), semantic (10%)",
        "        phase_weights = {",
        "            PHASE_INDEXING: 0.6,",
        "            PHASE_FAST_ANALYSIS: 0.1,",
        "            PHASE_FULL_ANALYSIS: 0.2,",
        "            PHASE_SEMANTIC_RELATIONS: 0.1,",
        "        }",
        "",
        "        progress = 0.0",
        "",
        "        # Add completed phases",
        "        for phase in self.phases_completed:",
        "            progress += phase_weights.get(phase, 0) * 100",
        "",
        "        # Add current phase progress",
        "        if self.current_phase == PHASE_INDEXING and self.total_files > 0:",
        "            file_progress = len(self.files_indexed) / self.total_files",
        "            progress += phase_weights[PHASE_INDEXING] * 100 * file_progress",
        "",
        "        return min(progress, 100.0)",
        "",
        "",
        "def get_incremental_progress_path(base_path: Path) -> Path:",
        "    \"\"\"Get the incremental progress file path.\"\"\"",
        "    return base_path / '.index_incremental_progress.json'",
        "",
        "",
        "def load_incremental_progress(progress_path: Path) -> Optional[IncrementalProgress]:",
        "    \"\"\"Load incremental progress from file.\"\"\"",
        "    if not progress_path.exists():",
        "        return None",
        "    try:",
        "        with open(progress_path, 'r') as f:",
        "            data = json.load(f)",
        "        return IncrementalProgress.from_dict(data)",
        "    except (json.JSONDecodeError, IOError):",
        "        return None",
        "",
        "",
        "def save_incremental_progress(progress_path: Path, progress: IncrementalProgress) -> None:",
        "    \"\"\"Save incremental progress to file.\"\"\"",
        "    progress.last_updated = datetime.now().isoformat()",
        "    with open(progress_path, 'w') as f:",
        "        json.dump(progress.to_dict(), f, indent=2)",
        "",
        "",
        "def display_incremental_progress(progress: IncrementalProgress, progress_path: Path) -> None:",
        "    \"\"\"Display current incremental progress to the user.\"\"\"",
        "    print(\"\\n\" + \"=\" * 50)",
        "    print(\"INCREMENTAL FULL-ANALYSIS STATUS\")",
        "    print(\"=\" * 50)",
        "",
        "    print(f\"\\nStatus: {progress.status.upper()}\")",
        "    print(f\"Current phase: {progress.current_phase}\")",
        "",
        "    if progress.started_at:",
        "        print(f\"Started: {progress.started_at}\")",
        "    if progress.last_updated:",
        "        print(f\"Last updated: {progress.last_updated}\")",
        "",
        "    # Progress bar",
        "    pct = progress.progress_percent",
        "    bar_width = 40",
        "    filled = int(bar_width * pct / 100)",
        "    bar = \"█\" * filled + \"░\" * (bar_width - filled)",
        "    print(f\"\\nProgress: {pct:.1f}%\")",
        "    print(f\"[{bar}]\")",
        "",
        "    # File stats",
        "    indexed = len(progress.files_indexed)",
        "    pending = len(progress.files_pending)",
        "    total = progress.total_files",
        "    print(f\"\\nFiles: {indexed}/{total} indexed, {pending} pending\")",
        "    print(f\"Batches completed: {progress.batch_count}\")",
        "    print(f\"Total lines: {progress.total_lines:,}\")",
        "",
        "    # Phase status",
        "    if progress.phases_completed:",
        "        print(f\"\\nPhases completed: {', '.join(progress.phases_completed)}\")",
        "",
        "    if progress.status == \"completed\":",
        "        print(f\"\\n✓ Full analysis completed successfully!\")",
        "        if progress.output_path:",
        "            print(f\"Output saved to: {progress.output_path}\")",
        "    elif progress.status == \"in_progress\":",
        "        print(f\"\\nRun the command again to continue processing.\")",
        "",
        "    if progress.error:",
        "        print(f\"\\nLast error: {progress.error}\")",
        "",
        "    print(f\"\\nProgress file: {progress_path}\")",
        "    print(\"\")",
        "",
        "",
        "def run_incremental_full_analysis(",
        "    base_path: Path,",
        "    output_path: Path,",
        "    progress_path: Path,",
        "    batch_size: int,",
        "    tracker: ProgressTracker",
        ") -> bool:",
        "    \"\"\"",
        "    Run one batch of the incremental full analysis.",
        "",
        "    Returns True if there's more work to do, False if complete.",
        "    \"\"\"",
        "    # Load or create progress",
        "    progress = load_incremental_progress(progress_path)",
        "",
        "    if progress is None:",
        "        # First run - discover files and initialize",
        "        tracker.log(\"\\nInitializing incremental full analysis...\")",
        "        tracker.start_phase(\"Discovering files\")",
        "",
        "        python_files = get_python_files(base_path)",
        "        doc_files = get_doc_files(base_path)",
        "        all_files = python_files + doc_files",
        "",
        "        # Create list of doc_ids",
        "        all_doc_ids = [create_doc_id(f, base_path) for f in all_files]",
        "",
        "        progress = IncrementalProgress(",
        "            status=\"in_progress\",",
        "            started_at=datetime.now().isoformat(),",
        "            total_files=len(all_files),",
        "            files_pending=all_doc_ids,",
        "            current_phase=PHASE_INDEXING,",
        "            output_path=str(output_path),",
        "        )",
        "        save_incremental_progress(progress_path, progress)",
        "        tracker.end_phase(\"Discovering files\")",
        "        tracker.log(f\"  Found {len(all_files)} files to index\")",
        "",
        "    # Check if already complete",
        "    if progress.status == \"completed\":",
        "        tracker.log(\"\\n✓ Incremental full analysis already complete!\")",
        "        display_incremental_progress(progress, progress_path)",
        "        return False",
        "",
        "    # Load or create processor",
        "    if output_path.exists() and len(progress.files_indexed) > 0:",
        "        tracker.log(f\"\\nLoading existing corpus ({len(progress.files_indexed)} files indexed)...\")",
        "        try:",
        "            processor = CorticalTextProcessor.load(str(output_path))",
        "        except Exception as e:",
        "            tracker.warn(f\"Could not load corpus: {e}. Starting fresh.\")",
        "            processor = create_code_processor()",
        "            progress.files_indexed = []",
        "            progress.files_pending = [create_doc_id(f, base_path)",
        "                                      for f in get_python_files(base_path) + get_doc_files(base_path)]",
        "    else:",
        "        processor = create_code_processor()",
        "",
        "    # Phase: Indexing files",
        "    if progress.current_phase == PHASE_INDEXING:",
        "        if progress.files_pending:",
        "            # Process next batch",
        "            batch = progress.files_pending[:batch_size]",
        "            tracker.start_phase(f\"Indexing batch {progress.batch_count + 1}\", len(batch))",
        "",
        "            batch_lines = 0",
        "            for i, doc_id in enumerate(batch, 1):",
        "                file_path = base_path / doc_id",
        "                tracker.update_progress(i, doc_id)",
        "",
        "                if file_path.exists():",
        "                    metadata = index_file(processor, file_path, base_path, tracker)",
        "                    if metadata:",
        "                        batch_lines += metadata.get('line_count', 0)",
        "                        progress.files_indexed.append(doc_id)",
        "                else:",
        "                    tracker.warn(f\"File not found: {doc_id}\")",
        "",
        "                # Remove from pending",
        "                if doc_id in progress.files_pending:",
        "                    progress.files_pending.remove(doc_id)",
        "",
        "            progress.batch_count += 1",
        "            progress.total_lines += batch_lines",
        "            tracker.end_phase(f\"Indexing batch {progress.batch_count}\")",
        "            tracker.log(f\"  Indexed {len(batch)} files ({batch_lines:,} lines)\")",
        "",
        "            # Run fast analysis after each batch so corpus is usable",
        "            tracker.start_phase(\"Fast analysis (batch)\")",
        "            processor.propagate_activation(verbose=False)",
        "            processor.compute_importance(verbose=False)",
        "            processor.compute_tfidf(verbose=False)",
        "            tracker.end_phase(\"Fast analysis (batch)\")",
        "",
        "            # Save corpus and progress",
        "            tracker.start_phase(\"Saving checkpoint\")",
        "            processor.save(str(output_path))",
        "            save_incremental_progress(progress_path, progress)",
        "            tracker.end_phase(\"Saving checkpoint\")",
        "",
        "            # Show status",
        "            display_incremental_progress(progress, progress_path)",
        "",
        "            if progress.files_pending:",
        "                tracker.log(f\"\\n→ {len(progress.files_pending)} files remaining. Run again to continue.\")",
        "                return True",
        "",
        "        # All files indexed - move to next phase",
        "        progress.phases_completed.append(PHASE_INDEXING)",
        "        progress.current_phase = PHASE_FAST_ANALYSIS",
        "        save_incremental_progress(progress_path, progress)",
        "        tracker.log(\"\\n✓ All files indexed! Moving to analysis phase...\")",
        "",
        "    # Phase: Fast analysis (full corpus)",
        "    if progress.current_phase == PHASE_FAST_ANALYSIS:",
        "        tracker.start_phase(\"Fast analysis (full corpus)\")",
        "        processor.propagate_activation(verbose=False)",
        "        processor.compute_importance(verbose=False)",
        "        processor.compute_tfidf(verbose=False)",
        "        processor.compute_document_connections(verbose=False)",
        "        tracker.end_phase(\"Fast analysis (full corpus)\")",
        "",
        "        progress.phases_completed.append(PHASE_FAST_ANALYSIS)",
        "        progress.current_phase = PHASE_FULL_ANALYSIS",
        "",
        "        # Save checkpoint",
        "        processor.save(str(output_path))",
        "        save_incremental_progress(progress_path, progress)",
        "",
        "        tracker.log(\"\\n→ Fast analysis complete. Run again for full semantic analysis.\")",
        "        display_incremental_progress(progress, progress_path)",
        "        return True",
        "",
        "    # Phase: Full analysis (expensive)",
        "    if progress.current_phase == PHASE_FULL_ANALYSIS:",
        "        tracker.start_phase(\"Full semantic analysis\")",
        "        tracker.log(\"  This may take a few minutes...\")",
        "",
        "        try:",
        "            processor.compute_all(",
        "                build_concepts=True,",
        "                pagerank_method='semantic',",
        "                connection_strategy='hybrid',",
        "                verbose=False",
        "            )",
        "            tracker.end_phase(\"Full semantic analysis\")",
        "",
        "            progress.phases_completed.append(PHASE_FULL_ANALYSIS)",
        "            progress.current_phase = PHASE_SEMANTIC_RELATIONS",
        "",
        "            # Save checkpoint",
        "            processor.save(str(output_path))",
        "            save_incremental_progress(progress_path, progress)",
        "",
        "            tracker.log(\"\\n→ Full analysis complete. Run again for semantic relations.\")",
        "            display_incremental_progress(progress, progress_path)",
        "            return True",
        "",
        "        except Exception as e:",
        "            progress.error = str(e)",
        "            save_incremental_progress(progress_path, progress)",
        "            tracker.error(f\"Full analysis failed: {e}\")",
        "            tracker.log(\"  You can retry by running the command again.\")",
        "            return True",
        "",
        "    # Phase: Semantic relations",
        "    if progress.current_phase == PHASE_SEMANTIC_RELATIONS:",
        "        tracker.start_phase(\"Extracting semantic relations\")",
        "",
        "        try:",
        "            processor.extract_corpus_semantics(",
        "                use_pattern_extraction=True,",
        "                verbose=False",
        "            )",
        "            tracker.end_phase(\"Extracting semantic relations\")",
        "",
        "            progress.phases_completed.append(PHASE_SEMANTIC_RELATIONS)",
        "            progress.current_phase = PHASE_COMPLETED",
        "            progress.status = \"completed\"",
        "            progress.completed_at = datetime.now().isoformat()",
        "            progress.error = None",
        "",
        "            # Final save",
        "            processor.save(str(output_path))",
        "",
        "            # Save manifest",
        "            manifest_path = get_manifest_path(output_path)",
        "            file_mtimes = {}",
        "            for doc_id in progress.files_indexed:",
        "                file_path = base_path / doc_id",
        "                if file_path.exists():",
        "                    file_mtimes[doc_id] = get_file_mtime(file_path)",
        "",
        "            stats = {",
        "                'documents': len(processor.documents),",
        "                'tokens': processor.layers[0].column_count(),",
        "                'bigrams': processor.layers[1].column_count(),",
        "                'concepts': processor.layers[2].column_count(),",
        "                'semantic_relations': len(processor.semantic_relations),",
        "                'full_analysis': True,",
        "                'incremental': True,",
        "            }",
        "            save_manifest(manifest_path, file_mtimes, str(output_path), stats, tracker)",
        "            save_incremental_progress(progress_path, progress)",
        "",
        "            tracker.log(\"\\n\" + \"=\" * 50)",
        "            tracker.log(\"✓ INCREMENTAL FULL ANALYSIS COMPLETE!\")",
        "            tracker.log(\"=\" * 50)",
        "            tracker.log(f\"\\nCorpus Statistics:\")",
        "            tracker.log(f\"  Documents: {len(processor.documents)}\")",
        "            tracker.log(f\"  Tokens: {processor.layers[0].column_count()}\")",
        "            tracker.log(f\"  Bigrams: {processor.layers[1].column_count()}\")",
        "            tracker.log(f\"  Concepts: {processor.layers[2].column_count()}\")",
        "            tracker.log(f\"  Semantic relations: {len(processor.semantic_relations)}\")",
        "            tracker.log(f\"\\nOutput saved to: {output_path}\")",
        "",
        "            return False",
        "",
        "        except Exception as e:",
        "            progress.error = str(e)",
        "            save_incremental_progress(progress_path, progress)",
        "            tracker.error(f\"Semantic extraction failed: {e}\")",
        "            tracker.log(\"  You can retry by running the command again.\")",
        "            return True",
        "",
        "    return False",
        "",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "    print(f\"Output will be saved to: {output_path}\")",
        "    print(\"\\nThe analysis is running in the background.\")",
        "    print(\"You can safely close this terminal.\")",
        "    print(\"\\nTo check progress, run:\")",
        "    print(f\"  python scripts/index_codebase.py --full-analysis\")",
        "    print(\"\\nOr monitor the progress file directly:\")",
        "    print(f\"  cat {progress_path}\")",
        "    print(\"=\" * 50 + \"\\n\")",
        "",
        ""
      ],
      "context_after": [
        "# =============================================================================",
        "# Manifest Operations",
        "# =============================================================================",
        "",
        "def get_manifest_path(corpus_path: Path) -> Path:",
        "    \"\"\"Get the manifest file path based on corpus path.\"\"\"",
        "    return corpus_path.with_suffix('.manifest.json')",
        "",
        "",
        "def load_manifest("
      ],
      "change_type": "add"
    },
    {
      "file": "scripts/index_codebase.py",
      "function": "def main():",
      "start_line": 1503,
      "lines_added": [
        "Background Full Analysis (for long-running environments):",
        "",
        "Incremental Full Analysis (resumable, for short-lived processes):",
        "  python scripts/index_codebase.py --full-analysis --batch      # Process one batch",
        "  python scripts/index_codebase.py --full-analysis --batch      # Continue (run again)",
        "  python scripts/index_codebase.py --full-analysis --batch --batch-size 10  # Smaller batches",
        "  python scripts/index_codebase.py --full-analysis --batch --status  # Check progress"
      ],
      "lines_removed": [
        "Background Full Analysis:",
        "  python scripts/index_codebase.py --full-analysis --force      # Restart even if completed"
      ],
      "context_before": [
        "        formatter_class=argparse.RawDescriptionHelpFormatter,",
        "        epilog=\"\"\"",
        "Examples:",
        "  python scripts/index_codebase.py                   # Full rebuild (fast mode)",
        "  python scripts/index_codebase.py --incremental    # Update changed files only",
        "  python scripts/index_codebase.py --status         # Show what would change",
        "  python scripts/index_codebase.py --force          # Force full rebuild",
        "  python scripts/index_codebase.py --log index.log  # Log to file",
        "  python scripts/index_codebase.py --timeout 60     # Timeout after 60s",
        ""
      ],
      "context_after": [
        "  python scripts/index_codebase.py --full-analysis              # Start in background",
        "  python scripts/index_codebase.py --full-analysis              # Check progress",
        "  python scripts/index_codebase.py --full-analysis --foreground # Run synchronously",
        "        \"\"\"",
        "    )",
        "    parser.add_argument('--output', '-o', default='corpus_dev.pkl',",
        "                        help='Output file path (default: corpus_dev.pkl)')",
        "    parser.add_argument('--incremental', '-i', action='store_true',",
        "                        help='Only index changed files (requires existing corpus)')",
        "    parser.add_argument('--force', '-f', action='store_true',",
        "                        help='Force full rebuild even if manifest exists')",
        "    parser.add_argument('--status', '-s', action='store_true',",
        "                        help='Show what would change without indexing')"
      ],
      "change_type": "modify"
    },
    {
      "file": "scripts/index_codebase.py",
      "function": "Background Full Analysis:",
      "start_line": 1533,
      "lines_added": [
        "    # Incremental batch mode options",
        "    parser.add_argument('--batch', action='store_true',",
        "                        help='Use incremental batch mode (resumable, for short-lived processes)')",
        "    parser.add_argument('--batch-size', type=int, default=DEFAULT_BATCH_SIZE,",
        "                        help=f'Number of files per batch (default: {DEFAULT_BATCH_SIZE})')",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "                        help='Log file path (writes detailed log)')",
        "    parser.add_argument('--timeout', '-t', type=int, default=DEFAULT_TIMEOUT,",
        "                        help=f'Timeout in seconds (0=none, default={DEFAULT_TIMEOUT})')",
        "    parser.add_argument('--full-analysis', action='store_true',",
        "                        help='Use full semantic analysis (runs in background by default)')",
        "    parser.add_argument('--foreground', action='store_true',",
        "                        help='Run full analysis in foreground (blocking) instead of background')",
        "    parser.add_argument('--progress-file', type=str, default=None,",
        "                        help='Custom path for progress file (default: .index_progress.json)')",
        ""
      ],
      "context_after": [
        "    # Chunk-based indexing options",
        "    parser.add_argument('--use-chunks', action='store_true',",
        "                        help='Use git-compatible chunk-based indexing')",
        "    parser.add_argument('--chunks-dir', default='corpus_chunks',",
        "                        help='Directory for chunk files (default: corpus_chunks)')",
        "    parser.add_argument('--compact', action='store_true',",
        "                        help='Compact old chunks into a single file')",
        "    parser.add_argument('--compact-before', type=str, default=None,",
        "                        help='Only compact chunks before this date (YYYY-MM-DD)')",
        "    parser.add_argument('--compact-keep', type=int, default=0,"
      ],
      "change_type": "add"
    },
    {
      "file": "scripts/index_codebase.py",
      "function": "Background Full Analysis:",
      "start_line": 1564,
      "lines_added": [
        "    # Handle incremental batch mode for full-analysis",
        "    if args.full_analysis and args.batch:",
        "        incremental_progress_path = get_incremental_progress_path(base_path)",
        "",
        "        # Status check only",
        "        if args.status:",
        "            existing = load_incremental_progress(incremental_progress_path)",
        "            if existing:",
        "                display_incremental_progress(existing, incremental_progress_path)",
        "            else:",
        "                print(\"\\nNo incremental full-analysis in progress.\")",
        "                print(f\"Run with --full-analysis --batch to start.\\n\")",
        "            return",
        "",
        "        # Force restart",
        "        if args.force:",
        "            if incremental_progress_path.exists():",
        "                incremental_progress_path.unlink()",
        "                print(\"Cleared previous incremental progress.\\n\")",
        "",
        "        # Initialize tracker for batch mode",
        "        tracker = ProgressTracker(",
        "            log_file=log_path,",
        "            verbose=args.verbose,",
        "            quiet=args.quiet",
        "        )",
        "",
        "        tracker.log(\"Cortical Text Processor - Incremental Full Analysis\")",
        "        tracker.log(\"=\" * 50)",
        "        tracker.log(f\"Batch size: {args.batch_size} files\")",
        "",
        "        # Run one batch",
        "        more_work = run_incremental_full_analysis(",
        "            base_path=base_path,",
        "            output_path=output_path,",
        "            progress_path=incremental_progress_path,",
        "            batch_size=args.batch_size,",
        "            tracker=tracker",
        "        )",
        "",
        "        tracker.print_summary()",
        "",
        "        if more_work:",
        "            print(\"\\nRun the command again to continue processing.\")",
        "        return",
        "",
        "    if args.full_analysis and not args.foreground and not args.batch:"
      ],
      "lines_removed": [
        "    if args.full_analysis and not args.foreground:"
      ],
      "context_before": [
        "        if not progress_path.is_absolute():",
        "            progress_path = base_path / args.progress_file",
        "    else:",
        "        progress_path = get_progress_file_path(base_path)",
        "",
        "    # Set up log file path",
        "    log_path = None",
        "    if args.log:",
        "        log_path = args.log if os.path.isabs(args.log) else str(base_path / args.log)",
        ""
      ],
      "context_after": [
        "    # Handle background full-analysis mode",
        "        # Check for existing progress",
        "        existing_progress = load_background_progress(progress_path)",
        "",
        "        if existing_progress:",
        "            if existing_progress.status == \"running\":",
        "                # Check if the process is actually alive",
        "                if existing_progress.pid and is_process_alive(existing_progress.pid):",
        "                    # Show progress and exit",
        "                    display_progress(existing_progress, progress_path)",
        "                    return"
      ],
      "change_type": "modify"
    }
  ],
  "hour_of_day": 18,
  "day_of_week": "Thursday",
  "seconds_since_last_commit": -327049,
  "is_merge": false,
  "is_initial": false,
  "parent_count": 1,
  "session_id": null,
  "related_chats": [],
  "ci_result": null,
  "reverted": false,
  "amended": false
}