{
  "hash": "f6436c37d6365d198279d839c384a94cb7a5a46a",
  "message": "Add intelligence documentation (Tasks #53-55)",
  "author": "Claude",
  "timestamp": "2025-12-10 15:34:35 +0000",
  "branch": "claude/multi-index-design-DvifZ",
  "files_changed": [
    "TASK_LIST.md",
    "docs/algorithms.md",
    "docs/architecture.md",
    "docs/glossary.md",
    "scripts/index_codebase.py"
  ],
  "insertions": 1148,
  "deletions": 3,
  "hunks": [
    {
      "file": "TASK_LIST.md",
      "function": "Each query recomputes expansions and scores against all documents. For interacti",
      "start_line": 1801,
      "lines_added": [
        "## Intelligence Documentation",
        "",
        "The following tasks create self-describing documentation that improves the system's ability to understand itself when indexed. This creates a feedback loop: better documentation → better semantic search → better AI understanding of the codebase.",
        "",
        "---",
        "",
        "### 53. Create Algorithm Intelligence Documentation",
        "",
        "**File:** New `docs/algorithms.md`",
        "**Status:** [ ] In Progress",
        "**Priority:** High",
        "",
        "**Goal:**",
        "Document the core IR algorithms in a way that helps semantic search understand what each algorithm does, when to use it, and how components relate.",
        "",
        "**Content:**",
        "- PageRank explanation with use cases",
        "- TF-IDF calculation and per-document vs global variants",
        "- Label propagation for concept clustering",
        "- Co-occurrence counting (\"Hebbian learning\" metaphor)",
        "- Relation extraction patterns",
        "- Query expansion strategies",
        "",
        "---",
        "",
        "### 54. Create Architecture Intelligence Documentation",
        "",
        "**File:** New `docs/architecture.md`",
        "**Status:** [ ] Not Started",
        "**Priority:** High",
        "",
        "**Goal:**",
        "Document the 4-layer architecture and data flow in searchable prose that helps answer \"where is X handled?\" and \"how does X work?\" queries.",
        "",
        "**Content:**",
        "- Layer 0 (Tokens): Word-level processing",
        "- Layer 1 (Bigrams): Phrase patterns",
        "- Layer 2 (Concepts): Topic clusters",
        "- Layer 3 (Documents): Full document representations",
        "- Cross-layer connections (feedforward/feedback)",
        "- Minicolumn data structure",
        "",
        "---",
        "",
        "### 55. Create Pattern Glossary",
        "",
        "**File:** New `docs/glossary.md`",
        "**Status:** [ ] Not Started",
        "**Priority:** Medium",
        "",
        "**Goal:**",
        "Define terminology used throughout the codebase so searches for concepts find relevant definitions.",
        "",
        "**Terms:**",
        "- Minicolumn, Edge, HierarchicalLayer",
        "- Lateral connections, typed connections",
        "- Feedforward/feedback connections",
        "- PageRank, TF-IDF, damping factor",
        "- Semantic relations (IsA, PartOf, etc.)",
        "- Query expansion, spreading activation",
        "",
        "---",
        "",
        "### 56. Create Usage Patterns Documentation",
        "",
        "**File:** New `docs/patterns.md`",
        "**Status:** [ ] Not Started",
        "**Priority:** Medium",
        "",
        "**Goal:**",
        "Document common usage patterns and code examples that help answer \"how do I...\" queries.",
        "",
        "**Patterns:**",
        "- Basic document processing workflow",
        "- RAG retrieval with passages",
        "- Code search with intent parsing",
        "- Fingerprint comparison for similarity",
        "- Batch operations for performance",
        "- Incremental updates",
        "",
        "---",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "4. Added processor wrappers: `fast_find_documents()`, `build_search_index()`, `search_with_index()`",
        "5. Added `--fast` flag to search_codebase.py script",
        "6. Added 20 tests in `tests/test_query_optimization.py`",
        "",
        "**Performance:**",
        "- `fast_find_documents()`: ~2-3x faster than full search",
        "- `search_with_index()`: Fastest when index is cached",
        "",
        "---",
        ""
      ],
      "context_after": [
        "*Updated 2025-12-10*"
      ],
      "change_type": "add"
    },
    {
      "file": "docs/algorithms.md",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "# Core Algorithms",
        "",
        "This document describes the information retrieval algorithms implemented in the Cortical Text Processor. These algorithms work together to build a semantic understanding of text corpora.",
        "",
        "## Overview",
        "",
        "The system uses standard IR algorithms with a hierarchical, layered architecture:",
        "",
        "| Algorithm | Purpose | Primary File |",
        "|-----------|---------|--------------|",
        "| PageRank | Importance scoring | `analysis.py:22-95` |",
        "| TF-IDF | Term weighting | `analysis.py:394-433` |",
        "| Label Propagation | Concept clustering | `analysis.py:502-636` |",
        "| Query Expansion | Semantic search | `query.py:55-176` |",
        "| Relation Extraction | Knowledge building | `semantics.py:109-186` |",
        "",
        "---",
        "",
        "## PageRank - Importance Scoring",
        "",
        "PageRank measures term importance based on network structure. Terms connected to other important terms receive higher scores.",
        "",
        "### Standard PageRank",
        "",
        "**Location:** `analysis.py:22-95`",
        "",
        "**Algorithm:**",
        "1. Initialize each term with equal PageRank: `1.0 / n`",
        "2. Iterate until convergence:",
        "   ```",
        "   PR(i) = (1 - damping) / n + damping * Σ(PR(j) * weight(j→i) / outgoing(j))",
        "   ```",
        "3. Stop when max change < tolerance (1e-6) or after 20 iterations",
        "",
        "**Parameters:**",
        "- `damping`: 0.85 (probability of following links vs random jump)",
        "- `iterations`: 20 maximum",
        "- `tolerance`: 1e-6 convergence threshold",
        "",
        "**Use Case:** Identify the most important terms in the corpus based on how they connect to other important terms.",
        "",
        "### Semantic PageRank",
        "",
        "**Location:** `analysis.py:113-235`",
        "",
        "Enhances PageRank by weighting edges according to semantic relation types.",
        "",
        "**Relation Weights:**",
        "```",
        "IsA: 1.5           (hypernym relationships are strong)",
        "SimilarTo: 1.4     (similarity is important)",
        "PartOf: 1.3        (part-whole relationships)",
        "HasProperty: 1.2   (property associations)",
        "DerivedFrom: 1.2   (morphological derivation)",
        "Causes: 1.1        (causal relationships)",
        "RelatedTo: 1.0     (general association - baseline)",
        "UsedFor: 1.0       (functional relationships)",
        "CoOccurs: 0.8      (basic co-occurrence - lower weight)",
        "Antonym: 0.3       (opposing concepts - penalized)",
        "```",
        "",
        "**Use Case:** When semantic relations have been extracted, use semantic PageRank for importance that respects relationship types.",
        "",
        "### Hierarchical PageRank",
        "",
        "**Location:** `analysis.py:238-391`",
        "",
        "Propagates importance across all 4 layers bidirectionally:",
        "- Upward: tokens → bigrams → concepts → documents",
        "- Downward: documents → concepts → bigrams → tokens",
        "",
        "**Algorithm:**",
        "1. Compute local PageRank within each layer",
        "2. Propagate scores upward via `feedback_connections`",
        "3. Propagate scores downward via `feedforward_connections`",
        "4. Normalize within each layer",
        "5. Repeat until convergence",
        "",
        "**Parameters:**",
        "- `layer_iterations`: 10 (within-layer iterations)",
        "- `global_iterations`: 5 (cross-layer iterations)",
        "- `cross_layer_damping`: 0.7 (damping at layer boundaries)",
        "",
        "**Use Case:** When you want importance to flow through the full hierarchy, enabling documents to boost their constituent terms and vice versa.",
        "",
        "---",
        "",
        "## TF-IDF - Term Weighting",
        "",
        "**Location:** `analysis.py:394-433`",
        "",
        "TF-IDF (Term Frequency - Inverse Document Frequency) measures how distinctive a term is to the corpus.",
        "",
        "**Formula:**",
        "```",
        "TF-IDF = TF × IDF",
        "TF = log(1 + occurrence_count)",
        "IDF = log(num_documents / document_frequency)",
        "```",
        "",
        "**Two Variants:**",
        "",
        "1. **Global TF-IDF** (`col.tfidf`):",
        "   - Uses total corpus occurrence count",
        "   - Good for corpus-wide term importance",
        "",
        "2. **Per-Document TF-IDF** (`col.tfidf_per_doc[doc_id]`):",
        "   - Uses occurrence count within specific document",
        "   - Better for document-specific relevance scoring",
        "",
        "**Important:** Always use `tfidf_per_doc[doc_id]` for per-document scoring. The global `tfidf` field uses total occurrence count across all documents.",
        "",
        "---",
        "",
        "## Label Propagation - Concept Clustering",
        "",
        "**Location:** `analysis.py:502-636`",
        "",
        "Label propagation is a semi-supervised community detection algorithm that clusters tokens into semantic concepts.",
        "",
        "**Algorithm:**",
        "1. Each token starts with a unique label",
        "2. Iterate up to 20 times:",
        "   - Count neighbor labels weighted by connection strength",
        "   - Adopt most common label if it exceeds change threshold",
        "3. Group tokens by final label into clusters",
        "4. Filter clusters smaller than `min_cluster_size`",
        "",
        "**Parameters:**",
        "- `cluster_strictness` (0.0-1.0): Higher = more separate clusters",
        "- `bridge_weight` (0.0-1.0): Synthetic connections between documents",
        "- `min_cluster_size`: Minimum tokens per cluster (default 3)",
        "",
        "**Concept Creation:**",
        "After clustering, each cluster becomes a concept in Layer 2:",
        "- Named after top 3 members by PageRank: `\"neural/networks/learning\"`",
        "- Connected bidirectionally to member tokens",
        "- Aggregates member properties (documents, activation, pagerank)",
        "",
        "---",
        "",
        "## Query Expansion",
        "",
        "### Basic Expansion",
        "",
        "**Location:** `query.py:55-176`",
        "",
        "Expands query terms to find semantically related words.",
        "",
        "**Three Expansion Methods:**",
        "",
        "1. **Lateral Connections** - Direct word associations from co-occurrence",
        "   - Score: `connection_weight × neighbor_pagerank × 0.6`",
        "",
        "2. **Concept Clusters** - Words from same semantic category",
        "   - Score: `concept_pagerank × member_pagerank × 0.4`",
        "",
        "3. **Code Concepts** - Programming synonyms (optional)",
        "   - Example: \"get\" → \"fetch\", \"load\", \"retrieve\"",
        "   - Score: `0.6`",
        "",
        "### Multi-Hop Expansion",
        "",
        "**Location:** `query.py:407-531`",
        "",
        "Finds related terms through transitive relation chains.",
        "",
        "**Example Chains:**",
        "- `\"dog\" → IsA → \"animal\" → HasProperty → \"living\"`",
        "- `\"car\" → PartOf → \"engine\" → UsedFor → \"transportation\"`",
        "",
        "**Chain Validity Scoring:**",
        "Not all relation chains are equally valid:",
        "```",
        "(IsA, IsA): 1.0           - Fully transitive hypernymy",
        "(IsA, HasProperty): 0.9   - Property inheritance",
        "(RelatedTo, RelatedTo): 0.6 - Weak association",
        "(Antonym, Antonym): 0.3   - Double negation, unreliable",
        "```",
        "",
        "**Parameters:**",
        "- `max_hops`: Maximum chain depth (default 2)",
        "- `decay_factor`: Weight decay per hop (default 0.5)",
        "- `min_path_score`: Minimum chain validity (default 0.2)",
        "",
        "### Intent-Based Query Parsing",
        "",
        "**Location:** `query.py:179-284`",
        "",
        "Parses natural language queries to extract intent.",
        "",
        "**Intent Types:**",
        "- `\"where\"` → `location` (find file/function location)",
        "- `\"how\"` → `implementation` (find implementation details)",
        "- `\"what\"` → `definition` (find definitions)",
        "- `\"why\"` → `rationale` (find explanations/comments)",
        "- `\"when\"` → `lifecycle` (find lifecycle events)",
        "",
        "**Example:**",
        "```",
        "Input: \"where do we handle authentication?\"",
        "Output: ParsedIntent(",
        "    action='handle',",
        "    subject='authentication',",
        "    intent='location',",
        "    expanded_terms=['handle', 'manage', 'authentication', 'auth', ...]",
        ")",
        "```",
        "",
        "---",
        "",
        "## Relation Extraction",
        "",
        "### Pattern-Based Extraction",
        "",
        "**Location:** `semantics.py:109-186`",
        "",
        "Extracts semantic relations from text using regex patterns.",
        "",
        "**Relation Types:**",
        "- **IsA**: \"dogs are animals\", \"a kind of\"",
        "- **HasA**: \"dogs have ears\", \"contains\"",
        "- **PartOf**: \"wheel is part of car\"",
        "- **UsedFor**: \"hammer is used for nailing\"",
        "- **Causes**: \"rain causes floods\"",
        "- **CapableOf**: \"dog can bark\"",
        "- **AtLocation**: \"found in\", \"lives in\"",
        "- **HasProperty**: \"dog is loyal\"",
        "- **Antonym**: \"big vs small\", \"opposite of\"",
        "- **DerivedFrom**: \"comes from\"",
        "",
        "Each pattern has a confidence score (0.5-0.95) based on how reliable it is.",
        "",
        "### Co-occurrence Relations",
        "",
        "**Location:** `semantics.py:251-292`",
        "",
        "Extracts relations from statistical co-occurrence.",
        "",
        "**Algorithm:**",
        "1. Count term pairs within sliding window (5 tokens)",
        "2. Compute PMI (Pointwise Mutual Information):",
        "   ```",
        "   PMI = log((co-occurrence + 1) / (expected + 1))",
        "   expected = (count_term1 × count_term2) / corpus_size",
        "   ```",
        "3. Create `CoOccurs` relations for high-PMI pairs",
        "",
        "### Similarity Relations",
        "",
        "**Location:** `semantics.py:294-363`",
        "",
        "Finds similar terms based on context vectors.",
        "",
        "**Algorithm:**",
        "1. Build context vectors: what words appear near each term",
        "2. Compute cosine similarity between context vectors",
        "3. Create `SimilarTo` relations for pairs with similarity > 0.3",
        "",
        "---",
        "",
        "## Retrofitting",
        "",
        "**Location:** `semantics.py:378-476`",
        "",
        "Adjusts connection weights to align with semantic relations.",
        "",
        "**Algorithm:**",
        "1. Store original lateral connection weights",
        "2. Build semantic neighbor lookup",
        "3. Iterate 10 times:",
        "   - Blend original and semantic weights:",
        "     ```",
        "     new_weight = alpha × original + (1 - alpha) × semantic",
        "     ```",
        "   - Add new semantic connections that didn't exist",
        "",
        "**Parameter:**",
        "- `alpha`: 0.3 (mostly semantic, some original)",
        "",
        "**Use Case:** If \"dog\" and \"cat\" aren't connected by co-occurrence but both have \"IsA animal\" relation, retrofitting strengthens their connection.",
        "",
        "---",
        "",
        "## Performance Optimizations",
        "",
        "| Optimization | Location | Benefit |",
        "|--------------|----------|---------|",
        "| O(1) ID lookups | `layer.get_by_id()` | Avoid O(n) iteration |",
        "| Query cache | `expand_query_cached()` | Skip repeated expansions |",
        "| Pre-computed lookups | `precompute_term_cols()` | Faster chunk scoring |",
        "| Fast search | `fast_find_documents()` | 2-3x faster via candidate filtering |",
        "| Inverted index | `build_document_index()` | Fastest repeated queries |",
        "",
        "---",
        "",
        "## Quick Reference",
        "",
        "**When to use which algorithm:**",
        "",
        "| Goal | Algorithm | Method |",
        "|------|-----------|--------|",
        "| Find important terms | PageRank | `compute_pagerank()` |",
        "| Respect semantic relations | Semantic PageRank | `compute_semantic_importance()` |",
        "| Cross-layer importance | Hierarchical PageRank | `compute_hierarchical_importance()` |",
        "| Term distinctiveness | TF-IDF | `compute_tfidf()` |",
        "| Group related terms | Label Propagation | `build_concept_clusters()` |",
        "| Expand search queries | Query Expansion | `expand_query()` |",
        "| Find distant relations | Multi-hop Expansion | `expand_query_multihop()` |",
        "| Extract knowledge | Relation Extraction | `extract_corpus_semantics()` |",
        "| Improve connections | Retrofitting | `retrofit_connections()` |"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "docs/architecture.md",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "# System Architecture",
        "",
        "This document describes the 4-layer hierarchical architecture of the Cortical Text Processor. The design is inspired by visual cortex organization, processing text at increasing levels of abstraction.",
        "",
        "## Layer Overview",
        "",
        "```",
        "Layer 3 (DOCUMENTS)  ← Full documents        [IT analogy: objects]",
        "    ↑↓",
        "Layer 2 (CONCEPTS)   ← Semantic clusters     [V4 analogy: shapes]",
        "    ↑↓",
        "Layer 1 (BIGRAMS)    ← Word pairs            [V2 analogy: patterns]",
        "    ↑↓",
        "Layer 0 (TOKENS)     ← Individual words      [V1 analogy: edges]",
        "```",
        "",
        "Information flows both upward (abstraction) and downward (grounding) through feedforward and feedback connections.",
        "",
        "---",
        "",
        "## Core Data Structures",
        "",
        "### CorticalLayer Enum",
        "",
        "**Location:** `layers.py:21-56`",
        "",
        "```python",
        "class CorticalLayer(Enum):",
        "    TOKENS = 0      # Individual words",
        "    BIGRAMS = 1     # Word pairs",
        "    CONCEPTS = 2    # Semantic clusters",
        "    DOCUMENTS = 3   # Full documents",
        "```",
        "",
        "### HierarchicalLayer",
        "",
        "**Location:** `layers.py:59-273`",
        "",
        "Container for minicolumns at each layer:",
        "",
        "```python",
        "class HierarchicalLayer:",
        "    layer_type: CorticalLayer",
        "    minicolumns: Dict[str, Minicolumn]  # content → minicolumn",
        "    _id_index: Dict[str, str]           # id → content (O(1) lookup)",
        "```",
        "",
        "**Key Methods:**",
        "- `get_or_create_minicolumn(content)` - Create or retrieve minicolumn",
        "- `get_minicolumn(content)` - Retrieve by content",
        "- `get_by_id(col_id)` - O(1) lookup by ID (critical for performance)",
        "- `column_count()` - Number of minicolumns",
        "",
        "### Minicolumn",
        "",
        "**Location:** `minicolumn.py:56-357`",
        "",
        "The fundamental unit of representation:",
        "",
        "```python",
        "class Minicolumn:",
        "    # Identity",
        "    id: str              # \"L0_neural\", \"L1_neural networks\"",
        "    content: str         # \"neural\", \"neural networks\"",
        "    layer: int           # 0, 1, 2, or 3",
        "",
        "    # Statistics",
        "    activation: float           # Neural activation level",
        "    occurrence_count: int       # Total occurrences in corpus",
        "    pagerank: float            # Importance score",
        "    tfidf: float               # Global TF-IDF weight",
        "    tfidf_per_doc: Dict[str, float]  # Per-document TF-IDF",
        "",
        "    # Document association",
        "    document_ids: Set[str]     # Which documents contain this",
        "    doc_occurrence_counts: Dict[str, int]  # Occurrences per document",
        "",
        "    # Connections (see Connection Types below)",
        "    lateral_connections: Dict[str, float]",
        "    typed_connections: Dict[str, Edge]",
        "    feedforward_connections: Dict[str, float]",
        "    feedback_connections: Dict[str, float]",
        "",
        "    # Clustering",
        "    cluster_id: Optional[int]  # For Layer 0 tokens",
        "```",
        "",
        "**ID Pattern:** `f\"L{layer}_{content}\"`",
        "- Token: `\"L0_neural\"`",
        "- Bigram: `\"L1_neural networks\"`",
        "- Concept: `\"L2_neural/networks/learning\"`",
        "- Document: `\"L3_doc_001\"`",
        "",
        "### Edge",
        "",
        "**Location:** `minicolumn.py:16-53`",
        "",
        "Typed connection with metadata (ConceptNet-style):",
        "",
        "```python",
        "@dataclass",
        "class Edge:",
        "    target_id: str                      # \"L0_network\"",
        "    weight: float = 1.0                 # Connection strength",
        "    relation_type: str = 'co_occurrence'  # 'IsA', 'PartOf', etc.",
        "    confidence: float = 1.0             # [0.0, 1.0]",
        "    source: str = 'corpus'              # 'corpus', 'semantic', 'inferred'",
        "```",
        "",
        "---",
        "",
        "## Connection Types",
        "",
        "### 1. Lateral Connections",
        "",
        "**Within-layer** associations from co-occurrence.",
        "",
        "```python",
        "minicolumn.lateral_connections: Dict[str, float]",
        "# {\"L0_networks\": 0.8, \"L0_learning\": 0.5}",
        "```",
        "",
        "- **Layer 0:** Tokens appearing near each other in text",
        "- **Layer 1:** Bigrams sharing components or co-occurring",
        "- **Layer 2:** Concepts with overlapping documents or semantics",
        "- **Layer 3:** Documents sharing vocabulary",
        "",
        "### 2. Typed Connections",
        "",
        "**Within-layer** with semantic metadata.",
        "",
        "```python",
        "minicolumn.typed_connections: Dict[str, Edge]",
        "# {\"L0_animal\": Edge(weight=0.9, relation_type='IsA', confidence=0.95)}",
        "```",
        "",
        "Used for ConceptNet-style reasoning with relation types.",
        "",
        "### 3. Feedforward Connections",
        "",
        "**Downward** links to components (higher → lower layer).",
        "",
        "```python",
        "minicolumn.feedforward_connections: Dict[str, float]",
        "```",
        "",
        "- Bigram → component tokens: `\"neural networks\" → [\"neural\", \"networks\"]`",
        "- Concept → member tokens: `\"neural/networks/learning\" → [member tokens]`",
        "- Document → contained tokens: `\"doc1\" → [all tokens in doc1]`",
        "",
        "### 4. Feedback Connections",
        "",
        "**Upward** links to containers (lower → higher layer).",
        "",
        "```python",
        "minicolumn.feedback_connections: Dict[str, float]",
        "```",
        "",
        "- Token → containing bigrams: `\"neural\" → [\"neural networks\", \"neural processing\"]`",
        "- Token → containing concepts: `\"neural\" → [\"neural/networks/learning\"]`",
        "- Token → containing documents: `\"neural\" → [\"doc1\", \"doc2\"]`",
        "",
        "---",
        "",
        "## Data Flow",
        "",
        "### Document Processing",
        "",
        "**Location:** `processor.py:54-137`",
        "",
        "When a document is processed:",
        "",
        "```",
        "INPUT: \"Neural networks process data.\"",
        "",
        "1. TOKENIZATION",
        "   → [\"neural\", \"networks\", \"process\", \"data\"]",
        "   → Create Layer 0 minicolumns",
        "",
        "2. DOCUMENT-TOKEN CONNECTIONS",
        "   → doc.feedforward_connections[\"L0_neural\"] = 1.0",
        "   → token.feedback_connections[\"L3_doc1\"] = 1.0",
        "",
        "3. LATERAL TOKEN CONNECTIONS",
        "   → \"neural\" ↔ \"networks\" (co-occurrence)",
        "   → \"networks\" ↔ \"process\" (co-occurrence)",
        "",
        "4. BIGRAM EXTRACTION",
        "   → [\"neural networks\", \"networks process\", \"process data\"]",
        "   → Create Layer 1 minicolumns",
        "",
        "5. BIGRAM-TOKEN CONNECTIONS",
        "   → bigram.feedforward_connections[\"L0_neural\"] = 1.0",
        "   → token.feedback_connections[\"L1_neural networks\"] = 1.0",
        "```",
        "",
        "**Important:** Bigrams use SPACE separators: `\"neural networks\"`, not `\"neural_networks\"`.",
        "",
        "### Network Computation",
        "",
        "**Location:** `processor.py:452-596` (`compute_all()`)",
        "",
        "After processing documents, compute the full network:",
        "",
        "```",
        "1. ACTIVATION PROPAGATION",
        "   → Spread activation through connections",
        "   → Simulates information flow",
        "",
        "2. PAGERANK",
        "   → Compute importance for Layer 0 and Layer 1",
        "   → Options: standard, semantic, hierarchical",
        "",
        "3. TF-IDF",
        "   → Compute term weights for Layer 0",
        "   → Both global and per-document variants",
        "",
        "4. DOCUMENT CONNECTIONS",
        "   → Connect Layer 3 documents by shared vocabulary",
        "   → Weight by sum of shared term TF-IDF scores",
        "",
        "5. BIGRAM CONNECTIONS",
        "   → Connect Layer 1 bigrams by:",
        "     - Shared components (\"neural networks\" ↔ \"neural processing\")",
        "     - Chain patterns (\"machine learning\" ↔ \"learning algorithms\")",
        "     - Document co-occurrence",
        "",
        "6. CONCEPT CLUSTERING",
        "   → Run label propagation on Layer 0",
        "   → Create Layer 2 concepts from clusters",
        "   → Connect concepts to member tokens",
        "",
        "7. CONCEPT CONNECTIONS",
        "   → Connect Layer 2 concepts by:",
        "     - Document overlap (Jaccard similarity)",
        "     - Semantic relations between members",
        "     - Embedding similarity (optional)",
        "```",
        "",
        "### Query Flow",
        "",
        "**Location:** `query.py`",
        "",
        "When a query is executed:",
        "",
        "```",
        "INPUT: \"neural networks\"",
        "",
        "1. TOKENIZE QUERY",
        "   → [\"neural\", \"networks\"]",
        "",
        "2. EXPAND QUERY",
        "   → Add related terms from lateral connections",
        "   → Add terms from concept clusters",
        "   → Result: {\"neural\": 1.0, \"networks\": 1.0, \"learning\": 0.5, ...}",
        "",
        "3. SCORE DOCUMENTS",
        "   → For each document, sum term scores:",
        "     score = Σ(term_weight × token.tfidf_per_doc[doc_id])",
        "",
        "4. RANK AND RETURN",
        "   → Sort documents by score",
        "   → Return top_n results",
        "```",
        "",
        "---",
        "",
        "## Layer Details",
        "",
        "### Layer 0: Tokens",
        "",
        "**Purpose:** Represent individual words after tokenization.",
        "",
        "**Content:** Lowercase stemmed words (stop words removed).",
        "",
        "**Connections:**",
        "- Lateral: Co-occurring tokens within window",
        "- Feedback: Containing bigrams, concepts, documents",
        "- Feedforward: None (lowest layer)",
        "",
        "**Key Fields:**",
        "- `occurrence_count`: Total times seen in corpus",
        "- `document_ids`: Set of documents containing token",
        "- `pagerank`: Importance score",
        "- `tfidf`: Global TF-IDF weight",
        "- `cluster_id`: Assigned concept cluster",
        "",
        "### Layer 1: Bigrams",
        "",
        "**Purpose:** Represent word pairs for phrase-level patterns.",
        "",
        "**Content:** Space-separated word pairs: `\"neural networks\"`.",
        "",
        "**Connections:**",
        "- Lateral: Bigrams sharing components or co-occurring",
        "- Feedforward: Component tokens",
        "- Feedback: None typically (no Layer 2 → Layer 1 direct)",
        "",
        "**Key Fields:**",
        "- Same as Layer 0",
        "- Bigrams inherit properties from component tokens",
        "",
        "### Layer 2: Concepts",
        "",
        "**Purpose:** Represent semantic topic clusters.",
        "",
        "**Content:** Named by top members: `\"neural/networks/learning\"`.",
        "",
        "**Connections:**",
        "- Lateral: Concepts with overlapping documents or semantics",
        "- Feedforward: Member tokens",
        "- Feedback: None typically",
        "",
        "**Creation:** Built by `build_concept_clusters()` using label propagation on Layer 0 tokens.",
        "",
        "### Layer 3: Documents",
        "",
        "**Purpose:** Represent full documents in the corpus.",
        "",
        "**Content:** Document ID string.",
        "",
        "**Connections:**",
        "- Lateral: Documents sharing vocabulary",
        "- Feedforward: All tokens in document",
        "- Feedback: None (highest layer)",
        "",
        "**Key Fields:**",
        "- `document_ids`: Contains only self",
        "- `occurrence_count`: 1 (single document)",
        "",
        "---",
        "",
        "## Performance Patterns",
        "",
        "### O(1) ID Lookups",
        "",
        "**Critical:** Always use `layer.get_by_id(col_id)` instead of iterating:",
        "",
        "```python",
        "# WRONG - O(n):",
        "for col in layer.minicolumns.values():",
        "    if col.id == target_id:",
        "        neighbor = col",
        "",
        "# RIGHT - O(1):",
        "neighbor = layer.get_by_id(target_id)",
        "```",
        "",
        "Used throughout `analysis.py` and `query.py`.",
        "",
        "### Staleness Tracking",
        "",
        "**Location:** `processor.py:49`",
        "",
        "```python",
        "self._stale_computations: set",
        "```",
        "",
        "Tracks which computations need rerunning after corpus changes:",
        "- `COMP_TFIDF`",
        "- `COMP_PAGERANK`",
        "- `COMP_ACTIVATION`",
        "- `COMP_DOC_CONNECTIONS`",
        "- `COMP_BIGRAM_CONNECTIONS`",
        "- `COMP_CONCEPTS`",
        "",
        "### Query Caching",
        "",
        "**Location:** `processor.py:51-52`",
        "",
        "```python",
        "self._query_expansion_cache: Dict[str, Dict[str, float]]",
        "self._query_cache_max_size: int = 100",
        "```",
        "",
        "LRU cache for query expansion results. Cleared after `compute_all()`.",
        "",
        "---",
        "",
        "## File Reference",
        "",
        "| Component | File | Lines |",
        "|-----------|------|-------|",
        "| CorticalLayer enum | `layers.py` | 21-56 |",
        "| HierarchicalLayer | `layers.py` | 59-273 |",
        "| Minicolumn | `minicolumn.py` | 56-357 |",
        "| Edge | `minicolumn.py` | 16-53 |",
        "| process_document() | `processor.py` | 54-137 |",
        "| compute_all() | `processor.py` | 452-596 |",
        "| Tokenizer | `tokenizer.py` | Full file |",
        "",
        "---",
        "",
        "## Visual Summary",
        "",
        "```",
        "┌─────────────────────────────────────────────────────────────┐",
        "│                    Layer 3: DOCUMENTS                        │",
        "│  ┌─────────┐    ┌─────────┐                                 │",
        "│  │  doc1   │←──→│  doc2   │  (lateral: shared vocab)        │",
        "│  └────┬────┘    └────┬────┘                                 │",
        "│       │              │      (feedforward: contained tokens) │",
        "└───────┼──────────────┼──────────────────────────────────────┘",
        "        ↓              ↓",
        "┌───────┼──────────────┼──────────────────────────────────────┐",
        "│       │   Layer 2: CONCEPTS                                 │",
        "│  ┌────┴────┐    ┌────┴────┐                                │",
        "│  │ concept1│←──→│ concept2│  (lateral: doc overlap)        │",
        "│  └────┬────┘    └────┬────┘                                │",
        "│       │              │      (feedforward: member tokens)    │",
        "└───────┼──────────────┼──────────────────────────────────────┘",
        "        ↓              ↓",
        "┌───────┼──────────────┼──────────────────────────────────────┐",
        "│       │   Layer 1: BIGRAMS                                  │",
        "│  ┌────┴──────┐  ┌────┴──────┐                              │",
        "│  │neural     │←→│networks   │  (lateral: shared component) │",
        "│  │networks   │  │process    │                              │",
        "│  └────┬──────┘  └────┬──────┘                              │",
        "│       │              │      (feedforward: component tokens) │",
        "└───────┼──────────────┼──────────────────────────────────────┘",
        "        ↓              ↓",
        "┌───────┼──────────────┼──────────────────────────────────────┐",
        "│       │   Layer 0: TOKENS                                   │",
        "│  ┌────┴────┐ ┌──────┐ ┌────┴────┐ ┌────────┐              │",
        "│  │ neural  │←→│networks│←→│ process │←→│  data  │           │",
        "│  └─────────┘ └──────┘ └─────────┘ └────────┘              │",
        "│              (lateral: co-occurrence within window)         │",
        "└─────────────────────────────────────────────────────────────┘",
        "```"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "docs/glossary.md",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "# Glossary",
        "",
        "This glossary defines terminology used throughout the Cortical Text Processor codebase. Terms are organized by category for easy reference.",
        "",
        "---",
        "",
        "## Core Data Structures",
        "",
        "### Minicolumn",
        "The fundamental unit of representation at each layer. Named after cortical minicolumns in neuroscience, but implemented as a data structure holding connections, statistics, and metadata.",
        "",
        "**Location:** `minicolumn.py:56-357`",
        "",
        "**Fields:**",
        "- `id`: Unique identifier (e.g., \"L0_neural\")",
        "- `content`: The actual content (word, bigram, concept name, or doc_id)",
        "- `layer`: Layer number (0-3)",
        "- Various connection dictionaries and statistics",
        "",
        "### Edge",
        "A typed connection with metadata, used for ConceptNet-style semantic edges.",
        "",
        "**Location:** `minicolumn.py:16-53`",
        "",
        "**Fields:**",
        "- `target_id`: Target minicolumn ID",
        "- `weight`: Connection strength",
        "- `relation_type`: Semantic type ('IsA', 'PartOf', 'CoOccurs', etc.)",
        "- `confidence`: Reliability score [0.0, 1.0]",
        "- `source`: Origin ('corpus', 'semantic', 'inferred')",
        "",
        "### HierarchicalLayer",
        "Container that holds all minicolumns at a specific layer level.",
        "",
        "**Location:** `layers.py:59-273`",
        "",
        "**Key Features:**",
        "- `minicolumns` dict maps content to Minicolumn objects",
        "- `_id_index` provides O(1) lookup by minicolumn ID",
        "- Methods: `get_or_create_minicolumn()`, `get_by_id()`, `column_count()`",
        "",
        "### CorticalLayer",
        "Enumeration defining the 4 processing layers.",
        "",
        "**Location:** `layers.py:21-56`",
        "",
        "```",
        "TOKENS = 0      # Individual words",
        "BIGRAMS = 1     # Word pairs",
        "CONCEPTS = 2    # Semantic clusters",
        "DOCUMENTS = 3   # Full documents",
        "```",
        "",
        "---",
        "",
        "## Connection Types",
        "",
        "### Lateral Connections",
        "**Within-layer** connections between minicolumns at the same level. Built from co-occurrence patterns (tokens appearing near each other in text).",
        "",
        "**Storage:** `minicolumn.lateral_connections: Dict[str, float]`",
        "",
        "**Use:** Query expansion, PageRank computation, spreading activation.",
        "",
        "### Typed Connections",
        "**Within-layer** connections with semantic metadata. Store relation type, confidence, and source information.",
        "",
        "**Storage:** `minicolumn.typed_connections: Dict[str, Edge]`",
        "",
        "**Use:** Semantic PageRank, ConceptNet-style reasoning.",
        "",
        "### Feedforward Connections",
        "**Cross-layer** connections pointing downward (higher layer → lower layer). Connect containers to their components.",
        "",
        "**Storage:** `minicolumn.feedforward_connections: Dict[str, float]`",
        "",
        "**Examples:**",
        "- Bigram → component tokens",
        "- Concept → member tokens",
        "- Document → contained tokens",
        "",
        "### Feedback Connections",
        "**Cross-layer** connections pointing upward (lower layer → higher layer). Connect components to their containers.",
        "",
        "**Storage:** `minicolumn.feedback_connections: Dict[str, float]`",
        "",
        "**Examples:**",
        "- Token → containing bigrams",
        "- Token → containing concepts",
        "- Token → containing documents",
        "",
        "---",
        "",
        "## Algorithms",
        "",
        "### PageRank",
        "Graph algorithm measuring importance based on connection structure. Terms connected to other important terms receive higher scores.",
        "",
        "**Formula:** `PR(i) = (1-d)/n + d × Σ(PR(j) × w(j→i) / out(j))`",
        "",
        "**Location:** `analysis.py:22-95`",
        "",
        "**Variants:**",
        "- Standard PageRank: Equal edge weights",
        "- Semantic PageRank: Weights edges by relation type",
        "- Hierarchical PageRank: Propagates across layers",
        "",
        "### TF-IDF",
        "Term Frequency - Inverse Document Frequency. Measures how distinctive a term is to documents in the corpus.",
        "",
        "**Formula:** `TF-IDF = log(1 + count) × log(num_docs / doc_frequency)`",
        "",
        "**Location:** `analysis.py:394-433`",
        "",
        "**Variants:**",
        "- Global: Uses total corpus occurrence (`col.tfidf`)",
        "- Per-document: Uses document-specific count (`col.tfidf_per_doc[doc_id]`)",
        "",
        "### Label Propagation",
        "Community detection algorithm for clustering. Tokens adopt the most common label among their neighbors, causing related tokens to converge to the same cluster.",
        "",
        "**Location:** `analysis.py:502-636`",
        "",
        "**Parameters:**",
        "- `cluster_strictness`: Higher = more separate clusters",
        "- `bridge_weight`: Synthetic inter-document connections",
        "",
        "### Damping Factor",
        "PageRank parameter (default 0.85) representing probability of following a link vs. random jump. Lower damping = more randomness in importance distribution.",
        "",
        "### Query Expansion",
        "Process of adding related terms to a search query based on lateral connections, concept membership, or semantic relations.",
        "",
        "**Location:** `query.py:55-176`",
        "",
        "### Spreading Activation",
        "Information propagation through connections. Activation starts at query terms and spreads to connected nodes, simulating neural activation patterns.",
        "",
        "---",
        "",
        "## Semantic Relations",
        "",
        "### IsA",
        "Hypernym/hyponym relationship. \"A dog IsA animal\" means dog is a type of animal.",
        "",
        "**Weight:** 1.5 (highest)",
        "",
        "### PartOf",
        "Meronym/holonym relationship. \"Wheel PartOf car\" means wheel is a component of car.",
        "",
        "**Weight:** 1.3",
        "",
        "### HasA / HasProperty",
        "Property or component ownership. \"Dog HasProperty loyal\" or \"Dog HasA tail\".",
        "",
        "**Weight:** 1.2",
        "",
        "### SimilarTo",
        "Similarity without hierarchy. \"Dog SimilarTo cat\" - both are pets/animals.",
        "",
        "**Weight:** 1.4",
        "",
        "### RelatedTo",
        "General association from co-occurrence. Default relation type.",
        "",
        "**Weight:** 1.0",
        "",
        "### CoOccurs",
        "Statistical co-occurrence in text. Lower confidence than explicit relations.",
        "",
        "**Weight:** 0.8",
        "",
        "### Causes",
        "Causal relationship. \"Rain Causes floods\".",
        "",
        "**Weight:** 1.1",
        "",
        "### UsedFor",
        "Functional purpose. \"Hammer UsedFor nailing\".",
        "",
        "**Weight:** 1.0",
        "",
        "### Antonym",
        "Opposition/contrast. \"Big Antonym small\".",
        "",
        "**Weight:** 0.3 (penalized)",
        "",
        "### DerivedFrom",
        "Morphological or etymological derivation.",
        "",
        "**Weight:** 1.2",
        "",
        "---",
        "",
        "## Processing Concepts",
        "",
        "### Tokenization",
        "Breaking text into individual word tokens. Includes lowercasing, stop word removal, and optional stemming.",
        "",
        "**Location:** `tokenizer.py`",
        "",
        "### Bigram",
        "A pair of consecutive tokens. Stored with SPACE separator: \"neural networks\" (not underscore).",
        "",
        "**Location:** `tokenizer.py:303-316`",
        "",
        "### Concept Cluster",
        "Group of semantically related tokens discovered through label propagation. Becomes a minicolumn in Layer 2.",
        "",
        "### Corpus",
        "The collection of all documents processed by the system.",
        "",
        "### Retrofitting",
        "Post-processing that adjusts lateral connection weights to align with semantic relations. Blends co-occurrence patterns with semantic knowledge.",
        "",
        "**Location:** `semantics.py:378-476`",
        "",
        "---",
        "",
        "## Architecture Concepts",
        "",
        "### 4-Layer Hierarchy",
        "The core architecture organizing text at increasing abstraction levels:",
        "- Layer 0: TOKENS (words)",
        "- Layer 1: BIGRAMS (word pairs)",
        "- Layer 2: CONCEPTS (topic clusters)",
        "- Layer 3: DOCUMENTS (full texts)",
        "",
        "### Cortical Metaphor",
        "The naming convention draws from neuroscience (V1→V2→V4→IT visual cortex pathway) but implementations are standard IR algorithms, not neural models.",
        "",
        "### Staleness Tracking",
        "System for knowing which computations need rerunning after corpus changes. Prevents unnecessary recomputation.",
        "",
        "**Location:** `processor.py:49`",
        "",
        "---",
        "",
        "## Search Concepts",
        "",
        "### Intent Parsing",
        "Extracting user intent from natural language queries. Maps question words to intent types (where→location, how→implementation).",
        "",
        "**Location:** `query.py:179-284`",
        "",
        "### Multi-hop Expansion",
        "Query expansion through chains of semantic relations. Finds terms 2+ hops away through valid relation paths.",
        "",
        "**Location:** `query.py:407-531`",
        "",
        "### Chunk",
        "A segment of document text for passage retrieval. Created with configurable size and overlap.",
        "",
        "**Location:** `query.py:937-978`",
        "",
        "### Inverted Index",
        "Pre-computed mapping from terms to containing documents. Enables fast candidate filtering.",
        "",
        "**Location:** `query.py` (fast search functions)",
        "",
        "---",
        "",
        "## Code Concepts",
        "",
        "### Programming Concept Groups",
        "Collections of synonymous programming terms. \"get\", \"fetch\", \"load\", \"retrieve\" are grouped together.",
        "",
        "**Location:** `code_concepts.py`",
        "",
        "### Code-Aware Tokenization",
        "Tokenization that splits identifiers: `getUserName` → `[\"getusername\", \"get\", \"user\", \"name\"]`.",
        "",
        "**Location:** `tokenizer.py` (split_identifiers parameter)",
        "",
        "### Semantic Fingerprint",
        "Vector representation of a text's semantic content for similarity comparison.",
        "",
        "**Location:** `fingerprint.py`",
        "",
        "---",
        "",
        "## Performance Concepts",
        "",
        "### O(1) ID Lookup",
        "Using `layer.get_by_id(col_id)` instead of iterating minicolumns. Critical for algorithm performance.",
        "",
        "### Query Cache",
        "LRU cache storing query expansion results to avoid recomputation for repeated queries.",
        "",
        "**Location:** `processor.py:51-52`",
        "",
        "### Batch Processing",
        "Processing multiple queries or documents together to amortize overhead.",
        "",
        "**Functions:** `find_documents_batch()`, `find_passages_batch()`, `add_documents_batch()`",
        "",
        "---",
        "",
        "## File Locations Quick Reference",
        "",
        "| Term | Primary File |",
        "|------|--------------|",
        "| Minicolumn | `minicolumn.py` |",
        "| Edge | `minicolumn.py` |",
        "| HierarchicalLayer | `layers.py` |",
        "| CorticalLayer | `layers.py` |",
        "| PageRank | `analysis.py` |",
        "| TF-IDF | `analysis.py` |",
        "| Label Propagation | `analysis.py` |",
        "| Query Expansion | `query.py` |",
        "| Relation Extraction | `semantics.py` |",
        "| Retrofitting | `semantics.py` |",
        "| Tokenization | `tokenizer.py` |",
        "| Fingerprint | `fingerprint.py` |",
        "| Code Concepts | `code_concepts.py` |"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "scripts/index_codebase.py",
      "function": "def get_python_files(base_path: Path) -> list:",
      "start_line": 26,
      "lines_added": [
        "    \"\"\"Get documentation files from root and docs/ directory.\"\"\"",
        "    # Root documentation files",
        "    root_docs = ['CLAUDE.md', 'TASK_LIST.md', 'README.md', 'KNOWLEDGE_TRANSFER.md']",
        "    for doc in root_docs:",
        "",
        "    # Intelligence documentation in docs/",
        "    docs_dir = base_path / 'docs'",
        "    if docs_dir.exists():",
        "        for md_file in docs_dir.glob('*.md'):",
        "            files.append(md_file)",
        ""
      ],
      "lines_removed": [
        "    \"\"\"Get documentation files.\"\"\"",
        "    doc_files = ['CLAUDE.md', 'TASK_LIST.md', 'README.md', 'KNOWLEDGE_TRANSFER.md']",
        "    for doc in doc_files:"
      ],
      "context_before": [
        "    for directory in ['cortical', 'tests']:",
        "        dir_path = base_path / directory",
        "        if dir_path.exists():",
        "            for py_file in dir_path.rglob('*.py'):",
        "                if not py_file.name.startswith('__'):",
        "                    files.append(py_file)",
        "    return sorted(files)",
        "",
        "",
        "def get_doc_files(base_path: Path) -> list:"
      ],
      "context_after": [
        "    files = []",
        "        doc_path = base_path / doc",
        "        if doc_path.exists():",
        "            files.append(doc_path)",
        "    return files",
        "",
        "",
        "def create_doc_id(file_path: Path, base_path: Path) -> str:",
        "    \"\"\"Create a document ID from file path.\"\"\"",
        "    rel_path = file_path.relative_to(base_path)",
        "    return str(rel_path)",
        "",
        "",
        "def index_file(processor: CorticalTextProcessor, file_path: Path, base_path: Path) -> dict:"
      ],
      "change_type": "modify"
    }
  ],
  "hour_of_day": 15,
  "day_of_week": "Wednesday",
  "seconds_since_last_commit": -425413,
  "is_merge": false,
  "is_initial": false,
  "parent_count": 1,
  "session_id": null,
  "related_chats": [],
  "ci_result": null,
  "reverted": false,
  "amended": false
}