{
  "hash": "14a09da9a2b5237db3bbaaddbd41d6a8747aa479",
  "message": "Merge pull request #28 from scrawlsbenches/claude/run-showcase-01JbkiDV4QQv7c1K4oS2K9wa",
  "author": "scrawlsbenches",
  "timestamp": "2025-12-10 21:13:51 -0500",
  "branch": "claude/multi-index-design-DvifZ",
  "files_changed": [
    "TASK_LIST.md",
    "scripts/ask_codebase.py",
    "scripts/index_codebase.py",
    "scripts/search_codebase.py",
    "showcase.py",
    "tests/test_ask_codebase.py",
    "tests/test_search_codebase.py",
    "tests/test_showcase.py"
  ],
  "insertions": 1773,
  "deletions": 37,
  "hunks": [
    {
      "file": "TASK_LIST.md",
      "function": null,
      "start_line": 1,
      "lines_added": [
        "**Last Updated:** 2025-12-11",
        "**Status:** Bug fixes complete | Developer experience enhancements planned"
      ],
      "lines_removed": [
        "**Last Updated:** 2025-12-09",
        "**Status:** Bug fixes complete | RAG enhancements planned"
      ],
      "context_before": [
        "# Task List: Bug Fixes & RAG Enhancements",
        "",
        "This document tracks bug fixes and feature enhancements for the Cortical Text Processor.",
        ""
      ],
      "context_after": [
        "",
        "---",
        "",
        "## Critical Priority",
        "",
        "### 1. Fix Per-Document TF-IDF Calculation Bug",
        "",
        "**File:** `cortical/analysis.py`",
        "**Line:** 131",
        "**Status:** [x] Completed"
      ],
      "change_type": "modify"
    },
    {
      "file": "TASK_LIST.md",
      "function": "def find_passages_for_query(..., apply_doc_boost: bool = True):",
      "start_line": 2401,
      "lines_added": [
        "",
        "---",
        "",
        "# Developer Experience Enhancements",
        "",
        "These tasks focus on making the Cortical Text Processor genuinely enjoyable to use for day-to-day development work.",
        "",
        "---",
        "",
        "## Showcase Improvements",
        "",
        "### 67. Fix O(n) Lookup in Showcase find_concept_associations",
        "",
        "**File:** `showcase.py`",
        "**Lines:** 213-218",
        "**Status:** [x] Completed",
        "**Priority:** Low",
        "",
        "**Problem:**",
        "The `find_concept_associations` method iterates all minicolumns to find neighbor content:",
        "```python",
        "for c in layer0.minicolumns.values():",
        "    if c.id == neighbor_id:",
        "        # found it",
        "```",
        "",
        "This is O(n) when we have O(1) `get_by_id()` available.",
        "",
        "**Solution:**",
        "```python",
        "neighbor = layer0.get_by_id(neighbor_id)",
        "if neighbor:",
        "    # use neighbor.content",
        "```",
        "",
        "---",
        "",
        "### 68. Add Code-Specific Features to Showcase",
        "",
        "**File:** `showcase.py`",
        "**Status:** [x] Completed",
        "**Priority:** Medium",
        "",
        "**Problem:**",
        "The showcase demonstrates general IR capabilities but not code-specific features documented in CLAUDE.md:",
        "- `expand_query_for_code()` - programming-aware query expansion",
        "- `search_by_intent()` - natural language intent queries",
        "- `get_fingerprint()` / `compare_fingerprints()` - code similarity",
        "- `is_conceptual_query()` - query type detection",
        "",
        "**Solution:**",
        "Add new demonstration sections:",
        "1. **Code Query Expansion** - show how \"fetch data\" expands to include \"get\", \"load\", \"retrieve\"",
        "2. **Intent-Based Search** - demonstrate \"where do we handle errors?\" style queries",
        "3. **Code Fingerprinting** - compare two similar functions and explain their similarity",
        "4. **Query Intent Detection** - show how system distinguishes \"what is PageRank\" vs \"compute pagerank\"",
        "",
        "---",
        "",
        "### 69. Add Passage-Level Search Demo to Showcase",
        "",
        "**File:** `showcase.py`",
        "**Status:** [x] Completed",
        "**Priority:** Medium",
        "",
        "**Problem:**",
        "`find_passages_for_query()` is the key RAG capability for retrieving relevant code snippets, but it's not demonstrated. This is arguably the most useful feature for LLM integration.",
        "",
        "**Solution:**",
        "Add \"RAG DEMONSTRATION\" section showing:",
        "1. Query â†’ relevant passages with file:line references",
        "2. How passage chunking works",
        "3. Overlap handling for context preservation",
        "4. Use case: feeding context to an LLM",
        "",
        "---",
        "",
        "### 70. Add Performance Timing to Showcase",
        "",
        "**File:** `showcase.py`",
        "**Status:** [x] Completed",
        "**Priority:** Low",
        "",
        "**Problem:**",
        "No timing information shown. Users can't gauge performance characteristics.",
        "",
        "**Solution:**",
        "Add timing for key operations:",
        "- Document processing time",
        "- `compute_all()` time",
        "- Query expansion time",
        "- Document search time",
        "- Passage retrieval time",
        "",
        "---",
        "",
        "## Code Index Improvements",
        "",
        "### 71. Enable Code-Aware Tokenization in Index",
        "",
        "**File:** `scripts/index_codebase.py`",
        "**Status:** [x] Completed",
        "**Priority:** High",
        "",
        "**Problem:**",
        "The indexer uses default tokenization which doesn't split identifiers. Searching for \"user\" won't find `getUserCredentials` or `user_credentials`.",
        "",
        "**Solution:**",
        "Enable `split_identifiers=True` when creating the processor:",
        "```python",
        "processor = CorticalTextProcessor(",
        "    tokenizer_config={'split_identifiers': True}",
        ")",
        "```",
        "",
        "Or configure per-document based on file type (.py files get identifier splitting).",
        "",
        "**Impact:** Much better code search - \"auth\" would find `authenticate`, `AuthService`, `user_auth`, etc.",
        "",
        "---",
        "",
        "### 72. Use Programming Query Expansion in Search",
        "",
        "**File:** `scripts/search_codebase.py`",
        "**Status:** [x] Completed",
        "**Priority:** High",
        "",
        "**Problem:**",
        "Search uses `expand_query()` but not `expand_query_for_code()`. Programming synonyms aren't utilized.",
        "",
        "**Solution:**",
        "```python",
        "# In search_codebase.py",
        "if is_code_query(query):  # Detect if searching for code patterns",
        "    expanded = processor.expand_query_for_code(query)",
        "else:",
        "    expanded = processor.expand_query(query)",
        "```",
        "",
        "**Impact:** \"get data\" would expand to include \"fetch\", \"load\", \"retrieve\", \"read\" variations.",
        "",
        "---",
        "",
        "### 73. Add \"Find Similar Code\" Command",
        "",
        "**Files:** `scripts/search_codebase.py`",
        "**Status:** [x] Completed",
        "**Priority:** Medium",
        "",
        "**Problem:**",
        "No way to find code similar to a given snippet or function. Fingerprinting exists but isn't exposed.",
        "",
        "**Solution Applied:**",
        "1. Added `find_similar_code()` function (~60 lines) implementing:",
        "   - Parses file:line references to extract target text from indexed documents",
        "   - Falls back to raw text comparison for direct code input",
        "   - Uses `get_fingerprint()` and `compare_fingerprints()` for similarity scoring",
        "   - Chunks documents and compares fingerprints against target",
        "   - Returns top-N similar passages with scores and shared terms",
        "2. Added `display_similar_results()` function for formatted output",
        "3. Added `--similar-to` / `-s` argument to CLI",
        "4. Added tests in `tests/test_search_codebase.py` (5 tests)",
        "",
        "**Usage:**",
        "```bash",
        "# Find code similar to a file:line reference",
        "python scripts/search_codebase.py --similar-to cortical/processor.py:100",
        "",
        "# Find code similar to raw text",
        "python scripts/search_codebase.py -s \"def compute_score(items, weights)\"",
        "",
        "# With more results",
        "python scripts/search_codebase.py --similar-to processor.py:50 --top 10",
        "```",
        "",
        "**Files Modified:**",
        "- `scripts/search_codebase.py` - Added `find_similar_code()`, `display_similar_results()`, CLI argument (~120 lines)",
        "- `tests/test_search_codebase.py` - New file with 23 tests for search functions",
        "",
        "---",
        "",
        "## Creative Developer Experience Features",
        "",
        "### 74. Add \"Explain This Code\" Command",
        "",
        "**Files:** `scripts/explain_code.py` (new)",
        "**Status:** [ ] Not Started",
        "**Priority:** Medium",
        "",
        "**Problem:**",
        "When jumping into unfamiliar code, it's hard to understand what it does and how it fits into the larger system.",
        "",
        "**Solution:**",
        "Create `explain_code.py` that uses semantic analysis to explain code:",
        "```bash",
        "python scripts/explain_code.py cortical/analysis.py:compute_pagerank",
        "",
        "# Output:",
        "# Function: compute_pagerank",
        "# Purpose: Computes importance scores for tokens using iterative graph algorithm",
        "# ",
        "# Key Concepts: pagerank, damping, convergence, graph, centrality",
        "# Related Files:",
        "#   - cortical/processor.py:789 (calls this function)",
        "#   - tests/test_analysis.py:45 (tests this function)",
        "#   - CLAUDE.md:142 (documents this feature)",
        "# ",
        "# Similar Functions:",
        "#   - compute_tfidf (same file) - also computes term importance",
        "#   - compute_importance (processor.py) - wrapper method",
        "```",
        "",
        "**Implementation:**",
        "1. Parse target location",
        "2. Get semantic fingerprint",
        "3. Find related documents (callers, tests, docs)",
        "4. Find similar code patterns",
        "5. Extract key concepts from local context",
        "",
        "---",
        "",
        "### 75. Add \"What Changed?\" Semantic Diff",
        "",
        "**Files:** `scripts/what_changed.py` (new)",
        "**Status:** [ ] Not Started",
        "**Priority:** Medium",
        "",
        "**Problem:**",
        "Git diff shows line-by-line changes but doesn't explain semantic impact. Hard to review large changes.",
        "",
        "**Solution:**",
        "Create semantic diff tool:",
        "```bash",
        "python scripts/what_changed.py HEAD~5..HEAD",
        "",
        "# Output:",
        "# Semantic Summary of Changes (5 commits)",
        "# ",
        "# New Capabilities:",
        "#   - Doc-type boosting for search results",
        "#   - Chunk-based indexing for git compatibility",
        "# ",
        "# Modified Behaviors:",
        "#   - Query expansion now considers document type",
        "#   - Passage search has new boosting parameter",
        "# ",
        "# Files Most Affected:",
        "#   - cortical/query.py (3 new functions, 2 modified)",
        "#   - scripts/search_codebase.py (new --prefer-docs flag)",
        "# ",
        "# Concepts Impacted: search, ranking, documentation, indexing",
        "```",
        "",
        "**Implementation:**",
        "1. Get changed files from git",
        "2. Re-index changed files",
        "3. Compare fingerprints before/after",
        "4. Identify new concepts, modified concepts, removed concepts",
        "5. Generate natural language summary",
        "",
        "---",
        "",
        "### 76. Add \"Suggest Related Files\" Feature",
        "",
        "**Files:** `scripts/related_files.py` (new), integration with editor",
        "**Status:** [ ] Not Started",
        "**Priority:** Medium",
        "",
        "**Problem:**",
        "When editing a file, you often need to update related files (tests, docs, callers). Easy to miss something.",
        "",
        "**Solution:**",
        "```bash",
        "python scripts/related_files.py cortical/query.py",
        "",
        "# Output:",
        "# Files related to cortical/query.py:",
        "# ",
        "# Tests (should update if changing behavior):",
        "#   - tests/test_processor.py (47 references)",
        "#   - tests/test_query.py (if exists)",
        "# ",
        "# Documentation (should update if changing API):",
        "#   - CLAUDE.md (references: expand_query, find_documents)",
        "#   - docs/usage-patterns.md (examples using query functions)",
        "# ",
        "# Callers (may be affected):",
        "#   - cortical/processor.py (imports 12 functions)",
        "#   - scripts/search_codebase.py (uses find_documents)",
        "# ",
        "# Similar Files (might need same changes):",
        "#   - cortical/analysis.py (similar structure, shared patterns)",
        "```",
        "",
        "**Implementation:**",
        "1. Find all files that import/reference target",
        "2. Find test files that test target",
        "3. Find docs that mention target functions",
        "4. Find files with similar fingerprints",
        "5. Rank by relevance",
        "",
        "---",
        "",
        "### 77. Add Interactive \"Ask the Codebase\" Mode",
        "",
        "**Files:** `scripts/ask_codebase.py` (new)",
        "**Status:** [x] Completed",
        "**Priority:** High",
        "",
        "**Problem:**",
        "Current search returns passages but doesn't synthesize answers. You have to read multiple results.",
        "",
        "**Solution:**",
        "Create conversational interface that uses RAG to answer questions:",
        "```bash",
        "python scripts/ask_codebase.py",
        "",
        "Ask> How does query expansion work?",
        "",
        "Based on cortical/query.py:234-298 and CLAUDE.md:156:",
        "",
        "Query expansion works by finding semantically related terms to add to a search:",
        "",
        "1. Tokenizes the query into individual terms",
        "2. For each term, finds lateral connections (co-occurring terms)",
        "3. Weights expansions by connection strength and PageRank",
        "4. Returns expanded query as termâ†’weight dictionary",
        "",
        "Key parameters:",
        "- max_expansions: limit number of added terms (default: 10)",
        "- use_semantic: include typed semantic relations (default: True)",
        "",
        "Sources:",
        "- cortical/query.py:234 (get_expanded_query_terms)",
        "- cortical/query.py:298 (expand_query)",
        "- CLAUDE.md:156 (Quick Reference)",
        "",
        "Ask> What's different about expand_query_for_code?",
        "...",
        "```",
        "",
        "**Implementation:**",
        "1. Take natural language question",
        "2. Detect intent (conceptual vs implementation)",
        "3. Retrieve relevant passages with boosting",
        "4. Synthesize answer from passages (or format for LLM)",
        "5. Include source references",
        "",
        "---",
        "",
        "### 78. Add Code Pattern Detection",
        "",
        "**Files:** `cortical/patterns.py` (new), `scripts/find_patterns.py` (new)",
        "**Status:** [ ] Not Started",
        "**Priority:** Low",
        "",
        "**Problem:**",
        "Hard to find all instances of a pattern (e.g., \"all functions that iterate over minicolumns\").",
        "",
        "**Solution:**",
        "```bash",
        "# Find all places that iterate minicolumns (potential O(n) â†’ O(1) optimization)",
        "python scripts/find_patterns.py \"for.*in.*minicolumns\"",
        "",
        "# Find all TF-IDF calculations",
        "python scripts/find_patterns.py --semantic \"tfidf calculation\"",
        "",
        "# Find potential bugs: linear search where O(1) exists",
        "python scripts/find_patterns.py --smell \"linear-search-with-index\"",
        "```",
        "",
        "**Implementation:**",
        "1. Regex patterns for syntactic search",
        "2. Semantic patterns using fingerprint similarity",
        "3. \"Smell\" patterns for common anti-patterns",
        "4. Report with file:line references",
        "",
        "---",
        "",
        "### 79. Add Corpus Health Dashboard",
        "",
        "**Files:** `scripts/corpus_health.py` (new)",
        "**Status:** [ ] Not Started",
        "**Priority:** Low",
        "",
        "**Problem:**",
        "No visibility into corpus quality - are docs well-connected? Any orphaned files? Coverage gaps?",
        "",
        "**Solution:**",
        "```bash",
        "python scripts/corpus_health.py",
        "",
        "# Output:",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•",
        "#            CORPUS HEALTH REPORT",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•",
        "# ",
        "# Overall Health: 87% (Good)",
        "# ",
        "# Coverage:",
        "#   âœ“ 45/47 Python files indexed",
        "#   âœ“ 8/8 documentation files indexed",
        "#   âœ— 2 files not indexed: __init__.py, __pycache__",
        "# ",
        "# Connectivity:",
        "#   âœ“ Average doc connections: 12.3",
        "#   âœ“ No orphaned documents",
        "#   âš  3 weakly connected: test_*.py files",
        "# ",
        "# Documentation Quality:",
        "#   âœ“ All public functions mentioned in docs",
        "#   âš  5 functions lack docstrings",
        "#   âœ— TASK_LIST.md references deleted function",
        "# ",
        "# Index Freshness:",
        "#   âœ“ Corpus updated 2 minutes ago",
        "#   âœ“ All files current",
        "# ",
        "# Recommendations:",
        "#   1. Add tests for cortical/chunk_index.py (0% coverage)",
        "#   2. Document new functions in query.py:get_doc_type_boost",
        "#   3. Update TASK_LIST.md references",
        "```",
        "",
        "---",
        "",
        "### 80. Add \"Learning Mode\" for New Contributors",
        "",
        "**Files:** `scripts/learn_codebase.py` (new)",
        "**Status:** [ ] Not Started",
        "**Priority:** Low",
        "",
        "**Problem:**",
        "New contributors struggle to understand unfamiliar codebases. Where to start? What's important?",
        "",
        "**Solution:**",
        "Interactive learning mode that guides exploration:",
        "```bash",
        "python scripts/learn_codebase.py",
        "",
        "Welcome to the Cortical Text Processor codebase!",
        "",
        "This codebase implements a hierarchical text analysis system inspired by",
        "how the visual cortex processes information.",
        "",
        "Would you like to:",
        "1. Start with the architecture overview",
        "2. Explore a specific feature",
        "3. See the most important files",
        "4. Take a guided tour",
        "",
        "> 1",
        "",
        "ARCHITECTURE OVERVIEW",
        "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•",
        "",
        "The system has 4 layers, like visual cortex V1â†’IT:",
        "",
        "  Layer 0 (Tokens)   â†’ Individual words",
        "  Layer 1 (Bigrams)  â†’ Word pairs",
        "  Layer 2 (Concepts) â†’ Semantic clusters",
        "  Layer 3 (Documents)â†’ Full documents",
        "",
        "Key files to understand:",
        "  1. cortical/processor.py - Main API (start here)",
        "  2. cortical/minicolumn.py - Core data structure",
        "  3. cortical/analysis.py - Graph algorithms",
        "",
        "[Press Enter to explore processor.py, or type a question]",
        "> How does PageRank work here?",
        "",
        "[Retrieves and explains relevant passages...]",
        "```",
        "",
        "---",
        "",
        "## Summary Table",
        "",
        "| # | Priority | Task | Status | Category |",
        "|---|----------|------|--------|----------|",
        "| 67 | Low | Fix O(n) lookup in showcase | âœ“ Done | Showcase |",
        "| 68 | Medium | Add code-specific features to showcase | âœ“ Done | Showcase |",
        "| 69 | Medium | Add passage-level search demo | âœ“ Done | Showcase |",
        "| 70 | Low | Add performance timing to showcase | âœ“ Done | Showcase |",
        "| 71 | High | Enable code-aware tokenization in index | âœ“ Done | Code Index |",
        "| 72 | High | Use programming query expansion in search | âœ“ Done | Code Index |",
        "| 73 | Medium | Add \"Find Similar Code\" command | | Code Index |",
        "| 74 | Medium | Add \"Explain This Code\" command | | Developer Experience |",
        "| 75 | Medium | Add \"What Changed?\" semantic diff | | Developer Experience |",
        "| 76 | Medium | Add \"Suggest Related Files\" feature | | Developer Experience |",
        "| 77 | High | Add interactive \"Ask the Codebase\" mode | âœ“ Done | Developer Experience |",
        "| 78 | Low | Add code pattern detection | | Developer Experience |",
        "| 79 | Low | Add corpus health dashboard | | Developer Experience |",
        "| 80 | Low | Add \"Learning Mode\" for new contributors | | Developer Experience |",
        "",
        "---",
        "",
        "*Added 2025-12-11, completions updated 2025-12-11*"
      ],
      "lines_removed": [],
      "context_before": [
        "| 41 | Medium | Create Configuration Dataclass | [x] Completed | Code Quality |",
        "| 56 | Medium | Create Usage Patterns Documentation | [x] Completed | Documentation |",
        "| 66 | Medium | Add doc-type boost to passage search | [ ] Not Started | Search Quality |",
        "| 42 | Low | Add Simple Query Language Support | [ ] Not Started | Feature |",
        "| 44 | Low | Remove Deprecated feedforward_sources | [ ] Not Started | Code Quality |",
        "| 46 | Low | Standardize Return Types with Dataclasses | [ ] Not Started | Code Quality |",
        "",
        "---",
        "",
        "*Updated 2025-12-11*"
      ],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "scripts/ask_codebase.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "#!/usr/bin/env python3",
        "\"\"\"",
        "Ask the Codebase - Interactive Q&A using RAG.",
        "",
        "This script provides a conversational interface for asking questions",
        "about the codebase, using the Cortical Text Processor's passage retrieval",
        "to find relevant context.",
        "",
        "Usage:",
        "    python scripts/ask_codebase.py                    # Interactive mode",
        "    python scripts/ask_codebase.py \"How does X work?\" # Single question",
        "    python scripts/ask_codebase.py --sources          # Show source references",
        "\"\"\"",
        "",
        "import argparse",
        "import sys",
        "from pathlib import Path",
        "from typing import List, Tuple, Dict, Optional",
        "",
        "# Add parent directory to path for imports",
        "sys.path.insert(0, str(Path(__file__).parent.parent))",
        "",
        "from cortical.processor import CorticalTextProcessor",
        "",
        "",
        "def find_line_number(doc_content: str, char_position: int) -> int:",
        "    \"\"\"Find the line number for a character position.\"\"\"",
        "    return doc_content[:char_position].count('\\n') + 1",
        "",
        "",
        "def format_reference(doc_id: str, line_num: int) -> str:",
        "    \"\"\"Format a file:line reference.\"\"\"",
        "    return f\"{doc_id}:{line_num}\"",
        "",
        "",
        "def get_doc_type_emoji(doc_id: str) -> str:",
        "    \"\"\"Get emoji indicator for document type.\"\"\"",
        "    if doc_id.endswith('.md'):",
        "        return \"ðŸ“–\"",
        "    elif doc_id.startswith('tests/'):",
        "        return \"ðŸ§ª\"",
        "    else:",
        "        return \"ðŸ’»\"",
        "",
        "",
        "class CodebaseQA:",
        "    \"\"\"Interactive Q&A system for the codebase.\"\"\"",
        "",
        "    def __init__(self, processor: CorticalTextProcessor):",
        "        self.processor = processor",
        "",
        "    def find_relevant_passages(",
        "        self,",
        "        question: str,",
        "        top_n: int = 5,",
        "        chunk_size: int = 400",
        "    ) -> List[Tuple[str, str, int, float]]:",
        "        \"\"\"",
        "        Find passages relevant to the question.",
        "",
        "        Returns:",
        "            List of (passage_text, reference, line_num, score) tuples",
        "        \"\"\"",
        "        # Detect if this is a conceptual or implementation question",
        "        is_conceptual = self.processor.is_conceptual_query(question)",
        "",
        "        # Get relevant documents with boosting",
        "        doc_results = self.processor.find_documents_with_boost(",
        "            question,",
        "            top_n=top_n * 2,",
        "            auto_detect_intent=True",
        "        )",
        "",
        "        # Get passages from those documents",
        "        doc_ids = [doc_id for doc_id, _ in doc_results]",
        "        passages = self.processor.find_passages_for_query(",
        "            question,",
        "            top_n=top_n,",
        "            chunk_size=chunk_size,",
        "            overlap=100,",
        "            doc_filter=doc_ids if doc_ids else None",
        "        )",
        "",
        "        results = []",
        "        for passage_text, doc_id, start, end, score in passages:",
        "            doc_content = self.processor.documents.get(doc_id, \"\")",
        "            line_num = find_line_number(doc_content, start)",
        "            reference = format_reference(doc_id, line_num)",
        "            results.append((passage_text, reference, line_num, score))",
        "",
        "        return results",
        "",
        "    def format_answer(",
        "        self,",
        "        question: str,",
        "        passages: List[Tuple[str, str, int, float]],",
        "        show_sources: bool = True,",
        "        verbose: bool = False",
        "    ) -> str:",
        "        \"\"\"",
        "        Format the answer with context from passages.",
        "",
        "        This creates a structured response showing:",
        "        1. Question type (conceptual vs implementation)",
        "        2. Relevant passages with references",
        "        3. Source list for verification",
        "        \"\"\"",
        "        lines = []",
        "",
        "        # Detect question type",
        "        is_conceptual = self.processor.is_conceptual_query(question)",
        "        intent = \"conceptual\" if is_conceptual else \"implementation\"",
        "",
        "        lines.append(f\"\\n{'â”€' * 60}\")",
        "        lines.append(f\"Question: {question}\")",
        "        lines.append(f\"Type: {intent}\")",
        "        lines.append(f\"{'â”€' * 60}\\n\")",
        "",
        "        if not passages:",
        "            lines.append(\"No relevant passages found for this question.\")",
        "            lines.append(\"Try rephrasing or using different keywords.\")",
        "            return '\\n'.join(lines)",
        "",
        "        lines.append(\"ðŸ“š Relevant Context:\\n\")",
        "",
        "        for i, (passage_text, reference, line_num, score) in enumerate(passages, 1):",
        "            doc_id = reference.split(':')[0]",
        "            emoji = get_doc_type_emoji(doc_id)",
        "",
        "            lines.append(f\"[{i}] {emoji} {reference} (relevance: {score:.2f})\")",
        "            lines.append(\"â”€\" * 50)",
        "",
        "            # Show passage content",
        "            if verbose:",
        "                # Full passage",
        "                for line in passage_text.strip().split('\\n'):",
        "                    lines.append(f\"  {line}\")",
        "            else:",
        "                # Truncated passage (first 5 lines)",
        "                passage_lines = passage_text.strip().split('\\n')[:5]",
        "                for line in passage_lines:",
        "                    if len(line) > 70:",
        "                        line = line[:67] + \"...\"",
        "                    lines.append(f\"  {line}\")",
        "                if len(passage_text.strip().split('\\n')) > 5:",
        "                    lines.append(f\"  ... (more content in source)\")",
        "",
        "            lines.append(\"\")",
        "",
        "        if show_sources:",
        "            lines.append(\"\\nðŸ“Œ Sources:\")",
        "            seen_docs = set()",
        "            for passage_text, reference, line_num, score in passages:",
        "                doc_id = reference.split(':')[0]",
        "                if doc_id not in seen_docs:",
        "                    emoji = get_doc_type_emoji(doc_id)",
        "                    lines.append(f\"  {emoji} {doc_id}\")",
        "                    seen_docs.add(doc_id)",
        "",
        "        lines.append(\"\")",
        "        return '\\n'.join(lines)",
        "",
        "    def answer(",
        "        self,",
        "        question: str,",
        "        top_n: int = 3,",
        "        show_sources: bool = True,",
        "        verbose: bool = False",
        "    ) -> str:",
        "        \"\"\"Answer a question about the codebase.\"\"\"",
        "        passages = self.find_relevant_passages(question, top_n=top_n)",
        "        return self.format_answer(question, passages, show_sources, verbose)",
        "",
        "",
        "def interactive_mode(qa: CodebaseQA, verbose: bool = False):",
        "    \"\"\"Run interactive Q&A mode.\"\"\"",
        "    print(\"\\n\" + \"=\" * 60)",
        "    print(\"       ðŸ§  ASK THE CODEBASE - Interactive Q&A\")",
        "    print(\"=\" * 60)",
        "    print(\"\\nAsk questions about the codebase in natural language.\")",
        "    print(\"The system will find relevant passages to help answer.\\n\")",
        "    print(\"Commands:\")",
        "    print(\"  /verbose    - Toggle verbose mode (show full passages)\")",
        "    print(\"  /top N      - Set number of results (default: 3)\")",
        "    print(\"  /help       - Show this help\")",
        "    print(\"  /quit       - Exit\")",
        "    print()",
        "",
        "    top_n = 3",
        "    show_verbose = verbose",
        "",
        "    while True:",
        "        try:",
        "            question = input(\"Ask> \").strip()",
        "        except (EOFError, KeyboardInterrupt):",
        "            print(\"\\nGoodbye!\")",
        "            break",
        "",
        "        if not question:",
        "            continue",
        "",
        "        if question.startswith('/'):",
        "            cmd_parts = question.split(maxsplit=1)",
        "            cmd = cmd_parts[0].lower()",
        "",
        "            if cmd in ('/quit', '/exit', '/q'):",
        "                print(\"Goodbye!\")",
        "                break",
        "            elif cmd == '/help':",
        "                print(\"\\nCommands: /verbose, /top N, /quit\")",
        "                print(\"Or just type your question!\\n\")",
        "            elif cmd == '/verbose':",
        "                show_verbose = not show_verbose",
        "                status = \"ON\" if show_verbose else \"OFF\"",
        "                print(f\"Verbose mode: {status}\")",
        "            elif cmd == '/top' and len(cmd_parts) > 1:",
        "                try:",
        "                    top_n = int(cmd_parts[1])",
        "                    print(f\"Now showing top {top_n} results\")",
        "                except ValueError:",
        "                    print(\"Usage: /top N (where N is a number)\")",
        "            else:",
        "                print(f\"Unknown command: {cmd}\")",
        "        else:",
        "            answer = qa.answer(question, top_n=top_n, verbose=show_verbose)",
        "            print(answer)",
        "",
        "",
        "def main():",
        "    parser = argparse.ArgumentParser(",
        "        description='Ask questions about the codebase',",
        "        formatter_class=argparse.RawDescriptionHelpFormatter,",
        "        epilog=\"\"\"",
        "Examples:",
        "  %(prog)s                              # Interactive mode",
        "  %(prog)s \"How does PageRank work?\"    # Single question",
        "  %(prog)s \"where is TF-IDF computed\" --verbose",
        "  %(prog)s \"authentication\" --top 5     # More results",
        "        \"\"\"",
        "    )",
        "    parser.add_argument('question', nargs='?', help='Question to ask')",
        "    parser.add_argument('--corpus', '-c', default='corpus_dev.pkl',",
        "                        help='Corpus file path (default: corpus_dev.pkl)')",
        "    parser.add_argument('--top', '-n', type=int, default=3,",
        "                        help='Number of passages to retrieve (default: 3)')",
        "    parser.add_argument('--verbose', '-v', action='store_true',",
        "                        help='Show full passage content')",
        "    parser.add_argument('--no-sources', action='store_true',",
        "                        help='Hide source list')",
        "    args = parser.parse_args()",
        "",
        "    base_path = Path(__file__).parent.parent",
        "    corpus_path = base_path / args.corpus",
        "",
        "    # Check if corpus exists",
        "    if not corpus_path.exists():",
        "        print(f\"Error: Corpus file not found: {corpus_path}\")",
        "        print(\"Run 'python scripts/index_codebase.py' first to create it.\")",
        "        sys.exit(1)",
        "",
        "    # Load corpus",
        "    print(f\"Loading corpus from {corpus_path}...\")",
        "    processor = CorticalTextProcessor.load(str(corpus_path))",
        "    print(f\"Loaded {len(processor.documents)} documents\")",
        "",
        "    qa = CodebaseQA(processor)",
        "",
        "    if args.question:",
        "        # Single question mode",
        "        answer = qa.answer(",
        "            args.question,",
        "            top_n=args.top,",
        "            show_sources=not args.no_sources,",
        "            verbose=args.verbose",
        "        )",
        "        print(answer)",
        "    else:",
        "        # Interactive mode",
        "        interactive_mode(qa, verbose=args.verbose)",
        "",
        "",
        "if __name__ == '__main__':",
        "    main()"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "scripts/index_codebase.py",
      "function": "import time",
      "start_line": 27,
      "lines_added": [
        "from cortical.tokenizer import Tokenizer",
        "def create_code_processor() -> CorticalTextProcessor:",
        "    \"\"\"",
        "    Create a CorticalTextProcessor configured for code indexing.",
        "",
        "    Enables split_identifiers so that:",
        "    - getUserCredentials â†’ ['getusercredentials', 'get', 'user', 'credentials']",
        "    - auth_service â†’ ['auth_service', 'auth', 'service']",
        "",
        "    This dramatically improves code search - \"auth\" will find AuthService,",
        "    authenticate, user_auth, etc.",
        "    \"\"\"",
        "    tokenizer = Tokenizer(split_identifiers=True)",
        "    return CorticalTextProcessor(tokenizer=tokenizer)",
        "",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "from contextlib import contextmanager",
        "from dataclasses import dataclass, field",
        "from datetime import datetime",
        "from pathlib import Path",
        "from typing import Dict, List, Optional, Tuple, Any",
        "",
        "# Add parent directory to path for imports",
        "sys.path.insert(0, str(Path(__file__).parent.parent))",
        "",
        "from cortical.processor import CorticalTextProcessor"
      ],
      "context_after": [
        "from cortical.chunk_index import (",
        "    ChunkWriter, ChunkLoader, ChunkCompactor,",
        "    get_changes_from_manifest as get_chunk_changes",
        ")",
        "",
        "",
        "# Manifest file version for compatibility checking",
        "MANIFEST_VERSION = \"1.0\"",
        "",
        "# Default timeout in seconds (0 = no timeout)",
        "DEFAULT_TIMEOUT = 300  # 5 minutes",
        "",
        "",
        "# =============================================================================",
        "# Progress Tracking System",
        "# ============================================================================="
      ],
      "change_type": "add"
    },
    {
      "file": "scripts/index_codebase.py",
      "function": "def index_with_chunks(",
      "start_line": 959,
      "lines_added": [
        "        processor = create_code_processor()"
      ],
      "lines_removed": [
        "        processor = CorticalTextProcessor()"
      ],
      "context_before": [
        "    tracker.end_phase(\"Loading documents from chunks\")",
        "",
        "    # Check if we can use cached pkl",
        "    cache_valid = loader.is_cache_valid(str(output_path))",
        "    if cache_valid and not (added or modified or deleted):",
        "        tracker.log(\"\\nCache is valid, loading from pkl...\")",
        "        processor = CorticalTextProcessor.load(str(output_path))",
        "    else:",
        "        # Build processor from documents (with metadata)",
        "        tracker.start_phase(\"Building processor from chunks\")"
      ],
      "context_after": [
        "        documents = [",
        "            (doc_id, content, all_metadata.get(doc_id))",
        "            for doc_id, content in all_docs.items()",
        "        ]",
        "        processor.add_documents_batch(documents, recompute='none', verbose=False)",
        "        tracker.log(f\"  Added {len(documents)} documents\")",
        "        tracker.end_phase(\"Building processor from chunks\")",
        "",
        "        # Compute analysis",
        "        fast_mode = not args.full_analysis"
      ],
      "change_type": "modify"
    },
    {
      "file": "scripts/index_codebase.py",
      "function": "def run_indexer(",
      "start_line": 1193,
      "lines_added": [
        "            processor = create_code_processor()",
        "        processor = create_code_processor()"
      ],
      "lines_removed": [
        "            processor = CorticalTextProcessor()",
        "        processor = CorticalTextProcessor()"
      ],
      "context_before": [
        "    if args.incremental and manifest and output_path.exists():",
        "        tracker.start_phase(\"Loading existing corpus\")",
        "        try:",
        "            processor = CorticalTextProcessor.load(str(output_path))",
        "            tracker.log(f\"  Loaded {len(processor.documents)} documents\")",
        "            tracker.end_phase(\"Loading existing corpus\")",
        "        except Exception as e:",
        "            tracker.warn(f\"Error loading corpus: {e}\")",
        "            tracker.log(\"  Falling back to full rebuild...\")",
        "            tracker.end_phase(\"Loading existing corpus\", status=\"failed\")"
      ],
      "context_after": [
        "            added, modified, deleted = all_files, [], []",
        "    else:",
        "        # Full index - treat all files as \"added\"",
        "        added = all_files",
        "        modified = []",
        "        deleted = []",
        "",
        "    # Perform indexing",
        "    if args.incremental and manifest:",
        "        incremental_index(processor, added, modified, deleted, base_path, tracker)",
        "    else:",
        "        full_index(processor, all_files, base_path, tracker)"
      ],
      "change_type": "modify"
    },
    {
      "file": "scripts/search_codebase.py",
      "function": "def get_doc_type_label(doc_id: str) -> str:",
      "start_line": 43,
      "lines_added": [
        "def find_similar_code(",
        "    processor: CorticalTextProcessor,",
        "    target: str,",
        "    top_n: int = 5,",
        "    chunk_size: int = 400",
        ") -> list:",
        "    \"\"\"",
        "    Find code similar to a target reference or text.",
        "",
        "    Args:",
        "        processor: CorticalTextProcessor instance",
        "        target: Either a file:line reference (e.g., \"cortical/processor.py:100\")",
        "                or raw text to compare against",
        "        top_n: Number of similar results to return",
        "        chunk_size: Size of text chunks to compare",
        "",
        "    Returns:",
        "        List of result dicts with similarity scores",
        "    \"\"\"",
        "    # Determine if target is a file reference or raw text",
        "    if ':' in target and not target.startswith('http'):",
        "        # Parse file:line reference",
        "        parts = target.split(':')",
        "        file_path = parts[0]",
        "        try:",
        "            line_num = int(parts[1]) if len(parts) > 1 else 1",
        "        except ValueError:",
        "            line_num = 1",
        "",
        "        # Get content from indexed document",
        "        doc_content = processor.documents.get(file_path, '')",
        "        if not doc_content:",
        "            # Try without extension",
        "            for doc_id in processor.documents:",
        "                if doc_id.endswith(file_path) or file_path.endswith(doc_id):",
        "                    doc_content = processor.documents[doc_id]",
        "                    file_path = doc_id",
        "                    break",
        "",
        "        if not doc_content:",
        "            return []",
        "",
        "        # Extract passage around the line",
        "        lines = doc_content.split('\\n')",
        "        start_line = max(0, line_num - 1)",
        "        end_line = min(len(lines), start_line + 20)  # ~20 lines of context",
        "        target_text = '\\n'.join(lines[start_line:end_line])",
        "    else:",
        "        target_text = target",
        "",
        "    if not target_text.strip():",
        "        return []",
        "",
        "    # Get fingerprint of target",
        "    target_fp = processor.get_fingerprint(target_text, top_n=20)",
        "",
        "    # Compare against all documents",
        "    results = []",
        "    for doc_id, doc_content in processor.documents.items():",
        "        # Skip the source document if we're searching from a file reference",
        "        if ':' in target and doc_id in target:",
        "            continue",
        "",
        "        # Chunk the document and compare each chunk",
        "        for start in range(0, len(doc_content), chunk_size // 2):",
        "            end = min(start + chunk_size, len(doc_content))",
        "            chunk = doc_content[start:end]",
        "",
        "            if len(chunk.strip()) < 50:  # Skip very short chunks",
        "                continue",
        "",
        "            chunk_fp = processor.get_fingerprint(chunk, top_n=20)",
        "            comparison = processor.compare_fingerprints(target_fp, chunk_fp)",
        "",
        "            similarity = comparison.get('overall_similarity', 0)",
        "            if similarity > 0.1:  # Only include somewhat similar results",
        "                line_num = find_line_number(doc_content, start)",
        "                results.append({",
        "                    'file': doc_id,",
        "                    'line': line_num,",
        "                    'passage': chunk,",
        "                    'score': similarity,",
        "                    'reference': f\"{doc_id}:{line_num}\",",
        "                    'doc_type': get_doc_type_label(doc_id),",
        "                    'shared_terms': list(comparison.get('shared_terms', []))[:5]",
        "                })",
        "",
        "    # Sort by similarity and return top results",
        "    results.sort(key=lambda x: x['score'], reverse=True)",
        "    return results[:top_n]",
        "",
        "",
        "def display_similar_results(results: list, target: str, verbose: bool = False):",
        "    \"\"\"Display similar code results.\"\"\"",
        "    if not results:",
        "        print(f\"\\nNo similar code found for: {target}\")",
        "        return",
        "",
        "    print(f\"\\n{'â”€' * 60}\")",
        "    print(f\"Code similar to: {target}\")",
        "    print(f\"{'â”€' * 60}\\n\")",
        "",
        "    for i, result in enumerate(results, 1):",
        "        print(f\"[{i}] [{result['doc_type']}] {result['reference']}\")",
        "        print(f\"    Similarity: {result['score']:.1%}\")",
        "        if result.get('shared_terms'):",
        "            print(f\"    Shared: {', '.join(result['shared_terms'])}\")",
        "        print(\"â”€\" * 50)",
        "",
        "        if verbose:",
        "            print(format_passage(result['passage']))",
        "        else:",
        "            lines = result['passage'].split('\\n')[:5]",
        "            for line in lines:",
        "                if len(line) > 70:",
        "                    line = line[:67] + '...'",
        "                print(f\"  {line}\")",
        "            if len(result['passage'].split('\\n')) > 5:",
        "                print(f\"  ...\")",
        "        print()",
        "",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "    \"\"\"Get a display label for document type.\"\"\"",
        "    if doc_id.endswith('.md'):",
        "        if doc_id.startswith('docs/'):",
        "            return 'DOCS'",
        "        return 'DOC'",
        "    elif doc_id.startswith('tests/'):",
        "        return 'TEST'",
        "    return 'CODE'",
        "",
        ""
      ],
      "context_after": [
        "def search_codebase(",
        "    processor: CorticalTextProcessor,",
        "    query: str,",
        "    top_n: int = 5,",
        "    chunk_size: int = 400,",
        "    fast: bool = False,",
        "    prefer_docs: bool = False,",
        "    no_boost: bool = False",
        ") -> list:",
        "    \"\"\""
      ],
      "change_type": "add"
    },
    {
      "file": "scripts/search_codebase.py",
      "function": "def display_results(results: list, verbose: bool = False, show_doc_type: bool =",
      "start_line": 162,
      "lines_added": [
        "def expand_query_display(processor: CorticalTextProcessor, query: str, use_code: bool = False):",
        "    if use_code:",
        "        expanded = processor.expand_query_for_code(query, max_expansions=15)",
        "        print(\"\\nQuery expansion (code-aware):\")",
        "        print(\"  (includes programming synonyms: get/fetch/load, etc.)\")",
        "    else:",
        "        expanded = processor.expand_query(query, max_expansions=10)",
        "        print(\"\\nQuery expansion:\")",
        "    print(\"  /code <query>    - Show code-aware expansion (programming synonyms)\")"
      ],
      "lines_removed": [
        "def expand_query_display(processor: CorticalTextProcessor, query: str):",
        "    expanded = processor.expand_query(query, max_expansions=10)",
        "    print(\"\\nQuery expansion:\")"
      ],
      "context_before": [
        "            lines = result['passage'].split('\\n')[:5]",
        "            for line in lines:",
        "                if len(line) > 76:",
        "                    line = line[:73] + '...'",
        "                print(f\"  {line}\")",
        "            if len(result['passage'].split('\\n')) > 5:",
        "                print(f\"  ... ({len(result['passage'].split(chr(10))) - 5} more lines)\")",
        "        print()",
        "",
        ""
      ],
      "context_after": [
        "    \"\"\"Show expanded query terms.\"\"\"",
        "    for term, weight in sorted(expanded.items(), key=lambda x: -x[1])[:10]:",
        "        print(f\"  {term}: {weight:.3f}\")",
        "",
        "",
        "def interactive_mode(processor: CorticalTextProcessor):",
        "    \"\"\"Run interactive search mode.\"\"\"",
        "    print(\"\\nInteractive Search Mode\")",
        "    print(\"=\" * 40)",
        "    print(\"Commands:\")",
        "    print(\"  /expand <query>  - Show query expansion\")",
        "    print(\"  /concepts        - List concept clusters\")",
        "    print(\"  /stats           - Show corpus statistics\")",
        "    print(\"  /help            - Show this help\")",
        "    print(\"  /quit            - Exit\")",
        "    print()",
        "",
        "    while True:",
        "        try:",
        "            query = input(\"Search> \").strip()",
        "        except (EOFError, KeyboardInterrupt):"
      ],
      "change_type": "modify"
    },
    {
      "file": "scripts/search_codebase.py",
      "function": "def interactive_mode(processor: CorticalTextProcessor):",
      "start_line": 200,
      "lines_added": [
        "                print(\"Commands: /expand, /code, /concepts, /stats, /quit\")",
        "            elif cmd == '/code' and len(cmd_parts) > 1:",
        "                expand_query_display(processor, cmd_parts[1], use_code=True)"
      ],
      "lines_removed": [
        "                print(\"Commands: /expand, /concepts, /stats, /quit\")"
      ],
      "context_before": [
        "            continue",
        "",
        "        if query.startswith('/'):",
        "            cmd_parts = query.split(maxsplit=1)",
        "            cmd = cmd_parts[0].lower()",
        "",
        "            if cmd == '/quit' or cmd == '/exit':",
        "                print(\"Goodbye!\")",
        "                break",
        "            elif cmd == '/help':"
      ],
      "context_after": [
        "            elif cmd == '/stats':",
        "                print(f\"\\nCorpus Statistics:\")",
        "                print(f\"  Documents: {len(processor.documents)}\")",
        "                print(f\"  Tokens: {processor.layers[0].column_count()}\")",
        "                print(f\"  Bigrams: {processor.layers[1].column_count()}\")",
        "                print(f\"  Concepts: {processor.layers[2].column_count()}\")",
        "                print(f\"  Relations: {len(processor.semantic_relations)}\")",
        "            elif cmd == '/expand' and len(cmd_parts) > 1:",
        "                expand_query_display(processor, cmd_parts[1])",
        "            elif cmd == '/concepts':",
        "                layer2 = processor.layers[2]",
        "                concepts = list(layer2.minicolumns.values())[:10]",
        "                print(f\"\\nTop concepts ({layer2.column_count()} total):\")",
        "                for c in concepts:",
        "                    print(f\"  {c.content[:50]}\")",
        "            else:",
        "                print(f\"Unknown command: {cmd}\")",
        "        else:",
        "            results = search_codebase(processor, query, top_n=5)"
      ],
      "change_type": "modify"
    },
    {
      "file": "scripts/search_codebase.py",
      "function": "def interactive_mode(processor: CorticalTextProcessor):",
      "start_line": 232,
      "lines_added": [
        "  %(prog)s \"fetch data\" --code            # Code-aware (also finds get/load/retrieve)",
        "  %(prog)s \"auth\" --code --expand         # Show programming synonyms",
        "  %(prog)s --similar-to cortical/processor.py:100  # Find similar code",
        "  %(prog)s -s \"def compute_pagerank\"      # Find code similar to text",
        "    parser.add_argument('--code', action='store_true',",
        "                        help='Use code-aware query expansion (getâ†’fetch/load/retrieve)')",
        "    parser.add_argument('--similar-to', '-s', metavar='TARGET',",
        "                        help='Find code similar to file:line reference or text')",
        "    elif args.similar_to:",
        "        # Find similar code mode",
        "        results = find_similar_code(",
        "            processor,",
        "            args.similar_to,",
        "            top_n=args.top",
        "        )",
        "        display_similar_results(results, args.similar_to, verbose=args.verbose)",
        "            expand_query_display(processor, args.query, use_code=args.code)"
      ],
      "lines_removed": [
        "            expand_query_display(processor, args.query)"
      ],
      "context_before": [
        "",
        "def main():",
        "    parser = argparse.ArgumentParser(",
        "        description='Search the indexed codebase',",
        "        epilog=\"\"\"",
        "Examples:",
        "  %(prog)s \"PageRank algorithm\"           # Search with auto-boost",
        "  %(prog)s \"what is PageRank\" --prefer-docs  # Always boost docs",
        "  %(prog)s \"compute pagerank\" --no-boost  # Disable boosting",
        "  %(prog)s \"architecture\" --fast          # Fast document-level search"
      ],
      "context_after": [
        "        \"\"\"",
        "    )",
        "    parser.add_argument('query', nargs='?', help='Search query')",
        "    parser.add_argument('--corpus', '-c', default='corpus_dev.pkl',",
        "                        help='Corpus file path (default: corpus_dev.pkl)')",
        "    parser.add_argument('--top', '-n', type=int, default=5,",
        "                        help='Number of results (default: 5)')",
        "    parser.add_argument('--verbose', '-v', action='store_true',",
        "                        help='Show full passage text')",
        "    parser.add_argument('--expand', '-e', action='store_true',",
        "                        help='Show query expansion')",
        "    parser.add_argument('--interactive', '-i', action='store_true',",
        "                        help='Interactive search mode')",
        "    parser.add_argument('--fast', '-f', action='store_true',",
        "                        help='Fast search mode (document-level, ~2-3x faster)')",
        "    parser.add_argument('--prefer-docs', '-d', action='store_true',",
        "                        help='Always boost documentation files in results')",
        "    parser.add_argument('--no-boost', action='store_true',",
        "                        help='Disable document type boosting (raw TF-IDF)')",
        "    args = parser.parse_args()",
        "",
        "    base_path = Path(__file__).parent.parent",
        "    corpus_path = base_path / args.corpus",
        "",
        "    # Check if corpus exists",
        "    if not corpus_path.exists():",
        "        print(f\"Error: Corpus file not found: {corpus_path}\")",
        "        print(\"Run 'python scripts/index_codebase.py' first to create it.\")",
        "        sys.exit(1)",
        "",
        "    # Load corpus",
        "    print(f\"Loading corpus from {corpus_path}...\")",
        "    processor = CorticalTextProcessor.load(str(corpus_path))",
        "    print(f\"Loaded {len(processor.documents)} documents\\n\")",
        "",
        "    if args.interactive:",
        "        interactive_mode(processor)",
        "    elif args.query:",
        "        if args.expand:",
        "            print()",
        "",
        "        # Show query intent detection",
        "        is_conceptual = processor.is_conceptual_query(args.query)",
        "        if not args.no_boost:",
        "            intent_str = \"conceptual\" if is_conceptual else \"implementation\"",
        "            print(f\"(Query type: {intent_str})\")",
        "",
        "        results = search_codebase(",
        "            processor,"
      ],
      "change_type": "modify"
    },
    {
      "file": "showcase.py",
      "function": null,
      "start_line": 2,
      "lines_added": [
        "import time",
        "class Timer:",
        "    \"\"\"Simple timer for measuring operation durations.\"\"\"",
        "",
        "    def __init__(self):",
        "        self.times: Dict[str, float] = {}",
        "        self._start: float = 0",
        "",
        "    def start(self, name: str):",
        "        \"\"\"Start timing an operation.\"\"\"",
        "        self._start = time.perf_counter()",
        "        self._current = name",
        "",
        "    def stop(self) -> float:",
        "        \"\"\"Stop timing and record the duration.\"\"\"",
        "        elapsed = time.perf_counter() - self._start",
        "        self.times[self._current] = elapsed",
        "        return elapsed",
        "",
        "    def get(self, name: str) -> float:",
        "        \"\"\"Get recorded time for an operation.\"\"\"",
        "        return self.times.get(name, 0)",
        "",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "Cortical Text Processor Showcase",
        "================================",
        "",
        "This showcase processes a corpus of documents, demonstrating the",
        "hierarchical analysis of relationships between concepts, documents,",
        "and ideas across diverse topics.",
        "\"\"\"",
        "",
        "import os",
        "import sys"
      ],
      "context_after": [
        "from typing import Dict, List, Tuple",
        "",
        "from cortical import CorticalTextProcessor, CorticalLayer",
        "",
        "",
        "def print_header(title: str, char: str = \"=\"):",
        "    \"\"\"Print a formatted section header.\"\"\"",
        "    width = 70",
        "    print(f\"\\n{char * width}\")",
        "    print(f\"{title:^{width}}\")",
        "    print(f\"{char * width}\\n\")",
        "",
        "",
        "def print_subheader(title: str):",
        "    \"\"\"Print a formatted subsection header.\"\"\""
      ],
      "change_type": "add"
    },
    {
      "file": "showcase.py",
      "function": "def print_subheader(title: str):",
      "start_line": 31,
      "lines_added": [
        "",
        "        self.timer = Timer()",
        "",
        "        self.demonstrate_passage_search()",
        "        self.demonstrate_code_features()",
        "",
        "",
        "",
        "",
        "",
        "        # Time document loading",
        "        self.timer.start('document_loading')",
        "",
        "        load_time = self.timer.stop()",
        "",
        "        self.timer.start('compute_all')",
        "        compute_time = self.timer.stop()",
        "",
        "",
        "        print(f\"\\nâ±  Document loading: {load_time:.2f}s\")",
        "        print(f\"â±  Compute all:      {compute_time:.2f}s\")",
        ""
      ],
      "lines_removed": [
        "    ",
        "    ",
        "        ",
        "        ",
        "        ",
        "        ",
        "        ",
        "            ",
        "        ",
        "        ",
        "        ",
        "        "
      ],
      "context_before": [
        "def render_bar(value: float, max_value: float, width: int = 30) -> str:",
        "    \"\"\"Render a text-based progress bar.\"\"\"",
        "    if max_value == 0:",
        "        return \" \" * width",
        "    filled = int((value / max_value) * width)",
        "    return \"â–ˆ\" * filled + \"â–‘\" * (width - filled)",
        "",
        "",
        "class CorticalShowcase:",
        "    \"\"\"Showcases the cortical text processor with interesting analysis.\"\"\""
      ],
      "context_after": [
        "    def __init__(self, samples_dir: str = \"samples\"):",
        "        self.samples_dir = samples_dir",
        "        self.processor = CorticalTextProcessor()",
        "        self.loaded_files = []",
        "    def run(self):",
        "        \"\"\"Run the complete demo.\"\"\"",
        "        self.print_intro()",
        "",
        "        if not self.ingest_corpus():",
        "            print(\"No documents found!\")",
        "            return",
        "",
        "        self.analyze_hierarchy()",
        "        self.discover_key_concepts()",
        "        self.analyze_tfidf()",
        "        self.find_concept_associations()",
        "        self.analyze_document_relationships()",
        "        self.demonstrate_queries()",
        "        self.demonstrate_polysemy()",
        "        self.demonstrate_gap_analysis()",
        "        self.demonstrate_embeddings()",
        "        self.print_insights()",
        "    ",
        "    def print_intro(self):",
        "        \"\"\"Print introduction.\"\"\"",
        "        print(\"\"\"",
        "    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—",
        "    â•‘                                                                      â•‘",
        "    â•‘            ðŸ§   CORTICAL TEXT PROCESSOR SHOWCASE  ðŸ§                   â•‘",
        "    â•‘                                                                      â•‘",
        "    â•‘     Mimicking how the neocortex processes and understands text       â•‘",
        "    â•‘                                                                      â•‘",
        "    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•",
        "        \"\"\")",
        "    ",
        "    def ingest_corpus(self) -> bool:",
        "        \"\"\"Ingest the document corpus from disk.\"\"\"",
        "        print_header(\"DOCUMENT INGESTION\", \"â•\")",
        "        print(f\"Loading documents from: {self.samples_dir}\")",
        "        print(\"Processing through cortical hierarchy...\")",
        "        print(\"(Like visual information flowing V1 â†’ V2 â†’ V4 â†’ IT)\\n\")",
        "        if not os.path.exists(self.samples_dir):",
        "            print(f\"  âŒ Directory not found: {self.samples_dir}\")",
        "            return False",
        "        txt_files = sorted([f for f in os.listdir(self.samples_dir) if f.endswith('.txt')])",
        "        if not txt_files:",
        "            return False",
        "        for filename in txt_files:",
        "            filepath = os.path.join(self.samples_dir, filename)",
        "            with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:",
        "                content = f.read()",
        "            doc_id = filename.replace('.txt', '')",
        "            self.processor.process_document(doc_id, content)",
        "            word_count = len(content.split())",
        "            self.loaded_files.append((doc_id, word_count))",
        "            print(f\"  ðŸ“„ {doc_id:30} ({word_count:3} words)\")",
        "        # Run all computations with hybrid strategy for better Layer 2 connectivity",
        "        print(\"\\nComputing cortical representations...\")",
        "        self.processor.compute_all(",
        "            verbose=False,",
        "            connection_strategy='hybrid',",
        "            cluster_strictness=0.5,",
        "            bridge_weight=0.3",
        "        )",
        "        layer0 = self.processor.get_layer(CorticalLayer.TOKENS)",
        "        layer1 = self.processor.get_layer(CorticalLayer.BIGRAMS)",
        "        print(f\"\\nâœ“ Processed {len(self.loaded_files)} documents\")",
        "        print(f\"âœ“ Created {layer0.column_count()} token minicolumns\")",
        "        print(f\"âœ“ Created {layer1.column_count()} bigram minicolumns\")",
        "        print(f\"âœ“ Formed {layer0.total_connections()} lateral connections\")",
        "        return True",
        "    ",
        "    def analyze_hierarchy(self):",
        "        \"\"\"Show the hierarchical structure.\"\"\"",
        "        print_header(\"HIERARCHICAL STRUCTURE\", \"â•\")",
        "        ",
        "        print(\"The cortical model has 4 layers (like visual cortex V1â†’IT):\\n\")",
        "        ",
        "        layers = [",
        "            (CorticalLayer.TOKENS, \"Token Layer (V1)\", \"Individual words\"),"
      ],
      "change_type": "modify"
    },
    {
      "file": "showcase.py",
      "function": "class CorticalShowcase:",
      "start_line": 202,
      "lines_added": [
        "                    # O(1) lookup using _id_index",
        "                    neighbor = layer0.get_by_id(neighbor_id)",
        "                    if neighbor:",
        "                        bar_len = int(min(weight, 10) * 3)",
        "                        bar = \"â”€\" * bar_len + \">\"",
        "                        print(f\"    {bar} {neighbor.content} (weight: {weight:.2f})\")"
      ],
      "lines_removed": [
        "                    # Find neighbor content",
        "                    for c in layer0.minicolumns.values():",
        "                        if c.id == neighbor_id:",
        "                            bar_len = int(min(weight, 10) * 3)",
        "                            bar = \"â”€\" * bar_len + \">\"",
        "                            print(f\"    {bar} {c.content} (weight: {weight:.2f})\")",
        "                            break"
      ],
      "context_before": [
        "        for concept in test_concepts:",
        "            col = layer0.get_minicolumn(concept)",
        "            if col and col.lateral_connections:",
        "                print_subheader(f\"ðŸ”— '{concept}' connects to:\")",
        "                ",
        "                # Get top connections",
        "                sorted_conns = sorted(col.lateral_connections.items(), ",
        "                                     key=lambda x: x[1], reverse=True)[:6]",
        "                ",
        "                for neighbor_id, weight in sorted_conns:"
      ],
      "context_after": [
        "                print()",
        "    ",
        "    def analyze_document_relationships(self):",
        "        \"\"\"Show document-level relationships.\"\"\"",
        "        print_header(\"DOCUMENT RELATIONSHIPS\", \"â•\")",
        "        ",
        "        print(\"Documents connect based on shared concepts and term overlap:\\n\")",
        "        ",
        "        layer3 = self.processor.get_layer(CorticalLayer.DOCUMENTS)",
        "        "
      ],
      "change_type": "modify"
    },
    {
      "file": "showcase.py",
      "function": "class CorticalShowcase:",
      "start_line": 242,
      "lines_added": [
        "",
        "",
        "        total_query_time = 0",
        "",
        "",
        "            # Time expansion + search",
        "            start = time.perf_counter()",
        "",
        "",
        "",
        "            elapsed = time.perf_counter() - start",
        "            total_query_time += elapsed",
        "",
        "            print(f\"    â±  {elapsed*1000:.1f}ms\")",
        "",
        "        self.timer.times['queries'] = total_query_time",
        "        print(f\"Average query time: {total_query_time/len(test_queries)*1000:.1f}ms\")",
        "",
        "    def demonstrate_passage_search(self):",
        "        \"\"\"Demonstrate passage-level retrieval for RAG applications.\"\"\"",
        "        print_header(\"PASSAGE RETRIEVAL (RAG)\", \"â•\")",
        "",
        "        print(\"Passage search retrieves specific text chunks, ideal for RAG:\")",
        "        print(\"(Retrieval-Augmented Generation - feeding context to LLMs)\\n\")",
        "",
        "        # Demonstrate with a specific query",
        "        query = \"PageRank algorithm convergence\"",
        "        print_subheader(f\"ðŸ” Query: '{query}'\")",
        "",
        "        # Time passage retrieval",
        "        self.timer.start('passage_search')",
        "",
        "        # Get passages",
        "        passages = self.processor.find_passages_for_query(",
        "            query,",
        "            top_n=3,",
        "            chunk_size=300,",
        "            overlap=50",
        "        )",
        "        passage_time = self.timer.stop()",
        "",
        "        if passages:",
        "            print(f\"\\n    Found {len(passages)} relevant passages:\\n\")",
        "",
        "            for i, (passage_text, doc_id, start, end, score) in enumerate(passages, 1):",
        "                # Calculate line number from character position",
        "                doc_content = self.processor.documents.get(doc_id, \"\")",
        "                line_num = doc_content[:start].count('\\n') + 1",
        "",
        "                print(f\"    [{i}] {doc_id}:{line_num} (score: {score:.3f})\")",
        "                print(\"    \" + \"â”€\" * 50)",
        "",
        "                # Show truncated passage",
        "                lines = passage_text.strip().split('\\n')[:4]",
        "                for line in lines:",
        "                    if len(line) > 60:",
        "                        line = line[:57] + \"...\"",
        "                    print(f\"      {line}\")",
        "                if len(passage_text.strip().split('\\n')) > 4:",
        "                    print(f\"      ...\")",
        "                print()",
        "",
        "        print(f\"    â±  Passage retrieval: {passage_time*1000:.1f}ms\")",
        "        print(\"\\n    ðŸ’¡ Use case: Feed these passages to an LLM as context\")",
        "        print(\"                 for answering questions about your corpus.\")",
        "        print()",
        ""
      ],
      "lines_removed": [
        "        ",
        "        ",
        "        ",
        "            ",
        "            ",
        "            ",
        "    "
      ],
      "context_before": [
        "            doc = sorted_docs[0]",
        "            print(f\"\\n  '{doc.content}' connects to:\")",
        "            ",
        "            related = self.processor.find_related_documents(doc.content)[:5]",
        "            for related_doc, weight in related:",
        "                print(f\"    â†’ {related_doc} (similarity: {weight:.3f})\")",
        "    ",
        "    def demonstrate_queries(self):",
        "        \"\"\"Demonstrate query capability with expansion.\"\"\"",
        "        print_header(\"QUERY DEMONSTRATION\", \"â•\")"
      ],
      "context_after": [
        "        print(\"Query expansion adds semantically related terms for better recall:\\n\")",
        "        test_queries = [\"neural networks\", \"fermentation\", \"distributed systems\"]",
        "        for query in test_queries:",
        "            print_subheader(f\"ðŸ” Query: '{query}'\")",
        "            # Show expansion",
        "            expanded = self.processor.expand_query(query, max_expansions=6)",
        "            original = set(self.processor.tokenizer.tokenize(query))",
        "            new_terms = [t for t in expanded.keys() if t not in original]",
        "            if new_terms:",
        "                print(f\"    Expanded with: {', '.join(new_terms[:6])}\")",
        "            # Find documents",
        "            results = self.processor.find_documents_for_query(query, top_n=3)",
        "            print(f\"\\n    Top documents:\")",
        "            for doc_id, score in results:",
        "                print(f\"      â€¢ {doc_id} (score: {score:.3f})\")",
        "            print()",
        "    def demonstrate_polysemy(self):",
        "        \"\"\"Demonstrate polysemy - same word, different meanings.\"\"\"",
        "        print_header(\"POLYSEMY DEMONSTRATION\", \"â•\")",
        "",
        "        print(\"Polysemy occurs when the same word has multiple meanings.\")",
        "        print(\"This affects retrieval when query terms are ambiguous.\\n\")",
        "",
        "        # Query for \"candle sticks\"",
        "        query = \"candle sticks\"",
        "        print_subheader(f\"ðŸ” Query: '{query}'\")"
      ],
      "change_type": "modify"
    },
    {
      "file": "showcase.py",
      "function": "class CorticalShowcase:",
      "start_line": 306,
      "lines_added": [
        "    def demonstrate_code_features(self):",
        "        \"\"\"Demonstrate code-specific search capabilities.\"\"\"",
        "        print_header(\"CODE SEARCH FEATURES\", \"â•\")",
        "",
        "        print(\"Features optimized for searching code and technical content:\\n\")",
        "",
        "        # 1. Query intent detection",
        "        print_subheader(\"ðŸŽ¯ Query Intent Detection\")",
        "        print(\"    The system detects whether queries are conceptual or implementation-focused:\\n\")",
        "",
        "        test_queries = [",
        "            (\"what is PageRank\", True),",
        "            (\"compute pagerank damping\", False),",
        "            (\"how does TF-IDF work\", True),",
        "            (\"find documents tfidf\", False),",
        "        ]",
        "",
        "        for query, expected_conceptual in test_queries:",
        "            is_conceptual = self.processor.is_conceptual_query(query)",
        "            intent = \"conceptual\" if is_conceptual else \"implementation\"",
        "            marker = \"ðŸ“–\" if is_conceptual else \"ðŸ’»\"",
        "            print(f\"    {marker} \\\"{query}\\\" â†’ {intent}\")",
        "",
        "        print(\"\\n    ðŸ’¡ Use case: Boost documentation for conceptual queries,\")",
        "        print(\"                 boost code files for implementation queries.\")",
        "",
        "        # 2. Code-aware query expansion",
        "        print_subheader(\"\\nðŸ”§ Code-Aware Query Expansion\")",
        "        print(\"    Programming synonyms expand queries for better code search:\\n\")",
        "",
        "        code_queries = [\"fetch data\", \"get results\", \"process input\"]",
        "",
        "        for query in code_queries:",
        "            # Regular expansion",
        "            regular = self.processor.expand_query(query, max_expansions=5)",
        "            # Code-aware expansion",
        "            code_exp = self.processor.expand_query_for_code(query, max_expansions=8)",
        "",
        "            # Find terms only in code expansion",
        "            regular_terms = set(regular.keys())",
        "            code_terms = set(code_exp.keys())",
        "            new_terms = code_terms - regular_terms",
        "",
        "            print(f\"    Query: \\\"{query}\\\"\")",
        "            if new_terms:",
        "                new_list = sorted(new_terms, key=lambda t: -code_exp.get(t, 0))[:4]",
        "                print(f\"      + Code terms: {', '.join(new_list)}\")",
        "            else:",
        "                print(f\"      (corpus lacks programming synonyms for this query)\")",
        "            print()",
        "",
        "        # 3. Semantic fingerprinting",
        "        print_subheader(\"ðŸ” Semantic Fingerprinting\")",
        "        print(\"    Compare text similarity using semantic fingerprints:\\n\")",
        "",
        "        # Get two related documents",
        "        if len(self.loaded_files) >= 2:",
        "            doc1_id = \"neural_pagerank\" if \"neural_pagerank\" in self.processor.documents else self.loaded_files[0][0]",
        "            doc2_id = \"pagerank_fundamentals\" if \"pagerank_fundamentals\" in self.processor.documents else self.loaded_files[1][0]",
        "",
        "            doc1_content = self.processor.documents.get(doc1_id, \"\")[:500]",
        "            doc2_content = self.processor.documents.get(doc2_id, \"\")[:500]",
        "",
        "            if doc1_content and doc2_content:",
        "                fp1 = self.processor.get_fingerprint(doc1_content, top_n=10)",
        "                fp2 = self.processor.get_fingerprint(doc2_content, top_n=10)",
        "",
        "                comparison = self.processor.compare_fingerprints(fp1, fp2)",
        "",
        "                print(f\"    Comparing: '{doc1_id}' vs '{doc2_id}'\")",
        "                print(f\"      Similarity: {comparison['overall_similarity']:.1%}\")",
        "                print(f\"      Shared concepts: {len(comparison.get('shared_concepts', []))}\")",
        "",
        "                if comparison['shared_terms']:",
        "                    shared = list(comparison['shared_terms'])[:5]",
        "                    print(f\"      Common terms: {', '.join(shared)}\")",
        "",
        "        print(\"\\n    ðŸ’¡ Use case: Find similar code, detect duplicates,\")",
        "        print(\"                 suggest related files when editing.\")",
        "        print()",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "            print(f\"\\n    'sticks' appears in {len(col.document_ids)} documents:\")",
        "            for doc_id in col.document_ids:",
        "                print(f\"      â€¢ {doc_id}\")",
        "",
        "        print(\"\\n    ðŸ’¡ Potential improvements:\")",
        "        print(\"      â€¢ Weight adjacent term matches higher (bigram boost)\")",
        "        print(\"      â€¢ Use document context for disambiguation\")",
        "        print(\"      â€¢ Implement word sense disambiguation\")",
        "        print()",
        ""
      ],
      "context_after": [
        "    def demonstrate_gap_analysis(self):",
        "        \"\"\"Show knowledge gap detection.\"\"\"",
        "        print_header(\"KNOWLEDGE GAP ANALYSIS\", \"â•\")",
        "        ",
        "        print(\"Detecting gaps and anomalies in the corpus:\\n\")",
        "        ",
        "        gaps = self.processor.analyze_knowledge_gaps()",
        "        ",
        "        print(f\"  Coverage Score: {gaps['coverage_score']:.1%}\")",
        "        print(f\"  Connectivity Score: {gaps['connectivity_score']:.4f}\")"
      ],
      "change_type": "add"
    },
    {
      "file": "showcase.py",
      "function": "class CorticalShowcase:",
      "start_line": 377,
      "lines_added": [
        "",
        "        # Performance summary",
        "        print(\"\\nâ±  PERFORMANCE SUMMARY\\n\")",
        "        if 'document_loading' in self.timer.times:",
        "            print(f\"  Document loading:    {self.timer.get('document_loading'):.2f}s\")",
        "        if 'compute_all' in self.timer.times:",
        "            print(f\"  Compute all:         {self.timer.get('compute_all'):.2f}s\")",
        "        if 'queries' in self.timer.times:",
        "            avg_query = self.timer.get('queries') / 3 * 1000  # 3 queries",
        "            print(f\"  Avg query time:      {avg_query:.1f}ms\")",
        "        if 'passage_search' in self.timer.times:",
        "            print(f\"  Passage retrieval:   {self.timer.get('passage_search')*1000:.1f}ms\")",
        "",
        "        print(\"  âœ“ Retrieved passages for RAG applications\")",
        "        print(\"  âœ“ Demonstrated code search features\")"
      ],
      "lines_removed": [
        "        "
      ],
      "context_before": [
        "        print(f\"  Total connections:       {layer0.total_connections():,}\")",
        "        ",
        "        # Find most central token",
        "        top_token = max(layer0.minicolumns.values(), key=lambda c: c.pagerank)",
        "        print(f\"\\n  Most central concept: '{top_token.content}'\")",
        "        ",
        "        # Find most connected document",
        "        if layer3.column_count() > 0:",
        "            top_doc = max(layer3.minicolumns.values(), key=lambda c: c.connection_count())",
        "            print(f\"  Most connected document: '{top_doc.content}'\")"
      ],
      "context_after": [
        "        print(\"\\n\" + \"â•\" * 70)",
        "        print(\"Demo complete! The cortical text processor successfully:\")",
        "        print(\"  âœ“ Built hierarchical representations (Layers 0-3)\")",
        "        print(\"  âœ“ Discovered key concepts via PageRank\")",
        "        print(\"  âœ“ Computed TF-IDF for discriminative analysis\")",
        "        print(\"  âœ“ Found associations through lateral connections\")",
        "        print(\"  âœ“ Identified document relationships\")",
        "        print(\"  âœ“ Detected knowledge gaps and anomalies\")",
        "        print(\"  âœ“ Computed graph embeddings\")",
        "        print(\"  âœ“ Enabled semantic queries with expansion\")",
        "        print(\"â•\" * 70 + \"\\n\")",
        "",
        "",
        "if __name__ == \"__main__\":",
        "    showcase = CorticalShowcase(samples_dir=\"samples\")",
        "    showcase.run()"
      ],
      "change_type": "modify"
    },
    {
      "file": "tests/test_ask_codebase.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "\"\"\"",
        "Tests for scripts/ask_codebase.py - CodebaseQA class and utilities.",
        "\"\"\"",
        "",
        "import unittest",
        "import sys",
        "from pathlib import Path",
        "",
        "# Add parent and scripts directories to path",
        "sys.path.insert(0, str(Path(__file__).parent.parent))",
        "sys.path.insert(0, str(Path(__file__).parent.parent / 'scripts'))",
        "",
        "from cortical.processor import CorticalTextProcessor",
        "from ask_codebase import (",
        "    CodebaseQA,",
        "    find_line_number,",
        "    format_reference,",
        "    get_doc_type_emoji",
        ")",
        "",
        "",
        "class TestUtilityFunctions(unittest.TestCase):",
        "    \"\"\"Tests for utility functions.\"\"\"",
        "",
        "    def test_find_line_number_start(self):",
        "        \"\"\"Test line number at start of document.\"\"\"",
        "        content = \"line1\\nline2\\nline3\"",
        "        self.assertEqual(find_line_number(content, 0), 1)",
        "",
        "    def test_find_line_number_second_line(self):",
        "        \"\"\"Test line number for second line.\"\"\"",
        "        content = \"line1\\nline2\\nline3\"",
        "        self.assertEqual(find_line_number(content, 6), 2)",
        "",
        "    def test_find_line_number_third_line(self):",
        "        \"\"\"Test line number for third line.\"\"\"",
        "        content = \"line1\\nline2\\nline3\"",
        "        self.assertEqual(find_line_number(content, 12), 3)",
        "",
        "    def test_format_reference(self):",
        "        \"\"\"Test reference formatting.\"\"\"",
        "        ref = format_reference(\"cortical/processor.py\", 42)",
        "        self.assertEqual(ref, \"cortical/processor.py:42\")",
        "",
        "    def test_get_doc_type_emoji_markdown(self):",
        "        \"\"\"Test emoji for markdown files.\"\"\"",
        "        self.assertEqual(get_doc_type_emoji(\"README.md\"), \"ðŸ“–\")",
        "        self.assertEqual(get_doc_type_emoji(\"docs/guide.md\"), \"ðŸ“–\")",
        "",
        "    def test_get_doc_type_emoji_test(self):",
        "        \"\"\"Test emoji for test files.\"\"\"",
        "        self.assertEqual(get_doc_type_emoji(\"tests/test_processor.py\"), \"ðŸ§ª\")",
        "        self.assertEqual(get_doc_type_emoji(\"tests/test_analysis.py\"), \"ðŸ§ª\")",
        "",
        "    def test_get_doc_type_emoji_code(self):",
        "        \"\"\"Test emoji for code files.\"\"\"",
        "        self.assertEqual(get_doc_type_emoji(\"cortical/processor.py\"), \"ðŸ’»\")",
        "        self.assertEqual(get_doc_type_emoji(\"scripts/index.py\"), \"ðŸ’»\")",
        "",
        "",
        "class TestCodebaseQA(unittest.TestCase):",
        "    \"\"\"Tests for the CodebaseQA class.\"\"\"",
        "",
        "    @classmethod",
        "    def setUpClass(cls):",
        "        \"\"\"Set up processor with test documents.\"\"\"",
        "        cls.processor = CorticalTextProcessor()",
        "",
        "        # Add test documents",
        "        cls.processor.process_document(",
        "            \"processor.py\",",
        "            \"\"\"",
        "            The CorticalTextProcessor is the main API for text analysis.",
        "            It uses PageRank for term importance and TF-IDF for relevance.",
        "            Query expansion adds related terms to improve recall.",
        "            \"\"\"",
        "        )",
        "        cls.processor.process_document(",
        "            \"README.md\",",
        "            \"\"\"",
        "            # Cortical Text Processor",
        "",
        "            A hierarchical text analysis library inspired by visual cortex.",
        "",
        "            ## Features",
        "            - PageRank for centrality",
        "            - TF-IDF for relevance",
        "            - Query expansion",
        "            \"\"\"",
        "        )",
        "        cls.processor.process_document(",
        "            \"tests/test_processor.py\",",
        "            \"\"\"",
        "            import unittest",
        "",
        "            class TestProcessor(unittest.TestCase):",
        "                def test_pagerank(self):",
        "                    processor = CorticalTextProcessor()",
        "                    processor.compute_all()",
        "            \"\"\"",
        "        )",
        "",
        "        cls.processor.compute_all()",
        "        cls.qa = CodebaseQA(cls.processor)",
        "",
        "    def test_find_relevant_passages(self):",
        "        \"\"\"Test finding relevant passages.\"\"\"",
        "        passages = self.qa.find_relevant_passages(\"PageRank algorithm\", top_n=2)",
        "",
        "        self.assertIsInstance(passages, list)",
        "        self.assertGreater(len(passages), 0)",
        "",
        "        # Each passage should be (text, reference, line_num, score)",
        "        passage = passages[0]",
        "        self.assertEqual(len(passage), 4)",
        "        self.assertIsInstance(passage[0], str)  # text",
        "        self.assertIsInstance(passage[1], str)  # reference",
        "        self.assertIsInstance(passage[2], int)  # line_num",
        "        self.assertIsInstance(passage[3], float)  # score",
        "",
        "    def test_find_relevant_passages_empty_query(self):",
        "        \"\"\"Test with empty query.\"\"\"",
        "        passages = self.qa.find_relevant_passages(\"\", top_n=2)",
        "        self.assertIsInstance(passages, list)",
        "",
        "    def test_format_answer_with_passages(self):",
        "        \"\"\"Test answer formatting with passages.\"\"\"",
        "        passages = self.qa.find_relevant_passages(\"PageRank\", top_n=2)",
        "        answer = self.qa.format_answer(\"What is PageRank?\", passages)",
        "",
        "        self.assertIsInstance(answer, str)",
        "        self.assertIn(\"PageRank\", answer)",
        "        self.assertIn(\"Relevant Context\", answer)",
        "",
        "    def test_format_answer_empty_passages(self):",
        "        \"\"\"Test answer formatting with no passages.\"\"\"",
        "        answer = self.qa.format_answer(\"What is XYZ123?\", [])",
        "",
        "        self.assertIn(\"No relevant passages found\", answer)",
        "",
        "    def test_format_answer_shows_sources(self):",
        "        \"\"\"Test that sources are shown.\"\"\"",
        "        passages = self.qa.find_relevant_passages(\"PageRank\", top_n=2)",
        "        answer = self.qa.format_answer(\"What is PageRank?\", passages, show_sources=True)",
        "",
        "        self.assertIn(\"Sources\", answer)",
        "",
        "    def test_format_answer_hides_sources(self):",
        "        \"\"\"Test that sources can be hidden.\"\"\"",
        "        passages = self.qa.find_relevant_passages(\"PageRank\", top_n=2)",
        "        answer = self.qa.format_answer(\"What is PageRank?\", passages, show_sources=False)",
        "",
        "        self.assertNotIn(\"Sources:\", answer)",
        "",
        "    def test_answer_method(self):",
        "        \"\"\"Test the main answer method.\"\"\"",
        "        answer = self.qa.answer(\"How does query expansion work?\")",
        "",
        "        self.assertIsInstance(answer, str)",
        "        self.assertGreater(len(answer), 0)",
        "",
        "    def test_answer_detects_conceptual_query(self):",
        "        \"\"\"Test that conceptual queries are detected.\"\"\"",
        "        answer = self.qa.answer(\"What is PageRank?\")",
        "        self.assertIn(\"conceptual\", answer.lower())",
        "",
        "    def test_answer_detects_implementation_query(self):",
        "        \"\"\"Test that implementation queries are detected.\"\"\"",
        "        answer = self.qa.answer(\"compute pagerank damping factor\")",
        "        self.assertIn(\"implementation\", answer.lower())",
        "",
        "",
        "class TestCodebaseQAEmpty(unittest.TestCase):",
        "    \"\"\"Tests for CodebaseQA with empty processor.\"\"\"",
        "",
        "    def test_empty_processor(self):",
        "        \"\"\"Test with empty processor.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        qa = CodebaseQA(processor)",
        "",
        "        passages = qa.find_relevant_passages(\"anything\", top_n=2)",
        "        self.assertEqual(passages, [])",
        "",
        "    def test_empty_processor_answer(self):",
        "        \"\"\"Test answer with empty processor.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        qa = CodebaseQA(processor)",
        "",
        "        answer = qa.answer(\"How does X work?\")",
        "        self.assertIn(\"No relevant passages\", answer)",
        "",
        "",
        "if __name__ == '__main__':",
        "    unittest.main()"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "tests/test_search_codebase.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "\"\"\"",
        "Tests for scripts/search_codebase.py - search functions and utilities.",
        "\"\"\"",
        "",
        "import unittest",
        "import sys",
        "from pathlib import Path",
        "",
        "# Add parent and scripts directories to path",
        "sys.path.insert(0, str(Path(__file__).parent.parent))",
        "sys.path.insert(0, str(Path(__file__).parent.parent / 'scripts'))",
        "",
        "from cortical.processor import CorticalTextProcessor",
        "from search_codebase import (",
        "    find_line_number,",
        "    format_passage,",
        "    get_doc_type_label,",
        "    search_codebase,",
        "    find_similar_code",
        ")",
        "",
        "",
        "class TestUtilityFunctions(unittest.TestCase):",
        "    \"\"\"Tests for utility functions.\"\"\"",
        "",
        "    def test_find_line_number_start(self):",
        "        \"\"\"Test line number at start of document.\"\"\"",
        "        content = \"line1\\nline2\\nline3\"",
        "        self.assertEqual(find_line_number(content, 0), 1)",
        "",
        "    def test_find_line_number_second_line(self):",
        "        \"\"\"Test line number for second line.\"\"\"",
        "        content = \"line1\\nline2\\nline3\"",
        "        self.assertEqual(find_line_number(content, 6), 2)",
        "",
        "    def test_find_line_number_third_line(self):",
        "        \"\"\"Test line number for third line.\"\"\"",
        "        content = \"line1\\nline2\\nline3\"",
        "        self.assertEqual(find_line_number(content, 12), 3)",
        "",
        "    def test_format_passage_short(self):",
        "        \"\"\"Test formatting a short passage.\"\"\"",
        "        passage = \"Line 1\\nLine 2\\nLine 3\"",
        "        result = format_passage(passage)",
        "        self.assertIn(\"Line 1\", result)",
        "        self.assertIn(\"Line 2\", result)",
        "",
        "    def test_format_passage_truncates_long_lines(self):",
        "        \"\"\"Test that long lines are truncated.\"\"\"",
        "        long_line = \"x\" * 100",
        "        passage = long_line",
        "        result = format_passage(passage, max_width=50)",
        "        self.assertIn(\"...\", result)",
        "        self.assertLessEqual(len(result), 50)",
        "",
        "    def test_format_passage_limits_lines(self):",
        "        \"\"\"Test that many lines are limited.\"\"\"",
        "        passage = \"\\n\".join([f\"Line {i}\" for i in range(20)])",
        "        result = format_passage(passage)",
        "        self.assertIn(\"more lines\", result)",
        "",
        "    def test_get_doc_type_label_docs_markdown(self):",
        "        \"\"\"Test label for docs/ markdown files.\"\"\"",
        "        self.assertEqual(get_doc_type_label(\"docs/guide.md\"), \"DOCS\")",
        "",
        "    def test_get_doc_type_label_markdown(self):",
        "        \"\"\"Test label for other markdown files.\"\"\"",
        "        self.assertEqual(get_doc_type_label(\"README.md\"), \"DOC\")",
        "",
        "    def test_get_doc_type_label_test(self):",
        "        \"\"\"Test label for test files.\"\"\"",
        "        self.assertEqual(get_doc_type_label(\"tests/test_processor.py\"), \"TEST\")",
        "",
        "    def test_get_doc_type_label_code(self):",
        "        \"\"\"Test label for code files.\"\"\"",
        "        self.assertEqual(get_doc_type_label(\"cortical/processor.py\"), \"CODE\")",
        "",
        "",
        "class TestSearchCodebase(unittest.TestCase):",
        "    \"\"\"Tests for the search_codebase function.\"\"\"",
        "",
        "    @classmethod",
        "    def setUpClass(cls):",
        "        \"\"\"Set up processor with test documents.\"\"\"",
        "        cls.processor = CorticalTextProcessor()",
        "",
        "        # Add test documents",
        "        cls.processor.process_document(",
        "            \"processor.py\",",
        "            \"\"\"",
        "            The CorticalTextProcessor is the main API for text analysis.",
        "            It uses PageRank for term importance and TF-IDF for relevance.",
        "            Query expansion adds related terms to improve recall.",
        "            \"\"\"",
        "        )",
        "        cls.processor.process_document(",
        "            \"docs/guide.md\",",
        "            \"\"\"",
        "            # User Guide",
        "",
        "            This guide explains how PageRank works in the system.",
        "            PageRank measures the importance of terms based on connections.",
        "            \"\"\"",
        "        )",
        "        cls.processor.process_document(",
        "            \"tests/test_processor.py\",",
        "            \"\"\"",
        "            import unittest",
        "",
        "            class TestProcessor(unittest.TestCase):",
        "                def test_pagerank(self):",
        "                    processor = CorticalTextProcessor()",
        "                    processor.compute_all()",
        "            \"\"\"",
        "        )",
        "",
        "        cls.processor.compute_all()",
        "",
        "    def test_search_returns_results(self):",
        "        \"\"\"Test that search returns results.\"\"\"",
        "        results = search_codebase(self.processor, \"PageRank\", top_n=3)",
        "",
        "        self.assertIsInstance(results, list)",
        "        self.assertGreater(len(results), 0)",
        "",
        "    def test_search_result_structure(self):",
        "        \"\"\"Test result dict structure.\"\"\"",
        "        results = search_codebase(self.processor, \"PageRank\", top_n=1)",
        "",
        "        if results:",
        "            result = results[0]",
        "            self.assertIn('file', result)",
        "            self.assertIn('line', result)",
        "            self.assertIn('passage', result)",
        "            self.assertIn('score', result)",
        "            self.assertIn('reference', result)",
        "            self.assertIn('doc_type', result)",
        "",
        "    def test_search_fast_mode(self):",
        "        \"\"\"Test fast search mode.\"\"\"",
        "        results = search_codebase(",
        "            self.processor, \"PageRank\", top_n=3, fast=True",
        "        )",
        "",
        "        self.assertIsInstance(results, list)",
        "        # Fast mode always returns line 1",
        "        for result in results:",
        "            self.assertEqual(result['line'], 1)",
        "",
        "    def test_search_no_boost_mode(self):",
        "        \"\"\"Test search with boosting disabled.\"\"\"",
        "        results = search_codebase(",
        "            self.processor, \"PageRank\", top_n=3, no_boost=True",
        "        )",
        "",
        "        self.assertIsInstance(results, list)",
        "",
        "    def test_search_prefer_docs(self):",
        "        \"\"\"Test search with prefer_docs flag.\"\"\"",
        "        results = search_codebase(",
        "            self.processor, \"PageRank\", top_n=3, prefer_docs=True",
        "        )",
        "",
        "        self.assertIsInstance(results, list)",
        "",
        "    def test_search_empty_query(self):",
        "        \"\"\"Test search with empty query.\"\"\"",
        "        results = search_codebase(self.processor, \"\", top_n=3)",
        "        self.assertIsInstance(results, list)",
        "",
        "",
        "class TestFindSimilarCode(unittest.TestCase):",
        "    \"\"\"Tests for the find_similar_code function.\"\"\"",
        "",
        "    @classmethod",
        "    def setUpClass(cls):",
        "        \"\"\"Set up processor with test documents.\"\"\"",
        "        cls.processor = CorticalTextProcessor()",
        "",
        "        cls.processor.process_document(",
        "            \"module_a.py\",",
        "            \"\"\"",
        "            def calculate_score(items, weights):",
        "                total = 0",
        "                for item, weight in zip(items, weights):",
        "                    total += item * weight",
        "                return total / len(items) if items else 0",
        "            \"\"\"",
        "        )",
        "        cls.processor.process_document(",
        "            \"module_b.py\",",
        "            \"\"\"",
        "            def compute_weighted_average(values, factors):",
        "                result = 0",
        "                for value, factor in zip(values, factors):",
        "                    result += value * factor",
        "                return result / len(values) if values else 0",
        "            \"\"\"",
        "        )",
        "        cls.processor.process_document(",
        "            \"unrelated.py\",",
        "            \"\"\"",
        "            class UserAuthentication:",
        "                def verify_password(self, password, hash):",
        "                    return bcrypt.check(password, hash)",
        "            \"\"\"",
        "        )",
        "",
        "        cls.processor.compute_all()",
        "",
        "    def test_find_similar_with_text(self):",
        "        \"\"\"Test finding similar code with raw text.\"\"\"",
        "        target_code = \"def compute_score(data, weights): return sum()\"",
        "        results = find_similar_code(",
        "            self.processor, target_code, top_n=3",
        "        )",
        "",
        "        self.assertIsInstance(results, list)",
        "",
        "    def test_find_similar_result_structure(self):",
        "        \"\"\"Test that results have expected structure.\"\"\"",
        "        results = find_similar_code(",
        "            self.processor, \"def calculate total weighted\", top_n=1",
        "        )",
        "",
        "        if results:",
        "            result = results[0]",
        "            self.assertIn('file', result)",
        "            self.assertIn('line', result)",
        "            self.assertIn('passage', result)",
        "            self.assertIn('score', result)",
        "            self.assertIn('reference', result)",
        "            self.assertIn('doc_type', result)",
        "",
        "    def test_find_similar_with_file_reference(self):",
        "        \"\"\"Test finding similar code with file:line reference.\"\"\"",
        "        results = find_similar_code(",
        "            self.processor, \"module_a.py:1\", top_n=3",
        "        )",
        "",
        "        self.assertIsInstance(results, list)",
        "        # Should not include the source file itself",
        "        for result in results:",
        "            self.assertNotIn('module_a.py', result['reference'])",
        "",
        "    def test_find_similar_empty_text(self):",
        "        \"\"\"Test with empty target text.\"\"\"",
        "        results = find_similar_code(self.processor, \"\", top_n=3)",
        "        self.assertEqual(results, [])",
        "",
        "    def test_find_similar_nonexistent_file(self):",
        "        \"\"\"Test with nonexistent file reference.\"\"\"",
        "        results = find_similar_code(",
        "            self.processor, \"nonexistent.py:100\", top_n=3",
        "        )",
        "        self.assertEqual(results, [])",
        "",
        "",
        "class TestSearchCodebaseEmpty(unittest.TestCase):",
        "    \"\"\"Tests with empty processor.\"\"\"",
        "",
        "    def test_search_empty_processor(self):",
        "        \"\"\"Test search with empty processor.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        results = search_codebase(processor, \"anything\", top_n=3)",
        "        self.assertEqual(results, [])",
        "",
        "    def test_find_similar_empty_processor(self):",
        "        \"\"\"Test find_similar with empty processor.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        results = find_similar_code(processor, \"some code\", top_n=3)",
        "        self.assertEqual(results, [])",
        "",
        "",
        "if __name__ == '__main__':",
        "    unittest.main()"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "tests/test_showcase.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "\"\"\"",
        "Tests for showcase.py - Timer class and utility functions.",
        "\"\"\"",
        "",
        "import unittest",
        "import time",
        "import sys",
        "from pathlib import Path",
        "",
        "# Add parent directory to path for imports",
        "sys.path.insert(0, str(Path(__file__).parent.parent))",
        "",
        "from showcase import Timer, print_header, print_subheader, render_bar",
        "",
        "",
        "class TestTimer(unittest.TestCase):",
        "    \"\"\"Tests for the Timer class.\"\"\"",
        "",
        "    def test_timer_start_stop(self):",
        "        \"\"\"Test basic start/stop timing.\"\"\"",
        "        timer = Timer()",
        "        timer.start('test_op')",
        "        time.sleep(0.01)  # Small delay",
        "        elapsed = timer.stop()",
        "",
        "        self.assertGreater(elapsed, 0.005)",
        "        self.assertLess(elapsed, 0.1)",
        "",
        "    def test_timer_records_time(self):",
        "        \"\"\"Test that timer records time in times dict.\"\"\"",
        "        timer = Timer()",
        "        timer.start('operation')",
        "        time.sleep(0.01)",
        "        timer.stop()",
        "",
        "        self.assertIn('operation', timer.times)",
        "        self.assertGreater(timer.times['operation'], 0)",
        "",
        "    def test_timer_get(self):",
        "        \"\"\"Test get method returns recorded time.\"\"\"",
        "        timer = Timer()",
        "        timer.start('op1')",
        "        time.sleep(0.01)",
        "        timer.stop()",
        "",
        "        recorded = timer.get('op1')",
        "        self.assertGreater(recorded, 0)",
        "        self.assertEqual(recorded, timer.times['op1'])",
        "",
        "    def test_timer_get_missing(self):",
        "        \"\"\"Test get returns 0 for unrecorded operation.\"\"\"",
        "        timer = Timer()",
        "        self.assertEqual(timer.get('nonexistent'), 0)",
        "",
        "    def test_timer_multiple_operations(self):",
        "        \"\"\"Test timing multiple operations.\"\"\"",
        "        timer = Timer()",
        "",
        "        timer.start('op1')",
        "        time.sleep(0.01)",
        "        timer.stop()",
        "",
        "        timer.start('op2')",
        "        time.sleep(0.02)",
        "        timer.stop()",
        "",
        "        self.assertIn('op1', timer.times)",
        "        self.assertIn('op2', timer.times)",
        "        self.assertGreater(timer.get('op2'), timer.get('op1'))",
        "",
        "    def test_timer_overwrite(self):",
        "        \"\"\"Test that timing same operation overwrites previous.\"\"\"",
        "        timer = Timer()",
        "",
        "        timer.start('op')",
        "        time.sleep(0.01)",
        "        timer.stop()",
        "        first_time = timer.get('op')",
        "",
        "        timer.start('op')",
        "        time.sleep(0.02)",
        "        timer.stop()",
        "        second_time = timer.get('op')",
        "",
        "        # Second time should overwrite and be longer",
        "        self.assertNotEqual(first_time, second_time)",
        "",
        "",
        "class TestRenderBar(unittest.TestCase):",
        "    \"\"\"Tests for the render_bar function.\"\"\"",
        "",
        "    def test_render_bar_full(self):",
        "        \"\"\"Test render_bar at 100%.\"\"\"",
        "        bar = render_bar(100, 100, width=10)",
        "        self.assertEqual(bar, \"â–ˆ\" * 10)",
        "",
        "    def test_render_bar_empty(self):",
        "        \"\"\"Test render_bar at 0%.\"\"\"",
        "        bar = render_bar(0, 100, width=10)",
        "        self.assertEqual(bar, \"â–‘\" * 10)",
        "",
        "    def test_render_bar_half(self):",
        "        \"\"\"Test render_bar at 50%.\"\"\"",
        "        bar = render_bar(50, 100, width=10)",
        "        self.assertEqual(bar, \"â–ˆ\" * 5 + \"â–‘\" * 5)",
        "",
        "    def test_render_bar_zero_max(self):",
        "        \"\"\"Test render_bar with zero max value.\"\"\"",
        "        bar = render_bar(50, 0, width=10)",
        "        self.assertEqual(bar, \" \" * 10)",
        "",
        "    def test_render_bar_custom_width(self):",
        "        \"\"\"Test render_bar with custom width.\"\"\"",
        "        bar = render_bar(75, 100, width=20)",
        "        self.assertEqual(len(bar), 20)",
        "        self.assertEqual(bar.count(\"â–ˆ\"), 15)",
        "",
        "",
        "class TestPrintFunctions(unittest.TestCase):",
        "    \"\"\"Tests for print helper functions.\"\"\"",
        "",
        "    def test_print_header_returns_none(self):",
        "        \"\"\"Test print_header doesn't raise.\"\"\"",
        "        # Just verify it doesn't raise",
        "        result = print_header(\"Test Header\")",
        "        self.assertIsNone(result)",
        "",
        "    def test_print_subheader_returns_none(self):",
        "        \"\"\"Test print_subheader doesn't raise.\"\"\"",
        "        result = print_subheader(\"Test Subheader\")",
        "        self.assertIsNone(result)",
        "",
        "",
        "if __name__ == '__main__':",
        "    unittest.main()"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    }
  ],
  "hour_of_day": 2,
  "day_of_week": "Thursday",
  "seconds_since_last_commit": -387057,
  "is_merge": true,
  "is_initial": false,
  "parent_count": 2,
  "session_id": null,
  "related_chats": [],
  "ci_result": null,
  "reverted": false,
  "amended": false
}