{
  "hash": "6f3a1cc8df987075bb9764ca69676b69e96ff54e",
  "message": "feat: Add HMAC signature verification for pickle files (SEC-003)",
  "author": "Claude",
  "timestamp": "2025-12-14 12:11:20 +0000",
  "branch": "claude/multi-index-design-DvifZ",
  "files_changed": [
    "cortical/__init__.py",
    "cortical/persistence.py",
    "cortical/processor.py",
    "docs/session-2025-12-14-review-and-testing.md",
    "tasks/2025-12-14_11-15-01_41d5.json",
    "tests/unit/test_diff.py",
    "tests/unit/test_persistence.py"
  ],
  "insertions": 1872,
  "deletions": 16,
  "hunks": [
    {
      "file": "cortical/__init__.py",
      "function": "from .results import (",
      "start_line": 36,
      "lines_added": [
        "from .persistence import SignatureVerificationError"
      ],
      "lines_removed": [],
      "context_before": [
        ")",
        "from .diff import (",
        "    SemanticDiff,",
        "    TermChange,",
        "    RelationChange,",
        "    ClusterChange,",
        "    compare_processors,",
        "    compare_documents,",
        "    what_changed",
        ")"
      ],
      "context_after": [
        "",
        "# MCP Server support (optional import)",
        "try:",
        "    from .mcp_server import CorticalMCPServer, create_mcp_server",
        "    _has_mcp = True",
        "except ImportError:",
        "    _has_mcp = False",
        "    CorticalMCPServer = None",
        "    create_mcp_server = None",
        ""
      ],
      "change_type": "add"
    },
    {
      "file": "cortical/__init__.py",
      "function": "__all__ = [",
      "start_line": 76,
      "lines_added": [
        "    # Security",
        "    \"SignatureVerificationError\","
      ],
      "lines_removed": [],
      "context_before": [
        "    \"convert_document_matches\",",
        "    \"convert_passage_matches\",",
        "    # Semantic diff",
        "    \"SemanticDiff\",",
        "    \"TermChange\",",
        "    \"RelationChange\",",
        "    \"ClusterChange\",",
        "    \"compare_processors\",",
        "    \"compare_documents\",",
        "    \"what_changed\","
      ],
      "context_after": [
        "]",
        "",
        "# Add MCP exports if available",
        "if _has_mcp:",
        "    __all__.extend([",
        "        \"CorticalMCPServer\",",
        "        \"create_mcp_server\",",
        "    ])"
      ],
      "change_type": "add"
    },
    {
      "file": "cortical/persistence.py",
      "function": null,
      "start_line": 1,
      "lines_added": [
        "- HMAC signature verification for pickle files (SEC-003)",
        "import hmac",
        "import hashlib",
        "from typing import Dict, Optional, Any, Union",
        "class SignatureVerificationError(Exception):",
        "    \"\"\"",
        "    Raised when HMAC signature verification fails.",
        "",
        "    This indicates the file has been tampered with or the wrong key was used.",
        "    \"\"\"",
        "    pass",
        "",
        "",
        "def _get_signature_path(filepath: str) -> str:",
        "    \"\"\"Get the path to the signature file for a given data file.\"\"\"",
        "    return f\"{filepath}.sig\"",
        "",
        "",
        "def _compute_signature(data: bytes, key: bytes) -> bytes:",
        "    \"\"\"",
        "    Compute HMAC-SHA256 signature for data.",
        "",
        "    Args:",
        "        data: The data to sign",
        "        key: The secret key for HMAC",
        "",
        "    Returns:",
        "        The HMAC signature (32 bytes)",
        "    \"\"\"",
        "    return hmac.new(key, data, hashlib.sha256).digest()",
        "",
        "",
        "def _save_signature(filepath: str, signature: bytes) -> None:",
        "    \"\"\"Save signature to a .sig file.\"\"\"",
        "    sig_path = _get_signature_path(filepath)",
        "    with open(sig_path, 'wb') as f:",
        "        f.write(signature)",
        "",
        "",
        "def _load_signature(filepath: str) -> Optional[bytes]:",
        "    \"\"\"",
        "    Load signature from a .sig file.",
        "",
        "    Returns:",
        "        The signature bytes, or None if file doesn't exist",
        "    \"\"\"",
        "    sig_path = _get_signature_path(filepath)",
        "    if not os.path.exists(sig_path):",
        "        return None",
        "    with open(sig_path, 'rb') as f:",
        "        return f.read()",
        "",
        "",
        "def _verify_signature(data: bytes, signature: bytes, key: bytes) -> bool:",
        "    \"\"\"",
        "    Verify HMAC signature using constant-time comparison.",
        "",
        "    Args:",
        "        data: The data that was signed",
        "        signature: The signature to verify",
        "        key: The secret key used for signing",
        "",
        "    Returns:",
        "        True if signature is valid, False otherwise",
        "    \"\"\"",
        "    expected = _compute_signature(data, key)",
        "    # Use constant-time comparison to prevent timing attacks",
        "    return hmac.compare_digest(signature, expected)",
        "",
        "",
        "    format: str = 'pickle',",
        "    signing_key: Optional[bytes] = None",
        "        signing_key: Optional HMAC key for signing pickle files (SEC-003).",
        "            If provided, creates a .sig file alongside the pickle file.",
        "            The same key must be used to verify when loading."
      ],
      "lines_removed": [
        "from typing import Dict, Optional, Any",
        "    format: str = 'pickle'"
      ],
      "context_before": [
        "\"\"\"",
        "Persistence Module",
        "==================",
        "",
        "Save and load functionality for the cortical processor.",
        "",
        "Supports:",
        "- Pickle serialization for full state",
        "- JSON export for graph visualization",
        "- Incremental updates"
      ],
      "context_after": [
        "\"\"\"",
        "",
        "import pickle",
        "import json",
        "import os",
        "import logging",
        "import warnings",
        "",
        "from .layers import CorticalLayer, HierarchicalLayer",
        "from .minicolumn import Minicolumn",
        "from .proto import PROTOBUF_AVAILABLE, serialize_state, deserialize_state",
        "",
        "logger = logging.getLogger(__name__)",
        "",
        "",
        "def save_processor(",
        "    filepath: str,",
        "    layers: Dict[CorticalLayer, HierarchicalLayer],",
        "    documents: Dict[str, str],",
        "    document_metadata: Optional[Dict[str, Dict[str, Any]]] = None,",
        "    embeddings: Optional[Dict[str, list]] = None,",
        "    semantic_relations: Optional[list] = None,",
        "    metadata: Optional[Dict] = None,",
        "    verbose: bool = True,",
        ") -> None:",
        "    \"\"\"",
        "    Save processor state to a file.",
        "",
        "    Args:",
        "        filepath: Path to save file",
        "        layers: Dictionary of all layers",
        "        documents: Document collection",
        "        document_metadata: Per-document metadata (source, timestamp, etc.)",
        "        embeddings: Graph embeddings for terms (optional)",
        "        semantic_relations: Extracted semantic relations (optional)",
        "        metadata: Optional processor metadata (version, settings, etc.)",
        "        verbose: Print progress",
        "        format: Serialization format ('pickle' or 'protobuf'). Default: 'pickle'",
        "",
        "    Raises:",
        "        ValueError: If format is not 'pickle' or 'protobuf'",
        "        ImportError: If format='protobuf' but protobuf package is not installed",
        "    \"\"\"",
        "    if format not in ['pickle', 'protobuf']:",
        "        raise ValueError(f\"Invalid format '{format}'. Must be 'pickle' or 'protobuf'.\")",
        "",
        "    if format == 'pickle':",
        "        # Emit deprecation warning for pickle format due to security concerns"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/persistence.py",
      "function": "def save_processor(",
      "start_line": 73,
      "lines_added": [
        "        # Serialize to bytes",
        "        pickle_data = pickle.dumps(state, protocol=pickle.HIGHEST_PROTOCOL)",
        "",
        "        # Sign if key provided (SEC-003)",
        "        if signing_key is not None:",
        "            signature = _compute_signature(pickle_data, signing_key)",
        "            _save_signature(filepath, signature)",
        "            if verbose:",
        "                logger.info(f\"  - HMAC signature saved to {_get_signature_path(filepath)}\")",
        "",
        "        # Write pickle data",
        "            f.write(pickle_data)"
      ],
      "lines_removed": [
        "            pickle.dump(state, f, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "context_before": [
        "            'document_metadata': document_metadata or {},",
        "            'embeddings': embeddings or {},",
        "            'semantic_relations': semantic_relations or [],",
        "            'metadata': metadata or {}",
        "        }",
        "",
        "        # Serialize layers",
        "        for layer_enum, layer in layers.items():",
        "            state['layers'][layer_enum.value] = layer.to_dict()",
        ""
      ],
      "context_after": [
        "        with open(filepath, 'wb') as f:",
        "",
        "    elif format == 'protobuf':",
        "        # Protocol Buffers serialization (text format for git-friendliness)",
        "        if not PROTOBUF_AVAILABLE:",
        "            raise ImportError(",
        "                \"protobuf package is required for Protocol Buffers serialization. \"",
        "                \"Install it with: pip install protobuf\"",
        "            )",
        "",
        "        text_output = serialize_state("
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/persistence.py",
      "function": "def save_processor(",
      "start_line": 108,
      "lines_added": [
        "    format: Optional[str] = None,",
        "    verify_key: Optional[bytes] = None",
        "        verify_key: Optional HMAC key for verifying pickle file signatures (SEC-003).",
        "            If provided, the signature file (.sig) must exist and match.",
        "            This protects against tampering of pickle files.",
        "        SignatureVerificationError: If verify_key is provided and signature verification fails",
        "        FileNotFoundError: If verify_key is provided but no .sig file exists"
      ],
      "lines_removed": [
        "    format: Optional[str] = None"
      ],
      "context_before": [
        "        logger.info(f\"  - {total_conns} connections\")",
        "        if embeddings:",
        "            logger.info(f\"  - {len(embeddings)} embeddings\")",
        "        if semantic_relations:",
        "            logger.info(f\"  - {len(semantic_relations)} semantic relations\")",
        "",
        "",
        "def load_processor(",
        "    filepath: str,",
        "    verbose: bool = True,"
      ],
      "context_after": [
        ") -> tuple:",
        "    \"\"\"",
        "    Load processor state from a file.",
        "",
        "    Args:",
        "        filepath: Path to saved file",
        "        verbose: Print progress",
        "        format: Serialization format ('pickle' or 'protobuf'). If None, auto-detect.",
        "",
        "    Returns:",
        "        Tuple of (layers, documents, document_metadata, embeddings, semantic_relations, metadata)",
        "",
        "    Raises:",
        "        ValueError: If layer values are invalid (must be 0-3) or format is invalid",
        "        ImportError: If format='protobuf' but protobuf package is not installed",
        "    \"\"\"",
        "    # Auto-detect format if not specified",
        "    if format is None:",
        "        # Try to detect based on file content",
        "        with open(filepath, 'rb') as f:",
        "            # Read first few bytes",
        "            header = f.read(64)",
        "",
        "            # Pickle files start with 0x80 (protocol marker)",
        "            if header[0:1] == b'\\x80':"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/persistence.py",
      "function": "def load_processor(",
      "start_line": 162,
      "lines_added": [
        "",
        "        # Read pickle data as bytes (for signature verification)",
        "            pickle_data = f.read()",
        "",
        "        # Verify signature if key provided (SEC-003)",
        "        if verify_key is not None:",
        "            signature = _load_signature(filepath)",
        "            if signature is None:",
        "                raise FileNotFoundError(",
        "                    f\"Signature file not found: {_get_signature_path(filepath)}. \"",
        "                    f\"Cannot verify file integrity without signature.\"",
        "                )",
        "            if not _verify_signature(pickle_data, signature, verify_key):",
        "                raise SignatureVerificationError(",
        "                    f\"Signature verification failed for {filepath}. \"",
        "                    f\"The file may have been tampered with or the wrong key was used.\"",
        "                )",
        "            if verbose:",
        "                logger.info(f\"  - HMAC signature verified successfully\")",
        "",
        "        # Deserialize pickle",
        "        state = pickle.loads(pickle_data)"
      ],
      "lines_removed": [
        "        # Original pickle deserialization",
        "            state = pickle.load(f)"
      ],
      "context_before": [
        "    if format == 'pickle':",
        "        # Emit deprecation warning for pickle format due to security concerns",
        "        warnings.warn(",
        "            \"Pickle format is deprecated due to security concerns (arbitrary code execution). \"",
        "            \"Only load pickle files from trusted sources. \"",
        "            \"Consider migrating to format='protobuf' or the StateLoader JSON format. \"",
        "            \"See README.md 'Security Considerations' for details.\",",
        "            DeprecationWarning,",
        "            stacklevel=2",
        "        )"
      ],
      "context_after": [
        "        with open(filepath, 'rb') as f:",
        "",
        "        # Reconstruct layers",
        "        layers = {}",
        "        for level_value, layer_data in state.get('layers', {}).items():",
        "            # Validate layer value before creating enum",
        "            level_int = int(level_value)",
        "            if level_int not in [0, 1, 2, 3]:",
        "                raise ValueError(",
        "                    f\"Invalid layer value {level_int} in saved state. \"",
        "                    f\"Layer values must be 0-3 (TOKENS=0, BIGRAMS=1, CONCEPTS=2, DOCUMENTS=3).\""
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/processor.py",
      "function": "class CorticalTextProcessor:",
      "start_line": 2935,
      "lines_added": [
        "    def save(",
        "        self,",
        "        filepath: str,",
        "        verbose: bool = True,",
        "        signing_key: Optional[bytes] = None",
        "    ) -> None:",
        "",
        "        Args:",
        "            filepath: Path to save file",
        "            verbose: Print progress",
        "            signing_key: Optional HMAC key for signing pickle files (SEC-003).",
        "                If provided, creates a .sig file alongside the pickle file.",
        "                Use the same key with verify_key when loading.",
        "            verbose,",
        "            signing_key=signing_key",
        "    def load(",
        "        cls,",
        "        filepath: str,",
        "        verbose: bool = True,",
        "        verify_key: Optional[bytes] = None",
        "    ) -> 'CorticalTextProcessor':",
        "",
        "        Args:",
        "            filepath: Path to saved file",
        "            verbose: Print progress",
        "            verify_key: Optional HMAC key for verifying pickle file signatures (SEC-003).",
        "                If provided, the .sig file must exist and the signature must match.",
        "                This protects against tampering of pickle files.",
        "",
        "        Raises:",
        "            SignatureVerificationError: If verify_key is provided and verification fails",
        "            FileNotFoundError: If verify_key is provided but no .sig file exists",
        "        result = persistence.load_processor(filepath, verbose, verify_key=verify_key)"
      ],
      "lines_removed": [
        "    def save(self, filepath: str, verbose: bool = True) -> None:",
        "            verbose",
        "    def load(cls, filepath: str, verbose: bool = True) -> 'CorticalTextProcessor':",
        "        result = persistence.load_processor(filepath, verbose)"
      ],
      "context_before": [
        "        Example:",
        "            >>> diff = processor.what_changed(",
        "            ...     old_content=\"def hello(): print('hi')\",",
        "            ...     new_content=\"def hello(): print('hello world')\"",
        "            ... )",
        "            >>> print(f\"Similarity: {diff['summary']['content_similarity']:.2%}\")",
        "        \"\"\"",
        "        from . import diff as diff_module",
        "        return diff_module.what_changed(self, old_content, new_content)",
        ""
      ],
      "context_after": [
        "        \"\"\"",
        "        Save processor state to a file.",
        "",
        "        Saves all computed state including embeddings, semantic relations,",
        "        and configuration, so they don't need to be recomputed when loading.",
        "        \"\"\"",
        "        metadata = {",
        "            'has_embeddings': bool(self.embeddings),",
        "            'has_relations': bool(self.semantic_relations),",
        "            'config': self.config.to_dict()  # Save config in metadata",
        "        }",
        "        persistence.save_processor(",
        "            filepath,",
        "            self.layers,",
        "            self.documents,",
        "            self.document_metadata,",
        "            self.embeddings,",
        "            self.semantic_relations,",
        "            metadata,",
        "        )",
        "",
        "    @classmethod",
        "        \"\"\"",
        "        Load processor state from a file.",
        "",
        "        Restores all computed state including embeddings, semantic relations,",
        "        and configuration.",
        "        \"\"\"",
        "        layers, documents, document_metadata, embeddings, semantic_relations, metadata = result",
        "",
        "        # Restore config if available, otherwise use defaults",
        "        config = None",
        "        if metadata and 'config' in metadata:",
        "            try:",
        "                config = CorticalConfig.from_dict(metadata['config'])",
        "            except (KeyError, TypeError):",
        "                # Fall back to default config if restoration fails",
        "                config = None"
      ],
      "change_type": "modify"
    },
    {
      "file": "docs/session-2025-12-14-review-and-testing.md",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "# Knowledge Transfer: Security Review & Test Coverage",
        "",
        "**Date:** 2025-12-14",
        "**Branch:** `claude/review-security-features-OtjPE`",
        "**Focus:** Review security features, add missing tests, complete SEC-003",
        "**Status:** COMPLETED",
        "",
        "---",
        "",
        "## Executive Summary",
        "",
        "This session completed three major tasks:",
        "1. **Added 70 tests for diff.py** - Coverage improved from 17% to 87%",
        "2. **Implemented SEC-003: HMAC verification** - Pickle file integrity protection",
        "3. **Added 23 tests for SEC-003** - Comprehensive security test coverage",
        "",
        "### Commits Made",
        "",
        "| Commit | Description |",
        "|--------|-------------|",
        "| (pending) | feat: Add HMAC signature verification for pickle files (SEC-003) |",
        "| (pending) | test: Add comprehensive tests for diff.py and SEC-003 |",
        "",
        "---",
        "",
        "## Current State Assessment",
        "",
        "### What Was Done (Previous Session)",
        "| Task | Status | Notes |",
        "|------|--------|-------|",
        "| SEC-001: README security warning | Done | Pickle risks documented |",
        "| SEC-002: Bandit SAST in CI | Done | Added to security-scan job |",
        "| SEC-004: pip-audit in CI | Done | Dependency scanning |",
        "| SEC-005: detect-secrets in CI | Done | Secret scanning |",
        "| SEC-006: MCP security docs | Done | docs/mcp-security.md |",
        "| SEC-007: task-manager permissions | Done | SKILL.md updated |",
        "| SEC-008: Pickle deprecation warning | Done | persistence.py |",
        "| LEGACY-075: Semantic diff | Done | cortical/diff.py |",
        "",
        "### Test Status",
        "```",
        "Smoke tests:  18 passed (0.49s)",
        "Quick tests:  2166 passed (13s)",
        "diff.py:      17% coverage (279 statements, 210 missed)",
        "```",
        "",
        "### Key Gap: diff.py Has No Tests",
        "The new 625-line `cortical/diff.py` module has:",
        "- 4 dataclasses: `TermChange`, `RelationChange`, `ClusterChange`, `SemanticDiff`",
        "- 3 public functions: `compare_processors()`, `compare_documents()`, `what_changed()`",
        "- 2 private functions: `_compare_relations()`, `_compare_clusters()`",
        "- 0 dedicated test files",
        "",
        "The module works (verified manually), but untested code is risky.",
        "",
        "---",
        "",
        "## Work Plan",
        "",
        "### Phase 1: Add Tests for diff.py (Priority: HIGH)",
        "",
        "**Why first:** \"Did I do it right?\" - We can't ship untested code.",
        "",
        "**Deliverable:** `tests/unit/test_diff.py`",
        "",
        "**Test cases needed:**",
        "1. `TestTermChange` - Dataclass property tests",
        "   - `pagerank_delta` calculation",
        "   - `tfidf_delta` calculation",
        "   - `documents_added` / `documents_removed`",
        "2. `TestRelationChange` - Basic dataclass",
        "3. `TestClusterChange` - Basic dataclass",
        "4. `TestSemanticDiff` - Summary and to_dict methods",
        "5. `TestCompareProcessors` - Main comparison function",
        "   - Empty processors",
        "   - Single document change",
        "   - Multiple document changes",
        "   - PageRank importance shifts",
        "6. `TestCompareDocuments` - Within-corpus comparison",
        "7. `TestWhatChanged` - Quick text comparison",
        "",
        "**Estimated effort:** 2 hours",
        "**Sub-agent friendly:** Yes - bounded, well-defined task",
        "",
        "### Phase 2: Implement SEC-003 (Priority: MEDIUM)",
        "",
        "**Why second:** Completes the pickle security story.",
        "",
        "**Deliverable:** HMAC verification in `cortical/persistence.py`",
        "",
        "**Implementation:**",
        "```python",
        "import hmac",
        "import hashlib",
        "",
        "def _compute_signature(data: bytes, key: bytes) -> bytes:",
        "    return hmac.new(key, data, hashlib.sha256).digest()",
        "",
        "def save_processor(..., signing_key: Optional[bytes] = None):",
        "    # If key provided, write signature to .pkl.sig",
        "",
        "def load_processor(..., verify_key: Optional[bytes] = None):",
        "    # If key provided, verify signature before loading",
        "```",
        "",
        "**Design decisions:**",
        "- Signature file: `{filename}.sig` (separate file, not embedded)",
        "- Algorithm: HMAC-SHA256 (standard, no external deps)",
        "- Key management: User-provided (not auto-generated)",
        "- Behavior on mismatch: Raise `SecurityError` or similar",
        "",
        "**Estimated effort:** 4 hours",
        "**Sub-agent friendly:** No - security code needs careful review",
        "",
        "### Phase 3: Update Tests for SEC-003 (Priority: LOW)",
        "",
        "**Deliverable:** Tests in `tests/unit/test_persistence.py` or `tests/security/`",
        "",
        "**Test cases:**",
        "1. Save with signing key creates .sig file",
        "2. Load with verify key and valid signature succeeds",
        "3. Load with verify key and invalid signature fails",
        "4. Load with verify key and missing signature fails",
        "5. Backward compatibility: Load without key still works (with warning)",
        "",
        "---",
        "",
        "## Sub-Agent Guidance",
        "",
        "### For diff.py Test Agent",
        "",
        "**Context:** You're writing tests for a new semantic diff module.",
        "",
        "**Files to read first:**",
        "- `cortical/diff.py` - The module under test",
        "- `tests/unit/test_*.py` - Existing test patterns",
        "- `tests/conftest.py` - Available fixtures",
        "",
        "**Fixtures available:**",
        "- `small_processor` - Pre-loaded 25-doc corpus",
        "- `fresh_processor` - Empty processor",
        "- `small_corpus_docs` - Raw document dict",
        "",
        "**Test file location:** `tests/unit/test_diff.py`",
        "",
        "**Style requirements:**",
        "- Use pytest, not unittest",
        "- Group tests by class being tested",
        "- Include docstrings explaining what each test verifies",
        "- Test edge cases: empty processors, single doc, identical inputs",
        "",
        "**Do NOT:**",
        "- Modify any existing files",
        "- Add tests to existing test files",
        "- Change the diff.py implementation",
        "",
        "### For SEC-003 Implementation",
        "",
        "**Context:** Adding HMAC signature verification to pickle save/load.",
        "",
        "**Files to modify:**",
        "- `cortical/persistence.py` - Add signing logic",
        "",
        "**Design constraints:**",
        "- No external dependencies (use stdlib hmac/hashlib)",
        "- Signature in separate file (.sig extension)",
        "- Backward compatible (signing optional)",
        "- Clear error messages on verification failure",
        "",
        "**Security considerations:**",
        "- Use constant-time comparison (hmac.compare_digest)",
        "- Don't leak timing information",
        "- Clear documentation of threat model",
        "",
        "---",
        "",
        "## Decision Log",
        "",
        "| Decision | Rationale |",
        "|----------|-----------|",
        "| Tests before SEC-003 | Can't verify security code without tests |",
        "| diff.py tests first | Lower risk, enables sub-agent delegation |",
        "| HMAC not embedded in pickle | Separate .sig file is cleaner, easier to audit |",
        "| User-provided keys | No key management complexity in library |",
        "",
        "---",
        "",
        "## Files Expected to Change",
        "",
        "### New Files",
        "- `tests/unit/test_diff.py` - Tests for diff module",
        "",
        "### Modified Files",
        "- `cortical/persistence.py` - SEC-003 HMAC verification",
        "- Possibly `tests/unit/test_persistence.py` - SEC-003 tests",
        "",
        "---",
        "",
        "## Verification Checklist",
        "",
        "Before marking complete:",
        "- [x] All tests pass: `python -m pytest tests/ -v` - 2259 passed",
        "- [x] Coverage for diff.py > 80%: 87% achieved",
        "- [x] SEC-003 implementation reviewed for security issues",
        "- [x] Documentation updated if API changed",
        "- [x] Commit messages reference task IDs",
        "",
        "---",
        "",
        "## What Was Implemented",
        "",
        "### SEC-003: HMAC Signature Verification",
        "",
        "**Files Modified:**",
        "- `cortical/persistence.py` - Added signature functions and verification logic",
        "- `cortical/processor.py` - Added `signing_key` and `verify_key` parameters",
        "- `cortical/__init__.py` - Exported `SignatureVerificationError`",
        "",
        "**New Functions:**",
        "```python",
        "# Helper functions in persistence.py",
        "_get_signature_path(filepath)      # Returns filepath + \".sig\"",
        "_compute_signature(data, key)      # HMAC-SHA256 computation",
        "_save_signature(filepath, sig)     # Save signature to .sig file",
        "_load_signature(filepath)          # Load signature from .sig file",
        "_verify_signature(data, sig, key)  # Constant-time signature verification",
        "```",
        "",
        "**Usage:**",
        "```python",
        "from cortical import CorticalTextProcessor, SignatureVerificationError",
        "",
        "# Save with signature",
        "key = b'my-secret-key'",
        "processor.save(\"corpus.pkl\", signing_key=key)",
        "# Creates corpus.pkl and corpus.pkl.sig",
        "",
        "# Load with verification",
        "loaded = CorticalTextProcessor.load(\"corpus.pkl\", verify_key=key)",
        "# Raises SignatureVerificationError if tampered",
        "",
        "# Backward compatible - no key required",
        "loaded = CorticalTextProcessor.load(\"corpus.pkl\")",
        "```",
        "",
        "**Security Features:**",
        "- HMAC-SHA256 for signing (32-byte signature)",
        "- Constant-time comparison (`hmac.compare_digest`) prevents timing attacks",
        "- Separate .sig file (cleaner, easier to audit)",
        "- Clear error messages on verification failure",
        "",
        "### Tests Added",
        "",
        "**tests/unit/test_diff.py** - 70 tests covering:",
        "- TermChange, RelationChange, ClusterChange dataclasses",
        "- SemanticDiff summary and to_dict methods",
        "- compare_processors() function",
        "- compare_documents() function",
        "- what_changed() function",
        "",
        "**tests/unit/test_persistence.py** - 23 tests covering:",
        "- Signature helper functions",
        "- SignatureVerificationError exception",
        "- Save/load with signing and verification",
        "- Tamper detection",
        "- Backward compatibility",
        "",
        "---",
        "",
        "## Files Changed",
        "",
        "### New Files",
        "- `tests/unit/test_diff.py` - 70 tests for diff module",
        "",
        "### Modified Files",
        "- `cortical/persistence.py` - SEC-003 HMAC implementation",
        "- `cortical/processor.py` - signing_key/verify_key parameters",
        "- `cortical/__init__.py` - Export SignatureVerificationError",
        "- `tests/unit/test_persistence.py` - SEC-003 tests",
        "- `docs/session-2025-12-14-review-and-testing.md` - This document",
        "",
        "---",
        "",
        "## Remaining Tasks",
        "",
        "### Security Tasks (Still Pending)",
        "| Task | Priority | Effort | Description |",
        "|------|----------|--------|-------------|",
        "| SEC-009 | Low | 4h | Security-focused test suite |",
        "| SEC-010 | Low | 8h | Input fuzzing with Hypothesis |",
        "",
        "### Feature Tasks (Still Pending)",
        "| Task | Priority | Description |",
        "|------|----------|-------------|",
        "| LEGACY-078 | Medium | Code pattern detection |",
        "| LEGACY-095 | Medium | Split processor.py (2,301 lines) |",
        "| LEGACY-187 | Medium | Async API support |",
        "| LEGACY-190 | Medium | REST API wrapper (FastAPI) |",
        "",
        "---",
        "",
        "*Document created: 2025-12-14*",
        "*Document completed: 2025-12-14*"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "tasks/2025-12-14_11-15-01_41d5.json",
      "function": null,
      "start_line": 44,
      "lines_added": [
        "      \"status\": \"completed\",",
        "      \"updated_at\": \"2025-12-14T12:30:00.000000\",",
        "      \"completed_at\": \"2025-12-14T12:30:00.000000\",",
        "          \"cortical/persistence.py\",",
        "          \"cortical/processor.py\",",
        "          \"cortical/__init__.py\"",
        "          \"load_processor()\",",
        "          \"save_processor()\"",
        "      \"retrospective\": {",
        "        \"notes\": \"Implemented HMAC-SHA256 signature verification. Added signing_key parameter to save() and verify_key parameter to load(). Exported SignatureVerificationError exception. Added 23 tests covering all edge cases. Uses constant-time comparison to prevent timing attacks.\",",
        "        \"branch\": \"claude/review-security-features-OtjPE\"",
        "      }"
      ],
      "lines_removed": [
        "      \"status\": \"pending\",",
        "      \"updated_at\": null,",
        "      \"completed_at\": null,",
        "          \"cortical/persistence.py\"",
        "          \"load_processor()\"",
        "      \"retrospective\": null"
      ],
      "context_before": [
        "          \".github/workflows/ci.yml\"",
        "        ]",
        "      },",
        "      \"retrospective\": {",
        "        \"notes\": \"Implemented in commit 90b989f\"",
        "      }",
        "    },",
        "    {",
        "      \"id\": \"T-20251214-111501-41d5-003\",",
        "      \"title\": \"SEC-003: Implement pickle HMAC verification\","
      ],
      "context_after": [
        "      \"priority\": \"medium\",",
        "      \"category\": \"security\",",
        "      \"description\": \"Add HMAC signature verification before loading pickle files.\\n\\nImplementation:\\n1. Generate HMAC when saving: hmac.new(secret_key, pickle_data, 'sha256')\\n2. Store signature alongside pickle file (.pkl.sig)\\n3. Verify signature before pickle.load()\\n4. Reject files with invalid/missing signatures\\n\\nThis prevents loading tampered pickle files even from trusted sources.\\n\\nEffort: 4 hours\",",
        "      \"depends_on\": [],",
        "      \"effort\": \"medium\",",
        "      \"created_at\": \"2025-12-14T11:15:01.281835\",",
        "      \"context\": {",
        "        \"files\": [",
        "        ],",
        "        \"methods\": [",
        "        ]",
        "      },",
        "    },",
        "    {",
        "      \"id\": \"T-20251214-111501-41d5-004\",",
        "      \"title\": \"SEC-004: Add pip-audit dependency scanning to CI\",",
        "      \"status\": \"completed\",",
        "      \"priority\": \"medium\",",
        "      \"category\": \"security\",",
        "      \"description\": \"Add pip-audit to check for known vulnerabilities in dependencies.\\n\\nAdd step to security-scan job:\\n```yaml\\n- name: Check Dependencies\\n  run: |\\n    pip install pip-audit\\n    pip install -e \\\".[dev]\\\"\\n    pip-audit --desc || echo \\\"Review required\\\"\\n```\\n\\nEffort: 1 hour\",",
        "      \"depends_on\": [],",
        "      \"effort\": \"small\","
      ],
      "change_type": "modify"
    },
    {
      "file": "tests/unit/test_diff.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "\"\"\"",
        "Unit Tests for Diff Module",
        "===========================",
        "",
        "Tests the semantic diff module that compares processor states and documents:",
        "- TermChange: Dataclass for term/concept changes",
        "- RelationChange: Dataclass for semantic relation changes",
        "- ClusterChange: Dataclass for cluster membership changes",
        "- SemanticDiff: Complete diff container with summary methods",
        "- compare_processors: Compare two processor states",
        "- compare_documents: Compare two documents within same corpus",
        "- what_changed: Quick text comparison using tokenizer",
        "",
        "These tests verify that semantic differences are correctly detected",
        "and reported when comparing before/after states.",
        "\"\"\"",
        "",
        "import pytest",
        "",
        "from cortical.diff import (",
        "    TermChange,",
        "    RelationChange,",
        "    ClusterChange,",
        "    SemanticDiff,",
        "    compare_processors,",
        "    compare_documents,",
        "    what_changed,",
        ")",
        "from cortical.layers import CorticalLayer",
        "",
        "",
        "# =============================================================================",
        "# TERMCHANGE TESTS",
        "# =============================================================================",
        "",
        "",
        "class TestTermChange:",
        "    \"\"\"Tests for TermChange dataclass.\"\"\"",
        "",
        "    def test_pagerank_delta_both_values_present(self):",
        "        \"\"\"pagerank_delta returns correct delta when both values present.\"\"\"",
        "        tc = TermChange(",
        "            term=\"test\",",
        "            change_type=\"modified\",",
        "            old_pagerank=0.1,",
        "            new_pagerank=0.3",
        "        )",
        "",
        "        assert tc.pagerank_delta == pytest.approx(0.2)",
        "",
        "    def test_pagerank_delta_positive(self):",
        "        \"\"\"pagerank_delta handles positive change.\"\"\"",
        "        tc = TermChange(",
        "            term=\"rising\",",
        "            change_type=\"modified\",",
        "            old_pagerank=0.05,",
        "            new_pagerank=0.15",
        "        )",
        "",
        "        assert tc.pagerank_delta == pytest.approx(0.1)",
        "        assert tc.pagerank_delta > 0",
        "",
        "    def test_pagerank_delta_negative(self):",
        "        \"\"\"pagerank_delta handles negative change.\"\"\"",
        "        tc = TermChange(",
        "            term=\"falling\",",
        "            change_type=\"modified\",",
        "            old_pagerank=0.2,",
        "            new_pagerank=0.1",
        "        )",
        "",
        "        assert tc.pagerank_delta == -0.1",
        "        assert tc.pagerank_delta < 0",
        "",
        "    def test_pagerank_delta_missing_old(self):",
        "        \"\"\"pagerank_delta returns None when old value missing.\"\"\"",
        "        tc = TermChange(",
        "            term=\"new\",",
        "            change_type=\"added\",",
        "            old_pagerank=None,",
        "            new_pagerank=0.1",
        "        )",
        "",
        "        assert tc.pagerank_delta is None",
        "",
        "    def test_pagerank_delta_missing_new(self):",
        "        \"\"\"pagerank_delta returns None when new value missing.\"\"\"",
        "        tc = TermChange(",
        "            term=\"removed\",",
        "            change_type=\"removed\",",
        "            old_pagerank=0.1,",
        "            new_pagerank=None",
        "        )",
        "",
        "        assert tc.pagerank_delta is None",
        "",
        "    def test_pagerank_delta_both_missing(self):",
        "        \"\"\"pagerank_delta returns None when both values missing.\"\"\"",
        "        tc = TermChange(",
        "            term=\"test\",",
        "            change_type=\"modified\"",
        "        )",
        "",
        "        assert tc.pagerank_delta is None",
        "",
        "    def test_tfidf_delta_calculation(self):",
        "        \"\"\"tfidf_delta returns correct delta.\"\"\"",
        "        tc = TermChange(",
        "            term=\"test\",",
        "            change_type=\"modified\",",
        "            old_tfidf=2.0,",
        "            new_tfidf=5.0",
        "        )",
        "",
        "        assert tc.tfidf_delta == 3.0",
        "",
        "    def test_tfidf_delta_missing_values(self):",
        "        \"\"\"tfidf_delta returns None when values missing.\"\"\"",
        "        tc1 = TermChange(term=\"test\", change_type=\"added\", old_tfidf=None, new_tfidf=2.0)",
        "        tc2 = TermChange(term=\"test\", change_type=\"removed\", old_tfidf=2.0, new_tfidf=None)",
        "        tc3 = TermChange(term=\"test\", change_type=\"modified\")",
        "",
        "        assert tc1.tfidf_delta is None",
        "        assert tc2.tfidf_delta is None",
        "        assert tc3.tfidf_delta is None",
        "",
        "    def test_documents_added_set_difference(self):",
        "        \"\"\"documents_added returns correct set difference.\"\"\"",
        "        tc = TermChange(",
        "            term=\"test\",",
        "            change_type=\"modified\",",
        "            old_documents={\"doc1\", \"doc2\"},",
        "            new_documents={\"doc2\", \"doc3\", \"doc4\"}",
        "        )",
        "",
        "        added = tc.documents_added",
        "        assert added == {\"doc3\", \"doc4\"}",
        "        assert isinstance(added, set)",
        "",
        "    def test_documents_added_none_added(self):",
        "        \"\"\"documents_added returns empty set when no new documents.\"\"\"",
        "        tc = TermChange(",
        "            term=\"test\",",
        "            change_type=\"modified\",",
        "            old_documents={\"doc1\", \"doc2\"},",
        "            new_documents={\"doc1\", \"doc2\"}",
        "        )",
        "",
        "        assert tc.documents_added == set()",
        "",
        "    def test_documents_added_all_new(self):",
        "        \"\"\"documents_added returns all new documents when old is empty.\"\"\"",
        "        tc = TermChange(",
        "            term=\"test\",",
        "            change_type=\"modified\",",
        "            old_documents=set(),",
        "            new_documents={\"doc1\", \"doc2\"}",
        "        )",
        "",
        "        assert tc.documents_added == {\"doc1\", \"doc2\"}",
        "",
        "    def test_documents_added_missing_values(self):",
        "        \"\"\"documents_added returns empty set when values missing.\"\"\"",
        "        tc1 = TermChange(term=\"test\", change_type=\"added\", old_documents=None, new_documents={\"doc1\"})",
        "        tc2 = TermChange(term=\"test\", change_type=\"removed\", old_documents={\"doc1\"}, new_documents=None)",
        "        tc3 = TermChange(term=\"test\", change_type=\"modified\")",
        "",
        "        assert tc1.documents_added == set()",
        "        assert tc2.documents_added == set()",
        "        assert tc3.documents_added == set()",
        "",
        "    def test_documents_removed_set_difference(self):",
        "        \"\"\"documents_removed returns correct set difference.\"\"\"",
        "        tc = TermChange(",
        "            term=\"test\",",
        "            change_type=\"modified\",",
        "            old_documents={\"doc1\", \"doc2\", \"doc3\"},",
        "            new_documents={\"doc2\", \"doc4\"}",
        "        )",
        "",
        "        removed = tc.documents_removed",
        "        assert removed == {\"doc1\", \"doc3\"}",
        "        assert isinstance(removed, set)",
        "",
        "    def test_documents_removed_none_removed(self):",
        "        \"\"\"documents_removed returns empty set when no documents removed.\"\"\"",
        "        tc = TermChange(",
        "            term=\"test\",",
        "            change_type=\"modified\",",
        "            old_documents={\"doc1\"},",
        "            new_documents={\"doc1\", \"doc2\"}",
        "        )",
        "",
        "        assert tc.documents_removed == set()",
        "",
        "    def test_documents_removed_all_removed(self):",
        "        \"\"\"documents_removed returns all old documents when new is empty.\"\"\"",
        "        tc = TermChange(",
        "            term=\"test\",",
        "            change_type=\"removed\",",
        "            old_documents={\"doc1\", \"doc2\"},",
        "            new_documents=set()",
        "        )",
        "",
        "        assert tc.documents_removed == {\"doc1\", \"doc2\"}",
        "",
        "    def test_documents_removed_missing_values(self):",
        "        \"\"\"documents_removed returns empty set when values missing.\"\"\"",
        "        tc1 = TermChange(term=\"test\", change_type=\"added\", old_documents=None, new_documents={\"doc1\"})",
        "        tc2 = TermChange(term=\"test\", change_type=\"removed\", old_documents={\"doc1\"}, new_documents=None)",
        "        tc3 = TermChange(term=\"test\", change_type=\"modified\")",
        "",
        "        assert tc1.documents_removed == set()",
        "        assert tc2.documents_removed == set()",
        "        assert tc3.documents_removed == set()",
        "",
        "",
        "# =============================================================================",
        "# RELATIONCHANGE TESTS",
        "# =============================================================================",
        "",
        "",
        "class TestRelationChange:",
        "    \"\"\"Tests for RelationChange dataclass.\"\"\"",
        "",
        "    def test_basic_creation(self):",
        "        \"\"\"Basic RelationChange creation.\"\"\"",
        "        rc = RelationChange(",
        "            source=\"term1\",",
        "            target=\"term2\",",
        "            relation_type=\"synonym\",",
        "            change_type=\"added\"",
        "        )",
        "",
        "        assert rc.source == \"term1\"",
        "        assert rc.target == \"term2\"",
        "        assert rc.relation_type == \"synonym\"",
        "        assert rc.change_type == \"added\"",
        "",
        "    def test_with_weights(self):",
        "        \"\"\"RelationChange with weight values.\"\"\"",
        "        rc = RelationChange(",
        "            source=\"a\",",
        "            target=\"b\",",
        "            relation_type=\"related_to\",",
        "            change_type=\"modified\",",
        "            old_weight=0.5,",
        "            new_weight=0.8",
        "        )",
        "",
        "        assert rc.old_weight == 0.5",
        "        assert rc.new_weight == 0.8",
        "",
        "    def test_with_confidence(self):",
        "        \"\"\"RelationChange with confidence values.\"\"\"",
        "        rc = RelationChange(",
        "            source=\"x\",",
        "            target=\"y\",",
        "            relation_type=\"is_a\",",
        "            change_type=\"added\",",
        "            new_weight=1.0,",
        "            new_confidence=0.9",
        "        )",
        "",
        "        assert rc.new_confidence == 0.9",
        "        assert rc.old_confidence is None",
        "",
        "    def test_removed_relation(self):",
        "        \"\"\"Removed relation has old values only.\"\"\"",
        "        rc = RelationChange(",
        "            source=\"old\",",
        "            target=\"gone\",",
        "            relation_type=\"was_related\",",
        "            change_type=\"removed\",",
        "            old_weight=0.7,",
        "            old_confidence=0.8",
        "        )",
        "",
        "        assert rc.old_weight == 0.7",
        "        assert rc.old_confidence == 0.8",
        "        assert rc.new_weight is None",
        "        assert rc.new_confidence is None",
        "",
        "",
        "# =============================================================================",
        "# CLUSTERCHANGE TESTS",
        "# =============================================================================",
        "",
        "",
        "class TestClusterChange:",
        "    \"\"\"Tests for ClusterChange dataclass.\"\"\"",
        "",
        "    def test_created_cluster(self):",
        "        \"\"\"Created cluster has new members only.\"\"\"",
        "        cc = ClusterChange(",
        "            cluster_id=42,",
        "            change_type=\"created\",",
        "            new_members={\"term1\", \"term2\", \"term3\"}",
        "        )",
        "",
        "        assert cc.cluster_id == 42",
        "        assert cc.change_type == \"created\"",
        "        assert cc.new_members == {\"term1\", \"term2\", \"term3\"}",
        "        assert len(cc.old_members) == 0",
        "",
        "    def test_dissolved_cluster(self):",
        "        \"\"\"Dissolved cluster has old members only.\"\"\"",
        "        cc = ClusterChange(",
        "            cluster_id=7,",
        "            change_type=\"dissolved\",",
        "            old_members={\"a\", \"b\", \"c\"}",
        "        )",
        "",
        "        assert cc.cluster_id == 7",
        "        assert cc.change_type == \"dissolved\"",
        "        assert cc.old_members == {\"a\", \"b\", \"c\"}",
        "        assert len(cc.new_members) == 0",
        "",
        "    def test_modified_cluster(self):",
        "        \"\"\"Modified cluster tracks member changes.\"\"\"",
        "        cc = ClusterChange(",
        "            cluster_id=10,",
        "            change_type=\"modified\",",
        "            old_members={\"a\", \"b\", \"c\"},",
        "            new_members={\"b\", \"c\", \"d\", \"e\"},",
        "            members_added={\"d\", \"e\"},",
        "            members_removed={\"a\"}",
        "        )",
        "",
        "        assert cc.members_added == {\"d\", \"e\"}",
        "        assert cc.members_removed == {\"a\"}",
        "        assert cc.old_members == {\"a\", \"b\", \"c\"}",
        "        assert cc.new_members == {\"b\", \"c\", \"d\", \"e\"}",
        "",
        "    def test_default_empty_sets(self):",
        "        \"\"\"Default values are empty sets.\"\"\"",
        "        cc = ClusterChange(",
        "            cluster_id=1,",
        "            change_type=\"created\"",
        "        )",
        "",
        "        assert cc.old_members == set()",
        "        assert cc.new_members == set()",
        "        assert cc.members_added == set()",
        "        assert cc.members_removed == set()",
        "",
        "    def test_none_cluster_id(self):",
        "        \"\"\"Cluster ID can be None.\"\"\"",
        "        cc = ClusterChange(",
        "            cluster_id=None,",
        "            change_type=\"dissolved\"",
        "        )",
        "",
        "        assert cc.cluster_id is None",
        "",
        "",
        "# =============================================================================",
        "# SEMANTICDIFF TESTS",
        "# =============================================================================",
        "",
        "",
        "class TestSemanticDiff:",
        "    \"\"\"Tests for SemanticDiff dataclass.\"\"\"",
        "",
        "    def test_empty_diff(self):",
        "        \"\"\"Empty diff has no changes.\"\"\"",
        "        diff = SemanticDiff()",
        "",
        "        assert len(diff.documents_added) == 0",
        "        assert len(diff.documents_removed) == 0",
        "        assert len(diff.terms_added) == 0",
        "        assert len(diff.terms_removed) == 0",
        "        assert diff.total_term_changes == 0",
        "        assert diff.total_relation_changes == 0",
        "        assert diff.total_cluster_changes == 0",
        "",
        "    def test_summary_empty_diff(self):",
        "        \"\"\"Summary of empty diff is minimal.\"\"\"",
        "        diff = SemanticDiff()",
        "        summary = diff.summary()",
        "",
        "        assert isinstance(summary, str)",
        "        assert \"Statistics\" in summary",
        "        assert \"Total term changes: 0\" in summary",
        "",
        "    def test_summary_with_document_changes(self):",
        "        \"\"\"Summary includes document changes.\"\"\"",
        "        diff = SemanticDiff(",
        "            documents_added=[\"doc1\", \"doc2\"],",
        "            documents_removed=[\"doc3\"]",
        "        )",
        "",
        "        summary = diff.summary()",
        "",
        "        assert \"Documents\" in summary",
        "        assert \"Added: 2 documents\" in summary",
        "        assert \"Removed: 1 documents\" in summary",
        "        assert \"doc1\" in summary",
        "        assert \"doc3\" in summary",
        "",
        "    def test_summary_with_many_documents(self):",
        "        \"\"\"Summary limits document listing.\"\"\"",
        "        many_docs = [f\"doc{i}\" for i in range(20)]",
        "        diff = SemanticDiff(documents_added=many_docs)",
        "",
        "        summary = diff.summary()",
        "",
        "        assert \"Added: 20 documents\" in summary",
        "        assert \"and 15 more\" in summary  # Shows first 5, then \"and N more\"",
        "",
        "    def test_summary_with_term_changes(self):",
        "        \"\"\"Summary includes term changes.\"\"\"",
        "        diff = SemanticDiff(",
        "            terms_added=[",
        "                TermChange(term=\"new1\", change_type=\"added\"),",
        "                TermChange(term=\"new2\", change_type=\"added\"),",
        "            ],",
        "            terms_removed=[",
        "                TermChange(term=\"old1\", change_type=\"removed\"),",
        "            ]",
        "        )",
        "",
        "        summary = diff.summary()",
        "",
        "        assert \"Terms\" in summary",
        "        assert \"New terms: 2\" in summary",
        "        assert \"Removed terms: 1\" in summary",
        "        assert \"new1\" in summary",
        "        assert \"old1\" in summary",
        "",
        "    def test_summary_with_importance_shifts(self):",
        "        \"\"\"Summary includes PageRank changes.\"\"\"",
        "        diff = SemanticDiff(",
        "            importance_increased=[",
        "                TermChange(",
        "                    term=\"rising\",",
        "                    change_type=\"modified\",",
        "                    old_pagerank=0.1,",
        "                    new_pagerank=0.3",
        "                )",
        "            ],",
        "            importance_decreased=[",
        "                TermChange(",
        "                    term=\"falling\",",
        "                    change_type=\"modified\",",
        "                    old_pagerank=0.3,",
        "                    new_pagerank=0.1",
        "                )",
        "            ]",
        "        )",
        "",
        "        summary = diff.summary()",
        "",
        "        assert \"Importance Shifts\" in summary",
        "        assert \"Rising Terms\" in summary",
        "        assert \"Falling Terms\" in summary",
        "        assert \"rising\" in summary",
        "        assert \"falling\" in summary",
        "",
        "    def test_summary_with_relations(self):",
        "        \"\"\"Summary includes relation changes.\"\"\"",
        "        diff = SemanticDiff(",
        "            relations_added=[",
        "                RelationChange(\"a\", \"b\", \"synonym\", \"added\"),",
        "                RelationChange(\"c\", \"d\", \"is_a\", \"added\"),",
        "            ],",
        "            relations_removed=[",
        "                RelationChange(\"x\", \"y\", \"related\", \"removed\"),",
        "            ]",
        "        )",
        "",
        "        summary = diff.summary()",
        "",
        "        assert \"Relations\" in summary",
        "        assert \"New relations: 2\" in summary",
        "        assert \"Removed relations: 1\" in summary",
        "",
        "    def test_summary_with_clusters(self):",
        "        \"\"\"Summary includes cluster changes.\"\"\"",
        "        diff = SemanticDiff(",
        "            clusters_created=[",
        "                ClusterChange(cluster_id=1, change_type=\"created\"),",
        "                ClusterChange(cluster_id=2, change_type=\"created\"),",
        "            ],",
        "            clusters_dissolved=[",
        "                ClusterChange(cluster_id=99, change_type=\"dissolved\"),",
        "            ],",
        "            clusters_modified=[",
        "                ClusterChange(cluster_id=5, change_type=\"modified\"),",
        "            ]",
        "        )",
        "",
        "        summary = diff.summary()",
        "",
        "        assert \"Clusters\" in summary",
        "        assert \"New clusters: 2\" in summary",
        "        assert \"Dissolved clusters: 1\" in summary",
        "        assert \"Modified clusters: 1\" in summary",
        "",
        "    def test_to_dict_structure(self):",
        "        \"\"\"to_dict returns proper structure.\"\"\"",
        "        diff = SemanticDiff(",
        "            documents_added=[\"doc1\"],",
        "            terms_added=[TermChange(term=\"new\", change_type=\"added\", new_pagerank=0.1)],",
        "            total_term_changes=5,",
        "            total_relation_changes=2,",
        "            total_cluster_changes=1",
        "        )",
        "",
        "        result = diff.to_dict()",
        "",
        "        assert isinstance(result, dict)",
        "        assert result['documents_added'] == [\"doc1\"]",
        "        assert len(result['terms_added']) == 1",
        "        assert result['terms_added'][0]['term'] == \"new\"",
        "        assert result['total_term_changes'] == 5",
        "        assert result['total_relation_changes'] == 2",
        "        assert result['total_cluster_changes'] == 1",
        "",
        "    def test_to_dict_importance_shifts(self):",
        "        \"\"\"to_dict includes importance deltas.\"\"\"",
        "        diff = SemanticDiff(",
        "            importance_increased=[",
        "                TermChange(",
        "                    term=\"up\",",
        "                    change_type=\"modified\",",
        "                    old_pagerank=0.1,",
        "                    new_pagerank=0.3",
        "                )",
        "            ]",
        "        )",
        "",
        "        result = diff.to_dict()",
        "",
        "        assert len(result['importance_increased']) == 1",
        "        assert result['importance_increased'][0]['term'] == \"up\"",
        "        assert result['importance_increased'][0]['delta'] == pytest.approx(0.2)",
        "",
        "    def test_to_dict_counts_only(self):",
        "        \"\"\"to_dict returns counts for relations and clusters.\"\"\"",
        "        diff = SemanticDiff(",
        "            relations_added=[",
        "                RelationChange(\"a\", \"b\", \"syn\", \"added\"),",
        "                RelationChange(\"c\", \"d\", \"is_a\", \"added\"),",
        "            ],",
        "            clusters_created=[",
        "                ClusterChange(cluster_id=1, change_type=\"created\"),",
        "            ]",
        "        )",
        "",
        "        result = diff.to_dict()",
        "",
        "        # Relations and clusters are summarized as counts",
        "        assert result['relations_added'] == 2",
        "        assert result['clusters_created'] == 1",
        "",
        "",
        "# =============================================================================",
        "# COMPARE PROCESSORS TESTS",
        "# =============================================================================",
        "",
        "",
        "class TestCompareProcessors:",
        "    \"\"\"Tests for compare_processors function.\"\"\"",
        "",
        "    def test_empty_processors(self, fresh_processor):",
        "        \"\"\"Comparing empty processors returns empty diff.\"\"\"",
        "        proc1 = fresh_processor",
        "        from cortical import CorticalTextProcessor",
        "        proc2 = CorticalTextProcessor()",
        "",
        "        diff = compare_processors(proc1, proc2)",
        "",
        "        assert isinstance(diff, SemanticDiff)",
        "        assert len(diff.documents_added) == 0",
        "        assert len(diff.documents_removed) == 0",
        "        assert len(diff.terms_added) == 0",
        "        assert len(diff.terms_removed) == 0",
        "",
        "    def test_adding_document(self):",
        "        \"\"\"Adding a document shows in diff.\"\"\"",
        "        from cortical import CorticalTextProcessor",
        "",
        "        # Both processors need non-empty layers for term comparison",
        "        # Start with a baseline document in proc1",
        "        proc1 = CorticalTextProcessor()",
        "        proc1.process_document(\"baseline\", \"baseline content\")",
        "        proc1.compute_all()",
        "",
        "        # Proc2 has baseline plus new document",
        "        proc2 = CorticalTextProcessor()",
        "        proc2.process_document(\"baseline\", \"baseline content\")",
        "        proc2.process_document(\"doc1\", \"neural networks process data\")",
        "        proc2.compute_all()",
        "",
        "        diff = compare_processors(proc1, proc2)",
        "",
        "        # Should detect doc1 as added",
        "        assert \"doc1\" in diff.documents_added",
        "        assert \"baseline\" not in diff.documents_added",
        "        assert len(diff.documents_removed) == 0",
        "        # New terms from doc1 should be detected (those not in baseline)",
        "        assert len(diff.terms_added) > 0 or len(diff.terms_modified) > 0",
        "",
        "    def test_removing_document(self):",
        "        \"\"\"Removing a document shows in diff.\"\"\"",
        "        from cortical import CorticalTextProcessor",
        "        from cortical.layers import CorticalLayer, HierarchicalLayer",
        "",
        "        proc1 = CorticalTextProcessor()",
        "        proc1.process_document(\"doc1\", \"test content\")",
        "        proc1.compute_all()",
        "",
        "        # Proc2 needs to have layers initialized for comparison to work",
        "        proc2 = CorticalTextProcessor()",
        "        proc2.layers[CorticalLayer.TOKENS] = HierarchicalLayer(CorticalLayer.TOKENS)",
        "",
        "        diff = compare_processors(proc1, proc2)",
        "",
        "        assert \"doc1\" in diff.documents_removed",
        "        assert len(diff.documents_added) == 0",
        "        # Note: terms_removed may be 0 if proc2 has no layer, which is expected behavior",
        "",
        "    def test_modified_document(self):",
        "        \"\"\"Modified document content is detected.\"\"\"",
        "        from cortical import CorticalTextProcessor",
        "",
        "        proc1 = CorticalTextProcessor()",
        "        proc1.process_document(\"doc1\", \"original content\")",
        "        proc1.compute_all()",
        "",
        "        proc2 = CorticalTextProcessor()",
        "        proc2.process_document(\"doc1\", \"modified content\")",
        "        proc2.compute_all()",
        "",
        "        diff = compare_processors(proc1, proc2)",
        "",
        "        assert \"doc1\" in diff.documents_modified",
        "        assert \"doc1\" not in diff.documents_added",
        "        assert \"doc1\" not in diff.documents_removed",
        "",
        "    def test_pagerank_changes_detected(self):",
        "        \"\"\"PageRank importance shifts are detected.\"\"\"",
        "        from cortical import CorticalTextProcessor",
        "",
        "        # Initial state with \"neural\"",
        "        proc1 = CorticalTextProcessor()",
        "        proc1.process_document(\"doc1\", \"neural networks\")",
        "        proc1.compute_all()",
        "",
        "        # Add more documents mentioning \"neural\" to boost its importance",
        "        proc2 = CorticalTextProcessor()",
        "        proc2.process_document(\"doc1\", \"neural networks\")",
        "        proc2.process_document(\"doc2\", \"neural systems\")",
        "        proc2.process_document(\"doc3\", \"neural processing\")",
        "        proc2.compute_all()",
        "",
        "        diff = compare_processors(proc1, proc2, min_pagerank_delta=0.0001)",
        "",
        "        # \"neural\" should appear in both, may have importance change",
        "        # Either in modified terms or importance shifts",
        "        all_modified_terms = [tc.term for tc in diff.terms_modified]",
        "        all_importance_changed = [tc.term for tc in diff.importance_increased + diff.importance_decreased]",
        "",
        "        # Neural appears in both processors, so should be in one of these lists",
        "        # (depending on whether PageRank changed significantly)",
        "        assert len(all_modified_terms) > 0 or len(all_importance_changed) > 0",
        "",
        "    def test_top_movers_parameter(self):",
        "        \"\"\"top_movers parameter limits importance changes.\"\"\"",
        "        from cortical import CorticalTextProcessor",
        "",
        "        proc1 = CorticalTextProcessor()",
        "        proc1.process_document(\"doc1\", \"alpha beta gamma delta epsilon\")",
        "        proc1.compute_all()",
        "",
        "        proc2 = CorticalTextProcessor()",
        "        proc2.process_document(\"doc1\", \"alpha beta gamma delta epsilon\")",
        "        proc2.process_document(\"doc2\", \"alpha alpha alpha beta\")  # Boost some terms",
        "        proc2.compute_all()",
        "",
        "        diff = compare_processors(proc1, proc2, top_movers=2, min_pagerank_delta=0.0001)",
        "",
        "        # Should limit total importance shifts to top_movers",
        "        total_shifts = len(diff.importance_increased) + len(diff.importance_decreased)",
        "        assert total_shifts <= 2",
        "",
        "    def test_min_pagerank_delta_threshold(self):",
        "        \"\"\"min_pagerank_delta filters small changes.\"\"\"",
        "        from cortical import CorticalTextProcessor",
        "",
        "        proc1 = CorticalTextProcessor()",
        "        proc1.process_document(\"doc1\", \"test content\")",
        "        proc1.compute_all()",
        "",
        "        proc2 = CorticalTextProcessor()",
        "        proc2.process_document(\"doc1\", \"test content\")",
        "        proc2.process_document(\"doc2\", \"other stuff\")  # Small change to \"test\" PageRank",
        "        proc2.compute_all()",
        "",
        "        # High threshold - should filter out small changes",
        "        diff_strict = compare_processors(proc1, proc2, min_pagerank_delta=0.5)",
        "",
        "        # Low threshold - should catch more changes",
        "        diff_loose = compare_processors(proc1, proc2, min_pagerank_delta=0.0001)",
        "",
        "        # Loose threshold should catch more or equal changes",
        "        total_strict = len(diff_strict.importance_increased) + len(diff_strict.importance_decreased)",
        "        total_loose = len(diff_loose.importance_increased) + len(diff_loose.importance_decreased)",
        "        assert total_loose >= total_strict",
        "",
        "    def test_total_term_changes_statistic(self):",
        "        \"\"\"total_term_changes is calculated correctly.\"\"\"",
        "        from cortical import CorticalTextProcessor",
        "",
        "        proc1 = CorticalTextProcessor()",
        "        proc1.process_document(\"doc1\", \"old content\")",
        "        proc1.compute_all()",
        "",
        "        proc2 = CorticalTextProcessor()",
        "        proc2.process_document(\"doc2\", \"new content\")",
        "        proc2.compute_all()",
        "",
        "        diff = compare_processors(proc1, proc2)",
        "",
        "        expected = len(diff.terms_added) + len(diff.terms_removed) + len(diff.terms_modified)",
        "        assert diff.total_term_changes == expected",
        "",
        "    def test_new_terms_detected(self):",
        "        \"\"\"New terms appear in terms_added.\"\"\"",
        "        from cortical import CorticalTextProcessor",
        "",
        "        # Both processors need non-empty layers",
        "        # Proc1 has one set of terms",
        "        proc1 = CorticalTextProcessor()",
        "        proc1.process_document(\"doc1\", \"common baseline\")",
        "        proc1.compute_all()",
        "",
        "        # Proc2 has baseline plus unique terms",
        "        proc2 = CorticalTextProcessor()",
        "        proc2.process_document(\"doc1\", \"common baseline\")",
        "        proc2.process_document(\"doc2\", \"unique special remarkable\")",
        "        proc2.compute_all()",
        "",
        "        diff = compare_processors(proc1, proc2)",
        "",
        "        # Should detect new terms from doc2",
        "        assert len(diff.terms_added) > 0",
        "        # Check that added terms have new_pagerank set",
        "        for tc in diff.terms_added:",
        "            assert tc.change_type == \"added\"",
        "            assert tc.new_pagerank is not None or tc.new_pagerank == 0",
        "",
        "    def test_removed_terms_detected(self):",
        "        \"\"\"Removed terms appear in terms_removed.\"\"\"",
        "        from cortical import CorticalTextProcessor",
        "",
        "        # Proc1 has baseline plus document with terms to be removed",
        "        proc1 = CorticalTextProcessor()",
        "        proc1.process_document(\"baseline\", \"common content\")",
        "        proc1.process_document(\"doc1\", \"removed obsolete deprecated\")",
        "        proc1.compute_all()",
        "",
        "        # Proc2 has only baseline (doc1 removed)",
        "        proc2 = CorticalTextProcessor()",
        "        proc2.process_document(\"baseline\", \"common content\")",
        "        proc2.compute_all()",
        "",
        "        diff = compare_processors(proc1, proc2)",
        "",
        "        # Should detect terms that only appeared in doc1",
        "        assert len(diff.terms_removed) > 0",
        "        for tc in diff.terms_removed:",
        "            assert tc.change_type == \"removed\"",
        "            assert tc.old_pagerank is not None or tc.old_pagerank == 0",
        "",
        "",
        "# =============================================================================",
        "# COMPARE DOCUMENTS TESTS",
        "# =============================================================================",
        "",
        "",
        "class TestCompareDocuments:",
        "    \"\"\"Tests for compare_documents function.\"\"\"",
        "",
        "    def test_same_document_high_similarity(self, small_processor):",
        "        \"\"\"Comparing same document returns high similarity.\"\"\"",
        "        # Get a document ID from the processor",
        "        if not small_processor.documents:",
        "            pytest.skip(\"Small processor has no documents\")",
        "",
        "        doc_id = list(small_processor.documents.keys())[0]",
        "",
        "        result = compare_documents(small_processor, doc_id, doc_id)",
        "",
        "        assert result['jaccard_similarity'] == 1.0",
        "        assert result['shared_terms'] > 0",
        "        assert result['unique_to_old'] == 0",
        "        assert result['unique_to_new'] == 0",
        "",
        "    def test_different_documents_lower_similarity(self, small_processor):",
        "        \"\"\"Comparing different documents returns lower similarity.\"\"\"",
        "        if len(small_processor.documents) < 2:",
        "            pytest.skip(\"Need at least 2 documents\")",
        "",
        "        doc_ids = list(small_processor.documents.keys())",
        "        doc1, doc2 = doc_ids[0], doc_ids[1]",
        "",
        "        result = compare_documents(small_processor, doc1, doc2)",
        "",
        "        assert 0 <= result['jaccard_similarity'] <= 1.0",
        "        assert 'shared_terms' in result",
        "        assert 'unique_to_old' in result",
        "        assert 'unique_to_new' in result",
        "",
        "    def test_shared_terms_calculation(self, fresh_processor):",
        "        \"\"\"Shared terms are correctly calculated.\"\"\"",
        "        proc = fresh_processor",
        "        proc.process_document(\"doc1\", \"apple banana orange\")",
        "        proc.process_document(\"doc2\", \"banana orange grape\")",
        "        proc.compute_all()",
        "",
        "        result = compare_documents(proc, \"doc1\", \"doc2\")",
        "",
        "        # Should have some shared terms (banana, orange)",
        "        assert result['shared_terms'] >= 2  # At least banana and orange (possibly stemmed)",
        "        assert result['unique_to_old'] >= 1  # apple",
        "        assert result['unique_to_new'] >= 1  # grape",
        "",
        "    def test_no_shared_terms(self, fresh_processor):",
        "        \"\"\"Documents with no shared terms.\"\"\"",
        "        proc = fresh_processor",
        "        proc.process_document(\"doc1\", \"alpha beta gamma\")",
        "        proc.process_document(\"doc2\", \"delta epsilon zeta\")",
        "        proc.compute_all()",
        "",
        "        result = compare_documents(proc, \"doc1\", \"doc2\")",
        "",
        "        assert result['shared_terms'] == 0",
        "        assert result['jaccard_similarity'] == 0.0",
        "        assert result['unique_to_old'] > 0",
        "        assert result['unique_to_new'] > 0",
        "",
        "    def test_result_structure(self, fresh_processor):",
        "        \"\"\"Result has all expected fields.\"\"\"",
        "        proc = fresh_processor",
        "        proc.process_document(\"doc1\", \"test content\")",
        "        proc.process_document(\"doc2\", \"other content\")",
        "        proc.compute_all()",
        "",
        "        result = compare_documents(proc, \"doc1\", \"doc2\")",
        "",
        "        assert 'doc_id_old' in result",
        "        assert 'doc_id_new' in result",
        "        assert 'terms_in_old' in result",
        "        assert 'terms_in_new' in result",
        "        assert 'shared_terms' in result",
        "        assert 'unique_to_old' in result",
        "        assert 'unique_to_new' in result",
        "        assert 'jaccard_similarity' in result",
        "        assert 'shared_bigrams' in result",
        "        assert 'top_shared_terms' in result",
        "        assert 'top_unique_to_old' in result",
        "        assert 'top_unique_to_new' in result",
        "        assert 'top_shared_bigrams' in result",
        "",
        "    def test_bigram_detection(self, fresh_processor):",
        "        \"\"\"Bigrams are detected and compared.\"\"\"",
        "        proc = fresh_processor",
        "        proc.process_document(\"doc1\", \"quick brown fox jumps\")",
        "        proc.process_document(\"doc2\", \"quick brown dog runs\")",
        "        proc.compute_all()",
        "",
        "        result = compare_documents(proc, \"doc1\", \"doc2\")",
        "",
        "        # Should have at least one shared bigram (quick brown)",
        "        assert result['shared_bigrams'] >= 0  # May be 0 if bigrams not computed",
        "",
        "    def test_top_lists_limited(self, fresh_processor):",
        "        \"\"\"Top lists are limited to reasonable sizes.\"\"\"",
        "        proc = fresh_processor",
        "        many_terms = \" \".join([f\"term{i}\" for i in range(100)])",
        "        proc.process_document(\"doc1\", many_terms)",
        "        proc.process_document(\"doc2\", many_terms + \" extra unique\")",
        "        proc.compute_all()",
        "",
        "        result = compare_documents(proc, \"doc1\", \"doc2\")",
        "",
        "        assert len(result['top_shared_terms']) <= 20",
        "        assert len(result['top_unique_to_old']) <= 20",
        "        assert len(result['top_unique_to_new']) <= 20",
        "        assert len(result['top_shared_bigrams']) <= 10",
        "",
        "    def test_empty_processor_error(self, fresh_processor):",
        "        \"\"\"Empty processor returns error.\"\"\"",
        "        result = compare_documents(fresh_processor, \"doc1\", \"doc2\")",
        "",
        "        assert 'error' in result",
        "",
        "    def test_jaccard_formula(self, fresh_processor):",
        "        \"\"\"Jaccard similarity uses correct formula.\"\"\"",
        "        proc = fresh_processor",
        "        proc.process_document(\"doc1\", \"alpha beta gamma\")",
        "        proc.process_document(\"doc2\", \"beta gamma delta\")",
        "        proc.compute_all()",
        "",
        "        result = compare_documents(proc, \"doc1\", \"doc2\")",
        "",
        "        # Jaccard = |intersection| / |union|",
        "        # Shared: beta, gamma (2)",
        "        # Unique to old: alpha (1)",
        "        # Unique to new: delta (1)",
        "        # Union = 2 + 1 + 1 = 4",
        "        # Jaccard = 2/4 = 0.5",
        "        assert result['shared_terms'] >= 2",
        "        expected_jaccard = result['shared_terms'] / (",
        "            result['shared_terms'] + result['unique_to_old'] + result['unique_to_new']",
        "        )",
        "        assert result['jaccard_similarity'] == pytest.approx(expected_jaccard, abs=0.01)",
        "",
        "",
        "# =============================================================================",
        "# WHAT CHANGED TESTS",
        "# =============================================================================",
        "",
        "",
        "class TestWhatChanged:",
        "    \"\"\"Tests for what_changed function.\"\"\"",
        "",
        "    def test_identical_texts_high_similarity(self, fresh_processor):",
        "        \"\"\"Identical texts return high similarity.\"\"\"",
        "        text = \"neural networks process data\"",
        "",
        "        result = what_changed(fresh_processor, text, text)",
        "",
        "        assert result['summary']['content_similarity'] == 1.0",
        "        assert result['summary']['is_significant_change'] is False",
        "        assert len(result['tokens']['added']) == 0",
        "        assert len(result['tokens']['removed']) == 0",
        "",
        "    def test_different_texts_show_changes(self, fresh_processor):",
        "        \"\"\"Different texts show added/removed tokens.\"\"\"",
        "        # Use words that won't all be filtered as stop words",
        "        old_text = \"machine learning algorithms\"",
        "        new_text = \"deep neural networks\"",
        "",
        "        result = what_changed(fresh_processor, old_text, new_text)",
        "",
        "        assert len(result['tokens']['added']) > 0",
        "        assert len(result['tokens']['removed']) > 0",
        "        assert result['summary']['is_significant_change'] is True  # < 0.8 similarity",
        "",
        "    def test_empty_old_text(self, fresh_processor):",
        "        \"\"\"Empty old text shows all tokens as added.\"\"\"",
        "        old_text = \"\"",
        "        new_text = \"new content\"",
        "",
        "        result = what_changed(fresh_processor, old_text, new_text)",
        "",
        "        assert len(result['tokens']['added']) > 0",
        "        assert len(result['tokens']['removed']) == 0",
        "        assert result['tokens']['total_old'] == 0",
        "        assert result['tokens']['total_new'] > 0",
        "",
        "    def test_empty_new_text(self, fresh_processor):",
        "        \"\"\"Empty new text shows all tokens as removed.\"\"\"",
        "        old_text = \"old content\"",
        "        new_text = \"\"",
        "",
        "        result = what_changed(fresh_processor, old_text, new_text)",
        "",
        "        assert len(result['tokens']['removed']) > 0",
        "        assert len(result['tokens']['added']) == 0",
        "        assert result['tokens']['total_old'] > 0",
        "        assert result['tokens']['total_new'] == 0",
        "",
        "    def test_both_empty_texts(self, fresh_processor):",
        "        \"\"\"Both empty texts handled gracefully.\"\"\"",
        "        result = what_changed(fresh_processor, \"\", \"\")",
        "",
        "        assert result['tokens']['similarity'] == 0.0  # No tokens to compare",
        "        assert len(result['tokens']['added']) == 0",
        "        assert len(result['tokens']['removed']) == 0",
        "",
        "    def test_token_counts(self, fresh_processor):",
        "        \"\"\"Token counts are accurate.\"\"\"",
        "        old_text = \"alpha beta gamma\"",
        "        new_text = \"beta gamma delta epsilon\"",
        "",
        "        result = what_changed(fresh_processor, old_text, new_text)",
        "",
        "        # Old: alpha, beta, gamma (3)",
        "        # New: beta, gamma, delta, epsilon (4)",
        "        assert result['tokens']['total_old'] >= 3",
        "        assert result['tokens']['total_new'] >= 4",
        "",
        "    def test_bigram_detection(self, fresh_processor):",
        "        \"\"\"Bigrams are detected in changes.\"\"\"",
        "        old_text = \"quick brown fox\"",
        "        new_text = \"quick brown dog\"",
        "",
        "        result = what_changed(fresh_processor, old_text, new_text)",
        "",
        "        assert 'bigrams' in result",
        "        assert 'added' in result['bigrams']",
        "        assert 'removed' in result['bigrams']",
        "        assert 'unchanged_count' in result['bigrams']",
        "        assert 'similarity' in result['bigrams']",
        "",
        "    def test_bigram_similarity(self, fresh_processor):",
        "        \"\"\"Bigram similarity is calculated.\"\"\"",
        "        old_text = \"neural networks deep learning\"",
        "        new_text = \"neural networks machine learning\"",
        "",
        "        result = what_changed(fresh_processor, old_text, new_text)",
        "",
        "        # Should have some shared bigrams",
        "        assert 0 <= result['bigrams']['similarity'] <= 1",
        "        assert result['bigrams']['unchanged_count'] > 0  # \"neural networks\" shared",
        "",
        "    def test_content_similarity_formula(self, fresh_processor):",
        "        \"\"\"Content similarity is average of token and bigram similarity.\"\"\"",
        "        old_text = \"test content\"",
        "        new_text = \"test data\"",
        "",
        "        result = what_changed(fresh_processor, old_text, new_text)",
        "",
        "        expected = (result['tokens']['similarity'] + result['bigrams']['similarity']) / 2",
        "        assert result['summary']['content_similarity'] == pytest.approx(expected, abs=0.01)",
        "",
        "    def test_significant_change_threshold(self, fresh_processor):",
        "        \"\"\"Significant change threshold is 0.8.\"\"\"",
        "        # Very similar",
        "        result1 = what_changed(",
        "            fresh_processor,",
        "            \"alpha beta gamma delta epsilon\",",
        "            \"alpha beta gamma delta zeta\"",
        "        )",
        "",
        "        # Very different",
        "        result2 = what_changed(",
        "            fresh_processor,",
        "            \"alpha beta\",",
        "            \"gamma delta epsilon zeta eta theta\"",
        "        )",
        "",
        "        # First should not be significant (high similarity)",
        "        # Second should be significant (low similarity)",
        "        # Note: Actual values depend on tokenization",
        "        assert isinstance(result1['summary']['is_significant_change'], bool)",
        "        assert isinstance(result2['summary']['is_significant_change'], bool)",
        "",
        "    def test_result_structure(self, fresh_processor):",
        "        \"\"\"Result has expected structure.\"\"\"",
        "        result = what_changed(fresh_processor, \"old\", \"new\")",
        "",
        "        assert 'tokens' in result",
        "        assert 'bigrams' in result",
        "        assert 'summary' in result",
        "",
        "        assert 'added' in result['tokens']",
        "        assert 'removed' in result['tokens']",
        "        assert 'unchanged_count' in result['tokens']",
        "        assert 'total_old' in result['tokens']",
        "        assert 'total_new' in result['tokens']",
        "        assert 'similarity' in result['tokens']",
        "",
        "        assert 'added' in result['bigrams']",
        "        assert 'removed' in result['bigrams']",
        "        assert 'unchanged_count' in result['bigrams']",
        "        assert 'similarity' in result['bigrams']",
        "",
        "        assert 'content_similarity' in result['summary']",
        "        assert 'is_significant_change' in result['summary']",
        "",
        "    def test_added_tokens_sorted(self, fresh_processor):",
        "        \"\"\"Added tokens are sorted.\"\"\"",
        "        old_text = \"alpha\"",
        "        new_text = \"alpha zeta beta gamma\"",
        "",
        "        result = what_changed(fresh_processor, old_text, new_text)",
        "",
        "        added = result['tokens']['added']",
        "        if len(added) > 1:",
        "            assert added == sorted(added)",
        "",
        "    def test_removed_tokens_sorted(self, fresh_processor):",
        "        \"\"\"Removed tokens are sorted.\"\"\"",
        "        old_text = \"alpha zeta beta gamma\"",
        "        new_text = \"alpha\"",
        "",
        "        result = what_changed(fresh_processor, old_text, new_text)",
        "",
        "        removed = result['tokens']['removed']",
        "        if len(removed) > 1:",
        "            assert removed == sorted(removed)",
        "",
        "    def test_limits_token_lists(self, fresh_processor):",
        "        \"\"\"Token lists are limited to 50 items.\"\"\"",
        "        many_old = \" \".join([f\"old{i}\" for i in range(100)])",
        "        many_new = \" \".join([f\"new{i}\" for i in range(100)])",
        "",
        "        result = what_changed(fresh_processor, many_old, many_new)",
        "",
        "        assert len(result['tokens']['added']) <= 50",
        "        assert len(result['tokens']['removed']) <= 50",
        "",
        "    def test_limits_bigram_lists(self, fresh_processor):",
        "        \"\"\"Bigram lists are limited to 30 items.\"\"\"",
        "        many_old = \" \".join([f\"old{i}\" for i in range(100)])",
        "        many_new = \" \".join([f\"new{i}\" for i in range(100)])",
        "",
        "        result = what_changed(fresh_processor, many_old, many_new)",
        "",
        "        assert len(result['bigrams']['added']) <= 30",
        "        assert len(result['bigrams']['removed']) <= 30"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "tests/unit/test_persistence.py",
      "function": "import pytest",
      "start_line": 23,
      "lines_added": [
        "    # SEC-003: HMAC signature verification",
        "    SignatureVerificationError,",
        "    _compute_signature,",
        "    _save_signature,",
        "    _load_signature,",
        "    _verify_signature,",
        "    _get_signature_path,"
      ],
      "lines_removed": [],
      "context_before": [
        "from cortical.persistence import (",
        "    _get_relation_color,",
        "    _count_edge_types,",
        "    _count_relation_types,",
        "    LAYER_COLORS,",
        "    LAYER_NAMES,",
        "    export_embeddings_json,",
        "    load_embeddings_json,",
        "    export_semantic_relations_json,",
        "    load_semantic_relations_json,"
      ],
      "context_after": [
        ")",
        "from cortical.layers import CorticalLayer",
        "",
        "",
        "# =============================================================================",
        "# GET RELATION COLOR TESTS",
        "# =============================================================================",
        "",
        "",
        "class TestGetRelationColor:"
      ],
      "change_type": "add"
    },
    {
      "file": "tests/unit/test_persistence.py",
      "function": "class TestExportConceptnetJson:",
      "start_line": 1219,
      "lines_added": [
        "",
        "",
        "# =============================================================================",
        "# SEC-003: HMAC SIGNATURE VERIFICATION TESTS",
        "# =============================================================================",
        "",
        "",
        "class TestSignatureHelpers:",
        "    \"\"\"Tests for HMAC signature helper functions (SEC-003).\"\"\"",
        "",
        "    def test_get_signature_path(self):",
        "        \"\"\"Signature path is filename + .sig extension.\"\"\"",
        "        assert _get_signature_path(\"/path/to/file.pkl\") == \"/path/to/file.pkl.sig\"",
        "        assert _get_signature_path(\"data.pkl\") == \"data.pkl.sig\"",
        "",
        "    def test_compute_signature_returns_32_bytes(self):",
        "        \"\"\"HMAC-SHA256 signature is 32 bytes.\"\"\"",
        "        data = b\"test data\"",
        "        key = b\"secret key\"",
        "        sig = _compute_signature(data, key)",
        "        assert len(sig) == 32  # SHA256 produces 32 bytes",
        "",
        "    def test_compute_signature_deterministic(self):",
        "        \"\"\"Same data and key produces same signature.\"\"\"",
        "        data = b\"test data\"",
        "        key = b\"secret key\"",
        "        sig1 = _compute_signature(data, key)",
        "        sig2 = _compute_signature(data, key)",
        "        assert sig1 == sig2",
        "",
        "    def test_compute_signature_different_with_different_key(self):",
        "        \"\"\"Different keys produce different signatures.\"\"\"",
        "        data = b\"test data\"",
        "        key1 = b\"key1\"",
        "        key2 = b\"key2\"",
        "        sig1 = _compute_signature(data, key1)",
        "        sig2 = _compute_signature(data, key2)",
        "        assert sig1 != sig2",
        "",
        "    def test_compute_signature_different_with_different_data(self):",
        "        \"\"\"Different data produces different signatures.\"\"\"",
        "        data1 = b\"data1\"",
        "        data2 = b\"data2\"",
        "        key = b\"key\"",
        "        sig1 = _compute_signature(data1, key)",
        "        sig2 = _compute_signature(data2, key)",
        "        assert sig1 != sig2",
        "",
        "    def test_verify_signature_correct(self):",
        "        \"\"\"Correct signature verifies successfully.\"\"\"",
        "        data = b\"test data\"",
        "        key = b\"secret key\"",
        "        sig = _compute_signature(data, key)",
        "        assert _verify_signature(data, sig, key) is True",
        "",
        "    def test_verify_signature_wrong_key(self):",
        "        \"\"\"Signature fails with wrong key.\"\"\"",
        "        data = b\"test data\"",
        "        key = b\"secret key\"",
        "        wrong_key = b\"wrong key\"",
        "        sig = _compute_signature(data, key)",
        "        assert _verify_signature(data, sig, wrong_key) is False",
        "",
        "    def test_verify_signature_tampered_data(self):",
        "        \"\"\"Signature fails with tampered data.\"\"\"",
        "        data = b\"test data\"",
        "        tampered = b\"tampered data\"",
        "        key = b\"secret key\"",
        "        sig = _compute_signature(data, key)",
        "        assert _verify_signature(tampered, sig, key) is False",
        "",
        "    def test_save_and_load_signature(self):",
        "        \"\"\"Signature can be saved and loaded.\"\"\"",
        "        with tempfile.TemporaryDirectory() as tmpdir:",
        "            filepath = os.path.join(tmpdir, \"test.pkl\")",
        "            signature = b\"x\" * 32",
        "",
        "            _save_signature(filepath, signature)",
        "            loaded = _load_signature(filepath)",
        "",
        "            assert loaded == signature",
        "",
        "    def test_load_signature_nonexistent_returns_none(self):",
        "        \"\"\"Loading nonexistent signature returns None.\"\"\"",
        "        result = _load_signature(\"/nonexistent/file.pkl\")",
        "        assert result is None",
        "",
        "",
        "class TestSignatureVerificationError:",
        "    \"\"\"Tests for SignatureVerificationError exception.\"\"\"",
        "",
        "    def test_exception_is_raisable(self):",
        "        \"\"\"Exception can be raised and caught.\"\"\"",
        "        with pytest.raises(SignatureVerificationError):",
        "            raise SignatureVerificationError(\"test error\")",
        "",
        "    def test_exception_message(self):",
        "        \"\"\"Exception preserves message.\"\"\"",
        "        try:",
        "            raise SignatureVerificationError(\"custom message\")",
        "        except SignatureVerificationError as e:",
        "            assert \"custom message\" in str(e)",
        "",
        "",
        "class TestSaveLoadWithSignature:",
        "    \"\"\"Tests for save_processor and load_processor with HMAC signatures (SEC-003).\"\"\"",
        "",
        "    def test_save_creates_signature_file(self):",
        "        \"\"\"Saving with signing_key creates .sig file.\"\"\"",
        "        layers = create_test_layers()",
        "        documents = {\"doc1\": \"Test.\"}",
        "        key = b\"my-secret-key\"",
        "",
        "        with tempfile.TemporaryDirectory() as tmpdir:",
        "            filepath = os.path.join(tmpdir, \"test.pkl\")",
        "            sig_path = filepath + \".sig\"",
        "",
        "            save_processor(filepath, layers, documents, signing_key=key, verbose=False)",
        "",
        "            assert os.path.exists(filepath)",
        "            assert os.path.exists(sig_path)",
        "",
        "    def test_save_without_key_no_signature(self):",
        "        \"\"\"Saving without signing_key creates no .sig file.\"\"\"",
        "        layers = create_test_layers()",
        "        documents = {\"doc1\": \"Test.\"}",
        "",
        "        with tempfile.TemporaryDirectory() as tmpdir:",
        "            filepath = os.path.join(tmpdir, \"test.pkl\")",
        "            sig_path = filepath + \".sig\"",
        "",
        "            save_processor(filepath, layers, documents, verbose=False)",
        "",
        "            assert os.path.exists(filepath)",
        "            assert not os.path.exists(sig_path)",
        "",
        "    def test_load_with_correct_key_succeeds(self):",
        "        \"\"\"Loading with correct verify_key succeeds.\"\"\"",
        "        layers = create_test_layers()",
        "        documents = {\"doc1\": \"Test.\"}",
        "        key = b\"my-secret-key\"",
        "",
        "        with tempfile.TemporaryDirectory() as tmpdir:",
        "            filepath = os.path.join(tmpdir, \"test.pkl\")",
        "",
        "            save_processor(filepath, layers, documents, signing_key=key, verbose=False)",
        "            result = load_processor(filepath, verify_key=key, verbose=False)",
        "",
        "            loaded_layers, loaded_docs, _, _, _, _ = result",
        "            assert loaded_docs == documents",
        "",
        "    def test_load_with_wrong_key_fails(self):",
        "        \"\"\"Loading with wrong verify_key raises SignatureVerificationError.\"\"\"",
        "        layers = create_test_layers()",
        "        documents = {\"doc1\": \"Test.\"}",
        "        key = b\"correct-key\"",
        "        wrong_key = b\"wrong-key\"",
        "",
        "        with tempfile.TemporaryDirectory() as tmpdir:",
        "            filepath = os.path.join(tmpdir, \"test.pkl\")",
        "",
        "            save_processor(filepath, layers, documents, signing_key=key, verbose=False)",
        "",
        "            with pytest.raises(SignatureVerificationError):",
        "                load_processor(filepath, verify_key=wrong_key, verbose=False)",
        "",
        "    def test_load_tampered_file_fails(self):",
        "        \"\"\"Loading tampered file raises SignatureVerificationError.\"\"\"",
        "        layers = create_test_layers()",
        "        documents = {\"doc1\": \"Test.\"}",
        "        key = b\"my-secret-key\"",
        "",
        "        with tempfile.TemporaryDirectory() as tmpdir:",
        "            filepath = os.path.join(tmpdir, \"test.pkl\")",
        "",
        "            save_processor(filepath, layers, documents, signing_key=key, verbose=False)",
        "",
        "            # Tamper with the file",
        "            with open(filepath, \"r+b\") as f:",
        "                f.seek(100)",
        "                f.write(b\"TAMPERED\")",
        "",
        "            with pytest.raises(SignatureVerificationError):",
        "                load_processor(filepath, verify_key=key, verbose=False)",
        "",
        "    def test_load_missing_signature_file_fails(self):",
        "        \"\"\"Loading with verify_key but missing .sig raises FileNotFoundError.\"\"\"",
        "        layers = create_test_layers()",
        "        documents = {\"doc1\": \"Test.\"}",
        "        key = b\"my-secret-key\"",
        "",
        "        with tempfile.TemporaryDirectory() as tmpdir:",
        "            filepath = os.path.join(tmpdir, \"test.pkl\")",
        "",
        "            # Save without signing key (no .sig file)",
        "            save_processor(filepath, layers, documents, verbose=False)",
        "",
        "            # Try to load with verify_key",
        "            with pytest.raises(FileNotFoundError) as exc_info:",
        "                load_processor(filepath, verify_key=key, verbose=False)",
        "            assert \".sig\" in str(exc_info.value)",
        "",
        "    def test_backward_compatibility_no_key(self):",
        "        \"\"\"Loading without verify_key works (backward compatible).\"\"\"",
        "        layers = create_test_layers()",
        "        documents = {\"doc1\": \"Test.\"}",
        "",
        "        with tempfile.TemporaryDirectory() as tmpdir:",
        "            filepath = os.path.join(tmpdir, \"test.pkl\")",
        "",
        "            # Save without signing",
        "            save_processor(filepath, layers, documents, verbose=False)",
        "",
        "            # Load without verify_key",
        "            result = load_processor(filepath, verbose=False)",
        "            loaded_layers, loaded_docs, _, _, _, _ = result",
        "            assert loaded_docs == documents",
        "",
        "    def test_save_signed_load_unsigned_works(self):",
        "        \"\"\"Loading signed file without verify_key works (ignores signature).\"\"\"",
        "        layers = create_test_layers()",
        "        documents = {\"doc1\": \"Test.\"}",
        "        key = b\"my-secret-key\"",
        "",
        "        with tempfile.TemporaryDirectory() as tmpdir:",
        "            filepath = os.path.join(tmpdir, \"test.pkl\")",
        "",
        "            # Save with signing key",
        "            save_processor(filepath, layers, documents, signing_key=key, verbose=False)",
        "",
        "            # Load without verify_key (ignores .sig file)",
        "            result = load_processor(filepath, verbose=False)",
        "            loaded_layers, loaded_docs, _, _, _, _ = result",
        "            assert loaded_docs == documents",
        "",
        "    def test_signature_is_32_bytes(self):",
        "        \"\"\"Signature file contains 32-byte HMAC-SHA256.\"\"\"",
        "        layers = create_test_layers()",
        "        documents = {\"doc1\": \"Test.\"}",
        "        key = b\"my-secret-key\"",
        "",
        "        with tempfile.TemporaryDirectory() as tmpdir:",
        "            filepath = os.path.join(tmpdir, \"test.pkl\")",
        "            sig_path = filepath + \".sig\"",
        "",
        "            save_processor(filepath, layers, documents, signing_key=key, verbose=False)",
        "",
        "            with open(sig_path, \"rb\") as f:",
        "                signature = f.read()",
        "            assert len(signature) == 32",
        "",
        "    def test_different_keys_different_signatures(self):",
        "        \"\"\"Different signing keys produce different signatures.\"\"\"",
        "        layers = create_test_layers()",
        "        documents = {\"doc1\": \"Test.\"}",
        "        key1 = b\"key1\"",
        "        key2 = b\"key2\"",
        "",
        "        with tempfile.TemporaryDirectory() as tmpdir:",
        "            filepath1 = os.path.join(tmpdir, \"test1.pkl\")",
        "            filepath2 = os.path.join(tmpdir, \"test2.pkl\")",
        "",
        "            save_processor(filepath1, layers, documents, signing_key=key1, verbose=False)",
        "            save_processor(filepath2, layers, documents, signing_key=key2, verbose=False)",
        "",
        "            sig1 = _load_signature(filepath1)",
        "            sig2 = _load_signature(filepath2)",
        "",
        "            assert sig1 != sig2",
        "",
        "    def test_verbose_logging_with_signature(self):",
        "        \"\"\"Verbose mode logs signature operations.\"\"\"",
        "        layers = create_test_layers()",
        "        documents = {\"doc1\": \"Test.\"}",
        "        key = b\"my-secret-key\"",
        "",
        "        with tempfile.TemporaryDirectory() as tmpdir:",
        "            filepath = os.path.join(tmpdir, \"test.pkl\")",
        "",
        "            # Should not raise with verbose=True",
        "            save_processor(filepath, layers, documents, signing_key=key, verbose=True)",
        "            load_processor(filepath, verify_key=key, verbose=True)"
      ],
      "lines_removed": [],
      "context_before": [
        "        try:",
        "            graph = export_conceptnet_json(",
        "                filepath, layers,",
        "                semantic_relations=relations,",
        "                verbose=False",
        "            )",
        "            # Should handle large relations list",
        "            assert \"edges\" in graph",
        "        finally:",
        "            os.unlink(filepath)"
      ],
      "context_after": [],
      "change_type": "add"
    }
  ],
  "hour_of_day": 12,
  "day_of_week": "Sunday",
  "seconds_since_last_commit": -92008,
  "is_merge": false,
  "is_initial": false,
  "parent_count": 1,
  "session_id": null,
  "related_chats": [],
  "ci_result": null,
  "reverted": false,
  "amended": false
}