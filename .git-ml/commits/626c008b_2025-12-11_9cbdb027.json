{
  "hash": "626c008b49691d797aadbd81e61d83bcf434ef9e",
  "message": "Add semantic chunk boundaries for code (Task #86)",
  "author": "Claude",
  "timestamp": "2025-12-11 03:22:43 +0000",
  "branch": "claude/multi-index-design-DvifZ",
  "files_changed": [
    "TASK_LIST.md",
    "cortical/processor.py",
    "cortical/query.py",
    "tests/test_query.py"
  ],
  "insertions": 442,
  "deletions": 31,
  "hunks": [
    {
      "file": "TASK_LIST.md",
      "function": "results = processor.find_passages_for_query(\"class Minicolumn\", use_definition_s",
      "start_line": 2959,
      "lines_added": [
        "**Files:** `cortical/query.py`",
        "**Status:** [x] Completed (2025-12-11) - Addressed by Tasks #84 and #66",
        "**Solution Applied:**",
        "This is now addressed by two mechanisms:",
        "",
        "1. **Definition search (Task #84)** applies 0.6x penalty to test files for definition queries like \"class Minicolumn\" and \"def compute_pagerank\"",
        "2. **Doc-type boost (Task #66)** applies 0.8x boost to test files via `DOC_TYPE_BOOSTS`. This is applied for:",
        "   - Conceptual queries (auto-detected when `auto_detect_intent=True`)",
        "   - When `prefer_docs=True` is set",
        "   - Custom boosts via `custom_boosts` parameter",
        "",
        "**Test file detection:**",
        "```python",
        "is_test = (doc_id.startswith('tests/') or '_test' in doc_id or 'test_' in doc_id)",
        "```",
        "",
        "**Result:**",
        "For \"class Minicolumn\" queries, the actual class definition in `minicolumn.py` now ranks higher than test mentions in `test_layers.py`",
        "**Files:** `cortical/query.py`, `cortical/processor.py`, `tests/test_query.py`",
        "**Status:** [x] Completed (2025-12-11)",
        "**Solution Applied:**",
        "1. Added `find_code_boundaries()` to detect class/function/decorator boundaries",
        "2. Added `create_code_aware_chunks()` to split at semantic boundaries",
        "3. Added `is_code_file()` to detect code files by extension",
        "4. Added `use_code_aware_chunks` parameter to `find_passages_for_query()` (default True)",
        "5. Code files automatically use semantic chunking, other files use fixed chunking",
        "",
        "**Files Modified:**",
        "- `cortical/query.py` - Added code-aware chunking functions (~150 lines)",
        "- `cortical/processor.py` - Updated processor wrapper",
        "- `tests/test_query.py` - Added 15 new tests",
        "**Supported boundaries:**",
        "- Class definitions (`class Foo:`)",
        "- Function/method definitions (`def bar():`, `async def baz():`)",
        "- Decorators (`@decorator`)",
        "- Comment separators (`# ---` or `# ===`)",
        "- Blank line sequences",
        "",
        "**Usage:**",
        "# Enabled by default for code files",
        "results = processor.find_passages_for_query(\"class Minicolumn\")",
        "",
        "# Disable if needed",
        "results = processor.find_passages_for_query(\"class Minicolumn\", use_code_aware_chunks=False)"
      ],
      "lines_removed": [
        "**Files:** `cortical/query.py`, `scripts/search_codebase.py`",
        "**Status:** [ ] Not Started",
        "**Solution Options:**",
        "1. Add file path-based scoring penalty for test files when query intent is \"implementation\"",
        "2. Detect test files (tests/, *_test.py, test_*.py) and apply negative boost",
        "3. Add user preference for \"prefer source over tests\" in search",
        "4. Use query intent detection to auto-adjust (implementation queries → penalize tests)",
        "**Considerations:**",
        "- Should be configurable (sometimes users want to find tests)",
        "- Could integrate with existing `get_doc_type_boost()` function",
        "- Balance: tests are valuable for understanding usage patterns",
        "**Files:** `cortical/query.py`",
        "**Status:** [ ] Not Started",
        "**Solution:**",
        "1. Detect code structure boundaries (class, def, blank lines)",
        "2. Adjust chunk boundaries to align with semantic units",
        "3. For Python: use indentation changes as boundary hints",
        "4. Consider AST-based chunking for supported languages",
        "**Example improvement:**",
        "def create_code_aware_chunks(text: str, target_size: int = 500):",
        "    \"\"\"Create chunks aligned to code structure boundaries.\"\"\"",
        "    # Find all class/def boundaries",
        "    boundaries = [m.start() for m in re.finditer(r'^(?:class |def )', text, re.M)]",
        "    # Create chunks that start at boundaries",
        "    ..."
      ],
      "context_before": [
        "",
        "# Check if query is a definition query",
        "is_def, def_type, name = processor.is_definition_query(\"def compute_pagerank\")",
        "# (True, 'function', 'compute_pagerank')",
        "```",
        "",
        "---",
        "",
        "### 85. Improve Test File vs Source File Ranking",
        ""
      ],
      "context_after": [
        "**Priority:** Medium",
        "**Category:** Code Search",
        "",
        "**Problem:**",
        "Test files often rank higher than source files because they mention class/function names more frequently (in test method names, assertions, etc.). This is counterintuitive for users looking for implementations.",
        "",
        "**Found via dog-fooding:** Searching \"class Minicolumn\" returns test_layers.py results before minicolumn.py results.",
        "",
        "",
        "",
        "---",
        "",
        "### 86. Add Semantic Chunk Boundaries for Code",
        "",
        "**Priority:** Medium",
        "**Category:** Code Search",
        "",
        "**Problem:**",
        "Current chunking uses fixed character boundaries which can split code mid-function or mid-class. This creates passages that lack context and score poorly.",
        "",
        "**Found via dog-fooding:** The chunk containing `class Minicolumn:` starts mid-way through the previous function's code.",
        "",
        "",
        "```python",
        "```",
        "",
        "---",
        "",
        "## Summary Table",
        "",
        "| # | Priority | Task | Status | Category |",
        "|---|----------|------|--------|----------|",
        "| 67 | Low | Fix O(n) lookup in showcase | ✓ Done | Showcase |",
        "| 68 | Medium | Add code-specific features to showcase | ✓ Done | Showcase |"
      ],
      "change_type": "modify"
    },
    {
      "file": "TASK_LIST.md",
      "function": "def create_code_aware_chunks(text: str, target_size: int = 500):",
      "start_line": 3034,
      "lines_added": [
        "| 85 | Medium | Improve test file vs source file ranking | ✓ Done | Code Search |",
        "| 86 | Medium | Add semantic chunk boundaries for code | ✓ Done | Code Search |"
      ],
      "lines_removed": [
        "| 85 | Medium | Improve test file vs source file ranking | | Code Search |",
        "| 86 | Medium | Add semantic chunk boundaries for code | | Code Search |"
      ],
      "context_before": [
        "| 75 | Medium | Add \"What Changed?\" semantic diff | | Developer Experience |",
        "| 76 | Medium | Add \"Suggest Related Files\" feature | | Developer Experience |",
        "| 77 | High | Add interactive \"Ask the Codebase\" mode | ✓ Done | Developer Experience |",
        "| 78 | Low | Add code pattern detection | | Developer Experience |",
        "| 79 | Low | Add corpus health dashboard | | Developer Experience |",
        "| 80 | Low | Add \"Learning Mode\" for new contributors | | Developer Experience |",
        "| 81 | High | Fix tokenizer underscore-prefixed identifiers | ✓ Done | Code Search |",
        "| 82 | High | Add code stop words filter for query expansion | ✓ Done | Code Search |",
        "| 83 | Medium | Add definition-aware boosting for class/def queries | ✓ Done | Code Search |",
        "| 84 | High | Add direct definition pattern search | ✓ Done | Code Search |"
      ],
      "context_after": [
        "",
        "---",
        "",
        "*Added 2025-12-11, completions updated 2025-12-11*"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/processor.py",
      "function": "class CorticalTextProcessor:",
      "start_line": 1771,
      "lines_added": [
        "        custom_boosts: Optional[Dict[str, float]] = None,",
        "        use_code_aware_chunks: bool = True",
        "        For code files (.py, .js, etc.), semantic chunk boundaries are used to",
        "        align chunks with class/function definitions rather than fixed positions.",
        "",
        "            use_code_aware_chunks: Use semantic boundaries for code files (default True)"
      ],
      "lines_removed": [
        "        custom_boosts: Optional[Dict[str, float]] = None"
      ],
      "context_before": [
        "        chunk_size: int = 512,",
        "        overlap: int = 128,",
        "        use_expansion: bool = True,",
        "        doc_filter: Optional[List[str]] = None,",
        "        use_semantic: bool = True,",
        "        use_definition_search: bool = True,",
        "        definition_boost: float = 5.0,",
        "        apply_doc_boost: bool = True,",
        "        auto_detect_intent: bool = True,",
        "        prefer_docs: bool = False,"
      ],
      "context_after": [
        "    ) -> List[Tuple[str, str, int, int, float]]:",
        "        \"\"\"",
        "        Find text passages most relevant to a query (for RAG systems).",
        "",
        "        Instead of returning just document IDs, this returns actual text passages",
        "        with position information suitable for context windows and citations.",
        "",
        "        For definition queries (e.g., \"class Minicolumn\", \"def compute_pagerank\"),",
        "        the function will directly search for definition patterns and inject those",
        "        results with a high score, ensuring actual definitions appear in top results.",
        "",
        "        For conceptual queries (e.g., \"what is PageRank\", \"explain architecture\"),",
        "        documentation passages are boosted when auto_detect_intent=True.",
        "",
        "        Args:",
        "            query_text: Search query",
        "            top_n: Number of passages to return",
        "            chunk_size: Size of each chunk in characters (default 512)",
        "            overlap: Overlap between chunks in characters (default 128)",
        "            use_expansion: Whether to expand query terms",
        "            doc_filter: Optional list of doc_ids to restrict search to",
        "            use_semantic: Whether to use semantic relations for expansion (if available)",
        "            use_definition_search: Whether to search for definition patterns (default True)",
        "            definition_boost: Score boost for definition matches (default 5.0)",
        "            apply_doc_boost: Whether to apply document-type boosting (default True)",
        "            auto_detect_intent: Auto-detect conceptual queries and boost docs (default True)",
        "            prefer_docs: Always boost documentation regardless of query type (default False)",
        "            custom_boosts: Optional custom boost factors for doc types",
        "",
        "        Returns:",
        "            List of (passage_text, doc_id, start_char, end_char, score) tuples",
        "            ranked by relevance",
        "",
        "        Example:",
        "            >>> # For conceptual queries, docs are auto-boosted",
        "            >>> results = processor.find_passages_for_query(\"what is PageRank\")",
        "            >>> for passage, doc_id, start, end, score in results:",
        "            ...     print(f\"[{doc_id}:{start}-{end}] {passage[:50]}... (score: {score:.3f})\")"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/processor.py",
      "function": "class CorticalTextProcessor:",
      "start_line": 1829,
      "lines_added": [
        "            custom_boosts=custom_boosts,",
        "            use_code_aware_chunks=use_code_aware_chunks"
      ],
      "lines_removed": [
        "            custom_boosts=custom_boosts"
      ],
      "context_before": [
        "            use_expansion=use_expansion,",
        "            doc_filter=doc_filter,",
        "            semantic_relations=self.semantic_relations if use_semantic else None,",
        "            use_semantic=use_semantic,",
        "            use_definition_search=use_definition_search,",
        "            definition_boost=definition_boost,",
        "            apply_doc_boost=apply_doc_boost,",
        "            doc_metadata=self.document_metadata,",
        "            auto_detect_intent=auto_detect_intent,",
        "            prefer_docs=prefer_docs,"
      ],
      "context_after": [
        "        )",
        "",
        "    def is_definition_query(self, query_text: str) -> Tuple[bool, Optional[str], Optional[str]]:",
        "        \"\"\"",
        "        Detect if a query is looking for a code definition.",
        "",
        "        Recognizes patterns like:",
        "        - \"class Minicolumn\"",
        "        - \"def compute_pagerank\"",
        "        - \"function tokenize\""
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/query.py",
      "function": "def create_chunks(",
      "start_line": 1471,
      "lines_added": [
        "# Pattern to detect code structure boundaries",
        "CODE_BOUNDARY_PATTERN = re.compile(",
        "    r'^(?:'",
        "    r'class\\s+\\w+|'          # Class definitions",
        "    r'def\\s+\\w+|'            # Function definitions",
        "    r'async\\s+def\\s+\\w+|'    # Async function definitions",
        "    r'@\\w+|'                 # Decorators",
        "    r'#\\s*[-=]{3,}|'         # Comment separators (# --- or # ===)",
        "    r'\"\"\"[^\"]*\"\"\"|'          # Module/class docstrings (simple)",
        "    r\"'''[^']*'''\"           # Module/class docstrings (simple, single quotes)",
        "    r')',",
        "    re.MULTILINE",
        ")",
        "",
        "",
        "def find_code_boundaries(text: str) -> List[int]:",
        "    \"\"\"",
        "    Find semantic boundaries in code (class/function definitions, decorators).",
        "",
        "    Args:",
        "        text: Source code text",
        "",
        "    Returns:",
        "        Sorted list of character positions where semantic units begin",
        "    \"\"\"",
        "    boundaries = set([0])  # Always include start",
        "",
        "    # Find class/def boundaries",
        "    for match in CODE_BOUNDARY_PATTERN.finditer(text):",
        "        # Find the start of the line containing this match",
        "        line_start = text.rfind('\\n', 0, match.start()) + 1",
        "        boundaries.add(line_start)",
        "",
        "    # Also add positions after blank line sequences (natural section breaks)",
        "    blank_line_pattern = re.compile(r'\\n\\n+')",
        "    for match in blank_line_pattern.finditer(text):",
        "        boundaries.add(match.end())",
        "",
        "    return sorted(boundaries)",
        "",
        "",
        "def create_code_aware_chunks(",
        "    text: str,",
        "    target_size: int = 512,",
        "    min_size: int = 100,",
        "    max_size: int = 1024",
        ") -> List[Tuple[str, int, int]]:",
        "    \"\"\"",
        "    Create chunks aligned to code structure boundaries.",
        "",
        "    Unlike fixed-size chunking, this function tries to split code at",
        "    natural boundaries (class definitions, function definitions, blank lines)",
        "    to preserve semantic context within each chunk.",
        "",
        "    Args:",
        "        text: Source code text to chunk",
        "        target_size: Target chunk size in characters (default 512)",
        "        min_size: Minimum chunk size - won't create chunks smaller than this (default 100)",
        "        max_size: Maximum chunk size - will split even mid-code if exceeded (default 1024)",
        "",
        "    Returns:",
        "        List of (chunk_text, start_char, end_char) tuples",
        "",
        "    Example:",
        "        >>> text = '''",
        "        ... class Foo:",
        "        ...     def bar(self):",
        "        ...         pass",
        "        ...",
        "        ... class Baz:",
        "        ...     def qux(self):",
        "        ...         pass",
        "        ... '''",
        "        >>> chunks = create_code_aware_chunks(text, target_size=100)",
        "        >>> # Chunks will be aligned to class/function boundaries",
        "    \"\"\"",
        "    if not text:",
        "        return []",
        "",
        "    if len(text) <= target_size:",
        "        return [(text, 0, len(text))]",
        "",
        "    boundaries = find_code_boundaries(text)",
        "    boundaries.append(len(text))  # Add end of text",
        "",
        "    chunks = []",
        "    chunk_start = 0",
        "    i = 1",
        "",
        "    while chunk_start < len(text):",
        "        # Find the next boundary that would exceed target_size",
        "        best_end = chunk_start + max_size  # Default to max_size if no boundary found",
        "",
        "        # Look for a boundary between target_size and max_size",
        "        for j in range(i, len(boundaries)):",
        "            boundary = boundaries[j]",
        "            chunk_len = boundary - chunk_start",
        "",
        "            if chunk_len >= target_size:",
        "                if chunk_len <= max_size:",
        "                    # Good boundary within range",
        "                    best_end = boundary",
        "                    i = j + 1",
        "                    break",
        "                else:",
        "                    # Boundary too far, use previous one or force split",
        "                    if j > i:",
        "                        prev_boundary = boundaries[j - 1]",
        "                        prev_len = prev_boundary - chunk_start",
        "                        if prev_len >= min_size:",
        "                            best_end = prev_boundary",
        "                            i = j",
        "                            break",
        "                    # Force split at max_size",
        "                    best_end = chunk_start + max_size",
        "                    # Find the next boundary after our split point",
        "                    for k in range(i, len(boundaries)):",
        "                        if boundaries[k] > best_end:",
        "                            i = k",
        "                            break",
        "                    break",
        "        else:",
        "            # Reached end of boundaries, use text end",
        "            best_end = len(text)",
        "            i = len(boundaries)",
        "",
        "        # Ensure we don't exceed max_size",
        "        if best_end - chunk_start > max_size:",
        "            best_end = chunk_start + max_size",
        "",
        "        # Create chunk if non-empty",
        "        chunk_text = text[chunk_start:best_end]",
        "        if chunk_text.strip():",
        "            chunks.append((chunk_text, chunk_start, best_end))",
        "",
        "        chunk_start = best_end",
        "",
        "    return chunks",
        "",
        "",
        "def is_code_file(doc_id: str) -> bool:",
        "    \"\"\"",
        "    Determine if a document is a code file based on its path/extension.",
        "",
        "    Args:",
        "        doc_id: Document identifier (typically a file path)",
        "",
        "    Returns:",
        "        True if the document appears to be a code file",
        "    \"\"\"",
        "    code_extensions = {",
        "        '.py', '.js', '.ts', '.jsx', '.tsx', '.java', '.c', '.cpp', '.h',",
        "        '.go', '.rs', '.rb', '.php', '.swift', '.kt', '.scala', '.cs'",
        "    }",
        "    for ext in code_extensions:",
        "        if doc_id.endswith(ext):",
        "            return True",
        "    return False",
        "",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "        end = min(start + chunk_size, text_len)",
        "        chunk = text[start:end]",
        "        chunks.append((chunk, start, end))",
        "",
        "        if end >= text_len:",
        "            break",
        "",
        "    return chunks",
        "",
        ""
      ],
      "context_after": [
        "def precompute_term_cols(",
        "    query_terms: Dict[str, float],",
        "    layer0: HierarchicalLayer",
        ") -> Dict[str, 'Minicolumn']:",
        "    \"\"\"",
        "    Pre-compute minicolumn lookups for query terms.",
        "",
        "    This avoids repeated O(1) dictionary lookups for each chunk,",
        "    enabling faster scoring when processing many chunks.",
        ""
      ],
      "change_type": "add"
    },
    {
      "file": "cortical/query.py",
      "function": "def find_passages_for_query(",
      "start_line": 1600,
      "lines_added": [
        "    custom_boosts: Optional[Dict[str, float]] = None,",
        "    use_code_aware_chunks: bool = True",
        "    For code files, semantic chunk boundaries can be used to align chunks",
        "    with class/function definitions rather than fixed character positions.",
        "",
        "        use_code_aware_chunks: Use semantic boundaries for code files (default True)"
      ],
      "lines_removed": [
        "    custom_boosts: Optional[Dict[str, float]] = None"
      ],
      "context_before": [
        "    use_expansion: bool = True,",
        "    doc_filter: Optional[List[str]] = None,",
        "    semantic_relations: Optional[List[Tuple[str, str, str, float]]] = None,",
        "    use_semantic: bool = True,",
        "    use_definition_search: bool = True,",
        "    definition_boost: float = DEFINITION_BOOST,",
        "    apply_doc_boost: bool = True,",
        "    doc_metadata: Optional[Dict[str, Dict[str, Any]]] = None,",
        "    auto_detect_intent: bool = True,",
        "    prefer_docs: bool = False,"
      ],
      "context_after": [
        ") -> List[Tuple[str, str, int, int, float]]:",
        "    \"\"\"",
        "    Find text passages most relevant to a query.",
        "",
        "    This is the key function for RAG systems - instead of returning document IDs,",
        "    it returns actual text passages with position information for citations.",
        "",
        "    For definition queries (e.g., \"class Minicolumn\", \"def compute_pagerank\"),",
        "    this function will directly search for the definition pattern and inject",
        "    those results with a high score, ensuring definitions appear in top results.",
        "",
        "    For conceptual queries (e.g., \"what is PageRank\", \"explain architecture\"),",
        "    documentation passages are boosted to appear higher in results when",
        "    auto_detect_intent=True.",
        "",
        "    Args:",
        "        query_text: Search query",
        "        layers: Dictionary of layers",
        "        tokenizer: Tokenizer instance",
        "        documents: Dict mapping doc_id to document text",
        "        top_n: Number of passages to return",
        "        chunk_size: Size of each chunk in characters (default 512)",
        "        overlap: Overlap between chunks in characters (default 128)",
        "        use_expansion: Whether to expand query terms",
        "        doc_filter: Optional list of doc_ids to restrict search to",
        "        semantic_relations: Optional list of semantic relations for expansion",
        "        use_semantic: Whether to use semantic relations for expansion (if available)",
        "        use_definition_search: Whether to search for definition patterns (default True)",
        "        definition_boost: Score boost for definition matches (default 5.0)",
        "        apply_doc_boost: Whether to apply document-type boosting (default True)",
        "        doc_metadata: Optional metadata dict {doc_id: {doc_type: ..., ...}}",
        "        auto_detect_intent: Auto-detect conceptual queries and boost docs (default True)",
        "        prefer_docs: Always boost documentation regardless of query type (default False)",
        "        custom_boosts: Optional custom boost factors for doc types",
        "",
        "    Returns:",
        "        List of (passage_text, doc_id, start_char, end_char, score) tuples",
        "        ranked by relevance",
        "    \"\"\"",
        "    layer0 = layers[CorticalLayer.TOKENS]",
        "",
        "    # Determine if we should apply doc-type boosting",
        "    should_boost = apply_doc_boost and (",
        "        prefer_docs or (auto_detect_intent and is_conceptual_query(query_text))"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/query.py",
      "function": "def find_passages_for_query(",
      "start_line": 1707,
      "lines_added": [
        "",
        "        # Use code-aware chunking for code files if enabled",
        "        if use_code_aware_chunks and is_code_file(doc_id):",
        "            chunks = create_code_aware_chunks(",
        "                text,",
        "                target_size=chunk_size,",
        "                min_size=max(50, chunk_size // 4),",
        "                max_size=chunk_size * 2",
        "            )",
        "        else:",
        "            chunks = create_chunks(text, chunk_size, overlap)"
      ],
      "lines_removed": [
        "        chunks = create_chunks(text, chunk_size, overlap)"
      ],
      "context_before": [
        "    passages: List[Tuple[str, str, int, int, float]] = []",
        "",
        "    # Track definition passage locations to avoid duplicates",
        "    def_locations = {(p[1], p[2], p[3]) for p in definition_passages}",
        "",
        "    for doc_id, doc_score in doc_scores:",
        "        if doc_id not in documents:",
        "            continue",
        "",
        "        text = documents[doc_id]"
      ],
      "context_after": [
        "",
        "        # Pre-compute doc-type boost for this document",
        "        doc_type_boost = get_doc_type_boost(doc_id, doc_metadata, custom_boosts) if should_boost else 1.0",
        "",
        "        for chunk_text, start_char, end_char in chunks:",
        "            # Skip if this overlaps with a definition passage",
        "            if (doc_id, start_char, end_char) in def_locations:",
        "                continue",
        "",
        "            # Use fast scoring with pre-computed lookups"
      ],
      "change_type": "modify"
    },
    {
      "file": "tests/test_query.py",
      "function": "from cortical.query import (",
      "start_line": 37,
      "lines_added": [
        "    find_code_boundaries,",
        "    create_code_aware_chunks,",
        "    is_code_file,"
      ],
      "lines_removed": [],
      "context_before": [
        "    complete_analogy,",
        "    complete_analogy_simple,",
        "    query_with_spreading_activation,",
        "    VALID_RELATION_CHAINS,",
        "    is_definition_query,",
        "    find_definition_in_text,",
        "    find_definition_passages,",
        "    DEFINITION_QUERY_PATTERNS,",
        "    DEFINITION_SOURCE_PATTERNS,",
        "    DEFINITION_BOOST,"
      ],
      "context_after": [
        ")",
        "",
        "",
        "class TestScoreRelationPath(unittest.TestCase):",
        "    \"\"\"Test relation path scoring.\"\"\"",
        "",
        "    def test_empty_path(self):",
        "        \"\"\"Empty path should return 1.0.\"\"\"",
        "        self.assertEqual(score_relation_path([]), 1.0)",
        ""
      ],
      "change_type": "add"
    },
    {
      "file": "tests/test_query.py",
      "function": "class TestPassageDocTypeBoostIntegration(unittest.TestCase):",
      "start_line": 1466,
      "lines_added": [
        "class TestCodeAwareChunking(unittest.TestCase):",
        "    \"\"\"Test code-aware chunking functions.\"\"\"",
        "",
        "    def test_is_code_file_python(self):",
        "        \"\"\"Python files should be detected as code.\"\"\"",
        "        self.assertTrue(is_code_file('module.py'))",
        "        self.assertTrue(is_code_file('path/to/file.py'))",
        "",
        "    def test_is_code_file_javascript(self):",
        "        \"\"\"JavaScript files should be detected as code.\"\"\"",
        "        self.assertTrue(is_code_file('app.js'))",
        "        self.assertTrue(is_code_file('component.tsx'))",
        "",
        "    def test_is_code_file_markdown(self):",
        "        \"\"\"Markdown files should not be detected as code.\"\"\"",
        "        self.assertFalse(is_code_file('README.md'))",
        "        self.assertFalse(is_code_file('docs/guide.md'))",
        "",
        "    def test_is_code_file_other(self):",
        "        \"\"\"Other extensions should not be detected as code.\"\"\"",
        "        self.assertFalse(is_code_file('data.json'))",
        "        self.assertFalse(is_code_file('config.yaml'))",
        "",
        "    def test_find_code_boundaries_class(self):",
        "        \"\"\"Should find class definition boundaries.\"\"\"",
        "        code = '''",
        "import os",
        "",
        "class MyClass:",
        "    def method(self):",
        "        pass",
        "'''",
        "        boundaries = find_code_boundaries(code)",
        "        self.assertIn(0, boundaries)  # Start",
        "        # Should find the class line",
        "        class_line_start = code.find('class MyClass')",
        "        line_start = code.rfind('\\n', 0, class_line_start) + 1",
        "        self.assertIn(line_start, boundaries)",
        "",
        "    def test_find_code_boundaries_function(self):",
        "        \"\"\"Should find function definition boundaries.\"\"\"",
        "        code = '''def foo():",
        "    pass",
        "",
        "def bar():",
        "    pass",
        "'''",
        "        boundaries = find_code_boundaries(code)",
        "        # Should find both function boundaries",
        "        self.assertGreater(len(boundaries), 1)",
        "",
        "    def test_find_code_boundaries_blank_lines(self):",
        "        \"\"\"Should find blank line boundaries.\"\"\"",
        "        code = '''first section",
        "",
        "second section",
        "",
        "third section",
        "'''",
        "        boundaries = find_code_boundaries(code)",
        "        # Should include positions after blank lines",
        "        self.assertGreater(len(boundaries), 1)",
        "",
        "    def test_create_code_aware_chunks_empty(self):",
        "        \"\"\"Empty text should return empty list.\"\"\"",
        "        chunks = create_code_aware_chunks('')",
        "        self.assertEqual(chunks, [])",
        "",
        "    def test_create_code_aware_chunks_small_text(self):",
        "        \"\"\"Text smaller than target should return single chunk.\"\"\"",
        "        code = 'def foo(): pass'",
        "        chunks = create_code_aware_chunks(code, target_size=100)",
        "        self.assertEqual(len(chunks), 1)",
        "        self.assertEqual(chunks[0][0], code)",
        "",
        "    def test_create_code_aware_chunks_splits_at_boundaries(self):",
        "        \"\"\"Should split at class/function boundaries.\"\"\"",
        "        # Create code long enough to require multiple chunks",
        "        code = '''class FirstClass:",
        "    \"\"\"First class docstring with enough text to make this substantial.\"\"\"",
        "",
        "    def __init__(self):",
        "        self.value = 0",
        "        self.data = {}",
        "        self.cache = []",
        "",
        "    def method1(self):",
        "        \"\"\"A method that does something important.\"\"\"",
        "        result = self.value * 2",
        "        return result",
        "",
        "",
        "class SecondClass:",
        "    \"\"\"Second class docstring with substantial documentation text here.\"\"\"",
        "",
        "    def __init__(self):",
        "        self.items = []",
        "        self.count = 0",
        "",
        "    def method2(self):",
        "        \"\"\"Another method with documentation.\"\"\"",
        "        for item in self.items:",
        "            self.count += item",
        "        return self.count",
        "'''",
        "        # With target_size=200, this ~600 char code should split into multiple chunks",
        "        chunks = create_code_aware_chunks(code, target_size=200, min_size=50, max_size=400)",
        "        self.assertGreater(len(chunks), 1)",
        "",
        "        # Check that chunks start at sensible boundaries",
        "        for chunk_text, start, end in chunks:",
        "            # Chunks should not be empty",
        "            self.assertTrue(chunk_text.strip())",
        "",
        "    def test_create_code_aware_chunks_respects_max_size(self):",
        "        \"\"\"Should not exceed max_size.\"\"\"",
        "        # Create code with a very long function",
        "        long_function = 'def long_func():\\n' + '    x = 1\\n' * 100",
        "        chunks = create_code_aware_chunks(long_function, target_size=200, max_size=400)",
        "",
        "        for chunk_text, start, end in chunks:",
        "            self.assertLessEqual(len(chunk_text), 400)",
        "",
        "    def test_create_code_aware_chunks_no_whitespace_only(self):",
        "        \"\"\"Should not return whitespace-only chunks.\"\"\"",
        "        code = '''class A:",
        "    pass",
        "",
        "",
        "",
        "class B:",
        "    pass",
        "'''",
        "        chunks = create_code_aware_chunks(code, target_size=50, min_size=10)",
        "        for chunk_text, start, end in chunks:",
        "            self.assertTrue(chunk_text.strip())",
        "",
        "",
        "class TestCodeAwareChunkingIntegration(unittest.TestCase):",
        "    \"\"\"Integration tests for code-aware chunking in passage search.\"\"\"",
        "",
        "    def setUp(self):",
        "        \"\"\"Set up processor with code documents.\"\"\"",
        "        self.processor = CorticalTextProcessor()",
        "",
        "        # Add a code file with multiple classes/functions",
        "        self.processor.process_document('cortical/example.py', '''",
        "\"\"\"Example module with multiple classes and functions.\"\"\"",
        "",
        "import os",
        "from typing import Dict, List",
        "",
        "class FirstProcessor:",
        "    \"\"\"First processor class for demonstration.\"\"\"",
        "",
        "    def __init__(self):",
        "        self.data = {}",
        "",
        "    def process(self, item):",
        "        \"\"\"Process a single item.\"\"\"",
        "        return item * 2",
        "",
        "",
        "class SecondProcessor:",
        "    \"\"\"Second processor class for demonstration.\"\"\"",
        "",
        "    def __init__(self):",
        "        self.cache = []",
        "",
        "    def process_batch(self, items):",
        "        \"\"\"Process multiple items at once.\"\"\"",
        "        return [x * 3 for x in items]",
        "",
        "",
        "def utility_function(x, y):",
        "    \"\"\"A utility function outside classes.\"\"\"",
        "    return x + y",
        "''')",
        "",
        "        self.processor.compute_all()",
        "",
        "    def test_code_aware_chunks_enabled_by_default(self):",
        "        \"\"\"Code-aware chunking should be enabled by default.\"\"\"",
        "        results = self.processor.find_passages_for_query(",
        "            \"SecondProcessor\",",
        "            top_n=5",
        "        )",
        "        self.assertTrue(len(results) > 0)",
        "",
        "    def test_code_aware_chunks_can_be_disabled(self):",
        "        \"\"\"Should be able to disable code-aware chunking.\"\"\"",
        "        results = self.processor.find_passages_for_query(",
        "            \"SecondProcessor\",",
        "            top_n=5,",
        "            use_code_aware_chunks=False",
        "        )",
        "        # Should still return results, just with fixed chunking",
        "        self.assertTrue(len(results) >= 0)",
        "",
        "    def test_processor_has_code_chunk_param(self):",
        "        \"\"\"Processor should accept use_code_aware_chunks parameter.\"\"\"",
        "        # Should not raise",
        "        results = self.processor.find_passages_for_query(",
        "            \"utility\",",
        "            use_code_aware_chunks=True",
        "        )",
        "",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "        results = processor.find_passages_for_query(",
        "            \"foo\",",
        "            apply_doc_boost=True,",
        "            auto_detect_intent=True,",
        "            prefer_docs=False,",
        "            custom_boosts={'code': 1.0}",
        "        )",
        "        # Results may be empty for simple doc but params should work",
        "",
        ""
      ],
      "context_after": [
        "if __name__ == '__main__':",
        "    unittest.main()"
      ],
      "change_type": "add"
    }
  ],
  "hour_of_day": 3,
  "day_of_week": "Thursday",
  "seconds_since_last_commit": -382925,
  "is_merge": false,
  "is_initial": false,
  "parent_count": 1,
  "session_id": null,
  "related_chats": [],
  "ci_result": null,
  "reverted": false,
  "amended": false
}