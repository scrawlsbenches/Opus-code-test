{
  "hash": "4c816ae9ebdec3d43c66180d87ff7ccce3902d2b",
  "message": "Optimize chunk scoring performance (Task #43)",
  "author": "Claude",
  "timestamp": "2025-12-10 14:58:54 +0000",
  "branch": "claude/multi-index-design-DvifZ",
  "files_changed": [
    "cortical/query.py",
    "tests/test_query.py"
  ],
  "insertions": 148,
  "deletions": 5,
  "hunks": [
    {
      "file": "cortical/query.py",
      "function": "def create_chunks(",
      "start_line": 971,
      "lines_added": [
        "def precompute_term_cols(",
        "    query_terms: Dict[str, float],",
        "    layer0: HierarchicalLayer",
        ") -> Dict[str, 'Minicolumn']:",
        "    \"\"\"",
        "    Pre-compute minicolumn lookups for query terms.",
        "",
        "    This avoids repeated O(1) dictionary lookups for each chunk,",
        "    enabling faster scoring when processing many chunks.",
        "",
        "    Args:",
        "        query_terms: Dict mapping query terms to weights",
        "        layer0: Token layer for lookups",
        "",
        "    Returns:",
        "        Dict mapping term to Minicolumn (only for terms that exist in corpus)",
        "    \"\"\"",
        "    term_cols = {}",
        "    for term in query_terms:",
        "        col = layer0.get_minicolumn(term)",
        "        if col:",
        "            term_cols[term] = col",
        "    return term_cols",
        "",
        "",
        "def score_chunk_fast(",
        "    chunk_tokens: List[str],",
        "    query_terms: Dict[str, float],",
        "    term_cols: Dict[str, 'Minicolumn'],",
        "    doc_id: Optional[str] = None",
        ") -> float:",
        "    \"\"\"",
        "    Fast chunk scoring using pre-computed minicolumn lookups.",
        "",
        "    This is an optimized version of score_chunk that accepts pre-tokenized",
        "    text and pre-computed minicolumn lookups. Use when scoring many chunks",
        "    from the same document.",
        "",
        "    Args:",
        "        chunk_tokens: Pre-tokenized chunk tokens",
        "        query_terms: Dict mapping query terms to weights",
        "        term_cols: Pre-computed term->Minicolumn mapping from precompute_term_cols()",
        "        doc_id: Optional document ID for per-document TF-IDF",
        "",
        "    Returns:",
        "        Relevance score for the chunk",
        "    \"\"\"",
        "    if not chunk_tokens:",
        "        return 0.0",
        "",
        "    # Count token occurrences in chunk",
        "    token_counts: Dict[str, int] = {}",
        "    for token in chunk_tokens:",
        "        token_counts[token] = token_counts.get(token, 0) + 1",
        "",
        "    score = 0.0",
        "    for term, term_weight in query_terms.items():",
        "        if term in token_counts and term in term_cols:",
        "            col = term_cols[term]",
        "            # Use per-document TF-IDF if available, otherwise global",
        "            tfidf = col.tfidf_per_doc.get(doc_id, col.tfidf) if doc_id else col.tfidf",
        "            # Weight by occurrence in chunk and query weight",
        "            score += tfidf * token_counts[term] * term_weight",
        "",
        "    # Normalize by chunk length to avoid bias toward longer chunks",
        "    return score / len(chunk_tokens)",
        "",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "        end = min(start + chunk_size, text_len)",
        "        chunk = text[start:end]",
        "        chunks.append((chunk, start, end))",
        "",
        "        if end >= text_len:",
        "            break",
        "",
        "    return chunks",
        "",
        ""
      ],
      "context_after": [
        "def score_chunk(",
        "    chunk_text: str,",
        "    query_terms: Dict[str, float],",
        "    layer0: HierarchicalLayer,",
        "    tokenizer: Tokenizer,",
        "    doc_id: Optional[str] = None",
        ") -> float:",
        "    \"\"\"",
        "    Score a chunk against query terms using TF-IDF.",
        ""
      ],
      "change_type": "add"
    },
    {
      "file": "cortical/query.py",
      "function": "def find_passages_for_query(",
      "start_line": 1063,
      "lines_added": [
        "    # Pre-compute minicolumn lookups for query terms (optimization)",
        "    term_cols = precompute_term_cols(query_terms, layer0)",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "    query_terms = get_expanded_query_terms(",
        "        query_text, layers, tokenizer,",
        "        use_expansion=use_expansion,",
        "        semantic_relations=semantic_relations,",
        "        use_semantic=use_semantic",
        "    )",
        "",
        "    if not query_terms:",
        "        return []",
        ""
      ],
      "context_after": [
        "    # First, get candidate documents (more than we need, since we'll rank passages)",
        "    doc_scores = find_documents_for_query(",
        "        query_text, layers, tokenizer,",
        "        top_n=min(len(documents), top_n * 3),",
        "        use_expansion=use_expansion,",
        "        semantic_relations=semantic_relations,",
        "        use_semantic=use_semantic",
        "    )",
        "",
        "    # Apply document filter if provided"
      ],
      "change_type": "add"
    },
    {
      "file": "cortical/query.py",
      "function": "def find_passages_for_query(",
      "start_line": 1087,
      "lines_added": [
        "            # Use fast scoring with pre-computed lookups",
        "            chunk_tokens = tokenizer.tokenize(chunk_text)",
        "            chunk_score = score_chunk_fast(",
        "                chunk_tokens, query_terms, term_cols, doc_id"
      ],
      "lines_removed": [
        "            chunk_score = score_chunk(",
        "                chunk_text, query_terms, layer0, tokenizer, doc_id"
      ],
      "context_before": [
        "    passages: List[Tuple[str, str, int, int, float]] = []",
        "",
        "    for doc_id, doc_score in doc_scores:",
        "        if doc_id not in documents:",
        "            continue",
        "",
        "        text = documents[doc_id]",
        "        chunks = create_chunks(text, chunk_size, overlap)",
        "",
        "        for chunk_text, start_char, end_char in chunks:"
      ],
      "context_after": [
        "            )",
        "            # Combine chunk score with document score for final ranking",
        "            combined_score = chunk_score * (1 + doc_score * 0.1)",
        "",
        "            passages.append((",
        "                chunk_text,",
        "                doc_id,",
        "                start_char,",
        "                end_char,",
        "                combined_score"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/query.py",
      "function": "def find_passages_batch(",
      "start_line": 1247,
      "lines_added": [
        "        # Pre-compute minicolumn lookups for query terms (optimization)",
        "        term_cols = precompute_term_cols(query_terms, layer0)",
        "",
        "        # Score passages using cached chunks and fast scoring",
        "                # Use fast scoring with pre-computed lookups",
        "                chunk_tokens = tokenizer.tokenize(chunk_text)",
        "                chunk_score = score_chunk_fast(",
        "                    chunk_tokens, query_terms, term_cols, doc_id"
      ],
      "lines_removed": [
        "        # Score passages using cached chunks",
        "                chunk_score = score_chunk(",
        "                    chunk_text, query_terms, layer0, tokenizer, doc_id"
      ],
      "context_before": [
        "                use_expansion=use_expansion,",
        "                semantic_relations=semantic_relations,",
        "                use_semantic=use_semantic",
        "            )",
        "            expansion_cache[query_text] = query_terms",
        "",
        "        if not query_terms:",
        "            all_results.append([])",
        "            continue",
        ""
      ],
      "context_after": [
        "        # Get candidate documents",
        "        doc_scores = find_documents_for_query(",
        "            query_text, layers, tokenizer,",
        "            top_n=min(len(documents), top_n * 3),",
        "            use_expansion=use_expansion,",
        "            semantic_relations=semantic_relations,",
        "            use_semantic=use_semantic",
        "        )",
        "",
        "        # Apply document filter",
        "        if doc_filter:",
        "            doc_scores = [(doc_id, score) for doc_id, score in doc_scores if doc_id in doc_filter]",
        "",
        "        passages: List[Tuple[str, str, int, int, float]] = []",
        "",
        "        for doc_id, doc_score in doc_scores:",
        "            if doc_id not in doc_chunks_cache:",
        "                continue",
        "",
        "            for chunk_text, start_char, end_char in doc_chunks_cache[doc_id]:",
        "                )",
        "                combined_score = chunk_score * (1 + doc_score * 0.1)",
        "                passages.append((chunk_text, doc_id, start_char, end_char, combined_score))",
        "",
        "        passages.sort(key=lambda x: x[4], reverse=True)",
        "        all_results.append(passages[:top_n])",
        "",
        "    return all_results",
        "",
        ""
      ],
      "change_type": "modify"
    },
    {
      "file": "tests/test_query.py",
      "function": "class TestScoreChunk(unittest.TestCase):",
      "start_line": 729,
      "lines_added": [
        "class TestChunkScoringOptimization(unittest.TestCase):",
        "    \"\"\"Test optimized chunk scoring functions.\"\"\"",
        "",
        "    @classmethod",
        "    def setUpClass(cls):",
        "        \"\"\"Set up processor.\"\"\"",
        "        cls.processor = CorticalTextProcessor()",
        "        cls.processor.process_document(",
        "            \"doc1\",",
        "            \"Neural networks are powerful tools for data analysis.\"",
        "        )",
        "        cls.processor.compute_all(verbose=False)",
        "",
        "    def test_precompute_term_cols_returns_dict(self):",
        "        \"\"\"precompute_term_cols should return dict of Minicolumns.\"\"\"",
        "        from cortical.query import precompute_term_cols",
        "        query_terms = {\"neural\": 1.0, \"networks\": 0.8}",
        "        layer0 = self.processor.layers[CorticalLayer.TOKENS]",
        "",
        "        term_cols = precompute_term_cols(query_terms, layer0)",
        "",
        "        self.assertIsInstance(term_cols, dict)",
        "        self.assertIn(\"neural\", term_cols)",
        "        self.assertIn(\"networks\", term_cols)",
        "",
        "    def test_precompute_term_cols_excludes_unknown(self):",
        "        \"\"\"precompute_term_cols should exclude terms not in corpus.\"\"\"",
        "        from cortical.query import precompute_term_cols",
        "        query_terms = {\"neural\": 1.0, \"xyz_unknown\": 0.5}",
        "        layer0 = self.processor.layers[CorticalLayer.TOKENS]",
        "",
        "        term_cols = precompute_term_cols(query_terms, layer0)",
        "",
        "        self.assertIn(\"neural\", term_cols)",
        "        self.assertNotIn(\"xyz_unknown\", term_cols)",
        "",
        "    def test_score_chunk_fast_matches_regular(self):",
        "        \"\"\"score_chunk_fast should produce same results as score_chunk.\"\"\"",
        "        from cortical.query import precompute_term_cols, score_chunk_fast",
        "        query_terms = {\"neural\": 1.0, \"networks\": 0.8}",
        "        layer0 = self.processor.layers[CorticalLayer.TOKENS]",
        "        chunk_text = \"Neural networks process data\"",
        "",
        "        # Regular score",
        "        regular_score = score_chunk(",
        "            chunk_text, query_terms, layer0, self.processor.tokenizer",
        "        )",
        "",
        "        # Fast score",
        "        term_cols = precompute_term_cols(query_terms, layer0)",
        "        chunk_tokens = self.processor.tokenizer.tokenize(chunk_text)",
        "        fast_score = score_chunk_fast(chunk_tokens, query_terms, term_cols)",
        "",
        "        self.assertAlmostEqual(regular_score, fast_score, places=6)",
        "",
        "    def test_score_chunk_fast_empty_tokens(self):",
        "        \"\"\"score_chunk_fast should handle empty tokens list.\"\"\"",
        "        from cortical.query import score_chunk_fast",
        "        query_terms = {\"neural\": 1.0}",
        "        term_cols = {}",
        "",
        "        score = score_chunk_fast([], query_terms, term_cols)",
        "        self.assertEqual(score, 0.0)",
        "",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "",
        "        score = score_chunk(",
        "            \"\",",
        "            query_terms,",
        "            layer0,",
        "            self.processor.tokenizer",
        "        )",
        "        self.assertEqual(score, 0.0)",
        "",
        ""
      ],
      "context_after": [
        "class TestEdgeCases(unittest.TestCase):",
        "    \"\"\"Test edge cases and error handling.\"\"\"",
        "",
        "    @classmethod",
        "    def setUpClass(cls):",
        "        \"\"\"Set up empty and minimal processors.\"\"\"",
        "        cls.empty_processor = CorticalTextProcessor()",
        "",
        "        cls.minimal_processor = CorticalTextProcessor()",
        "        cls.minimal_processor.process_document(\"doc1\", \"Hello world\")"
      ],
      "change_type": "add"
    }
  ],
  "hour_of_day": 14,
  "day_of_week": "Wednesday",
  "seconds_since_last_commit": -427554,
  "is_merge": false,
  "is_initial": false,
  "parent_count": 1,
  "session_id": null,
  "related_chats": [],
  "ci_result": null,
  "reverted": false,
  "amended": false
}