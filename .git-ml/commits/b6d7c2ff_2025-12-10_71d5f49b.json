{
  "hash": "b6d7c2ffbd50a18db0f1d908fd5b78653e092332",
  "message": "Merge pull request #19 from scrawlsbenches/claude/code-review-tasks-01APLnkRURmSbYma6KCRaWc7",
  "author": "scrawlsbenches",
  "timestamp": "2025-12-10 10:34:03 -0500",
  "branch": "claude/multi-index-design-DvifZ",
  "files_changed": [
    ".claude/skills/codebase-search/SKILL.md",
    ".claude/skills/corpus-indexer/SKILL.md",
    ".gitignore",
    "CLAUDE.md",
    "TASK_LIST.md",
    "cortical/analysis.py",
    "cortical/code_concepts.py",
    "cortical/fingerprint.py",
    "cortical/processor.py",
    "cortical/query.py",
    "cortical/semantics.py",
    "cortical/tokenizer.py",
    "scripts/index_codebase.py",
    "scripts/search_codebase.py",
    "tests/test_analysis.py",
    "tests/test_code_concepts.py",
    "tests/test_fingerprint.py",
    "tests/test_intent_query.py",
    "tests/test_processor.py",
    "tests/test_query.py",
    "tests/test_query_optimization.py",
    "tests/test_tokenizer.py"
  ],
  "insertions": 5306,
  "deletions": 98,
  "hunks": [
    {
      "file": ".claude/skills/codebase-search/SKILL.md",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "---",
        "name: codebase-search",
        "description: Search the Cortical Text Processor codebase using semantic search. Use when looking for code patterns, understanding how features work, or finding relevant implementations. This skill uses the system's own IR algorithms to search its own codebase (dog-fooding).",
        "allowed-tools: Read, Bash, Glob",
        "---",
        "# Codebase Search Skill",
        "",
        "This skill enables **meaning-based search** over the Cortical Text Processor codebase. It finds relevant code by understanding intent and concepts, not just exact keyword matching.",
        "",
        "## Key Capabilities",
        "",
        "- **Meaning-based retrieval**: Finds related code even when exact words don't match",
        "- **Query expansion**: Automatically includes related terms via co-occurrence and semantic relations",
        "- **Code concept groups**: Knows \"fetch\", \"get\", \"load\" are synonyms in code context",
        "- **Intent understanding**: Parses \"where do we handle X?\" into location + action + subject",
        "- **Semantic fingerprinting**: Compare and explain code similarity",
        "- **Fast search mode**: ~2-3x faster for large codebases",
        "- **No ML required**: Works through graph algorithms on corpus statistics",
        "",
        "## When to Use",
        "",
        "- Finding implementations: \"how does PageRank work\"",
        "- Locating code by intent: \"where do we handle errors\"",
        "- Understanding relationships: \"what connects to the tokenizer\"",
        "- Exploring concepts: \"authentication and validation\"",
        "",
        "## Prerequisites",
        "",
        "Ensure the corpus is indexed:",
        "",
        "```bash",
        "python scripts/index_codebase.py",
        "```",
        "",
        "This creates `corpus_dev.pkl` with the indexed codebase.",
        "",
        "## Search Commands",
        "",
        "### Basic Search",
        "",
        "```bash",
        "python scripts/search_codebase.py \"your query here\"",
        "```",
        "",
        "### Options",
        "",
        "| Option | Description |",
        "|--------|-------------|",
        "| `--top N` | Number of results (default: 5) |",
        "| `--verbose` | Show full passage text |",
        "| `--expand` | Show query expansion terms |",
        "| `--fast` | Fast search mode (~2-3x faster, document-level) |",
        "| `--interactive` | Interactive search mode |",
        "",
        "### Example Queries",
        "",
        "```bash",
        "# Find by concept (not exact words)",
        "python scripts/search_codebase.py \"graph importance algorithm\"",
        "",
        "# Natural language intent (parses action + subject)",
        "python scripts/search_codebase.py \"where do we handle authentication\"",
        "",
        "# Code concept synonyms (fetch finds get/load/retrieve too)",
        "python scripts/search_codebase.py \"fetch user data\"",
        "",
        "# Fast mode for quick lookups",
        "python scripts/search_codebase.py \"PageRank\" --fast",
        "",
        "# See what terms the system associates",
        "python scripts/search_codebase.py \"lateral connections\" --expand",
        "",
        "# Interactive exploration",
        "python scripts/search_codebase.py --interactive",
        "```",
        "",
        "## Understanding Results",
        "",
        "Results include:",
        "- **File:Line** reference (e.g., `cortical/analysis.py:127`)",
        "- **Score** indicating relevance (higher is better)",
        "- **Passage** showing relevant code or text",
        "",
        "## Tips",
        "",
        "1. **Use natural language** - ask questions as you would to a colleague",
        "2. **Check expansion** (`--expand`) - see what related terms are being searched",
        "3. **Broad then narrow** - start general, refine based on results",
        "4. **Interactive mode** - use `/expand`, `/concepts`, `/stats` for exploration"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": ".claude/skills/corpus-indexer/SKILL.md",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "---",
        "name: corpus-indexer",
        "description: Index or re-index the codebase for semantic search. Use after making significant code changes to keep the search corpus up-to-date.",
        "allowed-tools: Bash",
        "---",
        "# Corpus Indexer Skill",
        "",
        "This skill manages the codebase index used by the semantic search system.",
        "",
        "## When to Use",
        "",
        "- After adding new files to the codebase",
        "- After significant code changes",
        "- When search results seem outdated",
        "- To verify indexing statistics",
        "",
        "## Index the Codebase",
        "",
        "```bash",
        "python scripts/index_codebase.py",
        "```",
        "",
        "### Options",
        "",
        "- `--output FILE` or `-o FILE`: Custom output path (default: corpus_dev.pkl)",
        "- `--verbose` or `-v`: Show detailed indexing progress",
        "",
        "### Example",
        "",
        "```bash",
        "# Standard indexing",
        "python scripts/index_codebase.py",
        "",
        "# Verbose output to see what's being indexed",
        "python scripts/index_codebase.py --verbose",
        "",
        "# Custom output location",
        "python scripts/index_codebase.py --output my_corpus.pkl",
        "```",
        "",
        "## What Gets Indexed",
        "",
        "The indexer processes:",
        "- All Python files in `cortical/` (source code)",
        "- All Python files in `tests/` (test code)",
        "- Documentation: `CLAUDE.md`, `TASK_LIST.md`, `README.md`, `KNOWLEDGE_TRANSFER.md`",
        "",
        "## Output Statistics",
        "",
        "After indexing, you'll see:",
        "- Number of documents indexed",
        "- Total lines of code",
        "- Token count (unique terms)",
        "- Bigram count (word pairs)",
        "- Concept clusters",
        "- Semantic relations extracted",
        "",
        "## Maintenance",
        "",
        "Re-index periodically to keep search accurate:",
        "- After adding new modules",
        "- After major refactoring",
        "- Before deep codebase exploration sessions"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": ".gitignore",
      "function": "__pycache__/",
      "start_line": 3,
      "lines_added": [
        "# Generated corpus files",
        "corpus_dev.pkl",
        "*.pkl",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "*.py[cod]",
        "*$py.class",
        "*.so",
        ".Python",
        "*.egg-info/",
        ".eggs/",
        "dist/",
        "build/",
        ".pytest_cache/",
        ""
      ],
      "context_after": [
        "# Coverage",
        ".coverage",
        ".coverage.*",
        "coverage.xml",
        "htmlcov/",
        "",
        "## A streamlined .gitignore for modern .NET projects",
        "## including temporary files, build results, and",
        "## files generated by popular .NET tools. If you are",
        "## developing with Visual Studio, the VS .gitignore"
      ],
      "change_type": "add"
    },
    {
      "file": "CLAUDE.md",
      "function": "cortical/",
      "start_line": 52,
      "lines_added": [
        "### Fixed Bugs (2025-12-10)",
        "The bigram separator mismatch bugs in `query.py:1442-1468` and `analysis.py:927` have been **fixed**. Bigrams now correctly use space separators throughout the codebase."
      ],
      "lines_removed": [
        "### Known Bug (Unfixed)",
        "**Bigram separator mismatch in analogy completion** (`query.py:1442-1468`):",
        "```python",
        "# BUG: Uses underscore, but bigrams are stored with spaces",
        "ab_bigram = f\"{term_a}_{term_b}\"  # Wrong: \"neural_networks\"",
        "# Should be:",
        "ab_bigram = f\"{term_a} {term_b}\"  # Correct: \"neural networks\"",
        "```"
      ],
      "context_before": [
        "",
        "**Key data structures:**",
        "- `Minicolumn`: Core unit with `lateral_connections`, `typed_connections`, `feedforward_connections`, `feedback_connections`",
        "- `Edge`: Typed connection with `relation_type`, `weight`, `confidence`, `source`",
        "- `HierarchicalLayer`: Container with `minicolumns` dict and `_id_index` for O(1) lookups",
        "",
        "---",
        "",
        "## Critical Knowledge",
        ""
      ],
      "context_after": [
        "",
        "### Important Implementation Details",
        "",
        "1. **Bigrams use SPACE separators** (from `tokenizer.py:179`):",
        "   ```python",
        "   ' '.join(tokens[i:i+n])  # \"neural networks\", not \"neural_networks\"",
        "   ```",
        "",
        "2. **Global `col.tfidf` is NOT per-document TF-IDF** - it uses total corpus occurrence count. Use `col.tfidf_per_doc[doc_id]` for true per-document TF-IDF.",
        ""
      ],
      "change_type": "modify"
    },
    {
      "file": "CLAUDE.md",
      "function": "def find_documents(",
      "start_line": 236,
      "lines_added": [
        "6. **Use `fast_find_documents()`** for ~2-3x faster search on large corpora",
        "7. **Pre-build index** with `build_search_index()` for fastest repeated queries",
        "",
        "---",
        "",
        "## Code Search Capabilities",
        "",
        "### Code-Aware Tokenization",
        "```python",
        "# Enable identifier splitting for code search",
        "tokenizer = Tokenizer(split_identifiers=True)",
        "tokens = tokenizer.tokenize(\"getUserCredentials\")",
        "# ['getusercredentials', 'get', 'user', 'credentials']",
        "```",
        "",
        "### Programming Concept Expansion",
        "```python",
        "# Expand queries with programming synonyms (get/fetch/load)",
        "results = processor.expand_query(\"fetch data\", use_code_concepts=True)",
        "# Or use the convenience method",
        "results = processor.expand_query_for_code(\"fetch data\")",
        "```",
        "",
        "### Intent-Based Search",
        "```python",
        "# Parse natural language queries",
        "parsed = processor.parse_intent_query(\"where do we handle authentication?\")",
        "# {'intent': 'location', 'action': 'handle', 'subject': 'authentication', ...}",
        "",
        "# Search with intent understanding",
        "results = processor.search_by_intent(\"how do we validate input?\")",
        "```",
        "",
        "### Semantic Fingerprinting",
        "```python",
        "# Compare code similarity",
        "fp1 = processor.get_fingerprint(code_block_1)",
        "fp2 = processor.get_fingerprint(code_block_2)",
        "comparison = processor.compare_fingerprints(fp1, fp2)",
        "explanation = processor.explain_similarity(fp1, fp2)",
        "```",
        "",
        "### Fast Search",
        "```python",
        "# Fast document search (~2-3x faster)",
        "results = processor.fast_find_documents(\"authentication\")",
        "",
        "# Pre-built index for fastest search",
        "index = processor.build_search_index()",
        "results = processor.search_with_index(\"query\", index)",
        "```"
      ],
      "lines_removed": [],
      "context_before": [
        "",
        "---",
        "",
        "## Performance Considerations",
        "",
        "1. **Use `get_by_id()` for ID lookups** - O(1) vs O(n) iteration",
        "2. **Batch document additions** with `add_documents_batch()` for bulk imports",
        "3. **Use incremental updates** with `add_document_incremental()` for live systems",
        "4. **Cache query expansions** when processing multiple similar queries",
        "5. **Pre-compute chunks** in `find_passages_batch()` to avoid redundant work"
      ],
      "context_after": [
        "",
        "---",
        "",
        "## Debugging Tips",
        "",
        "### Inspecting Layer State",
        "```python",
        "processor = CorticalTextProcessor()",
        "processor.process_document(\"test\", \"Neural networks process data.\")",
        "processor.compute_all()"
      ],
      "change_type": "add"
    },
    {
      "file": "CLAUDE.md",
      "function": "for t1, rel, t2, weight in processor.semantic_relations[:10]:",
      "start_line": 282,
      "lines_added": [
        "| Fast search | `processor.fast_find_documents(query)` |",
        "| Code search | `processor.expand_query_for_code(query)` |",
        "| Intent search | `processor.search_by_intent(\"where do we...\")` |",
        "| Fingerprint | `processor.get_fingerprint(text)` |",
        "| Compare | `processor.compare_fingerprints(fp1, fp2)` |",
        "## Dog-Fooding: Search the Codebase",
        "",
        "The Cortical Text Processor can index and search its own codebase, providing semantic search capabilities during development.",
        "",
        "### Quick Start",
        "",
        "```bash",
        "# Index the codebase (creates corpus_dev.pkl)",
        "python scripts/index_codebase.py",
        "",
        "# Search for code",
        "python scripts/search_codebase.py \"PageRank algorithm\"",
        "python scripts/search_codebase.py \"bigram separator\" --verbose",
        "python scripts/search_codebase.py --interactive",
        "```",
        "",
        "### Claude Skills",
        "",
        "Two skills are available in `.claude/skills/`:",
        "",
        "1. **codebase-search**: Search the indexed codebase for code patterns and implementations",
        "2. **corpus-indexer**: Re-index the codebase after making changes",
        "",
        "### Search Options",
        "",
        "| Option | Description |",
        "|--------|-------------|",
        "| `--top N` | Number of results (default: 5) |",
        "| `--verbose` | Show full passage text |",
        "| `--expand` | Show query expansion terms |",
        "| `--interactive` | Interactive search mode |",
        "",
        "### Interactive Mode Commands",
        "",
        "| Command | Description |",
        "|---------|-------------|",
        "| `/expand <query>` | Show query expansion |",
        "| `/concepts` | List concept clusters |",
        "| `/stats` | Show corpus statistics |",
        "| `/quit` | Exit interactive mode |",
        "",
        "### Example Queries",
        "",
        "```bash",
        "# Find how PageRank is implemented",
        "python scripts/search_codebase.py \"compute pagerank damping factor\"",
        "",
        "# Find test patterns",
        "python scripts/search_codebase.py \"unittest setUp processor\"",
        "",
        "# Explore query expansion code",
        "python scripts/search_codebase.py \"expand query semantic lateral\"",
        "```",
        "",
        "---",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "",
        "---",
        "",
        "## Quick Reference",
        "",
        "| Task | Command/Method |",
        "|------|----------------|",
        "| Process document | `processor.process_document(id, text)` |",
        "| Build network | `processor.compute_all()` |",
        "| Search | `processor.find_documents_for_query(query)` |"
      ],
      "context_after": [
        "| RAG passages | `processor.find_passages_for_query(query)` |",
        "| Save state | `processor.save(\"corpus.pkl\")` |",
        "| Load state | `processor = CorticalTextProcessor.load(\"corpus.pkl\")` |",
        "| Run tests | `python -m unittest discover -s tests -v` |",
        "| Run showcase | `python showcase.py` |",
        "",
        "---",
        "",
        "## File Quick Links",
        "",
        "- **Main API**: `cortical/processor.py` - `CorticalTextProcessor` class",
        "- **Graph algorithms**: `cortical/analysis.py` - PageRank, TF-IDF, clustering",
        "- **Search**: `cortical/query.py` - query expansion, document retrieval",
        "- **Data structures**: `cortical/minicolumn.py` - `Minicolumn`, `Edge`",
        "- **Tests**: `tests/test_processor.py` - most comprehensive test file",
        "- **Demo**: `showcase.py` - interactive demonstration",
        "",
        "---"
      ],
      "change_type": "add"
    },
    {
      "file": "TASK_LIST.md",
      "function": "for term, score in results:",
      "start_line": 1103,
      "lines_added": [
        "## Actionable Tasks (2025-12-10)",
        "The following tasks were identified during comprehensive code review and are prioritized for implementation:",
        "---",
        "### 47. Dog-Food the System During Development",
        "",
        "**Files:** New `scripts/index_codebase.py`, `scripts/search_codebase.py`, `.claude/skills/`",
        "**Status:** [x] Completed (2025-12-10)",
        "**Priority:** High",
        "",
        "**Goal:**",
        "Use the Cortical Text Processor to index and search its own codebase during development.",
        "",
        "**Solution Applied:**",
        "1. Created `scripts/index_codebase.py`:",
        "   - Indexes all 19 Python files in `cortical/` and `tests/`",
        "   - Indexes 4 documentation files (CLAUDE.md, TASK_LIST.md, README.md, KNOWLEDGE_TRANSFER.md)",
        "   - Saves indexed corpus to `corpus_dev.pkl` (23 documents, ~15,600 lines)",
        "   - Computes semantic PageRank, TF-IDF, concepts, and semantic relations",
        "",
        "2. Created `scripts/search_codebase.py`:",
        "   - Loads indexed corpus and performs semantic search",
        "   - Returns file:line references for each result",
        "   - Supports `--top N`, `--verbose`, `--expand`, `--interactive` options",
        "   - Interactive mode with `/expand`, `/concepts`, `/stats`, `/quit` commands",
        "",
        "3. Created Claude Skills in `.claude/skills/`:",
        "   - `codebase-search/SKILL.md` - Search skill for finding code patterns",
        "   - `corpus-indexer/SKILL.md` - Indexing skill for updating corpus",
        "",
        "4. Updated `CLAUDE.md` with Dog-Fooding section documenting usage",
        "",
        "**Example Usage:**",
        "```bash",
        "python scripts/index_codebase.py",
        "python scripts/search_codebase.py \"PageRank algorithm\" --top 3",
        "python scripts/search_codebase.py \"bigram separator\" --expand",
        "python scripts/search_codebase.py --interactive",
        "```",
        "",
        "**Success Criteria:** All met",
        "- Can find relevant code when searching for concepts",
        "- Passages include accurate file:line references (e.g., `cortical/analysis.py:127`)",
        "- System handles its own codebase without errors",
        "- Identified usability issue: return value order in find_passages_for_query (fixed)",
        "",
        "---",
        "",
        "### 37. Create Dedicated Query Module Tests",
        "",
        "**File:** `tests/test_query.py` (new file)",
        "**Status:** [x] Completed (2025-12-10)",
        "**Priority:** High",
        "",
        "**Problem:**",
        "`cortical/query.py` (1,503 lines, 20+ functions) has NO dedicated test file. Functions are tested only indirectly through `test_processor.py`.",
        "",
        "**Functions Needing Coverage:**",
        "- `expand_query_multihop()` - Multi-hop inference with relation chains",
        "- `score_relation_path()` - Relation path validation",
        "- `get_expanded_query_terms()` - Helper for all expansion methods",
        "- `find_relevant_concepts()` - Concept filtering for RAG",
        "- `find_relation_between()` and `find_terms_with_relation()` - Relation discovery",
        "- Chunking and batch operations",
        "",
        "**Deliverable:** Create `tests/test_query.py` with 30+ unit tests.",
        "",
        "**Solution Applied:**",
        "Created `tests/test_query.py` with 48 comprehensive tests covering:",
        "- `TestScoreRelationPath` (4 tests) - Relation path validation",
        "- `TestCreateChunks` (4 tests) - Text chunking",
        "- `TestFindRelationBetween` (4 tests) - Relation discovery",
        "- `TestFindTermsWithRelation` (4 tests) - Term relation lookup",
        "- `TestExpandQuery` (4 tests) - Basic query expansion",
        "- `TestExpandQueryMultihop` (4 tests) - Multi-hop expansion",
        "- `TestGetExpandedQueryTerms` (3 tests) - Unified expansion helper",
        "- `TestFindDocumentsForQuery` (4 tests) - Document retrieval",
        "- `TestFindDocumentsBatch` (3 tests) - Batch document retrieval",
        "- `TestFindPassagesForQuery` (2 tests) - Passage retrieval",
        "- `TestFindRelevantConcepts` (2 tests) - Concept filtering",
        "- `TestCompleteAnalogy` (3 tests) - Analogy completion",
        "- `TestQueryWithSpreadingActivation` (2 tests) - Activation search",
        "- `TestScoreChunk` (3 tests) - Chunk scoring",
        "- `TestEdgeCases` (2 tests) - Edge case handling",
        "",
        "Test count increased from 340 to 388.",
        "",
        "---",
        "",
        "### 38. Add Input Validation to Public API",
        "",
        "**Files:** `cortical/processor.py`",
        "**Status:** [x] Completed (2025-12-10)",
        "**Priority:** High",
        "",
        "**Problem:**",
        "Public API methods silently accept invalid inputs, leading to confusing behavior.",
        "",
        "**Solution Applied:**",
        "Added input validation to 4 key public API methods:",
        "",
        "1. **`process_document()`** - Validates doc_id (non-empty string) and content (non-empty string)",
        "2. **`find_documents_for_query()`** - Validates query_text (non-empty string) and top_n (positive int)",
        "3. **`complete_analogy()`** - Validates all 3 terms (non-empty strings) and top_n (positive int)",
        "4. **`add_documents_batch()`** - Validates documents list format, doc_id/content types, and recompute level",
        "",
        "All methods now raise `ValueError` with descriptive messages for invalid input.",
        "",
        "**Tests Added:** 20 new tests in `TestInputValidation` class covering:",
        "- Empty/None/non-string doc_id",
        "- Empty/whitespace-only/non-string content",
        "- Empty/whitespace-only query_text",
        "- Invalid top_n values (0, negative)",
        "- Invalid document batch formats",
        "- Valid input acceptance",
        "",
        "Test count increased from 388 to 408.",
        "",
        "---",
        "",
        "### 39. Move Inline Imports to Module Top",
        "",
        "**Files:** `cortical/processor.py:161`, `cortical/semantics.py:493`",
        "**Status:** [x] Completed (2025-12-10)",
        "**Problem:**",
        "`import copy` statements inside methods pollute namespaces and impact readability.",
        "",
        "**Solution Applied:**",
        "Moved `import copy` to module-level imports in both files.",
        "### 40. Add Parameter Range Validation",
        "**Files:** Multiple",
        "**Status:** [x] Completed (2025-12-10)",
        "**Problem:**",
        "No validation for invalid parameter ranges.",
        "**Solution Applied:**",
        "Added validation to key functions:",
        "- `compute_pagerank()`: damping must be in range (0, 1)",
        "- `compute_semantic_pagerank()`: damping must be in range (0, 1)",
        "- `compute_hierarchical_pagerank()`: damping and cross_layer_damping must be in range (0, 1)",
        "- `retrofit_connections()`: alpha must be in range [0, 1]",
        "- `retrofit_embeddings()`: alpha must be in range (0, 1]",
        "- `create_chunks()`: chunk_size > 0, overlap >= 0, overlap < chunk_size",
        "",
        "Added 9 new tests for parameter validation.",
        "### 41. Create Configuration Dataclass",
        "**Files:** New `cortical/config.py`",
        "**Status:** [ ] Not Started",
        "**Priority:** Medium",
        "",
        "**Problem:**",
        "Magic numbers scattered across modules with no central configuration:",
        "- `gaps.py`: ISOLATION_THRESHOLD=0.02, WELL_CONNECTED_THRESHOLD=0.03",
        "- `query.py`: VALID_RELATION_CHAINS (15 entries)",
        "- `analysis.py`: damping=0.85, iterations=20, tolerance=1e-6",
        "",
        "**Solution:**",
        "```python",
        "@dataclass",
        "class CorticalConfig:",
        "    # PageRank",
        "    pagerank_damping: float = 0.85",
        "    pagerank_iterations: int = 20",
        "    pagerank_tolerance: float = 1e-6",
        "",
        "    # Clustering",
        "    min_cluster_size: int = 3",
        "    cluster_strictness: float = 1.0",
        "",
        "    # Gap detection",
        "    isolation_threshold: float = 0.02",
        "    well_connected_threshold: float = 0.03",
        "```",
        "",
        "---",
        "",
        "### 42. Add Simple Query Language Support",
        "",
        "**File:** `cortical/query.py`",
        "**Status:** [ ] Not Started",
        "**Priority:** Low",
        "",
        "**Problem:**",
        "Only natural language queries supported. No structured filtering.",
        "",
        "**Solution:** Add minimal syntax:",
        "- `\"term1 AND term2\"` - require both terms",
        "- `\"term1 OR term2\"` - either term",
        "- `\"-term1\"` - exclude term",
        "- `\"term1\"` (quoted) - exact match",
        "",
        "---",
        "",
        "### 43. Optimize Chunk Scoring Performance",
        "",
        "**File:** `cortical/query.py:590-630`",
        "**Status:** [x] Completed (2025-12-10)",
        "**Priority:** Medium",
        "",
        "**Problem:**",
        "`score_chunk()` tokenizes chunk text every call with no caching.",
        "",
        "**Solution Applied:**",
        "1. Added `precompute_term_cols()` to cache minicolumn lookups for query terms",
        "2. Added `score_chunk_fast()` for optimized scoring with pre-computed lookups",
        "3. Updated `find_passages_for_query()` to use fast scoring",
        "4. Updated `find_passages_batch()` to use fast scoring",
        "",
        "Added 4 new tests for optimization functions.",
        "",
        "---",
        "",
        "### 44. Remove Deprecated feedforward_sources",
        "",
        "**Files:** `cortical/minicolumn.py:117`, `analysis.py:457`, `query.py:105`",
        "**Status:** [ ] Not Started",
        "**Priority:** Low",
        "",
        "**Problem:**",
        "`feedforward_sources` is marked deprecated but still used in 4+ locations.",
        "",
        "**Solution:** Migrate all usages to `feedforward_connections` and remove deprecated attribute.",
        "",
        "---",
        "",
        "### 45. Add LRU Cache for Query Results",
        "",
        "**File:** `cortical/processor.py`",
        "**Status:** [x] Completed (2025-12-10)",
        "**Priority:** Medium",
        "",
        "**Problem:**",
        "Every query re-expands terms and rescores documents. Repeated queries (common in RAG loops) are slow.",
        "",
        "**Solution Applied:**",
        "1. Added `_query_expansion_cache` dict and `_query_cache_max_size` to processor",
        "2. Added `expand_query_cached()` method with cache lookup and LRU-style eviction",
        "3. Added `clear_query_cache()` to manually invalidate cache",
        "4. Added `set_query_cache_size()` to configure cache size",
        "5. Auto-invalidate cache on `compute_all()` since corpus state changes",
        "",
        "Added 8 new tests for cache functionality.",
        "",
        "---",
        "",
        "### 46. Standardize Return Types with Dataclasses",
        "",
        "**File:** `cortical/query.py`",
        "**Status:** [ ] Not Started",
        "**Problem:**",
        "Inconsistent return types across query functions:",
        "- `find_documents_for_query()` → `List[Tuple[str, float]]`",
        "- `find_passages_for_query()` → `List[Tuple[str, str, int, int, float]]`",
        "- `complete_analogy()` → `List[Tuple[str, float, str]]`",
        "",
        "**Solution:**",
        "```python",
        "@dataclass",
        "class DocumentMatch:",
        "    doc_id: str",
        "    score: float",
        "",
        "@dataclass",
        "class PassageMatch:",
        "    doc_id: str",
        "    text: str",
        "    start: int",
        "    end: int",
        "    score: float",
        "```"
      ],
      "lines_removed": [
        "## Code Review Concerns",
        "The following concerns were identified during code review and should be addressed in future iterations:",
        "### 31. Consider Splitting processor.py",
        "**File:** `cortical/processor.py`",
        "**Status:** [ ] Future Enhancement",
        "**Concern:**",
        "The `processor.py` file has grown to 800+ lines with the addition of incremental indexing, batch APIs, and multi-stage ranking. Consider splitting into smaller modules:",
        "- `processor_core.py` - Core document processing",
        "- `processor_batch.py` - Batch operations (add_documents_batch, find_*_batch)",
        "- `processor_incremental.py` - Incremental indexing and staleness tracking",
        "### 32. Semantic Lookup Memory Optimization",
        "**File:** `cortical/analysis.py`",
        "**Function:** `compute_concept_connections()`",
        "**Status:** [ ] Future Enhancement",
        "**Concern:**",
        "The semantic lookup builds a double-nested dictionary (`Dict[str, Dict[str, Tuple[str, float]]]`) which stores relations in both directions. For large semantic relation sets (10K+ relations), this could consume significant memory.",
        "**Potential Solution:**",
        "- Use a single direction and check both orderings at lookup time",
        "- Or use a frozenset key: `{(t1, t2): (relation, weight)}`",
        "### 33. Tune Semantic Bonus Cap",
        "**File:** `cortical/analysis.py`",
        "**Line:** ~408",
        "**Status:** [ ] Future Enhancement",
        "**Concern:**",
        "Semantic bonus is capped at 50% boost (`min(avg_semantic, 0.5)`). This is a reasonable default but may benefit from:",
        "- Making it a configurable parameter",
        "- Empirical testing on different corpus types"
      ],
      "context_before": [
        "# Control which strategies to use",
        "results = processor.complete_analogy(",
        "    \"neural\", \"networks\", \"knowledge\",",
        "    use_embeddings=True,   # Enable vector arithmetic",
        "    use_relations=True     # Enable relation matching",
        ")",
        "```",
        "",
        "---",
        ""
      ],
      "context_after": [
        "",
        "",
        "",
        "**Priority:** Low",
        "",
        "",
        "---",
        "",
        "",
        "**Priority:** Medium",
        "",
        "",
        "",
        "---",
        "",
        "",
        "**Priority:** Low",
        "",
        "",
        "---",
        "",
        "## Summary",
        "",
        "| Priority | Task | Status | Category |",
        "|----------|------|--------|----------|",
        "| Critical | Fix TF-IDF per-doc calculation | ✅ Completed | Bug Fix |",
        "| High | Add ID lookup optimization | ✅ Completed | Bug Fix |",
        "| Medium | Fix type annotations (semantics.py) | ✅ Completed | Bug Fix |"
      ],
      "change_type": "modify"
    },
    {
      "file": "TASK_LIST.md",
      "function": "Semantic bonus is capped at 50% boost (`min(avg_semantic, 0.5)`). This is a reas",
      "start_line": 1194,
      "lines_added": [
        "Ran 408 tests in 0.336s"
      ],
      "lines_removed": [
        "Ran 321 tests in 0.280s"
      ],
      "context_before": [
        "",
        "**Bug Fix Completion:** 8/8 tasks (100%)",
        "**RAG Enhancement Completion:** 8/8 tasks (100%)",
        "**ConceptNet Enhancement Completion:** 12/12 tasks (100%)",
        "",
        "---",
        "",
        "## Test Results",
        "",
        "```"
      ],
      "context_after": [
        "OK",
        "```",
        "",
        "All tests passing as of 2025-12-10.",
        "",
        "---",
        "",
        "## Layer 2 Connection Improvements (2025-12-10)",
        "",
        "### Problem Statement"
      ],
      "change_type": "modify"
    },
    {
      "file": "TASK_LIST.md",
      "function": "Added `get_expanded_query_terms()` helper function (~60 lines) that consolidates",
      "start_line": 1294,
      "lines_added": [
        "",
        "---",
        "",
        "## Critical Bug Fixes (2025-12-10)",
        "",
        "The following critical bugs were identified during code review and must be fixed:",
        "",
        "### 34. Fix Bigram Separator Mismatch in Analogy Completion",
        "",
        "**File:** `cortical/query.py`",
        "**Lines:** 1442-1468",
        "**Status:** [x] Completed (2025-12-10)",
        "**Priority:** Critical",
        "",
        "**Problem:**",
        "The `complete_analogy_simple()` function uses underscore separators for bigram lookup and parsing, but bigrams are stored with **space** separators (defined in `tokenizer.py:179`).",
        "",
        "**Affected Code:**",
        "```python",
        "# Line 1442-1443: WRONG - uses underscore",
        "ab_bigram = f\"{term_a}_{term_b}\"  # Creates \"neural_networks\"",
        "ba_bigram = f\"{term_b}_{term_a}\"",
        "",
        "# Line 1452: WRONG - splits by underscore",
        "parts = bigram.split('_')",
        "",
        "# But bigrams are stored with spaces (tokenizer.py:179):",
        "# ' '.join(tokens[i:i+n])  # Creates \"neural networks\"",
        "```",
        "",
        "**Impact:**",
        "- The bigram pattern matching strategy in `complete_analogy_simple()` is completely non-functional",
        "- `ab_col` and `ba_col` will always be `None` because \"neural_networks\" doesn't exist in the corpus",
        "- The `parts` split will never produce valid component extraction",
        "",
        "**Solution:**",
        "```python",
        "# Line 1442-1443: Should use space",
        "ab_bigram = f\"{term_a} {term_b}\"  # Correct: \"neural networks\"",
        "ba_bigram = f\"{term_b} {term_a}\"",
        "",
        "# Line 1452: Should split by space",
        "parts = bigram.split(' ')",
        "```",
        "",
        "**Files to Modify:**",
        "- `cortical/query.py` - Fix separator in lines 1442, 1443, 1452",
        "- `tests/test_processor.py` - Add tests for bigram-based analogy completion",
        "",
        "---",
        "",
        "### 35. Fix Bigram Separator Mismatch in Bigram Connections",
        "",
        "**File:** `cortical/analysis.py`",
        "**Line:** 927",
        "**Status:** [x] Completed (2025-12-10)",
        "**Priority:** Critical",
        "",
        "**Problem:**",
        "The `compute_bigram_connections()` function splits bigram content by underscore, but bigrams are stored with **space** separators.",
        "",
        "**Affected Code:**",
        "```python",
        "# Line 927: WRONG - splits by underscore",
        "for bigram in bigrams:",
        "    parts = bigram.content.split('_')",
        "    if len(parts) == 2:",
        "        left_index[parts[0]].append(bigram)",
        "        right_index[parts[1]].append(bigram)",
        "",
        "# But bigrams are stored with spaces:",
        "# \"neural networks\" not \"neural_networks\"",
        "```",
        "",
        "**Impact:**",
        "- `left_index` and `right_index` dictionaries are never populated",
        "- Component-sharing connections (e.g., \"neural networks\" ↔ \"neural processing\") are never created",
        "- Chain connections (e.g., \"machine learning\" ↔ \"learning algorithms\") are never created",
        "- Only document co-occurrence connections work correctly",
        "",
        "**Solution:**",
        "```python",
        "# Line 927: Should split by space",
        "parts = bigram.content.split(' ')",
        "```",
        "",
        "**Verification:**",
        "After fixing, the `compute_bigram_connections()` stats should show non-zero values for:",
        "- `component_connections`",
        "- `chain_connections`",
        "",
        "Currently these are always 0 due to the bug.",
        "",
        "**Files to Modify:**",
        "- `cortical/analysis.py` - Fix separator in line 927",
        "- `tests/test_analysis.py` - Add tests verifying component/chain connections work",
        "",
        "---",
        "",
        "---",
        "",
        "## New Task Summary (2025-12-10)",
        "",
        "| # | Priority | Task | Status | Category |",
        "|---|----------|------|--------|----------|",
        "| 34 | **Critical** | Fix bigram separator in analogy completion | ✅ Completed | Bug Fix |",
        "| 35 | **Critical** | Fix bigram separator in bigram connections | ✅ Completed | Bug Fix |",
        "| 47 | **High** | Dog-food the system during development | ✅ Completed | Validation |",
        "| 37 | **High** | Create dedicated query module tests | ✅ Completed | Testing |",
        "| 38 | **High** | Add input validation to public API | ✅ Completed | Code Quality |",
        "| 39 | Low | Move inline imports to module top | ✅ Completed | Code Quality |",
        "| 40 | Medium | Add parameter range validation | ✅ Completed | Code Quality |",
        "| 41 | Medium | Create configuration dataclass | [ ] Not Started | Architecture |",
        "| 42 | Low | Add simple query language support | [ ] Not Started | Feature |",
        "| 43 | Medium | Optimize chunk scoring performance | ✅ Completed | Performance |",
        "| 44 | Low | Remove deprecated feedforward_sources | [ ] Not Started | Cleanup |",
        "| 45 | Medium | Add LRU cache for query results | ✅ Completed | Performance |",
        "| 46 | Low | Standardize return types with dataclasses | [ ] Not Started | API |",
        "",
        "**Completed:** 9/13 tasks",
        "**High Priority Remaining:** 0 tasks",
        "**Medium Priority Remaining:** 1 task (#41)",
        "**Low Priority Remaining:** 3 tasks (#42, #44, #46)",
        "",
        "**Total Tests:** 546 (all passing)",
        "",
        "---",
        "",
        "## Intent-Based Code Search Enhancements",
        "",
        "The following tasks enhance the system's ability to understand developer intent and retrieve code by meaning rather than exact keyword matching.",
        "",
        "---",
        "",
        "### 48. Add Code-Aware Tokenization",
        "",
        "**Files:** `cortical/tokenizer.py`, `tests/test_tokenizer.py`",
        "**Status:** [x] Completed",
        "**Priority:** High",
        "",
        "**Problem:**",
        "Current tokenizer treats code like prose. It doesn't understand that `getUserCredentials`, `get_user_credentials`, and `fetch user credentials` are semantically equivalent.",
        "",
        "**Solution Applied:**",
        "1. Added `split_identifier()` function to break camelCase, PascalCase, underscore_style, and CONSTANT_STYLE",
        "2. Added `PROGRAMMING_KEYWORDS` constant for common code terms (function, class, def, get, set, etc.)",
        "3. Added `split_identifiers` parameter to `Tokenizer.__init__()` and `tokenize()` method",
        "4. Tokens include both original identifier and split components when enabled",
        "5. Split parts don't duplicate already-seen tokens, preserving proper bigram extraction",
        "",
        "**Example:**",
        "```python",
        "tokenizer = Tokenizer(split_identifiers=True)",
        "tokens = tokenizer.tokenize(\"getUserCredentials\")",
        "# ['getusercredentials', 'get', 'user', 'credentials']",
        "```",
        "",
        "**Tests Added:**",
        "- 8 tests for `split_identifier()` function (camelCase, PascalCase, underscore_style, acronyms)",
        "- 8 tests for code-aware tokenization (splitting, stop word filtering, min length, deduplication)",
        "",
        "---",
        "",
        "### 49. Add Synonym/Concept Mapping for Code Patterns",
        "",
        "**Files:** `cortical/code_concepts.py`, `cortical/query.py`, `cortical/processor.py`",
        "**Status:** [x] Completed",
        "**Priority:** High",
        "",
        "**Problem:**",
        "The system doesn't know that \"fetch\", \"get\", \"retrieve\", \"load\" are often interchangeable in code contexts, or that \"auth\", \"authentication\", \"credentials\", \"login\" form a concept cluster.",
        "",
        "**Solution Applied:**",
        "1. Created `cortical/code_concepts.py` with 16 programming concept groups",
        "2. Added `expand_code_concepts()` function for query expansion",
        "3. Integrated with `expand_query()` via `use_code_concepts` parameter",
        "4. Added `expand_query_for_code()` convenience method to processor",
        "5. Added 33 tests in `tests/test_code_concepts.py`",
        "",
        "**Concept Groups Implemented:**",
        "- retrieval, storage, deletion, auth, error, validation",
        "- transform, network, database, async, config, logging",
        "- testing, file, iteration, lifecycle, events",
        "",
        "---",
        "",
        "### 50. Add Intent-Based Query Understanding",
        "",
        "**Files:** `cortical/query.py`, `cortical/processor.py`, `tests/test_intent_query.py`",
        "**Status:** [x] Completed",
        "**Priority:** High",
        "",
        "**Problem:**",
        "Natural language queries like \"where do we handle authentication?\" aren't decomposed into searchable intents.",
        "",
        "**Solution Applied:**",
        "1. Added `parse_intent_query()` to extract action + subject + intent + expanded terms",
        "2. Added `ParsedIntent` TypedDict for structured results",
        "3. Added `QUESTION_INTENTS` mapping (where→location, how→implementation, what→definition, why→rationale, when→lifecycle)",
        "4. Added `ACTION_VERBS` frozenset with 50+ common programming verbs",
        "5. Added `search_by_intent()` for intent-aware document search",
        "6. Added processor wrapper methods",
        "7. Added 24 tests in `tests/test_intent_query.py`",
        "",
        "**Example:**",
        "```python",
        "parse_intent_query(\"where do we handle authentication?\")",
        "# Returns: {",
        "#   'action': 'handle',",
        "#   'subject': 'authentication',",
        "#   'intent': 'location',",
        "#   'question_word': 'where',",
        "#   'expanded_terms': ['handle', 'authentication', 'auth', 'login', ...]",
        "# }",
        "```",
        "",
        "---",
        "",
        "### 51. Add Fingerprint Export API",
        "",
        "**Files:** `cortical/fingerprint.py`, `cortical/processor.py`, `tests/test_fingerprint.py`",
        "**Status:** [x] Completed",
        "**Priority:** Medium",
        "",
        "**Problem:**",
        "No way to export or compare the semantic representation of code blocks.",
        "",
        "**Solution Applied:**",
        "1. Created `cortical/fingerprint.py` with `SemanticFingerprint` TypedDict",
        "2. Added `compute_fingerprint()` returning terms, concepts, bigrams, top_terms",
        "3. Added `compare_fingerprints()` for cosine similarity scoring",
        "4. Added `explain_fingerprint()` showing top contributing terms and concepts",
        "5. Added `explain_similarity()` for human-readable explanations",
        "6. Added processor methods: `get_fingerprint()`, `compare_fingerprints()`, `explain_fingerprint()`, `explain_similarity()`, `find_similar_texts()`",
        "7. Added 24 tests in `tests/test_fingerprint.py`",
        "",
        "**Use Cases:**",
        "- Compare similarity between functions",
        "- Find duplicate/similar code blocks",
        "- Explain why two code blocks are related",
        "",
        "---",
        "",
        "### 52. Optimize Query-to-Corpus Comparison",
        "",
        "**Files:** `cortical/query.py`, `cortical/processor.py`, `scripts/search_codebase.py`",
        "**Status:** [x] Completed",
        "**Priority:** Medium",
        "",
        "**Problem:**",
        "Each query recomputes expansions and scores against all documents. For interactive use, this should be faster.",
        "",
        "**Solution Applied:**",
        "1. Added `fast_find_documents()` using candidate pre-filtering",
        "2. Added `build_document_index()` for pre-computed inverted index",
        "3. Added `search_with_index()` for fastest cached search",
        "4. Added processor wrappers: `fast_find_documents()`, `build_search_index()`, `search_with_index()`",
        "5. Added `--fast` flag to search_codebase.py script",
        "6. Added 20 tests in `tests/test_query_optimization.py`",
        "",
        "**Performance:**",
        "- `fast_find_documents()`: ~2-3x faster than full search",
        "- `search_with_index()`: Fastest when index is cached",
        "",
        "---",
        "",
        "*Updated 2025-12-10*"
      ],
      "lines_removed": [],
      "context_before": [
        "- Lateral connection expansion via `expand_query()`",
        "- Semantic relation expansion via `expand_query_semantic()`",
        "- Merging of expansion results with appropriate weighting",
        "- Configurable parameters: `max_expansions`, `semantic_discount`",
        "",
        "All 6 functions now use this helper, reducing code duplication by ~100 lines.",
        "",
        "---",
        "",
        "*Updated from code review on 2025-12-10*"
      ],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "cortical/analysis.py",
      "function": "def compute_pagerank(",
      "start_line": 33,
      "lines_added": [
        "",
        "    Raises:",
        "        ValueError: If damping is not in range (0, 1)",
        "    if not (0 < damping < 1):",
        "        raise ValueError(f\"damping must be between 0 and 1, got {damping}\")",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "    columns receive higher scores.",
        "",
        "    Args:",
        "        layer: The layer to compute PageRank for",
        "        damping: Damping factor (probability of following links)",
        "        iterations: Maximum number of iterations",
        "        tolerance: Convergence threshold",
        "",
        "    Returns:",
        "        Dictionary mapping column IDs to PageRank scores"
      ],
      "context_after": [
        "    \"\"\"",
        "    n = len(layer.minicolumns)",
        "    if n == 0:",
        "        return {}",
        "",
        "    # Initialize PageRank uniformly",
        "    pagerank = {col.id: 1.0 / n for col in layer.minicolumns.values()}",
        "",
        "    # Build incoming links map",
        "    incoming: Dict[str, List[Tuple[str, float]]] = defaultdict(list)",
        "    outgoing_sum: Dict[str, float] = defaultdict(float)"
      ],
      "change_type": "add"
    },
    {
      "file": "cortical/analysis.py",
      "function": "def compute_semantic_pagerank(",
      "start_line": 130,
      "lines_added": [
        "",
        "    Raises:",
        "        ValueError: If damping is not in range (0, 1)",
        "    if not (0 < damping < 1):",
        "        raise ValueError(f\"damping must be between 0 and 1, got {damping}\")",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "    Returns:",
        "        Dict containing:",
        "        - pagerank: Dict mapping column IDs to PageRank scores",
        "        - iterations_run: Number of iterations until convergence",
        "        - edges_with_relations: Number of edges that had semantic relation info",
        "",
        "    Example:",
        "        >>> relations = [(\"neural\", \"RelatedTo\", \"networks\", 0.8)]",
        "        >>> result = compute_semantic_pagerank(layer, relations)",
        "        >>> print(f\"PageRank converged in {result['iterations_run']} iterations\")"
      ],
      "context_after": [
        "    \"\"\"",
        "    n = len(layer.minicolumns)",
        "    if n == 0:",
        "        return {'pagerank': {}, 'iterations_run': 0, 'edges_with_relations': 0}",
        "",
        "    # Use default weights if not provided",
        "    weights = relation_weights or RELATION_WEIGHTS",
        "",
        "    # Build semantic relation lookup: (term1, term2) -> (relation_type, weight)",
        "    semantic_lookup: Dict[Tuple[str, str], Tuple[str, float]] = {}",
        "    for t1, relation, t2, rel_weight in semantic_relations:"
      ],
      "change_type": "add"
    },
    {
      "file": "cortical/analysis.py",
      "function": "def compute_hierarchical_pagerank(",
      "start_line": 253,
      "lines_added": [
        "",
        "    Raises:",
        "        ValueError: If damping or cross_layer_damping is not in range (0, 1)",
        "    if not (0 < damping < 1):",
        "        raise ValueError(f\"damping must be between 0 and 1, got {damping}\")",
        "    if not (0 < cross_layer_damping < 1):",
        "        raise ValueError(f\"cross_layer_damping must be between 0 and 1, got {cross_layer_damping}\")",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "",
        "    Returns:",
        "        Dict containing:",
        "        - iterations_run: Number of global iterations",
        "        - converged: Whether the algorithm converged",
        "        - layer_stats: Per-layer statistics",
        "",
        "    Example:",
        "        >>> result = compute_hierarchical_pagerank(layers)",
        "        >>> print(f\"Converged in {result['iterations_run']} iterations\")"
      ],
      "context_after": [
        "    \"\"\"",
        "    # Define layer order for propagation",
        "    layer_order = [",
        "        CorticalLayer.TOKENS,",
        "        CorticalLayer.BIGRAMS,",
        "        CorticalLayer.CONCEPTS,",
        "        CorticalLayer.DOCUMENTS",
        "    ]",
        "",
        "    # Filter to only existing layers with minicolumns",
        "    active_layers = [l for l in layer_order if l in layers and layers[l].column_count() > 0]"
      ],
      "change_type": "add"
    },
    {
      "file": "cortical/analysis.py",
      "function": "def compute_bigram_connections(",
      "start_line": 913,
      "lines_added": [
        "    # Note: Bigrams use space separators (e.g., \"neural networks\")",
        "        parts = bigram.content.split(' ')"
      ],
      "lines_removed": [
        "        parts = bigram.content.split('_')"
      ],
      "context_before": [
        "            'component_connections': 0,",
        "            'chain_connections': 0,",
        "            'cooccurrence_connections': 0",
        "        }",
        "",
        "    bigrams = list(layer1.minicolumns.values())",
        "",
        "    # Build indexes for efficient lookup",
        "    # left_component_index: {\"neural\": [bigram1, bigram2, ...]}",
        "    # right_component_index: {\"networks\": [bigram1, bigram3, ...]}"
      ],
      "context_after": [
        "    left_index: Dict[str, List[Minicolumn]] = defaultdict(list)",
        "    right_index: Dict[str, List[Minicolumn]] = defaultdict(list)",
        "",
        "    for bigram in bigrams:",
        "        if len(parts) == 2:",
        "            left_index[parts[0]].append(bigram)",
        "            right_index[parts[1]].append(bigram)",
        "",
        "    # Track connection types for statistics",
        "    component_connections = 0",
        "    chain_connections = 0",
        "    cooccurrence_connections = 0",
        "",
        "    # Track which pairs we've already connected (avoid duplicates)"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/code_concepts.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "\"\"\"",
        "Code Concepts Module",
        "====================",
        "",
        "Programming concept groups for semantic code search.",
        "",
        "Maps common programming synonyms and related terms to enable",
        "intent-based code retrieval. When a developer searches for \"get user\",",
        "the system can also find \"fetch user\", \"load user\", \"retrieve user\".",
        "\"\"\"",
        "",
        "from typing import Dict, List, Set, FrozenSet",
        "",
        "",
        "# Programming concept groups - terms that are often interchangeable in code",
        "CODE_CONCEPT_GROUPS: Dict[str, FrozenSet[str]] = {",
        "    # Data retrieval operations",
        "    'retrieval': frozenset([",
        "        'get', 'fetch', 'load', 'retrieve', 'read', 'query', 'find',",
        "        'lookup', 'obtain', 'acquire', 'pull', 'select'",
        "    ]),",
        "",
        "    # Data storage operations",
        "    'storage': frozenset([",
        "        'save', 'store', 'write', 'persist', 'cache', 'put', 'set',",
        "        'insert', 'add', 'create', 'commit', 'push', 'update'",
        "    ]),",
        "",
        "    # Deletion operations",
        "    'deletion': frozenset([",
        "        'delete', 'remove', 'drop', 'clear', 'destroy', 'purge',",
        "        'erase', 'clean', 'reset', 'dispose', 'unset'",
        "    ]),",
        "",
        "    # Authentication and security",
        "    'auth': frozenset([",
        "        'auth', 'authentication', 'login', 'logout', 'credentials',",
        "        'token', 'session', 'password', 'user', 'permission', 'role',",
        "        'access', 'authorize', 'verify', 'validate', 'identity'",
        "    ]),",
        "",
        "    # Error handling",
        "    'error': frozenset([",
        "        'error', 'exception', 'fail', 'failure', 'catch', 'handle',",
        "        'throw', 'raise', 'try', 'recover', 'retry', 'fallback',",
        "        'invalid', 'warning', 'fault', 'crash'",
        "    ]),",
        "",
        "    # Validation and checking",
        "    'validation': frozenset([",
        "        'validate', 'check', 'verify', 'assert', 'ensure', 'confirm',",
        "        'test', 'inspect', 'examine', 'sanitize', 'filter', 'guard'",
        "    ]),",
        "",
        "    # Transformation operations",
        "    'transform': frozenset([",
        "        'transform', 'convert', 'parse', 'format', 'serialize',",
        "        'deserialize', 'encode', 'decode', 'map', 'reduce', 'filter',",
        "        'normalize', 'process', 'translate', 'render'",
        "    ]),",
        "",
        "    # Network and API",
        "    'network': frozenset([",
        "        'request', 'response', 'api', 'endpoint', 'http', 'rest',",
        "        'client', 'server', 'socket', 'connection', 'send', 'receive',",
        "        'url', 'route', 'handler', 'middleware'",
        "    ]),",
        "",
        "    # Database operations",
        "    'database': frozenset([",
        "        'database', 'db', 'sql', 'query', 'table', 'record', 'row',",
        "        'column', 'index', 'schema', 'migration', 'model', 'entity',",
        "        'repository', 'orm', 'transaction'",
        "    ]),",
        "",
        "    # Async and concurrency",
        "    'async': frozenset([",
        "        'async', 'await', 'promise', 'future', 'callback', 'thread',",
        "        'concurrent', 'parallel', 'worker', 'queue', 'task', 'job',",
        "        'schedule', 'spawn', 'sync', 'lock', 'mutex'",
        "    ]),",
        "",
        "    # Configuration and settings",
        "    'config': frozenset([",
        "        'config', 'configuration', 'settings', 'options', 'preferences',",
        "        'env', 'environment', 'property', 'parameter', 'argument',",
        "        'flag', 'constant', 'default', 'override'",
        "    ]),",
        "",
        "    # Logging and monitoring",
        "    'logging': frozenset([",
        "        'log', 'logger', 'logging', 'debug', 'info', 'warn', 'trace',",
        "        'monitor', 'metric', 'telemetry', 'track', 'audit', 'record',",
        "        'print', 'output', 'verbose'",
        "    ]),",
        "",
        "    # Testing",
        "    'testing': frozenset([",
        "        'test', 'spec', 'mock', 'stub', 'fake', 'fixture', 'assert',",
        "        'expect', 'verify', 'unit', 'integration', 'coverage', 'suite',",
        "        'setup', 'teardown', 'before', 'after'",
        "    ]),",
        "",
        "    # File operations",
        "    'file': frozenset([",
        "        'file', 'path', 'directory', 'folder', 'read', 'write', 'open',",
        "        'close', 'stream', 'buffer', 'io', 'filesystem', 'upload',",
        "        'download', 'copy', 'move', 'rename'",
        "    ]),",
        "",
        "    # Iteration and collections",
        "    'iteration': frozenset([",
        "        'iterate', 'loop', 'each', 'map', 'filter', 'reduce', 'fold',",
        "        'list', 'array', 'collection', 'set', 'dict', 'hash', 'tree',",
        "        'queue', 'stack', 'sort', 'search', 'find'",
        "    ]),",
        "",
        "    # Initialization and lifecycle",
        "    'lifecycle': frozenset([",
        "        'init', 'initialize', 'setup', 'start', 'stop', 'shutdown',",
        "        'bootstrap', 'create', 'destroy', 'build', 'configure',",
        "        'register', 'unregister', 'connect', 'disconnect', 'close'",
        "    ]),",
        "",
        "    # Events and messaging",
        "    'events': frozenset([",
        "        'event', 'emit', 'listen', 'subscribe', 'publish', 'dispatch',",
        "        'handler', 'callback', 'hook', 'trigger', 'notify', 'observe',",
        "        'broadcast', 'signal', 'message', 'channel'",
        "    ]),",
        "}",
        "",
        "# Build reverse index: term -> list of concept groups it belongs to",
        "_TERM_TO_CONCEPTS: Dict[str, List[str]] = {}",
        "for concept, terms in CODE_CONCEPT_GROUPS.items():",
        "    for term in terms:",
        "        if term not in _TERM_TO_CONCEPTS:",
        "            _TERM_TO_CONCEPTS[term] = []",
        "        _TERM_TO_CONCEPTS[term].append(concept)",
        "",
        "",
        "def get_related_terms(term: str, max_terms: int = 5) -> List[str]:",
        "    \"\"\"",
        "    Get programming terms related to the given term.",
        "",
        "    Args:",
        "        term: A programming term (e.g., \"fetch\", \"authenticate\")",
        "        max_terms: Maximum number of related terms to return",
        "",
        "    Returns:",
        "        List of related terms, excluding the input term",
        "",
        "    Example:",
        "        >>> get_related_terms(\"fetch\")",
        "        ['get', 'load', 'retrieve', 'read', 'query']",
        "    \"\"\"",
        "    term_lower = term.lower()",
        "    related: Set[str] = set()",
        "",
        "    # Find all concept groups this term belongs to",
        "    concepts = _TERM_TO_CONCEPTS.get(term_lower, [])",
        "",
        "    for concept in concepts:",
        "        terms = CODE_CONCEPT_GROUPS.get(concept, frozenset())",
        "        related.update(terms)",
        "",
        "    # Remove the original term",
        "    related.discard(term_lower)",
        "",
        "    # Return top terms sorted alphabetically for consistent results",
        "    return sorted(related)[:max_terms]",
        "",
        "",
        "def expand_code_concepts(",
        "    terms: List[str],",
        "    max_expansions_per_term: int = 3,",
        "    weight: float = 0.6",
        ") -> Dict[str, float]:",
        "    \"\"\"",
        "    Expand a list of terms using code concept groups.",
        "",
        "    Args:",
        "        terms: List of query terms to expand",
        "        max_expansions_per_term: Max related terms to add per input term",
        "        weight: Weight to assign to expanded terms (0.0-1.0)",
        "",
        "    Returns:",
        "        Dict mapping expanded terms to weights",
        "",
        "    Example:",
        "        >>> expand_code_concepts([\"fetch\", \"user\"])",
        "        {'get': 0.6, 'load': 0.6, 'retrieve': 0.6, ...}",
        "    \"\"\"",
        "    expanded: Dict[str, float] = {}",
        "    input_terms = set(t.lower() for t in terms)",
        "",
        "    for term in terms:",
        "        related = get_related_terms(term, max_terms=max_expansions_per_term)",
        "        for related_term in related:",
        "            # Don't add terms that were in the original query",
        "            if related_term not in input_terms:",
        "                # Keep highest weight if term appears multiple times",
        "                if related_term not in expanded or expanded[related_term] < weight:",
        "                    expanded[related_term] = weight",
        "",
        "    return expanded",
        "",
        "",
        "def get_concept_group(term: str) -> List[str]:",
        "    \"\"\"",
        "    Get the concept group names a term belongs to.",
        "",
        "    Args:",
        "        term: A programming term",
        "",
        "    Returns:",
        "        List of concept group names",
        "",
        "    Example:",
        "        >>> get_concept_group(\"fetch\")",
        "        ['retrieval']",
        "        >>> get_concept_group(\"validate\")",
        "        ['validation', 'testing']",
        "    \"\"\"",
        "    return _TERM_TO_CONCEPTS.get(term.lower(), [])",
        "",
        "",
        "def list_concept_groups() -> List[str]:",
        "    \"\"\"",
        "    List all available concept group names.",
        "",
        "    Returns:",
        "        Sorted list of concept group names",
        "    \"\"\"",
        "    return sorted(CODE_CONCEPT_GROUPS.keys())",
        "",
        "",
        "def get_group_terms(group_name: str) -> List[str]:",
        "    \"\"\"",
        "    Get all terms in a concept group.",
        "",
        "    Args:",
        "        group_name: Name of the concept group",
        "",
        "    Returns:",
        "        Sorted list of terms in the group, or empty list if group not found",
        "    \"\"\"",
        "    terms = CODE_CONCEPT_GROUPS.get(group_name, frozenset())",
        "    return sorted(terms)"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "cortical/fingerprint.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "\"\"\"",
        "Fingerprint Module",
        "==================",
        "",
        "Semantic fingerprinting for code comparison and similarity analysis.",
        "",
        "A fingerprint is an interpretable representation of a text's semantic",
        "content, including term weights, concept memberships, and relations.",
        "Fingerprints can be compared to find similar code blocks or to explain",
        "why two pieces of code are related.",
        "\"\"\"",
        "",
        "from typing import Dict, List, Tuple, Optional, TypedDict, Any",
        "from collections import defaultdict",
        "import math",
        "",
        "from .layers import CorticalLayer, HierarchicalLayer",
        "from .tokenizer import Tokenizer",
        "from .code_concepts import get_concept_group",
        "",
        "",
        "class SemanticFingerprint(TypedDict):",
        "    \"\"\"Structured representation of a text's semantic fingerprint.\"\"\"",
        "    terms: Dict[str, float]           # Term -> TF-IDF weight",
        "    concepts: Dict[str, float]        # Concept group -> coverage score",
        "    bigrams: Dict[str, float]         # Bigram -> weight",
        "    top_terms: List[Tuple[str, float]]  # Top N terms by weight",
        "    term_count: int                    # Total unique terms",
        "    raw_text_hash: int                 # Hash of original text for identity check",
        "",
        "",
        "def compute_fingerprint(",
        "    text: str,",
        "    tokenizer: Tokenizer,",
        "    layers: Optional[Dict[CorticalLayer, HierarchicalLayer]] = None,",
        "    top_n: int = 20",
        ") -> SemanticFingerprint:",
        "    \"\"\"",
        "    Compute the semantic fingerprint of a text.",
        "",
        "    The fingerprint captures the semantic essence of the text in an",
        "    interpretable format that can be compared with other fingerprints.",
        "",
        "    Args:",
        "        text: Input text to fingerprint",
        "        tokenizer: Tokenizer instance",
        "        layers: Optional corpus layers for TF-IDF weighting",
        "        top_n: Number of top terms to include",
        "",
        "    Returns:",
        "        SemanticFingerprint with terms, concepts, bigrams, and metadata",
        "    \"\"\"",
        "    # Tokenize",
        "    tokens = tokenizer.tokenize(text)",
        "    bigrams = tokenizer.extract_ngrams(tokens, n=2)",
        "",
        "    # Compute term frequencies",
        "    term_freq: Dict[str, int] = defaultdict(int)",
        "    for token in tokens:",
        "        term_freq[token] += 1",
        "",
        "    # Compute bigram frequencies",
        "    bigram_freq: Dict[str, int] = defaultdict(int)",
        "    for bigram in bigrams:",
        "        bigram_freq[bigram] += 1",
        "",
        "    # Normalize to TF weights (or use corpus TF-IDF if available)",
        "    total_terms = len(tokens) if tokens else 1",
        "    term_weights: Dict[str, float] = {}",
        "",
        "    for term, freq in term_freq.items():",
        "        tf = freq / total_terms",
        "",
        "        # If we have corpus layers, use IDF weighting",
        "        if layers:",
        "            layer0 = layers.get(CorticalLayer.TOKENS)",
        "            if layer0:",
        "                col = layer0.get_minicolumn(term)",
        "                if col and col.tfidf > 0:",
        "                    # Use corpus TF-IDF as weight",
        "                    term_weights[term] = tf * col.tfidf",
        "                else:",
        "                    term_weights[term] = tf",
        "            else:",
        "                term_weights[term] = tf",
        "        else:",
        "            term_weights[term] = tf",
        "",
        "    # Normalize bigram weights",
        "    total_bigrams = len(bigrams) if bigrams else 1",
        "    bigram_weights: Dict[str, float] = {}",
        "    for bigram, freq in bigram_freq.items():",
        "        bigram_weights[bigram] = freq / total_bigrams",
        "",
        "    # Compute concept coverage",
        "    concept_scores: Dict[str, float] = defaultdict(float)",
        "    for term, weight in term_weights.items():",
        "        groups = get_concept_group(term)",
        "        for group in groups:",
        "            concept_scores[group] += weight",
        "",
        "    # Get top terms",
        "    sorted_terms = sorted(term_weights.items(), key=lambda x: x[1], reverse=True)",
        "    top_terms = sorted_terms[:top_n]",
        "",
        "    return SemanticFingerprint(",
        "        terms=term_weights,",
        "        concepts=dict(concept_scores),",
        "        bigrams=bigram_weights,",
        "        top_terms=top_terms,",
        "        term_count=len(term_weights),",
        "        raw_text_hash=hash(text)",
        "    )",
        "",
        "",
        "def compare_fingerprints(",
        "    fp1: SemanticFingerprint,",
        "    fp2: SemanticFingerprint",
        ") -> Dict[str, Any]:",
        "    \"\"\"",
        "    Compare two fingerprints and compute similarity metrics.",
        "",
        "    Args:",
        "        fp1: First fingerprint",
        "        fp2: Second fingerprint",
        "",
        "    Returns:",
        "        Dict with similarity scores and shared terms",
        "    \"\"\"",
        "    # Check for identical text",
        "    if fp1['raw_text_hash'] == fp2['raw_text_hash']:",
        "        return {",
        "            'identical': True,",
        "            'term_similarity': 1.0,",
        "            'concept_similarity': 1.0,",
        "            'overall_similarity': 1.0,",
        "            'shared_terms': list(fp1['terms'].keys()),",
        "            'shared_concepts': list(fp1['concepts'].keys()),",
        "        }",
        "",
        "    # Compute cosine similarity for terms",
        "    term_sim = _cosine_similarity(fp1['terms'], fp2['terms'])",
        "",
        "    # Compute cosine similarity for concepts",
        "    concept_sim = _cosine_similarity(fp1['concepts'], fp2['concepts'])",
        "",
        "    # Compute bigram similarity",
        "    bigram_sim = _cosine_similarity(fp1['bigrams'], fp2['bigrams'])",
        "",
        "    # Find shared terms",
        "    shared_terms = set(fp1['terms'].keys()) & set(fp2['terms'].keys())",
        "",
        "    # Find shared concepts",
        "    shared_concepts = set(fp1['concepts'].keys()) & set(fp2['concepts'].keys())",
        "",
        "    # Compute overall similarity (weighted average)",
        "    overall = 0.5 * term_sim + 0.3 * concept_sim + 0.2 * bigram_sim",
        "",
        "    return {",
        "        'identical': False,",
        "        'term_similarity': term_sim,",
        "        'concept_similarity': concept_sim,",
        "        'bigram_similarity': bigram_sim,",
        "        'overall_similarity': overall,",
        "        'shared_terms': sorted(shared_terms),",
        "        'shared_concepts': sorted(shared_concepts),",
        "        'unique_to_fp1': sorted(set(fp1['terms'].keys()) - shared_terms),",
        "        'unique_to_fp2': sorted(set(fp2['terms'].keys()) - shared_terms),",
        "    }",
        "",
        "",
        "def explain_fingerprint(",
        "    fp: SemanticFingerprint,",
        "    top_n: int = 10",
        ") -> Dict[str, Any]:",
        "    \"\"\"",
        "    Generate a human-readable explanation of a fingerprint.",
        "",
        "    Args:",
        "        fp: Fingerprint to explain",
        "        top_n: Number of top items to include in explanation",
        "",
        "    Returns:",
        "        Dict with explanation components",
        "    \"\"\"",
        "    # Get top terms",
        "    top_terms = fp['top_terms'][:top_n]",
        "",
        "    # Get top concepts",
        "    sorted_concepts = sorted(",
        "        fp['concepts'].items(),",
        "        key=lambda x: x[1],",
        "        reverse=True",
        "    )",
        "    top_concepts = sorted_concepts[:top_n]",
        "",
        "    # Get top bigrams",
        "    sorted_bigrams = sorted(",
        "        fp['bigrams'].items(),",
        "        key=lambda x: x[1],",
        "        reverse=True",
        "    )",
        "    top_bigrams = sorted_bigrams[:top_n]",
        "",
        "    # Generate summary",
        "    summary_parts = []",
        "    if top_concepts:",
        "        concept_names = [c[0] for c in top_concepts[:3]]",
        "        summary_parts.append(f\"Concepts: {', '.join(concept_names)}\")",
        "",
        "    if top_terms:",
        "        term_names = [t[0] for t in top_terms[:5]]",
        "        summary_parts.append(f\"Key terms: {', '.join(term_names)}\")",
        "",
        "    return {",
        "        'summary': ' | '.join(summary_parts) if summary_parts else 'No significant terms',",
        "        'top_terms': top_terms,",
        "        'top_concepts': top_concepts,",
        "        'top_bigrams': top_bigrams,",
        "        'term_count': fp['term_count'],",
        "        'concept_coverage': len(fp['concepts']),",
        "    }",
        "",
        "",
        "def explain_similarity(",
        "    fp1: SemanticFingerprint,",
        "    fp2: SemanticFingerprint,",
        "    comparison: Optional[Dict[str, Any]] = None",
        ") -> str:",
        "    \"\"\"",
        "    Generate a human-readable explanation of why two fingerprints are similar.",
        "",
        "    Args:",
        "        fp1: First fingerprint",
        "        fp2: Second fingerprint",
        "        comparison: Optional pre-computed comparison result",
        "",
        "    Returns:",
        "        Human-readable explanation string",
        "    \"\"\"",
        "    if comparison is None:",
        "        comparison = compare_fingerprints(fp1, fp2)",
        "",
        "    if comparison['identical']:",
        "        return \"These texts are identical.\"",
        "",
        "    lines = []",
        "    similarity = comparison['overall_similarity']",
        "",
        "    if similarity > 0.8:",
        "        lines.append(\"These texts are highly similar.\")",
        "    elif similarity > 0.5:",
        "        lines.append(\"These texts have moderate similarity.\")",
        "    elif similarity > 0.2:",
        "        lines.append(\"These texts have some common elements.\")",
        "    else:",
        "        lines.append(\"These texts are quite different.\")",
        "",
        "    # Explain shared concepts",
        "    shared_concepts = comparison.get('shared_concepts', [])",
        "    if shared_concepts:",
        "        lines.append(f\"Shared concept domains: {', '.join(shared_concepts[:5])}\")",
        "",
        "    # Explain shared terms",
        "    shared_terms = comparison.get('shared_terms', [])",
        "    if shared_terms:",
        "        # Get top shared terms by combined weight",
        "        term_importance = []",
        "        for term in shared_terms:",
        "            weight = fp1['terms'].get(term, 0) + fp2['terms'].get(term, 0)",
        "            term_importance.append((term, weight))",
        "        term_importance.sort(key=lambda x: x[1], reverse=True)",
        "        top_shared = [t[0] for t in term_importance[:5]]",
        "        lines.append(f\"Key shared terms: {', '.join(top_shared)}\")",
        "",
        "    # Note differences",
        "    unique1 = comparison.get('unique_to_fp1', [])",
        "    unique2 = comparison.get('unique_to_fp2', [])",
        "    if unique1 or unique2:",
        "        lines.append(f\"First text has {len(unique1)} unique terms, second has {len(unique2)}.\")",
        "",
        "    return '\\n'.join(lines)",
        "",
        "",
        "def _cosine_similarity(vec1: Dict[str, float], vec2: Dict[str, float]) -> float:",
        "    \"\"\"",
        "    Compute cosine similarity between two sparse vectors.",
        "",
        "    Args:",
        "        vec1: First vector as {dimension: value} dict",
        "        vec2: Second vector as {dimension: value} dict",
        "",
        "    Returns:",
        "        Cosine similarity in range [0, 1]",
        "    \"\"\"",
        "    if not vec1 or not vec2:",
        "        return 0.0",
        "",
        "    # Find common dimensions",
        "    common_keys = set(vec1.keys()) & set(vec2.keys())",
        "",
        "    if not common_keys:",
        "        return 0.0",
        "",
        "    # Compute dot product",
        "    dot_product = sum(vec1[k] * vec2[k] for k in common_keys)",
        "",
        "    # Compute magnitudes",
        "    mag1 = math.sqrt(sum(v * v for v in vec1.values()))",
        "    mag2 = math.sqrt(sum(v * v for v in vec2.values()))",
        "",
        "    if mag1 == 0 or mag2 == 0:",
        "        return 0.0",
        "",
        "    return dot_product / (mag1 * mag2)"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "cortical/processor.py",
      "function": null,
      "start_line": 1,
      "lines_added": [
        "import copy",
        "from . import fingerprint as fp_module"
      ],
      "lines_removed": [],
      "context_before": [
        "\"\"\"",
        "Cortical Text Processor - Main processor class that orchestrates all components.",
        "\"\"\"",
        "",
        "import os",
        "import re",
        "from typing import Dict, List, Tuple, Optional, Any"
      ],
      "context_after": [
        "from collections import defaultdict",
        "",
        "from .tokenizer import Tokenizer",
        "from .minicolumn import Minicolumn",
        "from .layers import CorticalLayer, HierarchicalLayer",
        "from . import analysis",
        "from . import semantics",
        "from . import embeddings as emb_module",
        "from . import query as query_module",
        "from . import gaps as gaps_module",
        "from . import persistence",
        "",
        "",
        "class CorticalTextProcessor:",
        "    \"\"\"Neocortex-inspired text processing system.\"\"\"",
        "",
        "    # Computation types for tracking staleness",
        "    COMP_TFIDF = 'tfidf'",
        "    COMP_PAGERANK = 'pagerank'",
        "    COMP_ACTIVATION = 'activation'",
        "    COMP_DOC_CONNECTIONS = 'doc_connections'"
      ],
      "change_type": "add"
    },
    {
      "file": "cortical/processor.py",
      "function": "class CorticalTextProcessor:",
      "start_line": 38,
      "lines_added": [
        "        # LRU cache for query expansion results",
        "        self._query_expansion_cache: Dict[str, Dict[str, float]] = {}",
        "        self._query_cache_max_size: int = 100",
        "",
        "        Raises:",
        "            ValueError: If doc_id or content is empty or not a string",
        "        # Input validation",
        "        if not isinstance(doc_id, str) or not doc_id:",
        "            raise ValueError(\"doc_id must be a non-empty string\")",
        "        if not isinstance(content, str):",
        "            raise ValueError(\"content must be a string\")",
        "        if not content.strip():",
        "            raise ValueError(\"content must not be empty or whitespace-only\")",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "            CorticalLayer.BIGRAMS: HierarchicalLayer(CorticalLayer.BIGRAMS),",
        "            CorticalLayer.CONCEPTS: HierarchicalLayer(CorticalLayer.CONCEPTS),",
        "            CorticalLayer.DOCUMENTS: HierarchicalLayer(CorticalLayer.DOCUMENTS),",
        "        }",
        "        self.documents: Dict[str, str] = {}",
        "        self.document_metadata: Dict[str, Dict[str, Any]] = {}",
        "        self.embeddings: Dict[str, List[float]] = {}",
        "        self.semantic_relations: List[Tuple[str, str, str, float]] = []",
        "        # Track which computations are stale and need recomputation",
        "        self._stale_computations: set = set()"
      ],
      "context_after": [
        "",
        "    def process_document(",
        "        self,",
        "        doc_id: str,",
        "        content: str,",
        "        metadata: Optional[Dict[str, Any]] = None",
        "    ) -> Dict[str, int]:",
        "        \"\"\"",
        "        Process a document and add it to the corpus.",
        "",
        "        Args:",
        "            doc_id: Unique identifier for the document",
        "            content: Document text content",
        "            metadata: Optional metadata dict (source, timestamp, author, etc.)",
        "",
        "        Returns:",
        "            Dict with processing statistics (tokens, bigrams, unique_tokens)",
        "        \"\"\"",
        "        self.documents[doc_id] = content",
        "",
        "        # Store metadata if provided",
        "        if metadata:",
        "            self.document_metadata[doc_id] = metadata.copy()",
        "        elif doc_id not in self.document_metadata:",
        "            self.document_metadata[doc_id] = {}",
        "",
        "        tokens = self.tokenizer.tokenize(content)",
        "        bigrams = self.tokenizer.extract_ngrams(tokens, n=2)"
      ],
      "change_type": "add"
    },
    {
      "file": "cortical/processor.py",
      "function": "class CorticalTextProcessor:",
      "start_line": 151,
      "lines_added": [],
      "lines_removed": [
        "        import copy"
      ],
      "context_before": [
        "        \"\"\"",
        "        return self.document_metadata.get(doc_id, {})",
        "",
        "    def get_all_document_metadata(self) -> Dict[str, Dict[str, Any]]:",
        "        \"\"\"",
        "        Get metadata for all documents.",
        "",
        "        Returns:",
        "            Dict mapping doc_id to metadata dict (deep copy)",
        "        \"\"\""
      ],
      "context_after": [
        "        return copy.deepcopy(self.document_metadata)",
        "",
        "    def _mark_all_stale(self) -> None:",
        "        \"\"\"Mark all computations as stale (needing recomputation).\"\"\"",
        "        self._stale_computations = {",
        "            self.COMP_TFIDF,",
        "            self.COMP_PAGERANK,",
        "            self.COMP_ACTIVATION,",
        "            self.COMP_DOC_CONNECTIONS,",
        "            self.COMP_BIGRAM_CONNECTIONS,"
      ],
      "change_type": "delete"
    },
    {
      "file": "cortical/processor.py",
      "function": "class CorticalTextProcessor:",
      "start_line": 274,
      "lines_added": [
        "",
        "        Raises:",
        "            ValueError: If documents list is invalid or recompute level is unknown",
        "        # Input validation",
        "        if not isinstance(documents, list):",
        "            raise ValueError(\"documents must be a list\")",
        "        if not documents:",
        "            raise ValueError(\"documents list must not be empty\")",
        "",
        "        valid_recompute = {'none', 'tfidf', 'full'}",
        "        if recompute not in valid_recompute:",
        "            raise ValueError(f\"recompute must be one of {valid_recompute}\")",
        "",
        "        for i, doc in enumerate(documents):",
        "            if not isinstance(doc, (tuple, list)) or len(doc) < 2:",
        "                raise ValueError(",
        "                    f\"documents[{i}] must be a tuple of (doc_id, content) or \"",
        "                    f\"(doc_id, content, metadata)\"",
        "                )",
        "            doc_id, content = doc[0], doc[1]",
        "            if not isinstance(doc_id, str) or not doc_id:",
        "                raise ValueError(f\"documents[{i}][0] (doc_id) must be a non-empty string\")",
        "            if not isinstance(content, str):",
        "                raise ValueError(f\"documents[{i}][1] (content) must be a string\")",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "                - total_tokens: Total tokens across all documents",
        "                - recomputation: Type of recomputation performed",
        "",
        "        Example:",
        "            >>> docs = [",
        "            ...     (\"doc1\", \"First document content\", {\"source\": \"web\"}),",
        "            ...     (\"doc2\", \"Second document content\", None),",
        "            ...     (\"doc3\", \"Third document content\", {\"author\": \"AI\"}),",
        "            ... ]",
        "            >>> processor.add_documents_batch(docs, recompute='full')"
      ],
      "context_after": [
        "        \"\"\"",
        "        total_tokens = 0",
        "        total_bigrams = 0",
        "",
        "        if verbose:",
        "            print(f\"Adding {len(documents)} documents...\")",
        "",
        "        for doc_id, content, metadata in documents:",
        "            # Use process_document directly (not add_document_incremental)",
        "            # to avoid per-document recomputation",
        "            stats = self.process_document(doc_id, content, metadata)"
      ],
      "change_type": "add"
    },
    {
      "file": "cortical/processor.py",
      "function": "class CorticalTextProcessor:",
      "start_line": 539,
      "lines_added": [
        "",
        "        # Invalidate query cache since corpus state changed",
        "        self._query_expansion_cache.clear()",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "        fresh_comps = [",
        "            self.COMP_ACTIVATION,",
        "            self.COMP_PAGERANK,",
        "            self.COMP_TFIDF,",
        "            self.COMP_DOC_CONNECTIONS,",
        "            self.COMP_BIGRAM_CONNECTIONS,",
        "        ]",
        "        if build_concepts:",
        "            fresh_comps.append(self.COMP_CONCEPTS)",
        "        self._mark_fresh(*fresh_comps)"
      ],
      "context_after": [
        "        if verbose:",
        "            print(\"Done.\")",
        "",
        "        return stats",
        "    ",
        "    def propagate_activation(self, iterations: int = 3, decay: float = 0.8, verbose: bool = True) -> None:",
        "        analysis.propagate_activation(self.layers, iterations, decay)",
        "        if verbose: print(f\"Propagated activation ({iterations} iterations)\")",
        "    ",
        "    def compute_importance(self, verbose: bool = True) -> None:"
      ],
      "change_type": "add"
    },
    {
      "file": "cortical/processor.py",
      "function": "class CorticalTextProcessor:",
      "start_line": 1050,
      "lines_added": [
        "    def expand_query(",
        "        self,",
        "        query_text: str,",
        "        max_expansions: int = 10,",
        "        use_variants: bool = True,",
        "        use_code_concepts: bool = False,",
        "        verbose: bool = False",
        "    ) -> Dict[str, float]:",
        "        \"\"\"",
        "        Expand a query using lateral connections and concept clusters.",
        "",
        "        Args:",
        "            query_text: Original query string",
        "            max_expansions: Maximum expansion terms to add",
        "            use_variants: Try word variants when direct match fails",
        "            use_code_concepts: Include programming synonym expansions",
        "",
        "        Returns:",
        "            Dict mapping terms to weights",
        "        \"\"\"",
        "        return query_module.expand_query(",
        "            query_text,",
        "            self.layers,",
        "            self.tokenizer,",
        "            max_expansions=max_expansions,",
        "            use_variants=use_variants,",
        "            use_code_concepts=use_code_concepts",
        "        )",
        "",
        "    def expand_query_for_code(self, query_text: str, max_expansions: int = 15) -> Dict[str, float]:",
        "        \"\"\"",
        "        Expand a query optimized for code search.",
        "",
        "        Enables code concept expansion to find programming synonyms",
        "        (e.g., \"fetch\" also matches \"get\", \"load\", \"retrieve\").",
        "",
        "        Args:",
        "            query_text: Original query string",
        "            max_expansions: Maximum expansion terms to add",
        "",
        "        Returns:",
        "            Dict mapping terms to weights",
        "        \"\"\"",
        "        return query_module.expand_query(",
        "            query_text,",
        "            self.layers,",
        "            self.tokenizer,",
        "            max_expansions=max_expansions,",
        "            use_variants=True,",
        "            use_code_concepts=True",
        "        )",
        "",
        "    def expand_query_cached(",
        "        self,",
        "        query_text: str,",
        "        max_expansions: int = 10,",
        "        use_variants: bool = True,",
        "        use_code_concepts: bool = False",
        "    ) -> Dict[str, float]:",
        "        \"\"\"",
        "        Expand a query with caching for faster repeated lookups.",
        "",
        "        Uses an LRU-style cache to avoid recomputing expansion for",
        "        frequently repeated queries. Useful in RAG loops where the",
        "        same queries may be issued multiple times.",
        "",
        "        Args:",
        "            query_text: Original query string",
        "            max_expansions: Maximum expansion terms to add",
        "            use_variants: Try word variants when direct match fails",
        "            use_code_concepts: Include programming synonym expansions",
        "",
        "        Returns:",
        "            Dict mapping terms to weights",
        "        \"\"\"",
        "        # Create cache key from parameters",
        "        cache_key = f\"{query_text}|{max_expansions}|{use_variants}|{use_code_concepts}\"",
        "",
        "        # Check cache",
        "        if cache_key in self._query_expansion_cache:",
        "            return self._query_expansion_cache[cache_key].copy()",
        "",
        "        # Compute expansion",
        "        result = query_module.expand_query(",
        "            query_text,",
        "            self.layers,",
        "            self.tokenizer,",
        "            max_expansions=max_expansions,",
        "            use_variants=use_variants,",
        "            use_code_concepts=use_code_concepts",
        "        )",
        "",
        "        # Add to cache (with LRU eviction if at max size)",
        "        if len(self._query_expansion_cache) >= self._query_cache_max_size:",
        "            # Remove oldest entry (first key in dict - approximates LRU)",
        "            oldest_key = next(iter(self._query_expansion_cache))",
        "            del self._query_expansion_cache[oldest_key]",
        "",
        "        self._query_expansion_cache[cache_key] = result.copy()",
        "        return result",
        "",
        "    def clear_query_cache(self) -> int:",
        "        \"\"\"",
        "        Clear the query expansion cache.",
        "",
        "        Should be called after modifying the corpus (adding documents,",
        "        recomputing connections) to ensure fresh expansions.",
        "",
        "        Returns:",
        "            Number of cache entries cleared",
        "        \"\"\"",
        "        count = len(self._query_expansion_cache)",
        "        self._query_expansion_cache.clear()",
        "        return count",
        "",
        "    def set_query_cache_size(self, max_size: int) -> None:",
        "        \"\"\"",
        "        Set the maximum size of the query expansion cache.",
        "",
        "        Args:",
        "            max_size: Maximum number of queries to cache (must be > 0)",
        "",
        "        Raises:",
        "            ValueError: If max_size <= 0",
        "        \"\"\"",
        "        if max_size <= 0:",
        "            raise ValueError(f\"max_size must be positive, got {max_size}\")",
        "        self._query_cache_max_size = max_size",
        "",
        "        # Trim cache if it exceeds new size",
        "        while len(self._query_expansion_cache) > max_size:",
        "            oldest_key = next(iter(self._query_expansion_cache))",
        "            del self._query_expansion_cache[oldest_key]",
        "",
        "    def parse_intent_query(self, query_text: str) -> Dict:",
        "        \"\"\"",
        "        Parse a natural language query to extract intent and searchable terms.",
        "",
        "        Analyzes queries like \"where do we handle authentication?\" to identify:",
        "        - Question word (where) -> intent type (location)",
        "        - Action verb (handle) -> search for handling code",
        "        - Subject (authentication) -> main topic with synonyms",
        "",
        "        Args:",
        "            query_text: Natural language query string",
        "",
        "        Returns:",
        "            Dict with 'action', 'subject', 'intent', 'question_word', 'expanded_terms'",
        "        \"\"\"",
        "        return query_module.parse_intent_query(query_text)",
        "",
        "    def search_by_intent(self, query_text: str, top_n: int = 5) -> List[Tuple[str, float, Dict]]:",
        "        \"\"\"",
        "        Search the corpus using intent-based query understanding.",
        "",
        "        Parses the query to understand intent, expands terms using code concepts,",
        "        then searches with appropriate weighting based on intent type.",
        "",
        "        Args:",
        "            query_text: Natural language query string",
        "            top_n: Number of results to return",
        "",
        "        Returns:",
        "            List of (doc_id, score, parsed_intent) tuples",
        "        \"\"\"",
        "        return query_module.search_by_intent(",
        "            query_text,",
        "            self.layers,",
        "            self.tokenizer,",
        "            top_n=top_n",
        "        )",
        ""
      ],
      "lines_removed": [
        "    def expand_query(self, query_text: str, max_expansions: int = 10, use_variants: bool = True, verbose: bool = False) -> Dict[str, float]:",
        "        return query_module.expand_query(query_text, self.layers, self.tokenizer, max_expansions=max_expansions, use_variants=use_variants)",
        "    "
      ],
      "context_before": [
        "        stats = semantics.retrofit_embeddings(self.embeddings, self.semantic_relations, iterations, alpha)",
        "        if verbose: print(f\"Retrofitted embeddings (moved {stats['total_movement']:.2f} total)\")",
        "        return stats",
        "    ",
        "    def embedding_similarity(self, term1: str, term2: str) -> float:",
        "        return emb_module.embedding_similarity(self.embeddings, term1, term2)",
        "    ",
        "    def find_similar_by_embedding(self, term: str, top_n: int = 10) -> List[Tuple[str, float]]:",
        "        return emb_module.find_similar_by_embedding(self.embeddings, term, top_n)",
        "    "
      ],
      "context_after": [
        "    def expand_query_semantic(self, query_text: str, max_expansions: int = 10) -> Dict[str, float]:",
        "        return query_module.expand_query_semantic(query_text, self.layers, self.tokenizer, self.semantic_relations, max_expansions)",
        "",
        "    def complete_analogy(",
        "        self,",
        "        term_a: str,",
        "        term_b: str,",
        "        term_c: str,",
        "        top_n: int = 5,",
        "        use_embeddings: bool = True,"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/processor.py",
      "function": "class CorticalTextProcessor:",
      "start_line": 1094,
      "lines_added": [
        "",
        "        Raises:",
        "            ValueError: If any term is empty or top_n is not positive",
        "        # Input validation",
        "        for name, term in [('term_a', term_a), ('term_b', term_b), ('term_c', term_c)]:",
        "            if not isinstance(term, str) or not term.strip():",
        "                raise ValueError(f\"{name} must be a non-empty string\")",
        "        if not isinstance(top_n, int) or top_n < 1:",
        "            raise ValueError(\"top_n must be a positive integer\")",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "            List of (candidate_term, confidence, method) tuples, where method",
        "            describes which approach found this candidate ('relation:IsA',",
        "            'embedding', 'pattern')",
        "",
        "        Example:",
        "            >>> processor.extract_corpus_semantics()",
        "            >>> processor.compute_graph_embeddings()",
        "            >>> results = processor.complete_analogy(\"neural\", \"networks\", \"knowledge\")",
        "            >>> for term, score, method in results:",
        "            ...     print(f\"{term}: {score:.3f} ({method})\")"
      ],
      "context_after": [
        "        \"\"\"",
        "        if not self.semantic_relations:",
        "            self.extract_corpus_semantics(verbose=False)",
        "",
        "        return query_module.complete_analogy(",
        "            term_a, term_b, term_c,",
        "            self.layers,",
        "            self.semantic_relations,",
        "            embeddings=self.embeddings,",
        "            top_n=top_n,",
        "            use_embeddings=use_embeddings,"
      ],
      "change_type": "add"
    },
    {
      "file": "cortical/processor.py",
      "function": "class CorticalTextProcessor:",
      "start_line": 1215,
      "lines_added": [
        "",
        "        Raises:",
        "            ValueError: If query_text is empty or top_n is not positive",
        "        # Input validation",
        "        if not isinstance(query_text, str) or not query_text.strip():",
        "            raise ValueError(\"query_text must be a non-empty string\")",
        "        if not isinstance(top_n, int) or top_n < 1:",
        "            raise ValueError(\"top_n must be a positive integer\")",
        "",
        "    def fast_find_documents(",
        "        self,",
        "        query_text: str,",
        "        top_n: int = 5,",
        "        candidate_multiplier: int = 3,",
        "        use_code_concepts: bool = True",
        "    ) -> List[Tuple[str, float]]:",
        "        \"\"\"",
        "        Fast document search using candidate filtering.",
        "",
        "        Optimizes search by:",
        "        1. Using set intersection to find candidate documents",
        "        2. Only scoring top candidates fully",
        "        3. Using code concept expansion for better recall",
        "",
        "        ~2-3x faster than find_documents_for_query on large corpora.",
        "",
        "        Args:",
        "            query_text: Search query",
        "            top_n: Number of results to return",
        "            candidate_multiplier: Multiplier for candidate set size",
        "            use_code_concepts: Whether to use code concept expansion",
        "",
        "        Returns:",
        "            List of (doc_id, score) tuples ranked by relevance",
        "        \"\"\"",
        "        return query_module.fast_find_documents(",
        "            query_text,",
        "            self.layers,",
        "            self.tokenizer,",
        "            top_n=top_n,",
        "            candidate_multiplier=candidate_multiplier,",
        "            use_code_concepts=use_code_concepts",
        "        )",
        "",
        "    def build_search_index(self) -> Dict[str, Dict[str, float]]:",
        "        \"\"\"",
        "        Build an optimized inverted index for fast querying.",
        "",
        "        Pre-compute this once, then use search_with_index() for",
        "        fastest possible search.",
        "",
        "        Returns:",
        "            Dict mapping terms to {doc_id: tfidf_score} dicts",
        "        \"\"\"",
        "        return query_module.build_document_index(self.layers)",
        "",
        "    def search_with_index(",
        "        self,",
        "        query_text: str,",
        "        index: Dict[str, Dict[str, float]],",
        "        top_n: int = 5",
        "    ) -> List[Tuple[str, float]]:",
        "        \"\"\"",
        "        Search using a pre-built inverted index.",
        "",
        "        This is the fastest search method. Build the index once with",
        "        build_search_index(), then reuse for multiple queries.",
        "",
        "        Args:",
        "            query_text: Search query",
        "            index: Pre-built index from build_search_index()",
        "            top_n: Number of results to return",
        "",
        "        Returns:",
        "            List of (doc_id, score) tuples ranked by relevance",
        "        \"\"\"",
        "        return query_module.search_with_index(",
        "            query_text,",
        "            index,",
        "            self.tokenizer,",
        "            top_n=top_n",
        "        )",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "        Find documents most relevant to a query.",
        "",
        "        Args:",
        "            query_text: Search query",
        "            top_n: Number of documents to return",
        "            use_expansion: Whether to expand query terms using lateral connections",
        "            use_semantic: Whether to use semantic relations for expansion (if available)",
        "",
        "        Returns:",
        "            List of (doc_id, score) tuples ranked by relevance"
      ],
      "context_after": [
        "        \"\"\"",
        "        return query_module.find_documents_for_query(",
        "            query_text,",
        "            self.layers,",
        "            self.tokenizer,",
        "            top_n=top_n,",
        "            use_expansion=use_expansion,",
        "            semantic_relations=self.semantic_relations if use_semantic else None,",
        "            use_semantic=use_semantic",
        "        )",
        "",
        "    def find_passages_for_query(",
        "        self,",
        "        query_text: str,",
        "        top_n: int = 5,",
        "        chunk_size: int = 512,",
        "        overlap: int = 128,",
        "        use_expansion: bool = True,",
        "        doc_filter: Optional[List[str]] = None,",
        "        use_semantic: bool = True",
        "    ) -> List[Tuple[str, str, int, int, float]]:"
      ],
      "change_type": "add"
    },
    {
      "file": "cortical/processor.py",
      "function": "class CorticalTextProcessor:",
      "start_line": 1477,
      "lines_added": [
        "",
        "    # Fingerprint methods for semantic comparison",
        "    def get_fingerprint(self, text: str, top_n: int = 20) -> Dict:",
        "        \"\"\"",
        "        Compute the semantic fingerprint of a text.",
        "",
        "        The fingerprint captures the semantic essence of the text including",
        "        term weights, concept memberships, and bigrams. Fingerprints can be",
        "        compared to find similar code blocks.",
        "",
        "        Args:",
        "            text: Input text to fingerprint",
        "            top_n: Number of top terms to include",
        "",
        "        Returns:",
        "            Dict with 'terms', 'concepts', 'bigrams', 'top_terms', 'term_count'",
        "        \"\"\"",
        "        return fp_module.compute_fingerprint(text, self.tokenizer, self.layers, top_n)",
        "",
        "    def compare_fingerprints(self, fp1: Dict, fp2: Dict) -> Dict:",
        "        \"\"\"",
        "        Compare two fingerprints and compute similarity metrics.",
        "",
        "        Args:",
        "            fp1: First fingerprint from get_fingerprint()",
        "            fp2: Second fingerprint from get_fingerprint()",
        "",
        "        Returns:",
        "            Dict with similarity scores and shared terms",
        "        \"\"\"",
        "        return fp_module.compare_fingerprints(fp1, fp2)",
        "",
        "    def explain_fingerprint(self, fp: Dict, top_n: int = 10) -> Dict:",
        "        \"\"\"",
        "        Generate a human-readable explanation of a fingerprint.",
        "",
        "        Args:",
        "            fp: Fingerprint from get_fingerprint()",
        "            top_n: Number of top items to include",
        "",
        "        Returns:",
        "            Dict with explanation components including summary",
        "        \"\"\"",
        "        return fp_module.explain_fingerprint(fp, top_n)",
        "",
        "    def explain_similarity(self, fp1: Dict, fp2: Dict) -> str:",
        "        \"\"\"",
        "        Generate a human-readable explanation of why two fingerprints are similar.",
        "",
        "        Args:",
        "            fp1: First fingerprint",
        "            fp2: Second fingerprint",
        "",
        "        Returns:",
        "            Human-readable explanation string",
        "        \"\"\"",
        "        return fp_module.explain_similarity(fp1, fp2)",
        "",
        "    def find_similar_texts(",
        "        self,",
        "        text: str,",
        "        candidates: List[Tuple[str, str]],",
        "        top_n: int = 5",
        "    ) -> List[Tuple[str, float, Dict]]:",
        "        \"\"\"",
        "        Find texts most similar to the given text.",
        "",
        "        Args:",
        "            text: Query text to compare",
        "            candidates: List of (id, text) tuples to search",
        "            top_n: Number of results to return",
        "",
        "        Returns:",
        "            List of (id, similarity_score, comparison) tuples sorted by similarity",
        "        \"\"\"",
        "        query_fp = self.get_fingerprint(text)",
        "        results = []",
        "",
        "        for candidate_id, candidate_text in candidates:",
        "            candidate_fp = self.get_fingerprint(candidate_text)",
        "            comparison = self.compare_fingerprints(query_fp, candidate_fp)",
        "            results.append((candidate_id, comparison['overall_similarity'], comparison))",
        "",
        "        # Sort by similarity descending",
        "        results.sort(key=lambda x: x[1], reverse=True)",
        "        return results[:top_n]",
        ""
      ],
      "lines_removed": [
        "    "
      ],
      "context_before": [
        "        return self.layers[layer]",
        "    ",
        "    def get_document_signature(self, doc_id: str, n: int = 10) -> List[Tuple[str, float]]:",
        "        layer0 = self.layers[CorticalLayer.TOKENS]",
        "        terms = [(col.content, col.tfidf_per_doc.get(doc_id, col.tfidf)) ",
        "                 for col in layer0.minicolumns.values() if doc_id in col.document_ids]",
        "        return sorted(terms, key=lambda x: x[1], reverse=True)[:n]",
        "    ",
        "    def get_corpus_summary(self) -> Dict:",
        "        return persistence.get_state_summary(self.layers, self.documents)"
      ],
      "context_after": [
        "    def save(self, filepath: str, verbose: bool = True) -> None:",
        "        \"\"\"",
        "        Save processor state to a file.",
        "",
        "        Saves all computed state including embeddings and semantic relations,",
        "        so they don't need to be recomputed when loading.",
        "        \"\"\"",
        "        metadata = {",
        "            'has_embeddings': bool(self.embeddings),",
        "            'has_relations': bool(self.semantic_relations)"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/query.py",
      "function": null,
      "start_line": 2,
      "lines_added": [
        "from typing import Dict, List, Tuple, Optional, TypedDict",
        "import re",
        "from .code_concepts import expand_code_concepts, get_related_terms",
        "",
        "",
        "# Intent types for query understanding",
        "class ParsedIntent(TypedDict):",
        "    \"\"\"Structured representation of a parsed query intent.\"\"\"",
        "    action: Optional[str]       # The verb/action (e.g., \"handle\", \"implement\")",
        "    subject: Optional[str]      # The main subject (e.g., \"authentication\")",
        "    intent: str                 # Query intent type (location, implementation, definition, etc.)",
        "    question_word: Optional[str]  # Original question word if present",
        "    expanded_terms: List[str]   # All searchable terms with synonyms",
        "",
        "",
        "# Question word to intent mapping",
        "QUESTION_INTENTS = {",
        "    'where': 'location',      # Find location/file",
        "    'how': 'implementation',  # Find implementation details",
        "    'what': 'definition',     # Find definitions",
        "    'why': 'rationale',       # Find comments/documentation explaining reasoning",
        "    'when': 'lifecycle',      # Find when something happens (init, shutdown, etc.)",
        "    'which': 'selection',     # Find choices/options",
        "    'who': 'attribution',     # Find ownership/authorship (git blame territory)",
        "}",
        "",
        "# Common action verbs in code queries",
        "ACTION_VERBS = frozenset([",
        "    'handle', 'process', 'create', 'delete', 'update', 'fetch', 'get', 'set',",
        "    'load', 'save', 'store', 'validate', 'check', 'parse', 'format', 'convert',",
        "    'transform', 'render', 'display', 'show', 'hide', 'enable', 'disable',",
        "    'start', 'stop', 'init', 'initialize', 'setup', 'configure', 'connect',",
        "    'disconnect', 'send', 'receive', 'read', 'write', 'open', 'close',",
        "    'authenticate', 'authorize', 'login', 'logout', 'register', 'subscribe',",
        "    'publish', 'emit', 'listen', 'dispatch', 'trigger', 'call', 'invoke',",
        "    'execute', 'run', 'build', 'compile', 'test', 'deploy', 'implement',",
        "])",
        "    use_variants: bool = True,",
        "    use_code_concepts: bool = False",
        "",
        "    - Code concepts: programming synonym groups (get/fetch/load)",
        "",
        "        use_code_concepts: Include programming synonym expansions",
        ""
      ],
      "lines_removed": [
        "from typing import Dict, List, Tuple, Optional",
        "    use_variants: bool = True",
        "    ",
        "    ",
        "        "
      ],
      "context_before": [
        "Query Module",
        "============",
        "",
        "Query expansion and search functionality.",
        "",
        "Provides methods for expanding queries using lateral connections,",
        "concept clusters, and word variants, then searching the corpus",
        "using TF-IDF and graph-based scoring.",
        "\"\"\"",
        ""
      ],
      "context_after": [
        "from collections import defaultdict",
        "",
        "from .layers import CorticalLayer, HierarchicalLayer",
        "from .tokenizer import Tokenizer",
        "",
        "",
        "def expand_query(",
        "    query_text: str,",
        "    layers: Dict[CorticalLayer, HierarchicalLayer],",
        "    tokenizer: Tokenizer,",
        "    max_expansions: int = 10,",
        "    use_lateral: bool = True,",
        "    use_concepts: bool = True,",
        ") -> Dict[str, float]:",
        "    \"\"\"",
        "    Expand a query using lateral connections and concept clusters.",
        "    This mimics how the brain retrieves related memories when given a cue:",
        "    - Lateral connections: direct word associations (like priming)",
        "    - Concept clusters: semantic category membership",
        "    - Word variants: stemming and synonym mapping",
        "    Args:",
        "        query_text: Original query string",
        "        layers: Dictionary of layers",
        "        tokenizer: Tokenizer instance",
        "        max_expansions: Maximum number of expansion terms to add",
        "        use_lateral: Include terms from lateral connections",
        "        use_concepts: Include terms from concept clusters",
        "        use_variants: Try word variants when direct match fails",
        "    Returns:",
        "        Dict mapping terms to weights (original terms get weight 1.0)",
        "    \"\"\"",
        "    tokens = tokenizer.tokenize(query_text)",
        "    layer0 = layers[CorticalLayer.TOKENS]",
        "    layer2 = layers.get(CorticalLayer.CONCEPTS)",
        "    ",
        "    # Start with original terms at full weight",
        "    expanded: Dict[str, float] = {}",
        "    unmatched_tokens = []"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/query.py",
      "function": "def expand_query(",
      "start_line": 103,
      "lines_added": [
        "",
        "    # Method 3: Code concept groups (programming synonyms)",
        "    if use_code_concepts:",
        "        code_expansions = expand_code_concepts(",
        "            list(expanded.keys()),",
        "            max_expansions_per_term=3,",
        "            weight=0.6",
        "        )",
        "        for term, weight in code_expansions.items():",
        "            if term not in expanded:",
        "                candidate_expansions[term] = max(",
        "                    candidate_expansions[term], weight",
        "                )",
        "",
        "def parse_intent_query(query_text: str) -> ParsedIntent:",
        "    \"\"\"",
        "    Parse a natural language query to extract intent and searchable terms.",
        "",
        "    Analyzes queries like \"where do we handle authentication?\" to identify:",
        "    - Question word (where) -> intent type (location)",
        "    - Action verb (handle) -> search for handling code",
        "    - Subject (authentication) -> main topic with synonyms",
        "",
        "    Args:",
        "        query_text: Natural language query string",
        "",
        "    Returns:",
        "        ParsedIntent with action, subject, intent type, and expanded terms",
        "",
        "    Example:",
        "        >>> parse_intent_query(\"where do we handle authentication?\")",
        "        {",
        "            'action': 'handle',",
        "            'subject': 'authentication',",
        "            'intent': 'location',",
        "            'question_word': 'where',",
        "            'expanded_terms': ['handle', 'authentication', 'auth', 'login', ...]",
        "        }",
        "    \"\"\"",
        "    # Normalize query",
        "    query_lower = query_text.lower().strip()",
        "    query_lower = re.sub(r'[?!.,;:]', '', query_lower)  # Remove punctuation",
        "    words = query_lower.split()",
        "",
        "    if not words:",
        "        return ParsedIntent(",
        "            action=None,",
        "            subject=None,",
        "            intent='search',",
        "            question_word=None,",
        "            expanded_terms=[]",
        "        )",
        "",
        "    # Detect question word and intent",
        "    question_word = None",
        "    intent = 'search'  # Default intent",
        "",
        "    for word in words:",
        "        if word in QUESTION_INTENTS:",
        "            question_word = word",
        "            intent = QUESTION_INTENTS[word]",
        "            break",
        "",
        "    # Remove common filler words for parsing",
        "    filler_words = {'do', 'we', 'i', 'you', 'the', 'a', 'an', 'is', 'are', 'was',",
        "                    'were', 'can', 'could', 'should', 'would', 'does', 'did',",
        "                    'have', 'has', 'had', 'be', 'been', 'being', 'will', 'to'}",
        "    content_words = [w for w in words if w not in filler_words and w not in QUESTION_INTENTS]",
        "",
        "    # Find action verb",
        "    action = None",
        "    for word in content_words:",
        "        if word in ACTION_VERBS:",
        "            action = word",
        "            break",
        "",
        "    # Find subject (first non-action content word, or last content word)",
        "    subject = None",
        "    for word in content_words:",
        "        if word != action:",
        "            subject = word",
        "            break",
        "    if not subject and content_words:",
        "        subject = content_words[-1]",
        "",
        "    # Build expanded terms list",
        "    expanded_terms = []",
        "",
        "    # Add action and its synonyms",
        "    if action:",
        "        expanded_terms.append(action)",
        "        action_synonyms = get_related_terms(action, max_terms=5)",
        "        expanded_terms.extend(action_synonyms)",
        "",
        "    # Add subject and its synonyms",
        "    if subject:",
        "        expanded_terms.append(subject)",
        "        subject_synonyms = get_related_terms(subject, max_terms=5)",
        "        expanded_terms.extend(subject_synonyms)",
        "",
        "    # Add remaining content words",
        "    for word in content_words:",
        "        if word not in expanded_terms:",
        "            expanded_terms.append(word)",
        "",
        "    # Remove duplicates while preserving order",
        "    seen = set()",
        "    unique_terms = []",
        "    for term in expanded_terms:",
        "        if term not in seen:",
        "            seen.add(term)",
        "            unique_terms.append(term)",
        "",
        "    return ParsedIntent(",
        "        action=action,",
        "        subject=subject,",
        "        intent=intent,",
        "        question_word=question_word,",
        "        expanded_terms=unique_terms",
        "    )",
        "",
        "",
        "def search_by_intent(",
        "    query_text: str,",
        "    layers: Dict[CorticalLayer, HierarchicalLayer],",
        "    tokenizer: Tokenizer,",
        "    top_n: int = 5",
        ") -> List[Tuple[str, float, ParsedIntent]]:",
        "    \"\"\"",
        "    Search the corpus using intent-based query understanding.",
        "",
        "    Parses the query to understand intent, expands terms using code concepts,",
        "    then searches with appropriate weighting based on intent type.",
        "",
        "    Args:",
        "        query_text: Natural language query string",
        "        layers: Dictionary of layers",
        "        tokenizer: Tokenizer instance",
        "        top_n: Number of results to return",
        "",
        "    Returns:",
        "        List of (doc_id, score, parsed_intent) tuples",
        "",
        "    Example:",
        "        >>> search_by_intent(\"how do we validate user input?\", layers, tokenizer)",
        "        [('validation.py', 0.85, {...}), ('forms.py', 0.72, {...}), ...]",
        "    \"\"\"",
        "    # Parse the query intent",
        "    parsed = parse_intent_query(query_text)",
        "",
        "    if not parsed['expanded_terms']:",
        "        return []",
        "",
        "    # Build weighted query from expanded terms",
        "    layer0 = layers[CorticalLayer.TOKENS]",
        "    layer3 = layers[CorticalLayer.DOCUMENTS]",
        "",
        "    # Score documents based on term matches",
        "    doc_scores: Dict[str, float] = defaultdict(float)",
        "",
        "    for i, term in enumerate(parsed['expanded_terms']):",
        "        # Earlier terms (action, subject) get higher weight",
        "        term_weight = 1.0 / (1 + i * 0.2)",
        "",
        "        col = layer0.get_minicolumn(term)",
        "        if col:",
        "            for doc_id in col.document_ids:",
        "                # Use TF-IDF if available",
        "                tfidf = col.tfidf_per_doc.get(doc_id, col.tfidf)",
        "                doc_scores[doc_id] += term_weight * tfidf",
        "",
        "    # Sort by score",
        "    sorted_docs = sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)",
        "",
        "    # Return top results with parsed intent",
        "    results = []",
        "    for doc_id, score in sorted_docs[:top_n]:",
        "        results.append((doc_id, score, parsed))",
        "",
        "    return results",
        "",
        ""
      ],
      "lines_removed": [
        "    "
      ],
      "context_before": [
        "                for concept in layer2.minicolumns.values():",
        "                    if col.id in concept.feedforward_sources:",
        "                        for member_id in concept.feedforward_sources:",
        "                            # Use O(1) ID lookup instead of linear search",
        "                            member = layer0.get_by_id(member_id)",
        "                            if member and member.content not in expanded:",
        "                                score = concept.pagerank * member.pagerank * 0.4",
        "                                candidate_expansions[member.content] = max(",
        "                                    candidate_expansions[member.content], score",
        "                                )"
      ],
      "context_after": [
        "    # Select top expansions",
        "    sorted_candidates = sorted(",
        "        candidate_expansions.items(),",
        "        key=lambda x: x[1],",
        "        reverse=True",
        "    )[:max_expansions]",
        "    ",
        "    for term, score in sorted_candidates:",
        "        expanded[term] = score",
        "    ",
        "    return expanded",
        "",
        "",
        "# Valid relation chain patterns for multi-hop inference",
        "# Key: (relation1, relation2) → validity score (0.0 = invalid, 1.0 = fully valid)",
        "VALID_RELATION_CHAINS = {",
        "    # Transitive hierarchies",
        "    ('IsA', 'IsA'): 1.0,           # dog IsA animal IsA living_thing",
        "    ('PartOf', 'PartOf'): 1.0,     # wheel PartOf car PartOf vehicle",
        "    ('IsA', 'HasProperty'): 0.9,   # dog IsA animal HasProperty alive",
        "    ('PartOf', 'HasProperty'): 0.8,  # wheel PartOf car HasProperty fast",
        "",
        "    # Association chains"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/query.py",
      "function": "def find_documents_for_query(",
      "start_line": 461,
      "lines_added": [
        "def fast_find_documents(",
        "    query_text: str,",
        "    layers: Dict[CorticalLayer, HierarchicalLayer],",
        "    tokenizer: Tokenizer,",
        "    top_n: int = 5,",
        "    candidate_multiplier: int = 3,",
        "    use_code_concepts: bool = True",
        ") -> List[Tuple[str, float]]:",
        "    \"\"\"",
        "    Fast document search using candidate filtering.",
        "",
        "    Optimizes search by:",
        "    1. Using set intersection to find candidate documents",
        "    2. Only scoring top candidates fully",
        "    3. Using code concept expansion for better recall",
        "",
        "    This is ~2-3x faster than full search on large corpora while",
        "    maintaining similar result quality.",
        "",
        "    Args:",
        "        query_text: Search query",
        "        layers: Dictionary of layers",
        "        tokenizer: Tokenizer instance",
        "        top_n: Number of results to return",
        "        candidate_multiplier: Multiplier for candidate set size",
        "        use_code_concepts: Whether to use code concept expansion",
        "",
        "    Returns:",
        "        List of (doc_id, score) tuples ranked by relevance",
        "    \"\"\"",
        "    layer0 = layers[CorticalLayer.TOKENS]",
        "",
        "    # Tokenize query",
        "    tokens = tokenizer.tokenize(query_text)",
        "    if not tokens:",
        "        return []",
        "",
        "    # Phase 1: Find candidate documents (fast set operations)",
        "    # Get documents containing ANY query term",
        "    candidate_docs: Dict[str, int] = defaultdict(int)  # doc_id -> match count",
        "",
        "    for token in tokens:",
        "        col = layer0.get_minicolumn(token)",
        "        if col:",
        "            for doc_id in col.document_ids:",
        "                candidate_docs[doc_id] += 1",
        "",
        "    # If no candidates, try code concept expansion for recall",
        "    if not candidate_docs and use_code_concepts:",
        "        for token in tokens:",
        "            related = get_related_terms(token, max_terms=3)",
        "            for related_term in related:",
        "                col = layer0.get_minicolumn(related_term)",
        "                if col:",
        "                    for doc_id in col.document_ids:",
        "                        candidate_docs[doc_id] += 0.5  # Lower weight for expansion",
        "",
        "    if not candidate_docs:",
        "        return []",
        "",
        "    # Rank candidates by match count first (fast pre-filter)",
        "    sorted_candidates = sorted(",
        "        candidate_docs.items(),",
        "        key=lambda x: x[1],",
        "        reverse=True",
        "    )",
        "",
        "    # Take top N * multiplier candidates for full scoring",
        "    max_candidates = top_n * candidate_multiplier",
        "    top_candidates = sorted_candidates[:max_candidates]",
        "",
        "    # Phase 2: Full scoring only on top candidates",
        "    doc_scores: Dict[str, float] = {}",
        "",
        "    for doc_id, match_count in top_candidates:",
        "        score = 0.0",
        "        for token in tokens:",
        "            col = layer0.get_minicolumn(token)",
        "            if col and doc_id in col.document_ids:",
        "                tfidf = col.tfidf_per_doc.get(doc_id, col.tfidf)",
        "                score += tfidf",
        "",
        "        # Boost by match coverage",
        "        coverage_boost = match_count / len(tokens)",
        "        doc_scores[doc_id] = score * (1 + 0.5 * coverage_boost)",
        "",
        "    # Return top results",
        "    sorted_docs = sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)",
        "    return sorted_docs[:top_n]",
        "",
        "",
        "def build_document_index(",
        "    layers: Dict[CorticalLayer, HierarchicalLayer]",
        ") -> Dict[str, Dict[str, float]]:",
        "    \"\"\"",
        "    Build an optimized inverted index for fast querying.",
        "",
        "    Creates a term -> {doc_id: score} mapping that can be used",
        "    for fast set operations during search.",
        "",
        "    Args:",
        "        layers: Dictionary of layers",
        "",
        "    Returns:",
        "        Dict mapping terms to {doc_id: tfidf_score} dicts",
        "    \"\"\"",
        "    layer0 = layers.get(CorticalLayer.TOKENS)",
        "    if not layer0:",
        "        return {}",
        "",
        "    index: Dict[str, Dict[str, float]] = {}",
        "",
        "    for col in layer0.minicolumns.values():",
        "        term = col.content",
        "        term_index: Dict[str, float] = {}",
        "",
        "        for doc_id in col.document_ids:",
        "            tfidf = col.tfidf_per_doc.get(doc_id, col.tfidf)",
        "            term_index[doc_id] = tfidf",
        "",
        "        if term_index:",
        "            index[term] = term_index",
        "",
        "    return index",
        "",
        "",
        "def search_with_index(",
        "    query_text: str,",
        "    index: Dict[str, Dict[str, float]],",
        "    tokenizer: Tokenizer,",
        "    top_n: int = 5",
        ") -> List[Tuple[str, float]]:",
        "    \"\"\"",
        "    Search using a pre-built inverted index.",
        "",
        "    This is the fastest search method when the index is cached.",
        "",
        "    Args:",
        "        query_text: Search query",
        "        index: Pre-built index from build_document_index()",
        "        tokenizer: Tokenizer instance",
        "        top_n: Number of results to return",
        "",
        "    Returns:",
        "        List of (doc_id, score) tuples ranked by relevance",
        "    \"\"\"",
        "    tokens = tokenizer.tokenize(query_text)",
        "    if not tokens:",
        "        return []",
        "",
        "    doc_scores: Dict[str, float] = defaultdict(float)",
        "",
        "    for token in tokens:",
        "        if token in index:",
        "            for doc_id, score in index[token].items():",
        "                doc_scores[doc_id] += score",
        "",
        "    sorted_docs = sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)",
        "    return sorted_docs[:top_n]",
        "",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "        col = layer0.get_minicolumn(term)",
        "        if col:",
        "            for doc_id in col.document_ids:",
        "                tfidf = col.tfidf_per_doc.get(doc_id, col.tfidf)",
        "                doc_scores[doc_id] += tfidf * term_weight",
        "",
        "    sorted_docs = sorted(doc_scores.items(), key=lambda x: -x[1])",
        "    return sorted_docs[:top_n]",
        "",
        ""
      ],
      "context_after": [
        "def query_with_spreading_activation(",
        "    query_text: str,",
        "    layers: Dict[CorticalLayer, HierarchicalLayer],",
        "    tokenizer: Tokenizer,",
        "    top_n: int = 10,",
        "    max_expansions: int = 8",
        ") -> List[Tuple[str, float]]:",
        "    \"\"\"",
        "    Query with automatic expansion using spreading activation.",
        "    "
      ],
      "change_type": "add"
    },
    {
      "file": "cortical/query.py",
      "function": "def create_chunks(",
      "start_line": 561,
      "lines_added": [
        "",
        "    Raises:",
        "        ValueError: If chunk_size <= 0 or overlap < 0 or overlap >= chunk_size",
        "    if chunk_size <= 0:",
        "        raise ValueError(f\"chunk_size must be positive, got {chunk_size}\")",
        "    if overlap < 0:",
        "        raise ValueError(f\"overlap must be non-negative, got {overlap}\")",
        "    if overlap >= chunk_size:",
        "        raise ValueError(f\"overlap must be less than chunk_size, got overlap={overlap}, chunk_size={chunk_size}\")",
        "",
        "def precompute_term_cols(",
        "    query_terms: Dict[str, float],",
        "    layer0: HierarchicalLayer",
        ") -> Dict[str, 'Minicolumn']:",
        "    \"\"\"",
        "    Pre-compute minicolumn lookups for query terms.",
        "",
        "    This avoids repeated O(1) dictionary lookups for each chunk,",
        "    enabling faster scoring when processing many chunks.",
        "",
        "    Args:",
        "        query_terms: Dict mapping query terms to weights",
        "        layer0: Token layer for lookups",
        "",
        "    Returns:",
        "        Dict mapping term to Minicolumn (only for terms that exist in corpus)",
        "    \"\"\"",
        "    term_cols = {}",
        "    for term in query_terms:",
        "        col = layer0.get_minicolumn(term)",
        "        if col:",
        "            term_cols[term] = col",
        "    return term_cols",
        "",
        "",
        "def score_chunk_fast(",
        "    chunk_tokens: List[str],",
        "    query_terms: Dict[str, float],",
        "    term_cols: Dict[str, 'Minicolumn'],",
        "    doc_id: Optional[str] = None",
        ") -> float:",
        "    \"\"\"",
        "    Fast chunk scoring using pre-computed minicolumn lookups.",
        "",
        "    This is an optimized version of score_chunk that accepts pre-tokenized",
        "    text and pre-computed minicolumn lookups. Use when scoring many chunks",
        "    from the same document.",
        "",
        "    Args:",
        "        chunk_tokens: Pre-tokenized chunk tokens",
        "        query_terms: Dict mapping query terms to weights",
        "        term_cols: Pre-computed term->Minicolumn mapping from precompute_term_cols()",
        "        doc_id: Optional document ID for per-document TF-IDF",
        "",
        "    Returns:",
        "        Relevance score for the chunk",
        "    \"\"\"",
        "    if not chunk_tokens:",
        "        return 0.0",
        "",
        "    # Count token occurrences in chunk",
        "    token_counts: Dict[str, int] = {}",
        "    for token in chunk_tokens:",
        "        token_counts[token] = token_counts.get(token, 0) + 1",
        "",
        "    score = 0.0",
        "    for term, term_weight in query_terms.items():",
        "        if term in token_counts and term in term_cols:",
        "            col = term_cols[term]",
        "            # Use per-document TF-IDF if available, otherwise global",
        "            tfidf = col.tfidf_per_doc.get(doc_id, col.tfidf) if doc_id else col.tfidf",
        "            # Weight by occurrence in chunk and query weight",
        "            score += tfidf * token_counts[term] * term_weight",
        "",
        "    # Normalize by chunk length to avoid bias toward longer chunks",
        "    return score / len(chunk_tokens)",
        "",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "    \"\"\"",
        "    Split text into overlapping chunks.",
        "",
        "    Args:",
        "        text: Document text to chunk",
        "        chunk_size: Target size of each chunk in characters",
        "        overlap: Number of overlapping characters between chunks",
        "",
        "    Returns:",
        "        List of (chunk_text, start_char, end_char) tuples"
      ],
      "context_after": [
        "    \"\"\"",
        "    if not text:",
        "        return []",
        "",
        "    chunks = []",
        "    stride = max(1, chunk_size - overlap)",
        "    text_len = len(text)",
        "",
        "    for start in range(0, text_len, stride):",
        "        end = min(start + chunk_size, text_len)",
        "        chunk = text[start:end]",
        "        chunks.append((chunk, start, end))",
        "",
        "        if end >= text_len:",
        "            break",
        "",
        "    return chunks",
        "",
        "",
        "def score_chunk(",
        "    chunk_text: str,",
        "    query_terms: Dict[str, float],",
        "    layer0: HierarchicalLayer,",
        "    tokenizer: Tokenizer,",
        "    doc_id: Optional[str] = None",
        ") -> float:",
        "    \"\"\"",
        "    Score a chunk against query terms using TF-IDF.",
        ""
      ],
      "change_type": "add"
    },
    {
      "file": "cortical/query.py",
      "function": "def find_passages_for_query(",
      "start_line": 672,
      "lines_added": [
        "    # Pre-compute minicolumn lookups for query terms (optimization)",
        "    term_cols = precompute_term_cols(query_terms, layer0)",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "    query_terms = get_expanded_query_terms(",
        "        query_text, layers, tokenizer,",
        "        use_expansion=use_expansion,",
        "        semantic_relations=semantic_relations,",
        "        use_semantic=use_semantic",
        "    )",
        "",
        "    if not query_terms:",
        "        return []",
        ""
      ],
      "context_after": [
        "    # First, get candidate documents (more than we need, since we'll rank passages)",
        "    doc_scores = find_documents_for_query(",
        "        query_text, layers, tokenizer,",
        "        top_n=min(len(documents), top_n * 3),",
        "        use_expansion=use_expansion,",
        "        semantic_relations=semantic_relations,",
        "        use_semantic=use_semantic",
        "    )",
        "",
        "    # Apply document filter if provided"
      ],
      "change_type": "add"
    },
    {
      "file": "cortical/query.py",
      "function": "def find_passages_for_query(",
      "start_line": 696,
      "lines_added": [
        "            # Use fast scoring with pre-computed lookups",
        "            chunk_tokens = tokenizer.tokenize(chunk_text)",
        "            chunk_score = score_chunk_fast(",
        "                chunk_tokens, query_terms, term_cols, doc_id"
      ],
      "lines_removed": [
        "            chunk_score = score_chunk(",
        "                chunk_text, query_terms, layer0, tokenizer, doc_id"
      ],
      "context_before": [
        "    passages: List[Tuple[str, str, int, int, float]] = []",
        "",
        "    for doc_id, doc_score in doc_scores:",
        "        if doc_id not in documents:",
        "            continue",
        "",
        "        text = documents[doc_id]",
        "        chunks = create_chunks(text, chunk_size, overlap)",
        "",
        "        for chunk_text, start_char, end_char in chunks:"
      ],
      "context_after": [
        "            )",
        "            # Combine chunk score with document score for final ranking",
        "            combined_score = chunk_score * (1 + doc_score * 0.1)",
        "",
        "            passages.append((",
        "                chunk_text,",
        "                doc_id,",
        "                start_char,",
        "                end_char,",
        "                combined_score"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/query.py",
      "function": "def find_passages_batch(",
      "start_line": 856,
      "lines_added": [
        "        # Pre-compute minicolumn lookups for query terms (optimization)",
        "        term_cols = precompute_term_cols(query_terms, layer0)",
        "",
        "        # Score passages using cached chunks and fast scoring",
        "                # Use fast scoring with pre-computed lookups",
        "                chunk_tokens = tokenizer.tokenize(chunk_text)",
        "                chunk_score = score_chunk_fast(",
        "                    chunk_tokens, query_terms, term_cols, doc_id"
      ],
      "lines_removed": [
        "        # Score passages using cached chunks",
        "                chunk_score = score_chunk(",
        "                    chunk_text, query_terms, layer0, tokenizer, doc_id"
      ],
      "context_before": [
        "                use_expansion=use_expansion,",
        "                semantic_relations=semantic_relations,",
        "                use_semantic=use_semantic",
        "            )",
        "            expansion_cache[query_text] = query_terms",
        "",
        "        if not query_terms:",
        "            all_results.append([])",
        "            continue",
        ""
      ],
      "context_after": [
        "        # Get candidate documents",
        "        doc_scores = find_documents_for_query(",
        "            query_text, layers, tokenizer,",
        "            top_n=min(len(documents), top_n * 3),",
        "            use_expansion=use_expansion,",
        "            semantic_relations=semantic_relations,",
        "            use_semantic=use_semantic",
        "        )",
        "",
        "        # Apply document filter",
        "        if doc_filter:",
        "            doc_scores = [(doc_id, score) for doc_id, score in doc_scores if doc_id in doc_filter]",
        "",
        "        passages: List[Tuple[str, str, int, int, float]] = []",
        "",
        "        for doc_id, doc_score in doc_scores:",
        "            if doc_id not in doc_chunks_cache:",
        "                continue",
        "",
        "            for chunk_text, start_char, end_char in doc_chunks_cache[doc_id]:",
        "                )",
        "                combined_score = chunk_score * (1 + doc_score * 0.1)",
        "                passages.append((chunk_text, doc_id, start_char, end_char, combined_score))",
        "",
        "        passages.sort(key=lambda x: x[4], reverse=True)",
        "        all_results.append(passages[:top_n])",
        "",
        "    return all_results",
        "",
        ""
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/query.py",
      "function": "def complete_analogy_simple(",
      "start_line": 1431,
      "lines_added": [
        "        # Find bigrams containing \"a b\" pattern (bigrams use space separators)",
        "        ab_bigram = f\"{term_a} {term_b}\"",
        "        ba_bigram = f\"{term_b} {term_a}\"",
        "        # If \"a b\" is a bigram, look for \"c ?\" bigrams",
        "                parts = bigram.split(' ')"
      ],
      "lines_removed": [
        "        # Find bigrams containing a_b pattern",
        "        ab_bigram = f\"{term_a}_{term_b}\"",
        "        ba_bigram = f\"{term_b}_{term_a}\"",
        "        # If a_b is a bigram, look for c_? bigrams",
        "                parts = bigram.split('_')"
      ],
      "context_before": [
        "",
        "    col_a = layer0.get_minicolumn(term_a)",
        "    col_b = layer0.get_minicolumn(term_b)",
        "    col_c = layer0.get_minicolumn(term_c)",
        "",
        "    if not col_a or not col_b or not col_c:",
        "        return []",
        "",
        "    # Strategy 1: Bigram pattern matching",
        "    if layer1:"
      ],
      "context_after": [
        "",
        "        ab_col = layer1.get_minicolumn(ab_bigram)",
        "        ba_col = layer1.get_minicolumn(ba_bigram)",
        "",
        "        if ab_col or ba_col:",
        "            for bigram_col in layer1.minicolumns.values():",
        "                bigram = bigram_col.content",
        "                if len(parts) != 2:",
        "                    continue",
        "",
        "                first, second = parts",
        "",
        "                # Look for bigrams starting with c",
        "                if first == term_c and second not in {term_a, term_b, term_c}:",
        "                    score = bigram_col.pagerank * 0.8",
        "                    if second not in candidates or candidates[second] < score:",
        "                        candidates[second] = score"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/semantics.py",
      "function": "Semantics Module",
      "start_line": 5,
      "lines_added": [
        "import copy"
      ],
      "lines_removed": [],
      "context_before": [
        "Corpus-derived semantic relations and retrofitting.",
        "",
        "Extracts semantic relationships from co-occurrence patterns,",
        "then uses them to adjust connection weights (retrofitting).",
        "This is like building a \"poor man's ConceptNet\" from the corpus itself.",
        "\"\"\"",
        "",
        "import math",
        "import re",
        "from typing import Any, Dict, List, Tuple, Set, Optional"
      ],
      "context_after": [
        "from collections import defaultdict",
        "",
        "try:",
        "    import numpy as np",
        "    HAS_NUMPY = True",
        "except ImportError:",
        "    HAS_NUMPY = False",
        "",
        "from .layers import CorticalLayer, HierarchicalLayer",
        "from .minicolumn import Minicolumn"
      ],
      "change_type": "add"
    },
    {
      "file": "cortical/semantics.py",
      "function": "def retrofit_connections(",
      "start_line": 385,
      "lines_added": [
        "",
        "",
        "    Raises:",
        "        ValueError: If alpha is not in range [0, 1]",
        "    if not (0 <= alpha <= 1):",
        "        raise ValueError(f\"alpha must be between 0 and 1, got {alpha}\")",
        "",
        ""
      ],
      "lines_removed": [
        "        ",
        "    "
      ],
      "context_before": [
        "    ",
        "    Adjusts connection weights by blending co-occurrence patterns",
        "    with semantic relations. This is inspired by Faruqui et al.'s",
        "    retrofitting algorithm for word vectors.",
        "    ",
        "    Args:",
        "        layers: Dictionary of layers",
        "        semantic_relations: List of (term1, relation, term2, weight) tuples",
        "        iterations: Number of retrofitting iterations",
        "        alpha: Blend factor (0=all semantic, 1=all original)"
      ],
      "context_after": [
        "    Returns:",
        "        Dictionary with retrofitting statistics",
        "    \"\"\"",
        "    layer0 = layers[CorticalLayer.TOKENS]",
        "    # Store original weights",
        "    original_weights: Dict[str, Dict[str, float]] = {}",
        "    for col in layer0.minicolumns.values():",
        "        original_weights[col.content] = dict(col.lateral_connections)",
        "    ",
        "    # Build semantic neighbor lookup",
        "    semantic_neighbors: Dict[str, List[Tuple[str, float]]] = defaultdict(list)",
        "    ",
        "    for t1, relation, t2, weight in semantic_relations:",
        "        relation_weight = RELATION_WEIGHTS.get(relation, 0.5)"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/semantics.py",
      "function": "def retrofit_embeddings(",
      "start_line": 479,
      "lines_added": [
        "",
        "",
        "    Raises:",
        "        ValueError: If alpha is not in range (0, 1]",
        "    if not (0 < alpha <= 1):",
        "        raise ValueError(f\"alpha must be between 0 and 1 (exclusive of 0), got {alpha}\")",
        ""
      ],
      "lines_removed": [
        "        ",
        "    import copy",
        "    "
      ],
      "context_before": [
        "    Retrofit embeddings using semantic relations.",
        "    ",
        "    Like Faruqui et al.'s retrofitting, but for graph embeddings.",
        "    Pulls semantically related terms closer in embedding space.",
        "    ",
        "    Args:",
        "        embeddings: Dictionary mapping terms to embedding vectors",
        "        semantic_relations: List of (term1, relation, term2, weight) tuples",
        "        iterations: Number of iterations",
        "        alpha: Blend factor (higher = more original embedding)"
      ],
      "context_after": [
        "    Returns:",
        "        Dictionary with retrofitting statistics",
        "    \"\"\"",
        "    # Store original embeddings",
        "    original = copy.deepcopy(embeddings)",
        "    ",
        "    # Build neighbor lookup",
        "    neighbors: Dict[str, List[Tuple[str, float]]] = defaultdict(list)",
        "    ",
        "    for t1, relation, t2, weight in semantic_relations:",
        "        if t1 in embeddings and t2 in embeddings:",
        "            relation_weight = RELATION_WEIGHTS.get(relation, 0.5)",
        "            combined = weight * relation_weight"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/tokenizer.py",
      "function": "Tokenizer Module",
      "start_line": 3,
      "lines_added": [
        "from typing import List, Set, Optional, Dict, Tuple",
        "",
        "",
        "# Programming keywords that should be preserved even if in stop words",
        "PROGRAMMING_KEYWORDS = frozenset({",
        "    'def', 'class', 'function', 'return', 'import', 'from', 'if', 'else',",
        "    'elif', 'for', 'while', 'try', 'except', 'finally', 'with', 'as',",
        "    'yield', 'async', 'await', 'lambda', 'pass', 'break', 'continue',",
        "    'raise', 'assert', 'global', 'nonlocal', 'del', 'true', 'false',",
        "    'none', 'null', 'void', 'int', 'str', 'float', 'bool', 'list',",
        "    'dict', 'set', 'tuple', 'self', 'cls', 'init', 'main', 'args',",
        "    'kwargs', 'super', 'property', 'staticmethod', 'classmethod',",
        "    'isinstance', 'hasattr', 'getattr', 'setattr', 'len', 'range',",
        "    'enumerate', 'zip', 'map', 'filter', 'print', 'open', 'read',",
        "    'write', 'close', 'append', 'extend', 'insert', 'remove', 'pop',",
        "    'const', 'let', 'var', 'public', 'private', 'protected', 'static',",
        "    'final', 'abstract', 'interface', 'implements', 'extends', 'new',",
        "    'this', 'constructor', 'module', 'export', 'require', 'package',",
        "    # Common identifier components that shouldn't be filtered",
        "    'get', 'set', 'add', 'put', 'has', 'can', 'run', 'max', 'min',",
        "})",
        "",
        "",
        "def split_identifier(identifier: str) -> List[str]:",
        "    \"\"\"",
        "    Split a code identifier into component words.",
        "",
        "    Handles camelCase, PascalCase, underscore_style, and CONSTANT_STYLE.",
        "",
        "    Args:",
        "        identifier: A code identifier like \"getUserCredentials\" or \"get_user_data\"",
        "",
        "    Returns:",
        "        List of component words in lowercase",
        "",
        "    Examples:",
        "        >>> split_identifier(\"getUserCredentials\")",
        "        ['get', 'user', 'credentials']",
        "        >>> split_identifier(\"get_user_data\")",
        "        ['get', 'user', 'data']",
        "        >>> split_identifier(\"XMLParser\")",
        "        ['xml', 'parser']",
        "        >>> split_identifier(\"parseHTTPResponse\")",
        "        ['parse', 'http', 'response']",
        "    \"\"\"",
        "    if not identifier:",
        "        return []",
        "",
        "    # Handle underscore_style and CONSTANT_STYLE",
        "    if '_' in identifier:",
        "        parts = [p for p in identifier.split('_') if p]",
        "        # Recursively split any camelCase parts",
        "        result = []",
        "        for part in parts:",
        "            if any(c.isupper() for c in part):  # Has any capitals - could be camelCase",
        "                result.extend(split_identifier(part))",
        "            else:",
        "                result.append(part.lower())",
        "        return [p for p in result if p]",
        "",
        "    # Handle camelCase and PascalCase",
        "    # Insert space before uppercase letters, handling acronyms",
        "    # \"parseHTTPResponse\" -> \"parse HTTP Response\" -> [\"parse\", \"http\", \"response\"]",
        "    result = []",
        "    current = []",
        "",
        "    for i, char in enumerate(identifier):",
        "        if char.isupper():",
        "            # Check if this starts a new word",
        "            if current:",
        "                # If previous was lowercase, this starts a new word",
        "                if current[-1].islower():",
        "                    result.append(''.join(current).lower())",
        "                    current = [char]",
        "                # If next char is lowercase, this uppercase starts a new word (end of acronym)",
        "                elif i + 1 < len(identifier) and identifier[i + 1].islower():",
        "                    result.append(''.join(current).lower())",
        "                    current = [char]",
        "                else:",
        "                    # Continue building acronym",
        "                    current.append(char)",
        "            else:",
        "                current.append(char)",
        "        else:",
        "            current.append(char)",
        "",
        "    if current:",
        "        result.append(''.join(current).lower())",
        "",
        "    return [p for p in result if p]"
      ],
      "lines_removed": [
        "from typing import List, Set, Optional, Dict"
      ],
      "context_before": [
        "================",
        "",
        "Text tokenization with stemming and word variant support.",
        "",
        "Like early visual processing, the tokenizer extracts basic features",
        "(words) from raw input, filtering noise (stop words) and normalizing",
        "representations (lowercase, stemming).",
        "\"\"\"",
        "",
        "import re"
      ],
      "context_after": [
        "",
        "",
        "class Tokenizer:",
        "    \"\"\"",
        "    Text tokenizer with stemming and word variant support.",
        "    ",
        "    Extracts tokens from text, filters stop words, and provides",
        "    word variant expansion for query normalization.",
        "    ",
        "    Attributes:"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/tokenizer.py",
      "function": "class Tokenizer:",
      "start_line": 90,
      "lines_added": [
        "    def __init__(",
        "        self,",
        "        stop_words: Optional[Set[str]] = None,",
        "        min_word_length: int = 3,",
        "        split_identifiers: bool = False",
        "    ):",
        "",
        "            split_identifiers: If True, split camelCase/underscore_style and include",
        "                               both original and component tokens.",
        "        self.split_identifiers = split_identifiers"
      ],
      "lines_removed": [
        "    def __init__(self, stop_words: Optional[Set[str]] = None, min_word_length: int = 3):",
        "        "
      ],
      "context_before": [
        "        # Common nouns (too generic)",
        "        'way', 'ways', 'thing', 'things', 'time', 'times', 'year', 'years',",
        "        'day', 'days', 'place', 'part', 'parts', 'case', 'cases', 'point',",
        "        'fact', 'kind', 'type', 'form', 'forms', 'level', 'area', 'areas',",
        "        # Common adjectives (too generic)",
        "        'new', 'old', 'good', 'bad', 'great', 'small', 'large', 'big', 'long',",
        "        'high', 'low', 'right', 'left', 'possible', 'important', 'major',",
        "        'available', 'able', 'like', 'different', 'similar'",
        "    })",
        "    "
      ],
      "context_after": [
        "        \"\"\"",
        "        Initialize tokenizer.",
        "        Args:",
        "            stop_words: Set of words to filter out. Uses defaults if None.",
        "            min_word_length: Minimum word length to keep.",
        "        \"\"\"",
        "        self.stop_words = stop_words if stop_words is not None else self.DEFAULT_STOP_WORDS",
        "        self.min_word_length = min_word_length",
        "        ",
        "        # Simple suffix rules for stemming (Porter-lite)",
        "        self._suffix_rules = [",
        "            ('ational', 'ate'), ('tional', 'tion'), ('enci', 'ence'),",
        "            ('anci', 'ance'), ('izer', 'ize'), ('isation', 'ize'),",
        "            ('ization', 'ize'), ('ation', 'ate'), ('ator', 'ate'),",
        "            ('alism', 'al'), ('iveness', 'ive'), ('fulness', 'ful'),",
        "            ('ousness', 'ous'), ('aliti', 'al'), ('iviti', 'ive'),",
        "            ('biliti', 'ble'), ('ement', ''), ('ment', ''), ('ness', ''),",
        "            ('ling', ''), ('ing', ''), ('ies', 'y'), ('ied', 'y'),"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/tokenizer.py",
      "function": "class Tokenizer:",
      "start_line": 137,
      "lines_added": [
        "    def tokenize(self, text: str, split_identifiers: Optional[bool] = None) -> List[str]:",
        "",
        "            split_identifiers: Override instance setting. If True, split",
        "                              camelCase/underscore_style identifiers into components.",
        "",
        "",
        "        Examples:",
        "            >>> t = Tokenizer(split_identifiers=True)",
        "            >>> t.tokenize(\"getUserCredentials fetches data\")",
        "            ['getusercredentials', 'get', 'user', 'credentials', 'fetches', 'data']",
        "        should_split = split_identifiers if split_identifiers is not None else self.split_identifiers",
        "",
        "        # Extract potential identifiers (including camelCase with internal caps)",
        "        # Pattern matches: word2vec, getUserData, get_user_data, XMLParser",
        "        raw_tokens = re.findall(r'\\b[a-zA-Z][a-zA-Z0-9_]*\\b', text)",
        "",
        "        result = []",
        "        seen_splits = set()  # Only track splits to avoid duplicates from them",
        "",
        "        for token in raw_tokens:",
        "            token_lower = token.lower()",
        "",
        "            # Skip stop words and short words",
        "            if token_lower in self.stop_words or len(token_lower) < self.min_word_length:",
        "                continue",
        "",
        "            # Add the original token (allow duplicates for proper bigram extraction)",
        "            result.append(token_lower)",
        "            # Track this token to prevent splits from duplicating it",
        "            seen_splits.add(token_lower)",
        "",
        "            # Split identifier if enabled and token looks like an identifier",
        "            if should_split and (",
        "                '_' in token or",
        "                any(c.isupper() for c in token[1:])  # Has internal capitals",
        "            ):",
        "                parts = split_identifier(token)",
        "                for part in parts:",
        "                    # Allow programming keywords even if in stop words",
        "                    is_programming_keyword = part in PROGRAMMING_KEYWORDS",
        "                    # Only add split parts once per token to avoid bloating",
        "                    if (",
        "                        part not in seen_splits and",
        "                        part != token_lower and  # Don't duplicate the original",
        "                        (is_programming_keyword or part not in self.stop_words) and",
        "                        len(part) >= self.min_word_length",
        "                    ):",
        "                        result.append(part)",
        "                        seen_splits.add(part)",
        "",
        "        return result"
      ],
      "lines_removed": [
        "    def tokenize(self, text: str) -> List[str]:",
        "        ",
        "            ",
        "        # Convert to lowercase and extract words (including alphanumeric like word2vec)",
        "        words = re.findall(r'\\b[a-z][a-z0-9]*\\b', text.lower())",
        "        ",
        "        # Filter stop words and short words",
        "        return [",
        "            w for w in words ",
        "            if w not in self.stop_words and len(w) >= self.min_word_length",
        "        ]"
      ],
      "context_before": [
        "            # Common abbreviations",
        "            'nlp': ['natural', 'language', 'processing', 'text'],",
        "            'api': ['interface', 'endpoint', 'service'],",
        "            # Synonyms",
        "            'fast': ['quick', 'rapid', 'speed'],",
        "            'slow': ['latency', 'delay'],",
        "            'big': ['large', 'scale', 'massive'],",
        "            'small': ['tiny', 'minimal', 'compact'],",
        "        }",
        "    "
      ],
      "context_after": [
        "        \"\"\"",
        "        Extract tokens from text.",
        "        Args:",
        "            text: Input text to tokenize.",
        "        Returns:",
        "            List of filtered, lowercase tokens.",
        "        \"\"\"",
        "    ",
        "    def extract_ngrams(self, tokens: List[str], n: int = 2) -> List[str]:",
        "        \"\"\"",
        "        Extract n-grams from token list.",
        "        ",
        "        Args:",
        "            tokens: List of tokens.",
        "            n: Size of n-grams to extract.",
        "            ",
        "        Returns:"
      ],
      "change_type": "modify"
    },
    {
      "file": "scripts/index_codebase.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "#!/usr/bin/env python3",
        "\"\"\"",
        "Index the Cortical Text Processor codebase for dog-fooding.",
        "",
        "This script indexes all Python files and documentation to enable",
        "semantic search over the codebase using the Cortical Text Processor itself.",
        "",
        "Usage:",
        "    python scripts/index_codebase.py [--output corpus_dev.pkl]",
        "\"\"\"",
        "",
        "import argparse",
        "import os",
        "import sys",
        "from pathlib import Path",
        "",
        "# Add parent directory to path for imports",
        "sys.path.insert(0, str(Path(__file__).parent.parent))",
        "",
        "from cortical.processor import CorticalTextProcessor",
        "",
        "",
        "def get_python_files(base_path: Path) -> list:",
        "    \"\"\"Get all Python files in cortical/ and tests/ directories.\"\"\"",
        "    files = []",
        "    for directory in ['cortical', 'tests']:",
        "        dir_path = base_path / directory",
        "        if dir_path.exists():",
        "            for py_file in dir_path.rglob('*.py'):",
        "                if not py_file.name.startswith('__'):",
        "                    files.append(py_file)",
        "    return sorted(files)",
        "",
        "",
        "def get_doc_files(base_path: Path) -> list:",
        "    \"\"\"Get documentation files.\"\"\"",
        "    doc_files = ['CLAUDE.md', 'TASK_LIST.md', 'README.md', 'KNOWLEDGE_TRANSFER.md']",
        "    files = []",
        "    for doc in doc_files:",
        "        doc_path = base_path / doc",
        "        if doc_path.exists():",
        "            files.append(doc_path)",
        "    return files",
        "",
        "",
        "def create_doc_id(file_path: Path, base_path: Path) -> str:",
        "    \"\"\"Create a document ID from file path.\"\"\"",
        "    rel_path = file_path.relative_to(base_path)",
        "    return str(rel_path)",
        "",
        "",
        "def index_file(processor: CorticalTextProcessor, file_path: Path, base_path: Path) -> dict:",
        "    \"\"\"Index a single file with line number metadata.\"\"\"",
        "    doc_id = create_doc_id(file_path, base_path)",
        "",
        "    try:",
        "        content = file_path.read_text(encoding='utf-8')",
        "    except Exception as e:",
        "        print(f\"  Warning: Could not read {doc_id}: {e}\")",
        "        return None",
        "",
        "    # Create metadata with file info",
        "    metadata = {",
        "        'file_path': str(file_path),",
        "        'relative_path': doc_id,",
        "        'file_type': file_path.suffix,",
        "        'line_count': content.count('\\n') + 1,",
        "    }",
        "",
        "    # For Python files, extract additional metadata",
        "    if file_path.suffix == '.py':",
        "        metadata['language'] = 'python'",
        "        # Count functions and classes",
        "        metadata['function_count'] = content.count('\\ndef ')",
        "        metadata['class_count'] = content.count('\\nclass ')",
        "",
        "    processor.process_document(doc_id, content, metadata=metadata)",
        "    return metadata",
        "",
        "",
        "def main():",
        "    parser = argparse.ArgumentParser(description='Index the codebase for semantic search')",
        "    parser.add_argument('--output', '-o', default='corpus_dev.pkl',",
        "                        help='Output file path (default: corpus_dev.pkl)')",
        "    parser.add_argument('--verbose', '-v', action='store_true',",
        "                        help='Show verbose output')",
        "    args = parser.parse_args()",
        "",
        "    base_path = Path(__file__).parent.parent",
        "    output_path = base_path / args.output",
        "",
        "    print(\"Cortical Text Processor - Codebase Indexer\")",
        "    print(\"=\" * 50)",
        "",
        "    # Initialize processor",
        "    processor = CorticalTextProcessor()",
        "",
        "    # Get files to index",
        "    python_files = get_python_files(base_path)",
        "    doc_files = get_doc_files(base_path)",
        "    all_files = python_files + doc_files",
        "",
        "    print(f\"\\nFound {len(python_files)} Python files and {len(doc_files)} documentation files\")",
        "",
        "    # Index all files",
        "    print(\"\\nIndexing files...\")",
        "    indexed = 0",
        "    total_lines = 0",
        "",
        "    for file_path in all_files:",
        "        if args.verbose:",
        "            print(f\"  Indexing: {create_doc_id(file_path, base_path)}\")",
        "",
        "        metadata = index_file(processor, file_path, base_path)",
        "        if metadata:",
        "            indexed += 1",
        "            total_lines += metadata.get('line_count', 0)",
        "",
        "    print(f\"  Indexed {indexed} files ({total_lines:,} total lines)\")",
        "",
        "    # Compute all analysis",
        "    print(\"\\nComputing analysis...\")",
        "    processor.compute_all(",
        "        build_concepts=True,",
        "        pagerank_method='semantic',",
        "        connection_strategy='hybrid',",
        "        verbose=args.verbose",
        "    )",
        "",
        "    # Extract semantic relations",
        "    print(\"Extracting semantic relations...\")",
        "    processor.extract_corpus_semantics(",
        "        use_pattern_extraction=True,",
        "        verbose=args.verbose",
        "    )",
        "",
        "    # Print statistics",
        "    print(\"\\nCorpus Statistics:\")",
        "    print(f\"  Documents: {len(processor.documents)}\")",
        "    print(f\"  Tokens (Layer 0): {processor.layers[0].column_count()}\")",
        "    print(f\"  Bigrams (Layer 1): {processor.layers[1].column_count()}\")",
        "    print(f\"  Concepts (Layer 2): {processor.layers[2].column_count()}\")",
        "    print(f\"  Semantic relations: {len(processor.semantic_relations)}\")",
        "",
        "    # Save corpus",
        "    print(f\"\\nSaving corpus to {output_path}...\")",
        "    processor.save(str(output_path))",
        "",
        "    file_size = output_path.stat().st_size / 1024",
        "    print(f\"  Saved ({file_size:.1f} KB)\")",
        "",
        "    print(\"\\nDone! Use search_codebase.py to query the indexed corpus.\")",
        "",
        "",
        "if __name__ == '__main__':",
        "    main()"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "scripts/search_codebase.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "#!/usr/bin/env python3",
        "\"\"\"",
        "Search the indexed codebase using Cortical Text Processor.",
        "",
        "This script provides semantic search over the codebase with file:line references.",
        "",
        "Usage:",
        "    python scripts/search_codebase.py \"how does PageRank work\"",
        "    python scripts/search_codebase.py \"bigram separator\" --top 10",
        "    python scripts/search_codebase.py --interactive",
        "\"\"\"",
        "",
        "import argparse",
        "import os",
        "import sys",
        "from pathlib import Path",
        "",
        "# Add parent directory to path for imports",
        "sys.path.insert(0, str(Path(__file__).parent.parent))",
        "",
        "from cortical.processor import CorticalTextProcessor",
        "",
        "",
        "def find_line_number(doc_content: str, passage_start: int) -> int:",
        "    \"\"\"Find the line number for a character position.\"\"\"",
        "    return doc_content[:passage_start].count('\\n') + 1",
        "",
        "",
        "def format_passage(passage: str, max_width: int = 80) -> str:",
        "    \"\"\"Format a passage for display, truncating long lines.\"\"\"",
        "    lines = passage.split('\\n')",
        "    formatted = []",
        "    for line in lines[:10]:  # Limit to 10 lines",
        "        if len(line) > max_width:",
        "            line = line[:max_width - 3] + '...'",
        "        formatted.append(line)",
        "    if len(lines) > 10:",
        "        formatted.append(f'  ... ({len(lines) - 10} more lines)')",
        "    return '\\n'.join(formatted)",
        "",
        "",
        "def search_codebase(processor: CorticalTextProcessor, query: str,",
        "                    top_n: int = 5, chunk_size: int = 400, fast: bool = False) -> list:",
        "    \"\"\"",
        "    Search the codebase and return results with file:line references.",
        "",
        "    Args:",
        "        processor: CorticalTextProcessor instance",
        "        query: Search query string",
        "        top_n: Number of results to return",
        "        chunk_size: Size of text chunks for passage extraction",
        "        fast: Use fast search mode (documents only, no passages)",
        "",
        "    Returns:",
        "        List of result dicts with 'file', 'line', 'passage', 'score', 'reference'",
        "    \"\"\"",
        "    if fast:",
        "        # Fast mode: just find documents, return first lines",
        "        doc_results = processor.fast_find_documents(query, top_n=top_n)",
        "        formatted_results = []",
        "        for doc_id, score in doc_results:",
        "            doc_content = processor.documents.get(doc_id, '')",
        "            # Get first 400 chars as passage",
        "            passage = doc_content[:400] if doc_content else ''",
        "            formatted_results.append({",
        "                'file': doc_id,",
        "                'line': 1,",
        "                'passage': passage,",
        "                'score': score,",
        "                'reference': f\"{doc_id}:1\"",
        "            })",
        "        return formatted_results",
        "",
        "    # Full passage search",
        "    results = processor.find_passages_for_query(",
        "        query,",
        "        top_n=top_n,",
        "        chunk_size=chunk_size,",
        "        overlap=100",
        "    )",
        "",
        "    formatted_results = []",
        "    for passage, doc_id, start, end, score in results:",
        "        # Get the full document content to find line number",
        "        doc_content = processor.documents.get(doc_id, '')",
        "        line_num = find_line_number(doc_content, start)",
        "",
        "        formatted_results.append({",
        "            'file': doc_id,",
        "            'line': line_num,",
        "            'passage': passage,",
        "            'score': score,",
        "            'reference': f\"{doc_id}:{line_num}\"",
        "        })",
        "",
        "    return formatted_results",
        "",
        "",
        "def display_results(results: list, verbose: bool = False):",
        "    \"\"\"Display search results.\"\"\"",
        "    if not results:",
        "        print(\"No results found.\")",
        "        return",
        "",
        "    print(f\"\\nFound {len(results)} relevant passages:\\n\")",
        "",
        "    for i, result in enumerate(results, 1):",
        "        print(\"=\" * 60)",
        "        print(f\"[{i}] {result['reference']}\")",
        "        print(f\"    Score: {result['score']:.3f}\")",
        "        print(\"-\" * 60)",
        "",
        "        if verbose:",
        "            print(format_passage(result['passage']))",
        "        else:",
        "            # Show first 5 lines",
        "            lines = result['passage'].split('\\n')[:5]",
        "            for line in lines:",
        "                if len(line) > 76:",
        "                    line = line[:73] + '...'",
        "                print(f\"  {line}\")",
        "            if len(result['passage'].split('\\n')) > 5:",
        "                print(f\"  ... ({len(result['passage'].split(chr(10))) - 5} more lines)\")",
        "        print()",
        "",
        "",
        "def expand_query_display(processor: CorticalTextProcessor, query: str):",
        "    \"\"\"Show expanded query terms.\"\"\"",
        "    expanded = processor.expand_query(query, max_expansions=10)",
        "    print(\"\\nQuery expansion:\")",
        "    for term, weight in sorted(expanded.items(), key=lambda x: -x[1])[:10]:",
        "        print(f\"  {term}: {weight:.3f}\")",
        "",
        "",
        "def interactive_mode(processor: CorticalTextProcessor):",
        "    \"\"\"Run interactive search mode.\"\"\"",
        "    print(\"\\nInteractive Search Mode\")",
        "    print(\"=\" * 40)",
        "    print(\"Commands:\")",
        "    print(\"  /expand <query>  - Show query expansion\")",
        "    print(\"  /concepts        - List concept clusters\")",
        "    print(\"  /stats           - Show corpus statistics\")",
        "    print(\"  /help            - Show this help\")",
        "    print(\"  /quit            - Exit\")",
        "    print()",
        "",
        "    while True:",
        "        try:",
        "            query = input(\"Search> \").strip()",
        "        except (EOFError, KeyboardInterrupt):",
        "            print(\"\\nGoodbye!\")",
        "            break",
        "",
        "        if not query:",
        "            continue",
        "",
        "        if query.startswith('/'):",
        "            cmd_parts = query.split(maxsplit=1)",
        "            cmd = cmd_parts[0].lower()",
        "",
        "            if cmd == '/quit' or cmd == '/exit':",
        "                print(\"Goodbye!\")",
        "                break",
        "            elif cmd == '/help':",
        "                print(\"Commands: /expand, /concepts, /stats, /quit\")",
        "            elif cmd == '/stats':",
        "                print(f\"\\nCorpus Statistics:\")",
        "                print(f\"  Documents: {len(processor.documents)}\")",
        "                print(f\"  Tokens: {processor.layers[0].column_count()}\")",
        "                print(f\"  Bigrams: {processor.layers[1].column_count()}\")",
        "                print(f\"  Concepts: {processor.layers[2].column_count()}\")",
        "                print(f\"  Relations: {len(processor.semantic_relations)}\")",
        "            elif cmd == '/expand' and len(cmd_parts) > 1:",
        "                expand_query_display(processor, cmd_parts[1])",
        "            elif cmd == '/concepts':",
        "                layer2 = processor.layers[2]",
        "                concepts = list(layer2.minicolumns.values())[:10]",
        "                print(f\"\\nTop concepts ({layer2.column_count()} total):\")",
        "                for c in concepts:",
        "                    print(f\"  {c.content[:50]}\")",
        "            else:",
        "                print(f\"Unknown command: {cmd}\")",
        "        else:",
        "            results = search_codebase(processor, query, top_n=5)",
        "            display_results(results, verbose=True)",
        "",
        "",
        "def main():",
        "    parser = argparse.ArgumentParser(description='Search the indexed codebase')",
        "    parser.add_argument('query', nargs='?', help='Search query')",
        "    parser.add_argument('--corpus', '-c', default='corpus_dev.pkl',",
        "                        help='Corpus file path (default: corpus_dev.pkl)')",
        "    parser.add_argument('--top', '-n', type=int, default=5,",
        "                        help='Number of results (default: 5)')",
        "    parser.add_argument('--verbose', '-v', action='store_true',",
        "                        help='Show full passage text')",
        "    parser.add_argument('--expand', '-e', action='store_true',",
        "                        help='Show query expansion')",
        "    parser.add_argument('--interactive', '-i', action='store_true',",
        "                        help='Interactive search mode')",
        "    parser.add_argument('--fast', '-f', action='store_true',",
        "                        help='Fast search mode (document-level, ~2-3x faster)')",
        "    args = parser.parse_args()",
        "",
        "    base_path = Path(__file__).parent.parent",
        "    corpus_path = base_path / args.corpus",
        "",
        "    # Check if corpus exists",
        "    if not corpus_path.exists():",
        "        print(f\"Error: Corpus file not found: {corpus_path}\")",
        "        print(\"Run 'python scripts/index_codebase.py' first to create it.\")",
        "        sys.exit(1)",
        "",
        "    # Load corpus",
        "    print(f\"Loading corpus from {corpus_path}...\")",
        "    processor = CorticalTextProcessor.load(str(corpus_path))",
        "    print(f\"Loaded {len(processor.documents)} documents\\n\")",
        "",
        "    if args.interactive:",
        "        interactive_mode(processor)",
        "    elif args.query:",
        "        if args.expand:",
        "            expand_query_display(processor, args.query)",
        "            print()",
        "",
        "        results = search_codebase(processor, args.query, top_n=args.top, fast=args.fast)",
        "        if args.fast:",
        "            print(\"(Fast mode: document-level results)\")",
        "        display_results(results, verbose=args.verbose)",
        "    else:",
        "        parser.print_help()",
        "",
        "",
        "if __name__ == '__main__':",
        "    main()"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "tests/test_analysis.py",
      "function": "class TestGetByIdOptimization(unittest.TestCase):",
      "start_line": 274,
      "lines_added": [
        "class TestParameterValidation(unittest.TestCase):",
        "    \"\"\"Test parameter validation in analysis functions.\"\"\"",
        "",
        "    def test_pagerank_invalid_damping_zero(self):",
        "        \"\"\"Test PageRank rejects damping=0.\"\"\"",
        "        layer = HierarchicalLayer(CorticalLayer.TOKENS)",
        "        layer.get_or_create_minicolumn(\"test\")",
        "        with self.assertRaises(ValueError) as ctx:",
        "            compute_pagerank(layer, damping=0)",
        "        self.assertIn(\"damping\", str(ctx.exception))",
        "",
        "    def test_pagerank_invalid_damping_one(self):",
        "        \"\"\"Test PageRank rejects damping=1.\"\"\"",
        "        layer = HierarchicalLayer(CorticalLayer.TOKENS)",
        "        layer.get_or_create_minicolumn(\"test\")",
        "        with self.assertRaises(ValueError) as ctx:",
        "            compute_pagerank(layer, damping=1.0)",
        "        self.assertIn(\"damping\", str(ctx.exception))",
        "",
        "    def test_pagerank_invalid_damping_negative(self):",
        "        \"\"\"Test PageRank rejects negative damping.\"\"\"",
        "        layer = HierarchicalLayer(CorticalLayer.TOKENS)",
        "        layer.get_or_create_minicolumn(\"test\")",
        "        with self.assertRaises(ValueError) as ctx:",
        "            compute_pagerank(layer, damping=-0.5)",
        "        self.assertIn(\"damping\", str(ctx.exception))",
        "",
        "    def test_pagerank_invalid_damping_greater_than_one(self):",
        "        \"\"\"Test PageRank rejects damping > 1.\"\"\"",
        "        layer = HierarchicalLayer(CorticalLayer.TOKENS)",
        "        layer.get_or_create_minicolumn(\"test\")",
        "        with self.assertRaises(ValueError) as ctx:",
        "            compute_pagerank(layer, damping=1.5)",
        "        self.assertIn(\"damping\", str(ctx.exception))",
        "",
        "    def test_pagerank_valid_damping(self):",
        "        \"\"\"Test PageRank accepts valid damping values.\"\"\"",
        "        layer = HierarchicalLayer(CorticalLayer.TOKENS)",
        "        layer.get_or_create_minicolumn(\"test\")",
        "        # Should not raise",
        "        result = compute_pagerank(layer, damping=0.85)",
        "        self.assertIsInstance(result, dict)",
        "",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "",
        "    def test_get_by_id_returns_none_for_missing(self):",
        "        \"\"\"Test that get_by_id returns None for missing ID.\"\"\"",
        "        layer = HierarchicalLayer(CorticalLayer.TOKENS)",
        "        layer.get_or_create_minicolumn(\"test\")",
        "",
        "        result = layer.get_by_id(\"nonexistent_id\")",
        "        self.assertIsNone(result)",
        "",
        ""
      ],
      "context_after": [
        "if __name__ == \"__main__\":",
        "    unittest.main(verbosity=2)"
      ],
      "change_type": "add"
    },
    {
      "file": "tests/test_code_concepts.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "\"\"\"",
        "Tests for code_concepts module.",
        "",
        "Tests the programming concept groups and expansion functions",
        "used for semantic code search.",
        "\"\"\"",
        "",
        "import unittest",
        "from cortical.code_concepts import (",
        "    CODE_CONCEPT_GROUPS,",
        "    get_related_terms,",
        "    expand_code_concepts,",
        "    get_concept_group,",
        "    list_concept_groups,",
        "    get_group_terms,",
        ")",
        "",
        "",
        "class TestCodeConceptGroups(unittest.TestCase):",
        "    \"\"\"Test the CODE_CONCEPT_GROUPS structure.\"\"\"",
        "",
        "    def test_groups_exist(self):",
        "        \"\"\"Test that concept groups are defined.\"\"\"",
        "        self.assertGreater(len(CODE_CONCEPT_GROUPS), 0)",
        "",
        "    def test_retrieval_group(self):",
        "        \"\"\"Test the retrieval concept group.\"\"\"",
        "        self.assertIn('retrieval', CODE_CONCEPT_GROUPS)",
        "        retrieval = CODE_CONCEPT_GROUPS['retrieval']",
        "        self.assertIn('get', retrieval)",
        "        self.assertIn('fetch', retrieval)",
        "        self.assertIn('load', retrieval)",
        "        self.assertIn('retrieve', retrieval)",
        "",
        "    def test_storage_group(self):",
        "        \"\"\"Test the storage concept group.\"\"\"",
        "        self.assertIn('storage', CODE_CONCEPT_GROUPS)",
        "        storage = CODE_CONCEPT_GROUPS['storage']",
        "        self.assertIn('save', storage)",
        "        self.assertIn('store', storage)",
        "        self.assertIn('write', storage)",
        "        self.assertIn('persist', storage)",
        "",
        "    def test_auth_group(self):",
        "        \"\"\"Test the authentication concept group.\"\"\"",
        "        self.assertIn('auth', CODE_CONCEPT_GROUPS)",
        "        auth = CODE_CONCEPT_GROUPS['auth']",
        "        self.assertIn('login', auth)",
        "        self.assertIn('credentials', auth)",
        "        self.assertIn('token', auth)",
        "",
        "    def test_error_group(self):",
        "        \"\"\"Test the error handling concept group.\"\"\"",
        "        self.assertIn('error', CODE_CONCEPT_GROUPS)",
        "        error = CODE_CONCEPT_GROUPS['error']",
        "        self.assertIn('exception', error)",
        "        self.assertIn('catch', error)",
        "        self.assertIn('throw', error)",
        "",
        "    def test_groups_are_frozensets(self):",
        "        \"\"\"Test that groups are immutable frozensets.\"\"\"",
        "        for group_name, terms in CODE_CONCEPT_GROUPS.items():",
        "            self.assertIsInstance(terms, frozenset)",
        "",
        "",
        "class TestGetRelatedTerms(unittest.TestCase):",
        "    \"\"\"Test the get_related_terms function.\"\"\"",
        "",
        "    def test_fetch_related_terms(self):",
        "        \"\"\"Test getting terms related to 'fetch'.\"\"\"",
        "        related = get_related_terms('fetch', max_terms=10)",
        "        self.assertIn('get', related)",
        "        self.assertIn('load', related)",
        "        self.assertNotIn('fetch', related)  # Should not include input term",
        "",
        "    def test_save_related_terms(self):",
        "        \"\"\"Test getting terms related to 'save'.\"\"\"",
        "        related = get_related_terms('save', max_terms=12)",
        "        self.assertIn('store', related)",
        "        self.assertIn('write', related)",
        "        self.assertNotIn('save', related)",
        "",
        "    def test_unknown_term(self):",
        "        \"\"\"Test with a term not in any concept group.\"\"\"",
        "        related = get_related_terms('xyzabc123')",
        "        self.assertEqual(related, [])",
        "",
        "    def test_max_terms_limit(self):",
        "        \"\"\"Test that max_terms limits the output.\"\"\"",
        "        related = get_related_terms('get', max_terms=3)",
        "        self.assertLessEqual(len(related), 3)",
        "",
        "    def test_case_insensitive(self):",
        "        \"\"\"Test that lookup is case insensitive.\"\"\"",
        "        related_lower = get_related_terms('fetch')",
        "        related_upper = get_related_terms('FETCH')",
        "        related_mixed = get_related_terms('Fetch')",
        "        self.assertEqual(set(related_lower), set(related_upper))",
        "        self.assertEqual(set(related_lower), set(related_mixed))",
        "",
        "",
        "class TestExpandCodeConcepts(unittest.TestCase):",
        "    \"\"\"Test the expand_code_concepts function.\"\"\"",
        "",
        "    def test_expand_single_term(self):",
        "        \"\"\"Test expanding a single term.\"\"\"",
        "        expanded = expand_code_concepts(['fetch'], max_expansions_per_term=10)",
        "        self.assertIn('get', expanded)",
        "        self.assertIn('load', expanded)",
        "        self.assertNotIn('fetch', expanded)  # Input terms not in output",
        "",
        "    def test_expand_multiple_terms(self):",
        "        \"\"\"Test expanding multiple terms.\"\"\"",
        "        expanded = expand_code_concepts(['fetch', 'save'], max_expansions_per_term=10)",
        "        # Should have terms from both retrieval and storage groups",
        "        self.assertIn('get', expanded)",
        "        self.assertIn('store', expanded)",
        "",
        "    def test_expand_empty_list(self):",
        "        \"\"\"Test expanding empty list.\"\"\"",
        "        expanded = expand_code_concepts([])",
        "        self.assertEqual(expanded, {})",
        "",
        "    def test_expand_unknown_terms(self):",
        "        \"\"\"Test expanding terms not in any group.\"\"\"",
        "        expanded = expand_code_concepts(['xyzabc123'])",
        "        self.assertEqual(expanded, {})",
        "",
        "    def test_weights_are_floats(self):",
        "        \"\"\"Test that expansion weights are floats.\"\"\"",
        "        expanded = expand_code_concepts(['fetch'])",
        "        for term, weight in expanded.items():",
        "            self.assertIsInstance(weight, float)",
        "            self.assertGreater(weight, 0.0)",
        "            self.assertLessEqual(weight, 1.0)",
        "",
        "    def test_custom_weight(self):",
        "        \"\"\"Test custom weight parameter.\"\"\"",
        "        expanded = expand_code_concepts(['fetch'], weight=0.8)",
        "        for term, weight in expanded.items():",
        "            self.assertEqual(weight, 0.8)",
        "",
        "    def test_max_expansions_per_term(self):",
        "        \"\"\"Test limiting expansions per term.\"\"\"",
        "        expanded = expand_code_concepts(['fetch'], max_expansions_per_term=2)",
        "        self.assertLessEqual(len(expanded), 2)",
        "",
        "    def test_no_duplicate_original_terms(self):",
        "        \"\"\"Test that original terms are not in expansions.\"\"\"",
        "        terms = ['get', 'fetch', 'load']",
        "        expanded = expand_code_concepts(terms)",
        "        for term in terms:",
        "            self.assertNotIn(term, expanded)",
        "",
        "",
        "class TestGetConceptGroup(unittest.TestCase):",
        "    \"\"\"Test the get_concept_group function.\"\"\"",
        "",
        "    def test_single_group_membership(self):",
        "        \"\"\"Test term that belongs to one group.\"\"\"",
        "        groups = get_concept_group('fetch')",
        "        self.assertIn('retrieval', groups)",
        "",
        "    def test_multiple_group_membership(self):",
        "        \"\"\"Test term that might belong to multiple groups.\"\"\"",
        "        # 'validate' is in both 'validation' and possibly 'testing'",
        "        groups = get_concept_group('validate')",
        "        self.assertIn('validation', groups)",
        "",
        "    def test_unknown_term(self):",
        "        \"\"\"Test unknown term returns empty list.\"\"\"",
        "        groups = get_concept_group('xyzabc123')",
        "        self.assertEqual(groups, [])",
        "",
        "    def test_case_insensitive(self):",
        "        \"\"\"Test case insensitive lookup.\"\"\"",
        "        groups_lower = get_concept_group('fetch')",
        "        groups_upper = get_concept_group('FETCH')",
        "        self.assertEqual(groups_lower, groups_upper)",
        "",
        "",
        "class TestListConceptGroups(unittest.TestCase):",
        "    \"\"\"Test the list_concept_groups function.\"\"\"",
        "",
        "    def test_returns_list(self):",
        "        \"\"\"Test that function returns a list.\"\"\"",
        "        groups = list_concept_groups()",
        "        self.assertIsInstance(groups, list)",
        "",
        "    def test_contains_known_groups(self):",
        "        \"\"\"Test that known groups are in the list.\"\"\"",
        "        groups = list_concept_groups()",
        "        self.assertIn('retrieval', groups)",
        "        self.assertIn('storage', groups)",
        "        self.assertIn('auth', groups)",
        "        self.assertIn('error', groups)",
        "",
        "    def test_list_is_sorted(self):",
        "        \"\"\"Test that list is sorted alphabetically.\"\"\"",
        "        groups = list_concept_groups()",
        "        self.assertEqual(groups, sorted(groups))",
        "",
        "",
        "class TestGetGroupTerms(unittest.TestCase):",
        "    \"\"\"Test the get_group_terms function.\"\"\"",
        "",
        "    def test_retrieval_terms(self):",
        "        \"\"\"Test getting terms from retrieval group.\"\"\"",
        "        terms = get_group_terms('retrieval')",
        "        self.assertIn('get', terms)",
        "        self.assertIn('fetch', terms)",
        "",
        "    def test_unknown_group(self):",
        "        \"\"\"Test unknown group returns empty list.\"\"\"",
        "        terms = get_group_terms('nonexistent_group')",
        "        self.assertEqual(terms, [])",
        "",
        "    def test_terms_are_sorted(self):",
        "        \"\"\"Test that terms are sorted alphabetically.\"\"\"",
        "        terms = get_group_terms('retrieval')",
        "        self.assertEqual(terms, sorted(terms))",
        "",
        "",
        "class TestQueryExpansionIntegration(unittest.TestCase):",
        "    \"\"\"Test code concepts integration with query expansion.\"\"\"",
        "",
        "    def setUp(self):",
        "        \"\"\"Set up test processor.\"\"\"",
        "        from cortical import CorticalTextProcessor",
        "        self.processor = CorticalTextProcessor()",
        "        # Use terms that won't be filtered as stop words",
        "        self.processor.process_document(\"doc1\", \"\"\"",
        "            The retrieve function obtains user information from the database.",
        "            It will fetch data and load settings internally.",
        "            The query method returns user profiles.",
        "        \"\"\")",
        "        self.processor.process_document(\"doc2\", \"\"\"",
        "            The persist function stores user information to the database.",
        "            It handles save operations and caching of user profiles.",
        "            The store method writes data.",
        "        \"\"\")",
        "        self.processor.compute_all()",
        "",
        "    def test_expand_query_with_code_concepts(self):",
        "        \"\"\"Test expand_query with use_code_concepts enabled.\"\"\"",
        "        expanded = self.processor.expand_query(",
        "            \"fetch data\",",
        "            use_code_concepts=True",
        "        )",
        "        # Should include original terms",
        "        self.assertIn('fetch', expanded)",
        "        self.assertIn('data', expanded)",
        "        # With code concepts enabled, should also include related terms",
        "        # like 'load', 'retrieve' (if expansion finds them)",
        "",
        "    def test_expand_query_for_code(self):",
        "        \"\"\"Test the expand_query_for_code convenience method.\"\"\"",
        "        expanded = self.processor.expand_query_for_code(\"fetch data\")",
        "        self.assertIn('fetch', expanded)",
        "        self.assertIn('data', expanded)",
        "",
        "    def test_code_concepts_adds_synonyms(self):",
        "        \"\"\"Test that code concepts adds programming synonyms.\"\"\"",
        "        # Expand 'fetch' with code concepts - not a stop word",
        "        expanded_with_code = self.processor.expand_query(",
        "            \"fetch\",",
        "            use_code_concepts=True,",
        "            max_expansions=20",
        "        )",
        "        # Should include 'fetch' as original term",
        "        self.assertIn('fetch', expanded_with_code)",
        "        # Code concepts should add related retrieval terms",
        "        # Check that at least one synonym is added",
        "        retrieval_synonyms = {'load', 'retrieve', 'query', 'obtain'}",
        "        has_synonym = any(s in expanded_with_code for s in retrieval_synonyms)",
        "        self.assertTrue(has_synonym, f\"Expected synonyms in {expanded_with_code}\")",
        "",
        "    def test_code_concepts_disabled_by_default(self):",
        "        \"\"\"Test that code concepts are disabled by default.\"\"\"",
        "        # This test verifies the parameter exists and doesn't crash",
        "        expanded_default = self.processor.expand_query(\"fetch\")",
        "        self.assertIn('fetch', expanded_default)",
        "",
        "",
        "if __name__ == '__main__':",
        "    unittest.main()"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "tests/test_fingerprint.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "\"\"\"",
        "Tests for fingerprint module.",
        "",
        "Tests the semantic fingerprinting functionality for code comparison.",
        "\"\"\"",
        "",
        "import unittest",
        "from cortical.fingerprint import (",
        "    compute_fingerprint,",
        "    compare_fingerprints,",
        "    explain_fingerprint,",
        "    explain_similarity,",
        "    SemanticFingerprint,",
        ")",
        "from cortical.tokenizer import Tokenizer",
        "",
        "",
        "class TestComputeFingerprint(unittest.TestCase):",
        "    \"\"\"Test the compute_fingerprint function.\"\"\"",
        "",
        "    def setUp(self):",
        "        \"\"\"Set up test tokenizer.\"\"\"",
        "        self.tokenizer = Tokenizer()",
        "",
        "    def test_basic_fingerprint(self):",
        "        \"\"\"Test basic fingerprint computation.\"\"\"",
        "        text = \"The function validates user input and handles errors.\"",
        "        fp = compute_fingerprint(text, self.tokenizer)",
        "",
        "        self.assertIn('terms', fp)",
        "        self.assertIn('concepts', fp)",
        "        self.assertIn('bigrams', fp)",
        "        self.assertIn('top_terms', fp)",
        "        self.assertIn('term_count', fp)",
        "        self.assertIn('raw_text_hash', fp)",
        "",
        "    def test_fingerprint_contains_terms(self):",
        "        \"\"\"Test that fingerprint contains expected terms.\"\"\"",
        "        text = \"fetch user data from database\"",
        "        fp = compute_fingerprint(text, self.tokenizer)",
        "",
        "        self.assertIn('fetch', fp['terms'])",
        "        self.assertIn('user', fp['terms'])",
        "        self.assertIn('data', fp['terms'])",
        "        self.assertIn('database', fp['terms'])",
        "",
        "    def test_fingerprint_concepts(self):",
        "        \"\"\"Test that fingerprint captures concept membership.\"\"\"",
        "        text = \"fetch data and save results\"",
        "        fp = compute_fingerprint(text, self.tokenizer)",
        "",
        "        # 'fetch' is in retrieval group, 'save' is in storage group",
        "        # (if code_concepts recognizes them)",
        "        self.assertIsInstance(fp['concepts'], dict)",
        "",
        "    def test_fingerprint_bigrams(self):",
        "        \"\"\"Test that fingerprint captures bigrams.\"\"\"",
        "        text = \"neural networks process data efficiently\"",
        "        fp = compute_fingerprint(text, self.tokenizer)",
        "",
        "        self.assertIn('bigrams', fp)",
        "        self.assertIsInstance(fp['bigrams'], dict)",
        "",
        "    def test_fingerprint_top_terms_limit(self):",
        "        \"\"\"Test that top_n limits top terms.\"\"\"",
        "        text = \"word1 word2 word3 word4 word5 word6 word7 word8 word9 word10\"",
        "        fp = compute_fingerprint(text, self.tokenizer, top_n=5)",
        "",
        "        self.assertLessEqual(len(fp['top_terms']), 5)",
        "",
        "    def test_empty_text_fingerprint(self):",
        "        \"\"\"Test fingerprint of empty text.\"\"\"",
        "        fp = compute_fingerprint(\"\", self.tokenizer)",
        "",
        "        self.assertEqual(fp['term_count'], 0)",
        "        self.assertEqual(fp['terms'], {})",
        "",
        "    def test_fingerprint_term_weights_positive(self):",
        "        \"\"\"Test that term weights are positive.\"\"\"",
        "        text = \"validate user input data\"",
        "        fp = compute_fingerprint(text, self.tokenizer)",
        "",
        "        for term, weight in fp['terms'].items():",
        "            self.assertGreater(weight, 0)",
        "",
        "    def test_fingerprint_with_layers(self):",
        "        \"\"\"Test fingerprint with corpus layers for TF-IDF.\"\"\"",
        "        from cortical import CorticalTextProcessor",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(\"doc1\", \"test document content\")",
        "        processor.compute_all()",
        "",
        "        # Compute fingerprint with layers",
        "        fp = compute_fingerprint(",
        "            \"test content\",",
        "            processor.tokenizer,",
        "            processor.layers",
        "        )",
        "",
        "        self.assertIn('terms', fp)",
        "        self.assertGreater(len(fp['terms']), 0)",
        "",
        "",
        "class TestCompareFingerprints(unittest.TestCase):",
        "    \"\"\"Test the compare_fingerprints function.\"\"\"",
        "",
        "    def setUp(self):",
        "        \"\"\"Set up test tokenizer.\"\"\"",
        "        self.tokenizer = Tokenizer()",
        "",
        "    def test_identical_texts(self):",
        "        \"\"\"Test comparing identical texts.\"\"\"",
        "        text = \"validate user input data\"",
        "        fp1 = compute_fingerprint(text, self.tokenizer)",
        "        fp2 = compute_fingerprint(text, self.tokenizer)",
        "",
        "        result = compare_fingerprints(fp1, fp2)",
        "",
        "        self.assertTrue(result['identical'])",
        "        self.assertEqual(result['overall_similarity'], 1.0)",
        "",
        "    def test_similar_texts(self):",
        "        \"\"\"Test comparing similar texts.\"\"\"",
        "        fp1 = compute_fingerprint(\"validate user input\", self.tokenizer)",
        "        fp2 = compute_fingerprint(\"check user data\", self.tokenizer)",
        "",
        "        result = compare_fingerprints(fp1, fp2)",
        "",
        "        self.assertFalse(result['identical'])",
        "        self.assertIn('user', result['shared_terms'])",
        "        self.assertGreater(result['term_similarity'], 0)",
        "",
        "    def test_different_texts(self):",
        "        \"\"\"Test comparing different texts.\"\"\"",
        "        fp1 = compute_fingerprint(\"neural network training\", self.tokenizer)",
        "        fp2 = compute_fingerprint(\"database query optimization\", self.tokenizer)",
        "",
        "        result = compare_fingerprints(fp1, fp2)",
        "",
        "        self.assertFalse(result['identical'])",
        "        # Should have lower similarity",
        "        self.assertLess(result['overall_similarity'], 0.5)",
        "",
        "    def test_comparison_contains_metrics(self):",
        "        \"\"\"Test that comparison contains all expected metrics.\"\"\"",
        "        fp1 = compute_fingerprint(\"text one\", self.tokenizer)",
        "        fp2 = compute_fingerprint(\"text two\", self.tokenizer)",
        "",
        "        result = compare_fingerprints(fp1, fp2)",
        "",
        "        self.assertIn('identical', result)",
        "        self.assertIn('term_similarity', result)",
        "        self.assertIn('concept_similarity', result)",
        "        self.assertIn('overall_similarity', result)",
        "        self.assertIn('shared_terms', result)",
        "",
        "    def test_similarity_in_valid_range(self):",
        "        \"\"\"Test that similarity scores are in [0, 1].\"\"\"",
        "        fp1 = compute_fingerprint(\"fetch user data\", self.tokenizer)",
        "        fp2 = compute_fingerprint(\"save user results\", self.tokenizer)",
        "",
        "        result = compare_fingerprints(fp1, fp2)",
        "",
        "        self.assertGreaterEqual(result['term_similarity'], 0)",
        "        self.assertLessEqual(result['term_similarity'], 1)",
        "        self.assertGreaterEqual(result['overall_similarity'], 0)",
        "        self.assertLessEqual(result['overall_similarity'], 1)",
        "",
        "    def test_shared_terms_correct(self):",
        "        \"\"\"Test that shared terms are correctly identified.\"\"\"",
        "        fp1 = compute_fingerprint(\"user data validation\", self.tokenizer)",
        "        fp2 = compute_fingerprint(\"user input checking\", self.tokenizer)",
        "",
        "        result = compare_fingerprints(fp1, fp2)",
        "",
        "        self.assertIn('user', result['shared_terms'])",
        "",
        "",
        "class TestExplainFingerprint(unittest.TestCase):",
        "    \"\"\"Test the explain_fingerprint function.\"\"\"",
        "",
        "    def setUp(self):",
        "        \"\"\"Set up test tokenizer.\"\"\"",
        "        self.tokenizer = Tokenizer()",
        "",
        "    def test_explanation_structure(self):",
        "        \"\"\"Test that explanation has expected structure.\"\"\"",
        "        text = \"validate user input and handle errors\"",
        "        fp = compute_fingerprint(text, self.tokenizer)",
        "        explanation = explain_fingerprint(fp)",
        "",
        "        self.assertIn('summary', explanation)",
        "        self.assertIn('top_terms', explanation)",
        "        self.assertIn('top_concepts', explanation)",
        "        self.assertIn('term_count', explanation)",
        "",
        "    def test_explanation_top_n_limit(self):",
        "        \"\"\"Test that top_n limits items in explanation.\"\"\"",
        "        text = \"word1 word2 word3 word4 word5 word6\"",
        "        fp = compute_fingerprint(text, self.tokenizer)",
        "        explanation = explain_fingerprint(fp, top_n=3)",
        "",
        "        self.assertLessEqual(len(explanation['top_terms']), 3)",
        "",
        "    def test_summary_is_string(self):",
        "        \"\"\"Test that summary is a string.\"\"\"",
        "        text = \"process data\"",
        "        fp = compute_fingerprint(text, self.tokenizer)",
        "        explanation = explain_fingerprint(fp)",
        "",
        "        self.assertIsInstance(explanation['summary'], str)",
        "",
        "",
        "class TestExplainSimilarity(unittest.TestCase):",
        "    \"\"\"Test the explain_similarity function.\"\"\"",
        "",
        "    def setUp(self):",
        "        \"\"\"Set up test tokenizer.\"\"\"",
        "        self.tokenizer = Tokenizer()",
        "",
        "    def test_explanation_is_string(self):",
        "        \"\"\"Test that similarity explanation is a string.\"\"\"",
        "        fp1 = compute_fingerprint(\"fetch user data\", self.tokenizer)",
        "        fp2 = compute_fingerprint(\"load user info\", self.tokenizer)",
        "",
        "        explanation = explain_similarity(fp1, fp2)",
        "",
        "        self.assertIsInstance(explanation, str)",
        "        self.assertGreater(len(explanation), 0)",
        "",
        "    def test_identical_texts_explanation(self):",
        "        \"\"\"Test explanation for identical texts.\"\"\"",
        "        text = \"validate input\"",
        "        fp1 = compute_fingerprint(text, self.tokenizer)",
        "        fp2 = compute_fingerprint(text, self.tokenizer)",
        "",
        "        explanation = explain_similarity(fp1, fp2)",
        "",
        "        self.assertIn('identical', explanation.lower())",
        "",
        "",
        "class TestProcessorIntegration(unittest.TestCase):",
        "    \"\"\"Test fingerprint integration with processor.\"\"\"",
        "",
        "    def setUp(self):",
        "        \"\"\"Set up test processor.\"\"\"",
        "        from cortical import CorticalTextProcessor",
        "        self.processor = CorticalTextProcessor()",
        "        self.processor.process_document(\"auth\", \"\"\"",
        "            Authentication module handles user login and credentials.",
        "            Validates tokens and manages sessions.",
        "        \"\"\")",
        "        self.processor.process_document(\"data\", \"\"\"",
        "            Data processing module fetches and transforms data.",
        "            Handles database queries and result formatting.",
        "        \"\"\")",
        "        self.processor.compute_all()",
        "",
        "    def test_processor_get_fingerprint(self):",
        "        \"\"\"Test processor get_fingerprint method.\"\"\"",
        "        fp = self.processor.get_fingerprint(\"validate user credentials\")",
        "",
        "        self.assertIn('terms', fp)",
        "        self.assertIn('concepts', fp)",
        "        self.assertGreater(fp['term_count'], 0)",
        "",
        "    def test_processor_compare_fingerprints(self):",
        "        \"\"\"Test processor compare_fingerprints method.\"\"\"",
        "        fp1 = self.processor.get_fingerprint(\"user authentication\")",
        "        fp2 = self.processor.get_fingerprint(\"user validation\")",
        "",
        "        result = self.processor.compare_fingerprints(fp1, fp2)",
        "",
        "        self.assertIn('overall_similarity', result)",
        "        self.assertIn('user', result['shared_terms'])",
        "",
        "    def test_processor_explain_fingerprint(self):",
        "        \"\"\"Test processor explain_fingerprint method.\"\"\"",
        "        fp = self.processor.get_fingerprint(\"fetch data from database\")",
        "        explanation = self.processor.explain_fingerprint(fp)",
        "",
        "        self.assertIn('summary', explanation)",
        "        self.assertIn('top_terms', explanation)",
        "",
        "    def test_processor_explain_similarity(self):",
        "        \"\"\"Test processor explain_similarity method.\"\"\"",
        "        fp1 = self.processor.get_fingerprint(\"fetch data\")",
        "        fp2 = self.processor.get_fingerprint(\"load data\")",
        "",
        "        explanation = self.processor.explain_similarity(fp1, fp2)",
        "",
        "        self.assertIsInstance(explanation, str)",
        "",
        "    def test_processor_find_similar_texts(self):",
        "        \"\"\"Test processor find_similar_texts method.\"\"\"",
        "        candidates = [",
        "            (\"auth_code\", \"validate user credentials and create session\"),",
        "            (\"data_code\", \"fetch records from database and transform\"),",
        "            (\"ui_code\", \"render button and handle click event\"),",
        "        ]",
        "",
        "        results = self.processor.find_similar_texts(",
        "            \"authenticate user login\",",
        "            candidates,",
        "            top_n=2",
        "        )",
        "",
        "        self.assertLessEqual(len(results), 2)",
        "        # Results should be sorted by similarity",
        "        if len(results) >= 2:",
        "            self.assertGreaterEqual(results[0][1], results[1][1])",
        "",
        "",
        "if __name__ == '__main__':",
        "    unittest.main()"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "tests/test_intent_query.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "\"\"\"",
        "Tests for intent-based query understanding.",
        "",
        "Tests the parse_intent_query and search_by_intent functions",
        "used for natural language code search.",
        "\"\"\"",
        "",
        "import unittest",
        "from cortical.query import (",
        "    parse_intent_query,",
        "    search_by_intent,",
        "    QUESTION_INTENTS,",
        "    ACTION_VERBS,",
        "    ParsedIntent,",
        ")",
        "",
        "",
        "class TestParseIntentQuery(unittest.TestCase):",
        "    \"\"\"Test the parse_intent_query function.\"\"\"",
        "",
        "    def test_where_query(self):",
        "        \"\"\"Test parsing 'where' queries for location intent.\"\"\"",
        "        result = parse_intent_query(\"where do we handle authentication?\")",
        "        self.assertEqual(result['intent'], 'location')",
        "        self.assertEqual(result['question_word'], 'where')",
        "        self.assertEqual(result['action'], 'handle')",
        "        self.assertEqual(result['subject'], 'authentication')",
        "",
        "    def test_how_query(self):",
        "        \"\"\"Test parsing 'how' queries for implementation intent.\"\"\"",
        "        result = parse_intent_query(\"how do we validate user input?\")",
        "        self.assertEqual(result['intent'], 'implementation')",
        "        self.assertEqual(result['question_word'], 'how')",
        "        self.assertEqual(result['action'], 'validate')",
        "        self.assertIn('user', [result['subject'], result['expanded_terms']])",
        "",
        "    def test_what_query(self):",
        "        \"\"\"Test parsing 'what' queries for definition intent.\"\"\"",
        "        result = parse_intent_query(\"what is the database schema?\")",
        "        self.assertEqual(result['intent'], 'definition')",
        "        self.assertEqual(result['question_word'], 'what')",
        "",
        "    def test_why_query(self):",
        "        \"\"\"Test parsing 'why' queries for rationale intent.\"\"\"",
        "        result = parse_intent_query(\"why do we cache this data?\")",
        "        self.assertEqual(result['intent'], 'rationale')",
        "        self.assertEqual(result['question_word'], 'why')",
        "",
        "    def test_when_query(self):",
        "        \"\"\"Test parsing 'when' queries for lifecycle intent.\"\"\"",
        "        result = parse_intent_query(\"when does initialization happen?\")",
        "        self.assertEqual(result['intent'], 'lifecycle')",
        "        self.assertEqual(result['question_word'], 'when')",
        "",
        "    def test_no_question_word(self):",
        "        \"\"\"Test parsing queries without question words.\"\"\"",
        "        result = parse_intent_query(\"fetch user data\")",
        "        self.assertEqual(result['intent'], 'search')",
        "        self.assertIsNone(result['question_word'])",
        "        self.assertEqual(result['action'], 'fetch')",
        "",
        "    def test_empty_query(self):",
        "        \"\"\"Test parsing empty query.\"\"\"",
        "        result = parse_intent_query(\"\")",
        "        self.assertEqual(result['intent'], 'search')",
        "        self.assertIsNone(result['action'])",
        "        self.assertIsNone(result['subject'])",
        "        self.assertEqual(result['expanded_terms'], [])",
        "",
        "    def test_punctuation_handling(self):",
        "        \"\"\"Test that punctuation is handled correctly.\"\"\"",
        "        result = parse_intent_query(\"where is authentication???\")",
        "        self.assertEqual(result['intent'], 'location')",
        "        self.assertIn('authentication', result['expanded_terms'])",
        "",
        "    def test_expanded_terms_include_synonyms(self):",
        "        \"\"\"Test that expanded terms include code concept synonyms.\"\"\"",
        "        result = parse_intent_query(\"how to fetch data\")",
        "        # 'fetch' should expand to include related terms",
        "        self.assertIn('fetch', result['expanded_terms'])",
        "        # Should have some related terms (from retrieval group)",
        "        self.assertGreater(len(result['expanded_terms']), 1)",
        "",
        "    def test_action_verb_detection(self):",
        "        \"\"\"Test detection of various action verbs.\"\"\"",
        "        test_cases = [",
        "            (\"validate input\", \"validate\"),",
        "            (\"process request\", \"process\"),",
        "            (\"save user data\", \"save\"),",
        "            (\"delete old records\", \"delete\"),",
        "            (\"transform response\", \"transform\"),",
        "        ]",
        "        for query, expected_action in test_cases:",
        "            result = parse_intent_query(query)",
        "            self.assertEqual(result['action'], expected_action,",
        "                           f\"Failed for query: {query}\")",
        "",
        "    def test_subject_extraction(self):",
        "        \"\"\"Test extraction of query subject.\"\"\"",
        "        result = parse_intent_query(\"handle errors gracefully\")",
        "        self.assertEqual(result['subject'], 'errors')",
        "",
        "    def test_filler_words_removed(self):",
        "        \"\"\"Test that filler words don't become subject/action.\"\"\"",
        "        result = parse_intent_query(\"do we have a database connection?\")",
        "        self.assertNotEqual(result['subject'], 'do')",
        "        self.assertNotEqual(result['subject'], 'we')",
        "        self.assertNotEqual(result['subject'], 'have')",
        "",
        "",
        "class TestQuestionIntents(unittest.TestCase):",
        "    \"\"\"Test the QUESTION_INTENTS mapping.\"\"\"",
        "",
        "    def test_all_question_words_mapped(self):",
        "        \"\"\"Test that common question words are mapped.\"\"\"",
        "        expected_words = ['where', 'how', 'what', 'why', 'when', 'which', 'who']",
        "        for word in expected_words:",
        "            self.assertIn(word, QUESTION_INTENTS)",
        "",
        "    def test_intent_types(self):",
        "        \"\"\"Test that intent types are meaningful.\"\"\"",
        "        self.assertEqual(QUESTION_INTENTS['where'], 'location')",
        "        self.assertEqual(QUESTION_INTENTS['how'], 'implementation')",
        "        self.assertEqual(QUESTION_INTENTS['what'], 'definition')",
        "        self.assertEqual(QUESTION_INTENTS['why'], 'rationale')",
        "",
        "",
        "class TestActionVerbs(unittest.TestCase):",
        "    \"\"\"Test the ACTION_VERBS set.\"\"\"",
        "",
        "    def test_common_verbs_included(self):",
        "        \"\"\"Test that common programming action verbs are included.\"\"\"",
        "        expected_verbs = [",
        "            'handle', 'process', 'create', 'delete', 'update', 'fetch',",
        "            'validate', 'parse', 'transform', 'authenticate', 'initialize'",
        "        ]",
        "        for verb in expected_verbs:",
        "            self.assertIn(verb, ACTION_VERBS)",
        "",
        "    def test_is_frozenset(self):",
        "        \"\"\"Test that ACTION_VERBS is immutable.\"\"\"",
        "        self.assertIsInstance(ACTION_VERBS, frozenset)",
        "",
        "",
        "class TestSearchByIntent(unittest.TestCase):",
        "    \"\"\"Test the search_by_intent function.\"\"\"",
        "",
        "    def setUp(self):",
        "        \"\"\"Set up test processor.\"\"\"",
        "        from cortical import CorticalTextProcessor",
        "        self.processor = CorticalTextProcessor()",
        "        self.processor.process_document(\"auth_handler\", \"\"\"",
        "            Authentication handler module.",
        "            This module handles user authentication and login.",
        "            It validates credentials and creates sessions.",
        "        \"\"\")",
        "        self.processor.process_document(\"data_fetcher\", \"\"\"",
        "            Data fetching utilities.",
        "            Functions to fetch and retrieve data from external APIs.",
        "            Handles HTTP requests and response parsing.",
        "        \"\"\")",
        "        self.processor.process_document(\"validator\", \"\"\"",
        "            Input validation module.",
        "            Validates and sanitizes user input.",
        "            Checks for required fields and data types.",
        "        \"\"\")",
        "        self.processor.compute_all()",
        "",
        "    def test_search_returns_results(self):",
        "        \"\"\"Test that search returns results.\"\"\"",
        "        results = self.processor.search_by_intent(\"where do we handle authentication?\")",
        "        self.assertIsInstance(results, list)",
        "        # Should find auth_handler document",
        "        if results:",
        "            doc_ids = [r[0] for r in results]",
        "            self.assertIn('auth_handler', doc_ids)",
        "",
        "    def test_search_returns_parsed_intent(self):",
        "        \"\"\"Test that search returns parsed intent with results.\"\"\"",
        "        results = self.processor.search_by_intent(\"how to validate input?\")",
        "        if results:",
        "            doc_id, score, parsed = results[0]",
        "            self.assertIn('intent', parsed)",
        "            self.assertIn('action', parsed)",
        "            self.assertIn('expanded_terms', parsed)",
        "",
        "    def test_search_empty_query(self):",
        "        \"\"\"Test search with empty query.\"\"\"",
        "        results = self.processor.search_by_intent(\"\")",
        "        self.assertEqual(results, [])",
        "",
        "    def test_search_top_n_limit(self):",
        "        \"\"\"Test that top_n limits results.\"\"\"",
        "        results = self.processor.search_by_intent(\"fetch data\", top_n=2)",
        "        self.assertLessEqual(len(results), 2)",
        "",
        "    def test_processor_parse_intent_query(self):",
        "        \"\"\"Test the processor wrapper for parse_intent_query.\"\"\"",
        "        result = self.processor.parse_intent_query(\"where is the login function?\")",
        "        self.assertEqual(result['intent'], 'location')",
        "        # 'login' is detected as action verb, so 'function' becomes subject",
        "        self.assertEqual(result['action'], 'login')",
        "        self.assertEqual(result['subject'], 'function')",
        "",
        "",
        "class TestParsedIntentStructure(unittest.TestCase):",
        "    \"\"\"Test the ParsedIntent TypedDict structure.\"\"\"",
        "",
        "    def test_all_keys_present(self):",
        "        \"\"\"Test that all expected keys are in parsed result.\"\"\"",
        "        result = parse_intent_query(\"where do we handle errors?\")",
        "        expected_keys = ['action', 'subject', 'intent', 'question_word', 'expanded_terms']",
        "        for key in expected_keys:",
        "            self.assertIn(key, result)",
        "",
        "    def test_expanded_terms_is_list(self):",
        "        \"\"\"Test that expanded_terms is a list.\"\"\"",
        "        result = parse_intent_query(\"handle authentication\")",
        "        self.assertIsInstance(result['expanded_terms'], list)",
        "",
        "    def test_no_duplicate_expanded_terms(self):",
        "        \"\"\"Test that expanded_terms has no duplicates.\"\"\"",
        "        result = parse_intent_query(\"handle handle authentication\")",
        "        self.assertEqual(",
        "            len(result['expanded_terms']),",
        "            len(set(result['expanded_terms']))",
        "        )",
        "",
        "",
        "if __name__ == '__main__':",
        "    unittest.main()"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "tests/test_processor.py",
      "function": "class TestBigramConnections(unittest.TestCase):",
      "start_line": 1612,
      "lines_added": [
        "        # \"neural networks\" and \"neural processing\" share \"neural\"",
        "        # Note: bigrams use space separators (tokenizer.py:179)",
        "        neural_networks = layer1.get_minicolumn(\"neural networks\")",
        "        neural_processing = layer1.get_minicolumn(\"neural processing\")",
        "        # Verify bigrams exist",
        "        self.assertIsNotNone(neural_networks, \"Bigram 'neural networks' should exist\")",
        "        self.assertIsNotNone(neural_processing, \"Bigram 'neural processing' should exist\")",
        "",
        "        # They should be connected via shared \"neural\" component",
        "        self.assertIn(neural_processing.id, neural_networks.lateral_connections)",
        "        self.assertIn(neural_networks.id, neural_processing.lateral_connections)",
        "        # \"machine learning\" and \"deep learning\" share \"learning\"",
        "        machine_learning = layer1.get_minicolumn(\"machine learning\")",
        "        deep_learning = layer1.get_minicolumn(\"deep learning\")",
        "",
        "        # Verify bigrams exist",
        "        self.assertIsNotNone(machine_learning, \"Bigram 'machine learning' should exist\")",
        "        self.assertIsNotNone(deep_learning, \"Bigram 'deep learning' should exist\")",
        "        # They should be connected via shared \"learning\" component",
        "        self.assertIn(deep_learning.id, machine_learning.lateral_connections)",
        "        self.assertIn(machine_learning.id, deep_learning.lateral_connections)",
        "        # \"machine learning\" and \"learning algorithms\" form a chain",
        "        machine_learning = layer1.get_minicolumn(\"machine learning\")",
        "        learning_algorithms = layer1.get_minicolumn(\"learning algorithms\")",
        "",
        "        # Verify bigrams exist",
        "        self.assertIsNotNone(machine_learning, \"Bigram 'machine learning' should exist\")",
        "        self.assertIsNotNone(learning_algorithms, \"Bigram 'learning algorithms' should exist\")",
        "        # They should be connected via chain relationship",
        "        self.assertIn(learning_algorithms.id, machine_learning.lateral_connections)",
        "        self.assertIn(machine_learning.id, learning_algorithms.lateral_connections)"
      ],
      "lines_removed": [
        "        # \"neural_networks\" and \"neural_processing\" share \"neural\"",
        "        neural_networks = layer1.get_minicolumn(\"neural_networks\")",
        "        neural_processing = layer1.get_minicolumn(\"neural_processing\")",
        "        if neural_networks and neural_processing:",
        "            # They should be connected via shared \"neural\" component",
        "            self.assertIn(neural_processing.id, neural_networks.lateral_connections)",
        "            self.assertIn(neural_networks.id, neural_processing.lateral_connections)",
        "        # \"machine_learning\" and \"deep_learning\" share \"learning\"",
        "        machine_learning = layer1.get_minicolumn(\"machine_learning\")",
        "        deep_learning = layer1.get_minicolumn(\"deep_learning\")",
        "        if machine_learning and deep_learning:",
        "            # They should be connected via shared \"learning\" component",
        "            self.assertIn(deep_learning.id, machine_learning.lateral_connections)",
        "            self.assertIn(machine_learning.id, deep_learning.lateral_connections)",
        "        # \"machine_learning\" and \"learning_algorithms\" form a chain",
        "        machine_learning = layer1.get_minicolumn(\"machine_learning\")",
        "        learning_algorithms = layer1.get_minicolumn(\"learning_algorithms\")",
        "        if machine_learning and learning_algorithms:",
        "            # They should be connected via chain relationship",
        "            self.assertIn(learning_algorithms.id, machine_learning.lateral_connections)",
        "            self.assertIn(machine_learning.id, learning_algorithms.lateral_connections)"
      ],
      "context_before": [
        "        stats = processor.compute_bigram_connections(verbose=False)",
        "",
        "        self.assertIn('connections_created', stats)",
        "        self.assertIn('bigrams', stats)",
        "        self.assertIn('component_connections', stats)",
        "        self.assertIn('chain_connections', stats)",
        "        self.assertIn('cooccurrence_connections', stats)",
        "",
        "    def test_shared_left_component_connection(self):",
        "        \"\"\"Test that bigrams sharing left component are connected.\"\"\""
      ],
      "context_after": [
        "        layer1 = self.processor.get_layer(CorticalLayer.BIGRAMS)",
        "",
        "",
        "",
        "    def test_shared_right_component_connection(self):",
        "        \"\"\"Test that bigrams sharing right component are connected.\"\"\"",
        "        layer1 = self.processor.get_layer(CorticalLayer.BIGRAMS)",
        "",
        "",
        "",
        "    def test_chain_connections(self):",
        "        \"\"\"Test that chain bigrams are connected (right of one = left of other).\"\"\"",
        "        layer1 = self.processor.get_layer(CorticalLayer.BIGRAMS)",
        "",
        "",
        "",
        "    def test_cooccurrence_connections(self):",
        "        \"\"\"Test that bigrams co-occurring in documents are connected.\"\"\"",
        "        layer1 = self.processor.get_layer(CorticalLayer.BIGRAMS)",
        "",
        "        # Bigrams that appear in same documents should have co-occurrence connections",
        "        for bigram in layer1.minicolumns.values():",
        "            if bigram.document_ids and len(bigram.lateral_connections) > 0:",
        "                # If a bigram has connections, some should be from co-occurrence",
        "                # This is a general check that connections exist"
      ],
      "change_type": "modify"
    },
    {
      "file": "tests/test_processor.py",
      "function": "class TestBigramConnections(unittest.TestCase):",
      "start_line": 1734,
      "lines_added": [
        "    def test_component_and_chain_connections_nonzero(self):",
        "        \"\"\"Test that component and chain connections are created (verifies bigram separator fix).\"\"\"",
        "        # Create fresh processor with bigrams that share components",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(",
        "            \"doc1\",",
        "            \"Neural networks are powerful. Neural processing enables deep learning. \"",
        "            \"Machine learning is related to deep learning approaches.\"",
        "        )",
        "        processor.compute_tfidf(verbose=False)",
        "",
        "        stats = processor.compute_bigram_connections(verbose=False)",
        "",
        "        # With the bigram separator fix, component_connections should be > 0",
        "        # because \"neural networks\" and \"neural processing\" share \"neural\"",
        "        self.assertGreater(",
        "            stats['component_connections'], 0,",
        "            \"Component connections should be > 0 when bigrams share components. \"",
        "            \"If this fails, check that bigram.content.split(' ') is used (not split('_')).\"",
        "        )",
        "",
        "    def test_bigram_separator_is_space(self):",
        "        \"\"\"Test that bigrams use space separator (regression test for separator bug).\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(\"doc1\", \"Neural networks process data\")",
        "        processor.compute_all(verbose=False)",
        "",
        "        layer1 = processor.layers[CorticalLayer.BIGRAMS]",
        "",
        "        # Bigrams should use space separators",
        "        self.assertIsNotNone(",
        "            layer1.get_minicolumn(\"neural networks\"),",
        "            \"Bigram 'neural networks' (with space) should exist\"",
        "        )",
        "        self.assertIsNone(",
        "            layer1.get_minicolumn(\"neural_networks\"),",
        "            \"Bigram 'neural_networks' (with underscore) should NOT exist\"",
        "        )",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "        \"\"\"Test that connection weights accumulate for multiple reasons.\"\"\"",
        "        layer1 = self.processor.get_layer(CorticalLayer.BIGRAMS)",
        "",
        "        # Find bigrams that could be connected by multiple reasons",
        "        # (shared component AND co-occurrence)",
        "        for bigram in layer1.minicolumns.values():",
        "            for target_id, weight in bigram.lateral_connections.items():",
        "                # Weights should be positive",
        "                self.assertGreater(weight, 0)",
        ""
      ],
      "context_after": [
        "",
        "class TestSemanticPageRank(unittest.TestCase):",
        "    \"\"\"Test semantic PageRank functionality.\"\"\"",
        "",
        "    @classmethod",
        "    def setUpClass(cls):",
        "        \"\"\"Set up processor with documents for semantic PageRank testing.\"\"\"",
        "        cls.processor = CorticalTextProcessor()",
        "        cls.processor.process_document(",
        "            \"doc1\","
      ],
      "change_type": "add"
    },
    {
      "file": "tests/test_processor.py",
      "function": "class TestAnalogyCompletion(unittest.TestCase):",
      "start_line": 2388,
      "lines_added": [
        "    def test_complete_analogy_simple_uses_bigram_patterns(self):",
        "        \"\"\"Test that analogy completion uses bigram patterns (verifies separator fix).\"\"\"",
        "        # Create processor with documents that should create \"a b\" and \"c d\" bigrams",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(",
        "            \"doc1\",",
        "            \"Neural networks are powerful tools for data analysis. \"",
        "            \"Machine learning helps neural networks improve performance.\"",
        "        )",
        "        processor.process_document(",
        "            \"doc2\",",
        "            \"Neural algorithms process neural signals. \"",
        "            \"Machine processing uses machine algorithms.\"",
        "        )",
        "        processor.compute_all(verbose=False)",
        "",
        "        # Verify bigrams exist with space separators",
        "        layer1 = processor.layers[CorticalLayer.BIGRAMS]",
        "        self.assertIsNotNone(",
        "            layer1.get_minicolumn(\"neural networks\"),",
        "            \"Bigram 'neural networks' should exist for bigram strategy\"",
        "        )",
        "",
        "        # The analogy should work using bigram patterns",
        "        # a:b :: c:? where \"a b\" is a bigram and we look for \"c ?\" bigrams",
        "        results = processor.complete_analogy_simple(\"neural\", \"networks\", \"machine\")",
        "",
        "        # Results should be a list (may be empty if no bigram pattern matches)",
        "        self.assertIsInstance(results, list)",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "        \"\"\"Test that input terms are excluded from simple results.\"\"\"",
        "        results = self.processor.complete_analogy_simple(",
        "            \"neural\", \"networks\", \"machine\"",
        "        )",
        "",
        "        result_terms = [term for term, _ in results]",
        "        self.assertNotIn(\"neural\", result_terms)",
        "        self.assertNotIn(\"networks\", result_terms)",
        "        self.assertNotIn(\"machine\", result_terms)",
        ""
      ],
      "context_after": [
        "",
        "class TestAnalogyHelperFunctions(unittest.TestCase):",
        "    \"\"\"Test analogy helper functions.\"\"\"",
        "",
        "    def test_find_relation_between(self):",
        "        \"\"\"Test finding relations between terms.\"\"\"",
        "        from cortical.query import find_relation_between",
        "",
        "        relations = [",
        "            (\"dog\", \"IsA\", \"animal\", 1.0),"
      ],
      "change_type": "add"
    },
    {
      "file": "tests/test_processor.py",
      "function": "class TestAnalogyHelperFunctions(unittest.TestCase):",
      "start_line": 2445,
      "lines_added": [
        "class TestInputValidation(unittest.TestCase):",
        "    \"\"\"Test input validation for public API methods.\"\"\"",
        "",
        "    def setUp(self):",
        "        self.processor = CorticalTextProcessor()",
        "",
        "    # Tests for process_document validation",
        "    def test_process_document_empty_doc_id(self):",
        "        \"\"\"process_document should reject empty doc_id.\"\"\"",
        "        with self.assertRaises(ValueError) as ctx:",
        "            self.processor.process_document(\"\", \"Some content\")",
        "        self.assertIn(\"doc_id\", str(ctx.exception))",
        "",
        "    def test_process_document_none_doc_id(self):",
        "        \"\"\"process_document should reject None doc_id.\"\"\"",
        "        with self.assertRaises(ValueError) as ctx:",
        "            self.processor.process_document(None, \"Some content\")",
        "        self.assertIn(\"doc_id\", str(ctx.exception))",
        "",
        "    def test_process_document_non_string_doc_id(self):",
        "        \"\"\"process_document should reject non-string doc_id.\"\"\"",
        "        with self.assertRaises(ValueError) as ctx:",
        "            self.processor.process_document(123, \"Some content\")",
        "        self.assertIn(\"doc_id\", str(ctx.exception))",
        "",
        "    def test_process_document_empty_content(self):",
        "        \"\"\"process_document should reject empty content.\"\"\"",
        "        with self.assertRaises(ValueError) as ctx:",
        "            self.processor.process_document(\"doc1\", \"\")",
        "        self.assertIn(\"content\", str(ctx.exception))",
        "",
        "    def test_process_document_whitespace_content(self):",
        "        \"\"\"process_document should reject whitespace-only content.\"\"\"",
        "        with self.assertRaises(ValueError) as ctx:",
        "            self.processor.process_document(\"doc1\", \"   \\n\\t  \")",
        "        self.assertIn(\"content\", str(ctx.exception))",
        "",
        "    def test_process_document_non_string_content(self):",
        "        \"\"\"process_document should reject non-string content.\"\"\"",
        "        with self.assertRaises(ValueError) as ctx:",
        "            self.processor.process_document(\"doc1\", 123)",
        "        self.assertIn(\"content\", str(ctx.exception))",
        "",
        "    def test_process_document_valid_input(self):",
        "        \"\"\"process_document should accept valid input.\"\"\"",
        "        stats = self.processor.process_document(\"doc1\", \"Valid content here.\")",
        "        self.assertIn(\"doc1\", self.processor.documents)",
        "",
        "    # Tests for find_documents_for_query validation",
        "    def test_find_documents_empty_query(self):",
        "        \"\"\"find_documents_for_query should reject empty query.\"\"\"",
        "        self.processor.process_document(\"doc1\", \"Some content here.\")",
        "        self.processor.compute_all()",
        "",
        "        with self.assertRaises(ValueError) as ctx:",
        "            self.processor.find_documents_for_query(\"\")",
        "        self.assertIn(\"query_text\", str(ctx.exception))",
        "",
        "    def test_find_documents_whitespace_query(self):",
        "        \"\"\"find_documents_for_query should reject whitespace-only query.\"\"\"",
        "        self.processor.process_document(\"doc1\", \"Some content here.\")",
        "        self.processor.compute_all()",
        "",
        "        with self.assertRaises(ValueError) as ctx:",
        "            self.processor.find_documents_for_query(\"   \")",
        "        self.assertIn(\"query_text\", str(ctx.exception))",
        "",
        "    def test_find_documents_invalid_top_n(self):",
        "        \"\"\"find_documents_for_query should reject invalid top_n.\"\"\"",
        "        self.processor.process_document(\"doc1\", \"Some content here.\")",
        "        self.processor.compute_all()",
        "",
        "        with self.assertRaises(ValueError) as ctx:",
        "            self.processor.find_documents_for_query(\"content\", top_n=0)",
        "        self.assertIn(\"top_n\", str(ctx.exception))",
        "",
        "        with self.assertRaises(ValueError) as ctx:",
        "            self.processor.find_documents_for_query(\"content\", top_n=-1)",
        "        self.assertIn(\"top_n\", str(ctx.exception))",
        "",
        "    def test_find_documents_valid_input(self):",
        "        \"\"\"find_documents_for_query should accept valid input.\"\"\"",
        "        self.processor.process_document(\"doc1\", \"Neural networks process data.\")",
        "        self.processor.compute_all()",
        "",
        "        results = self.processor.find_documents_for_query(\"neural\", top_n=5)",
        "        self.assertIsInstance(results, list)",
        "",
        "    # Tests for complete_analogy validation",
        "    def test_complete_analogy_empty_term(self):",
        "        \"\"\"complete_analogy should reject empty terms.\"\"\"",
        "        self.processor.process_document(\"doc1\", \"Neural networks and data.\")",
        "        self.processor.compute_all()",
        "",
        "        with self.assertRaises(ValueError) as ctx:",
        "            self.processor.complete_analogy(\"\", \"b\", \"c\")",
        "        self.assertIn(\"term_a\", str(ctx.exception))",
        "",
        "        with self.assertRaises(ValueError) as ctx:",
        "            self.processor.complete_analogy(\"a\", \"\", \"c\")",
        "        self.assertIn(\"term_b\", str(ctx.exception))",
        "",
        "        with self.assertRaises(ValueError) as ctx:",
        "            self.processor.complete_analogy(\"a\", \"b\", \"\")",
        "        self.assertIn(\"term_c\", str(ctx.exception))",
        "",
        "    def test_complete_analogy_invalid_top_n(self):",
        "        \"\"\"complete_analogy should reject invalid top_n.\"\"\"",
        "        self.processor.process_document(\"doc1\", \"Neural networks and data.\")",
        "        self.processor.compute_all()",
        "",
        "        with self.assertRaises(ValueError) as ctx:",
        "            self.processor.complete_analogy(\"a\", \"b\", \"c\", top_n=0)",
        "        self.assertIn(\"top_n\", str(ctx.exception))",
        "",
        "    def test_complete_analogy_valid_input(self):",
        "        \"\"\"complete_analogy should accept valid input.\"\"\"",
        "        self.processor.process_document(\"doc1\", \"Neural networks process data.\")",
        "        self.processor.compute_all()",
        "",
        "        results = self.processor.complete_analogy(\"neural\", \"networks\", \"data\", top_n=3)",
        "        self.assertIsInstance(results, list)",
        "",
        "    # Tests for add_documents_batch validation",
        "    def test_add_documents_batch_not_list(self):",
        "        \"\"\"add_documents_batch should reject non-list input.\"\"\"",
        "        with self.assertRaises(ValueError) as ctx:",
        "            self.processor.add_documents_batch(\"not a list\")",
        "        self.assertIn(\"must be a list\", str(ctx.exception))",
        "",
        "    def test_add_documents_batch_empty_list(self):",
        "        \"\"\"add_documents_batch should reject empty list.\"\"\"",
        "        with self.assertRaises(ValueError) as ctx:",
        "            self.processor.add_documents_batch([])",
        "        self.assertIn(\"must not be empty\", str(ctx.exception))",
        "",
        "    def test_add_documents_batch_invalid_recompute(self):",
        "        \"\"\"add_documents_batch should reject invalid recompute level.\"\"\"",
        "        with self.assertRaises(ValueError) as ctx:",
        "            self.processor.add_documents_batch(",
        "                [(\"doc1\", \"content\")],",
        "                recompute='invalid'",
        "            )",
        "        self.assertIn(\"recompute\", str(ctx.exception))",
        "",
        "    def test_add_documents_batch_invalid_tuple(self):",
        "        \"\"\"add_documents_batch should reject invalid tuple format.\"\"\"",
        "        with self.assertRaises(ValueError) as ctx:",
        "            self.processor.add_documents_batch([(\"only_one_element\",)])",
        "        self.assertIn(\"documents[0]\", str(ctx.exception))",
        "",
        "    def test_add_documents_batch_invalid_doc_id(self):",
        "        \"\"\"add_documents_batch should reject invalid doc_id in tuple.\"\"\"",
        "        with self.assertRaises(ValueError) as ctx:",
        "            self.processor.add_documents_batch([(123, \"content\")])",
        "        self.assertIn(\"doc_id\", str(ctx.exception))",
        "",
        "    def test_add_documents_batch_valid_input(self):",
        "        \"\"\"add_documents_batch should accept valid input.\"\"\"",
        "        docs = [",
        "            (\"doc1\", \"First document.\", None),",
        "            (\"doc2\", \"Second document.\", {\"source\": \"test\"}),",
        "        ]",
        "        stats = self.processor.add_documents_batch(docs, recompute='none', verbose=False)",
        "        self.assertEqual(stats['documents_added'], 2)",
        "",
        "",
        "class TestQueryCache(unittest.TestCase):",
        "    \"\"\"Test query expansion caching functionality.\"\"\"",
        "",
        "    @classmethod",
        "    def setUpClass(cls):",
        "        \"\"\"Set up test processor.\"\"\"",
        "        cls.processor = CorticalTextProcessor()",
        "        cls.processor.process_document(\"doc1\", \"Neural networks process data.\")",
        "        cls.processor.process_document(\"doc2\", \"Machine learning algorithms.\")",
        "        cls.processor.compute_all(verbose=False)",
        "",
        "    def test_expand_query_cached_returns_dict(self):",
        "        \"\"\"expand_query_cached should return a dict.\"\"\"",
        "        result = self.processor.expand_query_cached(\"neural\")",
        "        self.assertIsInstance(result, dict)",
        "",
        "    def test_expand_query_cached_same_result(self):",
        "        \"\"\"expand_query_cached should return same result for same query.\"\"\"",
        "        result1 = self.processor.expand_query_cached(\"neural networks\")",
        "        result2 = self.processor.expand_query_cached(\"neural networks\")",
        "        self.assertEqual(result1, result2)",
        "",
        "    def test_expand_query_cached_different_params(self):",
        "        \"\"\"Different parameters should use different cache entries.\"\"\"",
        "        result1 = self.processor.expand_query_cached(\"neural\", use_code_concepts=False)",
        "        result2 = self.processor.expand_query_cached(\"neural\", use_code_concepts=True)",
        "        # Results may differ (code concepts add synonyms)",
        "        self.assertIsInstance(result1, dict)",
        "        self.assertIsInstance(result2, dict)",
        "",
        "    def test_clear_query_cache(self):",
        "        \"\"\"clear_query_cache should return count and clear cache.\"\"\"",
        "        # Populate cache",
        "        self.processor.expand_query_cached(\"test1\")",
        "        self.processor.expand_query_cached(\"test2\")",
        "",
        "        # Clear and verify",
        "        count = self.processor.clear_query_cache()",
        "        self.assertGreaterEqual(count, 2)",
        "",
        "        # Verify cache is empty",
        "        count2 = self.processor.clear_query_cache()",
        "        self.assertEqual(count2, 0)",
        "",
        "    def test_set_query_cache_size(self):",
        "        \"\"\"set_query_cache_size should update max size.\"\"\"",
        "        self.processor.set_query_cache_size(50)",
        "        self.assertEqual(self.processor._query_cache_max_size, 50)",
        "",
        "        # Reset to default",
        "        self.processor.set_query_cache_size(100)",
        "",
        "    def test_set_query_cache_size_invalid(self):",
        "        \"\"\"set_query_cache_size should reject invalid sizes.\"\"\"",
        "        with self.assertRaises(ValueError):",
        "            self.processor.set_query_cache_size(0)",
        "        with self.assertRaises(ValueError):",
        "            self.processor.set_query_cache_size(-10)",
        "",
        "    def test_cache_lru_eviction(self):",
        "        \"\"\"Cache should evict oldest entries when full.\"\"\"",
        "        self.processor.clear_query_cache()",
        "        self.processor.set_query_cache_size(3)",
        "",
        "        # Fill cache",
        "        self.processor.expand_query_cached(\"query1\")",
        "        self.processor.expand_query_cached(\"query2\")",
        "        self.processor.expand_query_cached(\"query3\")",
        "",
        "        # Add another - should evict oldest",
        "        self.processor.expand_query_cached(\"query4\")",
        "",
        "        # Cache should still be at max size",
        "        self.assertLessEqual(len(self.processor._query_expansion_cache), 3)",
        "",
        "        # Reset",
        "        self.processor.set_query_cache_size(100)",
        "        self.processor.clear_query_cache()",
        "",
        "    def test_compute_all_invalidates_cache(self):",
        "        \"\"\"compute_all should clear the query cache.\"\"\"",
        "        # Populate cache",
        "        self.processor.expand_query_cached(\"cached_query\")",
        "        self.assertGreater(len(self.processor._query_expansion_cache), 0)",
        "",
        "        # Recompute",
        "        self.processor.compute_all(verbose=False)",
        "",
        "        # Cache should be cleared",
        "        self.assertEqual(len(self.processor._query_expansion_cache), 0)",
        "",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "",
        "        relations = [",
        "            (\"dog\", \"HasProperty\", \"loyal\", 1.0),",
        "            (\"dog\", \"HasProperty\", \"friendly\", 0.8),",
        "        ]",
        "",
        "        result = find_terms_with_relation(\"dog\", \"HasProperty\", relations, direction='forward')",
        "        self.assertEqual(len(result), 2)",
        "",
        ""
      ],
      "context_after": [
        "if __name__ == \"__main__\":",
        "    unittest.main(verbosity=2)"
      ],
      "change_type": "add"
    },
    {
      "file": "tests/test_query.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "\"\"\"",
        "Query Module Tests",
        "==================",
        "",
        "Comprehensive tests for cortical/query.py functions.",
        "",
        "Tests cover:",
        "- Query expansion (lateral, semantic, multihop)",
        "- Relation path scoring",
        "- Chunking and chunk scoring",
        "- Batch operations",
        "- Relation discovery functions",
        "- Analogy completion",
        "\"\"\"",
        "",
        "import unittest",
        "from typing import Dict, List, Tuple",
        "",
        "from cortical import CorticalTextProcessor",
        "from cortical.layers import CorticalLayer, HierarchicalLayer",
        "from cortical.tokenizer import Tokenizer",
        "from cortical.query import (",
        "    expand_query,",
        "    expand_query_multihop,",
        "    expand_query_semantic,",
        "    get_expanded_query_terms,",
        "    score_relation_path,",
        "    create_chunks,",
        "    score_chunk,",
        "    find_documents_for_query,",
        "    find_passages_for_query,",
        "    find_documents_batch,",
        "    find_passages_batch,",
        "    find_relevant_concepts,",
        "    find_relation_between,",
        "    find_terms_with_relation,",
        "    complete_analogy,",
        "    complete_analogy_simple,",
        "    query_with_spreading_activation,",
        "    VALID_RELATION_CHAINS,",
        ")",
        "",
        "",
        "class TestScoreRelationPath(unittest.TestCase):",
        "    \"\"\"Test relation path scoring.\"\"\"",
        "",
        "    def test_empty_path(self):",
        "        \"\"\"Empty path should return 1.0.\"\"\"",
        "        self.assertEqual(score_relation_path([]), 1.0)",
        "",
        "    def test_single_relation(self):",
        "        \"\"\"Single relation path should return 1.0.\"\"\"",
        "        self.assertEqual(score_relation_path(['IsA']), 1.0)",
        "        self.assertEqual(score_relation_path(['HasProperty']), 1.0)",
        "",
        "    def test_valid_chain(self):",
        "        \"\"\"Valid chain should return high score.\"\"\"",
        "        # IsA → HasProperty is typically valid",
        "        score = score_relation_path(['IsA', 'HasProperty'])",
        "        self.assertGreater(score, 0.5)",
        "",
        "    def test_long_path_degrades(self):",
        "        \"\"\"Longer paths should have lower scores due to multiplication.\"\"\"",
        "        score_2 = score_relation_path(['IsA', 'HasProperty'])",
        "        score_3 = score_relation_path(['IsA', 'HasProperty', 'RelatedTo'])",
        "        self.assertLessEqual(score_3, score_2)",
        "",
        "    def test_valid_relation_chains_constant(self):",
        "        \"\"\"VALID_RELATION_CHAINS should be a non-empty dict.\"\"\"",
        "        self.assertIsInstance(VALID_RELATION_CHAINS, dict)",
        "        self.assertGreater(len(VALID_RELATION_CHAINS), 0)",
        "",
        "",
        "class TestCreateChunks(unittest.TestCase):",
        "    \"\"\"Test text chunking.\"\"\"",
        "",
        "    def test_empty_text(self):",
        "        \"\"\"Empty text should return empty list.\"\"\"",
        "        self.assertEqual(create_chunks(\"\"), [])",
        "",
        "    def test_short_text(self):",
        "        \"\"\"Text shorter than chunk_size should return single chunk.\"\"\"",
        "        text = \"Short text.\"",
        "        chunks = create_chunks(text, chunk_size=100, overlap=20)",
        "        self.assertEqual(len(chunks), 1)",
        "        self.assertEqual(chunks[0][0], text)",
        "        self.assertEqual(chunks[0][1], 0)  # start",
        "        self.assertEqual(chunks[0][2], len(text))  # end",
        "",
        "    def test_chunk_overlap(self):",
        "        \"\"\"Chunks should overlap by specified amount.\"\"\"",
        "        text = \"A\" * 100",
        "        chunks = create_chunks(text, chunk_size=50, overlap=10)",
        "        # With chunk_size=50 and overlap=10, stride=40",
        "        # Chunks at: 0-50, 40-90, 80-100",
        "        self.assertGreater(len(chunks), 1)",
        "        # First chunk ends at 50, second starts at 40",
        "        if len(chunks) >= 2:",
        "            first_end = chunks[0][2]",
        "            second_start = chunks[1][1]",
        "            self.assertLess(second_start, first_end)  # Overlap exists",
        "",
        "    def test_chunk_boundaries(self):",
        "        \"\"\"Chunk boundaries should be valid.\"\"\"",
        "        text = \"Hello world, this is a test of chunking functionality.\"",
        "        chunks = create_chunks(text, chunk_size=20, overlap=5)",
        "",
        "        for chunk_text, start, end in chunks:",
        "            self.assertEqual(chunk_text, text[start:end])",
        "            self.assertLessEqual(end, len(text))",
        "",
        "    def test_no_overlap(self):",
        "        \"\"\"Chunks with zero overlap should not overlap.\"\"\"",
        "        text = \"A\" * 100",
        "        chunks = create_chunks(text, chunk_size=25, overlap=0)",
        "        # Should have exactly 4 chunks",
        "        self.assertEqual(len(chunks), 4)",
        "        # Check no overlap",
        "        for i in range(len(chunks) - 1):",
        "            self.assertEqual(chunks[i][2], chunks[i + 1][1])",
        "",
        "    def test_invalid_chunk_size_zero(self):",
        "        \"\"\"Chunk size of zero should raise ValueError.\"\"\"",
        "        with self.assertRaises(ValueError) as ctx:",
        "            create_chunks(\"test\", chunk_size=0)",
        "        self.assertIn(\"chunk_size\", str(ctx.exception))",
        "",
        "    def test_invalid_chunk_size_negative(self):",
        "        \"\"\"Negative chunk size should raise ValueError.\"\"\"",
        "        with self.assertRaises(ValueError) as ctx:",
        "            create_chunks(\"test\", chunk_size=-10)",
        "        self.assertIn(\"chunk_size\", str(ctx.exception))",
        "",
        "    def test_invalid_overlap_negative(self):",
        "        \"\"\"Negative overlap should raise ValueError.\"\"\"",
        "        with self.assertRaises(ValueError) as ctx:",
        "            create_chunks(\"test\", chunk_size=50, overlap=-5)",
        "        self.assertIn(\"overlap\", str(ctx.exception))",
        "",
        "    def test_invalid_overlap_greater_than_chunk_size(self):",
        "        \"\"\"Overlap >= chunk_size should raise ValueError.\"\"\"",
        "        with self.assertRaises(ValueError) as ctx:",
        "            create_chunks(\"test\", chunk_size=50, overlap=50)",
        "        self.assertIn(\"overlap\", str(ctx.exception))",
        "",
        "        with self.assertRaises(ValueError):",
        "            create_chunks(\"test\", chunk_size=50, overlap=100)",
        "",
        "",
        "class TestFindRelationBetween(unittest.TestCase):",
        "    \"\"\"Test finding relations between terms.\"\"\"",
        "",
        "    def setUp(self):",
        "        \"\"\"Set up sample relations.\"\"\"",
        "        self.relations = [",
        "            (\"dog\", \"IsA\", \"animal\", 1.0),",
        "            (\"cat\", \"IsA\", \"animal\", 1.0),",
        "            (\"dog\", \"HasProperty\", \"loyal\", 0.8),",
        "            (\"neural\", \"RelatedTo\", \"networks\", 0.7),",
        "            (\"machine\", \"RelatedTo\", \"learning\", 0.9),",
        "        ]",
        "",
        "    def test_forward_relation(self):",
        "        \"\"\"Find relation in forward direction.\"\"\"",
        "        results = find_relation_between(\"dog\", \"animal\", self.relations)",
        "        self.assertEqual(len(results), 1)",
        "        self.assertEqual(results[0][0], \"IsA\")",
        "        self.assertEqual(results[0][1], 1.0)",
        "",
        "    def test_reverse_relation(self):",
        "        \"\"\"Find relation in reverse direction with penalty.\"\"\"",
        "        results = find_relation_between(\"animal\", \"dog\", self.relations)",
        "        self.assertEqual(len(results), 1)",
        "        self.assertEqual(results[0][0], \"IsA\")",
        "        self.assertLess(results[0][1], 1.0)  # Penalty applied",
        "",
        "    def test_no_relation(self):",
        "        \"\"\"Return empty list when no relation exists.\"\"\"",
        "        results = find_relation_between(\"dog\", \"neural\", self.relations)",
        "        self.assertEqual(results, [])",
        "",
        "    def test_multiple_relations(self):",
        "        \"\"\"Multiple relations between same terms.\"\"\"",
        "        relations = [",
        "            (\"dog\", \"IsA\", \"animal\", 1.0),",
        "            (\"dog\", \"RelatedTo\", \"animal\", 0.5),",
        "        ]",
        "        results = find_relation_between(\"dog\", \"animal\", relations)",
        "        self.assertEqual(len(results), 2)",
        "        # Should be sorted by weight",
        "        self.assertEqual(results[0][0], \"IsA\")",
        "",
        "",
        "class TestFindTermsWithRelation(unittest.TestCase):",
        "    \"\"\"Test finding terms with specific relation.\"\"\"",
        "",
        "    def setUp(self):",
        "        \"\"\"Set up sample relations.\"\"\"",
        "        self.relations = [",
        "            (\"dog\", \"IsA\", \"animal\", 1.0),",
        "            (\"cat\", \"IsA\", \"animal\", 0.9),",
        "            (\"bird\", \"IsA\", \"animal\", 0.8),",
        "            (\"dog\", \"HasProperty\", \"loyal\", 0.8),",
        "            (\"cat\", \"HasProperty\", \"independent\", 0.7),",
        "        ]",
        "",
        "    def test_forward_direction(self):",
        "        \"\"\"Find terms in forward direction (term → x).\"\"\"",
        "        results = find_terms_with_relation(",
        "            \"dog\", \"IsA\", self.relations, direction='forward'",
        "        )",
        "        self.assertEqual(len(results), 1)",
        "        self.assertEqual(results[0][0], \"animal\")",
        "",
        "    def test_backward_direction(self):",
        "        \"\"\"Find terms in backward direction (x → term).\"\"\"",
        "        results = find_terms_with_relation(",
        "            \"animal\", \"IsA\", self.relations, direction='backward'",
        "        )",
        "        self.assertEqual(len(results), 3)  # dog, cat, bird",
        "        terms = [r[0] for r in results]",
        "        self.assertIn(\"dog\", terms)",
        "        self.assertIn(\"cat\", terms)",
        "        self.assertIn(\"bird\", terms)",
        "",
        "    def test_no_matching_relation(self):",
        "        \"\"\"Return empty when no matching relation type.\"\"\"",
        "        results = find_terms_with_relation(",
        "            \"dog\", \"PartOf\", self.relations, direction='forward'",
        "        )",
        "        self.assertEqual(results, [])",
        "",
        "    def test_results_sorted_by_weight(self):",
        "        \"\"\"Results should be sorted by weight descending.\"\"\"",
        "        results = find_terms_with_relation(",
        "            \"animal\", \"IsA\", self.relations, direction='backward'",
        "        )",
        "        weights = [r[1] for r in results]",
        "        self.assertEqual(weights, sorted(weights, reverse=True))",
        "",
        "",
        "class TestExpandQuery(unittest.TestCase):",
        "    \"\"\"Test basic query expansion.\"\"\"",
        "",
        "    @classmethod",
        "    def setUpClass(cls):",
        "        \"\"\"Set up processor with documents.\"\"\"",
        "        cls.processor = CorticalTextProcessor()",
        "        cls.processor.process_document(",
        "            \"doc1\",",
        "            \"Neural networks are fundamental to machine learning. \"",
        "            \"Deep learning uses neural architectures for complex tasks.\"",
        "        )",
        "        cls.processor.process_document(",
        "            \"doc2\",",
        "            \"Machine learning algorithms process data patterns. \"",
        "            \"Neural models learn from training examples.\"",
        "        )",
        "        cls.processor.compute_all(verbose=False)",
        "",
        "    def test_expand_query_returns_dict(self):",
        "        \"\"\"expand_query should return a dictionary.\"\"\"",
        "        result = expand_query(",
        "            \"neural networks\",",
        "            self.processor.layers,",
        "            self.processor.tokenizer",
        "        )",
        "        self.assertIsInstance(result, dict)",
        "",
        "    def test_expand_query_includes_original_terms(self):",
        "        \"\"\"Expanded query should include original terms.\"\"\"",
        "        result = expand_query(",
        "            \"neural learning\",",
        "            self.processor.layers,",
        "            self.processor.tokenizer",
        "        )",
        "        # Original terms should have high weight",
        "        self.assertIn(\"neural\", result)",
        "        self.assertIn(\"learning\", result)",
        "",
        "    def test_expand_query_unknown_terms(self):",
        "        \"\"\"Unknown terms should be handled gracefully.\"\"\"",
        "        result = expand_query(",
        "            \"xyznonexistent\",",
        "            self.processor.layers,",
        "            self.processor.tokenizer",
        "        )",
        "        self.assertIsInstance(result, dict)",
        "",
        "",
        "class TestExpandQueryMultihop(unittest.TestCase):",
        "    \"\"\"Test multi-hop query expansion.\"\"\"",
        "",
        "    @classmethod",
        "    def setUpClass(cls):",
        "        \"\"\"Set up processor with semantic relations.\"\"\"",
        "        cls.processor = CorticalTextProcessor()",
        "        cls.processor.process_document(",
        "            \"doc1\",",
        "            \"Dogs are loyal animals. Cats are independent pets.\"",
        "        )",
        "        cls.processor.process_document(",
        "            \"doc2\",",
        "            \"Animals need food and water. Pets require care.\"",
        "        )",
        "        cls.processor.compute_all(verbose=False)",
        "        cls.processor.extract_corpus_semantics(verbose=False)",
        "",
        "    def test_multihop_returns_dict(self):",
        "        \"\"\"expand_query_multihop should return a dictionary.\"\"\"",
        "        result = expand_query_multihop(",
        "            \"dogs\",",
        "            self.processor.layers,",
        "            self.processor.tokenizer,",
        "            self.processor.semantic_relations",
        "        )",
        "        self.assertIsInstance(result, dict)",
        "",
        "    def test_multihop_with_max_hops(self):",
        "        \"\"\"max_hops should limit expansion depth.\"\"\"",
        "        result_1 = expand_query_multihop(",
        "            \"dogs\",",
        "            self.processor.layers,",
        "            self.processor.tokenizer,",
        "            self.processor.semantic_relations,",
        "            max_hops=1",
        "        )",
        "        result_2 = expand_query_multihop(",
        "            \"dogs\",",
        "            self.processor.layers,",
        "            self.processor.tokenizer,",
        "            self.processor.semantic_relations,",
        "            max_hops=2",
        "        )",
        "        # More hops could mean more terms (or same if no valid chains)",
        "        self.assertIsInstance(result_1, dict)",
        "        self.assertIsInstance(result_2, dict)",
        "",
        "",
        "class TestGetExpandedQueryTerms(unittest.TestCase):",
        "    \"\"\"Test the unified query expansion helper.\"\"\"",
        "",
        "    @classmethod",
        "    def setUpClass(cls):",
        "        \"\"\"Set up processor.\"\"\"",
        "        cls.processor = CorticalTextProcessor()",
        "        cls.processor.process_document(",
        "            \"doc1\",",
        "            \"Neural networks process information efficiently. \"",
        "            \"Machine learning improves with data.\"",
        "        )",
        "        cls.processor.compute_all(verbose=False)",
        "        cls.processor.extract_corpus_semantics(verbose=False)",
        "",
        "    def test_get_expanded_returns_dict(self):",
        "        \"\"\"get_expanded_query_terms should return dict.\"\"\"",
        "        result = get_expanded_query_terms(",
        "            \"neural\",",
        "            self.processor.layers,",
        "            self.processor.tokenizer,",
        "            self.processor.semantic_relations",
        "        )",
        "        self.assertIsInstance(result, dict)",
        "",
        "    def test_max_expansions_limits_results(self):",
        "        \"\"\"max_expansions should limit number of terms.\"\"\"",
        "        result = get_expanded_query_terms(",
        "            \"neural networks machine\",",
        "            self.processor.layers,",
        "            self.processor.tokenizer,",
        "            self.processor.semantic_relations,",
        "            max_expansions=5",
        "        )",
        "        # Should have at most 5 expansion terms + original terms",
        "        self.assertIsInstance(result, dict)",
        "",
        "    def test_semantic_discount_affects_weights(self):",
        "        \"\"\"semantic_discount should reduce semantic expansion weights.\"\"\"",
        "        result_high = get_expanded_query_terms(",
        "            \"neural\",",
        "            self.processor.layers,",
        "            self.processor.tokenizer,",
        "            self.processor.semantic_relations,",
        "            semantic_discount=1.0",
        "        )",
        "        result_low = get_expanded_query_terms(",
        "            \"neural\",",
        "            self.processor.layers,",
        "            self.processor.tokenizer,",
        "            self.processor.semantic_relations,",
        "            semantic_discount=0.1",
        "        )",
        "        self.assertIsInstance(result_high, dict)",
        "        self.assertIsInstance(result_low, dict)",
        "",
        "",
        "class TestFindDocumentsForQuery(unittest.TestCase):",
        "    \"\"\"Test document retrieval.\"\"\"",
        "",
        "    @classmethod",
        "    def setUpClass(cls):",
        "        \"\"\"Set up processor with documents.\"\"\"",
        "        cls.processor = CorticalTextProcessor()",
        "        cls.processor.process_document(",
        "            \"neural_doc\",",
        "            \"Neural networks are powerful models for pattern recognition. \"",
        "            \"Deep learning architectures use multiple neural layers.\"",
        "        )",
        "        cls.processor.process_document(",
        "            \"ml_doc\",",
        "            \"Machine learning algorithms learn from data. \"",
        "            \"Supervised learning requires labeled examples.\"",
        "        )",
        "        cls.processor.process_document(",
        "            \"unrelated_doc\",",
        "            \"The weather today is sunny with clear skies. \"",
        "            \"Tomorrow expects rain in the afternoon.\"",
        "        )",
        "        cls.processor.compute_all(verbose=False)",
        "",
        "    def test_returns_list_of_tuples(self):",
        "        \"\"\"Should return list of (doc_id, score) tuples.\"\"\"",
        "        results = find_documents_for_query(",
        "            \"neural networks\",",
        "            self.processor.layers,",
        "            self.processor.tokenizer",
        "        )",
        "        self.assertIsInstance(results, list)",
        "        if results:",
        "            self.assertEqual(len(results[0]), 2)",
        "            self.assertIsInstance(results[0][0], str)",
        "            self.assertIsInstance(results[0][1], float)",
        "",
        "    def test_relevant_docs_ranked_higher(self):",
        "        \"\"\"Relevant documents should be ranked higher.\"\"\"",
        "        results = find_documents_for_query(",
        "            \"neural networks deep learning\",",
        "            self.processor.layers,",
        "            self.processor.tokenizer,",
        "            top_n=3",
        "        )",
        "        if len(results) >= 2:",
        "            doc_ids = [r[0] for r in results]",
        "            # neural_doc should rank higher than unrelated_doc",
        "            if \"neural_doc\" in doc_ids and \"unrelated_doc\" in doc_ids:",
        "                self.assertLess(",
        "                    doc_ids.index(\"neural_doc\"),",
        "                    doc_ids.index(\"unrelated_doc\")",
        "                )",
        "",
        "    def test_top_n_limits_results(self):",
        "        \"\"\"top_n should limit number of results.\"\"\"",
        "        results = find_documents_for_query(",
        "            \"learning\",",
        "            self.processor.layers,",
        "            self.processor.tokenizer,",
        "            top_n=1",
        "        )",
        "        self.assertLessEqual(len(results), 1)",
        "",
        "",
        "class TestFindDocumentsBatch(unittest.TestCase):",
        "    \"\"\"Test batch document retrieval.\"\"\"",
        "",
        "    @classmethod",
        "    def setUpClass(cls):",
        "        \"\"\"Set up processor with documents.\"\"\"",
        "        cls.processor = CorticalTextProcessor()",
        "        cls.processor.process_document(\"doc1\", \"Neural networks learn patterns.\")",
        "        cls.processor.process_document(\"doc2\", \"Machine learning uses algorithms.\")",
        "        cls.processor.compute_all(verbose=False)",
        "",
        "    def test_batch_returns_list_of_lists(self):",
        "        \"\"\"Should return list of result lists.\"\"\"",
        "        queries = [\"neural\", \"machine\"]",
        "        results = find_documents_batch(",
        "            queries,",
        "            self.processor.layers,",
        "            self.processor.tokenizer",
        "        )",
        "        self.assertIsInstance(results, list)",
        "        self.assertEqual(len(results), 2)",
        "        for result_list in results:",
        "            self.assertIsInstance(result_list, list)",
        "",
        "    def test_batch_empty_queries(self):",
        "        \"\"\"Empty query list should return empty list.\"\"\"",
        "        results = find_documents_batch(",
        "            [],",
        "            self.processor.layers,",
        "            self.processor.tokenizer",
        "        )",
        "        self.assertEqual(results, [])",
        "",
        "",
        "class TestFindPassagesForQuery(unittest.TestCase):",
        "    \"\"\"Test passage retrieval.\"\"\"",
        "",
        "    @classmethod",
        "    def setUpClass(cls):",
        "        \"\"\"Set up processor with documents.\"\"\"",
        "        cls.processor = CorticalTextProcessor()",
        "        cls.processor.process_document(",
        "            \"doc1\",",
        "            \"Neural networks are computational models. \"",
        "            \"They process data through layers of neurons. \"",
        "            \"Deep learning uses many layers for complex tasks.\"",
        "        )",
        "        cls.processor.compute_all(verbose=False)",
        "",
        "    def test_returns_list_of_tuples(self):",
        "        \"\"\"Should return list with passage info.\"\"\"",
        "        results = find_passages_for_query(",
        "            \"neural networks\",",
        "            self.processor.layers,",
        "            self.processor.tokenizer,",
        "            self.processor.documents",
        "        )",
        "        self.assertIsInstance(results, list)",
        "        if results:",
        "            # Should have (doc_id, passage_text, start, end, score)",
        "            self.assertEqual(len(results[0]), 5)",
        "",
        "    def test_passage_contains_query_terms(self):",
        "        \"\"\"Returned passages should be relevant to query.\"\"\"",
        "        results = find_passages_for_query(",
        "            \"neural networks\",",
        "            self.processor.layers,",
        "            self.processor.tokenizer,",
        "            self.processor.documents,",
        "            top_n=1",
        "        )",
        "        # Should return at least one passage",
        "        self.assertIsInstance(results, list)",
        "        # If results exist, check format is correct",
        "        if results:",
        "            doc_id, passage_text, start, end, score = results[0]",
        "            self.assertIsInstance(doc_id, str)",
        "            self.assertIsInstance(passage_text, str)",
        "            self.assertIsInstance(score, float)",
        "            self.assertGreater(len(passage_text), 0)",
        "",
        "",
        "class TestFindRelevantConcepts(unittest.TestCase):",
        "    \"\"\"Test concept filtering for RAG.\"\"\"",
        "",
        "    @classmethod",
        "    def setUpClass(cls):",
        "        \"\"\"Set up processor with concepts.\"\"\"",
        "        cls.processor = CorticalTextProcessor()",
        "        cls.processor.process_document(",
        "            \"doc1\",",
        "            \"Neural networks process information. Machine learning improves results.\"",
        "        )",
        "        cls.processor.process_document(",
        "            \"doc2\",",
        "            \"Deep learning architectures use neural layers. Data processing is key.\"",
        "        )",
        "        cls.processor.compute_all(verbose=False)",
        "",
        "    def test_returns_list(self):",
        "        \"\"\"Should return list of concept info.\"\"\"",
        "        # find_relevant_concepts takes query_terms dict, not string",
        "        query_terms = {\"neural\": 1.0, \"learning\": 0.8}",
        "        result = find_relevant_concepts(",
        "            query_terms,",
        "            self.processor.layers",
        "        )",
        "        self.assertIsInstance(result, list)",
        "",
        "    def test_top_n_limits_results(self):",
        "        \"\"\"top_n should limit results.\"\"\"",
        "        query_terms = {\"neural\": 1.0, \"learning\": 0.8}",
        "        result = find_relevant_concepts(",
        "            query_terms,",
        "            self.processor.layers,",
        "            top_n=2",
        "        )",
        "        self.assertLessEqual(len(result), 2)",
        "",
        "",
        "class TestCompleteAnalogy(unittest.TestCase):",
        "    \"\"\"Test analogy completion functions.\"\"\"",
        "",
        "    @classmethod",
        "    def setUpClass(cls):",
        "        \"\"\"Set up processor for analogy tests.\"\"\"",
        "        cls.processor = CorticalTextProcessor()",
        "        cls.processor.process_document(",
        "            \"doc1\",",
        "            \"Neural networks are like brain structures. \"",
        "            \"Machine learning uses algorithms for patterns. \"",
        "            \"Deep learning processes complex data.\"",
        "        )",
        "        cls.processor.process_document(",
        "            \"doc2\",",
        "            \"Data science analyzes information. \"",
        "            \"Neural processing enables artificial intelligence.\"",
        "        )",
        "        cls.processor.compute_all(verbose=False)",
        "        cls.processor.extract_corpus_semantics(verbose=False)",
        "",
        "    def test_complete_analogy_returns_list(self):",
        "        \"\"\"complete_analogy should return list.\"\"\"",
        "        results = complete_analogy(",
        "            \"neural\", \"networks\", \"machine\",",
        "            self.processor.layers,",
        "            self.processor.semantic_relations",
        "        )",
        "        self.assertIsInstance(results, list)",
        "",
        "    def test_complete_analogy_excludes_input(self):",
        "        \"\"\"Input terms should not appear in results.\"\"\"",
        "        results = complete_analogy(",
        "            \"neural\", \"networks\", \"machine\",",
        "            self.processor.layers,",
        "            self.processor.semantic_relations",
        "        )",
        "        result_terms = [r[0] for r in results]",
        "        self.assertNotIn(\"neural\", result_terms)",
        "        self.assertNotIn(\"networks\", result_terms)",
        "        self.assertNotIn(\"machine\", result_terms)",
        "",
        "    def test_complete_analogy_simple_returns_list(self):",
        "        \"\"\"complete_analogy_simple should return list.\"\"\"",
        "        results = complete_analogy_simple(",
        "            \"neural\", \"networks\", \"machine\",",
        "            self.processor.layers,",
        "            self.processor.tokenizer",
        "        )",
        "        self.assertIsInstance(results, list)",
        "",
        "    def test_complete_analogy_simple_format(self):",
        "        \"\"\"Simple analogy results should be (term, score) tuples.\"\"\"",
        "        results = complete_analogy_simple(",
        "            \"neural\", \"networks\", \"learning\",",
        "            self.processor.layers,",
        "            self.processor.tokenizer,",
        "            top_n=3",
        "        )",
        "        for result in results:",
        "            self.assertEqual(len(result), 2)",
        "            self.assertIsInstance(result[0], str)",
        "            self.assertIsInstance(result[1], float)",
        "",
        "",
        "class TestQueryWithSpreadingActivation(unittest.TestCase):",
        "    \"\"\"Test spreading activation search.\"\"\"",
        "",
        "    @classmethod",
        "    def setUpClass(cls):",
        "        \"\"\"Set up processor.\"\"\"",
        "        cls.processor = CorticalTextProcessor()",
        "        cls.processor.process_document(",
        "            \"doc1\",",
        "            \"Neural networks process signals. Deep learning improves accuracy.\"",
        "        )",
        "        cls.processor.compute_all(verbose=False)",
        "",
        "    def test_returns_list(self):",
        "        \"\"\"Should return list of results.\"\"\"",
        "        results = query_with_spreading_activation(",
        "            \"neural\",",
        "            self.processor.layers,",
        "            self.processor.tokenizer",
        "        )",
        "        self.assertIsInstance(results, list)",
        "",
        "    def test_max_expansions_parameter(self):",
        "        \"\"\"max_expansions parameter should be accepted.\"\"\"",
        "        results = query_with_spreading_activation(",
        "            \"neural\",",
        "            self.processor.layers,",
        "            self.processor.tokenizer,",
        "            max_expansions=5",
        "        )",
        "        self.assertIsInstance(results, list)",
        "",
        "",
        "class TestScoreChunk(unittest.TestCase):",
        "    \"\"\"Test chunk scoring.\"\"\"",
        "",
        "    @classmethod",
        "    def setUpClass(cls):",
        "        \"\"\"Set up processor.\"\"\"",
        "        cls.processor = CorticalTextProcessor()",
        "        cls.processor.process_document(",
        "            \"doc1\",",
        "            \"Neural networks are powerful tools for data analysis.\"",
        "        )",
        "        cls.processor.compute_all(verbose=False)",
        "",
        "    def test_score_chunk_returns_float(self):",
        "        \"\"\"score_chunk should return a float.\"\"\"",
        "        query_terms = {\"neural\": 1.0, \"networks\": 0.8}",
        "        layer0 = self.processor.layers[CorticalLayer.TOKENS]",
        "",
        "        score = score_chunk(",
        "            \"Neural networks process data\",",
        "            query_terms,",
        "            layer0,",
        "            self.processor.tokenizer",
        "        )",
        "        self.assertIsInstance(score, float)",
        "",
        "    def test_relevant_chunk_higher_score(self):",
        "        \"\"\"Chunks with query terms should score higher.\"\"\"",
        "        query_terms = {\"neural\": 1.0, \"networks\": 0.8}",
        "        layer0 = self.processor.layers[CorticalLayer.TOKENS]",
        "",
        "        relevant_score = score_chunk(",
        "            \"Neural networks are amazing\",",
        "            query_terms,",
        "            layer0,",
        "            self.processor.tokenizer",
        "        )",
        "        irrelevant_score = score_chunk(",
        "            \"Weather is nice today\",",
        "            query_terms,",
        "            layer0,",
        "            self.processor.tokenizer",
        "        )",
        "        self.assertGreaterEqual(relevant_score, irrelevant_score)",
        "",
        "    def test_empty_chunk(self):",
        "        \"\"\"Empty chunk should return 0.\"\"\"",
        "        query_terms = {\"neural\": 1.0}",
        "        layer0 = self.processor.layers[CorticalLayer.TOKENS]",
        "",
        "        score = score_chunk(",
        "            \"\",",
        "            query_terms,",
        "            layer0,",
        "            self.processor.tokenizer",
        "        )",
        "        self.assertEqual(score, 0.0)",
        "",
        "",
        "class TestChunkScoringOptimization(unittest.TestCase):",
        "    \"\"\"Test optimized chunk scoring functions.\"\"\"",
        "",
        "    @classmethod",
        "    def setUpClass(cls):",
        "        \"\"\"Set up processor.\"\"\"",
        "        cls.processor = CorticalTextProcessor()",
        "        cls.processor.process_document(",
        "            \"doc1\",",
        "            \"Neural networks are powerful tools for data analysis.\"",
        "        )",
        "        cls.processor.compute_all(verbose=False)",
        "",
        "    def test_precompute_term_cols_returns_dict(self):",
        "        \"\"\"precompute_term_cols should return dict of Minicolumns.\"\"\"",
        "        from cortical.query import precompute_term_cols",
        "        query_terms = {\"neural\": 1.0, \"networks\": 0.8}",
        "        layer0 = self.processor.layers[CorticalLayer.TOKENS]",
        "",
        "        term_cols = precompute_term_cols(query_terms, layer0)",
        "",
        "        self.assertIsInstance(term_cols, dict)",
        "        self.assertIn(\"neural\", term_cols)",
        "        self.assertIn(\"networks\", term_cols)",
        "",
        "    def test_precompute_term_cols_excludes_unknown(self):",
        "        \"\"\"precompute_term_cols should exclude terms not in corpus.\"\"\"",
        "        from cortical.query import precompute_term_cols",
        "        query_terms = {\"neural\": 1.0, \"xyz_unknown\": 0.5}",
        "        layer0 = self.processor.layers[CorticalLayer.TOKENS]",
        "",
        "        term_cols = precompute_term_cols(query_terms, layer0)",
        "",
        "        self.assertIn(\"neural\", term_cols)",
        "        self.assertNotIn(\"xyz_unknown\", term_cols)",
        "",
        "    def test_score_chunk_fast_matches_regular(self):",
        "        \"\"\"score_chunk_fast should produce same results as score_chunk.\"\"\"",
        "        from cortical.query import precompute_term_cols, score_chunk_fast",
        "        query_terms = {\"neural\": 1.0, \"networks\": 0.8}",
        "        layer0 = self.processor.layers[CorticalLayer.TOKENS]",
        "        chunk_text = \"Neural networks process data\"",
        "",
        "        # Regular score",
        "        regular_score = score_chunk(",
        "            chunk_text, query_terms, layer0, self.processor.tokenizer",
        "        )",
        "",
        "        # Fast score",
        "        term_cols = precompute_term_cols(query_terms, layer0)",
        "        chunk_tokens = self.processor.tokenizer.tokenize(chunk_text)",
        "        fast_score = score_chunk_fast(chunk_tokens, query_terms, term_cols)",
        "",
        "        self.assertAlmostEqual(regular_score, fast_score, places=6)",
        "",
        "    def test_score_chunk_fast_empty_tokens(self):",
        "        \"\"\"score_chunk_fast should handle empty tokens list.\"\"\"",
        "        from cortical.query import score_chunk_fast",
        "        query_terms = {\"neural\": 1.0}",
        "        term_cols = {}",
        "",
        "        score = score_chunk_fast([], query_terms, term_cols)",
        "        self.assertEqual(score, 0.0)",
        "",
        "",
        "class TestEdgeCases(unittest.TestCase):",
        "    \"\"\"Test edge cases and error handling.\"\"\"",
        "",
        "    @classmethod",
        "    def setUpClass(cls):",
        "        \"\"\"Set up empty and minimal processors.\"\"\"",
        "        cls.empty_processor = CorticalTextProcessor()",
        "",
        "        cls.minimal_processor = CorticalTextProcessor()",
        "        cls.minimal_processor.process_document(\"doc1\", \"Hello world\")",
        "        cls.minimal_processor.compute_all(verbose=False)",
        "",
        "    def test_expand_query_empty_corpus(self):",
        "        \"\"\"expand_query should handle empty corpus.\"\"\"",
        "        result = expand_query(",
        "            \"test query\",",
        "            self.empty_processor.layers,",
        "            self.empty_processor.tokenizer",
        "        )",
        "        self.assertIsInstance(result, dict)",
        "",
        "    def test_find_documents_empty_corpus(self):",
        "        \"\"\"find_documents should handle empty corpus.\"\"\"",
        "        result = find_documents_for_query(",
        "            \"test\",",
        "            self.empty_processor.layers,",
        "            self.empty_processor.tokenizer",
        "        )",
        "        self.assertEqual(result, [])",
        "",
        "    def test_find_relation_empty_relations(self):",
        "        \"\"\"find_relation_between should handle empty relations.\"\"\"",
        "        result = find_relation_between(\"a\", \"b\", [])",
        "        self.assertEqual(result, [])",
        "",
        "    def test_find_terms_empty_relations(self):",
        "        \"\"\"find_terms_with_relation should handle empty relations.\"\"\"",
        "        result = find_terms_with_relation(\"a\", \"IsA\", [])",
        "        self.assertEqual(result, [])",
        "",
        "",
        "if __name__ == '__main__':",
        "    unittest.main()"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "tests/test_query_optimization.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "\"\"\"",
        "Tests for query optimization functions.",
        "",
        "Tests the fast search and indexing functionality for improved query performance.",
        "\"\"\"",
        "",
        "import unittest",
        "from cortical.query import (",
        "    fast_find_documents,",
        "    build_document_index,",
        "    search_with_index,",
        ")",
        "from cortical.tokenizer import Tokenizer",
        "from cortical.layers import CorticalLayer",
        "",
        "",
        "class TestFastFindDocuments(unittest.TestCase):",
        "    \"\"\"Test the fast_find_documents function.\"\"\"",
        "",
        "    def setUp(self):",
        "        \"\"\"Set up test processor.\"\"\"",
        "        from cortical import CorticalTextProcessor",
        "        self.processor = CorticalTextProcessor()",
        "        self.processor.process_document(\"auth\", \"\"\"",
        "            Authentication module handles user login and credentials.",
        "            Validates tokens and manages sessions securely.",
        "        \"\"\")",
        "        self.processor.process_document(\"data\", \"\"\"",
        "            Data processing module fetches and transforms data.",
        "            Handles database queries and result formatting.",
        "        \"\"\")",
        "        self.processor.process_document(\"validation\", \"\"\"",
        "            Input validation module checks user input.",
        "            Sanitizes and validates form data securely.",
        "        \"\"\")",
        "        self.processor.compute_all()",
        "",
        "    def test_fast_find_returns_results(self):",
        "        \"\"\"Test that fast_find_documents returns results.\"\"\"",
        "        results = fast_find_documents(",
        "            \"authentication login\",",
        "            self.processor.layers,",
        "            self.processor.tokenizer",
        "        )",
        "        self.assertIsInstance(results, list)",
        "        self.assertGreater(len(results), 0)",
        "",
        "    def test_fast_find_finds_relevant_doc(self):",
        "        \"\"\"Test that fast_find_documents finds relevant document.\"\"\"",
        "        results = fast_find_documents(",
        "            \"authentication login\",",
        "            self.processor.layers,",
        "            self.processor.tokenizer",
        "        )",
        "        doc_ids = [r[0] for r in results]",
        "        self.assertIn('auth', doc_ids)",
        "",
        "    def test_fast_find_respects_top_n(self):",
        "        \"\"\"Test that fast_find_documents respects top_n.\"\"\"",
        "        results = fast_find_documents(",
        "            \"user data\",",
        "            self.processor.layers,",
        "            self.processor.tokenizer,",
        "            top_n=2",
        "        )",
        "        self.assertLessEqual(len(results), 2)",
        "",
        "    def test_fast_find_empty_query(self):",
        "        \"\"\"Test fast_find_documents with empty query.\"\"\"",
        "        results = fast_find_documents(",
        "            \"\",",
        "            self.processor.layers,",
        "            self.processor.tokenizer",
        "        )",
        "        self.assertEqual(results, [])",
        "",
        "    def test_fast_find_with_code_concepts(self):",
        "        \"\"\"Test fast_find_documents with code concept expansion.\"\"\"",
        "        # 'fetch' should expand to find 'data' doc which has 'fetches'",
        "        results = fast_find_documents(",
        "            \"fetch\",",
        "            self.processor.layers,",
        "            self.processor.tokenizer,",
        "            use_code_concepts=True",
        "        )",
        "        # Should find data doc",
        "        if results:",
        "            doc_ids = [r[0] for r in results]",
        "            self.assertIn('data', doc_ids)",
        "",
        "    def test_fast_find_without_code_concepts(self):",
        "        \"\"\"Test fast_find_documents without code concept expansion.\"\"\"",
        "        results = fast_find_documents(",
        "            \"nonexistent term xyz\",",
        "            self.processor.layers,",
        "            self.processor.tokenizer,",
        "            use_code_concepts=False",
        "        )",
        "        self.assertEqual(results, [])",
        "",
        "",
        "class TestBuildDocumentIndex(unittest.TestCase):",
        "    \"\"\"Test the build_document_index function.\"\"\"",
        "",
        "    def setUp(self):",
        "        \"\"\"Set up test processor.\"\"\"",
        "        from cortical import CorticalTextProcessor",
        "        self.processor = CorticalTextProcessor()",
        "        self.processor.process_document(\"doc1\", \"neural network training data\")",
        "        self.processor.process_document(\"doc2\", \"database query optimization\")",
        "        self.processor.compute_all()",
        "",
        "    def test_build_index_returns_dict(self):",
        "        \"\"\"Test that build_document_index returns a dict.\"\"\"",
        "        index = build_document_index(self.processor.layers)",
        "        self.assertIsInstance(index, dict)",
        "",
        "    def test_index_contains_terms(self):",
        "        \"\"\"Test that index contains expected terms.\"\"\"",
        "        index = build_document_index(self.processor.layers)",
        "        self.assertIn('neural', index)",
        "        self.assertIn('database', index)",
        "",
        "    def test_index_maps_to_docs(self):",
        "        \"\"\"Test that index maps terms to documents.\"\"\"",
        "        index = build_document_index(self.processor.layers)",
        "",
        "        # 'neural' should map to doc1",
        "        self.assertIn('neural', index)",
        "        self.assertIn('doc1', index['neural'])",
        "",
        "        # 'database' should map to doc2",
        "        self.assertIn('database', index)",
        "        self.assertIn('doc2', index['database'])",
        "",
        "    def test_index_values_are_scores(self):",
        "        \"\"\"Test that index values are positive scores.\"\"\"",
        "        index = build_document_index(self.processor.layers)",
        "",
        "        for term, doc_scores in index.items():",
        "            for doc_id, score in doc_scores.items():",
        "                self.assertGreater(score, 0)",
        "",
        "",
        "class TestSearchWithIndex(unittest.TestCase):",
        "    \"\"\"Test the search_with_index function.\"\"\"",
        "",
        "    def setUp(self):",
        "        \"\"\"Set up test processor and index.\"\"\"",
        "        from cortical import CorticalTextProcessor",
        "        self.processor = CorticalTextProcessor()",
        "        self.processor.process_document(\"auth\", \"authentication login credentials\")",
        "        self.processor.process_document(\"data\", \"database query optimization\")",
        "        self.processor.process_document(\"network\", \"neural network training\")",
        "        self.processor.compute_all()",
        "        self.index = build_document_index(self.processor.layers)",
        "",
        "    def test_search_with_index_returns_results(self):",
        "        \"\"\"Test that search_with_index returns results.\"\"\"",
        "        results = search_with_index(",
        "            \"authentication\",",
        "            self.index,",
        "            self.processor.tokenizer",
        "        )",
        "        self.assertIsInstance(results, list)",
        "        self.assertGreater(len(results), 0)",
        "",
        "    def test_search_with_index_finds_relevant(self):",
        "        \"\"\"Test that search_with_index finds relevant document.\"\"\"",
        "        results = search_with_index(",
        "            \"authentication login\",",
        "            self.index,",
        "            self.processor.tokenizer",
        "        )",
        "        doc_ids = [r[0] for r in results]",
        "        self.assertIn('auth', doc_ids)",
        "",
        "    def test_search_with_index_respects_top_n(self):",
        "        \"\"\"Test that search_with_index respects top_n.\"\"\"",
        "        results = search_with_index(",
        "            \"network\",",
        "            self.index,",
        "            self.processor.tokenizer,",
        "            top_n=1",
        "        )",
        "        self.assertLessEqual(len(results), 1)",
        "",
        "    def test_search_with_index_empty_query(self):",
        "        \"\"\"Test search_with_index with empty query.\"\"\"",
        "        results = search_with_index(",
        "            \"\",",
        "            self.index,",
        "            self.processor.tokenizer",
        "        )",
        "        self.assertEqual(results, [])",
        "",
        "    def test_search_with_index_no_matches(self):",
        "        \"\"\"Test search_with_index with no matching terms.\"\"\"",
        "        results = search_with_index(",
        "            \"xyznonexistent\",",
        "            self.index,",
        "            self.processor.tokenizer",
        "        )",
        "        self.assertEqual(results, [])",
        "",
        "",
        "class TestProcessorIntegration(unittest.TestCase):",
        "    \"\"\"Test query optimization integration with processor.\"\"\"",
        "",
        "    def setUp(self):",
        "        \"\"\"Set up test processor.\"\"\"",
        "        from cortical import CorticalTextProcessor",
        "        self.processor = CorticalTextProcessor()",
        "        self.processor.process_document(\"auth\", \"\"\"",
        "            Authentication module handles user login and session management.",
        "            Validates credentials and issues tokens.",
        "        \"\"\")",
        "        self.processor.process_document(\"data\", \"\"\"",
        "            Data processing module fetches records from database.",
        "            Transforms and validates data for export.",
        "        \"\"\")",
        "        self.processor.compute_all()",
        "",
        "    def test_processor_fast_find_documents(self):",
        "        \"\"\"Test processor fast_find_documents method.\"\"\"",
        "        results = self.processor.fast_find_documents(\"authentication\")",
        "        self.assertIsInstance(results, list)",
        "        if results:",
        "            doc_ids = [r[0] for r in results]",
        "            self.assertIn('auth', doc_ids)",
        "",
        "    def test_processor_build_search_index(self):",
        "        \"\"\"Test processor build_search_index method.\"\"\"",
        "        index = self.processor.build_search_index()",
        "        self.assertIsInstance(index, dict)",
        "        self.assertGreater(len(index), 0)",
        "",
        "    def test_processor_search_with_index(self):",
        "        \"\"\"Test processor search_with_index method.\"\"\"",
        "        index = self.processor.build_search_index()",
        "        results = self.processor.search_with_index(\"database\", index)",
        "        self.assertIsInstance(results, list)",
        "        if results:",
        "            doc_ids = [r[0] for r in results]",
        "            self.assertIn('data', doc_ids)",
        "",
        "    def test_fast_vs_regular_same_results(self):",
        "        \"\"\"Test that fast and regular search return similar results.\"\"\"",
        "        query = \"authentication login\"",
        "",
        "        regular_results = self.processor.find_documents_for_query(query)",
        "        fast_results = self.processor.fast_find_documents(query)",
        "",
        "        # Both should find 'auth' as top result",
        "        if regular_results and fast_results:",
        "            self.assertEqual(regular_results[0][0], fast_results[0][0])",
        "",
        "    def test_index_search_reusable(self):",
        "        \"\"\"Test that built index can be reused for multiple queries.\"\"\"",
        "        index = self.processor.build_search_index()",
        "",
        "        results1 = self.processor.search_with_index(\"authentication\", index)",
        "        results2 = self.processor.search_with_index(\"database\", index)",
        "",
        "        # Should return different results for different queries",
        "        if results1 and results2:",
        "            self.assertNotEqual(results1[0][0], results2[0][0])",
        "",
        "",
        "if __name__ == '__main__':",
        "    unittest.main()"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "tests/test_tokenizer.py",
      "function": "class TestTokenizerStemming(unittest.TestCase):",
      "start_line": 84,
      "lines_added": [
        "class TestSplitIdentifier(unittest.TestCase):",
        "    \"\"\"Test the split_identifier function.\"\"\"",
        "",
        "    def test_camel_case(self):",
        "        \"\"\"Test splitting camelCase identifiers.\"\"\"",
        "        from cortical.tokenizer import split_identifier",
        "        self.assertEqual(split_identifier(\"getUserCredentials\"), [\"get\", \"user\", \"credentials\"])",
        "        self.assertEqual(split_identifier(\"processData\"), [\"process\", \"data\"])",
        "",
        "    def test_pascal_case(self):",
        "        \"\"\"Test splitting PascalCase identifiers.\"\"\"",
        "        from cortical.tokenizer import split_identifier",
        "        self.assertEqual(split_identifier(\"UserCredentials\"), [\"user\", \"credentials\"])",
        "        self.assertEqual(split_identifier(\"DataProcessor\"), [\"data\", \"processor\"])",
        "",
        "    def test_underscore_style(self):",
        "        \"\"\"Test splitting underscore_style identifiers.\"\"\"",
        "        from cortical.tokenizer import split_identifier",
        "        self.assertEqual(split_identifier(\"get_user_data\"), [\"get\", \"user\", \"data\"])",
        "        self.assertEqual(split_identifier(\"process_http_request\"), [\"process\", \"http\", \"request\"])",
        "",
        "    def test_constant_style(self):",
        "        \"\"\"Test splitting CONSTANT_STYLE identifiers.\"\"\"",
        "        from cortical.tokenizer import split_identifier",
        "        self.assertEqual(split_identifier(\"MAX_RETRY_COUNT\"), [\"max\", \"retry\", \"count\"])",
        "",
        "    def test_acronyms(self):",
        "        \"\"\"Test handling of acronyms in identifiers.\"\"\"",
        "        from cortical.tokenizer import split_identifier",
        "        self.assertEqual(split_identifier(\"XMLParser\"), [\"xml\", \"parser\"])",
        "        self.assertEqual(split_identifier(\"parseHTTPResponse\"), [\"parse\", \"http\", \"response\"])",
        "        self.assertEqual(split_identifier(\"getURLString\"), [\"get\", \"url\", \"string\"])",
        "",
        "    def test_mixed_case_with_underscore(self):",
        "        \"\"\"Test mixed camelCase and underscore_style.\"\"\"",
        "        from cortical.tokenizer import split_identifier",
        "        result = split_identifier(\"get_UserData\")",
        "        self.assertIn(\"get\", result)",
        "        self.assertIn(\"user\", result)",
        "        self.assertIn(\"data\", result)",
        "",
        "    def test_single_word(self):",
        "        \"\"\"Test single word identifiers.\"\"\"",
        "        from cortical.tokenizer import split_identifier",
        "        self.assertEqual(split_identifier(\"process\"), [\"process\"])",
        "        self.assertEqual(split_identifier(\"data\"), [\"data\"])",
        "",
        "    def test_empty_string(self):",
        "        \"\"\"Test empty string input.\"\"\"",
        "        from cortical.tokenizer import split_identifier",
        "        self.assertEqual(split_identifier(\"\"), [])",
        "",
        "",
        "class TestCodeAwareTokenization(unittest.TestCase):",
        "    \"\"\"Test code-aware tokenization with identifier splitting.\"\"\"",
        "",
        "    def test_split_identifiers_disabled_by_default(self):",
        "        \"\"\"Test that identifier splitting is disabled by default.\"\"\"",
        "        tokenizer = Tokenizer()",
        "        tokens = tokenizer.tokenize(\"getUserCredentials\")",
        "        self.assertEqual(tokens, [\"getusercredentials\"])",
        "",
        "    def test_split_identifiers_enabled(self):",
        "        \"\"\"Test tokenization with identifier splitting enabled.\"\"\"",
        "        tokenizer = Tokenizer(split_identifiers=True)",
        "        tokens = tokenizer.tokenize(\"getUserCredentials\")",
        "        self.assertIn(\"getusercredentials\", tokens)",
        "        self.assertIn(\"get\", tokens)",
        "        self.assertIn(\"user\", tokens)",
        "        self.assertIn(\"credentials\", tokens)",
        "",
        "    def test_split_identifiers_underscore_style(self):",
        "        \"\"\"Test splitting underscore_style in tokenization.\"\"\"",
        "        tokenizer = Tokenizer(split_identifiers=True)",
        "        tokens = tokenizer.tokenize(\"process_user_data\")",
        "        self.assertIn(\"process_user_data\", tokens)",
        "        self.assertIn(\"process\", tokens)",
        "        self.assertIn(\"user\", tokens)",
        "        self.assertIn(\"data\", tokens)",
        "",
        "    def test_split_identifiers_preserves_context(self):",
        "        \"\"\"Test that split tokens appear alongside regular tokens.\"\"\"",
        "        tokenizer = Tokenizer(split_identifiers=True)",
        "        tokens = tokenizer.tokenize(\"The getUserCredentials function returns data\")",
        "        self.assertIn(\"getusercredentials\", tokens)",
        "        self.assertIn(\"credentials\", tokens)",
        "        self.assertIn(\"function\", tokens)",
        "        self.assertIn(\"returns\", tokens)",
        "        self.assertIn(\"data\", tokens)",
        "",
        "    def test_split_identifiers_override(self):",
        "        \"\"\"Test overriding split_identifiers at call time.\"\"\"",
        "        tokenizer = Tokenizer(split_identifiers=False)",
        "        # Override to True",
        "        tokens = tokenizer.tokenize(\"getUserData\", split_identifiers=True)",
        "        self.assertIn(\"get\", tokens)",
        "        self.assertIn(\"user\", tokens)",
        "",
        "    def test_no_duplicate_tokens(self):",
        "        \"\"\"Test that split tokens don't create duplicates.\"\"\"",
        "        tokenizer = Tokenizer(split_identifiers=True)",
        "        tokens = tokenizer.tokenize(\"data process_data getData\")",
        "        # 'data' should appear only once",
        "        self.assertEqual(tokens.count(\"data\"), 1)",
        "",
        "    def test_stop_words_filtered_from_splits(self):",
        "        \"\"\"Test that stop words in split parts are filtered.\"\"\"",
        "        tokenizer = Tokenizer(split_identifiers=True)",
        "        # 'the' is a stop word",
        "        tokens = tokenizer.tokenize(\"getTheData\")",
        "        self.assertNotIn(\"the\", tokens)",
        "        self.assertIn(\"data\", tokens)",
        "",
        "    def test_min_length_applied_to_splits(self):",
        "        \"\"\"Test that min_word_length applies to split parts.\"\"\"",
        "        tokenizer = Tokenizer(split_identifiers=True, min_word_length=4)",
        "        tokens = tokenizer.tokenize(\"getUserID\")",
        "        # 'id' is too short (length 2)",
        "        self.assertNotIn(\"id\", tokens)",
        "        self.assertIn(\"user\", tokens)",
        "",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "        self.assertIn(\"network\", variants)",
        "        self.assertIn(\"networks\", variants)",
        "    ",
        "    def test_word_mappings_brain(self):",
        "        \"\"\"Test brain-related word mappings.\"\"\"",
        "        variants = self.tokenizer.get_word_variants(\"brain\")",
        "        self.assertIn(\"neural\", variants)",
        "        self.assertIn(\"cortical\", variants)",
        "",
        ""
      ],
      "context_after": [
        "if __name__ == \"__main__\":",
        "    unittest.main(verbosity=2)"
      ],
      "change_type": "add"
    }
  ],
  "hour_of_day": 15,
  "day_of_week": "Wednesday",
  "seconds_since_last_commit": -425445,
  "is_merge": true,
  "is_initial": false,
  "parent_count": 2,
  "session_id": null,
  "related_chats": [],
  "ci_result": null,
  "reverted": false,
  "amended": false
}