{
  "hash": "fcce0c282848b7371238f5df4f3e19f04603227b",
  "message": "feat: Optimize compute_all and add Graph-Boosted search (GB-BM25)",
  "author": "Claude",
  "timestamp": "2025-12-15 05:46:55 +0000",
  "branch": "claude/multi-index-design-DvifZ",
  "files_changed": [
    "cortical/analysis.py",
    "cortical/processor/query_api.py",
    "cortical/query/__init__.py",
    "cortical/query/search.py",
    "cortical/semantics.py"
  ],
  "insertions": 244,
  "deletions": 62,
  "hunks": [
    {
      "file": "cortical/analysis.py",
      "function": "def compute_bigram_connections(",
      "start_line": 2012,
      "lines_added": [
        "    # OPTIMIZED: Use inverted index approach instead of O(n²) matrix multiplication",
        "    # Additional optimization: importance-based filtering and early termination",
        "    skipped_low_importance = 0",
        "    # Build inverted index: doc_id -> list of bigram minicolumns",
        "    # Sort by TF-IDF importance within each document for priority processing",
        "    doc_to_bigrams: Dict[str, List[Minicolumn]] = defaultdict(list)",
        "        for doc_id in bigram.document_ids:",
        "            doc_to_bigrams[doc_id].append(bigram)",
        "",
        "    # Compute importance threshold (median TF-IDF) for filtering",
        "    tfidf_values = [b.tfidf for b in bigrams if b.tfidf > 0]",
        "    importance_threshold = sorted(tfidf_values)[len(tfidf_values) // 4] if tfidf_values else 0",
        "",
        "    # Process each document's bigram pairs",
        "    for doc_id, doc_bigrams in doc_to_bigrams.items():",
        "        # Skip large documents to avoid O(n²) explosion",
        "        if len(doc_bigrams) > max_bigrams_per_doc:",
        "        # Filter to important bigrams only (reduces pairs quadratically)",
        "        important_bigrams = [b for b in doc_bigrams if b.tfidf >= importance_threshold]",
        "        if len(important_bigrams) < 2:",
        "            continue",
        "        # Sort by importance for priority connections",
        "        important_bigrams.sort(key=lambda b: b.tfidf, reverse=True)",
        "        # Connect pairs of important bigrams in this document",
        "        # Limit to top connections per bigram to avoid explosion",
        "        for i, b1 in enumerate(important_bigrams):",
        "            # Early termination if this bigram is at connection limit",
        "            if connection_counts[b1.id] >= max_connections_per_bigram:",
        "                continue",
        "            for b2 in important_bigrams[i+1:]:",
        "                if connection_counts[b2.id] >= max_connections_per_bigram:",
        "                    continue",
        "                # Fast path: they share at least this document",
        "                if len(shared_docs) < min_shared_docs:",
        "                    continue"
      ],
      "lines_removed": [
        "    # Use sparse matrix multiplication for efficient co-occurrence computation",
        "    # Build document-term matrix using sparse representation",
        "    # Create mappings: doc_id -> row index, bigram -> col index",
        "    doc_to_row: Dict[str, int] = {}",
        "    bigram_to_col: Dict[str, int] = {}",
        "",
        "    # Collect all documents first",
        "    all_docs: Set[str] = set()",
        "        all_docs.update(bigram.document_ids)",
        "",
        "    # Assign row indices to documents (excluding large docs)",
        "    row_idx = 0",
        "    for doc_id in sorted(all_docs):",
        "        # Count bigrams in this doc",
        "        doc_bigram_count = sum(1 for b in bigrams if doc_id in b.document_ids)",
        "        if doc_bigram_count > max_bigrams_per_doc:",
        "        doc_to_row[doc_id] = row_idx",
        "        row_idx += 1",
        "",
        "    # Assign column indices to bigrams",
        "    for col_idx, bigram in enumerate(bigrams):",
        "        bigram_to_col[bigram.id] = col_idx",
        "",
        "    # Build sparse document-term matrix",
        "    # Rows = documents, Cols = bigrams",
        "    # Entry [d, b] = 1 if bigram b appears in document d",
        "    if doc_to_row:  # Only if we have valid documents",
        "        doc_term_matrix = SparseMatrix(len(doc_to_row), len(bigrams))",
        "",
        "        for bigram in bigrams:",
        "            col_idx = bigram_to_col[bigram.id]",
        "            for doc_id in bigram.document_ids:",
        "                if doc_id in doc_to_row:  # Skip large docs",
        "                    row_idx = doc_to_row[doc_id]",
        "                    doc_term_matrix.set(row_idx, col_idx, 1.0)",
        "",
        "        # Compute bigram-bigram co-occurrence matrix: D^T * D",
        "        # Result[i, j] = number of shared documents between bigram i and bigram j",
        "        cooccur_matrix = doc_term_matrix.multiply_transpose()",
        "",
        "        # Create inverse mapping: col_idx -> bigram",
        "        col_to_bigram = {col_idx: bigram for bigram, col_idx in bigram_to_col.items()}",
        "",
        "        # Process co-occurrence matrix to create connections",
        "        for col1, col2, shared_count in cooccur_matrix.get_nonzero():",
        "            # Skip diagonal (bigram with itself)",
        "            if col1 == col2:",
        "                continue",
        "",
        "            # Skip if below threshold",
        "            if shared_count < min_shared_docs:",
        "                continue",
        "            # Get bigrams",
        "            bigram1_id = col_to_bigram[col1]",
        "            bigram2_id = col_to_bigram[col2]",
        "            # Find the actual minicolumn objects",
        "            b1 = layer1.get_by_id(bigram1_id)",
        "            b2 = layer1.get_by_id(bigram2_id)",
        "            if b1 and b2:",
        "                # Compute Jaccard similarity"
      ],
      "context_before": [
        "            # Skip overly common terms",
        "            if len(left_index[term]) > max_bigrams_per_term or len(right_index[term]) > max_bigrams_per_term:",
        "                continue",
        "            # term appears as right component in some bigrams and left in others",
        "            for b_left in right_index[term]:  # ends with term",
        "                for b_right in left_index[term]:  # starts with term",
        "                    if b_left.id != b_right.id:",
        "                        add_connection(b_left, b_right, chain_weight, 'chain')",
        "",
        "    # 3. Connect bigrams that co-occur in the same documents"
      ],
      "context_after": [
        "    skipped_large_docs = 0",
        "",
        "    for bigram in bigrams:",
        "            skipped_large_docs += 1",
        "            continue",
        "",
        "",
        "",
        "                docs1 = b1.document_ids",
        "                docs2 = b2.document_ids",
        "                shared_docs = docs1 & docs2",
        "                jaccard = len(shared_docs) / len(docs1 | docs2)",
        "                weight = cooccurrence_weight * jaccard",
        "                add_connection(b1, b2, weight, 'cooccurrence')",
        "",
        "    return {",
        "        'connections_created': len(connected_pairs),",
        "        'bigrams': len(bigrams),",
        "        'component_connections': component_connections,",
        "        'chain_connections': chain_connections,",
        "        'cooccurrence_connections': cooccurrence_connections,"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/processor/query_api.py",
      "function": "class QueryMixin:",
      "start_line": 380,
      "lines_added": [
        "    def graph_boosted_search(",
        "        self,",
        "        query_text: str,",
        "        top_n: int = 5,",
        "        pagerank_weight: float = 0.3,",
        "        proximity_weight: float = 0.2,",
        "        use_expansion: bool = True",
        "    ) -> List[Tuple[str, float]]:",
        "        \"\"\"",
        "        Graph-Boosted BM25 (GB-BM25): Hybrid scoring combining BM25 with graph signals.",
        "",
        "        This algorithm combines multiple signals for improved code search:",
        "        1. BM25/TF-IDF base score (term relevance)",
        "        2. PageRank boost (matched term importance)",
        "        3. Proximity boost (query terms connected in graph)",
        "        4. Coverage boost (documents with more unique query term matches)",
        "",
        "        Args:",
        "            query_text: Search query",
        "            top_n: Number of results to return",
        "            pagerank_weight: Weight for PageRank boost (0-1, default 0.3)",
        "            proximity_weight: Weight for term proximity boost (0-1, default 0.2)",
        "            use_expansion: Whether to use query expansion",
        "",
        "        Returns:",
        "            List of (doc_id, score) tuples ranked by combined relevance",
        "",
        "        Raises:",
        "            ValueError: If query_text is empty or parameters are invalid",
        "        \"\"\"",
        "        if not isinstance(query_text, str) or not query_text.strip():",
        "            raise ValueError(\"query_text must be a non-empty string\")",
        "        if not isinstance(top_n, int) or top_n < 1:",
        "            raise ValueError(\"top_n must be a positive integer\")",
        "        if not 0 <= pagerank_weight <= 1:",
        "            raise ValueError(\"pagerank_weight must be between 0 and 1\")",
        "        if not 0 <= proximity_weight <= 1:",
        "            raise ValueError(\"proximity_weight must be between 0 and 1\")",
        "",
        "        return query_module.graph_boosted_search(",
        "            query_text,",
        "            self.layers,",
        "            self.tokenizer,",
        "            top_n=top_n,",
        "            pagerank_weight=pagerank_weight,",
        "            proximity_weight=proximity_weight,",
        "            use_expansion=use_expansion,",
        "            semantic_relations=self.semantic_relations",
        "        )",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "",
        "        return query_module.fast_find_documents(",
        "            query_text,",
        "            self.layers,",
        "            self.tokenizer,",
        "            top_n=top_n,",
        "            candidate_multiplier=candidate_multiplier,",
        "            use_code_concepts=use_code_concepts",
        "        )",
        ""
      ],
      "context_after": [
        "    def quick_search(self, query: str, top_n: int = 5) -> List[str]:",
        "        \"\"\"",
        "        One-call document search with sensible defaults.",
        "",
        "        Args:",
        "            query: Search query string",
        "            top_n: Number of results to return (default 5)",
        "",
        "        Returns:",
        "            List of document IDs ranked by relevance"
      ],
      "change_type": "add"
    },
    {
      "file": "cortical/query/__init__.py",
      "function": "from .expansion import (",
      "start_line": 59,
      "lines_added": [
        "    graph_boosted_search,"
      ],
      "lines_removed": [],
      "context_before": [
        ")",
        "",
        "# Document search",
        "from .search import (",
        "    find_documents_for_query,",
        "    fast_find_documents,",
        "    build_document_index,",
        "    search_with_index,",
        "    query_with_spreading_activation,",
        "    find_related_documents,"
      ],
      "context_after": [
        ")",
        "",
        "# Document type boosting and ranking",
        "from .ranking import (",
        "    DOC_TYPE_BOOSTS,",
        "    CONCEPTUAL_KEYWORDS,",
        "    IMPLEMENTATION_KEYWORDS,",
        "    is_conceptual_query,",
        "    get_doc_type_boost,",
        "    apply_doc_type_boost,"
      ],
      "change_type": "add"
    },
    {
      "file": "cortical/query/__init__.py",
      "function": "__all__ = [",
      "start_line": 136,
      "lines_added": [
        "    'graph_boosted_search',"
      ],
      "lines_removed": [],
      "context_before": [
        "    'expand_query_semantic',",
        "    'expand_query_multihop',",
        "    'get_expanded_query_terms',",
        "    # Search",
        "    'find_documents_for_query',",
        "    'fast_find_documents',",
        "    'build_document_index',",
        "    'search_with_index',",
        "    'query_with_spreading_activation',",
        "    'find_related_documents',"
      ],
      "context_after": [
        "    # Ranking",
        "    'DOC_TYPE_BOOSTS',",
        "    'CONCEPTUAL_KEYWORDS',",
        "    'IMPLEMENTATION_KEYWORDS',",
        "    'is_conceptual_query',",
        "    'get_doc_type_boost',",
        "    'apply_doc_type_boost',",
        "    'find_documents_with_boost',",
        "    'find_relevant_concepts',",
        "    'multi_stage_rank',"
      ],
      "change_type": "add"
    },
    {
      "file": "cortical/query/search.py",
      "function": "def find_related_documents(",
      "start_line": 413,
      "lines_added": [
        "",
        "",
        "def graph_boosted_search(",
        "    query_text: str,",
        "    layers: Dict[CorticalLayer, HierarchicalLayer],",
        "    tokenizer: Tokenizer,",
        "    top_n: int = 5,",
        "    pagerank_weight: float = 0.3,",
        "    proximity_weight: float = 0.2,",
        "    use_expansion: bool = True,",
        "    semantic_relations: Optional[List[Tuple[str, str, str, float]]] = None",
        ") -> List[Tuple[str, float]]:",
        "    \"\"\"",
        "    Graph-Boosted BM25 (GB-BM25): Hybrid scoring combining BM25 with graph signals.",
        "",
        "    This creative algorithm combines multiple signals:",
        "    1. BM25/TF-IDF base score (term relevance)",
        "    2. PageRank boost (matched term importance)",
        "    3. Proximity boost (query terms connected in graph)",
        "    4. Coverage boost (documents with more unique query term matches)",
        "",
        "    This approach is designed for code search where:",
        "    - Important functions/classes should rank higher (PageRank)",
        "    - Related concepts should boost each other (graph connections)",
        "    - Comprehensive matches beat partial matches (coverage)",
        "",
        "    Args:",
        "        query_text: Search query",
        "        layers: Dictionary of layers",
        "        tokenizer: Tokenizer instance",
        "        top_n: Number of results to return",
        "        pagerank_weight: Weight for PageRank boost (0-1, default 0.3)",
        "        proximity_weight: Weight for term proximity boost (0-1, default 0.2)",
        "        use_expansion: Whether to use query expansion",
        "        semantic_relations: Optional semantic relations for expansion",
        "",
        "    Returns:",
        "        List of (doc_id, score) tuples ranked by combined relevance",
        "    \"\"\"",
        "    layer0 = layers[CorticalLayer.TOKENS]",
        "    layer3 = layers[CorticalLayer.DOCUMENTS]",
        "",
        "    # Get expanded query terms",
        "    query_terms = get_expanded_query_terms(",
        "        query_text, layers, tokenizer,",
        "        use_expansion=use_expansion,",
        "        semantic_relations=semantic_relations,",
        "        use_semantic=True",
        "    )",
        "",
        "    if not query_terms:",
        "        return []",
        "",
        "    # Phase 1: Compute base BM25/TF-IDF scores per document",
        "    doc_scores: Dict[str, float] = defaultdict(float)",
        "    doc_term_matches: Dict[str, set] = defaultdict(set)  # Track unique term matches",
        "    doc_pagerank_sum: Dict[str, float] = defaultdict(float)  # Sum of PageRank for matched terms",
        "",
        "    for term, term_weight in query_terms.items():",
        "        col = layer0.get_minicolumn(term)",
        "        if col:",
        "            # Get term's PageRank importance",
        "            term_pagerank = getattr(col, 'pagerank', 0.0) or 0.0",
        "",
        "            for doc_id in col.document_ids:",
        "                # Base BM25/TF-IDF score",
        "                tfidf = col.tfidf_per_doc.get(doc_id, col.tfidf)",
        "                doc_scores[doc_id] += tfidf * term_weight",
        "",
        "                # Track term match for coverage",
        "                doc_term_matches[doc_id].add(term)",
        "",
        "                # Accumulate PageRank boost",
        "                doc_pagerank_sum[doc_id] += term_pagerank * term_weight",
        "",
        "    if not doc_scores:",
        "        return []",
        "",
        "    # Phase 2: Compute proximity boost using lateral connections",
        "    # Boost documents where query terms are connected in the graph",
        "    proximity_scores: Dict[str, float] = defaultdict(float)",
        "",
        "    original_tokens = tokenizer.tokenize(query_text)",
        "    if len(original_tokens) > 1:",
        "        # Check if query terms have lateral connections to each other",
        "        for i, t1 in enumerate(original_tokens):",
        "            col1 = layer0.get_minicolumn(t1)",
        "            if not col1:",
        "                continue",
        "",
        "            for t2 in original_tokens[i+1:]:",
        "                col2 = layer0.get_minicolumn(t2)",
        "                if not col2:",
        "                    continue",
        "",
        "                # Check for connection between terms",
        "                conn_weight = col1.lateral_connections.get(col2.id, 0.0)",
        "                if conn_weight > 0:",
        "                    # Boost documents containing both terms",
        "                    shared_docs = col1.document_ids & col2.document_ids",
        "                    for doc_id in shared_docs:",
        "                        proximity_scores[doc_id] += conn_weight",
        "",
        "    # Phase 3: Combine all signals",
        "    max_base_score = max(doc_scores.values()) if doc_scores else 1.0",
        "    max_pagerank = max(doc_pagerank_sum.values()) if doc_pagerank_sum else 1.0",
        "    max_proximity = max(proximity_scores.values()) if proximity_scores else 1.0",
        "",
        "    final_scores: Dict[str, float] = {}",
        "    num_query_terms = len(set(original_tokens))",
        "",
        "    for doc_id, base_score in doc_scores.items():",
        "        # Normalize base score",
        "        norm_base = base_score / max_base_score if max_base_score > 0 else 0",
        "",
        "        # Normalize PageRank boost",
        "        pagerank_boost = doc_pagerank_sum.get(doc_id, 0.0)",
        "        norm_pagerank = pagerank_boost / max_pagerank if max_pagerank > 0 else 0",
        "",
        "        # Normalize proximity boost",
        "        prox_boost = proximity_scores.get(doc_id, 0.0)",
        "        norm_proximity = prox_boost / max_proximity if max_proximity > 0 else 0",
        "",
        "        # Coverage boost: reward documents matching more unique query terms",
        "        coverage = len(doc_term_matches.get(doc_id, set())) / num_query_terms if num_query_terms > 0 else 0",
        "",
        "        # Combine signals with weights",
        "        # Base score dominates, with boosts from graph signals",
        "        combined = (",
        "            (1 - pagerank_weight - proximity_weight) * norm_base +",
        "            pagerank_weight * norm_pagerank +",
        "            proximity_weight * norm_proximity",
        "        )",
        "",
        "        # Apply coverage multiplier (0.5 to 1.5 range)",
        "        coverage_mult = 0.5 + coverage",
        "",
        "        # Final score preserves relative magnitude",
        "        final_scores[doc_id] = combined * coverage_mult * max_base_score",
        "",
        "    sorted_docs = sorted(final_scores.items(), key=lambda x: -x[1])",
        "    return sorted_docs[:top_n]"
      ],
      "lines_removed": [],
      "context_before": [
        "        return []",
        "",
        "    related = []",
        "    for neighbor_id, weight in col.lateral_connections.items():",
        "        # Use O(1) ID lookup instead of linear search",
        "        neighbor = layer3.get_by_id(neighbor_id)",
        "        if neighbor:",
        "            related.append((neighbor.content, weight))",
        "",
        "    return sorted(related, key=lambda x: -x[1])"
      ],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "cortical/semantics.py",
      "function": "def extract_corpus_semantics(",
      "start_line": 309,
      "lines_added": [
        "        # Pre-filter terms by minimum context keys AND TF-IDF importance",
        "        tfidf_scores: Dict[str, float] = {}",
        "            # Get TF-IDF score for importance ranking",
        "            col = layer0.get_minicolumn(term)",
        "            tfidf_scores[term] = col.tfidf if col else 0.0",
        "        # Get filtered terms with enough context, sorted by importance",
        "        # OPTIMIZATION: Sort by TF-IDF importance and limit to top terms",
        "        # This focuses similarity computation on the most important terms",
        "        filtered_terms.sort(key=lambda t: tfidf_scores.get(t, 0), reverse=True)",
        "",
        "        # Limit to top N important terms to avoid O(n²) explosion",
        "        # sqrt(max_similarity_pairs) gives balanced coverage",
        "        max_terms = int(math.sqrt(max_similarity_pairs * 2)) if max_similarity_pairs > 0 else len(filtered_terms)",
        "        filtered_terms = filtered_terms[:max_terms]",
        ""
      ],
      "lines_removed": [
        "        # Pre-filter terms by minimum context keys",
        "        # Get filtered terms with enough context"
      ],
      "context_before": [
        "        for i in range(n_terms):",
        "            row_i = nonzero_counts[i]",
        "            for j in range(i + 1, n_terms):",
        "                if similarities[i, j] > 0.3:",
        "                    common_count = np.sum(row_i & nonzero_counts[j])",
        "                    if common_count >= 3:",
        "                        relations.append((terms[i], 'SimilarTo', terms[j], float(similarities[i, j])))",
        "",
        "    elif n_terms > 1:",
        "        # Fallback: pure Python implementation with optimizations"
      ],
      "context_after": [
        "        key_sets: Dict[str, set] = {}",
        "        magnitudes: Dict[str, float] = {}",
        "",
        "        for term in terms:",
        "            vec = context_vectors[term]",
        "            keys = set(vec.keys())",
        "            # Skip terms with too few context keys (can't meet min_context_keys threshold)",
        "            if len(keys) < min_context_keys:",
        "                continue",
        "            key_sets[term] = keys",
        "            mag = math.sqrt(sum(v * v for v in vec.values()))",
        "            magnitudes[term] = mag",
        "",
        "        filtered_terms = [t for t in terms if t in key_sets and magnitudes.get(t, 0) > 0]",
        "",
        "        # Track pairs checked for early termination",
        "        pairs_checked = 0",
        "",
        "        for i, t1 in enumerate(filtered_terms):",
        "            vec1 = context_vectors[t1]",
        "            mag1 = magnitudes[t1]",
        "            keys1 = key_sets[t1]",
        "",
        "            for t2 in filtered_terms[i+1:]:",
        "                # Check pair limit"
      ],
      "change_type": "modify"
    }
  ],
  "hour_of_day": 5,
  "day_of_week": "Monday",
  "seconds_since_last_commit": -28673,
  "is_merge": false,
  "is_initial": false,
  "parent_count": 1,
  "session_id": null,
  "related_chats": [],
  "ci_result": null,
  "reverted": false,
  "amended": false
}