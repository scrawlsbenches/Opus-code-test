{
  "hash": "ce283a6ed458269dee82f945395d6ec4fa6c32b0",
  "message": "Merge pull request #69 from scrawlsbenches/claude/replace-pkl-git-friendly-01DD3ra3P5hj9NK57johJff5",
  "author": "scrawlsbenches",
  "timestamp": "2025-12-13 20:24:12 -0500",
  "branch": "claude/multi-index-design-DvifZ",
  "files_changed": [
    "TASK_LIST.md",
    "cortical/state_storage.py",
    "docs/director-continuation-prompt.md",
    "tasks/2025-12-14_00-21-46_8d66.json",
    "tests/unit/test_state_storage.py"
  ],
  "insertions": 1617,
  "deletions": 3,
  "hunks": [
    {
      "file": "TASK_LIST.md",
      "function": null,
      "start_line": 1,
      "lines_added": [
        "**Pending Tasks:** 10"
      ],
      "lines_removed": [
        "**Pending Tasks:** 9"
      ],
      "context_before": [
        "# Task List: Cortical Text Processor",
        "",
        "Active backlog for the Cortical Text Processor project. Completed tasks are archived in [TASK_ARCHIVE.md](TASK_ARCHIVE.md).",
        "",
        "**Last Updated:** 2025-12-13"
      ],
      "context_after": [
        "**Completed Tasks:** 238 (see archive)",
        "",
        "**Legacy Test Cleanup:** âœ… COMPLETE - All 8 tasks investigated (#198-205)",
        "- **KEEP (7 files, 506 tests):** Provide unique coverage not duplicated in unit tests",
        "  - #198 test_coverage_gaps.py (91 tests) - edge case coverage",
        "  - #199 test_cli_wrapper.py (96 tests) - CLI wrapper framework",
        "  - #200 test_edge_cases.py (53 tests) - robustness tests",
        "  - #201 test_incremental_indexing.py (47 tests) - script integration",
        "  - #205 Script tests: 6 files (132 tests) - scripts/ directory",
        "- **DELETED (3 files, 53 tests):** Covered by unit tests"
      ],
      "change_type": "modify"
    },
    {
      "file": "TASK_LIST.md",
      "function": "Active backlog for the Cortical Text Processor project. Completed tasks are arch",
      "start_line": 25,
      "lines_added": [
        "| 206 | Replace pkl with git-friendly JSON state storage | Arch | - | Large |"
      ],
      "lines_removed": [
        "| *None - all high priority completed* |||||"
      ],
      "context_before": [
        "---",
        "",
        "## Active Backlog",
        "",
        "<!-- Machine-parseable format for automation -->",
        "",
        "### ðŸŸ  High (Do This Week)",
        "",
        "| # | Task | Category | Depends | Effort |",
        "|---|------|----------|---------|--------|"
      ],
      "context_after": [
        "",
        "### ðŸŸ¡ Medium (Do This Month)",
        "",
        "| # | Task | Category | Depends | Effort |",
        "|---|------|----------|---------|--------|",
        "| 133 | Implement WAL + snapshot persistence (fault-tolerant rebuild) | Arch | 132 | Large |",
        "| 134 | Implement protobuf serialization for corpus | Arch | 132 | Medium |",
        "| 135 | Implement chunked parallel processing for full-analysis | Arch | 132 | Large |",
        "| 95 | Split processor.py into modules | Arch | - | Large |",
        ""
      ],
      "change_type": "modify"
    },
    {
      "file": "TASK_LIST.md",
      "function": "All completed tasks are now archived in [TASK_ARCHIVE.md](TASK_ARCHIVE.md).",
      "start_line": 109,
      "lines_added": [
        "### 206. Replace pkl with Git-Friendly JSON State Storage",
        "",
        "**Meta:** `status:pending` `priority:high` `category:arch`",
        "**Files:** `cortical/persistence.py`, `cortical/state_storage.py` (new), `cortical/chunk_index.py`",
        "**Effort:** Large",
        "",
        "**Problem:** Pickle files cause merge conflicts in git collaboration. When multiple team members index documents, the binary `.pkl` files cannot be merged, and one version must be discarded. Additionally:",
        "- Pickle is Python-version specific and can break across upgrades",
        "- Binary format cannot be code-reviewed or diff'd",
        "- Security concerns with pickle deserialization",
        "",
        "**Current State:**",
        "- `chunk_index.py` already stores documents as git-friendly JSON chunks âœ…",
        "- Full processor state (layers, connections, TF-IDF, PageRank) still uses pickle âŒ",
        "",
        "**Solution:** Extend the chunk-based architecture to store ALL processor state as JSON:",
        "",
        "```",
        "corpus_state/",
        "â”œâ”€â”€ manifest.json           # Version, checksums, staleness flags",
        "â”œâ”€â”€ chunks/                  # Document content (existing chunk_index.py)",
        "â”œâ”€â”€ layers/",
        "â”‚   â”œâ”€â”€ L0_tokens.json      # Token minicolumns",
        "â”‚   â”œâ”€â”€ L1_bigrams.json     # Bigram minicolumns",
        "â”‚   â”œâ”€â”€ L2_concepts.json    # Concept clusters",
        "â”‚   â””â”€â”€ L3_documents.json   # Document minicolumns",
        "â”œâ”€â”€ connections/",
        "â”‚   â”œâ”€â”€ lateral.json        # Lateral connections (or split by layer)",
        "â”‚   â”œâ”€â”€ typed.json          # Typed edges with relations",
        "â”‚   â””â”€â”€ cross_layer.json    # Feedforward/feedback connections",
        "â”œâ”€â”€ computed/",
        "â”‚   â”œâ”€â”€ tfidf.json          # TF-IDF scores per term",
        "â”‚   â”œâ”€â”€ pagerank.json       # PageRank values",
        "â”‚   â””â”€â”€ embeddings.json     # Graph embeddings (optional, can be large)",
        "â””â”€â”€ semantic_relations.json # Extracted relations",
        "```",
        "",
        "**Incremental Efficiency Strategy:**",
        "1. Use content hashing to detect changes (like chunk_index.py)",
        "2. Only re-serialize layers/files that changed",
        "3. Store computed values separately (can be regenerated from documents)",
        "4. Add `--rebuild-computed` flag to regenerate TF-IDF/PageRank from chunks",
        "",
        "**Implementation Phases:**",
        "",
        "**Phase 1: State Serialization Module**",
        "- Create `cortical/state_storage.py` with JSON serialization",
        "- Add `StateWriter` class (mirrors `ChunkWriter` pattern)",
        "- Add `StateLoader` class with hash validation",
        "- Implement incremental save (only changed components)",
        "",
        "**Phase 2: Integration with Processor**",
        "- Add `save_json(path)` and `load_json(path)` methods to processor",
        "- Maintain backward compatibility with `save()`/`load()` for pkl",
        "- Add migration utility: `migrate_pkl_to_json()`",
        "",
        "**Phase 3: Incremental Computed Value Updates**",
        "- Track which documents changed since last computation",
        "- Implement incremental TF-IDF update (add/remove terms)",
        "- Add manifest tracking for staleness",
        "",
        "**Quick Context:**",
        "- Entry point: `cortical/persistence.py::save_processor()` (line 25)",
        "- State structure: layers, documents, document_metadata, embeddings, semantic_relations",
        "- Existing pattern: `cortical/chunk_index.py::ChunkWriter` and `ChunkLoader` classes",
        "- Minicolumn serialization: `cortical/minicolumn.py::Minicolumn.to_dict()` (already exists)",
        "- Layer serialization: `cortical/layers.py::HierarchicalLayer.to_dict()` (already exists)",
        "",
        "**Acceptance Criteria:**",
        "- [ ] All processor state can be saved/loaded as JSON",
        "- [ ] No merge conflicts when multiple users index concurrently",
        "- [ ] Incremental saves only update changed components",
        "- [ ] Backward compatibility: can still load existing pkl files",
        "- [ ] Migration script converts pkl â†’ JSON",
        "- [ ] Full test coverage for new module",
        "- [ ] Performance: save/load within 2x of pkl for typical corpus",
        "",
        "---",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "- #198-205 Legacy test investigation COMPLETE - 8 tasks, 10 files reviewed",
        "- #197 Task list validation in CI - Added validate-task-list job to workflow",
        "- #186 Simplified facade methods - quick_search(), rag_retrieve(), explore() (23 tests)",
        "- #196 Spectral embeddings warning - RuntimeWarning for large graphs (>5000 terms)",
        "- Unit Test Coverage Initiative: 1,729 tests, 85% coverage, 19 modules at 90%+",
        "",
        "---",
        "",
        "## Pending Task Details",
        ""
      ],
      "context_after": [
        "### 184. Implement MCP Server for Claude Desktop Integration",
        "",
        "**Meta:** `status:pending` `priority:high` `category:integration`",
        "**Files:** `cortical/mcp_server.py` (new), `mcp_config.json` (new)",
        "**Effort:** Large",
        "",
        "**Problem:** AI agents must call subprocess scripts instead of native integration. Claude Desktop users can't access the processor directly.",
        "",
        "**Solution:** Create MCP (Model Context Protocol) server with tools:",
        "- `search(query, top_n)` â†’ document results"
      ],
      "change_type": "add"
    },
    {
      "file": "TASK_LIST.md",
      "function": "All completed tasks are now archived in [TASK_ARCHIVE.md](TASK_ARCHIVE.md).",
      "start_line": 323,
      "lines_added": [
        "| Arch | 6 | Architecture refactoring (#206, 133, 134, 135, 95, 100) |"
      ],
      "lines_removed": [
        "| Arch | 5 | Architecture refactoring (#133, 134, 135, 95, 100) |"
      ],
      "context_before": [
        "| processor.py | 85% | ðŸ”¶ | #165-166 |",
        "",
        "**19 of 21 modules at 90%+ coverage**",
        "",
        "---",
        "",
        "## Category Index",
        "",
        "| Category | Pending | Description |",
        "|----------|---------|-------------|"
      ],
      "context_after": [
        "| DevEx | 3 | Developer experience, scripts (#75, 78, 80) |",
        "| Samples | 1 | Sample document improvements (#130) |",
        "",
        "*Updated 2025-12-13 - 15 tasks completed via parallel sub-agents*",
        "",
        "---",
        "",
        "## Notes",
        "",
        "- **Effort estimates:** Small (<1 hour), Medium (1-4 hours), Large (1+ days)"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/state_storage.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "\"\"\"",
        "Git-friendly State Storage Module",
        "=================================",
        "",
        "Replaces pickle-based persistence with JSON files that:",
        "- Can be diff'd and reviewed in git",
        "- Won't cause merge conflicts",
        "- Support incremental updates",
        "- Are language/version independent",
        "",
        "Architecture:",
        "    corpus_state/",
        "    â”œâ”€â”€ manifest.json           # Version, checksums, staleness",
        "    â”œâ”€â”€ documents.json          # Document content and metadata",
        "    â”œâ”€â”€ layers/",
        "    â”‚   â”œâ”€â”€ L0_tokens.json      # Token minicolumns",
        "    â”‚   â”œâ”€â”€ L1_bigrams.json     # Bigram minicolumns",
        "    â”‚   â”œâ”€â”€ L2_concepts.json    # Concept clusters",
        "    â”‚   â””â”€â”€ L3_documents.json   # Document minicolumns",
        "    â””â”€â”€ computed/",
        "        â”œâ”€â”€ semantic_relations.json",
        "        â””â”€â”€ embeddings.json",
        "",
        "Usage:",
        "    # Save processor state",
        "    writer = StateWriter('corpus_state')",
        "    writer.save_processor(processor)",
        "",
        "    # Load processor state",
        "    loader = StateLoader('corpus_state')",
        "    layers, documents, metadata, embeddings, relations = loader.load_all()",
        "\"\"\"",
        "",
        "import hashlib",
        "import json",
        "import os",
        "import logging",
        "from dataclasses import dataclass, field, asdict",
        "from datetime import datetime",
        "from pathlib import Path",
        "from typing import Dict, List, Optional, Any, Tuple, Set",
        "",
        "from .layers import CorticalLayer, HierarchicalLayer",
        "from .minicolumn import Minicolumn",
        "",
        "logger = logging.getLogger(__name__)",
        "",
        "# Version for state format (increment on breaking changes)",
        "STATE_VERSION = 1",
        "",
        "# Layer enum value to filename mapping",
        "LAYER_FILENAMES = {",
        "    0: 'L0_tokens.json',",
        "    1: 'L1_bigrams.json',",
        "    2: 'L2_concepts.json',",
        "    3: 'L3_documents.json',",
        "}",
        "",
        "",
        "@dataclass",
        "class StateManifest:",
        "    \"\"\"",
        "    Manifest file tracking state version and component checksums.",
        "",
        "    Used to:",
        "    - Detect which components changed since last save",
        "    - Validate loaded state integrity",
        "    - Track staleness of computed values",
        "    \"\"\"",
        "    version: int = STATE_VERSION",
        "    created_at: str = field(default_factory=lambda: datetime.now().isoformat())",
        "    updated_at: str = field(default_factory=lambda: datetime.now().isoformat())",
        "    checksums: Dict[str, str] = field(default_factory=dict)",
        "    stale_computations: List[str] = field(default_factory=list)",
        "    document_count: int = 0",
        "    layer_stats: Dict[str, int] = field(default_factory=dict)",
        "",
        "    def to_dict(self) -> Dict[str, Any]:",
        "        \"\"\"Convert to dictionary for JSON serialization.\"\"\"",
        "        return asdict(self)",
        "",
        "    @classmethod",
        "    def from_dict(cls, data: Dict[str, Any]) -> 'StateManifest':",
        "        \"\"\"Create manifest from dictionary.\"\"\"",
        "        return cls(",
        "            version=data.get('version', STATE_VERSION),",
        "            created_at=data.get('created_at', datetime.now().isoformat()),",
        "            updated_at=data.get('updated_at', datetime.now().isoformat()),",
        "            checksums=data.get('checksums', {}),",
        "            stale_computations=data.get('stale_computations', []),",
        "            document_count=data.get('document_count', 0),",
        "            layer_stats=data.get('layer_stats', {})",
        "        )",
        "",
        "    def update_checksum(self, component: str, content: str) -> bool:",
        "        \"\"\"",
        "        Update checksum for a component.",
        "",
        "        Args:",
        "            component: Component name (e.g., 'L0_tokens', 'documents')",
        "            content: Serialized content to hash",
        "",
        "        Returns:",
        "            True if checksum changed, False if same",
        "        \"\"\"",
        "        new_hash = hashlib.sha256(content.encode('utf-8')).hexdigest()[:16]",
        "        old_hash = self.checksums.get(component)",
        "        self.checksums[component] = new_hash",
        "        self.updated_at = datetime.now().isoformat()",
        "        return new_hash != old_hash",
        "",
        "",
        "class StateWriter:",
        "    \"\"\"",
        "    Writes processor state to git-friendly JSON files.",
        "",
        "    Features:",
        "    - Incremental saves (only writes changed components)",
        "    - Atomic writes (write to temp, then rename)",
        "    - Content hashing for change detection",
        "",
        "    Usage:",
        "        writer = StateWriter('corpus_state')",
        "        writer.save_all(layers, documents, metadata, embeddings, relations)",
        "",
        "        # Or incrementally:",
        "        writer.save_layer(layers[CorticalLayer.TOKENS])",
        "        writer.save_documents(documents, metadata)",
        "        writer.save_manifest()",
        "    \"\"\"",
        "",
        "    def __init__(self, state_dir: str):",
        "        \"\"\"",
        "        Initialize state writer.",
        "",
        "        Args:",
        "            state_dir: Directory to write state files",
        "        \"\"\"",
        "        self.state_dir = Path(state_dir)",
        "        self.layers_dir = self.state_dir / 'layers'",
        "        self.computed_dir = self.state_dir / 'computed'",
        "        self.manifest: Optional[StateManifest] = None",
        "        self._load_or_create_manifest()",
        "",
        "    def _load_or_create_manifest(self) -> None:",
        "        \"\"\"Load existing manifest or create new one.\"\"\"",
        "        manifest_path = self.state_dir / 'manifest.json'",
        "        if manifest_path.exists():",
        "            try:",
        "                with open(manifest_path, 'r', encoding='utf-8') as f:",
        "                    data = json.load(f)",
        "                self.manifest = StateManifest.from_dict(data)",
        "            except (json.JSONDecodeError, IOError):",
        "                self.manifest = StateManifest()",
        "        else:",
        "            self.manifest = StateManifest()",
        "",
        "    def _ensure_dirs(self) -> None:",
        "        \"\"\"Create directories if they don't exist.\"\"\"",
        "        self.state_dir.mkdir(parents=True, exist_ok=True)",
        "        self.layers_dir.mkdir(exist_ok=True)",
        "        self.computed_dir.mkdir(exist_ok=True)",
        "",
        "    def _atomic_write(self, filepath: Path, content: str) -> None:",
        "        \"\"\"",
        "        Write content atomically using temp file + rename.",
        "",
        "        This prevents data corruption if the process crashes mid-write.",
        "        \"\"\"",
        "        temp_path = filepath.with_suffix('.json.tmp')",
        "        try:",
        "            with open(temp_path, 'w', encoding='utf-8') as f:",
        "                f.write(content)",
        "            temp_path.replace(filepath)",
        "        except Exception:",
        "            if temp_path.exists():",
        "                temp_path.unlink()",
        "            raise",
        "",
        "    def save_layer(",
        "        self,",
        "        layer: HierarchicalLayer,",
        "        force: bool = False",
        "    ) -> bool:",
        "        \"\"\"",
        "        Save a single layer to its JSON file.",
        "",
        "        Args:",
        "            layer: The layer to save",
        "            force: Save even if checksum unchanged",
        "",
        "        Returns:",
        "            True if file was written, False if skipped (unchanged)",
        "        \"\"\"",
        "        self._ensure_dirs()",
        "",
        "        filename = LAYER_FILENAMES.get(layer.level)",
        "        if not filename:",
        "            raise ValueError(f\"Unknown layer level: {layer.level}\")",
        "",
        "        filepath = self.layers_dir / filename",
        "        component_name = f\"layer_{layer.level}\"",
        "",
        "        # Serialize layer",
        "        layer_data = layer.to_dict()",
        "        content = json.dumps(layer_data, indent=2, ensure_ascii=False)",
        "",
        "        # Check if changed",
        "        changed = self.manifest.update_checksum(component_name, content)",
        "",
        "        if not changed and not force:",
        "            return False",
        "",
        "        self._atomic_write(filepath, content)",
        "        self.manifest.layer_stats[f\"L{layer.level}\"] = len(layer.minicolumns)",
        "",
        "        return True",
        "",
        "    def save_documents(",
        "        self,",
        "        documents: Dict[str, str],",
        "        document_metadata: Optional[Dict[str, Dict[str, Any]]] = None,",
        "        force: bool = False",
        "    ) -> bool:",
        "        \"\"\"",
        "        Save documents and metadata.",
        "",
        "        Args:",
        "            documents: Document ID to content mapping",
        "            document_metadata: Document ID to metadata mapping",
        "            force: Save even if unchanged",
        "",
        "        Returns:",
        "            True if file was written",
        "        \"\"\"",
        "        self._ensure_dirs()",
        "",
        "        filepath = self.state_dir / 'documents.json'",
        "",
        "        data = {",
        "            'documents': documents,",
        "            'metadata': document_metadata or {}",
        "        }",
        "        content = json.dumps(data, indent=2, ensure_ascii=False)",
        "",
        "        changed = self.manifest.update_checksum('documents', content)",
        "",
        "        if not changed and not force:",
        "            return False",
        "",
        "        self._atomic_write(filepath, content)",
        "        self.manifest.document_count = len(documents)",
        "",
        "        return True",
        "",
        "    def save_semantic_relations(",
        "        self,",
        "        relations: List[Tuple],",
        "        force: bool = False",
        "    ) -> bool:",
        "        \"\"\"",
        "        Save semantic relations.",
        "",
        "        Args:",
        "            relations: List of (term1, relation, term2, weight) tuples",
        "            force: Save even if unchanged",
        "",
        "        Returns:",
        "            True if file was written",
        "        \"\"\"",
        "        self._ensure_dirs()",
        "",
        "        filepath = self.computed_dir / 'semantic_relations.json'",
        "",
        "        # Convert tuples to lists for JSON",
        "        data = {",
        "            'relations': [list(r) for r in relations],",
        "            'count': len(relations)",
        "        }",
        "        content = json.dumps(data, indent=2, ensure_ascii=False)",
        "",
        "        changed = self.manifest.update_checksum('semantic_relations', content)",
        "",
        "        if not changed and not force:",
        "            return False",
        "",
        "        self._atomic_write(filepath, content)",
        "        return True",
        "",
        "    def save_embeddings(",
        "        self,",
        "        embeddings: Dict[str, List[float]],",
        "        force: bool = False",
        "    ) -> bool:",
        "        \"\"\"",
        "        Save graph embeddings.",
        "",
        "        Args:",
        "            embeddings: Term to embedding vector mapping",
        "            force: Save even if unchanged",
        "",
        "        Returns:",
        "            True if file was written",
        "        \"\"\"",
        "        self._ensure_dirs()",
        "",
        "        filepath = self.computed_dir / 'embeddings.json'",
        "",
        "        data = {",
        "            'embeddings': embeddings,",
        "            'dimensions': len(next(iter(embeddings.values()))) if embeddings else 0,",
        "            'count': len(embeddings)",
        "        }",
        "        content = json.dumps(data, indent=2, ensure_ascii=False)",
        "",
        "        changed = self.manifest.update_checksum('embeddings', content)",
        "",
        "        if not changed and not force:",
        "            return False",
        "",
        "        self._atomic_write(filepath, content)",
        "        return True",
        "",
        "    def save_manifest(self) -> None:",
        "        \"\"\"Save the manifest file.\"\"\"",
        "        self._ensure_dirs()",
        "        filepath = self.state_dir / 'manifest.json'",
        "        content = json.dumps(self.manifest.to_dict(), indent=2, ensure_ascii=False)",
        "        self._atomic_write(filepath, content)",
        "",
        "    def save_all(",
        "        self,",
        "        layers: Dict[CorticalLayer, HierarchicalLayer],",
        "        documents: Dict[str, str],",
        "        document_metadata: Optional[Dict[str, Dict[str, Any]]] = None,",
        "        embeddings: Optional[Dict[str, List[float]]] = None,",
        "        semantic_relations: Optional[List[Tuple]] = None,",
        "        stale_computations: Optional[Set[str]] = None,",
        "        force: bool = False,",
        "        verbose: bool = True",
        "    ) -> Dict[str, bool]:",
        "        \"\"\"",
        "        Save all processor state.",
        "",
        "        Args:",
        "            layers: Dictionary of all layers",
        "            documents: Document collection",
        "            document_metadata: Per-document metadata",
        "            embeddings: Graph embeddings",
        "            semantic_relations: Extracted relations",
        "            stale_computations: Set of stale computation names",
        "            force: Force save even if unchanged",
        "            verbose: Log progress",
        "",
        "        Returns:",
        "            Dictionary of component -> was_written",
        "        \"\"\"",
        "        results = {}",
        "",
        "        # Save layers",
        "        for layer_enum, layer in layers.items():",
        "            key = f\"layer_{layer_enum.value}\"",
        "            results[key] = self.save_layer(layer, force=force)",
        "            if verbose and results[key]:",
        "                logger.info(f\"  Saved {LAYER_FILENAMES[layer_enum.value]}: {len(layer.minicolumns)} minicolumns\")",
        "",
        "        # Save documents",
        "        results['documents'] = self.save_documents(documents, document_metadata, force=force)",
        "        if verbose and results['documents']:",
        "            logger.info(f\"  Saved documents.json: {len(documents)} documents\")",
        "",
        "        # Save computed values",
        "        if semantic_relations is not None:",
        "            results['semantic_relations'] = self.save_semantic_relations(semantic_relations, force=force)",
        "            if verbose and results['semantic_relations']:",
        "                logger.info(f\"  Saved semantic_relations.json: {len(semantic_relations)} relations\")",
        "",
        "        if embeddings is not None:",
        "            results['embeddings'] = self.save_embeddings(embeddings, force=force)",
        "            if verbose and results['embeddings']:",
        "                logger.info(f\"  Saved embeddings.json: {len(embeddings)} embeddings\")",
        "",
        "        # Update staleness tracking",
        "        if stale_computations is not None:",
        "            self.manifest.stale_computations = list(stale_computations)",
        "",
        "        # Save manifest",
        "        self.save_manifest()",
        "",
        "        if verbose:",
        "            saved_count = sum(1 for v in results.values() if v)",
        "            logger.info(f\"âœ“ Saved state to {self.state_dir} ({saved_count} files updated)\")",
        "",
        "        return results",
        "",
        "",
        "class StateLoader:",
        "    \"\"\"",
        "    Loads processor state from git-friendly JSON files.",
        "",
        "    Features:",
        "    - Validates checksums before loading",
        "    - Reports missing or corrupted components",
        "    - Provides incremental loading (load only what you need)",
        "",
        "    Usage:",
        "        loader = StateLoader('corpus_state')",
        "",
        "        # Load everything",
        "        state = loader.load_all()",
        "",
        "        # Or load selectively",
        "        layer0 = loader.load_layer(0)",
        "        docs = loader.load_documents()",
        "    \"\"\"",
        "",
        "    def __init__(self, state_dir: str):",
        "        \"\"\"",
        "        Initialize state loader.",
        "",
        "        Args:",
        "            state_dir: Directory containing state files",
        "        \"\"\"",
        "        self.state_dir = Path(state_dir)",
        "        self.layers_dir = self.state_dir / 'layers'",
        "        self.computed_dir = self.state_dir / 'computed'",
        "        self.manifest: Optional[StateManifest] = None",
        "",
        "    def exists(self) -> bool:",
        "        \"\"\"Check if state directory exists and has manifest.\"\"\"",
        "        return (self.state_dir / 'manifest.json').exists()",
        "",
        "    def load_manifest(self) -> StateManifest:",
        "        \"\"\"",
        "        Load the manifest file.",
        "",
        "        Returns:",
        "            StateManifest object",
        "",
        "        Raises:",
        "            FileNotFoundError: If manifest doesn't exist",
        "        \"\"\"",
        "        manifest_path = self.state_dir / 'manifest.json'",
        "        if not manifest_path.exists():",
        "            raise FileNotFoundError(f\"No manifest found at {manifest_path}\")",
        "",
        "        with open(manifest_path, 'r', encoding='utf-8') as f:",
        "            data = json.load(f)",
        "",
        "        self.manifest = StateManifest.from_dict(data)",
        "        return self.manifest",
        "",
        "    def validate_checksum(self, component: str, filepath: Path) -> bool:",
        "        \"\"\"",
        "        Validate a component's checksum.",
        "",
        "        Args:",
        "            component: Component name",
        "            filepath: Path to the component file",
        "",
        "        Returns:",
        "            True if checksum matches, False otherwise",
        "        \"\"\"",
        "        if self.manifest is None:",
        "            self.load_manifest()",
        "",
        "        expected = self.manifest.checksums.get(component)",
        "        if expected is None:",
        "            return True  # No checksum stored, assume valid",
        "",
        "        if not filepath.exists():",
        "            return False",
        "",
        "        with open(filepath, 'r', encoding='utf-8') as f:",
        "            content = f.read()",
        "",
        "        actual = hashlib.sha256(content.encode('utf-8')).hexdigest()[:16]",
        "        return actual == expected",
        "",
        "    def load_layer(self, level: int) -> HierarchicalLayer:",
        "        \"\"\"",
        "        Load a single layer.",
        "",
        "        Args:",
        "            level: Layer level (0-3)",
        "",
        "        Returns:",
        "            HierarchicalLayer object",
        "",
        "        Raises:",
        "            FileNotFoundError: If layer file doesn't exist",
        "            ValueError: If layer data is invalid",
        "        \"\"\"",
        "        filename = LAYER_FILENAMES.get(level)",
        "        if not filename:",
        "            raise ValueError(f\"Unknown layer level: {level}\")",
        "",
        "        filepath = self.layers_dir / filename",
        "        if not filepath.exists():",
        "            raise FileNotFoundError(f\"Layer file not found: {filepath}\")",
        "",
        "        with open(filepath, 'r', encoding='utf-8') as f:",
        "            data = json.load(f)",
        "",
        "        return HierarchicalLayer.from_dict(data)",
        "",
        "    def load_documents(self) -> Tuple[Dict[str, str], Dict[str, Dict[str, Any]]]:",
        "        \"\"\"",
        "        Load documents and metadata.",
        "",
        "        Returns:",
        "            Tuple of (documents, metadata)",
        "",
        "        Raises:",
        "            FileNotFoundError: If documents file doesn't exist",
        "        \"\"\"",
        "        filepath = self.state_dir / 'documents.json'",
        "        if not filepath.exists():",
        "            raise FileNotFoundError(f\"Documents file not found: {filepath}\")",
        "",
        "        with open(filepath, 'r', encoding='utf-8') as f:",
        "            data = json.load(f)",
        "",
        "        return data.get('documents', {}), data.get('metadata', {})",
        "",
        "    def load_semantic_relations(self) -> List[Tuple]:",
        "        \"\"\"",
        "        Load semantic relations.",
        "",
        "        Returns:",
        "            List of (term1, relation, term2, weight) tuples",
        "        \"\"\"",
        "        filepath = self.computed_dir / 'semantic_relations.json'",
        "        if not filepath.exists():",
        "            return []",
        "",
        "        with open(filepath, 'r', encoding='utf-8') as f:",
        "            data = json.load(f)",
        "",
        "        # Convert lists back to tuples",
        "        return [tuple(r) for r in data.get('relations', [])]",
        "",
        "    def load_embeddings(self) -> Dict[str, List[float]]:",
        "        \"\"\"",
        "        Load graph embeddings.",
        "",
        "        Returns:",
        "            Term to embedding vector mapping",
        "        \"\"\"",
        "        filepath = self.computed_dir / 'embeddings.json'",
        "        if not filepath.exists():",
        "            return {}",
        "",
        "        with open(filepath, 'r', encoding='utf-8') as f:",
        "            data = json.load(f)",
        "",
        "        return data.get('embeddings', {})",
        "",
        "    def load_all(",
        "        self,",
        "        validate: bool = True,",
        "        verbose: bool = True",
        "    ) -> Tuple[",
        "        Dict[CorticalLayer, HierarchicalLayer],",
        "        Dict[str, str],",
        "        Dict[str, Dict[str, Any]],",
        "        Dict[str, List[float]],",
        "        List[Tuple],",
        "        Dict[str, Any]",
        "    ]:",
        "        \"\"\"",
        "        Load all processor state.",
        "",
        "        Args:",
        "            validate: Validate checksums before loading",
        "            verbose: Log progress",
        "",
        "        Returns:",
        "            Tuple of (layers, documents, metadata, embeddings, relations, manifest_data)",
        "",
        "        Raises:",
        "            FileNotFoundError: If required files don't exist",
        "            ValueError: If checksums don't match (when validate=True)",
        "        \"\"\"",
        "        # Load manifest first",
        "        manifest = self.load_manifest()",
        "",
        "        if verbose:",
        "            logger.info(f\"Loading state from {self.state_dir}\")",
        "",
        "        # Load layers",
        "        layers = {}",
        "        for level in range(4):",
        "            try:",
        "                layer = self.load_layer(level)",
        "                layers[CorticalLayer(level)] = layer",
        "                if verbose:",
        "                    logger.info(f\"  Loaded {LAYER_FILENAMES[level]}: {len(layer.minicolumns)} minicolumns\")",
        "            except FileNotFoundError:",
        "                if verbose:",
        "                    logger.warning(f\"  Layer {level} not found, creating empty\")",
        "                layers[CorticalLayer(level)] = HierarchicalLayer(CorticalLayer(level))",
        "",
        "        # Load documents",
        "        try:",
        "            documents, metadata = self.load_documents()",
        "            if verbose:",
        "                logger.info(f\"  Loaded documents.json: {len(documents)} documents\")",
        "        except FileNotFoundError:",
        "            documents = {}",
        "            metadata = {}",
        "            if verbose:",
        "                logger.warning(\"  Documents not found, starting empty\")",
        "",
        "        # Load computed values",
        "        relations = self.load_semantic_relations()",
        "        if verbose and relations:",
        "            logger.info(f\"  Loaded semantic_relations.json: {len(relations)} relations\")",
        "",
        "        embeddings = self.load_embeddings()",
        "        if verbose and embeddings:",
        "            logger.info(f\"  Loaded embeddings.json: {len(embeddings)} embeddings\")",
        "",
        "        if verbose:",
        "            logger.info(f\"âœ“ Loaded state from {self.state_dir}\")",
        "",
        "        # Build metadata dict similar to pkl format",
        "        manifest_data = {",
        "            'version': manifest.version,",
        "            'stale_computations': set(manifest.stale_computations)",
        "        }",
        "",
        "        return layers, documents, metadata, embeddings, relations, manifest_data",
        "",
        "    def get_stats(self) -> Dict[str, Any]:",
        "        \"\"\"",
        "        Get statistics about stored state without loading everything.",
        "",
        "        Returns:",
        "            Dictionary of statistics",
        "        \"\"\"",
        "        if self.manifest is None:",
        "            try:",
        "                self.load_manifest()",
        "            except FileNotFoundError:",
        "                return {'exists': False}",
        "",
        "        return {",
        "            'exists': True,",
        "            'version': self.manifest.version,",
        "            'created_at': self.manifest.created_at,",
        "            'updated_at': self.manifest.updated_at,",
        "            'document_count': self.manifest.document_count,",
        "            'layer_stats': self.manifest.layer_stats,",
        "            'stale_computations': self.manifest.stale_computations,",
        "            'components': list(self.manifest.checksums.keys())",
        "        }",
        "",
        "",
        "def migrate_pkl_to_json(",
        "    pkl_path: str,",
        "    json_dir: str,",
        "    verbose: bool = True",
        ") -> bool:",
        "    \"\"\"",
        "    Migrate a pickle file to git-friendly JSON format.",
        "",
        "    Args:",
        "        pkl_path: Path to existing .pkl file",
        "        json_dir: Directory to write JSON state",
        "        verbose: Log progress",
        "",
        "    Returns:",
        "        True if migration successful",
        "",
        "    Raises:",
        "        FileNotFoundError: If pkl file doesn't exist",
        "    \"\"\"",
        "    from .persistence import load_processor",
        "",
        "    if verbose:",
        "        logger.info(f\"Migrating {pkl_path} to {json_dir}\")",
        "",
        "    # Load from pkl",
        "    layers, documents, metadata, embeddings, relations, pkl_metadata = load_processor(",
        "        pkl_path, verbose=False",
        "    )",
        "",
        "    # Get stale computations from pkl metadata if present",
        "    stale = pkl_metadata.get('stale_computations', set()) if pkl_metadata else set()",
        "",
        "    # Save as JSON",
        "    writer = StateWriter(json_dir)",
        "    writer.save_all(",
        "        layers=layers,",
        "        documents=documents,",
        "        document_metadata=metadata,",
        "        embeddings=embeddings,",
        "        semantic_relations=relations,",
        "        stale_computations=stale,",
        "        force=True,",
        "        verbose=verbose",
        "    )",
        "",
        "    if verbose:",
        "        logger.info(f\"âœ“ Migration complete: {pkl_path} â†’ {json_dir}\")",
        "",
        "    return True"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "docs/director-continuation-prompt.md",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "# Director Agent Continuation Prompt",
        "",
        "Use this prompt to start a new conversation where Claude acts as a Director orchestrating parallel sub-agents.",
        "",
        "---",
        "",
        "## Prompt",
        "",
        "```",
        "You are the Director Agent for the Cortical Text Processor project. Your role is to orchestrate parallel sub-agents to accomplish complex tasks efficiently.",
        "",
        "## Project Context",
        "",
        "**Cortical Text Processor** is a zero-dependency Python library for hierarchical text analysis (~10,700 lines). It implements PageRank, TF-IDF, Louvain clustering, and semantic relation extraction.",
        "",
        "**Repository:** Working on branch `claude/replace-pkl-git-friendly-01DD3ra3P5hj9NK57johJff5`",
        "",
        "**Key Documentation:**",
        "- `CLAUDE.md` - Complete development guide (READ THIS FIRST)",
        "- `TASK_LIST.md` - Active backlog with pending tasks",
        "- `docs/merge-friendly-tasks.md` - Task ID system for parallel agents",
        "- `tasks/*.json` - Merge-friendly task files",
        "",
        "## Current State (as of 2025-12-14)",
        "",
        "**Recently Completed:**",
        "- Phase 1 of Task #206: Created `cortical/state_storage.py` with StateWriter/StateLoader for git-friendly JSON storage",
        "- 28 unit tests in `tests/unit/test_state_storage.py`",
        "- Merge-friendly task management system fully operational",
        "",
        "**Pending Tasks (from tasks/*.json):**",
        "| Task ID | Title | Priority |",
        "|---------|-------|----------|",
        "| T-8d66-001 | Add session context generator for agent handoff | high |",
        "| T-8d66-003 | Add progress checkpointing for compute_all() | medium |",
        "| T-8d66-004 | Auto-suggest tasks from code changes | low |",
        "| T-e233-03 | Test task recovery from crash scenario | medium |",
        "| T-6ac7-06 | Add task retrospective metadata capture | low |",
        "| T-1a1d-001 | Optimize doc_name_boost: cache tokenized document names | low |",
        "",
        "**From TASK_LIST.md (legacy format):**",
        "| # | Task | Priority |",
        "|---|------|----------|",
        "| 206 | Replace pkl with git-friendly JSON (Phase 2: processor integration) | high |",
        "| 133 | Implement WAL + snapshot persistence | medium |",
        "| 134 | Implement protobuf serialization | medium |",
        "| 135 | Chunked parallel processing for compute_all() | medium |",
        "",
        "## Your Director Responsibilities",
        "",
        "1. **Analyze the request** - Understand what the user wants accomplished",
        "2. **Decompose into sub-tasks** - Break into independent, parallelizable units",
        "3. **Group by dependency** - Tasks with no dependencies can run in parallel",
        "4. **Delegate with full context** - Each sub-agent gets everything they need",
        "5. **Consolidate results** - Merge outputs, resolve conflicts, report status",
        "",
        "## Delegation Protocol",
        "",
        "When spawning sub-agents, use the Task tool with this context structure:",
        "",
        "```",
        "GOAL: [Single measurable outcome]",
        "",
        "SCOPE:",
        "- Files to READ: [list]",
        "- Files to MODIFY: [list - non-overlapping across agents]",
        "- Files to CREATE: [list]",
        "",
        "CONTEXT:",
        "- [Key fact 1 they need to know]",
        "- [Key fact 2]",
        "- [Entry point: file:line]",
        "",
        "CONSTRAINTS:",
        "- Do NOT modify: [files owned by other agents]",
        "- Do NOT add: [unnecessary features]",
        "- Must maintain: [test coverage, backwards compatibility]",
        "",
        "DELIVERABLE:",
        "- [Exact output format]",
        "- [What to report back]",
        "",
        "TASK ID: T-YYYYMMDD-HHMMSS-XXXX (use scripts/task_utils.py to generate)",
        "```",
        "",
        "## Parallel Execution Groups",
        "",
        "**Group A - Agent Infrastructure (no code dependencies):**",
        "- T-8d66-001: Session context generator",
        "- T-6ac7-06: Task retrospective metadata",
        "",
        "**Group B - Processor Enhancements (touches cortical/):**",
        "- Task #206 Phase 2: Processor integration for state_storage.py",
        "- T-8d66-003: Progress checkpointing",
        "",
        "**Group C - Performance (isolated optimizations):**",
        "- T-1a1d-001: doc_name_boost caching",
        "",
        "**Rule:** Never assign agents in the same group to modify overlapping files.",
        "",
        "## Merge-Friendly Task IDs",
        "",
        "All new tasks must use the merge-friendly format:",
        "```bash",
        "python scripts/task_utils.py generate",
        "# Output: T-20251214-123456-a1b2",
        "```",
        "",
        "Each agent session writes to its own file in `tasks/`. No merge conflicts.",
        "",
        "## Git Protocol",
        "",
        "- Always work on branch: `claude/replace-pkl-git-friendly-01DD3ra3P5hj9NK57johJff5`",
        "- Push with: `git push -u origin <branch-name>`",
        "- Pull latest before starting: `git fetch origin main && git merge origin/main`",
        "",
        "## Success Criteria for This Session",
        "",
        "1. All delegated tasks have clear ownership (no overlaps)",
        "2. Sub-agents can work without asking clarifying questions",
        "3. Results are consolidated into commits with clear messages",
        "4. Task files updated to reflect completed work",
        "5. No merge conflicts between parallel agents",
        "",
        "---",
        "",
        "Start by:",
        "1. Reading CLAUDE.md for full context",
        "2. Checking `git status` and pulling latest from main",
        "3. Reviewing tasks/*.json for current pending work",
        "4. Asking the user what they want to accomplish, OR",
        "5. Proposing which task groups to tackle based on priority",
        "```",
        "",
        "---",
        "",
        "## Usage",
        "",
        "Copy everything between the ``` marks above and paste as the first message in a new Claude conversation. The agent will have full context to act as a Director.",
        "",
        "## Customization Points",
        "",
        "- **Update \"Current State\"** section after each session",
        "- **Add completed tasks** to the \"Recently Completed\" section",
        "- **Remove tasks** that are done from the pending lists",
        "- **Adjust parallel groups** as dependencies change"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "tasks/2025-12-14_00-21-46_8d66.json",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "{",
        "  \"version\": 1,",
        "  \"session_id\": \"8d66\",",
        "  \"started_at\": \"2025-12-14T00:21:46.447704\",",
        "  \"saved_at\": \"2025-12-14T00:27:34.846899\",",
        "  \"tasks\": [",
        "    {",
        "      \"id\": \"T-20251214-002146-8d66-001\",",
        "      \"title\": \"Add session context generator for agent handoff\",",
        "      \"status\": \"pending\",",
        "      \"priority\": \"high\",",
        "      \"category\": \"agent-dx\",",
        "      \"description\": \"When a new agent session starts, generate a concise \\\"catch-up\\\" summary:\\n- What work was done in the last N sessions\\n- Current pending tasks sorted by priority\\n- Recent file changes with semantic summaries\\n- Key decisions made (from commit messages)\\n\\nThis reduces the 'cold start' problem where agents spend time re-understanding context.\\n\\nImplementation:\\n- scripts/session_context.py with SessionContextGenerator class\\n- Read from tasks/*.json for pending/completed work\\n- Use git log for recent changes\\n- Output: markdown summary suitable for agent consumption\",",
        "      \"depends_on\": [],",
        "      \"effort\": \"medium\",",
        "      \"created_at\": \"2025-12-14T00:21:46.447746\",",
        "      \"updated_at\": null,",
        "      \"completed_at\": null,",
        "      \"context\": {",
        "        \"files\": [",
        "          \"scripts/task_utils.py\",",
        "          \"scripts/consolidate_tasks.py\"",
        "        ],",
        "        \"patterns\": [",
        "          \"chunk_index.py for similar git-friendly storage\"",
        "        ]",
        "      }",
        "    },",
        "    {",
        "      \"id\": \"T-20251214-002146-8d66-002\",",
        "      \"title\": \"Implement state_storage.py (Phase 1 of Task #206)\",",
        "      \"status\": \"completed\",",
        "      \"priority\": \"high\",",
        "      \"category\": \"arch\",",
        "      \"description\": \"Create cortical/state_storage.py with StateWriter and StateLoader classes.\\nThis is Phase 1 of Task #206 (git-friendly pkl replacement).\\n\\nKey classes:\\n- StateWriter: serialize layers, connections, computed values to JSON\\n- StateLoader: load state with hash validation  \\n- StateManifest: track versions, checksums, staleness\\n\\nDesign follows chunk_index.py pattern:\\n- Append-only where possible\\n- Content hashing for change detection\\n- Split large state into multiple files\\n\\nFiles to create:\\n- cortical/state_storage.py (main module)\\n- tests/unit/test_state_storage.py (unit tests)\\n\\nLeverage existing:\\n- Minicolumn.to_dict()/from_dict()\\n- HierarchicalLayer.to_dict()/from_dict()\\n- ChunkWriter/ChunkLoader patterns\",",
        "      \"depends_on\": [],",
        "      \"effort\": \"large\",",
        "      \"created_at\": \"2025-12-14T00:21:46.447756\",",
        "      \"updated_at\": \"2025-12-14T00:27:34.846812\",",
        "      \"completed_at\": \"2025-12-14T00:27:34.846812\",",
        "      \"context\": {",
        "        \"files\": [",
        "          \"cortical/persistence.py\",",
        "          \"cortical/chunk_index.py\",",
        "          \"cortical/minicolumn.py\",",
        "          \"cortical/layers.py\"",
        "        ],",
        "        \"methods\": [",
        "          \"save_processor()\",",
        "          \"load_processor()\",",
        "          \"ChunkWriter\",",
        "          \"ChunkLoader\"",
        "        ]",
        "      }",
        "    },",
        "    {",
        "      \"id\": \"T-20251214-002146-8d66-003\",",
        "      \"title\": \"Add progress checkpointing for compute_all()\",",
        "      \"status\": \"pending\",",
        "      \"priority\": \"medium\",",
        "      \"category\": \"agent-dx\",",
        "      \"description\": \"When compute_all() runs on large corpora, it can take 10+ minutes.\\nIf the agent session times out, all work is lost.\\n\\nSolution: Add checkpointing after each major phase:\\n1. After TF-IDF computation - save partial state\\n2. After bigram connections - save partial state  \\n3. After PageRank - save partial state\\n4. After concepts - save partial state\\n5. After semantics - final save\\n\\nBenefits:\\n- Resume from checkpoint if session times out\\n- Progress visibility (which phase are we on)\\n- Partial results usable even if later phases fail\\n\\nImplementation:\\n- Add checkpoint_dir parameter to compute_all()\\n- Save phase completions as JSON markers\\n- Add resume_from_checkpoint() method\",",
        "      \"depends_on\": [],",
        "      \"effort\": \"medium\",",
        "      \"created_at\": \"2025-12-14T00:21:46.447764\",",
        "      \"updated_at\": null,",
        "      \"completed_at\": null,",
        "      \"context\": {",
        "        \"files\": [",
        "          \"cortical/processor.py\"",
        "        ],",
        "        \"methods\": [",
        "          \"compute_all()\",",
        "          \"compute_tfidf()\",",
        "          \"compute_bigram_connections()\"",
        "        ]",
        "      }",
        "    },",
        "    {",
        "      \"id\": \"T-20251214-002146-8d66-004\",",
        "      \"title\": \"Auto-suggest tasks from code changes\",",
        "      \"status\": \"pending\",",
        "      \"priority\": \"low\",",
        "      \"category\": \"agent-dx\",",
        "      \"description\": \"When code is modified, automatically suggest follow-up tasks:\\n- Tests added? Suggest running test suite\\n- New public method? Suggest adding docstring\\n- Performance-sensitive code changed? Suggest profiling\\n- Validation logic changed? Suggest checking all related tests\\n\\nUses the semantic understanding from Cortical to identify patterns.\\n\\nImplementation:\\n- scripts/suggest_tasks.py\\n- Hook into git pre-commit or post-commit\\n- Output task suggestions in merge-friendly format\",",
        "      \"depends_on\": [],",
        "      \"effort\": \"medium\",",
        "      \"created_at\": \"2025-12-14T00:21:46.447781\",",
        "      \"updated_at\": null,",
        "      \"completed_at\": null,",
        "      \"context\": {",
        "        \"files\": [",
        "          \"scripts/task_utils.py\",",
        "          \"cortical/semantics.py\"",
        "        ],",
        "        \"patterns\": [",
        "          \"Could use search_codebase.py patterns\"",
        "        ]",
        "      }",
        "    }",
        "  ]",
        "}"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "tests/unit/test_state_storage.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "\"\"\"",
        "Unit tests for cortical/state_storage.py",
        "",
        "Tests the git-friendly JSON state storage system that replaces pickle-based persistence.",
        "\"\"\"",
        "",
        "import json",
        "import os",
        "import tempfile",
        "import shutil",
        "import unittest",
        "from pathlib import Path",
        "",
        "from cortical.state_storage import (",
        "    StateManifest,",
        "    StateWriter,",
        "    StateLoader,",
        "    migrate_pkl_to_json,",
        "    STATE_VERSION,",
        "    LAYER_FILENAMES,",
        ")",
        "from cortical.layers import CorticalLayer, HierarchicalLayer",
        "from cortical.minicolumn import Minicolumn, Edge",
        "",
        "",
        "class TestStateManifest(unittest.TestCase):",
        "    \"\"\"Tests for StateManifest dataclass.\"\"\"",
        "",
        "    def test_default_values(self):",
        "        \"\"\"Test manifest creates with sensible defaults.\"\"\"",
        "        manifest = StateManifest()",
        "        self.assertEqual(manifest.version, STATE_VERSION)",
        "        self.assertIsNotNone(manifest.created_at)",
        "        self.assertIsNotNone(manifest.updated_at)",
        "        self.assertEqual(manifest.checksums, {})",
        "        self.assertEqual(manifest.stale_computations, [])",
        "        self.assertEqual(manifest.document_count, 0)",
        "",
        "    def test_to_dict_from_dict_roundtrip(self):",
        "        \"\"\"Test manifest serialization roundtrip.\"\"\"",
        "        manifest = StateManifest(",
        "            version=1,",
        "            checksums={'layer_0': 'abc123'},",
        "            stale_computations=['pagerank', 'tfidf'],",
        "            document_count=42",
        "        )",
        "        data = manifest.to_dict()",
        "        restored = StateManifest.from_dict(data)",
        "",
        "        self.assertEqual(restored.version, manifest.version)",
        "        self.assertEqual(restored.checksums, manifest.checksums)",
        "        self.assertEqual(restored.stale_computations, manifest.stale_computations)",
        "        self.assertEqual(restored.document_count, manifest.document_count)",
        "",
        "    def test_update_checksum_detects_change(self):",
        "        \"\"\"Test checksum update returns True for new/changed content.\"\"\"",
        "        manifest = StateManifest()",
        "",
        "        # First update should return True (new)",
        "        changed = manifest.update_checksum('test', 'content1')",
        "        self.assertTrue(changed)",
        "",
        "        # Same content should return False",
        "        changed = manifest.update_checksum('test', 'content1')",
        "        self.assertFalse(changed)",
        "",
        "        # Different content should return True",
        "        changed = manifest.update_checksum('test', 'content2')",
        "        self.assertTrue(changed)",
        "",
        "    def test_update_checksum_updates_timestamp(self):",
        "        \"\"\"Test that updating checksum updates the timestamp.\"\"\"",
        "        manifest = StateManifest()",
        "        original_time = manifest.updated_at",
        "",
        "        # Small delay to ensure different timestamp",
        "        import time",
        "        time.sleep(0.01)",
        "",
        "        manifest.update_checksum('test', 'content')",
        "        self.assertNotEqual(manifest.updated_at, original_time)",
        "",
        "",
        "class TestStateWriter(unittest.TestCase):",
        "    \"\"\"Tests for StateWriter class.\"\"\"",
        "",
        "    def setUp(self):",
        "        \"\"\"Create a temporary directory for each test.\"\"\"",
        "        self.temp_dir = tempfile.mkdtemp()",
        "        self.state_dir = os.path.join(self.temp_dir, 'corpus_state')",
        "",
        "    def tearDown(self):",
        "        \"\"\"Clean up temporary directory.\"\"\"",
        "        shutil.rmtree(self.temp_dir)",
        "",
        "    def _create_test_layer(self, level: int, num_cols: int = 3) -> HierarchicalLayer:",
        "        \"\"\"Create a test layer with sample minicolumns.\"\"\"",
        "        layer = HierarchicalLayer(CorticalLayer(level))",
        "        for i in range(num_cols):",
        "            col = Minicolumn(f\"L{level}_term{i}\", f\"term{i}\", level)",
        "            col.occurrence_count = i + 1",
        "            col.tfidf = 0.5 + i * 0.1",
        "            col.pagerank = 0.1 + i * 0.05",
        "            layer.minicolumns[f\"term{i}\"] = col",
        "            layer._id_index[col.id] = f\"term{i}\"",
        "        return layer",
        "",
        "    def test_creates_directory_structure(self):",
        "        \"\"\"Test that writer creates required directories.\"\"\"",
        "        writer = StateWriter(self.state_dir)",
        "        layer = self._create_test_layer(0)",
        "        writer.save_layer(layer)",
        "",
        "        self.assertTrue(os.path.exists(self.state_dir))",
        "        self.assertTrue(os.path.exists(os.path.join(self.state_dir, 'layers')))",
        "        self.assertTrue(os.path.exists(os.path.join(self.state_dir, 'computed')))",
        "",
        "    def test_save_layer(self):",
        "        \"\"\"Test saving a single layer.\"\"\"",
        "        writer = StateWriter(self.state_dir)",
        "        layer = self._create_test_layer(0, num_cols=5)",
        "",
        "        result = writer.save_layer(layer)",
        "",
        "        self.assertTrue(result)  # File was written",
        "        filepath = os.path.join(self.state_dir, 'layers', 'L0_tokens.json')",
        "        self.assertTrue(os.path.exists(filepath))",
        "",
        "        # Verify content",
        "        with open(filepath) as f:",
        "            data = json.load(f)",
        "        self.assertEqual(data['level'], 0)",
        "        self.assertEqual(len(data['minicolumns']), 5)",
        "",
        "    def test_save_layer_skips_unchanged(self):",
        "        \"\"\"Test that saving same layer twice skips second write.\"\"\"",
        "        writer = StateWriter(self.state_dir)",
        "        layer = self._create_test_layer(0)",
        "",
        "        first_result = writer.save_layer(layer)",
        "        second_result = writer.save_layer(layer)",
        "",
        "        self.assertTrue(first_result)",
        "        self.assertFalse(second_result)  # Skipped, unchanged",
        "",
        "    def test_save_layer_force_writes_unchanged(self):",
        "        \"\"\"Test that force=True writes even if unchanged.\"\"\"",
        "        writer = StateWriter(self.state_dir)",
        "        layer = self._create_test_layer(0)",
        "",
        "        writer.save_layer(layer)",
        "        result = writer.save_layer(layer, force=True)",
        "",
        "        self.assertTrue(result)  # Force write",
        "",
        "    def test_save_documents(self):",
        "        \"\"\"Test saving documents and metadata.\"\"\"",
        "        writer = StateWriter(self.state_dir)",
        "        documents = {'doc1': 'Content one', 'doc2': 'Content two'}",
        "        metadata = {'doc1': {'source': 'test'}}",
        "",
        "        result = writer.save_documents(documents, metadata)",
        "",
        "        self.assertTrue(result)",
        "        filepath = os.path.join(self.state_dir, 'documents.json')",
        "        self.assertTrue(os.path.exists(filepath))",
        "",
        "        with open(filepath) as f:",
        "            data = json.load(f)",
        "        self.assertEqual(data['documents'], documents)",
        "        self.assertEqual(data['metadata'], metadata)",
        "",
        "    def test_save_semantic_relations(self):",
        "        \"\"\"Test saving semantic relations.\"\"\"",
        "        writer = StateWriter(self.state_dir)",
        "        relations = [",
        "            ('neural', 'RelatedTo', 'network', 0.8),",
        "            ('machine', 'PartOf', 'learning', 0.9),",
        "        ]",
        "",
        "        result = writer.save_semantic_relations(relations)",
        "",
        "        self.assertTrue(result)",
        "        filepath = os.path.join(self.state_dir, 'computed', 'semantic_relations.json')",
        "        self.assertTrue(os.path.exists(filepath))",
        "",
        "        with open(filepath) as f:",
        "            data = json.load(f)",
        "        self.assertEqual(len(data['relations']), 2)",
        "        self.assertEqual(data['count'], 2)",
        "",
        "    def test_save_embeddings(self):",
        "        \"\"\"Test saving embeddings.\"\"\"",
        "        writer = StateWriter(self.state_dir)",
        "        embeddings = {",
        "            'neural': [0.1, 0.2, 0.3],",
        "            'network': [0.4, 0.5, 0.6],",
        "        }",
        "",
        "        result = writer.save_embeddings(embeddings)",
        "",
        "        self.assertTrue(result)",
        "        filepath = os.path.join(self.state_dir, 'computed', 'embeddings.json')",
        "        self.assertTrue(os.path.exists(filepath))",
        "",
        "        with open(filepath) as f:",
        "            data = json.load(f)",
        "        self.assertEqual(data['embeddings'], embeddings)",
        "        self.assertEqual(data['dimensions'], 3)",
        "        self.assertEqual(data['count'], 2)",
        "",
        "    def test_save_all(self):",
        "        \"\"\"Test saving complete processor state.\"\"\"",
        "        writer = StateWriter(self.state_dir)",
        "",
        "        layers = {",
        "            CorticalLayer(i): self._create_test_layer(i)",
        "            for i in range(4)",
        "        }",
        "        documents = {'doc1': 'Test content'}",
        "        metadata = {'doc1': {'source': 'unit_test'}}",
        "        embeddings = {'term': [0.1, 0.2]}",
        "        relations = [('a', 'rel', 'b', 0.5)]",
        "",
        "        results = writer.save_all(",
        "            layers=layers,",
        "            documents=documents,",
        "            document_metadata=metadata,",
        "            embeddings=embeddings,",
        "            semantic_relations=relations,",
        "            stale_computations={'pagerank'},",
        "            verbose=False",
        "        )",
        "",
        "        # All components should be written",
        "        self.assertTrue(results['layer_0'])",
        "        self.assertTrue(results['layer_1'])",
        "        self.assertTrue(results['layer_2'])",
        "        self.assertTrue(results['layer_3'])",
        "        self.assertTrue(results['documents'])",
        "        self.assertTrue(results['embeddings'])",
        "        self.assertTrue(results['semantic_relations'])",
        "",
        "        # Manifest should exist",
        "        manifest_path = os.path.join(self.state_dir, 'manifest.json')",
        "        self.assertTrue(os.path.exists(manifest_path))",
        "",
        "    def test_atomic_write_creates_valid_file(self):",
        "        \"\"\"Test atomic write produces valid JSON.\"\"\"",
        "        writer = StateWriter(self.state_dir)",
        "        layer = self._create_test_layer(0)",
        "        writer.save_layer(layer)",
        "",
        "        filepath = os.path.join(self.state_dir, 'layers', 'L0_tokens.json')",
        "",
        "        # Should be valid JSON",
        "        with open(filepath) as f:",
        "            data = json.load(f)",
        "        self.assertIsInstance(data, dict)",
        "",
        "        # No temp files should remain",
        "        temp_path = filepath + '.tmp'",
        "        self.assertFalse(os.path.exists(temp_path))",
        "",
        "",
        "class TestStateLoader(unittest.TestCase):",
        "    \"\"\"Tests for StateLoader class.\"\"\"",
        "",
        "    def setUp(self):",
        "        \"\"\"Create temp directory and write test state.\"\"\"",
        "        self.temp_dir = tempfile.mkdtemp()",
        "        self.state_dir = os.path.join(self.temp_dir, 'corpus_state')",
        "        self._write_test_state()",
        "",
        "    def tearDown(self):",
        "        \"\"\"Clean up temporary directory.\"\"\"",
        "        shutil.rmtree(self.temp_dir)",
        "",
        "    def _create_test_layer(self, level: int, num_cols: int = 3) -> HierarchicalLayer:",
        "        \"\"\"Create a test layer with sample minicolumns.\"\"\"",
        "        layer = HierarchicalLayer(CorticalLayer(level))",
        "        for i in range(num_cols):",
        "            col = Minicolumn(f\"L{level}_term{i}\", f\"term{i}\", level)",
        "            col.occurrence_count = i + 1",
        "            col.tfidf = 0.5 + i * 0.1",
        "            col.pagerank = 0.1 + i * 0.05",
        "            layer.minicolumns[f\"term{i}\"] = col",
        "            layer._id_index[col.id] = f\"term{i}\"",
        "        return layer",
        "",
        "    def _write_test_state(self):",
        "        \"\"\"Write test state files.\"\"\"",
        "        writer = StateWriter(self.state_dir)",
        "        layers = {",
        "            CorticalLayer(i): self._create_test_layer(i)",
        "            for i in range(4)",
        "        }",
        "        documents = {'doc1': 'Test content one', 'doc2': 'Test content two'}",
        "        metadata = {'doc1': {'source': 'test'}}",
        "        embeddings = {'term0': [0.1, 0.2, 0.3]}",
        "        relations = [('neural', 'RelatedTo', 'network', 0.8)]",
        "",
        "        writer.save_all(",
        "            layers=layers,",
        "            documents=documents,",
        "            document_metadata=metadata,",
        "            embeddings=embeddings,",
        "            semantic_relations=relations,",
        "            stale_computations={'pagerank'},",
        "            verbose=False",
        "        )",
        "",
        "    def test_exists(self):",
        "        \"\"\"Test exists() returns True for valid state.\"\"\"",
        "        loader = StateLoader(self.state_dir)",
        "        self.assertTrue(loader.exists())",
        "",
        "    def test_exists_false_for_missing(self):",
        "        \"\"\"Test exists() returns False for missing state.\"\"\"",
        "        loader = StateLoader('/nonexistent/path')",
        "        self.assertFalse(loader.exists())",
        "",
        "    def test_load_manifest(self):",
        "        \"\"\"Test loading manifest file.\"\"\"",
        "        loader = StateLoader(self.state_dir)",
        "        manifest = loader.load_manifest()",
        "",
        "        self.assertEqual(manifest.version, STATE_VERSION)",
        "        self.assertEqual(manifest.document_count, 2)",
        "        self.assertIn('pagerank', manifest.stale_computations)",
        "",
        "    def test_load_layer(self):",
        "        \"\"\"Test loading a single layer.\"\"\"",
        "        loader = StateLoader(self.state_dir)",
        "        layer = loader.load_layer(0)",
        "",
        "        self.assertEqual(layer.level, 0)",
        "        self.assertEqual(len(layer.minicolumns), 3)",
        "        self.assertIn('term0', layer.minicolumns)",
        "",
        "    def test_load_layer_invalid_level(self):",
        "        \"\"\"Test loading invalid layer level raises error.\"\"\"",
        "        loader = StateLoader(self.state_dir)",
        "",
        "        with self.assertRaises(ValueError):",
        "            loader.load_layer(5)",
        "",
        "    def test_load_documents(self):",
        "        \"\"\"Test loading documents and metadata.\"\"\"",
        "        loader = StateLoader(self.state_dir)",
        "        documents, metadata = loader.load_documents()",
        "",
        "        self.assertEqual(len(documents), 2)",
        "        self.assertIn('doc1', documents)",
        "        self.assertEqual(metadata['doc1']['source'], 'test')",
        "",
        "    def test_load_semantic_relations(self):",
        "        \"\"\"Test loading semantic relations.\"\"\"",
        "        loader = StateLoader(self.state_dir)",
        "        relations = loader.load_semantic_relations()",
        "",
        "        self.assertEqual(len(relations), 1)",
        "        self.assertEqual(relations[0], ('neural', 'RelatedTo', 'network', 0.8))",
        "",
        "    def test_load_semantic_relations_empty(self):",
        "        \"\"\"Test loading when no relations file exists.\"\"\"",
        "        # Remove the relations file",
        "        relations_path = os.path.join(self.state_dir, 'computed', 'semantic_relations.json')",
        "        os.remove(relations_path)",
        "",
        "        loader = StateLoader(self.state_dir)",
        "        relations = loader.load_semantic_relations()",
        "",
        "        self.assertEqual(relations, [])",
        "",
        "    def test_load_embeddings(self):",
        "        \"\"\"Test loading embeddings.\"\"\"",
        "        loader = StateLoader(self.state_dir)",
        "        embeddings = loader.load_embeddings()",
        "",
        "        self.assertIn('term0', embeddings)",
        "        self.assertEqual(embeddings['term0'], [0.1, 0.2, 0.3])",
        "",
        "    def test_load_all(self):",
        "        \"\"\"Test loading complete state.\"\"\"",
        "        loader = StateLoader(self.state_dir)",
        "        layers, documents, metadata, embeddings, relations, manifest_data = loader.load_all(verbose=False)",
        "",
        "        # Check layers",
        "        self.assertEqual(len(layers), 4)",
        "        for level in range(4):",
        "            self.assertIn(CorticalLayer(level), layers)",
        "            self.assertEqual(len(layers[CorticalLayer(level)].minicolumns), 3)",
        "",
        "        # Check documents",
        "        self.assertEqual(len(documents), 2)",
        "        self.assertIn('doc1', documents)",
        "",
        "        # Check computed values",
        "        self.assertEqual(len(relations), 1)",
        "        self.assertIn('term0', embeddings)",
        "",
        "        # Check manifest data",
        "        self.assertEqual(manifest_data['version'], STATE_VERSION)",
        "        self.assertIn('pagerank', manifest_data['stale_computations'])",
        "",
        "    def test_get_stats(self):",
        "        \"\"\"Test getting state statistics.\"\"\"",
        "        loader = StateLoader(self.state_dir)",
        "        stats = loader.get_stats()",
        "",
        "        self.assertTrue(stats['exists'])",
        "        self.assertEqual(stats['version'], STATE_VERSION)",
        "        self.assertEqual(stats['document_count'], 2)",
        "        self.assertIn('layer_0', stats['components'])",
        "",
        "    def test_get_stats_missing_state(self):",
        "        \"\"\"Test getting stats for missing state.\"\"\"",
        "        loader = StateLoader('/nonexistent')",
        "        stats = loader.get_stats()",
        "",
        "        self.assertFalse(stats['exists'])",
        "",
        "",
        "class TestStateRoundtrip(unittest.TestCase):",
        "    \"\"\"Tests for complete save/load roundtrip.\"\"\"",
        "",
        "    def setUp(self):",
        "        \"\"\"Create temp directory.\"\"\"",
        "        self.temp_dir = tempfile.mkdtemp()",
        "        self.state_dir = os.path.join(self.temp_dir, 'corpus_state')",
        "",
        "    def tearDown(self):",
        "        \"\"\"Clean up.\"\"\"",
        "        shutil.rmtree(self.temp_dir)",
        "",
        "    def test_full_roundtrip(self):",
        "        \"\"\"Test complete save and load cycle preserves data.\"\"\"",
        "        # Create test data",
        "        layers = {}",
        "        for level in range(4):",
        "            layer = HierarchicalLayer(CorticalLayer(level))",
        "            for i in range(5):",
        "                col = Minicolumn(f\"L{level}_term{i}\", f\"term{i}\", level)",
        "                col.occurrence_count = i * 10",
        "                col.tfidf = 0.1 * (i + 1)",
        "                col.pagerank = 0.05 * (i + 1)",
        "                col.document_ids = {f\"doc{j}\" for j in range(i + 1)}",
        "                layer.minicolumns[f\"term{i}\"] = col",
        "                layer._id_index[col.id] = f\"term{i}\"",
        "            layers[CorticalLayer(level)] = layer",
        "",
        "        documents = {f\"doc{i}\": f\"Content for document {i}\" for i in range(10)}",
        "        metadata = {f\"doc{i}\": {'index': i, 'source': 'test'} for i in range(10)}",
        "        embeddings = {f\"term{i}\": [0.1 * i, 0.2 * i, 0.3 * i] for i in range(5)}",
        "        relations = [",
        "            ('term0', 'RelatedTo', 'term1', 0.8),",
        "            ('term1', 'PartOf', 'term2', 0.6),",
        "        ]",
        "",
        "        # Save",
        "        writer = StateWriter(self.state_dir)",
        "        writer.save_all(",
        "            layers=layers,",
        "            documents=documents,",
        "            document_metadata=metadata,",
        "            embeddings=embeddings,",
        "            semantic_relations=relations,",
        "            stale_computations={'pagerank', 'concepts'},",
        "            verbose=False",
        "        )",
        "",
        "        # Load",
        "        loader = StateLoader(self.state_dir)",
        "        (loaded_layers, loaded_docs, loaded_meta,",
        "         loaded_embed, loaded_rels, manifest_data) = loader.load_all(verbose=False)",
        "",
        "        # Verify layers",
        "        self.assertEqual(len(loaded_layers), 4)",
        "        for level in range(4):",
        "            original = layers[CorticalLayer(level)]",
        "            loaded = loaded_layers[CorticalLayer(level)]",
        "            self.assertEqual(len(loaded.minicolumns), len(original.minicolumns))",
        "",
        "            for content, orig_col in original.minicolumns.items():",
        "                loaded_col = loaded.minicolumns[content]",
        "                self.assertEqual(loaded_col.id, orig_col.id)",
        "                self.assertEqual(loaded_col.occurrence_count, orig_col.occurrence_count)",
        "                self.assertAlmostEqual(loaded_col.tfidf, orig_col.tfidf, places=5)",
        "                self.assertAlmostEqual(loaded_col.pagerank, orig_col.pagerank, places=5)",
        "",
        "        # Verify documents",
        "        self.assertEqual(loaded_docs, documents)",
        "        self.assertEqual(loaded_meta, metadata)",
        "",
        "        # Verify computed values",
        "        self.assertEqual(loaded_embed, embeddings)",
        "        self.assertEqual(loaded_rels, relations)",
        "",
        "        # Verify staleness",
        "        self.assertEqual(manifest_data['stale_computations'], {'pagerank', 'concepts'})",
        "",
        "",
        "class TestMigration(unittest.TestCase):",
        "    \"\"\"Tests for pkl to JSON migration.\"\"\"",
        "",
        "    def setUp(self):",
        "        \"\"\"Create temp directory.\"\"\"",
        "        self.temp_dir = tempfile.mkdtemp()",
        "",
        "    def tearDown(self):",
        "        \"\"\"Clean up.\"\"\"",
        "        shutil.rmtree(self.temp_dir)",
        "",
        "    def test_migrate_requires_pkl_file(self):",
        "        \"\"\"Test migration fails for missing pkl file.\"\"\"",
        "        with self.assertRaises(FileNotFoundError):",
        "            migrate_pkl_to_json(",
        "                '/nonexistent/file.pkl',",
        "                os.path.join(self.temp_dir, 'output'),",
        "                verbose=False",
        "            )",
        "",
        "",
        "class TestIncrementalSave(unittest.TestCase):",
        "    \"\"\"Tests for incremental saving behavior.\"\"\"",
        "",
        "    def setUp(self):",
        "        \"\"\"Create temp directory.\"\"\"",
        "        self.temp_dir = tempfile.mkdtemp()",
        "        self.state_dir = os.path.join(self.temp_dir, 'corpus_state')",
        "",
        "    def tearDown(self):",
        "        \"\"\"Clean up.\"\"\"",
        "        shutil.rmtree(self.temp_dir)",
        "",
        "    def test_only_changed_layers_written(self):",
        "        \"\"\"Test that only modified layers are written on second save.\"\"\"",
        "        # Initial save",
        "        writer = StateWriter(self.state_dir)",
        "",
        "        layer0 = HierarchicalLayer(CorticalLayer.TOKENS)",
        "        col0 = Minicolumn(\"L0_test\", \"test\", 0)",
        "        layer0.minicolumns[\"test\"] = col0",
        "        layer0._id_index[col0.id] = \"test\"",
        "",
        "        layer1 = HierarchicalLayer(CorticalLayer.BIGRAMS)",
        "        col1 = Minicolumn(\"L1_test bigram\", \"test bigram\", 1)",
        "        layer1.minicolumns[\"test bigram\"] = col1",
        "        layer1._id_index[col1.id] = \"test bigram\"",
        "",
        "        layers = {",
        "            CorticalLayer.TOKENS: layer0,",
        "            CorticalLayer.BIGRAMS: layer1,",
        "        }",
        "",
        "        writer.save_layer(layer0)",
        "        writer.save_layer(layer1)",
        "",
        "        # Get initial modification times",
        "        l0_path = os.path.join(self.state_dir, 'layers', 'L0_tokens.json')",
        "        l1_path = os.path.join(self.state_dir, 'layers', 'L1_bigrams.json')",
        "",
        "        # Modify only layer0",
        "        col_new = Minicolumn(\"L0_new\", \"new\", 0)",
        "        layer0.minicolumns[\"new\"] = col_new",
        "        layer0._id_index[col_new.id] = \"new\"",
        "",
        "        # Save both again",
        "        result0 = writer.save_layer(layer0)",
        "        result1 = writer.save_layer(layer1)",
        "",
        "        # Only layer0 should be written",
        "        self.assertTrue(result0)",
        "        self.assertFalse(result1)",
        "",
        "",
        "if __name__ == '__main__':",
        "    unittest.main()"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    }
  ],
  "hour_of_day": 1,
  "day_of_week": "Sunday",
  "seconds_since_last_commit": -130836,
  "is_merge": true,
  "is_initial": false,
  "parent_count": 2,
  "session_id": null,
  "related_chats": [],
  "ci_result": null,
  "reverted": false,
  "amended": false
}