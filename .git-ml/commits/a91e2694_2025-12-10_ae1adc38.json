{
  "hash": "a91e2694449fed484b40e4525adca7ca23c05fa9",
  "message": "Merge pull request #16 from scrawlsbenches/claude/delete-claude-md-01B23cSpMx61eu6RpheB82eC",
  "author": "scrawlsbenches",
  "timestamp": "2025-12-10 07:02:31 -0500",
  "branch": "claude/multi-index-design-DvifZ",
  "files_changed": [
    "CLAUDE.md",
    "CODE_REVIEW.md",
    "README.md",
    "TASK_LIST.md",
    "TASK_list.md",
    "pyproject.toml"
  ],
  "insertions": 82,
  "deletions": 595,
  "hunks": [
    {
      "file": "CLAUDE.md",
      "function": null,
      "start_line": 1,
      "lines_added": [],
      "lines_removed": [
        "# Claude.md - Project Guide for Claude Code",
        "",
        "This file provides guidance for Claude Code when working with the Cortical Text Processor codebase.",
        "",
        "## Project Overview",
        "",
        "A neocortex-inspired text processing library with zero external dependencies that performs semantic analysis and document retrieval using a hierarchical biological computing model.",
        "",
        "## Quick Start",
        "",
        "```python",
        "from cortical import CorticalTextProcessor",
        "",
        "processor = CorticalTextProcessor()",
        "processor.process_document(\"doc1\", \"Neural networks process information...\")",
        "processor.compute_all()",
        "results = processor.find_documents_for_query(\"neural processing\")",
        "```",
        "",
        "## Project Structure",
        "",
        "```",
        "cortical/",
        "├── __init__.py          # Public API exports",
        "├── processor.py         # Main orchestrator (CorticalTextProcessor)",
        "├── minicolumn.py        # Core data structure",
        "├── layers.py            # Hierarchical layer definitions",
        "├── tokenizer.py         # Text tokenization",
        "├── analysis.py          # PageRank, TF-IDF, clustering",
        "├── semantics.py         # Semantic extraction, retrofitting",
        "├── embeddings.py        # Graph-based embeddings",
        "├── query.py             # Query expansion and search",
        "├── gaps.py              # Knowledge gap detection",
        "└── persistence.py       # Save/load functionality",
        "",
        "tests/",
        "├── test_tokenizer.py",
        "├── test_processor.py",
        "├── test_layers.py",
        "├── test_analysis.py",
        "├── test_embeddings.py",
        "├── test_semantics.py",
        "├── test_gaps.py",
        "└── test_persistence.py",
        "```",
        "",
        "## Running Tests",
        "",
        "```bash",
        "python -m unittest discover -s tests -v",
        "```",
        "",
        "All 331+ tests should pass.",
        "",
        "## Running the Showcase",
        "",
        "```bash",
        "python showcase.py",
        "```",
        "",
        "## Key Classes",
        "",
        "### CorticalTextProcessor",
        "Main entry point. Coordinates document processing, computations, and queries.",
        "",
        "### HierarchicalLayer",
        "Manages minicolumns at a given hierarchy level. Has `get_by_id()` for O(1) lookups.",
        "",
        "### Minicolumn",
        "Represents a concept/feature. Tracks:",
        "- `doc_occurrence_counts`: Per-document term frequencies",
        "- `lateral_connections`: Associations with other terms",
        "- `pagerank`, `tfidf`: Importance scores",
        "",
        "## Recent Changes (2025-12-10)",
        "",
        "### Layer 2 Connection Improvements",
        "Multiple strategies for connecting concepts in Layer 2:",
        "",
        "1. **Connection Strategies** - `compute_all(connection_strategy='...')`:",
        "   - `'document_overlap'`: Traditional Jaccard similarity (default)",
        "   - `'semantic'`: Connect via semantic relations between members",
        "   - `'embedding'`: Connect via embedding centroid similarity",
        "   - `'hybrid'`: Combine all three for maximum connectivity",
        "",
        "2. **Clustering Parameters** - `compute_all(cluster_strictness=0.5, bridge_weight=0.3)`:",
        "   - `cluster_strictness` (0.0-1.0): Lower = fewer, larger clusters",
        "   - `bridge_weight` (0.0-1.0): Adds cross-document token bridging",
        "",
        "3. **Configurable Thresholds** - `compute_concept_connections(...)`:",
        "   - `min_shared_docs=0`: Disable document overlap requirement",
        "   - `min_jaccard=0.0`: Disable Jaccard similarity threshold",
        "   - `use_member_semantics=True`: Connect via member token relations",
        "   - `use_embedding_similarity=True`: Connect via embedding similarity",
        "",
        "### Bug Fixes Applied (2025-12-09)",
        "1. **TF-IDF calculation** - Now uses actual per-document occurrence counts",
        "2. **O(1) ID lookups** - Added `_id_index` and `get_by_id()` method",
        "3. **Type annotations** - Fixed `any` → `Any` in semantics.py",
        "4. **Unused imports** - Removed `Counter` from analysis.py",
        "5. **Verbose parameter** - Added to `export_graph_json()`",
        "",
        "### Performance Improvements",
        "- Graph algorithms improved from O(n²) to O(n) via ID index",
        "",
        "## Coding Conventions",
        "",
        "- Use type hints for all function parameters and returns",
        "- Follow Google-style docstrings",
        "- Line length: 100 characters (per pyproject.toml)",
        "- Run tests before committing changes",
        "",
        "## Common Tasks",
        "",
        "### Add a new document",
        "```python",
        "processor.process_document(\"doc_id\", \"document content\")",
        "processor.compute_all(verbose=False)",
        "```",
        "",
        "### Use hybrid connectivity for diverse documents",
        "```python",
        "# When documents cover different topics with no overlap",
        "processor.compute_all(",
        "    connection_strategy='hybrid',  # Use all connection methods",
        "    cluster_strictness=0.5,        # Allow more cross-topic clustering",
        "    bridge_weight=0.3              # Add inter-document bridges",
        ")",
        "```",
        "",
        "### Query the corpus",
        "```python",
        "results = processor.find_documents_for_query(\"search terms\", top_n=5)",
        "expanded = processor.expand_query(\"term\", max_expansions=10)",
        "```",
        "",
        "### Analyze knowledge gaps",
        "```python",
        "gaps = processor.analyze_knowledge_gaps()",
        "anomalies = processor.detect_anomalies(threshold=0.1)",
        "```",
        "",
        "### Compute embeddings",
        "```python",
        "stats = processor.compute_graph_embeddings(dimensions=32, method='adjacency')",
        "similar = processor.find_similar_by_embedding(\"term\", top_n=5)",
        "```",
        "",
        "### Save/Load state",
        "```python",
        "processor.save(\"model.pkl\")",
        "loaded = CorticalTextProcessor.load(\"model.pkl\")",
        "```"
      ],
      "context_before": [],
      "context_after": [],
      "change_type": "delete"
    },
    {
      "file": "CODE_REVIEW.md",
      "function": null,
      "start_line": 1,
      "lines_added": [],
      "lines_removed": [
        "# Comprehensive Code Review: Cortical Text Processor",
        "",
        "**Reviewer:** Claude (Opus 4)",
        "**Date:** 2025-12-09",
        "**Version Reviewed:** 2.0.0",
        "**Repository:** Opus-code-test",
        "",
        "",
        "## Executive Summary",
        "",
        "The Cortical Text Processor is a well-designed, educational NLP library implementing a biologically-inspired hierarchical text processing system. The codebase demonstrates **solid software engineering practices** with clean architecture, comprehensive documentation, and good test coverage. All 39 unit tests pass successfully.",
        "",
        "**Overall Assessment:** Good quality codebase with minor issues and opportunities for improvement.",
        "",
        "| Category | Rating | Notes |",
        "|----------|--------|-------|",
        "| Architecture | A | Clean hierarchical design, good separation of concerns |",
        "| Code Quality | B+ | Generally clean, some minor style inconsistencies |",
        "| Test Coverage | B | Good coverage of core functionality, some gaps |",
        "| Documentation | A | Excellent docstrings and README |",
        "| Security | B | No major issues, minor concerns noted |",
        "| Performance | B- | Some O(n²) operations could be optimized |",
        "",
        "",
        "## 1. Architecture & Design",
        "",
        "### Strengths",
        "",
        "1. **Clean Hierarchical Design**: The four-layer cortical model (Tokens → Bigrams → Concepts → Documents) is well-implemented and mirrors the biological analogy effectively.",
        "",
        "2. **Good Separation of Concerns**: Each module has a clear, single responsibility:",
        "   - `processor.py` - Orchestration",
        "   - `minicolumn.py` - Core data structure",
        "   - `layers.py` - Layer management",
        "   - `analysis.py` - Graph algorithms",
        "   - `semantics.py` - Semantic relations",
        "   - `query.py` - Search functionality",
        "   - `gaps.py` - Gap detection",
        "   - `persistence.py` - Save/load",
        "",
        "3. **Zero External Dependencies**: The library is self-contained, which simplifies deployment and reduces dependency conflicts.",
        "",
        "4. **Public API Design**: The `CorticalTextProcessor` class provides a clean, intuitive facade for all functionality.",
        "",
        "### Areas for Improvement",
        "",
        "1. **Inconsistent ID Lookup Pattern**: Throughout the codebase, there's a repeated pattern of looking up minicolumns by ID that's inefficient:",
        "",
        "   ```python",
        "   # This pattern appears in multiple files (analysis.py:57-59, query.py:97-105, etc.)",
        "   if neighbor_id in layer.minicolumns:",
        "       neighbor = layer.minicolumns[neighbor_id]",
        "   else:",
        "       for c in layer.minicolumns.values():",
        "           if c.id == neighbor_id:",
        "               # found it",
        "   ```",
        "",
        "   **Recommendation:** Add a `get_by_id()` method to `HierarchicalLayer` that maintains a secondary ID → content mapping for O(1) lookups.",
        "",
        "2. **Missing Type Hints in Some Return Types**: Some functions use `Dict` without specifying value types (e.g., `semantics.py:153` returns `Dict[str, any]`).",
        "",
        "",
        "## 2. Code Quality Issues",
        "",
        "### Bug: Incorrect Per-Document TF Calculation",
        "",
        "**Location:** `analysis.py:131`",
        "",
        "```python",
        "# Current code (incorrect)",
        "for doc_id in col.document_ids:",
        "    doc_tf = sum(1 for d in [doc_id] if d in col.document_ids)  # Always = 1",
        "    col.tfidf_per_doc[doc_id] = math.log1p(doc_tf) * idf",
        "```",
        "",
        "The `doc_tf` calculation always results in 1 because it's checking if `doc_id` is in `col.document_ids` (which it always is, since we're iterating over that same set). The code should count actual term occurrences per document.",
        "",
        "**Severity:** Medium",
        "**Impact:** Per-document TF-IDF scores are inaccurate, affecting document-specific ranking.",
        "",
        "### Minor Issues",
        "",
        "1. **Unused Import**: `analysis.py` imports `Counter` from collections but never uses it.",
        "",
        "2. **Type Annotation Inconsistency**: Return type `Dict[str, any]` should be `Dict[str, Any]` (capital A) per PEP 484:",
        "   - `semantics.py:153`",
        "   - `semantics.py:248`",
        "",
        "3. **Magic Numbers**: Several threshold values are hardcoded without clear justification:",
        "   - `gaps.py:62`: `avg_sim < 0.02` for isolation detection",
        "   - `gaps.py:76`: `tfidf > 0.005` for weak topics",
        "   - `gaps.py:99`: `0.005 < sim < 0.03` for bridge opportunities",
        "",
        "4. **Inconsistent Verbose Parameter**: Some functions print regardless of verbose flag:",
        "   - `persistence.py:175` prints unconditionally in `export_graph_json`",
        "",
        "### Style Observations",
        "",
        "1. **Line Length**: Some lines exceed 100 characters (the configured limit in pyproject.toml).",
        "",
        "2. **Docstring Format**: Most docstrings follow Google style, which is good, but a few are missing return type descriptions.",
        "",
        "",
        "## 3. Performance Concerns",
        "",
        "### O(n²) Operations",
        "",
        "1. **PageRank ID Lookup** (`analysis.py:57-59`):",
        "   ```python",
        "   if target_id in layer.minicolumns or any(",
        "       c.id == target_id for c in layer.minicolumns.values()",
        "   ):",
        "   ```",
        "   This iterates over all minicolumns for each connection.",
        "",
        "2. **Activation Propagation** (`analysis.py:176-179`):",
        "   ```python",
        "   for c in layer.minicolumns.values():",
        "       if c.id == neighbor_id:",
        "           # ...",
        "   ```",
        "   Same linear search pattern.",
        "",
        "3. **Document Similarity Matrix** (`gaps.py:44-68`):",
        "   Computes full N×N similarity matrix. For large corpora, this is expensive.",
        "",
        "**Recommendation:**",
        "- Add ID → minicolumn mapping for O(1) lookups",
        "- Consider sparse similarity computation or sampling for gap analysis",
        "",
        "### Memory Considerations",
        "",
        "1. The `Minicolumn` class uses `__slots__` effectively to reduce memory overhead - good practice.",
        "",
        "2. `cooccurrence` dictionary in `semantics.py` could grow very large with large corpora. Consider using sparse data structures or streaming computation.",
        "",
        "",
        "## 4. Security Considerations",
        "",
        "### Pickle Serialization",
        "",
        "**Location:** `persistence.py:50-51, 76-77`",
        "",
        "```python",
        "with open(filepath, 'wb') as f:",
        "    pickle.dump(state, f, protocol=pickle.HIGHEST_PROTOCOL)",
        "",
        "with open(filepath, 'rb') as f:",
        "    state = pickle.load(f)",
        "```",
        "",
        "**Risk:** Pickle is inherently unsafe for untrusted data. Loading a maliciously crafted pickle file can execute arbitrary code.",
        "",
        "**Severity:** Low (internal tool, not user-facing deserialization)",
        "**Recommendation:** Document the security implications in the docstring. For production use, consider JSON-based serialization for the full state.",
        "",
        "### Path Handling",
        "",
        "File operations in `demo.py` and `persistence.py` don't validate paths, which is acceptable for a library but should be noted if exposed to external input.",
        "",
        "",
        "## 5. Test Coverage Analysis",
        "",
        "### Covered Areas (Good)",
        "- Tokenizer (all core methods)",
        "- Minicolumn (creation, connections, serialization)",
        "- HierarchicalLayer (CRUD operations)",
        "- CorticalTextProcessor (document processing, queries)",
        "- Persistence (save/load)",
        "- Gap detection (structure tests)",
        "",
        "### Missing Tests (Gaps)",
        "",
        "1. **`embeddings.py`**: No dedicated tests for:",
        "   - `_random_walk_embeddings`",
        "   - `_spectral_embeddings`",
        "   - `embedding_similarity`",
        "   - `find_similar_by_embedding`",
        "",
        "2. **`semantics.py`**: No tests for:",
        "   - `extract_corpus_semantics`",
        "   - `retrofit_connections`",
        "   - `retrofit_embeddings`",
        "",
        "3. **Edge Cases**: Missing tests for:",
        "   - Empty corpus handling",
        "   - Unicode/special character handling",
        "   - Very large documents",
        "   - Single document corpus",
        "",
        "4. **Integration Tests**: No end-to-end tests verifying the complete pipeline.",
        "",
        "**Test Count:** 39 tests, ~3,600 LOC → approximately 1 test per 92 lines of code.",
        "",
        "",
        "## 6. Documentation Quality",
        "",
        "### Excellent",
        "- Module-level docstrings explain purpose and analogy clearly",
        "- Most functions have comprehensive docstrings with Args/Returns",
        "- README provides good overview and usage examples",
        "- Code comments explain non-obvious algorithms",
        "",
        "### Suggestions",
        "",
        "1. Add type hints to all function signatures for better IDE support",
        "2. Document the expected format of semantic relations tuples",
        "3. Add examples to complex functions like `retrofit_connections`",
        "",
        "",
        "## 7. Specific File Reviews",
        "",
        "### `processor.py` (207 lines)",
        "- **Quality:** Excellent",
        "- **Notes:** Clean orchestration, good method organization",
        "",
        "### `minicolumn.py` (158 lines)",
        "- **Quality:** Excellent",
        "- **Notes:** Good use of `__slots__`, complete serialization support",
        "",
        "### `layers.py` (252 lines)",
        "- **Quality:** Excellent",
        "- **Notes:** Clean enum design, good statistical methods",
        "",
        "### `tokenizer.py` (245 lines)",
        "- **Quality:** Good",
        "- **Notes:** Comprehensive stop words, Porter-lite stemmer is simple but effective",
        "",
        "### `analysis.py` (412 lines)",
        "- **Quality:** Good (with noted bug)",
        "- **Notes:** Solid algorithm implementations, needs ID lookup optimization",
        "",
        "### `semantics.py` (337 lines)",
        "- **Quality:** Good",
        "- **Notes:** PMI calculation is correct, retrofitting well-implemented",
        "",
        "### `query.py` (320 lines)",
        "- **Quality:** Good",
        "- **Notes:** Query expansion logic is sound, spreading activation well-designed",
        "",
        "### `embeddings.py` (210 lines)",
        "- **Quality:** Good",
        "- **Notes:** Three embedding methods provide flexibility, normalization handled correctly",
        "",
        "### `gaps.py` (215 lines)",
        "- **Quality:** Good",
        "- **Notes:** Useful gap detection, magic numbers should be configurable",
        "",
        "### `persistence.py` (295 lines)",
        "- **Quality:** Good",
        "- **Notes:** Complete save/load support, JSON export useful for visualization",
        "",
        "",
        "## 8. Recommendations Summary",
        "",
        "### Critical (Should Fix)",
        "",
        "1. **Fix TF calculation bug** in `analysis.py:131` - per-document term frequency is always 1.",
        "",
        "### High Priority",
        "",
        "2. **Add ID → minicolumn mapping** to `HierarchicalLayer` to eliminate O(n) lookups.",
        "",
        "3. **Add tests for embeddings and semantics modules**.",
        "",
        "### Medium Priority",
        "",
        "4. Fix type annotation `any` → `Any` in semantics.py.",
        "",
        "5. Remove unused `Counter` import from analysis.py.",
        "",
        "6. Make threshold values configurable or document their rationale.",
        "",
        "7. Add verbose flag check to `export_graph_json`.",
        "",
        "### Low Priority (Nice to Have)",
        "",
        "8. Add integration tests for the complete pipeline.",
        "",
        "9. Document pickle security considerations.",
        "",
        "10. Consider adding progress callbacks for long-running operations.",
        "",
        "",
        "## 9. Conclusion",
        "",
        "The Cortical Text Processor is a **well-crafted educational and research tool** with a thoughtful architecture inspired by neuroscience. The code is readable, well-documented, and follows good practices.",
        "",
        "The main areas for improvement are:",
        "- One calculation bug in TF-IDF per-document scoring",
        "- Performance optimization opportunities in ID lookups",
        "- Expanded test coverage for embeddings and semantics modules",
        "",
        "The codebase is suitable for its intended purpose of demonstrating biologically-inspired NLP concepts and performing text analysis on small-to-medium corpora.",
        "",
        "**Recommended Action:** Address the TF calculation bug and add ID mapping optimization before any production use.",
        "",
        "",
        "*Review completed by Claude (Opus 4)*"
      ],
      "context_before": [],
      "context_after": [],
      "change_type": "delete"
    },
    {
      "file": "README.md",
      "function": "results = processor.find_documents_for_query(\"neural processing\")",
      "start_line": 73,
      "lines_added": [
        "**Output:**",
        "```",
        "Computing activation propagation...",
        "Computing importance (PageRank)...",
        "Computing TF-IDF...",
        "Computing document connections...",
        "Computing bigram connections...",
        "Building concept clusters...",
        "Computing concept connections (document_overlap)...",
        "Done.",
        "[('doc1', 0.8774208144843981), ('doc2', 0.8317923190728529)]",
        "Documents: 3, Connections: 66",
        "✓ Saved processor to my_corpus.pkl",
        "  - 3 documents",
        "  - 29 minicolumns",
        "  - 66 connections",
        "```",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "print(results)  # [('doc1', 0.85), ('doc2', 0.72), ...]",
        "",
        "# Get corpus summary",
        "summary = processor.get_corpus_summary()",
        "print(f\"Documents: {summary['documents']}, Connections: {summary['total_connections']}\")",
        "",
        "# Save for later",
        "processor.save(\"my_corpus.pkl\")",
        "```",
        ""
      ],
      "context_after": [
        "## Core API",
        "",
        "### Document Processing",
        "",
        "```python",
        "processor.process_document(doc_id, content, metadata=None)",
        "processor.add_document_incremental(doc_id, content)  # Incremental indexing",
        "processor.add_documents_batch([(doc_id, content, metadata), ...])  # Batch processing",
        "```",
        ""
      ],
      "change_type": "add"
    },
    {
      "file": "TASK_LIST.md",
      "function": "Semantic bonus is capped at 50% boost (`min(avg_semantic, 0.5)`). This is a reas",
      "start_line": 1202,
      "lines_added": [
        "## Layer 2 Connection Improvements (2025-12-10)",
        "",
        "### Problem Statement",
        "",
        "Layer 2 (Concept Layer/V4) shows 0 connections when documents cover diverse topics because:",
        "- Label propagation creates topic-specific clusters",
        "- Concepts inherit only their members' documents",
        "- Connection filter requires shared documents (Jaccard ≥ 0.1)",
        "- No document overlap → no connections",
        "",
        "### Task L2-1: Add Configurable Connection Thresholds ✅ COMPLETED",
        "**File:** `cortical/analysis.py` (lines 614-812)",
        "",
        "- [x] Add `min_shared_docs=0` option to allow connections without document overlap",
        "- [x] Add `min_jaccard=0.0` option to disable Jaccard filtering",
        "- [x] Expose these parameters in `CorticalTextProcessor.compute_concept_connections()`",
        "- [x] Update docstrings to explain threshold behavior",
        "- [x] Add tests for edge cases (zero thresholds, negative values)",
        "",
        "### Task L2-2: Connect Concepts via Semantic Relations ✅ COMPLETED",
        "**File:** `cortical/analysis.py`",
        "",
        "- [x] Add new connection method that links concepts when their member tokens have semantic relations",
        "- [x] For each concept pair, check if any (token1, relation, token2) exists in semantic_relations",
        "- [x] Weight connections by number of semantic links between members",
        "- [x] Make this work independently of document overlap",
        "- [x] Add `use_member_semantics=True` parameter to `compute_concept_connections()`",
        "- [x] Add tests verifying semantic-based connections",
        "",
        "### Task L2-3: Connect Concepts via Shared Vocabulary/Embeddings ✅ COMPLETED",
        "**File:** `cortical/analysis.py`",
        "",
        "- [x] Add connection method based on embedding similarity between concept centroids",
        "- [x] Compute concept centroid as average of member token embeddings",
        "- [x] Connect concepts with cosine similarity above threshold",
        "- [x] Add `use_embedding_similarity=True` and `embedding_threshold=0.3` parameters",
        "- [x] Falls back gracefully if embeddings not computed",
        "- [x] Add tests for embedding-based connections",
        "",
        "### Task L2-4: Improve Clustering to Reduce Topic Isolation ✅ COMPLETED",
        "**File:** `cortical/analysis.py` (lines 482-616)",
        "",
        "- [x] Add `cluster_strictness` parameter to label propagation (0.0-1.0)",
        "- [x] Lower strictness = more cross-topic token mixing in clusters",
        "- [x] Add `bridge_weight` parameter for inter-document token bridging",
        "- [x] Add tests for different strictness levels and bridging",
        "",
        "### Task L2-5: Integration and API Updates ✅ COMPLETED",
        "**File:** `cortical/processor.py`",
        "",
        "- [x] Update `compute_all()` to accept connection strategy parameters",
        "- [x] Add `connection_strategy` enum: 'document_overlap', 'semantic', 'embedding', 'hybrid'",
        "- [x] 'hybrid' combines all three methods with configurable weights",
        "- [x] Add documentation in CLAUDE.md",
        "- [x] Add 6 new tests for compute_all strategies",
        "",
        "**Success Criteria:** ✅ ALL MET",
        "- Layer 2 shows meaningful connections even with diverse document topics",
        "- User can choose connection strategy based on their use case",
        "- All existing tests continue to pass (337 tests)",
        "- New tests cover the added functionality (17 new tests added)",
        "",
        "---",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "",
        "```",
        "Ran 321 tests in 0.280s",
        "OK",
        "```",
        "",
        "All tests passing as of 2025-12-10.",
        "",
        "---",
        ""
      ],
      "context_after": [
        "## Code Quality Improvements (2025-12-10)",
        "",
        "### Query Expansion Helper Refactoring",
        "",
        "**File:** `cortical/query.py`",
        "**Status:** [x] Completed",
        "",
        "**Problem:**",
        "Query expansion logic (expand + semantic merge) was duplicated in 6 functions:",
        "- `find_documents_for_query()`"
      ],
      "change_type": "add"
    },
    {
      "file": "TASK_list.md",
      "function": null,
      "start_line": 1,
      "lines_added": [],
      "lines_removed": [
        "# Task List: Layer 2 Connection Improvements",
        "",
        "## Problem Statement",
        "",
        "Layer 2 (Concept Layer/V4) shows 0 connections when documents cover diverse topics because:",
        "- Label propagation creates topic-specific clusters",
        "- Concepts inherit only their members' documents",
        "- Connection filter requires shared documents (Jaccard ≥ 0.1)",
        "- No document overlap → no connections",
        "",
        "## Tasks",
        "",
        "### Task 1: Add Configurable Connection Thresholds ✅ COMPLETED",
        "**File:** `cortical/analysis.py` (lines 614-812)",
        "",
        "- [x] Add `min_shared_docs=0` option to allow connections without document overlap",
        "- [x] Add `min_jaccard=0.0` option to disable Jaccard filtering",
        "- [x] Expose these parameters in `CorticalTextProcessor.compute_concept_connections()`",
        "- [x] Update docstrings to explain threshold behavior",
        "- [x] Add tests for edge cases (zero thresholds, negative values)",
        "",
        "### Task 2: Connect Concepts via Semantic Relations ✅ COMPLETED",
        "**File:** `cortical/analysis.py`",
        "",
        "- [x] Add new connection method that links concepts when their member tokens have semantic relations",
        "- [x] For each concept pair, check if any (token1, relation, token2) exists in semantic_relations",
        "- [x] Weight connections by number of semantic links between members",
        "- [x] Make this work independently of document overlap",
        "- [x] Add `use_member_semantics=True` parameter to `compute_concept_connections()`",
        "- [x] Add tests verifying semantic-based connections",
        "",
        "### Task 3: Connect Concepts via Shared Vocabulary/Embeddings ✅ COMPLETED",
        "**File:** `cortical/analysis.py`",
        "",
        "- [x] Add connection method based on embedding similarity between concept centroids",
        "- [x] Compute concept centroid as average of member token embeddings",
        "- [x] Connect concepts with cosine similarity above threshold",
        "- [x] Add `use_embedding_similarity=True` and `embedding_threshold=0.3` parameters",
        "- [x] Falls back gracefully if embeddings not computed",
        "- [x] Add tests for embedding-based connections",
        "",
        "### Task 4: Improve Clustering to Reduce Topic Isolation ✅ COMPLETED",
        "**File:** `cortical/analysis.py` (lines 482-616)",
        "",
        "- [x] Add `cluster_strictness` parameter to label propagation (0.0-1.0)",
        "- [x] Lower strictness = more cross-topic token mixing in clusters",
        "- [x] Add `bridge_weight` parameter for inter-document token bridging",
        "- [x] Add tests for different strictness levels and bridging",
        "",
        "### Task 5: Integration and API Updates ✅ COMPLETED",
        "**File:** `cortical/processor.py`",
        "",
        "- [x] Update `compute_all()` to accept connection strategy parameters",
        "- [x] Add `connection_strategy` enum: 'document_overlap', 'semantic', 'embedding', 'hybrid'",
        "- [x] 'hybrid' combines all three methods with configurable weights",
        "- [x] Add documentation in CLAUDE.md",
        "- [x] Add 6 new tests for compute_all strategies",
        "",
        "## Priority Order",
        "",
        "1. **Task 1** (Quick win - just parameter changes) ✅",
        "2. **Task 2** (High value - semantic relations already exist) ✅",
        "3. **Task 3** (Medium - requires embeddings computed first) ✅",
        "4. **Task 4** (Lower priority - more invasive change) ✅",
        "5. **Task 5** (Final - ties everything together) ✅",
        "",
        "## Success Criteria ✅ ALL MET",
        "",
        "- ✅ Layer 2 shows meaningful connections even with diverse document topics",
        "- ✅ User can choose connection strategy based on their use case",
        "- ✅ All existing tests continue to pass (337 tests)",
        "- ✅ New tests cover the added functionality (17 new tests added)"
      ],
      "context_before": [],
      "context_after": [],
      "change_type": "delete"
    },
    {
      "file": "pyproject.toml",
      "function": null,
      "start_line": 1,
      "lines_added": [],
      "lines_removed": [
        "[build-system]",
        "requires = [\"setuptools>=61.0\", \"wheel\"]",
        "build-backend = \"setuptools.build_meta\"",
        "",
        "[project]",
        "name = \"cortical-text-processor\"",
        "version = \"2.0.0\"",
        "description = \"Neocortex-inspired text processing for semantic analysis and document retrieval\"",
        "readme = \"README.md\"",
        "license = {text = \"MIT\"}",
        "requires-python = \">=3.8\"",
        "authors = [",
        "    {name = \"Claude\", email = \"claude@anthropic.com\"}",
        "]",
        "keywords = [",
        "    \"nlp\",",
        "    \"text-processing\",",
        "    \"semantic-analysis\",",
        "    \"document-retrieval\",",
        "    \"knowledge-graph\",",
        "    \"neural-inspired\"",
        "]",
        "classifiers = [",
        "    \"Development Status :: 4 - Beta\",",
        "    \"Intended Audience :: Developers\",",
        "    \"Intended Audience :: Science/Research\",",
        "    \"License :: OSI Approved :: MIT License\",",
        "    \"Programming Language :: Python :: 3\",",
        "    \"Programming Language :: Python :: 3.8\",",
        "    \"Programming Language :: Python :: 3.9\",",
        "    \"Programming Language :: Python :: 3.10\",",
        "    \"Programming Language :: Python :: 3.11\",",
        "    \"Programming Language :: Python :: 3.12\",",
        "    \"Topic :: Scientific/Engineering :: Artificial Intelligence\",",
        "    \"Topic :: Text Processing :: Linguistic\",",
        "]",
        "",
        "[project.urls]",
        "Homepage = \"https://github.com/anthropic/cortical-text-processor\"",
        "Documentation = \"https://github.com/anthropic/cortical-text-processor#readme\"",
        "Repository = \"https://github.com/anthropic/cortical-text-processor\"",
        "",
        "[tool.setuptools.packages.find]",
        "where = [\".\"]",
        "include = [\"cortical*\"]",
        "",
        "[tool.pytest.ini_options]",
        "testpaths = [\"tests\"]",
        "python_files = [\"test_*.py\"]",
        "python_classes = [\"Test*\"]",
        "python_functions = [\"test_*\"]",
        "",
        "[tool.black]",
        "line-length = 100",
        "target-version = ['py38', 'py39', 'py310', 'py311']",
        "",
        "[tool.isort]",
        "profile = \"black\"",
        "line_length = 100"
      ],
      "context_before": [],
      "context_after": [],
      "change_type": "delete"
    }
  ],
  "hour_of_day": 12,
  "day_of_week": "Wednesday",
  "seconds_since_last_commit": -438137,
  "is_merge": true,
  "is_initial": false,
  "parent_count": 2,
  "session_id": null,
  "related_chats": [],
  "ci_result": null,
  "reverted": false,
  "amended": false
}