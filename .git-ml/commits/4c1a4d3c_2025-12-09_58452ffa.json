{
  "hash": "4c1a4d3cab4bc08d636892b60d252057df95863e",
  "message": "Merge pull request #4 from scrawlsbenches/claude/code-review-01Trn6rLWUEuUqrxHGmvBFnu",
  "author": "scrawlsbenches",
  "timestamp": "2025-12-09 14:12:52 -0500",
  "branch": "claude/multi-index-design-DvifZ",
  "files_changed": [
    "CLAUDE.md",
    "CODE_REVIEW.md",
    "KNOWLEDGE_TRANSFER.md",
    "TASK_LIST.md",
    "cortical/analysis.py",
    "cortical/layers.py",
    "cortical/minicolumn.py",
    "cortical/persistence.py",
    "cortical/processor.py",
    "cortical/query.py",
    "cortical/semantics.py",
    "samples/candlestick_patterns.txt",
    "samples/compilers.txt",
    "samples/computational_theory.txt",
    "samples/data_structures.txt",
    "samples/elliot_wave_theory.txt",
    "samples/financial_analysis.txt",
    "samples/neocortex.txt",
    "tests/test_analysis.py",
    "tests/test_embeddings.py",
    "tests/test_gaps.py",
    "tests/test_persistence.py",
    "tests/test_semantics.py"
  ],
  "insertions": 2492,
  "deletions": 89,
  "hunks": [
    {
      "file": "CLAUDE.md",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "# Claude.md - Project Guide for Claude Code",
        "",
        "This file provides guidance for Claude Code when working with the Cortical Text Processor codebase.",
        "",
        "## Project Overview",
        "",
        "A neocortex-inspired text processing library with zero external dependencies that performs semantic analysis and document retrieval using a hierarchical biological computing model.",
        "",
        "## Quick Start",
        "",
        "```python",
        "from cortical import CorticalTextProcessor",
        "",
        "processor = CorticalTextProcessor()",
        "processor.process_document(\"doc1\", \"Neural networks process information...\")",
        "processor.compute_all()",
        "results = processor.find_documents_for_query(\"neural processing\")",
        "```",
        "",
        "## Project Structure",
        "",
        "```",
        "cortical/",
        "├── __init__.py          # Public API exports",
        "├── processor.py         # Main orchestrator (CorticalTextProcessor)",
        "├── minicolumn.py        # Core data structure",
        "├── layers.py            # Hierarchical layer definitions",
        "├── tokenizer.py         # Text tokenization",
        "├── analysis.py          # PageRank, TF-IDF, clustering",
        "├── semantics.py         # Semantic extraction, retrofitting",
        "├── embeddings.py        # Graph-based embeddings",
        "├── query.py             # Query expansion and search",
        "├── gaps.py              # Knowledge gap detection",
        "└── persistence.py       # Save/load functionality",
        "",
        "tests/",
        "├── test_tokenizer.py",
        "├── test_processor.py",
        "├── test_layers.py",
        "├── test_analysis.py",
        "├── test_embeddings.py",
        "├── test_semantics.py",
        "├── test_gaps.py",
        "└── test_persistence.py",
        "```",
        "",
        "## Running Tests",
        "",
        "```bash",
        "python -m unittest discover -s tests -v",
        "```",
        "",
        "All 109 tests should pass.",
        "",
        "## Running the Demo",
        "",
        "```bash",
        "python demo.py",
        "```",
        "",
        "## Key Classes",
        "",
        "### CorticalTextProcessor",
        "Main entry point. Coordinates document processing, computations, and queries.",
        "",
        "### HierarchicalLayer",
        "Manages minicolumns at a given hierarchy level. Has `get_by_id()` for O(1) lookups.",
        "",
        "### Minicolumn",
        "Represents a concept/feature. Tracks:",
        "- `doc_occurrence_counts`: Per-document term frequencies",
        "- `lateral_connections`: Associations with other terms",
        "- `pagerank`, `tfidf`: Importance scores",
        "",
        "## Recent Changes (2025-12-09)",
        "",
        "### Bug Fixes Applied",
        "1. **TF-IDF calculation** - Now uses actual per-document occurrence counts",
        "2. **O(1) ID lookups** - Added `_id_index` and `get_by_id()` method",
        "3. **Type annotations** - Fixed `any` → `Any` in semantics.py",
        "4. **Unused imports** - Removed `Counter` from analysis.py",
        "5. **Verbose parameter** - Added to `export_graph_json()`",
        "",
        "### Performance Improvements",
        "- Graph algorithms improved from O(n²) to O(n) via ID index",
        "",
        "## Coding Conventions",
        "",
        "- Use type hints for all function parameters and returns",
        "- Follow Google-style docstrings",
        "- Line length: 100 characters (per pyproject.toml)",
        "- Run tests before committing changes",
        "",
        "## Common Tasks",
        "",
        "### Add a new document",
        "```python",
        "processor.process_document(\"doc_id\", \"document content\")",
        "processor.compute_all(verbose=False)",
        "```",
        "",
        "### Query the corpus",
        "```python",
        "results = processor.find_documents_for_query(\"search terms\", top_n=5)",
        "expanded = processor.expand_query(\"term\", max_expansions=10)",
        "```",
        "",
        "### Analyze knowledge gaps",
        "```python",
        "gaps = processor.analyze_knowledge_gaps()",
        "anomalies = processor.detect_anomalies(threshold=0.1)",
        "```",
        "",
        "### Compute embeddings",
        "```python",
        "stats = processor.compute_graph_embeddings(dimensions=32, method='adjacency')",
        "similar = processor.find_similar_by_embedding(\"term\", top_n=5)",
        "```",
        "",
        "### Save/Load state",
        "```python",
        "processor.save(\"model.pkl\")",
        "loaded = CorticalTextProcessor.load(\"model.pkl\")",
        "```"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "CODE_REVIEW.md",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "# Comprehensive Code Review: Cortical Text Processor",
        "",
        "**Reviewer:** Claude (Opus 4)",
        "**Date:** 2025-12-09",
        "**Version Reviewed:** 2.0.0",
        "**Repository:** Opus-code-test",
        "",
        "---",
        "",
        "## Executive Summary",
        "",
        "The Cortical Text Processor is a well-designed, educational NLP library implementing a biologically-inspired hierarchical text processing system. The codebase demonstrates **solid software engineering practices** with clean architecture, comprehensive documentation, and good test coverage. All 39 unit tests pass successfully.",
        "",
        "**Overall Assessment:** Good quality codebase with minor issues and opportunities for improvement.",
        "",
        "| Category | Rating | Notes |",
        "|----------|--------|-------|",
        "| Architecture | A | Clean hierarchical design, good separation of concerns |",
        "| Code Quality | B+ | Generally clean, some minor style inconsistencies |",
        "| Test Coverage | B | Good coverage of core functionality, some gaps |",
        "| Documentation | A | Excellent docstrings and README |",
        "| Security | B | No major issues, minor concerns noted |",
        "| Performance | B- | Some O(n²) operations could be optimized |",
        "",
        "---",
        "",
        "## 1. Architecture & Design",
        "",
        "### Strengths",
        "",
        "1. **Clean Hierarchical Design**: The four-layer cortical model (Tokens → Bigrams → Concepts → Documents) is well-implemented and mirrors the biological analogy effectively.",
        "",
        "2. **Good Separation of Concerns**: Each module has a clear, single responsibility:",
        "   - `processor.py` - Orchestration",
        "   - `minicolumn.py` - Core data structure",
        "   - `layers.py` - Layer management",
        "   - `analysis.py` - Graph algorithms",
        "   - `semantics.py` - Semantic relations",
        "   - `query.py` - Search functionality",
        "   - `gaps.py` - Gap detection",
        "   - `persistence.py` - Save/load",
        "",
        "3. **Zero External Dependencies**: The library is self-contained, which simplifies deployment and reduces dependency conflicts.",
        "",
        "4. **Public API Design**: The `CorticalTextProcessor` class provides a clean, intuitive facade for all functionality.",
        "",
        "### Areas for Improvement",
        "",
        "1. **Inconsistent ID Lookup Pattern**: Throughout the codebase, there's a repeated pattern of looking up minicolumns by ID that's inefficient:",
        "",
        "   ```python",
        "   # This pattern appears in multiple files (analysis.py:57-59, query.py:97-105, etc.)",
        "   if neighbor_id in layer.minicolumns:",
        "       neighbor = layer.minicolumns[neighbor_id]",
        "   else:",
        "       for c in layer.minicolumns.values():",
        "           if c.id == neighbor_id:",
        "               # found it",
        "   ```",
        "",
        "   **Recommendation:** Add a `get_by_id()` method to `HierarchicalLayer` that maintains a secondary ID → content mapping for O(1) lookups.",
        "",
        "2. **Missing Type Hints in Some Return Types**: Some functions use `Dict` without specifying value types (e.g., `semantics.py:153` returns `Dict[str, any]`).",
        "",
        "---",
        "",
        "## 2. Code Quality Issues",
        "",
        "### Bug: Incorrect Per-Document TF Calculation",
        "",
        "**Location:** `analysis.py:131`",
        "",
        "```python",
        "# Current code (incorrect)",
        "for doc_id in col.document_ids:",
        "    doc_tf = sum(1 for d in [doc_id] if d in col.document_ids)  # Always = 1",
        "    col.tfidf_per_doc[doc_id] = math.log1p(doc_tf) * idf",
        "```",
        "",
        "The `doc_tf` calculation always results in 1 because it's checking if `doc_id` is in `col.document_ids` (which it always is, since we're iterating over that same set). The code should count actual term occurrences per document.",
        "",
        "**Severity:** Medium",
        "**Impact:** Per-document TF-IDF scores are inaccurate, affecting document-specific ranking.",
        "",
        "### Minor Issues",
        "",
        "1. **Unused Import**: `analysis.py` imports `Counter` from collections but never uses it.",
        "",
        "2. **Type Annotation Inconsistency**: Return type `Dict[str, any]` should be `Dict[str, Any]` (capital A) per PEP 484:",
        "   - `semantics.py:153`",
        "   - `semantics.py:248`",
        "",
        "3. **Magic Numbers**: Several threshold values are hardcoded without clear justification:",
        "   - `gaps.py:62`: `avg_sim < 0.02` for isolation detection",
        "   - `gaps.py:76`: `tfidf > 0.005` for weak topics",
        "   - `gaps.py:99`: `0.005 < sim < 0.03` for bridge opportunities",
        "",
        "4. **Inconsistent Verbose Parameter**: Some functions print regardless of verbose flag:",
        "   - `persistence.py:175` prints unconditionally in `export_graph_json`",
        "",
        "### Style Observations",
        "",
        "1. **Line Length**: Some lines exceed 100 characters (the configured limit in pyproject.toml).",
        "",
        "2. **Docstring Format**: Most docstrings follow Google style, which is good, but a few are missing return type descriptions.",
        "",
        "---",
        "",
        "## 3. Performance Concerns",
        "",
        "### O(n²) Operations",
        "",
        "1. **PageRank ID Lookup** (`analysis.py:57-59`):",
        "   ```python",
        "   if target_id in layer.minicolumns or any(",
        "       c.id == target_id for c in layer.minicolumns.values()",
        "   ):",
        "   ```",
        "   This iterates over all minicolumns for each connection.",
        "",
        "2. **Activation Propagation** (`analysis.py:176-179`):",
        "   ```python",
        "   for c in layer.minicolumns.values():",
        "       if c.id == neighbor_id:",
        "           # ...",
        "   ```",
        "   Same linear search pattern.",
        "",
        "3. **Document Similarity Matrix** (`gaps.py:44-68`):",
        "   Computes full N×N similarity matrix. For large corpora, this is expensive.",
        "",
        "**Recommendation:**",
        "- Add ID → minicolumn mapping for O(1) lookups",
        "- Consider sparse similarity computation or sampling for gap analysis",
        "",
        "### Memory Considerations",
        "",
        "1. The `Minicolumn` class uses `__slots__` effectively to reduce memory overhead - good practice.",
        "",
        "2. `cooccurrence` dictionary in `semantics.py` could grow very large with large corpora. Consider using sparse data structures or streaming computation.",
        "",
        "---",
        "",
        "## 4. Security Considerations",
        "",
        "### Pickle Serialization",
        "",
        "**Location:** `persistence.py:50-51, 76-77`",
        "",
        "```python",
        "with open(filepath, 'wb') as f:",
        "    pickle.dump(state, f, protocol=pickle.HIGHEST_PROTOCOL)",
        "",
        "with open(filepath, 'rb') as f:",
        "    state = pickle.load(f)",
        "```",
        "",
        "**Risk:** Pickle is inherently unsafe for untrusted data. Loading a maliciously crafted pickle file can execute arbitrary code.",
        "",
        "**Severity:** Low (internal tool, not user-facing deserialization)",
        "**Recommendation:** Document the security implications in the docstring. For production use, consider JSON-based serialization for the full state.",
        "",
        "### Path Handling",
        "",
        "File operations in `demo.py` and `persistence.py` don't validate paths, which is acceptable for a library but should be noted if exposed to external input.",
        "",
        "---",
        "",
        "## 5. Test Coverage Analysis",
        "",
        "### Covered Areas (Good)",
        "- Tokenizer (all core methods)",
        "- Minicolumn (creation, connections, serialization)",
        "- HierarchicalLayer (CRUD operations)",
        "- CorticalTextProcessor (document processing, queries)",
        "- Persistence (save/load)",
        "- Gap detection (structure tests)",
        "",
        "### Missing Tests (Gaps)",
        "",
        "1. **`embeddings.py`**: No dedicated tests for:",
        "   - `_random_walk_embeddings`",
        "   - `_spectral_embeddings`",
        "   - `embedding_similarity`",
        "   - `find_similar_by_embedding`",
        "",
        "2. **`semantics.py`**: No tests for:",
        "   - `extract_corpus_semantics`",
        "   - `retrofit_connections`",
        "   - `retrofit_embeddings`",
        "",
        "3. **Edge Cases**: Missing tests for:",
        "   - Empty corpus handling",
        "   - Unicode/special character handling",
        "   - Very large documents",
        "   - Single document corpus",
        "",
        "4. **Integration Tests**: No end-to-end tests verifying the complete pipeline.",
        "",
        "**Test Count:** 39 tests, ~3,600 LOC → approximately 1 test per 92 lines of code.",
        "",
        "---",
        "",
        "## 6. Documentation Quality",
        "",
        "### Excellent",
        "- Module-level docstrings explain purpose and analogy clearly",
        "- Most functions have comprehensive docstrings with Args/Returns",
        "- README provides good overview and usage examples",
        "- Code comments explain non-obvious algorithms",
        "",
        "### Suggestions",
        "",
        "1. Add type hints to all function signatures for better IDE support",
        "2. Document the expected format of semantic relations tuples",
        "3. Add examples to complex functions like `retrofit_connections`",
        "",
        "---",
        "",
        "## 7. Specific File Reviews",
        "",
        "### `processor.py` (207 lines)",
        "- **Quality:** Excellent",
        "- **Notes:** Clean orchestration, good method organization",
        "",
        "### `minicolumn.py` (158 lines)",
        "- **Quality:** Excellent",
        "- **Notes:** Good use of `__slots__`, complete serialization support",
        "",
        "### `layers.py` (252 lines)",
        "- **Quality:** Excellent",
        "- **Notes:** Clean enum design, good statistical methods",
        "",
        "### `tokenizer.py` (245 lines)",
        "- **Quality:** Good",
        "- **Notes:** Comprehensive stop words, Porter-lite stemmer is simple but effective",
        "",
        "### `analysis.py` (412 lines)",
        "- **Quality:** Good (with noted bug)",
        "- **Notes:** Solid algorithm implementations, needs ID lookup optimization",
        "",
        "### `semantics.py` (337 lines)",
        "- **Quality:** Good",
        "- **Notes:** PMI calculation is correct, retrofitting well-implemented",
        "",
        "### `query.py` (320 lines)",
        "- **Quality:** Good",
        "- **Notes:** Query expansion logic is sound, spreading activation well-designed",
        "",
        "### `embeddings.py` (210 lines)",
        "- **Quality:** Good",
        "- **Notes:** Three embedding methods provide flexibility, normalization handled correctly",
        "",
        "### `gaps.py` (215 lines)",
        "- **Quality:** Good",
        "- **Notes:** Useful gap detection, magic numbers should be configurable",
        "",
        "### `persistence.py` (295 lines)",
        "- **Quality:** Good",
        "- **Notes:** Complete save/load support, JSON export useful for visualization",
        "",
        "---",
        "",
        "## 8. Recommendations Summary",
        "",
        "### Critical (Should Fix)",
        "",
        "1. **Fix TF calculation bug** in `analysis.py:131` - per-document term frequency is always 1.",
        "",
        "### High Priority",
        "",
        "2. **Add ID → minicolumn mapping** to `HierarchicalLayer` to eliminate O(n) lookups.",
        "",
        "3. **Add tests for embeddings and semantics modules**.",
        "",
        "### Medium Priority",
        "",
        "4. Fix type annotation `any` → `Any` in semantics.py.",
        "",
        "5. Remove unused `Counter` import from analysis.py.",
        "",
        "6. Make threshold values configurable or document their rationale.",
        "",
        "7. Add verbose flag check to `export_graph_json`.",
        "",
        "### Low Priority (Nice to Have)",
        "",
        "8. Add integration tests for the complete pipeline.",
        "",
        "9. Document pickle security considerations.",
        "",
        "10. Consider adding progress callbacks for long-running operations.",
        "",
        "---",
        "",
        "## 9. Conclusion",
        "",
        "The Cortical Text Processor is a **well-crafted educational and research tool** with a thoughtful architecture inspired by neuroscience. The code is readable, well-documented, and follows good practices.",
        "",
        "The main areas for improvement are:",
        "- One calculation bug in TF-IDF per-document scoring",
        "- Performance optimization opportunities in ID lookups",
        "- Expanded test coverage for embeddings and semantics modules",
        "",
        "The codebase is suitable for its intended purpose of demonstrating biologically-inspired NLP concepts and performing text analysis on small-to-medium corpora.",
        "",
        "**Recommended Action:** Address the TF calculation bug and add ID mapping optimization before any production use.",
        "",
        "---",
        "",
        "*Review completed by Claude (Opus 4)*"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "KNOWLEDGE_TRANSFER.md",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "# Knowledge Transfer Document: Cortical Text Processor",
        "",
        "**Document Version:** 1.0",
        "**Date:** 2025-12-09",
        "**Author:** Claude (Opus 4)",
        "**Project Version:** 2.0.0",
        "",
        "---",
        "",
        "## Table of Contents",
        "",
        "1. [Executive Summary](#1-executive-summary)",
        "2. [Project Context and Goals](#2-project-context-and-goals)",
        "3. [Technical Architecture](#3-technical-architecture)",
        "4. [Key Data Structures](#4-key-data-structures)",
        "5. [Processing Pipeline](#5-processing-pipeline)",
        "6. [Recent Development Work](#6-recent-development-work)",
        "7. [Testing Strategy](#7-testing-strategy)",
        "8. [Known Issues and Future Work](#8-known-issues-and-future-work)",
        "9. [Development Workflow](#9-development-workflow)",
        "10. [Quick Reference](#10-quick-reference)",
        "",
        "---",
        "",
        "## 1. Executive Summary",
        "",
        "The Cortical Text Processor is a biologically-inspired NLP library that models text processing on the hierarchical structure of the human neocortex. It provides semantic analysis, document retrieval, and knowledge gap detection with **zero external dependencies**.",
        "",
        "### Key Statistics",
        "",
        "| Metric | Value |",
        "|--------|-------|",
        "| Library Version | 2.0.0 |",
        "| Source Files | 11 Python modules |",
        "| Lines of Code | ~4,300 (including tests) |",
        "| Test Count | 109 tests |",
        "| Test Coverage | Core functionality covered |",
        "| Sample Documents | 44 documents |",
        "| External Dependencies | None |",
        "",
        "### Project Health",
        "",
        "- All 109 unit tests passing",
        "- 6 of 7 identified bugs fixed",
        "- Demo produces 90.1% quality score",
        "- Documentation complete (README, CLAUDE.md, CODE_REVIEW.md)",
        "",
        "---",
        "",
        "## 2. Project Context and Goals",
        "",
        "### Original Vision",
        "",
        "The project implements a hierarchical text processing system inspired by neuroscience:",
        "- **Visual Cortex Analogy**: Just as the visual cortex processes images through layers (V1→V2→V4→IT), this system processes text through hierarchical layers (Tokens→Bigrams→Concepts→Documents)",
        "- **Cortical Columns**: The core data structure, `Minicolumn`, models cortical minicolumns that respond to specific features",
        "- **Lateral Connections**: Words/concepts form associations through co-occurrence, similar to lateral inhibition and excitation in the brain",
        "",
        "### Design Principles",
        "",
        "1. **Zero Dependencies**: Pure Python stdlib only",
        "2. **Educational Clarity**: Code explains biological analogies",
        "3. **Modular Architecture**: Each module has single responsibility",
        "4. **Self-Contained Semantics**: Derives meaning from corpus, not external knowledge bases",
        "",
        "---",
        "",
        "## 3. Technical Architecture",
        "",
        "### Module Dependency Graph",
        "",
        "```",
        "                    ┌─────────────────┐",
        "                    │   processor.py  │ (Orchestrator)",
        "                    └────────┬────────┘",
        "           ┌─────────────────┼─────────────────┐",
        "           │                 │                 │",
        "    ┌──────▼──────┐   ┌──────▼──────┐   ┌──────▼──────┐",
        "    │ tokenizer.py │   │  layers.py  │   │ analysis.py │",
        "    └─────────────┘   └──────┬──────┘   └─────────────┘",
        "                             │",
        "                      ┌──────▼──────┐",
        "                      │ minicolumn.py│",
        "                      └─────────────┘",
        "                             │",
        "    ┌────────────┬───────────┼───────────┬────────────┐",
        "    │            │           │           │            │",
        "┌───▼───┐   ┌────▼────┐ ┌────▼────┐ ┌────▼────┐ ┌─────▼─────┐",
        "│query.py│   │semantics│ │embeddings│ │ gaps.py │ │persistence│",
        "└───────┘   └─────────┘ └─────────┘ └─────────┘ └───────────┘",
        "```",
        "",
        "### Layer Architecture",
        "",
        "| Layer | Type | Content | Purpose |",
        "|-------|------|---------|---------|",
        "| 0 | TOKENS | Individual words | Feature detection |",
        "| 1 | BIGRAMS | Word pairs | Pattern recognition |",
        "| 2 | CONCEPTS | Term clusters | Semantic grouping |",
        "| 3 | DOCUMENTS | Full documents | Object recognition |",
        "",
        "### Core Classes",
        "",
        "1. **CorticalTextProcessor** (`processor.py:31`)",
        "   - Main facade class",
        "   - Coordinates all processing",
        "   - Entry point for all operations",
        "",
        "2. **HierarchicalLayer** (`layers.py:54`)",
        "   - Manages minicolumns at a hierarchy level",
        "   - O(1) lookup via `_id_index` (recently added)",
        "   - Statistics computation",
        "",
        "3. **Minicolumn** (`minicolumn.py:22`)",
        "   - Core data structure",
        "   - Uses `__slots__` for memory efficiency",
        "   - Tracks: content, connections, scores, document associations",
        "",
        "---",
        "",
        "## 4. Key Data Structures",
        "",
        "### Minicolumn Fields",
        "",
        "```python",
        "class Minicolumn:",
        "    __slots__ = [",
        "        'content',              # str: The term/phrase",
        "        'id',                   # str: Unique identifier",
        "        'layer',                # CorticalLayer: Hierarchy level",
        "        'activation',           # float: Current activation level",
        "        'lateral_connections',  # Dict[str, float]: Term→Weight",
        "        'feedforward_children', # List[str]: Lower-level IDs",
        "        'feedback_parents',     # List[str]: Higher-level IDs",
        "        'document_ids',         # Set[str]: Documents containing this term",
        "        'doc_occurrence_counts',# Dict[str, int]: Per-doc term frequency (NEW)",
        "        'metadata',             # Dict: Arbitrary metadata",
        "        'pagerank',             # float: Importance score",
        "        'tfidf',                # float: Global TF-IDF",
        "        'tfidf_per_doc',        # Dict[str, float]: Per-document TF-IDF",
        "        'cluster_id',           # Optional[int]: Cluster assignment",
        "        'embedding',            # Optional[List[float]]: Vector representation",
        "    ]",
        "```",
        "",
        "### HierarchicalLayer Index",
        "",
        "```python",
        "class HierarchicalLayer:",
        "    minicolumns: Dict[str, Minicolumn]  # content → Minicolumn",
        "    _id_index: Dict[str, str]           # id → content (NEW: O(1) lookup)",
        "",
        "    def get_by_id(self, minicolumn_id: str) -> Optional[Minicolumn]:",
        "        \"\"\"O(1) lookup by ID - added during bug fixes\"\"\"",
        "```",
        "",
        "---",
        "",
        "## 5. Processing Pipeline",
        "",
        "### Standard Processing Flow",
        "",
        "```python",
        "# 1. Initialize",
        "processor = CorticalTextProcessor()",
        "",
        "# 2. Ingest Documents",
        "processor.process_document(\"doc_id\", \"content...\")",
        "# Creates: Tokens → Bigrams → Lateral connections",
        "",
        "# 3. Build Network (or use compute_all())",
        "processor.propagate_activation()    # Spread activation through layers",
        "processor.compute_importance()      # PageRank scoring",
        "processor.compute_tfidf()          # TF-IDF weighting",
        "processor.build_concept_clusters() # Semantic clustering",
        "processor.compute_document_connections()  # Doc-doc similarity",
        "",
        "# 4. Enhance with Semantics (optional)",
        "processor.extract_corpus_semantics()  # PMI-based relations",
        "processor.retrofit_connections()      # Blend semantic weights",
        "",
        "# 5. Compute Embeddings (optional)",
        "processor.compute_graph_embeddings(dimensions=32, method='adjacency')",
        "processor.retrofit_embeddings()       # Improve with semantics",
        "",
        "# 6. Query",
        "results = processor.find_documents_for_query(\"search terms\")",
        "expanded = processor.expand_query(\"term\")",
        "related = processor.find_related_documents(\"doc_id\")",
        "",
        "# 7. Analysis",
        "gaps = processor.analyze_knowledge_gaps()",
        "anomalies = processor.detect_anomalies(threshold=0.1)",
        "health = processor.compute_corpus_health()",
        "```",
        "",
        "### compute_all() Convenience Method",
        "",
        "The `compute_all(verbose=True)` method runs steps 3-6 in optimal order:",
        "1. propagate_activation()",
        "2. compute_importance()",
        "3. compute_tfidf()",
        "4. extract_corpus_semantics()",
        "5. retrofit_connections()",
        "6. build_concept_clusters()",
        "7. compute_document_connections()",
        "8. compute_graph_embeddings()",
        "9. retrofit_embeddings()",
        "",
        "---",
        "",
        "## 6. Recent Development Work",
        "",
        "### Code Review (2025-12-09)",
        "",
        "A comprehensive code review was performed, resulting in:",
        "- CODE_REVIEW.md documenting findings",
        "- TASK_LIST.md tracking required fixes",
        "- CLAUDE.md project guide",
        "",
        "### Bug Fixes Applied",
        "",
        "| Issue | File | Fix |",
        "|-------|------|-----|",
        "| TF-IDF always 1 | analysis.py:131 | Added `doc_occurrence_counts` field |",
        "| O(n) ID lookups | layers.py | Added `_id_index` + `get_by_id()` |",
        "| Type error `any` | semantics.py | Changed to `Any` |",
        "| Unused import | analysis.py | Removed `Counter` |",
        "| Verbose ignored | persistence.py | Added verbose param |",
        "",
        "### Tests Added",
        "",
        "70 new tests across 5 test files:",
        "",
        "- **test_analysis.py**: 17 tests (PageRank, TF-IDF, clustering)",
        "- **test_embeddings.py**: 15 tests (all embedding methods)",
        "- **test_semantics.py**: 12 tests (PMI, retrofitting)",
        "- **test_gaps.py**: 15 tests (gap detection, anomalies)",
        "- **test_persistence.py**: 12 tests (save/load, export)",
        "",
        "### Sample Documents Added",
        "",
        "7 new domain documents for diverse corpus testing:",
        "- financial_analysis.txt",
        "- elliot_wave_theory.txt",
        "- candlestick_patterns.txt",
        "- data_structures.txt",
        "- computational_theory.txt",
        "- compilers.txt",
        "- neocortex.txt",
        "",
        "---",
        "",
        "## 7. Testing Strategy",
        "",
        "### Running Tests",
        "",
        "```bash",
        "# All tests",
        "python -m unittest discover -s tests -v",
        "",
        "# Specific test file",
        "python -m unittest tests.test_analysis -v",
        "",
        "# Single test",
        "python -m unittest tests.test_analysis.TestPageRank.test_pagerank_convergence",
        "```",
        "",
        "### Test Organization",
        "",
        "```",
        "tests/",
        "├── test_tokenizer.py    # Tokenization, stemming, stop words",
        "├── test_processor.py    # Document processing, main workflow",
        "├── test_layers.py       # Layer CRUD, statistics",
        "├── test_analysis.py     # PageRank, TF-IDF, clustering",
        "├── test_embeddings.py   # All embedding methods",
        "├── test_semantics.py    # PMI, retrofitting",
        "├── test_gaps.py         # Gap detection, anomalies",
        "└── test_persistence.py  # Save/load, JSON export",
        "```",
        "",
        "### Test Coverage Summary",
        "",
        "| Module | Status |",
        "|--------|--------|",
        "| tokenizer.py | Covered |",
        "| processor.py | Covered |",
        "| minicolumn.py | Covered |",
        "| layers.py | Covered |",
        "| analysis.py | Covered |",
        "| semantics.py | Covered |",
        "| embeddings.py | Covered |",
        "| gaps.py | Covered |",
        "| persistence.py | Covered |",
        "| query.py | Partially covered (via processor tests) |",
        "",
        "---",
        "",
        "## 8. Known Issues and Future Work",
        "",
        "### Outstanding Issue",
        "",
        "**Magic Numbers in gaps.py**",
        "",
        "```python",
        "# Line 62: Isolation threshold",
        "avg_sim < 0.02",
        "",
        "# Line 76: Weak topic threshold",
        "tfidf > 0.005",
        "",
        "# Line 99: Bridge opportunity range",
        "0.005 < sim < 0.03",
        "```",
        "",
        "**Recommendation**: Make these configurable or document rationale.",
        "",
        "### Potential Enhancements",
        "",
        "1. **Performance**",
        "   - Sparse similarity computation for large corpora",
        "   - Streaming/incremental document processing",
        "   - Progress callbacks for long operations",
        "",
        "2. **Features**",
        "   - More embedding methods (word2vec-style)",
        "   - Configurable threshold parameters",
        "   - Document summarization improvements",
        "",
        "3. **Security**",
        "   - JSON-based persistence as pickle alternative",
        "   - Path validation for file operations",
        "",
        "---",
        "",
        "## 9. Development Workflow",
        "",
        "### Making Changes",
        "",
        "1. **Read existing code** before modifying",
        "2. **Run tests** before and after changes",
        "3. **Follow conventions**:",
        "   - Type hints on all functions",
        "   - Google-style docstrings",
        "   - 100-char line limit",
        "",
        "### Adding New Features",
        "",
        "```python",
        "# 1. Add to appropriate module",
        "def new_feature(self, param: str) -> List[str]:",
        "    \"\"\"Short description.",
        "",
        "    Args:",
        "        param: Description of parameter.",
        "",
        "    Returns:",
        "        Description of return value.",
        "    \"\"\"",
        "    pass",
        "",
        "# 2. Export via processor.py if user-facing",
        "",
        "# 3. Add tests in tests/test_<module>.py",
        "",
        "# 4. Update CLAUDE.md if significant",
        "```",
        "",
        "### Commit Message Format",
        "",
        "```",
        "<type>: <description>",
        "",
        "Types: Add, Fix, Update, Remove, Refactor",
        "Example: Fix per-document TF-IDF calculation bug",
        "```",
        "",
        "---",
        "",
        "## 10. Quick Reference",
        "",
        "### File Locations",
        "",
        "| Purpose | File |",
        "|---------|------|",
        "| Main entry point | cortical/processor.py |",
        "| Core data structure | cortical/minicolumn.py |",
        "| Layer management | cortical/layers.py |",
        "| Text tokenization | cortical/tokenizer.py |",
        "| PageRank/TF-IDF | cortical/analysis.py |",
        "| Semantic extraction | cortical/semantics.py |",
        "| Graph embeddings | cortical/embeddings.py |",
        "| Search/retrieval | cortical/query.py |",
        "| Gap detection | cortical/gaps.py |",
        "| Save/load | cortical/persistence.py |",
        "",
        "### Common Operations",
        "",
        "```python",
        "# Load saved model",
        "processor = CorticalTextProcessor.load(\"model.pkl\")",
        "",
        "# Add documents",
        "processor.process_document(\"id\", \"text\")",
        "processor.process_documents_from_directory(\"./samples\")",
        "",
        "# Compute everything",
        "processor.compute_all(verbose=True)",
        "",
        "# Search",
        "results = processor.find_documents_for_query(\"query\", top_n=5)",
        "",
        "# Expand query with synonyms",
        "expanded = processor.expand_query(\"term\", max_expansions=10)",
        "",
        "# Find similar terms",
        "similar = processor.find_similar_by_embedding(\"term\", top_n=5)",
        "",
        "# Corpus health",
        "health = processor.compute_corpus_health()",
        "gaps = processor.analyze_knowledge_gaps()",
        "",
        "# Export for visualization",
        "processor.export_graph_json(\"graph.json\", verbose=False)",
        "processor.export_embeddings_json(\"embeddings.json\")",
        "```",
        "",
        "### Key Metrics from Demo",
        "",
        "```",
        "Documents: 44",
        "Token minicolumns: ~3,350",
        "Bigram minicolumns: ~6,530",
        "Lateral connections: ~38,200",
        "Quality score: 90.1%",
        "```",
        "",
        "---",
        "",
        "## Related Documentation",
        "",
        "- **README.md**: User-facing documentation and installation",
        "- **CLAUDE.md**: Project guide for Claude Code",
        "- **CODE_REVIEW.md**: Detailed code review findings",
        "- **TASK_LIST.md**: Bug fix tracking and status",
        "",
        "---",
        "",
        "*Document prepared for knowledge transfer on 2025-12-09*"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "TASK_LIST.md",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "# Task List: Required Bug Fixes",
        "",
        "This document tracks required bug fixes identified during the code review of the Cortical Text Processor.",
        "",
        "**Last Updated:** 2025-12-09",
        "**Status:** All critical and high-priority tasks completed",
        "",
        "---",
        "",
        "## Critical Priority",
        "",
        "### 1. Fix Per-Document TF-IDF Calculation Bug",
        "",
        "**File:** `cortical/analysis.py`",
        "**Line:** 131",
        "**Status:** [x] Completed",
        "",
        "**Problem:**",
        "The per-document term frequency calculation was incorrect. The code always returned 1.",
        "",
        "**Solution Applied:**",
        "1. Added `doc_occurrence_counts: Dict[str, int]` field to `Minicolumn` class",
        "2. Updated `processor.py` to track per-document token occurrences during document processing",
        "3. Fixed TF-IDF calculation to use actual occurrence counts: `col.doc_occurrence_counts.get(doc_id, 1)`",
        "",
        "**Files Modified:**",
        "- `cortical/minicolumn.py` - Added new field and serialization support",
        "- `cortical/processor.py` - Track occurrences per document",
        "- `cortical/analysis.py` - Use actual counts in TF-IDF calculation",
        "",
        "---",
        "",
        "## High Priority",
        "",
        "### 2. Add ID-to-Minicolumn Lookup Optimization",
        "",
        "**Files:** `cortical/layers.py`, `cortical/analysis.py`, `cortical/query.py`",
        "**Status:** [x] Completed",
        "",
        "**Problem:**",
        "Multiple O(n) linear searches occurred when looking up minicolumns by ID.",
        "",
        "**Solution Applied:**",
        "1. Added `_id_index: Dict[str, str]` secondary index to `HierarchicalLayer`",
        "2. Added `get_by_id()` method for O(1) lookups",
        "3. Updated `from_dict()` to rebuild index when loading",
        "4. Replaced all linear searches with `get_by_id()` calls",
        "",
        "**Files Modified:**",
        "- `cortical/layers.py` - Added `_id_index` and `get_by_id()` method",
        "- `cortical/analysis.py` - Updated PageRank, activation propagation, label propagation",
        "- `cortical/query.py` - Updated query expansion, spreading activation, related documents",
        "",
        "**Performance Impact:** Graph algorithms improved from O(n²) to O(n)",
        "",
        "---",
        "",
        "## Medium Priority",
        "",
        "### 3. Fix Type Annotation Errors",
        "",
        "**File:** `cortical/semantics.py`",
        "**Lines:** 153, 248",
        "**Status:** [x] Completed",
        "",
        "**Solution Applied:**",
        "1. Added `Any` to imports",
        "2. Changed `any` to `Any` on both lines",
        "",
        "---",
        "",
        "### 4. Remove Unused Import",
        "",
        "**File:** `cortical/analysis.py`",
        "**Line:** 16",
        "**Status:** [x] Completed",
        "",
        "**Solution Applied:**",
        "Removed `Counter` from the import statement.",
        "",
        "---",
        "",
        "### 5. Fix Unconditional Print in Export Function",
        "",
        "**File:** `cortical/persistence.py`",
        "**Lines:** 175-176",
        "**Status:** [x] Completed",
        "",
        "**Solution Applied:**",
        "1. Added `verbose: bool = True` parameter to `export_graph_json()`",
        "2. Wrapped print statements in `if verbose:` conditional",
        "",
        "---",
        "",
        "## Low Priority",
        "",
        "### 6. Add Missing Test Coverage",
        "",
        "**Files:** `tests/test_embeddings.py`, `tests/test_semantics.py`, `tests/test_gaps.py`, `tests/test_analysis.py`, `tests/test_persistence.py`",
        "**Status:** [x] Completed",
        "",
        "**Tests Added:**",
        "",
        "**test_embeddings.py (15 tests):**",
        "- `test_compute_graph_embeddings_adjacency`",
        "- `test_compute_graph_embeddings_random_walk`",
        "- `test_compute_graph_embeddings_spectral`",
        "- `test_compute_graph_embeddings_invalid_method`",
        "- `test_embedding_similarity`",
        "- `test_embedding_similarity_self`",
        "- `test_embedding_similarity_missing_term`",
        "- `test_find_similar_by_embedding`",
        "- `test_find_similar_by_embedding_missing_term`",
        "- `test_embedding_dimensions`",
        "- `test_embedding_normalization`",
        "- `test_empty_layer_embeddings`",
        "",
        "**test_semantics.py (12 tests):**",
        "- `test_extract_corpus_semantics`",
        "- `test_extract_corpus_semantics_cooccurs`",
        "- `test_retrofit_connections`",
        "- `test_retrofit_connections_affects_weights`",
        "- `test_retrofit_embeddings`",
        "- `test_get_relation_type_weight`",
        "- `test_relation_weights_constant`",
        "- `test_empty_corpus_semantics`",
        "- `test_retrofit_empty_relations`",
        "- `test_larger_window_more_relations`",
        "",
        "**test_gaps.py (15 tests):**",
        "- `test_analyze_knowledge_gaps_structure`",
        "- `test_analyze_knowledge_gaps_summary`",
        "- `test_analyze_knowledge_gaps_isolated_documents`",
        "- `test_analyze_knowledge_gaps_weak_topics`",
        "- `test_analyze_knowledge_gaps_coverage_score`",
        "- `test_detect_anomalies_structure`",
        "- `test_detect_anomalies_reasons`",
        "- `test_detect_anomalies_sorted`",
        "- `test_detect_anomalies_threshold`",
        "- `test_empty_corpus_gaps`",
        "- `test_single_document_gaps`",
        "- `test_single_document_anomalies`",
        "- `test_bridge_opportunities_format`",
        "",
        "**test_analysis.py (17 tests):**",
        "- `test_pagerank_empty_layer`",
        "- `test_pagerank_single_node`",
        "- `test_pagerank_multiple_nodes`",
        "- `test_pagerank_convergence`",
        "- `test_tfidf_empty_corpus`",
        "- `test_tfidf_single_document`",
        "- `test_tfidf_multiple_documents`",
        "- `test_tfidf_per_document`",
        "- `test_propagation_empty_layers`",
        "- `test_propagation_preserves_activation`",
        "- `test_clustering_empty_layer`",
        "- `test_clustering_returns_dict`",
        "- `test_clustering_min_size`",
        "- `test_build_concept_clusters`",
        "- `test_document_connections`",
        "- `test_cosine_similarity` (5 sub-tests)",
        "- `test_get_by_id_returns_correct_minicolumn`",
        "- `test_get_by_id_returns_none_for_missing`",
        "",
        "**test_persistence.py (12 tests):**",
        "- `test_save_and_load`",
        "- `test_save_load_preserves_id_index`",
        "- `test_save_load_preserves_doc_occurrence_counts`",
        "- `test_save_load_empty_processor`",
        "- `test_export_graph_json`",
        "- `test_export_graph_json_layer_filter`",
        "- `test_export_graph_json_min_weight`",
        "- `test_export_graph_json_max_nodes`",
        "- `test_export_graph_json_verbose_false`",
        "- `test_export_embeddings_json`",
        "- `test_export_embeddings_json_with_metadata`",
        "- `test_get_state_summary`",
        "- `test_get_state_summary_empty`",
        "",
        "**Test Coverage Summary:**",
        "- Previous: 39 tests",
        "- Added: 70 new tests",
        "- **Total: 109 tests (all passing)**",
        "",
        "---",
        "",
        "### 7. Document Magic Numbers",
        "",
        "**File:** `cortical/gaps.py`",
        "**Lines:** 62, 76, 99",
        "**Status:** [ ] Deferred",
        "",
        "**Note:** This task remains as a future enhancement. The magic numbers are functional but could benefit from documentation or configuration options.",
        "",
        "---",
        "",
        "## Summary",
        "",
        "| Priority | Task | Status |",
        "|----------|------|--------|",
        "| Critical | Fix TF-IDF per-doc calculation | ✅ Completed |",
        "| High | Add ID lookup optimization | ✅ Completed |",
        "| Medium | Fix type annotations | ✅ Completed |",
        "| Medium | Remove unused import | ✅ Completed |",
        "| Medium | Add verbose parameter | ✅ Completed |",
        "| Low | Add test coverage | ✅ Completed |",
        "| Low | Document magic numbers | ⏳ Deferred |",
        "",
        "**Completion Rate:** 6/7 tasks (86%)",
        "",
        "---",
        "",
        "## Test Results",
        "",
        "```",
        "Ran 109 tests in 0.131s",
        "OK",
        "```",
        "",
        "All tests passing as of 2025-12-09.",
        "",
        "---",
        "",
        "*Updated from code review on 2025-12-09*"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "cortical/analysis.py",
      "function": "Graph analysis algorithms for the cortical network.",
      "start_line": 6,
      "lines_added": [
        "from collections import defaultdict"
      ],
      "lines_removed": [
        "from collections import defaultdict, Counter"
      ],
      "context_before": [
        "",
        "Contains implementations of:",
        "- PageRank for importance scoring",
        "- TF-IDF for term weighting",
        "- Label propagation for clustering",
        "- Activation propagation for information flow",
        "\"\"\"",
        "",
        "import math",
        "from typing import Dict, List, Tuple, Set, Optional"
      ],
      "context_after": [
        "",
        "from .layers import CorticalLayer, HierarchicalLayer",
        "from .minicolumn import Minicolumn",
        "",
        "",
        "def compute_pagerank(",
        "    layer: HierarchicalLayer,",
        "    damping: float = 0.85,",
        "    iterations: int = 20,",
        "    tolerance: float = 1e-6"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/analysis.py",
      "function": "def compute_pagerank(",
      "start_line": 47,
      "lines_added": [
        "            # Use O(1) lookup via get_by_id instead of O(n) linear search",
        "            if layer.get_by_id(target_id) is not None:"
      ],
      "lines_removed": [
        "            if target_id in layer.minicolumns or any(",
        "                c.id == target_id for c in layer.minicolumns.values()",
        "            ):"
      ],
      "context_before": [
        "    ",
        "    # Initialize PageRank uniformly",
        "    pagerank = {col.id: 1.0 / n for col in layer.minicolumns.values()}",
        "    ",
        "    # Build incoming links map",
        "    incoming: Dict[str, List[Tuple[str, float]]] = defaultdict(list)",
        "    outgoing_sum: Dict[str, float] = defaultdict(float)",
        "    ",
        "    for col in layer.minicolumns.values():",
        "        for target_id, weight in col.lateral_connections.items():"
      ],
      "context_after": [
        "                incoming[target_id].append((col.id, weight))",
        "                outgoing_sum[col.id] += weight",
        "    ",
        "    # Iterate until convergence",
        "    for iteration in range(iterations):",
        "        new_pagerank = {}",
        "        max_diff = 0.0",
        "        ",
        "        for col in layer.minicolumns.values():",
        "            # Sum of weighted incoming PageRank"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/analysis.py",
      "function": "def compute_tfidf(",
      "start_line": 118,
      "lines_added": [
        "            # Per-document TF-IDF using actual occurrence counts",
        "                # Get actual term frequency in this document",
        "                doc_tf = col.doc_occurrence_counts.get(doc_id, 1)"
      ],
      "lines_removed": [
        "            # Per-document TF-IDF",
        "                # Count occurrences in this document",
        "                doc_tf = sum(1 for d in [doc_id] if d in col.document_ids)"
      ],
      "context_before": [
        "        if df > 0:",
        "            # Inverse document frequency",
        "            idf = math.log(num_docs / df)",
        "            ",
        "            # Term frequency (normalized by occurrence count)",
        "            tf = math.log1p(col.occurrence_count)",
        "            ",
        "            # TF-IDF",
        "            col.tfidf = tf * idf",
        "            "
      ],
      "context_after": [
        "            for doc_id in col.document_ids:",
        "                col.tfidf_per_doc[doc_id] = math.log1p(doc_tf) * idf",
        "",
        "",
        "def propagate_activation(",
        "    layers: Dict[CorticalLayer, HierarchicalLayer],",
        "    iterations: int = 3,",
        "    decay: float = 0.8,",
        "    lateral_weight: float = 0.3",
        ") -> None:",
        "    \"\"\""
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/analysis.py",
      "function": "def propagate_activation(",
      "start_line": 159,
      "lines_added": [
        "                # Add lateral input using O(1) ID lookup",
        "                    neighbor = layer.get_by_id(neighbor_id)",
        "                    if neighbor:",
        "                # Add feedforward input using O(1) ID lookup",
        "                        source = lower_layer.get_by_id(source_id)",
        "                        if source:",
        "                            new_act += source.activation * 0.5",
        "                            break"
      ],
      "lines_removed": [
        "                # Add lateral input",
        "                    if neighbor_id in layer.minicolumns:",
        "                        neighbor = layer.minicolumns[neighbor_id]",
        "                    else:",
        "                        # Look up by ID",
        "                        for c in layer.minicolumns.values():",
        "                            if c.id == neighbor_id:",
        "                                new_act += c.activation * weight * lateral_weight",
        "                                break",
        "                # Add feedforward input",
        "                        for source in lower_layer.minicolumns.values():",
        "                            if source.id == source_id:",
        "                                new_act += source.activation * 0.5",
        "                                break"
      ],
      "context_before": [
        "        # Process each layer",
        "        for layer_enum in CorticalLayer:",
        "            if layer_enum not in layers:",
        "                continue",
        "            layer = layers[layer_enum]",
        "            ",
        "            for col in layer.minicolumns.values():",
        "                # Start with decayed current activation",
        "                new_act = col.activation * decay",
        "                "
      ],
      "context_after": [
        "                for neighbor_id, weight in col.lateral_connections.items():",
        "                        new_act += neighbor.activation * weight * lateral_weight",
        "                ",
        "                for source_id in col.feedforward_sources:",
        "                    # Find source in lower layers",
        "                    for lower_enum in CorticalLayer:",
        "                        if lower_enum >= layer_enum:",
        "                            break",
        "                        if lower_enum not in layers:",
        "                            continue",
        "                        lower_layer = layers[lower_enum]",
        "                ",
        "                new_activations[col.id] = new_act",
        "        ",
        "        # Apply new activations",
        "        for layer_enum in CorticalLayer:",
        "            if layer_enum not in layers:",
        "                continue",
        "            layer = layers[layer_enum]",
        "            for col in layer.minicolumns.values():",
        "                if col.id in new_activations:"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/analysis.py",
      "function": "def cluster_by_label_propagation(",
      "start_line": 235,
      "lines_added": [
        "                # Use O(1) ID lookup instead of linear search",
        "                neighbor = layer.get_by_id(neighbor_id)",
        "                if neighbor and neighbor.content in labels:",
        "                    label_weights[labels[neighbor.content]] += weight"
      ],
      "lines_removed": [
        "                # Find neighbor content",
        "                neighbor_content = None",
        "                for c in layer.minicolumns.values():",
        "                    if c.id == neighbor_id:",
        "                        neighbor_content = c.content",
        "                        break",
        "                ",
        "                if neighbor_content and neighbor_content in labels:",
        "                    label_weights[labels[neighbor_content]] += weight"
      ],
      "context_before": [
        "        changed = False",
        "        ",
        "        # Process in order (could shuffle for better results)",
        "        for content in columns:",
        "            col = layer.minicolumns[content]",
        "            ",
        "            # Count neighbor labels weighted by connection strength",
        "            label_weights: Dict[int, float] = defaultdict(float)",
        "            ",
        "            for neighbor_id, weight in col.lateral_connections.items():"
      ],
      "context_after": [
        "            ",
        "            # Adopt most common label",
        "            if label_weights:",
        "                best_label = max(label_weights.items(), key=lambda x: x[1])[0]",
        "                if labels[content] != best_label:",
        "                    labels[content] = best_label",
        "                    changed = True",
        "        ",
        "        if not changed:",
        "            break"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/layers.py",
      "function": "class HierarchicalLayer:",
      "start_line": 60,
      "lines_added": [
        "        minicolumns: Dictionary mapping content to Minicolumn objects",
        "        _id_index: Secondary index mapping minicolumn IDs to content for O(1) lookups",
        "",
        "        self._id_index: Dict[str, str] = {}  # Maps minicolumn ID to content for O(1) lookup",
        "            self._id_index[col_id] = content  # Maintain ID index for O(1) lookup",
        "",
        "",
        "",
        "    def get_by_id(self, col_id: str) -> Optional[Minicolumn]:",
        "        \"\"\"",
        "        Get a minicolumn by its ID in O(1) time.",
        "",
        "        This method uses a secondary index to avoid O(n) linear searches",
        "        when looking up minicolumns by their ID rather than content.",
        "",
        "        Args:",
        "            col_id: The minicolumn ID (e.g., \"L0_neural\")",
        "",
        "        Returns:",
        "            The Minicolumn if found, None otherwise",
        "        \"\"\"",
        "        content = self._id_index.get(col_id)",
        "        return self.minicolumns.get(content) if content else None",
        ""
      ],
      "lines_removed": [
        "        minicolumns: Dictionary mapping IDs to Minicolumn objects",
        "        ",
        "        ",
        "            ",
        "    "
      ],
      "context_before": [
        "    \"\"\"",
        "    A layer in the cortical hierarchy containing minicolumns.",
        "    ",
        "    Each layer contains a collection of minicolumns and provides",
        "    methods for managing them. Layers are organized hierarchically,",
        "    with feedforward connections from lower to higher layers and",
        "    lateral connections within each layer.",
        "    ",
        "    Attributes:",
        "        level: The layer number (0-3)"
      ],
      "context_after": [
        "    Example:",
        "        layer = HierarchicalLayer(CorticalLayer.TOKENS)",
        "        col = layer.get_or_create_minicolumn(\"neural\")",
        "        col.occurrence_count += 1",
        "    \"\"\"",
        "    ",
        "    def __init__(self, level: CorticalLayer):",
        "        \"\"\"",
        "        Initialize a hierarchical layer.",
        "        ",
        "        Args:",
        "            level: The CorticalLayer enum value for this layer",
        "        \"\"\"",
        "        self.level = level",
        "        self.minicolumns: Dict[str, Minicolumn] = {}",
        "    ",
        "    def get_or_create_minicolumn(self, content: str) -> Minicolumn:",
        "        \"\"\"",
        "        Get existing minicolumn or create new one.",
        "        ",
        "        This is the primary way to add content to a layer. If a",
        "        minicolumn for this content already exists, return it.",
        "        Otherwise, create a new one.",
        "        ",
        "        Args:",
        "            content: The content for this minicolumn",
        "            ",
        "        Returns:",
        "            The existing or newly created Minicolumn",
        "        \"\"\"",
        "        if content not in self.minicolumns:",
        "            col_id = f\"L{self.level}_{content}\"",
        "            self.minicolumns[content] = Minicolumn(col_id, content, self.level)",
        "        return self.minicolumns[content]",
        "    ",
        "    def get_minicolumn(self, content: str) -> Optional[Minicolumn]:",
        "        \"\"\"",
        "        Get a minicolumn by content, or None if not found.",
        "        Args:",
        "            content: The content to look up",
        "        Returns:",
        "            The Minicolumn if found, None otherwise",
        "        \"\"\"",
        "        return self.minicolumns.get(content)",
        "    def column_count(self) -> int:",
        "        \"\"\"Return the number of minicolumns in this layer.\"\"\"",
        "        return len(self.minicolumns)",
        "    ",
        "    def total_connections(self) -> int:",
        "        \"\"\"Return total number of lateral connections in this layer.\"\"\"",
        "        return sum(col.connection_count() for col in self.minicolumns.values())",
        "    ",
        "    def average_activation(self) -> float:",
        "        \"\"\"Calculate average activation across all minicolumns.\"\"\""
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/layers.py",
      "function": "class HierarchicalLayer:",
      "start_line": 238,
      "lines_added": [
        "            col = Minicolumn.from_dict(col_data)",
        "            layer.minicolumns[content] = col",
        "            layer._id_index[col.id] = content  # Rebuild ID index"
      ],
      "lines_removed": [
        "            layer.minicolumns[content] = Minicolumn.from_dict(col_data)"
      ],
      "context_before": [
        "        Create a layer from dictionary representation.",
        "        ",
        "        Args:",
        "            data: Dictionary with layer data",
        "            ",
        "        Returns:",
        "            New HierarchicalLayer instance",
        "        \"\"\"",
        "        layer = cls(CorticalLayer(data['level']))",
        "        for content, col_data in data.get('minicolumns', {}).items():"
      ],
      "context_after": [
        "        return layer",
        "    ",
        "    def __repr__(self) -> str:",
        "        return f\"HierarchicalLayer(level={self.level.name}, columns={len(self.minicolumns)})\""
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/minicolumn.py",
      "function": "class Minicolumn:",
      "start_line": 30,
      "lines_added": [
        "        doc_occurrence_counts: Per-document occurrence counts for accurate TF-IDF",
        "        'tfidf', 'tfidf_per_doc', 'pagerank', 'cluster_id',",
        "        'doc_occurrence_counts'"
      ],
      "lines_removed": [
        "        'tfidf', 'tfidf_per_doc', 'pagerank', 'cluster_id'"
      ],
      "context_before": [
        "        layer: Which layer this column belongs to",
        "        activation: Current activation level (like neural firing rate)",
        "        occurrence_count: How many times this has been observed",
        "        document_ids: Which documents contain this content",
        "        lateral_connections: Connections to other columns at same layer",
        "        feedforward_sources: IDs of columns that feed into this one",
        "        tfidf: TF-IDF weight for this term",
        "        tfidf_per_doc: Document-specific TF-IDF scores",
        "        pagerank: Importance score from PageRank algorithm",
        "        cluster_id: Which cluster this belongs to (for Layer 0)"
      ],
      "context_after": [
        "        ",
        "    Example:",
        "        col = Minicolumn(\"L0_neural\", \"neural\", 0)",
        "        col.occurrence_count = 15",
        "        col.add_lateral_connection(\"L0_network\", 0.8)",
        "    \"\"\"",
        "    ",
        "    __slots__ = [",
        "        'id', 'content', 'layer', 'activation', 'occurrence_count',",
        "        'document_ids', 'lateral_connections', 'feedforward_sources',",
        "    ]",
        "    ",
        "    def __init__(self, id: str, content: str, layer: int):",
        "        \"\"\"",
        "        Initialize a minicolumn.",
        "        ",
        "        Args:",
        "            id: Unique identifier for this column",
        "            content: The content this column represents",
        "            layer: Layer number (0-3)"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/minicolumn.py",
      "function": "class Minicolumn:",
      "start_line": 64,
      "lines_added": [
        "        self.doc_occurrence_counts: Dict[str, int] = {}"
      ],
      "lines_removed": [],
      "context_before": [
        "        self.layer = layer",
        "        self.activation = 0.0",
        "        self.occurrence_count = 0",
        "        self.document_ids: Set[str] = set()",
        "        self.lateral_connections: Dict[str, float] = {}",
        "        self.feedforward_sources: Set[str] = set()",
        "        self.tfidf = 0.0",
        "        self.tfidf_per_doc: Dict[str, float] = {}",
        "        self.pagerank = 1.0",
        "        self.cluster_id: Optional[int] = None"
      ],
      "context_after": [
        "    ",
        "    def add_lateral_connection(self, target_id: str, weight: float = 1.0) -> None:",
        "        \"\"\"",
        "        Add or strengthen a lateral connection to another column.",
        "        ",
        "        Lateral connections represent associations learned through",
        "        co-occurrence (like Hebbian learning: \"neurons that fire together",
        "        wire together\").",
        "        ",
        "        Args:"
      ],
      "change_type": "add"
    },
    {
      "file": "cortical/minicolumn.py",
      "function": "class Minicolumn:",
      "start_line": 121,
      "lines_added": [
        "            'cluster_id': self.cluster_id,",
        "            'doc_occurrence_counts': self.doc_occurrence_counts"
      ],
      "lines_removed": [
        "            'cluster_id': self.cluster_id"
      ],
      "context_before": [
        "            'content': self.content,",
        "            'layer': self.layer,",
        "            'activation': self.activation,",
        "            'occurrence_count': self.occurrence_count,",
        "            'document_ids': list(self.document_ids),",
        "            'lateral_connections': self.lateral_connections,",
        "            'feedforward_sources': list(self.feedforward_sources),",
        "            'tfidf': self.tfidf,",
        "            'tfidf_per_doc': self.tfidf_per_doc,",
        "            'pagerank': self.pagerank,"
      ],
      "context_after": [
        "        }",
        "    ",
        "    @classmethod",
        "    def from_dict(cls, data: Dict) -> 'Minicolumn':",
        "        \"\"\"",
        "        Create a minicolumn from dictionary representation.",
        "        ",
        "        Args:",
        "            data: Dictionary with minicolumn data",
        "            "
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/minicolumn.py",
      "function": "class Minicolumn:",
      "start_line": 145,
      "lines_added": [
        "        col.doc_occurrence_counts = data.get('doc_occurrence_counts', {})"
      ],
      "lines_removed": [],
      "context_before": [
        "        col = cls(data['id'], data['content'], data['layer'])",
        "        col.activation = data.get('activation', 0.0)",
        "        col.occurrence_count = data.get('occurrence_count', 0)",
        "        col.document_ids = set(data.get('document_ids', []))",
        "        col.lateral_connections = data.get('lateral_connections', {})",
        "        col.feedforward_sources = set(data.get('feedforward_sources', []))",
        "        col.tfidf = data.get('tfidf', 0.0)",
        "        col.tfidf_per_doc = data.get('tfidf_per_doc', {})",
        "        col.pagerank = data.get('pagerank', 1.0)",
        "        col.cluster_id = data.get('cluster_id')"
      ],
      "context_after": [
        "        return col",
        "    ",
        "    def __repr__(self) -> str:",
        "        return f\"Minicolumn(id={self.id}, content={self.content}, layer={self.layer})\""
      ],
      "change_type": "add"
    },
    {
      "file": "cortical/persistence.py",
      "function": "def load_processor(",
      "start_line": 94,
      "lines_added": [
        "    max_nodes: int = 500,",
        "    verbose: bool = True",
        "",
        "",
        "        verbose: Print progress messages",
        ""
      ],
      "lines_removed": [
        "    max_nodes: int = 500",
        "    ",
        "    ",
        "        "
      ],
      "context_before": [
        "        print(f\"  - {total_conns} connections\")",
        "    ",
        "    return layers, documents, metadata",
        "",
        "",
        "def export_graph_json(",
        "    filepath: str,",
        "    layers: Dict[CorticalLayer, HierarchicalLayer],",
        "    layer_filter: Optional[CorticalLayer] = None,",
        "    min_weight: float = 0.0,"
      ],
      "context_after": [
        ") -> Dict:",
        "    \"\"\"",
        "    Export graph structure as JSON for visualization.",
        "    Creates a format compatible with D3.js, vis.js, etc.",
        "    Args:",
        "        filepath: Output file path",
        "        layers: Dictionary of layers",
        "        layer_filter: Only export specific layer (None = all)",
        "        min_weight: Minimum edge weight to include",
        "        max_nodes: Maximum nodes to export",
        "    Returns:",
        "        The exported graph data",
        "    \"\"\"",
        "    nodes = []",
        "    edges = []",
        "    node_ids = set()",
        "    ",
        "    # Determine which layers to export",
        "    if layer_filter is not None:",
        "        layer_list = [layers.get(layer_filter)]"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/persistence.py",
      "function": "def export_graph_json(",
      "start_line": 164,
      "lines_added": [
        "",
        "    if verbose:",
        "        print(f\"Graph exported to {filepath}\")",
        "        print(f\"  - {len(nodes)} nodes, {len(edges)} edges\")",
        ""
      ],
      "lines_removed": [
        "    ",
        "    print(f\"Graph exported to {filepath}\")",
        "    print(f\"  - {len(nodes)} nodes, {len(edges)} edges\")",
        "    "
      ],
      "context_before": [
        "        'edges': edges,",
        "        'metadata': {",
        "            'node_count': len(nodes),",
        "            'edge_count': len(edges),",
        "            'layers': [l.value for l in layers.keys() if l is not None]",
        "        }",
        "    }",
        "    ",
        "    with open(filepath, 'w') as f:",
        "        json.dump(graph, f, indent=2)"
      ],
      "context_after": [
        "    return graph",
        "",
        "",
        "def export_embeddings_json(",
        "    filepath: str,",
        "    embeddings: Dict[str, list],",
        "    metadata: Optional[Dict] = None",
        ") -> None:",
        "    \"\"\"",
        "    Export embeddings as JSON."
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/processor.py",
      "function": "class CorticalTextProcessor:",
      "start_line": 45,
      "lines_added": [
        "            # Track per-document occurrence count for accurate TF-IDF",
        "            col.doc_occurrence_counts[doc_id] = col.doc_occurrence_counts.get(doc_id, 0) + 1"
      ],
      "lines_removed": [],
      "context_before": [
        "        ",
        "        doc_col = layer3.get_or_create_minicolumn(doc_id)",
        "        doc_col.occurrence_count += 1",
        "        ",
        "        for token in tokens:",
        "            col = layer0.get_or_create_minicolumn(token)",
        "            col.occurrence_count += 1",
        "            col.document_ids.add(doc_id)",
        "            col.activation += 1.0",
        "            doc_col.feedforward_sources.add(col.id)"
      ],
      "context_after": [
        "        ",
        "        for i, token in enumerate(tokens):",
        "            col = layer0.get_minicolumn(token)",
        "            if col:",
        "                for j in range(max(0, i-3), min(len(tokens), i+4)):",
        "                    if i != j:",
        "                        other = layer0.get_minicolumn(tokens[j])",
        "                        if other:",
        "                            col.add_lateral_connection(other.id, 1.0)",
        "        "
      ],
      "change_type": "add"
    },
    {
      "file": "cortical/query.py",
      "function": "def expand_query(",
      "start_line": 80,
      "lines_added": [
        "                    # Use O(1) ID lookup instead of linear search",
        "                    neighbor = layer0.get_by_id(neighbor_id)",
        "                    if neighbor and neighbor.content not in expanded:",
        "                        score = weight * neighbor.pagerank * 0.6",
        "                        candidate_expansions[neighbor.content] = max(",
        "                            candidate_expansions[neighbor.content], score",
        "                        )",
        "                            # Use O(1) ID lookup instead of linear search",
        "                            member = layer0.get_by_id(member_id)",
        "                            if member and member.content not in expanded:",
        "                                score = concept.pagerank * member.pagerank * 0.4",
        "                                candidate_expansions[member.content] = max(",
        "                                    candidate_expansions[member.content], score",
        "                                )"
      ],
      "lines_removed": [
        "                    if neighbor_id in layer0.minicolumns:",
        "                        neighbor = layer0.minicolumns[neighbor_id]",
        "                        if neighbor.content not in expanded:",
        "                            score = weight * neighbor.pagerank * 0.6",
        "                            candidate_expansions[neighbor.content] = max(",
        "                                candidate_expansions[neighbor.content], score",
        "                            )",
        "                    else:",
        "                        # Look up by ID",
        "                        for c in layer0.minicolumns.values():",
        "                            if c.id == neighbor_id and c.content not in expanded:",
        "                                score = weight * c.pagerank * 0.6",
        "                                candidate_expansions[c.content] = max(",
        "                                    candidate_expansions[c.content], score",
        "                                )",
        "                                break",
        "                            if member_id in layer0.minicolumns:",
        "                                member = layer0.minicolumns[member_id]",
        "                                if member.content not in expanded:",
        "                                    score = concept.pagerank * member.pagerank * 0.4",
        "                                    candidate_expansions[member.content] = max(",
        "                                        candidate_expansions[member.content], score",
        "                                    )"
      ],
      "context_before": [
        "        for token in list(expanded.keys()):",
        "            col = layer0.get_minicolumn(token)",
        "            if col:",
        "                sorted_neighbors = sorted(",
        "                    col.lateral_connections.items(),",
        "                    key=lambda x: x[1],",
        "                    reverse=True",
        "                )[:5]",
        "                ",
        "                for neighbor_id, weight in sorted_neighbors:"
      ],
      "context_after": [
        "    ",
        "    # Method 2: Concept cluster membership",
        "    if use_concepts and layer2 and layer2.column_count() > 0:",
        "        for token in list(expanded.keys()):",
        "            col = layer0.get_minicolumn(token)",
        "            if col:",
        "                for concept in layer2.minicolumns.values():",
        "                    if col.id in concept.feedforward_sources:",
        "                        for member_id in concept.feedforward_sources:",
        "    ",
        "    # Select top expansions",
        "    sorted_candidates = sorted(",
        "        candidate_expansions.items(),",
        "        key=lambda x: x[1],",
        "        reverse=True",
        "    )[:max_expansions]",
        "    ",
        "    for term, score in sorted_candidates:",
        "        expanded[term] = score"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/query.py",
      "function": "def query_with_spreading_activation(",
      "start_line": 260,
      "lines_added": [
        "            # Spread to neighbors using O(1) ID lookup",
        "                neighbor = layer0.get_by_id(neighbor_id)",
        "                if neighbor:"
      ],
      "lines_removed": [
        "            # Spread to neighbors",
        "                if neighbor_id in layer0.minicolumns:",
        "                    neighbor = layer0.minicolumns[neighbor_id]",
        "                else:",
        "                    for c in layer0.minicolumns.values():",
        "                        if c.id == neighbor_id:",
        "                            spread_score = c.pagerank * conn_weight * term_weight * 0.3",
        "                            activated[c.content] = activated.get(c.content, 0) + spread_score",
        "                            break"
      ],
      "context_before": [
        "    activated: Dict[str, float] = {}",
        "    ",
        "    # Activate based on expanded query",
        "    for term, term_weight in expanded_terms.items():",
        "        col = layer0.get_minicolumn(term)",
        "        if col:",
        "            # Direct activation",
        "            score = col.pagerank * col.activation * term_weight",
        "            activated[col.content] = activated.get(col.content, 0) + score",
        "            "
      ],
      "context_after": [
        "            for neighbor_id, conn_weight in col.lateral_connections.items():",
        "                    spread_score = neighbor.pagerank * conn_weight * term_weight * 0.3",
        "                    activated[neighbor.content] = activated.get(neighbor.content, 0) + spread_score",
        "    ",
        "    sorted_concepts = sorted(activated.items(), key=lambda x: -x[1])",
        "    return sorted_concepts[:top_n]",
        "",
        "",
        "def find_related_documents(",
        "    doc_id: str,",
        "    layers: Dict[CorticalLayer, HierarchicalLayer]",
        ") -> List[Tuple[str, float]]:",
        "    \"\"\""
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/query.py",
      "function": "def find_related_documents(",
      "start_line": 301,
      "lines_added": [
        "        # Use O(1) ID lookup instead of linear search",
        "        neighbor = layer3.get_by_id(neighbor_id)",
        "        if neighbor:",
        ""
      ],
      "lines_removed": [
        "        if neighbor_id in layer3.minicolumns:",
        "            neighbor = layer3.minicolumns[neighbor_id]",
        "        else:",
        "            for c in layer3.minicolumns.values():",
        "                if c.id == neighbor_id:",
        "                    related.append((c.content, weight))",
        "                    break",
        "    "
      ],
      "context_before": [
        "    layer3 = layers.get(CorticalLayer.DOCUMENTS)",
        "    if not layer3:",
        "        return []",
        "    ",
        "    col = layer3.get_minicolumn(doc_id)",
        "    if not col:",
        "        return []",
        "    ",
        "    related = []",
        "    for neighbor_id, weight in col.lateral_connections.items():"
      ],
      "context_after": [
        "            related.append((neighbor.content, weight))",
        "    return sorted(related, key=lambda x: -x[1])"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/semantics.py",
      "function": "Semantics Module",
      "start_line": 4,
      "lines_added": [
        "from typing import Any, Dict, List, Tuple, Set, Optional"
      ],
      "lines_removed": [
        "from typing import Dict, List, Tuple, Set, Optional"
      ],
      "context_before": [
        "",
        "Corpus-derived semantic relations and retrofitting.",
        "",
        "Extracts semantic relationships from co-occurrence patterns,",
        "then uses them to adjust connection weights (retrofitting).",
        "This is like building a \"poor man's ConceptNet\" from the corpus itself.",
        "\"\"\"",
        "",
        "import math",
        "import re"
      ],
      "context_after": [
        "from collections import defaultdict",
        "",
        "from .layers import CorticalLayer, HierarchicalLayer",
        "from .minicolumn import Minicolumn",
        "",
        "",
        "# Relation type weights for retrofitting",
        "RELATION_WEIGHTS = {",
        "    'IsA': 1.5,",
        "    'PartOf': 1.2,"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/semantics.py",
      "function": "def extract_corpus_semantics(",
      "start_line": 143,
      "lines_added": [
        ") -> Dict[str, Any]:"
      ],
      "lines_removed": [
        ") -> Dict[str, any]:"
      ],
      "context_before": [
        "                    relations.append((t1, 'IsA', t2, 1.0))",
        "    ",
        "    return relations",
        "",
        "",
        "def retrofit_connections(",
        "    layers: Dict[CorticalLayer, HierarchicalLayer],",
        "    semantic_relations: List[Tuple[str, str, str, float]],",
        "    iterations: int = 10,",
        "    alpha: float = 0.3"
      ],
      "context_after": [
        "    \"\"\"",
        "    Retrofit lateral connections using semantic relations.",
        "    ",
        "    Adjusts connection weights by blending co-occurrence patterns",
        "    with semantic relations. This is inspired by Faruqui et al.'s",
        "    retrofitting algorithm for word vectors.",
        "    ",
        "    Args:",
        "        layers: Dictionary of layers",
        "        semantic_relations: List of (term1, relation, term2, weight) tuples"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/semantics.py",
      "function": "def retrofit_connections(",
      "start_line": 238,
      "lines_added": [
        ") -> Dict[str, Any]:"
      ],
      "lines_removed": [
        ") -> Dict[str, any]:"
      ],
      "context_before": [
        "        'total_adjustment': total_adjustment,",
        "        'relations_used': len(semantic_relations)",
        "    }",
        "",
        "",
        "def retrofit_embeddings(",
        "    embeddings: Dict[str, List[float]],",
        "    semantic_relations: List[Tuple[str, str, str, float]],",
        "    iterations: int = 10,",
        "    alpha: float = 0.4"
      ],
      "context_after": [
        "    \"\"\"",
        "    Retrofit embeddings using semantic relations.",
        "    ",
        "    Like Faruqui et al.'s retrofitting, but for graph embeddings.",
        "    Pulls semantically related terms closer in embedding space.",
        "    ",
        "    Args:",
        "        embeddings: Dictionary mapping terms to embedding vectors",
        "        semantic_relations: List of (term1, relation, term2, weight) tuples",
        "        iterations: Number of iterations"
      ],
      "change_type": "modify"
    },
    {
      "file": "samples/candlestick_patterns.txt",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "Japanese Candlestick Pattern Analysis",
        "",
        "Candlestick charts originated in Japanese rice trading during the 18th century. Each candlestick displays four price points: open, high, low, and close. The body represents the range between open and close, while wicks or shadows extend to session extremes. Hollow or green bodies indicate closes above opens; filled or red bodies show closes below opens.",
        "",
        "Single candle patterns signal potential reversals. Doji candles with equal opens and closes indicate indecision. Hammer patterns at downtrend bottoms show rejection of lower prices with small bodies and long lower shadows. Shooting stars at uptrend tops mirror hammers inverted, signaling selling pressure. Marubozu candles with no shadows demonstrate strong conviction.",
        "",
        "Two-candle patterns provide stronger signals. Bullish engulfing occurs when a large green candle completely engulfs the prior red candle's body at support levels. Bearish engulfing reverses this at resistance. Piercing patterns show strong buying when a green candle opens below and closes above the prior red candle's midpoint. Dark cloud cover represents the bearish equivalent.",
        "",
        "Three-candle formations offer high-probability setups. Morning star bottoming patterns begin with a red candle, followed by a small-bodied star candle, then a strong green candle closing into the first candle's body. Evening stars invert this sequence at tops. Three white soldiers show consecutive green candles with higher closes signaling bullish momentum. Three black crows indicate the opposite bearish pressure.",
        "",
        "Context enhances pattern reliability. Volume confirmation validates breakouts and reversals. Support and resistance levels provide structural context for pattern interpretation. Trend direction determines whether patterns signal continuation or reversal. Multiple timeframe analysis confirms setups across different chart periods. Risk management through stop-loss placement beneath pattern lows protects against failed signals."
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "samples/compilers.txt",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "Compiler Design and Implementation",
        "",
        "Compilers transform source code into executable machine instructions through multiple processing phases. Lexical analysis tokenizes input streams, recognizing keywords, identifiers, literals, and operators using finite automata derived from regular expressions. Scanner generators like lex automate lexer construction from token specifications.",
        "",
        "Parsing constructs abstract syntax trees from token sequences according to context-free grammars. Top-down recursive descent parsers implement grammar productions as mutually recursive procedures. Bottom-up parsers like LR and LALR use shift-reduce actions guided by parsing tables. Parser generators including yacc and ANTLR produce parsers from grammar specifications.",
        "",
        "Semantic analysis decorates syntax trees with type information and enforces language rules. Symbol tables track identifier declarations and scopes. Type checking verifies operator-operand compatibility and enforces type system constraints. Type inference algorithms deduce types without explicit annotations through unification and constraint solving.",
        "",
        "Intermediate representations decouple front-end language processing from back-end code generation. Three-address code linearizes expressions into simple operations. Static single assignment form simplifies dataflow analysis by ensuring each variable receives exactly one definition. Control flow graphs represent program structure for optimization.",
        "",
        "Optimization transforms improve code efficiency while preserving semantics. Local optimizations like constant folding and strength reduction operate within basic blocks. Global optimizations including common subexpression elimination and dead code elimination require dataflow analysis. Loop optimizations—invariant code motion, induction variable elimination, and unrolling—target performance-critical regions. Register allocation maps virtual registers to limited physical registers using graph coloring algorithms.",
        "",
        "Code generation translates intermediate representations to target architecture instructions. Instruction selection matches IR patterns to machine operations. Instruction scheduling reorders operations to exploit pipeline parallelism and avoid hazards."
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "samples/computational_theory.txt",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "Computational Theory and Complexity",
        "",
        "Computational theory establishes fundamental limits of computing through mathematical formalism. Turing machines provide the foundational model—an infinite tape, read-write head, and finite state control. Church-Turing thesis posits that any effectively calculable function is Turing computable. Universal Turing machines simulate arbitrary Turing machines, establishing programmable computation.",
        "",
        "Decidability classifies problems by algorithmic solvability. The halting problem proves undecidable—no algorithm determines whether arbitrary programs terminate. Rice's theorem extends undecidability to all non-trivial semantic properties of programs. Reductions demonstrate undecidability by transforming known undecidable problems into new ones.",
        "",
        "Complexity theory measures computational resource requirements. Time complexity counts primitive operations as functions of input size. Space complexity measures memory usage. Asymptotic analysis using big-O notation captures growth rates, abstracting constant factors and lower-order terms.",
        "",
        "Complexity classes categorize problems by resource bounds. P contains problems solvable in polynomial time. NP encompasses problems with polynomial-time verifiable solutions. The P versus NP question—whether these classes differ—remains open. NP-complete problems, including SAT, traveling salesman, and graph coloring, are hardest within NP; solving any in polynomial time would prove P equals NP.",
        "",
        "Beyond NP, PSPACE includes problems solvable with polynomial space. EXPTIME requires exponential time. The polynomial hierarchy stratifies problems between P and PSPACE. Randomized complexity classes like BPP capture probabilistic algorithms. Circuit complexity measures non-uniform computation. Quantum complexity explores computational power of quantum mechanical systems, with BQP capturing efficient quantum computation."
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "samples/data_structures.txt",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "Fundamental Data Structures in Computer Science",
        "",
        "Data structures organize and store information for efficient access and modification. Arrays provide contiguous memory allocation enabling O(1) random access through index arithmetic. Dynamic arrays resize automatically, amortizing insertion costs across operations. Linked lists trade random access for O(1) insertions and deletions through pointer manipulation.",
        "",
        "Trees introduce hierarchical organization. Binary search trees maintain sorted order with O(log n) average operations. Self-balancing variants like AVL trees and red-black trees guarantee logarithmic worst-case complexity through rotation operations. B-trees optimize for disk access patterns, supporting databases with high branching factors minimizing tree height.",
        "",
        "Hash tables achieve O(1) average-case lookup, insertion, and deletion through hash functions mapping keys to array indices. Collision resolution strategies include chaining with linked lists and open addressing with linear or quadratic probing. Load factor monitoring triggers resizing to maintain performance guarantees.",
        "",
        "Heaps implement priority queues with O(log n) insertion and extraction. Binary heaps use array representation with parent-child relationships defined by index arithmetic. Fibonacci heaps achieve O(1) amortized insertion supporting efficient decrease-key operations for graph algorithms.",
        "",
        "Graphs represent relationships between entities through vertices and edges. Adjacency matrices enable O(1) edge queries at O(V^2) space cost. Adjacency lists reduce space complexity to O(V+E) for sparse graphs. Graph traversals include breadth-first search using queues and depth-first search using recursion or explicit stacks. Specialized structures like tries optimize string operations, while bloom filters provide probabilistic set membership testing with space efficiency."
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "samples/elliot_wave_theory.txt",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "Elliott Wave Theory in Market Analysis",
        "",
        "Elliott Wave Theory posits that financial markets move in predictable wave patterns reflecting collective investor psychology. Ralph Nelson Elliott discovered that market prices unfold in specific structures—five waves in the direction of the main trend followed by three corrective waves. This 5-3 pattern repeats at multiple time scales creating fractal market structure.",
        "",
        "Impulse waves contain five sub-waves labeled 1-2-3-4-5. Wave 1 initiates the trend, wave 2 corrects but never retraces beyond wave 1's origin. Wave 3 typically extends furthest and cannot be the shortest impulse wave. Wave 4 corrects wave 3 without overlapping wave 1 territory. Wave 5 completes the impulse sequence with diminishing momentum.",
        "",
        "Corrective waves follow impulse completions in A-B-C patterns. Zigzags form sharp corrections with wave C exceeding wave A. Flats show sideways consolidation where waves A, B, and C terminate near similar price levels. Triangles contract through five waves before breakout continuation. Complex corrections combine multiple simple patterns through connecting X waves.",
        "",
        "Fibonacci relationships pervade Elliott patterns. Retracements commonly reach 38.2%, 50%, or 61.8% of prior waves. Extensions project wave lengths using 1.618 and 2.618 multipliers. Wave equality and alternation guidelines help forecast likely wave termination zones. Time relationships between waves also exhibit Fibonacci proportions.",
        "",
        "Wave counting requires identifying the current position within the fractal hierarchy. Higher degree waves contain lower degree sub-waves—supercycles decompose to cycles, cycles to primary waves, continuing through intermediate, minor, minute, and minuette degrees. Proper wave labeling enables forecasting subsequent movements and identifying high-probability trade setups."
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "samples/financial_analysis.txt",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "Financial Analysis Fundamentals",
        "",
        "Financial analysis evaluates business performance through examination of financial statements, ratios, and market data. Fundamental analysis assesses intrinsic value by analyzing revenue growth, profit margins, debt levels, and cash flow generation. Analysts construct discounted cash flow models projecting future earnings and discounting them to present value using weighted average cost of capital.",
        "",
        "Ratio analysis reveals operational efficiency and financial health. Liquidity ratios like current ratio and quick ratio measure short-term solvency. Profitability ratios including return on equity, return on assets, and net profit margin indicate management effectiveness. Leverage ratios such as debt-to-equity and interest coverage assess capital structure risk.",
        "",
        "Income statement analysis tracks revenue recognition, cost of goods sold, operating expenses, and earnings per share trends. Balance sheet examination evaluates asset quality, working capital management, and liability structure. Cash flow statement analysis distinguishes operating, investing, and financing activities, revealing actual cash generation versus accrual accounting earnings.",
        "",
        "Comparative analysis benchmarks performance against industry peers and historical trends. Common-size statements normalize figures as percentages for cross-company comparison. Trend analysis identifies growth patterns and cyclical behavior. DuPont analysis decomposes return on equity into profit margin, asset turnover, and financial leverage components.",
        "",
        "Valuation metrics guide investment decisions. Price-to-earnings ratios compare market price to profitability. Enterprise value to EBITDA accounts for capital structure differences. Price-to-book ratios assess market premium over accounting value. Dividend yield and payout ratios evaluate income characteristics. Terminal value calculations in DCF models significantly impact fair value estimates."
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "samples/neocortex.txt",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "The Neocortex: Architecture of Intelligence",
        "",
        "The neocortex comprises approximately 80% of human brain volume and underlies higher cognitive functions including perception, reasoning, language, and planning. This thin layered sheet, just 2-4 millimeters thick, folds extensively to maximize surface area within the skull. Humans possess roughly 16 billion neocortical neurons organized into stereotyped circuits repeated across the entire structure.",
        "",
        "Six distinct layers characterize neocortical organization. Layer 4 receives thalamic input carrying sensory information. Layers 2 and 3 process and integrate information through extensive lateral connections. Layer 5 pyramidal neurons project to subcortical structures and motor systems. Layer 6 provides feedback to the thalamus, modulating incoming signals. This layered architecture suggests a canonical microcircuit implementing universal cortical algorithms.",
        "",
        "Cortical columns span all layers vertically, functioning as fundamental processing units. Minicolumns containing roughly 100 neurons form the finest columnar structure. Macrocolumns aggregate hundreds of minicolumns sharing similar response properties. This columnar organization appears throughout sensory, motor, and association cortex, suggesting common computational principles across diverse functions.",
        "",
        "Hierarchical processing flows through cortical regions. Primary sensory areas extract basic features—edges in visual cortex, frequencies in auditory cortex. Successive regions construct increasingly abstract representations. Temporal cortex recognizes objects and faces. Prefrontal cortex maintains working memory and executes cognitive control. Feedforward connections carry information up the hierarchy while feedback connections provide contextual modulation.",
        "",
        "Synaptic plasticity enables learning through experience-dependent modification of connection strengths. Long-term potentiation strengthens frequently co-activated synapses. Spike-timing-dependent plasticity refines temporal relationships. Sleep consolidates memories by replaying neural patterns. Understanding neocortical computation could illuminate principles for artificial intelligence systems exhibiting human-level cognition."
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "tests/test_analysis.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "\"\"\"Tests for the analysis module.\"\"\"",
        "",
        "import unittest",
        "import math",
        "import sys",
        "sys.path.insert(0, '..')",
        "",
        "from cortical import CorticalTextProcessor, CorticalLayer, HierarchicalLayer",
        "from cortical.analysis import (",
        "    compute_pagerank,",
        "    compute_tfidf,",
        "    propagate_activation,",
        "    cluster_by_label_propagation,",
        "    build_concept_clusters,",
        "    compute_document_connections,",
        "    cosine_similarity",
        ")",
        "",
        "",
        "class TestPageRank(unittest.TestCase):",
        "    \"\"\"Test PageRank computation.\"\"\"",
        "",
        "    def test_pagerank_empty_layer(self):",
        "        \"\"\"Test PageRank on empty layer.\"\"\"",
        "        layer = HierarchicalLayer(CorticalLayer.TOKENS)",
        "        result = compute_pagerank(layer)",
        "        self.assertEqual(result, {})",
        "",
        "    def test_pagerank_single_node(self):",
        "        \"\"\"Test PageRank with single node.\"\"\"",
        "        layer = HierarchicalLayer(CorticalLayer.TOKENS)",
        "        layer.get_or_create_minicolumn(\"test\")",
        "        result = compute_pagerank(layer)",
        "        self.assertEqual(len(result), 1)",
        "        # With damping 0.85, single node gets (1-0.85)/1 = 0.15",
        "        self.assertAlmostEqual(list(result.values())[0], 0.15, places=5)",
        "",
        "    def test_pagerank_multiple_nodes(self):",
        "        \"\"\"Test PageRank with multiple connected nodes.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(\"doc1\", \"neural networks learning deep\")",
        "        processor.process_document(\"doc2\", \"neural learning patterns data\")",
        "",
        "        layer0 = processor.get_layer(CorticalLayer.TOKENS)",
        "        result = compute_pagerank(layer0)",
        "",
        "        # All nodes should have positive PageRank",
        "        for col in layer0.minicolumns.values():",
        "            self.assertGreater(col.pagerank, 0)",
        "",
        "    def test_pagerank_convergence(self):",
        "        \"\"\"Test that PageRank converges.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(\"doc1\", \"word1 word2 word3 word4\")",
        "",
        "        layer0 = processor.get_layer(CorticalLayer.TOKENS)",
        "        result = compute_pagerank(layer0, iterations=100)",
        "",
        "        # Sum should be approximately 1.0",
        "        total = sum(result.values())",
        "        self.assertAlmostEqual(total, 1.0, places=3)",
        "",
        "",
        "class TestTFIDF(unittest.TestCase):",
        "    \"\"\"Test TF-IDF computation.\"\"\"",
        "",
        "    def test_tfidf_empty_corpus(self):",
        "        \"\"\"Test TF-IDF on empty corpus.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        compute_tfidf(processor.layers, processor.documents)",
        "        # Should not raise",
        "",
        "    def test_tfidf_single_document(self):",
        "        \"\"\"Test TF-IDF with single document.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(\"doc1\", \"word1 word2 word3\")",
        "        compute_tfidf(processor.layers, processor.documents)",
        "",
        "        layer0 = processor.get_layer(CorticalLayer.TOKENS)",
        "        # With single doc, IDF = log(1/1) = 0, so TF-IDF = 0",
        "        for col in layer0.minicolumns.values():",
        "            self.assertEqual(col.tfidf, 0.0)",
        "",
        "    def test_tfidf_multiple_documents(self):",
        "        \"\"\"Test TF-IDF with multiple documents.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(\"doc1\", \"neural networks learning\")",
        "        processor.process_document(\"doc2\", \"machine learning algorithms\")",
        "        processor.process_document(\"doc3\", \"database systems storage\")",
        "        compute_tfidf(processor.layers, processor.documents)",
        "",
        "        layer0 = processor.get_layer(CorticalLayer.TOKENS)",
        "",
        "        # Terms unique to one doc should have higher TF-IDF",
        "        unique_term = layer0.get_minicolumn(\"database\")",
        "        common_term = layer0.get_minicolumn(\"learning\")",
        "",
        "        if unique_term and common_term:",
        "            # database appears in 1 doc, learning in 2",
        "            self.assertGreater(unique_term.tfidf, 0)",
        "",
        "    def test_tfidf_per_document(self):",
        "        \"\"\"Test per-document TF-IDF.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(\"doc1\", \"neural neural neural\")  # 3 occurrences",
        "        processor.process_document(\"doc2\", \"neural learning\")  # 1 occurrence",
        "        processor.process_document(\"doc3\", \"different content here\")  # No neural - needed for IDF > 0",
        "        compute_tfidf(processor.layers, processor.documents)",
        "",
        "        layer0 = processor.get_layer(CorticalLayer.TOKENS)",
        "        neural = layer0.get_minicolumn(\"neural\")",
        "",
        "        # Check per-document TF-IDF uses actual occurrence counts",
        "        self.assertIn(\"doc1\", neural.tfidf_per_doc)",
        "        self.assertIn(\"doc2\", neural.tfidf_per_doc)",
        "        # doc1 has 3 occurrences, doc2 has 1",
        "        # log1p(3) > log1p(1), so doc1 should have higher per-doc TF-IDF",
        "        self.assertGreater(neural.tfidf_per_doc[\"doc1\"], neural.tfidf_per_doc[\"doc2\"])",
        "",
        "",
        "class TestActivationPropagation(unittest.TestCase):",
        "    \"\"\"Test activation propagation.\"\"\"",
        "",
        "    def test_propagation_empty_layers(self):",
        "        \"\"\"Test propagation on empty layers.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        propagate_activation(processor.layers)",
        "        # Should not raise",
        "",
        "    def test_propagation_preserves_activation(self):",
        "        \"\"\"Test that propagation modifies activations.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(\"doc1\", \"neural networks learning\")",
        "",
        "        layer0 = processor.get_layer(CorticalLayer.TOKENS)",
        "        initial_activations = {col.content: col.activation for col in layer0}",
        "",
        "        propagate_activation(processor.layers, iterations=3)",
        "",
        "        # Activations should have changed",
        "        for col in layer0.minicolumns.values():",
        "            # With decay, activation should decrease or stay same",
        "            self.assertGreaterEqual(col.activation, 0)",
        "",
        "",
        "class TestLabelPropagation(unittest.TestCase):",
        "    \"\"\"Test label propagation clustering.\"\"\"",
        "",
        "    def test_clustering_empty_layer(self):",
        "        \"\"\"Test clustering on empty layer.\"\"\"",
        "        layer = HierarchicalLayer(CorticalLayer.TOKENS)",
        "        clusters = cluster_by_label_propagation(layer)",
        "        self.assertEqual(clusters, {})",
        "",
        "    def test_clustering_returns_dict(self):",
        "        \"\"\"Test that clustering returns dictionary.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(\"doc1\", \"neural networks learning deep patterns\")",
        "        processor.process_document(\"doc2\", \"neural learning patterns data\")",
        "",
        "        layer0 = processor.get_layer(CorticalLayer.TOKENS)",
        "        clusters = cluster_by_label_propagation(layer0, min_cluster_size=2)",
        "",
        "        self.assertIsInstance(clusters, dict)",
        "",
        "    def test_clustering_min_size(self):",
        "        \"\"\"Test that clusters respect minimum size.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(\"doc1\", \"neural networks learning deep patterns\")",
        "        processor.process_document(\"doc2\", \"neural learning patterns data\")",
        "",
        "        layer0 = processor.get_layer(CorticalLayer.TOKENS)",
        "        clusters = cluster_by_label_propagation(layer0, min_cluster_size=3)",
        "",
        "        for members in clusters.values():",
        "            self.assertGreaterEqual(len(members), 3)",
        "",
        "",
        "class TestConceptClusters(unittest.TestCase):",
        "    \"\"\"Test concept cluster building.\"\"\"",
        "",
        "    def test_build_concept_clusters(self):",
        "        \"\"\"Test building concept layer from clusters.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(\"doc1\", \"neural networks learning deep\")",
        "        processor.process_document(\"doc2\", \"neural learning patterns data\")",
        "        processor.compute_importance(verbose=False)",
        "",
        "        layer0 = processor.get_layer(CorticalLayer.TOKENS)",
        "        clusters = cluster_by_label_propagation(layer0, min_cluster_size=2)",
        "        build_concept_clusters(processor.layers, clusters)",
        "",
        "        layer2 = processor.get_layer(CorticalLayer.CONCEPTS)",
        "        # May or may not have concepts depending on cluster size",
        "        self.assertIsInstance(layer2.minicolumns, dict)",
        "",
        "",
        "class TestDocumentConnections(unittest.TestCase):",
        "    \"\"\"Test document connection computation.\"\"\"",
        "",
        "    def test_document_connections(self):",
        "        \"\"\"Test building document connections.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(\"doc1\", \"neural networks learning deep patterns\")",
        "        processor.process_document(\"doc2\", \"neural learning patterns data\")",
        "        processor.process_document(\"doc3\", \"completely different content here\")",
        "        processor.compute_tfidf(verbose=False)",
        "",
        "        compute_document_connections(processor.layers, processor.documents, min_shared_terms=2)",
        "",
        "        layer3 = processor.get_layer(CorticalLayer.DOCUMENTS)",
        "        doc1 = layer3.get_minicolumn(\"doc1\")",
        "        doc2 = layer3.get_minicolumn(\"doc2\")",
        "",
        "        # doc1 and doc2 share terms, should be connected",
        "        if doc1 and doc2:",
        "            # Check if they have connections",
        "            has_connection = len(doc1.lateral_connections) > 0 or len(doc2.lateral_connections) > 0",
        "            self.assertTrue(has_connection)",
        "",
        "",
        "class TestCosineSimilarity(unittest.TestCase):",
        "    \"\"\"Test cosine similarity function.\"\"\"",
        "",
        "    def test_cosine_identical_vectors(self):",
        "        \"\"\"Test cosine similarity of identical vectors.\"\"\"",
        "        vec = {'a': 1.0, 'b': 2.0, 'c': 3.0}",
        "        sim = cosine_similarity(vec, vec)",
        "        self.assertAlmostEqual(sim, 1.0, places=5)",
        "",
        "    def test_cosine_orthogonal_vectors(self):",
        "        \"\"\"Test cosine similarity of non-overlapping vectors.\"\"\"",
        "        vec1 = {'a': 1.0, 'b': 2.0}",
        "        vec2 = {'c': 3.0, 'd': 4.0}",
        "        sim = cosine_similarity(vec1, vec2)",
        "        self.assertEqual(sim, 0.0)",
        "",
        "    def test_cosine_empty_vectors(self):",
        "        \"\"\"Test cosine similarity with empty vectors.\"\"\"",
        "        sim = cosine_similarity({}, {})",
        "        self.assertEqual(sim, 0.0)",
        "",
        "    def test_cosine_partial_overlap(self):",
        "        \"\"\"Test cosine similarity with partial overlap.\"\"\"",
        "        vec1 = {'a': 1.0, 'b': 2.0, 'c': 3.0}",
        "        vec2 = {'b': 2.0, 'c': 3.0, 'd': 4.0}",
        "        sim = cosine_similarity(vec1, vec2)",
        "        self.assertGreater(sim, 0.0)",
        "        self.assertLess(sim, 1.0)",
        "",
        "    def test_cosine_zero_magnitude(self):",
        "        \"\"\"Test cosine similarity with zero magnitude vector.\"\"\"",
        "        vec1 = {'a': 0.0}",
        "        vec2 = {'a': 1.0}",
        "        sim = cosine_similarity(vec1, vec2)",
        "        self.assertEqual(sim, 0.0)",
        "",
        "",
        "class TestGetByIdOptimization(unittest.TestCase):",
        "    \"\"\"Test that get_by_id optimization works correctly.\"\"\"",
        "",
        "    def test_get_by_id_returns_correct_minicolumn(self):",
        "        \"\"\"Test that get_by_id returns the correct minicolumn.\"\"\"",
        "        layer = HierarchicalLayer(CorticalLayer.TOKENS)",
        "        col1 = layer.get_or_create_minicolumn(\"neural\")",
        "        col2 = layer.get_or_create_minicolumn(\"network\")",
        "",
        "        # Get by ID should return the same minicolumn",
        "        retrieved = layer.get_by_id(col1.id)",
        "        self.assertIs(retrieved, col1)",
        "",
        "        retrieved2 = layer.get_by_id(col2.id)",
        "        self.assertIs(retrieved2, col2)",
        "",
        "    def test_get_by_id_returns_none_for_missing(self):",
        "        \"\"\"Test that get_by_id returns None for missing ID.\"\"\"",
        "        layer = HierarchicalLayer(CorticalLayer.TOKENS)",
        "        layer.get_or_create_minicolumn(\"test\")",
        "",
        "        result = layer.get_by_id(\"nonexistent_id\")",
        "        self.assertIsNone(result)",
        "",
        "",
        "if __name__ == \"__main__\":",
        "    unittest.main(verbosity=2)"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "tests/test_embeddings.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "\"\"\"Tests for the embeddings module.\"\"\"",
        "",
        "import unittest",
        "import sys",
        "sys.path.insert(0, '..')",
        "",
        "from cortical import CorticalTextProcessor, CorticalLayer",
        "from cortical.embeddings import (",
        "    compute_graph_embeddings,",
        "    embedding_similarity,",
        "    find_similar_by_embedding,",
        "    _adjacency_embeddings,",
        "    _random_walk_embeddings,",
        "    _spectral_embeddings",
        ")",
        "",
        "",
        "class TestEmbeddings(unittest.TestCase):",
        "    \"\"\"Test the embeddings module.\"\"\"",
        "",
        "    @classmethod",
        "    def setUpClass(cls):",
        "        \"\"\"Set up processor with sample data.\"\"\"",
        "        cls.processor = CorticalTextProcessor()",
        "        cls.processor.process_document(\"doc1\", \"\"\"",
        "            Neural networks process information through layers.",
        "            Deep learning enables pattern recognition.",
        "        \"\"\")",
        "        cls.processor.process_document(\"doc2\", \"\"\"",
        "            Machine learning algorithms learn from data.",
        "            Training neural networks requires optimization.",
        "        \"\"\")",
        "        cls.processor.process_document(\"doc3\", \"\"\"",
        "            Graph algorithms traverse nodes and edges.",
        "            Network analysis reveals structure.",
        "        \"\"\")",
        "        cls.processor.compute_all(verbose=False)",
        "",
        "    def test_compute_graph_embeddings_adjacency(self):",
        "        \"\"\"Test adjacency-based embeddings.\"\"\"",
        "        embeddings, stats = compute_graph_embeddings(",
        "            self.processor.layers,",
        "            dimensions=16,",
        "            method='adjacency'",
        "        )",
        "        self.assertIsInstance(embeddings, dict)",
        "        self.assertGreater(len(embeddings), 0)",
        "        self.assertEqual(stats['method'], 'adjacency')",
        "        self.assertEqual(stats['dimensions'], 16)",
        "        self.assertEqual(stats['terms_embedded'], len(embeddings))",
        "",
        "    def test_compute_graph_embeddings_random_walk(self):",
        "        \"\"\"Test random walk embeddings.\"\"\"",
        "        embeddings, stats = compute_graph_embeddings(",
        "            self.processor.layers,",
        "            dimensions=16,",
        "            method='random_walk'",
        "        )",
        "        self.assertIsInstance(embeddings, dict)",
        "        self.assertGreater(len(embeddings), 0)",
        "        self.assertEqual(stats['method'], 'random_walk')",
        "",
        "    def test_compute_graph_embeddings_spectral(self):",
        "        \"\"\"Test spectral embeddings.\"\"\"",
        "        embeddings, stats = compute_graph_embeddings(",
        "            self.processor.layers,",
        "            dimensions=16,",
        "            method='spectral'",
        "        )",
        "        self.assertIsInstance(embeddings, dict)",
        "        self.assertGreater(len(embeddings), 0)",
        "        self.assertEqual(stats['method'], 'spectral')",
        "",
        "    def test_compute_graph_embeddings_invalid_method(self):",
        "        \"\"\"Test that invalid method raises error.\"\"\"",
        "        with self.assertRaises(ValueError):",
        "            compute_graph_embeddings(",
        "                self.processor.layers,",
        "                dimensions=16,",
        "                method='invalid'",
        "            )",
        "",
        "    def test_embedding_similarity(self):",
        "        \"\"\"Test cosine similarity between embeddings.\"\"\"",
        "        embeddings, _ = compute_graph_embeddings(",
        "            self.processor.layers,",
        "            dimensions=16,",
        "            method='adjacency'",
        "        )",
        "",
        "        # Find two terms that exist in embeddings",
        "        terms = list(embeddings.keys())",
        "        if len(terms) >= 2:",
        "            sim = embedding_similarity(embeddings, terms[0], terms[1])",
        "            self.assertIsInstance(sim, float)",
        "            self.assertGreaterEqual(sim, -1.0)",
        "            self.assertLessEqual(sim, 1.0)",
        "",
        "    def test_embedding_similarity_self(self):",
        "        \"\"\"Test that a term has similarity 1.0 with itself.\"\"\"",
        "        embeddings, _ = compute_graph_embeddings(",
        "            self.processor.layers,",
        "            dimensions=16,",
        "            method='adjacency'",
        "        )",
        "",
        "        terms = list(embeddings.keys())",
        "        if terms:",
        "            sim = embedding_similarity(embeddings, terms[0], terms[0])",
        "            self.assertAlmostEqual(sim, 1.0, places=5)",
        "",
        "    def test_embedding_similarity_missing_term(self):",
        "        \"\"\"Test similarity with missing term returns 0.\"\"\"",
        "        embeddings, _ = compute_graph_embeddings(",
        "            self.processor.layers,",
        "            dimensions=16,",
        "            method='adjacency'",
        "        )",
        "",
        "        sim = embedding_similarity(embeddings, \"nonexistent_term\", \"another_missing\")",
        "        self.assertEqual(sim, 0.0)",
        "",
        "    def test_find_similar_by_embedding(self):",
        "        \"\"\"Test finding similar terms by embedding.\"\"\"",
        "        embeddings, _ = compute_graph_embeddings(",
        "            self.processor.layers,",
        "            dimensions=16,",
        "            method='adjacency'",
        "        )",
        "",
        "        terms = list(embeddings.keys())",
        "        if terms:",
        "            similar = find_similar_by_embedding(embeddings, terms[0], top_n=5)",
        "            self.assertIsInstance(similar, list)",
        "            self.assertLessEqual(len(similar), 5)",
        "",
        "            # Check format of results",
        "            for term, score in similar:",
        "                self.assertIsInstance(term, str)",
        "                self.assertIsInstance(score, float)",
        "",
        "    def test_find_similar_by_embedding_missing_term(self):",
        "        \"\"\"Test finding similar for missing term returns empty list.\"\"\"",
        "        embeddings, _ = compute_graph_embeddings(",
        "            self.processor.layers,",
        "            dimensions=16,",
        "            method='adjacency'",
        "        )",
        "",
        "        similar = find_similar_by_embedding(embeddings, \"nonexistent_term\", top_n=5)",
        "        self.assertEqual(similar, [])",
        "",
        "    def test_embedding_dimensions(self):",
        "        \"\"\"Test that embeddings have correct dimensions.\"\"\"",
        "        dimensions = 16",
        "        embeddings, stats = compute_graph_embeddings(",
        "            self.processor.layers,",
        "            dimensions=dimensions,",
        "            method='adjacency'",
        "        )",
        "",
        "        # Dimensions are min(requested, num_terms)",
        "        expected_dims = min(dimensions, stats['terms_embedded'])",
        "        for term, vec in embeddings.items():",
        "            self.assertEqual(len(vec), expected_dims)",
        "",
        "    def test_embedding_normalization(self):",
        "        \"\"\"Test that adjacency embeddings are normalized.\"\"\"",
        "        import math",
        "",
        "        embeddings, _ = compute_graph_embeddings(",
        "            self.processor.layers,",
        "            dimensions=16,",
        "            method='adjacency'",
        "        )",
        "",
        "        for term, vec in embeddings.items():",
        "            magnitude = math.sqrt(sum(v * v for v in vec))",
        "            # Should be approximately 1.0 (normalized)",
        "            self.assertAlmostEqual(magnitude, 1.0, places=5)",
        "",
        "",
        "class TestEmbeddingsEmptyLayer(unittest.TestCase):",
        "    \"\"\"Test embeddings with empty layer.\"\"\"",
        "",
        "    def test_empty_layer_embeddings(self):",
        "        \"\"\"Test embeddings on empty processor.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        embeddings, stats = compute_graph_embeddings(",
        "            processor.layers,",
        "            dimensions=16,",
        "            method='adjacency'",
        "        )",
        "        self.assertEqual(len(embeddings), 0)",
        "        self.assertEqual(stats['terms_embedded'], 0)",
        "",
        "",
        "if __name__ == \"__main__\":",
        "    unittest.main(verbosity=2)"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "tests/test_gaps.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "\"\"\"Tests for the gaps module.\"\"\"",
        "",
        "import unittest",
        "import sys",
        "sys.path.insert(0, '..')",
        "",
        "from cortical import CorticalTextProcessor, CorticalLayer",
        "from cortical.gaps import analyze_knowledge_gaps, detect_anomalies",
        "",
        "",
        "class TestGaps(unittest.TestCase):",
        "    \"\"\"Test the gaps module.\"\"\"",
        "",
        "    @classmethod",
        "    def setUpClass(cls):",
        "        \"\"\"Set up processor with sample data including an outlier.\"\"\"",
        "        cls.processor = CorticalTextProcessor()",
        "        # Create cluster of related documents",
        "        for i in range(3):",
        "            cls.processor.process_document(f\"tech_{i}\", \"\"\"",
        "                Machine learning neural networks deep learning.",
        "                Training models data processing algorithms.",
        "                Pattern recognition artificial intelligence.",
        "            \"\"\")",
        "        # Add outlier document with different topic",
        "        cls.processor.process_document(\"outlier\", \"\"\"",
        "            Medieval falconry birds hunting prey.",
        "            Falcons hawks eagles training techniques.",
        "            Ancient hunting traditions wildlife.",
        "        \"\"\")",
        "        cls.processor.compute_all(verbose=False)",
        "",
        "    def test_analyze_knowledge_gaps_structure(self):",
        "        \"\"\"Test that gap analysis returns expected structure.\"\"\"",
        "        gaps = analyze_knowledge_gaps(",
        "            self.processor.layers,",
        "            self.processor.documents",
        "        )",
        "",
        "        # Check all expected keys are present",
        "        self.assertIn('isolated_documents', gaps)",
        "        self.assertIn('weak_topics', gaps)",
        "        self.assertIn('bridge_opportunities', gaps)",
        "        self.assertIn('connector_terms', gaps)",
        "        self.assertIn('coverage_score', gaps)",
        "        self.assertIn('connectivity_score', gaps)",
        "        self.assertIn('summary', gaps)",
        "",
        "    def test_analyze_knowledge_gaps_summary(self):",
        "        \"\"\"Test that summary contains expected fields.\"\"\"",
        "        gaps = analyze_knowledge_gaps(",
        "            self.processor.layers,",
        "            self.processor.documents",
        "        )",
        "",
        "        summary = gaps['summary']",
        "        self.assertIn('total_documents', summary)",
        "        self.assertIn('isolated_count', summary)",
        "        self.assertIn('well_connected_count', summary)",
        "        self.assertIn('weak_topic_count', summary)",
        "",
        "        self.assertEqual(summary['total_documents'], 4)",
        "",
        "    def test_analyze_knowledge_gaps_isolated_documents(self):",
        "        \"\"\"Test isolated documents detection.\"\"\"",
        "        gaps = analyze_knowledge_gaps(",
        "            self.processor.layers,",
        "            self.processor.documents",
        "        )",
        "",
        "        isolated = gaps['isolated_documents']",
        "        self.assertIsInstance(isolated, list)",
        "",
        "        # Each isolated doc should have expected fields",
        "        for doc in isolated:",
        "            self.assertIn('doc_id', doc)",
        "            self.assertIn('avg_similarity', doc)",
        "            self.assertIn('max_similarity', doc)",
        "",
        "    def test_analyze_knowledge_gaps_weak_topics(self):",
        "        \"\"\"Test weak topics detection.\"\"\"",
        "        gaps = analyze_knowledge_gaps(",
        "            self.processor.layers,",
        "            self.processor.documents",
        "        )",
        "",
        "        weak_topics = gaps['weak_topics']",
        "        self.assertIsInstance(weak_topics, list)",
        "",
        "        for topic in weak_topics:",
        "            self.assertIn('term', topic)",
        "            self.assertIn('tfidf', topic)",
        "            self.assertIn('doc_count', topic)",
        "            self.assertIn('documents', topic)",
        "",
        "    def test_analyze_knowledge_gaps_coverage_score(self):",
        "        \"\"\"Test coverage score is valid.\"\"\"",
        "        gaps = analyze_knowledge_gaps(",
        "            self.processor.layers,",
        "            self.processor.documents",
        "        )",
        "",
        "        self.assertIsInstance(gaps['coverage_score'], float)",
        "        self.assertGreaterEqual(gaps['coverage_score'], 0.0)",
        "        self.assertLessEqual(gaps['coverage_score'], 1.0)",
        "",
        "    def test_detect_anomalies_structure(self):",
        "        \"\"\"Test anomaly detection returns expected structure.\"\"\"",
        "        anomalies = detect_anomalies(",
        "            self.processor.layers,",
        "            self.processor.documents,",
        "            threshold=0.3",
        "        )",
        "",
        "        self.assertIsInstance(anomalies, list)",
        "",
        "        for anomaly in anomalies:",
        "            self.assertIn('doc_id', anomaly)",
        "            self.assertIn('avg_similarity', anomaly)",
        "            self.assertIn('max_similarity', anomaly)",
        "            self.assertIn('connections', anomaly)",
        "            self.assertIn('reasons', anomaly)",
        "            self.assertIn('distinctive_terms', anomaly)",
        "",
        "    def test_detect_anomalies_reasons(self):",
        "        \"\"\"Test that anomalies have reasons.\"\"\"",
        "        anomalies = detect_anomalies(",
        "            self.processor.layers,",
        "            self.processor.documents,",
        "            threshold=0.3",
        "        )",
        "",
        "        for anomaly in anomalies:",
        "            self.assertIsInstance(anomaly['reasons'], list)",
        "            # Each anomaly should have at least one reason",
        "            self.assertGreater(len(anomaly['reasons']), 0)",
        "",
        "    def test_detect_anomalies_sorted(self):",
        "        \"\"\"Test that anomalies are sorted by similarity (ascending).\"\"\"",
        "        anomalies = detect_anomalies(",
        "            self.processor.layers,",
        "            self.processor.documents,",
        "            threshold=0.5",
        "        )",
        "",
        "        if len(anomalies) > 1:",
        "            similarities = [a['avg_similarity'] for a in anomalies]",
        "            self.assertEqual(similarities, sorted(similarities))",
        "",
        "    def test_detect_anomalies_threshold(self):",
        "        \"\"\"Test that threshold affects anomaly detection.\"\"\"",
        "        anomalies_low = detect_anomalies(",
        "            self.processor.layers,",
        "            self.processor.documents,",
        "            threshold=0.1",
        "        )",
        "",
        "        anomalies_high = detect_anomalies(",
        "            self.processor.layers,",
        "            self.processor.documents,",
        "            threshold=0.5",
        "        )",
        "",
        "        # Higher threshold should find more or equal anomalies",
        "        self.assertGreaterEqual(len(anomalies_high), len(anomalies_low))",
        "",
        "",
        "class TestGapsEmptyCorpus(unittest.TestCase):",
        "    \"\"\"Test gaps module with empty or minimal corpus.\"\"\"",
        "",
        "    def test_empty_corpus_gaps(self):",
        "        \"\"\"Test gap analysis on empty processor.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        gaps = analyze_knowledge_gaps(",
        "            processor.layers,",
        "            processor.documents",
        "        )",
        "",
        "        self.assertEqual(gaps['summary']['total_documents'], 0)",
        "        self.assertEqual(gaps['isolated_documents'], [])",
        "        self.assertEqual(gaps['weak_topics'], [])",
        "",
        "    def test_single_document_gaps(self):",
        "        \"\"\"Test gap analysis with single document.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(\"only_doc\", \"Single document content here.\")",
        "        processor.compute_all(verbose=False)",
        "",
        "        gaps = analyze_knowledge_gaps(",
        "            processor.layers,",
        "            processor.documents",
        "        )",
        "",
        "        self.assertEqual(gaps['summary']['total_documents'], 1)",
        "",
        "    def test_single_document_anomalies(self):",
        "        \"\"\"Test anomaly detection with single document.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(\"only_doc\", \"Single document content here.\")",
        "        processor.compute_all(verbose=False)",
        "",
        "        anomalies = detect_anomalies(",
        "            processor.layers,",
        "            processor.documents,",
        "            threshold=0.3",
        "        )",
        "",
        "        # Single doc can't have similarity to others",
        "        self.assertIsInstance(anomalies, list)",
        "",
        "",
        "class TestGapsBridgeOpportunities(unittest.TestCase):",
        "    \"\"\"Test bridge opportunity detection.\"\"\"",
        "",
        "    def test_bridge_opportunities_format(self):",
        "        \"\"\"Test bridge opportunities have correct format.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(\"doc1\", \"neural networks learning deep\")",
        "        processor.process_document(\"doc2\", \"machine learning algorithms\")",
        "        processor.process_document(\"doc3\", \"database systems storage\")",
        "        processor.compute_all(verbose=False)",
        "",
        "        gaps = analyze_knowledge_gaps(",
        "            processor.layers,",
        "            processor.documents",
        "        )",
        "",
        "        bridges = gaps['bridge_opportunities']",
        "        self.assertIsInstance(bridges, list)",
        "",
        "        for bridge in bridges:",
        "            self.assertIn('doc1', bridge)",
        "            self.assertIn('doc2', bridge)",
        "            self.assertIn('similarity', bridge)",
        "            self.assertIn('shared_terms', bridge)",
        "",
        "",
        "if __name__ == \"__main__\":",
        "    unittest.main(verbosity=2)"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "tests/test_persistence.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "\"\"\"Tests for the persistence module.\"\"\"",
        "",
        "import unittest",
        "import tempfile",
        "import os",
        "import json",
        "import sys",
        "sys.path.insert(0, '..')",
        "",
        "from cortical import CorticalTextProcessor, CorticalLayer",
        "from cortical.persistence import (",
        "    save_processor,",
        "    load_processor,",
        "    export_graph_json,",
        "    export_embeddings_json,",
        "    get_state_summary",
        ")",
        "from cortical.embeddings import compute_graph_embeddings",
        "",
        "",
        "class TestSaveLoad(unittest.TestCase):",
        "    \"\"\"Test save and load functionality.\"\"\"",
        "",
        "    def test_save_and_load(self):",
        "        \"\"\"Test saving and loading processor state.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(\"doc1\", \"Neural networks process information.\")",
        "        processor.process_document(\"doc2\", \"Machine learning algorithms learn.\")",
        "        processor.compute_all(verbose=False)",
        "",
        "        with tempfile.TemporaryDirectory() as tmpdir:",
        "            filepath = os.path.join(tmpdir, \"test.pkl\")",
        "            save_processor(filepath, processor.layers, processor.documents, verbose=False)",
        "",
        "            layers, documents, metadata = load_processor(filepath, verbose=False)",
        "",
        "            self.assertEqual(len(documents), 2)",
        "            self.assertIn(\"doc1\", documents)",
        "            self.assertIn(\"doc2\", documents)",
        "",
        "            # Check layers were restored",
        "            layer0 = layers[CorticalLayer.TOKENS]",
        "            self.assertGreater(len(layer0.minicolumns), 0)",
        "",
        "    def test_save_load_preserves_id_index(self):",
        "        \"\"\"Test that save/load preserves the ID index.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(\"doc1\", \"neural networks deep learning\")",
        "        processor.compute_all(verbose=False)",
        "",
        "        with tempfile.TemporaryDirectory() as tmpdir:",
        "            filepath = os.path.join(tmpdir, \"test.pkl\")",
        "            save_processor(filepath, processor.layers, processor.documents, verbose=False)",
        "",
        "            layers, documents, _ = load_processor(filepath, verbose=False)",
        "",
        "            layer0 = layers[CorticalLayer.TOKENS]",
        "            neural = layer0.get_minicolumn(\"neural\")",
        "",
        "            # get_by_id should work after load",
        "            retrieved = layer0.get_by_id(neural.id)",
        "            self.assertEqual(retrieved.content, \"neural\")",
        "",
        "    def test_save_load_preserves_doc_occurrence_counts(self):",
        "        \"\"\"Test that save/load preserves doc_occurrence_counts.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(\"doc1\", \"neural neural neural\")  # 3 times",
        "        processor.process_document(\"doc2\", \"neural\")  # 1 time",
        "        processor.compute_all(verbose=False)",
        "",
        "        with tempfile.TemporaryDirectory() as tmpdir:",
        "            filepath = os.path.join(tmpdir, \"test.pkl\")",
        "            save_processor(filepath, processor.layers, processor.documents, verbose=False)",
        "",
        "            layers, documents, _ = load_processor(filepath, verbose=False)",
        "",
        "            layer0 = layers[CorticalLayer.TOKENS]",
        "            neural = layer0.get_minicolumn(\"neural\")",
        "",
        "            self.assertEqual(neural.doc_occurrence_counts.get(\"doc1\"), 3)",
        "            self.assertEqual(neural.doc_occurrence_counts.get(\"doc2\"), 1)",
        "",
        "    def test_save_load_empty_processor(self):",
        "        \"\"\"Test saving and loading empty processor.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "",
        "        with tempfile.TemporaryDirectory() as tmpdir:",
        "            filepath = os.path.join(tmpdir, \"test.pkl\")",
        "            save_processor(filepath, processor.layers, processor.documents, verbose=False)",
        "",
        "            layers, documents, metadata = load_processor(filepath, verbose=False)",
        "",
        "            self.assertEqual(len(documents), 0)",
        "",
        "",
        "class TestExportGraphJSON(unittest.TestCase):",
        "    \"\"\"Test graph JSON export.\"\"\"",
        "",
        "    def test_export_graph_json(self):",
        "        \"\"\"Test exporting graph to JSON.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(\"doc1\", \"neural networks learning\")",
        "        processor.process_document(\"doc2\", \"machine learning algorithms\")",
        "        processor.compute_all(verbose=False)",
        "",
        "        with tempfile.TemporaryDirectory() as tmpdir:",
        "            filepath = os.path.join(tmpdir, \"graph.json\")",
        "            result = export_graph_json(filepath, processor.layers, verbose=False)",
        "",
        "            # Check file was created",
        "            self.assertTrue(os.path.exists(filepath))",
        "",
        "            # Check result structure",
        "            self.assertIn('nodes', result)",
        "            self.assertIn('edges', result)",
        "            self.assertIn('metadata', result)",
        "",
        "            # Verify file contents",
        "            with open(filepath) as f:",
        "                data = json.load(f)",
        "            self.assertEqual(data['metadata']['node_count'], len(data['nodes']))",
        "",
        "    def test_export_graph_json_layer_filter(self):",
        "        \"\"\"Test exporting specific layer.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(\"doc1\", \"neural networks learning\")",
        "        processor.compute_all(verbose=False)",
        "",
        "        with tempfile.TemporaryDirectory() as tmpdir:",
        "            filepath = os.path.join(tmpdir, \"graph.json\")",
        "            result = export_graph_json(",
        "                filepath,",
        "                processor.layers,",
        "                layer_filter=CorticalLayer.TOKENS,",
        "                verbose=False",
        "            )",
        "",
        "            # All nodes should be from layer 0",
        "            for node in result['nodes']:",
        "                self.assertEqual(node['layer'], 0)",
        "",
        "    def test_export_graph_json_min_weight(self):",
        "        \"\"\"Test filtering edges by minimum weight.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(\"doc1\", \"neural networks learning deep\")",
        "        processor.compute_all(verbose=False)",
        "",
        "        with tempfile.TemporaryDirectory() as tmpdir:",
        "            filepath = os.path.join(tmpdir, \"graph.json\")",
        "            result = export_graph_json(",
        "                filepath,",
        "                processor.layers,",
        "                min_weight=0.5,",
        "                verbose=False",
        "            )",
        "",
        "            # All edges should have weight >= 0.5",
        "            for edge in result['edges']:",
        "                self.assertGreaterEqual(edge['weight'], 0.5)",
        "",
        "    def test_export_graph_json_max_nodes(self):",
        "        \"\"\"Test limiting number of nodes.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(\"doc1\", \"word1 word2 word3 word4 word5 word6 word7 word8\")",
        "        processor.compute_all(verbose=False)",
        "",
        "        with tempfile.TemporaryDirectory() as tmpdir:",
        "            filepath = os.path.join(tmpdir, \"graph.json\")",
        "            result = export_graph_json(",
        "                filepath,",
        "                processor.layers,",
        "                max_nodes=3,",
        "                verbose=False",
        "            )",
        "",
        "            self.assertLessEqual(len(result['nodes']), 3)",
        "",
        "    def test_export_graph_json_verbose_false(self):",
        "        \"\"\"Test that verbose=False suppresses output.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(\"doc1\", \"test content\")",
        "        processor.compute_all(verbose=False)",
        "",
        "        with tempfile.TemporaryDirectory() as tmpdir:",
        "            filepath = os.path.join(tmpdir, \"graph.json\")",
        "            # This should not print anything",
        "            export_graph_json(filepath, processor.layers, verbose=False)",
        "",
        "",
        "class TestExportEmbeddingsJSON(unittest.TestCase):",
        "    \"\"\"Test embeddings JSON export.\"\"\"",
        "",
        "    def test_export_embeddings_json(self):",
        "        \"\"\"Test exporting embeddings to JSON.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(\"doc1\", \"neural networks learning\")",
        "        processor.compute_all(verbose=False)",
        "",
        "        embeddings, _ = compute_graph_embeddings(",
        "            processor.layers,",
        "            dimensions=16,",
        "            method='adjacency'",
        "        )",
        "",
        "        with tempfile.TemporaryDirectory() as tmpdir:",
        "            filepath = os.path.join(tmpdir, \"embeddings.json\")",
        "            export_embeddings_json(filepath, embeddings)",
        "",
        "            # Check file was created",
        "            self.assertTrue(os.path.exists(filepath))",
        "",
        "            # Check file contents",
        "            with open(filepath) as f:",
        "                data = json.load(f)",
        "            self.assertIn('embeddings', data)",
        "            self.assertIn('metadata', data)",
        "",
        "    def test_export_embeddings_json_with_metadata(self):",
        "        \"\"\"Test exporting embeddings with custom metadata.\"\"\"",
        "        embeddings = {'term1': [1.0, 2.0], 'term2': [3.0, 4.0]}",
        "        metadata = {'custom_key': 'custom_value'}",
        "",
        "        with tempfile.TemporaryDirectory() as tmpdir:",
        "            filepath = os.path.join(tmpdir, \"embeddings.json\")",
        "            export_embeddings_json(filepath, embeddings, metadata)",
        "",
        "            with open(filepath) as f:",
        "                data = json.load(f)",
        "            self.assertIn('custom_key', data['metadata'])",
        "",
        "",
        "class TestGetStateSummary(unittest.TestCase):",
        "    \"\"\"Test state summary functionality.\"\"\"",
        "",
        "    def test_get_state_summary(self):",
        "        \"\"\"Test getting state summary.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(\"doc1\", \"neural networks learning\")",
        "        processor.process_document(\"doc2\", \"machine learning algorithms\")",
        "        processor.compute_all(verbose=False)",
        "",
        "        summary = get_state_summary(processor.layers, processor.documents)",
        "",
        "        # Check expected keys (actual keys from get_state_summary)",
        "        self.assertIn('documents', summary)",
        "        self.assertIn('layers', summary)",
        "        self.assertIn('total_connections', summary)",
        "        self.assertIn('total_columns', summary)",
        "",
        "        self.assertEqual(summary['documents'], 2)",
        "",
        "    def test_get_state_summary_empty(self):",
        "        \"\"\"Test summary for empty processor.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "",
        "        summary = get_state_summary(processor.layers, processor.documents)",
        "",
        "        self.assertEqual(summary['documents'], 0)",
        "",
        "",
        "if __name__ == \"__main__\":",
        "    unittest.main(verbosity=2)"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "tests/test_semantics.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "\"\"\"Tests for the semantics module.\"\"\"",
        "",
        "import unittest",
        "import sys",
        "sys.path.insert(0, '..')",
        "",
        "from cortical import CorticalTextProcessor, CorticalLayer",
        "from cortical.semantics import (",
        "    extract_corpus_semantics,",
        "    retrofit_connections,",
        "    retrofit_embeddings,",
        "    get_relation_type_weight,",
        "    RELATION_WEIGHTS",
        ")",
        "from cortical.embeddings import compute_graph_embeddings",
        "",
        "",
        "class TestSemantics(unittest.TestCase):",
        "    \"\"\"Test the semantics module.\"\"\"",
        "",
        "    @classmethod",
        "    def setUpClass(cls):",
        "        \"\"\"Set up processor with sample data.\"\"\"",
        "        cls.processor = CorticalTextProcessor()",
        "        cls.processor.process_document(\"doc1\", \"\"\"",
        "            Neural networks are a type of machine learning model.",
        "            Deep learning uses neural networks for pattern recognition.",
        "            Neural processing happens in the brain cortex.",
        "        \"\"\")",
        "        cls.processor.process_document(\"doc2\", \"\"\"",
        "            Machine learning algorithms learn from data examples.",
        "            Training models requires optimization techniques.",
        "            Learning neural networks needs backpropagation.",
        "        \"\"\")",
        "        cls.processor.process_document(\"doc3\", \"\"\"",
        "            The brain processes information through neurons.",
        "            Cortical columns are like neural networks.",
        "            Processing patterns requires learning.",
        "        \"\"\")",
        "        cls.processor.compute_all(verbose=False)",
        "",
        "    def test_extract_corpus_semantics(self):",
        "        \"\"\"Test semantic relation extraction.\"\"\"",
        "        relations = extract_corpus_semantics(",
        "            self.processor.layers,",
        "            self.processor.documents,",
        "            self.processor.tokenizer",
        "        )",
        "        self.assertIsInstance(relations, list)",
        "        # Should find some relations",
        "        self.assertGreater(len(relations), 0)",
        "",
        "        # Check relation format",
        "        for relation in relations:",
        "            self.assertEqual(len(relation), 4)",
        "            term1, rel_type, term2, weight = relation",
        "            self.assertIsInstance(term1, str)",
        "            self.assertIsInstance(rel_type, str)",
        "            self.assertIsInstance(term2, str)",
        "            self.assertIsInstance(weight, float)",
        "",
        "    def test_extract_corpus_semantics_cooccurs(self):",
        "        \"\"\"Test that CoOccurs relations are found.\"\"\"",
        "        relations = extract_corpus_semantics(",
        "            self.processor.layers,",
        "            self.processor.documents,",
        "            self.processor.tokenizer",
        "        )",
        "        relation_types = set(r[1] for r in relations)",
        "        self.assertIn('CoOccurs', relation_types)",
        "",
        "    def test_retrofit_connections(self):",
        "        \"\"\"Test retrofitting lateral connections.\"\"\"",
        "        relations = extract_corpus_semantics(",
        "            self.processor.layers,",
        "            self.processor.documents,",
        "            self.processor.tokenizer",
        "        )",
        "",
        "        stats = retrofit_connections(",
        "            self.processor.layers,",
        "            relations,",
        "            iterations=5,",
        "            alpha=0.3",
        "        )",
        "",
        "        self.assertIsInstance(stats, dict)",
        "        self.assertIn('iterations', stats)",
        "        self.assertIn('alpha', stats)",
        "        self.assertIn('tokens_affected', stats)",
        "        self.assertIn('total_adjustment', stats)",
        "        self.assertIn('relations_used', stats)",
        "",
        "        self.assertEqual(stats['iterations'], 5)",
        "        self.assertEqual(stats['alpha'], 0.3)",
        "",
        "    def test_retrofit_connections_affects_weights(self):",
        "        \"\"\"Test that retrofitting changes connection weights.\"\"\"",
        "        # Create fresh processor",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(\"doc1\", \"neural networks learning deep\")",
        "        processor.process_document(\"doc2\", \"neural learning patterns data\")",
        "        processor.compute_all(verbose=False)",
        "",
        "        relations = extract_corpus_semantics(",
        "            processor.layers,",
        "            processor.documents,",
        "            processor.tokenizer",
        "        )",
        "",
        "        stats = retrofit_connections(",
        "            processor.layers,",
        "            relations,",
        "            iterations=10,",
        "            alpha=0.3",
        "        )",
        "",
        "        # If there are relations, some adjustment should occur",
        "        if stats['relations_used'] > 0:",
        "            self.assertGreaterEqual(stats['tokens_affected'], 0)",
        "",
        "    def test_retrofit_embeddings(self):",
        "        \"\"\"Test retrofitting embeddings.\"\"\"",
        "        relations = extract_corpus_semantics(",
        "            self.processor.layers,",
        "            self.processor.documents,",
        "            self.processor.tokenizer",
        "        )",
        "",
        "        embeddings, _ = compute_graph_embeddings(",
        "            self.processor.layers,",
        "            dimensions=16,",
        "            method='adjacency'",
        "        )",
        "",
        "        stats = retrofit_embeddings(",
        "            embeddings,",
        "            relations,",
        "            iterations=5,",
        "            alpha=0.4",
        "        )",
        "",
        "        self.assertIsInstance(stats, dict)",
        "        self.assertIn('iterations', stats)",
        "        self.assertIn('alpha', stats)",
        "        self.assertIn('terms_retrofitted', stats)",
        "        self.assertIn('total_movement', stats)",
        "",
        "        self.assertEqual(stats['iterations'], 5)",
        "        self.assertEqual(stats['alpha'], 0.4)",
        "",
        "    def test_get_relation_type_weight(self):",
        "        \"\"\"Test getting relation type weights.\"\"\"",
        "        # Test known relation types",
        "        self.assertEqual(get_relation_type_weight('IsA'), 1.5)",
        "        self.assertEqual(get_relation_type_weight('SameAs'), 2.0)",
        "        self.assertEqual(get_relation_type_weight('Antonym'), -0.5)",
        "        self.assertEqual(get_relation_type_weight('RelatedTo'), 0.5)",
        "",
        "        # Test unknown relation type defaults to 0.5",
        "        self.assertEqual(get_relation_type_weight('UnknownRelation'), 0.5)",
        "",
        "    def test_relation_weights_constant(self):",
        "        \"\"\"Test that RELATION_WEIGHTS contains expected keys.\"\"\"",
        "        expected_relations = ['IsA', 'PartOf', 'HasA', 'SameAs', 'RelatedTo', 'CoOccurs']",
        "        for rel in expected_relations:",
        "            self.assertIn(rel, RELATION_WEIGHTS)",
        "",
        "",
        "class TestSemanticsEmptyCorpus(unittest.TestCase):",
        "    \"\"\"Test semantics with empty corpus.\"\"\"",
        "",
        "    def test_empty_corpus_semantics(self):",
        "        \"\"\"Test semantic extraction on empty processor.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        relations = extract_corpus_semantics(",
        "            processor.layers,",
        "            processor.documents,",
        "            processor.tokenizer",
        "        )",
        "        self.assertEqual(relations, [])",
        "",
        "    def test_retrofit_empty_relations(self):",
        "        \"\"\"Test retrofitting with empty relations list.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(\"doc1\", \"test content here\")",
        "        processor.compute_all(verbose=False)",
        "",
        "        stats = retrofit_connections(",
        "            processor.layers,",
        "            [],  # Empty relations",
        "            iterations=5,",
        "            alpha=0.3",
        "        )",
        "",
        "        self.assertEqual(stats['tokens_affected'], 0)",
        "        self.assertEqual(stats['relations_used'], 0)",
        "",
        "",
        "class TestSemanticsWindowSize(unittest.TestCase):",
        "    \"\"\"Test semantic extraction with different window sizes.\"\"\"",
        "",
        "    def test_larger_window_more_relations(self):",
        "        \"\"\"Test that larger window finds more co-occurrences.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(\"doc1\", \"\"\"",
        "            word1 word2 word3 word4 word5 word6 word7 word8",
        "        \"\"\")",
        "        processor.compute_all(verbose=False)",
        "",
        "        relations_small = extract_corpus_semantics(",
        "            processor.layers,",
        "            processor.documents,",
        "            processor.tokenizer,",
        "            window_size=2",
        "        )",
        "",
        "        relations_large = extract_corpus_semantics(",
        "            processor.layers,",
        "            processor.documents,",
        "            processor.tokenizer,",
        "            window_size=10",
        "        )",
        "",
        "        # Larger window should find at least as many relations",
        "        self.assertGreaterEqual(len(relations_large), len(relations_small))",
        "",
        "",
        "if __name__ == \"__main__\":",
        "    unittest.main(verbosity=2)"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    }
  ],
  "hour_of_day": 19,
  "day_of_week": "Tuesday",
  "seconds_since_last_commit": -498716,
  "is_merge": true,
  "is_initial": false,
  "parent_count": 2,
  "session_id": null,
  "related_chats": [],
  "ci_result": null,
  "reverted": false,
  "amended": false
}