{
  "hash": "fd682e2e830db52bcf331d40fa7fd67f887341ff",
  "message": "Merge pull request #8 from scrawlsbenches/claude/review-last-checkin-01LwTVdbRpEiPDS4ApauR2RT",
  "author": "scrawlsbenches",
  "timestamp": "2025-12-09 19:31:06 -0500",
  "branch": "claude/multi-index-design-DvifZ",
  "files_changed": [
    "TASK_LIST.md",
    "cortical/__init__.py",
    "cortical/analysis.py",
    "cortical/minicolumn.py",
    "cortical/persistence.py",
    "cortical/processor.py",
    "cortical/query.py",
    "cortical/semantics.py",
    "tests/test_layers.py",
    "tests/test_persistence.py",
    "tests/test_processor.py",
    "tests/test_semantics.py"
  ],
  "insertions": 4772,
  "deletions": 204,
  "hunks": [
    {
      "file": "TASK_LIST.md",
      "function": "stats = processor.compute_concept_connections(",
      "start_line": 612,
      "lines_added": [
        "**Status:** [x] Completed",
        "**Solution Applied:**",
        "1. Added `compute_bigram_connections()` function to `analysis.py` (~140 lines)",
        "2. Connects bigrams sharing left component (e.g., \"neural_networks\" ↔ \"neural_processing\")",
        "3. Connects bigrams sharing right component (e.g., \"deep_learning\" ↔ \"machine_learning\")",
        "4. Connects chain bigrams where right of one = left of other (\"machine_learning\" ↔ \"learning_algorithms\")",
        "5. Adds document co-occurrence connections weighted by Jaccard similarity",
        "6. Added configurable weights: `component_weight=0.5`, `chain_weight=0.7`, `cooccurrence_weight=0.3`",
        "7. Added `compute_bigram_connections()` method to processor with full docstring",
        "8. Integrated into `compute_all()` pipeline",
        "9. Added `COMP_BIGRAM_CONNECTIONS` staleness tracking constant",
        "10. Updated `recompute()` method to handle bigram connections",
        "",
        "**Files Modified:**",
        "- `cortical/analysis.py` - Added `compute_bigram_connections()` (~140 lines)",
        "- `cortical/processor.py` - Added wrapper method and integrated into `compute_all()`",
        "- `tests/test_processor.py` - Added 11 tests for bigram connections",
        "",
        "**Usage:**",
        "```python",
        "# Automatic in compute_all()",
        "processor.compute_all()  # Calls compute_bigram_connections() automatically",
        "",
        "# Manual with options",
        "stats = processor.compute_bigram_connections(",
        "    component_weight=0.5,  # Weight for shared component connections",
        "    chain_weight=0.7,      # Weight for chain connections",
        "    cooccurrence_weight=0.3,  # Weight for document co-occurrence",
        "    verbose=True",
        ")",
        "print(f\"Created {stats['connections_created']} bigram connections\")",
        "print(f\"  Component: {stats['component_connections']}\")",
        "print(f\"  Chain: {stats['chain_connections']}\")",
        "print(f\"  Co-occurrence: {stats['cooccurrence_connections']}\")",
        "```",
        "**Files:** `cortical/analysis.py`, `cortical/processor.py`",
        "**Status:** [x] Completed",
        "**Solution Applied:**",
        "1. Added `RELATION_WEIGHTS` constant to `analysis.py` with default weights:",
        "   - IsA: 1.5, PartOf: 1.3, HasProperty: 1.2, SimilarTo: 1.4, RelatedTo: 1.0",
        "   - Causes: 1.1, UsedFor: 1.0, CoOccurs: 0.8, Antonym: 0.3, DerivedFrom: 1.2",
        "2. Created `compute_semantic_pagerank()` function (~120 lines):",
        "   - Builds semantic relation lookup from (term1, term2) pairs",
        "   - Applies relation-type multipliers to edge weights",
        "   - Returns stats: pagerank scores, iterations_run, edges_with_relations",
        "3. Added `compute_semantic_importance()` method to processor:",
        "   - Falls back to standard PageRank if no semantic relations",
        "   - Applies semantic PageRank to both token and bigram layers",
        "   - Returns comprehensive statistics",
        "4. Updated `compute_all()` with `pagerank_method` parameter:",
        "   - 'standard': Traditional PageRank (default)",
        "   - 'semantic': ConceptNet-style with relation weighting",
        "   - Automatically extracts semantic relations if needed",
        "",
        "**Files Modified:**",
        "- `cortical/analysis.py` - Added `RELATION_WEIGHTS` and `compute_semantic_pagerank()` (~130 lines)",
        "- `cortical/processor.py` - Added `compute_semantic_importance()`, updated `compute_all()`",
        "- `tests/test_processor.py` - Added 9 tests for semantic PageRank",
        "**Usage:**",
        "# Use semantic PageRank via compute_all",
        "processor.compute_all(pagerank_method='semantic')",
        "# Or call directly",
        "processor.extract_corpus_semantics()",
        "stats = processor.compute_semantic_importance()",
        "print(f\"Found {stats['total_edges_with_relations']} semantic edges\")",
        "",
        "# Custom relation weights",
        "custom_weights = {'IsA': 2.0, 'CoOccurs': 0.5}",
        "processor.compute_semantic_importance(relation_weights=custom_weights)",
        "```",
        "**Files:** `cortical/analysis.py`, `cortical/processor.py`",
        "**Status:** [x] Completed",
        "**Solution Applied:**",
        "1. Added `compute_hierarchical_pagerank()` function to `analysis.py` (~150 lines):",
        "   - Computes local PageRank within each layer",
        "   - Propagates scores up via feedback_connections (tokens → bigrams → concepts → documents)",
        "   - Propagates scores down via feedforward_connections (documents → concepts → bigrams → tokens)",
        "   - Normalizes PageRank within each layer after propagation",
        "   - Converges when cross-layer changes are minimal",
        "2. Added `compute_hierarchical_importance()` method to processor",
        "3. Updated `compute_all()` with `pagerank_method='hierarchical'` option",
        "4. Returns detailed statistics: iterations_run, converged, per-layer stats",
        "**Files Modified:**",
        "- `cortical/analysis.py` - Added `compute_hierarchical_pagerank()` (~150 lines)",
        "- `cortical/processor.py` - Added `compute_hierarchical_importance()`, updated `compute_all()`",
        "- `tests/test_processor.py` - Added 9 tests for hierarchical PageRank",
        "",
        "**Usage:**",
        "```python",
        "# Use hierarchical PageRank via compute_all",
        "processor.compute_all(pagerank_method='hierarchical')",
        "",
        "# Or call directly with custom parameters",
        "stats = processor.compute_hierarchical_importance(",
        "    layer_iterations=10,      # Iterations for intra-layer PageRank",
        "    global_iterations=5,      # Iterations for cross-layer propagation",
        "    cross_layer_damping=0.7   # Damping at layer boundaries",
        ")",
        "print(f\"Converged: {stats['converged']} in {stats['iterations_run']} iterations\")",
        "for layer, info in stats['layer_stats'].items():",
        "    print(f\"  {layer}: {info['nodes']} nodes, max PR={info['max_pagerank']:.4f}\")",
        "**Files:** `cortical/minicolumn.py`, `cortical/__init__.py`",
        "**Status:** [x] Completed",
        "**Solution Applied:**",
        "1. Created `Edge` dataclass in `minicolumn.py` with:",
        "   - `target_id`: Target minicolumn ID",
        "   - `weight`: Connection strength (accumulates)",
        "   - `relation_type`: Semantic type ('co_occurrence', 'IsA', 'PartOf', etc.)",
        "   - `confidence`: Confidence score (0.0 to 1.0)",
        "   - `source`: Origin ('corpus', 'semantic', 'inferred')",
        "2. Added `typed_connections: Dict[str, Edge]` field to Minicolumn",
        "3. Implemented `add_typed_connection()` with intelligent merging:",
        "   - Weights accumulate",
        "   - Specific relation types override 'co_occurrence'",
        "   - Higher confidence is kept",
        "   - Source priority: inferred > semantic > corpus",
        "4. Added query methods:",
        "   - `get_typed_connection(target_id)` - Get single edge",
        "   - `get_connections_by_type(relation_type)` - Filter by relation",
        "   - `get_connections_by_source(source)` - Filter by source",
        "5. Updated `to_dict()` and `from_dict()` for persistence",
        "6. Exported `Edge` class from package",
        "**Files Modified:**",
        "- `cortical/minicolumn.py` - Added Edge dataclass and typed_connections",
        "- `cortical/__init__.py` - Export Edge class",
        "- `tests/test_layers.py` - Added 15 tests for Edge and typed connections",
        "",
        "**Usage:**",
        "from cortical import Minicolumn, Edge",
        "col = Minicolumn(\"L0_test\", \"test\", 0)",
        "",
        "# Add typed connections",
        "col.add_typed_connection(\"L0_network\", 0.8, relation_type='RelatedTo')",
        "col.add_typed_connection(\"L0_brain\", 0.5, relation_type='IsA', source='semantic')",
        "",
        "# Query by type",
        "is_a_edges = col.get_connections_by_type('IsA')",
        "semantic_edges = col.get_connections_by_source('semantic')",
        "",
        "# Get single edge",
        "edge = col.get_typed_connection(\"L0_network\")",
        "print(f\"{edge.relation_type}: {edge.weight} ({edge.confidence})\")",
        "```",
        "**Files:** `cortical/query.py`, `cortical/processor.py`",
        "**Status:** [x] Completed",
        "**Solution Applied:**",
        "1. Added `VALID_RELATION_CHAINS` constant to `query.py` defining valid relation chain patterns with validity scores:",
        "   - Transitive hierarchies: IsA→IsA (1.0), PartOf→PartOf (1.0), IsA→HasProperty (0.9)",
        "   - Association chains: RelatedTo→RelatedTo (0.6), SimilarTo→SimilarTo (0.7)",
        "   - Causal chains: Causes→Causes (0.8), Causes→HasProperty (0.7)",
        "   - Invalid chains: Antonym→IsA (0.1) - contradictory",
        "2. Added `score_relation_path()` function to compute path validity scores",
        "3. Added `expand_query_multihop()` function (~90 lines) implementing:",
        "   - BFS-style expansion with hop tracking",
        "   - Weight decay by hop distance: `weight *= decay_factor ** hop`",
        "   - Path validity filtering with `min_path_score` threshold",
        "   - Configurable parameters: `max_hops`, `max_expansions`, `decay_factor`, `min_path_score`",
        "4. Added `expand_query_multihop()` method to processor with fallback to regular expansion",
        "",
        "**Files Modified:**",
        "- `cortical/query.py` - Added `VALID_RELATION_CHAINS`, `score_relation_path()`, `expand_query_multihop()` (~150 lines)",
        "- `cortical/processor.py` - Added processor wrapper method (~45 lines)",
        "- `tests/test_processor.py` - Added 18 tests for multi-hop inference and path scoring",
        "**Usage:**",
        "# Extract semantic relations first",
        "processor.extract_corpus_semantics()",
        "",
        "# Multi-hop expansion (finds 2-hop away terms)",
        "expanded = processor.expand_query_multihop(\"neural\", max_hops=2)",
        "",
        "# Custom parameters",
        "expanded = processor.expand_query_multihop(",
        "    \"neural\",",
        "    max_hops=3,           # Follow up to 3 hops",
        "    decay_factor=0.6,     # Slower weight decay",
        "    min_path_score=0.3,   # Filter low-validity paths",
        "    max_expansions=20     # More expansion terms",
        ")",
        "**Files:** `cortical/query.py`",
        "**Status:** [x] Completed (implemented with Task 25)",
        "**Solution Applied (with Task 25):**",
        "1. Added `VALID_RELATION_CHAINS` dict defining allowed transitions with validity scores:",
        "   - Transitive: (IsA, IsA)=1.0, (PartOf, PartOf)=1.0",
        "   - Property inheritance: (IsA, HasProperty)=0.9, (PartOf, HasProperty)=0.8",
        "   - Association: (RelatedTo, RelatedTo)=0.6, (SimilarTo, SimilarTo)=0.7",
        "   - Invalid: (Antonym, IsA)=0.1",
        "2. Added `score_relation_path()` function that multiplies consecutive pair validities",
        "3. Default validity score of 0.4 for unknown relation pairs",
        "4. Integrated into `expand_query_multihop()` with `min_path_score` parameter",
        "",
        "**Files Modified:**",
        "- `cortical/query.py` - Added constants and scoring function (~50 lines)",
        "- `tests/test_processor.py` - Added 7 tests in `TestMultiHopPathScoring` class",
        "",
        "**Usage:**",
        "```python",
        "from cortical.query import score_relation_path, VALID_RELATION_CHAINS",
        "",
        "# Score a relation path",
        "score = score_relation_path(['IsA', 'IsA'])  # 1.0 (transitive)",
        "score = score_relation_path(['IsA', 'HasProperty'])  # 0.9 (property inheritance)",
        "score = score_relation_path(['Antonym', 'IsA'])  # 0.1 (contradictory)",
        "",
        "# Check valid chain patterns",
        "print(VALID_RELATION_CHAINS[('IsA', 'IsA')])  # 1.0",
        "```",
        "**Files:** `cortical/semantics.py`, `cortical/processor.py`",
        "**Status:** [x] Completed",
        "**Solution Applied:**",
        "1. Added `build_isa_hierarchy()` function to extract parent-child relationships from IsA relations",
        "2. Added `get_ancestors()` and `get_descendants()` functions for hierarchy traversal with depth tracking",
        "3. Added `inherit_properties()` function that:",
        "   - Extracts direct properties from HasProperty, HasA, CapableOf, AtLocation, UsedFor relations",
        "   - Propagates properties down IsA chains with configurable decay factor",
        "   - Returns mapping of term → {property: (weight, source_ancestor, depth)}",
        "4. Added `compute_property_similarity()` for weighted Jaccard similarity based on shared properties",
        "5. Added `apply_inheritance_to_connections()` to boost lateral connections for shared inherited properties",
        "6. Added processor wrapper methods: `compute_property_inheritance()` and `compute_property_similarity()`",
        "",
        "**Files Modified:**",
        "- `cortical/semantics.py` - Added 6 new functions (~280 lines)",
        "- `cortical/processor.py` - Added 2 processor wrapper methods (~80 lines)",
        "- `tests/test_semantics.py` - Added 23 tests across 7 new test classes",
        "",
        "**Usage:**",
        "```python",
        "# Compute property inheritance",
        "processor.extract_corpus_semantics()",
        "stats = processor.compute_property_inheritance(",
        "    decay_factor=0.7,      # Weight decay per level",
        "    max_depth=5,           # Maximum inheritance depth",
        "    apply_to_connections=True,  # Boost lateral connections",
        "    boost_factor=0.3       # Boost weight for shared properties",
        ")",
        "",
        "# Check inherited properties for a term",
        "inherited = stats['inherited']",
        "if 'dog' in inherited:",
        "    for prop, (weight, source, depth) in inherited['dog'].items():",
        "        print(f\"  {prop}: {weight:.2f} (from {source}, depth {depth})\")",
        "",
        "# Compute similarity based on shared properties",
        "sim = processor.compute_property_similarity(\"dog\", \"cat\")",
        "```",
        "**Files:** `cortical/semantics.py`, `cortical/processor.py`",
        "**Status:** [x] Completed",
        "**Solution Applied:**",
        "1. Added `RELATION_PATTERNS` constant with 30+ regex patterns covering:",
        "   - IsA patterns: \"X is a type of Y\", \"X is a kind of Y\", \"X belongs to Y\"",
        "   - HasA patterns: \"X has Y\", \"X contains Y\", \"X consists of Y\"",
        "   - PartOf patterns: \"X is part of Y\", \"X is a component of Y\"",
        "   - UsedFor patterns: \"X is used for Y\", \"X helps Y\", \"X enables Y\"",
        "   - Causes patterns: \"X causes Y\", \"X leads to Y\", \"X produces Y\"",
        "   - CapableOf patterns: \"X can Y\", \"X is able to Y\"",
        "   - AtLocation patterns: \"X is found in Y\", \"X lives in Y\"",
        "   - HasProperty patterns: \"X is Y\" (with context)",
        "   - Antonym patterns: \"X is opposite of Y\"",
        "   - DerivedFrom patterns: \"X comes from Y\"",
        "   - DefinedBy patterns: \"X means Y\"",
        "2. Added `extract_pattern_relations()` function with filtering for:",
        "   - Invalid terms (not in corpus)",
        "   - Stopwords",
        "   - Self-relations",
        "   - Duplicate relations",
        "3. Added `get_pattern_statistics()` for relation type analysis",
        "4. Updated `extract_corpus_semantics()` with `use_pattern_extraction` parameter",
        "5. Added processor method `extract_pattern_relations()` for direct access",
        "",
        "**Files Modified:**",
        "- `cortical/semantics.py` - Added `RELATION_PATTERNS`, `extract_pattern_relations()`, `get_pattern_statistics()` (~180 lines)",
        "- `cortical/processor.py` - Updated `extract_corpus_semantics()`, added `extract_pattern_relations()` (~70 lines)",
        "- `tests/test_semantics.py` - Added 16 tests for pattern extraction",
        "",
        "**Usage:**",
        "```python",
        "# Automatic pattern extraction during semantic extraction",
        "processor.extract_corpus_semantics(",
        "    use_pattern_extraction=True,    # Enabled by default",
        "    min_pattern_confidence=0.6      # Minimum confidence threshold",
        ")",
        "",
        "# Direct pattern extraction",
        "relations = processor.extract_pattern_relations(min_confidence=0.5)",
        "for t1, rel_type, t2, confidence in relations:",
        "    print(f\"{t1} --{rel_type}--> {t2} ({confidence:.2f})\")",
        "```",
        "**Files:** `cortical/persistence.py`, `cortical/processor.py`",
        "**Status:** [x] Completed",
        "**Solution Applied:**",
        "1. Added `LAYER_COLORS` constant with color codes for each layer:",
        "   - Tokens: Royal Blue (#4169E1)",
        "   - Bigrams: Forest Green (#228B22)",
        "   - Concepts: Dark Orange (#FF8C00)",
        "   - Documents: Crimson (#DC143C)",
        "2. Added `LAYER_NAMES` constant for display names",
        "3. Added `export_conceptnet_json()` function (~200 lines) with:",
        "   - Color-coded nodes by layer with layer_name",
        "   - Typed edges with relation_type, confidence, and source_type",
        "   - Cross-layer edges (feedforward/feedback)",
        "   - Relation-based edge colors",
        "   - D3.js/Cytoscape/Gephi-compatible format",
        "4. Added `_get_relation_color()` helper with 16 relation type colors",
        "5. Added `_count_edge_types()` and `_count_relation_types()` helpers",
        "6. Added processor wrapper method `export_conceptnet_json()`",
        "",
        "**Files Modified:**",
        "- `cortical/persistence.py` - Added constants and export function (~270 lines)",
        "- `cortical/processor.py` - Added processor wrapper method (~50 lines)",
        "- `tests/test_persistence.py` - Added 13 tests for ConceptNet export",
        "",
        "**Usage:**",
        "```python",
        "# Export ConceptNet-style graph",
        "processor.extract_corpus_semantics(verbose=False)",
        "graph = processor.export_conceptnet_json(",
        "    \"graph.json\",",
        "    include_cross_layer=True,     # Include feedforward/feedback edges",
        "    include_typed_edges=True,     # Include typed_connections",
        "    min_weight=0.0,               # Minimum edge weight",
        "    max_nodes_per_layer=100       # Limit nodes per layer",
        ")",
        "",
        "# Open graph.json in D3.js, Cytoscape.js, or Gephi for visualization",
        "```",
        "**Files:** `cortical/query.py`, `cortical/processor.py`",
        "**Status:** [x] Completed",
        "**Solution Applied:**",
        "1. Added `find_relation_between()` helper function to find semantic relations between two terms",
        "2. Added `find_terms_with_relation()` helper to find terms connected by a specific relation type",
        "3. Added `complete_analogy()` function (~120 lines) with three strategies:",
        "   - **Relation matching**: Find a→b relation, apply to c",
        "   - **Vector arithmetic**: Use embeddings for a - b + c ≈ d",
        "   - **Pattern matching**: Use co-occurrence patterns as fallback",
        "4. Added `complete_analogy_simple()` lightweight version using only co-occurrence patterns",
        "5. Added processor wrapper methods: `complete_analogy()` and `complete_analogy_simple()`",
        "**Files Modified:**",
        "- `cortical/query.py` - Added helper functions and analogy completion (~180 lines)",
        "- `cortical/processor.py` - Added processor wrapper methods (~60 lines)",
        "- `tests/test_processor.py` - Added 14 tests for analogy completion",
        "",
        "**Usage:**",
        "# Full analogy completion with multiple strategies",
        "results = processor.complete_analogy(\"king\", \"queen\", \"man\", top_n=5)",
        "for term, score, method in results:",
        "    print(f\"  {term}: {score:.3f} ({method})\")",
        "",
        "# Simple version (co-occurrence only)",
        "results = processor.complete_analogy_simple(\"neural\", \"networks\", \"knowledge\")",
        "for term, score in results:",
        "    print(f\"  {term}: {score:.3f}\")",
        "",
        "# Control which strategies to use",
        "results = processor.complete_analogy(",
        "    \"neural\", \"networks\", \"knowledge\",",
        "    use_embeddings=True,   # Enable vector arithmetic",
        "    use_relations=True     # Enable relation matching",
        ")",
        "## Code Review Concerns",
        "",
        "The following concerns were identified during code review and should be addressed in future iterations:",
        "",
        "### 31. Consider Splitting processor.py",
        "",
        "**File:** `cortical/processor.py`",
        "**Status:** [ ] Future Enhancement",
        "**Priority:** Low",
        "",
        "**Concern:**",
        "The `processor.py` file has grown to 800+ lines with the addition of incremental indexing, batch APIs, and multi-stage ranking. Consider splitting into smaller modules:",
        "- `processor_core.py` - Core document processing",
        "- `processor_batch.py` - Batch operations (add_documents_batch, find_*_batch)",
        "- `processor_incremental.py` - Incremental indexing and staleness tracking",
        "",
        "---",
        "",
        "### 32. Semantic Lookup Memory Optimization",
        "",
        "**File:** `cortical/analysis.py`",
        "**Function:** `compute_concept_connections()`",
        "**Status:** [ ] Future Enhancement",
        "**Priority:** Medium",
        "",
        "**Concern:**",
        "The semantic lookup builds a double-nested dictionary (`Dict[str, Dict[str, Tuple[str, float]]]`) which stores relations in both directions. For large semantic relation sets (10K+ relations), this could consume significant memory.",
        "",
        "**Potential Solution:**",
        "- Use a single direction and check both orderings at lookup time",
        "- Or use a frozenset key: `{(t1, t2): (relation, weight)}`",
        "",
        "---",
        "",
        "### 33. Tune Semantic Bonus Cap",
        "",
        "**File:** `cortical/analysis.py`",
        "**Line:** ~408",
        "**Status:** [ ] Future Enhancement",
        "**Priority:** Low",
        "",
        "**Concern:**",
        "Semantic bonus is capped at 50% boost (`min(avg_semantic, 0.5)`). This is a reasonable default but may benefit from:",
        "- Making it a configurable parameter",
        "- Empirical testing on different corpus types",
        "",
        "---",
        ""
      ],
      "lines_removed": [
        "**Status:** [ ] Pending",
        "**Implementation Steps:**",
        "1. Add `compute_bigram_connections()` function to `analysis.py`",
        "2. Connect bigrams sharing a term (left or right component)",
        "3. Weight by co-occurrence count and component position",
        "4. Add document co-occurrence bonus",
        "5. Call from `compute_all()` after bigram layer is populated",
        "**Files:** `cortical/analysis.py`",
        "**Status:** [ ] Pending",
        "**Current PageRank:**",
        "```python",
        "for target_id, weight in col.lateral_connections.items():",
        "    # All weights treated the same",
        "```",
        "**Enhanced PageRank:**",
        "RELATION_WEIGHTS = {",
        "    'IsA': 1.5,        # Hypernym relationships are strong",
        "    'PartOf': 1.3,     # Meronym relationships",
        "    'HasProperty': 1.2,",
        "    'RelatedTo': 1.0,  # Default co-occurrence",
        "    'Antonym': 0.5,    # Opposing concepts",
        "}",
        "```",
        "**Implementation Steps:**",
        "1. Add `relation_type: str` field to connection edges (or use separate dict)",
        "2. Create `compute_semantic_pagerank()` function",
        "3. Apply relation-type multipliers during propagation",
        "4. Use semantic relations from `extract_corpus_semantics()` to label edges",
        "5. Add `pagerank_method` parameter to `compute_all()`: 'standard' | 'semantic'",
        "**Files:** `cortical/analysis.py`",
        "**Status:** [ ] Pending",
        "**Implementation Steps:**",
        "1. Add `compute_hierarchical_pagerank()` function",
        "2. Iterate: compute layer-local PageRank, then propagate to adjacent layers",
        "3. Use `feedforward_connections` and `feedback_connections` for cross-layer flow",
        "4. Apply damping factor at layer boundaries (e.g., 0.7)",
        "5. Converge when cross-layer changes are minimal",
        "**Algorithm:**",
        "```",
        "for iteration in range(max_iterations):",
        "    for layer in [TOKENS, BIGRAMS, CONCEPTS, DOCUMENTS]:",
        "        compute_local_pagerank(layer)",
        "    propagate_up(TOKENS → BIGRAMS → CONCEPTS → DOCUMENTS)",
        "    propagate_down(DOCUMENTS → CONCEPTS → BIGRAMS → TOKENS)",
        "    if converged: break",
        "**Files:** `cortical/minicolumn.py`, `cortical/analysis.py`",
        "**Status:** [ ] Pending",
        "**Current:**",
        "```python",
        "lateral_connections: Dict[str, float] = {}  # {id: weight}",
        "```",
        "**Enhanced:**",
        "@dataclass",
        "class Edge:",
        "    target_id: str",
        "    weight: float",
        "    relation_type: str = 'co_occurrence'",
        "    confidence: float = 1.0",
        "    source: str = 'corpus'  # 'corpus', 'semantic', 'inferred'",
        "",
        "typed_connections: Dict[str, Edge] = {}",
        "```",
        "**Implementation Steps:**",
        "1. Create `Edge` dataclass in `minicolumn.py`",
        "2. Add `typed_connections` field alongside `lateral_connections` (backward compat)",
        "3. Update connection-building code to populate edge metadata",
        "4. Update persistence to save/load typed connections",
        "5. Migrate algorithms to use typed connections when available",
        "**Files:** `cortical/query.py`, `cortical/semantics.py`",
        "**Status:** [ ] Pending",
        "**Implementation Steps:**",
        "1. Add `expand_query_multihop()` function to `query.py`",
        "2. Follow relation chains up to `max_hops` (default: 2)",
        "3. Decay weight by hop distance: `weight *= 0.5 ** hop`",
        "4. Filter by relation path validity (IsA chains are good, random walks less so)",
        "5. Use for enhanced document retrieval",
        "**Example:**",
        "expand_query_multihop(\"neural\", max_hops=2)",
        "**Files:** `cortical/semantics.py`",
        "**Status:** [ ] Pending",
        "**Implementation Steps:**",
        "1. Create `VALID_RELATION_CHAINS` matrix defining allowed transitions",
        "2. Add `score_relation_path()` function",
        "3. Penalize invalid transitions, reward coherent chains",
        "4. Use in multi-hop expansion to filter low-quality paths",
        "**Files:** `cortical/analysis.py`, `cortical/semantics.py`",
        "**Status:** [ ] Pending",
        "**Implementation Steps:**",
        "1. Build IsA hierarchy from semantic relations",
        "2. Add `inherit_properties()` function",
        "3. Propagate properties down IsA chains with decay",
        "4. Store inherited properties separately from direct properties",
        "5. Use inherited properties in similarity calculations",
        "**Files:** `cortical/semantics.py`",
        "**Status:** [ ] Pending",
        "**Implementation Steps:**",
        "1. Add pattern-based relation extraction",
        "2. Create regex/rule patterns for common relation expressions",
        "3. Extract during `extract_corpus_semantics()`",
        "4. Store relation type with confidence score",
        "5. Weight by pattern specificity",
        "**Files:** `cortical/persistence.py`",
        "**Status:** [ ] Pending",
        "**Implementation Steps:**",
        "1. Add `export_conceptnet_json()` function",
        "2. Include edge relation types and confidence",
        "3. Color-code by layer (tokens=blue, bigrams=green, concepts=orange, docs=red)",
        "4. Export in format compatible with graph visualization tools (D3.js, Cytoscape)",
        "5. Include cross-layer edges",
        "**Files:** `cortical/query.py`",
        "**Status:** [ ] Pending",
        "**Implementation Steps:**",
        "1. Add `complete_analogy(a, b, c)` function",
        "2. Find relation between a→b",
        "3. Apply same relation from c to find d",
        "4. Use graph embeddings + relation type matching",
        "5. Return top candidates with confidence",
        "**Example:**",
        "complete_analogy(\"neural\", \"networks\", \"knowledge\")",
        "# → \"graphs\" (both form compound technical terms)"
      ],
      "context_before": [
        "    min_shared_docs=1,     # Minimum shared documents",
        "    min_jaccard=0.1        # Minimum Jaccard similarity",
        ")",
        "```",
        "",
        "---",
        "",
        "### 21. Add Bigram Lateral Connections",
        "",
        "**Files:** `cortical/analysis.py`, `cortical/processor.py`"
      ],
      "context_after": [
        "",
        "**Problem:**",
        "Layer 1 (Bigrams) has 0 lateral connections. Bigrams should connect when they:",
        "- Share a component term (\"neural_networks\" ↔ \"neural_processing\")",
        "- Co-occur in the same documents",
        "- Form chains (\"machine_learning\" ↔ \"learning_algorithms\")",
        "",
        "",
        "---",
        "",
        "## ConceptNet High Priority",
        "",
        "### 22. Implement Relation-Weighted PageRank",
        "",
        "",
        "**Problem:**",
        "Current PageRank treats all `lateral_connections` equally. ConceptNet-style PageRank should weight edges by semantic relation type.",
        "",
        "",
        "```python",
        "",
        "",
        "---",
        "",
        "### 23. Implement Cross-Layer PageRank Propagation",
        "",
        "",
        "**Problem:**",
        "PageRank only flows within a single layer. Importance should propagate across layers:",
        "- Important tokens boost their bigrams",
        "- Important bigrams boost their concepts",
        "- Important concepts boost their documents (and vice versa)",
        "",
        "",
        "```",
        "",
        "---",
        "",
        "### 24. Add Typed Edge Storage",
        "",
        "",
        "**Problem:**",
        "`lateral_connections` only stores `{target_id: weight}`. ConceptNet-style graphs need edge metadata: relation type, confidence, source.",
        "",
        "",
        "```python",
        "",
        "",
        "---",
        "",
        "## ConceptNet Medium Priority",
        "",
        "### 25. Implement Multi-Hop Semantic Inference",
        "",
        "",
        "**Problem:**",
        "Query expansion only follows single-hop connections. ConceptNet enables multi-hop inference:",
        "- \"dog\" → IsA → \"animal\" → HasProperty → \"living\"",
        "- \"car\" → PartOf → \"engine\" → UsedFor → \"transportation\"",
        "",
        "",
        "```python",
        "# Hop 1: networks (co-occur), learning (co-occur), brain (RelatedTo)",
        "# Hop 2: deep (via learning), cortex (via brain), AI (via networks)",
        "```",
        "",
        "---",
        "",
        "### 26. Add Relation Path Scoring",
        "",
        "",
        "**Problem:**",
        "Not all relation paths are equally valid for inference. Need to score paths by semantic coherence.",
        "",
        "**Valid Paths:**",
        "- IsA → IsA (transitive hypernymy): \"poodle\" → \"dog\" → \"animal\" ✓",
        "- PartOf → HasA (part inheritance): \"wheel\" → \"car\" → \"engine\" ✓",
        "- RelatedTo → RelatedTo (association): loose but acceptable",
        "",
        "**Invalid Paths:**",
        "- Antonym → IsA: contradictory",
        "- Random oscillation: low confidence",
        "",
        "",
        "---",
        "",
        "### 27. Implement Concept Inheritance",
        "",
        "",
        "**Problem:**",
        "IsA relations should enable property inheritance. If \"dog IsA animal\" and \"animal HasProperty living\", then \"dog\" should inherit \"living\".",
        "",
        "",
        "---",
        "",
        "## ConceptNet Low Priority",
        "",
        "### 28. Add Commonsense Relation Extraction",
        "",
        "",
        "**Problem:**",
        "Current relation extraction is limited to co-occurrence patterns. Could extract richer relations:",
        "- \"X is a type of Y\" → IsA",
        "- \"X contains Y\" → HasA",
        "- \"X is used for Y\" → UsedFor",
        "- \"X causes Y\" → Causes",
        "",
        "",
        "---",
        "",
        "### 29. Visualize ConceptNet-Style Graph",
        "",
        "",
        "**Problem:**",
        "Current `export_graph_json()` doesn't distinguish edge types or layers. Need ConceptNet-style visualization export.",
        "",
        "",
        "---",
        "",
        "### 30. Add Analogy Completion",
        "",
        "",
        "**Problem:**",
        "ConceptNet enables analogy completion: \"king is to queen as man is to ?\" → \"woman\"",
        "This requires relation-aware vector arithmetic.",
        "",
        "",
        "```python",
        "```",
        "",
        "---",
        "",
        "## Summary",
        "",
        "| Priority | Task | Status | Category |",
        "|----------|------|--------|----------|",
        "| Critical | Fix TF-IDF per-doc calculation | ✅ Completed | Bug Fix |",
        "| High | Add ID lookup optimization | ✅ Completed | Bug Fix |",
        "| Medium | Fix type annotations (semantics.py) | ✅ Completed | Bug Fix |",
        "| Medium | Remove unused import | ✅ Completed | Bug Fix |",
        "| Medium | Add verbose parameter | ✅ Completed | Bug Fix |",
        "| Low | Add test coverage | ✅ Completed | Bug Fix |"
      ],
      "change_type": "modify"
    },
    {
      "file": "TASK_LIST.md",
      "function": "complete_analogy(\"neural\", \"networks\", \"knowledge\")",
      "start_line": 887,
      "lines_added": [
        "| **Critical** | **Add bigram lateral connections** | ✅ Completed | **ConceptNet** |",
        "| **High** | **Implement relation-weighted PageRank** | ✅ Completed | **ConceptNet** |",
        "| **High** | **Implement cross-layer PageRank propagation** | ✅ Completed | **ConceptNet** |",
        "| **High** | **Add typed edge storage** | ✅ Completed | **ConceptNet** |",
        "| Medium | Implement multi-hop semantic inference | ✅ Completed | ConceptNet |",
        "| Medium | Add relation path scoring | ✅ Completed | ConceptNet |",
        "| Medium | Implement concept inheritance | ✅ Completed | ConceptNet |",
        "| Low | Add commonsense relation extraction | ✅ Completed | ConceptNet |",
        "| Low | Visualize ConceptNet-style graph | ✅ Completed | ConceptNet |",
        "| Low | Add analogy completion | ✅ Completed | ConceptNet |",
        "**ConceptNet Enhancement Completion:** 12/12 tasks (100%)",
        "Ran 321 tests in 0.280s",
        "All tests passing as of 2025-12-10.",
        "*Updated from code review on 2025-12-10*"
      ],
      "lines_removed": [
        "| **Critical** | **Add bigram lateral connections** | ⏳ Pending | **ConceptNet** |",
        "| **High** | **Implement relation-weighted PageRank** | ⏳ Pending | **ConceptNet** |",
        "| **High** | **Implement cross-layer PageRank propagation** | ⏳ Pending | **ConceptNet** |",
        "| **High** | **Add typed edge storage** | ⏳ Pending | **ConceptNet** |",
        "| Medium | Implement multi-hop semantic inference | ⏳ Pending | ConceptNet |",
        "| Medium | Add relation path scoring | ⏳ Pending | ConceptNet |",
        "| Medium | Implement concept inheritance | ⏳ Pending | ConceptNet |",
        "| Low | Add commonsense relation extraction | ⏳ Pending | ConceptNet |",
        "| Low | Visualize ConceptNet-style graph | ⏳ Pending | ConceptNet |",
        "| Low | Add analogy completion | ⏳ Pending | ConceptNet |",
        "**ConceptNet Enhancement Completion:** 2/12 tasks (17%)",
        "Ran 193 tests in 0.162s",
        "All tests passing as of 2025-12-09.",
        "*Updated from code review on 2025-12-09*"
      ],
      "context_before": [
        "| **High** | **Integrate semantic relations** | ✅ Completed | **RAG** |",
        "| **High** | **Persist full computed state** | ✅ Completed | **RAG** |",
        "| Medium | Fix type annotation (embeddings.py) | ✅ Completed | Bug Fix |",
        "| Medium | Optimize spectral embeddings | ✅ Completed | Performance |",
        "| Medium | Add incremental indexing | ✅ Completed | RAG |",
        "| Low | Document magic numbers | ⏳ Deferred | Documentation |",
        "| Low | Multi-stage ranking pipeline | ✅ Completed | RAG |",
        "| Low | Batch query API | ✅ Completed | RAG |",
        "| **Critical** | **Build cross-layer feedforward connections** | ✅ Completed | **ConceptNet** |",
        "| **Critical** | **Add concept-level lateral connections** | ✅ Completed | **ConceptNet** |"
      ],
      "context_after": [
        "",
        "**Bug Fix Completion:** 7/7 tasks (100%)",
        "**RAG Enhancement Completion:** 8/8 tasks (100%)",
        "",
        "---",
        "",
        "## Test Results",
        "",
        "```",
        "OK",
        "```",
        "",
        "",
        "---",
        ""
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/__init__.py",
      "function": "document retrieval, and knowledge gap detection.",
      "start_line": 8,
      "lines_added": [
        "from .minicolumn import Minicolumn, Edge",
        "    \"Edge\","
      ],
      "lines_removed": [
        "from .minicolumn import Minicolumn"
      ],
      "context_before": [
        "Example:",
        "    from cortical import CorticalTextProcessor",
        "    ",
        "    processor = CorticalTextProcessor()",
        "    processor.process_document(\"doc1\", \"Neural networks process information...\")",
        "    processor.compute_all()",
        "    results = processor.find_documents_for_query(\"neural processing\")",
        "\"\"\"",
        "",
        "from .tokenizer import Tokenizer"
      ],
      "context_after": [
        "from .layers import CorticalLayer, HierarchicalLayer",
        "from .processor import CorticalTextProcessor",
        "",
        "__version__ = \"2.0.0\"",
        "__all__ = [",
        "    \"CorticalTextProcessor\",",
        "    \"CorticalLayer\",",
        "    \"HierarchicalLayer\",",
        "    \"Minicolumn\",",
        "    \"Tokenizer\",",
        "]"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/analysis.py",
      "function": "from .minicolumn import Minicolumn",
      "start_line": 20,
      "lines_added": [
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "# Default relation weights for semantic PageRank",
        "RELATION_WEIGHTS = {",
        "    'IsA': 1.5,           # Hypernym relationships are strong",
        "    'PartOf': 1.3,        # Meronym relationships",
        "    'HasProperty': 1.2,   # Property associations",
        "    'RelatedTo': 1.0,     # Default co-occurrence",
        "    'SimilarTo': 1.4,     # Similarity relationships",
        "    'Causes': 1.1,        # Causal relationships",
        "    'UsedFor': 1.0,       # Functional relationships",
        "    'CoOccurs': 0.8,      # Basic co-occurrence",
        "    'Antonym': 0.3,       # Opposing concepts (lower weight)",
        "    'DerivedFrom': 1.2,   # Morphological derivation",
        "}",
        "",
        "",
        "def compute_semantic_pagerank(",
        "    layer: HierarchicalLayer,",
        "    semantic_relations: List[Tuple[str, str, str, float]],",
        "    relation_weights: Optional[Dict[str, float]] = None,",
        "    damping: float = 0.85,",
        "    iterations: int = 20,",
        "    tolerance: float = 1e-6",
        ") -> Dict[str, Any]:",
        "    \"\"\"",
        "    Compute PageRank with semantic relation type weighting.",
        "",
        "    This ConceptNet-style PageRank applies different multipliers based on",
        "    the semantic relation type between nodes. For example, IsA relationships",
        "    are weighted more heavily than simple co-occurrence.",
        "",
        "    Args:",
        "        layer: The layer to compute PageRank for",
        "        semantic_relations: List of (term1, relation, term2, weight) tuples",
        "        relation_weights: Optional custom relation weights dict. If None, uses defaults.",
        "        damping: Damping factor (probability of following links)",
        "        iterations: Maximum number of iterations",
        "        tolerance: Convergence threshold",
        "",
        "    Returns:",
        "        Dict containing:",
        "        - pagerank: Dict mapping column IDs to PageRank scores",
        "        - iterations_run: Number of iterations until convergence",
        "        - edges_with_relations: Number of edges that had semantic relation info",
        "",
        "    Example:",
        "        >>> relations = [(\"neural\", \"RelatedTo\", \"networks\", 0.8)]",
        "        >>> result = compute_semantic_pagerank(layer, relations)",
        "        >>> print(f\"PageRank converged in {result['iterations_run']} iterations\")",
        "    \"\"\"",
        "    n = len(layer.minicolumns)",
        "    if n == 0:",
        "        return {'pagerank': {}, 'iterations_run': 0, 'edges_with_relations': 0}",
        "",
        "    # Use default weights if not provided",
        "    weights = relation_weights or RELATION_WEIGHTS",
        "",
        "    # Build semantic relation lookup: (term1, term2) -> (relation_type, weight)",
        "    semantic_lookup: Dict[Tuple[str, str], Tuple[str, float]] = {}",
        "    for t1, relation, t2, rel_weight in semantic_relations:",
        "        # Store in both directions for undirected lookup",
        "        semantic_lookup[(t1, t2)] = (relation, rel_weight)",
        "        semantic_lookup[(t2, t1)] = (relation, rel_weight)",
        "",
        "    # Initialize PageRank uniformly",
        "    pagerank = {col.id: 1.0 / n for col in layer.minicolumns.values()}",
        "",
        "    # Build incoming links map with relation-weighted edges",
        "    incoming: Dict[str, List[Tuple[str, float]]] = defaultdict(list)",
        "    outgoing_sum: Dict[str, float] = defaultdict(float)",
        "    edges_with_relations = 0",
        "",
        "    # Build content -> id mapping for semantic lookup",
        "    content_to_id: Dict[str, str] = {}",
        "    for col in layer.minicolumns.values():",
        "        content_to_id[col.content] = col.id",
        "",
        "    for col in layer.minicolumns.values():",
        "        for target_id, base_weight in col.lateral_connections.items():",
        "            target = layer.get_by_id(target_id)",
        "            if target is None:",
        "                continue",
        "",
        "            # Check if there's a semantic relation between these terms",
        "            lookup_key = (col.content, target.content)",
        "            if lookup_key in semantic_lookup:",
        "                relation_type, rel_weight = semantic_lookup[lookup_key]",
        "                # Apply relation type multiplier",
        "                type_multiplier = weights.get(relation_type, 1.0)",
        "                # Combined weight: base_weight * relation_weight * type_multiplier",
        "                adjusted_weight = base_weight * rel_weight * type_multiplier",
        "                edges_with_relations += 1",
        "            else:",
        "                # No semantic relation, use base weight",
        "                adjusted_weight = base_weight",
        "",
        "            incoming[target_id].append((col.id, adjusted_weight))",
        "            outgoing_sum[col.id] += adjusted_weight",
        "",
        "    # Iterate until convergence",
        "    iterations_run = 0",
        "    for iteration in range(iterations):",
        "        iterations_run = iteration + 1",
        "        new_pagerank = {}",
        "        max_diff = 0.0",
        "",
        "        for col in layer.minicolumns.values():",
        "            # Sum of weighted incoming PageRank",
        "            incoming_sum = 0.0",
        "            for source_id, weight in incoming[col.id]:",
        "                if source_id in pagerank and outgoing_sum[source_id] > 0:",
        "                    incoming_sum += pagerank[source_id] * weight / outgoing_sum[source_id]",
        "",
        "            # Apply damping",
        "            new_rank = (1 - damping) / n + damping * incoming_sum",
        "            new_pagerank[col.id] = new_rank",
        "",
        "            max_diff = max(max_diff, abs(new_rank - pagerank.get(col.id, 0)))",
        "",
        "        pagerank = new_pagerank",
        "",
        "        if max_diff < tolerance:",
        "            break",
        "",
        "    # Update minicolumn pagerank values",
        "    for col in layer.minicolumns.values():",
        "        col.pagerank = pagerank.get(col.id, 1.0 / n)",
        "",
        "    return {",
        "        'pagerank': pagerank,",
        "        'iterations_run': iterations_run,",
        "        'edges_with_relations': edges_with_relations",
        "    }",
        "",
        "",
        "def compute_hierarchical_pagerank(",
        "    layers: Dict[CorticalLayer, HierarchicalLayer],",
        "    layer_iterations: int = 10,",
        "    global_iterations: int = 5,",
        "    damping: float = 0.85,",
        "    cross_layer_damping: float = 0.7,",
        "    tolerance: float = 1e-4",
        ") -> Dict[str, Any]:",
        "    \"\"\"",
        "    Compute PageRank with cross-layer propagation.",
        "",
        "    This hierarchical PageRank allows importance to flow between layers:",
        "    - Upward: tokens → bigrams → concepts → documents",
        "    - Downward: documents → concepts → bigrams → tokens",
        "",
        "    The algorithm alternates between:",
        "    1. Computing local PageRank within each layer",
        "    2. Propagating scores up the hierarchy (via feedback_connections)",
        "    3. Propagating scores down the hierarchy (via feedforward_connections)",
        "",
        "    Args:",
        "        layers: Dictionary of all layers",
        "        layer_iterations: Max iterations for intra-layer PageRank",
        "        global_iterations: Max iterations for cross-layer propagation",
        "        damping: Damping factor for intra-layer PageRank",
        "        cross_layer_damping: Damping factor for cross-layer propagation (default 0.7)",
        "        tolerance: Convergence threshold for global iterations",
        "",
        "    Returns:",
        "        Dict containing:",
        "        - iterations_run: Number of global iterations",
        "        - converged: Whether the algorithm converged",
        "        - layer_stats: Per-layer statistics",
        "",
        "    Example:",
        "        >>> result = compute_hierarchical_pagerank(layers)",
        "        >>> print(f\"Converged in {result['iterations_run']} iterations\")",
        "    \"\"\"",
        "    # Define layer order for propagation",
        "    layer_order = [",
        "        CorticalLayer.TOKENS,",
        "        CorticalLayer.BIGRAMS,",
        "        CorticalLayer.CONCEPTS,",
        "        CorticalLayer.DOCUMENTS",
        "    ]",
        "",
        "    # Filter to only existing layers with minicolumns",
        "    active_layers = [l for l in layer_order if l in layers and layers[l].column_count() > 0]",
        "",
        "    if not active_layers:",
        "        return {'iterations_run': 0, 'converged': True, 'layer_stats': {}}",
        "",
        "    # Store previous PageRank values for convergence check",
        "    prev_pageranks: Dict[CorticalLayer, Dict[str, float]] = {}",
        "",
        "    iterations_run = 0",
        "    converged = False",
        "",
        "    for global_iter in range(global_iterations):",
        "        iterations_run = global_iter + 1",
        "        max_global_diff = 0.0",
        "",
        "        # Step 1: Compute local PageRank for each layer",
        "        for layer_enum in active_layers:",
        "            layer = layers[layer_enum]",
        "            compute_pagerank(layer, damping=damping, iterations=layer_iterations, tolerance=1e-6)",
        "",
        "        # Step 2: Propagate up (tokens → bigrams → concepts → documents)",
        "        for i in range(len(active_layers) - 1):",
        "            lower_layer_enum = active_layers[i]",
        "            upper_layer_enum = active_layers[i + 1]",
        "            lower_layer = layers[lower_layer_enum]",
        "            upper_layer = layers[upper_layer_enum]",
        "",
        "            # Propagate from lower to upper via feedback connections",
        "            for col in lower_layer.minicolumns.values():",
        "                if not col.feedback_connections:",
        "                    continue",
        "",
        "                for target_id, weight in col.feedback_connections.items():",
        "                    target = upper_layer.get_by_id(target_id)",
        "                    if target:",
        "                        # Boost upper layer node based on lower layer importance",
        "                        boost = col.pagerank * weight * cross_layer_damping",
        "                        target.pagerank += boost",
        "",
        "        # Step 3: Propagate down (documents → concepts → bigrams → tokens)",
        "        for i in range(len(active_layers) - 1, 0, -1):",
        "            upper_layer_enum = active_layers[i]",
        "            lower_layer_enum = active_layers[i - 1]",
        "            upper_layer = layers[upper_layer_enum]",
        "            lower_layer = layers[lower_layer_enum]",
        "",
        "            # Propagate from upper to lower via feedforward connections",
        "            for col in upper_layer.minicolumns.values():",
        "                if not col.feedforward_connections:",
        "                    continue",
        "",
        "                for target_id, weight in col.feedforward_connections.items():",
        "                    target = lower_layer.get_by_id(target_id)",
        "                    if target:",
        "                        # Boost lower layer node based on upper layer importance",
        "                        boost = col.pagerank * weight * cross_layer_damping",
        "                        target.pagerank += boost",
        "",
        "        # Normalize PageRank within each layer",
        "        for layer_enum in active_layers:",
        "            layer = layers[layer_enum]",
        "            total = sum(col.pagerank for col in layer.minicolumns.values())",
        "            if total > 0:",
        "                for col in layer.minicolumns.values():",
        "                    col.pagerank /= total",
        "",
        "        # Check convergence",
        "        for layer_enum in active_layers:",
        "            layer = layers[layer_enum]",
        "            current_pr = {col.id: col.pagerank for col in layer.minicolumns.values()}",
        "",
        "            if layer_enum in prev_pageranks:",
        "                for col_id, pr in current_pr.items():",
        "                    prev_pr = prev_pageranks[layer_enum].get(col_id, 0)",
        "                    max_global_diff = max(max_global_diff, abs(pr - prev_pr))",
        "",
        "            prev_pageranks[layer_enum] = current_pr",
        "",
        "        if max_global_diff < tolerance and global_iter > 0:",
        "            converged = True",
        "            break",
        "",
        "    # Collect layer statistics",
        "    layer_stats = {}",
        "    for layer_enum in active_layers:",
        "        layer = layers[layer_enum]",
        "        pageranks = [col.pagerank for col in layer.minicolumns.values()]",
        "        layer_stats[layer_enum.name] = {",
        "            'nodes': len(pageranks),",
        "            'max_pagerank': max(pageranks) if pageranks else 0,",
        "            'min_pagerank': min(pageranks) if pageranks else 0,",
        "            'avg_pagerank': sum(pageranks) / len(pageranks) if pageranks else 0",
        "        }",
        "",
        "    return {",
        "        'iterations_run': iterations_run,",
        "        'converged': converged,",
        "        'layer_stats': layer_stats",
        "    }",
        "",
        ""
      ],
      "lines_removed": [
        "    ",
        "    ",
        "        ",
        "    ",
        "    ",
        "    ",
        "    ",
        "        ",
        "            ",
        "            ",
        "        ",
        "        ",
        "    ",
        "    "
      ],
      "context_before": [
        "",
        "",
        "def compute_pagerank(",
        "    layer: HierarchicalLayer,",
        "    damping: float = 0.85,",
        "    iterations: int = 20,",
        "    tolerance: float = 1e-6",
        ") -> Dict[str, float]:",
        "    \"\"\"",
        "    Compute PageRank scores for minicolumns in a layer."
      ],
      "context_after": [
        "    PageRank measures importance based on connection structure.",
        "    Highly connected columns that are connected to other important",
        "    columns receive higher scores.",
        "    Args:",
        "        layer: The layer to compute PageRank for",
        "        damping: Damping factor (probability of following links)",
        "        iterations: Maximum number of iterations",
        "        tolerance: Convergence threshold",
        "    Returns:",
        "        Dictionary mapping column IDs to PageRank scores",
        "    \"\"\"",
        "    n = len(layer.minicolumns)",
        "    if n == 0:",
        "        return {}",
        "    # Initialize PageRank uniformly",
        "    pagerank = {col.id: 1.0 / n for col in layer.minicolumns.values()}",
        "    # Build incoming links map",
        "    incoming: Dict[str, List[Tuple[str, float]]] = defaultdict(list)",
        "    outgoing_sum: Dict[str, float] = defaultdict(float)",
        "    for col in layer.minicolumns.values():",
        "        for target_id, weight in col.lateral_connections.items():",
        "            # Use O(1) lookup via get_by_id instead of O(n) linear search",
        "            if layer.get_by_id(target_id) is not None:",
        "                incoming[target_id].append((col.id, weight))",
        "                outgoing_sum[col.id] += weight",
        "    # Iterate until convergence",
        "    for iteration in range(iterations):",
        "        new_pagerank = {}",
        "        max_diff = 0.0",
        "        for col in layer.minicolumns.values():",
        "            # Sum of weighted incoming PageRank",
        "            incoming_sum = 0.0",
        "            for source_id, weight in incoming[col.id]:",
        "                if source_id in pagerank and outgoing_sum[source_id] > 0:",
        "                    incoming_sum += pagerank[source_id] * weight / outgoing_sum[source_id]",
        "            # Apply damping",
        "            new_rank = (1 - damping) / n + damping * incoming_sum",
        "            new_pagerank[col.id] = new_rank",
        "            max_diff = max(max_diff, abs(new_rank - pagerank.get(col.id, 0)))",
        "        pagerank = new_pagerank",
        "        if max_diff < tolerance:",
        "            break",
        "    # Update minicolumn pagerank values",
        "    for col in layer.minicolumns.values():",
        "        col.pagerank = pagerank.get(col.id, 1.0 / n)",
        "    return pagerank",
        "",
        "",
        "def compute_tfidf(",
        "    layers: Dict[CorticalLayer, HierarchicalLayer],",
        "    documents: Dict[str, str]",
        ") -> None:",
        "    \"\"\"",
        "    Compute TF-IDF scores for tokens.",
        "    ",
        "    TF-IDF (Term Frequency - Inverse Document Frequency) measures",
        "    how distinctive a term is to the corpus. High TF-IDF terms are",
        "    both frequent in their documents and rare across the corpus."
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/analysis.py",
      "function": "def compute_concept_connections(",
      "start_line": 435,
      "lines_added": [
        "def compute_bigram_connections(",
        "    layers: Dict[CorticalLayer, HierarchicalLayer],",
        "    min_shared_docs: int = 1,",
        "    component_weight: float = 0.5,",
        "    chain_weight: float = 0.7,",
        "    cooccurrence_weight: float = 0.3",
        ") -> Dict[str, Any]:",
        "    \"\"\"",
        "    Build lateral connections between bigrams in Layer 1.",
        "",
        "    Bigrams are connected based on:",
        "    1. Shared component terms (\"neural_networks\" ↔ \"neural_processing\")",
        "    2. Document co-occurrence (appear in same documents)",
        "    3. Chains (\"machine_learning\" ↔ \"learning_algorithms\" where right=left)",
        "",
        "    Args:",
        "        layers: Dictionary of all layers",
        "        min_shared_docs: Minimum shared documents for co-occurrence connection",
        "        component_weight: Weight for shared component connections (default 0.5)",
        "        chain_weight: Weight for chain connections (default 0.7)",
        "        cooccurrence_weight: Weight for document co-occurrence (default 0.3)",
        "",
        "    Returns:",
        "        Statistics about connections created:",
        "        - connections_created: Total bidirectional connections",
        "        - component_connections: Connections from shared components",
        "        - chain_connections: Connections from chains",
        "        - cooccurrence_connections: Connections from document co-occurrence",
        "    \"\"\"",
        "    layer1 = layers[CorticalLayer.BIGRAMS]",
        "",
        "    if layer1.column_count() == 0:",
        "        return {",
        "            'connections_created': 0,",
        "            'bigrams': 0,",
        "            'component_connections': 0,",
        "            'chain_connections': 0,",
        "            'cooccurrence_connections': 0",
        "        }",
        "",
        "    bigrams = list(layer1.minicolumns.values())",
        "",
        "    # Build indexes for efficient lookup",
        "    # left_component_index: {\"neural\": [bigram1, bigram2, ...]}",
        "    # right_component_index: {\"networks\": [bigram1, bigram3, ...]}",
        "    left_index: Dict[str, List[Minicolumn]] = defaultdict(list)",
        "    right_index: Dict[str, List[Minicolumn]] = defaultdict(list)",
        "",
        "    for bigram in bigrams:",
        "        parts = bigram.content.split('_')",
        "        if len(parts) == 2:",
        "            left_index[parts[0]].append(bigram)",
        "            right_index[parts[1]].append(bigram)",
        "",
        "    # Track connection types for statistics",
        "    component_connections = 0",
        "    chain_connections = 0",
        "    cooccurrence_connections = 0",
        "",
        "    # Track which pairs we've already connected (avoid duplicates)",
        "    connected_pairs: Set[Tuple[str, str]] = set()",
        "",
        "    def add_connection(b1: Minicolumn, b2: Minicolumn, weight: float, conn_type: str) -> bool:",
        "        \"\"\"Add bidirectional connection if not already connected.\"\"\"",
        "        nonlocal component_connections, chain_connections, cooccurrence_connections",
        "",
        "        pair = tuple(sorted([b1.id, b2.id]))",
        "        if pair in connected_pairs:",
        "            # Already connected, just strengthen the connection",
        "            b1.add_lateral_connection(b2.id, weight)",
        "            b2.add_lateral_connection(b1.id, weight)",
        "            return False",
        "",
        "        connected_pairs.add(pair)",
        "        b1.add_lateral_connection(b2.id, weight)",
        "        b2.add_lateral_connection(b1.id, weight)",
        "",
        "        if conn_type == 'component':",
        "            component_connections += 1",
        "        elif conn_type == 'chain':",
        "            chain_connections += 1",
        "        elif conn_type == 'cooccurrence':",
        "            cooccurrence_connections += 1",
        "",
        "        return True",
        "",
        "    # 1. Connect bigrams sharing a component",
        "    # Left component matches: \"neural_networks\" ↔ \"neural_processing\"",
        "    for component, bigram_list in left_index.items():",
        "        for i, b1 in enumerate(bigram_list):",
        "            for b2 in bigram_list[i+1:]:",
        "                # Weight by component's PageRank importance (if available)",
        "                weight = component_weight",
        "                add_connection(b1, b2, weight, 'component')",
        "",
        "    # Right component matches: \"deep_learning\" ↔ \"machine_learning\"",
        "    for component, bigram_list in right_index.items():",
        "        for i, b1 in enumerate(bigram_list):",
        "            for b2 in bigram_list[i+1:]:",
        "                weight = component_weight",
        "                add_connection(b1, b2, weight, 'component')",
        "",
        "    # 2. Connect chain bigrams (right of one = left of other)",
        "    # \"machine_learning\" ↔ \"learning_algorithms\"",
        "    for term in left_index:",
        "        if term in right_index:",
        "            # term appears as right component in some bigrams and left in others",
        "            for b_left in right_index[term]:  # ends with term",
        "                for b_right in left_index[term]:  # starts with term",
        "                    if b_left.id != b_right.id:",
        "                        add_connection(b_left, b_right, chain_weight, 'chain')",
        "",
        "    # 3. Connect bigrams that co-occur in the same documents",
        "    for i, b1 in enumerate(bigrams):",
        "        docs1 = b1.document_ids",
        "        if not docs1:",
        "            continue",
        "",
        "        for b2 in bigrams[i+1:]:",
        "            docs2 = b2.document_ids",
        "            if not docs2:",
        "                continue",
        "",
        "            shared_docs = docs1 & docs2",
        "            if len(shared_docs) >= min_shared_docs:",
        "                # Weight by Jaccard similarity of document sets",
        "                jaccard = len(shared_docs) / len(docs1 | docs2)",
        "                weight = cooccurrence_weight * jaccard",
        "                add_connection(b1, b2, weight, 'cooccurrence')",
        "",
        "    return {",
        "        'connections_created': len(connected_pairs),",
        "        'bigrams': len(bigrams),",
        "        'component_connections': component_connections,",
        "        'chain_connections': chain_connections,",
        "        'cooccurrence_connections': cooccurrence_connections",
        "    }",
        "",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "            concept1.add_lateral_connection(concept2.id, weight)",
        "            concept2.add_lateral_connection(concept1.id, weight)",
        "            connections_created += 1",
        "",
        "    return {",
        "        'connections_created': connections_created,",
        "        'concepts': len(concepts)",
        "    }",
        "",
        ""
      ],
      "context_after": [
        "def compute_document_connections(",
        "    layers: Dict[CorticalLayer, HierarchicalLayer],",
        "    documents: Dict[str, str],",
        "    min_shared_terms: int = 3",
        ") -> None:",
        "    \"\"\"",
        "    Build lateral connections between documents.",
        "    ",
        "    Documents are connected based on shared vocabulary,",
        "    weighted by TF-IDF scores of shared terms."
      ],
      "change_type": "add"
    },
    {
      "file": "cortical/minicolumn.py",
      "function": null,
      "start_line": 2,
      "lines_added": [
        "from typing import Set, Dict, Optional, List",
        "from dataclasses import dataclass, field, asdict",
        "",
        "",
        "@dataclass",
        "class Edge:",
        "    \"\"\"",
        "    Typed edge with metadata for ConceptNet-style graph representation.",
        "",
        "    Stores not just the connection weight, but also relation type,",
        "    confidence, and source information.",
        "",
        "    Attributes:",
        "        target_id: ID of the target minicolumn",
        "        weight: Connection strength (accumulated from multiple sources)",
        "        relation_type: Semantic relation type ('co_occurrence', 'IsA', 'PartOf', etc.)",
        "        confidence: Confidence score for this edge (0.0 to 1.0)",
        "        source: Where this edge came from ('corpus', 'semantic', 'inferred')",
        "",
        "    Example:",
        "        edge = Edge(\"L0_network\", 0.8, relation_type='RelatedTo', confidence=0.9)",
        "    \"\"\"",
        "    target_id: str",
        "    weight: float = 1.0",
        "    relation_type: str = 'co_occurrence'",
        "    confidence: float = 1.0",
        "    source: str = 'corpus'",
        "",
        "    def to_dict(self) -> Dict:",
        "        \"\"\"Convert to dictionary for serialization.\"\"\"",
        "        return asdict(self)",
        "",
        "    @classmethod",
        "    def from_dict(cls, data: Dict) -> 'Edge':",
        "        \"\"\"Create an Edge from dictionary representation.\"\"\"",
        "        return cls(",
        "            target_id=data['target_id'],",
        "            weight=data.get('weight', 1.0),",
        "            relation_type=data.get('relation_type', 'co_occurrence'),",
        "            confidence=data.get('confidence', 1.0),",
        "            source=data.get('source', 'corpus')",
        "        )",
        "        lateral_connections: Connections to other columns at same layer (simple weight dict)",
        "        typed_connections: Typed edges with metadata (ConceptNet-style)",
        "",
        "        col.add_typed_connection(\"L0_network\", 0.8, relation_type='RelatedTo')",
        "",
        "        'document_ids', 'lateral_connections', 'typed_connections',",
        "        'feedforward_sources', 'feedforward_connections', 'feedback_connections',",
        "        self.typed_connections: Dict[str, Edge] = {}  # ConceptNet-style typed edges"
      ],
      "lines_removed": [
        "from typing import Set, Dict, Optional",
        "from collections import defaultdict",
        "        lateral_connections: Connections to other columns at same layer",
        "        ",
        "    ",
        "        'document_ids', 'lateral_connections', 'feedforward_sources',",
        "        'feedforward_connections', 'feedback_connections',"
      ],
      "context_before": [
        "Minicolumn Module",
        "=================",
        "",
        "Core data structure representing a cortical minicolumn.",
        "",
        "In the neocortex, minicolumns are vertical structures containing",
        "~80-100 neurons that respond to similar features. This class models",
        "that concept for text processing.",
        "\"\"\"",
        ""
      ],
      "context_after": [
        "",
        "",
        "class Minicolumn:",
        "    \"\"\"",
        "    A minicolumn represents a single concept/feature at a given hierarchy level.",
        "    ",
        "    In the biological neocortex, minicolumns are the fundamental processing",
        "    units. Here, each minicolumn represents:",
        "    - Layer 0: A single token/word",
        "    - Layer 1: A bigram pattern",
        "    - Layer 2: A concept cluster",
        "    - Layer 3: A document",
        "    ",
        "    Attributes:",
        "        id: Unique identifier (e.g., \"L0_neural\")",
        "        content: The actual content (word, bigram, doc_id)",
        "        layer: Which layer this column belongs to",
        "        activation: Current activation level (like neural firing rate)",
        "        occurrence_count: How many times this has been observed",
        "        document_ids: Which documents contain this content",
        "        feedforward_sources: IDs of columns that feed into this one (deprecated, use feedforward_connections)",
        "        feedforward_connections: Weighted connections to lower layer columns",
        "        feedback_connections: Weighted connections to higher layer columns",
        "        tfidf: TF-IDF weight for this term",
        "        tfidf_per_doc: Document-specific TF-IDF scores",
        "        pagerank: Importance score from PageRank algorithm",
        "        cluster_id: Which cluster this belongs to (for Layer 0)",
        "        doc_occurrence_counts: Per-document occurrence counts for accurate TF-IDF",
        "    Example:",
        "        col = Minicolumn(\"L0_neural\", \"neural\", 0)",
        "        col.occurrence_count = 15",
        "        col.add_lateral_connection(\"L0_network\", 0.8)",
        "    \"\"\"",
        "    __slots__ = [",
        "        'id', 'content', 'layer', 'activation', 'occurrence_count',",
        "        'tfidf', 'tfidf_per_doc', 'pagerank', 'cluster_id',",
        "        'doc_occurrence_counts'",
        "    ]",
        "    ",
        "    def __init__(self, id: str, content: str, layer: int):",
        "        \"\"\"",
        "        Initialize a minicolumn.",
        "        ",
        "        Args:",
        "            id: Unique identifier for this column",
        "            content: The content this column represents",
        "            layer: Layer number (0-3)",
        "        \"\"\"",
        "        self.id = id",
        "        self.content = content",
        "        self.layer = layer",
        "        self.activation = 0.0",
        "        self.occurrence_count = 0",
        "        self.document_ids: Set[str] = set()",
        "        self.lateral_connections: Dict[str, float] = {}",
        "        self.feedforward_sources: Set[str] = set()  # Deprecated: use feedforward_connections",
        "        self.feedforward_connections: Dict[str, float] = {}  # Weighted links to lower layer",
        "        self.feedback_connections: Dict[str, float] = {}  # Weighted links to higher layer",
        "        self.tfidf = 0.0",
        "        self.tfidf_per_doc: Dict[str, float] = {}",
        "        self.pagerank = 1.0",
        "        self.cluster_id: Optional[int] = None",
        "        self.doc_occurrence_counts: Dict[str, int] = {}",
        "    ",
        "    def add_lateral_connection(self, target_id: str, weight: float = 1.0) -> None:"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/minicolumn.py",
      "function": "class Minicolumn:",
      "start_line": 89,
      "lines_added": [
        "    def add_typed_connection(",
        "        self,",
        "        target_id: str,",
        "        weight: float = 1.0,",
        "        relation_type: str = 'co_occurrence',",
        "        confidence: float = 1.0,",
        "        source: str = 'corpus'",
        "    ) -> None:",
        "        \"\"\"",
        "        Add or update a typed connection with metadata.",
        "",
        "        Typed connections store ConceptNet-style edge information including",
        "        relation type, confidence, and source. If a connection to the target",
        "        already exists, the weight is accumulated and metadata is updated.",
        "",
        "        Args:",
        "            target_id: ID of the target minicolumn",
        "            weight: Connection strength to add (accumulates with existing)",
        "            relation_type: Semantic relation type ('co_occurrence', 'IsA', etc.)",
        "            confidence: Confidence score for this edge (0.0 to 1.0)",
        "            source: Where this edge came from ('corpus', 'semantic', 'inferred')",
        "",
        "        Example:",
        "            col.add_typed_connection(\"L0_network\", 0.8, relation_type='RelatedTo')",
        "            col.add_typed_connection(\"L0_brain\", 0.5, relation_type='IsA', source='semantic')",
        "        \"\"\"",
        "        if target_id in self.typed_connections:",
        "            # Accumulate weight, keep most informative metadata",
        "            existing = self.typed_connections[target_id]",
        "            new_weight = existing.weight + weight",
        "            # Prefer more specific relation types over 'co_occurrence'",
        "            new_relation = relation_type if relation_type != 'co_occurrence' else existing.relation_type",
        "            # Use higher confidence",
        "            new_confidence = max(confidence, existing.confidence)",
        "            # Prefer semantic/inferred over corpus",
        "            source_priority = {'inferred': 3, 'semantic': 2, 'corpus': 1}",
        "            new_source = source if source_priority.get(source, 0) > source_priority.get(existing.source, 0) else existing.source",
        "            self.typed_connections[target_id] = Edge(",
        "                target_id=target_id,",
        "                weight=new_weight,",
        "                relation_type=new_relation,",
        "                confidence=new_confidence,",
        "                source=new_source",
        "            )",
        "        else:",
        "            self.typed_connections[target_id] = Edge(",
        "                target_id=target_id,",
        "                weight=weight,",
        "                relation_type=relation_type,",
        "                confidence=confidence,",
        "                source=source",
        "            )",
        "",
        "        # Also update simple lateral_connections for backward compatibility",
        "        self.lateral_connections[target_id] = (",
        "            self.lateral_connections.get(target_id, 0) + weight",
        "        )",
        "",
        "    def get_typed_connection(self, target_id: str) -> Optional[Edge]:",
        "        \"\"\"",
        "        Get a typed connection by target ID.",
        "",
        "        Args:",
        "            target_id: ID of the target minicolumn",
        "",
        "        Returns:",
        "            Edge object if exists, None otherwise",
        "        \"\"\"",
        "        return self.typed_connections.get(target_id)",
        "",
        "    def get_connections_by_type(self, relation_type: str) -> List[Edge]:",
        "        \"\"\"",
        "        Get all typed connections with a specific relation type.",
        "",
        "        Args:",
        "            relation_type: Relation type to filter by (e.g., 'IsA', 'PartOf')",
        "",
        "        Returns:",
        "            List of Edge objects matching the relation type",
        "        \"\"\"",
        "        return [",
        "            edge for edge in self.typed_connections.values()",
        "            if edge.relation_type == relation_type",
        "        ]",
        "",
        "    def get_connections_by_source(self, source: str) -> List[Edge]:",
        "        \"\"\"",
        "        Get all typed connections from a specific source.",
        "",
        "        Args:",
        "            source: Source to filter by ('corpus', 'semantic', 'inferred')",
        "",
        "        Returns:",
        "            List of Edge objects from the specified source",
        "        \"\"\"",
        "        return [",
        "            edge for edge in self.typed_connections.values()",
        "            if edge.source == source",
        "        ]",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "        wire together\").",
        "",
        "        Args:",
        "            target_id: ID of the target minicolumn",
        "            weight: Connection strength to add",
        "        \"\"\"",
        "        self.lateral_connections[target_id] = (",
        "            self.lateral_connections.get(target_id, 0) + weight",
        "        )",
        ""
      ],
      "context_after": [
        "    def add_feedforward_connection(self, target_id: str, weight: float = 1.0) -> None:",
        "        \"\"\"",
        "        Add or strengthen a feedforward connection to a lower layer column.",
        "",
        "        Feedforward connections link higher-level representations to their",
        "        component parts (e.g., bigram → tokens, concept → tokens).",
        "",
        "        Args:",
        "            target_id: ID of the lower-layer minicolumn",
        "            weight: Connection strength to add"
      ],
      "change_type": "add"
    },
    {
      "file": "cortical/minicolumn.py",
      "function": "class Minicolumn:",
      "start_line": 157,
      "lines_added": [
        "            'typed_connections': {",
        "                target_id: edge.to_dict()",
        "                for target_id, edge in self.typed_connections.items()",
        "            },"
      ],
      "lines_removed": [],
      "context_before": [
        "            Dictionary representation of this minicolumn",
        "        \"\"\"",
        "        return {",
        "            'id': self.id,",
        "            'content': self.content,",
        "            'layer': self.layer,",
        "            'activation': self.activation,",
        "            'occurrence_count': self.occurrence_count,",
        "            'document_ids': list(self.document_ids),",
        "            'lateral_connections': self.lateral_connections,"
      ],
      "context_after": [
        "            'feedforward_sources': list(self.feedforward_sources),",
        "            'feedforward_connections': self.feedforward_connections,",
        "            'feedback_connections': self.feedback_connections,",
        "            'tfidf': self.tfidf,",
        "            'tfidf_per_doc': self.tfidf_per_doc,",
        "            'pagerank': self.pagerank,",
        "            'cluster_id': self.cluster_id,",
        "            'doc_occurrence_counts': self.doc_occurrence_counts",
        "        }",
        "    "
      ],
      "change_type": "add"
    },
    {
      "file": "cortical/minicolumn.py",
      "function": "class Minicolumn:",
      "start_line": 183,
      "lines_added": [
        "        # Deserialize typed connections",
        "        typed_conn_data = data.get('typed_connections', {})",
        "        col.typed_connections = {",
        "            target_id: Edge.from_dict(edge_data)",
        "            for target_id, edge_data in typed_conn_data.items()",
        "        }"
      ],
      "lines_removed": [],
      "context_before": [
        "            data: Dictionary with minicolumn data",
        "",
        "        Returns:",
        "            New Minicolumn instance",
        "        \"\"\"",
        "        col = cls(data['id'], data['content'], data['layer'])",
        "        col.activation = data.get('activation', 0.0)",
        "        col.occurrence_count = data.get('occurrence_count', 0)",
        "        col.document_ids = set(data.get('document_ids', []))",
        "        col.lateral_connections = data.get('lateral_connections', {})"
      ],
      "context_after": [
        "        col.feedforward_sources = set(data.get('feedforward_sources', []))",
        "        col.feedforward_connections = data.get('feedforward_connections', {})",
        "        col.feedback_connections = data.get('feedback_connections', {})",
        "        col.tfidf = data.get('tfidf', 0.0)",
        "        col.tfidf_per_doc = data.get('tfidf_per_doc', {})",
        "        col.pagerank = data.get('pagerank', 1.0)",
        "        col.cluster_id = data.get('cluster_id')",
        "        col.doc_occurrence_counts = data.get('doc_occurrence_counts', {})",
        "        return col",
        "    "
      ],
      "change_type": "add"
    },
    {
      "file": "cortical/persistence.py",
      "function": "def load_semantic_relations_json(filepath: str) -> list:",
      "start_line": 280,
      "lines_added": [
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "# Layer colors for visualization",
        "LAYER_COLORS = {",
        "    CorticalLayer.TOKENS: '#4169E1',     # Royal Blue",
        "    CorticalLayer.BIGRAMS: '#228B22',    # Forest Green",
        "    CorticalLayer.CONCEPTS: '#FF8C00',   # Dark Orange",
        "    CorticalLayer.DOCUMENTS: '#DC143C',  # Crimson",
        "}",
        "",
        "# Layer display names",
        "LAYER_NAMES = {",
        "    CorticalLayer.TOKENS: 'Tokens',",
        "    CorticalLayer.BIGRAMS: 'Bigrams',",
        "    CorticalLayer.CONCEPTS: 'Concepts',",
        "    CorticalLayer.DOCUMENTS: 'Documents',",
        "}",
        "",
        "",
        "def export_conceptnet_json(",
        "    filepath: str,",
        "    layers: Dict[CorticalLayer, HierarchicalLayer],",
        "    semantic_relations: Optional[list] = None,",
        "    include_cross_layer: bool = True,",
        "    include_typed_edges: bool = True,",
        "    min_weight: float = 0.0,",
        "    min_confidence: float = 0.0,",
        "    max_nodes_per_layer: int = 100,",
        "    verbose: bool = True",
        ") -> Dict[str, Any]:",
        "    \"\"\"",
        "    Export ConceptNet-style graph for visualization.",
        "",
        "    Creates a rich graph format with:",
        "    - Color-coded nodes by layer",
        "    - Typed edges with relation types and confidence",
        "    - Cross-layer connections (feedforward/feedback)",
        "    - D3.js/Cytoscape-compatible output",
        "",
        "    Args:",
        "        filepath: Output file path (JSON)",
        "        layers: Dictionary of layers",
        "        semantic_relations: Optional list of (t1, rel, t2, weight) tuples",
        "        include_cross_layer: Include feedforward/feedback edges",
        "        include_typed_edges: Include typed_connections with relation types",
        "        min_weight: Minimum edge weight to include",
        "        min_confidence: Minimum confidence for typed edges",
        "        max_nodes_per_layer: Maximum nodes per layer (by PageRank)",
        "        verbose: Print progress messages",
        "",
        "    Returns:",
        "        The exported graph data",
        "",
        "    Example:",
        "        >>> export_conceptnet_json(",
        "        ...     \"graph.json\", processor.layers,",
        "        ...     semantic_relations=processor.semantic_relations",
        "        ... )",
        "    \"\"\"",
        "    nodes = []",
        "    edges = []",
        "    node_ids = set()",
        "    edge_set = set()  # Track unique edges",
        "",
        "    # Collect nodes from each layer",
        "    for layer_enum, layer in layers.items():",
        "        if layer is None or layer.column_count() == 0:",
        "            continue",
        "",
        "        color = LAYER_COLORS.get(layer_enum, '#808080')",
        "        layer_name = LAYER_NAMES.get(layer_enum, f'Layer {layer_enum.value}')",
        "",
        "        # Sort by PageRank and take top nodes",
        "        sorted_cols = sorted(",
        "            layer.minicolumns.values(),",
        "            key=lambda c: c.pagerank,",
        "            reverse=True",
        "        )[:max_nodes_per_layer]",
        "",
        "        for col in sorted_cols:",
        "            node = {",
        "                'id': col.id,",
        "                'label': col.content,",
        "                'layer': layer_enum.value,",
        "                'layer_name': layer_name,",
        "                'color': color,",
        "                'pagerank': round(col.pagerank, 6),",
        "                'tfidf': round(col.tfidf, 6),",
        "                'activation': round(col.activation, 6),",
        "                'occurrence_count': col.occurrence_count,",
        "                'document_count': len(col.document_ids),",
        "                'cluster_id': col.cluster_id",
        "            }",
        "            nodes.append(node)",
        "            node_ids.add(col.id)",
        "",
        "    # Collect lateral edges (same-layer connections)",
        "    for layer_enum, layer in layers.items():",
        "        if layer is None:",
        "            continue",
        "",
        "        for col in layer.minicolumns.values():",
        "            if col.id not in node_ids:",
        "                continue",
        "",
        "            # Add typed edges with relation information",
        "            if include_typed_edges:",
        "                for target_id, edge_obj in col.typed_connections.items():",
        "                    if target_id in node_ids and edge_obj.weight >= min_weight:",
        "                        if edge_obj.confidence >= min_confidence:",
        "                            edge_key = (col.id, target_id, edge_obj.relation_type)",
        "                            if edge_key not in edge_set:",
        "                                edge_set.add(edge_key)",
        "                                edges.append({",
        "                                    'source': col.id,",
        "                                    'target': target_id,",
        "                                    'weight': round(edge_obj.weight, 4),",
        "                                    'relation_type': edge_obj.relation_type,",
        "                                    'confidence': round(edge_obj.confidence, 4),",
        "                                    'source_type': edge_obj.source,",
        "                                    'edge_type': 'lateral',",
        "                                    'color': _get_relation_color(edge_obj.relation_type)",
        "                                })",
        "",
        "            # Add regular lateral connections (without typed info)",
        "            for target_id, weight in col.lateral_connections.items():",
        "                if target_id in node_ids and weight >= min_weight:",
        "                    # Skip if already added as typed edge",
        "                    if include_typed_edges and target_id in col.typed_connections:",
        "                        continue",
        "                    edge_key = (col.id, target_id, 'co_occurrence')",
        "                    if edge_key not in edge_set:",
        "                        edge_set.add(edge_key)",
        "                        edges.append({",
        "                            'source': col.id,",
        "                            'target': target_id,",
        "                            'weight': round(weight, 4),",
        "                            'relation_type': 'co_occurrence',",
        "                            'confidence': 1.0,",
        "                            'source_type': 'corpus',",
        "                            'edge_type': 'lateral',",
        "                            'color': '#999999'",
        "                        })",
        "",
        "    # Add cross-layer edges (feedforward/feedback)",
        "    if include_cross_layer:",
        "        for layer_enum, layer in layers.items():",
        "            if layer is None:",
        "                continue",
        "",
        "            for col in layer.minicolumns.values():",
        "                if col.id not in node_ids:",
        "                    continue",
        "",
        "                # Feedforward connections (to lower layers)",
        "                for target_id, weight in col.feedforward_connections.items():",
        "                    if target_id in node_ids and weight >= min_weight:",
        "                        edge_key = (col.id, target_id, 'feedforward')",
        "                        if edge_key not in edge_set:",
        "                            edge_set.add(edge_key)",
        "                            edges.append({",
        "                                'source': col.id,",
        "                                'target': target_id,",
        "                                'weight': round(weight, 4),",
        "                                'relation_type': 'feedforward',",
        "                                'confidence': 1.0,",
        "                                'source_type': 'structure',",
        "                                'edge_type': 'cross_layer',",
        "                                'color': '#4CAF50'  # Green",
        "                            })",
        "",
        "                # Feedback connections (to higher layers)",
        "                for target_id, weight in col.feedback_connections.items():",
        "                    if target_id in node_ids and weight >= min_weight:",
        "                        edge_key = (col.id, target_id, 'feedback')",
        "                        if edge_key not in edge_set:",
        "                            edge_set.add(edge_key)",
        "                            edges.append({",
        "                                'source': col.id,",
        "                                'target': target_id,",
        "                                'weight': round(weight, 4),",
        "                                'relation_type': 'feedback',",
        "                                'confidence': 1.0,",
        "                                'source_type': 'structure',",
        "                                'edge_type': 'cross_layer',",
        "                                'color': '#9C27B0'  # Purple",
        "                            })",
        "",
        "    # Add edges from semantic relations if provided",
        "    if semantic_relations:",
        "        for rel in semantic_relations:",
        "            if len(rel) >= 4:",
        "                t1, rel_type, t2, weight = rel[:4]",
        "                # Find node IDs",
        "                source_id = f\"L0_{t1}\"",
        "                target_id = f\"L0_{t2}\"",
        "                if source_id in node_ids and target_id in node_ids:",
        "                    if weight >= min_weight:",
        "                        edge_key = (source_id, target_id, rel_type)",
        "                        if edge_key not in edge_set:",
        "                            edge_set.add(edge_key)",
        "                            edges.append({",
        "                                'source': source_id,",
        "                                'target': target_id,",
        "                                'weight': round(weight, 4),",
        "                                'relation_type': rel_type,",
        "                                'confidence': 1.0,",
        "                                'source_type': 'semantic',",
        "                                'edge_type': 'semantic',",
        "                                'color': _get_relation_color(rel_type)",
        "                            })",
        "",
        "    # Build graph structure",
        "    graph = {",
        "        'nodes': nodes,",
        "        'edges': edges,",
        "        'metadata': {",
        "            'node_count': len(nodes),",
        "            'edge_count': len(edges),",
        "            'layers': {",
        "                layer_enum.value: {",
        "                    'name': LAYER_NAMES.get(layer_enum, f'Layer {layer_enum.value}'),",
        "                    'color': LAYER_COLORS.get(layer_enum, '#808080'),",
        "                    'node_count': sum(1 for n in nodes if n['layer'] == layer_enum.value)",
        "                }",
        "                for layer_enum in layers.keys()",
        "            },",
        "            'edge_types': _count_edge_types(edges),",
        "            'relation_types': _count_relation_types(edges),",
        "            'format_version': '1.0',",
        "            'compatible_with': ['D3.js', 'Cytoscape.js', 'vis.js', 'Gephi']",
        "        }",
        "    }",
        "",
        "    # Write to file",
        "    with open(filepath, 'w') as f:",
        "        json.dump(graph, f, indent=2)",
        "",
        "    if verbose:",
        "        print(f\"ConceptNet-style graph exported to {filepath}\")",
        "        print(f\"  Nodes: {len(nodes)}\")",
        "        print(f\"  Edges: {len(edges)}\")",
        "        print(f\"  Layers: {list(graph['metadata']['layers'].keys())}\")",
        "        print(f\"  Edge types: {graph['metadata']['edge_types']}\")",
        "",
        "    return graph",
        "",
        "",
        "def _get_relation_color(relation_type: str) -> str:",
        "    \"\"\"Get color for a relation type.\"\"\"",
        "    relation_colors = {",
        "        'IsA': '#E91E63',         # Pink",
        "        'PartOf': '#9C27B0',      # Purple",
        "        'HasA': '#673AB7',        # Deep Purple",
        "        'UsedFor': '#3F51B5',     # Indigo",
        "        'Causes': '#F44336',      # Red",
        "        'HasProperty': '#FF9800', # Orange",
        "        'AtLocation': '#4CAF50',  # Green",
        "        'CapableOf': '#00BCD4',   # Cyan",
        "        'SimilarTo': '#2196F3',   # Blue",
        "        'Antonym': '#795548',     # Brown",
        "        'RelatedTo': '#607D8B',   # Blue Grey",
        "        'CoOccurs': '#9E9E9E',    # Grey",
        "        'DerivedFrom': '#8BC34A', # Light Green",
        "        'DefinedBy': '#FFEB3B',   # Yellow",
        "        'feedforward': '#4CAF50', # Green",
        "        'feedback': '#9C27B0',    # Purple",
        "        'co_occurrence': '#999999',  # Grey",
        "    }",
        "    return relation_colors.get(relation_type, '#808080')",
        "",
        "",
        "def _count_edge_types(edges: list) -> Dict[str, int]:",
        "    \"\"\"Count edges by edge_type.\"\"\"",
        "    counts: Dict[str, int] = {}",
        "    for edge in edges:",
        "        edge_type = edge.get('edge_type', 'unknown')",
        "        counts[edge_type] = counts.get(edge_type, 0) + 1",
        "    return counts",
        "",
        "",
        "def _count_relation_types(edges: list) -> Dict[str, int]:",
        "    \"\"\"Count edges by relation_type.\"\"\"",
        "    counts: Dict[str, int] = {}",
        "    for edge in edges:",
        "        rel_type = edge.get('relation_type', 'unknown')",
        "        counts[rel_type] = counts.get(rel_type, 0) + 1",
        "    return counts"
      ],
      "lines_removed": [
        "    ",
        "        ",
        "    ",
        "    ",
        "    "
      ],
      "context_before": [
        "    ",
        "    return data.get('relations', [])",
        "",
        "",
        "def get_state_summary(",
        "    layers: Dict[CorticalLayer, HierarchicalLayer],",
        "    documents: Dict[str, str]",
        ") -> Dict:",
        "    \"\"\"",
        "    Get a summary of the current processor state."
      ],
      "context_after": [
        "    Args:",
        "        layers: Dictionary of layers",
        "        documents: Document collection",
        "    Returns:",
        "        Summary statistics",
        "    \"\"\"",
        "    summary = {",
        "        'documents': len(documents),",
        "        'layers': {}",
        "    }",
        "    for layer_enum, layer in layers.items():",
        "        summary['layers'][layer_enum.name] = {",
        "            'columns': len(layer.minicolumns),",
        "            'connections': layer.total_connections(),",
        "            'avg_activation': layer.average_activation(),",
        "            'sparsity': layer.sparsity()",
        "        }",
        "    summary['total_columns'] = sum(",
        "        len(layer.minicolumns) for layer in layers.values()",
        "    )",
        "    summary['total_connections'] = sum(",
        "        layer.total_connections() for layer in layers.values()",
        "    )",
        "    return summary"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/processor.py",
      "function": "from . import persistence",
      "start_line": 19,
      "lines_added": [
        "    COMP_BIGRAM_CONNECTIONS = 'bigram_connections'"
      ],
      "lines_removed": [],
      "context_before": [
        "",
        "",
        "class CorticalTextProcessor:",
        "    \"\"\"Neocortex-inspired text processing system.\"\"\"",
        "",
        "    # Computation types for tracking staleness",
        "    COMP_TFIDF = 'tfidf'",
        "    COMP_PAGERANK = 'pagerank'",
        "    COMP_ACTIVATION = 'activation'",
        "    COMP_DOC_CONNECTIONS = 'doc_connections'"
      ],
      "context_after": [
        "    COMP_CONCEPTS = 'concepts'",
        "    COMP_EMBEDDINGS = 'embeddings'",
        "    COMP_SEMANTICS = 'semantics'",
        "",
        "    def __init__(self, tokenizer: Optional[Tokenizer] = None):",
        "        self.tokenizer = tokenizer or Tokenizer()",
        "        self.layers: Dict[CorticalLayer, HierarchicalLayer] = {",
        "            CorticalLayer.TOKENS: HierarchicalLayer(CorticalLayer.TOKENS),",
        "            CorticalLayer.BIGRAMS: HierarchicalLayer(CorticalLayer.BIGRAMS),",
        "            CorticalLayer.CONCEPTS: HierarchicalLayer(CorticalLayer.CONCEPTS),"
      ],
      "change_type": "add"
    },
    {
      "file": "cortical/processor.py",
      "function": "class CorticalTextProcessor:",
      "start_line": 160,
      "lines_added": [
        "            self.COMP_BIGRAM_CONNECTIONS,"
      ],
      "lines_removed": [],
      "context_before": [
        "        import copy",
        "        return copy.deepcopy(self.document_metadata)",
        "",
        "    def _mark_all_stale(self) -> None:",
        "        \"\"\"Mark all computations as stale (needing recomputation).\"\"\"",
        "        self._stale_computations = {",
        "            self.COMP_TFIDF,",
        "            self.COMP_PAGERANK,",
        "            self.COMP_ACTIVATION,",
        "            self.COMP_DOC_CONNECTIONS,"
      ],
      "context_after": [
        "            self.COMP_CONCEPTS,",
        "            self.COMP_EMBEDDINGS,",
        "            self.COMP_SEMANTICS,",
        "        }",
        "",
        "    def _mark_fresh(self, *computation_types: str) -> None:",
        "        \"\"\"Mark specified computations as fresh (up-to-date).\"\"\"",
        "        for comp in computation_types:",
        "            self._stale_computations.discard(comp)",
        ""
      ],
      "change_type": "add"
    },
    {
      "file": "cortical/processor.py",
      "function": "class CorticalTextProcessor:",
      "start_line": 349,
      "lines_added": [
        "                self.COMP_BIGRAM_CONNECTIONS: True,"
      ],
      "lines_removed": [],
      "context_before": [
        "        recomputed = {}",
        "",
        "        if level == 'full':",
        "            self.compute_all(verbose=verbose)",
        "            self._stale_computations.clear()",
        "            recomputed = {",
        "                self.COMP_ACTIVATION: True,",
        "                self.COMP_PAGERANK: True,",
        "                self.COMP_TFIDF: True,",
        "                self.COMP_DOC_CONNECTIONS: True,"
      ],
      "context_after": [
        "                self.COMP_CONCEPTS: True,",
        "            }",
        "        elif level == 'tfidf':",
        "            self.compute_tfidf(verbose=verbose)",
        "            self._mark_fresh(self.COMP_TFIDF)",
        "            recomputed[self.COMP_TFIDF] = True",
        "        elif level == 'stale':",
        "            # Recompute only what's stale, in dependency order",
        "            if self.COMP_ACTIVATION in self._stale_computations:",
        "                self.propagate_activation(verbose=verbose)"
      ],
      "change_type": "add"
    },
    {
      "file": "cortical/processor.py",
      "function": "class CorticalTextProcessor:",
      "start_line": 377,
      "lines_added": [
        "            if self.COMP_BIGRAM_CONNECTIONS in self._stale_computations:",
        "                self.compute_bigram_connections(verbose=verbose)",
        "                self._mark_fresh(self.COMP_BIGRAM_CONNECTIONS)",
        "                recomputed[self.COMP_BIGRAM_CONNECTIONS] = True",
        "",
        "    def compute_all(",
        "        self,",
        "        verbose: bool = True,",
        "        build_concepts: bool = True,",
        "        pagerank_method: str = 'standard'",
        "    ) -> None:",
        "            pagerank_method: PageRank algorithm to use:",
        "                - 'standard': Traditional PageRank using connection weights",
        "                - 'semantic': ConceptNet-style PageRank with relation type weighting.",
        "                              Requires semantic relations (extracts automatically if needed).",
        "                - 'hierarchical': Cross-layer PageRank with importance propagation",
        "                                  between layers (tokens ↔ bigrams ↔ concepts ↔ documents).",
        "",
        "        if pagerank_method == 'semantic':",
        "            # Extract semantic relations if not already done",
        "            if not self.semantic_relations:",
        "                if verbose:",
        "                    print(\"Extracting semantic relations...\")",
        "                self.extract_corpus_semantics(verbose=False)",
        "            if verbose:",
        "                print(\"Computing importance (Semantic PageRank)...\")",
        "            self.compute_semantic_importance(verbose=False)",
        "        elif pagerank_method == 'hierarchical':",
        "            if verbose:",
        "                print(\"Computing importance (Hierarchical PageRank)...\")",
        "            self.compute_hierarchical_importance(verbose=False)",
        "        else:",
        "            if verbose:",
        "                print(\"Computing importance (PageRank)...\")",
        "            self.compute_importance(verbose=False)",
        "        if verbose:",
        "            print(\"Computing bigram connections...\")",
        "        self.compute_bigram_connections(verbose=False)",
        "            self.COMP_BIGRAM_CONNECTIONS,",
        "",
        "    def compute_semantic_importance(",
        "        self,",
        "        relation_weights: Optional[Dict[str, float]] = None,",
        "        verbose: bool = True",
        "    ) -> Dict[str, Any]:",
        "        \"\"\"",
        "        Compute PageRank with semantic relation weighting.",
        "",
        "        Uses semantic relations to weight edges in the PageRank graph.",
        "        Edges with stronger semantic relationships (e.g., IsA, PartOf) receive",
        "        higher weights, affecting importance propagation.",
        "",
        "        Args:",
        "            relation_weights: Optional custom relation type weights dict.",
        "                Defaults to built-in weights (IsA: 1.5, PartOf: 1.3, etc.)",
        "            verbose: Print progress messages",
        "",
        "        Returns:",
        "            Dict with statistics:",
        "            - total_edges_with_relations: Sum across layers",
        "            - token_layer: Stats for token layer",
        "            - bigram_layer: Stats for bigram layer",
        "",
        "        Example:",
        "            >>> # Use default relation weights",
        "            >>> stats = processor.compute_semantic_importance()",
        "            >>> print(f\"Found {stats['total_edges_with_relations']} semantic edges\")",
        "            >>>",
        "            >>> # Custom weights",
        "            >>> weights = {'IsA': 2.0, 'RelatedTo': 0.5}",
        "            >>> processor.compute_semantic_importance(relation_weights=weights)",
        "        \"\"\"",
        "        if not self.semantic_relations:",
        "            # Fall back to standard PageRank if no semantic relations",
        "            self.compute_importance(verbose=verbose)",
        "            return {",
        "                'total_edges_with_relations': 0,",
        "                'token_layer': {'edges_with_relations': 0},",
        "                'bigram_layer': {'edges_with_relations': 0}",
        "            }",
        "",
        "        total_edges = 0",
        "        layer_stats = {}",
        "",
        "        for layer_enum in [CorticalLayer.TOKENS, CorticalLayer.BIGRAMS]:",
        "            result = analysis.compute_semantic_pagerank(",
        "                self.layers[layer_enum],",
        "                self.semantic_relations,",
        "                relation_weights=relation_weights",
        "            )",
        "            layer_name = 'token_layer' if layer_enum == CorticalLayer.TOKENS else 'bigram_layer'",
        "            layer_stats[layer_name] = {",
        "                'iterations_run': result['iterations_run'],",
        "                'edges_with_relations': result['edges_with_relations']",
        "            }",
        "            total_edges += result['edges_with_relations']",
        "",
        "        if verbose:",
        "            print(f\"Computed semantic PageRank ({total_edges} relation-weighted edges)\")",
        "",
        "        return {",
        "            'total_edges_with_relations': total_edges,",
        "            **layer_stats",
        "        }",
        "",
        "    def compute_hierarchical_importance(",
        "        self,",
        "        layer_iterations: int = 10,",
        "        global_iterations: int = 5,",
        "        cross_layer_damping: float = 0.7,",
        "        verbose: bool = True",
        "    ) -> Dict[str, Any]:",
        "        \"\"\"",
        "        Compute PageRank with cross-layer propagation.",
        "",
        "        This hierarchical PageRank allows importance to flow between layers:",
        "        - Upward: tokens → bigrams → concepts → documents",
        "        - Downward: documents → concepts → bigrams → tokens",
        "",
        "        Important tokens boost their containing bigrams and concepts.",
        "        Important documents boost their contained terms. This creates",
        "        a more holistic importance score that considers the full hierarchy.",
        "",
        "        Args:",
        "            layer_iterations: Max iterations for intra-layer PageRank (default 10)",
        "            global_iterations: Max iterations for cross-layer propagation (default 5)",
        "            cross_layer_damping: Damping factor at layer boundaries (default 0.7)",
        "            verbose: Print progress messages",
        "",
        "        Returns:",
        "            Dict with statistics:",
        "            - iterations_run: Number of global iterations",
        "            - converged: Whether the algorithm converged",
        "            - layer_stats: Per-layer statistics (nodes, max/min/avg PageRank)",
        "",
        "        Example:",
        "            >>> stats = processor.compute_hierarchical_importance()",
        "            >>> print(f\"Converged: {stats['converged']}\")",
        "            >>> for layer, info in stats['layer_stats'].items():",
        "            ...     print(f\"{layer}: {info['nodes']} nodes, max PR={info['max_pagerank']:.4f}\")",
        "        \"\"\"",
        "        result = analysis.compute_hierarchical_pagerank(",
        "            self.layers,",
        "            layer_iterations=layer_iterations,",
        "            global_iterations=global_iterations,",
        "            cross_layer_damping=cross_layer_damping",
        "        )",
        "",
        "        if verbose:",
        "            status = \"converged\" if result['converged'] else \"did not converge\"",
        "            print(f\"Computed hierarchical PageRank ({result['iterations_run']} iterations, {status})\")",
        "",
        "        return result",
        "",
        "",
        "    def compute_bigram_connections(",
        "        self,",
        "        min_shared_docs: int = 1,",
        "        component_weight: float = 0.5,",
        "        chain_weight: float = 0.7,",
        "        cooccurrence_weight: float = 0.3,",
        "        verbose: bool = True",
        "    ) -> Dict[str, Any]:",
        "        \"\"\"",
        "        Build lateral connections between bigrams based on shared components and co-occurrence.",
        "",
        "        Bigrams are connected when they:",
        "        - Share a component term (\"neural_networks\" ↔ \"neural_processing\")",
        "        - Form chains (\"machine_learning\" ↔ \"learning_algorithms\")",
        "        - Co-occur in the same documents",
        "",
        "        Args:",
        "            min_shared_docs: Minimum shared documents for co-occurrence connection",
        "            component_weight: Weight for shared component connections (default 0.5)",
        "            chain_weight: Weight for chain connections (default 0.7)",
        "            cooccurrence_weight: Weight for document co-occurrence (default 0.3)",
        "            verbose: Print progress messages",
        "",
        "        Returns:",
        "            Statistics about connections created:",
        "            - connections_created: Total bidirectional connections",
        "            - component_connections: Connections from shared components",
        "            - chain_connections: Connections from chains",
        "            - cooccurrence_connections: Connections from document co-occurrence",
        "",
        "        Example:",
        "            >>> stats = processor.compute_bigram_connections()",
        "            >>> print(f\"Created {stats['connections_created']} bigram connections\")",
        "            >>> print(f\"  Component: {stats['component_connections']}\")",
        "            >>> print(f\"  Chain: {stats['chain_connections']}\")",
        "            >>> print(f\"  Co-occurrence: {stats['cooccurrence_connections']}\")",
        "        \"\"\"",
        "        stats = analysis.compute_bigram_connections(",
        "            self.layers,",
        "            min_shared_docs=min_shared_docs,",
        "            component_weight=component_weight,",
        "            chain_weight=chain_weight,",
        "            cooccurrence_weight=cooccurrence_weight",
        "        )",
        "        if verbose:",
        "            print(f\"Created {stats['connections_created']} bigram connections \"",
        "                  f\"(component: {stats['component_connections']}, \"",
        "                  f\"chain: {stats['chain_connections']}, \"",
        "                  f\"cooccur: {stats['cooccurrence_connections']})\")",
        "        return stats",
        ""
      ],
      "lines_removed": [
        "    def compute_all(self, verbose: bool = True, build_concepts: bool = True) -> None:",
        "        if verbose:",
        "            print(\"Computing importance (PageRank)...\")",
        "        self.compute_importance(verbose=False)",
        "    ",
        "    "
      ],
      "context_before": [
        "            if self.COMP_TFIDF in self._stale_computations:",
        "                self.compute_tfidf(verbose=verbose)",
        "                self._mark_fresh(self.COMP_TFIDF)",
        "                recomputed[self.COMP_TFIDF] = True",
        "",
        "            if self.COMP_DOC_CONNECTIONS in self._stale_computations:",
        "                self.compute_document_connections(verbose=verbose)",
        "                self._mark_fresh(self.COMP_DOC_CONNECTIONS)",
        "                recomputed[self.COMP_DOC_CONNECTIONS] = True",
        ""
      ],
      "context_after": [
        "            if self.COMP_CONCEPTS in self._stale_computations:",
        "                self.build_concept_clusters(verbose=verbose)",
        "                self._mark_fresh(self.COMP_CONCEPTS)",
        "                recomputed[self.COMP_CONCEPTS] = True",
        "",
        "            if self.COMP_EMBEDDINGS in self._stale_computations:",
        "                self.compute_graph_embeddings(verbose=verbose)",
        "                self._mark_fresh(self.COMP_EMBEDDINGS)",
        "                recomputed[self.COMP_EMBEDDINGS] = True",
        "",
        "            if self.COMP_SEMANTICS in self._stale_computations:",
        "                self.extract_corpus_semantics(verbose=verbose)",
        "                self._mark_fresh(self.COMP_SEMANTICS)",
        "                recomputed[self.COMP_SEMANTICS] = True",
        "",
        "        return recomputed",
        "",
        "        \"\"\"",
        "        Run all computation steps.",
        "",
        "        Args:",
        "            verbose: Print progress messages",
        "            build_concepts: Build concept clusters in Layer 2 (default True)",
        "                           This enables topic-based filtering and hierarchical search.",
        "        \"\"\"",
        "        if verbose:",
        "            print(\"Computing activation propagation...\")",
        "        self.propagate_activation(verbose=False)",
        "        if verbose:",
        "            print(\"Computing TF-IDF...\")",
        "        self.compute_tfidf(verbose=False)",
        "        if verbose:",
        "            print(\"Computing document connections...\")",
        "        self.compute_document_connections(verbose=False)",
        "        if build_concepts:",
        "            if verbose:",
        "                print(\"Building concept clusters...\")",
        "            self.build_concept_clusters(verbose=False)",
        "            if verbose:",
        "                print(\"Computing concept connections...\")",
        "            self.compute_concept_connections(verbose=False)",
        "        # Mark core computations as fresh",
        "        fresh_comps = [",
        "            self.COMP_ACTIVATION,",
        "            self.COMP_PAGERANK,",
        "            self.COMP_TFIDF,",
        "            self.COMP_DOC_CONNECTIONS,",
        "        ]",
        "        if build_concepts:",
        "            fresh_comps.append(self.COMP_CONCEPTS)",
        "        self._mark_fresh(*fresh_comps)",
        "        if verbose:",
        "            print(\"Done.\")",
        "    ",
        "    def propagate_activation(self, iterations: int = 3, decay: float = 0.8, verbose: bool = True) -> None:",
        "        analysis.propagate_activation(self.layers, iterations, decay)",
        "        if verbose: print(f\"Propagated activation ({iterations} iterations)\")",
        "    ",
        "    def compute_importance(self, verbose: bool = True) -> None:",
        "        for layer_enum in [CorticalLayer.TOKENS, CorticalLayer.BIGRAMS]:",
        "            analysis.compute_pagerank(self.layers[layer_enum])",
        "        if verbose: print(\"Computed PageRank importance\")",
        "    def compute_tfidf(self, verbose: bool = True) -> None:",
        "        analysis.compute_tfidf(self.layers, self.documents)",
        "        if verbose: print(\"Computed TF-IDF scores\")",
        "    ",
        "    def compute_document_connections(self, min_shared_terms: int = 3, verbose: bool = True) -> None:",
        "        analysis.compute_document_connections(self.layers, self.documents, min_shared_terms)",
        "        if verbose: print(\"Computed document connections\")",
        "    def build_concept_clusters(self, verbose: bool = True) -> Dict[int, List[str]]:",
        "        clusters = analysis.cluster_by_label_propagation(self.layers[CorticalLayer.TOKENS])",
        "        analysis.build_concept_clusters(self.layers, clusters)",
        "        if verbose: print(f\"Built {len(clusters)} concept clusters\")",
        "        return clusters",
        "",
        "    def compute_concept_connections(",
        "        self,",
        "        use_semantics: bool = True,",
        "        min_shared_docs: int = 1,"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/processor.py",
      "function": "class CorticalTextProcessor:",
      "start_line": 488,
      "lines_added": [
        "    def extract_corpus_semantics(",
        "        self,",
        "        use_pattern_extraction: bool = True,",
        "        min_pattern_confidence: float = 0.6,",
        "        verbose: bool = True",
        "    ) -> int:",
        "        \"\"\"",
        "        Extract semantic relations from the corpus.",
        "",
        "        Combines co-occurrence analysis with pattern-based extraction to discover",
        "        semantic relationships like IsA, HasA, UsedFor, Causes, etc.",
        "",
        "        Args:",
        "            use_pattern_extraction: Extract relations from text patterns (e.g., \"X is a Y\")",
        "            min_pattern_confidence: Minimum confidence for pattern-based relations",
        "            verbose: Print progress messages",
        "",
        "        Returns:",
        "            Number of relations extracted",
        "",
        "        Example:",
        "            >>> count = processor.extract_corpus_semantics(verbose=False)",
        "            >>> print(f\"Found {count} semantic relations\")",
        "        \"\"\"",
        "        self.semantic_relations = semantics.extract_corpus_semantics(",
        "            self.layers,",
        "            self.documents,",
        "            self.tokenizer,",
        "            use_pattern_extraction=use_pattern_extraction,",
        "            min_pattern_confidence=min_pattern_confidence",
        "        )",
        "        if verbose:",
        "            print(f\"Extracted {len(self.semantic_relations)} semantic relations\")",
        "",
        "    def extract_pattern_relations(",
        "        self,",
        "        min_confidence: float = 0.6,",
        "        verbose: bool = True",
        "    ) -> List[Tuple[str, str, str, float]]:",
        "        \"\"\"",
        "        Extract semantic relations using pattern matching only.",
        "",
        "        Uses regex patterns to identify commonsense relations from text patterns",
        "        like \"X is a type of Y\" → IsA, \"X is used for Y\" → UsedFor, etc.",
        "",
        "        Args:",
        "            min_confidence: Minimum confidence for extracted relations",
        "            verbose: Print progress messages",
        "",
        "        Returns:",
        "            List of (term1, relation_type, term2, confidence) tuples",
        "",
        "        Example:",
        "            >>> relations = processor.extract_pattern_relations(verbose=False)",
        "            >>> for t1, rel, t2, conf in relations[:5]:",
        "            ...     print(f\"{t1} --{rel}--> {t2} ({conf:.2f})\")",
        "        \"\"\"",
        "        layer0 = self.get_layer(CorticalLayer.TOKENS)",
        "        valid_terms = set(layer0.minicolumns.keys())",
        "",
        "        relations = semantics.extract_pattern_relations(",
        "            self.documents,",
        "            valid_terms,",
        "            min_confidence=min_confidence",
        "        )",
        "",
        "        if verbose:",
        "            stats = semantics.get_pattern_statistics(relations)",
        "            print(f\"Extracted {stats['total_relations']} pattern-based relations\")",
        "            print(f\"  Types: {stats['relation_type_counts']}\")",
        "",
        "        return relations",
        "",
        "    def compute_property_inheritance(",
        "        self,",
        "        decay_factor: float = 0.7,",
        "        max_depth: int = 5,",
        "        apply_to_connections: bool = True,",
        "        boost_factor: float = 0.3,",
        "        verbose: bool = True",
        "    ) -> Dict[str, Any]:",
        "        \"\"\"",
        "        Compute property inheritance based on IsA hierarchy.",
        "",
        "        If \"dog IsA animal\" and \"animal HasProperty living\", then \"dog\" inherits",
        "        \"living\" with a decayed weight. This enables similarity computation between",
        "        terms that share inherited properties.",
        "",
        "        Args:",
        "            decay_factor: Weight multiplier per inheritance level (default 0.7)",
        "            max_depth: Maximum inheritance depth (default 5)",
        "            apply_to_connections: Boost lateral connections for shared properties",
        "            boost_factor: Weight boost for shared inherited properties",
        "            verbose: Print progress messages",
        "",
        "        Returns:",
        "            Dict with statistics:",
        "            - terms_with_inheritance: Number of terms that inherited properties",
        "            - total_properties_inherited: Total property inheritance relationships",
        "            - connections_boosted: Connections boosted (if apply_to_connections=True)",
        "            - inherited: The full inheritance mapping (for advanced use)",
        "",
        "        Example:",
        "            >>> processor.extract_corpus_semantics()",
        "            >>> stats = processor.compute_property_inheritance()",
        "            >>> print(f\"{stats['terms_with_inheritance']} terms inherited properties\")",
        "            >>>",
        "            >>> # Check inherited properties for a term",
        "            >>> inherited = stats['inherited']",
        "            >>> if 'dog' in inherited:",
        "            ...     for prop, (weight, source, depth) in inherited['dog'].items():",
        "            ...         print(f\"  {prop}: {weight:.2f} (from {source}, depth {depth})\")",
        "        \"\"\"",
        "        if not self.semantic_relations:",
        "            self.extract_corpus_semantics(verbose=False)",
        "",
        "        inherited = semantics.inherit_properties(",
        "            self.semantic_relations,",
        "            decay_factor=decay_factor,",
        "            max_depth=max_depth",
        "        )",
        "",
        "        total_props = sum(len(props) for props in inherited.values())",
        "",
        "        result = {",
        "            'terms_with_inheritance': len(inherited),",
        "            'total_properties_inherited': total_props,",
        "            'inherited': inherited",
        "        }",
        "",
        "        if apply_to_connections and inherited:",
        "            conn_stats = semantics.apply_inheritance_to_connections(",
        "                self.layers,",
        "                inherited,",
        "                boost_factor=boost_factor",
        "            )",
        "            result['connections_boosted'] = conn_stats['connections_boosted']",
        "            result['total_boost'] = conn_stats['total_boost']",
        "        else:",
        "            result['connections_boosted'] = 0",
        "            result['total_boost'] = 0.0",
        "",
        "        if verbose:",
        "            print(f\"Computed property inheritance: {result['terms_with_inheritance']} terms, \"",
        "                  f\"{total_props} properties, {result['connections_boosted']} connections boosted\")",
        "",
        "        return result",
        "",
        "    def compute_property_similarity(self, term1: str, term2: str) -> float:",
        "        \"\"\"",
        "        Compute similarity between terms based on shared properties (direct + inherited).",
        "",
        "        Requires that compute_property_inheritance() or extract_corpus_semantics()",
        "        has been called first.",
        "",
        "        Args:",
        "            term1: First term",
        "            term2: Second term",
        "",
        "        Returns:",
        "            Similarity score (0.0-1.0) based on Jaccard-like overlap of properties",
        "",
        "        Example:",
        "            >>> processor.extract_corpus_semantics()",
        "            >>> stats = processor.compute_property_inheritance()",
        "            >>> sim = processor.compute_property_similarity(\"dog\", \"cat\")",
        "            >>> # Both inherit \"living\" from \"animal\", so similarity > 0",
        "        \"\"\"",
        "        if not self.semantic_relations:",
        "            return 0.0",
        "",
        "        # Compute inherited properties on the fly if needed",
        "        inherited = semantics.inherit_properties(self.semantic_relations)",
        "",
        "        return semantics.compute_property_similarity(term1, term2, inherited)"
      ],
      "lines_removed": [
        "    def extract_corpus_semantics(self, verbose: bool = True) -> int:",
        "        self.semantic_relations = semantics.extract_corpus_semantics(self.layers, self.documents, self.tokenizer)",
        "        if verbose: print(f\"Extracted {len(self.semantic_relations)} semantic relations\")"
      ],
      "context_before": [
        "        stats = analysis.compute_concept_connections(",
        "            self.layers,",
        "            semantic_relations=semantic_rels,",
        "            min_shared_docs=min_shared_docs,",
        "            min_jaccard=min_jaccard",
        "        )",
        "        if verbose:",
        "            print(f\"Created {stats['connections_created']} concept connections\")",
        "        return stats",
        ""
      ],
      "context_after": [
        "        return len(self.semantic_relations)",
        "    ",
        "    def retrofit_connections(self, iterations: int = 10, alpha: float = 0.3, verbose: bool = True) -> Dict:",
        "        if not self.semantic_relations: self.extract_corpus_semantics(verbose=False)",
        "        stats = semantics.retrofit_connections(self.layers, self.semantic_relations, iterations, alpha)",
        "        if verbose: print(f\"Retrofitted {stats['tokens_affected']} tokens\")",
        "        return stats",
        "    ",
        "    def compute_graph_embeddings(self, dimensions: int = 64, method: str = 'adjacency', verbose: bool = True) -> Dict:",
        "        self.embeddings, stats = emb_module.compute_graph_embeddings(self.layers, dimensions, method)",
        "        if verbose: print(f\"Computed {stats['terms_embedded']} embeddings ({method})\")",
        "        return stats",
        "    ",
        "    def retrofit_embeddings(self, iterations: int = 10, alpha: float = 0.4, verbose: bool = True) -> Dict:",
        "        if not self.embeddings: self.compute_graph_embeddings(verbose=False)",
        "        if not self.semantic_relations: self.extract_corpus_semantics(verbose=False)",
        "        stats = semantics.retrofit_embeddings(self.embeddings, self.semantic_relations, iterations, alpha)"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/processor.py",
      "function": "class CorticalTextProcessor:",
      "start_line": 522,
      "lines_added": [
        "",
        "    def complete_analogy(",
        "        self,",
        "        term_a: str,",
        "        term_b: str,",
        "        term_c: str,",
        "        top_n: int = 5,",
        "        use_embeddings: bool = True,",
        "        use_relations: bool = True",
        "    ) -> List[Tuple[str, float, str]]:",
        "        \"\"\"",
        "        Complete an analogy: \"a is to b as c is to ?\"",
        "",
        "        Uses multiple strategies to find the best completion:",
        "        1. Relation matching: Find what relation connects a→b, then find terms",
        "           with the same relation from c",
        "        2. Vector arithmetic: Use embeddings to compute d = c + (b - a)",
        "        3. Pattern matching: Find terms that co-occur with c similarly to how",
        "           b co-occurs with a",
        "",
        "        Args:",
        "            term_a: First term of the known pair (e.g., \"king\")",
        "            term_b: Second term of the known pair (e.g., \"queen\")",
        "            term_c: First term of the query pair (e.g., \"man\")",
        "            top_n: Number of candidates to return",
        "            use_embeddings: Whether to use embedding-based completion",
        "            use_relations: Whether to use relation-based completion",
        "",
        "        Returns:",
        "            List of (candidate_term, confidence, method) tuples, where method",
        "            describes which approach found this candidate ('relation:IsA',",
        "            'embedding', 'pattern')",
        "",
        "        Example:",
        "            >>> processor.extract_corpus_semantics()",
        "            >>> processor.compute_graph_embeddings()",
        "            >>> results = processor.complete_analogy(\"neural\", \"networks\", \"knowledge\")",
        "            >>> for term, score, method in results:",
        "            ...     print(f\"{term}: {score:.3f} ({method})\")",
        "        \"\"\"",
        "        if not self.semantic_relations:",
        "            self.extract_corpus_semantics(verbose=False)",
        "",
        "        return query_module.complete_analogy(",
        "            term_a, term_b, term_c,",
        "            self.layers,",
        "            self.semantic_relations,",
        "            embeddings=self.embeddings,",
        "            top_n=top_n,",
        "            use_embeddings=use_embeddings,",
        "            use_relations=use_relations",
        "        )",
        "",
        "    def complete_analogy_simple(",
        "        self,",
        "        term_a: str,",
        "        term_b: str,",
        "        term_c: str,",
        "        top_n: int = 5",
        "    ) -> List[Tuple[str, float]]:",
        "        \"\"\"",
        "        Simplified analogy completion using only term relationships.",
        "",
        "        A lighter version that doesn't require embeddings. Uses bigram patterns",
        "        and co-occurrence to find analogies.",
        "",
        "        Args:",
        "            term_a: First term of the known pair",
        "            term_b: Second term of the known pair",
        "            term_c: First term of the query pair",
        "            top_n: Number of candidates to return",
        "",
        "        Returns:",
        "            List of (candidate_term, confidence) tuples",
        "",
        "        Example:",
        "            >>> results = processor.complete_analogy_simple(\"neural\", \"networks\", \"knowledge\")",
        "            >>> for term, score in results:",
        "            ...     print(f\"{term}: {score:.3f}\")",
        "        \"\"\"",
        "        return query_module.complete_analogy_simple(",
        "            term_a, term_b, term_c,",
        "            self.layers,",
        "            self.tokenizer,",
        "            semantic_relations=self.semantic_relations,",
        "            top_n=top_n",
        "        )",
        "",
        "    def expand_query_multihop(",
        "        self,",
        "        query_text: str,",
        "        max_hops: int = 2,",
        "        max_expansions: int = 15,",
        "        decay_factor: float = 0.5,",
        "        min_path_score: float = 0.2",
        "    ) -> Dict[str, float]:",
        "        \"\"\"",
        "        Expand query using multi-hop semantic inference.",
        "",
        "        Unlike single-hop expansion that only follows direct connections,",
        "        this follows relation chains to discover semantically related terms",
        "        through transitive relationships.",
        "",
        "        Example inference chains:",
        "            \"dog\" → IsA → \"animal\" → HasProperty → \"living\"",
        "            \"neural\" → RelatedTo → \"network\" → RelatedTo → \"deep\"",
        "",
        "        Args:",
        "            query_text: Original query string",
        "            max_hops: Maximum number of relation hops (default: 2)",
        "            max_expansions: Maximum expansion terms to return",
        "            decay_factor: Weight decay per hop (default: 0.5, so hop2 = 0.25)",
        "            min_path_score: Minimum path validity score to include (default: 0.2)",
        "",
        "        Returns:",
        "            Dict mapping terms to weights (original terms get weight 1.0,",
        "            expansions get decayed weights based on hop distance and path validity)",
        "",
        "        Example:",
        "            >>> # Extract semantic relations first",
        "            >>> processor.extract_corpus_semantics()",
        "            >>>",
        "            >>> # Multi-hop expansion",
        "            >>> expanded = processor.expand_query_multihop(\"neural\", max_hops=2)",
        "            >>> for term, weight in sorted(expanded.items(), key=lambda x: -x[1]):",
        "            ...     print(f\"{term}: {weight:.3f}\")",
        "        \"\"\"",
        "        if not self.semantic_relations:",
        "            # Fall back to regular expansion if no semantic relations",
        "            return self.expand_query(query_text, max_expansions=max_expansions)",
        "",
        "        return query_module.expand_query_multihop(",
        "            query_text,",
        "            self.layers,",
        "            self.tokenizer,",
        "            self.semantic_relations,",
        "            max_hops=max_hops,",
        "            max_expansions=max_expansions,",
        "            decay_factor=decay_factor,",
        "            min_path_score=min_path_score",
        "        )"
      ],
      "lines_removed": [],
      "context_before": [
        "        return emb_module.embedding_similarity(self.embeddings, term1, term2)",
        "    ",
        "    def find_similar_by_embedding(self, term: str, top_n: int = 10) -> List[Tuple[str, float]]:",
        "        return emb_module.find_similar_by_embedding(self.embeddings, term, top_n)",
        "    ",
        "    def expand_query(self, query_text: str, max_expansions: int = 10, use_variants: bool = True, verbose: bool = False) -> Dict[str, float]:",
        "        return query_module.expand_query(query_text, self.layers, self.tokenizer, max_expansions=max_expansions, use_variants=use_variants)",
        "    ",
        "    def expand_query_semantic(self, query_text: str, max_expansions: int = 10) -> Dict[str, float]:",
        "        return query_module.expand_query_semantic(query_text, self.layers, self.tokenizer, self.semantic_relations, max_expansions)"
      ],
      "context_after": [
        "    ",
        "    def find_documents_for_query(",
        "        self,",
        "        query_text: str,",
        "        top_n: int = 5,",
        "        use_expansion: bool = True,",
        "        use_semantic: bool = True",
        "    ) -> List[Tuple[str, float]]:",
        "        \"\"\"",
        "        Find documents most relevant to a query."
      ],
      "change_type": "add"
    },
    {
      "file": "cortical/processor.py",
      "function": "class CorticalTextProcessor:",
      "start_line": 845,
      "lines_added": [
        "",
        "    def export_conceptnet_json(",
        "        self,",
        "        filepath: str,",
        "        include_cross_layer: bool = True,",
        "        include_typed_edges: bool = True,",
        "        min_weight: float = 0.0,",
        "        min_confidence: float = 0.0,",
        "        max_nodes_per_layer: int = 100,",
        "        verbose: bool = True",
        "    ) -> Dict[str, Any]:",
        "        \"\"\"",
        "        Export ConceptNet-style graph for visualization.",
        "",
        "        Creates a rich graph format with:",
        "        - Color-coded nodes by layer (tokens=blue, bigrams=green, concepts=orange, docs=red)",
        "        - Typed edges with relation types and confidence scores",
        "        - Cross-layer connections (feedforward/feedback)",
        "        - D3.js/Cytoscape-compatible output",
        "",
        "        Args:",
        "            filepath: Output file path (JSON)",
        "            include_cross_layer: Include feedforward/feedback edges",
        "            include_typed_edges: Include typed_connections with relation types",
        "            min_weight: Minimum edge weight to include",
        "            min_confidence: Minimum confidence for typed edges",
        "            max_nodes_per_layer: Maximum nodes per layer (by PageRank)",
        "            verbose: Print progress messages",
        "",
        "        Returns:",
        "            The exported graph data",
        "",
        "        Example:",
        "            >>> processor.extract_corpus_semantics(verbose=False)",
        "            >>> graph = processor.export_conceptnet_json(\"graph.json\")",
        "            >>> # Open graph.json in D3.js or Cytoscape for visualization",
        "        \"\"\"",
        "        return persistence.export_conceptnet_json(",
        "            filepath,",
        "            self.layers,",
        "            semantic_relations=self.semantic_relations,",
        "            include_cross_layer=include_cross_layer,",
        "            include_typed_edges=include_typed_edges,",
        "            min_weight=min_weight,",
        "            min_confidence=min_confidence,",
        "            max_nodes_per_layer=max_nodes_per_layer,",
        "            verbose=verbose",
        "        )",
        ""
      ],
      "lines_removed": [
        "    "
      ],
      "context_before": [
        "        processor = cls()",
        "        processor.layers = layers",
        "        processor.documents = documents",
        "        processor.document_metadata = document_metadata",
        "        processor.embeddings = embeddings",
        "        processor.semantic_relations = semantic_relations",
        "        return processor",
        "    ",
        "    def export_graph(self, filepath: str, layer: Optional[CorticalLayer] = None, max_nodes: int = 500) -> Dict:",
        "        return persistence.export_graph_json(filepath, self.layers, layer, max_nodes=max_nodes)"
      ],
      "context_after": [
        "    def summarize_document(self, doc_id: str, num_sentences: int = 3) -> str:",
        "        if doc_id not in self.documents: return \"\"",
        "        content = self.documents[doc_id]",
        "        sentences = re.split(r'(?<=[.!?])\\s+', content)",
        "        if len(sentences) <= num_sentences: return content",
        "        ",
        "        layer0 = self.layers[CorticalLayer.TOKENS]",
        "        scored = []",
        "        for sent in sentences:",
        "            tokens = self.tokenizer.tokenize(sent)"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/query.py",
      "function": "def expand_query(",
      "start_line": 117,
      "lines_added": [
        "# Valid relation chain patterns for multi-hop inference",
        "# Key: (relation1, relation2) → validity score (0.0 = invalid, 1.0 = fully valid)",
        "VALID_RELATION_CHAINS = {",
        "    # Transitive hierarchies",
        "    ('IsA', 'IsA'): 1.0,           # dog IsA animal IsA living_thing",
        "    ('PartOf', 'PartOf'): 1.0,     # wheel PartOf car PartOf vehicle",
        "    ('IsA', 'HasProperty'): 0.9,   # dog IsA animal HasProperty alive",
        "    ('PartOf', 'HasProperty'): 0.8,  # wheel PartOf car HasProperty fast",
        "",
        "    # Association chains",
        "    ('RelatedTo', 'RelatedTo'): 0.6,",
        "    ('SimilarTo', 'SimilarTo'): 0.7,",
        "    ('CoOccurs', 'CoOccurs'): 0.5,",
        "    ('RelatedTo', 'IsA'): 0.7,",
        "    ('RelatedTo', 'SimilarTo'): 0.7,",
        "",
        "    # Causal chains",
        "    ('Causes', 'Causes'): 0.8,",
        "    ('Causes', 'HasProperty'): 0.7,",
        "",
        "    # Derivation chains",
        "    ('DerivedFrom', 'DerivedFrom'): 0.8,",
        "    ('DerivedFrom', 'IsA'): 0.7,",
        "",
        "    # Usage chains",
        "    ('UsedFor', 'UsedFor'): 0.6,",
        "    ('UsedFor', 'RelatedTo'): 0.5,",
        "",
        "    # Antonym - generally invalid for chaining",
        "    ('Antonym', 'Antonym'): 0.3,   # Double negation, weak",
        "    ('Antonym', 'IsA'): 0.1,       # Contradictory",
        "}",
        "",
        "",
        "def score_relation_path(path: List[str]) -> float:",
        "    \"\"\"",
        "    Score a relation path by its semantic coherence.",
        "",
        "    Args:",
        "        path: List of relation types traversed (e.g., ['IsA', 'HasProperty'])",
        "",
        "    Returns:",
        "        Score from 0.0 (invalid) to 1.0 (fully valid)",
        "    \"\"\"",
        "    if not path:",
        "        return 1.0",
        "    if len(path) == 1:",
        "        return 1.0",
        "",
        "    # Compute score as product of consecutive pair validities",
        "    total_score = 1.0",
        "    for i in range(len(path) - 1):",
        "        pair = (path[i], path[i + 1])",
        "        # Check both orderings",
        "        pair_score = VALID_RELATION_CHAINS.get(pair, 0.4)  # Default: moderate validity",
        "        total_score *= pair_score",
        "",
        "    return total_score",
        "",
        "",
        "def expand_query_multihop(",
        "    query_text: str,",
        "    layers: Dict[CorticalLayer, HierarchicalLayer],",
        "    tokenizer: Tokenizer,",
        "    semantic_relations: List[Tuple[str, str, str, float]],",
        "    max_hops: int = 2,",
        "    max_expansions: int = 15,",
        "    decay_factor: float = 0.5,",
        "    min_path_score: float = 0.2",
        ") -> Dict[str, float]:",
        "    \"\"\"",
        "    Expand query using multi-hop semantic inference.",
        "",
        "    Unlike single-hop expansion that only follows direct connections,",
        "    this follows relation chains to discover semantically related terms",
        "    through transitive relationships.",
        "",
        "    Example inference chains:",
        "        \"dog\" → IsA → \"animal\" → HasProperty → \"living\"",
        "        \"car\" → PartOf → \"engine\" → UsedFor → \"transportation\"",
        "",
        "    Args:",
        "        query_text: Original query string",
        "        layers: Dictionary of layers (needs TOKENS)",
        "        tokenizer: Tokenizer instance",
        "        semantic_relations: List of (term1, relation, term2, weight) tuples",
        "        max_hops: Maximum number of relation hops (default: 2)",
        "        max_expansions: Maximum expansion terms to return",
        "        decay_factor: Weight decay per hop (default: 0.5, so hop2 = 0.25)",
        "        min_path_score: Minimum path validity score to include (default: 0.2)",
        "",
        "    Returns:",
        "        Dict mapping terms to weights (original terms get weight 1.0,",
        "        expansions get decayed weights based on hop distance and path validity)",
        "",
        "    Example:",
        "        >>> expanded = expand_query_multihop(\"neural\", layers, tokenizer, relations)",
        "        >>> # Hop 1: networks (co-occur), learning (co-occur), brain (RelatedTo)",
        "        >>> # Hop 2: deep (via learning), cortex (via brain), AI (via networks)",
        "    \"\"\"",
        "    tokens = tokenizer.tokenize(query_text)",
        "    layer0 = layers[CorticalLayer.TOKENS]",
        "",
        "    # Start with original terms at full weight",
        "    expanded: Dict[str, float] = {}",
        "    for token in tokens:",
        "        if layer0.get_minicolumn(token):",
        "            expanded[token] = 1.0",
        "",
        "    if not expanded or not semantic_relations:",
        "        return expanded",
        "",
        "    # Build bidirectional neighbor lookup with relation types",
        "    # neighbors[term] = [(neighbor, relation_type, weight), ...]",
        "    neighbors: Dict[str, List[Tuple[str, str, float]]] = defaultdict(list)",
        "    for t1, relation, t2, weight in semantic_relations:",
        "        neighbors[t1].append((t2, relation, weight))",
        "        neighbors[t2].append((t1, relation, weight))",
        "",
        "    # Track expansions with their hop distance, weight, and relation path",
        "    # (term, weight, hop, relation_path)",
        "    candidates: Dict[str, Tuple[float, int, List[str]]] = {}",
        "",
        "    # BFS-style expansion with hop tracking",
        "    # frontier: [(term, current_weight, hop_count, relation_path)]",
        "    frontier: List[Tuple[str, float, int, List[str]]] = [",
        "        (term, 1.0, 0, []) for term in expanded.keys()",
        "    ]",
        "",
        "    visited_at_hop: Dict[str, int] = {term: 0 for term in expanded.keys()}",
        "",
        "    while frontier:",
        "        current_term, current_weight, hop, path = frontier.pop(0)",
        "",
        "        if hop >= max_hops:",
        "            continue",
        "",
        "        next_hop = hop + 1",
        "",
        "        for neighbor, relation, rel_weight in neighbors.get(current_term, []):",
        "            # Skip if already in original query terms",
        "            if neighbor in expanded:",
        "                continue",
        "",
        "            # Check if term exists in corpus",
        "            if not layer0.get_minicolumn(neighbor):",
        "                continue",
        "",
        "            # Skip if we've visited this term at an earlier or equal hop",
        "            if neighbor in visited_at_hop and visited_at_hop[neighbor] <= next_hop:",
        "                continue",
        "",
        "            # Compute new path and its validity",
        "            new_path = path + [relation]",
        "            path_score = score_relation_path(new_path)",
        "",
        "            if path_score < min_path_score:",
        "                continue",
        "",
        "            # Compute weight with decay and path validity",
        "            # weight = base_weight * relation_weight * decay^hop * path_validity",
        "            hop_decay = decay_factor ** next_hop",
        "            new_weight = current_weight * rel_weight * hop_decay * path_score",
        "",
        "            # Update candidate if this path gives higher weight",
        "            if neighbor not in candidates or candidates[neighbor][0] < new_weight:",
        "                candidates[neighbor] = (new_weight, next_hop, new_path)",
        "                visited_at_hop[neighbor] = next_hop",
        "",
        "                # Add to frontier for further expansion",
        "                if next_hop < max_hops:",
        "                    frontier.append((neighbor, new_weight, next_hop, new_path))",
        "",
        "    # Sort candidates by weight and take top expansions",
        "    sorted_candidates = sorted(",
        "        candidates.items(),",
        "        key=lambda x: x[1][0],  # Sort by weight",
        "        reverse=True",
        "    )[:max_expansions]",
        "",
        "    # Add to expanded dict",
        "    for term, (weight, hop, path) in sorted_candidates:",
        "        expanded[term] = weight",
        "",
        "    return expanded",
        "",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "        key=lambda x: x[1],",
        "        reverse=True",
        "    )[:max_expansions]",
        "    ",
        "    for term, score in sorted_candidates:",
        "        expanded[term] = score",
        "    ",
        "    return expanded",
        "",
        ""
      ],
      "context_after": [
        "def expand_query_semantic(",
        "    query_text: str,",
        "    layers: Dict[CorticalLayer, HierarchicalLayer],",
        "    tokenizer: Tokenizer,",
        "    semantic_relations: List[Tuple[str, str, str, float]],",
        "    max_expansions: int = 10",
        ") -> Dict[str, float]:",
        "    \"\"\"",
        "    Expand query using semantic relations extracted from corpus.",
        "    "
      ],
      "change_type": "add"
    },
    {
      "file": "cortical/query.py",
      "function": "def multi_stage_rank_documents(",
      "start_line": 986,
      "lines_added": [
        "",
        "",
        "def find_relation_between(",
        "    term_a: str,",
        "    term_b: str,",
        "    semantic_relations: List[Tuple[str, str, str, float]]",
        ") -> List[Tuple[str, float]]:",
        "    \"\"\"",
        "    Find semantic relations between two terms.",
        "",
        "    Args:",
        "        term_a: Source term",
        "        term_b: Target term",
        "        semantic_relations: List of (t1, relation, t2, weight) tuples",
        "",
        "    Returns:",
        "        List of (relation_type, weight) tuples",
        "    \"\"\"",
        "    relations = []",
        "    for t1, rel_type, t2, weight in semantic_relations:",
        "        if t1 == term_a and t2 == term_b:",
        "            relations.append((rel_type, weight))",
        "        elif t2 == term_a and t1 == term_b:",
        "            # Reverse direction",
        "            relations.append((rel_type, weight * 0.9))  # Slight penalty for reverse",
        "",
        "    return sorted(relations, key=lambda x: x[1], reverse=True)",
        "",
        "",
        "def find_terms_with_relation(",
        "    term: str,",
        "    relation_type: str,",
        "    semantic_relations: List[Tuple[str, str, str, float]],",
        "    direction: str = 'forward'",
        ") -> List[Tuple[str, float]]:",
        "    \"\"\"",
        "    Find terms connected to a given term by a specific relation type.",
        "",
        "    Args:",
        "        term: Source term",
        "        relation_type: Type of relation to follow",
        "        semantic_relations: List of (t1, relation, t2, weight) tuples",
        "        direction: 'forward' (term→x) or 'backward' (x→term)",
        "",
        "    Returns:",
        "        List of (target_term, weight) tuples",
        "    \"\"\"",
        "    results = []",
        "    for t1, rel_type, t2, weight in semantic_relations:",
        "        if rel_type != relation_type:",
        "            continue",
        "",
        "        if direction == 'forward' and t1 == term:",
        "            results.append((t2, weight))",
        "        elif direction == 'backward' and t2 == term:",
        "            results.append((t1, weight))",
        "",
        "    return sorted(results, key=lambda x: x[1], reverse=True)",
        "",
        "",
        "def complete_analogy(",
        "    term_a: str,",
        "    term_b: str,",
        "    term_c: str,",
        "    layers: Dict[CorticalLayer, HierarchicalLayer],",
        "    semantic_relations: List[Tuple[str, str, str, float]],",
        "    embeddings: Optional[Dict[str, List[float]]] = None,",
        "    top_n: int = 5,",
        "    use_embeddings: bool = True,",
        "    use_relations: bool = True",
        ") -> List[Tuple[str, float, str]]:",
        "    \"\"\"",
        "    Complete an analogy: \"a is to b as c is to ?\"",
        "",
        "    Uses multiple strategies to find the best completion:",
        "    1. Relation matching: Find what relation connects a→b, then find terms with",
        "       the same relation from c",
        "    2. Vector arithmetic: Use embeddings to compute d = c + (b - a)",
        "    3. Pattern matching: Find terms that co-occur with c similar to how b co-occurs with a",
        "",
        "    Example:",
        "        \"neural\" is to \"networks\" as \"knowledge\" is to ?",
        "        → \"graphs\" (both form compound technical terms with similar structure)",
        "",
        "    Args:",
        "        term_a: First term of the known pair",
        "        term_b: Second term of the known pair",
        "        term_c: First term of the query pair",
        "        layers: Dictionary of layers",
        "        semantic_relations: List of (t1, relation, t2, weight) tuples",
        "        embeddings: Optional graph embeddings for vector arithmetic",
        "        top_n: Number of candidates to return",
        "        use_embeddings: Whether to use embedding-based completion",
        "        use_relations: Whether to use relation-based completion",
        "",
        "    Returns:",
        "        List of (candidate_term, confidence, method) tuples, where method describes",
        "        which approach found this candidate ('relation', 'embedding', 'pattern')",
        "    \"\"\"",
        "    layer0 = layers[CorticalLayer.TOKENS]",
        "    candidates: Dict[str, Tuple[float, str]] = {}  # term → (score, method)",
        "",
        "    # Check that terms exist",
        "    if not layer0.get_minicolumn(term_a) or not layer0.get_minicolumn(term_b):",
        "        return []",
        "    if not layer0.get_minicolumn(term_c):",
        "        return []",
        "",
        "    # Strategy 1: Relation-based completion",
        "    if use_relations and semantic_relations:",
        "        # Find relation between a and b",
        "        relations_ab = find_relation_between(term_a, term_b, semantic_relations)",
        "",
        "        for rel_type, rel_weight in relations_ab:",
        "            # Find terms with same relation from c",
        "            c_targets = find_terms_with_relation(",
        "                term_c, rel_type, semantic_relations, direction='forward'",
        "            )",
        "",
        "            for target, target_weight in c_targets:",
        "                # Don't include the input terms",
        "                if target in {term_a, term_b, term_c}:",
        "                    continue",
        "",
        "                score = rel_weight * target_weight",
        "                if target not in candidates or candidates[target][0] < score:",
        "                    candidates[target] = (score, f'relation:{rel_type}')",
        "",
        "    # Strategy 2: Embedding-based completion (vector arithmetic)",
        "    if use_embeddings and embeddings:",
        "        if term_a in embeddings and term_b in embeddings and term_c in embeddings:",
        "            vec_a = embeddings[term_a]",
        "            vec_b = embeddings[term_b]",
        "            vec_c = embeddings[term_c]",
        "",
        "            # d = c + (b - a)  (the analogy vector)",
        "            vec_d = [",
        "                c + (b - a)",
        "                for a, b, c in zip(vec_a, vec_b, vec_c)",
        "            ]",
        "",
        "            # Find nearest terms to vec_d",
        "            best_matches = []",
        "            for term, vec in embeddings.items():",
        "                if term in {term_a, term_b, term_c}:",
        "                    continue",
        "",
        "                # Cosine similarity",
        "                dot = sum(d * v for d, v in zip(vec_d, vec))",
        "                mag_d = sum(d * d for d in vec_d) ** 0.5",
        "                mag_v = sum(v * v for v in vec) ** 0.5",
        "",
        "                if mag_d > 0 and mag_v > 0:",
        "                    similarity = dot / (mag_d * mag_v)",
        "                    best_matches.append((term, similarity))",
        "",
        "            # Sort by similarity and add to candidates",
        "            best_matches.sort(key=lambda x: x[1], reverse=True)",
        "            for term, sim in best_matches[:top_n * 2]:",
        "                if sim > 0.5:  # Only include reasonably similar terms",
        "                    if term not in candidates or candidates[term][0] < sim:",
        "                        candidates[term] = (sim, 'embedding')",
        "",
        "    # Strategy 3: Pattern matching (co-occurrence structure)",
        "    col_a = layer0.get_minicolumn(term_a)",
        "    col_b = layer0.get_minicolumn(term_b)",
        "    col_c = layer0.get_minicolumn(term_c)",
        "",
        "    if col_a and col_b and col_c:",
        "        # Find terms that relate to c similarly to how b relates to a",
        "        # I.e., if b co-occurs strongly with a, find terms that co-occur strongly with c",
        "",
        "        a_neighbors = set(col_a.lateral_connections.keys())",
        "        c_neighbors = set(col_c.lateral_connections.keys())",
        "",
        "        # Look at c's neighbors that aren't a's neighbors (new context)",
        "        for neighbor_id in c_neighbors:",
        "            neighbor = layer0.get_by_id(neighbor_id)",
        "            if not neighbor:",
        "                continue",
        "",
        "            term = neighbor.content",
        "            if term in {term_a, term_b, term_c}:",
        "                continue",
        "",
        "            # Score based on how similar the neighbor's connection to c is",
        "            # compared to b's connection to a",
        "            c_weight = col_c.lateral_connections.get(neighbor_id, 0)",
        "            b_to_a_weight = col_a.lateral_connections.get(col_b.id, 0)",
        "",
        "            if c_weight > 0 and b_to_a_weight > 0:",
        "                # The term should have similar connection strength pattern",
        "                score = min(c_weight, b_to_a_weight) * 0.5",
        "                if score > 0.1:",
        "                    if term not in candidates or candidates[term][0] < score:",
        "                        candidates[term] = (score, 'pattern')",
        "",
        "    # Sort and return top candidates",
        "    results = [",
        "        (term, score, method)",
        "        for term, (score, method) in candidates.items()",
        "    ]",
        "    results.sort(key=lambda x: x[1], reverse=True)",
        "",
        "    return results[:top_n]",
        "",
        "",
        "def complete_analogy_simple(",
        "    term_a: str,",
        "    term_b: str,",
        "    term_c: str,",
        "    layers: Dict[CorticalLayer, HierarchicalLayer],",
        "    tokenizer: Tokenizer,",
        "    semantic_relations: Optional[List[Tuple[str, str, str, float]]] = None,",
        "    top_n: int = 5",
        ") -> List[Tuple[str, float]]:",
        "    \"\"\"",
        "    Simplified analogy completion using only term relationships.",
        "",
        "    A lighter version of complete_analogy that doesn't require embeddings.",
        "    Uses bigram patterns and co-occurrence to find analogies.",
        "",
        "    Example:",
        "        \"neural\" is to \"networks\" as \"knowledge\" is to ?",
        "        → Looks for terms that form similar bigrams with \"knowledge\"",
        "",
        "    Args:",
        "        term_a: First term of the known pair",
        "        term_b: Second term of the known pair",
        "        term_c: First term of the query pair",
        "        layers: Dictionary of layers",
        "        tokenizer: Tokenizer instance",
        "        semantic_relations: Optional semantic relations",
        "        top_n: Number of candidates to return",
        "",
        "    Returns:",
        "        List of (candidate_term, confidence) tuples",
        "    \"\"\"",
        "    layer0 = layers[CorticalLayer.TOKENS]",
        "    layer1 = layers.get(CorticalLayer.BIGRAMS)",
        "",
        "    candidates: Dict[str, float] = {}",
        "",
        "    col_a = layer0.get_minicolumn(term_a)",
        "    col_b = layer0.get_minicolumn(term_b)",
        "    col_c = layer0.get_minicolumn(term_c)",
        "",
        "    if not col_a or not col_b or not col_c:",
        "        return []",
        "",
        "    # Strategy 1: Bigram pattern matching",
        "    if layer1:",
        "        # Find bigrams containing a_b pattern",
        "        ab_bigram = f\"{term_a}_{term_b}\"",
        "        ba_bigram = f\"{term_b}_{term_a}\"",
        "",
        "        ab_col = layer1.get_minicolumn(ab_bigram)",
        "        ba_col = layer1.get_minicolumn(ba_bigram)",
        "",
        "        # If a_b is a bigram, look for c_? bigrams",
        "        if ab_col or ba_col:",
        "            for bigram_col in layer1.minicolumns.values():",
        "                bigram = bigram_col.content",
        "                parts = bigram.split('_')",
        "                if len(parts) != 2:",
        "                    continue",
        "",
        "                first, second = parts",
        "",
        "                # Look for bigrams starting with c",
        "                if first == term_c and second not in {term_a, term_b, term_c}:",
        "                    score = bigram_col.pagerank * 0.8",
        "                    if second not in candidates or candidates[second] < score:",
        "                        candidates[second] = score",
        "",
        "                # Look for bigrams ending with c",
        "                if second == term_c and first not in {term_a, term_b, term_c}:",
        "                    score = bigram_col.pagerank * 0.6",
        "                    if first not in candidates or candidates[first] < score:",
        "                        candidates[first] = score",
        "",
        "    # Strategy 2: Co-occurrence similarity",
        "    # Find terms that co-occur with c like b co-occurs with a",
        "    a_neighbors = col_a.lateral_connections",
        "    c_neighbors = col_c.lateral_connections",
        "",
        "    for neighbor_id, c_weight in c_neighbors.items():",
        "        neighbor = layer0.get_by_id(neighbor_id)",
        "        if not neighbor:",
        "            continue",
        "",
        "        term = neighbor.content",
        "        if term in {term_a, term_b, term_c}:",
        "            continue",
        "",
        "        # Check if this term has similar connection pattern",
        "        score = c_weight * 0.3",
        "        if score > 0.05:",
        "            candidates[term] = candidates.get(term, 0) + score",
        "",
        "    # Strategy 3: Semantic relations (if available)",
        "    if semantic_relations:",
        "        relations_ab = find_relation_between(term_a, term_b, semantic_relations)",
        "        for rel_type, rel_weight in relations_ab[:2]:  # Top 2 relations",
        "            c_targets = find_terms_with_relation(",
        "                term_c, rel_type, semantic_relations, direction='forward'",
        "            )",
        "            for target, target_weight in c_targets[:3]:  # Top 3 targets",
        "                if target not in {term_a, term_b, term_c}:",
        "                    score = rel_weight * target_weight",
        "                    candidates[target] = candidates.get(target, 0) + score",
        "",
        "    # Sort and return",
        "    results = sorted(candidates.items(), key=lambda x: x[1], reverse=True)",
        "    return results[:top_n]"
      ],
      "lines_removed": [],
      "context_before": [
        "",
        "        stage_scores = {",
        "            'concept_score': concept_score,",
        "            'tfidf_score': tfidf_score,",
        "            'combined_score': combined",
        "        }",
        "        results.append((doc_id, combined, stage_scores))",
        "",
        "    results.sort(key=lambda x: x[1], reverse=True)",
        "    return results[:top_n]"
      ],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "cortical/semantics.py",
      "function": "RELATION_WEIGHTS = {",
      "start_line": 31,
      "lines_added": [
        "# Commonsense relation patterns with confidence scores",
        "# Format: (pattern_regex, relation_type, confidence, swap_order)",
        "# swap_order: if True, the captured groups are in reverse order (t2, t1)",
        "RELATION_PATTERNS = [",
        "    # IsA patterns (hypernym/type relations)",
        "    (r'(\\w+)\\s+(?:is|are)\\s+(?:a|an)\\s+(?:type\\s+of\\s+)?(\\w+)', 'IsA', 0.9, False),",
        "    (r'(\\w+),?\\s+(?:a|an)\\s+(?:kind|type|form)\\s+of\\s+(\\w+)', 'IsA', 0.95, False),",
        "    (r'(\\w+)\\s+(?:is|are)\\s+considered\\s+(?:a|an)?\\s*(\\w+)', 'IsA', 0.8, False),",
        "    (r'(?:a|an)\\s+(\\w+)\\s+is\\s+(?:a|an)\\s+(\\w+)', 'IsA', 0.85, False),",
        "    (r'(\\w+)\\s+(?:belongs?\\s+to|falls?\\s+under)\\s+(?:the\\s+)?(\\w+)', 'IsA', 0.8, False),",
        "",
        "    # HasA/Contains patterns (meronym relations)",
        "    (r'(\\w+)\\s+(?:has|have|contains?|includes?)\\s+(?:a|an|the)?\\s*(\\w+)', 'HasA', 0.85, False),",
        "    (r'(\\w+)\\s+(?:consists?\\s+of|comprises?|is\\s+made\\s+of)\\s+(\\w+)', 'HasA', 0.9, False),",
        "    (r'(?:a|an|the)\\s+(\\w+)\\s+(?:with|having)\\s+(?:a|an|the)?\\s*(\\w+)', 'HasA', 0.75, False),",
        "",
        "    # PartOf patterns (part-whole relations)",
        "    (r'(\\w+)\\s+(?:is|are)\\s+(?:a\\s+)?part\\s+of\\s+(?:a|an|the)?\\s*(\\w+)', 'PartOf', 0.95, False),",
        "    (r'(\\w+)\\s+(?:is|are)\\s+(?:a\\s+)?component\\s+of\\s+(\\w+)', 'PartOf', 0.9, False),",
        "    (r'(\\w+)\\s+(?:is|are)\\s+(?:in|within|inside)\\s+(?:a|an|the)?\\s*(\\w+)', 'PartOf', 0.7, False),",
        "",
        "    # UsedFor patterns (functional relations)",
        "    (r'(\\w+)\\s+(?:is|are)\\s+used\\s+(?:for|to|in)\\s+(\\w+)', 'UsedFor', 0.9, False),",
        "    (r'(\\w+)\\s+(?:helps?|enables?|allows?)\\s+(\\w+)', 'UsedFor', 0.75, False),",
        "    (r'(?:use|using)\\s+(\\w+)\\s+(?:for|to)\\s+(\\w+)', 'UsedFor', 0.85, False),",
        "    (r'(\\w+)\\s+(?:is|are)\\s+(?:useful|helpful)\\s+for\\s+(\\w+)', 'UsedFor', 0.8, False),",
        "",
        "    # Causes patterns (causal relations)",
        "    (r'(\\w+)\\s+(?:causes?|leads?\\s+to|results?\\s+in)\\s+(\\w+)', 'Causes', 0.9, False),",
        "    (r'(\\w+)\\s+(?:produces?|generates?|creates?)\\s+(\\w+)', 'Causes', 0.8, False),",
        "    (r'(\\w+)\\s+(?:can\\s+)?(?:cause|lead\\s+to|result\\s+in)\\s+(\\w+)', 'Causes', 0.85, False),",
        "    (r'(?:because\\s+of|due\\s+to)\\s+(\\w+),?\\s+(\\w+)', 'Causes', 0.7, True),  # Reversed order",
        "",
        "    # CapableOf patterns (ability relations)",
        "    (r'(\\w+)\\s+(?:can|could|is\\s+able\\s+to)\\s+(\\w+)', 'CapableOf', 0.85, False),",
        "    (r'(\\w+)\\s+(?:has\\s+the\\s+ability\\s+to|is\\s+capable\\s+of)\\s+(\\w+)', 'CapableOf', 0.9, False),",
        "",
        "    # AtLocation patterns (spatial relations)",
        "    (r'(\\w+)\\s+(?:is|are)\\s+(?:found|located|situated)\\s+(?:in|at|on)\\s+(\\w+)', 'AtLocation', 0.9, False),",
        "    (r'(\\w+)\\s+(?:lives?|exists?|occurs?)\\s+(?:in|at|on)\\s+(\\w+)', 'AtLocation', 0.85, False),",
        "",
        "    # HasProperty patterns (attribute relations)",
        "    (r'(\\w+)\\s+(?:is|are)\\s+(\\w+)', 'HasProperty', 0.5, False),  # Very general, low confidence",
        "    (r'(\\w+)\\s+(?:is|are)\\s+(?:typically|usually|often|generally)\\s+(\\w+)', 'HasProperty', 0.7, False),",
        "    (r'(?:a|an)\\s+(\\w+)\\s+(\\w+)\\s+(?:is|are)', 'HasProperty', 0.6, True),  # \"a big dog\" → dog HasProperty big",
        "",
        "    # Antonym patterns (opposite relations)",
        "    (r'(\\w+)\\s+(?:is|are)\\s+(?:the\\s+)?opposite\\s+of\\s+(\\w+)', 'Antonym', 0.95, False),",
        "    (r'(\\w+)\\s+(?:vs\\.?|versus|or)\\s+(\\w+)', 'Antonym', 0.5, False),  # Lower confidence",
        "    (r'(\\w+)\\s+(?:not|isn\\'t|aren\\'t)\\s+(\\w+)', 'Antonym', 0.6, False),",
        "",
        "    # DerivedFrom patterns (morphological/etymological relations)",
        "    (r'(\\w+)\\s+(?:comes?\\s+from|is\\s+derived\\s+from|originates?\\s+from)\\s+(\\w+)', 'DerivedFrom', 0.9, False),",
        "    (r'(\\w+)\\s+(?:is\\s+based\\s+on|stems?\\s+from)\\s+(\\w+)', 'DerivedFrom', 0.85, False),",
        "",
        "    # DefinedBy patterns (definitional relations)",
        "    (r'(\\w+)\\s+(?:means?|refers?\\s+to|denotes?)\\s+(\\w+)', 'DefinedBy', 0.85, False),",
        "    (r'(\\w+)\\s+(?:is\\s+defined\\s+as|is\\s+known\\s+as)\\s+(?:a|an|the)?\\s*(\\w+)', 'DefinedBy', 0.9, False),",
        "]",
        "",
        "",
        "def extract_pattern_relations(",
        "    documents: Dict[str, str],",
        "    valid_terms: Set[str],",
        "    min_confidence: float = 0.5",
        ") -> List[Tuple[str, str, str, float]]:",
        "    \"\"\"",
        "    Extract semantic relations using pattern matching on document text.",
        "",
        "    Uses regex patterns to identify commonsense relations like IsA, HasA,",
        "    UsedFor, Causes, etc. from natural language expressions.",
        "",
        "    Args:",
        "        documents: Dictionary mapping doc_id to document content",
        "        valid_terms: Set of terms that exist in the corpus (from layer0)",
        "        min_confidence: Minimum confidence threshold for extracted relations",
        "",
        "    Returns:",
        "        List of (term1, relation_type, term2, confidence) tuples",
        "",
        "    Example:",
        "        >>> relations = extract_pattern_relations(docs, {\"dog\", \"animal\", \"pet\"})",
        "        >>> # Finds relations like (\"dog\", \"IsA\", \"animal\", 0.9)",
        "    \"\"\"",
        "    relations: List[Tuple[str, str, str, float]] = []",
        "    seen_relations: Set[Tuple[str, str, str]] = set()",
        "",
        "    for doc_id, content in documents.items():",
        "        content_lower = content.lower()",
        "",
        "        for pattern, relation_type, confidence, swap_order in RELATION_PATTERNS:",
        "            if confidence < min_confidence:",
        "                continue",
        "",
        "            for match in re.finditer(pattern, content_lower):",
        "                groups = match.groups()",
        "                if len(groups) >= 2:",
        "                    t1, t2 = groups[0], groups[1]",
        "",
        "                    if swap_order:",
        "                        t1, t2 = t2, t1",
        "",
        "                    # Clean terms (remove leading/trailing non-alphanumeric)",
        "                    t1 = t1.strip().lower()",
        "                    t2 = t2.strip().lower()",
        "",
        "                    # Skip if terms are the same",
        "                    if t1 == t2:",
        "                        continue",
        "",
        "                    # Skip if terms don't exist in corpus",
        "                    if t1 not in valid_terms or t2 not in valid_terms:",
        "                        continue",
        "",
        "                    # Skip common stopwords that might slip through patterns",
        "                    stopwords = {'the', 'a', 'an', 'is', 'are', 'was', 'were', 'be',",
        "                                 'been', 'being', 'have', 'has', 'had', 'do', 'does',",
        "                                 'did', 'will', 'would', 'could', 'should', 'may',",
        "                                 'might', 'must', 'shall', 'can', 'this', 'that',",
        "                                 'these', 'those', 'it', 'its', 'they', 'them',",
        "                                 'their', 'we', 'us', 'our', 'you', 'your', 'i', 'me', 'my'}",
        "                    if t1 in stopwords or t2 in stopwords:",
        "                        continue",
        "",
        "                    # Create relation key to avoid duplicates",
        "                    rel_key = (t1, relation_type, t2)",
        "",
        "                    # For symmetric relations, also check reverse",
        "                    if relation_type in {'SimilarTo', 'Antonym', 'RelatedTo'}:",
        "                        rev_key = (t2, relation_type, t1)",
        "                        if rev_key in seen_relations:",
        "                            continue",
        "",
        "                    if rel_key not in seen_relations:",
        "                        seen_relations.add(rel_key)",
        "                        relations.append((t1, relation_type, t2, confidence))",
        "",
        "    return relations",
        "",
        "",
        "def get_pattern_statistics(relations: List[Tuple[str, str, str, float]]) -> Dict[str, Any]:",
        "    \"\"\"",
        "    Get statistics about extracted pattern-based relations.",
        "",
        "    Args:",
        "        relations: List of (term1, relation_type, term2, confidence) tuples",
        "",
        "    Returns:",
        "        Dictionary with statistics about relation types and counts",
        "    \"\"\"",
        "    type_counts: Dict[str, int] = defaultdict(int)",
        "    type_confidences: Dict[str, List[float]] = defaultdict(list)",
        "",
        "    for t1, rel_type, t2, conf in relations:",
        "        type_counts[rel_type] += 1",
        "        type_confidences[rel_type].append(conf)",
        "",
        "    # Compute average confidence per type",
        "    avg_confidences = {",
        "        rel_type: sum(confs) / len(confs)",
        "        for rel_type, confs in type_confidences.items()",
        "    }",
        "",
        "    return {",
        "        'total_relations': len(relations),",
        "        'relation_type_counts': dict(type_counts),",
        "        'average_confidence_by_type': avg_confidences,",
        "        'unique_types': len(type_counts)",
        "    }",
        "",
        "",
        "    min_cooccurrence: int = 2,",
        "    use_pattern_extraction: bool = True,",
        "    min_pattern_confidence: float = 0.6",
        "",
        "    - Words appearing together frequently → CoOccurs",
        "    - Pattern-based extraction → IsA, HasA, UsedFor, Causes, etc.",
        "",
        "        use_pattern_extraction: Whether to extract relations from text patterns",
        "        min_pattern_confidence: Minimum confidence for pattern-based extraction",
        ""
      ],
      "lines_removed": [
        "    min_cooccurrence: int = 2",
        "    ",
        "    - Words appearing together frequently → RelatedTo",
        "    - Words in definitional patterns → IsA, DefinedBy",
        "    ",
        "        "
      ],
      "context_before": [
        "    'SameAs': 2.0,",
        "    'RelatedTo': 0.5,",
        "    'Antonym': -0.5,",
        "    'DerivedFrom': 1.0,",
        "    'SimilarTo': 1.5,",
        "    'CoOccurs': 0.6,",
        "    'DefinedBy': 1.0,",
        "}",
        "",
        ""
      ],
      "context_after": [
        "def extract_corpus_semantics(",
        "    layers: Dict[CorticalLayer, HierarchicalLayer],",
        "    documents: Dict[str, str],",
        "    tokenizer,",
        "    window_size: int = 5,",
        ") -> List[Tuple[str, str, str, float]]:",
        "    \"\"\"",
        "    Extract semantic relations from corpus co-occurrence patterns.",
        "    Analyzes word co-occurrences to infer semantic relationships:",
        "    - Words appearing in similar contexts → SimilarTo",
        "    Args:",
        "        layers: Dictionary of layers (needs TOKENS)",
        "        documents: Dictionary of documents",
        "        tokenizer: Tokenizer instance for processing text",
        "        window_size: Co-occurrence window size",
        "        min_cooccurrence: Minimum co-occurrences to form relation",
        "    Returns:",
        "        List of (term1, relation, term2, weight) tuples",
        "    \"\"\"",
        "    layer0 = layers[CorticalLayer.TOKENS]",
        "    relations: List[Tuple[str, str, str, float]] = []",
        "    ",
        "    # Track co-occurrences within window",
        "    cooccurrence: Dict[Tuple[str, str], int] = defaultdict(int)",
        "    ",
        "    # Track context vectors for similarity"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/semantics.py",
      "function": "def extract_corpus_semantics(",
      "start_line": 104,
      "lines_added": [
        "",
        "",
        "",
        "",
        "    # Extract commonsense relations from text patterns",
        "    if use_pattern_extraction:",
        "        valid_terms = set(layer0.minicolumns.keys())",
        "        pattern_relations = extract_pattern_relations(",
        "            documents,",
        "            valid_terms,",
        "            min_confidence=min_pattern_confidence",
        "        )",
        "        relations.extend(pattern_relations)",
        ""
      ],
      "lines_removed": [
        "        ",
        "            ",
        "                ",
        "    ",
        "    # Extract IsA from definitional patterns",
        "    isa_patterns = [",
        "        r'(\\w+)\\s+(?:is|are)\\s+(?:a|an)\\s+(?:type\\s+of\\s+)?(\\w+)',",
        "        r'(\\w+),?\\s+(?:a|an)\\s+(?:kind|type)\\s+of\\s+(\\w+)',",
        "        r'(\\w+)\\s+(?:such\\s+as|like)\\s+(\\w+)',",
        "    ]",
        "    ",
        "    for doc_id, content in documents.items():",
        "        content_lower = content.lower()",
        "        for pattern in isa_patterns:",
        "            for match in re.finditer(pattern, content_lower):",
        "                t1, t2 = match.groups()",
        "                if t1 in layer0.minicolumns and t2 in layer0.minicolumns:",
        "                    relations.append((t1, 'IsA', t2, 1.0))",
        "    "
      ],
      "context_before": [
        "                expected = (col1.occurrence_count * col2.occurrence_count) / (total + 1)",
        "                pmi = math.log((count + 1) / (expected + 1))",
        "                ",
        "                if pmi > 0:",
        "                    relations.append((t1, 'CoOccurs', t2, min(pmi, 3.0)))",
        "    ",
        "    # Extract SimilarTo from context similarity",
        "    terms = list(context_vectors.keys())",
        "    for i, t1 in enumerate(terms):",
        "        vec1 = context_vectors[t1]"
      ],
      "context_after": [
        "        for t2 in terms[i+1:]:",
        "            vec2 = context_vectors[t2]",
        "            # Cosine similarity of context vectors",
        "            common = set(vec1.keys()) & set(vec2.keys())",
        "            if len(common) >= 3:",
        "                dot = sum(vec1[k] * vec2[k] for k in common)",
        "                mag1 = math.sqrt(sum(v*v for v in vec1.values()))",
        "                mag2 = math.sqrt(sum(v*v for v in vec2.values()))",
        "                if mag1 > 0 and mag2 > 0:",
        "                    sim = dot / (mag1 * mag2)",
        "                    if sim > 0.3:",
        "                        relations.append((t1, 'SimilarTo', t2, sim))",
        "    return relations",
        "",
        "",
        "def retrofit_connections(",
        "    layers: Dict[CorticalLayer, HierarchicalLayer],",
        "    semantic_relations: List[Tuple[str, str, str, float]],",
        "    iterations: int = 10,",
        "    alpha: float = 0.3",
        ") -> Dict[str, Any]:",
        "    \"\"\""
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/semantics.py",
      "function": "def retrofit_embeddings(",
      "start_line": 320,
      "lines_added": [
        "",
        "",
        "",
        "",
        "def build_isa_hierarchy(",
        "    semantic_relations: List[Tuple[str, str, str, float]]",
        ") -> Tuple[Dict[str, Set[str]], Dict[str, Set[str]]]:",
        "    \"\"\"",
        "    Build IsA parent-child hierarchy from semantic relations.",
        "",
        "    Extracts all IsA relations and builds bidirectional parent-child mappings.",
        "    For example, if \"dog IsA animal\", then:",
        "    - parents[\"dog\"] = {\"animal\"}",
        "    - children[\"animal\"] = {\"dog\"}",
        "",
        "    Args:",
        "        semantic_relations: List of (term1, relation, term2, weight) tuples",
        "",
        "    Returns:",
        "        Tuple of (parents, children) dicts:",
        "        - parents: Maps term to set of parent terms (hypernyms)",
        "        - children: Maps term to set of child terms (hyponyms)",
        "",
        "    Example:",
        "        >>> relations = [(\"dog\", \"IsA\", \"animal\", 1.0), (\"cat\", \"IsA\", \"animal\", 1.0)]",
        "        >>> parents, children = build_isa_hierarchy(relations)",
        "        >>> print(parents[\"dog\"])  # {\"animal\"}",
        "        >>> print(children[\"animal\"])  # {\"dog\", \"cat\"}",
        "    \"\"\"",
        "    parents: Dict[str, Set[str]] = defaultdict(set)",
        "    children: Dict[str, Set[str]] = defaultdict(set)",
        "",
        "    for t1, relation, t2, weight in semantic_relations:",
        "        if relation == 'IsA':",
        "            # t1 IsA t2 means t2 is a parent (hypernym) of t1",
        "            parents[t1].add(t2)",
        "            children[t2].add(t1)",
        "",
        "    return dict(parents), dict(children)",
        "",
        "",
        "def get_ancestors(",
        "    term: str,",
        "    parents: Dict[str, Set[str]],",
        "    max_depth: int = 10",
        ") -> Dict[str, int]:",
        "    \"\"\"",
        "    Get all ancestors of a term with their depth in the hierarchy.",
        "",
        "    Performs BFS traversal up the IsA hierarchy to find all ancestors.",
        "",
        "    Args:",
        "        term: Starting term",
        "        parents: Parent mapping from build_isa_hierarchy()",
        "        max_depth: Maximum depth to traverse (prevents infinite loops)",
        "",
        "    Returns:",
        "        Dict mapping ancestor terms to their depth (1 = direct parent, 2 = grandparent, etc.)",
        "",
        "    Example:",
        "        >>> # If dog IsA canine IsA animal",
        "        >>> ancestors = get_ancestors(\"dog\", parents)",
        "        >>> # ancestors = {\"canine\": 1, \"animal\": 2}",
        "    \"\"\"",
        "    ancestors: Dict[str, int] = {}",
        "    frontier = [(p, 1) for p in parents.get(term, set())]",
        "    visited = {term}",
        "",
        "    while frontier:",
        "        current, depth = frontier.pop(0)",
        "        if current in visited or depth > max_depth:",
        "            continue",
        "        visited.add(current)",
        "        ancestors[current] = depth",
        "",
        "        # Add parents of current term",
        "        for parent in parents.get(current, set()):",
        "            if parent not in visited:",
        "                frontier.append((parent, depth + 1))",
        "",
        "    return ancestors",
        "",
        "",
        "def get_descendants(",
        "    term: str,",
        "    children: Dict[str, Set[str]],",
        "    max_depth: int = 10",
        ") -> Dict[str, int]:",
        "    \"\"\"",
        "    Get all descendants of a term with their depth in the hierarchy.",
        "",
        "    Performs BFS traversal down the IsA hierarchy to find all descendants.",
        "",
        "    Args:",
        "        term: Starting term",
        "        children: Children mapping from build_isa_hierarchy()",
        "        max_depth: Maximum depth to traverse (prevents infinite loops)",
        "",
        "    Returns:",
        "        Dict mapping descendant terms to their depth (1 = direct child, 2 = grandchild, etc.)",
        "    \"\"\"",
        "    descendants: Dict[str, int] = {}",
        "    frontier = [(c, 1) for c in children.get(term, set())]",
        "    visited = {term}",
        "",
        "    while frontier:",
        "        current, depth = frontier.pop(0)",
        "        if current in visited or depth > max_depth:",
        "            continue",
        "        visited.add(current)",
        "        descendants[current] = depth",
        "",
        "        # Add children of current term",
        "        for child in children.get(current, set()):",
        "            if child not in visited:",
        "                frontier.append((child, depth + 1))",
        "",
        "    return descendants",
        "",
        "",
        "def inherit_properties(",
        "    semantic_relations: List[Tuple[str, str, str, float]],",
        "    decay_factor: float = 0.7,",
        "    max_depth: int = 5",
        ") -> Dict[str, Dict[str, Tuple[float, str, int]]]:",
        "    \"\"\"",
        "    Compute inherited properties for all terms based on IsA hierarchy.",
        "",
        "    If \"dog IsA animal\" and \"animal HasProperty living\", then \"dog\" inherits",
        "    \"living\" with a decayed weight. Properties propagate down the IsA hierarchy",
        "    with weight decaying at each level.",
        "",
        "    Args:",
        "        semantic_relations: List of (term1, relation, term2, weight) tuples",
        "        decay_factor: Weight multiplier per inheritance level (default 0.7)",
        "        max_depth: Maximum inheritance depth (default 5)",
        "",
        "    Returns:",
        "        Dict mapping terms to their inherited properties:",
        "        {",
        "            term: {",
        "                property: (weight, source_ancestor, depth)",
        "            }",
        "        }",
        "",
        "    Example:",
        "        >>> relations = [",
        "        ...     (\"dog\", \"IsA\", \"animal\", 1.0),",
        "        ...     (\"animal\", \"HasProperty\", \"living\", 0.9),",
        "        ...     (\"animal\", \"HasProperty\", \"mortal\", 0.8),",
        "        ... ]",
        "        >>> inherited = inherit_properties(relations)",
        "        >>> print(inherited[\"dog\"])",
        "        >>> # {\"living\": (0.63, \"animal\", 1), \"mortal\": (0.56, \"animal\", 1)}",
        "    \"\"\"",
        "    # Build hierarchy",
        "    parents, children = build_isa_hierarchy(semantic_relations)",
        "",
        "    # Extract direct properties for each term",
        "    # Properties come from HasProperty, HasA, CapableOf, etc.",
        "    property_relations = {'HasProperty', 'HasA', 'CapableOf', 'AtLocation', 'UsedFor'}",
        "    direct_properties: Dict[str, Dict[str, float]] = defaultdict(dict)",
        "",
        "    for t1, relation, t2, weight in semantic_relations:",
        "        if relation in property_relations:",
        "            # t1 HasProperty t2 means t2 is a property of t1",
        "            direct_properties[t1][t2] = max(direct_properties[t1].get(t2, 0), weight)",
        "",
        "    # Compute inherited properties for each term",
        "    inherited: Dict[str, Dict[str, Tuple[float, str, int]]] = {}",
        "",
        "    # Get all terms that have parents (i.e., can inherit)",
        "    all_terms = set(parents.keys())",
        "    # Also include terms with direct properties (they might be ancestors)",
        "    all_terms.update(direct_properties.keys())",
        "",
        "    for term in all_terms:",
        "        term_inherited: Dict[str, Tuple[float, str, int]] = {}",
        "",
        "        # Get all ancestors and their depths",
        "        ancestors = get_ancestors(term, parents, max_depth=max_depth)",
        "",
        "        # For each ancestor, inherit their properties",
        "        for ancestor, depth in ancestors.items():",
        "            if ancestor in direct_properties:",
        "                # Compute decayed weight",
        "                decay = decay_factor ** depth",
        "                for prop, prop_weight in direct_properties[ancestor].items():",
        "                    inherited_weight = prop_weight * decay",
        "",
        "                    # Keep the strongest inheritance path",
        "                    if prop not in term_inherited or term_inherited[prop][0] < inherited_weight:",
        "                        term_inherited[prop] = (inherited_weight, ancestor, depth)",
        "",
        "        if term_inherited:",
        "            inherited[term] = term_inherited",
        "",
        "    return inherited",
        "",
        "",
        "def compute_property_similarity(",
        "    term1: str,",
        "    term2: str,",
        "    inherited_properties: Dict[str, Dict[str, Tuple[float, str, int]]],",
        "    direct_properties: Optional[Dict[str, Dict[str, float]]] = None",
        ") -> float:",
        "    \"\"\"",
        "    Compute similarity between terms based on shared properties (direct + inherited).",
        "",
        "    Args:",
        "        term1: First term",
        "        term2: Second term",
        "        inherited_properties: Output from inherit_properties()",
        "        direct_properties: Optional dict of direct properties {term: {prop: weight}}",
        "",
        "    Returns:",
        "        Similarity score based on Jaccard-like overlap of properties",
        "",
        "    Example:",
        "        >>> sim = compute_property_similarity(\"dog\", \"cat\", inherited, direct)",
        "        >>> # Both inherit \"living\" from \"animal\", so similarity > 0",
        "    \"\"\"",
        "    # Get all properties for each term",
        "    props1: Dict[str, float] = {}",
        "    props2: Dict[str, float] = {}",
        "",
        "    # Add inherited properties",
        "    if term1 in inherited_properties:",
        "        for prop, (weight, _, _) in inherited_properties[term1].items():",
        "            props1[prop] = max(props1.get(prop, 0), weight)",
        "",
        "    if term2 in inherited_properties:",
        "        for prop, (weight, _, _) in inherited_properties[term2].items():",
        "            props2[prop] = max(props2.get(prop, 0), weight)",
        "",
        "    # Add direct properties if provided",
        "    if direct_properties:",
        "        if term1 in direct_properties:",
        "            for prop, weight in direct_properties[term1].items():",
        "                props1[prop] = max(props1.get(prop, 0), weight)",
        "        if term2 in direct_properties:",
        "            for prop, weight in direct_properties[term2].items():",
        "                props2[prop] = max(props2.get(prop, 0), weight)",
        "",
        "    if not props1 or not props2:",
        "        return 0.0",
        "",
        "    # Compute weighted Jaccard similarity",
        "    common_props = set(props1.keys()) & set(props2.keys())",
        "    all_props = set(props1.keys()) | set(props2.keys())",
        "",
        "    if not all_props:",
        "        return 0.0",
        "",
        "    # Sum of minimum weights for common properties",
        "    intersection_weight = sum(",
        "        min(props1[p], props2[p]) for p in common_props",
        "    )",
        "",
        "    # Sum of maximum weights for all properties",
        "    union_weight = sum(",
        "        max(props1.get(p, 0), props2.get(p, 0)) for p in all_props",
        "    )",
        "",
        "    return intersection_weight / union_weight if union_weight > 0 else 0.0",
        "",
        "",
        "def apply_inheritance_to_connections(",
        "    layers: Dict[CorticalLayer, HierarchicalLayer],",
        "    inherited_properties: Dict[str, Dict[str, Tuple[float, str, int]]],",
        "    boost_factor: float = 0.3",
        ") -> Dict[str, Any]:",
        "    \"\"\"",
        "    Boost lateral connections between terms that share inherited properties.",
        "",
        "    Terms that share properties through inheritance should have stronger",
        "    connections, even if they don't directly co-occur.",
        "",
        "    Args:",
        "        layers: Dictionary of layers",
        "        inherited_properties: Output from inherit_properties()",
        "        boost_factor: Weight boost for shared properties (default 0.3)",
        "",
        "    Returns:",
        "        Statistics about connections boosted",
        "",
        "    Example:",
        "        >>> # \"dog\" and \"cat\" both inherit \"living\" from \"animal\"",
        "        >>> # Their lateral connection gets boosted",
        "        >>> stats = apply_inheritance_to_connections(layers, inherited)",
        "    \"\"\"",
        "    layer0 = layers[CorticalLayer.TOKENS]",
        "    connections_boosted = 0",
        "    total_boost = 0.0",
        "",
        "    # Get terms that have inherited properties",
        "    terms_with_inheritance = set(inherited_properties.keys())",
        "",
        "    # For each pair of terms with inherited properties",
        "    terms_list = list(terms_with_inheritance)",
        "",
        "    for i, term1 in enumerate(terms_list):",
        "        col1 = layer0.get_minicolumn(term1)",
        "        if not col1:",
        "            continue",
        "",
        "        props1 = inherited_properties[term1]",
        "",
        "        for term2 in terms_list[i + 1:]:",
        "            col2 = layer0.get_minicolumn(term2)",
        "            if not col2:",
        "                continue",
        "",
        "            props2 = inherited_properties[term2]",
        "",
        "            # Find shared inherited properties",
        "            shared_props = set(props1.keys()) & set(props2.keys())",
        "            if not shared_props:",
        "                continue",
        "",
        "            # Compute boost based on shared properties",
        "            boost = 0.0",
        "            for prop in shared_props:",
        "                w1, _, _ = props1[prop]",
        "                w2, _, _ = props2[prop]",
        "                # Average of the two inheritance weights",
        "                boost += (w1 + w2) / 2 * boost_factor",
        "",
        "            if boost > 0:",
        "                # Add boost to lateral connections",
        "                col1.add_lateral_connection(col2.id, boost)",
        "                col2.add_lateral_connection(col1.id, boost)",
        "                connections_boosted += 1",
        "                total_boost += boost",
        "",
        "    return {",
        "        'connections_boosted': connections_boosted,",
        "        'total_boost': total_boost,",
        "        'terms_with_inheritance': len(terms_with_inheritance)",
        "    }"
      ],
      "lines_removed": [
        "    ",
        "        "
      ],
      "context_before": [
        "        'iterations': iterations,",
        "        'alpha': alpha,",
        "        'terms_retrofitted': len(terms_moved),",
        "        'total_movement': total_movement",
        "    }",
        "",
        "",
        "def get_relation_type_weight(relation_type: str) -> float:",
        "    \"\"\"",
        "    Get the weight for a relation type."
      ],
      "context_after": [
        "    Args:",
        "        relation_type: Type of semantic relation",
        "    Returns:",
        "        Weight multiplier for this relation type",
        "    \"\"\"",
        "    return RELATION_WEIGHTS.get(relation_type, 0.5)"
      ],
      "change_type": "modify"
    },
    {
      "file": "tests/test_layers.py",
      "function": null,
      "start_line": 1,
      "lines_added": [
        "\"\"\"Tests for Minicolumn, Edge, and Layer classes.\"\"\"",
        "from cortical import Minicolumn, Edge, CorticalLayer, HierarchicalLayer"
      ],
      "lines_removed": [
        "\"\"\"Tests for Minicolumn and Layer classes.\"\"\"",
        "from cortical import Minicolumn, CorticalLayer, HierarchicalLayer"
      ],
      "context_before": [],
      "context_after": [
        "",
        "import unittest",
        "import sys",
        "sys.path.insert(0, '..')",
        "",
        "",
        "",
        "class TestMinicolumn(unittest.TestCase):",
        "    \"\"\"Test the Minicolumn class.\"\"\"",
        "    ",
        "    def test_creation(self):",
        "        \"\"\"Test basic minicolumn creation.\"\"\"",
        "        col = Minicolumn(\"L0_test\", \"test\", 0)",
        "        self.assertEqual(col.id, \"L0_test\")",
        "        self.assertEqual(col.content, \"test\")"
      ],
      "change_type": "modify"
    },
    {
      "file": "tests/test_layers.py",
      "function": "class TestHierarchicalLayer(unittest.TestCase):",
      "start_line": 103,
      "lines_added": [
        "",
        "",
        "class TestEdge(unittest.TestCase):",
        "    \"\"\"Test the Edge dataclass.\"\"\"",
        "",
        "    def test_edge_creation(self):",
        "        \"\"\"Test basic Edge creation.\"\"\"",
        "        edge = Edge(\"L0_target\", 0.5)",
        "        self.assertEqual(edge.target_id, \"L0_target\")",
        "        self.assertEqual(edge.weight, 0.5)",
        "        self.assertEqual(edge.relation_type, 'co_occurrence')",
        "        self.assertEqual(edge.confidence, 1.0)",
        "        self.assertEqual(edge.source, 'corpus')",
        "",
        "    def test_edge_with_metadata(self):",
        "        \"\"\"Test Edge creation with full metadata.\"\"\"",
        "        edge = Edge(",
        "            target_id=\"L0_target\",",
        "            weight=0.8,",
        "            relation_type='IsA',",
        "            confidence=0.9,",
        "            source='semantic'",
        "        )",
        "        self.assertEqual(edge.relation_type, 'IsA')",
        "        self.assertEqual(edge.confidence, 0.9)",
        "        self.assertEqual(edge.source, 'semantic')",
        "",
        "    def test_edge_serialization(self):",
        "        \"\"\"Test Edge to_dict and from_dict.\"\"\"",
        "        edge = Edge(\"L0_target\", 0.8, 'RelatedTo', 0.9, 'semantic')",
        "        data = edge.to_dict()",
        "",
        "        restored = Edge.from_dict(data)",
        "        self.assertEqual(restored.target_id, edge.target_id)",
        "        self.assertEqual(restored.weight, edge.weight)",
        "        self.assertEqual(restored.relation_type, edge.relation_type)",
        "        self.assertEqual(restored.confidence, edge.confidence)",
        "        self.assertEqual(restored.source, edge.source)",
        "",
        "    def test_edge_from_dict_defaults(self):",
        "        \"\"\"Test Edge.from_dict with minimal data.\"\"\"",
        "        data = {'target_id': 'L0_test'}",
        "        edge = Edge.from_dict(data)",
        "        self.assertEqual(edge.target_id, 'L0_test')",
        "        self.assertEqual(edge.weight, 1.0)",
        "        self.assertEqual(edge.relation_type, 'co_occurrence')",
        "",
        "",
        "class TestTypedConnections(unittest.TestCase):",
        "    \"\"\"Test typed connection functionality on Minicolumn.\"\"\"",
        "",
        "    def test_add_typed_connection(self):",
        "        \"\"\"Test adding a typed connection.\"\"\"",
        "        col = Minicolumn(\"L0_test\", \"test\", 0)",
        "        col.add_typed_connection(\"L0_other\", 0.5, relation_type='RelatedTo')",
        "",
        "        self.assertIn(\"L0_other\", col.typed_connections)",
        "        edge = col.typed_connections[\"L0_other\"]",
        "        self.assertEqual(edge.weight, 0.5)",
        "        self.assertEqual(edge.relation_type, 'RelatedTo')",
        "",
        "    def test_typed_connection_also_updates_lateral(self):",
        "        \"\"\"Test that typed connections also update lateral_connections.\"\"\"",
        "        col = Minicolumn(\"L0_test\", \"test\", 0)",
        "        col.add_typed_connection(\"L0_other\", 0.5, relation_type='RelatedTo')",
        "",
        "        # Should also be in lateral_connections",
        "        self.assertIn(\"L0_other\", col.lateral_connections)",
        "        self.assertEqual(col.lateral_connections[\"L0_other\"], 0.5)",
        "",
        "    def test_typed_connection_weight_accumulation(self):",
        "        \"\"\"Test that typed connection weights accumulate.\"\"\"",
        "        col = Minicolumn(\"L0_test\", \"test\", 0)",
        "        col.add_typed_connection(\"L0_other\", 0.5, relation_type='RelatedTo')",
        "        col.add_typed_connection(\"L0_other\", 0.3, relation_type='RelatedTo')",
        "",
        "        edge = col.typed_connections[\"L0_other\"]",
        "        self.assertEqual(edge.weight, 0.8)",
        "",
        "    def test_typed_connection_relation_type_priority(self):",
        "        \"\"\"Test that specific relation types take priority over co_occurrence.\"\"\"",
        "        col = Minicolumn(\"L0_test\", \"test\", 0)",
        "        col.add_typed_connection(\"L0_other\", 0.5, relation_type='co_occurrence')",
        "        col.add_typed_connection(\"L0_other\", 0.3, relation_type='IsA')",
        "",
        "        edge = col.typed_connections[\"L0_other\"]",
        "        self.assertEqual(edge.relation_type, 'IsA')",
        "",
        "    def test_typed_connection_source_priority(self):",
        "        \"\"\"Test that semantic/inferred sources take priority over corpus.\"\"\"",
        "        col = Minicolumn(\"L0_test\", \"test\", 0)",
        "        col.add_typed_connection(\"L0_other\", 0.5, source='corpus')",
        "        col.add_typed_connection(\"L0_other\", 0.3, source='semantic')",
        "",
        "        edge = col.typed_connections[\"L0_other\"]",
        "        self.assertEqual(edge.source, 'semantic')",
        "",
        "    def test_typed_connection_confidence_max(self):",
        "        \"\"\"Test that confidence uses max value.\"\"\"",
        "        col = Minicolumn(\"L0_test\", \"test\", 0)",
        "        col.add_typed_connection(\"L0_other\", 0.5, confidence=0.7)",
        "        col.add_typed_connection(\"L0_other\", 0.3, confidence=0.9)",
        "",
        "        edge = col.typed_connections[\"L0_other\"]",
        "        self.assertEqual(edge.confidence, 0.9)",
        "",
        "    def test_get_typed_connection(self):",
        "        \"\"\"Test retrieving a typed connection.\"\"\"",
        "        col = Minicolumn(\"L0_test\", \"test\", 0)",
        "        col.add_typed_connection(\"L0_other\", 0.5, relation_type='IsA')",
        "",
        "        edge = col.get_typed_connection(\"L0_other\")",
        "        self.assertIsNotNone(edge)",
        "        self.assertEqual(edge.relation_type, 'IsA')",
        "",
        "        # Non-existent connection",
        "        self.assertIsNone(col.get_typed_connection(\"L0_missing\"))",
        "",
        "    def test_get_connections_by_type(self):",
        "        \"\"\"Test filtering connections by relation type.\"\"\"",
        "        col = Minicolumn(\"L0_test\", \"test\", 0)",
        "        col.add_typed_connection(\"L0_a\", 0.5, relation_type='IsA')",
        "        col.add_typed_connection(\"L0_b\", 0.3, relation_type='IsA')",
        "        col.add_typed_connection(\"L0_c\", 0.4, relation_type='PartOf')",
        "",
        "        is_a_edges = col.get_connections_by_type('IsA')",
        "        self.assertEqual(len(is_a_edges), 2)",
        "",
        "        part_of_edges = col.get_connections_by_type('PartOf')",
        "        self.assertEqual(len(part_of_edges), 1)",
        "",
        "    def test_get_connections_by_source(self):",
        "        \"\"\"Test filtering connections by source.\"\"\"",
        "        col = Minicolumn(\"L0_test\", \"test\", 0)",
        "        col.add_typed_connection(\"L0_a\", 0.5, source='corpus')",
        "        col.add_typed_connection(\"L0_b\", 0.3, source='semantic')",
        "        col.add_typed_connection(\"L0_c\", 0.4, source='semantic')",
        "",
        "        corpus_edges = col.get_connections_by_source('corpus')",
        "        self.assertEqual(len(corpus_edges), 1)",
        "",
        "        semantic_edges = col.get_connections_by_source('semantic')",
        "        self.assertEqual(len(semantic_edges), 2)",
        "",
        "    def test_typed_connections_serialization(self):",
        "        \"\"\"Test that typed connections survive serialization.\"\"\"",
        "        col = Minicolumn(\"L0_test\", \"test\", 0)",
        "        col.add_typed_connection(\"L0_other\", 0.8, relation_type='IsA', confidence=0.9)",
        "",
        "        data = col.to_dict()",
        "        restored = Minicolumn.from_dict(data)",
        "",
        "        self.assertIn(\"L0_other\", restored.typed_connections)",
        "        edge = restored.typed_connections[\"L0_other\"]",
        "        self.assertEqual(edge.weight, 0.8)",
        "        self.assertEqual(edge.relation_type, 'IsA')",
        "        self.assertEqual(edge.confidence, 0.9)",
        "",
        "    def test_empty_typed_connections_serialization(self):",
        "        \"\"\"Test serialization with no typed connections.\"\"\"",
        "        col = Minicolumn(\"L0_test\", \"test\", 0)",
        "",
        "        data = col.to_dict()",
        "        self.assertEqual(data['typed_connections'], {})",
        "",
        "        restored = Minicolumn.from_dict(data)",
        "        self.assertEqual(restored.typed_connections, {})",
        "",
        ""
      ],
      "lines_removed": [
        "    ",
        "    "
      ],
      "context_before": [
        "        layer = HierarchicalLayer(CorticalLayer.TOKENS)",
        "        layer.get_or_create_minicolumn(\"a\")",
        "        layer.get_or_create_minicolumn(\"b\")",
        "        ",
        "        contents = [col.content for col in layer]",
        "        self.assertEqual(set(contents), {\"a\", \"b\"})",
        "",
        "",
        "class TestCorticalLayerEnum(unittest.TestCase):",
        "    \"\"\"Test the CorticalLayer enum.\"\"\""
      ],
      "context_after": [
        "    def test_values(self):",
        "        \"\"\"Test layer values.\"\"\"",
        "        self.assertEqual(CorticalLayer.TOKENS.value, 0)",
        "        self.assertEqual(CorticalLayer.BIGRAMS.value, 1)",
        "        self.assertEqual(CorticalLayer.CONCEPTS.value, 2)",
        "        self.assertEqual(CorticalLayer.DOCUMENTS.value, 3)",
        "    def test_description(self):",
        "        \"\"\"Test layer descriptions.\"\"\"",
        "        self.assertIn(\"Token\", CorticalLayer.TOKENS.description)",
        "        self.assertIn(\"Document\", CorticalLayer.DOCUMENTS.description)",
        "",
        "",
        "if __name__ == \"__main__\":",
        "    unittest.main(verbosity=2)"
      ],
      "change_type": "modify"
    },
    {
      "file": "tests/test_persistence.py",
      "function": "import os",
      "start_line": 6,
      "lines_added": [
        "    get_state_summary,",
        "    export_conceptnet_json,",
        "    LAYER_COLORS,",
        "    LAYER_NAMES"
      ],
      "lines_removed": [
        "    get_state_summary"
      ],
      "context_before": [
        "import json",
        "import sys",
        "sys.path.insert(0, '..')",
        "",
        "from cortical import CorticalTextProcessor, CorticalLayer",
        "from cortical.persistence import (",
        "    save_processor,",
        "    load_processor,",
        "    export_graph_json,",
        "    export_embeddings_json,"
      ],
      "context_after": [
        ")",
        "from cortical.embeddings import compute_graph_embeddings",
        "",
        "",
        "class TestSaveLoad(unittest.TestCase):",
        "    \"\"\"Test save and load functionality.\"\"\"",
        "",
        "    def test_save_and_load(self):",
        "        \"\"\"Test saving and loading processor state.\"\"\"",
        "        processor = CorticalTextProcessor()"
      ],
      "change_type": "modify"
    },
    {
      "file": "tests/test_persistence.py",
      "function": "class TestGetStateSummary(unittest.TestCase):",
      "start_line": 329,
      "lines_added": [
        "class TestExportConceptNetJSON(unittest.TestCase):",
        "    \"\"\"Test ConceptNet-style graph export.\"\"\"",
        "",
        "    @classmethod",
        "    def setUpClass(cls):",
        "        \"\"\"Set up processor with sample data.\"\"\"",
        "        cls.processor = CorticalTextProcessor()",
        "        cls.processor.process_document(\"doc1\", \"\"\"",
        "            Neural networks are a type of machine learning model.",
        "            Deep learning uses neural networks for pattern recognition.",
        "        \"\"\")",
        "        cls.processor.process_document(\"doc2\", \"\"\"",
        "            Machine learning algorithms process data efficiently.",
        "            Pattern recognition is used for image classification.",
        "        \"\"\")",
        "        cls.processor.compute_all(verbose=False)",
        "        cls.processor.extract_corpus_semantics(verbose=False)",
        "",
        "    def test_export_conceptnet_json_creates_file(self):",
        "        \"\"\"Test that export creates a JSON file.\"\"\"",
        "        with tempfile.TemporaryDirectory() as tmpdir:",
        "            filepath = os.path.join(tmpdir, \"conceptnet.json\")",
        "            export_conceptnet_json(filepath, self.processor.layers, verbose=False)",
        "            self.assertTrue(os.path.exists(filepath))",
        "",
        "    def test_export_conceptnet_json_structure(self):",
        "        \"\"\"Test exported JSON structure.\"\"\"",
        "        with tempfile.TemporaryDirectory() as tmpdir:",
        "            filepath = os.path.join(tmpdir, \"conceptnet.json\")",
        "            result = export_conceptnet_json(filepath, self.processor.layers, verbose=False)",
        "",
        "            self.assertIn('nodes', result)",
        "            self.assertIn('edges', result)",
        "            self.assertIn('metadata', result)",
        "",
        "            # Check metadata",
        "            self.assertIn('node_count', result['metadata'])",
        "            self.assertIn('edge_count', result['metadata'])",
        "            self.assertIn('layers', result['metadata'])",
        "            self.assertIn('edge_types', result['metadata'])",
        "            self.assertIn('relation_types', result['metadata'])",
        "",
        "    def test_export_conceptnet_json_node_structure(self):",
        "        \"\"\"Test node structure in exported JSON.\"\"\"",
        "        with tempfile.TemporaryDirectory() as tmpdir:",
        "            filepath = os.path.join(tmpdir, \"conceptnet.json\")",
        "            result = export_conceptnet_json(filepath, self.processor.layers, verbose=False)",
        "",
        "            for node in result['nodes']:",
        "                self.assertIn('id', node)",
        "                self.assertIn('label', node)",
        "                self.assertIn('layer', node)",
        "                self.assertIn('layer_name', node)",
        "                self.assertIn('color', node)",
        "                self.assertIn('pagerank', node)",
        "                # Color should be valid hex",
        "                self.assertTrue(node['color'].startswith('#'))",
        "",
        "    def test_export_conceptnet_json_edge_structure(self):",
        "        \"\"\"Test edge structure in exported JSON.\"\"\"",
        "        with tempfile.TemporaryDirectory() as tmpdir:",
        "            filepath = os.path.join(tmpdir, \"conceptnet.json\")",
        "            result = export_conceptnet_json(filepath, self.processor.layers, verbose=False)",
        "",
        "            for edge in result['edges']:",
        "                self.assertIn('source', edge)",
        "                self.assertIn('target', edge)",
        "                self.assertIn('weight', edge)",
        "                self.assertIn('relation_type', edge)",
        "                self.assertIn('edge_type', edge)",
        "                self.assertIn('color', edge)",
        "",
        "    def test_export_conceptnet_json_layer_colors(self):",
        "        \"\"\"Test that nodes have correct layer colors.\"\"\"",
        "        with tempfile.TemporaryDirectory() as tmpdir:",
        "            filepath = os.path.join(tmpdir, \"conceptnet.json\")",
        "            result = export_conceptnet_json(filepath, self.processor.layers, verbose=False)",
        "",
        "            for node in result['nodes']:",
        "                layer = CorticalLayer(node['layer'])",
        "                expected_color = LAYER_COLORS.get(layer, '#808080')",
        "                self.assertEqual(node['color'], expected_color)",
        "",
        "    def test_export_conceptnet_json_with_semantic_relations(self):",
        "        \"\"\"Test export with semantic relations included.\"\"\"",
        "        with tempfile.TemporaryDirectory() as tmpdir:",
        "            filepath = os.path.join(tmpdir, \"conceptnet.json\")",
        "            result = export_conceptnet_json(",
        "                filepath,",
        "                self.processor.layers,",
        "                semantic_relations=self.processor.semantic_relations,",
        "                verbose=False",
        "            )",
        "",
        "            # Should have edges",
        "            self.assertGreater(len(result['edges']), 0)",
        "",
        "    def test_export_conceptnet_json_cross_layer_edges(self):",
        "        \"\"\"Test that cross-layer edges are included when requested.\"\"\"",
        "        with tempfile.TemporaryDirectory() as tmpdir:",
        "            filepath = os.path.join(tmpdir, \"conceptnet.json\")",
        "            result = export_conceptnet_json(",
        "                filepath,",
        "                self.processor.layers,",
        "                include_cross_layer=True,",
        "                verbose=False",
        "            )",
        "",
        "            edge_types = result['metadata'].get('edge_types', {})",
        "            # May have cross_layer edges if there are feedforward/feedback connections",
        "            self.assertIsInstance(edge_types, dict)",
        "",
        "    def test_export_conceptnet_json_no_cross_layer(self):",
        "        \"\"\"Test export without cross-layer edges.\"\"\"",
        "        with tempfile.TemporaryDirectory() as tmpdir:",
        "            filepath = os.path.join(tmpdir, \"conceptnet.json\")",
        "            result = export_conceptnet_json(",
        "                filepath,",
        "                self.processor.layers,",
        "                include_cross_layer=False,",
        "                verbose=False",
        "            )",
        "",
        "            # No cross_layer edges should be present",
        "            cross_layer_count = result['metadata'].get('edge_types', {}).get('cross_layer', 0)",
        "            self.assertEqual(cross_layer_count, 0)",
        "",
        "    def test_export_conceptnet_json_max_nodes(self):",
        "        \"\"\"Test limiting nodes per layer.\"\"\"",
        "        with tempfile.TemporaryDirectory() as tmpdir:",
        "            filepath = os.path.join(tmpdir, \"conceptnet.json\")",
        "            result = export_conceptnet_json(",
        "                filepath,",
        "                self.processor.layers,",
        "                max_nodes_per_layer=5,",
        "                verbose=False",
        "            )",
        "",
        "            # Count nodes per layer",
        "            layer_counts = {}",
        "            for node in result['nodes']:",
        "                layer = node['layer']",
        "                layer_counts[layer] = layer_counts.get(layer, 0) + 1",
        "",
        "            # Each layer should have at most 5 nodes",
        "            for layer, count in layer_counts.items():",
        "                self.assertLessEqual(count, 5)",
        "",
        "    def test_export_conceptnet_json_min_weight(self):",
        "        \"\"\"Test filtering edges by minimum weight.\"\"\"",
        "        with tempfile.TemporaryDirectory() as tmpdir:",
        "            filepath = os.path.join(tmpdir, \"conceptnet.json\")",
        "            result = export_conceptnet_json(",
        "                filepath,",
        "                self.processor.layers,",
        "                min_weight=0.5,",
        "                verbose=False",
        "            )",
        "",
        "            for edge in result['edges']:",
        "                self.assertGreaterEqual(edge['weight'], 0.5)",
        "",
        "    def test_layer_colors_constant(self):",
        "        \"\"\"Test that LAYER_COLORS constant is defined.\"\"\"",
        "        self.assertIn(CorticalLayer.TOKENS, LAYER_COLORS)",
        "        self.assertIn(CorticalLayer.BIGRAMS, LAYER_COLORS)",
        "        self.assertIn(CorticalLayer.CONCEPTS, LAYER_COLORS)",
        "        self.assertIn(CorticalLayer.DOCUMENTS, LAYER_COLORS)",
        "",
        "    def test_layer_names_constant(self):",
        "        \"\"\"Test that LAYER_NAMES constant is defined.\"\"\"",
        "        self.assertIn(CorticalLayer.TOKENS, LAYER_NAMES)",
        "        self.assertEqual(LAYER_NAMES[CorticalLayer.TOKENS], 'Tokens')",
        "        self.assertEqual(LAYER_NAMES[CorticalLayer.BIGRAMS], 'Bigrams')",
        "",
        "    def test_processor_export_conceptnet_json(self):",
        "        \"\"\"Test processor-level export method.\"\"\"",
        "        with tempfile.TemporaryDirectory() as tmpdir:",
        "            filepath = os.path.join(tmpdir, \"conceptnet.json\")",
        "            result = self.processor.export_conceptnet_json(filepath, verbose=False)",
        "",
        "            self.assertIn('nodes', result)",
        "            self.assertIn('edges', result)",
        "            self.assertTrue(os.path.exists(filepath))",
        "",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "",
        "    def test_get_state_summary_empty(self):",
        "        \"\"\"Test summary for empty processor.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "",
        "        summary = get_state_summary(processor.layers, processor.documents)",
        "",
        "        self.assertEqual(summary['documents'], 0)",
        "",
        ""
      ],
      "context_after": [
        "if __name__ == \"__main__\":",
        "    unittest.main(verbosity=2)"
      ],
      "change_type": "add"
    },
    {
      "file": "tests/test_processor.py",
      "function": "class TestConceptConnections(unittest.TestCase):",
      "start_line": 1162,
      "lines_added": [
        "class TestBigramConnections(unittest.TestCase):",
        "    \"\"\"Test bigram lateral connection functionality.\"\"\"",
        "",
        "    @classmethod",
        "    def setUpClass(cls):",
        "        \"\"\"Set up processor with documents containing related bigrams.\"\"\"",
        "        cls.processor = CorticalTextProcessor()",
        "        # Documents with overlapping bigrams to test connections",
        "        cls.processor.process_document(",
        "            \"doc1\",",
        "            \"Neural networks process information. Neural processing enables \"",
        "            \"deep learning. Machine learning algorithms process data.\"",
        "        )",
        "        cls.processor.process_document(",
        "            \"doc2\",",
        "            \"Deep learning models use neural networks. Machine learning \"",
        "            \"is related to deep learning and neural processing.\"",
        "        )",
        "        cls.processor.process_document(",
        "            \"doc3\",",
        "            \"Learning algorithms improve performance. Machine learning \"",
        "            \"and deep learning are popular approaches.\"",
        "        )",
        "        cls.processor.compute_all(verbose=False)",
        "",
        "    def test_compute_bigram_connections_returns_stats(self):",
        "        \"\"\"Test that compute_bigram_connections returns expected statistics.\"\"\"",
        "        # Connections are already computed by compute_all, so create new processor",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(\"doc1\", \"Neural networks process data. Neural processing works.\")",
        "        processor.compute_tfidf(verbose=False)",
        "",
        "        stats = processor.compute_bigram_connections(verbose=False)",
        "",
        "        self.assertIn('connections_created', stats)",
        "        self.assertIn('bigrams', stats)",
        "        self.assertIn('component_connections', stats)",
        "        self.assertIn('chain_connections', stats)",
        "        self.assertIn('cooccurrence_connections', stats)",
        "",
        "    def test_shared_left_component_connection(self):",
        "        \"\"\"Test that bigrams sharing left component are connected.\"\"\"",
        "        # \"neural_networks\" and \"neural_processing\" share \"neural\"",
        "        layer1 = self.processor.get_layer(CorticalLayer.BIGRAMS)",
        "",
        "        neural_networks = layer1.get_minicolumn(\"neural_networks\")",
        "        neural_processing = layer1.get_minicolumn(\"neural_processing\")",
        "",
        "        if neural_networks and neural_processing:",
        "            # They should be connected via shared \"neural\" component",
        "            self.assertIn(neural_processing.id, neural_networks.lateral_connections)",
        "            self.assertIn(neural_networks.id, neural_processing.lateral_connections)",
        "",
        "    def test_shared_right_component_connection(self):",
        "        \"\"\"Test that bigrams sharing right component are connected.\"\"\"",
        "        # \"machine_learning\" and \"deep_learning\" share \"learning\"",
        "        layer1 = self.processor.get_layer(CorticalLayer.BIGRAMS)",
        "",
        "        machine_learning = layer1.get_minicolumn(\"machine_learning\")",
        "        deep_learning = layer1.get_minicolumn(\"deep_learning\")",
        "",
        "        if machine_learning and deep_learning:",
        "            # They should be connected via shared \"learning\" component",
        "            self.assertIn(deep_learning.id, machine_learning.lateral_connections)",
        "            self.assertIn(machine_learning.id, deep_learning.lateral_connections)",
        "",
        "    def test_chain_connections(self):",
        "        \"\"\"Test that chain bigrams are connected (right of one = left of other).\"\"\"",
        "        # \"machine_learning\" and \"learning_algorithms\" form a chain",
        "        layer1 = self.processor.get_layer(CorticalLayer.BIGRAMS)",
        "",
        "        machine_learning = layer1.get_minicolumn(\"machine_learning\")",
        "        learning_algorithms = layer1.get_minicolumn(\"learning_algorithms\")",
        "",
        "        if machine_learning and learning_algorithms:",
        "            # They should be connected via chain relationship",
        "            self.assertIn(learning_algorithms.id, machine_learning.lateral_connections)",
        "            self.assertIn(machine_learning.id, learning_algorithms.lateral_connections)",
        "",
        "    def test_cooccurrence_connections(self):",
        "        \"\"\"Test that bigrams co-occurring in documents are connected.\"\"\"",
        "        layer1 = self.processor.get_layer(CorticalLayer.BIGRAMS)",
        "",
        "        # Bigrams that appear in same documents should have co-occurrence connections",
        "        for bigram in layer1.minicolumns.values():",
        "            if bigram.document_ids and len(bigram.lateral_connections) > 0:",
        "                # If a bigram has connections, some should be from co-occurrence",
        "                # This is a general check that connections exist",
        "                break",
        "",
        "    def test_bidirectional_connections(self):",
        "        \"\"\"Test that all bigram connections are bidirectional.\"\"\"",
        "        layer1 = self.processor.get_layer(CorticalLayer.BIGRAMS)",
        "",
        "        for bigram in layer1.minicolumns.values():",
        "            for target_id in bigram.lateral_connections:",
        "                target = layer1.get_by_id(target_id)",
        "                if target:",
        "                    self.assertIn(",
        "                        bigram.id, target.lateral_connections,",
        "                        f\"Connection from {bigram.content} to {target.content} is not bidirectional\"",
        "                    )",
        "",
        "    def test_empty_bigram_layer(self):",
        "        \"\"\"Test bigram connections with empty bigram layer.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(\"doc1\", \"Hello\")  # Single word, no bigrams",
        "        processor.compute_tfidf(verbose=False)",
        "",
        "        stats = processor.compute_bigram_connections(verbose=False)",
        "        self.assertEqual(stats['connections_created'], 0)",
        "        self.assertEqual(stats['bigrams'], 0)",
        "",
        "    def test_compute_all_includes_bigram_connections(self):",
        "        \"\"\"Test that compute_all includes bigram connections.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(\"doc1\", \"Neural networks process data. Neural processing works.\")",
        "        processor.compute_all(verbose=False)",
        "",
        "        # Check that bigram connections were marked fresh",
        "        self.assertFalse(processor.is_stale(processor.COMP_BIGRAM_CONNECTIONS))",
        "",
        "    def test_custom_weights(self):",
        "        \"\"\"Test that custom weights affect connection strengths.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(\"doc1\", \"Neural networks neural processing neural analysis\")",
        "        processor.compute_tfidf(verbose=False)",
        "",
        "        # Use different weights",
        "        stats = processor.compute_bigram_connections(",
        "            component_weight=1.0,",
        "            chain_weight=1.5,",
        "            cooccurrence_weight=0.5,",
        "            verbose=False",
        "        )",
        "",
        "        # Just verify it runs without error",
        "        self.assertIsNotNone(stats)",
        "",
        "    def test_recompute_handles_bigram_connections(self):",
        "        \"\"\"Test that recompute method handles bigram connections.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(\"doc1\", \"Neural networks process data\")",
        "",
        "        # Mark as stale",
        "        processor._mark_all_stale()",
        "        self.assertTrue(processor.is_stale(processor.COMP_BIGRAM_CONNECTIONS))",
        "",
        "        # Recompute",
        "        recomputed = processor.recompute(level='full', verbose=False)",
        "        self.assertTrue(recomputed.get(processor.COMP_BIGRAM_CONNECTIONS, False))",
        "        self.assertFalse(processor.is_stale(processor.COMP_BIGRAM_CONNECTIONS))",
        "",
        "    def test_bigram_connection_weights_accumulate(self):",
        "        \"\"\"Test that connection weights accumulate for multiple reasons.\"\"\"",
        "        layer1 = self.processor.get_layer(CorticalLayer.BIGRAMS)",
        "",
        "        # Find bigrams that could be connected by multiple reasons",
        "        # (shared component AND co-occurrence)",
        "        for bigram in layer1.minicolumns.values():",
        "            for target_id, weight in bigram.lateral_connections.items():",
        "                # Weights should be positive",
        "                self.assertGreater(weight, 0)",
        "",
        "",
        "class TestSemanticPageRank(unittest.TestCase):",
        "    \"\"\"Test semantic PageRank functionality.\"\"\"",
        "",
        "    @classmethod",
        "    def setUpClass(cls):",
        "        \"\"\"Set up processor with documents for semantic PageRank testing.\"\"\"",
        "        cls.processor = CorticalTextProcessor()",
        "        cls.processor.process_document(",
        "            \"doc1\",",
        "            \"Neural networks are a type of machine learning model. \"",
        "            \"Deep learning uses neural networks for complex tasks.\"",
        "        )",
        "        cls.processor.process_document(",
        "            \"doc2\",",
        "            \"Machine learning algorithms process data patterns. \"",
        "            \"Neural networks learn from examples.\"",
        "        )",
        "        cls.processor.process_document(",
        "            \"doc3\",",
        "            \"Deep learning is part of artificial intelligence. \"",
        "            \"Machine learning models improve with data.\"",
        "        )",
        "        # Extract semantic relations first",
        "        cls.processor.extract_corpus_semantics(verbose=False)",
        "",
        "    def test_compute_semantic_importance_returns_stats(self):",
        "        \"\"\"Test that compute_semantic_importance returns expected statistics.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(\"doc1\", \"Neural networks process data efficiently.\")",
        "        processor.extract_corpus_semantics(verbose=False)",
        "",
        "        stats = processor.compute_semantic_importance(verbose=False)",
        "",
        "        self.assertIn('total_edges_with_relations', stats)",
        "        self.assertIn('token_layer', stats)",
        "        self.assertIn('bigram_layer', stats)",
        "",
        "    def test_semantic_pagerank_with_relations(self):",
        "        \"\"\"Test that semantic PageRank uses relation weights.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(",
        "            \"doc1\",",
        "            \"Neural networks learn patterns. Neural systems process data.\"",
        "        )",
        "        processor.extract_corpus_semantics(verbose=False)",
        "",
        "        # Get initial PageRank with standard method",
        "        processor.compute_importance(verbose=False)",
        "        layer0 = processor.get_layer(CorticalLayer.TOKENS)",
        "        standard_pr = {col.content: col.pagerank for col in layer0.minicolumns.values()}",
        "",
        "        # Now compute with semantic method",
        "        stats = processor.compute_semantic_importance(verbose=False)",
        "",
        "        # PageRank values should be updated",
        "        semantic_pr = {col.content: col.pagerank for col in layer0.minicolumns.values()}",
        "",
        "        # Just verify it ran and produced valid PageRank values",
        "        for content, pr in semantic_pr.items():",
        "            self.assertGreater(pr, 0)",
        "",
        "    def test_semantic_pagerank_no_relations(self):",
        "        \"\"\"Test semantic PageRank falls back when no relations exist.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(\"doc1\", \"Hello world.\")",
        "        # Don't extract semantic relations",
        "",
        "        stats = processor.compute_semantic_importance(verbose=False)",
        "",
        "        self.assertEqual(stats['total_edges_with_relations'], 0)",
        "",
        "    def test_compute_all_with_semantic_pagerank(self):",
        "        \"\"\"Test compute_all with pagerank_method='semantic'.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(",
        "            \"doc1\",",
        "            \"Neural networks process information efficiently.\"",
        "        )",
        "",
        "        # Should work without errors",
        "        processor.compute_all(verbose=False, pagerank_method='semantic')",
        "",
        "        # Verify computations ran",
        "        self.assertFalse(processor.is_stale(processor.COMP_PAGERANK))",
        "        self.assertFalse(processor.is_stale(processor.COMP_TFIDF))",
        "",
        "    def test_compute_all_standard_pagerank(self):",
        "        \"\"\"Test compute_all with default pagerank_method='standard'.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(",
        "            \"doc1\",",
        "            \"Neural networks process information efficiently.\"",
        "        )",
        "",
        "        processor.compute_all(verbose=False, pagerank_method='standard')",
        "",
        "        self.assertFalse(processor.is_stale(processor.COMP_PAGERANK))",
        "",
        "    def test_custom_relation_weights(self):",
        "        \"\"\"Test semantic PageRank with custom relation weights.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(",
        "            \"doc1\",",
        "            \"Neural networks learn patterns. Machine learning improves.\"",
        "        )",
        "        processor.extract_corpus_semantics(verbose=False)",
        "",
        "        # Use custom weights",
        "        custom_weights = {",
        "            'CoOccurs': 2.0,  # Boost co-occurrence",
        "            'RelatedTo': 0.1,  # Reduce related",
        "        }",
        "",
        "        stats = processor.compute_semantic_importance(",
        "            relation_weights=custom_weights,",
        "            verbose=False",
        "        )",
        "",
        "        # Should run without errors",
        "        self.assertIsNotNone(stats)",
        "",
        "    def test_semantic_pagerank_empty_layer(self):",
        "        \"\"\"Test semantic PageRank handles empty layer gracefully.\"\"\"",
        "        from cortical.analysis import compute_semantic_pagerank",
        "        from cortical.layers import HierarchicalLayer, CorticalLayer",
        "",
        "        empty_layer = HierarchicalLayer(CorticalLayer.TOKENS)",
        "        relations = [(\"test\", \"RelatedTo\", \"example\", 0.5)]",
        "",
        "        result = compute_semantic_pagerank(empty_layer, relations)",
        "",
        "        self.assertEqual(result['pagerank'], {})",
        "        self.assertEqual(result['iterations_run'], 0)",
        "        self.assertEqual(result['edges_with_relations'], 0)",
        "",
        "    def test_semantic_pagerank_convergence(self):",
        "        \"\"\"Test that semantic PageRank converges.\"\"\"",
        "        from cortical.analysis import compute_semantic_pagerank",
        "",
        "        layer0 = self.processor.get_layer(CorticalLayer.TOKENS)",
        "",
        "        result = compute_semantic_pagerank(",
        "            layer0,",
        "            self.processor.semantic_relations,",
        "            iterations=100,",
        "            tolerance=1e-6",
        "        )",
        "",
        "        # Should converge in less than max iterations",
        "        self.assertLessEqual(result['iterations_run'], 100)",
        "",
        "    def test_relation_weights_applied(self):",
        "        \"\"\"Test that different relation types get different weights.\"\"\"",
        "        from cortical.analysis import RELATION_WEIGHTS",
        "",
        "        # Verify key relations have expected relative weights",
        "        self.assertGreater(RELATION_WEIGHTS['IsA'], RELATION_WEIGHTS['RelatedTo'])",
        "        self.assertGreater(RELATION_WEIGHTS['PartOf'], RELATION_WEIGHTS['CoOccurs'])",
        "        self.assertLess(RELATION_WEIGHTS['Antonym'], RELATION_WEIGHTS['RelatedTo'])",
        "",
        "",
        "class TestHierarchicalPageRank(unittest.TestCase):",
        "    \"\"\"Test hierarchical (cross-layer) PageRank functionality.\"\"\"",
        "",
        "    @classmethod",
        "    def setUpClass(cls):",
        "        \"\"\"Set up processor with documents for hierarchical PageRank testing.\"\"\"",
        "        cls.processor = CorticalTextProcessor()",
        "        cls.processor.process_document(",
        "            \"doc1\",",
        "            \"Neural networks are powerful machine learning models. \"",
        "            \"Deep learning uses neural networks for complex tasks.\"",
        "        )",
        "        cls.processor.process_document(",
        "            \"doc2\",",
        "            \"Machine learning algorithms process data patterns. \"",
        "            \"Neural networks learn from examples effectively.\"",
        "        )",
        "        cls.processor.process_document(",
        "            \"doc3\",",
        "            \"Deep learning is part of artificial intelligence. \"",
        "            \"Machine learning models improve with more data.\"",
        "        )",
        "        cls.processor.compute_all(verbose=False, build_concepts=True)",
        "",
        "    def test_compute_hierarchical_importance_returns_stats(self):",
        "        \"\"\"Test that compute_hierarchical_importance returns expected statistics.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(\"doc1\", \"Neural networks process data efficiently.\")",
        "        processor.compute_all(verbose=False)",
        "",
        "        stats = processor.compute_hierarchical_importance(verbose=False)",
        "",
        "        self.assertIn('iterations_run', stats)",
        "        self.assertIn('converged', stats)",
        "        self.assertIn('layer_stats', stats)",
        "",
        "    def test_hierarchical_pagerank_layer_stats(self):",
        "        \"\"\"Test that layer stats contain expected fields.\"\"\"",
        "        stats = self.processor.compute_hierarchical_importance(verbose=False)",
        "",
        "        for layer_name, layer_info in stats['layer_stats'].items():",
        "            self.assertIn('nodes', layer_info)",
        "            self.assertIn('max_pagerank', layer_info)",
        "            self.assertIn('min_pagerank', layer_info)",
        "            self.assertIn('avg_pagerank', layer_info)",
        "",
        "    def test_hierarchical_pagerank_convergence(self):",
        "        \"\"\"Test that hierarchical PageRank converges.\"\"\"",
        "        stats = self.processor.compute_hierarchical_importance(",
        "            global_iterations=10,",
        "            verbose=False",
        "        )",
        "",
        "        # Should run at least one iteration",
        "        self.assertGreaterEqual(stats['iterations_run'], 1)",
        "",
        "    def test_hierarchical_pagerank_affects_scores(self):",
        "        \"\"\"Test that hierarchical PageRank updates scores across layers.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(",
        "            \"doc1\",",
        "            \"Neural networks process information. Machine learning improves.\"",
        "        )",
        "        processor.compute_all(verbose=False, build_concepts=True)",
        "",
        "        # Get scores before hierarchical",
        "        layer0 = processor.get_layer(CorticalLayer.TOKENS)",
        "        before_scores = {col.content: col.pagerank for col in layer0.minicolumns.values()}",
        "",
        "        # Run hierarchical PageRank",
        "        processor.compute_hierarchical_importance(verbose=False)",
        "",
        "        # Scores should be updated (normalized to sum to 1)",
        "        after_scores = {col.content: col.pagerank for col in layer0.minicolumns.values()}",
        "",
        "        # Verify scores are valid probabilities",
        "        total = sum(after_scores.values())",
        "        self.assertAlmostEqual(total, 1.0, places=5)",
        "",
        "    def test_compute_all_with_hierarchical_pagerank(self):",
        "        \"\"\"Test compute_all with pagerank_method='hierarchical'.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(",
        "            \"doc1\",",
        "            \"Neural networks process information efficiently.\"",
        "        )",
        "",
        "        # Should work without errors",
        "        processor.compute_all(verbose=False, pagerank_method='hierarchical')",
        "",
        "        # Verify computations ran",
        "        self.assertFalse(processor.is_stale(processor.COMP_PAGERANK))",
        "        self.assertFalse(processor.is_stale(processor.COMP_TFIDF))",
        "",
        "    def test_hierarchical_empty_layers(self):",
        "        \"\"\"Test hierarchical PageRank handles empty layers gracefully.\"\"\"",
        "        from cortical.analysis import compute_hierarchical_pagerank",
        "        from cortical.layers import HierarchicalLayer, CorticalLayer",
        "",
        "        # Create empty layers dict",
        "        layers = {",
        "            CorticalLayer.TOKENS: HierarchicalLayer(CorticalLayer.TOKENS),",
        "            CorticalLayer.BIGRAMS: HierarchicalLayer(CorticalLayer.BIGRAMS),",
        "        }",
        "",
        "        result = compute_hierarchical_pagerank(layers)",
        "",
        "        self.assertEqual(result['iterations_run'], 0)",
        "        self.assertTrue(result['converged'])",
        "        self.assertEqual(result['layer_stats'], {})",
        "",
        "    def test_cross_layer_damping(self):",
        "        \"\"\"Test that cross-layer damping parameter affects propagation.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(",
        "            \"doc1\",",
        "            \"Neural networks learn from data patterns.\"",
        "        )",
        "        processor.compute_all(verbose=False, build_concepts=True)",
        "",
        "        # Run with different damping values",
        "        stats_low = processor.compute_hierarchical_importance(",
        "            cross_layer_damping=0.3,",
        "            verbose=False",
        "        )",
        "        stats_high = processor.compute_hierarchical_importance(",
        "            cross_layer_damping=0.9,",
        "            verbose=False",
        "        )",
        "",
        "        # Both should produce valid results",
        "        self.assertIsNotNone(stats_low)",
        "        self.assertIsNotNone(stats_high)",
        "",
        "    def test_hierarchical_with_concepts(self):",
        "        \"\"\"Test hierarchical PageRank includes concept layer.\"\"\"",
        "        stats = self.processor.compute_hierarchical_importance(verbose=False)",
        "",
        "        # Should include CONCEPTS layer if it has nodes",
        "        layer2 = self.processor.get_layer(CorticalLayer.CONCEPTS)",
        "        if layer2.column_count() > 0:",
        "            self.assertIn('CONCEPTS', stats['layer_stats'])",
        "",
        "    def test_feedforward_feedback_connections_used(self):",
        "        \"\"\"Test that cross-layer connections are used in propagation.\"\"\"",
        "        # Verify that tokens have feedback connections (to bigrams)",
        "        layer0 = self.processor.get_layer(CorticalLayer.TOKENS)",
        "",
        "        has_feedback = any(",
        "            col.feedback_connections",
        "            for col in layer0.minicolumns.values()",
        "        )",
        "        self.assertTrue(has_feedback, \"Tokens should have feedback connections to bigrams\")",
        "",
        "",
        "class TestMultiHopSemanticInference(unittest.TestCase):",
        "    \"\"\"Test multi-hop semantic inference query expansion.\"\"\"",
        "",
        "    @classmethod",
        "    def setUpClass(cls):",
        "        \"\"\"Set up processor with documents for multi-hop testing.\"\"\"",
        "        cls.processor = CorticalTextProcessor()",
        "        # Create a corpus with semantic chain potential",
        "        cls.processor.process_document(",
        "            \"doc1\",",
        "            \"Neural networks are a type of machine learning model. \"",
        "            \"Deep learning uses neural networks for complex pattern recognition.\"",
        "        )",
        "        cls.processor.process_document(",
        "            \"doc2\",",
        "            \"Machine learning algorithms process data efficiently. \"",
        "            \"Pattern recognition is important for image classification.\"",
        "        )",
        "        cls.processor.process_document(",
        "            \"doc3\",",
        "            \"Deep learning is part of artificial intelligence research. \"",
        "            \"Image classification improves with more training data.\"",
        "        )",
        "        cls.processor.process_document(",
        "            \"doc4\",",
        "            \"Artificial intelligence systems can learn from examples. \"",
        "            \"Training data is essential for model accuracy.\"",
        "        )",
        "        cls.processor.compute_all(verbose=False)",
        "        cls.processor.extract_corpus_semantics(verbose=False)",
        "",
        "    def test_expand_query_multihop_returns_dict(self):",
        "        \"\"\"Test that expand_query_multihop returns a dictionary.\"\"\"",
        "        expanded = self.processor.expand_query_multihop(\"neural\", max_hops=2)",
        "        self.assertIsInstance(expanded, dict)",
        "",
        "    def test_original_terms_weight_one(self):",
        "        \"\"\"Test that original query terms have weight 1.0.\"\"\"",
        "        expanded = self.processor.expand_query_multihop(\"neural networks\", max_hops=2)",
        "        self.assertEqual(expanded.get(\"neural\"), 1.0)",
        "        self.assertEqual(expanded.get(\"networks\"), 1.0)",
        "",
        "    def test_hop_1_expansions(self):",
        "        \"\"\"Test that single-hop expansions are included.\"\"\"",
        "        expanded = self.processor.expand_query_multihop(\"neural\", max_hops=1)",
        "",
        "        # Should have original term",
        "        self.assertIn(\"neural\", expanded)",
        "",
        "        # Should have some expansions (semantically related terms)",
        "        expansion_count = len([k for k in expanded if k != \"neural\"])",
        "        self.assertGreater(expansion_count, 0, \"Should have at least one expansion\")",
        "",
        "    def test_hop_2_expansions(self):",
        "        \"\"\"Test that two-hop expansions discover more terms.\"\"\"",
        "        expanded_1hop = self.processor.expand_query_multihop(\"neural\", max_hops=1)",
        "        expanded_2hop = self.processor.expand_query_multihop(\"neural\", max_hops=2)",
        "",
        "        # 2-hop should have >= terms than 1-hop",
        "        self.assertGreaterEqual(len(expanded_2hop), len(expanded_1hop))",
        "",
        "    def test_weight_decay_with_hops(self):",
        "        \"\"\"Test that expansion weights decay with hop distance.\"\"\"",
        "        expanded = self.processor.expand_query_multihop(",
        "            \"neural\", max_hops=2, decay_factor=0.5",
        "        )",
        "",
        "        # Original term should have weight 1.0",
        "        self.assertEqual(expanded.get(\"neural\"), 1.0)",
        "",
        "        # All expansions should have weight < 1.0",
        "        for term, weight in expanded.items():",
        "            if term != \"neural\":",
        "                self.assertLess(",
        "                    weight, 1.0,",
        "                    f\"Expansion '{term}' should have weight < 1.0, got {weight}\"",
        "                )",
        "",
        "    def test_custom_decay_factor(self):",
        "        \"\"\"Test that custom decay factor affects weights.\"\"\"",
        "        expanded_slow = self.processor.expand_query_multihop(",
        "            \"neural\", max_hops=2, decay_factor=0.8  # Slower decay",
        "        )",
        "        expanded_fast = self.processor.expand_query_multihop(",
        "            \"neural\", max_hops=2, decay_factor=0.3  # Faster decay",
        "        )",
        "",
        "        # Slower decay should give higher average weights to expansions",
        "        slow_avg = sum(w for t, w in expanded_slow.items() if t != \"neural\")",
        "        fast_avg = sum(w for t, w in expanded_fast.items() if t != \"neural\")",
        "",
        "        # If both have expansions, slow decay should have higher total",
        "        if slow_avg > 0 and fast_avg > 0:",
        "            self.assertGreater(slow_avg, fast_avg)",
        "",
        "    def test_max_expansions_limit(self):",
        "        \"\"\"Test that max_expansions limits the number of expansion terms.\"\"\"",
        "        expanded_3 = self.processor.expand_query_multihop(",
        "            \"neural\", max_hops=2, max_expansions=3",
        "        )",
        "        expanded_10 = self.processor.expand_query_multihop(",
        "            \"neural\", max_hops=2, max_expansions=10",
        "        )",
        "",
        "        # Count expansions (non-original terms)",
        "        expansions_3 = len([k for k in expanded_3 if k != \"neural\"])",
        "        expansions_10 = len([k for k in expanded_10 if k != \"neural\"])",
        "",
        "        self.assertLessEqual(expansions_3, 3)",
        "        self.assertLessEqual(expansions_10, 10)",
        "",
        "    def test_no_semantic_relations_fallback(self):",
        "        \"\"\"Test fallback to regular expansion when no semantic relations.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(\"doc1\", \"Neural networks process data.\")",
        "        processor.compute_all(verbose=False)",
        "        # Don't extract semantic relations",
        "",
        "        expanded = processor.expand_query_multihop(\"neural\", max_hops=2)",
        "",
        "        # Should fall back to regular expansion",
        "        self.assertIn(\"neural\", expanded)",
        "",
        "    def test_unknown_query_term(self):",
        "        \"\"\"Test handling of query terms not in corpus.\"\"\"",
        "        expanded = self.processor.expand_query_multihop(\"xyznonexistent\", max_hops=2)",
        "",
        "        # Should return empty dict for unknown terms",
        "        self.assertEqual(len(expanded), 0)",
        "",
        "    def test_min_path_score_filtering(self):",
        "        \"\"\"Test that min_path_score filters low-validity paths.\"\"\"",
        "        expanded_low = self.processor.expand_query_multihop(",
        "            \"neural\", max_hops=2, min_path_score=0.1  # Low threshold",
        "        )",
        "        expanded_high = self.processor.expand_query_multihop(",
        "            \"neural\", max_hops=2, min_path_score=0.8  # High threshold",
        "        )",
        "",
        "        # Low threshold should allow more expansions",
        "        self.assertGreaterEqual(len(expanded_low), len(expanded_high))",
        "",
        "    def test_multihop_integration_with_documents(self):",
        "        \"\"\"Test that multi-hop expansion finds relevant documents.\"\"\"",
        "        # Use multi-hop expansion to find documents",
        "        expanded = self.processor.expand_query_multihop(\"neural\", max_hops=2)",
        "",
        "        # Use expanded terms to score documents",
        "        layer0 = self.processor.get_layer(CorticalLayer.TOKENS)",
        "        doc_scores = {}",
        "",
        "        for term, weight in expanded.items():",
        "            col = layer0.get_minicolumn(term)",
        "            if col:",
        "                for doc_id in col.document_ids:",
        "                    doc_scores[doc_id] = doc_scores.get(doc_id, 0) + weight * col.tfidf",
        "",
        "        # Should find at least doc1 which contains \"neural\"",
        "        self.assertIn(\"doc1\", doc_scores)",
        "",
        "",
        "class TestMultiHopPathScoring(unittest.TestCase):",
        "    \"\"\"Test relation path scoring for multi-hop inference.\"\"\"",
        "",
        "    def test_score_relation_path_empty(self):",
        "        \"\"\"Test scoring empty path.\"\"\"",
        "        from cortical.query import score_relation_path",
        "        self.assertEqual(score_relation_path([]), 1.0)",
        "",
        "    def test_score_relation_path_single(self):",
        "        \"\"\"Test scoring single-hop path.\"\"\"",
        "        from cortical.query import score_relation_path",
        "        self.assertEqual(score_relation_path(['IsA']), 1.0)",
        "        self.assertEqual(score_relation_path(['RelatedTo']), 1.0)",
        "",
        "    def test_score_isa_chain(self):",
        "        \"\"\"Test that IsA chains get high scores.\"\"\"",
        "        from cortical.query import score_relation_path",
        "        # IsA → IsA is a valid transitive chain",
        "        score = score_relation_path(['IsA', 'IsA'])",
        "        self.assertEqual(score, 1.0)",
        "",
        "    def test_score_mixed_chain(self):",
        "        \"\"\"Test scoring mixed relation chains.\"\"\"",
        "        from cortical.query import score_relation_path",
        "        # IsA → HasProperty is a valid inference",
        "        score = score_relation_path(['IsA', 'HasProperty'])",
        "        self.assertGreater(score, 0.8)",
        "",
        "    def test_score_weak_chain(self):",
        "        \"\"\"Test that weak chains get low scores.\"\"\"",
        "        from cortical.query import score_relation_path",
        "        # Antonym → IsA is contradictory",
        "        score = score_relation_path(['Antonym', 'IsA'])",
        "        self.assertLess(score, 0.3)",
        "",
        "    def test_score_default_relation(self):",
        "        \"\"\"Test scoring unknown relation pairs.\"\"\"",
        "        from cortical.query import score_relation_path",
        "        # Unknown pair should get moderate default score",
        "        score = score_relation_path(['UnknownRel', 'AnotherUnknown'])",
        "        self.assertEqual(score, 0.4)  # Default moderate validity",
        "",
        "    def test_valid_relation_chains_constant(self):",
        "        \"\"\"Test that VALID_RELATION_CHAINS is defined.\"\"\"",
        "        from cortical.query import VALID_RELATION_CHAINS",
        "        self.assertIsInstance(VALID_RELATION_CHAINS, dict)",
        "        self.assertIn(('IsA', 'IsA'), VALID_RELATION_CHAINS)",
        "        self.assertIn(('PartOf', 'PartOf'), VALID_RELATION_CHAINS)",
        "",
        "",
        "class TestAnalogyCompletion(unittest.TestCase):",
        "    \"\"\"Test analogy completion functionality.\"\"\"",
        "",
        "    @classmethod",
        "    def setUpClass(cls):",
        "        \"\"\"Set up processor with documents for analogy testing.\"\"\"",
        "        cls.processor = CorticalTextProcessor()",
        "        # Create a corpus with semantic structure for analogies",
        "        cls.processor.process_document(\"doc1\", \"\"\"",
        "            Neural networks are powerful machine learning models.",
        "            Deep learning uses neural networks for complex tasks.",
        "            Knowledge graphs store semantic relationships.",
        "        \"\"\")",
        "        cls.processor.process_document(\"doc2\", \"\"\"",
        "            Machine learning algorithms process data efficiently.",
        "            Pattern recognition helps with image classification.",
        "            Data processing transforms raw information.",
        "        \"\"\")",
        "        cls.processor.process_document(\"doc3\", \"\"\"",
        "            Artificial intelligence enables intelligent systems.",
        "            Natural language processing understands text.",
        "            Computer vision analyzes images and video.",
        "        \"\"\")",
        "        cls.processor.compute_all(verbose=False)",
        "        cls.processor.extract_corpus_semantics(verbose=False)",
        "        cls.processor.compute_graph_embeddings(dimensions=16, verbose=False)",
        "",
        "    def test_complete_analogy_returns_list(self):",
        "        \"\"\"Test that complete_analogy returns a list.\"\"\"",
        "        results = self.processor.complete_analogy(",
        "            \"neural\", \"networks\", \"machine\"",
        "        )",
        "        self.assertIsInstance(results, list)",
        "",
        "    def test_complete_analogy_result_format(self):",
        "        \"\"\"Test that results have correct format (term, score, method).\"\"\"",
        "        results = self.processor.complete_analogy(",
        "            \"neural\", \"networks\", \"machine\", top_n=3",
        "        )",
        "",
        "        for result in results:",
        "            self.assertEqual(len(result), 3)",
        "            term, score, method = result",
        "            self.assertIsInstance(term, str)",
        "            self.assertIsInstance(score, float)",
        "            self.assertIsInstance(method, str)",
        "            self.assertGreater(score, 0)",
        "",
        "    def test_complete_analogy_excludes_input_terms(self):",
        "        \"\"\"Test that input terms are excluded from results.\"\"\"",
        "        results = self.processor.complete_analogy(",
        "            \"neural\", \"networks\", \"machine\"",
        "        )",
        "",
        "        result_terms = [term for term, _, _ in results]",
        "        self.assertNotIn(\"neural\", result_terms)",
        "        self.assertNotIn(\"networks\", result_terms)",
        "        self.assertNotIn(\"machine\", result_terms)",
        "",
        "    def test_complete_analogy_top_n_limit(self):",
        "        \"\"\"Test that top_n limits the number of results.\"\"\"",
        "        results_3 = self.processor.complete_analogy(",
        "            \"neural\", \"networks\", \"machine\", top_n=3",
        "        )",
        "        results_5 = self.processor.complete_analogy(",
        "            \"neural\", \"networks\", \"machine\", top_n=5",
        "        )",
        "",
        "        self.assertLessEqual(len(results_3), 3)",
        "        self.assertLessEqual(len(results_5), 5)",
        "",
        "    def test_complete_analogy_unknown_term(self):",
        "        \"\"\"Test handling of unknown terms.\"\"\"",
        "        results = self.processor.complete_analogy(",
        "            \"xyznonexistent\", \"abcnonexistent\", \"machine\"",
        "        )",
        "        self.assertEqual(results, [])",
        "",
        "    def test_complete_analogy_with_embeddings_only(self):",
        "        \"\"\"Test analogy completion using only embeddings.\"\"\"",
        "        results = self.processor.complete_analogy(",
        "            \"neural\", \"networks\", \"machine\",",
        "            use_embeddings=True,",
        "            use_relations=False",
        "        )",
        "        self.assertIsInstance(results, list)",
        "",
        "    def test_complete_analogy_with_relations_only(self):",
        "        \"\"\"Test analogy completion using only relations.\"\"\"",
        "        results = self.processor.complete_analogy(",
        "            \"neural\", \"networks\", \"machine\",",
        "            use_embeddings=False,",
        "            use_relations=True",
        "        )",
        "        self.assertIsInstance(results, list)",
        "",
        "    def test_complete_analogy_simple_returns_list(self):",
        "        \"\"\"Test that complete_analogy_simple returns a list.\"\"\"",
        "        results = self.processor.complete_analogy_simple(",
        "            \"neural\", \"networks\", \"machine\"",
        "        )",
        "        self.assertIsInstance(results, list)",
        "",
        "    def test_complete_analogy_simple_format(self):",
        "        \"\"\"Test that simple results have correct format (term, score).\"\"\"",
        "        results = self.processor.complete_analogy_simple(",
        "            \"neural\", \"networks\", \"machine\", top_n=3",
        "        )",
        "",
        "        for result in results:",
        "            self.assertEqual(len(result), 2)",
        "            term, score = result",
        "            self.assertIsInstance(term, str)",
        "            self.assertIsInstance(score, float)",
        "",
        "    def test_complete_analogy_simple_excludes_input(self):",
        "        \"\"\"Test that input terms are excluded from simple results.\"\"\"",
        "        results = self.processor.complete_analogy_simple(",
        "            \"neural\", \"networks\", \"machine\"",
        "        )",
        "",
        "        result_terms = [term for term, _ in results]",
        "        self.assertNotIn(\"neural\", result_terms)",
        "        self.assertNotIn(\"networks\", result_terms)",
        "        self.assertNotIn(\"machine\", result_terms)",
        "",
        "",
        "class TestAnalogyHelperFunctions(unittest.TestCase):",
        "    \"\"\"Test analogy helper functions.\"\"\"",
        "",
        "    def test_find_relation_between(self):",
        "        \"\"\"Test finding relations between terms.\"\"\"",
        "        from cortical.query import find_relation_between",
        "",
        "        relations = [",
        "            (\"dog\", \"IsA\", \"animal\", 1.0),",
        "            (\"cat\", \"IsA\", \"animal\", 1.0),",
        "            (\"dog\", \"HasProperty\", \"loyal\", 0.8),",
        "        ]",
        "",
        "        result = find_relation_between(\"dog\", \"animal\", relations)",
        "        self.assertEqual(len(result), 1)",
        "        self.assertEqual(result[0][0], \"IsA\")",
        "",
        "    def test_find_relation_between_no_match(self):",
        "        \"\"\"Test finding relations with no match.\"\"\"",
        "        from cortical.query import find_relation_between",
        "",
        "        relations = [",
        "            (\"dog\", \"IsA\", \"animal\", 1.0),",
        "        ]",
        "",
        "        result = find_relation_between(\"cat\", \"animal\", relations)",
        "        self.assertEqual(len(result), 0)",
        "",
        "    def test_find_terms_with_relation(self):",
        "        \"\"\"Test finding terms with specific relation.\"\"\"",
        "        from cortical.query import find_terms_with_relation",
        "",
        "        relations = [",
        "            (\"dog\", \"IsA\", \"animal\", 1.0),",
        "            (\"cat\", \"IsA\", \"animal\", 0.9),",
        "            (\"bird\", \"IsA\", \"animal\", 0.8),",
        "        ]",
        "",
        "        result = find_terms_with_relation(\"animal\", \"IsA\", relations, direction='backward')",
        "        self.assertEqual(len(result), 3)",
        "        # Should be sorted by weight",
        "        self.assertEqual(result[0][0], \"dog\")",
        "",
        "    def test_find_terms_with_relation_forward(self):",
        "        \"\"\"Test finding terms with forward relation.\"\"\"",
        "        from cortical.query import find_terms_with_relation",
        "",
        "        relations = [",
        "            (\"dog\", \"HasProperty\", \"loyal\", 1.0),",
        "            (\"dog\", \"HasProperty\", \"friendly\", 0.8),",
        "        ]",
        "",
        "        result = find_terms_with_relation(\"dog\", \"HasProperty\", relations, direction='forward')",
        "        self.assertEqual(len(result), 2)",
        "",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "        \"\"\"Test that concepts with no document overlap don't connect.\"\"\"",
        "        # The unrelated_doc about pottery should form isolated concepts",
        "        layer2 = self.processor.get_layer(CorticalLayer.CONCEPTS)",
        "",
        "        if layer2.column_count() > 0:",
        "            # At least some concepts should be isolated if topics are different",
        "            # This is a soft test since clustering may group differently",
        "            pass  # Concept isolation depends on clustering results",
        "",
        ""
      ],
      "context_after": [
        "if __name__ == \"__main__\":",
        "    unittest.main(verbosity=2)"
      ],
      "change_type": "add"
    },
    {
      "file": "tests/test_semantics.py",
      "function": null,
      "start_line": 3,
      "lines_added": [
        "    RELATION_WEIGHTS,",
        "    RELATION_PATTERNS,",
        "    build_isa_hierarchy,",
        "    get_ancestors,",
        "    get_descendants,",
        "    inherit_properties,",
        "    compute_property_similarity,",
        "    apply_inheritance_to_connections,",
        "    extract_pattern_relations,",
        "    get_pattern_statistics"
      ],
      "lines_removed": [
        "    RELATION_WEIGHTS"
      ],
      "context_before": [
        "import unittest",
        "import sys",
        "sys.path.insert(0, '..')",
        "",
        "from cortical import CorticalTextProcessor, CorticalLayer",
        "from cortical.semantics import (",
        "    extract_corpus_semantics,",
        "    retrofit_connections,",
        "    retrofit_embeddings,",
        "    get_relation_type_weight,"
      ],
      "context_after": [
        ")",
        "from cortical.embeddings import compute_graph_embeddings",
        "",
        "",
        "class TestSemantics(unittest.TestCase):",
        "    \"\"\"Test the semantics module.\"\"\"",
        "",
        "    @classmethod",
        "    def setUpClass(cls):",
        "        \"\"\"Set up processor with sample data.\"\"\""
      ],
      "change_type": "modify"
    },
    {
      "file": "tests/test_semantics.py",
      "function": "class TestSemanticsWindowSize(unittest.TestCase):",
      "start_line": 219,
      "lines_added": [
        "class TestIsAHierarchy(unittest.TestCase):",
        "    \"\"\"Test IsA hierarchy building.\"\"\"",
        "",
        "    def test_build_isa_hierarchy_basic(self):",
        "        \"\"\"Test building IsA hierarchy from relations.\"\"\"",
        "        relations = [",
        "            (\"dog\", \"IsA\", \"animal\", 1.0),",
        "            (\"cat\", \"IsA\", \"animal\", 1.0),",
        "            (\"animal\", \"IsA\", \"living_thing\", 1.0),",
        "        ]",
        "        parents, children = build_isa_hierarchy(relations)",
        "",
        "        self.assertIn(\"animal\", parents[\"dog\"])",
        "        self.assertIn(\"animal\", parents[\"cat\"])",
        "        self.assertIn(\"living_thing\", parents[\"animal\"])",
        "        self.assertIn(\"dog\", children[\"animal\"])",
        "        self.assertIn(\"cat\", children[\"animal\"])",
        "        self.assertIn(\"animal\", children[\"living_thing\"])",
        "",
        "    def test_build_isa_hierarchy_empty(self):",
        "        \"\"\"Test building hierarchy from empty relations.\"\"\"",
        "        parents, children = build_isa_hierarchy([])",
        "        self.assertEqual(parents, {})",
        "        self.assertEqual(children, {})",
        "",
        "    def test_build_isa_hierarchy_non_isa_ignored(self):",
        "        \"\"\"Test that non-IsA relations are ignored.\"\"\"",
        "        relations = [",
        "            (\"dog\", \"IsA\", \"animal\", 1.0),",
        "            (\"dog\", \"HasProperty\", \"furry\", 0.9),",
        "            (\"dog\", \"RelatedTo\", \"pet\", 0.8),",
        "        ]",
        "        parents, children = build_isa_hierarchy(relations)",
        "",
        "        # Only IsA relation should be captured",
        "        self.assertEqual(len(parents), 1)",
        "        self.assertIn(\"dog\", parents)",
        "        self.assertEqual(parents[\"dog\"], {\"animal\"})",
        "",
        "",
        "class TestAncestorsDescendants(unittest.TestCase):",
        "    \"\"\"Test ancestor and descendant traversal.\"\"\"",
        "",
        "    def setUp(self):",
        "        \"\"\"Set up a simple hierarchy.\"\"\"",
        "        relations = [",
        "            (\"poodle\", \"IsA\", \"dog\", 1.0),",
        "            (\"dog\", \"IsA\", \"canine\", 1.0),",
        "            (\"canine\", \"IsA\", \"mammal\", 1.0),",
        "            (\"mammal\", \"IsA\", \"animal\", 1.0),",
        "            (\"cat\", \"IsA\", \"feline\", 1.0),",
        "            (\"feline\", \"IsA\", \"mammal\", 1.0),",
        "        ]",
        "        self.parents, self.children = build_isa_hierarchy(relations)",
        "",
        "    def test_get_ancestors(self):",
        "        \"\"\"Test getting ancestors of a term.\"\"\"",
        "        ancestors = get_ancestors(\"poodle\", self.parents)",
        "",
        "        self.assertIn(\"dog\", ancestors)",
        "        self.assertIn(\"canine\", ancestors)",
        "        self.assertIn(\"mammal\", ancestors)",
        "        self.assertIn(\"animal\", ancestors)",
        "        self.assertEqual(ancestors[\"dog\"], 1)",
        "        self.assertEqual(ancestors[\"canine\"], 2)",
        "        self.assertEqual(ancestors[\"mammal\"], 3)",
        "        self.assertEqual(ancestors[\"animal\"], 4)",
        "",
        "    def test_get_ancestors_direct_only(self):",
        "        \"\"\"Test that max_depth limits ancestor traversal.\"\"\"",
        "        ancestors = get_ancestors(\"poodle\", self.parents, max_depth=2)",
        "",
        "        self.assertIn(\"dog\", ancestors)",
        "        self.assertIn(\"canine\", ancestors)",
        "        self.assertNotIn(\"mammal\", ancestors)",
        "",
        "    def test_get_ancestors_no_parents(self):",
        "        \"\"\"Test ancestors of a root term.\"\"\"",
        "        ancestors = get_ancestors(\"animal\", self.parents)",
        "        self.assertEqual(ancestors, {})",
        "",
        "    def test_get_descendants(self):",
        "        \"\"\"Test getting descendants of a term.\"\"\"",
        "        descendants = get_descendants(\"mammal\", self.children)",
        "",
        "        self.assertIn(\"canine\", descendants)",
        "        self.assertIn(\"dog\", descendants)",
        "        self.assertIn(\"poodle\", descendants)",
        "        self.assertIn(\"feline\", descendants)",
        "        self.assertIn(\"cat\", descendants)",
        "",
        "    def test_get_descendants_depth(self):",
        "        \"\"\"Test descendant depths are correct.\"\"\"",
        "        descendants = get_descendants(\"mammal\", self.children)",
        "",
        "        self.assertEqual(descendants[\"canine\"], 1)",
        "        self.assertEqual(descendants[\"feline\"], 1)",
        "        self.assertEqual(descendants[\"dog\"], 2)",
        "        self.assertEqual(descendants[\"cat\"], 2)",
        "        self.assertEqual(descendants[\"poodle\"], 3)",
        "",
        "",
        "class TestPropertyInheritance(unittest.TestCase):",
        "    \"\"\"Test property inheritance through IsA hierarchy.\"\"\"",
        "",
        "    def test_inherit_properties_basic(self):",
        "        \"\"\"Test basic property inheritance.\"\"\"",
        "        relations = [",
        "            (\"dog\", \"IsA\", \"animal\", 1.0),",
        "            (\"animal\", \"HasProperty\", \"living\", 0.9),",
        "            (\"animal\", \"HasProperty\", \"mortal\", 0.8),",
        "        ]",
        "        inherited = inherit_properties(relations)",
        "",
        "        self.assertIn(\"dog\", inherited)",
        "        self.assertIn(\"living\", inherited[\"dog\"])",
        "        self.assertIn(\"mortal\", inherited[\"dog\"])",
        "",
        "        # Check inherited weight is decayed",
        "        living_weight, source, depth = inherited[\"dog\"][\"living\"]",
        "        self.assertEqual(source, \"animal\")",
        "        self.assertEqual(depth, 1)",
        "        # Weight should be 0.9 * 0.7 (default decay) = 0.63",
        "        self.assertAlmostEqual(living_weight, 0.63, places=2)",
        "",
        "    def test_inherit_properties_multi_level(self):",
        "        \"\"\"Test property inheritance through multiple levels.\"\"\"",
        "        relations = [",
        "            (\"poodle\", \"IsA\", \"dog\", 1.0),",
        "            (\"dog\", \"IsA\", \"animal\", 1.0),",
        "            (\"animal\", \"HasProperty\", \"living\", 1.0),",
        "        ]",
        "        inherited = inherit_properties(relations, decay_factor=0.5)",
        "",
        "        # Poodle should inherit \"living\" through dog → animal",
        "        self.assertIn(\"poodle\", inherited)",
        "        self.assertIn(\"living\", inherited[\"poodle\"])",
        "",
        "        # Weight should be decayed twice: 1.0 * 0.5^2 = 0.25",
        "        weight, source, depth = inherited[\"poodle\"][\"living\"]",
        "        self.assertAlmostEqual(weight, 0.25, places=2)",
        "        self.assertEqual(depth, 2)",
        "",
        "    def test_inherit_properties_empty(self):",
        "        \"\"\"Test inheritance with no IsA relations.\"\"\"",
        "        relations = [",
        "            (\"dog\", \"RelatedTo\", \"pet\", 1.0),",
        "            (\"dog\", \"HasProperty\", \"furry\", 0.9),",
        "        ]",
        "        inherited = inherit_properties(relations)",
        "",
        "        # No inheritance should occur (no IsA hierarchy)",
        "        self.assertEqual(len(inherited), 0)",
        "",
        "    def test_inherit_properties_custom_decay(self):",
        "        \"\"\"Test custom decay factor.\"\"\"",
        "        relations = [",
        "            (\"dog\", \"IsA\", \"animal\", 1.0),",
        "            (\"animal\", \"HasProperty\", \"living\", 1.0),",
        "        ]",
        "",
        "        inherited_slow = inherit_properties(relations, decay_factor=0.9)",
        "        inherited_fast = inherit_properties(relations, decay_factor=0.3)",
        "",
        "        slow_weight, _, _ = inherited_slow[\"dog\"][\"living\"]",
        "        fast_weight, _, _ = inherited_fast[\"dog\"][\"living\"]",
        "",
        "        # Slower decay should give higher weight",
        "        self.assertGreater(slow_weight, fast_weight)",
        "",
        "    def test_inherit_properties_max_depth(self):",
        "        \"\"\"Test max_depth limits inheritance.\"\"\"",
        "        relations = [",
        "            (\"a\", \"IsA\", \"b\", 1.0),",
        "            (\"b\", \"IsA\", \"c\", 1.0),",
        "            (\"c\", \"IsA\", \"d\", 1.0),",
        "            (\"d\", \"HasProperty\", \"prop\", 1.0),",
        "        ]",
        "",
        "        inherited = inherit_properties(relations, max_depth=2)",
        "",
        "        # 'c' is at depth 2, so it should inherit",
        "        self.assertIn(\"c\", inherited)",
        "        # 'a' would need depth 3 to reach 'd', so it shouldn't inherit",
        "        self.assertNotIn(\"a\", inherited)",
        "",
        "",
        "class TestPropertySimilarity(unittest.TestCase):",
        "    \"\"\"Test property-based similarity computation.\"\"\"",
        "",
        "    def test_compute_property_similarity_shared(self):",
        "        \"\"\"Test similarity between terms with shared inherited properties.\"\"\"",
        "        relations = [",
        "            (\"dog\", \"IsA\", \"animal\", 1.0),",
        "            (\"cat\", \"IsA\", \"animal\", 1.0),",
        "            (\"animal\", \"HasProperty\", \"living\", 1.0),",
        "            (\"animal\", \"HasProperty\", \"mortal\", 1.0),",
        "        ]",
        "        inherited = inherit_properties(relations)",
        "",
        "        sim = compute_property_similarity(\"dog\", \"cat\", inherited)",
        "",
        "        # Both inherit same properties, so similarity should be 1.0",
        "        self.assertAlmostEqual(sim, 1.0, places=2)",
        "",
        "    def test_compute_property_similarity_disjoint(self):",
        "        \"\"\"Test similarity between terms with no shared properties.\"\"\"",
        "        relations = [",
        "            (\"dog\", \"IsA\", \"animal\", 1.0),",
        "            (\"car\", \"IsA\", \"vehicle\", 1.0),",
        "            (\"animal\", \"HasProperty\", \"living\", 1.0),",
        "            (\"vehicle\", \"HasProperty\", \"mechanical\", 1.0),",
        "        ]",
        "        inherited = inherit_properties(relations)",
        "",
        "        sim = compute_property_similarity(\"dog\", \"car\", inherited)",
        "",
        "        # No shared properties",
        "        self.assertEqual(sim, 0.0)",
        "",
        "    def test_compute_property_similarity_partial(self):",
        "        \"\"\"Test similarity with partial property overlap.\"\"\"",
        "        relations = [",
        "            (\"dog\", \"IsA\", \"pet\", 1.0),",
        "            (\"cat\", \"IsA\", \"pet\", 1.0),",
        "            (\"pet\", \"HasProperty\", \"domesticated\", 1.0),",
        "            (\"dog\", \"IsA\", \"canine\", 1.0),",
        "            (\"canine\", \"HasProperty\", \"pack_animal\", 1.0),",
        "        ]",
        "        inherited = inherit_properties(relations)",
        "",
        "        sim = compute_property_similarity(\"dog\", \"cat\", inherited)",
        "",
        "        # Partial overlap: both have \"domesticated\", only dog has \"pack_animal\"",
        "        self.assertGreater(sim, 0.0)",
        "        self.assertLess(sim, 1.0)",
        "",
        "    def test_compute_property_similarity_no_inheritance(self):",
        "        \"\"\"Test similarity when terms have no inherited properties.\"\"\"",
        "        inherited = {}",
        "        sim = compute_property_similarity(\"unknown1\", \"unknown2\", inherited)",
        "        self.assertEqual(sim, 0.0)",
        "",
        "",
        "class TestApplyInheritanceToConnections(unittest.TestCase):",
        "    \"\"\"Test applying inheritance to lateral connections.\"\"\"",
        "",
        "    def test_apply_inheritance_to_connections(self):",
        "        \"\"\"Test that inheritance boosts connections.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(\"doc1\", \"The dog and cat are both animals.\")",
        "        processor.compute_all(verbose=False)",
        "",
        "        relations = [",
        "            (\"dog\", \"IsA\", \"animal\", 1.0),",
        "            (\"cat\", \"IsA\", \"animal\", 1.0),",
        "            (\"animal\", \"HasProperty\", \"living\", 1.0),",
        "        ]",
        "        inherited = inherit_properties(relations)",
        "",
        "        # Get initial connection weight between dog and cat",
        "        layer0 = processor.get_layer(CorticalLayer.TOKENS)",
        "        dog = layer0.get_minicolumn(\"dog\")",
        "        cat = layer0.get_minicolumn(\"cat\")",
        "",
        "        if dog and cat:",
        "            initial_weight = dog.lateral_connections.get(cat.id, 0)",
        "",
        "            stats = apply_inheritance_to_connections(",
        "                processor.layers,",
        "                inherited,",
        "                boost_factor=0.5",
        "            )",
        "",
        "            # Should have boosted at least one connection",
        "            self.assertGreaterEqual(stats['connections_boosted'], 0)",
        "",
        "    def test_apply_inheritance_empty(self):",
        "        \"\"\"Test applying empty inheritance.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(\"doc1\", \"Test content.\")",
        "        processor.compute_all(verbose=False)",
        "",
        "        stats = apply_inheritance_to_connections(",
        "            processor.layers,",
        "            {},  # Empty inheritance",
        "            boost_factor=0.3",
        "        )",
        "",
        "        self.assertEqual(stats['connections_boosted'], 0)",
        "        self.assertEqual(stats['total_boost'], 0.0)",
        "",
        "",
        "class TestProcessorPropertyInheritance(unittest.TestCase):",
        "    \"\"\"Test processor-level property inheritance methods.\"\"\"",
        "",
        "    @classmethod",
        "    def setUpClass(cls):",
        "        \"\"\"Set up processor with sample data containing IsA patterns.\"\"\"",
        "        cls.processor = CorticalTextProcessor()",
        "        # Documents with IsA patterns",
        "        cls.processor.process_document(\"doc1\", \"\"\"",
        "            A dog is a type of animal that barks.",
        "            Dogs are loyal pets that live with humans.",
        "            Animals are living creatures that need food.",
        "        \"\"\")",
        "        cls.processor.process_document(\"doc2\", \"\"\"",
        "            Cats are animals that meow and purr.",
        "            A cat is a popular pet in many homes.",
        "            Pets are domesticated animals.",
        "        \"\"\")",
        "        cls.processor.process_document(\"doc3\", \"\"\"",
        "            Cars are vehicles used for transportation.",
        "            A vehicle is a machine that moves people.",
        "            Machines are mechanical devices.",
        "        \"\"\")",
        "        cls.processor.compute_all(verbose=False)",
        "",
        "    def test_compute_property_inheritance_returns_stats(self):",
        "        \"\"\"Test that compute_property_inheritance returns expected stats.\"\"\"",
        "        stats = self.processor.compute_property_inheritance(",
        "            apply_to_connections=False,",
        "            verbose=False",
        "        )",
        "",
        "        self.assertIn('terms_with_inheritance', stats)",
        "        self.assertIn('total_properties_inherited', stats)",
        "        self.assertIn('inherited', stats)",
        "        self.assertIn('connections_boosted', stats)",
        "",
        "    def test_compute_property_inheritance_with_connections(self):",
        "        \"\"\"Test inheritance applied to connections.\"\"\"",
        "        stats = self.processor.compute_property_inheritance(",
        "            apply_to_connections=True,",
        "            boost_factor=0.3,",
        "            verbose=False",
        "        )",
        "",
        "        # Should have processed without error",
        "        self.assertIsInstance(stats['connections_boosted'], int)",
        "        self.assertIsInstance(stats['total_boost'], float)",
        "",
        "    def test_compute_property_similarity_method(self):",
        "        \"\"\"Test processor compute_property_similarity method.\"\"\"",
        "        self.processor.extract_corpus_semantics(verbose=False)",
        "",
        "        # Compute similarity (may be 0 if no shared properties in this corpus)",
        "        sim = self.processor.compute_property_similarity(\"dog\", \"cat\")",
        "        self.assertIsInstance(sim, float)",
        "        self.assertGreaterEqual(sim, 0.0)",
        "        self.assertLessEqual(sim, 1.0)",
        "",
        "    def test_compute_property_inheritance_no_relations(self):",
        "        \"\"\"Test inheritance when no semantic relations extracted.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(\"doc1\", \"Simple test content.\")",
        "        processor.compute_all(verbose=False)",
        "        # Don't extract semantics",
        "",
        "        # Should work without error (extracts semantics automatically)",
        "        stats = processor.compute_property_inheritance(",
        "            apply_to_connections=False,",
        "            verbose=False",
        "        )",
        "        self.assertIn('terms_with_inheritance', stats)",
        "",
        "",
        "class TestPatternRelationExtraction(unittest.TestCase):",
        "    \"\"\"Test pattern-based relation extraction.\"\"\"",
        "",
        "    def test_relation_patterns_defined(self):",
        "        \"\"\"Test that RELATION_PATTERNS constant is defined.\"\"\"",
        "        self.assertIsInstance(RELATION_PATTERNS, list)",
        "        self.assertGreater(len(RELATION_PATTERNS), 0)",
        "",
        "        # Each pattern should be a tuple with 4 elements",
        "        for pattern in RELATION_PATTERNS:",
        "            self.assertEqual(len(pattern), 4)",
        "            regex, rel_type, confidence, swap = pattern",
        "            self.assertIsInstance(regex, str)",
        "            self.assertIsInstance(rel_type, str)",
        "            self.assertIsInstance(confidence, float)",
        "            self.assertIsInstance(swap, bool)",
        "",
        "    def test_extract_isa_pattern(self):",
        "        \"\"\"Test extraction of IsA relations from text patterns.\"\"\"",
        "        docs = {",
        "            \"doc1\": \"A dog is a type of animal. The cat is an animal too.\"",
        "        }",
        "        valid_terms = {\"dog\", \"animal\", \"cat\", \"type\"}",
        "",
        "        relations = extract_pattern_relations(docs, valid_terms)",
        "",
        "        # Should find at least some IsA relations",
        "        isa_relations = [r for r in relations if r[1] == 'IsA']",
        "        # Note: may or may not find depending on pattern specificity",
        "        self.assertIsInstance(relations, list)",
        "",
        "    def test_extract_hasa_pattern(self):",
        "        \"\"\"Test extraction of HasA relations from text patterns.\"\"\"",
        "        docs = {",
        "            \"doc1\": \"The car has an engine. A house contains rooms.\"",
        "        }",
        "        valid_terms = {\"car\", \"engine\", \"house\", \"rooms\"}",
        "",
        "        relations = extract_pattern_relations(docs, valid_terms, min_confidence=0.5)",
        "",
        "        # Check we got some relations",
        "        self.assertIsInstance(relations, list)",
        "",
        "    def test_extract_usedfor_pattern(self):",
        "        \"\"\"Test extraction of UsedFor relations from text patterns.\"\"\"",
        "        docs = {",
        "            \"doc1\": \"The hammer is used for construction. Tools are useful for building.\"",
        "        }",
        "        valid_terms = {\"hammer\", \"construction\", \"tools\", \"building\"}",
        "",
        "        relations = extract_pattern_relations(docs, valid_terms, min_confidence=0.5)",
        "",
        "        usedfor_relations = [r for r in relations if r[1] == 'UsedFor']",
        "        # May find UsedFor relations",
        "        self.assertIsInstance(usedfor_relations, list)",
        "",
        "    def test_extract_causes_pattern(self):",
        "        \"\"\"Test extraction of Causes relations from text patterns.\"\"\"",
        "        docs = {",
        "            \"doc1\": \"Rain causes floods. The virus leads to illness.\"",
        "        }",
        "        valid_terms = {\"rain\", \"floods\", \"virus\", \"illness\"}",
        "",
        "        relations = extract_pattern_relations(docs, valid_terms, min_confidence=0.5)",
        "",
        "        causes_relations = [r for r in relations if r[1] == 'Causes']",
        "        # Should find some causal relations",
        "        self.assertIsInstance(causes_relations, list)",
        "",
        "    def test_min_confidence_filtering(self):",
        "        \"\"\"Test that min_confidence filters low-confidence relations.\"\"\"",
        "        docs = {",
        "            \"doc1\": \"The dog is happy. A cat is a pet.\"",
        "        }",
        "        valid_terms = {\"dog\", \"happy\", \"cat\", \"pet\"}",
        "",
        "        # Low confidence threshold",
        "        relations_low = extract_pattern_relations(docs, valid_terms, min_confidence=0.3)",
        "",
        "        # High confidence threshold",
        "        relations_high = extract_pattern_relations(docs, valid_terms, min_confidence=0.9)",
        "",
        "        # Low threshold should find at least as many",
        "        self.assertGreaterEqual(len(relations_low), len(relations_high))",
        "",
        "    def test_stopwords_filtered(self):",
        "        \"\"\"Test that stopwords are filtered from extracted relations.\"\"\"",
        "        docs = {",
        "            \"doc1\": \"The is a the. A an is the a.\"",
        "        }",
        "        valid_terms = {\"the\", \"a\", \"an\", \"is\"}",
        "",
        "        relations = extract_pattern_relations(docs, valid_terms)",
        "",
        "        # Should not find relations between pure stopwords",
        "        self.assertEqual(len(relations), 0)",
        "",
        "    def test_same_term_filtered(self):",
        "        \"\"\"Test that relations between same terms are filtered.\"\"\"",
        "        docs = {",
        "            \"doc1\": \"The dog is a dog. Cat is cat.\"",
        "        }",
        "        valid_terms = {\"dog\", \"cat\"}",
        "",
        "        relations = extract_pattern_relations(docs, valid_terms)",
        "",
        "        # Should not find self-relations",
        "        for t1, rel, t2, conf in relations:",
        "            self.assertNotEqual(t1, t2)",
        "",
        "    def test_invalid_terms_filtered(self):",
        "        \"\"\"Test that relations with terms not in corpus are filtered.\"\"\"",
        "        docs = {",
        "            \"doc1\": \"A unicorn is a mythical creature.\"",
        "        }",
        "        valid_terms = {\"creature\"}  # \"unicorn\" and \"mythical\" not valid",
        "",
        "        relations = extract_pattern_relations(docs, valid_terms)",
        "",
        "        # Should not find relations with invalid terms",
        "        self.assertEqual(len(relations), 0)",
        "",
        "    def test_get_pattern_statistics(self):",
        "        \"\"\"Test pattern statistics computation.\"\"\"",
        "        relations = [",
        "            (\"dog\", \"IsA\", \"animal\", 0.9),",
        "            (\"cat\", \"IsA\", \"animal\", 0.9),",
        "            (\"hammer\", \"UsedFor\", \"construction\", 0.8),",
        "        ]",
        "",
        "        stats = get_pattern_statistics(relations)",
        "",
        "        self.assertEqual(stats['total_relations'], 3)",
        "        self.assertEqual(stats['unique_types'], 2)",
        "        self.assertEqual(stats['relation_type_counts']['IsA'], 2)",
        "        self.assertEqual(stats['relation_type_counts']['UsedFor'], 1)",
        "        self.assertAlmostEqual(stats['average_confidence_by_type']['IsA'], 0.9)",
        "",
        "    def test_empty_relations_statistics(self):",
        "        \"\"\"Test statistics with empty relations.\"\"\"",
        "        stats = get_pattern_statistics([])",
        "",
        "        self.assertEqual(stats['total_relations'], 0)",
        "        self.assertEqual(stats['unique_types'], 0)",
        "        self.assertEqual(stats['relation_type_counts'], {})",
        "",
        "",
        "class TestProcessorPatternExtraction(unittest.TestCase):",
        "    \"\"\"Test processor-level pattern extraction methods.\"\"\"",
        "",
        "    @classmethod",
        "    def setUpClass(cls):",
        "        \"\"\"Set up processor with documents containing various patterns.\"\"\"",
        "        cls.processor = CorticalTextProcessor()",
        "        cls.processor.process_document(\"doc1\", \"\"\"",
        "            A neural network is a type of machine learning model.",
        "            Machine learning is used for pattern recognition.",
        "            Deep learning enables complex feature extraction.",
        "        \"\"\")",
        "        cls.processor.process_document(\"doc2\", \"\"\"",
        "            The brain contains neurons that process information.",
        "            Neurons are connected by synapses.",
        "            Processing causes activation patterns.",
        "        \"\"\")",
        "        cls.processor.process_document(\"doc3\", \"\"\"",
        "            Algorithms are used for data processing.",
        "            Data processing leads to insights.",
        "            Insights help decision making.",
        "        \"\"\")",
        "        cls.processor.compute_all(verbose=False)",
        "",
        "    def test_extract_pattern_relations_returns_list(self):",
        "        \"\"\"Test that extract_pattern_relations returns a list.\"\"\"",
        "        relations = self.processor.extract_pattern_relations(verbose=False)",
        "        self.assertIsInstance(relations, list)",
        "",
        "    def test_extract_pattern_relations_format(self):",
        "        \"\"\"Test that extracted relations have correct format.\"\"\"",
        "        relations = self.processor.extract_pattern_relations(verbose=False)",
        "",
        "        for relation in relations:",
        "            self.assertEqual(len(relation), 4)",
        "            t1, rel_type, t2, confidence = relation",
        "            self.assertIsInstance(t1, str)",
        "            self.assertIsInstance(rel_type, str)",
        "            self.assertIsInstance(t2, str)",
        "            self.assertIsInstance(confidence, float)",
        "            self.assertGreater(confidence, 0)",
        "            self.assertLessEqual(confidence, 1.0)",
        "",
        "    def test_extract_corpus_semantics_with_patterns(self):",
        "        \"\"\"Test extract_corpus_semantics with pattern extraction enabled.\"\"\"",
        "        count = self.processor.extract_corpus_semantics(",
        "            use_pattern_extraction=True,",
        "            verbose=False",
        "        )",
        "",
        "        self.assertGreater(count, 0)",
        "        self.assertGreater(len(self.processor.semantic_relations), 0)",
        "",
        "    def test_extract_corpus_semantics_without_patterns(self):",
        "        \"\"\"Test extract_corpus_semantics without pattern extraction.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(\"doc1\", \"Neural networks process information quickly.\")",
        "        processor.compute_all(verbose=False)",
        "",
        "        count_with = processor.extract_corpus_semantics(",
        "            use_pattern_extraction=True,",
        "            verbose=False",
        "        )",
        "",
        "        processor.semantic_relations = []",
        "",
        "        count_without = processor.extract_corpus_semantics(",
        "            use_pattern_extraction=False,",
        "            verbose=False",
        "        )",
        "",
        "        # With patterns should find at least as many (usually more)",
        "        # But depending on corpus, might be same",
        "        self.assertGreaterEqual(count_with, 0)",
        "        self.assertGreaterEqual(count_without, 0)",
        "",
        "    def test_custom_min_confidence(self):",
        "        \"\"\"Test custom minimum confidence threshold.\"\"\"",
        "        relations_low = self.processor.extract_pattern_relations(",
        "            min_confidence=0.3,",
        "            verbose=False",
        "        )",
        "",
        "        relations_high = self.processor.extract_pattern_relations(",
        "            min_confidence=0.9,",
        "            verbose=False",
        "        )",
        "",
        "        # Lower confidence should find at least as many",
        "        self.assertGreaterEqual(len(relations_low), len(relations_high))",
        "",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "            processor.layers,",
        "            processor.documents,",
        "            processor.tokenizer,",
        "            window_size=10",
        "        )",
        "",
        "        # Larger window should find at least as many relations",
        "        self.assertGreaterEqual(len(relations_large), len(relations_small))",
        "",
        ""
      ],
      "context_after": [
        "if __name__ == \"__main__\":",
        "    unittest.main(verbosity=2)"
      ],
      "change_type": "add"
    }
  ],
  "hour_of_day": 0,
  "day_of_week": "Wednesday",
  "seconds_since_last_commit": -479622,
  "is_merge": true,
  "is_initial": false,
  "parent_count": 2,
  "session_id": null,
  "related_chats": [],
  "ci_result": null,
  "reverted": false,
  "amended": false
}