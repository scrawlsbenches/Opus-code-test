{
  "hash": "50075730c430f93cd9ed51335f4d6d99f1cf340e",
  "message": "Add merge-friendly task ID system for parallel agents",
  "author": "Claude",
  "timestamp": "2025-12-13 22:06:48 +0000",
  "branch": "claude/multi-index-design-DvifZ",
  "files_changed": [
    "docs/merge-friendly-tasks.md",
    "scripts/consolidate_tasks.py",
    "scripts/task_utils.py",
    "tests/unit/test_task_utils.py"
  ],
  "insertions": 1229,
  "deletions": 0,
  "hunks": [
    {
      "file": "docs/merge-friendly-tasks.md",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "# Merge-Friendly Task Management",
        "",
        "This document describes the merge-friendly task ID system for parallel agent workflows.",
        "",
        "## The Problem",
        "",
        "When multiple Claude agents run in parallel on the same repository:",
        "- Both might create task `#239` (next sequential ID)",
        "- Both modify `TASK_LIST.md` simultaneously",
        "- Git merge conflicts are guaranteed",
        "",
        "## The Solution",
        "",
        "Use **timestamp-based, session-scoped task IDs** that can't collide:",
        "",
        "```",
        "T-20251213-143052-a1b2",
        "‚îÇ ‚îÇ        ‚îÇ      ‚îÇ",
        "‚îÇ ‚îÇ        ‚îÇ      ‚îî‚îÄ‚îÄ 4-char session suffix (unique per agent session)",
        "‚îÇ ‚îÇ        ‚îî‚îÄ‚îÄ Time created (HHMMSS)",
        "‚îÇ ‚îî‚îÄ‚îÄ Date created (YYYYMMDD)",
        "‚îî‚îÄ‚îÄ Task prefix",
        "```",
        "",
        "Combined with **per-session task files**:",
        "",
        "```",
        "tasks/",
        "‚îú‚îÄ‚îÄ 2025-12-13_14-30-52_a1b2.json    # Agent A's session",
        "‚îú‚îÄ‚îÄ 2025-12-13_14-31-05_c3d4.json    # Agent B's session",
        "‚îî‚îÄ‚îÄ ...",
        "```",
        "",
        "## How It Works",
        "",
        "### 1. Each Agent Creates Its Own Session",
        "",
        "```python",
        "from scripts.task_utils import TaskSession",
        "",
        "# Start a session (gets unique suffix like \"a1b2\")",
        "session = TaskSession()",
        "",
        "# Create tasks (all get same suffix)",
        "task1 = session.create_task(",
        "    title=\"Implement feature X\",",
        "    priority=\"high\",",
        "    category=\"arch\",",
        "    description=\"...\",",
        "    effort=\"medium\"",
        ")",
        "",
        "task2 = session.create_task(",
        "    title=\"Add tests for feature X\",",
        "    priority=\"medium\",",
        "    category=\"test\",",
        "    depends_on=[task1.id]",
        ")",
        "",
        "# Save to tasks/2025-12-13_14-30-52_a1b2.json",
        "session.save()",
        "```",
        "",
        "### 2. No Merge Conflicts",
        "",
        "Each agent writes to a **unique filename**:",
        "- Agent A: `tasks/2025-12-13_14-30-52_a1b2.json`",
        "- Agent B: `tasks/2025-12-13_14-31-05_c3d4.json`",
        "",
        "Files never conflict because:",
        "1. Timestamps are different (even by milliseconds)",
        "2. Session IDs are randomly generated",
        "3. Each agent only writes to its own file",
        "",
        "### 3. Consolidation (Like `git gc`)",
        "",
        "Periodically consolidate task files:",
        "",
        "```bash",
        "# Show summary of all tasks",
        "python scripts/consolidate_tasks.py --summary",
        "",
        "# Auto-merge duplicates and consolidate",
        "python scripts/consolidate_tasks.py --update --auto-merge",
        "",
        "# Archive old session files after consolidation",
        "python scripts/consolidate_tasks.py --update --archive",
        "```",
        "",
        "## Task ID Formats",
        "",
        "### Full Format (Default)",
        "```",
        "T-20251213-143052-a1b2",
        "```",
        "- Sortable by creation time",
        "- Self-documenting (when it was created)",
        "- Session-traceable (which agent created it)",
        "",
        "### Short Format",
        "```",
        "T-a1b2c3d4",
        "```",
        "- More compact (8 hex chars)",
        "- Still practically unique",
        "- Good for quick references",
        "",
        "```python",
        "from scripts.task_utils import generate_short_task_id",
        "task_id = generate_short_task_id()  # T-a1b2c3d4",
        "```",
        "",
        "## CLI Commands",
        "",
        "### Generate Task ID",
        "```bash",
        "# Full format",
        "python scripts/task_utils.py generate",
        "# Output: T-20251213-143052-a1b2",
        "",
        "# Short format",
        "python scripts/task_utils.py generate --short",
        "# Output: T-a1b2c3d4",
        "```",
        "",
        "### List All Tasks",
        "```bash",
        "# List all tasks",
        "python scripts/task_utils.py list",
        "",
        "# Filter by status",
        "python scripts/task_utils.py list --status pending",
        "```",
        "",
        "### Consolidate Tasks",
        "```bash",
        "# Dry run (see what would happen)",
        "python scripts/consolidate_tasks.py --dry-run",
        "",
        "# Consolidate with summary",
        "python scripts/consolidate_tasks.py --update",
        "",
        "# Auto-merge duplicates",
        "python scripts/consolidate_tasks.py --update --auto-merge",
        "```",
        "",
        "## Comparison with Legacy System",
        "",
        "| Aspect | Legacy (`#133`) | New (`T-a1b2c3d4`) |",
        "|--------|-----------------|---------------------|",
        "| Collision risk | High (parallel agents) | ~Zero |",
        "| Human readable | Very easy | Moderate |",
        "| Git-friendly | Conflicts guaranteed | No conflicts |",
        "| Sorting | Natural numeric | Chronological |",
        "| Traceability | None | Session + timestamp |",
        "",
        "## Best Practices",
        "",
        "### For Parallel Agents",
        "",
        "1. **Always create a session** at the start of your work",
        "2. **Save the session** before your work is committed",
        "3. **Reference tasks by full ID** in commits and comments",
        "",
        "### For Consolidation",
        "",
        "1. **Run consolidation weekly** (or after parallel agent runs)",
        "2. **Use `--auto-merge`** to deduplicate similar tasks",
        "3. **Archive old files** to keep the directory clean",
        "",
        "### For Migration",
        "",
        "The new system can coexist with legacy `TASK_LIST.md`:",
        "- Legacy tasks keep their `#123` format",
        "- New tasks use `T-...` format",
        "- Both can be referenced and tracked",
        "",
        "## Architecture",
        "",
        "```",
        "tasks/",
        "‚îú‚îÄ‚îÄ 2025-12-13_14-30-52_a1b2.json    # Agent sessions (append-only)",
        "‚îú‚îÄ‚îÄ 2025-12-13_14-31-05_c3d4.json",
        "‚îú‚îÄ‚îÄ consolidated_2025-12-13.json     # Periodic consolidation",
        "‚îî‚îÄ‚îÄ archive/                          # Archived old files",
        "    ‚îî‚îÄ‚îÄ ...",
        "",
        "TASK_LIST.md                          # Optional: human-readable summary",
        "```",
        "",
        "This mirrors the `chunk_index.py` architecture for corpus indexing.",
        "",
        "## Task File Format",
        "",
        "```json",
        "{",
        "  \"version\": 1,",
        "  \"session_id\": \"a1b2\",",
        "  \"started_at\": \"2025-12-13T14:30:52\",",
        "  \"saved_at\": \"2025-12-13T14:35:00\",",
        "  \"tasks\": [",
        "    {",
        "      \"id\": \"T-20251213-143052-a1b2\",",
        "      \"title\": \"Implement feature X\",",
        "      \"status\": \"pending\",",
        "      \"priority\": \"high\",",
        "      \"category\": \"arch\",",
        "      \"description\": \"Detailed description...\",",
        "      \"depends_on\": [],",
        "      \"effort\": \"medium\",",
        "      \"created_at\": \"2025-12-13T14:30:52\",",
        "      \"updated_at\": null,",
        "      \"completed_at\": null,",
        "      \"context\": {",
        "        \"files\": [\"cortical/processor.py\"],",
        "        \"methods\": [\"compute_all()\"]",
        "      }",
        "    }",
        "  ]",
        "}",
        "```",
        "",
        "## Future Enhancements",
        "",
        "1. **Real-time sync**: Watch for file changes and auto-consolidate",
        "2. **Web UI**: Visual task board from consolidated data",
        "3. **GitHub Issues sync**: Two-way sync with GitHub Issues",
        "4. **Task dependencies**: Topological sorting for execution order",
        "",
        "---",
        "",
        "*This system follows the same principles as `cortical/chunk_index.py` - append-only, git-friendly, merge-conflict-free.*"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "scripts/consolidate_tasks.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "#!/usr/bin/env python3",
        "\"\"\"",
        "Consolidate task files from parallel agent sessions.",
        "",
        "This script merges task files created by parallel agents into a unified",
        "view, resolving any conflicts and generating an updated TASK_LIST.md.",
        "",
        "Similar to `git gc` for chunk files, this consolidates distributed task",
        "state into a coherent whole.",
        "",
        "Usage:",
        "    # Show what would be consolidated (dry run)",
        "    python scripts/consolidate_tasks.py --dry-run",
        "",
        "    # Consolidate and update TASK_LIST.md",
        "    python scripts/consolidate_tasks.py --update",
        "",
        "    # Just generate a summary without modifying anything",
        "    python scripts/consolidate_tasks.py --summary",
        "",
        "    # Consolidate tasks from a specific directory",
        "    python scripts/consolidate_tasks.py --dir tasks/ --update",
        "",
        "Architecture:",
        "    tasks/",
        "    ‚îú‚îÄ‚îÄ 2025-12-13_14-30-52_a1b2.json    # Agent A's session",
        "    ‚îú‚îÄ‚îÄ 2025-12-13_14-31-05_c3d4.json    # Agent B's session",
        "    ‚îî‚îÄ‚îÄ ...",
        "",
        "    After consolidation:",
        "    ‚îú‚îÄ‚îÄ consolidated_2025-12-13_15-00-00.json  # Merged state",
        "    ‚îî‚îÄ‚îÄ (old files can be archived or removed)",
        "\"\"\"",
        "",
        "import argparse",
        "import json",
        "import os",
        "import shutil",
        "from collections import defaultdict",
        "from datetime import datetime",
        "from pathlib import Path",
        "from typing import Dict, List, Optional, Tuple, Any",
        "",
        "from task_utils import (",
        "    Task, TaskSession, load_all_tasks, consolidate_tasks,",
        "    DEFAULT_TASKS_DIR, generate_session_id",
        ")",
        "",
        "",
        "def find_conflicts(tasks: List[Task]) -> Dict[str, List[Task]]:",
        "    \"\"\"",
        "    Find tasks that might be duplicates or conflicts.",
        "",
        "    Returns dict of potential duplicate groups (same title, different IDs).",
        "    \"\"\"",
        "    by_title = defaultdict(list)",
        "    for task in tasks:",
        "        # Normalize title for comparison",
        "        normalized = task.title.lower().strip()",
        "        by_title[normalized].append(task)",
        "",
        "    # Return only groups with potential conflicts",
        "    return {title: tasks for title, tasks in by_title.items() if len(tasks) > 1}",
        "",
        "",
        "def merge_duplicate_tasks(tasks: List[Task]) -> Task:",
        "    \"\"\"",
        "    Merge potentially duplicate tasks into one.",
        "",
        "    Strategy:",
        "    - Keep the earliest creation time",
        "    - Use the most complete description",
        "    - Prefer higher priority",
        "    - Prefer \"in_progress\" or \"completed\" status over \"pending\"",
        "    \"\"\"",
        "    if len(tasks) == 1:",
        "        return tasks[0]",
        "",
        "    # Sort by creation time (keep earliest ID)",
        "    sorted_tasks = sorted(tasks, key=lambda t: t.created_at)",
        "    merged = Task(",
        "        id=sorted_tasks[0].id,",
        "        title=sorted_tasks[0].title,",
        "        created_at=sorted_tasks[0].created_at",
        "    )",
        "",
        "    # Merge fields from all tasks",
        "    priority_order = {\"high\": 0, \"medium\": 1, \"low\": 2}",
        "    status_order = {\"completed\": 0, \"in_progress\": 1, \"pending\": 2, \"deferred\": 3}",
        "",
        "    best_priority = min(tasks, key=lambda t: priority_order.get(t.priority, 1))",
        "    best_status = min(tasks, key=lambda t: status_order.get(t.status, 2))",
        "",
        "    merged.priority = best_priority.priority",
        "    merged.status = best_status.status",
        "    merged.category = sorted_tasks[0].category",
        "",
        "    # Use longest description",
        "    merged.description = max(tasks, key=lambda t: len(t.description)).description",
        "",
        "    # Merge dependencies",
        "    all_deps = set()",
        "    for task in tasks:",
        "        all_deps.update(task.depends_on)",
        "    merged.depends_on = list(all_deps)",
        "",
        "    # Merge context",
        "    merged.context = {}",
        "    for task in tasks:",
        "        merged.context.update(task.context)",
        "",
        "    # Track completion",
        "    completed = [t for t in tasks if t.completed_at]",
        "    if completed:",
        "        merged.completed_at = min(t.completed_at for t in completed)",
        "",
        "    return merged",
        "",
        "",
        "def consolidate_and_dedupe(",
        "    tasks_dir: str = DEFAULT_TASKS_DIR,",
        "    auto_merge: bool = False",
        ") -> Tuple[List[Task], Dict[str, List[Task]]]:",
        "    \"\"\"",
        "    Load all tasks and identify/resolve duplicates.",
        "",
        "    Args:",
        "        tasks_dir: Directory containing task session files",
        "        auto_merge: If True, automatically merge duplicates",
        "",
        "    Returns:",
        "        Tuple of (final task list, conflicts dict)",
        "    \"\"\"",
        "    all_tasks = load_all_tasks(tasks_dir)",
        "    conflicts = find_conflicts(all_tasks)",
        "",
        "    if not auto_merge or not conflicts:",
        "        return all_tasks, conflicts",
        "",
        "    # Auto-merge duplicates",
        "    merged_ids = set()",
        "    final_tasks = []",
        "",
        "    for title, conflict_group in conflicts.items():",
        "        merged = merge_duplicate_tasks(conflict_group)",
        "        final_tasks.append(merged)",
        "        merged_ids.update(t.id for t in conflict_group)",
        "",
        "    # Add non-conflicting tasks",
        "    for task in all_tasks:",
        "        if task.id not in merged_ids:",
        "            final_tasks.append(task)",
        "",
        "    return final_tasks, conflicts",
        "",
        "",
        "def generate_markdown_section(",
        "    tasks: List[Task],",
        "    status_filter: str,",
        "    priority_filter: Optional[str] = None",
        ") -> List[str]:",
        "    \"\"\"Generate markdown table rows for tasks matching filters.\"\"\"",
        "    filtered = [t for t in tasks if t.status == status_filter]",
        "    if priority_filter:",
        "        filtered = [t for t in filtered if t.priority == priority_filter]",
        "",
        "    if not filtered:",
        "        return []",
        "",
        "    # Sort by priority then creation time",
        "    priority_order = {\"high\": 0, \"medium\": 1, \"low\": 2}",
        "    filtered.sort(key=lambda t: (priority_order.get(t.priority, 1), t.created_at))",
        "",
        "    lines = []",
        "    for task in filtered:",
        "        deps = \", \".join(task.depends_on) if task.depends_on else \"-\"",
        "        lines.append(",
        "            f\"| {task.id} | {task.title} | {task.category} | {deps} | {task.effort} |\"",
        "        )",
        "",
        "    return lines",
        "",
        "",
        "def write_consolidated_file(",
        "    tasks: List[Task],",
        "    output_dir: str,",
        "    session_id: Optional[str] = None",
        ") -> Path:",
        "    \"\"\"Write consolidated tasks to a single JSON file.\"\"\"",
        "    dir_path = Path(output_dir)",
        "    dir_path.mkdir(parents=True, exist_ok=True)",
        "",
        "    sid = session_id or generate_session_id()",
        "    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")",
        "    filename = f\"consolidated_{timestamp}_{sid}.json\"",
        "",
        "    filepath = dir_path / filename",
        "",
        "    data = {",
        "        \"version\": 1,",
        "        \"type\": \"consolidated\",",
        "        \"session_id\": sid,",
        "        \"created_at\": datetime.now().isoformat(),",
        "        \"task_count\": len(tasks),",
        "        \"tasks\": [t.to_dict() for t in tasks]",
        "    }",
        "",
        "    with open(filepath, 'w') as f:",
        "        json.dump(data, f, indent=2)",
        "",
        "    return filepath",
        "",
        "",
        "def archive_old_session_files(",
        "    tasks_dir: str,",
        "    archive_dir: Optional[str] = None,",
        "    keep_consolidated: bool = True",
        ") -> List[Path]:",
        "    \"\"\"",
        "    Move old session files to archive after consolidation.",
        "",
        "    Args:",
        "        tasks_dir: Directory containing task files",
        "        archive_dir: Where to move old files (default: tasks/archive/)",
        "        keep_consolidated: Don't archive consolidated_*.json files",
        "",
        "    Returns:",
        "        List of archived file paths",
        "    \"\"\"",
        "    dir_path = Path(tasks_dir)",
        "    archive_path = Path(archive_dir or (dir_path / \"archive\"))",
        "    archive_path.mkdir(parents=True, exist_ok=True)",
        "",
        "    archived = []",
        "    for filepath in dir_path.glob(\"*.json\"):",
        "        if keep_consolidated and filepath.name.startswith(\"consolidated_\"):",
        "            continue",
        "",
        "        dest = archive_path / filepath.name",
        "        shutil.move(str(filepath), str(dest))",
        "        archived.append(dest)",
        "",
        "    return archived",
        "",
        "",
        "def print_summary(tasks: List[Task], conflicts: Dict[str, List[Task]]) -> None:",
        "    \"\"\"Print a summary of task state.\"\"\"",
        "    by_status = defaultdict(list)",
        "    for task in tasks:",
        "        by_status[task.status].append(task)",
        "",
        "    print(\"\\n=== Task Summary ===\\n\")",
        "    print(f\"Total tasks: {len(tasks)}\")",
        "    print(f\"  In Progress: {len(by_status['in_progress'])}\")",
        "    print(f\"  Pending:     {len(by_status['pending'])}\")",
        "    print(f\"  Completed:   {len(by_status['completed'])}\")",
        "    print(f\"  Deferred:    {len(by_status['deferred'])}\")",
        "",
        "    if conflicts:",
        "        print(f\"\\n‚ö†Ô∏è  Found {len(conflicts)} potential duplicate groups:\")",
        "        for title, group in conflicts.items():",
        "            print(f\"  - \\\"{title[:50]}...\\\" ({len(group)} tasks)\")",
        "            for task in group:",
        "                print(f\"      {task.id} [{task.status}]\")",
        "",
        "",
        "def main():",
        "    parser = argparse.ArgumentParser(",
        "        description=\"Consolidate task files from parallel agent sessions\"",
        "    )",
        "    parser.add_argument(",
        "        \"--dir\", default=DEFAULT_TASKS_DIR,",
        "        help=\"Tasks directory (default: tasks/)\"",
        "    )",
        "    parser.add_argument(",
        "        \"--dry-run\", action=\"store_true\",",
        "        help=\"Show what would be done without making changes\"",
        "    )",
        "    parser.add_argument(",
        "        \"--summary\", action=\"store_true\",",
        "        help=\"Show summary only\"",
        "    )",
        "    parser.add_argument(",
        "        \"--update\", action=\"store_true\",",
        "        help=\"Write consolidated file and archive old files\"",
        "    )",
        "    parser.add_argument(",
        "        \"--auto-merge\", action=\"store_true\",",
        "        help=\"Automatically merge duplicate tasks\"",
        "    )",
        "    parser.add_argument(",
        "        \"--output\", help=\"Output file for consolidated JSON\"",
        "    )",
        "    parser.add_argument(",
        "        \"--archive\", action=\"store_true\",",
        "        help=\"Archive old session files after consolidation\"",
        "    )",
        "",
        "    args = parser.parse_args()",
        "",
        "    # Check if tasks directory exists",
        "    if not Path(args.dir).exists():",
        "        print(f\"Tasks directory '{args.dir}' does not exist.\")",
        "        print(\"No tasks to consolidate. Use task_utils.py to create tasks first.\")",
        "        return",
        "",
        "    # Load and analyze tasks",
        "    tasks, conflicts = consolidate_and_dedupe(args.dir, args.auto_merge)",
        "",
        "    if not tasks:",
        "        print(\"No tasks found.\")",
        "        return",
        "",
        "    # Always show summary",
        "    print_summary(tasks, conflicts)",
        "",
        "    if args.summary or args.dry_run:",
        "        if args.dry_run and args.update:",
        "            print(\"\\n[Dry run] Would consolidate to:\")",
        "            print(f\"  {args.dir}/consolidated_TIMESTAMP_XXXX.json\")",
        "            if args.archive:",
        "                print(f\"  Would archive {len(list(Path(args.dir).glob('*.json')))} files\")",
        "        return",
        "",
        "    if args.update:",
        "        # Write consolidated file",
        "        output_path = write_consolidated_file(tasks, args.dir)",
        "        print(f\"\\n‚úÖ Consolidated to: {output_path}\")",
        "",
        "        if args.archive:",
        "            archived = archive_old_session_files(args.dir)",
        "            print(f\"üì¶ Archived {len(archived)} session files\")",
        "",
        "    if conflicts and not args.auto_merge:",
        "        print(\"\\nüí° Tip: Use --auto-merge to automatically resolve duplicates\")",
        "",
        "",
        "if __name__ == \"__main__\":",
        "    main()"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "scripts/task_utils.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "#!/usr/bin/env python3",
        "\"\"\"",
        "Merge-friendly task ID management utilities.",
        "",
        "This module provides utilities for generating unique task IDs that won't",
        "conflict when multiple agents work in parallel. Follows the same pattern",
        "as cortical/chunk_index.py for append-only, git-friendly storage.",
        "",
        "Task ID Format:",
        "    T-YYYYMMDD-HHMMSS-XXXX",
        "",
        "    Where:",
        "    - T = Task prefix",
        "    - YYYYMMDD = Date created",
        "    - HHMMSS = Time created",
        "    - XXXX = 4-char random suffix (from session UUID)",
        "",
        "Example:",
        "    T-20251213-143052-a1b2",
        "",
        "Usage:",
        "    from scripts.task_utils import generate_task_id, TaskSession",
        "",
        "    # Simple ID generation",
        "    task_id = generate_task_id()  # T-20251213-143052-a1b2",
        "",
        "    # Session-based (all tasks in session share suffix)",
        "    session = TaskSession()",
        "    task1 = session.new_task_id()  # T-20251213-143052-a1b2",
        "    task2 = session.new_task_id()  # T-20251213-143053-a1b2",
        "\"\"\"",
        "",
        "import json",
        "import os",
        "import uuid",
        "from dataclasses import dataclass, field, asdict",
        "from datetime import datetime",
        "from pathlib import Path",
        "from typing import Dict, List, Optional, Any",
        "",
        "",
        "# Directory for per-session task files",
        "DEFAULT_TASKS_DIR = \"tasks\"",
        "",
        "",
        "def generate_session_id() -> str:",
        "    \"\"\"Generate a short session ID (4 hex chars).\"\"\"",
        "    return uuid.uuid4().hex[:4]",
        "",
        "",
        "def generate_task_id(session_id: Optional[str] = None) -> str:",
        "    \"\"\"",
        "    Generate a unique, merge-friendly task ID.",
        "",
        "    Args:",
        "        session_id: Optional session suffix. If None, generates random suffix.",
        "",
        "    Returns:",
        "        Task ID in format T-YYYYMMDD-HHMMSS-XXXX",
        "",
        "    Example:",
        "        >>> generate_task_id()",
        "        'T-20251213-143052-a1b2'",
        "        >>> generate_task_id(\"test\")",
        "        'T-20251213-143052-test'",
        "    \"\"\"",
        "    now = datetime.now()",
        "    date_str = now.strftime(\"%Y%m%d\")",
        "    time_str = now.strftime(\"%H%M%S\")",
        "    suffix = session_id or generate_session_id()",
        "    return f\"T-{date_str}-{time_str}-{suffix}\"",
        "",
        "",
        "def generate_short_task_id() -> str:",
        "    \"\"\"",
        "    Generate a shorter unique task ID (8 hex chars).",
        "",
        "    Returns:",
        "        Task ID in format T-XXXXXXXX",
        "",
        "    Example:",
        "        >>> generate_short_task_id()",
        "        'T-a1b2c3d4'",
        "    \"\"\"",
        "    return f\"T-{uuid.uuid4().hex[:8]}\"",
        "",
        "",
        "@dataclass",
        "class Task:",
        "    \"\"\"A single task with merge-friendly ID.\"\"\"",
        "    id: str",
        "    title: str",
        "    status: str = \"pending\"  # pending, in_progress, completed, deferred",
        "    priority: str = \"medium\"  # high, medium, low",
        "    category: str = \"general\"",
        "    description: str = \"\"",
        "    depends_on: List[str] = field(default_factory=list)",
        "    effort: str = \"medium\"  # small, medium, large",
        "    created_at: str = field(default_factory=lambda: datetime.now().isoformat())",
        "    updated_at: Optional[str] = None",
        "    completed_at: Optional[str] = None",
        "    context: Dict[str, Any] = field(default_factory=dict)",
        "",
        "    def to_dict(self) -> Dict[str, Any]:",
        "        \"\"\"Convert to dictionary for serialization.\"\"\"",
        "        return asdict(self)",
        "",
        "    @classmethod",
        "    def from_dict(cls, d: Dict[str, Any]) -> 'Task':",
        "        \"\"\"Create Task from dictionary.\"\"\"",
        "        return cls(**d)",
        "",
        "    def mark_complete(self) -> None:",
        "        \"\"\"Mark task as completed.\"\"\"",
        "        self.status = \"completed\"",
        "        self.completed_at = datetime.now().isoformat()",
        "        self.updated_at = self.completed_at",
        "",
        "    def mark_in_progress(self) -> None:",
        "        \"\"\"Mark task as in progress.\"\"\"",
        "        self.status = \"in_progress\"",
        "        self.updated_at = datetime.now().isoformat()",
        "",
        "",
        "@dataclass",
        "class TaskSession:",
        "    \"\"\"",
        "    A session for creating tasks with consistent session suffix.",
        "",
        "    All tasks created in a session share the same suffix, making it",
        "    easy to identify which tasks were created together.",
        "",
        "    Example:",
        "        session = TaskSession()",
        "        task1 = session.create_task(\"Implement feature X\")",
        "        task2 = session.create_task(\"Add tests for feature X\")",
        "        session.save()  # Writes to tasks/2025-12-13_14-30-52_a1b2.json",
        "    \"\"\"",
        "    session_id: str = field(default_factory=generate_session_id)",
        "    tasks: List[Task] = field(default_factory=list)",
        "    started_at: str = field(default_factory=lambda: datetime.now().isoformat())",
        "    tasks_dir: str = DEFAULT_TASKS_DIR",
        "",
        "    def new_task_id(self) -> str:",
        "        \"\"\"Generate a new task ID with this session's suffix.\"\"\"",
        "        return generate_task_id(self.session_id)",
        "",
        "    def create_task(",
        "        self,",
        "        title: str,",
        "        priority: str = \"medium\",",
        "        category: str = \"general\",",
        "        description: str = \"\",",
        "        depends_on: Optional[List[str]] = None,",
        "        effort: str = \"medium\",",
        "        context: Optional[Dict[str, Any]] = None",
        "    ) -> Task:",
        "        \"\"\"",
        "        Create a new task in this session.",
        "",
        "        Args:",
        "            title: Task title/summary",
        "            priority: high, medium, low",
        "            category: Task category (arch, devex, codequal, etc.)",
        "            description: Detailed description",
        "            depends_on: List of task IDs this depends on",
        "            effort: small, medium, large",
        "            context: Quick context dict (files, methods, etc.)",
        "",
        "        Returns:",
        "            The created Task object",
        "        \"\"\"",
        "        task = Task(",
        "            id=self.new_task_id(),",
        "            title=title,",
        "            priority=priority,",
        "            category=category,",
        "            description=description,",
        "            depends_on=depends_on or [],",
        "            effort=effort,",
        "            context=context or {}",
        "        )",
        "        self.tasks.append(task)",
        "        return task",
        "",
        "    def get_filename(self) -> str:",
        "        \"\"\"Get the session filename.\"\"\"",
        "        dt = datetime.fromisoformat(self.started_at)",
        "        timestamp = dt.strftime(\"%Y-%m-%d_%H-%M-%S\")",
        "        return f\"{timestamp}_{self.session_id}.json\"",
        "",
        "    def save(self, tasks_dir: Optional[str] = None) -> Path:",
        "        \"\"\"",
        "        Save session tasks to a JSON file.",
        "",
        "        Args:",
        "            tasks_dir: Directory for task files (default: tasks/)",
        "",
        "        Returns:",
        "            Path to the saved file",
        "        \"\"\"",
        "        dir_path = Path(tasks_dir or self.tasks_dir)",
        "        dir_path.mkdir(parents=True, exist_ok=True)",
        "",
        "        filepath = dir_path / self.get_filename()",
        "",
        "        data = {",
        "            \"version\": 1,",
        "            \"session_id\": self.session_id,",
        "            \"started_at\": self.started_at,",
        "            \"saved_at\": datetime.now().isoformat(),",
        "            \"tasks\": [t.to_dict() for t in self.tasks]",
        "        }",
        "",
        "        with open(filepath, 'w') as f:",
        "            json.dump(data, f, indent=2)",
        "",
        "        return filepath",
        "",
        "    @classmethod",
        "    def load(cls, filepath: Path) -> 'TaskSession':",
        "        \"\"\"Load a session from file.\"\"\"",
        "        with open(filepath) as f:",
        "            data = json.load(f)",
        "",
        "        session = cls(",
        "            session_id=data['session_id'],",
        "            started_at=data['started_at']",
        "        )",
        "        session.tasks = [Task.from_dict(t) for t in data['tasks']]",
        "        return session",
        "",
        "",
        "def load_all_tasks(tasks_dir: str = DEFAULT_TASKS_DIR) -> List[Task]:",
        "    \"\"\"",
        "    Load all tasks from all session files.",
        "",
        "    Args:",
        "        tasks_dir: Directory containing task session files",
        "",
        "    Returns:",
        "        List of all tasks, sorted by creation time",
        "    \"\"\"",
        "    dir_path = Path(tasks_dir)",
        "    if not dir_path.exists():",
        "        return []",
        "",
        "    all_tasks = []",
        "    for filepath in sorted(dir_path.glob(\"*.json\")):",
        "        try:",
        "            session = TaskSession.load(filepath)",
        "            all_tasks.extend(session.tasks)",
        "        except (json.JSONDecodeError, KeyError) as e:",
        "            print(f\"Warning: Could not load {filepath}: {e}\")",
        "",
        "    # Sort by creation time",
        "    all_tasks.sort(key=lambda t: t.created_at)",
        "    return all_tasks",
        "",
        "",
        "def get_task_by_id(task_id: str, tasks_dir: str = DEFAULT_TASKS_DIR) -> Optional[Task]:",
        "    \"\"\"Find a task by its ID across all session files.\"\"\"",
        "    for task in load_all_tasks(tasks_dir):",
        "        if task.id == task_id:",
        "            return task",
        "    return None",
        "",
        "",
        "def consolidate_tasks(",
        "    tasks_dir: str = DEFAULT_TASKS_DIR,",
        "    output_file: Optional[str] = None",
        ") -> Dict[str, List[Task]]:",
        "    \"\"\"",
        "    Consolidate all tasks from session files into a summary.",
        "",
        "    Args:",
        "        tasks_dir: Directory containing task session files",
        "        output_file: Optional path to write consolidated markdown",
        "",
        "    Returns:",
        "        Dict of tasks grouped by status",
        "    \"\"\"",
        "    all_tasks = load_all_tasks(tasks_dir)",
        "",
        "    # Group by status",
        "    grouped = {",
        "        \"in_progress\": [],",
        "        \"pending\": [],",
        "        \"completed\": [],",
        "        \"deferred\": []",
        "    }",
        "",
        "    for task in all_tasks:",
        "        status = task.status if task.status in grouped else \"pending\"",
        "        grouped[status].append(task)",
        "",
        "    # Sort within groups by priority",
        "    priority_order = {\"high\": 0, \"medium\": 1, \"low\": 2}",
        "    for status in grouped:",
        "        grouped[status].sort(key=lambda t: priority_order.get(t.priority, 1))",
        "",
        "    if output_file:",
        "        _write_consolidated_markdown(grouped, output_file)",
        "",
        "    return grouped",
        "",
        "",
        "def _write_consolidated_markdown(",
        "    grouped: Dict[str, List[Task]],",
        "    output_file: str",
        ") -> None:",
        "    \"\"\"Write consolidated tasks to markdown file.\"\"\"",
        "    lines = [",
        "        \"# Consolidated Task List\",",
        "        \"\",",
        "        f\"**Generated:** {datetime.now().isoformat()}\",",
        "        \"\",",
        "        \"---\",",
        "        \"\"",
        "    ]",
        "",
        "    status_headers = {",
        "        \"in_progress\": \"## üîÑ In Progress\",",
        "        \"pending\": \"## üìã Pending\",",
        "        \"completed\": \"## ‚úÖ Completed\",",
        "        \"deferred\": \"## ‚è∏Ô∏è Deferred\"",
        "    }",
        "",
        "    for status, tasks in grouped.items():",
        "        if not tasks:",
        "            continue",
        "",
        "        lines.append(status_headers.get(status, f\"## {status.title()}\"))",
        "        lines.append(\"\")",
        "        lines.append(\"| ID | Title | Priority | Category | Effort |\")",
        "        lines.append(\"|---|------|----------|----------|--------|\")",
        "",
        "        for task in tasks:",
        "            lines.append(",
        "                f\"| {task.id} | {task.title} | {task.priority} | \"",
        "                f\"{task.category} | {task.effort} |\"",
        "            )",
        "        lines.append(\"\")",
        "",
        "    with open(output_file, 'w') as f:",
        "        f.write('\\n'.join(lines))",
        "",
        "",
        "# CLI interface",
        "if __name__ == \"__main__\":",
        "    import argparse",
        "",
        "    parser = argparse.ArgumentParser(",
        "        description=\"Merge-friendly task management utilities\"",
        "    )",
        "    subparsers = parser.add_subparsers(dest=\"command\", help=\"Commands\")",
        "",
        "    # generate command",
        "    gen_parser = subparsers.add_parser(\"generate\", help=\"Generate a task ID\")",
        "    gen_parser.add_argument(\"--short\", action=\"store_true\", help=\"Short format (T-XXXXXXXX)\")",
        "",
        "    # consolidate command",
        "    cons_parser = subparsers.add_parser(\"consolidate\", help=\"Consolidate task files\")",
        "    cons_parser.add_argument(\"--dir\", default=DEFAULT_TASKS_DIR, help=\"Tasks directory\")",
        "    cons_parser.add_argument(\"--output\", help=\"Output markdown file\")",
        "",
        "    # list command",
        "    list_parser = subparsers.add_parser(\"list\", help=\"List all tasks\")",
        "    list_parser.add_argument(\"--dir\", default=DEFAULT_TASKS_DIR, help=\"Tasks directory\")",
        "    list_parser.add_argument(\"--status\", help=\"Filter by status\")",
        "",
        "    args = parser.parse_args()",
        "",
        "    if args.command == \"generate\":",
        "        if args.short:",
        "            print(generate_short_task_id())",
        "        else:",
        "            print(generate_task_id())",
        "",
        "    elif args.command == \"consolidate\":",
        "        grouped = consolidate_tasks(args.dir, args.output)",
        "        for status, tasks in grouped.items():",
        "            print(f\"{status}: {len(tasks)} tasks\")",
        "        if args.output:",
        "            print(f\"\\nWritten to {args.output}\")",
        "",
        "    elif args.command == \"list\":",
        "        tasks = load_all_tasks(args.dir)",
        "        if args.status:",
        "            tasks = [t for t in tasks if t.status == args.status]",
        "",
        "        for task in tasks:",
        "            print(f\"[{task.status}] {task.id}: {task.title}\")",
        "",
        "    else:",
        "        parser.print_help()"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "tests/unit/test_task_utils.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "\"\"\"Unit tests for merge-friendly task ID utilities.\"\"\"",
        "",
        "import json",
        "import os",
        "import sys",
        "import tempfile",
        "import unittest",
        "from datetime import datetime",
        "from pathlib import Path",
        "",
        "# Add scripts to path",
        "sys.path.insert(0, str(Path(__file__).parent.parent.parent / \"scripts\"))",
        "",
        "from task_utils import (",
        "    generate_task_id,",
        "    generate_short_task_id,",
        "    generate_session_id,",
        "    Task,",
        "    TaskSession,",
        "    load_all_tasks,",
        "    get_task_by_id,",
        "    consolidate_tasks,",
        ")",
        "",
        "",
        "class TestTaskIdGeneration(unittest.TestCase):",
        "    \"\"\"Tests for task ID generation.\"\"\"",
        "",
        "    def test_generate_task_id_format(self):",
        "        \"\"\"Task ID should have correct format.\"\"\"",
        "        task_id = generate_task_id()",
        "        # Format: T-YYYYMMDD-HHMMSS-XXXX",
        "        self.assertTrue(task_id.startswith(\"T-\"))",
        "        parts = task_id.split(\"-\")",
        "        self.assertEqual(len(parts), 4)",
        "        self.assertEqual(len(parts[1]), 8)  # YYYYMMDD",
        "        self.assertEqual(len(parts[2]), 6)  # HHMMSS",
        "        self.assertEqual(len(parts[3]), 4)  # session suffix",
        "",
        "    def test_generate_task_id_with_session(self):",
        "        \"\"\"Task ID should use provided session suffix.\"\"\"",
        "        task_id = generate_task_id(\"test\")",
        "        self.assertTrue(task_id.endswith(\"-test\"))",
        "",
        "    def test_generate_short_task_id(self):",
        "        \"\"\"Short task ID should be 10 characters.\"\"\"",
        "        task_id = generate_short_task_id()",
        "        # Format: T-XXXXXXXX",
        "        self.assertTrue(task_id.startswith(\"T-\"))",
        "        self.assertEqual(len(task_id), 10)",
        "",
        "    def test_generate_session_id(self):",
        "        \"\"\"Session ID should be 4 hex characters.\"\"\"",
        "        session_id = generate_session_id()",
        "        self.assertEqual(len(session_id), 4)",
        "        # Should be valid hex",
        "        int(session_id, 16)",
        "",
        "    def test_unique_task_ids(self):",
        "        \"\"\"Generated task IDs should be unique.\"\"\"",
        "        ids = {generate_task_id() for _ in range(100)}",
        "        self.assertEqual(len(ids), 100)",
        "",
        "",
        "class TestTask(unittest.TestCase):",
        "    \"\"\"Tests for Task dataclass.\"\"\"",
        "",
        "    def test_task_creation(self):",
        "        \"\"\"Task should be created with required fields.\"\"\"",
        "        task = Task(id=\"T-test\", title=\"Test task\")",
        "        self.assertEqual(task.id, \"T-test\")",
        "        self.assertEqual(task.title, \"Test task\")",
        "        self.assertEqual(task.status, \"pending\")",
        "",
        "    def test_task_to_dict(self):",
        "        \"\"\"Task should serialize to dict.\"\"\"",
        "        task = Task(",
        "            id=\"T-test\",",
        "            title=\"Test task\",",
        "            priority=\"high\",",
        "            category=\"arch\"",
        "        )",
        "        d = task.to_dict()",
        "        self.assertEqual(d[\"id\"], \"T-test\")",
        "        self.assertEqual(d[\"title\"], \"Test task\")",
        "        self.assertEqual(d[\"priority\"], \"high\")",
        "",
        "    def test_task_from_dict(self):",
        "        \"\"\"Task should deserialize from dict.\"\"\"",
        "        d = {",
        "            \"id\": \"T-test\",",
        "            \"title\": \"Test task\",",
        "            \"status\": \"completed\",",
        "            \"priority\": \"low\",",
        "            \"category\": \"test\",",
        "            \"description\": \"\",",
        "            \"depends_on\": [],",
        "            \"effort\": \"small\",",
        "            \"created_at\": \"2025-12-13T00:00:00\",",
        "            \"updated_at\": None,",
        "            \"completed_at\": None,",
        "            \"context\": {}",
        "        }",
        "        task = Task.from_dict(d)",
        "        self.assertEqual(task.id, \"T-test\")",
        "        self.assertEqual(task.status, \"completed\")",
        "",
        "    def test_mark_complete(self):",
        "        \"\"\"mark_complete should update status and timestamp.\"\"\"",
        "        task = Task(id=\"T-test\", title=\"Test task\")",
        "        self.assertEqual(task.status, \"pending\")",
        "        self.assertIsNone(task.completed_at)",
        "",
        "        task.mark_complete()",
        "        self.assertEqual(task.status, \"completed\")",
        "        self.assertIsNotNone(task.completed_at)",
        "",
        "    def test_mark_in_progress(self):",
        "        \"\"\"mark_in_progress should update status and timestamp.\"\"\"",
        "        task = Task(id=\"T-test\", title=\"Test task\")",
        "        task.mark_in_progress()",
        "        self.assertEqual(task.status, \"in_progress\")",
        "        self.assertIsNotNone(task.updated_at)",
        "",
        "",
        "class TestTaskSession(unittest.TestCase):",
        "    \"\"\"Tests for TaskSession.\"\"\"",
        "",
        "    def setUp(self):",
        "        \"\"\"Create temporary directory for task files.\"\"\"",
        "        self.temp_dir = tempfile.mkdtemp()",
        "",
        "    def tearDown(self):",
        "        \"\"\"Clean up temporary directory.\"\"\"",
        "        import shutil",
        "        shutil.rmtree(self.temp_dir)",
        "",
        "    def test_session_id_consistency(self):",
        "        \"\"\"All tasks in session should share same suffix.\"\"\"",
        "        session = TaskSession()",
        "        id1 = session.new_task_id()",
        "        id2 = session.new_task_id()",
        "",
        "        suffix1 = id1.split(\"-\")[-1]",
        "        suffix2 = id2.split(\"-\")[-1]",
        "",
        "        self.assertEqual(suffix1, suffix2)",
        "        self.assertEqual(suffix1, session.session_id)",
        "",
        "    def test_create_task(self):",
        "        \"\"\"create_task should add task to session.\"\"\"",
        "        session = TaskSession()",
        "        task = session.create_task(",
        "            title=\"Test task\",",
        "            priority=\"high\"",
        "        )",
        "",
        "        self.assertEqual(len(session.tasks), 1)",
        "        self.assertEqual(task.title, \"Test task\")",
        "        self.assertEqual(task.priority, \"high\")",
        "",
        "    def test_save_and_load(self):",
        "        \"\"\"Session should save and load correctly.\"\"\"",
        "        session = TaskSession()",
        "        session.create_task(title=\"Task 1\")",
        "        session.create_task(title=\"Task 2\")",
        "",
        "        filepath = session.save(self.temp_dir)",
        "        self.assertTrue(filepath.exists())",
        "",
        "        loaded = TaskSession.load(filepath)",
        "        self.assertEqual(len(loaded.tasks), 2)",
        "        self.assertEqual(loaded.tasks[0].title, \"Task 1\")",
        "",
        "    def test_session_filename_format(self):",
        "        \"\"\"Session filename should have correct format.\"\"\"",
        "        session = TaskSession()",
        "        filename = session.get_filename()",
        "        # Format: YYYY-MM-DD_HH-MM-SS_XXXX.json",
        "        self.assertTrue(filename.endswith(\".json\"))",
        "        parts = filename[:-5].split(\"_\")  # Remove .json",
        "        self.assertEqual(len(parts), 3)",
        "",
        "",
        "class TestTaskLoading(unittest.TestCase):",
        "    \"\"\"Tests for loading tasks from multiple files.\"\"\"",
        "",
        "    def setUp(self):",
        "        \"\"\"Create temporary directory with task files.\"\"\"",
        "        self.temp_dir = tempfile.mkdtemp()",
        "",
        "        # Create two sessions",
        "        session1 = TaskSession()",
        "        session1.create_task(title=\"Session 1 Task 1\")",
        "        session1.create_task(title=\"Session 1 Task 2\")",
        "        session1.save(self.temp_dir)",
        "",
        "        session2 = TaskSession()",
        "        session2.create_task(title=\"Session 2 Task 1\")",
        "        session2.save(self.temp_dir)",
        "",
        "    def tearDown(self):",
        "        \"\"\"Clean up temporary directory.\"\"\"",
        "        import shutil",
        "        shutil.rmtree(self.temp_dir)",
        "",
        "    def test_load_all_tasks(self):",
        "        \"\"\"load_all_tasks should load from all session files.\"\"\"",
        "        tasks = load_all_tasks(self.temp_dir)",
        "        self.assertEqual(len(tasks), 3)",
        "",
        "    def test_load_empty_directory(self):",
        "        \"\"\"load_all_tasks should return empty list for missing directory.\"\"\"",
        "        tasks = load_all_tasks(\"/nonexistent\")",
        "        self.assertEqual(tasks, [])",
        "",
        "    def test_get_task_by_id(self):",
        "        \"\"\"get_task_by_id should find task across sessions.\"\"\"",
        "        tasks = load_all_tasks(self.temp_dir)",
        "        target_id = tasks[0].id",
        "",
        "        found = get_task_by_id(target_id, self.temp_dir)",
        "        self.assertIsNotNone(found)",
        "        self.assertEqual(found.id, target_id)",
        "",
        "    def test_get_task_by_id_not_found(self):",
        "        \"\"\"get_task_by_id should return None for missing ID.\"\"\"",
        "        found = get_task_by_id(\"T-nonexistent\", self.temp_dir)",
        "        self.assertIsNone(found)",
        "",
        "",
        "class TestConsolidation(unittest.TestCase):",
        "    \"\"\"Tests for task consolidation.\"\"\"",
        "",
        "    def setUp(self):",
        "        \"\"\"Create temporary directory with task files.\"\"\"",
        "        self.temp_dir = tempfile.mkdtemp()",
        "",
        "    def tearDown(self):",
        "        \"\"\"Clean up temporary directory.\"\"\"",
        "        import shutil",
        "        shutil.rmtree(self.temp_dir)",
        "",
        "    def test_consolidate_groups_by_status(self):",
        "        \"\"\"consolidate_tasks should group by status.\"\"\"",
        "        session = TaskSession()",
        "        task1 = session.create_task(title=\"Pending task\")",
        "        task2 = session.create_task(title=\"In progress task\")",
        "        task2.mark_in_progress()",
        "        task3 = session.create_task(title=\"Completed task\")",
        "        task3.mark_complete()",
        "        session.save(self.temp_dir)",
        "",
        "        grouped = consolidate_tasks(self.temp_dir)",
        "",
        "        self.assertEqual(len(grouped[\"pending\"]), 1)",
        "        self.assertEqual(len(grouped[\"in_progress\"]), 1)",
        "        self.assertEqual(len(grouped[\"completed\"]), 1)",
        "",
        "",
        "if __name__ == \"__main__\":",
        "    unittest.main()"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    }
  ],
  "hour_of_day": 22,
  "day_of_week": "Saturday",
  "seconds_since_last_commit": -142680,
  "is_merge": false,
  "is_initial": false,
  "parent_count": 1,
  "session_id": null,
  "related_chats": [],
  "ci_result": null,
  "reverted": false,
  "amended": false
}