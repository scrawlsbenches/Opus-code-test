{
  "hash": "13590a929974fbc6cf5c492dbf92237651cbf6ff",
  "message": "Add code search improvements (tasks #82, #83)",
  "author": "Claude",
  "timestamp": "2025-12-11 02:54:52 +0000",
  "branch": "claude/multi-index-design-DvifZ",
  "files_changed": [
    "TASK_LIST.md",
    "cortical/processor.py",
    "cortical/query.py",
    "cortical/tokenizer.py",
    "scripts/search_codebase.py"
  ],
  "insertions": 225,
  "deletions": 23,
  "hunks": [
    {
      "file": "TASK_LIST.md",
      "function": "raw_tokens = re.findall(r'\\b[a-zA-Z][a-zA-Z0-9_]*\\b', text)",
      "start_line": 2935,
      "lines_added": [
        "| 82 | High | Add code stop words filter for query expansion | ✓ Done | Code Search |",
        "| 83 | Medium | Add definition-aware boosting for class/def queries | ✓ Done | Code Search |"
      ],
      "lines_removed": [],
      "context_before": [
        "| 72 | High | Use programming query expansion in search | ✓ Done | Code Index |",
        "| 73 | Medium | Add \"Find Similar Code\" command | | Code Index |",
        "| 74 | Medium | Add \"Explain This Code\" command | | Developer Experience |",
        "| 75 | Medium | Add \"What Changed?\" semantic diff | | Developer Experience |",
        "| 76 | Medium | Add \"Suggest Related Files\" feature | | Developer Experience |",
        "| 77 | High | Add interactive \"Ask the Codebase\" mode | ✓ Done | Developer Experience |",
        "| 78 | Low | Add code pattern detection | | Developer Experience |",
        "| 79 | Low | Add corpus health dashboard | | Developer Experience |",
        "| 80 | Low | Add \"Learning Mode\" for new contributors | | Developer Experience |",
        "| 81 | High | Fix tokenizer underscore-prefixed identifiers | ✓ Done | Code Search |"
      ],
      "context_after": [
        "",
        "---",
        "",
        "*Added 2025-12-11, completions updated 2025-12-11*"
      ],
      "change_type": "add"
    },
    {
      "file": "cortical/processor.py",
      "function": "class CorticalTextProcessor:",
      "start_line": 1261,
      "lines_added": [
        "        filter_code_stop_words: bool = False,",
        "            filter_code_stop_words: Filter ubiquitous code tokens (self, cls, etc.)",
        "            use_code_concepts=use_code_concepts,",
        "            filter_code_stop_words=filter_code_stop_words",
        "        Also filters ubiquitous code tokens (self, cls, etc.) from expansion.",
        "            use_code_concepts=True,",
        "            filter_code_stop_words=True  # Filter self, cls, etc."
      ],
      "lines_removed": [
        "            use_code_concepts=use_code_concepts",
        "            use_code_concepts=True"
      ],
      "context_before": [
        "    ",
        "    def find_similar_by_embedding(self, term: str, top_n: int = 10) -> List[Tuple[str, float]]:",
        "        return emb_module.find_similar_by_embedding(self.embeddings, term, top_n)",
        "    ",
        "    def expand_query(",
        "        self,",
        "        query_text: str,",
        "        max_expansions: int = 10,",
        "        use_variants: bool = True,",
        "        use_code_concepts: bool = False,"
      ],
      "context_after": [
        "        verbose: bool = False",
        "    ) -> Dict[str, float]:",
        "        \"\"\"",
        "        Expand a query using lateral connections and concept clusters.",
        "",
        "        Args:",
        "            query_text: Original query string",
        "            max_expansions: Maximum expansion terms to add",
        "            use_variants: Try word variants when direct match fails",
        "            use_code_concepts: Include programming synonym expansions",
        "",
        "        Returns:",
        "            Dict mapping terms to weights",
        "        \"\"\"",
        "        return query_module.expand_query(",
        "            query_text,",
        "            self.layers,",
        "            self.tokenizer,",
        "            max_expansions=max_expansions,",
        "            use_variants=use_variants,",
        "        )",
        "",
        "    def expand_query_for_code(self, query_text: str, max_expansions: int = 15) -> Dict[str, float]:",
        "        \"\"\"",
        "        Expand a query optimized for code search.",
        "",
        "        Enables code concept expansion to find programming synonyms",
        "        (e.g., \"fetch\" also matches \"get\", \"load\", \"retrieve\").",
        "",
        "        Args:",
        "            query_text: Original query string",
        "            max_expansions: Maximum expansion terms to add",
        "",
        "        Returns:",
        "            Dict mapping terms to weights",
        "        \"\"\"",
        "        return query_module.expand_query(",
        "            query_text,",
        "            self.layers,",
        "            self.tokenizer,",
        "            max_expansions=max_expansions,",
        "            use_variants=True,",
        "        )",
        "",
        "    def expand_query_cached(",
        "        self,",
        "        query_text: str,",
        "        max_expansions: int = 10,",
        "        use_variants: bool = True,",
        "        use_code_concepts: bool = False",
        "    ) -> Dict[str, float]:",
        "        \"\"\""
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/query.py",
      "function": "Query expansion and search functionality.",
      "start_line": 7,
      "lines_added": [
        "from .tokenizer import Tokenizer, CODE_EXPANSION_STOP_WORDS"
      ],
      "lines_removed": [
        "from .tokenizer import Tokenizer"
      ],
      "context_before": [
        "Provides methods for expanding queries using lateral connections,",
        "concept clusters, and word variants, then searching the corpus",
        "using TF-IDF and graph-based scoring.",
        "\"\"\"",
        "",
        "from typing import Dict, List, Tuple, Optional, TypedDict, Any",
        "from collections import defaultdict",
        "import re",
        "",
        "from .layers import CorticalLayer, HierarchicalLayer"
      ],
      "context_after": [
        "from .code_concepts import expand_code_concepts, get_related_terms",
        "",
        "",
        "# Intent types for query understanding",
        "class ParsedIntent(TypedDict):",
        "    \"\"\"Structured representation of a parsed query intent.\"\"\"",
        "    action: Optional[str]       # The verb/action (e.g., \"handle\", \"implement\")",
        "    subject: Optional[str]      # The main subject (e.g., \"authentication\")",
        "    intent: str                 # Query intent type (location, implementation, definition, etc.)",
        "    question_word: Optional[str]  # Original question word if present"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/query.py",
      "function": "def find_documents_with_boost(",
      "start_line": 234,
      "lines_added": [
        "    use_code_concepts: bool = False,",
        "    filter_code_stop_words: bool = False",
        "        filter_code_stop_words: Filter ubiquitous code tokens (self, cls, etc.)",
        "                                from expansion candidates. Useful for code search."
      ],
      "lines_removed": [
        "    use_code_concepts: bool = False"
      ],
      "context_before": [
        "",
        "",
        "def expand_query(",
        "    query_text: str,",
        "    layers: Dict[CorticalLayer, HierarchicalLayer],",
        "    tokenizer: Tokenizer,",
        "    max_expansions: int = 10,",
        "    use_lateral: bool = True,",
        "    use_concepts: bool = True,",
        "    use_variants: bool = True,"
      ],
      "context_after": [
        ") -> Dict[str, float]:",
        "    \"\"\"",
        "    Expand a query using lateral connections and concept clusters.",
        "",
        "    This mimics how the brain retrieves related memories when given a cue:",
        "    - Lateral connections: direct word associations (like priming)",
        "    - Concept clusters: semantic category membership",
        "    - Word variants: stemming and synonym mapping",
        "    - Code concepts: programming synonym groups (get/fetch/load)",
        "",
        "    Args:",
        "        query_text: Original query string",
        "        layers: Dictionary of layers",
        "        tokenizer: Tokenizer instance",
        "        max_expansions: Maximum number of expansion terms to add",
        "        use_lateral: Include terms from lateral connections",
        "        use_concepts: Include terms from concept clusters",
        "        use_variants: Try word variants when direct match fails",
        "        use_code_concepts: Include programming synonym expansions",
        "",
        "    Returns:",
        "        Dict mapping terms to weights (original terms get weight 1.0)",
        "    \"\"\"",
        "    tokens = tokenizer.tokenize(query_text)",
        "    layer0 = layers[CorticalLayer.TOKENS]",
        "    layer2 = layers.get(CorticalLayer.CONCEPTS)",
        "    ",
        "    # Start with original terms at full weight",
        "    expanded: Dict[str, float] = {}"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/query.py",
      "function": "def expand_query(",
      "start_line": 337,
      "lines_added": [
        "    # Filter out ubiquitous code tokens if requested",
        "    if filter_code_stop_words:",
        "        candidate_expansions = {",
        "            term: score for term, score in candidate_expansions.items()",
        "            if term not in CODE_EXPANSION_STOP_WORDS",
        "        }",
        "",
        "",
        "",
        "class DefinitionQuery(TypedDict):",
        "    \"\"\"Info about a definition-seeking query.\"\"\"",
        "    is_definition_query: bool",
        "    definition_type: Optional[str]  # 'class', 'function', 'method', 'variable'",
        "    identifier: Optional[str]       # The identifier being searched for",
        "    pattern: Optional[str]          # Regex pattern to find the definition",
        "",
        "",
        "def detect_definition_query(query_text: str) -> DefinitionQuery:",
        "    \"\"\"",
        "    Detect if a query is searching for a code definition.",
        "",
        "    Recognizes patterns like:",
        "    - \"class Minicolumn\" -> looking for class definition",
        "    - \"def compute_tfidf\" -> looking for function definition",
        "    - \"function handleClick\" -> looking for function definition",
        "",
        "    Args:",
        "        query_text: The search query",
        "",
        "    Returns:",
        "        DefinitionQuery with detection results and pattern to search for",
        "    \"\"\"",
        "    query_lower = query_text.lower().strip()",
        "",
        "    # Patterns for definition searches",
        "    patterns = [",
        "        # \"class ClassName\" or \"class ClassName definition\"",
        "        (r'\\bclass\\s+([A-Za-z_][A-Za-z0-9_]*)', 'class',",
        "         lambda name: rf'\\bclass\\s+{re.escape(name)}\\s*[:\\(]'),",
        "        # \"def function_name\" or \"function function_name\"",
        "        (r'\\b(?:def|function)\\s+([A-Za-z_][A-Za-z0-9_]*)', 'function',",
        "         lambda name: rf'\\bdef\\s+{re.escape(name)}\\s*\\('),",
        "        # \"method methodName\"",
        "        (r'\\bmethod\\s+([A-Za-z_][A-Za-z0-9_]*)', 'method',",
        "         lambda name: rf'\\bdef\\s+{re.escape(name)}\\s*\\('),",
        "    ]",
        "",
        "    for regex, def_type, pattern_fn in patterns:",
        "        match = re.search(regex, query_text, re.IGNORECASE)",
        "        if match:",
        "            identifier = match.group(1)",
        "            return DefinitionQuery(",
        "                is_definition_query=True,",
        "                definition_type=def_type,",
        "                identifier=identifier,",
        "                pattern=pattern_fn(identifier)",
        "            )",
        "",
        "    return DefinitionQuery(",
        "        is_definition_query=False,",
        "        definition_type=None,",
        "        identifier=None,",
        "        pattern=None",
        "    )",
        "",
        "",
        "def apply_definition_boost(",
        "    passages: List[Tuple[str, str, int, int, float]],",
        "    query_text: str,",
        "    boost_factor: float = 3.0",
        ") -> List[Tuple[str, str, int, int, float]]:",
        "    \"\"\"",
        "    Boost passages that contain actual code definitions matching the query.",
        "",
        "    When searching for \"class Minicolumn\", passages containing the actual",
        "    class definition (`class Minicolumn:`) get boosted over passages that",
        "    merely reference or use the class.",
        "",
        "    Args:",
        "        passages: List of (text, doc_id, start, end, score) tuples",
        "        query_text: The original search query",
        "        boost_factor: Multiplier for definition-containing passages (default 3.0)",
        "",
        "    Returns:",
        "        Re-scored passages with definition boost applied, sorted by new score",
        "    \"\"\"",
        "    definition_info = detect_definition_query(query_text)",
        "",
        "    if not definition_info['is_definition_query'] or not definition_info['pattern']:",
        "        return passages",
        "",
        "    pattern = re.compile(definition_info['pattern'], re.IGNORECASE)",
        "    boosted_passages = []",
        "",
        "    for text, doc_id, start, end, score in passages:",
        "        if pattern.search(text):",
        "            # This passage contains the actual definition",
        "            boosted_passages.append((text, doc_id, start, end, score * boost_factor))",
        "        else:",
        "            boosted_passages.append((text, doc_id, start, end, score))",
        "",
        "    # Re-sort by boosted scores",
        "    boosted_passages.sort(key=lambda x: x[4], reverse=True)",
        "    return boosted_passages",
        "",
        "",
        "def boost_definition_documents(",
        "    doc_results: List[Tuple[str, float]],",
        "    query_text: str,",
        "    documents: Dict[str, str],",
        "    boost_factor: float = 2.0",
        ") -> List[Tuple[str, float]]:",
        "    \"\"\"",
        "    Boost documents that contain the actual definition being searched for.",
        "",
        "    This helps ensure the source file containing a class/function definition",
        "    is included in the document candidates, even if test files mention the",
        "    identifier more frequently.",
        "",
        "    Args:",
        "        doc_results: List of (doc_id, score) tuples",
        "        query_text: The original search query",
        "        documents: Dict mapping doc_id to document text",
        "        boost_factor: Multiplier for definition-containing docs (default 2.0)",
        "",
        "    Returns:",
        "        Re-scored document results with definition boost applied",
        "    \"\"\"",
        "    definition_info = detect_definition_query(query_text)",
        "",
        "    if not definition_info['is_definition_query'] or not definition_info['pattern']:",
        "        return doc_results",
        "",
        "    pattern = re.compile(definition_info['pattern'], re.IGNORECASE)",
        "    boosted_docs = []",
        "",
        "    for doc_id, score in doc_results:",
        "        doc_text = documents.get(doc_id, '')",
        "        if pattern.search(doc_text):",
        "            # This document contains the actual definition",
        "            boosted_docs.append((doc_id, score * boost_factor))",
        "        else:",
        "            boosted_docs.append((doc_id, score))",
        "",
        "    # Re-sort by boosted scores",
        "    boosted_docs.sort(key=lambda x: x[1], reverse=True)",
        "    return boosted_docs",
        "",
        ""
      ],
      "lines_removed": [
        "    ",
        "    "
      ],
      "context_before": [
        "            list(expanded.keys()),",
        "            max_expansions_per_term=3,",
        "            weight=0.6",
        "        )",
        "        for term, weight in code_expansions.items():",
        "            if term not in expanded:",
        "                candidate_expansions[term] = max(",
        "                    candidate_expansions[term], weight",
        "                )",
        ""
      ],
      "context_after": [
        "    # Select top expansions",
        "    sorted_candidates = sorted(",
        "        candidate_expansions.items(),",
        "        key=lambda x: x[1],",
        "        reverse=True",
        "    )[:max_expansions]",
        "    for term, score in sorted_candidates:",
        "        expanded[term] = score",
        "    return expanded",
        "",
        "",
        "def parse_intent_query(query_text: str) -> ParsedIntent:",
        "    \"\"\"",
        "    Parse a natural language query to extract intent and searchable terms.",
        "",
        "    Analyzes queries like \"where do we handle authentication?\" to identify:",
        "    - Question word (where) -> intent type (location)",
        "    - Action verb (handle) -> search for handling code",
        "    - Subject (authentication) -> main topic with synonyms",
        "",
        "    Args:"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/query.py",
      "function": "def expand_query_semantic(",
      "start_line": 763,
      "lines_added": [
        "    semantic_discount: float = 0.8,",
        "    filter_code_stop_words: bool = False",
        "        filter_code_stop_words: Filter ubiquitous code tokens (self, cls, etc.)",
        "                                from expansion candidates. Useful for code search.",
        "        query_terms = expand_query(",
        "            query_text, layers, tokenizer,",
        "            max_expansions=max_expansions,",
        "            filter_code_stop_words=filter_code_stop_words",
        "        )"
      ],
      "lines_removed": [
        "    semantic_discount: float = 0.8",
        "        query_terms = expand_query(query_text, layers, tokenizer, max_expansions=max_expansions)"
      ],
      "context_before": [
        "",
        "",
        "def get_expanded_query_terms(",
        "    query_text: str,",
        "    layers: Dict[CorticalLayer, HierarchicalLayer],",
        "    tokenizer: Tokenizer,",
        "    use_expansion: bool = True,",
        "    semantic_relations: Optional[List[Tuple[str, str, str, float]]] = None,",
        "    use_semantic: bool = True,",
        "    max_expansions: int = 5,"
      ],
      "context_after": [
        ") -> Dict[str, float]:",
        "    \"\"\"",
        "    Get expanded query terms with optional semantic expansion.",
        "",
        "    This is a helper function that consolidates query expansion logic used",
        "    by multiple search functions. It handles:",
        "    - Lateral connection expansion via expand_query()",
        "    - Semantic relation expansion via expand_query_semantic()",
        "    - Merging of expansion results with appropriate weighting",
        "",
        "    Args:",
        "        query_text: Original query string",
        "        layers: Dictionary of layers",
        "        tokenizer: Tokenizer instance",
        "        use_expansion: Whether to expand query terms using lateral connections",
        "        semantic_relations: Optional list of semantic relations for expansion",
        "        use_semantic: Whether to use semantic relations for expansion",
        "        max_expansions: Maximum expansion terms per method (default 5)",
        "        semantic_discount: Weight multiplier for semantic expansions (default 0.8)",
        "",
        "    Returns:",
        "        Dict mapping terms to weights (original terms get weight 1.0,",
        "        expansions get lower weights based on connection strength)",
        "",
        "    Example:",
        "        >>> terms = get_expanded_query_terms(\"neural networks\", layers, tokenizer)",
        "        >>> # Returns: {'neural': 1.0, 'networks': 1.0, 'deep': 0.3, 'learning': 0.25, ...}",
        "    \"\"\"",
        "    if use_expansion:",
        "        # Start with lateral connection expansion",
        "",
        "        # Add semantic expansion if available",
        "        if use_semantic and semantic_relations:",
        "            semantic_terms = expand_query_semantic(",
        "                query_text, layers, tokenizer, semantic_relations, max_expansions=max_expansions",
        "            )",
        "            # Merge semantic expansions (don't override stronger weights)",
        "            for term, weight in semantic_terms.items():",
        "                if term not in query_terms:",
        "                    query_terms[term] = weight * semantic_discount"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/query.py",
      "function": "def find_passages_for_query(",
      "start_line": 1315,
      "lines_added": [
        "    # Get candidate documents",
        "        # Use provided filter directly as candidates (caller may have pre-boosted)",
        "        # Assign dummy scores since we'll re-score passages anyway",
        "        doc_scores = [(doc_id, 1.0) for doc_id in doc_filter if doc_id in documents]",
        "    else:",
        "        # No filter - get candidates via document search",
        "        doc_scores = find_documents_for_query(",
        "            query_text, layers, tokenizer,",
        "            top_n=min(len(documents), top_n * 3),",
        "            use_expansion=use_expansion,",
        "            semantic_relations=semantic_relations,",
        "            use_semantic=use_semantic",
        "        )"
      ],
      "lines_removed": [
        "    # First, get candidate documents (more than we need, since we'll rank passages)",
        "    doc_scores = find_documents_for_query(",
        "        query_text, layers, tokenizer,",
        "        top_n=min(len(documents), top_n * 3),",
        "        use_expansion=use_expansion,",
        "        semantic_relations=semantic_relations,",
        "        use_semantic=use_semantic",
        "    )",
        "",
        "    # Apply document filter if provided",
        "        doc_scores = [(doc_id, score) for doc_id, score in doc_scores if doc_id in doc_filter]"
      ],
      "context_before": [
        "        semantic_relations=semantic_relations,",
        "        use_semantic=use_semantic",
        "    )",
        "",
        "    if not query_terms:",
        "        return []",
        "",
        "    # Pre-compute minicolumn lookups for query terms (optimization)",
        "    term_cols = precompute_term_cols(query_terms, layer0)",
        ""
      ],
      "context_after": [
        "    if doc_filter:",
        "",
        "    # Score passages within candidate documents",
        "    passages: List[Tuple[str, str, int, int, float]] = []",
        "",
        "    for doc_id, doc_score in doc_scores:",
        "        if doc_id not in documents:",
        "            continue",
        "",
        "        text = documents[doc_id]",
        "        chunks = create_chunks(text, chunk_size, overlap)"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/tokenizer.py",
      "function": "Text tokenization with stemming and word variant support.",
      "start_line": 6,
      "lines_added": [
        "# Ubiquitous code tokens that pollute query expansion",
        "# These appear in almost every Python method/function, so they add noise",
        "# rather than signal when expanding queries for code search",
        "CODE_EXPANSION_STOP_WORDS = frozenset({",
        "    'self', 'cls',              # Class method parameters",
        "    'args', 'kwargs',           # Variadic parameters",
        "    'none', 'true', 'false',    # Literals (too common)",
        "    'return', 'pass',           # Control flow (too common)",
        "    'def', 'class',             # Definitions (search for these explicitly)",
        "})",
        "",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "",
        "Like early visual processing, the tokenizer extracts basic features",
        "(words) from raw input, filtering noise (stop words) and normalizing",
        "representations (lowercase, stemming).",
        "\"\"\"",
        "",
        "import re",
        "from typing import List, Set, Optional, Dict, Tuple",
        "",
        ""
      ],
      "context_after": [
        "# Programming keywords that should be preserved even if in stop words",
        "PROGRAMMING_KEYWORDS = frozenset({",
        "    'def', 'class', 'function', 'return', 'import', 'from', 'if', 'else',",
        "    'elif', 'for', 'while', 'try', 'except', 'finally', 'with', 'as',",
        "    'yield', 'async', 'await', 'lambda', 'pass', 'break', 'continue',",
        "    'raise', 'assert', 'global', 'nonlocal', 'del', 'true', 'false',",
        "    'none', 'null', 'void', 'int', 'str', 'float', 'bool', 'list',",
        "    'dict', 'set', 'tuple', 'self', 'cls', 'init', 'main', 'args',",
        "    'kwargs', 'super', 'property', 'staticmethod', 'classmethod',",
        "    'isinstance', 'hasattr', 'getattr', 'setattr', 'len', 'range',"
      ],
      "change_type": "add"
    },
    {
      "file": "scripts/search_codebase.py",
      "function": "Usage:",
      "start_line": 12,
      "lines_added": [
        "from cortical.query import (",
        "    apply_definition_boost,",
        "    boost_definition_documents,",
        "    detect_definition_query",
        ")"
      ],
      "lines_removed": [],
      "context_before": [
        "",
        "import argparse",
        "import os",
        "import sys",
        "from pathlib import Path",
        "",
        "# Add parent directory to path for imports",
        "sys.path.insert(0, str(Path(__file__).parent.parent))",
        "",
        "from cortical.processor import CorticalTextProcessor"
      ],
      "context_after": [
        "",
        "",
        "def find_line_number(doc_content: str, passage_start: int) -> int:",
        "    \"\"\"Find the line number for a character position.\"\"\"",
        "    return doc_content[:passage_start].count('\\n') + 1",
        "",
        "",
        "def format_passage(passage: str, max_width: int = 80) -> str:",
        "    \"\"\"Format a passage for display, truncating long lines.\"\"\"",
        "    lines = passage.split('\\n')"
      ],
      "change_type": "add"
    },
    {
      "file": "scripts/search_codebase.py",
      "function": "def search_codebase(",
      "start_line": 217,
      "lines_added": [
        "        doc_results = processor.find_documents_for_query(query, top_n=top_n * 3)",
        "            top_n=top_n * 3,  # Get more candidates for re-ranking",
        "    # Apply definition boost to documents (helps find class/def definitions)",
        "    doc_results = boost_definition_documents(",
        "        doc_results, query, processor.documents, boost_factor=2.0",
        "    )",
        "",
        "",
        "    # Check if this is a definition query - if so, fetch more candidates",
        "    definition_info = detect_definition_query(query)",
        "    candidate_multiplier = 5 if definition_info['is_definition_query'] else 2",
        "",
        "    # For definition queries, search fewer docs (boosted ones at top)",
        "    # For regular queries, search more broadly",
        "    doc_limit = top_n if definition_info['is_definition_query'] else top_n * 3",
        "",
        "        top_n=top_n * candidate_multiplier,  # More candidates for definition queries",
        "        doc_filter=doc_ids[:doc_limit] if doc_ids else None",
        "    # Apply definition boost for \"class X\" or \"def X\" queries",
        "    # This helps find actual definitions instead of just usages",
        "    results = apply_definition_boost(results, query, boost_factor=5.0)",
        "    results = results[:top_n]  # Trim after boosting",
        ""
      ],
      "lines_removed": [
        "        doc_results = processor.find_documents_for_query(query, top_n=top_n * 2)",
        "            top_n=top_n * 2,  # Get more candidates",
        "        top_n=top_n,",
        "        doc_filter=doc_ids[:top_n * 2] if doc_ids else None"
      ],
      "context_before": [
        "                'line': 1,",
        "                'passage': passage,",
        "                'score': score,",
        "                'reference': f\"{doc_id}:1\",",
        "                'doc_type': get_doc_type_label(doc_id)",
        "            })",
        "        return formatted_results",
        "",
        "    # Full passage search - first get top documents with boosting",
        "    if no_boost:"
      ],
      "context_after": [
        "    else:",
        "        doc_results = processor.find_documents_with_boost(",
        "            query,",
        "            auto_detect_intent=not prefer_docs,",
        "            prefer_docs=prefer_docs",
        "        )",
        "",
        "    # Then get passages from those documents",
        "    doc_ids = [doc_id for doc_id, _ in doc_results]",
        "    results = processor.find_passages_for_query(",
        "        query,",
        "        chunk_size=chunk_size,",
        "        overlap=100,",
        "    )",
        "",
        "    formatted_results = []",
        "    for passage, doc_id, start, end, score in results:",
        "        # Get the full document content to find line number",
        "        doc_content = processor.documents.get(doc_id, '')",
        "        line_num = find_line_number(doc_content, start)",
        "",
        "        formatted_results.append({",
        "            'file': doc_id,",
        "            'line': line_num,",
        "            'passage': passage,"
      ],
      "change_type": "modify"
    }
  ],
  "hour_of_day": 2,
  "day_of_week": "Thursday",
  "seconds_since_last_commit": -384596,
  "is_merge": false,
  "is_initial": false,
  "parent_count": 1,
  "session_id": null,
  "related_chats": [],
  "ci_result": null,
  "reverted": false,
  "amended": false
}