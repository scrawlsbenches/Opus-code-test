{
  "hash": "c7aaf9bb33e15aa8fe7e10f74536662227260d35",
  "message": "Merge main: task list cleanup and validation",
  "author": "Claude",
  "timestamp": "2025-12-13 13:18:42 +0000",
  "branch": "claude/multi-index-design-DvifZ",
  "files_changed": [
    "CLAUDE.md",
    "TASK_LIST.md",
    "scripts/validate_task_list.py"
  ],
  "insertions": 251,
  "deletions": 2195,
  "hunks": [
    {
      "file": "CLAUDE.md",
      "function": "You are a **senior computational neuroscience engineer** with deep expertise in:",
      "start_line": 28,
      "lines_added": [
        "- **Keep TASK_LIST.md current** - stale tasks waste investigative effort"
      ],
      "lines_removed": [],
      "context_before": [
        "",
        "**Test-Driven Confidence**",
        "- Maintain >89% code coverage before optimizations",
        "- Run the full test suite after every change",
        "- Write tests for the bug before writing the fix",
        "",
        "**Dog-Food Everything**",
        "- Use the system to test itself when possible",
        "- Real usage reveals issues that unit tests miss",
        "- Document all findings in TASK_LIST.md"
      ],
      "context_after": [
        "",
        "**Honest Assessment**",
        "- Acknowledge when something isn't working",
        "- Say \"I don't know\" when uncertain, then investigate",
        "- Correct course based on evidence, not pride",
        "",
        "When you see \"neural\" or \"cortical\" in this codebase, remember: these are metaphors for standard IR algorithms, not actual neural implementations.",
        "",
        "---",
        ""
      ],
      "change_type": "add"
    },
    {
      "file": "CLAUDE.md",
      "function": "python scripts/search_codebase.py \"expand query\"  # Find specific code",
      "start_line": 123,
      "lines_added": [
        "‚îú‚îÄ‚îÄ query/            # Search, retrieval, query expansion (split into 8 modules)",
        "‚îÇ   ‚îú‚îÄ‚îÄ __init__.py   # Re-exports public API",
        "‚îÇ   ‚îú‚îÄ‚îÄ expansion.py  # Query expansion",
        "‚îÇ   ‚îú‚îÄ‚îÄ search.py     # Document search",
        "‚îÇ   ‚îú‚îÄ‚îÄ passages.py   # Passage retrieval",
        "‚îÇ   ‚îú‚îÄ‚îÄ chunking.py   # Text chunking",
        "‚îÇ   ‚îú‚îÄ‚îÄ intent.py     # Intent-based queries",
        "‚îÇ   ‚îú‚îÄ‚îÄ definitions.py # Definition search",
        "‚îÇ   ‚îú‚îÄ‚îÄ ranking.py    # Multi-stage ranking",
        "‚îÇ   ‚îî‚îÄ‚îÄ analogy.py    # Analogy completion"
      ],
      "lines_removed": [
        "‚îú‚îÄ‚îÄ query.py          # Search, retrieval, query expansion (2,719 lines)"
      ],
      "context_before": [
        "```",
        "",
        "---",
        "",
        "## Architecture Map",
        "",
        "```",
        "cortical/",
        "‚îú‚îÄ‚îÄ processor.py      # Main orchestrator (2,301 lines) - START HERE",
        "‚îÇ                     # CorticalTextProcessor is the public API"
      ],
      "context_after": [
        "‚îú‚îÄ‚îÄ analysis.py       # Graph algorithms: PageRank, TF-IDF, clustering (1,123 lines)",
        "‚îú‚îÄ‚îÄ semantics.py      # Relation extraction, inheritance, retrofitting (915 lines)",
        "‚îú‚îÄ‚îÄ persistence.py    # Save/load with full state preservation (606 lines)",
        "‚îú‚îÄ‚îÄ chunk_index.py    # Git-friendly chunk-based storage (574 lines)",
        "‚îú‚îÄ‚îÄ tokenizer.py      # Tokenization, stemming, stop word removal (398 lines)",
        "‚îú‚îÄ‚îÄ minicolumn.py     # Core data structure with typed Edge connections (357 lines)",
        "‚îú‚îÄ‚îÄ config.py         # CorticalConfig dataclass with validation (352 lines)",
        "‚îú‚îÄ‚îÄ fingerprint.py    # Semantic fingerprinting and similarity (315 lines)",
        "‚îú‚îÄ‚îÄ layers.py         # HierarchicalLayer with O(1) ID lookups via _id_index (294 lines)",
        "‚îú‚îÄ‚îÄ code_concepts.py  # Programming concept synonyms for code search (249 lines)"
      ],
      "change_type": "modify"
    },
    {
      "file": "CLAUDE.md",
      "function": "Key defaults to know:",
      "start_line": 541,
      "lines_added": [
        "8. **Update TASK_LIST.md** - Mark task complete and move details to archive (see below)",
        "",
        "### Task Completion Checklist",
        "",
        "**CRITICAL:** Task list staleness caused 2,000+ lines of stale data. Always complete this checklist:",
        "",
        "1. **Mark task status** in TASK_LIST.md backlog tables (change to ‚úÖ or remove row)",
        "2. **Update \"Recently Completed\"** section with one-line summary",
        "3. **Move detailed task description** to TASK_ARCHIVE.md if substantial",
        "4. **Update counts** in header (`Pending Tasks:`, `Completed Tasks:`)",
        "5. **Remove from \"In Progress\"** section if applicable",
        "",
        "**Why this matters:**",
        "- Stale task lists waste time (agents investigate \"pending\" tasks that are done)",
        "- Inaccurate counts mislead planning",
        "- Bloated files slow navigation and context consumption",
        "",
        "**Quick validation:**",
        "```bash",
        "# Check for completed tasks still marked pending",
        "grep -n \"status:pending\" TASK_LIST.md | head -20",
        "# Should only show truly pending tasks",
        "```"
      ],
      "lines_removed": [],
      "context_before": [
        "   python -m coverage report --include=\"cortical/*\"",
        "   ```",
        "3. **Run the showcase** to verify integration:",
        "   ```bash",
        "   python showcase.py",
        "   ```",
        "4. **Check for regressions** in related functionality",
        "5. **Dog-food the feature** - test with real usage (see [dogfooding-checklist.md](docs/dogfooding-checklist.md))",
        "6. **Document all findings** - add issues to TASK_LIST.md (see [code-of-ethics.md](docs/code-of-ethics.md))",
        "7. **Verify completion** - use [definition-of-done.md](docs/definition-of-done.md) checklist"
      ],
      "context_after": [
        "",
        "---",
        "",
        "## Testing Patterns",
        "",
        "The codebase supports both `unittest` (legacy) and `pytest` (new categorized tests):",
        "",
        "### Pytest Pattern (Recommended for New Tests)",
        "",
        "```python"
      ],
      "change_type": "add"
    },
    {
      "file": "TASK_LIST.md",
      "function": null,
      "start_line": 1,
      "lines_added": [
        "**Pending Tasks:** 27",
        "**Completed Tasks:** 211 (see archive)",
        "- See Tasks #198-205 for legacy test investigation"
      ],
      "lines_removed": [
        "**Pending Tasks:** 48",
        "**Completed Tasks:** 189 (see archive)",
        "- See Tasks #197-204 for legacy test investigation"
      ],
      "context_before": [
        "# Task List: Cortical Text Processor",
        "",
        "Active backlog for the Cortical Text Processor project. Completed tasks are archived in [TASK_ARCHIVE.md](TASK_ARCHIVE.md).",
        "",
        "**Last Updated:** 2025-12-13"
      ],
      "context_after": [
        "",
        "**Legacy Test Cleanup:** 16 duplicated legacy tests removed, 13 remaining need investigation",
        "",
        "**Unit Test Initiative:** ‚úÖ COMPLETE - 85% coverage from unit tests (1,729 tests)",
        "- 19 modules at 90%+ coverage",
        "- See [Coverage Baseline](#unit-test-coverage-baseline) for per-module status",
        "",
        "---",
        "",
        "## Active Backlog",
        "",
        "<!-- Machine-parseable format for automation -->"
      ],
      "change_type": "modify"
    },
    {
      "file": "TASK_LIST.md",
      "function": "Active backlog for the Cortical Text Processor project. Completed tasks are arch",
      "start_line": 27,
      "lines_added": [
        "| 95 | Split processor.py into modules | Arch | - | Large |",
        "| 198 | Investigate legacy test_coverage_gaps.py (91 tests) | Testing | - | Medium |",
        "| 199 | Investigate legacy test_cli_wrapper.py (96 tests) | Testing | - | Medium |",
        "| 200 | Investigate legacy test_edge_cases.py (53 tests) | Testing | - | Small |",
        "| 201 | Investigate legacy test_incremental_indexing.py (47 tests) | Testing | - | Small |",
        "| 202 | Investigate legacy test_intent_query.py (24 tests) | Testing | - | Small |",
        "| 203 | Investigate legacy test_behavioral.py (9 tests) | Testing | - | Small |",
        "| 204 | Investigate legacy test_query_optimization.py (20 tests) | Testing | - | Small |",
        "| 205 | Investigate legacy script tests (6 files, 132 tests) | Testing | - | Medium |"
      ],
      "lines_removed": [
        "| 95 | Split processor.py into modules | Arch | 97 | Large |",
        "| 197 | Investigate legacy test_coverage_gaps.py (91 tests) - migrate or categorize | Testing | - | Medium |",
        "| 198 | Investigate legacy test_cli_wrapper.py (96 tests) - migrate or categorize | Testing | - | Medium |",
        "| 199 | Investigate legacy test_edge_cases.py (53 tests) - migrate or categorize | Testing | - | Small |",
        "| 200 | Investigate legacy test_incremental_indexing.py (47 tests) - migrate or categorize | Testing | - | Small |",
        "| 201 | Investigate legacy test_intent_query.py (24 tests) - migrate or categorize | Testing | - | Small |",
        "| 202 | Investigate legacy test_behavioral.py (9 tests) - keep as behavioral | Testing | - | Small |",
        "| 203 | Investigate legacy test_query_optimization.py (20 tests) - migrate to performance/ | Testing | - | Small |",
        "| 204 | Investigate legacy script tests (6 files) - categorize appropriately | Testing | - | Medium |"
      ],
      "context_before": [
        "| 192 | Deduplicate lateral_connections and typed_connections storage | Memory | - | Medium |",
        "",
        "### üü° Medium (Do This Month)",
        "",
        "| # | Task | Category | Depends | Effort |",
        "|---|------|----------|---------|--------|",
        "| 186 | Add simplified facade methods (quick_search, rag_retrieve) | API | - | Small |",
        "| 133 | Implement WAL + snapshot persistence (fault-tolerant rebuild) | Arch | 132 | Large |",
        "| 134 | Implement protobuf serialization for corpus | Arch | 132 | Medium |",
        "| 135 | Implement chunked parallel processing for full-analysis | Arch | 132 | Large |"
      ],
      "context_after": [
        "| 99 | Add input validation to public methods | CodeQual | - | Medium |",
        "| 107 | Add Quick Context to tasks | TaskMgmt | - | Medium |",
        "",
        "### üü¢ Low (Backlog)",
        "",
        "| # | Task | Category | Depends | Effort |",
        "|---|------|----------|---------|--------|",
        "| 73 | Add \"Find Similar Code\" command | DevEx | - | Medium |",
        "| 74 | Add \"Explain This Code\" command | DevEx | - | Medium |",
        "| 75 | Add \"What Changed?\" semantic diff | DevEx | - | Large |",
        "| 76 | Add \"Suggest Related Files\" feature | DevEx | - | Medium |",
        "| 78 | Add code pattern detection | DevEx | - | Large |"
      ],
      "change_type": "modify"
    },
    {
      "file": "TASK_LIST.md",
      "function": "Active backlog for the Cortical Text Processor project. Completed tasks are arch",
      "start_line": 61,
      "lines_added": [
        "| 197 | Add task list validation to CI | TaskMgmt | - | Small |"
      ],
      "lines_removed": [],
      "context_before": [
        "| 101 | Automate staleness tracking | Arch | - | Medium |",
        "| 106 | Add task dependency graph | TaskMgmt | - | Small |",
        "| 108 | Create task selection script | TaskMgmt | - | Medium |",
        "| 117 | Create debugging cookbook | AINav | - | Medium |",
        "| 118 | Add function complexity annotations | AINav | - | Small |",
        "| 140 | Analyze customer service cluster quality | Research | 127 | Small |",
        "| 129 | Test customer service retrieval quality | Testing | - | Small |",
        "| 130 | Expand customer service sample cluster | Samples | - | Medium |",
        "| 131 | Investigate cross-domain semantic bridges | Research | - | Medium |",
        "| 196 | Add runtime warning for spectral embeddings on large graphs | DevEx | - | Small |"
      ],
      "context_after": [
        "",
        "### ‚è∏Ô∏è Deferred",
        "",
        "| # | Task | Reason |",
        "|---|------|--------|",
        "| 110 | Add section markers to large files | Superseded by #119 (AI metadata generator) |",
        "| 111 | Add \"See Also\" cross-references | Superseded by #119 (AI metadata generator) |",
        "| 112 | Add docstring examples | Superseded by #119 (AI metadata generator) |",
        "| 7 | Document magic numbers in gaps.py | Low priority, functional as-is |",
        "| 42 | Add simple query language | Nice-to-have, not blocking |"
      ],
      "change_type": "add"
    },
    {
      "file": "TASK_LIST.md",
      "function": "Active backlog for the Cortical Text Processor project. Completed tasks are arch",
      "start_line": 88,
      "lines_added": [
        "<!-- Note: Task #87 was completed 2025-12-13, moved to archive -->",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "| 187 | Add async API support (AsyncCorticalTextProcessor) | Architecture | Enables FastAPI, async frameworks |",
        "| 188 | Add streaming query results | Architecture | Depends on #187 |",
        "| 189 | Add observability hooks (timing, traces, metrics) | DevEx | OpenTelemetry integration |",
        "| 190 | Create REST API wrapper (FastAPI) | Integration | Depends on #187 |",
        "| 191 | Add Interactive REPL mode | DevEx | `python -m cortical --interactive` |",
        "",
        "### üîÑ In Progress",
        "",
        "*No tasks currently in progress*",
        ""
      ],
      "context_after": [
        "---",
        "",
        "## Recently Completed",
        "",
        "All completed tasks are now archived in [TASK_ARCHIVE.md](TASK_ARCHIVE.md).",
        "",
        "**Latest completions (2025-12-13):**",
        "- #193 Unify alpha validation - retrofit_embeddings() now accepts [0,1] consistently",
        "- #194 Layer validation - Added checks for invalid layer values (0-3) in persistence/layers",
        "- #195 Stopwords import - semantics.py now uses Tokenizer.DEFAULT_STOP_WORDS"
      ],
      "change_type": "add"
    },
    {
      "file": "TASK_LIST.md",
      "function": "Every typed connection is duplicated in `lateral_connections` for backward compa",
      "start_line": 177,
      "lines_added": [
        "### 197. Add Task List Validation to CI",
        "**Meta:** `status:pending` `priority:low` `category:taskmgmt`",
        "**Files:** `.github/workflows/ci.yml`, `scripts/validate_task_list.py`",
        "**Effort:** Small",
        "**Problem:** Task list staleness can accumulate unnoticed between reviews. Manual validation catches issues but isn't enforced.",
        "**Solution:** Add validation step to CI:",
        "```yaml",
        "- name: Validate task list",
        "  run: python scripts/validate_task_list.py",
        "```",
        "**Acceptance:**",
        "- [ ] CI runs `validate_task_list.py` on every PR",
        "- [ ] Fails build if stale tasks detected",
        "- [ ] Clear error messages guide resolution",
        "## Legacy Test Investigation Tasks",
        "These tasks were created during the test coverage review (2025-12-13). 16 duplicated legacy tests were removed, and 13 remaining tests need investigation to determine if they should be migrated to categorized test directories or kept as-is. Check git history for any previous migration work.",
        "### 198. Investigate test_coverage_gaps.py (91 tests)",
        "**Meta:** `status:pending` `priority:medium` `category:testing`",
        "**Files:** `tests/test_coverage_gaps.py`",
        "**Problem:** 91 tests covering edge cases and coverage gaps. Investigate:",
        "1. Check git history for previous migration attempts",
        "2. Determine if tests should move to `tests/unit/` or `tests/regression/`",
        "3. Check for overlap with existing unit tests",
        "### 199. Investigate test_cli_wrapper.py (96 tests)",
        "**Meta:** `status:pending` `priority:medium` `category:testing`",
        "**Files:** `tests/test_cli_wrapper.py`",
        "**Problem:** 96 tests for CLI wrapper. Investigate:",
        "1. Check git history for migration attempts",
        "2. Determine if these belong in `tests/integration/`",
        "3. Verify no unit test coverage exists",
        "### 200. Investigate test_edge_cases.py (53 tests)",
        "**Meta:** `status:pending` `priority:medium` `category:testing`",
        "**Files:** `tests/test_edge_cases.py`",
        "**Problem:** 53 tests for edge cases (Unicode, large docs, malformed inputs). Investigate:",
        "1. Check git history",
        "2. Consider moving to `tests/unit/` or creating `tests/robustness/`",
        "### 201. Investigate test_incremental_indexing.py (47 tests)",
        "**Meta:** `status:pending` `priority:medium` `category:testing`",
        "**Files:** `tests/test_incremental_indexing.py`",
        "**Problem:** 47 tests for incremental document operations. Investigate:",
        "1. Check git history",
        "2. Verify no overlap with `tests/unit/test_processor_core.py`",
        "3. Consider moving to `tests/integration/`",
        "### 202. Investigate test_intent_query.py (24 tests)",
        "**Meta:** `status:pending` `priority:medium` `category:testing`",
        "**Files:** `tests/test_intent_query.py`",
        "**Effort:** Small",
        "**Problem:** 24 tests for intent-based query parsing. Investigate:",
        "1. Check git history",
        "2. Check if `tests/unit/test_query.py` covers this",
        "3. Consider moving to `tests/unit/test_query_intent.py`",
        "### 203. Investigate test_behavioral.py (9 tests)",
        "**Meta:** `status:pending` `priority:medium` `category:testing`",
        "**Effort:** Small",
        "**Problem:** 9 behavioral/acceptance tests. Investigate:",
        "1. Check git history",
        "2. Move to `tests/behavioral/` if appropriate",
        "### 204. Investigate test_query_optimization.py (20 tests)",
        "**Meta:** `status:pending` `priority:medium` `category:testing`",
        "**Files:** `tests/test_query_optimization.py`",
        "**Effort:** Small",
        "**Problem:** 20 tests for query performance. Investigate:",
        "1. Check git history",
        "2. Consider moving to `tests/performance/`",
        "### 205. Investigate legacy script tests (6 files)",
        "**Meta:** `status:pending` `priority:medium` `category:testing`",
        "**Files:** `tests/test_analyze_louvain_resolution.py`, `tests/test_ask_codebase.py`, `tests/test_evaluate_cluster.py`, `tests/test_generate_ai_metadata.py`, `tests/test_search_codebase.py`, `tests/test_showcase.py`",
        "**Problem:** 6 test files (132 tests total) for scripts and showcase. Investigate:",
        "1. Check git history for each",
        "2. Consider creating `tests/scripts/` directory",
        "3. Or moving to `tests/integration/`",
        "## Unit Test Coverage Baseline",
        "‚úÖ **Unit test coverage as of 2025-12-13 (1,729 tests, 85% overall):**",
        "| Module | Coverage | Status | Task |",
        "|--------|----------|--------|------|",
        "| config.py | 100% | ‚úÖ | #168 |",
        "| minicolumn.py | 100% | ‚úÖ | #162 |",
        "| definitions.py | 100% | ‚úÖ | #173 |",
        "| tokenizer.py | 99% | ‚úÖ | #159 |",
        "| layers.py | 99% | ‚úÖ | #161 |",
        "| ranking.py | 99% | ‚úÖ | #175 |",
        "| fingerprint.py | 99% | ‚úÖ | #163 |",
        "| chunk_index.py | 98% | ‚úÖ | #167 |",
        "| code_concepts.py | 98% | ‚úÖ | #168 |",
        "| embeddings.py | 98% | ‚úÖ | #160 |",
        "| gaps.py | 98% | ‚úÖ | #164 |",
        "| search.py | 95% | ‚úÖ | #171 |",
        "| persistence.py | 94% | ‚úÖ | #178 |",
        "| expansion.py | 94% | ‚úÖ | #170 |",
        "| analysis.py | 94% | ‚úÖ | #176 |",
        "| passages.py | 92% | ‚úÖ | #172 |",
        "| semantics.py | 91% | ‚úÖ | #177 |",
        "| chunking.py | 91% | ‚úÖ | #172 |",
        "| analogy.py | 90% | ‚úÖ | #174 |",
        "| intent.py | 87% | üî∂ | - |",
        "| processor.py | 85% | üî∂ | #165-166 |",
        "**19 of 21 modules at 90%+ coverage**",
        "| Arch | 5 | Architecture refactoring (#133, 134, 135, 95, 100, 101) |",
        "| CodeQual | 1 | Code quality improvements (#99) |",
        "| Testing | 9 | Test coverage and legacy investigation (#129, 198-205) |",
        "| TaskMgmt | 4 | Task management system (#106, 107, 108, 197) |",
        "| AINav | 2 | AI assistant navigation (#117, 118) |",
        "| DevEx | 8 | Developer experience, scripts (#73-80, 196) |",
        "| Integration | 1 | MCP Server (#184) |",
        "| Memory | 1 | Optimization (#192) |",
        "| API | 1 | Simplified facades (#186) |",
        "*Updated 2025-12-13 - Unit test initiative COMPLETE (85% coverage, 1,729 tests)*",
        "*Last restructured: 2025-12-13 (Major cleanup: removed 2,001 lines of stale completed task details)*"
      ],
      "lines_removed": [
        "### 193. Unify alpha parameter validation in semantics.py",
        "",
        "**Meta:** `status:pending` `priority:medium` `category:codequal`",
        "**Files:** `cortical/semantics.py`",
        "",
        "**Problem:**",
        "`retrofit_connections()` allows `alpha=0` but `retrofit_embeddings()` excludes it:",
        "- Line 405-408: `if not (0 <= alpha <= 1):`",
        "- Line 507-508: `if not (0 < alpha <= 1):`",
        "",
        "**Fix:** Either document why the difference exists or unify the validation.",
        "",
        "",
        "### 194. Add validation for invalid layer values in persistence.py load",
        "",
        "**Meta:** `status:pending` `priority:medium` `category:codequal`",
        "**Files:** `cortical/persistence.py`",
        "",
        "**Problem:**",
        "In `load_processor()` (line 97-99), if `level_value` isn't a valid `CorticalLayer` enum value (0-3), an unclear `ValueError` is raised.",
        "",
        "**Fix:** Add explicit validation with helpful error message:",
        "```python",
        "try:",
        "    layer_enum = CorticalLayer(int(level_value))",
        "except ValueError:",
        "    raise ValueError(f\"Invalid layer level in saved state: {level_value}\")",
        "```",
        "",
        "",
        "### 195. Import stopwords from tokenizer.py in semantics.py",
        "",
        "**Meta:** `status:pending` `priority:low` `category:codequal`",
        "**Files:** `cortical/semantics.py`, `cortical/tokenizer.py`",
        "",
        "**Problem:**",
        "`semantics.py` (lines 144-151) has its own hardcoded stopword set that duplicates `Tokenizer.DEFAULT_STOP_WORDS`. Changes to one won't affect the other.",
        "",
        "**Fix:** Import from `Tokenizer.DEFAULT_STOP_WORDS` or move to `constants.py`.",
        "",
        "",
        "## Unit Test Coverage Baseline",
        "",
        "‚úÖ **Unit test coverage as of 2025-12-13 (1,729 tests, 85% overall):**",
        "",
        "| Module | Coverage | Status | Task |",
        "|--------|----------|--------|------|",
        "| config.py | 100% | ‚úÖ | #168 |",
        "| minicolumn.py | 100% | ‚úÖ | #162 |",
        "| definitions.py | 100% | ‚úÖ | #173 |",
        "| tokenizer.py | 99% | ‚úÖ | #159 |",
        "| layers.py | 99% | ‚úÖ | #161 |",
        "| ranking.py | 99% | ‚úÖ | #175 |",
        "| fingerprint.py | 99% | ‚úÖ | #163 |",
        "| chunk_index.py | 98% | ‚úÖ | #167 |",
        "| code_concepts.py | 98% | ‚úÖ | #168 |",
        "| embeddings.py | 98% | ‚úÖ | #160 |",
        "| gaps.py | 98% | ‚úÖ | #164 |",
        "| search.py | 95% | ‚úÖ | #171 |",
        "| persistence.py | 94% | ‚úÖ | #178 |",
        "| expansion.py | 94% | ‚úÖ | #170 |",
        "| analysis.py | 94% | ‚úÖ | #176 |",
        "| passages.py | 92% | ‚úÖ | #172 |",
        "| semantics.py | 91% | ‚úÖ | #177 |",
        "| chunking.py | 91% | ‚úÖ | #172 |",
        "| analogy.py | 90% | ‚úÖ | #174 |",
        "| intent.py | 87% | üî∂ | - |",
        "| processor.py | 85% | üî∂ | #165-166 |",
        "",
        "**19 of 21 modules at 90%+ coverage**",
        "",
        "",
        "## Pending Task Details",
        "",
        "### 159. Unit Tests for tokenizer.py (9% ‚Üí 90%)",
        "",
        "**Meta:** `status:pending` `priority:medium` `category:unit-test`",
        "**Files:** `tests/unit/test_tokenizer.py` (new), `cortical/tokenizer.py`",
        "**Effort:** Medium",
        "**Depends:** ~~150~~",
        "",
        "**Problem:** tokenizer.py has only 9% unit test coverage. Core text processing logic is untested.",
        "",
        "**Testable Pure Functions:**",
        "- `tokenize()` - main tokenization entry point",
        "- `_normalize_text()` - text normalization",
        "- `_split_identifiers()` - camelCase/snake_case splitting",
        "- `stem_word()` - Porter stemming implementation",
        "- `is_stop_word()` - stop word detection",
        "- `extract_ngrams()` - n-gram extraction",
        "",
        "**Test Categories:**",
        "1. **Basic Tokenization** (10+ tests): words, punctuation, whitespace",
        "2. **Code Tokenization** (10+ tests): identifiers, operators, split_identifiers mode",
        "3. **Stop Words** (5+ tests): filtering, custom stop words",
        "4. **Stemming** (5+ tests): regular words, edge cases",
        "5. **N-grams** (5+ tests): bigrams, trigrams, boundary handling",
        "",
        "**Acceptance Criteria:**",
        "- [ ] 35+ unit tests",
        "- [ ] Coverage ‚â• 90%",
        "- [ ] Tests run in <2 seconds",
        "",
        "",
        "### 160. Unit Tests for embeddings.py (4% ‚Üí 90%)",
        "",
        "**Meta:** `status:pending` `priority:medium` `category:unit-test`",
        "**Files:** `tests/unit/test_embeddings.py` (new), `cortical/embeddings.py`",
        "**Effort:** Medium",
        "**Depends:** ~~150~~",
        "",
        "**Problem:** embeddings.py has only 4% coverage. Graph embedding algorithms untested.",
        "",
        "**Testable Pure Functions:**",
        "- `_adjacency_embeddings()` - adjacency-based embeddings",
        "- `_random_walk_embeddings()` - random walk embeddings",
        "- `_tfidf_embeddings()` - TF-IDF based embeddings",
        "- `_spectral_embeddings()` - spectral graph embeddings",
        "- `_cosine_similarity()` - similarity calculation",
        "- `_select_landmarks()` - landmark node selection",
        "",
        "**Test Categories:**",
        "1. **Embedding Generation** (15+ tests): each method, dimensions, normalization",
        "2. **Similarity Calculation** (10+ tests): cosine sim, edge cases",
        "3. **Landmark Selection** (5+ tests): PageRank-based, random",
        "4. **Edge Cases** (5+ tests): empty graph, single node, disconnected",
        "**Acceptance Criteria:**",
        "- [ ] 35+ unit tests",
        "- [ ] Coverage ‚â• 90%",
        "- [ ] Tests run in <3 seconds",
        "### 161. Unit Tests for layers.py (31% ‚Üí 90%)",
        "**Meta:** `status:pending` `priority:medium` `category:unit-test`",
        "**Files:** `tests/unit/test_layers.py` (new), `cortical/layers.py`",
        "**Effort:** Medium",
        "**Depends:** ~~150~~",
        "",
        "**Problem:** layers.py has 31% coverage. HierarchicalLayer container logic partially tested.",
        "",
        "**Testable Methods:**",
        "- `add_minicolumn()` - add to layer",
        "- `get_minicolumn()` - lookup by content",
        "- `get_by_id()` - O(1) ID lookup via _id_index",
        "- `remove_minicolumn()` - removal and cleanup",
        "- `column_count()` - count accessor",
        "- `get_activation_sparsity()` - sparsity calculation",
        "- `__iter__` - iteration support",
        "",
        "**Test Categories:**",
        "1. **CRUD Operations** (15+ tests): add, get, remove, update",
        "2. **ID Index** (10+ tests): O(1) lookup, consistency after mutations",
        "3. **Sparsity/Stats** (5+ tests): activation sparsity, counts",
        "4. **Iteration** (5+ tests): iter, values, items",
        "",
        "**Acceptance Criteria:**",
        "- [ ] 35+ unit tests",
        "- [ ] Coverage ‚â• 90%",
        "- [ ] Tests run in <2 seconds",
        "### 162. Unit Tests for minicolumn.py (31% ‚Üí 90%)",
        "",
        "**Meta:** `status:pending` `priority:medium` `category:unit-test`",
        "**Files:** `tests/unit/test_minicolumn.py` (new), `cortical/minicolumn.py`",
        "**Effort:** Medium",
        "**Depends:** ~~150~~",
        "",
        "**Problem:** minicolumn.py has 31% coverage. Core data structure methods untested.",
        "",
        "**Testable Methods:**",
        "- `add_lateral_connection()` - lateral connections",
        "- `add_lateral_connections_batch()` - batch add",
        "- `add_typed_connection()` - typed Edge connections",
        "- `update_typed_connection()` - confidence update logic",
        "- `to_dict()` / `from_dict()` - serialization",
        "- Edge class: `__init__`, `__eq__`, `__hash__`",
        "",
        "**Test Categories:**",
        "1. **Connection Management** (15+ tests): add, update, weights",
        "2. **Batch Operations** (5+ tests): batch add performance, correctness",
        "3. **Edge Class** (10+ tests): creation, equality, hashing",
        "4. **Serialization** (5+ tests): round-trip, backward compat",
        "",
        "**Acceptance Criteria:**",
        "- [ ] 35+ unit tests",
        "- [ ] Coverage ‚â• 90%",
        "- [ ] Tests run in <2 seconds",
        "### 163. Unit Tests for fingerprint.py (11% ‚Üí 90%)",
        "**Meta:** `status:pending` `priority:medium` `category:unit-test`",
        "**Files:** `tests/unit/test_fingerprint.py` (new), `cortical/fingerprint.py`",
        "**Depends:** ~~150~~",
        "**Problem:** fingerprint.py has 11% coverage. Semantic fingerprinting untested.",
        "",
        "**Testable Pure Functions:**",
        "- `create_fingerprint()` - fingerprint generation",
        "- `compare_fingerprints()` - similarity comparison",
        "- `_jaccard_similarity()` - Jaccard calculation",
        "- `_weighted_overlap()` - weighted overlap",
        "- `explain_similarity()` - similarity explanation",
        "",
        "**Test Categories:**",
        "1. **Fingerprint Creation** (10+ tests): text variations, empty text",
        "2. **Comparison** (15+ tests): identical, similar, different texts",
        "3. **Similarity Metrics** (5+ tests): Jaccard, weighted",
        "4. **Explanation** (5+ tests): human-readable output",
        "",
        "**Acceptance Criteria:**",
        "- [ ] 35+ unit tests",
        "- [ ] Coverage ‚â• 90%",
        "- [ ] Tests run in <2 seconds",
        "### 164. Unit Tests for gaps.py (9% ‚Üí 90%)",
        "**Meta:** `status:pending` `priority:medium` `category:unit-test`",
        "**Files:** `tests/unit/test_gaps.py` (new), `cortical/gaps.py`",
        "**Depends:** ~~150~~",
        "",
        "**Problem:** gaps.py has 9% coverage. Knowledge gap detection untested.",
        "",
        "**Testable Pure Functions:**",
        "- `detect_knowledge_gaps()` - main gap detection",
        "- `_find_isolated_nodes()` - disconnected term detection",
        "- `_find_weak_connections()` - low-weight edges",
        "- `_calculate_gap_score()` - gap severity scoring",
        "- `suggest_remediation()` - suggestions for filling gaps",
        "",
        "**Test Categories:**",
        "1. **Gap Detection** (15+ tests): various graph topologies",
        "2. **Isolated Nodes** (5+ tests): fully isolated, weakly connected",
        "3. **Weak Connections** (5+ tests): threshold behavior",
        "4. **Scoring** (5+ tests): score calculation, edge cases",
        "5. **Remediation** (5+ tests): suggestion generation",
        "",
        "**Acceptance Criteria:**",
        "- [ ] 35+ unit tests",
        "- [ ] Coverage ‚â• 90%",
        "- [ ] Tests run in <2 seconds",
        "",
        "",
        "### 165. Unit Tests for processor.py Phase 1 (11% ‚Üí 50%)",
        "",
        "**Meta:** `status:pending` `priority:medium` `category:unit-test`",
        "**Files:** `tests/unit/test_processor_core.py` (new), `cortical/processor.py`",
        "**Effort:** Large",
        "**Depends:** ~~150~~",
        "",
        "**Problem:** processor.py has 11% coverage. First phase targets core methods.",
        "",
        "**Phase 1 Focus:**",
        "1. **Document Management** (15+ tests): add, remove, metadata",
        "2. **Staleness Tracking** (15+ tests): is_stale, mark_fresh, dependencies",
        "3. **Configuration** (10+ tests): config application, overrides",
        "4. **Layer Access** (10+ tests): get_layer, layer initialization",
        "",
        "**Acceptance Criteria:**",
        "- [ ] 50+ unit tests",
        "- [ ] Coverage ‚â• 50%",
        "- [ ] Tests run in <5 seconds",
        "",
        "",
        "### 166. Unit Tests for processor.py Phase 2 (50% ‚Üí 90%)",
        "",
        "**Meta:** `status:pending` `priority:medium` `category:unit-test`",
        "**Files:** `tests/unit/test_processor_advanced.py` (new), `cortical/processor.py`",
        "**Effort:** Large",
        "**Depends:** 165",
        "",
        "**Problem:** Complete coverage requires testing computation orchestration.",
        "",
        "**Phase 2 Focus:**",
        "1. **Computation Methods** (20+ tests): compute_*, incremental updates",
        "2. **Query Delegation** (15+ tests): search, passages, expansion",
        "3. **State Management** (10+ tests): save/load round-trip",
        "4. **Edge Cases** (10+ tests): empty corpus, Unicode, large docs",
        "",
        "**Acceptance Criteria:**",
        "- [ ] 55+ additional unit tests",
        "- [ ] Coverage ‚â• 90%",
        "- [ ] Tests run in <10 seconds",
        "",
        "",
        "### 167. Unit Tests for chunk_index.py (0% ‚Üí 90%)",
        "",
        "**Meta:** `status:pending` `priority:medium` `category:unit-test`",
        "**Files:** `tests/unit/test_chunk_index.py` (new), `cortical/chunk_index.py`",
        "**Effort:** Large",
        "**Depends:** ~~150~~",
        "**Problem:** chunk_index.py has 0% unit coverage. Git-friendly storage untested.",
        "",
        "**Testable Pure Functions:**",
        "- `_generate_chunk_filename()` - timestamp-based naming",
        "- `_parse_chunk_filename()` - filename parsing",
        "- `_serialize_document()` - document serialization",
        "- `_deserialize_document()` - document deserialization",
        "- `_compact_chunks()` - compaction logic",
        "- `_merge_operations()` - operation merging",
        "",
        "**Test Categories:**",
        "1. **Filename Generation** (10+ tests): uniqueness, parsing",
        "2. **Serialization** (15+ tests): documents, metadata, edge cases",
        "3. **Compaction** (10+ tests): merge logic, conflict resolution",
        "4. **Operation Replay** (10+ tests): add, remove, update sequences",
        "",
        "**Acceptance Criteria:**",
        "- [ ] 45+ unit tests",
        "- [ ] Coverage ‚â• 90%",
        "- [ ] Tests run in <3 seconds",
        "### 168. Unit Tests for config.py (40% ‚Üí 90%)",
        "**Meta:** `status:pending` `priority:medium` `category:unit-test`",
        "**Files:** `tests/unit/test_config.py` (new), `cortical/config.py`",
        "**Depends:** ~~150~~",
        "",
        "**Problem:** config.py has 40% coverage. Validation and defaults partially tested.",
        "**Testable Components:**",
        "- `CorticalConfig` dataclass creation",
        "- Parameter validation (ranges, types)",
        "- Default value application",
        "- `to_dict()` / `from_dict()` serialization",
        "- Config merging/override logic",
        "",
        "**Test Categories:**",
        "1. **Defaults** (10+ tests): all default values correct",
        "2. **Validation** (10+ tests): invalid values rejected",
        "3. **Serialization** (5+ tests): round-trip, missing keys",
        "4. **Override** (5+ tests): per-call parameter override",
        "",
        "**Acceptance Criteria:**",
        "- [ ] 30+ unit tests",
        "- [ ] Coverage ‚â• 90%",
        "- [ ] Tests run in <1 second",
        "### 169. Unit Tests for code_concepts.py (58% ‚Üí 90%)",
        "**Meta:** `status:pending` `priority:medium` `category:unit-test`",
        "**Files:** `tests/unit/test_code_concepts.py` (new), `cortical/code_concepts.py`",
        "**Depends:** ~~150~~",
        "",
        "**Problem:** code_concepts.py has 58% coverage. Programming synonyms partially tested.",
        "",
        "**Testable Components:**",
        "- `CODE_CONCEPTS` dictionary completeness",
        "- `get_code_synonyms()` - synonym lookup",
        "- `expand_code_query()` - query expansion with code concepts",
        "- Coverage of all concept categories",
        "",
        "**Test Categories:**",
        "1. **Concept Lookup** (10+ tests): each major category",
        "2. **Synonym Expansion** (10+ tests): single term, multi-term",
        "3. **Edge Cases** (5+ tests): unknown terms, empty input",
        "",
        "**Acceptance Criteria:**",
        "- [ ] 25+ unit tests",
        "- [ ] Coverage ‚â• 90%",
        "- [ ] Tests run in <1 second",
        "",
        "",
        "### 170. Unit Tests for query/expansion.py (11% ‚Üí 90%)",
        "",
        "**Meta:** `status:pending` `priority:medium` `category:unit-test`",
        "**Files:** `tests/unit/test_query_expansion.py` (new), `cortical/query/expansion.py`",
        "**Effort:** Medium",
        "**Depends:** ~~150~~",
        "",
        "**Problem:** expansion.py has 11% coverage. Query expansion logic requires refactoring.",
        "",
        "**Strategy:** Extract pure functions for:",
        "- `_expand_from_lateral()` - lateral connection expansion",
        "- `_expand_from_semantic()` - semantic relation expansion",
        "- `_merge_expansions()` - combining multiple expansion sources",
        "- `_apply_decay()` - weight decay for multi-hop",
        "",
        "**Test Categories:**",
        "1. **Lateral Expansion** (15+ tests): weights, limits, decay",
        "2. **Semantic Expansion** (10+ tests): relation types, confidence",
        "3. **Merging** (5+ tests): conflict resolution, max terms",
        "4. **Multi-hop** (5+ tests): 2-hop, 3-hop, decay",
        "",
        "**Acceptance Criteria:**",
        "- [ ] 35+ unit tests",
        "- [ ] Coverage ‚â• 90%",
        "- [ ] Tests run in <2 seconds",
        "",
        "",
        "### 171. Unit Tests for query/search.py (6% ‚Üí 90%)",
        "",
        "**Meta:** `status:pending` `priority:medium` `category:unit-test`",
        "**Files:** `tests/unit/test_query_search.py` (new), `cortical/query/search.py`",
        "**Effort:** Medium",
        "**Depends:** ~~150~~",
        "",
        "**Problem:** search.py has 6% coverage. Document scoring requires refactoring.",
        "",
        "**Strategy:** Extract pure functions for:",
        "- `_score_document()` - per-document scoring",
        "- `_apply_boosts()` - doc name boost, type boost",
        "- `_rank_results()` - sorting and limiting",
        "- `_filter_results()` - threshold filtering",
        "",
        "**Test Categories:**",
        "1. **Scoring** (15+ tests): TF-IDF aggregation, normalization",
        "2. **Boosts** (10+ tests): name match, doc type, test penalty",
        "3. **Ranking** (5+ tests): sorting, ties, limits",
        "4. **Filtering** (5+ tests): thresholds, empty results",
        "",
        "**Acceptance Criteria:**",
        "- [ ] 35+ unit tests",
        "- [ ] Coverage ‚â• 90%",
        "- [ ] Tests run in <2 seconds",
        "",
        "",
        "### 172. Unit Tests for query/passages.py (8% ‚Üí 90%)",
        "",
        "**Meta:** `status:pending` `priority:medium` `category:unit-test`",
        "**Files:** `tests/unit/test_query_passages.py` (new), `cortical/query/passages.py`",
        "**Effort:** Medium",
        "**Depends:** ~~150~~",
        "",
        "**Problem:** passages.py has 8% coverage (chunking.py at 60%). Complete coverage needed.",
        "",
        "**Testable Pure Functions:**",
        "- `_create_chunks()` - text chunking (some coverage)",
        "- `_score_chunk()` - chunk relevance scoring",
        "- `_rank_passages()` - passage ranking",
        "- `_format_passage()` - output formatting",
        "",
        "**Test Categories:**",
        "1. **Chunking** (10+ tests): overlap, boundaries, edge cases",
        "2. **Scoring** (15+ tests): term density, position, exact match",
        "3. **Ranking** (5+ tests): sorting, diversity",
        "4. **Formatting** (5+ tests): context, highlighting",
        "",
        "**Acceptance Criteria:**",
        "- [ ] 35+ unit tests",
        "- [ ] Coverage ‚â• 90%",
        "- [ ] Tests run in <2 seconds",
        "",
        "",
        "### 173. Unit Tests for query/definitions.py (12% ‚Üí 90%)",
        "",
        "**Meta:** `status:pending` `priority:medium` `category:unit-test`",
        "**Files:** `tests/unit/test_query_definitions.py` (new), `cortical/query/definitions.py`",
        "**Effort:** Medium",
        "**Depends:** ~~150~~",
        "",
        "**Problem:** definitions.py has 12% coverage. Definition detection logic untested.",
        "",
        "**Testable Pure Functions:**",
        "- `is_definition_query()` - query classification",
        "- `_extract_definition_target()` - target term extraction",
        "- `_score_definition_passage()` - definition scoring",
        "- `_detect_definition_patterns()` - pattern matching",
        "",
        "**Test Categories:**",
        "1. **Query Classification** (10+ tests): \"what is X\", \"define X\", variations",
        "2. **Pattern Matching** (15+ tests): \"X is a\", \"X refers to\", etc.",
        "3. **Scoring** (5+ tests): definition quality ranking",
        "4. **Edge Cases** (5+ tests): ambiguous queries, no definitions",
        "",
        "**Acceptance Criteria:**",
        "- [ ] 35+ unit tests",
        "- [ ] Coverage ‚â• 90%",
        "- [ ] Tests run in <2 seconds",
        "",
        "",
        "### 174. Unit Tests for query/analogy.py (3% ‚Üí 90%)",
        "",
        "**Meta:** `status:pending` `priority:medium` `category:unit-test`",
        "**Files:** `tests/unit/test_query_analogy.py` (new), `cortical/query/analogy.py`",
        "**Effort:** Medium",
        "**Depends:** ~~150~~",
        "",
        "**Problem:** analogy.py has 3% coverage. Analogy completion entirely untested.",
        "",
        "**Testable Pure Functions:**",
        "- `complete_analogy()` - main entry point",
        "- `_find_relation()` - relation type detection",
        "- `_apply_relation()` - relation application",
        "- `_score_candidates()` - candidate ranking",
        "- `_validate_analogy()` - analogy validation",
        "",
        "**Test Categories:**",
        "1. **Relation Detection** (10+ tests): IsA, PartOf, SimilarTo, etc.",
        "2. **Completion** (15+ tests): A:B::C:?, various relation types",
        "3. **Scoring** (5+ tests): candidate ranking, confidence",
        "4. **Validation** (5+ tests): invalid input, no solution",
        "",
        "**Acceptance Criteria:**",
        "- [ ] 35+ unit tests",
        "- [ ] Coverage ‚â• 90%",
        "- [ ] Tests run in <2 seconds",
        "",
        "",
        "### 175. Unit Tests for query/ranking.py (25% ‚Üí 90%)",
        "",
        "**Meta:** `status:pending` `priority:medium` `category:unit-test`",
        "**Files:** `tests/unit/test_query_ranking.py` (new), `cortical/query/ranking.py`",
        "**Effort:** Medium",
        "**Depends:** ~~150~~",
        "",
        "**Problem:** ranking.py has 25% coverage. Multi-stage ranking partially tested.",
        "",
        "**Testable Pure Functions:**",
        "- `multi_stage_rank()` - main ranking entry",
        "- `_initial_rank()` - first-pass ranking",
        "- `_rerank_with_context()` - context-aware reranking",
        "- `_apply_diversity()` - diversity boosting",
        "- `_normalize_scores()` - score normalization",
        "",
        "**Test Categories:**",
        "1. **Initial Ranking** (10+ tests): TF-IDF, PageRank weights",
        "2. **Reranking** (10+ tests): context signals, boosts",
        "3. **Diversity** (5+ tests): diversity calculation, application",
        "4. **Normalization** (5+ tests): score ranges, edge cases",
        "",
        "**Acceptance Criteria:**",
        "- [ ] 30+ unit tests",
        "- [ ] Coverage ‚â• 90%",
        "- [ ] Tests run in <2 seconds",
        "",
        "",
        "### 176. Unit Tests for analysis.py (16% ‚Üí 90%)",
        "",
        "**Meta:** `status:pending` `priority:medium` `category:unit-test`",
        "**Files:** `tests/unit/test_analysis_algorithms.py` (new), `cortical/analysis.py`",
        "**Effort:** Large",
        "**Depends:** ~~150~~",
        "",
        "**Problem:** analysis.py has 16% coverage. Core algorithms need comprehensive testing.",
        "",
        "**Strategy:** Extract and test pure algorithm cores:",
        "- `_pagerank_iteration()` - single PageRank iteration",
        "- `_louvain_phase1()` - local modularity optimization",
        "- `_louvain_phase2()` - network aggregation",
        "- `_compute_modularity()` - modularity calculation",
        "- `_compute_silhouette()` - silhouette calculation",
        "- `_tfidf_score()` - TF-IDF scoring formula",
        "",
        "**Test Categories:**",
        "1. **PageRank** (15+ tests): convergence, damping, edge cases",
        "2. **TF-IDF** (10+ tests): scoring, IDF, normalization",
        "3. **Louvain** (15+ tests): phases, resolution, determinism",
        "4. **Metrics** (10+ tests): modularity, silhouette, balance",
        "",
        "**Acceptance Criteria:**",
        "- [ ] 50+ unit tests",
        "- [ ] Coverage ‚â• 90%",
        "- [ ] Tests run in <5 seconds",
        "",
        "",
        "### 177. Unit Tests for semantics.py (24% ‚Üí 90%)",
        "**Meta:** `status:pending` `priority:medium` `category:unit-test`",
        "**Files:** `tests/unit/test_semantics.py` (extend existing), `cortical/semantics.py`",
        "**Effort:** Medium",
        "**Depends:** ~~150~~",
        "",
        "**Problem:** semantics.py has 24% coverage (48 tests exist). Need comprehensive coverage.",
        "",
        "**Additional Tests Needed:**",
        "1. **Pattern Extraction** (15+ tests): all pattern types, edge cases",
        "2. **Relation Building** (15+ tests): hierarchy construction, retrofitting",
        "3. **Similarity** (10+ tests): context similarity, thresholds",
        "4. **Integration** (10+ tests): full extraction pipeline",
        "",
        "**Acceptance Criteria:**",
        "- [ ] 50+ total unit tests (48 existing + new)",
        "- [ ] Coverage ‚â• 90%",
        "- [ ] Tests run in <3 seconds",
        "### 178. Unit Tests for persistence.py (18% ‚Üí 90%)",
        "",
        "**Meta:** `status:pending` `priority:medium` `category:unit-test`",
        "**Files:** `tests/unit/test_persistence.py` (extend existing), `cortical/persistence.py`",
        "**Effort:** Medium",
        "**Depends:** ~~150~~",
        "",
        "**Problem:** persistence.py has 18% coverage (36 tests exist). Need comprehensive coverage.",
        "**Additional Tests Needed:**",
        "1. **Full Serialization** (15+ tests): processor state, layers, connections",
        "2. **Export Formats** (10+ tests): JSON, GraphML, adjacency",
        "3. **Import/Load** (10+ tests): version migration, backward compat",
        "4. **Edge Cases** (10+ tests): large state, special values",
        "**Acceptance Criteria:**",
        "- [ ] 45+ total unit tests (36 existing + new)",
        "- [ ] Coverage ‚â• 90%",
        "- [ ] Tests run in <3 seconds",
        "### 148. Investigate test_search_is_fast Taking 137s",
        "**Meta:** `status:pending` `priority:high` `category:testing`",
        "**Effort:** Medium",
        "",
        "**Problem:** `test_search_is_fast` in `TestPerformanceBehavior` takes 137 seconds to run, making it the slowest test in the suite.",
        "",
        "**Root Cause Analysis Needed:**",
        "- Test loads full sample corpus via `get_shared_processor()` singleton",
        "- `compute_all()` is called during corpus loading",
        "- The 137s includes corpus load time, not just search time",
        "- Search itself is fast (~100ms) but corpus loading dominates",
        "",
        "**Potential Solutions:**",
        "1. Use a smaller synthetic corpus for performance tests",
        "2. Pre-compute and cache the corpus as a pickle file in test fixtures",
        "3. Split into two tests: corpus load time + search time",
        "4. Mock `compute_all()` and only test search on pre-loaded state",
        "**Acceptance Criteria:**",
        "- [ ] Test completes in < 5 seconds",
        "- [ ] Search performance is still validated",
        "- [ ] No false positives (test should still catch slow searches)",
        "### 149. Fix test_compute_all_under_threshold Failing (135s > 30s)",
        "",
        "**Meta:** `status:pending` `priority:high` `category:testing`",
        "**Files:** `tests/test_behavioral.py`",
        "**Effort:** Medium",
        "",
        "**Problem:** `test_compute_all_under_threshold` takes 135 seconds, far exceeding the 30s threshold. Test is currently failing.",
        "",
        "**Context:**",
        "- Test is skipped under coverage (`@unittest.skipIf('coverage' in sys.modules...)`)",
        "- But fails when run directly: `python -m unittest discover`",
        "- Threshold was set to 30s assuming optimized corpus",
        "- Real corpus with 100+ docs takes much longer",
        "",
        "**Potential Solutions:**",
        "1. **Use smaller test corpus** - Create dedicated test fixture with ~20 docs",
        "2. **Increase threshold** - Accept that full corpus takes longer (not recommended)",
        "3. **Profile and optimize** - Find remaining bottlenecks in `compute_all()`",
        "4. **Make test conditional** - Only run on CI with pre-warmed cache",
        "5. **Split test** - Test individual `compute_*` methods with smaller inputs",
        "",
        "**Benchmark Data:**",
        "```",
        "Found 1196 tests",
        "test_search_is_fast: 136.60s (PASS - corpus already loaded)",
        "test_compute_all_under_threshold: 134.86s (FAIL - reloads corpus)",
        "All other tests: < 1s each",
        "```",
        "",
        "**Acceptance Criteria:**",
        "- [ ] Test passes consistently",
        "- [ ] Performance regression detection still works",
        "- [ ] CI build time < 5 minutes for test suite",
        "",
        "### 123. Replace Label Propagation with Louvain Community Detection ‚úÖ",
        "**Meta:** `status:completed` `priority:critical` `category:bugfix`",
        "**Files:** `cortical/analysis.py`, `cortical/processor.py`, `tests/test_analysis.py`",
        "**Effort:** Large",
        "**Completed:** 2025-12-11",
        "",
        "**Problem:** Label propagation clustering fails catastrophically on densely connected graphs:",
        "- 95 documents produce only 3 concept clusters",
        "- One mega-cluster contains 99.8% of tokens (6,667 of 6,679)",
        "- The algorithm converges to minimal clusters regardless of strictness parameters",
        "- This renders the concept layer (Layer 2) essentially useless",
        "",
        "**Root Cause Analysis:**",
        "Label propagation works by having each node adopt the most common label among neighbors. On a densely connected graph (avg 18.2 connections per token), information propagates everywhere, causing nearly all nodes to converge to a single label.",
        "",
        "This is NOT a parameter tuning problem - it's a fundamental algorithmic limitation. The `cluster_strictness` parameter only delays convergence, it cannot prevent it.",
        "",
        "**Solution Applied:**",
        "1. Implemented `cluster_by_louvain()` in `cortical/analysis.py` (300+ lines)",
        "   - Phase 1: Local modularity optimization with cached sigma_tot for O(1) lookups",
        "   - Phase 2: Network aggregation into super-nodes",
        "   - Resolution parameter for controlling cluster granularity",
        "2. Added `clustering_method` parameter to `processor.build_concept_clusters()`",
        "   - Default: 'louvain' (recommended)",
        "   - Alternative: 'label_propagation' (backward compatibility)",
        "3. Enabled previously skipped regression test `test_no_single_cluster_dominates`",
        "",
        "**Results:**",
        "- 34 concept clusters for 92-document corpus (6518 tokens)",
        "- Largest cluster: 10.2% of tokens (well under 50% threshold)",
        "- All 823 tests pass",
        "- compute_all() takes ~12s for full corpus",
        "",
        "**Acceptance Criteria:**",
        "- [x] Louvain algorithm implemented without external dependencies",
        "- [x] 34 clusters for 92-document showcase corpus (exceeds 10+)",
        "- [x] All 823 existing tests pass",
        "- [x] Regression test `test_no_single_cluster_dominates` enabled and passing",
        "- [x] showcase.py demonstrates improved clustering",
        "### 124. Add Minimum Cluster Count Regression Tests ‚úÖ",
        "**Meta:** `status:completed` `priority:critical` `category:testing`",
        "**Files:** `tests/test_analysis.py`",
        "**Completed:** 2025-12-11",
        "**Problem:** We had NO tests that would catch clustering failures:",
        "- Tests only checked that clustering returns valid dictionaries",
        "- No baseline for expected cluster counts",
        "- No quality thresholds for diverse corpora",
        "- The regression went undetected until manual inspection",
        "",
        "**Solution Applied:**",
        "Added comprehensive regression tests in two test classes:",
        "",
        "1. **TestClusteringQualityRegression** (existing, extended):",
        "   - `test_cluster_semantic_coherence` - verifies tokens in same cluster have lateral connections",
        "",
        "2. **TestShowcaseCorpusRegression** (new):",
        "   - `test_showcase_produces_expected_cluster_count` - 100+ docs ‚Üí 15+ clusters",
        "   - `test_showcase_no_mega_cluster` - no cluster >20% of tokens",
        "   - `test_showcase_cluster_distribution` - at least 5 substantial clusters, varied sizes",
        "",
        "**Test Results:**",
        "- 994 total tests (up from 990)",
        "- All showcase tests pass with 37 clusters, max 14.8% ratio",
        "- Semantic coherence >50% of clusters have internal connections",
        "",
        "**Acceptance Criteria:**",
        "- [x] 4+ new regression tests for clustering quality",
        "- [x] Tests pass after Louvain implementation (Task #123)",
        "### 125. Add Clustering Quality Metrics (Modularity, Silhouette) ‚úÖ",
        "",
        "**Meta:** `status:completed` `priority:critical` `category:devex` `depends:123`",
        "**Files:** `cortical/analysis.py`, `cortical/processor.py`, `showcase.py`, `tests/test_analysis.py`",
        "**Effort:** Medium",
        "**Completed:** 2025-12-11",
        "",
        "**Problem:** We have no way to measure if clustering is good or bad:",
        "- No modularity score to measure community quality",
        "- No silhouette score to measure cluster separation",
        "- No metrics in showcase output",
        "- No way to compare algorithm performance",
        "",
        "**Solution Applied:**",
        "",
        "Added `compute_clustering_quality()` to `cortical/analysis.py` with:",
        "",
        "1. **Modularity Score** (-1 to 1):",
        "   - Q > 0.3: Good community structure",
        "   - Q > 0.5: Strong community structure",
        "   - Implementation uses standard modularity formula",
        "",
        "2. **Silhouette Score** (-1 to 1):",
        "   - Uses graph-based distance (1 - connection similarity)",
        "   - Samples tokens for O(n¬≤) tractability",
        "   - s > 0.25: Reasonable structure",
        "3. **Balance Metric** (Gini coefficient):",
        "   - 0 = perfectly balanced",
        "   - 1 = all in one cluster",
        "4. **Quality Assessment**: Human-readable interpretation string",
        "",
        "**Example Output:**",
        "```",
        "Layer 2: Concept Layer (V4)",
        "       37 minicolumns, 686 connections",
        "       Quality: modularity=0.40, silhouette=0.15, balance=0.50",
        "```",
        "",
        "**Test Results:**",
        "- 1001 tests pass (7 new tests for quality metrics)",
        "- Showcase displays metrics in hierarchical structure section",
        "",
        "**Acceptance Criteria:**",
        "- [x] Modularity score implemented",
        "- [x] Silhouette score implemented",
        "- [x] Balance metric implemented",
        "- [x] Metrics displayed in showcase.py",
        "- [x] Quality thresholds documented",
        "",
        "",
        "### 126. Investigate Optimal Louvain Resolution for Sample Corpus ‚úÖ",
        "",
        "**Meta:** `status:completed` `priority:high` `category:research`",
        "**Files:** `scripts/analyze_louvain_resolution.py`, `docs/louvain_resolution_analysis.md`",
        "**Effort:** Medium",
        "**Depends:** 123",
        "**Completed:** 2025-12-11",
        "",
        "**Problem:** The Louvain algorithm's `resolution` parameter significantly affects cluster count:",
        "- Resolution 0.5 ‚Üí 38 clusters (coarse, 64% mega-cluster)",
        "- Resolution 1.0 ‚Üí 32 clusters (default, good balance)",
        "- Resolution 2.0 ‚Üí 79 clusters (fine)",
        "- Resolution 3.0 ‚Üí 125 clusters (very fine)",
        "",
        "**Research Findings:**",
        "",
        "1. **Tested 11 resolution values** (0.5 to 3.0) on 103-document corpus (7,102 tokens)",
        "",
        "2. **Key metric results at resolution 1.0 (default):**",
        "   - Modularity: 0.4036 (good, exceeds 0.3 threshold)",
        "   - Max cluster: 9.5% of tokens (no mega-clusters)",
        "   - Balance (Gini): 0.386 (reasonable distribution)",
        "",
        "3. **All resolutions maintain modularity > 0.3** (good community structure)",
        "",
        "4. **Low resolution (0.5) creates mega-clusters** (64% in one cluster) despite highest modularity (0.78)",
        "",
        "5. **Default 1.0 is the inflection point** where max cluster drops below 10%",
        "",
        "**Recommendation:** Keep default at 1.0, which provides:",
        "- Good modularity (0.40)",
        "- No mega-clusters (<10%)",
        "- Semantically coherent groupings",
        "- Standard Louvain interpretation",
        "",
        "**Deliverables:**",
        "- [x] Analysis script: `scripts/analyze_louvain_resolution.py`",
        "- [x] Research report: `docs/louvain_resolution_analysis.md`",
        "- [x] Recommendation: Keep default at 1.0 (already well-chosen)",
        "- [x] Use case guidelines documented for resolution tuning",
        "",
        "",
        "### 7. Document Magic Numbers",
        "",
        "**Meta:** `status:deferred` `priority:low` `category:docs`",
        "**Files:** `cortical/gaps.py:62,76,99`",
        "**Effort:** Small",
        "",
        "**Problem:** Magic numbers in gap detection lack documentation.",
        "",
        "**Note:** Deferred - functional as-is, low priority.",
        "",
        "",
        "### 42. Add Simple Query Language Support",
        "",
        "**Meta:** `status:deferred` `priority:low` `category:feature`",
        "**Files:** `cortical/query.py`",
        "**Effort:** Large",
        "",
        "**Problem:** Users must construct queries programmatically.",
        "",
        "**Solution:** Add simple query syntax like `\"neural AND networks NOT deep\"`.",
        "",
        "",
        "### 44. Remove Deprecated feedforward_sources",
        "",
        "**Meta:** `status:deferred` `priority:low` `category:cleanup`",
        "**Files:** `cortical/minicolumn.py:117`, `analysis.py:457`, `query.py:105`",
        "**Effort:** Small",
        "",
        "**Problem:** Deprecated field still exists in codebase.",
        "",
        "",
        "### 46. Standardize Return Types with Dataclasses",
        "",
        "**Meta:** `status:deferred` `priority:low` `category:code-quality`",
        "**Files:** `cortical/query.py`",
        "**Effort:** Medium",
        "",
        "**Problem:** Query functions return tuples instead of typed dataclasses.",
        "",
        "",
        "### 73. Add \"Find Similar Code\" Command",
        "",
        "**Meta:** `status:pending` `priority:low` `category:devex`",
        "**Files:** `scripts/similar_code.py` (new)",
        "**Effort:** Medium",
        "",
        "**Problem:** No easy way to find similar code blocks.",
        "",
        "**Solution:** Create script using semantic fingerprinting.",
        "",
        "",
        "### 74. Add \"Explain This Code\" Command",
        "",
        "**Meta:** `status:pending` `priority:low` `category:devex`",
        "**Files:** `scripts/explain_code.py` (new)",
        "**Effort:** Medium",
        "",
        "**Problem:** New contributors need help understanding code.",
        "",
        "**Solution:** Script that retrieves relevant docs/comments for a code block.",
        "",
        "",
        "### 75. Add \"What Changed?\" Semantic Diff",
        "",
        "**Meta:** `status:pending` `priority:low` `category:devex`",
        "**Files:** `scripts/what_changed.py` (new)",
        "**Effort:** Large",
        "",
        "**Problem:** Hard to understand semantic impact of changes.",
        "",
        "**Solution:** Show how fingerprints changed between commits.",
        "",
        "",
        "### 76. Add \"Suggest Related Files\" Feature",
        "",
        "**Meta:** `status:pending` `priority:low` `category:devex`",
        "**Files:** `scripts/related_files.py` (new)",
        "**Effort:** Medium",
        "",
        "**Problem:** When editing a file, don't know what else might need changes.",
        "",
        "",
        "### 78. Add Code Pattern Detection",
        "",
        "**Meta:** `status:pending` `priority:low` `category:devex`",
        "**Files:** `cortical/patterns.py` (new), `scripts/find_patterns.py` (new)",
        "**Effort:** Large",
        "",
        "**Problem:** Hard to find common patterns (factory, singleton, etc.) in codebase.",
        "",
        "",
        "### 79. Add Corpus Health Dashboard",
        "",
        "**Meta:** `status:pending` `priority:low` `category:devex`",
        "**Files:** `scripts/corpus_health.py` (new)",
        "**Effort:** Medium",
        "",
        "**Problem:** No quick overview of corpus health metrics.",
        "",
        "",
        "### 80. Add \"Learning Mode\" for New Contributors",
        "",
        "**Meta:** `status:pending` `priority:low` `category:devex`",
        "**Files:** `scripts/learn_codebase.py` (new)",
        "**Effort:** Large",
        "",
        "**Problem:** Steep learning curve for new contributors.",
        "",
        "**Solution:** Guided tour of codebase using semantic search.",
        "",
        "",
        "### 87. Add Python Code Samples and Update Showcase",
        "",
        "**Meta:** `status:in-progress` `priority:medium` `category:showcase`",
        "**Files:** `showcase.py`, `samples/data_processor.py`, `samples/search_engine.py`, `samples/test_data_processor.py`",
        "**Effort:** Medium",
        "",
        "**Started:** 2025-12-11",
        "",
        "**Progress:** Sample files created, showcase updates in progress.",
        "",
        "",
        "### 88. Create Package Installation Files ‚úì",
        "",
        "**Meta:** `status:completed` `priority:critical` `category:devex`",
        "**Files:** `pyproject.toml`, `requirements.txt`, `cortical/py.typed`",
        "**Completed:** 2025-12-11",
        "",
        "**Solution Applied:**",
        "- Created `pyproject.toml` with modern Python packaging",
        "- Created `requirements.txt` documenting dev dependencies",
        "- Added `py.typed` marker for PEP 561 compliance",
        "- Tested: `pip install -e .` and `pip install -e \".[dev]\"` both work",
        "",
        "",
        "### 89. Create CONTRIBUTING.md ‚úì",
        "",
        "**Meta:** `status:completed` `priority:high` `category:devex`",
        "**Files:** `CONTRIBUTING.md`",
        "**Completed:** 2025-12-11",
        "",
        "**Solution Applied:**",
        "- Created comprehensive CONTRIBUTING.md with:",
        "  - Quick start (8 steps from fork to PR)",
        "  - Development setup instructions",
        "  - Test running commands",
        "  - Code style guidelines with examples",
        "  - Project structure overview",
        "  - Links to code-of-ethics.md and definition-of-done.md",
        "",
        "",
        "### 90. Create docs/quickstart.md Tutorial ‚úì",
        "",
        "**Meta:** `status:completed` `priority:high` `category:docs`",
        "**Files:** `docs/quickstart.md`",
        "**Completed:** 2025-12-11",
        "",
        "**Solution Applied:**",
        "- Created 5-minute quickstart tutorial with:",
        "  - Installation instructions",
        "  - \"Your First 10 Lines\" minimal example",
        "  - Understanding results section",
        "  - Query expansion demo",
        "  - Passage retrieval for RAG",
        "  - Key concepts table",
        "  - Save/load instructions",
        "  - Common patterns (batch, incremental, metadata)",
        "  - Troubleshooting section",
        "",
        "",
        "### 91. Create docs/README.md Index ‚úÖ",
        "",
        "**Meta:** `status:completed` `priority:medium` `category:docs`",
        "**Files:** `docs/README.md`",
        "**Effort:** Small",
        "**Completed:** 2025-12-12",
        "",
        "**Problem:** 11 docs files with no navigation.",
        "",
        "**Solution Applied:** Created `docs/README.md` with navigation by audience (New Users, Developers, AI Agents), reading paths, and categorized documentation links.",
        "",
        "",
        "### 92. Add Badges to README.md ‚úÖ",
        "",
        "**Meta:** `status:completed` `priority:medium` `category:devex`",
        "**Files:** `README.md`",
        "**Effort:** Small",
        "**Completed:** 2025-12-12",
        "",
        "**Problem:** No visual project health indicators.",
        "",
        "**Solution Applied:** Added 5 badges to README.md: Python 3.8+, MIT License, Tests (1121 passing), Coverage (>89%), Zero Dependencies.",
        "",
        "",
        "### 93. Update README with Documentation References ‚úÖ",
        "",
        "**Meta:** `status:completed` `priority:medium` `category:docs`",
        "**Files:** `README.md`",
        "**Effort:** Small",
        "**Depends:** 91",
        "**Completed:** 2025-12-12",
        "",
        "**Problem:** README doesn't mention docs/ folder.",
        "",
        "**Solution Applied:** Added \"Documentation\" section to README.md with table linking to all docs/*.md files including quickstart, architecture, algorithms, query-guide, cookbook, and glossary.",
        "",
        "",
        "### 94. Split query.py into Focused Modules ‚úÖ",
        "",
        "**Meta:** `status:completed` `priority:high` `category:architecture`",
        "**Files:** `cortical/query.py` ‚Üí `cortical/query/` package (8 modules)",
        "**Effort:** Large",
        "**Completed:** 2025-12-12",
        "",
        "**Problem:** Single 2,719-line file violated Single Responsibility Principle.",
        "",
        "**Solution Applied:**",
        "```",
        "cortical/query/",
        "‚îú‚îÄ‚îÄ __init__.py       # Re-export public API (backward compatible)",
        "‚îú‚îÄ‚îÄ expansion.py      # expand_query, expand_query_semantic",
        "‚îú‚îÄ‚îÄ search.py         # find_documents_for_query, fast_find_documents",
        "‚îú‚îÄ‚îÄ passages.py       # find_passages_for_query",
        "‚îú‚îÄ‚îÄ chunking.py       # Chunk-based text processing",
        "‚îú‚îÄ‚îÄ intent.py         # parse_intent_query, search_by_intent",
        "‚îú‚îÄ‚îÄ definitions.py    # is_definition_query, find_definition_passages",
        "‚îú‚îÄ‚îÄ ranking.py        # multi_stage_rank",
        "‚îî‚îÄ‚îÄ analogy.py        # complete_analogy",
        "```",
        "",
        "**Acceptance Criteria:**",
        "- [x] No file >500 lines",
        "- [x] All existing tests pass",
        "- [x] Backward-compatible imports from `cortical.query`",
        "",
        "",
        "### 95. Split processor.py into Focused Modules",
        "",
        "**Meta:** `status:pending` `priority:medium` `category:architecture`",
        "**Files:** `cortical/processor.py` (2,301 lines)",
        "**Effort:** Large",
        "**Depends:** 97",
        "",
        "**Problem:** God Object with 100+ methods.",
        "",
        "**Solution:** Extract DocumentManager, ComputationManager as internal classes.",
        "",
        "",
        "### 96. Centralize Duplicate Constants ‚úÖ",
        "",
        "**Meta:** `status:completed` `priority:medium` `category:code-quality`",
        "**Files:** `cortical/constants.py`",
        "**Effort:** Small",
        "**Completed:** 2025-12-12",
        "",
        "**Problem:** `RELATION_WEIGHTS`, `DOC_TYPE_BOOSTS` defined in multiple places.",
        "",
        "**Solution Applied:** Created `cortical/constants.py` as single source of truth containing RELATION_WEIGHTS, DOC_TYPE_BOOSTS, query keywords, and other shared constants. Updated imports across modules.",
        "",
        "",
        "### 97. Integrate CorticalConfig into CorticalTextProcessor ‚úÖ",
        "",
        "**Meta:** `status:completed` `priority:high` `category:architecture`",
        "**Files:** `cortical/processor.py`, `cortical/config.py`",
        "**Effort:** Medium",
        "**Completed:** 2025-12-11",
        "",
        "**Problem:** Config exists but isn't used by processor.",
        "",
        "**Solution Applied:**",
        "- Added `config` parameter to `CorticalTextProcessor.__init__()`",
        "- Config stored on processor instance (`self.config`)",
        "- Methods use config defaults for parameters like `damping`, `resolution`, etc.",
        "- Config saved/loaded with processor state via persistence",
        "- Per-call override still supported",
        "",
        "**Acceptance Criteria:**",
        "- [x] Config parameter accepted",
        "- [x] Methods use config defaults",
        "- [x] Override still possible per-call",
        "",
        "",
        "### 98. Replace print() with Logging Module",
        "",
        "**Meta:** `status:pending` `priority:medium` `category:code-quality`",
        "**Files:** `cortical/processor.py`, other modules",
        "**Effort:** Medium",
        "",
        "**Problem:** Uses print() for progress - can't configure.",
        "",
        "**Solution:** Use `logging.getLogger(__name__)`.",
        "",
        "",
        "### 99. Add Input Validation to Public Methods",
        "",
        "**Meta:** `status:pending` `priority:medium` `category:code-quality`",
        "**Files:** `cortical/processor.py`, `cortical/query.py`",
        "**Effort:** Medium",
        "",
        "**Problem:** Some methods don't validate inputs.",
        "",
        "**Solution:** Add validation at API boundary.",
        "",
        "",
        "### 100. Implement Plugin/Extension Registry",
        "",
        "**Meta:** `status:pending` `priority:low` `category:architecture`",
        "**Files:** `cortical/registry.py` (new), `cortical/processor.py`",
        "**Effort:** Large",
        "",
        "**Problem:** Can't add algorithms without modifying core.",
        "",
        "**Solution:** Registry pattern for PageRank, clustering, expansion strategies.",
        "",
        "",
        "### 101. Automate Staleness Tracking with Decorators",
        "",
        "**Meta:** `status:pending` `priority:low` `category:architecture`",
        "**Files:** `cortical/processor.py`",
        "**Effort:** Medium",
        "",
        "**Problem:** Manual `_mark_stale()` calls are error-prone.",
        "",
        "**Solution:** `@invalidates('tfidf', 'pagerank')` decorator.",
        "",
        "",
        "### 102. Add Tests for Edge Cases",
        "",
        "**Meta:** `status:pending` `priority:medium` `category:testing`",
        "**Files:** `tests/test_edge_cases.py` (new)",
        "**Effort:** Medium",
        "",
        "**Problem:** Missing tests for Unicode, large docs, malformed inputs.",
        "",
        "",
        "### 103. Add Priority Backlog Summary to Top of TASK_LIST.md ‚úì",
        "",
        "**Meta:** `status:completed` `priority:high` `category:task-mgmt`",
        "**Files:** `TASK_LIST.md`",
        "**Completed:** 2025-12-11",
        "",
        "**Solution Applied:** Added machine-parseable backlog with priority tiers (Critical/High/Medium/Low/Deferred) at top of file.",
        "",
        "",
        "### 104. Create TASK_ARCHIVE.md for Completed Tasks ‚úì",
        "",
        "**Meta:** `status:completed` `priority:high` `category:task-mgmt`",
        "**Files:** `TASK_ARCHIVE.md`, `TASK_LIST.md`",
        "**Completed:** 2025-12-11",
        "",
        "**Solution Applied:** Created archive with 75+ completed tasks, reduced TASK_LIST.md from 3,565 to 604 lines (83% reduction).",
        "",
        "",
        "### 105. Standardize Task Format with Machine-Parseable Metadata ‚úì",
        "",
        "**Meta:** `status:completed` `priority:medium` `category:task-mgmt`",
        "**Files:** `TASK_LIST.md`",
        "**Completed:** 2025-12-11",
        "",
        "**Solution Applied:** All tasks now use `**Meta:** \\`status:...\\` \\`priority:...\\`` format with effort estimates.",
        "",
        "",
        "### 106. Add Task Dependency Graph",
        "",
        "**Meta:** `status:pending` `priority:low` `category:task-mgmt`",
        "**Files:** Section in TASK_LIST.md",
        "**Effort:** Small",
        "**Depends:** 105",
        "",
        "**Problem:** Dependencies not visualized.",
        "",
        "",
        "### 107. Add \"Quick Context\" Section to Each Task",
        "",
        "**Meta:** `status:pending` `priority:medium` `category:task-mgmt`",
        "**Files:** `TASK_LIST.md`",
        "**Effort:** Medium",
        "**Depends:** 105",
        "",
        "**Problem:** Must search codebase to understand task context.",
        "",
        "",
        "### 108. Create Task Selection Helper Script",
        "",
        "**Meta:** `status:pending` `priority:low` `category:task-mgmt`",
        "**Files:** `scripts/next_task.py` (new)",
        "**Effort:** Medium",
        "**Depends:** 103, 105",
        "",
        "**Problem:** Selecting next task is manual work.",
        "",
        "",
        "### 109. Add \"Recently Completed\" Section for Context ‚úì",
        "",
        "**Meta:** `status:completed` `priority:low` `category:task-mgmt`",
        "**Files:** `TASK_LIST.md`",
        "**Completed:** 2025-12-11",
        "",
        "**Solution Applied:** Added \"Recently Completed (Last 7 Days)\" section with table format showing task, date, and notes.",
        "",
        "",
        "### 110. Add Section Markers to Large Files",
        "",
        "**Meta:** `status:pending` `priority:high` `category:ai-nav`",
        "**Files:** `cortical/processor.py`, `cortical/query.py`",
        "**Effort:** Small",
        "",
        "**Problem:** Large files (processor.py: 2,301 lines, query.py: 2,719 lines) are hard to navigate. AI assistants must scan large portions to find relevant sections.",
        "",
        "**Solution:** Add clear section markers like:",
        "```python",
        "# =============================================================================",
        "# DOCUMENT MANAGEMENT",
        "# =============================================================================",
        "",
        "# =============================================================================",
        "# COMPUTATION METHODS",
        "# =============================================================================",
        "```",
        "",
        "**Acceptance Criteria:**",
        "- [ ] processor.py has 5-7 logical sections marked",
        "- [ ] query.py has 5-7 logical sections marked",
        "- [ ] Section names match CLAUDE.md terminology",
        "",
        "",
        "### 111. Add \"See Also\" Cross-References to Docstrings",
        "",
        "**Meta:** `status:pending` `priority:high` `category:ai-nav`",
        "**Files:** `cortical/processor.py`, `cortical/query.py`, `cortical/analysis.py`",
        "**Effort:** Medium",
        "",
        "**Problem:** When reading a function, AI assistants don't know about related functions without searching.",
        "",
        "**Solution:** Add \"See Also\" sections to docstrings:",
        "```python",
        "def find_documents_for_query(self, query: str, top_n: int = 5):",
        "    \"\"\"",
        "    Find documents matching query.",
        "",
        "    See Also:",
        "        fast_find_documents: Faster search, document-level only",
        "        find_passages_for_query: Chunk-level retrieval for RAG",
        "        expand_query: Get expanded terms before searching",
        "    \"\"\"",
        "```",
        "",
        "**Target Functions:** Top 20 most-used public methods.",
        "",
        "",
        "### 112. Add Docstring Examples for Complex Functions",
        "",
        "**Meta:** `status:pending` `priority:high` `category:ai-nav`",
        "**Files:** `cortical/query.py`, `cortical/analysis.py`, `cortical/processor.py`",
        "**Effort:** Medium",
        "",
        "**Problem:** Complex functions lack examples showing expected input/output.",
        "",
        "**Solution:** Add Examples section to docstrings:",
        "```python",
        "def expand_query(self, query: str, max_expansions: int = 10):",
        "    \"\"\"",
        "    Expand query with related terms.",
        "",
        "    Example:",
        "        >>> processor.expand_query(\"neural networks\")",
        "        {'neural': 1.0, 'networks': 1.0, 'network': 0.85,",
        "         'learning': 0.72, 'deep': 0.68}",
        "    \"\"\"",
        "```",
        "",
        "**Target Functions:**",
        "- `expand_query()`, `expand_query_semantic()`, `expand_query_multihop()`",
        "- `find_documents_for_query()`, `find_passages_for_query()`",
        "- `parse_intent_query()`, `search_by_intent()`",
        "- `complete_analogy()`",
        "- `compute_pagerank()`, `compute_tfidf()`",
        "",
        "",
        "### 113. Document Staleness Tracking System ‚úÖ",
        "",
        "**Meta:** `status:completed` `priority:medium` `category:ai-nav`",
        "**Files:** `CLAUDE.md`",
        "**Effort:** Small",
        "**Completed:** 2025-12-12",
        "",
        "**Problem:** The staleness tracking system was powerful but not documented.",
        "",
        "**Solution Applied:** Added comprehensive \"Staleness Tracking System\" section to CLAUDE.md including:",
        "- Table of all `COMP_*` constants and what they track",
        "- How staleness works (all start stale, computed marks fresh)",
        "- API methods (`is_stale()`, `get_stale_computations()`)",
        "- Incremental update behavior",
        "- When to check staleness before reading computed values",
        "",
        "",
        "### 114. Add Type Aliases for Complex Types ‚úÖ",
        "",
        "**Meta:** `status:completed` `priority:medium` `category:ai-nav`",
        "**Files:** `cortical/types.py`",
        "**Effort:** Small",
        "**Completed:** 2025-12-12",
        "",
        "**Problem:** Complex return types like `List[Tuple[str, float, Dict[str, Any]]]` are hard to understand at a glance.",
        "",
        "**Solution Applied:** Created `cortical/types.py` with 20+ type aliases organized by category:",
        "- **Score types:** `DocumentScore`, `TermScore`, `DocumentResults`, `TermResults`",
        "- **Passage types:** `PassageResult`, `PassageResults`, `PassageWithPosition`",
        "- **Semantic types:** `SemanticRelation`, `SemanticRelations`",
        "- **Graph types:** `ConnectionMap`, `LayerDict`, `ClusterAssignment`",
        "- **Expansion types:** `ExpansionTerms`, `IntentResult`",
        "- **Fingerprint types:** `Fingerprint`, `FingerprintComparison`",
        "",
        "All types include docstrings explaining their structure.",
        "",
        "",
        "### 115. Create Component Interaction Diagram",
        "",
        "**Meta:** `status:pending` `priority:medium` `category:ai-nav`",
        "**Files:** `docs/architecture.md` or `CLAUDE.md`",
        "**Effort:** Medium",
        "",
        "**Problem:** Understanding how modules call each other requires tracing imports and function calls.",
        "",
        "**Solution:** Add ASCII or Mermaid diagram showing:",
        "```",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê",
        "‚îÇ processor.py‚îÇ ‚Üê Public API entry point",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò",
        "       ‚îÇ calls",
        "       ‚ñº",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê",
        "‚îÇ analysis.py ‚îÇ query.py ‚îÇ semantics.py ‚îÇ ...  ‚îÇ",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò",
        "       ‚îÇ operates on",
        "       ‚ñº",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê",
        "‚îÇ      layers.py  ‚Üí  minicolumn.py             ‚îÇ",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò",
        "```",
        "",
        "Include which module calls which, and data flow direction.",
        "",
        "",
        "### 116. Document Return Value Semantics ‚úÖ",
        "",
        "**Meta:** `status:completed` `priority:medium` `category:ai-nav`",
        "**Files:** `CLAUDE.md`",
        "**Effort:** Medium",
        "**Completed:** 2025-12-12",
        "",
        "**Problem:** Inconsistent understanding of what functions return in edge cases.",
        "",
        "**Solution Applied:** Added \"Return Value Semantics\" section to CLAUDE.md documenting:",
        "- **Edge Case Returns:** Table showing what each scenario returns (empty corpus ‚Üí `[]`, unknown doc_id ‚Üí `{}`, etc.)",
        "- **Score Ranges:** Relevance (unbounded), PageRank (0-1), TF-IDF (unbounded), Similarity (0-1), Confidence (0-1)",
        "- **Lookup Functions:** `None` for missing items vs `KeyError` for invalid structure",
        "- **Default Parameters:** Table of key defaults (`top_n=5`, `damping=0.85`, `resolution=1.0`, etc.)",
        "",
        "",
        "### 117. Create Debugging Cookbook",
        "",
        "**Meta:** `status:pending` `priority:low` `category:ai-nav`",
        "**Files:** `docs/debugging.md` (new)",
        "**Effort:** Medium",
        "",
        "**Problem:** Common debugging scenarios require discovering patterns through trial and error.",
        "",
        "**Solution:** Create cookbook with scenarios:",
        "",
        "1. **\"Why is my query returning no results?\"**",
        "   - Check if corpus has documents",
        "   - Check if terms exist in corpus",
        "   - Use `--expand` to see what's being searched",
        "",
        "2. **\"Why are PageRank values all zero?\"**",
        "   - Check if `compute_all()` was called",
        "   - Check staleness with `is_stale()`",
        "",
        "3. **\"Why is search slow?\"**",
        "   - Use `fast_find_documents()` for document-level",
        "   - Pre-build index with `build_search_index()`",
        "",
        "4. **\"Why are bigrams not connecting?\"**",
        "   - Verify space separator (not underscore)",
        "   - Check `compute_bigram_connections()` was called",
        "",
        "",
        "### 118. Add Function Complexity Annotations",
        "",
        "**Meta:** `status:pending` `priority:low` `category:ai-nav`",
        "**Files:** `cortical/processor.py`, `cortical/analysis.py`",
        "**Effort:** Small",
        "",
        "**Problem:** AI assistants don't know which functions are expensive to call.",
        "",
        "**Solution:** Add complexity notes to expensive functions:",
        "```python",
        "def compute_all(self, verbose: bool = True):",
        "    \"\"\"",
        "    Compute all network properties.",
        "",
        "    Complexity: O(n¬≤) where n = total minicolumns across all layers.",
        "    Typical time: 2-5 seconds for 10K documents.",
        "",
        "    Note: For incremental updates, prefer add_document_incremental()",
        "    which is O(m) where m = tokens in new document.",
        "    \"\"\"",
        "```",
        "",
        "**Target Functions:**",
        "- `compute_all()` - O(n¬≤)",
        "- `compute_pagerank()` - O(iterations √ó edges)",
        "- `build_concept_clusters()` - O(n √ó iterations)",
        "- `find_passages_for_query()` - O(docs √ó chunks)",
        "",
        "",
        "### 119. Create AI Metadata Generator Script ‚úÖ",
        "",
        "**Meta:** `status:completed` `priority:high` `category:ai-nav`",
        "**Files:** `scripts/generate_ai_metadata.py`, `tests/test_generate_ai_metadata.py`",
        "**Effort:** Medium",
        "**Completed:** 2025-12-11",
        "",
        "**Problem:** AI navigation tasks (110-118) require modifying code files directly, cluttering them for human readers. We need a way to provide rich AI navigation aids without polluting the source code.",
        "",
        "**Solution:** Generate companion `.ai_meta` files (YAML) that provide:",
        "- Section markers with line ranges",
        "- Function cross-references (\"See Also\")",
        "- Complexity annotations",
        "- Return value semantics",
        "- Docstring examples (extracted or generated)",
        "- Import/dependency information",
        "",
        "**Output format:**",
        "```yaml",
        "# processor.py.ai_meta - Auto-generated",
        "file: cortical/processor.py",
        "lines: 2301",
        "generated: 2025-12-11T14:30:00",
        "",
        "sections:",
        "  - name: \"Document Management\"",
        "    lines: [54, 520]",
        "    functions: [process_document, add_document_incremental, remove_document]",
        "  - name: \"Computation Methods\"",
        "    lines: [613, 1140]",
        "    functions: [compute_all, compute_importance, compute_tfidf]",
        "",
        "functions:",
        "  find_documents_for_query:",
        "    line: 1596",
        "    signature: \"(query: str, top_n: int = 5) -> List[Tuple[str, float]]\"",
        "    see_also:",
        "      fast_find_documents: \"~2-3x faster, document-level only\"",
        "      find_passages_for_query: \"Chunk-level retrieval for RAG\"",
        "    complexity: \"O(terms √ó documents)\"",
        "    returns:",
        "      on_empty_corpus: \"[]\"",
        "      on_no_matches: \"[]\"",
        "",
        "dependencies:",
        "  imports: [analysis, query, semantics]",
        "  imported_by: [__init__]",
        "```",
        "",
        "**Acceptance Criteria:**",
        "- [ ] Script generates .ai_meta for all cortical/*.py files",
        "- [ ] *.ai_meta added to .gitignore",
        "- [ ] Output is valid YAML (AI-parseable)",
        "- [ ] Extracts sections by analyzing class/function groupings",
        "- [ ] Extracts function signatures and line numbers",
        "- [ ] Identifies related functions by naming patterns",
        "- [ ] Can be run incrementally (only changed files)",
        "",
        "",
        "### 120. Add AI Metadata Loader to Claude Skills ‚úÖ",
        "",
        "**Meta:** `status:completed` `priority:high` `category:ai-nav`",
        "**Files:** `.claude/skills/ai-metadata/SKILL.md`, `.claude/skills/corpus-indexer/SKILL.md`",
        "**Effort:** Small",
        "**Depends:** 119",
        "**Completed:** 2025-12-11",
        "",
        "**Problem:** Generated .ai_meta files need to be used by AI assistants during code navigation.",
        "",
        "**Solution:** Created new `ai-metadata` skill that:",
        "1. Provides structured documentation for using .ai_meta files",
        "2. Explains metadata fields (sections, see_also, complexity hints)",
        "3. Shows best practices for AI agent navigation",
        "4. Updated corpus-indexer skill to include metadata generation commands",
        "",
        "",
        "### 121. Auto-regenerate AI Metadata on File Changes ‚úÖ",
        "",
        "**Meta:** `status:completed` `priority:high` `category:ai-nav`",
        "**Files:** `CLAUDE.md`, `.claude/skills/corpus-indexer/SKILL.md`",
        "**Effort:** Medium",
        "**Depends:** 119",
        "**Completed:** 2025-12-11",
        "",
        "**Problem:** .ai_meta files become stale when source files change.",
        "",
        "**Solution implemented:**",
        "1. **Documented in CLAUDE.md** - AI Agent Onboarding section with startup command",
        "2. **Incremental mode** - `python scripts/generate_ai_metadata.py --incremental` only updates changed files",
        "3. **Combined workflow** - `python scripts/index_codebase.py --incremental && python scripts/generate_ai_metadata.py --incremental`",
        "4. **Skills documentation** - corpus-indexer skill explains metadata regeneration",
        "",
        "**Recommended workflow for new agents:**",
        "```bash",
        "# On arrival, check if metadata exists and regenerate if needed",
        "ls cortical/*.ai_meta || python scripts/generate_ai_metadata.py",
        "```",
        "",
        "",
        "### 122. Investigate Concept Layer & Embeddings Regressions ‚úÖ",
        "",
        "**Meta:** `status:completed` `priority:critical` `category:bugfix`",
        "**Files:** `cortical/analysis.py`, `cortical/embeddings.py`, `showcase.py`",
        "**Effort:** Medium",
        "**Completed:** 2025-12-11",
        "",
        "**Problem:** Showcase output reveals potential regressions or bugs:",
        "",
        "1. **Concept Layer has only 3 clusters** for 95 documents",
        "2. **Graph embeddings show nonsensical similarities** - \"neural\" similar to \"blockchain\"",
        "",
        "**Root Cause Analysis:**",
        "",
        "1. **Clustering strictness logic was inverted** (introduced in Task #4):",
        "   - Bug: `change_threshold = (1.0 - cluster_strictness) * 0.3` meant high strictness ‚Üí easy label changes",
        "   - Fix: Changed to `change_threshold = cluster_strictness * 0.3` so high strictness ‚Üí resist changes",
        "   - Also fixed bonus logic that had the same inversion",
        "",
        "2. **Adjacency embeddings were sparse** for large graphs:",
        "   - Bug: Only captured direct connections to landmark nodes, resulting in mostly-zero vectors",
        "   - Fix: Added multi-hop propagation to reach landmarks through neighbors",
        "   - Also changed showcase.py to use `random_walk` method (better semantic results)",
        "",
        "3. **Concept cluster count** (3-5 for 95 docs) is actually correct behavior:",
        "   - The corpus is highly connected (avg 18.2 connections per token)",
        "   - Label propagation correctly merges connected tokens into large clusters",
        "   - One giant cluster (6656 tokens) with a few small clusters is expected",
        "",
        "**Solution Applied:**",
        "",
        "1. Fixed `cluster_strictness` logic in `analysis.py:585` and `analysis.py:601-603`",
        "2. Improved `_adjacency_embeddings()` with multi-hop propagation in `embeddings.py`",
        "3. Changed showcase.py to use `method='random_walk'` for embeddings",
        "4. Added 3 regression tests:",
        "   - `test_cluster_strictness_direction` - ensures higher strictness ‚Üí more clusters",
        "   - `test_random_walk_semantic_similarity` - ensures \"neural\" similar to \"networks\"",
        "   - `test_adjacency_produces_nonzero_embeddings` - ensures dense embeddings",
        "",
        "**Results After Fix:**",
        "- Embeddings now show \"neural\" similar to \"networks\" (0.938), \"learn\" (0.928) ‚úÖ",
        "- Clustering direction now matches documentation ‚úÖ",
        "- All 820 tests pass ‚úÖ",
        "",
        "**Acceptance Criteria:**",
        "- [x] Root cause identified via git history",
        "- [x] Embedding similarities semantically meaningful",
        "- [x] Regression test added to prevent recurrence",
        "- [~] Concept clusters > 10: Not achievable due to highly connected corpus (correct behavior)",
        "",
        "",
        "### 127. Create Cluster Coverage Evaluation Script ‚úÖ",
        "",
        "**Meta:** `status:completed` `priority:high` `category:devex`",
        "**Files:** `scripts/evaluate_cluster.py`, `tests/test_evaluate_cluster.py`",
        "**Effort:** Medium",
        "**Depends:** 125",
        "**Completed:** 2025-12-11",
        "",
        "**Problem:** When adding sample documents to create topic clusters (e.g., customer service), there's no automated way to determine if the cluster has sufficient coverage or needs more documents.",
        "",
        "**Solution Applied:** Created `scripts/evaluate_cluster.py` with:",
        "",
        "**Usage:**",
        "```bash",
        "# Find and evaluate documents by topic search",
        "python scripts/evaluate_cluster.py --topic \"customer service\"",
        "",
        "# Evaluate specific documents",
        "python scripts/evaluate_cluster.py --documents doc1,doc2,doc3",
        "",
        "# Find documents by keywords",
        "python scripts/evaluate_cluster.py --keywords customer,ticket,escalation --min-keywords 2",
        "",
        "# Show expansion suggestions",
        "python scripts/evaluate_cluster.py --topic \"machine learning\" --suggest --verbose",
        "```",
        "",
        "**Features Implemented:**",
        "1. **Three cluster detection modes:**",
        "   - `--topic`: Semantic search for related documents",
        "   - `--documents`: Explicit document list",
        "   - `--keywords`: Documents containing specified terms",
        "",
        "2. **Coverage Metrics:**",
        "   - Internal Cohesion: Weighted Jaccard similarity within cluster",
        "   - External Separation: 1 - similarity to outside documents",
        "   - Concept Coverage: Number of concepts covered",
        "   - Term Diversity: Unique terms / total occurrences",
        "   - Hub Document: Most centrally connected document",
        "",
        "3. **Coverage Assessment:**",
        "   - STRONG: High cohesion + separation + coverage",
        "   - ADEQUATE: Usable but could improve",
        "   - NEEDS EXPANSION: Low metrics",
        "",
        "4. **Expansion Suggestions:** Related terms not well-covered by cluster",
        "",
        "**Example Output:**",
        "```",
        "Cluster Analysis: Keywords: customer, ticket, support (4 documents)",
        "===================================================================",
        "Documents:",
        "  * complaint_resolution",
        "  * customer_satisfaction_metrics",
        "  * customer_support_fundamentals (hub)",
        "  * ticket_escalation_procedures",
        "",
        "Metrics:",
        "  Internal Cohesion:    0.08 (weak)",
        "  External Separation:  0.98 (good)",
        "  Concept Coverage:     35 concepts",
        "  Term Diversity:       0.80",
        "",
        "Coverage Assessment: ADEQUATE [~]",
        "  Cluster is usable but could improve: weak internal connectivity.",
        "```",
        "",
        "**Test Results:**",
        "- 24 new tests in tests/test_evaluate_cluster.py",
        "- 1025 total tests pass",
        "",
        "**Acceptance Criteria:**",
        "- [x] Script identifies document clusters by topic/keywords",
        "- [x] Computes cohesion and separation metrics",
        "- [x] Provides clear coverage assessment (adequate/needs expansion)",
        "- [x] Suggests specific expansion topics when coverage is low",
        "- [x] Works with existing corpus or standalone document set",
        "",
        "",
        "### 140. Analyze Customer Service Cluster Quality",
        "",
        "**Meta:** `status:pending` `priority:low` `category:research`",
        "**Files:** Analysis output",
        "**Effort:** Small",
        "**Depends:** 127",
        "",
        "**Problem:** The customer service cluster was added but not deeply analyzed.",
        "",
        "**Tasks:**",
        "1. Run cluster evaluation script on customer service documents",
        "2. Check if cluster forms a distinct concept group in Layer 2",
        "3. Verify semantic coherence of the cluster",
        "4. Document findings",
        "",
        "**Expected Insights:**",
        "- Whether 6 documents is sufficient for a coherent cluster",
        "- How customer service concepts connect to other domains",
        "- Quality of the \"ticket ‚Üí learning\" semantic bridge observed in embeddings",
        "",
        "",
        "### 129. Test Customer Service Retrieval Quality",
        "",
        "**Meta:** `status:pending` `priority:low` `category:testing`",
        "**Files:** `tests/test_customer_service_retrieval.py` (new, optional)",
        "**Effort:** Small",
        "",
        "**Problem:** No systematic testing of retrieval quality for customer service queries.",
        "",
        "**Test Queries:**",
        "```python",
        "queries = [",
        "    (\"how to handle angry customer\", [\"complaint_resolution\", \"customer_support_fundamentals\"]),",
        "    (\"ticket escalation process\", [\"ticket_escalation_procedures\"]),",
        "    (\"measure customer satisfaction\", [\"customer_satisfaction_metrics\"]),",
        "    (\"reduce customer churn\", [\"customer_retention_strategies\"]),",
        "    (\"call center workforce management\", [\"call_center_operations\"]),",
        "]",
        "```",
        "",
        "**Evaluation:**",
        "- Precision@1: Does the top result match expected?",
        "- Precision@3: Are expected docs in top 3?",
        "- Compare with/without customer service docs in corpus",
        "",
        "",
        "### 130. Expand Customer Service Sample Cluster",
        "",
        "**Meta:** `status:pending` `priority:low` `category:samples`",
        "**Files:** `samples/*.txt` (new documents)",
        "**Effort:** Medium",
        "",
        "**Problem:** The current 6 customer service documents may benefit from expansion.",
        "",
        "**Potential New Documents:**",
        "1. `live_chat_support.txt` - Chat-specific support strategies",
        "2. `sla_management.txt` - Service level agreements and monitoring",
        "3. `crm_integration.txt` - CRM systems and helpdesk software",
        "4. `customer_journey_mapping.txt` - Journey analysis and touchpoints",
        "5. `support_automation.txt` - Chatbots, auto-responses, AI in support",
        "6. `multilingual_support.txt` - International customer service",
        "",
        "**Depends On:** Results from Task #128 (cluster quality analysis)",
        "",
        "",
        "### 131. Investigate Cross-Domain Semantic Bridges",
        "",
        "**Meta:** `status:pending` `priority:low` `category:research`",
        "**Files:** Analysis output, potentially `docs/semantic_bridges.md`",
        "**Effort:** Medium",
        "",
        "**Problem:** Interesting semantic connections exist between seemingly unrelated domains. The graph embeddings showed \"ticket\" similar to \"learning\" (0.937) - understanding these bridges could reveal insights about the corpus structure.",
        "",
        "**Research Questions:**",
        "1. What concepts bridge customer service to other domains?",
        "2. Why does \"ticket\" connect to \"learning\"? (ticket systems in education? ticketing in other contexts?)",
        "3. Are there other unexpected cross-domain connections?",
        "4. Can these bridges improve cross-domain search?",
        "",
        "**Approach:**",
        "1. Extract embedding neighbors for customer service terms",
        "2. Identify terms that appear in multiple domain clusters",
        "3. Trace connection paths in the concept graph",
        "4. Document interesting findings",
        "",
        "**Potential Applications:**",
        "- Improve query expansion with cross-domain terms",
        "- Suggest related documents from different domains",
        "- Identify knowledge gaps at domain boundaries",
        "",
        "",
        "### 141. Filter Python Keywords/Artifacts from Analysis ‚úÖ",
        "",
        "**Meta:** `status:completed` `priority:high` `category:quality`",
        "**Files:** `cortical/tokenizer.py`, `showcase.py`",
        "**Effort:** Medium",
        "**Completed:** 2025-12-12",
        "",
        "**Problem (Dog-fooding 2025-12-12):** When Python code files are added to the corpus (samples/data_processor.py, etc.), Python keywords pollute the analysis:",
        "- \"self\" is #2 in PageRank (0.0038) - meaningless for content analysis",
        "- TF-IDF top terms: `assertequal`, `def`, `mockdatarecord`, `str`, `doc_id`",
        "- These artifacts drown out meaningful content terms",
        "",
        "**Solution Applied:**",
        "1. Added `CODE_NOISE_TOKENS` constant with common Python/test tokens",
        "2. Added `filter_code_noise` parameter to Tokenizer",
        "3. showcase.py now uses `filter_code_noise=True` by default",
        "",
        "**Acceptance Criteria:**",
        "- [x] \"self\", \"def\", \"str\" not in top 20 PageRank terms when code files present",
        "- [x] TF-IDF surfaces meaningful terms, not syntax",
        "- [x] Existing code search features still work (1121 tests pass)",
        "",
        "",
        "### 142. Investigate 74s compute_all() Performance Regression ‚úÖ",
        "",
        "**Meta:** `status:completed` `priority:high` `category:perf`",
        "**Files:** `cortical/processor.py`, `cortical/embeddings.py`",
        "**Effort:** Medium",
        "**Completed:** 2025-12-12",
        "",
        "**Problem (Dog-fooding 2025-12-12):** compute_all() took 74.28s for 109 documents.",
        "",
        "**Root Cause Found:** `compute_graph_embeddings()` took 58s+ using adjacency method with multi-hop propagation on 7268 tokens.",
        "",
        "**Solution Applied:**",
        "1. Added `fast` embedding method (direct adjacency, no multi-hop)",
        "2. Added `max_terms` parameter for sampling top-N by PageRank",
        "3. Auto-selects sampling: <2000 tokens = all, 2000-5000 = 1500, >5000 = 1000",
        "4. Changed default method from 'adjacency' to 'fast'",
        "",
        "**Results:**",
        "- compute_graph_embeddings: 58s ‚Üí 0.01s",
        "- compute_all: 74.28s ‚Üí 14.21s (5.2x speedup)",
        "",
        "**Acceptance Criteria:**",
        "- [x] Identify root cause with profiling data",
        "- [x] compute_all() under 30s for 109 documents (achieved 14.21s)",
        "- [x] Document findings",
        "",
        "",
        "### 143. Investigate Negative Silhouette Score in Clustering ‚úÖ",
        "",
        "**Meta:** `status:completed` `priority:high` `category:quality`",
        "**Files:** `cortical/analysis.py`, `cortical/processor.py`",
        "**Effort:** Medium",
        "**Completed:** 2025-12-12",
        "",
        "**Problem (Dog-fooding 2025-12-12):** Clustering quality metrics show:",
        "- silhouette = -0.00 (negative indicates poor cluster separation)",
        "- Only 33 clusters for 109 documents",
        "- modularity = 0.40 (acceptable but not great)",
        "",
        "**Investigation Findings:**",
        "The negative silhouette score is **expected behavior**, not a bug. Key insights:",
        "",
        "1. **Modularity and silhouette measure different things:**",
        "   - Modularity: Graph edge density within clusters (what Louvain optimizes)",
        "   - Silhouette: Document co-occurrence similarity (semantic coherence)",
        "",
        "2. **Tokens that co-occur in sentences don't necessarily share documents:**",
        "   - Louvain groups tokens that appear together in sentences",
        "   - Silhouette measures if tokens appear in the same documents",
        "   - These are fundamentally different similarity metrics",
        "",
        "3. **Higher Louvain resolution makes silhouette worse:**",
        "   - Resolution 1.0: silhouette=-0.03",
        "   - Resolution 3.0: silhouette=-0.20",
        "   - More clusters = less document overlap per cluster",
        "",
        "**Solution Applied:**",
        "1. Changed silhouette to use document co-occurrence (Jaccard on document sets)",
        "2. Updated quality assessment to explain \"typical graph clustering\" for silhouette -0.1 to 0.1",
        "3. Updated docstrings to explain metric interpretation",
        "4. Added `_doc_similarity()` helper function",
        "",
        "**Results:**",
        "```",
        "34 clusters with Good community structure (modularity 0.37),",
        "typical graph clustering (silhouette -0.06), moderately balanced sizes",
        "```",
        "",
        "**Key Lesson:** Don't assume metrics must be positive. Understand what each metric measures before setting acceptance criteria.",
        "",
        "",
        "### 144. Boost Exact Document Name Matches in Search ‚úÖ",
        "",
        "**Meta:** `status:completed` `priority:high` `category:quality`",
        "**Files:** `cortical/query/search.py`",
        "**Effort:** Small",
        "**Completed:** 2025-12-12",
        "",
        "**Problem (Dog-fooding 2025-12-12):** Query \"distributed systems\" returns:",
        "1. unix_evolution (score: 19.804)",
        "2. database_design_patterns (score: 16.583)",
        "3. comprehensive_machine_learning (score: 13.244)",
        "",
        "But `distributed_systems` document exists and should rank #1 for this query.",
        "",
        "**Solution Applied:**",
        "Added `doc_name_boost` parameter (default 2.0) to `find_documents_for_query()` and `fast_find_documents()`. Boost is proportional to query term overlap with document ID.",
        "",
        "**Results:**",
        "- \"distributed systems\" ‚Üí `distributed_systems` now #1",
        "- \"fermentation\" ‚Üí `fermentation_science` still #1",
        "- \"quantum computing\" ‚Üí `quantum_computing_basics` now #1",
        "",
        "**Acceptance Criteria:**",
        "- [x] Query \"distributed systems\" returns `distributed_systems` in top 2",
        "- [x] Query \"fermentation\" keeps `fermentation_science` at #1",
        "- [x] No regression on other queries (1121 tests pass)",
        "",
        "",
        "### 145. Improve Graph Embedding Quality for Common Terms ‚úÖ",
        "",
        "**Meta:** `status:completed` `priority:high` `category:quality`",
        "**Files:** `cortical/embeddings.py`, `cortical/processor.py`",
        "**Effort:** Medium",
        "**Completed:** 2025-12-12",
        "",
        "**Problem (Dog-fooding 2025-12-12):** Graph embeddings show semantically odd similarities:",
        "- 'data' ‚Üí cognitive (0.968), alternatives (0.967), server (0.958)",
        "- 'machine' ‚Üí rate, experience, gradients (not semantically related)",
        "",
        "**Root Cause Analysis:**",
        "Graph-based embeddings use connection patterns to landmarks. Sparse connections",
        "(only 8-13 out of 64 landmarks) cause misleading high similarity when vectors",
        "are normalized. Terms sharing large documents have similar patterns regardless",
        "of semantic meaning.",
        "",
        "**Solution Applied:**",
        "1. Added 'tfidf' embedding method that uses document distribution as feature space",
        "2. Added IDF weighting to 'fast' method to down-weight common terms",
        "3. Updated docstrings to recommend 'tfidf' for semantic similarity tasks",
        "",
        "**Results:**",
        "```",
        "FAST (graph):   'machine' ‚Üí rate (0.82), gradients (0.77), experience (0.73)",
        "TF-IDF:         'machine' ‚Üí representations (0.71), learning (0.71), image (0.70)",
        "",
        "FAST (graph):   'learning' ‚Üí training (0.70), approaches (0.64)",
        "TF-IDF:         'learning' ‚Üí neural (0.82), networks (0.81), training (0.77)",
        "```",
        "",
        "TF-IDF embeddings capture semantic similarity much better because terms appearing",
        "in similar documents are usually semantically related.",
        "",
        "**Acceptance Criteria:**",
        "- [x] 'data' filtered out by CODE_NOISE_TOKENS (Task #141)",
        "- [x] High-frequency terms don't dominate (TF-IDF naturally down-weights)",
        "- [x] Existing good similarities preserved and improved",
        "- [x] New 'tfidf' method available for semantic similarity tasks",
        "",
        "",
        "### 146. Create Behavioral Tests for Core User Workflows ‚úÖ",
        "",
        "**Meta:** `status:completed` `priority:high` `category:testing`",
        "**Files:** `tests/test_behavioral.py`",
        "**Effort:** Medium",
        "**Completed:** 2025-12-12",
        "",
        "**Purpose:** Created acceptance/behavioral tests that verify the system delivers expected",
        "user outcomes, catching the kinds of issues discovered during dog-fooding.",
        "",
        "**Solution Applied:** Created `tests/test_behavioral.py` with 11 tests across 4 categories:",
        "",
        "1. **TestSearchBehavior** (3 tests)",
        "   - `test_document_name_matches_rank_highly` - \"distributed systems\" ‚Üí distributed_systems in top 2",
        "   - `test_query_expansion_improves_recall` - Expanded queries find more relevant docs",
        "   - `test_code_search_finds_implementations_not_tests` - Real code preferred over test files",
        "",
        "2. **TestPerformanceBehavior** (2 tests)",
        "   - `test_compute_all_under_threshold` - Full analysis < 30s for 100+ docs",
        "   - `test_search_is_fast` - Single query < 100ms",
        "",
        "3. **TestQualityBehavior** (4 tests)",
        "   - `test_pagerank_surfaces_meaningful_terms` - No 'self', 'def' in top 20",
        "   - `test_clustering_produces_coherent_groups` - modularity > 0.3",
        "   - `test_tfidf_embeddings_capture_semantic_similarity` - 'learning' similar to 'neural'",
        "   - `test_no_mega_cluster` - No cluster > 25% of tokens",
        "",
        "4. **TestRobustnessBehavior** (2 tests)",
        "   - `test_empty_query_raises_error` - ValueError for empty queries",
        "   - `test_unknown_terms_handled_gracefully` - Nonsense queries return empty results",
        "",
        "**Acceptance Criteria:**",
        "- [x] TestSearchBehavior class with 3+ tests",
        "- [x] TestPerformanceBehavior class with 2+ tests",
        "- [x] TestQualityBehavior class with 3+ tests (4 implemented)",
        "- [x] TestRobustnessBehavior class with 2+ tests",
        "- [x] All tests pass on current codebase",
        "- [x] Tests catch regressions from Tasks #141-145 if reverted",
        "",
        "",
        "### 147. Fix Misleading Hardcoded Values ‚úÖ",
        "",
        "**Meta:** `status:completed` `priority:high` `category:bugfix`",
        "**Files:** `cortical/query/definitions.py`, `cortical/layers.py`, `cortical/query/expansion.py`, `cortical/analysis.py`, `cortical/minicolumn.py`, `tests/test_layers.py`, `tests/test_query.py`",
        "**Effort:** Medium",
        "**Completed:** 2025-12-12",
        "",
        "**Problem:** Several hardcoded values in the codebase were misleading - they made something appear to be true when it wasn't.",
        "",
        "**Issues Fixed:**",
        "",
        "1. **Backwards parameter names** (`definitions.py:309-310`):",
        "   - `test_file_boost_factor=0.5` sounded like a boost but actually reduced scores by 50%",
        "   - `test_file_penalty=0.7` was called \"penalty\" but was lighter than the \"boost\"",
        "   - Renamed to `test_with_definition_penalty` and `test_without_definition_penalty`",
        "",
        "2. **Useless sparsity threshold** (`layers.py:192`):",
        "   - Hardcoded `threshold=1.0` only counted terms with activation=0 (unused terms)",
        "   - Changed to use `threshold_fraction * average_activation` for meaningful sparsity",
        "",
        "3. **Hardcoded config value** (`expansion.py:77`):",
        "   - Hardcoded `0.4` instead of using `DEFAULT_CHAIN_VALIDITY` constant",
        "   - Now imports and uses the config constant",
        "",
        "4. **Tolerance parameter ignored** (`analysis.py:301`):",
        "   - `compute_hierarchical_pagerank()` ignored caller's `tolerance` parameter",
        "   - Was hardcoded to `1e-6` instead of using the function parameter",
        "",
        "5. **Confidence only increasing** (`minicolumn.py:189`):",
        "   - Used `max(old, new)` so confidence could only increase, never decrease",
        "   - Changed to weighted average so confidence can decrease with lower-confidence evidence",
        "",
        "**Acceptance Criteria:**",
        "- [x] All misleading parameter names clarified",
        "- [x] Sparsity threshold made meaningful",
        "- [x] Config constants used consistently",
        "- [x] Function parameters respected (not ignored)",
        "- [x] Confidence semantics corrected",
        "- [x] All 1133 tests pass",
        "",
        "",
        "## Legacy Test Investigation Tasks",
        "",
        "These tasks were created during the test coverage review (2025-12-13). 16 duplicated legacy tests were removed, and 13 remaining tests need investigation to determine if they should be migrated to categorized test directories or kept as-is.",
        "",
        "### 197. Investigate test_coverage_gaps.py (91 tests)",
        "",
        "**Meta:** `status:pending` `priority:medium` `category:testing`",
        "**Files:** `tests/test_coverage_gaps.py`",
        "**Effort:** Medium",
        "",
        "**Problem:** This file contains 91 tests covering edge cases and coverage gaps. Need to investigate:",
        "1. Check git history for any previous migration attempts",
        "2. Determine if tests should move to `tests/unit/` or `tests/regression/`",
        "3. Check for overlap with existing unit tests",
        "",
        "**Investigation Steps:**",
        "```bash",
        "git log --oneline -- tests/test_coverage_gaps.py | head -10",
        "grep -l \"coverage_gap\\|edge.*case\" tests/unit/*.py",
        "```",
        "",
        "",
        "### 198. Investigate test_cli_wrapper.py (96 tests)",
        "",
        "**Meta:** `status:pending` `priority:medium` `category:testing`",
        "**Files:** `tests/test_cli_wrapper.py`",
        "**Effort:** Medium",
        "",
        "**Problem:** 96 tests for CLI wrapper. Need to investigate:",
        "1. Check git history for migration attempts",
        "2. Determine if these belong in `tests/integration/` as CLI integration tests",
        "3. Verify no unit test coverage exists",
        "",
        "",
        "### 199. Investigate test_edge_cases.py (53 tests)",
        "",
        "**Meta:** `status:pending` `priority:medium` `category:testing`",
        "**Files:** `tests/test_edge_cases.py`",
        "**Effort:** Small",
        "",
        "**Problem:** 53 tests for edge cases (Unicode, large docs, malformed inputs). Need to:",
        "1. Check git history",
        "2. Consider moving to `tests/unit/` or creating `tests/robustness/`",
        "3. These are likely unique and should be preserved",
        "",
        "",
        "### 200. Investigate test_incremental_indexing.py (47 tests)",
        "",
        "**Meta:** `status:pending` `priority:medium` `category:testing`",
        "**Files:** `tests/test_incremental_indexing.py`",
        "**Effort:** Small",
        "",
        "**Problem:** 47 tests for incremental document operations (add, remove, batch). Need to:",
        "1. Check git history",
        "2. Verify no overlap with `tests/unit/test_processor_core.py`",
        "3. Consider moving to `tests/integration/` if they test cross-module behavior",
        "",
        "",
        "### 201. Investigate test_intent_query.py (24 tests)",
        "",
        "**Meta:** `status:pending` `priority:medium` `category:testing`",
        "**Files:** `tests/test_intent_query.py`",
        "**Effort:** Small",
        "",
        "**Problem:** 24 tests for intent-based query parsing. Need to:",
        "1. Check git history",
        "2. Check if `tests/unit/test_query.py` or related files cover this",
        "3. Consider moving to `tests/unit/test_query_intent.py`",
        "",
        "",
        "### 202. Investigate test_behavioral.py (9 tests)",
        "",
        "**Meta:** `status:pending` `priority:medium` `category:testing`",
        "**Files:** `tests/test_behavioral.py`",
        "**Effort:** Small",
        "",
        "**Problem:** 9 behavioral/acceptance tests. These likely belong in `tests/behavioral/` which exists.",
        "1. Check git history",
        "2. Verify tests match behavioral test patterns",
        "3. Move to `tests/behavioral/` if appropriate",
        "",
        "",
        "### 203. Investigate test_query_optimization.py (20 tests)",
        "",
        "**Meta:** `status:pending` `priority:medium` `category:testing`",
        "**Files:** `tests/test_query_optimization.py`",
        "**Effort:** Small",
        "",
        "**Problem:** 20 tests for query performance. Need to:",
        "1. Check git history",
        "2. Consider moving to `tests/performance/` directory",
        "3. Verify no overlap with existing performance tests",
        "",
        "",
        "### 204. Investigate legacy script tests (6 files)",
        "",
        "**Meta:** `status:pending` `priority:medium` `category:testing`",
        "**Files:** `tests/test_analyze_louvain_resolution.py`, `tests/test_ask_codebase.py`, `tests/test_evaluate_cluster.py`, `tests/test_generate_ai_metadata.py`, `tests/test_search_codebase.py`, `tests/test_showcase.py`",
        "**Effort:** Medium",
        "",
        "**Problem:** 6 test files (132 tests total) for scripts and showcase. Need to:",
        "1. Check git history for each",
        "2. Consider creating `tests/scripts/` directory",
        "3. Or moving to `tests/integration/` if they test end-to-end functionality",
        "**Files to investigate:**",
        "- test_analyze_louvain_resolution.py (23 tests) - script test",
        "- test_ask_codebase.py (19 tests) - script test",
        "- test_evaluate_cluster.py (24 tests) - script test",
        "- test_generate_ai_metadata.py (29 tests) - script test",
        "- test_search_codebase.py (24 tests) - script test",
        "- test_showcase.py (13 tests) - integration test",
        "| UnitTest | 20 | Unit test coverage initiative (#159-178) - Target 90% |",
        "| Arch | 6 | Architecture refactoring (#133, 134, 135, 95, 100, 101) |",
        "| CodeQual | 2 | Code quality improvements (#98, 99) |",
        "| Testing | 12 | Test coverage, performance, legacy test investigation (#102, 129, 148, 149, 197-204) |",
        "| TaskMgmt | 3 | Task management system (#107, 106, 108) |",
        "| AINav | 3 | AI assistant navigation (#115, 117, 118) |",
        "| DevEx | 7 | Developer experience, scripts (#73-80) |",
        "| Showcase | 1 | In progress (#87) |",
        "| Deferred | 7 | Low priority or superseded |",
        "*Updated 2025-12-12 - Comprehensive unit test coverage initiative (#159-178) targeting 90% coverage from unit tests alone*",
        "*Last restructured: 2025-12-11 (Tasks 88, 89, 90, 103, 104, 105, 109 implemented)*"
      ],
      "context_before": [
        "1. Deprecate `lateral_connections` in favor of `typed_connections`",
        "2. Make `lateral_connections` a property that derives from `typed_connections`",
        "3. Keep both but document the trade-off",
        "",
        "**Context from code review (2025-12-13):**",
        "- Found in comprehensive code review of core classes",
        "- Memory concern for large corpora with millions of edges",
        "",
        "---",
        ""
      ],
      "context_after": [
        "### 196. Add runtime warning for spectral embeddings on large graphs",
        "",
        "**Meta:** `status:pending` `priority:low` `category:devex`",
        "**Files:** `cortical/embeddings.py`",
        "",
        "**Problem:**",
        "Spectral embeddings are O(n¬≤) but there's no runtime warning when called with large graphs. Users may wait unexpectedly.",
        "",
        "**Fix:** Add warning for large graphs:",
        "```python",
        "if n > 5000:",
        "    import warnings",
        "    warnings.warn(f\"Spectral embeddings with {n} terms will be slow (O(n¬≤))\")",
        "```",
        "",
        "---",
        "",
        "",
        "",
        "",
        "",
        "",
        "---",
        "",
        "",
        "",
        "",
        "**Effort:** Medium",
        "",
        "",
        "---",
        "",
        "",
        "**Effort:** Medium",
        "",
        "",
        "---",
        "",
        "",
        "**Effort:** Small",
        "",
        "",
        "---",
        "",
        "",
        "**Effort:** Small",
        "",
        "",
        "---",
        "",
        "",
        "",
        "",
        "---",
        "",
        "",
        "**Files:** `tests/test_behavioral.py`",
        "",
        "",
        "---",
        "",
        "",
        "",
        "",
        "---",
        "",
        "",
        "**Effort:** Medium",
        "",
        "",
        "---",
        "",
        "",
        "",
        "",
        "",
        "---",
        "",
        "## Category Index",
        "",
        "| Category | Pending | Description |",
        "|----------|---------|-------------|",
        "| Research | 2 | Research and analysis (#140, 131) |",
        "| Samples | 1 | Sample document improvements (#130) |",
        "",
        "",
        "---",
        "",
        "## Notes",
        "",
        "- **Effort estimates:** Small (<1 hour), Medium (1-4 hours), Large (1+ days)",
        "- **Dependencies:** Complete dependent tasks first",
        "- **Quick Context:** Key info to start task without searching",
        "- **Archive:** Full history in [TASK_ARCHIVE.md](TASK_ARCHIVE.md)",
        "",
        "---",
        ""
      ],
      "change_type": "modify"
    },
    {
      "file": "scripts/validate_task_list.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "#!/usr/bin/env python3",
        "\"\"\"",
        "Validate TASK_LIST.md for common issues.",
        "",
        "Usage:",
        "    python scripts/validate_task_list.py",
        "",
        "Checks for:",
        "- Tasks marked pending that have completed test files",
        "- Mismatched pending task counts",
        "- Stale \"In Progress\" tasks",
        "- Completed tasks with full details still in TASK_LIST.md",
        "\"\"\"",
        "",
        "import re",
        "import os",
        "from pathlib import Path",
        "",
        "",
        "def main():",
        "    task_list = Path(\"TASK_LIST.md\").read_text()",
        "    archive = Path(\"TASK_ARCHIVE.md\").read_text() if Path(\"TASK_ARCHIVE.md\").exists() else \"\"",
        "",
        "    issues = []",
        "",
        "    # Check 1: Header exists and has reasonable values",
        "    count_match = re.search(r'\\*\\*Pending Tasks:\\*\\* (\\d+)', task_list)",
        "    completed_match = re.search(r'\\*\\*Completed Tasks:\\*\\* (\\d+)', task_list)",
        "    if count_match:",
        "        claimed_pending = int(count_match.group(1))",
        "        if claimed_pending > 100:",
        "            issues.append(f\"‚ö†Ô∏è  Pending count seems high: {claimed_pending} tasks\")",
        "    if completed_match:",
        "        claimed_completed = int(completed_match.group(1))",
        "        # Sanity check - completed should be positive",
        "        if claimed_completed < 1:",
        "            issues.append(f\"‚ö†Ô∏è  Completed count seems low: {claimed_completed}\")",
        "",
        "    # Check 2: Tasks with status:pending that are in archive",
        "    pending_tasks = re.findall(r'`status:pending`.*?#(\\d+)', task_list)",
        "    archived_tasks = re.findall(r'\\| (\\d+) \\|.*?\\| \\d{4}-\\d{2}-\\d{2} \\|', archive)",
        "    stale = set(pending_tasks) & set(archived_tasks)",
        "    if stale:",
        "        issues.append(f\"‚ö†Ô∏è  Tasks marked pending but in archive: {', '.join(sorted(stale, key=int))}\")",
        "",
        "    # Check 3: Unit test tasks still marked pending but tests exist",
        "    unit_test_dir = Path(\"tests/unit\")",
        "    if unit_test_dir.exists():",
        "        test_files = list(unit_test_dir.glob(\"test_*.py\"))",
        "        if len(test_files) > 15:  # Substantial unit test coverage exists",
        "            # Check if unit test tasks (#159-178) are still marked pending",
        "            for task_num in range(159, 179):",
        "                if f\"### {task_num}.\" in task_list and \"status:pending\" in task_list:",
        "                    pattern = rf'### {task_num}\\.[^\\n]*\\n\\n\\*\\*Meta:\\*\\* `status:pending`'",
        "                    if re.search(pattern, task_list):",
        "                        issues.append(f\"‚ö†Ô∏è  Task #{task_num} marked pending but tests/unit/ has {len(test_files)} test files\")",
        "                        break",
        "",
        "    # Check 4: Completed task markers (‚úÖ, ‚úì) in pending task details (not coverage tables)",
        "    if \"## Pending Task Details\" in task_list:",
        "        pending_section = task_list.split(\"## Pending Task Details\")[1]",
        "        # Exclude coverage baseline table (which legitimately has ‚úÖ markers)",
        "        if \"## Unit Test Coverage Baseline\" in pending_section:",
        "            pending_section = pending_section.split(\"## Unit Test Coverage Baseline\")[0]",
        "        if \"## Category Index\" in pending_section:",
        "            pending_section = pending_section.split(\"## Category Index\")[0]",
        "        # Look for completion markers in actual task descriptions",
        "        completed_markers = len(re.findall(r'### \\d+\\..*?‚úÖ|### \\d+\\..*?‚úì|`status:completed`', pending_section))",
        "        if completed_markers > 0:",
        "            issues.append(f\"‚ö†Ô∏è  Found {completed_markers} completed tasks in Pending Task Details section\")",
        "",
        "    # Check 5: Large file size warning",
        "    lines = task_list.count('\\n')",
        "    if lines > 500:",
        "        issues.append(f\"‚ö†Ô∏è  TASK_LIST.md has {lines} lines - consider archiving completed task details\")",
        "",
        "    # Report",
        "    if issues:",
        "        print(\"üîç Task List Validation Issues Found:\\n\")",
        "        for issue in issues:",
        "            print(f\"  {issue}\")",
        "        print(f\"\\n‚ùå {len(issues)} issue(s) found\")",
        "        return 1",
        "    else:",
        "        print(\"‚úÖ Task list looks healthy!\")",
        "        print(f\"   - {lines} lines\")",
        "        print(f\"   - Pending: {claimed_pending if count_match else 'unknown'}\")",
        "        print(f\"   - Completed: {claimed_completed if completed_match else 'unknown'}\")",
        "        return 0",
        "",
        "",
        "if __name__ == \"__main__\":",
        "    os.chdir(Path(__file__).parent.parent)",
        "    exit(main())"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    }
  ],
  "hour_of_day": 13,
  "day_of_week": "Saturday",
  "seconds_since_last_commit": -174366,
  "is_merge": true,
  "is_initial": false,
  "parent_count": 2,
  "session_id": null,
  "related_chats": [],
  "ci_result": null,
  "reverted": false,
  "amended": false
}