{
  "hash": "051d924cae884986a86ebf3de91bd13cdb2a3c01",
  "message": "Merge pull request #55 from scrawlsbenches/claude/review-core-classes-012xcLZDyNh26Tt6ABrezcSU",
  "author": "scrawlsbenches",
  "timestamp": "2025-12-13 06:38:49 -0500",
  "branch": "claude/multi-index-design-DvifZ",
  "files_changed": [
    ".github/workflows/ci.yml",
    "CLAUDE.md",
    "TASK_LIST.md",
    "corpus_dev.pkl",
    "cortical/layers.py",
    "cortical/persistence.py",
    "cortical/semantics.py",
    "scripts/index_codebase.py",
    "tests/test_coverage_gaps.py",
    "tests/test_layers.py",
    "tests/test_persistence.py",
    "tests/unit/test_semantics.py"
  ],
  "insertions": 343,
  "deletions": 32,
  "hunks": [
    {
      "file": "workflows/ci.yml b/.github/workflows/ci.yml",
      "function": null,
      "start_line": 1,
      "lines_added": [
        "# =============================================================================",
        "# CI TEST ARCHITECTURE - READ BEFORE MODIFYING",
        "# =============================================================================",
        "#",
        "# CRITICAL: Pytest runs unittest-based tests natively!",
        "# DO NOT run both pytest and unittest on the same test files.",
        "# This doubles CI time and provides no benefit.",
        "#",
        "# ‚ùå WRONG (runs tests twice):",
        "#    coverage run -m pytest tests/",
        "#    coverage run --append -m unittest discover -s tests",
        "#",
        "# ‚úÖ CORRECT (pytest handles both):",
        "#    coverage run -m pytest tests/",
        "#",
        "# Test stages run in dependency order:",
        "#   smoke ‚Üí unit ‚Üí integration ‚Üí [regression, behavioral, performance]",
        "#                              ‚Üí coverage-report",
        "#",
        "# Each stage runs specific test directories/files to avoid duplication.",
        "# The final coverage-report stage runs ALL tests once for the combined number.",
        "# =============================================================================",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "name: CI - Test Suite",
        ""
      ],
      "context_after": [
        "on:",
        "  push:",
        "  pull_request:",
        "    branches: [ main ]",
        "  workflow_dispatch:  # Allow manual triggering",
        "",
        "concurrency:",
        "  group: ${{ github.workflow }}-${{ github.ref }}",
        "  cancel-in-progress: true",
        ""
      ],
      "change_type": "add"
    },
    {
      "file": "workflows/ci.yml b/.github/workflows/ci.yml",
      "function": "jobs:",
      "start_line": 234,
      "lines_added": [
        "        # ‚ö†Ô∏è  IMPORTANT: DO NOT add duplicate test runs here!",
        "        #",
        "        # Pytest runs unittest-based tests natively - no need to run both.",
        "        # Running tests twice doubles CI time (from ~7min to ~15min+).",
        "        #",
        "        # If you need to add tests, add them to the appropriate stage above",
        "        # (unit-tests, integration-tests, etc.) NOT here.",
        "        #",
        "        # This job combines coverage from artifacts, then runs a single",
        "        # comprehensive pass for the final coverage number.",
        "",
        "        # Single test run - pytest handles both pytest AND unittest style tests"
      ],
      "lines_removed": [
        "        # Run pytest-based tests (tests/unit/, tests/smoke/, etc.)",
        "        # Append unittest-based legacy tests",
        "        coverage run --append --source=cortical -m unittest discover -s tests -p \"test_*.py\" -v",
        ""
      ],
      "context_before": [
        "",
        "    - name: Download integration coverage",
        "      uses: actions/download-artifact@v4",
        "      with:",
        "        name: coverage-integration",
        "        path: .",
        "",
        "    - name: Generate combined coverage report",
        "      run: |",
        "        echo \"=== Combined Coverage Report ===\""
      ],
      "context_after": [
        "        coverage run --source=cortical -m pytest tests/ -v --ignore=tests/performance/",
        "",
        "        coverage report -m --include=\"cortical/*\"",
        "        coverage xml -o coverage.xml",
        "",
        "        # Check threshold",
        "        coverage report --fail-under=89 --include=\"cortical/*\"",
        "",
        "    - name: Upload final coverage report",
        "      uses: actions/upload-artifact@v4",
        "      with:",
        "        name: coverage-report"
      ],
      "change_type": "modify"
    },
    {
      "file": "CLAUDE.md",
      "function": "processor.add_document_incremental(doc_id, text)",
      "start_line": 332,
      "lines_added": [
        "### Changing Validation Logic (IMPORTANT!)",
        "",
        "When modifying validation rules (e.g., parameter ranges, input constraints), **tests are scattered across multiple files**. Missing any will cause CI failures.",
        "",
        "**Before changing validation:**",
        "```bash",
        "# Find ALL tests related to the parameter/function you're changing",
        "# Example: changing alpha parameter validation",
        "grep -rn \"alpha\" tests/ | grep -i \"invalid\\|error\\|raise\\|ValueError\"",
        "",
        "# More specific patterns:",
        "grep -rn \"alpha.*0\\|alpha.*1\\|invalid.*alpha\" tests/",
        "```",
        "",
        "**Checklist for validation changes:**",
        "1. ‚úÖ Search for the parameter name + \"invalid\", \"error\", \"raise\", \"ValueError\" in tests/",
        "2. ‚úÖ Check both `tests/unit/` AND legacy `tests/test_*.py` files",
        "3. ‚úÖ Check `tests/test_coverage_gaps.py` (often has validation edge cases)",
        "4. ‚úÖ Update ALL matching tests, not just the first one found",
        "5. ‚úÖ Run full test suite locally before pushing: `python -m pytest tests/ -v`",
        "",
        "**Example: Changing alpha from (0, 1] to [0, 1]**",
        "```bash",
        "# This finds tests expecting alpha=0 to be invalid:",
        "grep -rn \"alpha.*0\\.0\\|alpha.*=.*0[^.]\\|exclusive of 0\" tests/",
        "```",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "# WRONG - may be using stale data",
        "if processor.is_stale(processor.COMP_PAGERANK):",
        "    # PageRank values may be outdated!",
        "    pass",
        "",
        "# CORRECT - ensure freshness",
        "if processor.is_stale(processor.COMP_PAGERANK):",
        "    processor.compute_importance()",
        "```",
        ""
      ],
      "context_after": [
        "### Staleness Tracking System",
        "",
        "The processor tracks which computations are up-to-date vs needing recalculation. This prevents unnecessary recomputation while ensuring data consistency.",
        "",
        "#### Computation Types",
        "",
        "| Constant | What it tracks | Computed by |",
        "|----------|---------------|-------------|",
        "| `COMP_TFIDF` | TF-IDF scores per term | `compute_tfidf()` |",
        "| `COMP_PAGERANK` | PageRank importance | `compute_importance()` |"
      ],
      "change_type": "add"
    },
    {
      "file": "CLAUDE.md",
      "function": "class TestYourFeature(unittest.TestCase):",
      "start_line": 577,
      "lines_added": [
        "### CI/CD Best Practices",
        "",
        "**CRITICAL: Pytest runs unittest-based tests natively!**",
        "",
        "Never run both pytest and unittest on the same test files - this doubles CI time:",
        "",
        "```bash",
        "# ‚ùå WRONG - runs tests twice (doubles CI time from ~7min to ~15min+)",
        "coverage run -m pytest tests/",
        "coverage run --append -m unittest discover -s tests",
        "",
        "# ‚úÖ CORRECT - pytest handles both pytest AND unittest style tests",
        "coverage run -m pytest tests/",
        "```",
        "",
        "**Why this matters:**",
        "- All `test_*.py` files using `unittest.TestCase` are discovered and run by pytest",
        "- Running unittest separately re-runs the exact same tests",
        "- With 3000+ tests and coverage overhead, this can add 10+ minutes to CI",
        "",
        "**When modifying `.github/workflows/ci.yml`:**",
        "1. Read the header comment explaining the test architecture",
        "2. Add new tests to the appropriate stage (smoke, unit, integration, etc.)",
        "3. Never add duplicate test runners in the coverage-report job",
        "4. When in doubt, run locally first: `time python -m pytest tests/ -v`",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "| `fresh_processor` | function | Empty processor for isolated tests |",
        "| `small_corpus_docs` | function | Raw document dict |",
        "",
        "**Always test:**",
        "- Empty corpus case",
        "- Single document case",
        "- Multiple documents case",
        "- Edge cases specific to your feature",
        "- Add regression test if fixing a bug",
        ""
      ],
      "context_after": [
        "---",
        "",
        "## Common Tasks",
        "",
        "### Adding a New Analysis Function",
        "",
        "1. Add function to `analysis.py` with proper signature:",
        "   ```python",
        "   def compute_your_analysis(",
        "       layers: Dict[CorticalLayer, HierarchicalLayer],"
      ],
      "change_type": "add"
    },
    {
      "file": "TASK_LIST.md",
      "function": null,
      "start_line": 1,
      "lines_added": [
        "**Pending Tasks:** 40",
        "**Completed Tasks:** 189 (see archive)",
        "| 192 | Deduplicate lateral_connections and typed_connections storage | Memory | - | Medium |"
      ],
      "lines_removed": [
        "**Pending Tasks:** 44",
        "**Completed Tasks:** 184 (see archive)",
        "| 148 | Investigate test_search_is_fast taking 137s | Testing | - | Medium |",
        "| 149 | Fix test_compute_all_under_threshold failing (135s > 30s) | Testing | - | Medium |"
      ],
      "context_before": [
        "# Task List: Cortical Text Processor",
        "",
        "Active backlog for the Cortical Text Processor project. Completed tasks are archived in [TASK_ARCHIVE.md](TASK_ARCHIVE.md).",
        "",
        "**Last Updated:** 2025-12-13"
      ],
      "context_after": [
        "",
        "**Unit Test Initiative:** ‚úÖ COMPLETE - 85% coverage from unit tests (1,729 tests)",
        "- 19 modules at 90%+ coverage",
        "- See [Coverage Baseline](#unit-test-coverage-baseline) for per-module status",
        "",
        "---",
        "",
        "## Active Backlog",
        "",
        "<!-- Machine-parseable format for automation -->",
        "",
        "### üü† High (Do This Week)",
        "",
        "| # | Task | Category | Depends | Effort |",
        "|---|------|----------|---------|--------|",
        "| 184 | Implement MCP Server for Claude Desktop integration | Integration | - | Large |",
        "",
        "### üü° Medium (Do This Month)",
        "",
        "| # | Task | Category | Depends | Effort |",
        "|---|------|----------|---------|--------|",
        "| 186 | Add simplified facade methods (quick_search, rag_retrieve) | API | - | Small |",
        "| 133 | Implement WAL + snapshot persistence (fault-tolerant rebuild) | Arch | 132 | Large |",
        "| 134 | Implement protobuf serialization for corpus | Arch | 132 | Medium |",
        "| 135 | Implement chunked parallel processing for full-analysis | Arch | 132 | Large |",
        "| 95 | Split processor.py into modules | Arch | 97 | Large |"
      ],
      "change_type": "modify"
    },
    {
      "file": "TASK_LIST.md",
      "function": "Active backlog for the Cortical Text Processor project. Completed tasks are arch",
      "start_line": 50,
      "lines_added": [
        "| 196 | Add runtime warning for spectral embeddings on large graphs | DevEx | - | Small |"
      ],
      "lines_removed": [],
      "context_before": [
        "| 100 | Implement plugin/extension registry | Arch | - | Large |",
        "| 101 | Automate staleness tracking | Arch | - | Medium |",
        "| 106 | Add task dependency graph | TaskMgmt | - | Small |",
        "| 108 | Create task selection script | TaskMgmt | - | Medium |",
        "| 117 | Create debugging cookbook | AINav | - | Medium |",
        "| 118 | Add function complexity annotations | AINav | - | Small |",
        "| 140 | Analyze customer service cluster quality | Research | 127 | Small |",
        "| 129 | Test customer service retrieval quality | Testing | - | Small |",
        "| 130 | Expand customer service sample cluster | Samples | - | Medium |",
        "| 131 | Investigate cross-domain semantic bridges | Research | - | Medium |"
      ],
      "context_after": [
        "",
        "### ‚è∏Ô∏è Deferred",
        "",
        "| # | Task | Reason |",
        "|---|------|--------|",
        "| 110 | Add section markers to large files | Superseded by #119 (AI metadata generator) |",
        "| 111 | Add \"See Also\" cross-references | Superseded by #119 (AI metadata generator) |",
        "| 112 | Add docstring examples | Superseded by #119 (AI metadata generator) |",
        "| 7 | Document magic numbers in gaps.py | Low priority, functional as-is |",
        "| 42 | Add simple query language | Nice-to-have, not blocking |"
      ],
      "change_type": "add"
    },
    {
      "file": "TASK_LIST.md",
      "function": "Active backlog for the Cortical Text Processor project. Completed tasks are arch",
      "start_line": 84,
      "lines_added": [
        "- #193 Unify alpha validation - retrofit_embeddings() now accepts [0,1] consistently",
        "- #194 Layer validation - Added checks for invalid layer values (0-3) in persistence/layers",
        "- #195 Stopwords import - semantics.py now uses Tokenizer.DEFAULT_STOP_WORDS",
        "- #148 Performance test refactor - Moved to small synthetic corpus (25 docs)",
        "- #149 Performance test fix - Tests now use small_corpus.py fixtures"
      ],
      "lines_removed": [],
      "context_before": [
        "",
        "*No tasks currently in progress*",
        "",
        "---",
        "",
        "## Recently Completed",
        "",
        "All completed tasks are now archived in [TASK_ARCHIVE.md](TASK_ARCHIVE.md).",
        "",
        "**Latest completions (2025-12-13):**"
      ],
      "context_after": [
        "- #182 Fluent API - FluentProcessor with method chaining (44 tests)",
        "- #183 Progress Feedback - ConsoleProgressReporter, callbacks (30 tests)",
        "- #185 Result Dataclasses - DocumentMatch, PassageMatch, QueryResult (56 tests)",
        "- #179 Fix definition search - line boundary fix in `find_definition_in_text()`",
        "- #180 Fix doc-type boosting - filename pattern + empty metadata fallback",
        "- #181 Fix query ranking - hybrid boost strategy for exact name matches",
        "- Unit Test Coverage Initiative: 1,729 tests, 85% coverage, 19 modules at 90%+",
        "- Tasks #159-178 (unit tests for all modules)",
        "",
        "---"
      ],
      "change_type": "add"
    },
    {
      "file": "TASK_LIST.md",
      "function": "processor.rag_retrieve(query, top_n=3) # Pre-configured for RAG",
      "start_line": 142,
      "lines_added": [
        "### 192. Deduplicate lateral_connections and typed_connections storage",
        "",
        "**Meta:** `status:pending` `priority:high` `category:memory`",
        "**Files:** `cortical/minicolumn.py`",
        "",
        "**Problem:**",
        "Every typed connection is duplicated in `lateral_connections` for backward compatibility (`minicolumn.py:209-212`). For large graphs, this doubles memory for edge weights.",
        "",
        "**Options:**",
        "1. Deprecate `lateral_connections` in favor of `typed_connections`",
        "2. Make `lateral_connections` a property that derives from `typed_connections`",
        "3. Keep both but document the trade-off",
        "",
        "**Context from code review (2025-12-13):**",
        "- Found in comprehensive code review of core classes",
        "- Memory concern for large corpora with millions of edges",
        "",
        "---",
        "",
        "### 193. Unify alpha parameter validation in semantics.py",
        "",
        "**Meta:** `status:pending` `priority:medium` `category:codequal`",
        "**Files:** `cortical/semantics.py`",
        "",
        "**Problem:**",
        "`retrofit_connections()` allows `alpha=0` but `retrofit_embeddings()` excludes it:",
        "- Line 405-408: `if not (0 <= alpha <= 1):`",
        "- Line 507-508: `if not (0 < alpha <= 1):`",
        "",
        "**Fix:** Either document why the difference exists or unify the validation.",
        "",
        "---",
        "",
        "### 194. Add validation for invalid layer values in persistence.py load",
        "",
        "**Meta:** `status:pending` `priority:medium` `category:codequal`",
        "**Files:** `cortical/persistence.py`",
        "",
        "**Problem:**",
        "In `load_processor()` (line 97-99), if `level_value` isn't a valid `CorticalLayer` enum value (0-3), an unclear `ValueError` is raised.",
        "",
        "**Fix:** Add explicit validation with helpful error message:",
        "```python",
        "try:",
        "    layer_enum = CorticalLayer(int(level_value))",
        "except ValueError:",
        "    raise ValueError(f\"Invalid layer level in saved state: {level_value}\")",
        "```",
        "",
        "---",
        "",
        "### 195. Import stopwords from tokenizer.py in semantics.py",
        "",
        "**Meta:** `status:pending` `priority:low` `category:codequal`",
        "**Files:** `cortical/semantics.py`, `cortical/tokenizer.py`",
        "",
        "**Problem:**",
        "`semantics.py` (lines 144-151) has its own hardcoded stopword set that duplicates `Tokenizer.DEFAULT_STOP_WORDS`. Changes to one won't affect the other.",
        "",
        "**Fix:** Import from `Tokenizer.DEFAULT_STOP_WORDS` or move to `constants.py`.",
        "",
        "---",
        "",
        "### 196. Add runtime warning for spectral embeddings on large graphs",
        "",
        "**Meta:** `status:pending` `priority:low` `category:devex`",
        "**Files:** `cortical/embeddings.py`",
        "",
        "**Problem:**",
        "Spectral embeddings are O(n¬≤) but there's no runtime warning when called with large graphs. Users may wait unexpectedly.",
        "",
        "**Fix:** Add warning for large graphs:",
        "```python",
        "if n > 5000:",
        "    import warnings",
        "    warnings.warn(f\"Spectral embeddings with {n} terms will be slow (O(n¬≤))\")",
        "```",
        "",
        "---",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "processor.explore(query)               # With expansion visibility",
        "```",
        "",
        "**Acceptance:**",
        "- [ ] 3-4 facade methods added",
        "- [ ] Sensible defaults for each use case",
        "- [ ] Examples in quickstart.md",
        "",
        "---",
        ""
      ],
      "context_after": [
        "## Unit Test Coverage Baseline",
        "",
        "‚úÖ **Unit test coverage as of 2025-12-13 (1,729 tests, 85% overall):**",
        "",
        "| Module | Coverage | Status | Task |",
        "|--------|----------|--------|------|",
        "| config.py | 100% | ‚úÖ | #168 |",
        "| minicolumn.py | 100% | ‚úÖ | #162 |",
        "| definitions.py | 100% | ‚úÖ | #173 |",
        "| tokenizer.py | 99% | ‚úÖ | #159 |"
      ],
      "change_type": "add"
    },
    {
      "file": "cortical/layers.py",
      "function": "class HierarchicalLayer:",
      "start_line": 279,
      "lines_added": [
        "",
        "",
        "",
        "        Raises:",
        "            ValueError: If layer value is invalid (must be 0-3)",
        "        # Validate layer value before creating enum",
        "        level_value = data['level']",
        "        if level_value not in [0, 1, 2, 3]:",
        "            raise ValueError(",
        "                f\"Invalid layer value {level_value} in layer data. \"",
        "                f\"Layer values must be 0-3 (TOKENS=0, BIGRAMS=1, CONCEPTS=2, DOCUMENTS=3).\"",
        "            )",
        "        layer = cls(CorticalLayer(level_value))"
      ],
      "lines_removed": [
        "        ",
        "            ",
        "        layer = cls(CorticalLayer(data['level']))"
      ],
      "context_before": [
        "            'minicolumns': {",
        "                content: col.to_dict() ",
        "                for content, col in self.minicolumns.items()",
        "            }",
        "        }",
        "    ",
        "    @classmethod",
        "    def from_dict(cls, data: Dict) -> 'HierarchicalLayer':",
        "        \"\"\"",
        "        Create a layer from dictionary representation."
      ],
      "context_after": [
        "        Args:",
        "            data: Dictionary with layer data",
        "        Returns:",
        "            New HierarchicalLayer instance",
        "        \"\"\"",
        "        for content, col_data in data.get('minicolumns', {}).items():",
        "            col = Minicolumn.from_dict(col_data)",
        "            layer.minicolumns[content] = col",
        "            layer._id_index[col.id] = content  # Rebuild ID index",
        "        return layer",
        "    ",
        "    def __repr__(self) -> str:",
        "        return f\"HierarchicalLayer(level={self.level.name}, columns={len(self.minicolumns)})\""
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/persistence.py",
      "function": "def load_processor(",
      "start_line": 81,
      "lines_added": [
        "",
        "    Raises:",
        "        ValueError: If layer values are invalid (must be 0-3)",
        "        # Validate layer value before creating enum",
        "        level_int = int(level_value)",
        "        if level_int not in [0, 1, 2, 3]:",
        "            raise ValueError(",
        "                f\"Invalid layer value {level_int} in saved state. \"",
        "                f\"Layer values must be 0-3 (TOKENS=0, BIGRAMS=1, CONCEPTS=2, DOCUMENTS=3).\"",
        "            )",
        "        layers[CorticalLayer(level_int)] = layer"
      ],
      "lines_removed": [
        "        layers[CorticalLayer(int(level_value))] = layer"
      ],
      "context_before": [
        ") -> tuple:",
        "    \"\"\"",
        "    Load processor state from a file.",
        "",
        "    Args:",
        "        filepath: Path to saved file",
        "        verbose: Print progress",
        "",
        "    Returns:",
        "        Tuple of (layers, documents, document_metadata, embeddings, semantic_relations, metadata)"
      ],
      "context_after": [
        "    \"\"\"",
        "    with open(filepath, 'rb') as f:",
        "        state = pickle.load(f)",
        "",
        "    # Reconstruct layers",
        "    layers = {}",
        "    for level_value, layer_data in state.get('layers', {}).items():",
        "        layer = HierarchicalLayer.from_dict(layer_data)",
        "",
        "    documents = state.get('documents', {})",
        "    document_metadata = state.get('document_metadata', {})",
        "    embeddings = state.get('embeddings', {})",
        "    semantic_relations = state.get('semantic_relations', [])",
        "    metadata = state.get('metadata', {})",
        "",
        "    if verbose:",
        "        total_cols = sum(len(layer.minicolumns) for layer in layers.values())",
        "        total_conns = sum(layer.total_connections() for layer in layers.values())"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/semantics.py",
      "function": "from collections import defaultdict",
      "start_line": 17,
      "lines_added": [
        "from .tokenizer import Tokenizer"
      ],
      "lines_removed": [],
      "context_before": [
        "",
        "try:",
        "    import numpy as np",
        "    HAS_NUMPY = True",
        "except ImportError:",
        "    HAS_NUMPY = False",
        "",
        "from .layers import CorticalLayer, HierarchicalLayer",
        "from .minicolumn import Minicolumn",
        "from .constants import RELATION_WEIGHTS"
      ],
      "context_after": [
        "",
        "",
        "# Commonsense relation patterns with confidence scores",
        "# Format: (pattern_regex, relation_type, confidence, swap_order)",
        "# swap_order: if True, the captured groups are in reverse order (t2, t1)",
        "RELATION_PATTERNS = [",
        "    # IsA patterns (hypernym/type relations)",
        "    (r'(\\w+)\\s+(?:is|are)\\s+(?:a|an)\\s+(?:type\\s+of\\s+)?(\\w+)', 'IsA', 0.9, False),",
        "    (r'(\\w+),?\\s+(?:a|an)\\s+(?:kind|type|form)\\s+of\\s+(\\w+)', 'IsA', 0.95, False),",
        "    (r'(\\w+)\\s+(?:is|are)\\s+considered\\s+(?:a|an)?\\s*(\\w+)', 'IsA', 0.8, False),"
      ],
      "change_type": "add"
    },
    {
      "file": "cortical/semantics.py",
      "function": "def extract_pattern_relations(",
      "start_line": 134,
      "lines_added": [
        "                    if t1 in Tokenizer.DEFAULT_STOP_WORDS or t2 in Tokenizer.DEFAULT_STOP_WORDS:"
      ],
      "lines_removed": [
        "                    stopwords = {'the', 'a', 'an', 'is', 'are', 'was', 'were', 'be',",
        "                                 'been', 'being', 'have', 'has', 'had', 'do', 'does',",
        "                                 'did', 'will', 'would', 'could', 'should', 'may',",
        "                                 'might', 'must', 'shall', 'can', 'this', 'that',",
        "                                 'these', 'those', 'it', 'its', 'they', 'them',",
        "                                 'their', 'we', 'us', 'our', 'you', 'your', 'i', 'me', 'my'}",
        "                    if t1 in stopwords or t2 in stopwords:"
      ],
      "context_before": [
        "",
        "                    # Skip if terms are the same",
        "                    if t1 == t2:",
        "                        continue",
        "",
        "                    # Skip if terms don't exist in corpus",
        "                    if t1 not in valid_terms or t2 not in valid_terms:",
        "                        continue",
        "",
        "                    # Skip common stopwords that might slip through patterns"
      ],
      "context_after": [
        "                        continue",
        "",
        "                    # Create relation key to avoid duplicates",
        "                    rel_key = (t1, relation_type, t2)",
        "",
        "                    # For symmetric relations, also check reverse",
        "                    if relation_type in {'SimilarTo', 'Antonym', 'RelatedTo'}:",
        "                        rev_key = (t2, relation_type, t1)",
        "                        if rev_key in seen_relations:",
        "                            continue"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/semantics.py",
      "function": "def retrofit_embeddings(",
      "start_line": 495,
      "lines_added": [
        "        ValueError: If alpha is not in range [0, 1]",
        "    if not (0 <= alpha <= 1):",
        "        raise ValueError(f\"alpha must be between 0 and 1, got {alpha}\")"
      ],
      "lines_removed": [
        "        ValueError: If alpha is not in range (0, 1]",
        "    if not (0 < alpha <= 1):",
        "        raise ValueError(f\"alpha must be between 0 and 1 (exclusive of 0), got {alpha}\")"
      ],
      "context_before": [
        "    Args:",
        "        embeddings: Dictionary mapping terms to embedding vectors",
        "        semantic_relations: List of (term1, relation, term2, weight) tuples",
        "        iterations: Number of iterations",
        "        alpha: Blend factor (higher = more original embedding)",
        "",
        "    Returns:",
        "        Dictionary with retrofitting statistics",
        "",
        "    Raises:"
      ],
      "context_after": [
        "    \"\"\"",
        "",
        "    # Store original embeddings",
        "    original = copy.deepcopy(embeddings)",
        "    ",
        "    # Build neighbor lookup",
        "    neighbors: Dict[str, List[Tuple[str, float]]] = defaultdict(list)",
        "    ",
        "    for t1, relation, t2, weight in semantic_relations:",
        "        if t1 in embeddings and t2 in embeddings:",
        "            relation_weight = RELATION_WEIGHTS.get(relation, 0.5)"
      ],
      "change_type": "modify"
    },
    {
      "file": "scripts/index_codebase.py",
      "function": "from cortical.tokenizer import Tokenizer",
      "start_line": 60,
      "lines_added": [
        "    Enables:",
        "    - split_identifiers: getUserCredentials ‚Üí ['getusercredentials', 'get', 'user', 'credentials']",
        "    - filter_code_noise: Filters Python syntax tokens (self, def, return, etc.)",
        "    authenticate, user_auth, etc., without noise tokens polluting results.",
        "    tokenizer = Tokenizer(split_identifiers=True, filter_code_noise=True)"
      ],
      "lines_removed": [
        "    Enables split_identifiers so that:",
        "    - getUserCredentials ‚Üí ['getusercredentials', 'get', 'user', 'credentials']",
        "    - auth_service ‚Üí ['auth_service', 'auth', 'service']",
        "    authenticate, user_auth, etc.",
        "    tokenizer = Tokenizer(split_identifiers=True)"
      ],
      "context_before": [
        "from cortical.chunk_index import (",
        "    ChunkWriter, ChunkLoader, ChunkCompactor,",
        "    get_changes_from_manifest as get_chunk_changes",
        ")",
        "",
        "",
        "def create_code_processor() -> CorticalTextProcessor:",
        "    \"\"\"",
        "    Create a CorticalTextProcessor configured for code indexing.",
        ""
      ],
      "context_after": [
        "",
        "    This dramatically improves code search - \"auth\" will find AuthService,",
        "    \"\"\"",
        "    return CorticalTextProcessor(tokenizer=tokenizer)",
        "",
        "",
        "# Manifest file version for compatibility checking",
        "MANIFEST_VERSION = \"1.0\"",
        "",
        "# Default timeout in seconds (0 = no timeout)",
        "DEFAULT_TIMEOUT = 300  # 5 minutes",
        "",
        ""
      ],
      "change_type": "modify"
    },
    {
      "file": "tests/test_coverage_gaps.py",
      "function": "class TestSemanticsRetrofitCoverage(unittest.TestCase):",
      "start_line": 754,
      "lines_added": [
        "    def test_retrofit_embeddings_invalid_alpha_negative(self):",
        "        \"\"\"Test retrofit_embeddings raises ValueError when alpha < 0.\"\"\"",
        "        # alpha=0 is now valid (means 100% semantic, 0% original)",
        "        # Test negative alpha which is still invalid",
        "            retrofit_embeddings(embeddings, [], alpha=-0.1)",
        "        self.assertIn(\"between 0 and 1\", str(ctx.exception))"
      ],
      "lines_removed": [
        "    def test_retrofit_embeddings_invalid_alpha_zero(self):",
        "        \"\"\"Test retrofit_embeddings raises ValueError when alpha <= 0.\"\"\"",
        "            retrofit_embeddings(embeddings, [], alpha=0.0)",
        "        self.assertIn(\"exclusive of 0\", str(ctx.exception))"
      ],
      "context_before": [
        "        from cortical.semantics import retrofit_connections",
        "",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(\"doc1\", \"test content here\")",
        "        processor.compute_all(verbose=False)",
        "",
        "        with self.assertRaises(ValueError) as ctx:",
        "            retrofit_connections(processor.layers, [], alpha=-0.1)",
        "        self.assertIn(\"between 0 and 1\", str(ctx.exception))",
        ""
      ],
      "context_after": [
        "        from cortical.semantics import retrofit_embeddings",
        "",
        "        embeddings = {\"word1\": [0.1, 0.2], \"word2\": [0.3, 0.4]}",
        "",
        "        with self.assertRaises(ValueError) as ctx:",
        "",
        "    def test_retrofit_embeddings_invalid_alpha_too_high(self):",
        "        \"\"\"Test retrofit_embeddings raises ValueError when alpha > 1.\"\"\"",
        "        from cortical.semantics import retrofit_embeddings",
        "",
        "        embeddings = {\"word1\": [0.1, 0.2], \"word2\": [0.3, 0.4]}",
        "",
        "        with self.assertRaises(ValueError) as ctx:",
        "            retrofit_embeddings(embeddings, [], alpha=1.5)",
        "        self.assertIn(\"between 0 and 1\", str(ctx.exception))"
      ],
      "change_type": "modify"
    },
    {
      "file": "tests/test_layers.py",
      "function": "class TestHierarchicalLayer(unittest.TestCase):",
      "start_line": 196,
      "lines_added": [
        "    def test_from_dict_validates_layer_value(self):",
        "        \"\"\"Test that from_dict validates layer values.\"\"\"",
        "        # Valid layer values (0-3) should work",
        "        for valid_level in [0, 1, 2, 3]:",
        "            data = {'level': valid_level, 'minicolumns': {}}",
        "            layer = HierarchicalLayer.from_dict(data)",
        "            self.assertEqual(layer.level, CorticalLayer(valid_level))",
        "",
        "    def test_from_dict_rejects_invalid_positive_layer_value(self):",
        "        \"\"\"Test that from_dict rejects layer values > 3.\"\"\"",
        "        data = {'level': 5, 'minicolumns': {}}",
        "        with self.assertRaises(ValueError) as context:",
        "            HierarchicalLayer.from_dict(data)",
        "",
        "        # Check error message is informative",
        "        self.assertIn(\"Invalid layer value 5\", str(context.exception))",
        "        self.assertIn(\"must be 0-3\", str(context.exception))",
        "",
        "    def test_from_dict_rejects_negative_layer_value(self):",
        "        \"\"\"Test that from_dict rejects negative layer values.\"\"\"",
        "        data = {'level': -1, 'minicolumns': {}}",
        "        with self.assertRaises(ValueError) as context:",
        "            HierarchicalLayer.from_dict(data)",
        "",
        "        # Check error message is informative",
        "        self.assertIn(\"Invalid layer value -1\", str(context.exception))",
        "        self.assertIn(\"must be 0-3\", str(context.exception))",
        "",
        "    def test_from_dict_rejects_large_invalid_layer_value(self):",
        "        \"\"\"Test that from_dict rejects large invalid layer values.\"\"\"",
        "        data = {'level': 999, 'minicolumns': {}}",
        "        with self.assertRaises(ValueError) as context:",
        "            HierarchicalLayer.from_dict(data)",
        "",
        "        # Check error message is informative",
        "        self.assertIn(\"Invalid layer value 999\", str(context.exception))",
        "        self.assertIn(\"must be 0-3\", str(context.exception))",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "        col2.activation = 10.0",
        "        col3.activation = 5.0",
        "",
        "        top = layer.top_by_activation(n=2)",
        "        self.assertEqual(len(top), 2)",
        "        self.assertEqual(top[0][0], \"b\")  # Highest activation",
        "        self.assertEqual(top[0][1], 10.0)",
        "        self.assertEqual(top[1][0], \"c\")  # Second highest",
        "        self.assertEqual(top[1][1], 5.0)",
        ""
      ],
      "context_after": [
        "",
        "class TestCorticalLayerEnum(unittest.TestCase):",
        "    \"\"\"Test the CorticalLayer enum.\"\"\"",
        "",
        "    def test_values(self):",
        "        \"\"\"Test layer values.\"\"\"",
        "        self.assertEqual(CorticalLayer.TOKENS.value, 0)",
        "        self.assertEqual(CorticalLayer.BIGRAMS.value, 1)",
        "        self.assertEqual(CorticalLayer.CONCEPTS.value, 2)",
        "        self.assertEqual(CorticalLayer.DOCUMENTS.value, 3)"
      ],
      "change_type": "add"
    },
    {
      "file": "tests/test_persistence.py",
      "function": null,
      "start_line": 1,
      "lines_added": [
        "import pickle"
      ],
      "lines_removed": [],
      "context_before": [
        "\"\"\"Tests for the persistence module.\"\"\"",
        "",
        "import unittest",
        "import tempfile",
        "import os",
        "import json"
      ],
      "context_after": [
        "import sys",
        "sys.path.insert(0, '..')",
        "",
        "from cortical import CorticalTextProcessor, CorticalLayer",
        "from cortical.persistence import (",
        "    save_processor,",
        "    load_processor,",
        "    export_graph_json,",
        "    export_embeddings_json,",
        "    load_embeddings_json,"
      ],
      "change_type": "add"
    },
    {
      "file": "tests/test_persistence.py",
      "function": "class TestSaveLoad(unittest.TestCase):",
      "start_line": 248,
      "lines_added": [
        "    def test_load_invalid_layer_value(self):",
        "        \"\"\"Test that loading with invalid layer value raises ValueError.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(\"doc1\", \"Test content\")",
        "        processor.compute_all(verbose=False)",
        "",
        "        with tempfile.TemporaryDirectory() as tmpdir:",
        "            filepath = os.path.join(tmpdir, \"test.pkl\")",
        "            save_processor(",
        "                filepath, processor.layers, processor.documents,",
        "                processor.document_metadata, processor.embeddings,",
        "                processor.semantic_relations, verbose=False",
        "            )",
        "",
        "            # Corrupt the file by adding invalid layer value",
        "            with open(filepath, 'rb') as f:",
        "                state = pickle.load(f)",
        "",
        "            # Add an invalid layer value (5 is not valid, only 0-3 are valid)",
        "            state['layers'][5] = state['layers'][0].copy()",
        "",
        "            corrupted_filepath = os.path.join(tmpdir, \"corrupted.pkl\")",
        "            with open(corrupted_filepath, 'wb') as f:",
        "                pickle.dump(state, f)",
        "",
        "            # Try to load the corrupted file",
        "            with self.assertRaises(ValueError) as context:",
        "                load_processor(corrupted_filepath, verbose=False)",
        "",
        "            # Check error message is informative",
        "            self.assertIn(\"Invalid layer value 5\", str(context.exception))",
        "            self.assertIn(\"must be 0-3\", str(context.exception))",
        "",
        "    def test_load_negative_layer_value(self):",
        "        \"\"\"Test that loading with negative layer value raises ValueError.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(\"doc1\", \"Test content\")",
        "        processor.compute_all(verbose=False)",
        "",
        "        with tempfile.TemporaryDirectory() as tmpdir:",
        "            filepath = os.path.join(tmpdir, \"test.pkl\")",
        "            save_processor(",
        "                filepath, processor.layers, processor.documents,",
        "                processor.document_metadata, processor.embeddings,",
        "                processor.semantic_relations, verbose=False",
        "            )",
        "",
        "            # Corrupt the file by adding negative layer value",
        "            with open(filepath, 'rb') as f:",
        "                state = pickle.load(f)",
        "",
        "            # Add an invalid negative layer value",
        "            state['layers'][-1] = state['layers'][0].copy()",
        "",
        "            corrupted_filepath = os.path.join(tmpdir, \"corrupted.pkl\")",
        "            with open(corrupted_filepath, 'wb') as f:",
        "                pickle.dump(state, f)",
        "",
        "            # Try to load the corrupted file",
        "            with self.assertRaises(ValueError) as context:",
        "                load_processor(corrupted_filepath, verbose=False)",
        "",
        "            # Check error message is informative",
        "            self.assertIn(\"Invalid layer value -1\", str(context.exception))",
        "            self.assertIn(\"must be 0-3\", str(context.exception))",
        "",
        "    def test_load_valid_layer_values(self):",
        "        \"\"\"Test that loading with all valid layer values (0-3) works.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(\"doc1\", \"Test content for validation\")",
        "        processor.compute_all(verbose=False)",
        "",
        "        with tempfile.TemporaryDirectory() as tmpdir:",
        "            filepath = os.path.join(tmpdir, \"test.pkl\")",
        "            processor.save(filepath, verbose=False)",
        "",
        "            # Should load successfully without raising ValueError",
        "            loaded = CorticalTextProcessor.load(filepath, verbose=False)",
        "",
        "            # Verify all 4 layers are present and valid",
        "            for layer_enum in [CorticalLayer.TOKENS, CorticalLayer.BIGRAMS,",
        "                             CorticalLayer.CONCEPTS, CorticalLayer.DOCUMENTS]:",
        "                self.assertIn(layer_enum, loaded.layers)",
        "                self.assertEqual(loaded.layers[layer_enum].level, layer_enum)",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "            finally:",
        "                logger.removeHandler(handler)",
        "                logger.setLevel(logging.WARNING)",
        "",
        "            output = log_buffer.getvalue()",
        "            # Check verbose output mentions embeddings and relations",
        "            self.assertIn(\"Loaded processor\", output)",
        "            self.assertIn(\"embeddings\", output)",
        "            self.assertIn(\"semantic relations\", output)",
        ""
      ],
      "context_after": [
        "",
        "class TestExportGraphJSON(unittest.TestCase):",
        "    \"\"\"Test graph JSON export.\"\"\"",
        "",
        "    def test_export_graph_json(self):",
        "        \"\"\"Test exporting graph to JSON.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "        processor.process_document(\"doc1\", \"neural networks learning\")",
        "        processor.process_document(\"doc2\", \"machine learning algorithms\")",
        "        processor.compute_all(verbose=False)"
      ],
      "change_type": "add"
    },
    {
      "file": "tests/unit/test_semantics.py",
      "function": "class TestRetrofitEmbeddings:",
      "start_line": 734,
      "lines_added": [
        "        # alpha must be in [0, 1] - test values outside this range",
        "            retrofit_embeddings(embeddings, [], alpha=-0.1)"
      ],
      "lines_removed": [
        "            retrofit_embeddings(embeddings, [], alpha=0.0)"
      ],
      "context_before": [
        "",
        "    def test_empty_embeddings(self):",
        "        \"\"\"Empty embeddings produce no changes.\"\"\"",
        "        result = retrofit_embeddings({}, [])",
        "        assert result[\"terms_retrofitted\"] == 0",
        "        assert result[\"total_movement\"] == 0.0",
        "",
        "    def test_invalid_alpha(self):",
        "        \"\"\"Invalid alpha raises ValueError.\"\"\"",
        "        embeddings = {\"test\": [1.0, 2.0]}"
      ],
      "context_after": [
        "        with pytest.raises(ValueError, match=\"alpha must be between 0 and 1\"):",
        "",
        "        with pytest.raises(ValueError, match=\"alpha must be between 0 and 1\"):",
        "            retrofit_embeddings(embeddings, [], alpha=1.5)",
        "",
        "    def test_retrofitting_moves_embeddings(self):",
        "        \"\"\"Retrofitting moves related terms closer.\"\"\"",
        "        embeddings = {",
        "            \"dog\": [1.0, 0.0],",
        "            \"cat\": [0.0, 1.0],",
        "            \"animal\": [0.5, 0.5]"
      ],
      "change_type": "modify"
    }
  ],
  "hour_of_day": 11,
  "day_of_week": "Saturday",
  "seconds_since_last_commit": -180359,
  "is_merge": true,
  "is_initial": false,
  "parent_count": 2,
  "session_id": null,
  "related_chats": [],
  "ci_result": null,
  "reverted": false,
  "amended": false
}