{
  "hash": "87ea49a34fc3f6803e4143fea8d3b150568cbd6b",
  "message": "Add profiling script and tasks for full-analysis investigation",
  "author": "Claude",
  "timestamp": "2025-12-11 21:57:31 +0000",
  "branch": "claude/multi-index-design-DvifZ",
  "files_changed": [
    "TASK_LIST.md",
    "scripts/profile_full_analysis.py"
  ],
  "insertions": 350,
  "deletions": 1,
  "hunks": [
    {
      "file": "TASK_LIST.md",
      "function": null,
      "start_line": 1,
      "lines_added": [
        "**Pending Tasks:** 37",
        "| 132 | Profile full-analysis bottleneck (Louvain, semantics extraction) | Perf | - | Medium |",
        "| 133 | Implement WAL + snapshot persistence (fault-tolerant rebuild) | Arch | 132 | Large |",
        "| 134 | Implement protobuf serialization for corpus | Arch | 132 | Medium |",
        "| 135 | Implement chunked parallel processing for full-analysis | Arch | 132 | Large |"
      ],
      "lines_removed": [
        "**Pending Tasks:** 33"
      ],
      "context_before": [
        "# Task List: Cortical Text Processor",
        "",
        "Active backlog for the Cortical Text Processor project. Completed tasks are archived in [TASK_ARCHIVE.md](TASK_ARCHIVE.md).",
        "",
        "**Last Updated:** 2025-12-11"
      ],
      "context_after": [
        "**Completed Tasks:** 88+ (see archive)",
        "",
        "---",
        "",
        "## Active Backlog",
        "",
        "<!-- Machine-parseable format for automation -->",
        "",
        "### üî¥ Critical (Do Now)",
        "",
        "| # | Task | Category | Depends | Effort |",
        "|---|------|----------|---------|--------|",
        "| 128 | Definition boost favors test mocks over real implementations | Search | - | Medium |",
        "| 124 | Add minimum cluster count regression tests | Testing | - | Medium |",
        "| 125 | Add clustering quality metrics (modularity, silhouette) | DevEx | - | Medium |",
        "",
        "### üü† High (Do This Week)",
        "",
        "| # | Task | Category | Depends | Effort |",
        "|---|------|----------|---------|--------|",
        "| 94 | Split query.py into focused modules | Arch | - | Large |",
        "| 97 | Integrate CorticalConfig into processor | Arch | - | Medium |",
        "| 127 | Create cluster coverage evaluation script | DevEx | 125 | Medium |",
        "",
        "### üü° Medium (Do This Month)",
        "",
        "| # | Task | Category | Depends | Effort |",
        "|---|------|----------|---------|--------|",
        "| 91 | Create docs/README.md index | Docs | - | Small |",
        "| 92 | Add badges to README.md | DevEx | - | Small |",
        "| 93 | Update README with docs references | Docs | 91 | Small |",
        "| 95 | Split processor.py into modules | Arch | 97 | Large |",
        "| 96 | Centralize duplicate constants | CodeQual | - | Small |",
        "| 98 | Replace print() with logging | CodeQual | - | Medium |",
        "| 99 | Add input validation to public methods | CodeQual | - | Medium |",
        "| 102 | Add tests for edge cases | Testing | - | Medium |",
        "| 107 | Add Quick Context to tasks | TaskMgmt | - | Medium |",
        "| 113 | Document staleness tracking system | AINav | - | Small |"
      ],
      "change_type": "modify"
    },
    {
      "file": "scripts/profile_full_analysis.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "#!/usr/bin/env python3",
        "\"\"\"",
        "Profile full-analysis to identify bottlenecks.",
        "",
        "This script profiles each phase of compute_all() separately to identify",
        "where time is being spent and why full-analysis hangs.",
        "",
        "Usage:",
        "    python scripts/profile_full_analysis.py",
        "    python scripts/profile_full_analysis.py --phase louvain",
        "    python scripts/profile_full_analysis.py --phase semantics",
        "    python scripts/profile_full_analysis.py --timeout 30",
        "\"\"\"",
        "",
        "import argparse",
        "import cProfile",
        "import pstats",
        "import io",
        "import time",
        "import signal",
        "import sys",
        "from pathlib import Path",
        "from contextlib import contextmanager",
        "from typing import Optional, Callable, Any",
        "",
        "# Add parent directory to path",
        "sys.path.insert(0, str(Path(__file__).parent.parent))",
        "",
        "from cortical.processor import CorticalTextProcessor",
        "from cortical.analysis import cluster_by_louvain, cluster_by_label_propagation",
        "from cortical.layers import CorticalLayer",
        "",
        "",
        "class TimeoutError(Exception):",
        "    \"\"\"Raised when operation times out.\"\"\"",
        "    pass",
        "",
        "",
        "@contextmanager",
        "def timeout(seconds: int, operation: str = \"operation\"):",
        "    \"\"\"Context manager for timing out long operations.\"\"\"",
        "    def handler(signum, frame):",
        "        raise TimeoutError(f\"{operation} timed out after {seconds}s\")",
        "",
        "    old_handler = signal.signal(signal.SIGALRM, handler)",
        "    signal.alarm(seconds)",
        "    try:",
        "        yield",
        "    finally:",
        "        signal.alarm(0)",
        "        signal.signal(signal.SIGALRM, old_handler)",
        "",
        "",
        "def profile_function(func: Callable, *args, timeout_sec: int = 60, **kwargs) -> tuple:",
        "    \"\"\"",
        "    Profile a function and return stats.",
        "",
        "    Returns:",
        "        (result, elapsed_time, profile_stats, timed_out)",
        "    \"\"\"",
        "    profiler = cProfile.Profile()",
        "    start = time.time()",
        "    result = None",
        "    timed_out = False",
        "",
        "    try:",
        "        with timeout(timeout_sec, func.__name__):",
        "            profiler.enable()",
        "            result = func(*args, **kwargs)",
        "            profiler.disable()",
        "    except TimeoutError as e:",
        "        profiler.disable()",
        "        timed_out = True",
        "        print(f\"\\n‚ö†Ô∏è  {e}\")",
        "",
        "    elapsed = time.time() - start",
        "",
        "    # Get stats",
        "    stream = io.StringIO()",
        "    stats = pstats.Stats(profiler, stream=stream)",
        "    stats.sort_stats('cumulative')",
        "    stats.print_stats(20)",
        "",
        "    return result, elapsed, stream.getvalue(), timed_out",
        "",
        "",
        "def load_corpus(corpus_path: str) -> CorticalTextProcessor:",
        "    \"\"\"Load the corpus for profiling.\"\"\"",
        "    print(f\"Loading corpus from {corpus_path}...\")",
        "    processor = CorticalTextProcessor.load(corpus_path)",
        "",
        "    # Print corpus stats",
        "    layer0 = processor.layers.get(CorticalLayer.TOKENS)",
        "    layer1 = processor.layers.get(CorticalLayer.BIGRAMS)",
        "",
        "    print(f\"  Documents: {len(processor.documents)}\")",
        "    print(f\"  Tokens (Layer 0): {layer0.column_count() if layer0 else 0}\")",
        "    print(f\"  Bigrams (Layer 1): {layer1.column_count() if layer1 else 0}\")",
        "    print()",
        "",
        "    return processor",
        "",
        "",
        "def profile_louvain(processor: CorticalTextProcessor, timeout_sec: int = 60) -> dict:",
        "    \"\"\"Profile Louvain clustering.\"\"\"",
        "    print(\"=\" * 60)",
        "    print(\"PROFILING: Louvain Clustering\")",
        "    print(\"=\" * 60)",
        "",
        "    layer0 = processor.layers.get(CorticalLayer.TOKENS)",
        "    if not layer0:",
        "        print(\"No token layer found!\")",
        "        return {}",
        "",
        "    print(f\"Layer 0 has {layer0.column_count()} minicolumns\")",
        "    print(f\"Total connections: {layer0.total_connections()}\")",
        "    print()",
        "",
        "    result, elapsed, stats, timed_out = profile_function(",
        "        cluster_by_louvain,",
        "        layer0,",
        "        min_cluster_size=3,",
        "        resolution=1.0,",
        "        timeout_sec=timeout_sec",
        "    )",
        "",
        "    print(f\"\\nElapsed: {elapsed:.2f}s {'(TIMED OUT)' if timed_out else ''}\")",
        "    if result:",
        "        print(f\"Clusters found: {len(result)}\")",
        "    print(\"\\nTop 20 functions by cumulative time:\")",
        "    print(stats)",
        "",
        "    return {",
        "        'phase': 'louvain',",
        "        'elapsed': elapsed,",
        "        'timed_out': timed_out,",
        "        'clusters': len(result) if result else 0",
        "    }",
        "",
        "",
        "def profile_label_propagation(processor: CorticalTextProcessor, timeout_sec: int = 60) -> dict:",
        "    \"\"\"Profile label propagation clustering (legacy).\"\"\"",
        "    print(\"=\" * 60)",
        "    print(\"PROFILING: Label Propagation Clustering\")",
        "    print(\"=\" * 60)",
        "",
        "    layer0 = processor.layers.get(CorticalLayer.TOKENS)",
        "    if not layer0:",
        "        print(\"No token layer found!\")",
        "        return {}",
        "",
        "    print(f\"Layer 0 has {layer0.column_count()} minicolumns\")",
        "    print()",
        "",
        "    result, elapsed, stats, timed_out = profile_function(",
        "        cluster_by_label_propagation,",
        "        layer0,",
        "        min_cluster_size=3,",
        "        timeout_sec=timeout_sec",
        "    )",
        "",
        "    print(f\"\\nElapsed: {elapsed:.2f}s {'(TIMED OUT)' if timed_out else ''}\")",
        "    if result:",
        "        print(f\"Clusters found: {len(result)}\")",
        "    print(\"\\nTop 20 functions by cumulative time:\")",
        "    print(stats)",
        "",
        "    return {",
        "        'phase': 'label_propagation',",
        "        'elapsed': elapsed,",
        "        'timed_out': timed_out,",
        "        'clusters': len(result) if result else 0",
        "    }",
        "",
        "",
        "def profile_semantics(processor: CorticalTextProcessor, timeout_sec: int = 60) -> dict:",
        "    \"\"\"Profile semantic relation extraction.\"\"\"",
        "    print(\"=\" * 60)",
        "    print(\"PROFILING: Semantic Relation Extraction\")",
        "    print(\"=\" * 60)",
        "",
        "    print(f\"Documents: {len(processor.documents)}\")",
        "    total_chars = sum(len(doc) for doc in processor.documents.values())",
        "    print(f\"Total characters: {total_chars:,}\")",
        "    print()",
        "",
        "    result, elapsed, stats, timed_out = profile_function(",
        "        processor.extract_corpus_semantics,",
        "        use_pattern_extraction=True,",
        "        verbose=False,",
        "        timeout_sec=timeout_sec",
        "    )",
        "",
        "    print(f\"\\nElapsed: {elapsed:.2f}s {'(TIMED OUT)' if timed_out else ''}\")",
        "    print(f\"Relations extracted: {len(processor.semantic_relations)}\")",
        "    print(\"\\nTop 20 functions by cumulative time:\")",
        "    print(stats)",
        "",
        "    return {",
        "        'phase': 'semantics',",
        "        'elapsed': elapsed,",
        "        'timed_out': timed_out,",
        "        'relations': len(processor.semantic_relations)",
        "    }",
        "",
        "",
        "def profile_bigram_connections(processor: CorticalTextProcessor, timeout_sec: int = 60) -> dict:",
        "    \"\"\"Profile bigram connection computation.\"\"\"",
        "    print(\"=\" * 60)",
        "    print(\"PROFILING: Bigram Connections\")",
        "    print(\"=\" * 60)",
        "",
        "    layer1 = processor.layers.get(CorticalLayer.BIGRAMS)",
        "    if layer1:",
        "        print(f\"Bigrams: {layer1.column_count()}\")",
        "    print()",
        "",
        "    result, elapsed, stats, timed_out = profile_function(",
        "        processor.compute_bigram_connections,",
        "        verbose=False,",
        "        timeout_sec=timeout_sec",
        "    )",
        "",
        "    print(f\"\\nElapsed: {elapsed:.2f}s {'(TIMED OUT)' if timed_out else ''}\")",
        "    print(\"\\nTop 20 functions by cumulative time:\")",
        "    print(stats)",
        "",
        "    return {",
        "        'phase': 'bigram_connections',",
        "        'elapsed': elapsed,",
        "        'timed_out': timed_out",
        "    }",
        "",
        "",
        "def profile_all_phases(processor: CorticalTextProcessor, timeout_sec: int = 30) -> list:",
        "    \"\"\"Profile all phases of full-analysis.\"\"\"",
        "    results = []",
        "",
        "    # Fast phases first",
        "    print(\"\\n\" + \"=\" * 60)",
        "    print(\"PHASE 1: Activation Propagation\")",
        "    print(\"=\" * 60)",
        "    start = time.time()",
        "    processor.propagate_activation(verbose=False)",
        "    elapsed = time.time() - start",
        "    print(f\"Elapsed: {elapsed:.2f}s\")",
        "    results.append({'phase': 'activation', 'elapsed': elapsed, 'timed_out': False})",
        "",
        "    print(\"\\n\" + \"=\" * 60)",
        "    print(\"PHASE 2: PageRank\")",
        "    print(\"=\" * 60)",
        "    start = time.time()",
        "    processor.compute_importance(verbose=False)",
        "    elapsed = time.time() - start",
        "    print(f\"Elapsed: {elapsed:.2f}s\")",
        "    results.append({'phase': 'pagerank', 'elapsed': elapsed, 'timed_out': False})",
        "",
        "    print(\"\\n\" + \"=\" * 60)",
        "    print(\"PHASE 3: TF-IDF\")",
        "    print(\"=\" * 60)",
        "    start = time.time()",
        "    processor.compute_tfidf(verbose=False)",
        "    elapsed = time.time() - start",
        "    print(f\"Elapsed: {elapsed:.2f}s\")",
        "    results.append({'phase': 'tfidf', 'elapsed': elapsed, 'timed_out': False})",
        "",
        "    print(\"\\n\" + \"=\" * 60)",
        "    print(\"PHASE 4: Document Connections\")",
        "    print(\"=\" * 60)",
        "    start = time.time()",
        "    processor.compute_document_connections(verbose=False)",
        "    elapsed = time.time() - start",
        "    print(f\"Elapsed: {elapsed:.2f}s\")",
        "    results.append({'phase': 'doc_connections', 'elapsed': elapsed, 'timed_out': False})",
        "",
        "    # Potentially slow phases",
        "    results.append(profile_bigram_connections(processor, timeout_sec))",
        "    results.append(profile_louvain(processor, timeout_sec))",
        "    results.append(profile_semantics(processor, timeout_sec))",
        "",
        "    return results",
        "",
        "",
        "def print_summary(results: list):",
        "    \"\"\"Print profiling summary.\"\"\"",
        "    print(\"\\n\" + \"=\" * 60)",
        "    print(\"PROFILING SUMMARY\")",
        "    print(\"=\" * 60)",
        "",
        "    total_time = sum(r['elapsed'] for r in results)",
        "",
        "    print(f\"\\n{'Phase':<25} {'Time':>10} {'Status':>15}\")",
        "    print(\"-\" * 50)",
        "",
        "    for r in results:",
        "        status = \"TIMED OUT\" if r.get('timed_out') else \"OK\"",
        "        print(f\"{r['phase']:<25} {r['elapsed']:>9.2f}s {status:>15}\")",
        "",
        "    print(\"-\" * 50)",
        "    print(f\"{'TOTAL':<25} {total_time:>9.2f}s\")",
        "",
        "    # Identify bottleneck",
        "    slowest = max(results, key=lambda x: x['elapsed'])",
        "    print(f\"\\nüîç BOTTLENECK: {slowest['phase']} ({slowest['elapsed']:.2f}s)\")",
        "",
        "    timed_out = [r for r in results if r.get('timed_out')]",
        "    if timed_out:",
        "        print(f\"\\n‚ö†Ô∏è  TIMED OUT PHASES: {', '.join(r['phase'] for r in timed_out)}\")",
        "",
        "",
        "def main():",
        "    parser = argparse.ArgumentParser(description=\"Profile full-analysis bottlenecks\")",
        "    parser.add_argument('--corpus', default='corpus_dev.pkl',",
        "                       help='Path to corpus file')",
        "    parser.add_argument('--phase', choices=['all', 'louvain', 'label_propagation',",
        "                                            'semantics', 'bigram'],",
        "                       default='all', help='Phase to profile')",
        "    parser.add_argument('--timeout', type=int, default=30,",
        "                       help='Timeout per phase in seconds')",
        "",
        "    args = parser.parse_args()",
        "",
        "    corpus_path = Path(args.corpus)",
        "    if not corpus_path.exists():",
        "        print(f\"Corpus not found: {corpus_path}\")",
        "        print(\"Run: python scripts/index_codebase.py first\")",
        "        sys.exit(1)",
        "",
        "    processor = load_corpus(str(corpus_path))",
        "",
        "    if args.phase == 'all':",
        "        results = profile_all_phases(processor, args.timeout)",
        "        print_summary(results)",
        "    elif args.phase == 'louvain':",
        "        profile_louvain(processor, args.timeout)",
        "    elif args.phase == 'label_propagation':",
        "        profile_label_propagation(processor, args.timeout)",
        "    elif args.phase == 'semantics':",
        "        profile_semantics(processor, args.timeout)",
        "    elif args.phase == 'bigram':",
        "        profile_bigram_connections(processor, args.timeout)",
        "",
        "",
        "if __name__ == '__main__':",
        "    main()"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    }
  ],
  "hour_of_day": 21,
  "day_of_week": "Thursday",
  "seconds_since_last_commit": -316037,
  "is_merge": false,
  "is_initial": false,
  "parent_count": 1,
  "session_id": null,
  "related_chats": [],
  "ci_result": null,
  "reverted": false,
  "amended": false
}