{
  "hash": "e5dd3d5f1fc3260382c2c94666dfabc1b83016d3",
  "message": "Task #145: Improve graph embedding quality for common terms",
  "author": "Claude",
  "timestamp": "2025-12-12 01:55:38 +0000",
  "branch": "claude/multi-index-design-DvifZ",
  "files_changed": [
    "TASK_LIST.md",
    "cortical/embeddings.py",
    "cortical/processor.py"
  ],
  "insertions": 128,
  "deletions": 24,
  "hunks": [
    {
      "file": "TASK_LIST.md",
      "function": "Active backlog for the Cortical Text Processor project. Completed tasks are arch",
      "start_line": 11,
      "lines_added": [
        "*All high-priority tasks completed!*"
      ],
      "lines_removed": [
        "| # | Task | Category | Depends | Effort |",
        "|---|------|----------|---------|--------|",
        "| 145 | Improve graph embedding quality for common terms | Quality | - | Medium |"
      ],
      "context_before": [
        "## Active Backlog",
        "",
        "<!-- Machine-parseable format for automation -->",
        "",
        "### ðŸ”´ Critical (Do Now)",
        "",
        "*All critical tasks completed!*",
        "",
        "### ðŸŸ  High (Do This Week)",
        ""
      ],
      "context_after": [
        "",
        "### ðŸŸ¡ Medium (Do This Month)",
        "",
        "| # | Task | Category | Depends | Effort |",
        "|---|------|----------|---------|--------|",
        "| 137 | Cap bigram connections to top-K per bigram | Perf | - | Small |",
        "| 138 | Use sparse matrix multiplication for bigram connections | Perf | - | Medium |",
        "| 139 | Batch bigram connection updates to reduce dict overhead | Perf | - | Small |",
        "| 133 | Implement WAL + snapshot persistence (fault-tolerant rebuild) | Arch | 132 | Large |",
        "| 134 | Implement protobuf serialization for corpus | Arch | 132 | Medium |"
      ],
      "change_type": "modify"
    },
    {
      "file": "TASK_LIST.md",
      "function": "Active backlog for the Cortical Text Processor project. Completed tasks are arch",
      "start_line": 85,
      "lines_added": [
        "| 145 | Improve graph embedding quality for common terms | 2025-12-12 | Added 'tfidf' method, IDF weighting to 'fast' method |"
      ],
      "lines_removed": [],
      "context_before": [
        "| # | Task | Started | Notes |",
        "|---|------|---------|-------|",
        "| 87 | Add Python code samples and showcase | 2025-12-11 | samples/*.py created |",
        "",
        "---",
        "",
        "## Recently Completed (Last 7 Days)",
        "",
        "| # | Task | Completed | Notes |",
        "|---|------|-----------|-------|"
      ],
      "context_after": [
        "| 143 | Investigate negative silhouette score in clustering | 2025-12-12 | Expected behavior: modularity â‰  silhouette (graph vs doc similarity) |",
        "| 142 | Investigate 74s compute_all() performance regression | 2025-12-12 | 5.2x speedup via fast embeddings + sampling (74s â†’ 14s) |",
        "| 144 | Boost exact document name matches in search | 2025-12-12 | doc_name_boost parameter in search functions |",
        "| 141 | Filter Python keywords/artifacts from analysis | 2025-12-12 | CODE_NOISE_TOKENS + filter_code_noise tokenizer option |",
        "| 94 | Split query.py into focused modules | 2025-12-12 | 8 modules: expansion, search, passages, chunking, intent, definitions, ranking, analogy |",
        "| 97 | Integrate CorticalConfig into processor | 2025-12-11 | Config stored on processor, used in method defaults, saved/loaded |",
        "| 127 | Create cluster coverage evaluation script | 2025-12-11 | scripts/evaluate_cluster.py with 24 tests |",
        "| 125 | Add clustering quality metrics (modularity, silhouette) | 2025-12-11 | compute_clustering_quality() in analysis.py, showcase display |",
        "| 124 | Add minimum cluster count regression tests | 2025-12-11 | 4 new tests: coherence, showcase count, mega-cluster, distribution |",
        "| 128 | Fix definition boost that favors test mocks over real implementations | 2025-12-11 | Added is_test_file() and test file penalty |"
      ],
      "change_type": "add"
    },
    {
      "file": "TASK_LIST.md",
      "function": "Added `doc_name_boost` parameter (default 2.0) to `find_documents_for_query()` a",
      "start_line": 1426,
      "lines_added": [
        "### 145. Improve Graph Embedding Quality for Common Terms âœ…",
        "**Meta:** `status:completed` `priority:high` `category:quality`",
        "**Files:** `cortical/embeddings.py`, `cortical/processor.py`",
        "**Completed:** 2025-12-12",
        "- 'machine' â†’ rate, experience, gradients (not semantically related)",
        "**Root Cause Analysis:**",
        "Graph-based embeddings use connection patterns to landmarks. Sparse connections",
        "(only 8-13 out of 64 landmarks) cause misleading high similarity when vectors",
        "are normalized. Terms sharing large documents have similar patterns regardless",
        "of semantic meaning.",
        "**Solution Applied:**",
        "1. Added 'tfidf' embedding method that uses document distribution as feature space",
        "2. Added IDF weighting to 'fast' method to down-weight common terms",
        "3. Updated docstrings to recommend 'tfidf' for semantic similarity tasks",
        "",
        "**Results:**",
        "```",
        "FAST (graph):   'machine' â†’ rate (0.82), gradients (0.77), experience (0.73)",
        "TF-IDF:         'machine' â†’ representations (0.71), learning (0.71), image (0.70)",
        "",
        "FAST (graph):   'learning' â†’ training (0.70), approaches (0.64)",
        "TF-IDF:         'learning' â†’ neural (0.82), networks (0.81), training (0.77)",
        "```",
        "TF-IDF embeddings capture semantic similarity much better because terms appearing",
        "in similar documents are usually semantically related.",
        "- [x] 'data' filtered out by CODE_NOISE_TOKENS (Task #141)",
        "- [x] High-frequency terms don't dominate (TF-IDF naturally down-weights)",
        "- [x] Existing good similarities preserved and improved",
        "- [x] New 'tfidf' method available for semantic similarity tasks"
      ],
      "lines_removed": [
        "### 145. Improve Graph Embedding Quality for Common Terms",
        "**Meta:** `status:pending` `priority:high` `category:quality`",
        "**Files:** `cortical/embeddings.py`",
        "- These connections don't make semantic sense",
        "Random walk embeddings for 'neural' and 'learning' are better:",
        "- 'neural' â†’ resnet, networks, learn (good)",
        "- 'learning' â†’ rewards, rate, machine (good)",
        "**Hypothesis:** High-frequency terms like \"data\" connect to everything, making their embeddings indiscriminate. The random walk is capturing graph structure, not semantics.",
        "**Possible Fixes:**",
        "1. Use inverse document frequency weighting in random walks",
        "2. Cap connections per term before computing embeddings",
        "3. Use different embedding method for high-frequency terms",
        "4. Apply L2 normalization after embedding computation",
        "- [ ] 'data' similar to: processing, analysis, information (semantic neighbors)",
        "- [ ] High-frequency terms don't dominate similarity results",
        "- [ ] Existing good similarities preserved"
      ],
      "context_before": [
        "- \"fermentation\" â†’ `fermentation_science` still #1",
        "- \"quantum computing\" â†’ `quantum_computing_basics` now #1",
        "",
        "**Acceptance Criteria:**",
        "- [x] Query \"distributed systems\" returns `distributed_systems` in top 2",
        "- [x] Query \"fermentation\" keeps `fermentation_science` at #1",
        "- [x] No regression on other queries (1121 tests pass)",
        "",
        "---",
        ""
      ],
      "context_after": [
        "",
        "**Effort:** Medium",
        "",
        "**Problem (Dog-fooding 2025-12-12):** Graph embeddings show semantically odd similarities:",
        "- 'data' â†’ cognitive (0.968), alternatives (0.967), server (0.958)",
        "",
        "",
        "",
        "",
        "**Acceptance Criteria:**",
        "",
        "---",
        "",
        "## Category Index",
        "",
        "| Category | Pending | Description |",
        "|----------|---------|-------------|",
        "| Quality | 5 | Quality issues from dog-fooding |",
        "| Perf | 4 | Performance improvements |",
        "| AINav | 6 | AI assistant navigation & usability |"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/embeddings.py",
      "function": "def compute_graph_embeddings(",
      "start_line": 44,
      "lines_added": [
        "    elif method == 'tfidf':",
        "        # TF-IDF based embeddings (best for semantic similarity)",
        "        embeddings = _tfidf_embeddings(layer0, dimensions, sampled_terms)"
      ],
      "lines_removed": [],
      "context_before": [
        "    # Sample top terms if max_terms is specified",
        "    if max_terms is not None and max_terms < layer0.column_count():",
        "        sorted_cols = sorted(layer0.minicolumns.values(), key=lambda c: c.pagerank, reverse=True)",
        "        sampled_terms = {col.content for col in sorted_cols[:max_terms]}",
        "    else:",
        "        sampled_terms = None",
        "",
        "    if method == 'fast':",
        "        # Fast direct adjacency without multi-hop propagation",
        "        embeddings = _fast_adjacency_embeddings(layer0, dimensions, sampled_terms)"
      ],
      "context_after": [
        "    elif method == 'adjacency':",
        "        embeddings = _adjacency_embeddings(layer0, dimensions, sampled_terms)",
        "    elif method == 'random_walk':",
        "        embeddings = _random_walk_embeddings(layer0, dimensions, sampled_terms)",
        "    elif method == 'spectral':",
        "        embeddings = _spectral_embeddings(layer0, dimensions, sampled_terms)",
        "    else:",
        "        raise ValueError(f\"Unknown embedding method: {method}\")",
        "",
        "    stats = {"
      ],
      "change_type": "add"
    },
    {
      "file": "cortical/embeddings.py",
      "function": "def compute_graph_embeddings(",
      "start_line": 67,
      "lines_added": [
        "    sampled_terms: Optional[set] = None,",
        "    use_idf_weighting: bool = True",
        "        use_idf_weighting: If True, weight connections by IDF of the target term.",
        "                          This down-weights connections to very common terms,",
        "                          improving embedding quality for diverse corpora.",
        "    # Compute IDF weights for landmarks if enabled",
        "    # IDF = log(N / df) where df = number of documents containing the term",
        "    total_docs = len(set(doc_id for col in layer.minicolumns.values() for doc_id in col.document_ids))",
        "    landmark_idf = {}",
        "    if use_idf_weighting and total_docs > 0:",
        "        for lm in landmarks:",
        "            doc_freq = max(1, len(lm.document_ids))",
        "            # Using smoothed IDF: log((N + 1) / (df + 1)) + 1",
        "            landmark_idf[lm.id] = math.log((total_docs + 1) / (doc_freq + 1)) + 1.0",
        "    else:",
        "        # No weighting - all landmarks have weight 1.0",
        "        for lm in landmarks:",
        "            landmark_idf[lm.id] = 1.0",
        "",
        "        # Direct connections only, weighted by landmark IDF",
        "                raw_weight = col.lateral_connections[lm_id]",
        "                idf_weight = landmark_idf.get(lm_id, 1.0)",
        "                vec[lm_idx] = raw_weight * idf_weight",
        "",
        "        # Normalize",
        "        mag = math.sqrt(sum(v*v for v in vec)) + 1e-10",
        "        embeddings[col.content] = [v / mag for v in vec]",
        "",
        "    return embeddings",
        "",
        "",
        "def _tfidf_embeddings(",
        "    layer: HierarchicalLayer,",
        "    dimensions: int,",
        "    sampled_terms: Optional[set] = None",
        ") -> Dict[str, List[float]]:",
        "    \"\"\"",
        "    TF-IDF based embeddings using document distribution as feature space.",
        "",
        "    Each term's embedding is its TF-IDF scores across documents. This produces",
        "    embeddings where semantically similar terms (those appearing in similar",
        "    documents) have high cosine similarity.",
        "",
        "    This method is generally better for semantic similarity than graph-based",
        "    methods because:",
        "    1. Terms appearing in similar documents are likely semantically related",
        "    2. TF-IDF naturally down-weights common terms",
        "    3. Embeddings are dense (no sparse landmark issues)",
        "",
        "    Args:",
        "        layer: Layer to compute embeddings for",
        "        dimensions: Maximum number of document dimensions (uses top N docs by size)",
        "        sampled_terms: If set, only compute embeddings for these terms",
        "    \"\"\"",
        "    embeddings: Dict[str, List[float]] = {}",
        "",
        "    # Get all documents and sort by document \"importance\" (term count)",
        "    all_docs = set()",
        "    doc_term_count = defaultdict(int)",
        "    for col in layer.minicolumns.values():",
        "        for doc_id in col.document_ids:",
        "            all_docs.add(doc_id)",
        "            doc_term_count[doc_id] += 1",
        "",
        "    # Use top N documents as dimensions (by term coverage)",
        "    sorted_docs = sorted(all_docs, key=lambda d: -doc_term_count[d])",
        "    doc_dims = sorted_docs[:dimensions]",
        "    doc_to_idx = {doc: i for i, doc in enumerate(doc_dims)}",
        "",
        "    cols_to_process = layer.minicolumns.values()",
        "    if sampled_terms is not None:",
        "        cols_to_process = [c for c in cols_to_process if c.content in sampled_terms]",
        "",
        "    for col in cols_to_process:",
        "        vec = [0.0] * len(doc_dims)",
        "",
        "        # Fill in TF-IDF values for documents in our dimension space",
        "        for doc_id, tfidf_score in col.tfidf_per_doc.items():",
        "            if doc_id in doc_to_idx:",
        "                vec[doc_to_idx[doc_id]] = tfidf_score"
      ],
      "lines_removed": [
        "    sampled_terms: Optional[set] = None",
        "        # Direct connections only",
        "                vec[lm_idx] = col.lateral_connections[lm_id]"
      ],
      "context_before": [
        "        'max_terms': max_terms,",
        "        'sampled': max_terms is not None and max_terms < layer0.column_count()",
        "    }",
        "",
        "    return embeddings, stats",
        "",
        "",
        "def _fast_adjacency_embeddings(",
        "    layer: HierarchicalLayer,",
        "    dimensions: int,"
      ],
      "context_after": [
        ") -> Dict[str, List[float]]:",
        "    \"\"\"",
        "    Fast direct adjacency embeddings without multi-hop propagation.",
        "",
        "    Much faster than full adjacency but less expressive. Good for large corpora",
        "    where speed is more important than embedding quality.",
        "",
        "    Args:",
        "        layer: Layer to compute embeddings for",
        "        dimensions: Number of embedding dimensions (= number of landmarks)",
        "        sampled_terms: If set, only compute embeddings for these terms",
        "    \"\"\"",
        "    embeddings: Dict[str, List[float]] = {}",
        "",
        "    sorted_cols = sorted(layer.minicolumns.values(), key=lambda c: c.pagerank, reverse=True)",
        "    landmarks = sorted_cols[:dimensions]",
        "    landmark_ids = {lm.id: i for i, lm in enumerate(landmarks)}",
        "",
        "    cols_to_process = layer.minicolumns.values()",
        "    if sampled_terms is not None:",
        "        cols_to_process = [c for c in cols_to_process if c.content in sampled_terms]",
        "",
        "    for col in cols_to_process:",
        "        vec = [0.0] * dimensions",
        "",
        "        for lm_id, lm_idx in landmark_ids.items():",
        "            if lm_id in col.lateral_connections:",
        "",
        "        # Normalize",
        "        mag = math.sqrt(sum(v*v for v in vec)) + 1e-10",
        "        embeddings[col.content] = [v / mag for v in vec]",
        "",
        "    return embeddings",
        "",
        "",
        "def _adjacency_embeddings(",
        "    layer: HierarchicalLayer,"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/processor.py",
      "function": "class CorticalTextProcessor:",
      "start_line": 1370,
      "lines_added": [
        "            method: Embedding method:",
        "                - 'tfidf': TF-IDF based embeddings. Best for semantic similarity.",
        "                  Uses document distribution as feature space. Recommended for",
        "                  finding semantically similar terms.",
        "                - 'fast': Fast graph adjacency. Good for large corpora when speed",
        "                  matters more than semantic quality.",
        "                - 'adjacency': Multi-hop graph adjacency. More expressive but slower.",
        "                - 'random_walk': DeepWalk-style random walks. Good graph structure.",
        "                - 'spectral': Graph Laplacian eigenvectors. Mathematical approach.",
        "",
        "        Note:",
        "            For semantic similarity tasks (finding related terms), 'tfidf' method",
        "            produces significantly better results than graph-based methods because",
        "            terms appearing in similar documents are usually semantically related."
      ],
      "lines_removed": [
        "            method: Embedding method - 'fast', 'adjacency', 'random_walk', or 'spectral'",
        "                   'fast' is recommended for large corpora (>3000 tokens)"
      ],
      "context_before": [
        "        dimensions: int = 64,",
        "        method: str = 'fast',",
        "        max_terms: Optional[int] = None,",
        "        verbose: bool = True",
        "    ) -> Dict:",
        "        \"\"\"",
        "        Compute graph embeddings for tokens.",
        "",
        "        Args:",
        "            dimensions: Number of embedding dimensions (default 64)"
      ],
      "context_after": [
        "            max_terms: Maximum number of terms to embed (by PageRank).",
        "                      If None, auto-selects based on corpus size:",
        "                      - <2000 tokens: embed all",
        "                      - 2000-5000 tokens: embed top 1500",
        "                      - >5000 tokens: embed top 1000",
        "            verbose: Print progress messages",
        "",
        "        Returns:",
        "            Statistics dict with method, dimensions, terms_embedded",
        "        \"\"\"",
        "        # Auto-select max_terms based on corpus size",
        "        token_count = self.layers[CorticalLayer.TOKENS].column_count()",
        "        if max_terms is None:",
        "            if token_count < 2000:",
        "                max_terms = None  # Embed all",
        "            elif token_count < 5000:",
        "                max_terms = 1500",
        "            else:",
        "                max_terms = 1000"
      ],
      "change_type": "modify"
    }
  ],
  "hour_of_day": 1,
  "day_of_week": "Friday",
  "seconds_since_last_commit": -301750,
  "is_merge": false,
  "is_initial": false,
  "parent_count": 1,
  "session_id": null,
  "related_chats": [],
  "ci_result": null,
  "reverted": false,
  "amended": false
}