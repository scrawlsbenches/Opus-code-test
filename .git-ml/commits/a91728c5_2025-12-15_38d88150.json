{
  "hash": "a91728c587c6c1077433d55dc1b78e7684def0f4",
  "message": "Update MoE docs to integrate with BM25/GB-BM25 implementations",
  "author": "Claude",
  "timestamp": "2025-12-15 12:32:37 +0000",
  "branch": "claude/multi-index-design-DvifZ",
  "files_changed": [
    "docs/moe-index-design.md",
    "docs/moe-index-implementation-plan.md",
    "docs/moe-index-knowledge-transfer.md"
  ],
  "insertions": 183,
  "deletions": 102,
  "hunks": [
    {
      "file": "docs/moe-index-design.md",
      "function": null,
      "start_line": 1,
      "lines_added": [
        "**Updated:** 2025-12-15 (integrated with BM25/GB-BM25 implementations)",
        "**Version:** 1.1",
        "**Prerequisites:** [moe-index-knowledge-transfer.md](moe-index-knowledge-transfer.md), [knowledge-transfer-bm25-optimization.md](knowledge-transfer-bm25-optimization.md)"
      ],
      "lines_removed": [
        "**Version:** 1.0",
        "**Prerequisites:** [moe-index-knowledge-transfer.md](moe-index-knowledge-transfer.md)"
      ],
      "context_before": [
        "# Mixture of Expert Indexes: Technical Design Document",
        "",
        "**Author:** Claude (AI Assistant)",
        "**Date:** 2025-12-15"
      ],
      "context_after": [
        "**Status:** Design Proposal",
        "",
        "---",
        "",
        "## Table of Contents",
        "",
        "1. [Overview](#1-overview)",
        "2. [System Architecture](#2-system-architecture)",
        "3. [Expert Index Specifications](#3-expert-index-specifications)",
        "4. [Gating Network Design](#4-gating-network-design)",
        "5. [Result Fusion](#5-result-fusion)"
      ],
      "change_type": "modify"
    },
    {
      "file": "docs/moe-index-design.md",
      "function": null,
      "start_line": 28,
      "lines_added": [
        "### 1.2 Building on Existing Implementations",
        "",
        "> **Important:** This design builds on recent additions to the codebase:",
        "> - **BM25 scoring** (`cortical/analysis.py`) - Already implemented as default",
        "> - **GB-BM25** (`cortical/query/search.py:graph_boosted_search()`) - Proto-MoE combining BM25 + PageRank + Proximity",
        "> - **Document length tracking** (`processor.doc_lengths`, `processor.avg_doc_length`)",
        "> - **Performance optimizations** - 34.5% faster `compute_all()`",
        ">",
        "> The MoE architecture generalizes GB-BM25's signal fusion into separate experts with intelligent routing.",
        "",
        "### 1.3 Non-Goals",
        "### 1.4 Key Design Decisions"
      ],
      "lines_removed": [
        "### 1.2 Non-Goals",
        "### 1.3 Key Design Decisions"
      ],
      "context_before": [
        "## 1. Overview",
        "",
        "### 1.1 Design Goals",
        "",
        "1. **Improved query latency** for simple queries (target: <50ms for exact match)",
        "2. **Better relevance** through specialized handling",
        "3. **Extensibility** to add new expert indexes",
        "4. **Backward compatibility** with existing API",
        "5. **Zero external dependencies** (per project philosophy)",
        ""
      ],
      "context_after": [
        "",
        "1. ~~ML-based routing~~ (use rule-based/statistical instead)",
        "2. ~~Real-time expert retraining~~ (static specialization)",
        "3. ~~Distributed indexes~~ (single-machine focus for now)",
        "",
        "",
        "| Decision | Choice | Rationale |",
        "|----------|--------|-----------|",
        "| Routing approach | Feature + intent based | No ML dependencies |",
        "| Expert count | 5 initial | Cover primary use cases |",
        "| Default activation | Top-2 | Balance speed vs coverage |",
        "| Fusion strategy | Weighted RRF | Robust to score differences |",
        "| Storage | Shared + expert-specific | Minimize redundancy |",
        "",
        "---"
      ],
      "change_type": "modify"
    },
    {
      "file": "docs/moe-index-design.md",
      "function": "class ExpertIndex(ABC):",
      "start_line": 234,
      "lines_added": [
        "> **Builds on existing:** The codebase already has BM25 implemented in `cortical/analysis.py:_bm25_core()` and document length tracking in `processor.doc_lengths`. The Lexical Expert wraps this with a fast inverted index for O(1) term lookups.",
        "",
        "    \"\"\"Fast lexical search using existing BM25 + inverted index.\"\"\"",
        "    def __init__(self, config: LexicalConfig, processor: CorticalTextProcessor):",
        "        # Reuse processor's BM25 scores (already computed)",
        "        self._processor = processor",
        "        # Additional inverted index for fast lookups",
        "        # term → [(doc_id, positions)] - positions enable phrase queries",
        "        self._term_positions: Dict[str, Dict[str, List[int]]] = {}",
        "        # Reuse processor's document length stats",
        "        # self._processor.doc_lengths (already tracked)",
        "        # self._processor.avg_doc_length (already tracked)",
        "        # BM25 parameters from processor config",
        "        # self._processor.config.bm25_k1 (default 1.2)",
        "        # self._processor.config.bm25_b (default 0.75)",
        "class TermPositions:",
        "    \"\"\"Term positions within a document (for phrase queries).\"\"\"",
        "    doc_id: str",
        "    positions: List[int]  # Word positions",
        "**Scoring Algorithm:** Delegates to existing BM25",
        "def query(self, query_text: str, top_n: int = 10, **kwargs) -> ExpertResult:",
        "    \"\"\"Fast search using existing BM25 scores.\"\"\"",
        "    # Use fast_find_documents which already uses BM25/TF-IDF",
        "    results = fast_find_documents(",
        "        query_text,",
        "        self._processor.layers,",
        "        self._processor.tokenizer,",
        "        top_n=top_n,",
        "        use_code_concepts=False  # Pure lexical, no expansion",
        "    )",
        "    return ExpertResult(",
        "        documents=[doc for doc, _ in results],",
        "        scores={doc: score for doc, score in results},",
        "        confidence=0.9  # High confidence for exact matches",
        "**Key insight:** We don't need to reimplement BM25—it's already in `analysis.py:_bm25_core()`. The Lexical Expert adds:",
        "1. Positional index for phrase queries",
        "2. Prefix trie for autocomplete",
        "3. No query expansion (pure lexical matching)",
        "",
        "> **Builds on existing:** The codebase has `graph_boosted_search()` which combines BM25 + PageRank + Proximity. The Semantic Expert uses this or similar logic, focusing on the PageRank and query expansion signals.",
        ""
      ],
      "lines_removed": [
        "    \"\"\"Fast lexical search using inverted index.\"\"\"",
        "    def __init__(self, config: LexicalConfig):",
        "        # Core index: term → [(doc_id, positions, tf)]",
        "        self._inverted_index: Dict[str, List[Posting]] = {}",
        "",
        "        # Document metadata: doc_id → (length, max_tf)",
        "        self._doc_stats: Dict[str, DocStats] = {}",
        "        # Collection statistics",
        "        self._total_docs: int = 0",
        "        self._avg_doc_length: float = 0.0",
        "        # BM25 parameters",
        "        self._k1: float = config.bm25_k1  # 1.2",
        "        self._b: float = config.bm25_b    # 0.75",
        "@dataclass",
        "class Posting:",
        "    \"\"\"Inverted index posting.\"\"\"",
        "    doc_id: str",
        "    positions: List[int]  # Word positions for phrase queries",
        "    term_frequency: int",
        "class DocStats:",
        "    \"\"\"Per-document statistics.\"\"\"",
        "    length: int           # Token count",
        "    max_tf: int          # Maximum term frequency",
        "**Scoring Algorithm:** BM25",
        "def _bm25_score(self, term: str, doc_id: str) -> float:",
        "    \"\"\"Calculate BM25 score for term in document.\"\"\"",
        "    posting = self._get_posting(term, doc_id)",
        "    if not posting:",
        "        return 0.0",
        "",
        "    tf = posting.term_frequency",
        "    doc_length = self._doc_stats[doc_id].length",
        "",
        "    # IDF component",
        "    df = len(self._inverted_index.get(term, []))",
        "    idf = math.log((self._total_docs - df + 0.5) / (df + 0.5) + 1)",
        "",
        "    # TF component with saturation",
        "    tf_component = (tf * (self._k1 + 1)) / (",
        "        tf + self._k1 * (1 - self._b + self._b * doc_length / self._avg_doc_length)",
        "",
        "    return idf * tf_component"
      ],
      "context_before": [
        "        return {",
        "            'name': self.name,",
        "            'capabilities': list(self.capabilities),",
        "        }",
        "```",
        "",
        "### 3.2 Lexical Expert",
        "",
        "**Purpose:** Fast exact and near-exact matching.",
        ""
      ],
      "context_after": [
        "**Data Structures:**",
        "",
        "```python",
        "# cortical/moe/experts/lexical.py",
        "",
        "class LexicalExpert(ExpertIndex):",
        "",
        "",
        "",
        "        # Optional: prefix trie for autocomplete",
        "        self._prefix_trie: Optional[PrefixTrie] = None",
        "",
        "",
        "",
        "@dataclass",
        "```",
        "",
        "",
        "```python",
        "    )",
        "```",
        "",
        "**Capabilities:** `{'exact_match', 'phrase_match', 'autocomplete', 'fast'}`",
        "",
        "### 3.3 Semantic Expert",
        "",
        "**Purpose:** Meaning-based retrieval using existing system.",
        "",
        "**Implementation:** Wraps `CorticalTextProcessor` with minimal changes.",
        "",
        "```python",
        "# cortical/moe/experts/semantic.py",
        "",
        "class SemanticExpert(ExpertIndex):",
        "    \"\"\"Semantic search using existing Cortical Text Processor.\"\"\"",
        "",
        "    def __init__(self, processor: CorticalTextProcessor):",
        "        self._processor = processor"
      ],
      "change_type": "modify"
    },
    {
      "file": "docs/moe-index-implementation-plan.md",
      "function": null,
      "start_line": 1,
      "lines_added": [
        "**Updated:** 2025-12-15 (reduced scope: BM25 and GB-BM25 already implemented)",
        "- [knowledge-transfer-bm25-optimization.md](knowledge-transfer-bm25-optimization.md)",
        "> **Scope Reduction (2025-12-15):** The recent BM25 and GB-BM25 implementations significantly reduce the work needed:",
        "> - BM25 scoring already implemented (`analysis.py:_bm25_core()`)",
        "> - Document length tracking already exists (`processor.doc_lengths`)",
        "> - GB-BM25 provides a working proto-MoE (`query/search.py:graph_boosted_search()`)",
        "> - 34.5% performance improvement in `compute_all()` already achieved",
        ">",
        "> **Remaining work focuses on:**",
        "> - Expert abstraction layer and routing",
        "> - Structural, temporal, and episodic experts (new capabilities)",
        "> - Cross-pollination framework (generalized from GB-BM25)",
        "",
        "**Total Estimated Effort (Revised):**",
        "- New code: ~2,100 lines (down from ~2,800 - BM25 already done)",
        "- Test code: ~1,200 lines (down from ~1,500)",
        "- Documentation: ~300 lines (down from ~500)"
      ],
      "lines_removed": [
        "**Timeline:** ~6-8 weeks of focused development (no calendar estimates per project guidelines)",
        "",
        "**Total Estimated Effort:**",
        "- New code: ~2,800 lines",
        "- Test code: ~1,500 lines",
        "- Documentation: ~500 lines"
      ],
      "context_before": [
        "# Mixture of Expert Indexes: Implementation Plan",
        "",
        "**Author:** Claude (AI Assistant)",
        "**Date:** 2025-12-15"
      ],
      "context_after": [
        "**Status:** Proposed",
        "**Prerequisites:**",
        "- [moe-index-knowledge-transfer.md](moe-index-knowledge-transfer.md)",
        "- [moe-index-design.md](moe-index-design.md)",
        "",
        "---",
        "",
        "## Executive Summary",
        "",
        "This document outlines a **6-phase implementation plan** for the Mixture of Expert (MoE) index architecture. The plan prioritizes incremental value delivery, maintaining backward compatibility, and thorough testing at each phase.",
        "",
        "",
        "---",
        "",
        "## Phase Overview",
        "",
        "```",
        "Phase 1: Foundation        ████░░░░░░░░░░░░░░░░ 20%",
        "Phase 2: Lexical Expert    ████████░░░░░░░░░░░░ 40%",
        "Phase 3: Routing & Fusion  ████████████░░░░░░░░ 60%",
        "Phase 4: Additional Experts████████████████░░░░ 80%"
      ],
      "change_type": "modify"
    },
    {
      "file": "docs/moe-index-implementation-plan.md",
      "function": "tests/",
      "start_line": 119,
      "lines_added": [
        "> **Scope Reduction:** BM25 is already implemented in `analysis.py:_bm25_core()`. The Lexical Expert wraps existing functionality + adds phrase matching.",
        "",
        "#### 2.1 Lexical Expert Implementation (Simplified)",
        "**Already Available (reuse):**",
        "- [x] BM25 scoring algorithm (`analysis.py:_bm25_core()`)",
        "- [x] Document length tracking (`processor.doc_lengths`)",
        "- [x] Fast search infrastructure (`fast_find_documents()`)",
        "",
        "**New Deliverables:**",
        "- [ ] Implement `LexicalExpert` class wrapping existing BM25",
        "- [ ] Implement `TermPositions` dataclass for phrase queries",
        "- [ ] Build positional index during document processing",
        "- [ ] Implement phrase matching using positions",
        "- [ ] Implement prefix trie for autocomplete (optional)",
        "- Lexical expert delegates to existing BM25 scores",
        "- Simple queries return in <10ms (using `fast_find_documents`)",
        "- Phrase queries work correctly",
        "- No BM25 reimplementation needed",
        "    expert = LexicalExpert(processor)  # Wraps processor",
        "def test_lexical_uses_existing_bm25():",
        "    # Verify we're using processor's BM25, not reimplementing",
        "    expert = LexicalExpert(processor)",
        "    fast_result = fast_find_documents(\"neural\", processor.layers, processor.tokenizer)",
        "    assert result.documents == [d for d, _ in fast_result]",
        "    expert = LexicalExpert(processor)",
        "    # Uses positional index, not just term matching",
        "> **Builds on:** `graph_boosted_search()` already combines BM25 + PageRank + Proximity. The Semantic Expert wraps this with the expert interface.",
        "",
        "- [ ] Delegate to `graph_boosted_search()` for rich queries",
        "- [ ] Delegate to `find_documents_for_query()` for standard queries",
        "- Semantic expert uses existing `graph_boosted_search()`"
      ],
      "lines_removed": [
        "#### 2.1 Lexical Expert Implementation",
        "**Deliverables:**",
        "- [ ] Implement `Posting` dataclass (doc_id, positions, tf)",
        "- [ ] Implement `DocStats` dataclass",
        "- [ ] Implement inverted index structure",
        "- [ ] Implement `add_document()` method",
        "- [ ] Implement `remove_document()` method",
        "- [ ] Implement BM25 scoring algorithm",
        "- [ ] Implement `query()` method",
        "- [ ] Implement phrase matching (optional)",
        "- [ ] Implement persistence (`save`/`load`)",
        "- Lexical expert can index 1000 documents in <5 seconds",
        "- Simple queries return in <10ms",
        "- BM25 scores match reference implementation",
        "- Persistence round-trips correctly",
        "    expert = LexicalExpert()",
        "    expert.add_document(\"doc1\", \"neural networks process data\")",
        "def test_lexical_bm25_ranking():",
        "    expert = LexicalExpert()",
        "    expert.add_document(\"doc1\", \"neural neural neural\")  # High TF",
        "    expert.add_document(\"doc2\", \"neural networks\")",
        "    assert result.documents[0] == \"doc1\"  # Higher TF wins",
        "    expert = LexicalExpert()",
        "    expert.add_document(\"doc1\", \"machine learning algorithms\")",
        "- [ ] Implement delegation for `add_document`",
        "- [ ] Implement delegation for `query`",
        "- Semantic expert produces same results as raw processor"
      ],
      "context_before": [
        "- [ ] Test fixtures provide adequate coverage data",
        "- [ ] `python -m pytest tests/unit/moe/ -v` passes",
        "- [ ] Code review completed",
        "",
        "---",
        "",
        "## Phase 2: Lexical Expert + Semantic Wrapper",
        "",
        "**Goal:** Implement first two experts with basic querying.",
        ""
      ],
      "context_after": [
        "### Tasks",
        "",
        "",
        "",
        "**Acceptance Criteria:**",
        "",
        "**Test Cases:**",
        "```python",
        "def test_lexical_exact_match():",
        "    result = expert.query(\"neural\")",
        "    assert \"doc1\" in result.documents",
        "",
        "    result = expert.query(\"neural\")",
        "",
        "def test_lexical_phrase_query():",
        "    result = expert.query('\"machine learning\"')  # Phrase",
        "    assert \"doc1\" in result.documents",
        "```",
        "",
        "#### 2.2 Semantic Expert Wrapper",
        "",
        "**Deliverables:**",
        "- [ ] Implement `SemanticExpert` wrapping `CorticalTextProcessor`",
        "- [ ] Implement staleness pass-through",
        "- [ ] Implement persistence (reuse processor's)",
        "",
        "**Acceptance Criteria:**",
        "- No regression in existing functionality",
        "- Wrapper adds <5ms overhead",
        "",
        "**Test Cases:**",
        "```python",
        "def test_semantic_matches_processor():",
        "    processor = CorticalTextProcessor()",
        "    processor.process_document(\"doc1\", \"neural networks\")",
        "    processor.compute_all()",
        ""
      ],
      "change_type": "modify"
    },
    {
      "file": "docs/moe-index-knowledge-transfer.md",
      "function": null,
      "start_line": 1,
      "lines_added": [
        "**Updated:** 2025-12-15 (integrated with BM25/GB-BM25 implementations)",
        "**Related:** [architecture.md](architecture.md), [algorithms.md](algorithms.md), [knowledge-transfer-bm25-optimization.md](knowledge-transfer-bm25-optimization.md)",
        "> **Important Update (2025-12-15):** The codebase now includes BM25 scoring and Graph-Boosted Search (GB-BM25), which represent early steps toward the MoE architecture. GB-BM25 already combines BM25 + PageRank + Proximity signals—essentially a \"fused expert\" approach. The full MoE architecture generalizes this into cleanly separated experts with intelligent routing.",
        ""
      ],
      "lines_removed": [
        "**Related:** [architecture.md](architecture.md), [algorithms.md](algorithms.md)"
      ],
      "context_before": [
        "# Mixture of Expert Indexes: Knowledge Transfer Document",
        "",
        "**Author:** Claude (AI Assistant)",
        "**Date:** 2025-12-15"
      ],
      "context_after": [
        "**Status:** Design Proposal",
        "",
        "---",
        "",
        "## Executive Summary",
        "",
        "This document provides comprehensive background knowledge for implementing a **Mixture of Experts (MoE) architecture** for the Cortical Text Processor's indexing system. The core idea: instead of one general-purpose index, maintain multiple specialized \"expert\" indexes, each optimized for different query types, with a learned routing mechanism that selects which experts to activate for each query.",
        "",
        "**Key insight:** Different query types have fundamentally different optimal representations. A code navigation query needs call graph awareness; a semantic similarity query needs distributional embeddings; an exact lookup needs inverted indexes. One index cannot optimize for all.",
        "",
        "---",
        "",
        "## Table of Contents",
        "",
        "1. [Background: Mixture of Experts](#1-background-mixture-of-experts)",
        "2. [Information Retrieval Foundations](#2-information-retrieval-foundations)",
        "3. [Current System Analysis](#3-current-system-analysis)",
        "4. [MoE Applied to Indexing](#4-moe-applied-to-indexing)",
        "5. [Cognitive Science Connections](#5-cognitive-science-connections)",
        "6. [Key Concepts and Terminology](#6-key-concepts-and-terminology)"
      ],
      "change_type": "modify"
    },
    {
      "file": "docs/moe-index-knowledge-transfer.md",
      "function": "The Cortical Text Processor uses a 4-layer hierarchical architecture:",
      "start_line": 157,
      "lines_added": [
        "- Rich semantic relationships (PageRank, BM25, clustering)",
        "- **NEW:** BM25 scoring with term saturation and length normalization",
        "- **NEW:** Graph-Boosted Search (GB-BM25) combining multiple signals",
        "### 3.2 Recent Additions: BM25 and GB-BM25",
        "",
        "As of December 2025, the system includes two important additions that represent early steps toward MoE:",
        "",
        "#### BM25 Scoring (Now Default)",
        "**Location:** `cortical/analysis.py`",
        "",
        "BM25 replaces TF-IDF as the default scoring algorithm:",
        "- **Term saturation** via `k1=1.2`: Diminishing returns for repeated terms",
        "- **Length normalization** via `b=0.75`: Adjusts for document length",
        "- **Improved IDF**: Never returns 0 for single-document terms",
        "",
        "#### Graph-Boosted Search (GB-BM25)",
        "**Location:** `cortical/query/search.py:graph_boosted_search()`",
        "",
        "GB-BM25 is essentially a **proto-MoE** that fuses multiple signals in a single pass:",
        "",
        "```",
        "GB-BM25 Score = (1-α-β) × BM25 + α × PageRank + β × Proximity + Coverage",
        "```",
        "",
        "| Signal | Weight | Source | MoE Expert Analog |",
        "|--------|--------|--------|-------------------|",
        "| BM25 base | 0.5 | Term frequency | Lexical Expert |",
        "| PageRank boost | 0.3 | Graph importance | Semantic Expert |",
        "| Proximity boost | 0.2 | Lateral connections | Cross-pollination |",
        "| Coverage multiplier | 0.5-1.5 | Term coverage | Fusion quality signal |",
        "",
        "**Key insight:** GB-BM25 demonstrates the value of combining signals, but does so in a monolithic function. MoE architecture separates these into distinct experts with intelligent routing.",
        "### 3.3 Query Methods Comparison",
        "| Method | Speed | Signals Used | Best For |",
        "|--------|-------|--------------|----------|",
        "| `fast_find_documents()` | 0.06-0.66ms | BM25 only | Speed-critical |",
        "| `find_documents_for_query()` | 0.13-1.22ms | BM25 + expansion | General search |",
        "| `graph_boosted_search()` | 74ms | BM25 + PageRank + proximity + coverage | Code search |",
        "### 3.4 Data Flow Analysis",
        "**Traditional path:**",
        "Query → Tokenize → Expand → Score (BM25) → Rank → Return",
        "**GB-BM25 path (proto-MoE):**",
        "```",
        "Query → Tokenize → Expand → Score (BM25 + PageRank + Proximity) → Coverage → Rank",
        "                                      ↓",
        "                              (all signals always combined)",
        "```",
        "",
        "**Proposed MoE path:**",
        "```",
        "Query → Router → Select Experts → Parallel Query → Cross-Pollinate → Fuse → Rank",
        "                     ↓                    ↓",
        "            (conditional)        (experts inform each other)",
        "```",
        "",
        "### 3.5 Performance Characteristics",
        "",
        "From benchmarks (`docs/benchmarks.md`):",
        "| Corpus | compute_all() | Standard Search | Fast Search |",
        "|--------|---------------|-----------------|-------------|",
        "| Small (25 docs) | 141 ms | 0.13 ms | 0.06 ms |",
        "| Real (151 files) | 49.4 s | 1.22 ms | 0.66 ms |",
        "**BM25 optimization impact:**",
        "- `compute_all()` improved by **34.5%** (from 7.5s to 4.9s on 43-file corpus)",
        "- Primary savings from `compute_bigram_connections` optimization (50% faster)",
        "### 3.6 What MoE Adds Beyond GB-BM25",
        "| Capability | GB-BM25 | Full MoE |",
        "|------------|---------|----------|",
        "| Signal combination | Fixed weights | Adaptive per-query |",
        "| Expert selection | All signals always | Sparse activation (top-K) |",
        "| Structural queries | Not supported | Structural Expert |",
        "| Temporal queries | Not supported | Temporal Expert |",
        "| Session context | Not supported | Episodic Expert |",
        "| Routing intelligence | None | Feature + intent + feedback |",
        "| Expert specialization | Implicit | Explicit optimization |"
      ],
      "lines_removed": [
        "- Rich semantic relationships (PageRank, TF-IDF, clustering)",
        "### 3.2 Query Patterns in Current System",
        "Analyzing how the current system handles different queries:",
        "| Query Type | Current Handling | Limitation |",
        "|------------|------------------|------------|",
        "| Exact lookup | Full expansion anyway | Overhead |",
        "| Semantic search | Query expansion + TF-IDF | Good, but slow |",
        "| Code navigation | Treats code as text | Misses structure |",
        "| Definition lookup | Heuristic boosting | Not systematic |",
        "| Temporal queries | Not supported | Can't ask \"what changed?\" |",
        "### 3.3 Data Flow Analysis",
        "Query → Tokenize → Expand → Score Documents → Rank → Return",
        "**The problem:** Every query follows the same path regardless of type. A simple exact-match query goes through full semantic expansion.",
        "### 3.4 Performance Characteristics",
        "From profiling (`scripts/profile_full_analysis.py`):",
        "| Phase | Time | Notes |",
        "|-------|------|-------|",
        "| Tokenization | ~10ms | Fast |",
        "| Query expansion | ~50-200ms | Variable, depends on connections |",
        "| Document scoring | ~100-500ms | Scales with corpus size |",
        "| Ranking | ~20ms | Fast |",
        "For simple queries, expansion and scoring dominate—often unnecessarily."
      ],
      "context_before": [
        "Layer 3 (DOCUMENTS)  ← Full documents",
        "    ↑↓",
        "Layer 2 (CONCEPTS)   ← Semantic clusters (Louvain)",
        "    ↑↓",
        "Layer 1 (BIGRAMS)    ← Word pairs",
        "    ↑↓",
        "Layer 0 (TOKENS)     ← Individual words",
        "```",
        "",
        "**Current strengths:**"
      ],
      "context_after": [
        "- Query expansion through lateral connections",
        "- Code-aware tokenization",
        "- Intent parsing",
        "",
        "**Current limitations:**",
        "- Single monolithic index serves all query types",
        "- Simple queries pay semantic expansion overhead",
        "- No structural awareness for code queries",
        "- No temporal dimension",
        "",
        "",
        "",
        "",
        "",
        "```",
        "                     ↓",
        "              (always same path)",
        "```",
        "",
        "",
        "",
        "",
        "",
        "",
        "---",
        "",
        "## 4. MoE Applied to Indexing",
        "",
        "### 4.1 Core Architecture",
        "",
        "```",
        "                    Query",
        "                      │"
      ],
      "change_type": "modify"
    }
  ],
  "hour_of_day": 12,
  "day_of_week": "Monday",
  "seconds_since_last_commit": -4331,
  "is_merge": false,
  "is_initial": false,
  "parent_count": 1,
  "session_id": null,
  "related_chats": [],
  "ci_result": null,
  "reverted": false,
  "amended": false
}