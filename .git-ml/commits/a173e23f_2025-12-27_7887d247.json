{
  "hash": "a173e23f2309cb08ab3c2c7cd3552c909fc08862",
  "message": "docs(benchmarks): Add comprehensive training pipeline documentation",
  "author": "Claude",
  "timestamp": "2025-12-27 03:01:02 +0000",
  "branch": "claude/accept-handoff-ctrSI",
  "files_changed": [
    "benchmarks/codebase_slm/train_augmented.py"
  ],
  "insertions": 89,
  "deletions": 11,
  "hunks": [
    {
      "file": "benchmarks/codebase_slm/train_augmented.py",
      "function": null,
      "start_line": 1,
      "lines_added": [
        "================================================================================",
        "TRAINING PIPELINE OVERVIEW",
        "================================================================================",
        "This script is STEP 2 of a two-step process:",
        "  STEP 1: Generate corpus from codebase (REQUIRED FIRST!)",
        "  --------------------------------------------------------",
        "  python -m benchmarks.codebase_slm.generate_corpus --full",
        "  This extracts training patterns from:",
        "    - 149 Python files in cortical/ → functions, classes, imports",
        "    - 254 Markdown files in docs/ + samples/ → sections, Q&A pairs",
        "    - GoT entities → tasks, decisions, metadata",
        "  Output: corpus/training_patterns.jsonl (~35,000 patterns)",
        "",
        "",
        "  STEP 2: Train model (THIS SCRIPT)",
        "  ----------------------------------",
        "  python -m benchmarks.codebase_slm.train_augmented --dry-run",
        "",
        "  This combines:",
        "    - corpus/training_patterns.jsonl (~35,000 patterns) ← from Step 1",
        "    - data/augmented_corpus.txt (~2,000 lines) ← curated definitions",
        "",
        "  Output: models/prism_augmented.json (~13MB, 37K docs, 15K vocab)",
        "",
        "",
        "================================================================================",
        "WHAT WENT WRONG (Dec 27, 2025 Incident)",
        "================================================================================",
        "",
        "An agent ran this script WITHOUT running generate_corpus.py first:",
        "  - corpus/training_patterns.jsonl didn't exist (gitignored, not tracked)",
        "  - Script printed \"No existing patterns found\" and continued",
        "  - Trained on only 2,094 lines instead of 37,676",
        "  - Result: 329 vocab model replaced 15,814 vocab model (98% loss!)",
        "",
        "The model was restored from git, and this warning was added.",
        "",
        "See: samples/memories/2025-12-27-knowledge-transfer-prism-model-incident.md",
        "",
        "",
        "================================================================================",
        "DATA SOURCES",
        "================================================================================",
        "",
        "1. corpus/training_patterns.jsonl (GENERATED, gitignored)",
        "   - Created by: generate_corpus.py --full",
        "   - Contains: ~35,000 Q&A and completion patterns",
        "   - Format: {\"pattern_type\": \"qa\", \"input_text\": \"...\", \"target_text\": \"...\"}",
        "   - MUST BE REGENERATED after codebase changes",
        "",
        "2. data/augmented_corpus.txt (TRACKED in git)",
        "   - Contains: ~2,000 curated concept definitions",
        "   - Format: Plain text, one pattern per line",
        "   - Example: \"PageRank is a graph algorithm for computing node importance\"",
        "",
        "3. samples/knowledge-base/*.md (TRACKED in git)",
        "   - Contains: 3,144 lines of curated Q&A pairs",
        "   - Used by generate_corpus.py to create patterns",
        "",
        "",
        "================================================================================",
        "USAGE",
        "================================================================================",
        "",
        "# ALWAYS run generate_corpus.py first!",
        "python -m benchmarks.codebase_slm.generate_corpus --full",
        "",
        "# Then train with dry-run to verify (RECOMMENDED)",
        "python -m benchmarks.codebase_slm.train_augmented --dry-run",
        "",
        "# Save to custom path (safest)",
        "python -m benchmarks.codebase_slm.train_augmented --output models/my_model.json",
        "",
        "# Save to default path (creates timestamped backup first)",
        "python -m benchmarks.codebase_slm.train_augmented",
        "",
        "# Force overwrite without backup (use with caution!)",
        "python -m benchmarks.codebase_slm.train_augmented --force",
        "",
        "",
        "================================================================================",
        "SAFEGUARDS",
        "================================================================================",
        "",
        "1. --dry-run: Evaluate without saving anything",
        "2. --output: Explicit path prevents accidental overwrites",
        "3. Auto-backup: Creates models/backups/prism_augmented_TIMESTAMP.json",
        "4. Provenance: Saves corpus hash and metadata in model's _provenance field",
        "5. Loud warning: Shows error if corpus/training_patterns.jsonl is missing"
      ],
      "lines_removed": [
        "IMPORTANT: This script saves models to disk. Use --dry-run to evaluate",
        "without saving, or --output to specify a custom output path.",
        "Usage:",
        "    # Dry run (evaluate only, no save)",
        "    python -m benchmarks.codebase_slm.train_augmented --dry-run",
        "    # Save to custom path (recommended)",
        "    python -m benchmarks.codebase_slm.train_augmented --output models/my_model.json",
        "    # Save to default path (creates backup first)",
        "    python -m benchmarks.codebase_slm.train_augmented",
        "    # Force overwrite without backup",
        "    python -m benchmarks.codebase_slm.train_augmented --force"
      ],
      "context_before": [
        "#!/usr/bin/env python3",
        "\"\"\"",
        "Train PRISM-SLM with augmented corpus and run benchmarks.",
        ""
      ],
      "context_after": [
        "",
        "",
        "",
        "",
        "\"\"\"",
        "",
        "import argparse",
        "import hashlib",
        "import json",
        "import shutil",
        "import sys",
        "from datetime import datetime",
        "from pathlib import Path"
      ],
      "change_type": "modify"
    }
  ],
  "hour_of_day": 3,
  "day_of_week": "Saturday",
  "seconds_since_last_commit": 200,
  "is_merge": false,
  "is_initial": false,
  "parent_count": 1,
  "session_id": "c7e7f25e",
  "related_chats": [
    "chat-20251227-015100-015a56"
  ],
  "ci_result": null,
  "reverted": false,
  "amended": false
}