{
  "hash": "edc2ae971f491a0c2de8190a5c1082f3c68b0cc8",
  "message": "Merge pull request #46 from scrawlsbenches/claude/implement-task-list-01PDz6X7yhNtCNgMb4F8Safm",
  "author": "scrawlsbenches",
  "timestamp": "2025-12-12 03:47:52 -0500",
  "branch": "claude/multi-index-design-DvifZ",
  "files_changed": [
    "CLAUDE.md",
    "README.md",
    "TASK_LIST.md",
    "cortical/analysis.py",
    "cortical/constants.py",
    "cortical/minicolumn.py",
    "cortical/processor.py",
    "cortical/query/ranking.py",
    "cortical/semantics.py",
    "cortical/types.py",
    "docs/README.md",
    "tests/test_semantics.py"
  ],
  "insertions": 555,
  "deletions": 75,
  "hunks": [
    {
      "file": "CLAUDE.md",
      "function": "processor.add_document_incremental(doc_id, text)",
      "start_line": 282,
      "lines_added": [
        "### Staleness Tracking System",
        "",
        "The processor tracks which computations are up-to-date vs needing recalculation. This prevents unnecessary recomputation while ensuring data consistency.",
        "",
        "#### Computation Types",
        "",
        "| Constant | What it tracks | Computed by |",
        "|----------|---------------|-------------|",
        "| `COMP_TFIDF` | TF-IDF scores per term | `compute_tfidf()` |",
        "| `COMP_PAGERANK` | PageRank importance | `compute_importance()` |",
        "| `COMP_ACTIVATION` | Activation propagation | `propagate_activation()` |",
        "| `COMP_DOC_CONNECTIONS` | Document-to-document links | `compute_document_connections()` |",
        "| `COMP_BIGRAM_CONNECTIONS` | Bigram lateral connections | `compute_bigram_connections()` |",
        "| `COMP_CONCEPTS` | Concept clusters (Layer 2) | `build_concept_clusters()` |",
        "| `COMP_EMBEDDINGS` | Graph embeddings | `compute_graph_embeddings()` |",
        "| `COMP_SEMANTICS` | Semantic relations | `extract_corpus_semantics()` |",
        "",
        "#### How Staleness Works",
        "",
        "1. **All computations start stale** - `_mark_all_stale()` is called in `__init__`",
        "2. **Adding documents marks all stale** - `process_document()` calls `_mark_all_stale()`",
        "3. **Computing marks fresh** - Each `compute_*()` method calls `_mark_fresh()`",
        "4. **`compute_all()` recomputes only stale** - Checks each computation before running",
        "",
        "#### API Methods",
        "",
        "```python",
        "# Check if a computation is stale",
        "if processor.is_stale(processor.COMP_PAGERANK):",
        "    processor.compute_importance()",
        "",
        "# Get all stale computations",
        "stale = processor.get_stale_computations()",
        "# Returns: {'pagerank', 'tfidf', ...}",
        "```",
        "",
        "#### Incremental Updates",
        "",
        "`add_document_incremental()` is smarter - it can update TF-IDF without invalidating everything:",
        "",
        "```python",
        "# Only recomputes TF-IDF by default",
        "processor.add_document_incremental(doc_id, text, recompute='tfidf')",
        "",
        "# Recompute more",
        "processor.add_document_incremental(doc_id, text, recompute='all')",
        "",
        "# Don't recompute anything (fastest, but leaves data stale)",
        "processor.add_document_incremental(doc_id, text, recompute='none')",
        "```",
        "",
        "#### When to Check Staleness",
        "",
        "- **Before reading `col.pagerank`** - check `COMP_PAGERANK`",
        "- **Before reading `col.tfidf`** - check `COMP_TFIDF`",
        "- **Before using embeddings** - check `COMP_EMBEDDINGS`",
        "- **Before querying concepts** - check `COMP_CONCEPTS`",
        "",
        "#### Staleness After `load()`",
        "",
        "Loading a saved processor restores computation freshness state:",
        "```python",
        "processor = CorticalTextProcessor.load(\"corpus.pkl\")",
        "# Staleness state is preserved from when it was saved",
        "```",
        "",
        "### Return Value Semantics",
        "",
        "Understanding what functions return in edge cases prevents bugs and confusion.",
        "",
        "#### Edge Case Returns",
        "",
        "| Scenario | Return Value | Example Functions |",
        "|----------|--------------|-------------------|",
        "| Empty corpus | `[]` (empty list) | `find_documents_for_query()`, `find_passages_for_query()` |",
        "| No matches | `[]` (empty list) | `find_documents_for_query()`, `expand_query()` returns `{}` |",
        "| Unknown doc_id | `{}` (empty dict) | `get_document_metadata()` |",
        "| Unknown term | `None` | `layer.get_minicolumn()`, `layer.get_by_id()` |",
        "| Invalid layer | `KeyError` raised | `get_layer()` |",
        "| Empty query | `ValueError` raised | `find_documents_for_query()` |",
        "| Invalid top_n | `ValueError` raised | `find_documents_for_query()` |",
        "",
        "#### Score Ranges",
        "",
        "| Score Type | Range | Notes |",
        "|------------|-------|-------|",
        "| Relevance score | Unbounded (0+) | Sum of TF-IDF Ã— expansion weights |",
        "| PageRank | 0.0-1.0 | Normalized probability distribution |",
        "| TF-IDF | Unbounded (0+) | Higher = more distinctive |",
        "| Connection weight | Unbounded (0+) | Co-occurrence count or semantic weight |",
        "| Similarity | 0.0-1.0 | Cosine similarity, Jaccard, etc. |",
        "| Confidence | 0.0-1.0 | Relation extraction confidence |",
        "",
        "#### Lookup Functions: None vs Exception",
        "",
        "**Return `None` for missing items:**",
        "```python",
        "col = layer.get_minicolumn(\"nonexistent\")  # Returns None",
        "col = layer.get_by_id(\"L0_nonexistent\")    # Returns None",
        "```",
        "",
        "**Raise exception for invalid structure:**",
        "```python",
        "layer = processor.get_layer(CorticalLayer.TOKENS)  # OK",
        "layer = processor.get_layer(999)  # Raises KeyError",
        "```",
        "",
        "#### Default Parameter Values",
        "",
        "Key defaults to know:",
        "",
        "| Parameter | Default | In Function |",
        "|-----------|---------|-------------|",
        "| `top_n` | `5` | `find_documents_for_query()` |",
        "| `top_n` | `5` | `find_passages_for_query()` |",
        "| `max_expansions` | `10` | `expand_query()` |",
        "| `damping` | `0.85` | `compute_pagerank()` |",
        "| `resolution` | `1.0` | `build_concept_clusters()` |",
        "| `chunk_size` | `200` | `find_passages_for_query()` |",
        "| `chunk_overlap` | `50` | `find_passages_for_query()` |",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "# WRONG - may be using stale data",
        "if processor.is_stale(processor.COMP_PAGERANK):",
        "    # PageRank values may be outdated!",
        "    pass",
        "",
        "# CORRECT - ensure freshness",
        "if processor.is_stale(processor.COMP_PAGERANK):",
        "    processor.compute_importance()",
        "```",
        ""
      ],
      "context_after": [
        "---",
        "",
        "## Development Workflow",
        "",
        "### Before Writing Code",
        "",
        "1. **Read the relevant module** - understand existing patterns",
        "2. **Check TASK_LIST.md** - see if work is already planned/done",
        "3. **Run tests first** to establish baseline:",
        "   ```bash"
      ],
      "change_type": "add"
    },
    {
      "file": "README.md",
      "function": null,
      "start_line": 1,
      "lines_added": [
        "![Python 3.8+](https://img.shields.io/badge/python-3.8%2B-blue.svg)",
        "![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)",
        "![Tests](https://img.shields.io/badge/tests-1121%20passing-brightgreen.svg)",
        "![Coverage](https://img.shields.io/badge/coverage-%3E89%25-brightgreen.svg)",
        "![Zero Dependencies](https://img.shields.io/badge/dependencies-zero-orange.svg)",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "# Cortical Text Processor",
        ""
      ],
      "context_after": [
        "A neocortex-inspired text processing library with **zero external dependencies** for semantic analysis, document retrieval, and knowledge gap detection.",
        "",
        "---",
        "",
        "> *\"What if we built a text search engine the way evolution built a brain?\"*",
        "",
        "Your visual cortex doesn't grep through pixels looking for cats. It builds hierarchiesâ€”edges become patterns, patterns become shapes, shapes become objects. This library applies the same principle to text.",
        "",
        "Feed it documents. It tokenizes them into \"minicolumns\" (Layer 0), connects co-occurring words through Hebbian learning (\"neurons that fire together, wire together\"), clusters them into concepts (Layer 2), and links documents by shared meaning (Layer 3). The result: a graph that understands your corpus well enough to expand queries, complete analogies, and tell you where your knowledge has gaps.",
        ""
      ],
      "change_type": "add"
    },
    {
      "file": "README.md",
      "function": "The processor discovers that `neural` connects to `networks` (weight: 23), `arti",
      "start_line": 328,
      "lines_added": [
        "## Documentation",
        "",
        "Detailed documentation is available in the `docs/` directory:",
        "",
        "| Document | Description |",
        "|----------|-------------|",
        "| [docs/README.md](docs/README.md) | Documentation index with reading paths |",
        "| [docs/quickstart.md](docs/quickstart.md) | 5-minute getting started guide |",
        "| [docs/architecture.md](docs/architecture.md) | 4-layer system design |",
        "| [docs/algorithms.md](docs/algorithms.md) | Core IR algorithms (PageRank, TF-IDF, Louvain) |",
        "| [docs/query-guide.md](docs/query-guide.md) | Query formulation guide |",
        "| [docs/cookbook.md](docs/cookbook.md) | Common patterns and recipes |",
        "| [docs/glossary.md](docs/glossary.md) | Terminology definitions |",
        "",
        "For AI agents, see also [docs/claude-usage.md](docs/claude-usage.md) and [CLAUDE.md](CLAUDE.md).",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "```",
        "",
        "### The Polysemy Problem",
        "",
        "Search for \"candle sticks\" and you'll find `candlestick_patterns` (trading charts) at the topâ€”but also `letterpress_printing` (composing sticks) and `wine_tasting_vocabulary`. The query tokenizes to `['candle', 'sticks']`: \"candle\" matches the trading document (which discusses \"single candle patterns\"), while \"sticks\" matches the printing document. Classic information retrieval challenge: compound words fragment, partial matches surface, and the system can't read your mind about intent.",
        "",
        "### Knowledge Gap Detection",
        "",
        "The analyzer flags `sumo_wrestling` and `medieval_falconry` as isolated documentsâ€”they don't fit well with the rest of the corpus. It also identifies weak topics: terms like `patent` appear in only 1 document. This is how you find holes in your knowledge base.",
        ""
      ],
      "context_after": [
        "## Running Tests",
        "",
        "```bash",
        "python -m unittest discover -s tests -v",
        "```",
        "",
        "## License",
        "",
        "MIT License"
      ],
      "change_type": "add"
    },
    {
      "file": "TASK_LIST.md",
      "function": "Active backlog for the Cortical Text Processor project. Completed tasks are arch",
      "start_line": 11,
      "lines_added": [
        "*All high priority tasks completed!*"
      ],
      "lines_removed": [
        "| # | Task | Category | Depends | Effort |",
        "|---|------|----------|---------|--------|",
        "| 146 | Create behavioral tests for core user workflows | Testing | - | Medium |",
        "| 137 | Cap bigram connections to top-K per bigram | Perf | - | Small |",
        "| 139 | Batch bigram connection updates to reduce dict overhead | Perf | - | Small |",
        "| 91 | Create docs/README.md index | Docs | - | Small |",
        "| 92 | Add badges to README.md | DevEx | - | Small |",
        "| 93 | Update README with docs references | Docs | 91 | Small |",
        "| 96 | Centralize duplicate constants | CodeQual | - | Small |",
        "| 113 | Document staleness tracking system | AINav | - | Small |",
        "| 114 | Add type aliases for complex types | AINav | - | Small |",
        "| 116 | Document return value semantics | AINav | - | Medium |"
      ],
      "context_before": [
        "## Active Backlog",
        "",
        "<!-- Machine-parseable format for automation -->",
        "",
        "### ðŸ”´ Critical (Do Now)",
        "",
        "*All critical tasks completed!*",
        "",
        "### ðŸŸ  High (Do This Week)",
        ""
      ],
      "context_after": [
        "",
        "### ðŸŸ¡ Medium (Do This Month)",
        "",
        "| # | Task | Category | Depends | Effort |",
        "|---|------|----------|---------|--------|",
        "| 138 | Use sparse matrix multiplication for bigram connections | Perf | - | Medium |",
        "| 133 | Implement WAL + snapshot persistence (fault-tolerant rebuild) | Arch | 132 | Large |",
        "| 134 | Implement protobuf serialization for corpus | Arch | 132 | Medium |",
        "| 135 | Implement chunked parallel processing for full-analysis | Arch | 132 | Large |",
        "| 95 | Split processor.py into modules | Arch | 97 | Large |",
        "| 98 | Replace print() with logging | CodeQual | - | Medium |",
        "| 99 | Add input validation to public methods | CodeQual | - | Medium |",
        "| 102 | Add tests for edge cases | Testing | - | Medium |",
        "| 107 | Add Quick Context to tasks | TaskMgmt | - | Medium |",
        "| 115 | Create component interaction diagram | AINav | - | Medium |",
        "",
        "### ðŸŸ¢ Low (Backlog)",
        "",
        "| # | Task | Category | Depends | Effort |",
        "|---|------|----------|---------|--------|",
        "| 73 | Add \"Find Similar Code\" command | DevEx | - | Medium |",
        "| 74 | Add \"Explain This Code\" command | DevEx | - | Medium |",
        "| 75 | Add \"What Changed?\" semantic diff | DevEx | - | Large |",
        "| 76 | Add \"Suggest Related Files\" feature | DevEx | - | Medium |",
        "| 78 | Add code pattern detection | DevEx | - | Large |"
      ],
      "change_type": "modify"
    },
    {
      "file": "TASK_LIST.md",
      "function": "Active backlog for the Cortical Text Processor project. Completed tasks are arch",
      "start_line": 85,
      "lines_added": [
        "| 139 | Batch bigram connection updates to reduce dict overhead | 2025-12-12 | add_lateral_connections_batch() method in minicolumn.py |",
        "| 137 | Cap bigram connections to top-K per bigram | 2025-12-12 | max_connections_per_bigram parameter (default 50) in analysis.py |",
        "| 116 | Document return value semantics | 2025-12-12 | Edge cases, score ranges, None vs exceptions, default parameters |",
        "| 114 | Add type aliases for complex types | 2025-12-12 | cortical/types.py with 20+ aliases: DocumentScore, PassageResult, SemanticRelation, etc. |",
        "| 113 | Document staleness tracking system | 2025-12-12 | Comprehensive docs in CLAUDE.md: computation types, API, incremental updates |",
        "| 96 | Centralize duplicate constants | 2025-12-12 | cortical/constants.py with RELATION_WEIGHTS, DOC_TYPE_BOOSTS, query keywords |",
        "| 91 | Create docs/README.md index | 2025-12-12 | Navigation by audience, reading paths, categorized docs |",
        "| 92 | Add badges to README.md | 2025-12-12 | Python, License, Tests, Coverage, Zero Dependencies badges |",
        "| 93 | Update README with docs references | 2025-12-12 | Documentation section with table linking to docs/*.md |",
        "| 146 | Create behavioral tests for core user workflows | 2025-12-12 | 11 tests across 4 categories: Search, Performance, Quality, Robustness |"
      ],
      "lines_removed": [],
      "context_before": [
        "| # | Task | Started | Notes |",
        "|---|------|---------|-------|",
        "| 87 | Add Python code samples and showcase | 2025-12-11 | samples/*.py created |",
        "",
        "---",
        "",
        "## Recently Completed (Last 7 Days)",
        "",
        "| # | Task | Completed | Notes |",
        "|---|------|-----------|-------|"
      ],
      "context_after": [
        "| 145 | Improve graph embedding quality for common terms | 2025-12-12 | Added 'tfidf' method, IDF weighting to 'fast' method |",
        "| 143 | Investigate negative silhouette score in clustering | 2025-12-12 | Expected behavior: modularity â‰  silhouette (graph vs doc similarity) |",
        "| 142 | Investigate 74s compute_all() performance regression | 2025-12-12 | 5.2x speedup via fast embeddings + sampling (74s â†’ 14s) |",
        "| 144 | Boost exact document name matches in search | 2025-12-12 | doc_name_boost parameter in search functions |",
        "| 141 | Filter Python keywords/artifacts from analysis | 2025-12-12 | CODE_NOISE_TOKENS + filter_code_noise tokenizer option |",
        "| 94 | Split query.py into focused modules | 2025-12-12 | 8 modules: expansion, search, passages, chunking, intent, definitions, ranking, analogy |",
        "| 97 | Integrate CorticalConfig into processor | 2025-12-11 | Config stored on processor, used in method defaults, saved/loaded |",
        "| 127 | Create cluster coverage evaluation script | 2025-12-11 | scripts/evaluate_cluster.py with 24 tests |",
        "| 125 | Add clustering quality metrics (modularity, silhouette) | 2025-12-11 | compute_clustering_quality() in analysis.py, showcase display |",
        "| 124 | Add minimum cluster count regression tests | 2025-12-11 | 4 new tests: coherence, showcase count, mega-cluster, distribution |"
      ],
      "change_type": "add"
    },
    {
      "file": "cortical/analysis.py",
      "function": "Contains implementations of:",
      "start_line": 11,
      "lines_added": [
        "from .constants import RELATION_WEIGHTS"
      ],
      "lines_removed": [],
      "context_before": [
        "- Label propagation for clustering (legacy, for backward compatibility)",
        "- Activation propagation for information flow",
        "\"\"\"",
        "",
        "import math",
        "from typing import Dict, List, Tuple, Set, Optional, Any",
        "from collections import defaultdict",
        "",
        "from .layers import CorticalLayer, HierarchicalLayer",
        "from .minicolumn import Minicolumn"
      ],
      "context_after": [
        "",
        "",
        "def compute_pagerank(",
        "    layer: HierarchicalLayer,",
        "    damping: float = 0.85,",
        "    iterations: int = 20,",
        "    tolerance: float = 1e-6",
        ") -> Dict[str, float]:",
        "    \"\"\"",
        "    Compute PageRank scores for minicolumns in a layer."
      ],
      "change_type": "add"
    },
    {
      "file": "cortical/analysis.py",
      "function": "def compute_pagerank(",
      "start_line": 89,
      "lines_added": [
        "# RELATION_WEIGHTS imported from constants.py"
      ],
      "lines_removed": [
        "# Default relation weights for semantic PageRank",
        "RELATION_WEIGHTS = {",
        "    'IsA': 1.5,           # Hypernym relationships are strong",
        "    'PartOf': 1.3,        # Meronym relationships",
        "    'HasProperty': 1.2,   # Property associations",
        "    'RelatedTo': 1.0,     # Default co-occurrence",
        "    'SimilarTo': 1.4,     # Similarity relationships",
        "    'Causes': 1.1,        # Causal relationships",
        "    'UsedFor': 1.0,       # Functional relationships",
        "    'CoOccurs': 0.8,      # Basic co-occurrence",
        "    'Antonym': 0.3,       # Opposing concepts (lower weight)",
        "    'DerivedFrom': 1.2,   # Morphological derivation",
        "}"
      ],
      "context_before": [
        "        if max_diff < tolerance:",
        "            break",
        "",
        "    # Update minicolumn pagerank values",
        "    for col in layer.minicolumns.values():",
        "        col.pagerank = pagerank.get(col.id, 1.0 / n)",
        "",
        "    return pagerank",
        "",
        ""
      ],
      "context_after": [
        "",
        "",
        "def compute_semantic_pagerank(",
        "    layer: HierarchicalLayer,",
        "    semantic_relations: List[Tuple[str, str, str, float]],",
        "    relation_weights: Optional[Dict[str, float]] = None,",
        "    damping: float = 0.85,",
        "    iterations: int = 20,",
        "    tolerance: float = 1e-6",
        ") -> Dict[str, Any]:"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/analysis.py",
      "function": "def compute_concept_connections(",
      "start_line": 1213,
      "lines_added": [
        "    max_bigrams_per_doc: int = 500,",
        "    max_connections_per_bigram: int = 50",
        "        max_connections_per_bigram: Maximum lateral connections per bigram minicolumn",
        "            to keep graph sparse and focused on strongest connections (default 50)",
        "        - skipped_max_connections: Number of connections skipped due to per-bigram limit",
        "            'skipped_large_docs': 0,",
        "            'skipped_max_connections': 0"
      ],
      "lines_removed": [
        "    max_bigrams_per_doc: int = 500",
        "            'skipped_large_docs': 0"
      ],
      "context_before": [
        "    }",
        "",
        "",
        "def compute_bigram_connections(",
        "    layers: Dict[CorticalLayer, HierarchicalLayer],",
        "    min_shared_docs: int = 1,",
        "    component_weight: float = 0.5,",
        "    chain_weight: float = 0.7,",
        "    cooccurrence_weight: float = 0.3,",
        "    max_bigrams_per_term: int = 100,"
      ],
      "context_after": [
        ") -> Dict[str, Any]:",
        "    \"\"\"",
        "    Build lateral connections between bigrams in Layer 1.",
        "",
        "    Bigrams are connected based on:",
        "    1. Shared component terms (\"neural_networks\" â†” \"neural_processing\")",
        "    2. Document co-occurrence (appear in same documents)",
        "    3. Chains (\"machine_learning\" â†” \"learning_algorithms\" where right=left)",
        "",
        "    Args:",
        "        layers: Dictionary of all layers",
        "        min_shared_docs: Minimum shared documents for co-occurrence connection",
        "        component_weight: Weight for shared component connections (default 0.5)",
        "        chain_weight: Weight for chain connections (default 0.7)",
        "        cooccurrence_weight: Weight for document co-occurrence (default 0.3)",
        "        max_bigrams_per_term: Skip terms appearing in more than this many bigrams",
        "            to avoid O(nÂ²) explosion from common terms like \"self\", \"return\" (default 100)",
        "        max_bigrams_per_doc: Skip documents with more than this many bigrams for",
        "            co-occurrence connections to avoid O(nÂ²) explosion (default 500)",
        "",
        "    Returns:",
        "        Statistics about connections created:",
        "        - connections_created: Total bidirectional connections",
        "        - component_connections: Connections from shared components",
        "        - chain_connections: Connections from chains",
        "        - cooccurrence_connections: Connections from document co-occurrence",
        "        - skipped_common_terms: Number of terms skipped due to max_bigrams_per_term",
        "        - skipped_large_docs: Number of docs skipped due to max_bigrams_per_doc",
        "    \"\"\"",
        "    layer1 = layers[CorticalLayer.BIGRAMS]",
        "",
        "    if layer1.column_count() == 0:",
        "        return {",
        "            'connections_created': 0,",
        "            'bigrams': 0,",
        "            'component_connections': 0,",
        "            'chain_connections': 0,",
        "            'cooccurrence_connections': 0,",
        "            'skipped_common_terms': 0,",
        "        }",
        "",
        "    bigrams = list(layer1.minicolumns.values())",
        "",
        "    # Build indexes for efficient lookup",
        "    # left_component_index: {\"neural\": [bigram1, bigram2, ...]}",
        "    # right_component_index: {\"networks\": [bigram1, bigram3, ...]}",
        "    # Note: Bigrams use space separators (e.g., \"neural networks\")",
        "    left_index: Dict[str, List[Minicolumn]] = defaultdict(list)",
        "    right_index: Dict[str, List[Minicolumn]] = defaultdict(list)"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/analysis.py",
      "function": "def compute_bigram_connections(",
      "start_line": 1275,
      "lines_added": [
        "    skipped_max_connections = 0",
        "    # Track connection count per bigram to enforce max_connections_per_bigram",
        "    connection_counts: Dict[str, int] = defaultdict(int)",
        "",
        "        \"\"\"Add bidirectional connection if not already connected and under limit.\"\"\"",
        "        nonlocal component_connections, chain_connections, cooccurrence_connections, skipped_max_connections",
        "        # Check if either bigram has reached its connection limit",
        "        if (connection_counts[b1.id] >= max_connections_per_bigram or",
        "            connection_counts[b2.id] >= max_connections_per_bigram):",
        "            skipped_max_connections += 1",
        "            return False",
        "",
        "        connection_counts[b1.id] += 1",
        "        connection_counts[b2.id] += 1"
      ],
      "lines_removed": [
        "        \"\"\"Add bidirectional connection if not already connected.\"\"\"",
        "        nonlocal component_connections, chain_connections, cooccurrence_connections"
      ],
      "context_before": [
        "    for bigram in bigrams:",
        "        parts = bigram.content.split(' ')",
        "        if len(parts) == 2:",
        "            left_index[parts[0]].append(bigram)",
        "            right_index[parts[1]].append(bigram)",
        "",
        "    # Track connection types for statistics",
        "    component_connections = 0",
        "    chain_connections = 0",
        "    cooccurrence_connections = 0"
      ],
      "context_after": [
        "",
        "    # Track which pairs we've already connected (avoid duplicates)",
        "    connected_pairs: Set[Tuple[str, str]] = set()",
        "",
        "    def add_connection(b1: Minicolumn, b2: Minicolumn, weight: float, conn_type: str) -> bool:",
        "",
        "        pair = tuple(sorted([b1.id, b2.id]))",
        "        if pair in connected_pairs:",
        "            # Already connected, just strengthen the connection",
        "            b1.add_lateral_connection(b2.id, weight)",
        "            b2.add_lateral_connection(b1.id, weight)",
        "            return False",
        "",
        "        connected_pairs.add(pair)",
        "        b1.add_lateral_connection(b2.id, weight)",
        "        b2.add_lateral_connection(b1.id, weight)",
        "",
        "        if conn_type == 'component':",
        "            component_connections += 1",
        "        elif conn_type == 'chain':",
        "            chain_connections += 1",
        "        elif conn_type == 'cooccurrence':",
        "            cooccurrence_connections += 1",
        "",
        "        return True",
        ""
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/analysis.py",
      "function": "def compute_bigram_connections(",
      "start_line": 1385,
      "lines_added": [
        "        'skipped_large_docs': skipped_large_docs,",
        "        'skipped_max_connections': skipped_max_connections"
      ],
      "lines_removed": [
        "        'skipped_large_docs': skipped_large_docs"
      ],
      "context_before": [
        "                    weight = cooccurrence_weight * jaccard",
        "                    add_connection(b1, b2, weight, 'cooccurrence')",
        "",
        "    return {",
        "        'connections_created': len(connected_pairs),",
        "        'bigrams': len(bigrams),",
        "        'component_connections': component_connections,",
        "        'chain_connections': chain_connections,",
        "        'cooccurrence_connections': cooccurrence_connections,",
        "        'skipped_common_terms': skipped_common_terms,"
      ],
      "context_after": [
        "    }",
        "",
        "",
        "def compute_document_connections(",
        "    layers: Dict[CorticalLayer, HierarchicalLayer],",
        "    documents: Dict[str, str],",
        "    min_shared_terms: int = 3",
        ") -> None:",
        "    \"\"\"",
        "    Build lateral connections between documents."
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/constants.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "\"\"\"",
        "Centralized constants for the Cortical Text Processor.",
        "",
        "This module provides a single source of truth for constants used across",
        "multiple modules, preventing drift and inconsistencies.",
        "",
        "Task #96: Centralize duplicate constants",
        "\"\"\"",
        "",
        "from typing import Dict, FrozenSet",
        "",
        "# =============================================================================",
        "# RELATION TYPE WEIGHTS",
        "# =============================================================================",
        "",
        "# Weights for semantic relation types used in:",
        "# - PageRank computation (analysis.py)",
        "# - Semantic retrofitting (semantics.py)",
        "# - Query expansion (query/expansion.py)",
        "#",
        "# Higher values = stronger connections in the knowledge graph.",
        "# These are tuned based on ConceptNet-style relation semantics.",
        "",
        "RELATION_WEIGHTS: Dict[str, float] = {",
        "    # Strong semantic relationships",
        "    'SameAs': 2.0,          # Synonymy - strongest connection",
        "    'IsA': 1.5,             # Hypernym/type relationships",
        "    'SimilarTo': 1.5,       # High similarity",
        "",
        "    # Structural relationships",
        "    'PartOf': 1.3,          # Meronym relationships",
        "    'HasA': 1.2,            # Possession relationships",
        "    'HasProperty': 1.2,     # Property associations",
        "    'DerivedFrom': 1.2,     # Morphological derivation",
        "",
        "    # Causal and functional",
        "    'Causes': 1.1,          # Causal relationships",
        "    'UsedFor': 1.0,         # Functional relationships",
        "    'CapableOf': 0.9,       # Capability relationships",
        "    'DefinedBy': 1.0,       # Definition relationships",
        "",
        "    # Co-occurrence and spatial",
        "    'RelatedTo': 0.8,       # General relatedness",
        "    'CoOccurs': 0.7,        # Basic co-occurrence",
        "    'AtLocation': 0.6,      # Spatial relationships",
        "",
        "    # Negative/opposing",
        "    'Antonym': -0.5,        # Opposing concepts (negative weight)",
        "}",
        "",
        "",
        "# =============================================================================",
        "# DOCUMENT TYPE BOOSTS",
        "# =============================================================================",
        "",
        "# Boost factors for ranking documents by type in search results.",
        "# Used in query/ranking.py for multi_stage_rank().",
        "# Higher values = ranked higher in results.",
        "",
        "DOC_TYPE_BOOSTS: Dict[str, float] = {",
        "    'docs': 1.5,            # docs/ folder documentation",
        "    'root_docs': 1.3,       # Root-level markdown (CLAUDE.md, README.md)",
        "    'code': 1.0,            # Regular code files",
        "    'test': 0.8,            # Test files (often less relevant for conceptual queries)",
        "}",
        "",
        "",
        "# =============================================================================",
        "# QUERY TYPE KEYWORDS",
        "# =============================================================================",
        "",
        "# Keywords that suggest a conceptual query (should boost documentation)",
        "CONCEPTUAL_KEYWORDS: FrozenSet[str] = frozenset([",
        "    'what', 'explain', 'describe', 'overview', 'introduction', 'concept',",
        "    'architecture', 'design', 'pattern', 'algorithm', 'approach', 'method',",
        "    'how does', 'why does', 'purpose', 'goal', 'rationale', 'theory',",
        "    'understand', 'learn', 'documentation', 'guide', 'tutorial', 'example',",
        "])",
        "",
        "# Keywords that suggest an implementation query (should prefer code)",
        "IMPLEMENTATION_KEYWORDS: FrozenSet[str] = frozenset([",
        "    'where', 'implement', 'code', 'function', 'class', 'method', 'variable',",
        "    'line', 'file', 'bug', 'fix', 'error', 'exception', 'call', 'invoke',",
        "    'compute', 'calculate', 'return', 'parameter', 'argument',",
        "])",
        "",
        "",
        "# NOTE: LAYER_COLORS and LAYER_NAMES are defined in persistence.py",
        "# because they use CorticalLayer enum keys for type safety in exports."
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "cortical/minicolumn.py",
      "function": "class Minicolumn:",
      "start_line": 132,
      "lines_added": [
        "    def add_lateral_connections_batch(self, connections: Dict[str, float]) -> None:",
        "        \"\"\"",
        "        Add or strengthen multiple lateral connections at once.",
        "",
        "        More efficient than calling add_lateral_connection() in a loop",
        "        because it reduces function call overhead.",
        "",
        "        Args:",
        "            connections: Dictionary mapping target_id to weight to add",
        "        \"\"\"",
        "        lateral = self.lateral_connections",
        "        for target_id, weight in connections.items():",
        "            lateral[target_id] = lateral.get(target_id, 0) + weight",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "        wire together\").",
        "",
        "        Args:",
        "            target_id: ID of the target minicolumn",
        "            weight: Connection strength to add",
        "        \"\"\"",
        "        self.lateral_connections[target_id] = (",
        "            self.lateral_connections.get(target_id, 0) + weight",
        "        )",
        ""
      ],
      "context_after": [
        "    def add_typed_connection(",
        "        self,",
        "        target_id: str,",
        "        weight: float = 1.0,",
        "        relation_type: str = 'co_occurrence',",
        "        confidence: float = 1.0,",
        "        source: str = 'corpus'",
        "    ) -> None:",
        "        \"\"\"",
        "        Add or update a typed connection with metadata."
      ],
      "change_type": "add"
    },
    {
      "file": "cortical/processor.py",
      "function": "class CorticalTextProcessor:",
      "start_line": 904,
      "lines_added": [
        "        max_connections_per_bigram: int = 50,",
        "            max_connections_per_bigram: Maximum lateral connections per bigram minicolumn",
        "                to keep graph sparse and focused on strongest connections (default 50)",
        "            - skipped_max_connections: Number of connections skipped due to per-bigram limit",
        "            max_bigrams_per_doc=max_bigrams_per_doc,",
        "            max_connections_per_bigram=max_connections_per_bigram",
        "            skipped_conns = stats.get('skipped_max_connections', 0)",
        "            if skipped_conns:",
        "                skip_parts.append(f\"{skipped_conns} over-limit\")"
      ],
      "lines_removed": [
        "            max_bigrams_per_doc=max_bigrams_per_doc"
      ],
      "context_before": [
        "        if verbose: print(\"Computed document connections\")",
        "",
        "    def compute_bigram_connections(",
        "        self,",
        "        min_shared_docs: int = 1,",
        "        component_weight: float = 0.5,",
        "        chain_weight: float = 0.7,",
        "        cooccurrence_weight: float = 0.3,",
        "        max_bigrams_per_term: int = 100,",
        "        max_bigrams_per_doc: int = 500,"
      ],
      "context_after": [
        "        verbose: bool = True",
        "    ) -> Dict[str, Any]:",
        "        \"\"\"",
        "        Build lateral connections between bigrams based on shared components and co-occurrence.",
        "",
        "        Bigrams are connected when they:",
        "        - Share a component term (\"neural_networks\" â†” \"neural_processing\")",
        "        - Form chains (\"machine_learning\" â†” \"learning_algorithms\")",
        "        - Co-occur in the same documents",
        "",
        "        Args:",
        "            min_shared_docs: Minimum shared documents for co-occurrence connection",
        "            component_weight: Weight for shared component connections (default 0.5)",
        "            chain_weight: Weight for chain connections (default 0.7)",
        "            cooccurrence_weight: Weight for document co-occurrence (default 0.3)",
        "            max_bigrams_per_term: Skip terms appearing in more than this many bigrams",
        "                to avoid O(nÂ²) explosion from common terms like \"self\", \"return\" (default 100)",
        "            max_bigrams_per_doc: Skip documents with more than this many bigrams for",
        "                co-occurrence connections to avoid O(nÂ²) explosion (default 500)",
        "            verbose: Print progress messages",
        "",
        "        Returns:",
        "            Statistics about connections created:",
        "            - connections_created: Total bidirectional connections",
        "            - component_connections: Connections from shared components",
        "            - chain_connections: Connections from chains",
        "            - cooccurrence_connections: Connections from document co-occurrence",
        "            - skipped_common_terms: Number of terms skipped due to max_bigrams_per_term",
        "            - skipped_large_docs: Number of docs skipped due to max_bigrams_per_doc",
        "",
        "        Example:",
        "            >>> stats = processor.compute_bigram_connections()",
        "            >>> print(f\"Created {stats['connections_created']} bigram connections\")",
        "            >>> print(f\"  Component: {stats['component_connections']}\")",
        "            >>> print(f\"  Chain: {stats['chain_connections']}\")",
        "            >>> print(f\"  Co-occurrence: {stats['cooccurrence_connections']}\")",
        "        \"\"\"",
        "        stats = analysis.compute_bigram_connections(",
        "            self.layers,",
        "            min_shared_docs=min_shared_docs,",
        "            component_weight=component_weight,",
        "            chain_weight=chain_weight,",
        "            cooccurrence_weight=cooccurrence_weight,",
        "            max_bigrams_per_term=max_bigrams_per_term,",
        "        )",
        "        if verbose:",
        "            skipped_terms = stats.get('skipped_common_terms', 0)",
        "            skipped_docs = stats.get('skipped_large_docs', 0)",
        "            skip_parts = []",
        "            if skipped_terms:",
        "                skip_parts.append(f\"{skipped_terms} common terms\")",
        "            if skipped_docs:",
        "                skip_parts.append(f\"{skipped_docs} large docs\")",
        "            skip_msg = f\", skipped {', '.join(skip_parts)}\" if skip_parts else \"\"",
        "            print(f\"Created {stats['connections_created']} bigram connections \"",
        "                  f\"(component: {stats['component_connections']}, \"",
        "                  f\"chain: {stats['chain_connections']}, \"",
        "                  f\"cooccur: {stats['cooccurrence_connections']}{skip_msg})\")",
        "        return stats",
        "",
        "    def build_concept_clusters(",
        "        self,",
        "        min_cluster_size: Optional[int] = None,"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/query/ranking.py",
      "function": "This module provides:",
      "start_line": 8,
      "lines_added": [
        "from ..constants import DOC_TYPE_BOOSTS, CONCEPTUAL_KEYWORDS, IMPLEMENTATION_KEYWORDS",
        "# Constants imported from cortical/constants.py"
      ],
      "lines_removed": [
        "# Default boost factors for each document type",
        "# Higher values make documents of that type rank higher",
        "DOC_TYPE_BOOSTS = {",
        "    'docs': 1.5,       # docs/ folder documentation",
        "    'root_docs': 1.3,  # Root-level markdown (CLAUDE.md, README.md)",
        "    'code': 1.0,       # Regular code files",
        "    'test': 0.8,       # Test files (often less relevant for conceptual queries)",
        "}",
        "",
        "# Keywords that suggest a conceptual query (should boost documentation)",
        "CONCEPTUAL_KEYWORDS = frozenset([",
        "    'what', 'explain', 'describe', 'overview', 'introduction', 'concept',",
        "    'architecture', 'design', 'pattern', 'algorithm', 'approach', 'method',",
        "    'how does', 'why does', 'purpose', 'goal', 'rationale', 'theory',",
        "    'understand', 'learn', 'documentation', 'guide', 'tutorial', 'example',",
        "])",
        "",
        "# Keywords that suggest an implementation query (should prefer code)",
        "IMPLEMENTATION_KEYWORDS = frozenset([",
        "    'where', 'implement', 'code', 'function', 'class', 'method', 'variable',",
        "    'line', 'file', 'bug', 'fix', 'error', 'exception', 'call', 'invoke',",
        "    'compute', 'calculate', 'return', 'parameter', 'argument',",
        "])"
      ],
      "context_before": [
        "- Document type boosting (docs, code, tests)",
        "- Conceptual vs implementation query detection",
        "- Multi-stage ranking pipeline (concepts -> documents -> chunks)",
        "\"\"\"",
        "",
        "from typing import Dict, List, Tuple, Optional, Any",
        "from collections import defaultdict",
        "",
        "from ..layers import CorticalLayer, HierarchicalLayer",
        "from ..tokenizer import Tokenizer"
      ],
      "context_after": [
        "",
        "from .expansion import get_expanded_query_terms",
        "from .search import find_documents_for_query",
        "",
        "",
        "",
        "",
        "def is_conceptual_query(query_text: str) -> bool:",
        "    \"\"\"",
        "    Determine if a query is conceptual (should boost documentation).",
        "",
        "    Conceptual queries ask about concepts, architecture, design, or",
        "    explanations rather than specific code locations.",
        "",
        "    Args:"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/semantics.py",
      "function": "import copy",
      "start_line": 16,
      "lines_added": [
        "from .constants import RELATION_WEIGHTS"
      ],
      "lines_removed": [
        "",
        "",
        "# Relation type weights for retrofitting",
        "RELATION_WEIGHTS = {",
        "    'IsA': 1.5,",
        "    'PartOf': 1.2,",
        "    'HasA': 1.0,",
        "    'UsedFor': 0.8,",
        "    'CapableOf': 0.7,",
        "    'AtLocation': 0.6,",
        "    'Causes': 0.9,",
        "    'HasProperty': 0.8,",
        "    'SameAs': 2.0,",
        "    'RelatedTo': 0.5,",
        "    'Antonym': -0.5,",
        "    'DerivedFrom': 1.0,",
        "    'SimilarTo': 1.5,",
        "    'CoOccurs': 0.6,",
        "    'DefinedBy': 1.0,",
        "}"
      ],
      "context_before": [
        "from collections import defaultdict",
        "",
        "try:",
        "    import numpy as np",
        "    HAS_NUMPY = True",
        "except ImportError:",
        "    HAS_NUMPY = False",
        "",
        "from .layers import CorticalLayer, HierarchicalLayer",
        "from .minicolumn import Minicolumn"
      ],
      "context_after": [
        "",
        "",
        "# Commonsense relation patterns with confidence scores",
        "# Format: (pattern_regex, relation_type, confidence, swap_order)",
        "# swap_order: if True, the captured groups are in reverse order (t2, t1)",
        "RELATION_PATTERNS = [",
        "    # IsA patterns (hypernym/type relations)",
        "    (r'(\\w+)\\s+(?:is|are)\\s+(?:a|an)\\s+(?:type\\s+of\\s+)?(\\w+)', 'IsA', 0.9, False),",
        "    (r'(\\w+),?\\s+(?:a|an)\\s+(?:kind|type|form)\\s+of\\s+(\\w+)', 'IsA', 0.95, False),",
        "    (r'(\\w+)\\s+(?:is|are)\\s+considered\\s+(?:a|an)?\\s*(\\w+)', 'IsA', 0.8, False),"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/types.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "\"\"\"",
        "Type Aliases for the Cortical Text Processor.",
        "",
        "This module provides type aliases for complex return types used throughout",
        "the library, making function signatures more readable and maintainable.",
        "",
        "Task #114: Add type aliases for complex types",
        "",
        "Usage:",
        "    from cortical.types import DocumentScore, PassageResult, SemanticRelation",
        "",
        "Example:",
        "    def find_documents(query: str) -> DocumentResults:",
        "        ...",
        "        return results  # List of (doc_id, score) tuples",
        "\"\"\"",
        "",
        "from typing import Any, Dict, List, Optional, Tuple",
        "",
        "# =============================================================================",
        "# SCORE TYPES",
        "# =============================================================================",
        "",
        "# Basic score tuple: (item_id, score)",
        "DocumentScore = Tuple[str, float]",
        "\"\"\"A (doc_id, score) tuple representing a document with its relevance score.\"\"\"",
        "",
        "TermScore = Tuple[str, float]",
        "\"\"\"A (term, score) tuple representing a term with its importance score.\"\"\"",
        "",
        "# Result lists",
        "DocumentResults = List[DocumentScore]",
        "\"\"\"List of (doc_id, score) tuples, typically sorted by relevance.\"\"\"",
        "",
        "TermResults = List[TermScore]",
        "\"\"\"List of (term, score) tuples, typically sorted by importance.\"\"\"",
        "",
        "",
        "# =============================================================================",
        "# PASSAGE TYPES",
        "# =============================================================================",
        "",
        "PassageResult = Tuple[str, float, str]",
        "\"\"\"A (doc_id, score, passage_text) tuple for chunk-level retrieval.\"\"\"",
        "",
        "PassageResults = List[PassageResult]",
        "\"\"\"List of (doc_id, score, passage_text) tuples for RAG applications.\"\"\"",
        "",
        "# Passage with position information",
        "PassageWithPosition = Tuple[str, str, int, int, float]",
        "\"\"\"A (doc_id, passage_text, start_char, end_char, score) tuple.\"\"\"",
        "",
        "PassageWithPositionResults = List[PassageWithPosition]",
        "\"\"\"List of passages with character position information.\"\"\"",
        "",
        "# Passage with expanded terms",
        "PassageWithExpansion = Tuple[str, str, int, int, float, Dict[str, float]]",
        "\"\"\"A (doc_id, passage_text, start, end, score, expanded_terms) tuple.\"\"\"",
        "",
        "PassageWithExpansionResults = List[PassageWithExpansion]",
        "\"\"\"List of passages with the query expansion used to find them.\"\"\"",
        "",
        "",
        "# =============================================================================",
        "# SEMANTIC RELATION TYPES",
        "# =============================================================================",
        "",
        "SemanticRelation = Tuple[str, str, str, float]",
        "\"\"\"A (term1, relation_type, term2, confidence) semantic relation tuple.",
        "",
        "Example: ('dog', 'IsA', 'animal', 0.95)",
        "\"\"\"",
        "",
        "SemanticRelations = List[SemanticRelation]",
        "\"\"\"List of semantic relation tuples extracted from the corpus.\"\"\"",
        "",
        "",
        "# =============================================================================",
        "# EMBEDDING TYPES",
        "# =============================================================================",
        "",
        "EmbeddingVector = List[float]",
        "\"\"\"A dense vector representation of a term.\"\"\"",
        "",
        "EmbeddingDict = Dict[str, EmbeddingVector]",
        "\"\"\"Dictionary mapping terms to their embedding vectors.\"\"\"",
        "",
        "",
        "# =============================================================================",
        "# METADATA TYPES",
        "# =============================================================================",
        "",
        "DocumentMetadata = Dict[str, Any]",
        "\"\"\"Arbitrary metadata associated with a document.\"\"\"",
        "",
        "AllDocumentMetadata = Dict[str, DocumentMetadata]",
        "\"\"\"Dictionary mapping doc_ids to their metadata.\"\"\"",
        "",
        "",
        "# =============================================================================",
        "# GRAPH TYPES",
        "# =============================================================================",
        "",
        "ConnectionWeight = float",
        "\"\"\"Weight of a connection between minicolumns.\"\"\"",
        "",
        "ConnectionMap = Dict[str, ConnectionWeight]",
        "\"\"\"Dictionary mapping target_ids to connection weights.\"\"\"",
        "",
        "IncomingConnections = Dict[str, List[Tuple[str, float]]]",
        "\"\"\"Dictionary mapping node_ids to list of (source_id, weight) incoming edges.\"\"\"",
        "",
        "",
        "# =============================================================================",
        "# INTENT QUERY TYPES",
        "# =============================================================================",
        "",
        "IntentResult = Tuple[str, float, Dict[str, Any]]",
        "\"\"\"A (doc_id, score, intent_info) tuple from intent-based search.\"\"\"",
        "",
        "IntentResults = List[IntentResult]",
        "\"\"\"List of intent-based search results with metadata.\"\"\"",
        "",
        "",
        "# =============================================================================",
        "# BATCH TYPES",
        "# =============================================================================",
        "",
        "DocumentInput = Tuple[str, str, Optional[Dict[str, Any]]]",
        "\"\"\"A (doc_id, content, metadata) tuple for batch document processing.\"\"\"",
        "",
        "DocumentBatch = List[DocumentInput]",
        "\"\"\"List of documents to process in batch.\"\"\"",
        "",
        "BatchResults = List[DocumentResults]",
        "\"\"\"Results from batch query processing - one DocumentResults per query.\"\"\"",
        "",
        "BatchPassageResults = List[PassageWithPositionResults]",
        "\"\"\"Results from batch passage retrieval - one PassageResults per query.\"\"\"",
        "",
        "",
        "# =============================================================================",
        "# SEARCH INDEX TYPES",
        "# =============================================================================",
        "",
        "SearchIndex = Dict[str, Dict[str, float]]",
        "\"\"\"Pre-built search index mapping terms to doc_id -> score dictionaries.\"\"\"",
        "",
        "TermDocScores = Dict[str, float]",
        "\"\"\"Dictionary mapping doc_ids to scores for a single term.\"\"\"",
        "",
        "",
        "# =============================================================================",
        "# CLUSTER TYPES",
        "# =============================================================================",
        "",
        "ClusterAssignments = Dict[str, str]",
        "\"\"\"Dictionary mapping term/node content to cluster_id.\"\"\"",
        "",
        "ClusterQuality = Dict[str, Any]",
        "\"\"\"Dictionary with clustering quality metrics (modularity, silhouette, etc.).\"\"\""
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "docs/README.md",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "# Documentation Index",
        "",
        "Welcome to the Cortical Text Processor documentation. This index provides navigation and recommended reading paths for different audiences.",
        "",
        "---",
        "",
        "## Quick Navigation",
        "",
        "| Document | Description | Audience |",
        "|----------|-------------|----------|",
        "| [quickstart.md](quickstart.md) | 5-minute getting started guide | New users |",
        "| [architecture.md](architecture.md) | 4-layer hierarchical system design | All |",
        "| [algorithms.md](algorithms.md) | Core IR algorithms (PageRank, TF-IDF, Louvain) | Developers |",
        "| [query-guide.md](query-guide.md) | How to formulate effective queries | Users |",
        "| [cookbook.md](cookbook.md) | Common patterns and recipes | Users |",
        "| [patterns.md](patterns.md) | Advanced usage (code search, fingerprinting) | Advanced users |",
        "| [glossary.md](glossary.md) | Terminology definitions | All |",
        "",
        "---",
        "",
        "## Reading Paths",
        "",
        "### New Users",
        "",
        "1. **[quickstart.md](quickstart.md)** - Get running in 5 minutes",
        "2. **[query-guide.md](query-guide.md)** - Learn to search effectively",
        "3. **[cookbook.md](cookbook.md)** - See common patterns",
        "",
        "### Contributors",
        "",
        "1. **[quickstart.md](quickstart.md)** - Understand basic usage",
        "2. **[architecture.md](architecture.md)** - Understand the 4-layer design",
        "3. **[algorithms.md](algorithms.md)** - Learn the core algorithms",
        "4. **[code-of-ethics.md](code-of-ethics.md)** - Development standards",
        "5. **[definition-of-done.md](definition-of-done.md)** - Task completion criteria",
        "6. **[dogfooding-checklist.md](dogfooding-checklist.md)** - Testing with real usage",
        "",
        "### AI Agents (Claude)",
        "",
        "1. **[claude-usage.md](claude-usage.md)** - AI-specific search guidance",
        "2. **[cli-wrapper-guide.md](cli-wrapper-guide.md)** - CLI wrapper reference",
        "3. **[architecture.md](architecture.md)** - System structure",
        "4. **[glossary.md](glossary.md)** - Terminology reference",
        "",
        "---",
        "",
        "## Document Categories",
        "",
        "### Getting Started",
        "",
        "| Document | Purpose |",
        "|----------|---------|",
        "| [quickstart.md](quickstart.md) | 5-minute introduction |",
        "| [glossary.md](glossary.md) | Key terminology definitions |",
        "",
        "### Architecture & Algorithms",
        "",
        "| Document | Purpose |",
        "|----------|---------|",
        "| [architecture.md](architecture.md) | 4-layer system design (Tokens â†’ Bigrams â†’ Concepts â†’ Documents) |",
        "| [algorithms.md](algorithms.md) | PageRank, TF-IDF, Louvain clustering, co-occurrence |",
        "| [louvain_resolution_analysis.md](louvain_resolution_analysis.md) | Research on clustering resolution parameter |",
        "",
        "### Usage Guides",
        "",
        "| Document | Purpose |",
        "|----------|---------|",
        "| [query-guide.md](query-guide.md) | Query formulation and search tips |",
        "| [cookbook.md](cookbook.md) | Common patterns and recipes |",
        "| [patterns.md](patterns.md) | Advanced patterns: code search, fingerprinting, intent queries |",
        "",
        "### Development Process",
        "",
        "| Document | Purpose |",
        "|----------|---------|",
        "| [code-of-ethics.md](code-of-ethics.md) | Scientific rigor and documentation standards |",
        "| [definition-of-done.md](definition-of-done.md) | Task completion checklist |",
        "| [dogfooding.md](dogfooding.md) | Using the system to test itself |",
        "| [dogfooding-checklist.md](dogfooding-checklist.md) | Systematic dog-fooding checklist |",
        "",
        "### AI Agent Resources",
        "",
        "| Document | Purpose |",
        "|----------|---------|",
        "| [claude-usage.md](claude-usage.md) | Guide for Claude agents using the system |",
        "| [cli-wrapper-guide.md](cli-wrapper-guide.md) | CLI wrapper for AI assistants |",
        "",
        "---",
        "",
        "## Additional Resources",
        "",
        "- **[CLAUDE.md](../CLAUDE.md)** - Main development guide (in repo root)",
        "- **[CONTRIBUTING.md](../CONTRIBUTING.md)** - How to contribute",
        "- **[TASK_LIST.md](../TASK_LIST.md)** - Active task backlog",
        "- **[README.md](../README.md)** - Project overview",
        "",
        "---",
        "",
        "*Last updated: 2025-12-12*"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "tests/test_semantics.py",
      "function": "class TestSemantics(unittest.TestCase):",
      "start_line": 157,
      "lines_added": [
        "        self.assertEqual(get_relation_type_weight('RelatedTo'), 0.8)  # Centralized in constants.py"
      ],
      "lines_removed": [
        "        self.assertEqual(get_relation_type_weight('RelatedTo'), 0.5)"
      ],
      "context_before": [
        "",
        "        self.assertEqual(stats['iterations'], 5)",
        "        self.assertEqual(stats['alpha'], 0.4)",
        "",
        "    def test_get_relation_type_weight(self):",
        "        \"\"\"Test getting relation type weights.\"\"\"",
        "        # Test known relation types",
        "        self.assertEqual(get_relation_type_weight('IsA'), 1.5)",
        "        self.assertEqual(get_relation_type_weight('SameAs'), 2.0)",
        "        self.assertEqual(get_relation_type_weight('Antonym'), -0.5)"
      ],
      "context_after": [
        "",
        "        # Test unknown relation type defaults to 0.5",
        "        self.assertEqual(get_relation_type_weight('UnknownRelation'), 0.5)",
        "",
        "    def test_relation_weights_constant(self):",
        "        \"\"\"Test that RELATION_WEIGHTS contains expected keys.\"\"\"",
        "        expected_relations = ['IsA', 'PartOf', 'HasA', 'SameAs', 'RelatedTo', 'CoOccurs']",
        "        for rel in expected_relations:",
        "            self.assertIn(rel, RELATION_WEIGHTS)"
      ],
      "change_type": "modify"
    }
  ],
  "hour_of_day": 8,
  "day_of_week": "Friday",
  "seconds_since_last_commit": -277016,
  "is_merge": true,
  "is_initial": false,
  "parent_count": 2,
  "session_id": null,
  "related_chats": [],
  "ci_result": null,
  "reverted": false,
  "amended": false
}