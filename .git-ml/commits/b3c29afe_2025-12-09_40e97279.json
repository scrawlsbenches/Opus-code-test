{
  "hash": "b3c29afeb942ab3727669270f0f8dd7a7d18e8ab",
  "message": "Add multi-stage ranking pipeline (Task 17)",
  "author": "Claude",
  "timestamp": "2025-12-09 21:14:41 +0000",
  "branch": "claude/multi-index-design-DvifZ",
  "files_changed": [
    "TASK_LIST.md",
    "cortical/processor.py",
    "cortical/query.py",
    "tests/test_processor.py"
  ],
  "insertions": 592,
  "deletions": 10,
  "hunks": [
    {
      "file": "TASK_LIST.md",
      "function": "processor.add_documents_batch(docs, recompute='full')",
      "start_line": 450,
      "lines_added": [
        "**Files:** `cortical/query.py`, `cortical/processor.py`",
        "**Status:** [x] Completed",
        "Current ranking is flat (Token TF-IDF → Document Score). Better RAG performance with staged ranking.",
        "",
        "**Solution Applied:**",
        "Implemented a 4-stage ranking pipeline:",
        "",
        "1. **Stage 1 (Concepts):** Find relevant concepts from Layer 2 clusters, score by query term overlap",
        "2. **Stage 2 (Documents):** Rank documents using combined concept + TF-IDF scores",
        "3. **Stage 3 (Chunks):** Score passages within top documents using chunk-level TF-IDF",
        "4. **Stage 4 (Rerank):** Combine all signals (chunk 50%, TF-IDF 30%, concept 20%) for final scoring",
        "**Files Modified:**",
        "- `cortical/query.py` - Added `find_relevant_concepts()`, `multi_stage_rank()`, `multi_stage_rank_documents()` (~300 lines)",
        "- `cortical/processor.py` - Added processor wrapper methods (~90 lines)",
        "- `tests/test_processor.py` - Added 15 tests for multi-stage ranking",
        "",
        "**Usage Examples:**",
        "```python",
        "# Full 4-stage ranking (passages with stage breakdown)",
        "results = processor.multi_stage_rank(\"neural networks\", top_n=5, concept_boost=0.3)",
        "for passage, doc_id, start, end, score, stages in results:",
        "    print(f\"[{doc_id}] Score: {score:.3f}\")",
        "    print(f\"  Concept: {stages['concept_score']:.3f}\")",
        "    print(f\"  Doc: {stages['doc_score']:.3f}\")",
        "    print(f\"  Chunk: {stages['chunk_score']:.3f}\")",
        "",
        "# Document-level ranking (stages 1-2 only)",
        "results = processor.multi_stage_rank_documents(\"neural networks\", top_n=3)",
        "for doc_id, score, stages in results:",
        "    print(f\"{doc_id}: {score:.3f} (concept: {stages['concept_score']:.3f})\")",
        "```"
      ],
      "lines_removed": [
        "**Files:** `cortical/query.py`",
        "**Status:** [ ] Future Enhancement",
        "Current ranking is flat (Token TF-IDF → Document Score). Better RAG performance with staged ranking:",
        "1. **Stage 1 (Concepts):** Filter by topic relevance",
        "2. **Stage 2 (Documents):** Rank documents in topic",
        "3. **Stage 3 (Chunks):** Rank passages in documents",
        "4. **Stage 4 (Rerank):** Final relevance scoring"
      ],
      "context_before": [
        "- `avg_sim < 0.02` - isolation threshold",
        "- `tfidf > 0.005` - weak topic threshold",
        "- `0.005 < sim < 0.03` - bridge opportunity range",
        "",
        "**Implementation:** Add docstrings or make configurable parameters.",
        "",
        "---",
        "",
        "### 17. Add Multi-Stage Ranking Pipeline",
        ""
      ],
      "context_after": [
        "",
        "**Problem:**",
        "",
        "",
        "---",
        "",
        "### 18. Add Batch Query API",
        "",
        "**Files:** `cortical/query.py`, `cortical/processor.py`",
        "**Status:** [x] Completed",
        "",
        "**Problem:**",
        "No efficient way to run multiple queries. Each query repeats tokenization and expansion."
      ],
      "change_type": "modify"
    },
    {
      "file": "TASK_LIST.md",
      "function": "for query, passages in zip(queries, results):",
      "start_line": 517,
      "lines_added": [
        "| Low | Multi-stage ranking pipeline | ✅ Completed | RAG |",
        "**RAG Enhancement Completion:** 8/8 tasks (100%)",
        "Ran 173 tests in 0.154s"
      ],
      "lines_removed": [
        "| Low | Multi-stage ranking pipeline | ⬜ Future | RAG |",
        "**RAG Enhancement Completion:** 7/8 tasks (88%)",
        "Ran 158 tests in 0.153s"
      ],
      "context_before": [
        "| Low | Add test coverage | ✅ Completed | Bug Fix |",
        "| **Critical** | **Implement chunk-level retrieval** | ✅ Completed | **RAG** |",
        "| **Critical** | **Add document metadata support** | ✅ Completed | **RAG** |",
        "| **High** | **Activate Layer 2 concepts** | ✅ Completed | **RAG** |",
        "| **High** | **Integrate semantic relations** | ✅ Completed | **RAG** |",
        "| **High** | **Persist full computed state** | ✅ Completed | **RAG** |",
        "| Medium | Fix type annotation (embeddings.py) | ✅ Completed | Bug Fix |",
        "| Medium | Optimize spectral embeddings | ✅ Completed | Performance |",
        "| Medium | Add incremental indexing | ✅ Completed | RAG |",
        "| Low | Document magic numbers | ⏳ Deferred | Documentation |"
      ],
      "context_after": [
        "| Low | Batch query API | ✅ Completed | RAG |",
        "",
        "**Bug Fix Completion:** 7/7 tasks (100%)",
        "",
        "---",
        "",
        "## Test Results",
        "",
        "```",
        "OK",
        "```",
        "",
        "All tests passing as of 2025-12-09.",
        "",
        "---",
        "",
        "*Updated from code review on 2025-12-09*"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/processor.py",
      "function": "class CorticalTextProcessor:",
      "start_line": 648,
      "lines_added": [
        "    def multi_stage_rank(",
        "        self,",
        "        query_text: str,",
        "        top_n: int = 5,",
        "        chunk_size: int = 512,",
        "        overlap: int = 128,",
        "        concept_boost: float = 0.3,",
        "        use_expansion: bool = True,",
        "        use_semantic: bool = True",
        "    ) -> List[Tuple[str, str, int, int, float, Dict[str, float]]]:",
        "        \"\"\"",
        "        Multi-stage ranking pipeline for improved RAG performance.",
        "",
        "        Uses a 4-stage pipeline combining concept, document, and chunk signals:",
        "        1. Concepts: Filter by topic relevance using Layer 2 clusters",
        "        2. Documents: Rank documents within relevant topics",
        "        3. Chunks: Rank passages within top documents",
        "        4. Rerank: Combine all signals for final scoring",
        "",
        "        Args:",
        "            query_text: Search query",
        "            top_n: Number of passages to return",
        "            chunk_size: Size of each chunk in characters (default 512)",
        "            overlap: Overlap between chunks in characters (default 128)",
        "            concept_boost: Weight for concept relevance (0.0-1.0, default 0.3)",
        "            use_expansion: Whether to expand query terms",
        "            use_semantic: Whether to use semantic relations for expansion",
        "",
        "        Returns:",
        "            List of (passage_text, doc_id, start_char, end_char, final_score, stage_scores)",
        "            tuples. stage_scores contains: concept_score, doc_score, chunk_score, final_score",
        "",
        "        Example:",
        "            >>> results = processor.multi_stage_rank(\"neural networks\", top_n=5)",
        "            >>> for passage, doc_id, start, end, score, stages in results:",
        "            ...     print(f\"[{doc_id}] Final: {score:.3f}, Concept: {stages['concept_score']:.3f}\")",
        "        \"\"\"",
        "        return query_module.multi_stage_rank(",
        "            query_text,",
        "            self.layers,",
        "            self.tokenizer,",
        "            self.documents,",
        "            top_n=top_n,",
        "            chunk_size=chunk_size,",
        "            overlap=overlap,",
        "            concept_boost=concept_boost,",
        "            use_expansion=use_expansion,",
        "            semantic_relations=self.semantic_relations if use_semantic else None,",
        "            use_semantic=use_semantic",
        "        )",
        "",
        "    def multi_stage_rank_documents(",
        "        self,",
        "        query_text: str,",
        "        top_n: int = 5,",
        "        concept_boost: float = 0.3,",
        "        use_expansion: bool = True,",
        "        use_semantic: bool = True",
        "    ) -> List[Tuple[str, float, Dict[str, float]]]:",
        "        \"\"\"",
        "        Multi-stage ranking for documents (without chunk scoring).",
        "",
        "        Uses stages 1-2 of the pipeline for document-level ranking:",
        "        1. Concepts: Filter by topic relevance",
        "        2. Documents: Rank by combined concept + TF-IDF scores",
        "",
        "        Args:",
        "            query_text: Search query",
        "            top_n: Number of documents to return",
        "            concept_boost: Weight for concept relevance (0.0-1.0, default 0.3)",
        "            use_expansion: Whether to expand query terms",
        "            use_semantic: Whether to use semantic relations",
        "",
        "        Returns:",
        "            List of (doc_id, final_score, stage_scores) tuples.",
        "            stage_scores contains: concept_score, tfidf_score, combined_score",
        "",
        "        Example:",
        "            >>> results = processor.multi_stage_rank_documents(\"neural networks\")",
        "            >>> for doc_id, score, stages in results:",
        "            ...     print(f\"{doc_id}: {score:.3f} (concept: {stages['concept_score']:.3f})\")",
        "        \"\"\"",
        "        return query_module.multi_stage_rank_documents(",
        "            query_text,",
        "            self.layers,",
        "            self.tokenizer,",
        "            top_n=top_n,",
        "            concept_boost=concept_boost,",
        "            use_expansion=use_expansion,",
        "            semantic_relations=self.semantic_relations if use_semantic else None,",
        "            use_semantic=use_semantic",
        "        )",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "            self.documents,",
        "            top_n=top_n,",
        "            chunk_size=chunk_size,",
        "            overlap=overlap,",
        "            use_expansion=use_expansion,",
        "            doc_filter=doc_filter,",
        "            semantic_relations=self.semantic_relations if use_semantic else None,",
        "            use_semantic=use_semantic",
        "        )",
        ""
      ],
      "context_after": [
        "    def query_expanded(self, query_text: str, top_n: int = 10, max_expansions: int = 8) -> List[Tuple[str, float]]:",
        "        return query_module.query_with_spreading_activation(query_text, self.layers, self.tokenizer, top_n, max_expansions)",
        "    ",
        "    def find_related_documents(self, doc_id: str) -> List[Tuple[str, float]]:",
        "        return query_module.find_related_documents(doc_id, self.layers)",
        "    ",
        "    def analyze_knowledge_gaps(self) -> Dict:",
        "        return gaps_module.analyze_knowledge_gaps(self.layers, self.documents)",
        "    ",
        "    def detect_anomalies(self, threshold: float = 0.3) -> List[Dict]:"
      ],
      "change_type": "add"
    },
    {
      "file": "cortical/query.py",
      "function": "def find_passages_batch(",
      "start_line": 678,
      "lines_added": [
        "",
        "",
        "def find_relevant_concepts(",
        "    query_terms: Dict[str, float],",
        "    layers: Dict[CorticalLayer, HierarchicalLayer],",
        "    top_n: int = 5",
        ") -> List[Tuple[str, float, set]]:",
        "    \"\"\"",
        "    Stage 1: Find concepts relevant to query terms.",
        "",
        "    Args:",
        "        query_terms: Dict mapping query terms to weights",
        "        layers: Dictionary of layers",
        "        top_n: Maximum number of concepts to return",
        "",
        "    Returns:",
        "        List of (concept_name, relevance_score, document_ids) tuples",
        "    \"\"\"",
        "    layer0 = layers[CorticalLayer.TOKENS]",
        "    layer2 = layers.get(CorticalLayer.CONCEPTS)",
        "",
        "    if not layer2 or layer2.column_count() == 0:",
        "        return []",
        "",
        "    concept_scores: Dict[str, float] = {}",
        "    concept_docs: Dict[str, set] = {}",
        "",
        "    for term, weight in query_terms.items():",
        "        col = layer0.get_minicolumn(term)",
        "        if not col:",
        "            continue",
        "",
        "        # Find concepts that contain this token",
        "        for concept in layer2.minicolumns.values():",
        "            if col.id in concept.feedforward_sources:",
        "                # Score based on term weight, concept PageRank, and concept size",
        "                score = weight * concept.pagerank * (1 + len(concept.feedforward_sources) * 0.01)",
        "                concept_scores[concept.content] = concept_scores.get(concept.content, 0) + score",
        "                if concept.content not in concept_docs:",
        "                    concept_docs[concept.content] = set()",
        "                concept_docs[concept.content].update(concept.document_ids)",
        "",
        "    # Sort by score and return top concepts",
        "    sorted_concepts = sorted(concept_scores.items(), key=lambda x: -x[1])[:top_n]",
        "    return [(name, score, concept_docs.get(name, set())) for name, score in sorted_concepts]",
        "",
        "",
        "def multi_stage_rank(",
        "    query_text: str,",
        "    layers: Dict[CorticalLayer, HierarchicalLayer],",
        "    tokenizer: Tokenizer,",
        "    documents: Dict[str, str],",
        "    top_n: int = 5,",
        "    chunk_size: int = 512,",
        "    overlap: int = 128,",
        "    concept_boost: float = 0.3,",
        "    use_expansion: bool = True,",
        "    semantic_relations: Optional[List[Tuple[str, str, str, float]]] = None,",
        "    use_semantic: bool = True",
        ") -> List[Tuple[str, str, int, int, float, Dict[str, float]]]:",
        "    \"\"\"",
        "    Multi-stage ranking pipeline for improved RAG performance.",
        "",
        "    Unlike flat ranking (TF-IDF → score), this uses a 4-stage pipeline:",
        "    1. Concepts: Filter by topic relevance using Layer 2 clusters",
        "    2. Documents: Rank documents within relevant topics",
        "    3. Chunks: Rank passages within top documents",
        "    4. Rerank: Combine all signals for final scoring",
        "",
        "    Args:",
        "        query_text: Search query",
        "        layers: Dictionary of layers",
        "        tokenizer: Tokenizer instance",
        "        documents: Dict mapping doc_id to document text",
        "        top_n: Number of passages to return",
        "        chunk_size: Size of each chunk in characters",
        "        overlap: Overlap between chunks in characters",
        "        concept_boost: Weight for concept relevance in final score (0.0-1.0)",
        "        use_expansion: Whether to expand query terms",
        "        semantic_relations: Optional list of semantic relations for expansion",
        "        use_semantic: Whether to use semantic relations for expansion",
        "",
        "    Returns:",
        "        List of (passage_text, doc_id, start_char, end_char, final_score, stage_scores) tuples.",
        "        stage_scores dict contains: concept_score, doc_score, chunk_score, final_score",
        "",
        "    Example:",
        "        >>> results = multi_stage_rank(query, layers, tokenizer, documents)",
        "        >>> for passage, doc_id, start, end, score, stages in results:",
        "        ...     print(f\"[{doc_id}] Score: {score:.3f}\")",
        "        ...     print(f\"  Concept: {stages['concept_score']:.3f}\")",
        "        ...     print(f\"  Doc: {stages['doc_score']:.3f}\")",
        "        ...     print(f\"  Chunk: {stages['chunk_score']:.3f}\")",
        "    \"\"\"",
        "    layer0 = layers[CorticalLayer.TOKENS]",
        "",
        "    # Get expanded query terms",
        "    if use_expansion:",
        "        query_terms = expand_query(query_text, layers, tokenizer, max_expansions=5)",
        "        if use_semantic and semantic_relations:",
        "            semantic_terms = expand_query_semantic(",
        "                query_text, layers, tokenizer, semantic_relations, max_expansions=5",
        "            )",
        "            for term, weight in semantic_terms.items():",
        "                if term not in query_terms:",
        "                    query_terms[term] = weight * 0.8",
        "                else:",
        "                    query_terms[term] = max(query_terms[term], weight * 0.8)",
        "    else:",
        "        tokens = tokenizer.tokenize(query_text)",
        "        query_terms = {t: 1.0 for t in tokens}",
        "",
        "    if not query_terms:",
        "        return []",
        "",
        "    # ========== STAGE 1: CONCEPTS ==========",
        "    # Find relevant concepts to identify topic areas",
        "    relevant_concepts = find_relevant_concepts(query_terms, layers, top_n=10)",
        "",
        "    # Build concept score per document",
        "    doc_concept_scores: Dict[str, float] = defaultdict(float)",
        "    if relevant_concepts:",
        "        max_concept_score = max(score for _, score, _ in relevant_concepts) if relevant_concepts else 1.0",
        "        for concept_name, concept_score, doc_ids in relevant_concepts:",
        "            normalized_score = concept_score / max_concept_score if max_concept_score > 0 else 0",
        "            for doc_id in doc_ids:",
        "                doc_concept_scores[doc_id] = max(doc_concept_scores[doc_id], normalized_score)",
        "",
        "    # ========== STAGE 2: DOCUMENTS ==========",
        "    # Score documents using TF-IDF (standard approach)",
        "    doc_tfidf_scores: Dict[str, float] = defaultdict(float)",
        "    for term, term_weight in query_terms.items():",
        "        col = layer0.get_minicolumn(term)",
        "        if col:",
        "            for doc_id in col.document_ids:",
        "                tfidf = col.tfidf_per_doc.get(doc_id, col.tfidf)",
        "                doc_tfidf_scores[doc_id] += tfidf * term_weight",
        "",
        "    # Normalize TF-IDF scores",
        "    max_tfidf = max(doc_tfidf_scores.values()) if doc_tfidf_scores else 1.0",
        "    for doc_id in doc_tfidf_scores:",
        "        doc_tfidf_scores[doc_id] /= max_tfidf if max_tfidf > 0 else 1.0",
        "",
        "    # Combine concept and TF-IDF scores for document ranking",
        "    combined_doc_scores: Dict[str, float] = {}",
        "    all_docs = set(doc_concept_scores.keys()) | set(doc_tfidf_scores.keys())",
        "    for doc_id in all_docs:",
        "        concept_score = doc_concept_scores.get(doc_id, 0.0)",
        "        tfidf_score = doc_tfidf_scores.get(doc_id, 0.0)",
        "        # Weighted combination",
        "        combined_doc_scores[doc_id] = (",
        "            (1 - concept_boost) * tfidf_score +",
        "            concept_boost * concept_score",
        "        )",
        "",
        "    # Get top documents for chunk scoring",
        "    sorted_docs = sorted(combined_doc_scores.items(), key=lambda x: -x[1])",
        "    top_docs = sorted_docs[:min(len(sorted_docs), top_n * 3)]",
        "",
        "    # ========== STAGE 3: CHUNKS ==========",
        "    # Score passages within top documents",
        "    passages: List[Tuple[str, str, int, int, float, Dict[str, float]]] = []",
        "",
        "    for doc_id, doc_score in top_docs:",
        "        if doc_id not in documents:",
        "            continue",
        "",
        "        text = documents[doc_id]",
        "        chunks = create_chunks(text, chunk_size, overlap)",
        "",
        "        for chunk_text, start_char, end_char in chunks:",
        "            chunk_score = score_chunk(chunk_text, query_terms, layer0, tokenizer, doc_id)",
        "",
        "            # ========== STAGE 4: RERANK ==========",
        "            # Combine all signals for final score",
        "            concept_score = doc_concept_scores.get(doc_id, 0.0)",
        "            tfidf_score = doc_tfidf_scores.get(doc_id, 0.0)",
        "",
        "            # Normalize chunk score (avoid division by zero)",
        "            normalized_chunk = chunk_score",
        "",
        "            # Final score combines:",
        "            # - Chunk-level relevance (primary signal)",
        "            # - Document-level TF-IDF (context signal)",
        "            # - Concept relevance (topic signal)",
        "            final_score = (",
        "                0.5 * normalized_chunk +",
        "                0.3 * tfidf_score +",
        "                0.2 * concept_score",
        "            ) * (1 + doc_score * 0.1)  # Slight boost from combined doc score",
        "",
        "            stage_scores = {",
        "                'concept_score': concept_score,",
        "                'doc_score': tfidf_score,",
        "                'chunk_score': chunk_score,",
        "                'combined_doc_score': doc_score,",
        "                'final_score': final_score",
        "            }",
        "",
        "            passages.append((",
        "                chunk_text,",
        "                doc_id,",
        "                start_char,",
        "                end_char,",
        "                final_score,",
        "                stage_scores",
        "            ))",
        "",
        "    # Sort by final score and return top passages",
        "    passages.sort(key=lambda x: x[4], reverse=True)",
        "    return passages[:top_n]",
        "",
        "",
        "def multi_stage_rank_documents(",
        "    query_text: str,",
        "    layers: Dict[CorticalLayer, HierarchicalLayer],",
        "    tokenizer: Tokenizer,",
        "    top_n: int = 5,",
        "    concept_boost: float = 0.3,",
        "    use_expansion: bool = True,",
        "    semantic_relations: Optional[List[Tuple[str, str, str, float]]] = None,",
        "    use_semantic: bool = True",
        ") -> List[Tuple[str, float, Dict[str, float]]]:",
        "    \"\"\"",
        "    Multi-stage ranking for documents (without chunk scoring).",
        "",
        "    Uses the first 2 stages of the pipeline:",
        "    1. Concepts: Filter by topic relevance",
        "    2. Documents: Rank by combined concept + TF-IDF scores",
        "",
        "    Args:",
        "        query_text: Search query",
        "        layers: Dictionary of layers",
        "        tokenizer: Tokenizer instance",
        "        top_n: Number of documents to return",
        "        concept_boost: Weight for concept relevance (0.0-1.0)",
        "        use_expansion: Whether to expand query terms",
        "        semantic_relations: Optional list of semantic relations",
        "        use_semantic: Whether to use semantic relations",
        "",
        "    Returns:",
        "        List of (doc_id, final_score, stage_scores) tuples.",
        "        stage_scores dict contains: concept_score, tfidf_score, combined_score",
        "    \"\"\"",
        "    layer0 = layers[CorticalLayer.TOKENS]",
        "",
        "    # Get expanded query terms",
        "    if use_expansion:",
        "        query_terms = expand_query(query_text, layers, tokenizer, max_expansions=5)",
        "        if use_semantic and semantic_relations:",
        "            semantic_terms = expand_query_semantic(",
        "                query_text, layers, tokenizer, semantic_relations, max_expansions=5",
        "            )",
        "            for term, weight in semantic_terms.items():",
        "                if term not in query_terms:",
        "                    query_terms[term] = weight * 0.8",
        "                else:",
        "                    query_terms[term] = max(query_terms[term], weight * 0.8)",
        "    else:",
        "        tokens = tokenizer.tokenize(query_text)",
        "        query_terms = {t: 1.0 for t in tokens}",
        "",
        "    if not query_terms:",
        "        return []",
        "",
        "    # Stage 1: Concepts",
        "    relevant_concepts = find_relevant_concepts(query_terms, layers, top_n=10)",
        "",
        "    doc_concept_scores: Dict[str, float] = defaultdict(float)",
        "    if relevant_concepts:",
        "        max_concept_score = max(score for _, score, _ in relevant_concepts) if relevant_concepts else 1.0",
        "        for concept_name, concept_score, doc_ids in relevant_concepts:",
        "            normalized_score = concept_score / max_concept_score if max_concept_score > 0 else 0",
        "            for doc_id in doc_ids:",
        "                doc_concept_scores[doc_id] = max(doc_concept_scores[doc_id], normalized_score)",
        "",
        "    # Stage 2: Documents",
        "    doc_tfidf_scores: Dict[str, float] = defaultdict(float)",
        "    for term, term_weight in query_terms.items():",
        "        col = layer0.get_minicolumn(term)",
        "        if col:",
        "            for doc_id in col.document_ids:",
        "                tfidf = col.tfidf_per_doc.get(doc_id, col.tfidf)",
        "                doc_tfidf_scores[doc_id] += tfidf * term_weight",
        "",
        "    # Normalize TF-IDF",
        "    max_tfidf = max(doc_tfidf_scores.values()) if doc_tfidf_scores else 1.0",
        "    for doc_id in doc_tfidf_scores:",
        "        doc_tfidf_scores[doc_id] /= max_tfidf if max_tfidf > 0 else 1.0",
        "",
        "    # Combine scores",
        "    results: List[Tuple[str, float, Dict[str, float]]] = []",
        "    all_docs = set(doc_concept_scores.keys()) | set(doc_tfidf_scores.keys())",
        "",
        "    for doc_id in all_docs:",
        "        concept_score = doc_concept_scores.get(doc_id, 0.0)",
        "        tfidf_score = doc_tfidf_scores.get(doc_id, 0.0)",
        "        combined = (1 - concept_boost) * tfidf_score + concept_boost * concept_score",
        "",
        "        stage_scores = {",
        "            'concept_score': concept_score,",
        "            'tfidf_score': tfidf_score,",
        "            'combined_score': combined",
        "        }",
        "        results.append((doc_id, combined, stage_scores))",
        "",
        "    results.sort(key=lambda x: x[1], reverse=True)",
        "    return results[:top_n]"
      ],
      "lines_removed": [],
      "context_before": [
        "                chunk_score = score_chunk(",
        "                    chunk_text, query_terms, layer0, tokenizer, doc_id",
        "                )",
        "                combined_score = chunk_score * (1 + doc_score * 0.1)",
        "                passages.append((chunk_text, doc_id, start_char, end_char, combined_score))",
        "",
        "        passages.sort(key=lambda x: x[4], reverse=True)",
        "        all_results.append(passages[:top_n])",
        "",
        "    return all_results"
      ],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "tests/test_processor.py",
      "function": "class TestProcessorBatchQuery(unittest.TestCase):",
      "start_line": 496,
      "lines_added": [
        "class TestProcessorMultiStageRanking(unittest.TestCase):",
        "    \"\"\"Test multi-stage ranking pipeline for RAG systems.\"\"\"",
        "",
        "    @classmethod",
        "    def setUpClass(cls):",
        "        cls.processor = CorticalTextProcessor()",
        "        # Create a diverse corpus for testing multi-stage ranking",
        "        cls.processor.process_document(\"neural_doc\", \"\"\"",
        "            Neural networks are computational models inspired by biological neurons.",
        "            Deep learning uses many layers to learn hierarchical representations.",
        "            Backpropagation is the key algorithm for training neural networks.",
        "            Convolutional neural networks excel at image recognition tasks.",
        "        \"\"\")",
        "        cls.processor.process_document(\"ml_doc\", \"\"\"",
        "            Machine learning algorithms learn patterns from data automatically.",
        "            Supervised learning requires labeled training examples.",
        "            Unsupervised learning discovers hidden structure in data.",
        "            Model evaluation uses metrics like accuracy precision and recall.",
        "        \"\"\")",
        "        cls.processor.process_document(\"data_doc\", \"\"\"",
        "            Data preprocessing is essential for machine learning pipelines.",
        "            Feature engineering creates meaningful input representations.",
        "            Data normalization scales features to similar ranges.",
        "            Cross-validation ensures robust model performance estimates.",
        "        \"\"\")",
        "        cls.processor.process_document(\"nlp_doc\", \"\"\"",
        "            Natural language processing enables computers to understand text.",
        "            Word embeddings capture semantic relationships between words.",
        "            Transformers use attention mechanisms for sequence modeling.",
        "            Language models can generate coherent text passages.",
        "        \"\"\")",
        "        cls.processor.compute_all(verbose=False)",
        "",
        "    def test_multi_stage_rank_returns_list(self):",
        "        \"\"\"Test that multi_stage_rank returns a list.\"\"\"",
        "        results = self.processor.multi_stage_rank(\"neural networks\", top_n=3)",
        "        self.assertIsInstance(results, list)",
        "",
        "    def test_multi_stage_rank_result_structure(self):",
        "        \"\"\"Test that results have correct 6-tuple structure.\"\"\"",
        "        results = self.processor.multi_stage_rank(\"neural\", top_n=3)",
        "        self.assertGreater(len(results), 0)",
        "        passage, doc_id, start, end, score, stage_scores = results[0]",
        "        self.assertIsInstance(passage, str)",
        "        self.assertIsInstance(doc_id, str)",
        "        self.assertIsInstance(start, int)",
        "        self.assertIsInstance(end, int)",
        "        self.assertIsInstance(score, float)",
        "        self.assertIsInstance(stage_scores, dict)",
        "",
        "    def test_multi_stage_rank_stage_scores(self):",
        "        \"\"\"Test that stage_scores contains expected keys.\"\"\"",
        "        results = self.processor.multi_stage_rank(\"neural networks\", top_n=3)",
        "        self.assertGreater(len(results), 0)",
        "        _, _, _, _, _, stage_scores = results[0]",
        "        self.assertIn('concept_score', stage_scores)",
        "        self.assertIn('doc_score', stage_scores)",
        "        self.assertIn('chunk_score', stage_scores)",
        "        self.assertIn('final_score', stage_scores)",
        "",
        "    def test_multi_stage_rank_top_n(self):",
        "        \"\"\"Test that top_n limits results.\"\"\"",
        "        results = self.processor.multi_stage_rank(\"learning\", top_n=2)",
        "        self.assertLessEqual(len(results), 2)",
        "",
        "    def test_multi_stage_rank_chunk_size(self):",
        "        \"\"\"Test that chunk_size is respected.\"\"\"",
        "        results = self.processor.multi_stage_rank(",
        "            \"neural\", top_n=5, chunk_size=100, overlap=20",
        "        )",
        "        for passage, _, _, _, _, _ in results:",
        "            self.assertLessEqual(len(passage), 100)",
        "",
        "    def test_multi_stage_rank_concept_boost(self):",
        "        \"\"\"Test that concept_boost parameter is used.\"\"\"",
        "        # Test with high concept boost vs low",
        "        results_high = self.processor.multi_stage_rank(",
        "            \"neural\", top_n=3, concept_boost=0.8",
        "        )",
        "        results_low = self.processor.multi_stage_rank(",
        "            \"neural\", top_n=3, concept_boost=0.1",
        "        )",
        "        # Both should return results (exact ordering may differ)",
        "        self.assertGreater(len(results_high), 0)",
        "        self.assertGreater(len(results_low), 0)",
        "",
        "    def test_multi_stage_rank_sorted_descending(self):",
        "        \"\"\"Test that results are sorted by score descending.\"\"\"",
        "        results = self.processor.multi_stage_rank(\"neural networks\", top_n=5)",
        "        if len(results) > 1:",
        "            scores = [score for _, _, _, _, score, _ in results]",
        "            self.assertEqual(scores, sorted(scores, reverse=True))",
        "",
        "    def test_multi_stage_rank_documents_returns_list(self):",
        "        \"\"\"Test that multi_stage_rank_documents returns a list.\"\"\"",
        "        results = self.processor.multi_stage_rank_documents(\"neural networks\", top_n=3)",
        "        self.assertIsInstance(results, list)",
        "",
        "    def test_multi_stage_rank_documents_structure(self):",
        "        \"\"\"Test that document results have correct 3-tuple structure.\"\"\"",
        "        results = self.processor.multi_stage_rank_documents(\"neural\", top_n=3)",
        "        self.assertGreater(len(results), 0)",
        "        doc_id, score, stage_scores = results[0]",
        "        self.assertIsInstance(doc_id, str)",
        "        self.assertIsInstance(score, float)",
        "        self.assertIsInstance(stage_scores, dict)",
        "",
        "    def test_multi_stage_rank_documents_stage_scores(self):",
        "        \"\"\"Test that document stage_scores contains expected keys.\"\"\"",
        "        results = self.processor.multi_stage_rank_documents(\"neural networks\", top_n=3)",
        "        self.assertGreater(len(results), 0)",
        "        _, _, stage_scores = results[0]",
        "        self.assertIn('concept_score', stage_scores)",
        "        self.assertIn('tfidf_score', stage_scores)",
        "        self.assertIn('combined_score', stage_scores)",
        "",
        "    def test_multi_stage_rank_documents_top_n(self):",
        "        \"\"\"Test that top_n limits document results.\"\"\"",
        "        results = self.processor.multi_stage_rank_documents(\"learning\", top_n=2)",
        "        self.assertLessEqual(len(results), 2)",
        "",
        "    def test_multi_stage_rank_documents_sorted(self):",
        "        \"\"\"Test that document results are sorted by score descending.\"\"\"",
        "        results = self.processor.multi_stage_rank_documents(\"neural networks\", top_n=5)",
        "        if len(results) > 1:",
        "            scores = [score for _, score, _ in results]",
        "            self.assertEqual(scores, sorted(scores, reverse=True))",
        "",
        "    def test_multi_stage_rank_empty_query(self):",
        "        \"\"\"Test handling of query with no matches.\"\"\"",
        "        results = self.processor.multi_stage_rank(\"xyznonexistent123\", top_n=3)",
        "        self.assertEqual(len(results), 0)",
        "",
        "    def test_multi_stage_rank_without_expansion(self):",
        "        \"\"\"Test multi-stage ranking without query expansion.\"\"\"",
        "        results = self.processor.multi_stage_rank(",
        "            \"neural\", top_n=3, use_expansion=False",
        "        )",
        "        self.assertIsInstance(results, list)",
        "",
        "    def test_multi_stage_vs_flat_ranking(self):",
        "        \"\"\"Test that multi-stage ranking produces results comparable to flat ranking.\"\"\"",
        "        # Both should find relevant documents for the same query",
        "        multi_results = self.processor.multi_stage_rank(\"neural networks\", top_n=3)",
        "        flat_results = self.processor.find_passages_for_query(\"neural networks\", top_n=3)",
        "",
        "        # Both should return results",
        "        self.assertGreater(len(multi_results), 0)",
        "        self.assertGreater(len(flat_results), 0)",
        "",
        "        # Both should find the neural_doc",
        "        multi_docs = {doc_id for _, doc_id, _, _, _, _ in multi_results}",
        "        flat_docs = {doc_id for _, doc_id, _, _, _ in flat_results}",
        "        self.assertIn(\"neural_doc\", multi_docs)",
        "        self.assertIn(\"neural_doc\", flat_docs)",
        "",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "",
        "    def test_batch_handles_nonexistent_terms(self):",
        "        \"\"\"Test that batch handles queries with no matches.\"\"\"",
        "        queries = [\"xyznonexistent123\", \"neural networks\"]",
        "        results = self.processor.find_documents_batch(queries, top_n=3)",
        "        self.assertEqual(len(results), 2)",
        "        self.assertEqual(len(results[0]), 0)  # No matches for nonexistent",
        "        self.assertGreater(len(results[1]), 0)  # Matches for neural networks",
        "",
        ""
      ],
      "context_after": [
        "class TestProcessorIncrementalIndexing(unittest.TestCase):",
        "    \"\"\"Test incremental document indexing functionality.\"\"\"",
        "",
        "    def setUp(self):",
        "        self.processor = CorticalTextProcessor()",
        "",
        "    def test_add_document_incremental_returns_stats(self):",
        "        \"\"\"Test that add_document_incremental returns processing stats.\"\"\"",
        "        stats = self.processor.add_document_incremental(",
        "            \"doc1\", \"Neural networks process information.\", recompute='tfidf'"
      ],
      "change_type": "add"
    }
  ],
  "hour_of_day": 21,
  "day_of_week": "Tuesday",
  "seconds_since_last_commit": -491407,
  "is_merge": false,
  "is_initial": false,
  "parent_count": 1,
  "session_id": null,
  "related_chats": [],
  "ci_result": null,
  "reverted": false,
  "amended": false
}