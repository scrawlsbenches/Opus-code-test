{
  "hash": "36827390783d1eeb180ca6ee8b89b1402f4de633",
  "message": "Add incremental codebase indexing with progress tracking",
  "author": "Claude",
  "timestamp": "2025-12-10 20:21:51 +0000",
  "branch": "claude/multi-index-design-DvifZ",
  "files_changed": [
    "cortical/layers.py",
    "cortical/processor.py",
    "scripts/index_codebase.py",
    "tests/test_incremental_indexing.py"
  ],
  "insertions": 1387,
  "deletions": 54,
  "hunks": [
    {
      "file": "cortical/layers.py",
      "function": "class HierarchicalLayer:",
      "start_line": 128,
      "lines_added": [
        "    def remove_minicolumn(self, content: str) -> bool:",
        "        \"\"\"",
        "        Remove a minicolumn from this layer.",
        "",
        "        Args:",
        "            content: The content key of the minicolumn to remove",
        "",
        "        Returns:",
        "            True if the minicolumn was found and removed, False otherwise",
        "        \"\"\"",
        "        if content not in self.minicolumns:",
        "            return False",
        "",
        "        col = self.minicolumns[content]",
        "        # Remove from ID index",
        "        if col.id in self._id_index:",
        "            del self._id_index[col.id]",
        "        # Remove from minicolumns dict",
        "        del self.minicolumns[content]",
        "        return True",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "",
        "        Args:",
        "            col_id: The minicolumn ID (e.g., \"L0_neural\")",
        "",
        "        Returns:",
        "            The Minicolumn if found, None otherwise",
        "        \"\"\"",
        "        content = self._id_index.get(col_id)",
        "        return self.minicolumns.get(content) if content else None",
        ""
      ],
      "context_after": [
        "    def column_count(self) -> int:",
        "        \"\"\"Return the number of minicolumns in this layer.\"\"\"",
        "        return len(self.minicolumns)",
        "    ",
        "    def total_connections(self) -> int:",
        "        \"\"\"Return total number of lateral connections in this layer.\"\"\"",
        "        return sum(col.connection_count() for col in self.minicolumns.values())",
        "    ",
        "    def average_activation(self) -> float:",
        "        \"\"\"Calculate average activation across all minicolumns.\"\"\""
      ],
      "change_type": "add"
    },
    {
      "file": "cortical/processor.py",
      "function": "class CorticalTextProcessor:",
      "start_line": 353,
      "lines_added": [
        "    def remove_document(self, doc_id: str, verbose: bool = False) -> Dict[str, Any]:",
        "        \"\"\"",
        "        Remove a document from the corpus.",
        "",
        "        Removes the document and cleans up all references to it in the layers:",
        "        - Removes from documents dict and metadata",
        "        - Removes document minicolumn from Layer 3",
        "        - Removes doc_id from token and bigram document_ids sets",
        "        - Decrements occurrence counts appropriately",
        "        - Cleans up feedforward/feedback connections",
        "",
        "        Args:",
        "            doc_id: Document identifier to remove",
        "            verbose: Print progress messages",
        "",
        "        Returns:",
        "            Dict with removal statistics:",
        "                - found: Whether the document existed",
        "                - tokens_affected: Number of tokens that referenced this document",
        "                - bigrams_affected: Number of bigrams that referenced this document",
        "",
        "        Example:",
        "            >>> processor.remove_document(\"old_doc\")",
        "            {'found': True, 'tokens_affected': 42, 'bigrams_affected': 35}",
        "        \"\"\"",
        "        from .layers import CorticalLayer",
        "",
        "        if doc_id not in self.documents:",
        "            return {'found': False, 'tokens_affected': 0, 'bigrams_affected': 0}",
        "",
        "        if verbose:",
        "            print(f\"Removing document: {doc_id}\")",
        "",
        "        # Remove from documents and metadata",
        "        del self.documents[doc_id]",
        "        if doc_id in self.document_metadata:",
        "            del self.document_metadata[doc_id]",
        "",
        "        # Remove document minicolumn from Layer 3",
        "        layer3 = self.layers[CorticalLayer.DOCUMENTS]",
        "        doc_col = layer3.get_minicolumn(doc_id)",
        "        if doc_col:",
        "            # Get tokens/bigrams that were connected to this document",
        "            connected_ids = set(doc_col.feedforward_connections.keys())",
        "            layer3.remove_minicolumn(doc_id)",
        "",
        "        # Clean up token references in Layer 0",
        "        layer0 = self.layers[CorticalLayer.TOKENS]",
        "        tokens_affected = 0",
        "        for content, col in list(layer0.minicolumns.items()):",
        "            if doc_id in col.document_ids:",
        "                col.document_ids.discard(doc_id)",
        "                tokens_affected += 1",
        "",
        "                # Decrement occurrence count by per-doc count",
        "                if doc_id in col.doc_occurrence_counts:",
        "                    col.occurrence_count -= col.doc_occurrence_counts[doc_id]",
        "                    del col.doc_occurrence_counts[doc_id]",
        "",
        "                # Clean up feedback connections to document",
        "                doc_col_id = f\"L3_{doc_id}\"",
        "                if doc_col_id in col.feedback_connections:",
        "                    del col.feedback_connections[doc_col_id]",
        "",
        "        # Clean up bigram references in Layer 1",
        "        layer1 = self.layers[CorticalLayer.BIGRAMS]",
        "        bigrams_affected = 0",
        "        for content, col in list(layer1.minicolumns.items()):",
        "            if doc_id in col.document_ids:",
        "                col.document_ids.discard(doc_id)",
        "                bigrams_affected += 1",
        "",
        "                # Decrement occurrence count (approximate since we don't track per-doc for bigrams)",
        "                if doc_id in col.doc_occurrence_counts:",
        "                    col.occurrence_count -= col.doc_occurrence_counts[doc_id]",
        "                    del col.doc_occurrence_counts[doc_id]",
        "",
        "        # Mark all computations as stale",
        "        self._mark_all_stale()",
        "",
        "        # Invalidate query cache since corpus changed",
        "        if hasattr(self, '_query_expansion_cache'):",
        "            self._query_expansion_cache.clear()",
        "",
        "        if verbose:",
        "            print(f\"  Affected: {tokens_affected} tokens, {bigrams_affected} bigrams\")",
        "",
        "        return {",
        "            'found': True,",
        "            'tokens_affected': tokens_affected,",
        "            'bigrams_affected': bigrams_affected",
        "        }",
        "",
        "    def remove_documents_batch(",
        "        self,",
        "        doc_ids: List[str],",
        "        recompute: str = 'none',",
        "        verbose: bool = True",
        "    ) -> Dict[str, Any]:",
        "        \"\"\"",
        "        Remove multiple documents efficiently with single recomputation.",
        "",
        "        Args:",
        "            doc_ids: List of document identifiers to remove",
        "            recompute: Level of recomputation after removal:",
        "                - 'none': Just remove documents, mark computations stale",
        "                - 'tfidf': Recompute TF-IDF only",
        "                - 'full': Run full compute_all()",
        "            verbose: Print progress messages",
        "",
        "        Returns:",
        "            Dict with removal statistics:",
        "                - documents_removed: Number of documents actually removed",
        "                - documents_not_found: Number of doc_ids that didn't exist",
        "                - total_tokens_affected: Total tokens affected",
        "                - total_bigrams_affected: Total bigrams affected",
        "",
        "        Example:",
        "            >>> processor.remove_documents_batch([\"old1\", \"old2\", \"old3\"])",
        "        \"\"\"",
        "        removed = 0",
        "        not_found = 0",
        "        total_tokens = 0",
        "        total_bigrams = 0",
        "",
        "        if verbose:",
        "            print(f\"Removing {len(doc_ids)} documents...\")",
        "",
        "        for doc_id in doc_ids:",
        "            result = self.remove_document(doc_id, verbose=False)",
        "            if result['found']:",
        "                removed += 1",
        "                total_tokens += result['tokens_affected']",
        "                total_bigrams += result['bigrams_affected']",
        "            else:",
        "                not_found += 1",
        "",
        "        if verbose:",
        "            print(f\"  Removed: {removed}, Not found: {not_found}\")",
        "            print(f\"  Affected: {total_tokens} tokens, {total_bigrams} bigrams\")",
        "",
        "        # Perform recomputation",
        "        if recompute == 'tfidf':",
        "            if verbose:",
        "                print(\"Recomputing TF-IDF...\")",
        "            self.compute_tfidf(verbose=False)",
        "            self._mark_fresh(self.COMP_TFIDF)",
        "        elif recompute == 'full':",
        "            if verbose:",
        "                print(\"Running full recomputation...\")",
        "            self.compute_all(verbose=False)",
        "            self._stale_computations.clear()",
        "",
        "        return {",
        "            'documents_removed': removed,",
        "            'documents_not_found': not_found,",
        "            'total_tokens_affected': total_tokens,",
        "            'total_bigrams_affected': total_bigrams,",
        "            'recomputation': recompute",
        "        }",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "        if verbose:",
        "            print(\"Done.\")",
        "",
        "        return {",
        "            'documents_added': len(documents),",
        "            'total_tokens': total_tokens,",
        "            'total_bigrams': total_bigrams,",
        "            'recomputation': recompute",
        "        }",
        ""
      ],
      "context_after": [
        "    def recompute(",
        "        self,",
        "        level: str = 'stale',",
        "        verbose: bool = True",
        "    ) -> Dict[str, bool]:",
        "        \"\"\"",
        "        Recompute specified analysis levels.",
        "",
        "        Use this after adding documents with recompute='none' to batch",
        "        the recomputation step."
      ],
      "change_type": "add"
    },
    {
      "file": "scripts/index_codebase.py",
      "function": null,
      "start_line": 1,
      "lines_added": [
        "Supports incremental indexing to only re-index changed files.",
        "",
        "    python scripts/index_codebase.py --incremental  # Only index changes",
        "    python scripts/index_codebase.py --status       # Show what would change",
        "    python scripts/index_codebase.py --force        # Force full rebuild",
        "    python scripts/index_codebase.py --log indexer.log  # Log to file",
        "import json",
        "import logging",
        "import signal",
        "import time",
        "from contextlib import contextmanager",
        "from dataclasses import dataclass, field",
        "from datetime import datetime",
        "from typing import Dict, List, Optional, Tuple, Any",
        "# Manifest file version for compatibility checking",
        "MANIFEST_VERSION = \"1.0\"",
        "",
        "# Default timeout in seconds (0 = no timeout)",
        "DEFAULT_TIMEOUT = 300  # 5 minutes",
        "",
        "",
        "# =============================================================================",
        "# Progress Tracking System",
        "# =============================================================================",
        "",
        "@dataclass",
        "class PhaseStats:",
        "    \"\"\"Statistics for a single phase of indexing.\"\"\"",
        "    name: str",
        "    start_time: float = 0.0",
        "    end_time: float = 0.0",
        "    items_total: int = 0",
        "    items_processed: int = 0",
        "    status: str = \"pending\"  # pending, running, completed, failed",
        "",
        "    @property",
        "    def duration(self) -> float:",
        "        if self.end_time > 0:",
        "            return self.end_time - self.start_time",
        "        elif self.start_time > 0:",
        "            return time.time() - self.start_time",
        "        return 0.0",
        "",
        "    @property",
        "    def progress_pct(self) -> float:",
        "        if self.items_total == 0:",
        "            return 0.0",
        "        return (self.items_processed / self.items_total) * 100",
        "",
        "",
        "class ProgressTracker:",
        "    \"\"\"",
        "    Tracks progress through indexing phases with timing and logging.",
        "",
        "    Provides:",
        "    - Per-phase timing",
        "    - Per-file progress within phases",
        "    - Log file output",
        "    - Console progress updates",
        "    - Summary statistics",
        "    \"\"\"",
        "",
        "    def __init__(",
        "        self,",
        "        log_file: Optional[str] = None,",
        "        verbose: bool = False,",
        "        quiet: bool = False",
        "    ):",
        "        self.start_time = time.time()",
        "        self.phases: Dict[str, PhaseStats] = {}",
        "        self.current_phase: Optional[str] = None",
        "        self.verbose = verbose",
        "        self.quiet = quiet",
        "        self.warnings: List[str] = []",
        "        self.errors: List[str] = []",
        "",
        "        # Set up logging",
        "        self.logger = logging.getLogger(\"indexer\")",
        "        self.logger.setLevel(logging.DEBUG)",
        "",
        "        # Console handler (INFO level unless verbose)",
        "        if not quiet:",
        "            console_handler = logging.StreamHandler(sys.stdout)",
        "            console_handler.setLevel(logging.DEBUG if verbose else logging.INFO)",
        "            console_format = logging.Formatter('%(message)s')",
        "            console_handler.setFormatter(console_format)",
        "            self.logger.addHandler(console_handler)",
        "",
        "        # File handler (DEBUG level - captures everything)",
        "        if log_file:",
        "            file_handler = logging.FileHandler(log_file, mode='w')",
        "            file_handler.setLevel(logging.DEBUG)",
        "            file_format = logging.Formatter(",
        "                '%(asctime)s [%(levelname)s] %(message)s',",
        "                datefmt='%Y-%m-%d %H:%M:%S'",
        "            )",
        "            file_handler.setFormatter(file_format)",
        "            self.logger.addHandler(file_handler)",
        "            self.log_file = log_file",
        "        else:",
        "            self.log_file = None",
        "",
        "    def log(self, message: str, level: str = \"info\"):",
        "        \"\"\"Log a message at the specified level.\"\"\"",
        "        getattr(self.logger, level)(message)",
        "",
        "    def start_phase(self, name: str, total_items: int = 0):",
        "        \"\"\"Start a new phase of indexing.\"\"\"",
        "        self.current_phase = name",
        "        self.phases[name] = PhaseStats(",
        "            name=name,",
        "            start_time=time.time(),",
        "            items_total=total_items,",
        "            status=\"running\"",
        "        )",
        "        self.log(f\"\\n[PHASE] {name}\", \"info\")",
        "        if total_items > 0:",
        "            self.log(f\"  Items to process: {total_items}\", \"debug\")",
        "",
        "    def end_phase(self, name: Optional[str] = None, status: str = \"completed\"):",
        "        \"\"\"End a phase and record timing.\"\"\"",
        "        phase_name = name or self.current_phase",
        "        if phase_name and phase_name in self.phases:",
        "            phase = self.phases[phase_name]",
        "            phase.end_time = time.time()",
        "            phase.status = status",
        "            duration = phase.duration",
        "",
        "            status_symbol = \"✓\" if status == \"completed\" else \"✗\"",
        "            self.log(f\"  {status_symbol} {phase_name} completed in {duration:.2f}s\", \"info\")",
        "",
        "            if self.current_phase == phase_name:",
        "                self.current_phase = None",
        "",
        "    def update_progress(self, items_processed: int, item_name: Optional[str] = None):",
        "        \"\"\"Update progress within the current phase.\"\"\"",
        "        if self.current_phase and self.current_phase in self.phases:",
        "            phase = self.phases[self.current_phase]",
        "            phase.items_processed = items_processed",
        "",
        "            if phase.items_total > 0:",
        "                pct = phase.progress_pct",
        "                if item_name:",
        "                    self.log(",
        "                        f\"  [{items_processed}/{phase.items_total}] {pct:.0f}% - {item_name}\",",
        "                        \"debug\"",
        "                    )",
        "                # Show progress at 25%, 50%, 75% milestones",
        "                if items_processed in [",
        "                    phase.items_total // 4,",
        "                    phase.items_total // 2,",
        "                    (phase.items_total * 3) // 4",
        "                ]:",
        "                    self.log(f\"  Progress: {pct:.0f}% ({items_processed}/{phase.items_total})\", \"info\")",
        "",
        "    def warn(self, message: str):",
        "        \"\"\"Log a warning.\"\"\"",
        "        self.warnings.append(message)",
        "        self.log(f\"  WARNING: {message}\", \"warning\")",
        "",
        "    def error(self, message: str):",
        "        \"\"\"Log an error.\"\"\"",
        "        self.errors.append(message)",
        "        self.log(f\"  ERROR: {message}\", \"error\")",
        "",
        "    def get_summary(self) -> Dict[str, Any]:",
        "        \"\"\"Get a summary of all phases.\"\"\"",
        "        total_duration = time.time() - self.start_time",
        "        return {",
        "            \"total_duration\": total_duration,",
        "            \"phases\": {",
        "                name: {",
        "                    \"duration\": phase.duration,",
        "                    \"items_processed\": phase.items_processed,",
        "                    \"items_total\": phase.items_total,",
        "                    \"status\": phase.status",
        "                }",
        "                for name, phase in self.phases.items()",
        "            },",
        "            \"warnings\": len(self.warnings),",
        "            \"errors\": len(self.errors)",
        "        }",
        "",
        "    def print_summary(self):",
        "        \"\"\"Print a summary of the indexing run.\"\"\"",
        "        total_duration = time.time() - self.start_time",
        "",
        "        self.log(\"\\n\" + \"=\" * 50, \"info\")",
        "        self.log(\"INDEXING SUMMARY\", \"info\")",
        "        self.log(\"=\" * 50, \"info\")",
        "",
        "        self.log(f\"\\nTotal time: {total_duration:.2f}s\", \"info\")",
        "",
        "        self.log(\"\\nPhase breakdown:\", \"info\")",
        "        for name, phase in self.phases.items():",
        "            status_symbol = \"✓\" if phase.status == \"completed\" else \"✗\"",
        "            items_str = \"\"",
        "            if phase.items_total > 0:",
        "                items_str = f\" ({phase.items_processed}/{phase.items_total} items)\"",
        "            self.log(f\"  {status_symbol} {name}: {phase.duration:.2f}s{items_str}\", \"info\")",
        "",
        "        if self.warnings:",
        "            self.log(f\"\\nWarnings: {len(self.warnings)}\", \"warning\")",
        "            for w in self.warnings[:5]:",
        "                self.log(f\"  - {w}\", \"warning\")",
        "            if len(self.warnings) > 5:",
        "                self.log(f\"  ... and {len(self.warnings) - 5} more\", \"warning\")",
        "",
        "        if self.errors:",
        "            self.log(f\"\\nErrors: {len(self.errors)}\", \"error\")",
        "            for e in self.errors[:5]:",
        "                self.log(f\"  - {e}\", \"error\")",
        "",
        "        if self.log_file:",
        "            self.log(f\"\\nFull log written to: {self.log_file}\", \"info\")",
        "",
        "",
        "# =============================================================================",
        "# Timeout Handler",
        "# =============================================================================",
        "",
        "class TimeoutError(Exception):",
        "    \"\"\"Raised when indexing exceeds the timeout.\"\"\"",
        "    pass",
        "",
        "",
        "@contextmanager",
        "def timeout_handler(seconds: int, tracker: Optional[ProgressTracker] = None):",
        "    \"\"\"",
        "    Context manager for timeout handling.",
        "",
        "    Args:",
        "        seconds: Timeout in seconds (0 = no timeout)",
        "        tracker: Optional progress tracker for logging",
        "    \"\"\"",
        "    if seconds <= 0:",
        "        yield",
        "        return",
        "",
        "    def handler(signum, frame):",
        "        msg = f\"Indexing timed out after {seconds} seconds\"",
        "        if tracker:",
        "            tracker.error(msg)",
        "            tracker.print_summary()",
        "        raise TimeoutError(msg)",
        "",
        "    # Set the signal handler",
        "    old_handler = signal.signal(signal.SIGALRM, handler)",
        "    signal.alarm(seconds)",
        "",
        "    try:",
        "        yield",
        "    finally:",
        "        # Restore the old handler and cancel the alarm",
        "        signal.alarm(0)",
        "        signal.signal(signal.SIGALRM, old_handler)",
        "",
        "",
        "# =============================================================================",
        "# Manifest Operations",
        "# =============================================================================",
        "",
        "def get_manifest_path(corpus_path: Path) -> Path:",
        "    \"\"\"Get the manifest file path based on corpus path.\"\"\"",
        "    return corpus_path.with_suffix('.manifest.json')",
        "",
        "",
        "def load_manifest(",
        "    manifest_path: Path,",
        "    tracker: Optional[ProgressTracker] = None",
        ") -> Optional[Dict[str, Any]]:",
        "    \"\"\"",
        "    Load the manifest file if it exists.",
        "",
        "    Args:",
        "        manifest_path: Path to the manifest file",
        "        tracker: Optional progress tracker for logging",
        "",
        "    Returns:",
        "        Manifest dict if found and valid, None otherwise",
        "    \"\"\"",
        "    if not manifest_path.exists():",
        "        return None",
        "",
        "    try:",
        "        with open(manifest_path, 'r') as f:",
        "            manifest = json.load(f)",
        "",
        "        # Check version compatibility",
        "        if manifest.get('version') != MANIFEST_VERSION:",
        "            if tracker:",
        "                tracker.warn(f\"Manifest version mismatch (expected {MANIFEST_VERSION})\")",
        "            return None",
        "",
        "        return manifest",
        "    except (json.JSONDecodeError, IOError) as e:",
        "        if tracker:",
        "            tracker.warn(f\"Could not load manifest: {e}\")",
        "        return None",
        "",
        "",
        "def save_manifest(",
        "    manifest_path: Path,",
        "    files: Dict[str, float],",
        "    corpus_path: str,",
        "    stats: Dict[str, Any],",
        "    tracker: Optional[ProgressTracker] = None",
        ") -> None:",
        "    \"\"\"",
        "    Save the manifest file with current file state.",
        "",
        "    Args:",
        "        manifest_path: Path to save the manifest",
        "        files: Dict mapping file paths to modification times",
        "        corpus_path: Path to the corpus file",
        "        stats: Statistics about the indexed corpus",
        "        tracker: Optional progress tracker for logging",
        "    \"\"\"",
        "    manifest = {",
        "        'version': MANIFEST_VERSION,",
        "        'corpus_path': str(corpus_path),",
        "        'indexed_at': datetime.now().isoformat(),",
        "        'files': files,",
        "        'stats': stats,",
        "    }",
        "",
        "    with open(manifest_path, 'w') as f:",
        "        json.dump(manifest, f, indent=2)",
        "",
        "    if tracker:",
        "        tracker.log(f\"  Manifest saved to {manifest_path.name}\", \"debug\")",
        "",
        "",
        "# =============================================================================",
        "# File Operations",
        "# =============================================================================",
        "",
        "def get_file_mtime(file_path: Path) -> float:",
        "    \"\"\"Get the modification time of a file.\"\"\"",
        "    return file_path.stat().st_mtime",
        "",
        "",
        "def get_file_changes(",
        "    manifest: Dict[str, Any],",
        "    current_files: List[Path],",
        "    base_path: Path",
        ") -> Tuple[List[Path], List[Path], List[str]]:",
        "    \"\"\"",
        "    Compare current files to manifest and detect changes.",
        "",
        "    Args:",
        "        manifest: Previously saved manifest",
        "        current_files: List of current file paths",
        "        base_path: Base path for relative path calculation",
        "",
        "    Returns:",
        "        Tuple of (added_files, modified_files, deleted_doc_ids)",
        "    \"\"\"",
        "    old_files = manifest.get('files', {})",
        "",
        "    added = []",
        "    modified = []",
        "    deleted_ids = []",
        "",
        "    # Build set of current relative paths",
        "    current_rel_paths = {}",
        "    for file_path in current_files:",
        "        rel_path = str(file_path.relative_to(base_path))",
        "        current_rel_paths[rel_path] = file_path",
        "",
        "    # Check for added and modified files",
        "    for rel_path, file_path in current_rel_paths.items():",
        "        if rel_path not in old_files:",
        "            added.append(file_path)",
        "        else:",
        "            old_mtime = old_files[rel_path]",
        "            current_mtime = get_file_mtime(file_path)",
        "            if current_mtime > old_mtime:",
        "                modified.append(file_path)",
        "",
        "    # Check for deleted files",
        "    for rel_path in old_files:",
        "        if rel_path not in current_rel_paths:",
        "            deleted_ids.append(rel_path)  # doc_id is the relative path",
        "",
        "    return added, modified, deleted_ids",
        "",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "#!/usr/bin/env python3",
        "\"\"\"",
        "Index the Cortical Text Processor codebase for dog-fooding.",
        "",
        "This script indexes all Python files and documentation to enable",
        "semantic search over the codebase using the Cortical Text Processor itself.",
        ""
      ],
      "context_after": [
        "Usage:",
        "    python scripts/index_codebase.py [--output corpus_dev.pkl]",
        "\"\"\"",
        "",
        "import argparse",
        "import os",
        "import sys",
        "from pathlib import Path",
        "",
        "# Add parent directory to path for imports",
        "sys.path.insert(0, str(Path(__file__).parent.parent))",
        "",
        "from cortical.processor import CorticalTextProcessor",
        "",
        "",
        "def get_python_files(base_path: Path) -> list:",
        "    \"\"\"Get all Python files in cortical/ and tests/ directories.\"\"\"",
        "    files = []",
        "    for directory in ['cortical', 'tests']:",
        "        dir_path = base_path / directory",
        "        if dir_path.exists():",
        "            for py_file in dir_path.rglob('*.py'):",
        "                if not py_file.name.startswith('__'):",
        "                    files.append(py_file)",
        "    return sorted(files)"
      ],
      "change_type": "add"
    },
    {
      "file": "scripts/index_codebase.py",
      "function": "def get_doc_files(base_path: Path) -> list:",
      "start_line": 50,
      "lines_added": [
        "# =============================================================================",
        "# Indexing Operations",
        "# =============================================================================",
        "",
        "def index_file(",
        "    processor: CorticalTextProcessor,",
        "    file_path: Path,",
        "    base_path: Path,",
        "    tracker: Optional[ProgressTracker] = None",
        ") -> Optional[dict]:",
        "        if tracker:",
        "            tracker.warn(f\"Could not read {doc_id}: {e}\")",
        "        'mtime': get_file_mtime(file_path),",
        "def show_status(",
        "    added: List[Path],",
        "    modified: List[Path],",
        "    deleted: List[str],",
        "    base_path: Path,",
        "    tracker: ProgressTracker",
        ") -> None:",
        "    \"\"\"Display what would change without actually indexing.\"\"\"",
        "    tracker.log(\"\\n\" + \"=\" * 50)",
        "    tracker.log(\"STATUS: Changes detected (no indexing performed)\")",
        "    tracker.log(\"=\" * 50)",
        "",
        "    if not added and not modified and not deleted:",
        "        tracker.log(\"\\nNo changes detected. Corpus is up to date.\")",
        "        return",
        "",
        "    if added:",
        "        tracker.log(f\"\\n  Added ({len(added)} files):\")",
        "        for f in added[:10]:",
        "            tracker.log(f\"    + {create_doc_id(f, base_path)}\")",
        "        if len(added) > 10:",
        "            tracker.log(f\"    ... and {len(added) - 10} more\")",
        "",
        "    if modified:",
        "        tracker.log(f\"\\n  Modified ({len(modified)} files):\")",
        "        for f in modified[:10]:",
        "            tracker.log(f\"    ~ {create_doc_id(f, base_path)}\")",
        "        if len(modified) > 10:",
        "            tracker.log(f\"    ... and {len(modified) - 10} more\")",
        "",
        "    if deleted:",
        "        tracker.log(f\"\\n  Deleted ({len(deleted)} files):\")",
        "        for doc_id in deleted[:10]:",
        "            tracker.log(f\"    - {doc_id}\")",
        "        if len(deleted) > 10:",
        "            tracker.log(f\"    ... and {len(deleted) - 10} more\")",
        "",
        "    total = len(added) + len(modified) + len(deleted)",
        "    tracker.log(f\"\\nTotal: {total} files would be updated.\")",
        "    tracker.log(\"Run with --incremental to apply changes.\")",
        "",
        "",
        "def full_index(",
        "    processor: CorticalTextProcessor,",
        "    all_files: List[Path],",
        "    base_path: Path,",
        "    tracker: ProgressTracker",
        ") -> Tuple[int, int, Dict[str, float]]:",
        "    \"\"\"",
        "    Perform a full index of all files.",
        "",
        "    Returns:",
        "        Tuple of (indexed_count, total_lines, file_mtimes)",
        "    \"\"\"",
        "    tracker.start_phase(\"Indexing files\", len(all_files))",
        "",
        "    indexed = 0",
        "    total_lines = 0",
        "    file_mtimes = {}",
        "",
        "    for i, file_path in enumerate(all_files, 1):",
        "        doc_id = create_doc_id(file_path, base_path)",
        "        tracker.update_progress(i, doc_id)",
        "",
        "        metadata = index_file(processor, file_path, base_path, tracker)",
        "        if metadata:",
        "            indexed += 1",
        "            total_lines += metadata.get('line_count', 0)",
        "            file_mtimes[doc_id] = metadata.get('mtime', 0)",
        "",
        "    tracker.end_phase(\"Indexing files\")",
        "    tracker.log(f\"  Indexed {indexed} files ({total_lines:,} total lines)\")",
        "",
        "    return indexed, total_lines, file_mtimes",
        "",
        "",
        "def incremental_index(",
        "    processor: CorticalTextProcessor,",
        "    added: List[Path],",
        "    modified: List[Path],",
        "    deleted: List[str],",
        "    base_path: Path,",
        "    tracker: ProgressTracker",
        ") -> Tuple[int, int, int, int]:",
        "    \"\"\"",
        "    Perform an incremental index updating only changed files.",
        "",
        "    Returns:",
        "        Tuple of (added_count, modified_count, deleted_count, total_lines_updated)",
        "    \"\"\"",
        "    total_items = len(added) + len(modified) + len(deleted)",
        "    tracker.start_phase(\"Incremental update\", total_items)",
        "",
        "    added_count = 0",
        "    modified_count = 0",
        "    deleted_count = 0",
        "    total_lines = 0",
        "    processed = 0",
        "",
        "    # Remove deleted documents",
        "    if deleted:",
        "        tracker.log(f\"  Removing {len(deleted)} deleted files...\")",
        "        for doc_id in deleted:",
        "            result = processor.remove_document(doc_id, verbose=False)",
        "            if result['found']:",
        "                deleted_count += 1",
        "            processed += 1",
        "            tracker.update_progress(processed, f\"Deleted: {doc_id}\")",
        "",
        "    # Update modified documents (remove old, add new)",
        "    if modified:",
        "        tracker.log(f\"  Updating {len(modified)} modified files...\")",
        "        for file_path in modified:",
        "            doc_id = create_doc_id(file_path, base_path)",
        "            # Remove old version",
        "            processor.remove_document(doc_id, verbose=False)",
        "            # Add new version",
        "            metadata = index_file(processor, file_path, base_path, tracker)",
        "            if metadata:",
        "                modified_count += 1",
        "                total_lines += metadata.get('line_count', 0)",
        "            processed += 1",
        "            tracker.update_progress(processed, f\"Modified: {doc_id}\")",
        "",
        "    # Add new documents",
        "    if added:",
        "        tracker.log(f\"  Indexing {len(added)} new files...\")",
        "        for file_path in added:",
        "            doc_id = create_doc_id(file_path, base_path)",
        "            metadata = index_file(processor, file_path, base_path, tracker)",
        "            if metadata:",
        "                added_count += 1",
        "                total_lines += metadata.get('line_count', 0)",
        "            processed += 1",
        "            tracker.update_progress(processed, f\"Added: {doc_id}\")",
        "",
        "    tracker.end_phase(\"Incremental update\")",
        "    tracker.log(f\"  Added: {added_count}, Modified: {modified_count}, Deleted: {deleted_count}\")",
        "    tracker.log(f\"  Lines processed: {total_lines:,}\")",
        "",
        "    return added_count, modified_count, deleted_count, total_lines",
        "",
        "",
        "def compute_analysis(",
        "    processor: CorticalTextProcessor,",
        "    tracker: ProgressTracker,",
        "    fast_mode: bool = True",
        ") -> None:",
        "    \"\"\"",
        "    Run all analysis computations with progress tracking.",
        "",
        "    Args:",
        "        processor: The text processor",
        "        tracker: Progress tracker for logging",
        "        fast_mode: If True, use faster but simpler analysis (skips slow bigram connections).",
        "                   If False, use full semantic PageRank and hybrid connections.",
        "    \"\"\"",
        "    if fast_mode:",
        "        # Fast mode: Skip expensive operations",
        "        # - Use standard PageRank (not semantic/hierarchical)",
        "        # - Skip bigram connections (O(n²) on large corpora)",
        "        # - Skip concept cluster connections",
        "        # Completes in seconds for any size corpus",
        "        tracker.start_phase(\"Computing analysis (fast mode)\")",
        "",
        "        # Manual fast computation - skip compute_all() to avoid bigram connections",
        "        tracker.log(\"  Propagating activation...\", \"debug\")",
        "        processor.propagate_activation(verbose=False)",
        "",
        "        tracker.log(\"  Computing PageRank...\", \"debug\")",
        "        processor.compute_importance(verbose=False)",
        "",
        "        tracker.log(\"  Computing TF-IDF...\", \"debug\")",
        "        processor.compute_tfidf(verbose=False)",
        "",
        "        tracker.log(\"  Computing document connections...\", \"debug\")",
        "        processor.compute_document_connections(verbose=False)",
        "",
        "        # Skip bigram connections (too slow with large corpora)",
        "        # Skip concept clusters (not needed for basic search)",
        "",
        "        tracker.end_phase(\"Computing analysis (fast mode)\")",
        "    else:",
        "        # Full mode: semantic PageRank, hybrid connections",
        "        # More accurate but can take minutes for large codebases",
        "        tracker.start_phase(\"Computing analysis (full mode - may take several minutes)\")",
        "        processor.compute_all(",
        "            build_concepts=True,",
        "            pagerank_method='semantic',",
        "            connection_strategy='hybrid',",
        "            verbose=False",
        "        )",
        "        tracker.end_phase(\"Computing analysis (full mode - may take several minutes)\")",
        "",
        "        tracker.start_phase(\"Extracting semantic relations\")",
        "        processor.extract_corpus_semantics(",
        "            use_pattern_extraction=True,",
        "            verbose=False",
        "        )",
        "        tracker.end_phase(\"Extracting semantic relations\")",
        "",
        "",
        "# =============================================================================",
        "# Main Entry Point",
        "# =============================================================================",
        "",
        "    parser = argparse.ArgumentParser(",
        "        description='Index the codebase for semantic search',",
        "        formatter_class=argparse.RawDescriptionHelpFormatter,",
        "        epilog=\"\"\"",
        "Examples:",
        "  python scripts/index_codebase.py                   # Full rebuild",
        "  python scripts/index_codebase.py --incremental    # Update changed files only",
        "  python scripts/index_codebase.py --status         # Show what would change",
        "  python scripts/index_codebase.py --force          # Force full rebuild",
        "  python scripts/index_codebase.py --log index.log  # Log to file",
        "  python scripts/index_codebase.py --timeout 60     # Timeout after 60s",
        "        \"\"\"",
        "    )",
        "    parser.add_argument('--incremental', '-i', action='store_true',",
        "                        help='Only index changed files (requires existing corpus)')",
        "    parser.add_argument('--force', '-f', action='store_true',",
        "                        help='Force full rebuild even if manifest exists')",
        "    parser.add_argument('--status', '-s', action='store_true',",
        "                        help='Show what would change without indexing')",
        "                        help='Show verbose output (per-file progress)')",
        "    parser.add_argument('--quiet', '-q', action='store_true',",
        "                        help='Suppress console output (still writes to log)')",
        "    parser.add_argument('--log', '-l', type=str, default=None,",
        "                        help='Log file path (writes detailed log)')",
        "    parser.add_argument('--timeout', '-t', type=int, default=DEFAULT_TIMEOUT,",
        "                        help=f'Timeout in seconds (0=none, default={DEFAULT_TIMEOUT})')",
        "    parser.add_argument('--full-analysis', action='store_true',",
        "                        help='Use full semantic analysis (slower but more accurate)')",
        "    manifest_path = get_manifest_path(output_path)",
        "",
        "    # Set up log file path",
        "    log_path = None",
        "    if args.log:",
        "        log_path = args.log if os.path.isabs(args.log) else str(base_path / args.log)",
        "",
        "    # Initialize progress tracker",
        "    tracker = ProgressTracker(",
        "        log_file=log_path,",
        "        verbose=args.verbose,",
        "        quiet=args.quiet",
        "    )",
        "",
        "    tracker.log(\"Cortical Text Processor - Codebase Indexer\")",
        "    tracker.log(\"=\" * 50)",
        "    tracker.log(f\"Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")",
        "    if args.timeout > 0:",
        "        tracker.log(f\"Timeout: {args.timeout}s\")",
        "",
        "    try:",
        "        with timeout_handler(args.timeout, tracker):",
        "            run_indexer(args, base_path, output_path, manifest_path, tracker)",
        "    except TimeoutError:",
        "        tracker.log(\"\\nIndexing was terminated due to timeout.\", \"error\")",
        "        sys.exit(1)",
        "    except KeyboardInterrupt:",
        "        tracker.log(\"\\nIndexing was interrupted by user.\", \"warning\")",
        "        tracker.print_summary()",
        "        sys.exit(1)",
        "    except Exception as e:",
        "        tracker.error(f\"Unexpected error: {e}\")",
        "        tracker.print_summary()",
        "        raise",
        "def run_indexer(",
        "    args,",
        "    base_path: Path,",
        "    output_path: Path,",
        "    manifest_path: Path,",
        "    tracker: ProgressTracker",
        ") -> None:",
        "    \"\"\"Main indexing logic.\"\"\"",
        "    tracker.start_phase(\"Discovering files\")",
        "    tracker.end_phase(\"Discovering files\")",
        "",
        "    tracker.log(f\"\\nFound {len(python_files)} Python files and {len(doc_files)} documentation files\")",
        "",
        "    # Load existing manifest if doing incremental update",
        "    manifest = None",
        "    if (args.incremental or args.status) and not args.force:",
        "        if manifest_path.exists():",
        "            tracker.log(f\"\\nLoading manifest from {manifest_path.name}...\")",
        "            manifest = load_manifest(manifest_path, tracker)",
        "        else:",
        "            tracker.log(\"\\nNo manifest found - will perform full index\")",
        "",
        "    # Detect changes if we have a manifest",
        "    added, modified, deleted = [], [], []",
        "    if manifest:",
        "        added, modified, deleted = get_file_changes(manifest, all_files, base_path)",
        "        tracker.log(f\"\\nChanges detected:\")",
        "        tracker.log(f\"  Added: {len(added)}, Modified: {len(modified)}, Deleted: {len(deleted)}\")",
        "",
        "    # Status mode - just show what would change",
        "    if args.status:",
        "        show_status(added, modified, deleted, base_path, tracker)",
        "        return",
        "",
        "    # Determine if we need to do work",
        "    if manifest and not (added or modified or deleted):",
        "        tracker.log(\"\\nNo changes detected. Corpus is up to date.\")",
        "        tracker.log(\"Use --force to rebuild anyway.\")",
        "        return",
        "",
        "    # Initialize or load processor",
        "    if args.incremental and manifest and output_path.exists():",
        "        tracker.start_phase(\"Loading existing corpus\")",
        "        try:",
        "            processor = CorticalTextProcessor.load(str(output_path))",
        "            tracker.log(f\"  Loaded {len(processor.documents)} documents\")",
        "            tracker.end_phase(\"Loading existing corpus\")",
        "        except Exception as e:",
        "            tracker.warn(f\"Error loading corpus: {e}\")",
        "            tracker.log(\"  Falling back to full rebuild...\")",
        "            tracker.end_phase(\"Loading existing corpus\", status=\"failed\")",
        "            processor = CorticalTextProcessor()",
        "            added, modified, deleted = all_files, [], []",
        "    else:",
        "        processor = CorticalTextProcessor()",
        "        # Full index - treat all files as \"added\"",
        "        added = all_files",
        "        modified = []",
        "        deleted = []",
        "",
        "    # Perform indexing",
        "    if args.incremental and manifest:",
        "        incremental_index(processor, added, modified, deleted, base_path, tracker)",
        "    else:",
        "        full_index(processor, all_files, base_path, tracker)",
        "",
        "    # Compute analysis",
        "    fast_mode = not args.full_analysis",
        "    compute_analysis(processor, tracker, fast_mode=fast_mode)",
        "",
        "    # Print corpus statistics",
        "    tracker.log(\"\\nCorpus Statistics:\")",
        "    tracker.log(f\"  Documents: {len(processor.documents)}\")",
        "    tracker.log(f\"  Tokens (Layer 0): {processor.layers[0].column_count()}\")",
        "    tracker.log(f\"  Bigrams (Layer 1): {processor.layers[1].column_count()}\")",
        "    tracker.log(f\"  Concepts (Layer 2): {processor.layers[2].column_count()}\")",
        "    tracker.log(f\"  Semantic relations: {len(processor.semantic_relations)}\")",
        "    tracker.start_phase(\"Saving corpus\")",
        "    tracker.log(f\"  Saved to {output_path.name} ({file_size:.1f} KB)\")",
        "    tracker.end_phase(\"Saving corpus\")",
        "",
        "    # Build file_mtimes for manifest",
        "    file_mtimes = {}",
        "    for file_path in all_files:",
        "        rel_path = create_doc_id(file_path, base_path)",
        "        if rel_path in processor.documents:",
        "            file_mtimes[rel_path] = get_file_mtime(file_path)",
        "",
        "    # Save manifest",
        "    stats = {",
        "        'documents': len(processor.documents),",
        "        'tokens': processor.layers[0].column_count(),",
        "        'bigrams': processor.layers[1].column_count(),",
        "        'concepts': processor.layers[2].column_count(),",
        "        'semantic_relations': len(processor.semantic_relations),",
        "    }",
        "    save_manifest(manifest_path, file_mtimes, str(output_path), stats, tracker)",
        "",
        "    # Print summary",
        "    tracker.print_summary()",
        "    tracker.log(\"\\nDone! Use search_codebase.py to query the indexed corpus.\")"
      ],
      "lines_removed": [
        "def index_file(processor: CorticalTextProcessor, file_path: Path, base_path: Path) -> dict:",
        "        print(f\"  Warning: Could not read {doc_id}: {e}\")",
        "    parser = argparse.ArgumentParser(description='Index the codebase for semantic search')",
        "                        help='Show verbose output')",
        "    print(\"Cortical Text Processor - Codebase Indexer\")",
        "    print(\"=\" * 50)",
        "    # Initialize processor",
        "    processor = CorticalTextProcessor()",
        "",
        "    print(f\"\\nFound {len(python_files)} Python files and {len(doc_files)} documentation files\")",
        "",
        "    # Index all files",
        "    print(\"\\nIndexing files...\")",
        "    indexed = 0",
        "    total_lines = 0",
        "",
        "    for file_path in all_files:",
        "        if args.verbose:",
        "            print(f\"  Indexing: {create_doc_id(file_path, base_path)}\")",
        "",
        "        metadata = index_file(processor, file_path, base_path)",
        "        if metadata:",
        "            indexed += 1",
        "            total_lines += metadata.get('line_count', 0)",
        "",
        "    print(f\"  Indexed {indexed} files ({total_lines:,} total lines)\")",
        "",
        "    # Compute all analysis",
        "    print(\"\\nComputing analysis...\")",
        "    processor.compute_all(",
        "        build_concepts=True,",
        "        pagerank_method='semantic',",
        "        connection_strategy='hybrid',",
        "        verbose=args.verbose",
        "    )",
        "",
        "    # Extract semantic relations",
        "    print(\"Extracting semantic relations...\")",
        "    processor.extract_corpus_semantics(",
        "        use_pattern_extraction=True,",
        "        verbose=args.verbose",
        "    )",
        "",
        "    # Print statistics",
        "    print(\"\\nCorpus Statistics:\")",
        "    print(f\"  Documents: {len(processor.documents)}\")",
        "    print(f\"  Tokens (Layer 0): {processor.layers[0].column_count()}\")",
        "    print(f\"  Bigrams (Layer 1): {processor.layers[1].column_count()}\")",
        "    print(f\"  Concepts (Layer 2): {processor.layers[2].column_count()}\")",
        "    print(f\"  Semantic relations: {len(processor.semantic_relations)}\")",
        "    print(f\"\\nSaving corpus to {output_path}...\")",
        "",
        "    print(f\"  Saved ({file_size:.1f} KB)\")",
        "    print(\"\\nDone! Use search_codebase.py to query the indexed corpus.\")"
      ],
      "context_before": [
        "",
        "    return files",
        "",
        "",
        "def create_doc_id(file_path: Path, base_path: Path) -> str:",
        "    \"\"\"Create a document ID from file path.\"\"\"",
        "    rel_path = file_path.relative_to(base_path)",
        "    return str(rel_path)",
        "",
        ""
      ],
      "context_after": [
        "    \"\"\"Index a single file with line number metadata.\"\"\"",
        "    doc_id = create_doc_id(file_path, base_path)",
        "",
        "    try:",
        "        content = file_path.read_text(encoding='utf-8')",
        "    except Exception as e:",
        "        return None",
        "",
        "    # Create metadata with file info",
        "    metadata = {",
        "        'file_path': str(file_path),",
        "        'relative_path': doc_id,",
        "        'file_type': file_path.suffix,",
        "        'line_count': content.count('\\n') + 1,",
        "    }",
        "",
        "    # For Python files, extract additional metadata",
        "    if file_path.suffix == '.py':",
        "        metadata['language'] = 'python'",
        "        # Count functions and classes",
        "        metadata['function_count'] = content.count('\\ndef ')",
        "        metadata['class_count'] = content.count('\\nclass ')",
        "",
        "    processor.process_document(doc_id, content, metadata=metadata)",
        "    return metadata",
        "",
        "",
        "def main():",
        "    parser.add_argument('--output', '-o', default='corpus_dev.pkl',",
        "                        help='Output file path (default: corpus_dev.pkl)')",
        "    parser.add_argument('--verbose', '-v', action='store_true',",
        "    args = parser.parse_args()",
        "",
        "    base_path = Path(__file__).parent.parent",
        "    output_path = base_path / args.output",
        "",
        "",
        "",
        "    # Get files to index",
        "    python_files = get_python_files(base_path)",
        "    doc_files = get_doc_files(base_path)",
        "    all_files = python_files + doc_files",
        "",
        "    # Save corpus",
        "    processor.save(str(output_path))",
        "    file_size = output_path.stat().st_size / 1024",
        "",
        "",
        "",
        "if __name__ == '__main__':",
        "    main()"
      ],
      "change_type": "modify"
    },
    {
      "file": "tests/test_incremental_indexing.py",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "\"\"\"",
        "Tests for incremental indexing functionality.",
        "",
        "Tests cover:",
        "- remove_document() method in processor",
        "- remove_minicolumn() method in layers",
        "- Manifest file operations",
        "- File change detection",
        "\"\"\"",
        "",
        "import json",
        "import os",
        "import sys",
        "import tempfile",
        "import unittest",
        "from pathlib import Path",
        "from unittest.mock import patch, MagicMock",
        "",
        "# Add parent directory to path for imports",
        "sys.path.insert(0, str(Path(__file__).parent.parent))",
        "",
        "from cortical.processor import CorticalTextProcessor",
        "from cortical.layers import CorticalLayer, HierarchicalLayer",
        "from cortical.minicolumn import Minicolumn",
        "",
        "",
        "class TestRemoveDocument(unittest.TestCase):",
        "    \"\"\"Tests for CorticalTextProcessor.remove_document()\"\"\"",
        "",
        "    def setUp(self):",
        "        \"\"\"Set up a processor with test documents.\"\"\"",
        "        self.processor = CorticalTextProcessor()",
        "        self.processor.process_document(\"doc1\", \"Neural networks process information efficiently.\")",
        "        self.processor.process_document(\"doc2\", \"Machine learning algorithms learn patterns.\")",
        "        self.processor.process_document(\"doc3\", \"Neural machine translation uses deep learning.\")",
        "        self.processor.compute_all(verbose=False)",
        "",
        "    def test_remove_document_basic(self):",
        "        \"\"\"Test basic document removal.\"\"\"",
        "        self.assertEqual(len(self.processor.documents), 3)",
        "",
        "        result = self.processor.remove_document(\"doc1\")",
        "",
        "        self.assertTrue(result['found'])",
        "        self.assertEqual(len(self.processor.documents), 2)",
        "        self.assertNotIn(\"doc1\", self.processor.documents)",
        "",
        "    def test_remove_document_not_found(self):",
        "        \"\"\"Test removing a non-existent document.\"\"\"",
        "        result = self.processor.remove_document(\"nonexistent\")",
        "",
        "        self.assertFalse(result['found'])",
        "        self.assertEqual(result['tokens_affected'], 0)",
        "        self.assertEqual(result['bigrams_affected'], 0)",
        "",
        "    def test_remove_document_cleans_token_document_ids(self):",
        "        \"\"\"Test that document ID is removed from token document_ids sets.\"\"\"",
        "        layer0 = self.processor.layers[CorticalLayer.TOKENS]",
        "",
        "        # neural appears in doc1 and doc3",
        "        neural_col = layer0.get_minicolumn(\"neural\")",
        "        self.assertIn(\"doc1\", neural_col.document_ids)",
        "",
        "        self.processor.remove_document(\"doc1\")",
        "",
        "        # neural should no longer reference doc1",
        "        self.assertNotIn(\"doc1\", neural_col.document_ids)",
        "        # But should still reference doc3",
        "        self.assertIn(\"doc3\", neural_col.document_ids)",
        "",
        "    def test_remove_document_cleans_bigram_document_ids(self):",
        "        \"\"\"Test that document ID is removed from bigram document_ids sets.\"\"\"",
        "        layer1 = self.processor.layers[CorticalLayer.BIGRAMS]",
        "",
        "        # Find a bigram from doc1",
        "        bigram_col = layer1.get_minicolumn(\"neural networks\")",
        "        if bigram_col:",
        "            self.assertIn(\"doc1\", bigram_col.document_ids)",
        "            self.processor.remove_document(\"doc1\")",
        "            self.assertNotIn(\"doc1\", bigram_col.document_ids)",
        "",
        "    def test_remove_document_removes_layer3_minicolumn(self):",
        "        \"\"\"Test that the document minicolumn is removed from Layer 3.\"\"\"",
        "        layer3 = self.processor.layers[CorticalLayer.DOCUMENTS]",
        "",
        "        self.assertIn(\"doc1\", layer3.minicolumns)",
        "        self.processor.remove_document(\"doc1\")",
        "        self.assertNotIn(\"doc1\", layer3.minicolumns)",
        "",
        "    def test_remove_document_removes_metadata(self):",
        "        \"\"\"Test that document metadata is removed.\"\"\"",
        "        self.processor.set_document_metadata(\"doc1\", source=\"test\")",
        "        self.assertEqual(self.processor.get_document_metadata(\"doc1\"), {\"source\": \"test\"})",
        "",
        "        self.processor.remove_document(\"doc1\")",
        "",
        "        self.assertEqual(self.processor.get_document_metadata(\"doc1\"), {})",
        "",
        "    def test_remove_document_marks_stale(self):",
        "        \"\"\"Test that removal marks computations as stale.\"\"\"",
        "        # After compute_all, computations should not be stale",
        "        self.assertFalse(self.processor.is_stale(self.processor.COMP_TFIDF))",
        "",
        "        self.processor.remove_document(\"doc1\")",
        "",
        "        # After removal, computations should be stale",
        "        self.assertTrue(self.processor.is_stale(self.processor.COMP_TFIDF))",
        "",
        "    def test_remove_document_returns_affected_counts(self):",
        "        \"\"\"Test that removal returns correct affected counts.\"\"\"",
        "        result = self.processor.remove_document(\"doc1\")",
        "",
        "        self.assertTrue(result['found'])",
        "        self.assertGreater(result['tokens_affected'], 0)",
        "        self.assertGreater(result['bigrams_affected'], 0)",
        "",
        "    def test_remove_document_verbose(self):",
        "        \"\"\"Test verbose mode prints output.\"\"\"",
        "        with patch('builtins.print') as mock_print:",
        "            self.processor.remove_document(\"doc1\", verbose=True)",
        "            mock_print.assert_called()",
        "",
        "",
        "class TestRemoveDocumentsBatch(unittest.TestCase):",
        "    \"\"\"Tests for CorticalTextProcessor.remove_documents_batch()\"\"\"",
        "",
        "    def setUp(self):",
        "        \"\"\"Set up a processor with test documents.\"\"\"",
        "        self.processor = CorticalTextProcessor()",
        "        for i in range(5):",
        "            self.processor.process_document(f\"doc{i}\", f\"Document {i} content here.\")",
        "        self.processor.compute_all(verbose=False)",
        "",
        "    def test_remove_documents_batch_basic(self):",
        "        \"\"\"Test removing multiple documents.\"\"\"",
        "        result = self.processor.remove_documents_batch([\"doc0\", \"doc1\", \"doc2\"])",
        "",
        "        self.assertEqual(result['documents_removed'], 3)",
        "        self.assertEqual(result['documents_not_found'], 0)",
        "        self.assertEqual(len(self.processor.documents), 2)",
        "",
        "    def test_remove_documents_batch_with_missing(self):",
        "        \"\"\"Test removing documents when some don't exist.\"\"\"",
        "        result = self.processor.remove_documents_batch([\"doc0\", \"nonexistent\", \"doc1\"])",
        "",
        "        self.assertEqual(result['documents_removed'], 2)",
        "        self.assertEqual(result['documents_not_found'], 1)",
        "",
        "    def test_remove_documents_batch_with_recompute_tfidf(self):",
        "        \"\"\"Test batch removal with TF-IDF recomputation.\"\"\"",
        "        result = self.processor.remove_documents_batch([\"doc0\"], recompute='tfidf')",
        "",
        "        self.assertEqual(result['recomputation'], 'tfidf')",
        "        self.assertFalse(self.processor.is_stale(self.processor.COMP_TFIDF))",
        "",
        "    def test_remove_documents_batch_with_recompute_full(self):",
        "        \"\"\"Test batch removal with full recomputation.\"\"\"",
        "        result = self.processor.remove_documents_batch([\"doc0\"], recompute='full')",
        "",
        "        self.assertEqual(result['recomputation'], 'full')",
        "        self.assertEqual(len(self.processor.get_stale_computations()), 0)",
        "",
        "",
        "class TestRemoveMinicolumn(unittest.TestCase):",
        "    \"\"\"Tests for HierarchicalLayer.remove_minicolumn()\"\"\"",
        "",
        "    def setUp(self):",
        "        \"\"\"Set up a test layer with minicolumns.\"\"\"",
        "        self.layer = HierarchicalLayer(CorticalLayer.TOKENS)",
        "        self.layer.get_or_create_minicolumn(\"test\")",
        "        self.layer.get_or_create_minicolumn(\"neural\")",
        "        self.layer.get_or_create_minicolumn(\"network\")",
        "",
        "    def test_remove_minicolumn_basic(self):",
        "        \"\"\"Test basic minicolumn removal.\"\"\"",
        "        self.assertEqual(self.layer.column_count(), 3)",
        "",
        "        result = self.layer.remove_minicolumn(\"test\")",
        "",
        "        self.assertTrue(result)",
        "        self.assertEqual(self.layer.column_count(), 2)",
        "        self.assertNotIn(\"test\", self.layer.minicolumns)",
        "",
        "    def test_remove_minicolumn_not_found(self):",
        "        \"\"\"Test removing non-existent minicolumn.\"\"\"",
        "        result = self.layer.remove_minicolumn(\"nonexistent\")",
        "",
        "        self.assertFalse(result)",
        "        self.assertEqual(self.layer.column_count(), 3)",
        "",
        "    def test_remove_minicolumn_removes_from_id_index(self):",
        "        \"\"\"Test that removal updates the ID index.\"\"\"",
        "        col = self.layer.get_minicolumn(\"test\")",
        "        col_id = col.id",
        "",
        "        self.assertIsNotNone(self.layer.get_by_id(col_id))",
        "",
        "        self.layer.remove_minicolumn(\"test\")",
        "",
        "        self.assertIsNone(self.layer.get_by_id(col_id))",
        "",
        "",
        "class TestManifestOperations(unittest.TestCase):",
        "    \"\"\"Tests for manifest file operations in index_codebase.py\"\"\"",
        "",
        "    def setUp(self):",
        "        \"\"\"Set up temporary directory for tests.\"\"\"",
        "        self.temp_dir = tempfile.mkdtemp()",
        "        self.manifest_path = Path(self.temp_dir) / \"test.manifest.json\"",
        "",
        "    def tearDown(self):",
        "        \"\"\"Clean up temporary files.\"\"\"",
        "        import shutil",
        "        shutil.rmtree(self.temp_dir)",
        "",
        "    def test_save_manifest(self):",
        "        \"\"\"Test saving a manifest file.\"\"\"",
        "        # Import the functions from the script",
        "        sys.path.insert(0, str(Path(__file__).parent.parent / 'scripts'))",
        "        from index_codebase import save_manifest, load_manifest",
        "",
        "        files = {",
        "            \"cortical/processor.py\": 1234567890.0,",
        "            \"tests/test_processor.py\": 1234567891.0,",
        "        }",
        "        stats = {\"documents\": 2, \"tokens\": 100}",
        "",
        "        save_manifest(self.manifest_path, files, \"corpus.pkl\", stats)",
        "",
        "        self.assertTrue(self.manifest_path.exists())",
        "",
        "        # Verify content",
        "        with open(self.manifest_path) as f:",
        "            data = json.load(f)",
        "",
        "        self.assertEqual(data['version'], \"1.0\")",
        "        self.assertEqual(data['corpus_path'], \"corpus.pkl\")",
        "        self.assertEqual(len(data['files']), 2)",
        "        self.assertEqual(data['stats']['documents'], 2)",
        "",
        "    def test_load_manifest_valid(self):",
        "        \"\"\"Test loading a valid manifest file.\"\"\"",
        "        sys.path.insert(0, str(Path(__file__).parent.parent / 'scripts'))",
        "        from index_codebase import save_manifest, load_manifest",
        "",
        "        files = {\"test.py\": 1234567890.0}",
        "        save_manifest(self.manifest_path, files, \"corpus.pkl\", {})",
        "",
        "        manifest = load_manifest(self.manifest_path)",
        "",
        "        self.assertIsNotNone(manifest)",
        "        self.assertEqual(manifest['files'], files)",
        "",
        "    def test_load_manifest_not_exists(self):",
        "        \"\"\"Test loading a non-existent manifest file.\"\"\"",
        "        sys.path.insert(0, str(Path(__file__).parent.parent / 'scripts'))",
        "        from index_codebase import load_manifest",
        "",
        "        manifest = load_manifest(Path(self.temp_dir) / \"nonexistent.json\")",
        "",
        "        self.assertIsNone(manifest)",
        "",
        "    def test_load_manifest_invalid_version(self):",
        "        \"\"\"Test loading a manifest with wrong version.\"\"\"",
        "        sys.path.insert(0, str(Path(__file__).parent.parent / 'scripts'))",
        "        from index_codebase import load_manifest",
        "",
        "        # Write manifest with wrong version",
        "        with open(self.manifest_path, 'w') as f:",
        "            json.dump({\"version\": \"0.1\", \"files\": {}}, f)",
        "",
        "        manifest = load_manifest(self.manifest_path)",
        "",
        "        self.assertIsNone(manifest)",
        "",
        "",
        "class TestFileChangeDetection(unittest.TestCase):",
        "    \"\"\"Tests for file change detection.\"\"\"",
        "",
        "    def setUp(self):",
        "        \"\"\"Set up temporary directory with test files.\"\"\"",
        "        self.temp_dir = tempfile.mkdtemp()",
        "        self.base_path = Path(self.temp_dir)",
        "",
        "        # Create some test files",
        "        (self.base_path / \"file1.py\").write_text(\"content1\")",
        "        (self.base_path / \"file2.py\").write_text(\"content2\")",
        "        (self.base_path / \"file3.py\").write_text(\"content3\")",
        "",
        "    def tearDown(self):",
        "        \"\"\"Clean up temporary files.\"\"\"",
        "        import shutil",
        "        shutil.rmtree(self.temp_dir)",
        "",
        "    def test_get_file_changes_no_changes(self):",
        "        \"\"\"Test detecting no changes.\"\"\"",
        "        sys.path.insert(0, str(Path(__file__).parent.parent / 'scripts'))",
        "        from index_codebase import get_file_changes, get_file_mtime",
        "",
        "        current_files = list(self.base_path.glob(\"*.py\"))",
        "        manifest = {",
        "            'files': {",
        "                str(f.relative_to(self.base_path)): get_file_mtime(f)",
        "                for f in current_files",
        "            }",
        "        }",
        "",
        "        added, modified, deleted = get_file_changes(manifest, current_files, self.base_path)",
        "",
        "        self.assertEqual(len(added), 0)",
        "        self.assertEqual(len(modified), 0)",
        "        self.assertEqual(len(deleted), 0)",
        "",
        "    def test_get_file_changes_added_file(self):",
        "        \"\"\"Test detecting added files.\"\"\"",
        "        sys.path.insert(0, str(Path(__file__).parent.parent / 'scripts'))",
        "        from index_codebase import get_file_changes, get_file_mtime",
        "",
        "        # Create manifest without file3.py",
        "        manifest = {",
        "            'files': {",
        "                \"file1.py\": get_file_mtime(self.base_path / \"file1.py\"),",
        "                \"file2.py\": get_file_mtime(self.base_path / \"file2.py\"),",
        "            }",
        "        }",
        "",
        "        current_files = list(self.base_path.glob(\"*.py\"))",
        "        added, modified, deleted = get_file_changes(manifest, current_files, self.base_path)",
        "",
        "        self.assertEqual(len(added), 1)",
        "        self.assertEqual(added[0].name, \"file3.py\")",
        "        self.assertEqual(len(modified), 0)",
        "        self.assertEqual(len(deleted), 0)",
        "",
        "    def test_get_file_changes_deleted_file(self):",
        "        \"\"\"Test detecting deleted files.\"\"\"",
        "        sys.path.insert(0, str(Path(__file__).parent.parent / 'scripts'))",
        "        from index_codebase import get_file_changes, get_file_mtime",
        "",
        "        # Create manifest with an extra file that doesn't exist",
        "        manifest = {",
        "            'files': {",
        "                \"file1.py\": get_file_mtime(self.base_path / \"file1.py\"),",
        "                \"file2.py\": get_file_mtime(self.base_path / \"file2.py\"),",
        "                \"file3.py\": get_file_mtime(self.base_path / \"file3.py\"),",
        "                \"deleted.py\": 1234567890.0,  # This file doesn't exist",
        "            }",
        "        }",
        "",
        "        current_files = list(self.base_path.glob(\"*.py\"))",
        "        added, modified, deleted = get_file_changes(manifest, current_files, self.base_path)",
        "",
        "        self.assertEqual(len(added), 0)",
        "        self.assertEqual(len(modified), 0)",
        "        self.assertEqual(len(deleted), 1)",
        "        self.assertIn(\"deleted.py\", deleted)",
        "",
        "    def test_get_file_changes_modified_file(self):",
        "        \"\"\"Test detecting modified files.\"\"\"",
        "        sys.path.insert(0, str(Path(__file__).parent.parent / 'scripts'))",
        "        from index_codebase import get_file_changes, get_file_mtime",
        "        import time",
        "",
        "        # Create manifest with old mtime",
        "        manifest = {",
        "            'files': {",
        "                \"file1.py\": 0.0,  # Very old mtime",
        "                \"file2.py\": get_file_mtime(self.base_path / \"file2.py\"),",
        "                \"file3.py\": get_file_mtime(self.base_path / \"file3.py\"),",
        "            }",
        "        }",
        "",
        "        current_files = list(self.base_path.glob(\"*.py\"))",
        "        added, modified, deleted = get_file_changes(manifest, current_files, self.base_path)",
        "",
        "        self.assertEqual(len(added), 0)",
        "        self.assertEqual(len(modified), 1)",
        "        self.assertEqual(modified[0].name, \"file1.py\")",
        "        self.assertEqual(len(deleted), 0)",
        "",
        "",
        "class TestIncrementalIndexingIntegration(unittest.TestCase):",
        "    \"\"\"Integration tests for incremental indexing workflow.\"\"\"",
        "",
        "    def test_add_remove_reindex_workflow(self):",
        "        \"\"\"Test the full workflow of add, remove, and reindex.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "",
        "        # Initial indexing",
        "        processor.process_document(\"doc1\", \"Neural networks are powerful.\")",
        "        processor.process_document(\"doc2\", \"Machine learning is useful.\")",
        "        processor.compute_all(verbose=False)",
        "",
        "        initial_doc_count = len(processor.documents)",
        "        self.assertEqual(initial_doc_count, 2)",
        "",
        "        # Remove a document",
        "        result = processor.remove_document(\"doc1\")",
        "        self.assertTrue(result['found'])",
        "        self.assertEqual(len(processor.documents), 1)",
        "",
        "        # Add a new document",
        "        processor.process_document(\"doc3\", \"Deep learning advances rapidly.\")",
        "",
        "        # Recompute",
        "        processor.compute_all(verbose=False)",
        "",
        "        # Verify final state",
        "        self.assertEqual(len(processor.documents), 2)",
        "        self.assertNotIn(\"doc1\", processor.documents)",
        "        self.assertIn(\"doc2\", processor.documents)",
        "        self.assertIn(\"doc3\", processor.documents)",
        "",
        "    def test_incremental_preserves_other_documents(self):",
        "        \"\"\"Test that incremental updates don't affect unchanged documents.\"\"\"",
        "        processor = CorticalTextProcessor()",
        "",
        "        processor.process_document(\"doc1\", \"The quick brown fox.\")",
        "        processor.process_document(\"doc2\", \"Jumps over the lazy dog.\")",
        "        processor.compute_all(verbose=False)",
        "",
        "        # Store original state of doc2",
        "        layer0 = processor.layers[CorticalLayer.TOKENS]",
        "        original_quick_docs = layer0.get_minicolumn(\"quick\").document_ids.copy()",
        "",
        "        # Remove doc1",
        "        processor.remove_document(\"doc1\")",
        "",
        "        # doc2 tokens should still reference doc2",
        "        lazy_col = layer0.get_minicolumn(\"lazy\")",
        "        self.assertIn(\"doc2\", lazy_col.document_ids)",
        "",
        "",
        "if __name__ == '__main__':",
        "    unittest.main()"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    }
  ],
  "hour_of_day": 20,
  "day_of_week": "Wednesday",
  "seconds_since_last_commit": -408177,
  "is_merge": false,
  "is_initial": false,
  "parent_count": 1,
  "session_id": null,
  "related_chats": [],
  "ci_result": null,
  "reverted": false,
  "amended": false
}