{
  "hash": "acf0f984f98235e6283af1a09ddb1bb2343e3012",
  "message": "docs: Add text-as-memories knowledge transfer and samples",
  "author": "Claude",
  "timestamp": "2025-12-14 17:17:02 +0000",
  "branch": "claude/multi-index-design-DvifZ",
  "files_changed": [
    "docs/text-as-memories.md",
    "samples/decisions/adr-microseconds-task-id.md",
    "samples/memories/2025-12-14-dogfooding-session.md",
    "samples/memories/concept-hebbian-text-processing.md",
    "tasks/2025-12-14_17-13-01_6aa8.json"
  ],
  "insertions": 724,
  "deletions": 0,
  "hunks": [
    {
      "file": "docs/text-as-memories.md",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "# Text as Memories: A Knowledge Transfer Guide",
        "",
        "**Date:** 2025-12-14",
        "**Status:** Complete",
        "**Related Tasks:** Search relevance investigation (T-20251214-171301-6aa8-001)",
        "",
        "## Executive Summary",
        "",
        "This document explores how text documents can be conceptualized as **memories**—discrete units of knowledge that, when stored in git, form a persistent, versioned, and interconnected knowledge base. The Cortical Text Processor provides the algorithmic foundation for this metaphor, treating documents as \"episodes\" and terms as \"associations\" that strengthen through co-occurrence.",
        "",
        "---",
        "",
        "## The Memory Metaphor",
        "",
        "### Human Memory vs. Text Systems",
        "",
        "| Human Memory | Text in Git | Cortical Text Processor |",
        "|--------------|-------------|-------------------------|",
        "| **Encoding** | Writing a document | `process_document()` |",
        "| **Storage** | Committing to git | Minicolumns + connections |",
        "| **Consolidation** | Merging branches | PageRank identifies hubs |",
        "| **Retrieval** | Git search / blame | Query expansion + ranking |",
        "| **Association** | Related memories activate | Lateral connections fire |",
        "| **Forgetting** | Deleting files | `remove_document()` |",
        "",
        "### Why This Matters",
        "",
        "When you commit text to git, you're not just storing files—you're creating a **persistent memory system** that:",
        "",
        "1. **Tracks evolution**: Git history shows how ideas develop over time",
        "2. **Enables collaboration**: Multiple minds contribute to shared memory",
        "3. **Supports retrieval**: Semantic search finds related concepts",
        "4. **Preserves context**: Blame shows when and why knowledge was added",
        "",
        "---",
        "",
        "## The Cortical Processing Model",
        "",
        "### Four Layers of Memory",
        "",
        "The processor organizes text into hierarchical layers, inspired by how the visual cortex processes information:",
        "",
        "```",
        "Layer 3: DOCUMENTS   →  \"Episodes\" (complete experiences)",
        "         ↑",
        "Layer 2: CONCEPTS    →  \"Schemas\" (organized knowledge structures)",
        "         ↑",
        "Layer 1: BIGRAMS     →  \"Associations\" (paired ideas)",
        "         ↑",
        "Layer 0: TOKENS      →  \"Features\" (atomic elements)",
        "```",
        "",
        "**Example progression:**",
        "",
        "```",
        "Document: \"Neural networks learn patterns from data\"",
        "",
        "Layer 0 (Features):     [neural, networks, learn, patterns, data]",
        "Layer 1 (Associations): [neural networks, networks learn, learn patterns, patterns data]",
        "Layer 2 (Schema):       [machine_learning_concepts]",
        "Layer 3 (Episode):      [doc_ml_intro_001]",
        "```",
        "",
        "### Hebbian Learning: \"Neurons That Fire Together, Wire Together\"",
        "",
        "The processor applies this neuroscience principle to text:",
        "",
        "```python",
        "# When \"neural\" and \"networks\" appear together repeatedly:",
        "col_neural.lateral_connections[\"networks\"] += 1",
        "",
        "# After processing 100 documents:",
        "# \"neural\" → \"networks\": weight 87",
        "# \"neural\" → \"learning\": weight 45",
        "# \"neural\" → \"artificial\": weight 23",
        "```",
        "",
        "**Result**: Terms that co-occur form stronger associations, enabling retrieval through related concepts.",
        "",
        "---",
        "",
        "## Git as Memory Infrastructure",
        "",
        "### The Version Control Memory Model",
        "",
        "```",
        "Repository = Long-term Memory Store",
        "├── main branch = Consolidated, verified knowledge",
        "├── feature branches = Working memory (experimental ideas)",
        "├── commits = Memory encoding events",
        "├── merges = Memory consolidation",
        "└── tags = Landmark memories (releases, milestones)",
        "```",
        "",
        "### Memory Operations Mapped to Git",
        "",
        "| Memory Operation | Git Command | Effect |",
        "|-----------------|-------------|--------|",
        "| **Encode new memory** | `git add && git commit` | Store new knowledge |",
        "| **Recall** | `git log --grep` or semantic search | Retrieve by content |",
        "| **Update memory** | `git commit --amend` | Modify recent memory |",
        "| **Consolidate** | `git merge` | Integrate new with existing |",
        "| **Create association** | Cross-reference in docs | Link related memories |",
        "| **Time travel** | `git checkout <hash>` | Access past states |",
        "| **Attribution** | `git blame` | Who knew what when |",
        "",
        "### Practical Pattern: Memory Journaling",
        "",
        "Store thoughts, decisions, and learnings as versioned documents:",
        "",
        "```bash",
        "# Create a memory entry",
        "echo \"Today I learned that PageRank...\" > memories/2025-12-14-pagerank.md",
        "git add memories/",
        "git commit -m \"memory: PageRank insight from debugging session\"",
        "",
        "# Later, search your memories",
        "git log --all --grep=\"PageRank\" --oneline",
        "python scripts/search_codebase.py \"PageRank importance calculation\"",
        "```",
        "",
        "---",
        "",
        "## Building a Personal Knowledge Base",
        "",
        "### Directory Structure for Memory Storage",
        "",
        "```",
        "knowledge-base/",
        "├── memories/                    # Daily/episodic memories",
        "│   ├── 2025-12-14-security.md",
        "│   └── 2025-12-15-debugging.md",
        "├── concepts/                    # Consolidated knowledge",
        "│   ├── algorithms/",
        "│   │   ├── pagerank.md",
        "│   │   └── tfidf.md",
        "│   └── patterns/",
        "│       └── hebbian-learning.md",
        "├── decisions/                   # Architectural decisions",
        "│   └── adr-001-layer-structure.md",
        "└── corpus_dev.pkl              # Indexed for semantic search",
        "```",
        "",
        "### Indexing Your Memories",
        "",
        "```python",
        "from cortical import CorticalTextProcessor",
        "import glob",
        "",
        "# Create processor",
        "processor = CorticalTextProcessor()",
        "",
        "# Index all memories",
        "for filepath in glob.glob(\"knowledge-base/**/*.md\", recursive=True):",
        "    with open(filepath) as f:",
        "        processor.process_document(filepath, f.read())",
        "",
        "# Build connections",
        "processor.compute_all()",
        "",
        "# Save for future sessions",
        "processor.save(\"knowledge-base/corpus_dev.pkl\")",
        "```",
        "",
        "### Querying Your Knowledge",
        "",
        "```python",
        "# Load your memory index",
        "processor = CorticalTextProcessor.load(\"knowledge-base/corpus_dev.pkl\")",
        "",
        "# Semantic search",
        "results = processor.find_documents_for_query(\"debugging network issues\")",
        "for doc_id, score in results:",
        "    print(f\"{doc_id}: {score:.2f}\")",
        "",
        "# Find related concepts",
        "expanded = processor.expand_query(\"network\")",
        "# Returns: {network: 1.0, connection: 0.67, layer: 0.45, ...}",
        "```",
        "",
        "---",
        "",
        "## Memory Consolidation Patterns",
        "",
        "### Pattern 1: Daily Memory Capture",
        "",
        "```markdown",
        "<!-- memories/2025-12-14.md -->",
        "# Memory Entry: 2025-12-14",
        "",
        "## What I Learned",
        "- Fuzzing found a bug in config validation",
        "- NaN and infinity weren't being rejected",
        "",
        "## Connections",
        "- Related to: [[concepts/validation.md]]",
        "- Builds on: [[memories/2025-12-13.md]]",
        "",
        "## Future Exploration",
        "- [ ] Apply fuzzing to other validation code",
        "```",
        "",
        "### Pattern 2: Concept Consolidation",
        "",
        "When a topic appears in multiple memories, consolidate:",
        "",
        "```markdown",
        "<!-- concepts/input-validation.md -->",
        "# Input Validation",
        "",
        "## Core Principle",
        "Always validate at system boundaries.",
        "",
        "## Learned From",
        "- 2025-12-14: Fuzzing found NaN/inf bug",
        "- 2025-12-10: Path traversal prevention added",
        "",
        "## Implementation",
        "See: cortical/validation.py",
        "",
        "## Related Concepts",
        "- [[concepts/security.md]]",
        "- [[concepts/testing.md]]",
        "```",
        "",
        "### Pattern 3: Decision Records",
        "",
        "Capture **why** decisions were made:",
        "",
        "```markdown",
        "<!-- decisions/adr-003-microseconds-in-task-id.md -->",
        "# ADR-003: Add Microseconds to Task IDs",
        "",
        "## Status",
        "Accepted (2025-12-14)",
        "",
        "## Context",
        "Task IDs were colliding when generated in tight loops.",
        "Format was: T-YYYYMMDD-HHMMSS-XXXX (seconds precision)",
        "",
        "## Decision",
        "Add microseconds: T-YYYYMMDD-HHMMSSffffff-XXXX",
        "",
        "## Consequences",
        "- Pro: No more collisions in tight loops",
        "- Pro: 1,000,000x more unique IDs per second",
        "- Con: Longer IDs (6 more characters)",
        "```",
        "",
        "---",
        "",
        "## Semantic Search as Memory Recall",
        "",
        "### How Query Expansion Models Association",
        "",
        "When you search for \"neural\", the processor:",
        "",
        "1. **Activates** the \"neural\" minicolumn",
        "2. **Spreads** activation through lateral connections",
        "3. **Includes** associated terms (networks, learning, patterns)",
        "4. **Ranks** documents by cumulative relevance",
        "",
        "```python",
        "# Query: \"neural\"",
        "# Expanded: {neural: 1.0, networks: 0.87, learning: 0.45, artificial: 0.23}",
        "#",
        "# This mimics how thinking of \"neural\" naturally brings",
        "# \"networks\" and \"learning\" to mind",
        "```",
        "",
        "### Improving Recall with Metadata",
        "",
        "Add tags and links to strengthen associations:",
        "",
        "```markdown",
        "---",
        "tags: [machine-learning, neural-networks, debugging]",
        "related: [memories/2025-12-13.md, concepts/pagerank.md]",
        "---",
        "",
        "# Today's Learning",
        "",
        "The PageRank algorithm can be applied to...",
        "```",
        "",
        "---",
        "",
        "## Integration with Development Workflow",
        "",
        "### Memory-Aware Git Hooks",
        "",
        "```bash",
        "#!/bin/bash",
        "# .git/hooks/post-commit",
        "",
        "# Re-index after commits to docs/memories",
        "if git diff --cached --name-only | grep -q \"^docs/\\|^memories/\"; then",
        "    python scripts/index_codebase.py --incremental",
        "fi",
        "```",
        "",
        "### Semantic Commit Messages",
        "",
        "Treat commits as memory encoding:",
        "",
        "```bash",
        "# Bad: \"fix bug\"",
        "# Good: \"memory: discovered that TF-IDF scores need normalization\"",
        "",
        "git commit -m \"memory: fuzzing revealed NaN acceptance in config validation\"",
        "```",
        "",
        "### Branch as Working Memory",
        "",
        "```bash",
        "# Create a working memory space",
        "git checkout -b memory/exploring-pagerank",
        "",
        "# Capture discoveries",
        "echo \"# PageRank Exploration...\" > memories/pagerank-deep-dive.md",
        "git add memories/",
        "git commit -m \"memory: initial pagerank exploration\"",
        "",
        "# When ready, consolidate to main",
        "git checkout main",
        "git merge memory/exploring-pagerank",
        "```",
        "",
        "---",
        "",
        "## Benefits of Text-as-Memories",
        "",
        "1. **Searchable**: Semantic search finds related knowledge",
        "2. **Versioned**: See how understanding evolved",
        "3. **Shareable**: Collaborate on shared knowledge",
        "4. **Persistent**: Never lose insights",
        "5. **Attributable**: Know when and why you learned something",
        "6. **Interconnected**: Cross-references strengthen recall",
        "",
        "---",
        "",
        "## Getting Started",
        "",
        "1. **Create a memories directory** in your repo",
        "2. **Index with the processor**: `python scripts/index_codebase.py`",
        "3. **Search semantically**: `python scripts/search_codebase.py \"your query\"`",
        "4. **Link related concepts** with `[[wiki-style]]` references",
        "5. **Review periodically** to consolidate into concept documents",
        "",
        "---",
        "",
        "## Related Documentation",
        "",
        "- [Architecture Guide](architecture.md) - How layers work",
        "- [Algorithms Guide](algorithms.md) - PageRank, TF-IDF explained",
        "- [Dogfooding Guide](dogfooding.md) - Using the system on itself",
        "- [Query Guide](query-guide.md) - Search techniques",
        "",
        "---",
        "",
        "*\"The palest ink is better than the best memory.\" — Chinese proverb*",
        "",
        "*But versioned, indexed ink is even better.*"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "samples/decisions/adr-microseconds-task-id.md",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "# ADR-001: Add Microseconds to Task ID Generation",
        "",
        "**Status:** Accepted",
        "**Date:** 2025-12-14",
        "**Deciders:** Development team",
        "**Tags:** `task-management`, `uniqueness`, `concurrency`",
        "",
        "---",
        "",
        "## Context and Problem Statement",
        "",
        "Task IDs were generated with second-precision timestamps plus a 4-character hex suffix:",
        "",
        "```",
        "T-YYYYMMDD-HHMMSS-XXXX",
        "Example: T-20251214-163052-a1b2",
        "```",
        "",
        "During CI testing, the `test_unique_task_ids` test was intermittently failing:",
        "",
        "```",
        "AssertionError: 99 != 100",
        "```",
        "",
        "When generating 100 task IDs in a tight loop (same second), collisions occurred because:",
        "- Same timestamp for all IDs in that second",
        "- Only 4 hex chars = 65,536 possible suffixes",
        "- Birthday paradox: P(collision) ≈ 7% for 100 items from 65,536",
        "",
        "## Decision Drivers",
        "",
        "1. **Reliability**: Tests must not be flaky",
        "2. **Uniqueness**: Task IDs must be unique even under concurrent generation",
        "3. **Backwards Compatibility**: Existing task IDs shouldn't break",
        "4. **Readability**: IDs should remain human-readable",
        "",
        "## Considered Options",
        "",
        "### Option 1: Increase Random Suffix Length",
        "",
        "```",
        "T-YYYYMMDD-HHMMSS-XXXXXXXX  (8 hex chars)",
        "```",
        "",
        "**Pros:**",
        "- Simple change",
        "- 4 billion possibilities per second",
        "",
        "**Cons:**",
        "- Longer IDs",
        "- Doesn't leverage timestamp ordering",
        "",
        "### Option 2: Add Microseconds to Timestamp",
        "",
        "```",
        "T-YYYYMMDD-HHMMSSffffff-XXXX",
        "Example: T-20251214-163052123456-a1b2",
        "```",
        "",
        "**Pros:**",
        "- Timestamps remain sortable",
        "- 1 million unique timestamps per second",
        "- Combined with 4 hex suffix = practically unlimited uniqueness",
        "",
        "**Cons:**",
        "- IDs are 6 characters longer",
        "- Existing code parsing IDs needs update",
        "",
        "### Option 3: Use UUID Only",
        "",
        "```",
        "T-a1b2c3d4-e5f6-7890-abcd-ef1234567890",
        "```",
        "",
        "**Pros:**",
        "- Guaranteed uniqueness",
        "- Standard format",
        "",
        "**Cons:**",
        "- Not human-readable",
        "- Loses temporal ordering",
        "- Much longer",
        "",
        "## Decision Outcome",
        "",
        "**Chosen Option:** Option 2 - Add Microseconds to Timestamp",
        "",
        "**Rationale:**",
        "- Preserves temporal ordering (IDs sort chronologically)",
        "- Microseconds provide 1M unique slots per second",
        "- Combined with 4 hex chars: virtually collision-proof",
        "- Minimal change to existing format",
        "",
        "## Implementation",
        "",
        "```python",
        "def generate_task_id(session_id: Optional[str] = None) -> str:",
        "    now = datetime.now()",
        "    date_str = now.strftime(\"%Y%m%d\")",
        "    time_str = now.strftime(\"%H%M%S%f\")  # Added %f for microseconds",
        "    suffix = session_id or generate_session_id()",
        "    return f\"T-{date_str}-{time_str}-{suffix}\"",
        "```",
        "",
        "## Consequences",
        "",
        "### Positive",
        "- Tests no longer flaky",
        "- IDs unique even under heavy concurrent generation",
        "- Temporal ordering preserved",
        "",
        "### Negative",
        "- IDs 6 characters longer",
        "- Tests checking ID format needed update",
        "",
        "### Neutral",
        "- Existing IDs continue to work (no migration needed)",
        "- No performance impact",
        "",
        "## Validation",
        "",
        "```python",
        "# Verify uniqueness with 1000 IDs",
        "ids = {generate_task_id() for _ in range(1000)}",
        "assert len(ids) == 1000  # All unique",
        "```",
        "",
        "## Related Decisions",
        "",
        "- Task management system design (LEGACY-047)",
        "- Merge-friendly task format (docs/merge-friendly-tasks.md)",
        "",
        "---",
        "",
        "*This decision was made after a flaky test exposed the birthday paradox collision probability.*"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "samples/memories/2025-12-14-dogfooding-session.md",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "# Memory Entry: 2025-12-14 Dog-Fooding Session",
        "",
        "**Tags:** `security`, `testing`, `fuzzing`, `dog-fooding`",
        "**Related:** [[../decisions/adr-microseconds-task-id.md]], [[2025-12-13-security-review.md]]",
        "",
        "---",
        "",
        "## Context",
        "",
        "Resumed dog-fooding the Cortical Text Processor. Goal was to use the system to test itself and complete pending security tasks.",
        "",
        "## What I Learned",
        "",
        "### 1. Hypothesis Fuzzing Finds Real Bugs",
        "",
        "Property-based testing with Hypothesis discovered that `CorticalConfig` accepted `NaN` and `infinity` for `louvain_resolution`. The validation check:",
        "",
        "```python",
        "if self.louvain_resolution <= 0:  # BUG: NaN comparisons are always False!",
        "```",
        "",
        "**Fix:** Added explicit checks:",
        "```python",
        "if math.isnan(self.louvain_resolution) or math.isinf(self.louvain_resolution):",
        "    raise ValueError(...)",
        "```",
        "",
        "**Lesson:** Fuzzing with extreme values (NaN, inf, empty strings, unicode) catches edge cases that manual tests miss.",
        "",
        "### 2. Timestamp Precision Matters",
        "",
        "Task ID generation was using seconds precision:",
        "```",
        "T-20251214-163052-a1b2  # Only unique per second",
        "```",
        "",
        "When generating 100 IDs in a tight loop, collisions occurred (~7% probability via birthday paradox with 65,536 possible suffixes).",
        "",
        "**Fix:** Added microseconds:",
        "```",
        "T-20251214-163052123456-a1b2  # Unique per microsecond",
        "```",
        "",
        "### 3. Semantic Search Has Blind Spots",
        "",
        "Searching for \"security test fuzzing\" returned staleness tests instead of actual security code. The search seems to over-weight common terms like \"test\".",
        "",
        "**Created task:** T-20251214-171301-6aa8-001 to investigate.",
        "",
        "## Connections Made",
        "",
        "- **Fuzzing → Validation**: Property-based testing is essential for numeric validation",
        "- **Timestamps → Uniqueness**: Sub-second precision needed for concurrent operations",
        "- **Search → Relevance**: Domain-specific term weighting improves results",
        "",
        "## Emotional State",
        "",
        "Satisfying session. Finding a real bug through fuzzing validated the investment in security testing. The birthday paradox collision was a nice teachable moment about probability.",
        "",
        "## Future Exploration",
        "",
        "- [ ] Apply NaN/inf checks to other float config parameters",
        "- [ ] Investigate TF-IDF weighting for common programming terms",
        "- [ ] Consider security-specific synonym expansion",
        "",
        "## Artifacts Created",
        "",
        "- `tests/security/test_security.py` (22 tests)",
        "- `tests/security/test_fuzzing.py` (17 Hypothesis tests)",
        "- Task: T-20251214-171301-6aa8-001",
        "",
        "---",
        "",
        "*Committed to memory at: 2025-12-14T17:15:00Z*"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "samples/memories/concept-hebbian-text-processing.md",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "# Concept: Hebbian Learning in Text Processing",
        "",
        "**Tags:** `algorithms`, `neuroscience`, `information-retrieval`",
        "**Consolidated From:** Multiple sessions exploring the Cortical Text Processor",
        "",
        "---",
        "",
        "## The Core Principle",
        "",
        "> \"Neurons that fire together, wire together.\" — Donald Hebb, 1949",
        "",
        "In text processing, this translates to:",
        "",
        "> **Words that appear together, link together.**",
        "",
        "## How It Works",
        "",
        "### In Biology",
        "",
        "When two neurons are active simultaneously, their synaptic connection strengthens. Over time, activating one neuron makes the other more likely to fire.",
        "",
        "### In Text Processing",
        "",
        "When two words co-occur within a context window, their connection weight increases:",
        "",
        "```python",
        "# Processing: \"neural networks process data efficiently\"",
        "# Context window: ±3 words",
        "",
        "# Result:",
        "connections[\"neural\"][\"networks\"] += 1.0   # Adjacent",
        "connections[\"neural\"][\"process\"] += 1.0    # Within window",
        "connections[\"neural\"][\"data\"] += 1.0       # Within window",
        "connections[\"networks\"][\"process\"] += 1.0",
        "# etc.",
        "```",
        "",
        "### The Accumulation Effect",
        "",
        "After processing thousands of documents:",
        "",
        "```",
        "\"neural\" connections:",
        "  → \"networks\":    weight 87  (very strong - always together)",
        "  → \"learning\":    weight 45  (often together)",
        "  → \"artificial\":  weight 23  (sometimes together)",
        "  → \"bread\":       weight 0   (never together)",
        "```",
        "",
        "## Why This Matters",
        "",
        "### 1. Query Expansion",
        "",
        "When a user searches for \"neural\", the system can suggest:",
        "- \"networks\" (strong association)",
        "- \"learning\" (moderate association)",
        "",
        "This improves recall without requiring exact matches.",
        "",
        "### 2. Semantic Similarity",
        "",
        "Documents with similar Hebbian connection patterns are semantically related, even without shared vocabulary.",
        "",
        "### 3. Knowledge Discovery",
        "",
        "Strong unexpected connections reveal insights:",
        "- \"yeast\" → \"bread\" makes sense",
        "- \"yeast\" → \"genetic research\" reveals scientific usage",
        "",
        "## Implementation in the Cortical Processor",
        "",
        "From `cortical/processor/documents.py`:",
        "",
        "```python",
        "def _build_lateral_connections(self, tokens, layer0):",
        "    \"\"\"Build Hebbian-style connections from co-occurrence.\"\"\"",
        "    window_size = 3",
        "",
        "    for i, token in enumerate(tokens):",
        "        col = layer0.get_minicolumn(token)",
        "        if not col:",
        "            continue",
        "",
        "        # Connect to tokens within window",
        "        for j in range(max(0, i - window_size),",
        "                       min(len(tokens), i + window_size + 1)):",
        "            if i != j:",
        "                other = layer0.get_minicolumn(tokens[j])",
        "                if other:",
        "                    # Strengthen the connection",
        "                    col.add_lateral_connection(other.id, weight=1.0)",
        "```",
        "",
        "## Limitations",
        "",
        "### 1. Common Word Pollution",
        "",
        "Frequent words (\"the\", \"is\", \"self\") connect to everything, creating noise.",
        "",
        "**Solution:** Stop word filtering, TF-IDF weighting",
        "",
        "### 2. O(n²) Scaling",
        "",
        "Every pair within a window creates a connection.",
        "",
        "**Solution:** Limits on connections per term (`max_bigrams_per_term`)",
        "",
        "### 3. Context Blindness",
        "",
        "\"bank\" (financial) and \"bank\" (river) create the same connections.",
        "",
        "**Solution:** Bigram layer provides some disambiguation",
        "",
        "## Related Concepts",
        "",
        "- [[pagerank.md]] - Uses connection structure to rank importance",
        "- [[tfidf.md]] - Weights terms by distinctiveness",
        "- [[louvain-clustering.md]] - Groups connected terms into concepts",
        "",
        "## Sources",
        "",
        "- Hebb, D.O. (1949). *The Organization of Behavior*",
        "- Cortical Text Processor source: `cortical/processor/documents.py:88-106`",
        "- README.md visualization of connection weights",
        "",
        "---",
        "",
        "*Consolidated: 2025-12-14*",
        "*Last updated: 2025-12-14*"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "tasks/2025-12-14_17-13-01_6aa8.json",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "{",
        "  \"version\": 1,",
        "  \"session_id\": \"6aa8\",",
        "  \"started_at\": \"2025-12-14T17:13:01.505357\",",
        "  \"saved_at\": \"2025-12-14T17:13:01.505814\",",
        "  \"tasks\": [",
        "    {",
        "      \"id\": \"T-20251214-171301-6aa8-001\",",
        "      \"title\": \"Investigate semantic search relevance for domain-specific queries\",",
        "      \"status\": \"pending\",",
        "      \"priority\": \"medium\",",
        "      \"category\": \"arch\",",
        "      \"description\": \"During dog-fooding, searching for 'security test fuzzing' returned staleness tests instead of actual security-related code. The search seems to weight common terms like 'test' too heavily.\\n\\nInvestigation areas:\\n- Query expansion may be pulling in too many generic terms\\n- TF-IDF weighting may not properly discount common programming terms\\n- Domain-specific boosting could improve relevance for security/testing queries\\n\\nRelated: The code_concepts.py has programming synonyms but may need security-specific terms.\\n\\nDiscovered during: Dog-fooding session 2025-12-14\",",
        "      \"depends_on\": [],",
        "      \"effort\": \"medium\",",
        "      \"created_at\": \"2025-12-14T17:13:01.505725\",",
        "      \"updated_at\": null,",
        "      \"completed_at\": null,",
        "      \"context\": {},",
        "      \"retrospective\": null",
        "    }",
        "  ]",
        "}"
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    }
  ],
  "hour_of_day": 17,
  "day_of_week": "Sunday",
  "seconds_since_last_commit": -73666,
  "is_merge": false,
  "is_initial": false,
  "parent_count": 1,
  "session_id": null,
  "related_chats": [],
  "ci_result": null,
  "reverted": false,
  "amended": false
}