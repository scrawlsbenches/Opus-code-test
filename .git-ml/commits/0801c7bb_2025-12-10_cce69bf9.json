{
  "hash": "0801c7bb0dbdfd85a5bba7225505a2d931554e2c",
  "message": "Merge pull request #17 from scrawlsbenches/claude/setup-cortical-processor-01Nv69Xdoz9rNb12pGTb2WHN",
  "author": "scrawlsbenches",
  "timestamp": "2025-12-10 07:13:38 -0500",
  "branch": "claude/multi-index-design-DvifZ",
  "files_changed": [
    "README.md"
  ],
  "insertions": 57,
  "deletions": 25,
  "hunks": [
    {
      "file": "README.md",
      "function": "Install from source:",
      "start_line": 47,
      "lines_added": [
        "Run the showcase to see the processor analyze 92 documents covering everything from neural networks to medieval falconry:",
        "",
        "```bash",
        "python showcase.py",
        "```",
        "",
        "**Output:**",
        "```",
        "    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—",
        "    â•‘            ğŸ§   CORTICAL TEXT PROCESSOR SHOWCASE  ğŸ§                   â•‘",
        "    â•‘     Mimicking how the neocortex processes and understands text       â•‘",
        "    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•",
        "",
        "Loading documents from: samples",
        "Processing through cortical hierarchy...",
        "(Like visual information flowing V1 â†’ V2 â†’ V4 â†’ IT)",
        "",
        "  ğŸ“„ comprehensive_machine_learning (2445 words)",
        "  ğŸ“„ attention_mechanism_research   (644 words)",
        "  ğŸ“„ neural_network_optimization    (648 words)",
        "  ... 89 more documents ...",
        "",
        "âœ“ Processed 92 documents",
        "âœ“ Created 6,506 token minicolumns",
        "âœ“ Created 20,114 bigram minicolumns",
        "âœ“ Formed 116,332 lateral connections",
        "",
        "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•",
        "                       KEY CONCEPTS (PageRank)",
        "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•",
        "",
        "PageRank identifies central concepts - highly connected 'hub' words:",
        "",
        "  Rank  Concept            PageRank",
        "  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€",
        "    1.  data               â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 0.0046",
        "    2.  model              â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘ 0.0044",
        "    3.  learning           â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘ 0.0041",
        "    ...",
        "",
        "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•",
        "                        QUERY DEMONSTRATION",
        "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•",
        "",
        "ğŸ” Query: 'neural networks'",
        "   Expanded with: knowledge, data, graph, network, deep, artificial",
        "",
        "   Top documents:",
        "     â€¢ comprehensive_machine_learning (score: 26.384)",
        "     â€¢ attention_mechanism_research (score: 19.178)",
        "     â€¢ cortical_semantic_networks (score: 18.470)",
        "```",
        "",
        "### Programmatic Usage",
        "",
        "# Build the network",
        "print(results)  # [('doc1', 0.877), ('doc2', 0.832)]"
      ],
      "lines_removed": [
        "# Create processor",
        "# Build the network (runs all computations)",
        "print(results)  # [('doc1', 0.85), ('doc2', 0.72), ...]",
        "",
        "# Get corpus summary",
        "summary = processor.get_corpus_summary()",
        "print(f\"Documents: {summary['documents']}, Connections: {summary['total_connections']}\")",
        "**Output:**",
        "```",
        "Computing activation propagation...",
        "Computing importance (PageRank)...",
        "Computing TF-IDF...",
        "Computing document connections...",
        "Computing bigram connections...",
        "Building concept clusters...",
        "Computing concept connections (document_overlap)...",
        "Done.",
        "[('doc1', 0.8774208144843981), ('doc2', 0.8317923190728529)]",
        "Documents: 3, Connections: 66",
        "âœ“ Saved processor to my_corpus.pkl",
        "  - 3 documents",
        "  - 29 minicolumns",
        "  - 66 connections",
        "```",
        ""
      ],
      "context_before": [
        "```bash",
        "git clone <repository-url>",
        "cd cortical-text-processor",
        "pip install -e .",
        "```",
        "",
        "Or simply copy the `cortical/` directory into your projectâ€”zero dependencies means no pip required.",
        "",
        "## Quick Start",
        ""
      ],
      "context_after": [
        "```python",
        "from cortical import CorticalTextProcessor",
        "",
        "processor = CorticalTextProcessor()",
        "",
        "# Add documents",
        "processor.process_document(\"doc1\", \"Neural networks process information hierarchically.\")",
        "processor.process_document(\"doc2\", \"The brain uses layers of neurons for processing.\")",
        "processor.process_document(\"doc3\", \"Machine learning enables pattern recognition.\")",
        "",
        "processor.compute_all()",
        "",
        "# Query",
        "results = processor.find_documents_for_query(\"neural processing\")",
        "",
        "# Save for later",
        "processor.save(\"my_corpus.pkl\")",
        "```",
        "",
        "## Core API",
        "",
        "### Document Processing",
        "",
        "```python",
        "processor.process_document(doc_id, content, metadata=None)",
        "processor.add_document_incremental(doc_id, content)  # Incremental indexing",
        "processor.add_documents_batch([(doc_id, content, metadata), ...])  # Batch processing",
        "```"
      ],
      "change_type": "modify"
    }
  ],
  "hour_of_day": 12,
  "day_of_week": "Wednesday",
  "seconds_since_last_commit": -437470,
  "is_merge": true,
  "is_initial": false,
  "parent_count": 2,
  "session_id": null,
  "related_chats": [],
  "ci_result": null,
  "reverted": false,
  "amended": false
}