{
  "hash": "e57cd8361a0b2135a2035d621c506fafe4f5c592",
  "message": "Add parameter range validation and move inline imports (Tasks #39, #40)",
  "author": "Claude",
  "timestamp": "2025-12-10 14:54:53 +0000",
  "branch": "claude/multi-index-design-DvifZ",
  "files_changed": [
    "cortical/analysis.py",
    "cortical/processor.py",
    "cortical/query.py",
    "cortical/semantics.py",
    "tests/test_analysis.py",
    "tests/test_query.py"
  ],
  "insertions": 119,
  "deletions": 7,
  "hunks": [
    {
      "file": "cortical/analysis.py",
      "function": "def compute_pagerank(",
      "start_line": 33,
      "lines_added": [
        "",
        "    Raises:",
        "        ValueError: If damping is not in range (0, 1)",
        "    if not (0 < damping < 1):",
        "        raise ValueError(f\"damping must be between 0 and 1, got {damping}\")",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "    columns receive higher scores.",
        "",
        "    Args:",
        "        layer: The layer to compute PageRank for",
        "        damping: Damping factor (probability of following links)",
        "        iterations: Maximum number of iterations",
        "        tolerance: Convergence threshold",
        "",
        "    Returns:",
        "        Dictionary mapping column IDs to PageRank scores"
      ],
      "context_after": [
        "    \"\"\"",
        "    n = len(layer.minicolumns)",
        "    if n == 0:",
        "        return {}",
        "",
        "    # Initialize PageRank uniformly",
        "    pagerank = {col.id: 1.0 / n for col in layer.minicolumns.values()}",
        "",
        "    # Build incoming links map",
        "    incoming: Dict[str, List[Tuple[str, float]]] = defaultdict(list)",
        "    outgoing_sum: Dict[str, float] = defaultdict(float)"
      ],
      "change_type": "add"
    },
    {
      "file": "cortical/analysis.py",
      "function": "def compute_semantic_pagerank(",
      "start_line": 130,
      "lines_added": [
        "",
        "    Raises:",
        "        ValueError: If damping is not in range (0, 1)",
        "    if not (0 < damping < 1):",
        "        raise ValueError(f\"damping must be between 0 and 1, got {damping}\")",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "    Returns:",
        "        Dict containing:",
        "        - pagerank: Dict mapping column IDs to PageRank scores",
        "        - iterations_run: Number of iterations until convergence",
        "        - edges_with_relations: Number of edges that had semantic relation info",
        "",
        "    Example:",
        "        >>> relations = [(\"neural\", \"RelatedTo\", \"networks\", 0.8)]",
        "        >>> result = compute_semantic_pagerank(layer, relations)",
        "        >>> print(f\"PageRank converged in {result['iterations_run']} iterations\")"
      ],
      "context_after": [
        "    \"\"\"",
        "    n = len(layer.minicolumns)",
        "    if n == 0:",
        "        return {'pagerank': {}, 'iterations_run': 0, 'edges_with_relations': 0}",
        "",
        "    # Use default weights if not provided",
        "    weights = relation_weights or RELATION_WEIGHTS",
        "",
        "    # Build semantic relation lookup: (term1, term2) -> (relation_type, weight)",
        "    semantic_lookup: Dict[Tuple[str, str], Tuple[str, float]] = {}",
        "    for t1, relation, t2, rel_weight in semantic_relations:"
      ],
      "change_type": "add"
    },
    {
      "file": "cortical/analysis.py",
      "function": "def compute_hierarchical_pagerank(",
      "start_line": 253,
      "lines_added": [
        "",
        "    Raises:",
        "        ValueError: If damping or cross_layer_damping is not in range (0, 1)",
        "    if not (0 < damping < 1):",
        "        raise ValueError(f\"damping must be between 0 and 1, got {damping}\")",
        "    if not (0 < cross_layer_damping < 1):",
        "        raise ValueError(f\"cross_layer_damping must be between 0 and 1, got {cross_layer_damping}\")",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "",
        "    Returns:",
        "        Dict containing:",
        "        - iterations_run: Number of global iterations",
        "        - converged: Whether the algorithm converged",
        "        - layer_stats: Per-layer statistics",
        "",
        "    Example:",
        "        >>> result = compute_hierarchical_pagerank(layers)",
        "        >>> print(f\"Converged in {result['iterations_run']} iterations\")"
      ],
      "context_after": [
        "    \"\"\"",
        "    # Define layer order for propagation",
        "    layer_order = [",
        "        CorticalLayer.TOKENS,",
        "        CorticalLayer.BIGRAMS,",
        "        CorticalLayer.CONCEPTS,",
        "        CorticalLayer.DOCUMENTS",
        "    ]",
        "",
        "    # Filter to only existing layers with minicolumns",
        "    active_layers = [l for l in layer_order if l in layers and layers[l].column_count() > 0]"
      ],
      "change_type": "add"
    },
    {
      "file": "cortical/processor.py",
      "function": null,
      "start_line": 1,
      "lines_added": [
        "import copy"
      ],
      "lines_removed": [],
      "context_before": [
        "\"\"\"",
        "Cortical Text Processor - Main processor class that orchestrates all components.",
        "\"\"\"",
        "",
        "import os",
        "import re",
        "from typing import Dict, List, Tuple, Optional, Any"
      ],
      "context_after": [
        "from collections import defaultdict",
        "",
        "from .tokenizer import Tokenizer",
        "from .minicolumn import Minicolumn",
        "from .layers import CorticalLayer, HierarchicalLayer",
        "from . import analysis",
        "from . import semantics",
        "from . import embeddings as emb_module",
        "from . import query as query_module",
        "from . import gaps as gaps_module"
      ],
      "change_type": "add"
    },
    {
      "file": "cortical/processor.py",
      "function": "class CorticalTextProcessor:",
      "start_line": 163,
      "lines_added": [],
      "lines_removed": [
        "        import copy"
      ],
      "context_before": [
        "        \"\"\"",
        "        return self.document_metadata.get(doc_id, {})",
        "",
        "    def get_all_document_metadata(self) -> Dict[str, Dict[str, Any]]:",
        "        \"\"\"",
        "        Get metadata for all documents.",
        "",
        "        Returns:",
        "            Dict mapping doc_id to metadata dict (deep copy)",
        "        \"\"\""
      ],
      "context_after": [
        "        return copy.deepcopy(self.document_metadata)",
        "",
        "    def _mark_all_stale(self) -> None:",
        "        \"\"\"Mark all computations as stale (needing recomputation).\"\"\"",
        "        self._stale_computations = {",
        "            self.COMP_TFIDF,",
        "            self.COMP_PAGERANK,",
        "            self.COMP_ACTIVATION,",
        "            self.COMP_DOC_CONNECTIONS,",
        "            self.COMP_BIGRAM_CONNECTIONS,"
      ],
      "change_type": "delete"
    },
    {
      "file": "cortical/query.py",
      "function": "def create_chunks(",
      "start_line": 942,
      "lines_added": [
        "",
        "    Raises:",
        "        ValueError: If chunk_size <= 0 or overlap < 0 or overlap >= chunk_size",
        "    if chunk_size <= 0:",
        "        raise ValueError(f\"chunk_size must be positive, got {chunk_size}\")",
        "    if overlap < 0:",
        "        raise ValueError(f\"overlap must be non-negative, got {overlap}\")",
        "    if overlap >= chunk_size:",
        "        raise ValueError(f\"overlap must be less than chunk_size, got overlap={overlap}, chunk_size={chunk_size}\")",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "    \"\"\"",
        "    Split text into overlapping chunks.",
        "",
        "    Args:",
        "        text: Document text to chunk",
        "        chunk_size: Target size of each chunk in characters",
        "        overlap: Number of overlapping characters between chunks",
        "",
        "    Returns:",
        "        List of (chunk_text, start_char, end_char) tuples"
      ],
      "context_after": [
        "    \"\"\"",
        "    if not text:",
        "        return []",
        "",
        "    chunks = []",
        "    stride = max(1, chunk_size - overlap)",
        "    text_len = len(text)",
        "",
        "    for start in range(0, text_len, stride):",
        "        end = min(start + chunk_size, text_len)",
        "        chunk = text[start:end]"
      ],
      "change_type": "add"
    },
    {
      "file": "cortical/semantics.py",
      "function": "Semantics Module",
      "start_line": 5,
      "lines_added": [
        "import copy"
      ],
      "lines_removed": [],
      "context_before": [
        "Corpus-derived semantic relations and retrofitting.",
        "",
        "Extracts semantic relationships from co-occurrence patterns,",
        "then uses them to adjust connection weights (retrofitting).",
        "This is like building a \"poor man's ConceptNet\" from the corpus itself.",
        "\"\"\"",
        "",
        "import math",
        "import re",
        "from typing import Any, Dict, List, Tuple, Set, Optional"
      ],
      "context_after": [
        "from collections import defaultdict",
        "",
        "try:",
        "    import numpy as np",
        "    HAS_NUMPY = True",
        "except ImportError:",
        "    HAS_NUMPY = False",
        "",
        "from .layers import CorticalLayer, HierarchicalLayer",
        "from .minicolumn import Minicolumn"
      ],
      "change_type": "add"
    },
    {
      "file": "cortical/semantics.py",
      "function": "def retrofit_connections(",
      "start_line": 385,
      "lines_added": [
        "",
        "",
        "    Raises:",
        "        ValueError: If alpha is not in range [0, 1]",
        "    if not (0 <= alpha <= 1):",
        "        raise ValueError(f\"alpha must be between 0 and 1, got {alpha}\")",
        "",
        ""
      ],
      "lines_removed": [
        "        ",
        "    "
      ],
      "context_before": [
        "    ",
        "    Adjusts connection weights by blending co-occurrence patterns",
        "    with semantic relations. This is inspired by Faruqui et al.'s",
        "    retrofitting algorithm for word vectors.",
        "    ",
        "    Args:",
        "        layers: Dictionary of layers",
        "        semantic_relations: List of (term1, relation, term2, weight) tuples",
        "        iterations: Number of retrofitting iterations",
        "        alpha: Blend factor (0=all semantic, 1=all original)"
      ],
      "context_after": [
        "    Returns:",
        "        Dictionary with retrofitting statistics",
        "    \"\"\"",
        "    layer0 = layers[CorticalLayer.TOKENS]",
        "    # Store original weights",
        "    original_weights: Dict[str, Dict[str, float]] = {}",
        "    for col in layer0.minicolumns.values():",
        "        original_weights[col.content] = dict(col.lateral_connections)",
        "    ",
        "    # Build semantic neighbor lookup",
        "    semantic_neighbors: Dict[str, List[Tuple[str, float]]] = defaultdict(list)",
        "    ",
        "    for t1, relation, t2, weight in semantic_relations:",
        "        relation_weight = RELATION_WEIGHTS.get(relation, 0.5)"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/semantics.py",
      "function": "def retrofit_embeddings(",
      "start_line": 479,
      "lines_added": [
        "",
        "",
        "    Raises:",
        "        ValueError: If alpha is not in range (0, 1]",
        "    if not (0 < alpha <= 1):",
        "        raise ValueError(f\"alpha must be between 0 and 1 (exclusive of 0), got {alpha}\")",
        ""
      ],
      "lines_removed": [
        "        ",
        "    import copy",
        "    "
      ],
      "context_before": [
        "    Retrofit embeddings using semantic relations.",
        "    ",
        "    Like Faruqui et al.'s retrofitting, but for graph embeddings.",
        "    Pulls semantically related terms closer in embedding space.",
        "    ",
        "    Args:",
        "        embeddings: Dictionary mapping terms to embedding vectors",
        "        semantic_relations: List of (term1, relation, term2, weight) tuples",
        "        iterations: Number of iterations",
        "        alpha: Blend factor (higher = more original embedding)"
      ],
      "context_after": [
        "    Returns:",
        "        Dictionary with retrofitting statistics",
        "    \"\"\"",
        "    # Store original embeddings",
        "    original = copy.deepcopy(embeddings)",
        "    ",
        "    # Build neighbor lookup",
        "    neighbors: Dict[str, List[Tuple[str, float]]] = defaultdict(list)",
        "    ",
        "    for t1, relation, t2, weight in semantic_relations:",
        "        if t1 in embeddings and t2 in embeddings:",
        "            relation_weight = RELATION_WEIGHTS.get(relation, 0.5)",
        "            combined = weight * relation_weight"
      ],
      "change_type": "modify"
    },
    {
      "file": "tests/test_analysis.py",
      "function": "class TestGetByIdOptimization(unittest.TestCase):",
      "start_line": 274,
      "lines_added": [
        "class TestParameterValidation(unittest.TestCase):",
        "    \"\"\"Test parameter validation in analysis functions.\"\"\"",
        "",
        "    def test_pagerank_invalid_damping_zero(self):",
        "        \"\"\"Test PageRank rejects damping=0.\"\"\"",
        "        layer = HierarchicalLayer(CorticalLayer.TOKENS)",
        "        layer.get_or_create_minicolumn(\"test\")",
        "        with self.assertRaises(ValueError) as ctx:",
        "            compute_pagerank(layer, damping=0)",
        "        self.assertIn(\"damping\", str(ctx.exception))",
        "",
        "    def test_pagerank_invalid_damping_one(self):",
        "        \"\"\"Test PageRank rejects damping=1.\"\"\"",
        "        layer = HierarchicalLayer(CorticalLayer.TOKENS)",
        "        layer.get_or_create_minicolumn(\"test\")",
        "        with self.assertRaises(ValueError) as ctx:",
        "            compute_pagerank(layer, damping=1.0)",
        "        self.assertIn(\"damping\", str(ctx.exception))",
        "",
        "    def test_pagerank_invalid_damping_negative(self):",
        "        \"\"\"Test PageRank rejects negative damping.\"\"\"",
        "        layer = HierarchicalLayer(CorticalLayer.TOKENS)",
        "        layer.get_or_create_minicolumn(\"test\")",
        "        with self.assertRaises(ValueError) as ctx:",
        "            compute_pagerank(layer, damping=-0.5)",
        "        self.assertIn(\"damping\", str(ctx.exception))",
        "",
        "    def test_pagerank_invalid_damping_greater_than_one(self):",
        "        \"\"\"Test PageRank rejects damping > 1.\"\"\"",
        "        layer = HierarchicalLayer(CorticalLayer.TOKENS)",
        "        layer.get_or_create_minicolumn(\"test\")",
        "        with self.assertRaises(ValueError) as ctx:",
        "            compute_pagerank(layer, damping=1.5)",
        "        self.assertIn(\"damping\", str(ctx.exception))",
        "",
        "    def test_pagerank_valid_damping(self):",
        "        \"\"\"Test PageRank accepts valid damping values.\"\"\"",
        "        layer = HierarchicalLayer(CorticalLayer.TOKENS)",
        "        layer.get_or_create_minicolumn(\"test\")",
        "        # Should not raise",
        "        result = compute_pagerank(layer, damping=0.85)",
        "        self.assertIsInstance(result, dict)",
        "",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "",
        "    def test_get_by_id_returns_none_for_missing(self):",
        "        \"\"\"Test that get_by_id returns None for missing ID.\"\"\"",
        "        layer = HierarchicalLayer(CorticalLayer.TOKENS)",
        "        layer.get_or_create_minicolumn(\"test\")",
        "",
        "        result = layer.get_by_id(\"nonexistent_id\")",
        "        self.assertIsNone(result)",
        "",
        ""
      ],
      "context_after": [
        "if __name__ == \"__main__\":",
        "    unittest.main(verbosity=2)"
      ],
      "change_type": "add"
    },
    {
      "file": "tests/test_query.py",
      "function": "class TestScoreRelationPath(unittest.TestCase):",
      "start_line": 74,
      "lines_added": [
        "        chunks = create_chunks(text, chunk_size=100, overlap=20)"
      ],
      "lines_removed": [
        "        chunks = create_chunks(text, chunk_size=100)"
      ],
      "context_before": [
        "class TestCreateChunks(unittest.TestCase):",
        "    \"\"\"Test text chunking.\"\"\"",
        "",
        "    def test_empty_text(self):",
        "        \"\"\"Empty text should return empty list.\"\"\"",
        "        self.assertEqual(create_chunks(\"\"), [])",
        "",
        "    def test_short_text(self):",
        "        \"\"\"Text shorter than chunk_size should return single chunk.\"\"\"",
        "        text = \"Short text.\""
      ],
      "context_after": [
        "        self.assertEqual(len(chunks), 1)",
        "        self.assertEqual(chunks[0][0], text)",
        "        self.assertEqual(chunks[0][1], 0)  # start",
        "        self.assertEqual(chunks[0][2], len(text))  # end",
        "",
        "    def test_chunk_overlap(self):",
        "        \"\"\"Chunks should overlap by specified amount.\"\"\"",
        "        text = \"A\" * 100",
        "        chunks = create_chunks(text, chunk_size=50, overlap=10)",
        "        # With chunk_size=50 and overlap=10, stride=40"
      ],
      "change_type": "modify"
    },
    {
      "file": "tests/test_query.py",
      "function": "class TestCreateChunks(unittest.TestCase):",
      "start_line": 112,
      "lines_added": [
        "    def test_invalid_chunk_size_zero(self):",
        "        \"\"\"Chunk size of zero should raise ValueError.\"\"\"",
        "        with self.assertRaises(ValueError) as ctx:",
        "            create_chunks(\"test\", chunk_size=0)",
        "        self.assertIn(\"chunk_size\", str(ctx.exception))",
        "",
        "    def test_invalid_chunk_size_negative(self):",
        "        \"\"\"Negative chunk size should raise ValueError.\"\"\"",
        "        with self.assertRaises(ValueError) as ctx:",
        "            create_chunks(\"test\", chunk_size=-10)",
        "        self.assertIn(\"chunk_size\", str(ctx.exception))",
        "",
        "    def test_invalid_overlap_negative(self):",
        "        \"\"\"Negative overlap should raise ValueError.\"\"\"",
        "        with self.assertRaises(ValueError) as ctx:",
        "            create_chunks(\"test\", chunk_size=50, overlap=-5)",
        "        self.assertIn(\"overlap\", str(ctx.exception))",
        "",
        "    def test_invalid_overlap_greater_than_chunk_size(self):",
        "        \"\"\"Overlap >= chunk_size should raise ValueError.\"\"\"",
        "        with self.assertRaises(ValueError) as ctx:",
        "            create_chunks(\"test\", chunk_size=50, overlap=50)",
        "        self.assertIn(\"overlap\", str(ctx.exception))",
        "",
        "        with self.assertRaises(ValueError):",
        "            create_chunks(\"test\", chunk_size=50, overlap=100)",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "    def test_no_overlap(self):",
        "        \"\"\"Chunks with zero overlap should not overlap.\"\"\"",
        "        text = \"A\" * 100",
        "        chunks = create_chunks(text, chunk_size=25, overlap=0)",
        "        # Should have exactly 4 chunks",
        "        self.assertEqual(len(chunks), 4)",
        "        # Check no overlap",
        "        for i in range(len(chunks) - 1):",
        "            self.assertEqual(chunks[i][2], chunks[i + 1][1])",
        ""
      ],
      "context_after": [
        "",
        "class TestFindRelationBetween(unittest.TestCase):",
        "    \"\"\"Test finding relations between terms.\"\"\"",
        "",
        "    def setUp(self):",
        "        \"\"\"Set up sample relations.\"\"\"",
        "        self.relations = [",
        "            (\"dog\", \"IsA\", \"animal\", 1.0),",
        "            (\"cat\", \"IsA\", \"animal\", 1.0),",
        "            (\"dog\", \"HasProperty\", \"loyal\", 0.8),"
      ],
      "change_type": "add"
    }
  ],
  "hour_of_day": 14,
  "day_of_week": "Wednesday",
  "seconds_since_last_commit": -427795,
  "is_merge": false,
  "is_initial": false,
  "parent_count": 1,
  "session_id": null,
  "related_chats": [],
  "ci_result": null,
  "reverted": false,
  "amended": false
}