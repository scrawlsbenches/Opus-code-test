{
  "hash": "ef9fe796c5ab540448abefdeab98bc9667acf0b6",
  "message": "Fix critical search bugs #179, #180, #181",
  "author": "Claude",
  "timestamp": "2025-12-13 03:00:42 +0000",
  "branch": "claude/multi-index-design-DvifZ",
  "files_changed": [
    "cortical/query/definitions.py",
    "cortical/query/ranking.py",
    "cortical/query/search.py",
    "tests/unit/test_query_definitions.py",
    "tests/unit/test_query_passages.py",
    "tests/unit/test_query_search.py"
  ],
  "insertions": 309,
  "deletions": 15,
  "hunks": [
    {
      "file": "cortical/query/definitions.py",
      "function": "def find_definition_in_text(",
      "start_line": 110,
      "lines_added": [
        "            # Find the start of the line containing the definition",
        "            # This ensures the passage starts with the actual definition line",
        "            line_start = text.rfind('\\n', 0, match.start())",
        "            if line_start == -1:",
        "                # Match is on the first line of the text",
        "                start = 0",
        "            else:",
        "                # Start from the character after the newline",
        "                start = line_start + 1",
        "",
        "            # Extract context after the definition"
      ],
      "lines_removed": [
        "            # Extract context around the definition",
        "            start = max(0, match.start() - 50)  # Small lead-in for context"
      ],
      "context_before": [
        "            DEFINITION_SOURCE_PATTERNS['python_method'],",
        "            DEFINITION_SOURCE_PATTERNS['javascript_function'],",
        "            DEFINITION_SOURCE_PATTERNS['javascript_const_fn'],",
        "        ]",
        "",
        "    # Try each pattern",
        "    for pattern_template in patterns_to_try:",
        "        pattern = pattern_template.format(name=re.escape(identifier))",
        "        match = re.search(pattern, text, re.MULTILINE | re.IGNORECASE)",
        "        if match:"
      ],
      "context_after": [
        "            end = min(len(text), match.end() + context_chars)",
        "",
        "            # Try to extend to next blank line or class/function boundary",
        "            remaining = text[match.end():end]",
        "            # Look for a good boundary (blank line followed by non-indented text)",
        "            boundary_match = re.search(r'\\n\\n(?=[^\\s])', remaining)",
        "            if boundary_match:",
        "                end = match.end() + boundary_match.end()",
        "",
        "            passage = text[start:end]"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/query/ranking.py",
      "function": "def get_doc_type_boost(",
      "start_line": 67,
      "lines_added": [
        "    # If we have metadata with doc_type, use it",
        "    if doc_metadata and doc_id in doc_metadata and 'doc_type' in doc_metadata[doc_id]:",
        "        doc_type = doc_metadata[doc_id]['doc_type']",
        "    elif doc_id.startswith('tests/') or 'test' in doc_id.lower():",
        "        # Check both tests/ directory and files with 'test' in name"
      ],
      "lines_removed": [
        "    # If we have metadata, use doc_type",
        "    if doc_metadata and doc_id in doc_metadata:",
        "        doc_type = doc_metadata[doc_id].get('doc_type', 'code')",
        "    elif doc_id.startswith('tests/'):"
      ],
      "context_before": [
        "    Args:",
        "        doc_id: Document ID",
        "        doc_metadata: Optional metadata dict {doc_id: {doc_type: ..., ...}}",
        "        custom_boosts: Optional custom boost factors",
        "",
        "    Returns:",
        "        Boost factor (1.0 = no boost)",
        "    \"\"\"",
        "    boosts = custom_boosts or DOC_TYPE_BOOSTS",
        ""
      ],
      "context_after": [
        "        return boosts.get(doc_type, 1.0)",
        "",
        "    # Fallback: infer from doc_id path",
        "    if doc_id.endswith('.md'):",
        "        if doc_id.startswith('docs/'):",
        "            return boosts.get('docs', 1.5)",
        "        return boosts.get('root_docs', 1.3)",
        "        return boosts.get('test', 0.8)",
        "    return boosts.get('code', 1.0)",
        "",
        "",
        "def apply_doc_type_boost(",
        "    results: List[Tuple[str, float]],",
        "    doc_metadata: Optional[Dict[str, Dict[str, Any]]] = None,",
        "    boost_docs: bool = True,",
        "    custom_boosts: Optional[Dict[str, float]] = None",
        ") -> List[Tuple[str, float]]:"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/query/search.py",
      "function": "def find_documents_for_query(",
      "start_line": 61,
      "lines_added": [
        "    if doc_name_boost > 1.0 and doc_scores:",
        "        max_score = max(doc_scores.values()) if doc_scores else 0.0",
        "",
        "        # First pass: identify exact and partial matches",
        "        exact_matches = []",
        "        partial_matches = []",
        "",
        "",
        "                if match_ratio == 1.0:",
        "                    exact_matches.append(doc_id)",
        "                else:",
        "                    partial_matches.append((doc_id, match_ratio))",
        "",
        "        # Apply boosts:",
        "        # - Exact matches: ensure they rank above all non-exact matches",
        "        # - Partial matches: proportional boost",
        "        for doc_id in exact_matches:",
        "            # For exact matches, add max_score to ensure they rank first",
        "            # This guarantees exact match beats all other documents",
        "            doc_scores[doc_id] += max_score * doc_name_boost",
        "",
        "        for doc_id, match_ratio in partial_matches:",
        "            # Partial matches use proportional boost",
        "            boost = 1 + (doc_name_boost - 1) * match_ratio",
        "            doc_scores[doc_id] *= boost"
      ],
      "lines_removed": [
        "    if doc_name_boost > 1.0:",
        "                # Boost proportional to match ratio",
        "                boost = 1 + (doc_name_boost - 1) * match_ratio",
        "                doc_scores[doc_id] *= boost"
      ],
      "context_before": [
        "    doc_scores: Dict[str, float] = defaultdict(float)",
        "",
        "    for term, term_weight in query_terms.items():",
        "        col = layer0.get_minicolumn(term)",
        "        if col:",
        "            for doc_id in col.document_ids:",
        "                tfidf = col.tfidf_per_doc.get(doc_id, col.tfidf)",
        "                doc_scores[doc_id] += tfidf * term_weight",
        "",
        "    # Boost documents whose name matches query terms"
      ],
      "context_after": [
        "        query_tokens = set(tokenizer.tokenize(query_text))",
        "        for doc_id in doc_scores:",
        "            # Tokenize document ID (handle underscores as separators)",
        "            doc_name_tokens = set(tokenizer.tokenize(doc_id.replace('_', ' ')))",
        "            # Count how many query tokens appear in doc name",
        "            matches = len(query_tokens & doc_name_tokens)",
        "            if matches > 0:",
        "                match_ratio = matches / len(query_tokens) if query_tokens else 0",
        "",
        "    sorted_docs = sorted(doc_scores.items(), key=lambda x: -x[1])",
        "    return sorted_docs[:top_n]",
        "",
        "",
        "def fast_find_documents(",
        "    query_text: str,",
        "    layers: Dict[CorticalLayer, HierarchicalLayer],",
        "    tokenizer: Tokenizer,",
        "    top_n: int = 5,"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/query/search.py",
      "function": "def fast_find_documents(",
      "start_line": 139,
      "lines_added": [
        "    # Add documents whose names match query terms to candidates",
        "    # This ensures exact name matches are considered even if content doesn't match",
        "    if doc_name_boost > 1.0:",
        "        layer3 = layers.get(CorticalLayer.DOCUMENTS)",
        "        if layer3:",
        "            for doc_col in layer3.minicolumns.values():",
        "                doc_id = doc_col.content",
        "                doc_name_tokens = set(tokenizer.tokenize(doc_id.replace('_', ' ')))",
        "                matches = len(query_tokens & doc_name_tokens)",
        "                if matches > 0:",
        "                    # Ensure name-matching docs are in candidates",
        "                    # High initial score to prioritize them",
        "                    if doc_id not in candidate_docs:",
        "                        candidate_docs[doc_id] = matches * 2",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "    # If no candidates, try code concept expansion for recall",
        "    if not candidate_docs and use_code_concepts:",
        "        for token in tokens:",
        "            related = get_related_terms(token, max_terms=3)",
        "            for related_term in related:",
        "                col = layer0.get_minicolumn(related_term)",
        "                if col:",
        "                    for doc_id in col.document_ids:",
        "                        candidate_docs[doc_id] += 0.5  # Lower weight for expansion",
        ""
      ],
      "context_after": [
        "    if not candidate_docs:",
        "        return []",
        "",
        "    # Rank candidates by match count first (fast pre-filter)",
        "    sorted_candidates = sorted(",
        "        candidate_docs.items(),",
        "        key=lambda x: x[1],",
        "        reverse=True",
        "    )",
        ""
      ],
      "change_type": "add"
    },
    {
      "file": "cortical/query/search.py",
      "function": "def fast_find_documents(",
      "start_line": 168,
      "lines_added": [
        "        doc_scores[doc_id] = score",
        "",
        "    # Apply document name boost after all scores calculated",
        "    if doc_name_boost > 1.0 and doc_scores:",
        "        max_score = max(doc_scores.values())",
        "        exact_matches = []",
        "        partial_matches = []",
        "",
        "        for doc_id in doc_scores:",
        "                if match_ratio == 1.0:",
        "                    exact_matches.append(doc_id)",
        "                else:",
        "                    partial_matches.append((doc_id, match_ratio))",
        "",
        "        # Exact matches get additive boost to ensure top ranking",
        "        for doc_id in exact_matches:",
        "            doc_scores[doc_id] += max_score * doc_name_boost",
        "",
        "        # Partial matches get multiplicative boost",
        "        for doc_id, match_ratio in partial_matches:",
        "            boost = 1 + (doc_name_boost - 1) * match_ratio",
        "            doc_scores[doc_id] *= boost"
      ],
      "lines_removed": [
        "        # Boost documents whose name matches query terms",
        "        if doc_name_boost > 1.0:",
        "                boost = 1 + (doc_name_boost - 1) * match_ratio",
        "                score *= boost",
        "        doc_scores[doc_id] = score"
      ],
      "context_before": [
        "        for token in tokens:",
        "            col = layer0.get_minicolumn(token)",
        "            if col and doc_id in col.document_ids:",
        "                tfidf = col.tfidf_per_doc.get(doc_id, col.tfidf)",
        "                score += tfidf",
        "",
        "        # Boost by match coverage",
        "        coverage_boost = match_count / len(tokens)",
        "        score *= (1 + 0.5 * coverage_boost)",
        ""
      ],
      "context_after": [
        "            doc_name_tokens = set(tokenizer.tokenize(doc_id.replace('_', ' ')))",
        "            matches = len(query_tokens & doc_name_tokens)",
        "            if matches > 0:",
        "                match_ratio = matches / len(query_tokens)",
        "",
        "",
        "    # Return top results",
        "    sorted_docs = sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)",
        "    return sorted_docs[:top_n]",
        "",
        "",
        "def build_document_index(",
        "    layers: Dict[CorticalLayer, HierarchicalLayer]",
        ") -> Dict[str, Dict[str, float]]:",
        "    \"\"\""
      ],
      "change_type": "modify"
    },
    {
      "file": "tests/unit/test_query_definitions.py",
      "function": "def complex_function(",
      "start_line": 289,
      "lines_added": [
        "    def test_passage_starts_with_definition_line(self):",
        "        \"\"\"",
        "        Regression test for Task #179: Passage must start with definition line.",
        "",
        "        Bug: Previously, find_definition_in_text used `start = match.start() - 50`,",
        "        which could place start in the middle of an earlier line. When showcase.py",
        "        extracted the first line, it showed truncated/wrong content.",
        "",
        "        Fix: Now finds the start of the line containing the match, ensuring the",
        "        passage always starts with the actual definition line.",
        "        \"\"\"",
        "        # Simulate a realistic file structure with content before the definition",
        "        text = \"\"\"",
        "from typing import Dict, List",
        "from dataclasses import dataclass",
        "",
        "",
        "@dataclass",
        "class DataRecord:",
        "    id: str",
        "    content: str",
        "",
        "    def __post_init__(self):",
        "        if self.metadata is None:",
        "            self.metadata = {}",
        "",
        "",
        "class DataProcessor:",
        "    '''Main processor for handling data records.'''",
        "",
        "    def __init__(self):",
        "        self._records = {}",
        "",
        "    def clear(self):",
        "        '''Remove all records.'''",
        "        self._records.clear()",
        "",
        "",
        "def calculate_statistics(records: List[DataRecord]) -> Dict:",
        "    '''Calculate statistics for records.'''",
        "    if not records:",
        "        return {}",
        "    return {'count': len(records)}",
        "\"\"\"",
        "",
        "        # Test class definition",
        "        result = find_definition_in_text(text, \"DataProcessor\", \"class\")",
        "        assert result is not None",
        "        passage, start, end = result",
        "",
        "        # The passage should start with the actual definition line",
        "        first_line = passage.strip().split('\\n')[0]",
        "        assert first_line.startswith(\"class DataProcessor\"), (",
        "            f\"Expected first line to start with 'class DataProcessor', \"",
        "            f\"but got: {first_line!r}\"",
        "        )",
        "        # Should NOT start with truncated content like \"etadata is None\"",
        "        assert \"metadata\" not in first_line.lower() or \"dataprocessor\" in first_line.lower()",
        "",
        "        # Test function definition",
        "        result = find_definition_in_text(text, \"calculate_statistics\", \"function\")",
        "        assert result is not None",
        "        passage, start, end = result",
        "",
        "        # The passage should start with the function definition",
        "        first_line = passage.strip().split('\\n')[0]",
        "        assert first_line.startswith(\"def calculate_statistics\"), (",
        "            f\"Expected first line to start with 'def calculate_statistics', \"",
        "            f\"but got: {first_line!r}\"",
        "        )",
        "        # Should NOT start with truncated content like \"records.clear()\"",
        "        assert \"calculate_statistics\" in first_line",
        "",
        "    def test_definition_at_file_start(self):",
        "        \"\"\"Definition at the very start of file works correctly.\"\"\"",
        "        text = \"class FirstClass:\\n    pass\"",
        "        result = find_definition_in_text(text, \"FirstClass\", \"class\")",
        "        assert result is not None",
        "        passage, start, end = result",
        "",
        "        # Start should be 0 (beginning of file)",
        "        assert start == 0",
        "        # First line should be the definition",
        "        first_line = passage.strip().split('\\n')[0]",
        "        assert first_line.startswith(\"class FirstClass\")",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "        assert result is not None",
        "        passage, _, _ = result",
        "        assert \"__init__\" in passage",
        "",
        "    def test_invalid_def_type(self):",
        "        \"\"\"Invalid def_type returns None.\"\"\"",
        "        text = \"class Foo:\\n    pass\"",
        "        result = find_definition_in_text(text, \"Foo\", \"invalid_type\")",
        "        assert result is None",
        ""
      ],
      "context_after": [
        "",
        "# =============================================================================",
        "# DEFINITION PASSAGES SEARCH TESTS",
        "# =============================================================================",
        "",
        "",
        "class TestFindDefinitionPassages:",
        "    \"\"\"Tests for find_definition_passages() - main search function.\"\"\"",
        "",
        "    def test_non_definition_query(self):"
      ],
      "change_type": "add"
    },
    {
      "file": "tests/unit/test_query_passages.py",
      "function": "class MyClass:",
      "start_line": 1198,
      "lines_added": [
        "",
        "    def test_doc_type_boosting_changes_scores(self):",
        "        \"\"\"Doc-type boosting changes scores for test files.",
        "",
        "        Task #180: Verify that apply_doc_boost parameter actually affects scores.",
        "        Test files (with 'test' in name) should get lower scores when boosted.",
        "        \"\"\"",
        "        # Create two documents: regular code and test file",
        "        col = MockMinicolumn(",
        "            content=\"filter\",",
        "            tfidf=2.0,",
        "            document_ids={\"data_processor.py\", \"test_data_processor.py\"}",
        "        )",
        "        layers = MockLayers.empty()",
        "        layers[0] = MockHierarchicalLayer([col])",
        "        tokenizer = Tokenizer()",
        "        documents = {",
        "            \"data_processor.py\": \"filter data records efficiently\",",
        "            \"test_data_processor.py\": \"filter data records in tests\",",
        "        }",
        "",
        "        # Without boosting",
        "        results_no_boost = find_passages_for_query(",
        "            \"filter data\", layers, tokenizer, documents,",
        "            apply_doc_boost=False, use_expansion=False, use_definition_search=False",
        "        )",
        "",
        "        # With boosting (prefer_docs=True to enable boosting)",
        "        results_with_boost = find_passages_for_query(",
        "            \"filter data\", layers, tokenizer, documents,",
        "            apply_doc_boost=True, prefer_docs=True, use_expansion=False, use_definition_search=False",
        "        )",
        "",
        "        # Both should return results",
        "        assert len(results_no_boost) > 0",
        "        assert len(results_with_boost) > 0",
        "",
        "        # Extract scores for each document",
        "        def get_scores_by_doc(results):",
        "            scores = {}",
        "            for _, doc_id, _, _, score in results:",
        "                if doc_id not in scores or score > scores[doc_id]:",
        "                    scores[doc_id] = score",
        "            return scores",
        "",
        "        scores_no_boost = get_scores_by_doc(results_no_boost)",
        "        scores_with_boost = get_scores_by_doc(results_with_boost)",
        "",
        "        # Without boosting, both files should have similar scores",
        "        # (may differ slightly due to document length normalization)",
        "",
        "        # With boosting, test file should have lower score than regular file",
        "        if \"data_processor.py\" in scores_with_boost and \"test_data_processor.py\" in scores_with_boost:",
        "            # Test file should be penalized (0.8x boost vs 1.0x)",
        "            assert scores_with_boost[\"test_data_processor.py\"] < scores_with_boost[\"data_processor.py\"], \\",
        "                f\"Test file should have lower score with boosting. \" \\",
        "                f\"Got test={scores_with_boost['test_data_processor.py']:.3f}, \" \\",
        "                f\"regular={scores_with_boost['data_processor.py']:.3f}\"",
        "",
        "        # Verify scores actually changed between boosted and non-boosted",
        "        # At least one document's score should be different",
        "        changed = False",
        "        for doc_id in set(scores_no_boost.keys()) & set(scores_with_boost.keys()):",
        "            if abs(scores_no_boost[doc_id] - scores_with_boost[doc_id]) > 0.001:",
        "                changed = True",
        "                break",
        "",
        "        assert changed, \"Boosting should change at least one document's score\""
      ],
      "lines_removed": [],
      "context_before": [
        "        )",
        "",
        "        # Batch call",
        "        batch = find_passages_batch(",
        "            [\"test\"], layers, tokenizer, documents, use_expansion=False",
        "        )",
        "",
        "        # Results should match",
        "        assert len(batch) == 1",
        "        assert len(batch[0]) == len(single)"
      ],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "tests/unit/test_query_search.py",
      "function": "class TestFindDocumentsForQuery:",
      "start_line": 239,
      "lines_added": [
        "    def test_exact_doc_name_match_beats_high_tfidf(self):",
        "        \"\"\"",
        "        Task #181: Exact document name match ranks first even with lower TF-IDF.",
        "",
        "        Bug: Documents with high content scores could outrank exact name matches.",
        "        Fix: Exact matches get additive boost to ensure top ranking.",
        "        \"\"\"",
        "        layers = (",
        "            LayerBuilder()",
        "            .with_term(\"distributed\", tfidf=5.0)",
        "            .with_term(\"systems\", tfidf=5.0)",
        "            # distributed_systems doc has exact name match but low content",
        "            .with_document(\"distributed_systems\", [\"distributed\"])",
        "            # other_doc has high content score",
        "            .with_document(\"other_doc\", [\"distributed\", \"systems\"])",
        "            .build()",
        "        )",
        "",
        "        layer0 = layers[MockLayers.TOKENS]",
        "        # other_doc has MUCH higher TF-IDF scores",
        "        layer0.get_minicolumn(\"distributed\").tfidf_per_doc = {",
        "            \"distributed_systems\": 0.5,  # Low score",
        "            \"other_doc\": 10.0  # Very high score",
        "        }",
        "        layer0.get_minicolumn(\"systems\").tfidf_per_doc = {",
        "            \"other_doc\": 10.0  # Very high score",
        "        }",
        "",
        "        tokenizer = Tokenizer()",
        "        result = find_documents_for_query(",
        "            \"distributed systems\", layers, tokenizer,",
        "            use_expansion=False,",
        "            doc_name_boost=2.0",
        "        )",
        "",
        "        # distributed_systems should rank first due to exact name match",
        "        # despite having much lower TF-IDF score",
        "        assert result[0][0] == \"distributed_systems\"",
        "        assert result[0][1] > result[1][1]  # Score should be higher",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "        tokenizer = Tokenizer()",
        "        result = find_documents_for_query(",
        "            \"term\", layers, tokenizer,",
        "            use_expansion=False,",
        "            doc_name_boost=1.0  # No boost",
        "        )",
        "",
        "        # doc1 should win on TF-IDF alone",
        "        assert result[0][0] == \"doc1\"",
        ""
      ],
      "context_after": [
        "    def test_query_expansion_disabled(self):",
        "        \"\"\"use_expansion=False uses only query terms.\"\"\"",
        "        # Create connected terms",
        "        layers = (",
        "            LayerBuilder()",
        "            .with_term(\"neural\", tfidf=2.0, pagerank=0.8)",
        "            .with_term(\"network\", tfidf=2.0, pagerank=0.6)",
        "            .with_connection(\"neural\", \"network\", weight=5.0)",
        "            .with_document(\"doc1\", [\"neural\"])",
        "            .with_document(\"doc2\", [\"network\"])"
      ],
      "change_type": "add"
    },
    {
      "file": "tests/unit/test_query_search.py",
      "function": "class TestFastFindDocuments:",
      "start_line": 536,
      "lines_added": [
        "    def test_exact_name_match_added_to_candidates(self):",
        "        \"\"\"",
        "        Task #181: Exact name matches included in candidates even without content.",
        "",
        "        Bug: fast_find_documents excluded docs whose name matched but content didn't.",
        "        Fix: Add name-matching docs to candidate set.",
        "        \"\"\"",
        "        # Create doc that has exact name match but no matching content",
        "        layers = (",
        "            LayerBuilder()",
        "            .with_term(\"other\", tfidf=5.0)",
        "            .with_document(\"distributed_systems\", [\"other\"])  # No 'distributed' or 'systems' in content",
        "            .with_document(\"high_content_doc\", [\"other\"])",
        "            .build()",
        "        )",
        "",
        "        # Add layer3 (DOCUMENTS) for name matching",
        "        doc1 = MockMinicolumn(",
        "            content=\"distributed_systems\",",
        "            document_ids={\"distributed_systems\"}",
        "        )",
        "        doc2 = MockMinicolumn(",
        "            content=\"high_content_doc\",",
        "            document_ids={\"high_content_doc\"}",
        "        )",
        "",
        "        layers[MockLayers.DOCUMENTS] = MockHierarchicalLayer([doc1, doc2])",
        "",
        "        tokenizer = Tokenizer()",
        "        result = fast_find_documents(",
        "            \"distributed systems\", layers, tokenizer, doc_name_boost=2.0",
        "        )",
        "",
        "        # distributed_systems should be in results despite not having content match",
        "        doc_ids = [doc_id for doc_id, _ in result]",
        "        assert \"distributed_systems\" in doc_ids",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "        layers[MockLayers.TOKENS] = MockHierarchicalLayer([col])",
        "",
        "        tokenizer = Tokenizer()",
        "        result = fast_find_documents(",
        "            \"term\", layers, tokenizer, doc_name_boost=1.0",
        "        )",
        "",
        "        # high_score_doc should win on TF-IDF alone",
        "        assert result[0][0] == \"high_score_doc\"",
        ""
      ],
      "context_after": [
        "",
        "# =============================================================================",
        "# BUILD_DOCUMENT_INDEX TESTS",
        "# =============================================================================",
        "",
        "",
        "class TestBuildDocumentIndex:",
        "    \"\"\"Tests for build_document_index inverted index creation.\"\"\"",
        "",
        "    def test_empty_layer(self):"
      ],
      "change_type": "add"
    }
  ],
  "hour_of_day": 3,
  "day_of_week": "Saturday",
  "seconds_since_last_commit": -211446,
  "is_merge": false,
  "is_initial": false,
  "parent_count": 1,
  "session_id": null,
  "related_chats": [],
  "ci_result": null,
  "reverted": false,
  "amended": false
}