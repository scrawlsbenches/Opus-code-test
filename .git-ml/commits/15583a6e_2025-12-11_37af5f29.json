{
  "hash": "15583a6e56ed582de4bcbf776d86ebcd8ecea9b8",
  "message": "Implement developer experience tasks #67, #68, #71, #72",
  "author": "Claude",
  "timestamp": "2025-12-11 01:45:30 +0000",
  "branch": "claude/multi-index-design-DvifZ",
  "files_changed": [
    "TASK_LIST.md",
    "scripts/index_codebase.py",
    "scripts/search_codebase.py",
    "showcase.py"
  ],
  "insertions": 149,
  "deletions": 39,
  "hunks": [
    {
      "file": "TASK_LIST.md",
      "function": "def find_passages_for_query(..., apply_doc_boost: bool = True):",
      "start_line": 2416,
      "lines_added": [
        "**Status:** [x] Completed"
      ],
      "lines_removed": [
        "**Status:** [ ] Not Started"
      ],
      "context_before": [
        "These tasks focus on making the Cortical Text Processor genuinely enjoyable to use for day-to-day development work.",
        "",
        "---",
        "",
        "## Showcase Improvements",
        "",
        "### 67. Fix O(n) Lookup in Showcase find_concept_associations",
        "",
        "**File:** `showcase.py`",
        "**Lines:** 213-218"
      ],
      "context_after": [
        "**Priority:** Low",
        "",
        "**Problem:**",
        "The `find_concept_associations` method iterates all minicolumns to find neighbor content:",
        "```python",
        "for c in layer0.minicolumns.values():",
        "    if c.id == neighbor_id:",
        "        # found it",
        "```",
        ""
      ],
      "change_type": "modify"
    },
    {
      "file": "TASK_LIST.md",
      "function": "This is O(n) when we have O(1) `get_by_id()` available.",
      "start_line": 2441,
      "lines_added": [
        "**Status:** [x] Completed"
      ],
      "lines_removed": [
        "**Status:** [ ] Not Started"
      ],
      "context_before": [
        "neighbor = layer0.get_by_id(neighbor_id)",
        "if neighbor:",
        "    # use neighbor.content",
        "```",
        "",
        "---",
        "",
        "### 68. Add Code-Specific Features to Showcase",
        "",
        "**File:** `showcase.py`"
      ],
      "context_after": [
        "**Priority:** Medium",
        "",
        "**Problem:**",
        "The showcase demonstrates general IR capabilities but not code-specific features documented in CLAUDE.md:",
        "- `expand_query_for_code()` - programming-aware query expansion",
        "- `search_by_intent()` - natural language intent queries",
        "- `get_fingerprint()` / `compare_fingerprints()` - code similarity",
        "- `is_conceptual_query()` - query type detection",
        "",
        "**Solution:**"
      ],
      "change_type": "modify"
    },
    {
      "file": "TASK_LIST.md",
      "function": "Add timing for key operations:",
      "start_line": 2502,
      "lines_added": [
        "**Status:** [x] Completed"
      ],
      "lines_removed": [
        "**Status:** [ ] Not Started"
      ],
      "context_before": [
        "- Document search time",
        "- Passage retrieval time",
        "",
        "---",
        "",
        "## Code Index Improvements",
        "",
        "### 71. Enable Code-Aware Tokenization in Index",
        "",
        "**File:** `scripts/index_codebase.py`"
      ],
      "context_after": [
        "**Priority:** High",
        "",
        "**Problem:**",
        "The indexer uses default tokenization which doesn't split identifiers. Searching for \"user\" won't find `getUserCredentials` or `user_credentials`.",
        "",
        "**Solution:**",
        "Enable `split_identifiers=True` when creating the processor:",
        "```python",
        "processor = CorticalTextProcessor(",
        "    tokenizer_config={'split_identifiers': True}"
      ],
      "change_type": "modify"
    },
    {
      "file": "TASK_LIST.md",
      "function": "processor = CorticalTextProcessor(",
      "start_line": 2525,
      "lines_added": [
        "**Status:** [x] Completed"
      ],
      "lines_removed": [
        "**Status:** [ ] Not Started"
      ],
      "context_before": [
        "",
        "Or configure per-document based on file type (.py files get identifier splitting).",
        "",
        "**Impact:** Much better code search - \"auth\" would find `authenticate`, `AuthService`, `user_auth`, etc.",
        "",
        "---",
        "",
        "### 72. Use Programming Query Expansion in Search",
        "",
        "**File:** `scripts/search_codebase.py`"
      ],
      "context_after": [
        "**Priority:** High",
        "",
        "**Problem:**",
        "Search uses `expand_query()` but not `expand_query_for_code()`. Programming synonyms aren't utilized.",
        "",
        "**Solution:**",
        "```python",
        "# In search_codebase.py",
        "if is_code_query(query):  # Detect if searching for code patterns",
        "    expanded = processor.expand_query_for_code(query)"
      ],
      "change_type": "modify"
    },
    {
      "file": "TASK_LIST.md",
      "function": "Key files to understand:",
      "start_line": 2867,
      "lines_added": [
        "| # | Priority | Task | Status | Category |",
        "|---|----------|------|--------|----------|",
        "| 67 | Low | Fix O(n) lookup in showcase | ‚úì Done | Showcase |",
        "| 68 | Medium | Add code-specific features to showcase | ‚úì Done | Showcase |",
        "| 69 | Medium | Add passage-level search demo | | Showcase |",
        "| 70 | Low | Add performance timing to showcase | | Showcase |",
        "| 71 | High | Enable code-aware tokenization in index | ‚úì Done | Code Index |",
        "| 72 | High | Use programming query expansion in search | ‚úì Done | Code Index |",
        "| 73 | Medium | Add \"Find Similar Code\" command | | Code Index |",
        "| 74 | Medium | Add \"Explain This Code\" command | | Developer Experience |",
        "| 75 | Medium | Add \"What Changed?\" semantic diff | | Developer Experience |",
        "| 76 | Medium | Add \"Suggest Related Files\" feature | | Developer Experience |",
        "| 77 | High | Add interactive \"Ask the Codebase\" mode | | Developer Experience |",
        "| 78 | Low | Add code pattern detection | | Developer Experience |",
        "| 79 | Low | Add corpus health dashboard | | Developer Experience |",
        "| 80 | Low | Add \"Learning Mode\" for new contributors | | Developer Experience |",
        "",
        "---",
        "",
        "*Added 2025-12-11, completions updated 2025-12-11*"
      ],
      "lines_removed": [
        "| # | Priority | Task | Category |",
        "|---|----------|------|----------|",
        "| 67 | Low | Fix O(n) lookup in showcase | Showcase |",
        "| 68 | Medium | Add code-specific features to showcase | Showcase |",
        "| 69 | Medium | Add passage-level search demo | Showcase |",
        "| 70 | Low | Add performance timing to showcase | Showcase |",
        "| 71 | High | Enable code-aware tokenization in index | Code Index |",
        "| 72 | High | Use programming query expansion in search | Code Index |",
        "| 73 | Medium | Add \"Find Similar Code\" command | Code Index |",
        "| 74 | Medium | Add \"Explain This Code\" command | Developer Experience |",
        "| 75 | Medium | Add \"What Changed?\" semantic diff | Developer Experience |",
        "| 76 | Medium | Add \"Suggest Related Files\" feature | Developer Experience |",
        "| 77 | High | Add interactive \"Ask the Codebase\" mode | Developer Experience |",
        "| 78 | Low | Add code pattern detection | Developer Experience |",
        "| 79 | Low | Add corpus health dashboard | Developer Experience |",
        "| 80 | Low | Add \"Learning Mode\" for new contributors | Developer Experience |",
        "",
        "",
        "*Added 2025-12-11*"
      ],
      "context_before": [
        "[Press Enter to explore processor.py, or type a question]",
        "> How does PageRank work here?",
        "",
        "[Retrieves and explains relevant passages...]",
        "```",
        "",
        "---",
        "",
        "## Summary Table",
        ""
      ],
      "context_after": [],
      "change_type": "modify"
    },
    {
      "file": "scripts/index_codebase.py",
      "function": "import time",
      "start_line": 27,
      "lines_added": [
        "from cortical.tokenizer import Tokenizer",
        "def create_code_processor() -> CorticalTextProcessor:",
        "    \"\"\"",
        "    Create a CorticalTextProcessor configured for code indexing.",
        "",
        "    Enables split_identifiers so that:",
        "    - getUserCredentials ‚Üí ['getusercredentials', 'get', 'user', 'credentials']",
        "    - auth_service ‚Üí ['auth_service', 'auth', 'service']",
        "",
        "    This dramatically improves code search - \"auth\" will find AuthService,",
        "    authenticate, user_auth, etc.",
        "    \"\"\"",
        "    tokenizer = Tokenizer(split_identifiers=True)",
        "    return CorticalTextProcessor(tokenizer=tokenizer)",
        "",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "from contextlib import contextmanager",
        "from dataclasses import dataclass, field",
        "from datetime import datetime",
        "from pathlib import Path",
        "from typing import Dict, List, Optional, Tuple, Any",
        "",
        "# Add parent directory to path for imports",
        "sys.path.insert(0, str(Path(__file__).parent.parent))",
        "",
        "from cortical.processor import CorticalTextProcessor"
      ],
      "context_after": [
        "from cortical.chunk_index import (",
        "    ChunkWriter, ChunkLoader, ChunkCompactor,",
        "    get_changes_from_manifest as get_chunk_changes",
        ")",
        "",
        "",
        "# Manifest file version for compatibility checking",
        "MANIFEST_VERSION = \"1.0\"",
        "",
        "# Default timeout in seconds (0 = no timeout)",
        "DEFAULT_TIMEOUT = 300  # 5 minutes",
        "",
        "",
        "# =============================================================================",
        "# Progress Tracking System",
        "# ============================================================================="
      ],
      "change_type": "add"
    },
    {
      "file": "scripts/index_codebase.py",
      "function": "def index_with_chunks(",
      "start_line": 959,
      "lines_added": [
        "        processor = create_code_processor()"
      ],
      "lines_removed": [
        "        processor = CorticalTextProcessor()"
      ],
      "context_before": [
        "    tracker.end_phase(\"Loading documents from chunks\")",
        "",
        "    # Check if we can use cached pkl",
        "    cache_valid = loader.is_cache_valid(str(output_path))",
        "    if cache_valid and not (added or modified or deleted):",
        "        tracker.log(\"\\nCache is valid, loading from pkl...\")",
        "        processor = CorticalTextProcessor.load(str(output_path))",
        "    else:",
        "        # Build processor from documents (with metadata)",
        "        tracker.start_phase(\"Building processor from chunks\")"
      ],
      "context_after": [
        "        documents = [",
        "            (doc_id, content, all_metadata.get(doc_id))",
        "            for doc_id, content in all_docs.items()",
        "        ]",
        "        processor.add_documents_batch(documents, recompute='none', verbose=False)",
        "        tracker.log(f\"  Added {len(documents)} documents\")",
        "        tracker.end_phase(\"Building processor from chunks\")",
        "",
        "        # Compute analysis",
        "        fast_mode = not args.full_analysis"
      ],
      "change_type": "modify"
    },
    {
      "file": "scripts/index_codebase.py",
      "function": "def run_indexer(",
      "start_line": 1193,
      "lines_added": [
        "            processor = create_code_processor()",
        "        processor = create_code_processor()"
      ],
      "lines_removed": [
        "            processor = CorticalTextProcessor()",
        "        processor = CorticalTextProcessor()"
      ],
      "context_before": [
        "    if args.incremental and manifest and output_path.exists():",
        "        tracker.start_phase(\"Loading existing corpus\")",
        "        try:",
        "            processor = CorticalTextProcessor.load(str(output_path))",
        "            tracker.log(f\"  Loaded {len(processor.documents)} documents\")",
        "            tracker.end_phase(\"Loading existing corpus\")",
        "        except Exception as e:",
        "            tracker.warn(f\"Error loading corpus: {e}\")",
        "            tracker.log(\"  Falling back to full rebuild...\")",
        "            tracker.end_phase(\"Loading existing corpus\", status=\"failed\")"
      ],
      "context_after": [
        "            added, modified, deleted = all_files, [], []",
        "    else:",
        "        # Full index - treat all files as \"added\"",
        "        added = all_files",
        "        modified = []",
        "        deleted = []",
        "",
        "    # Perform indexing",
        "    if args.incremental and manifest:",
        "        incremental_index(processor, added, modified, deleted, base_path, tracker)",
        "    else:",
        "        full_index(processor, all_files, base_path, tracker)"
      ],
      "change_type": "modify"
    },
    {
      "file": "scripts/search_codebase.py",
      "function": "def display_results(results: list, verbose: bool = False, show_doc_type: bool =",
      "start_line": 162,
      "lines_added": [
        "def expand_query_display(processor: CorticalTextProcessor, query: str, use_code: bool = False):",
        "    if use_code:",
        "        expanded = processor.expand_query_for_code(query, max_expansions=15)",
        "        print(\"\\nQuery expansion (code-aware):\")",
        "        print(\"  (includes programming synonyms: get/fetch/load, etc.)\")",
        "    else:",
        "        expanded = processor.expand_query(query, max_expansions=10)",
        "        print(\"\\nQuery expansion:\")",
        "    print(\"  /code <query>    - Show code-aware expansion (programming synonyms)\")"
      ],
      "lines_removed": [
        "def expand_query_display(processor: CorticalTextProcessor, query: str):",
        "    expanded = processor.expand_query(query, max_expansions=10)",
        "    print(\"\\nQuery expansion:\")"
      ],
      "context_before": [
        "            lines = result['passage'].split('\\n')[:5]",
        "            for line in lines:",
        "                if len(line) > 76:",
        "                    line = line[:73] + '...'",
        "                print(f\"  {line}\")",
        "            if len(result['passage'].split('\\n')) > 5:",
        "                print(f\"  ... ({len(result['passage'].split(chr(10))) - 5} more lines)\")",
        "        print()",
        "",
        ""
      ],
      "context_after": [
        "    \"\"\"Show expanded query terms.\"\"\"",
        "    for term, weight in sorted(expanded.items(), key=lambda x: -x[1])[:10]:",
        "        print(f\"  {term}: {weight:.3f}\")",
        "",
        "",
        "def interactive_mode(processor: CorticalTextProcessor):",
        "    \"\"\"Run interactive search mode.\"\"\"",
        "    print(\"\\nInteractive Search Mode\")",
        "    print(\"=\" * 40)",
        "    print(\"Commands:\")",
        "    print(\"  /expand <query>  - Show query expansion\")",
        "    print(\"  /concepts        - List concept clusters\")",
        "    print(\"  /stats           - Show corpus statistics\")",
        "    print(\"  /help            - Show this help\")",
        "    print(\"  /quit            - Exit\")",
        "    print()",
        "",
        "    while True:",
        "        try:",
        "            query = input(\"Search> \").strip()",
        "        except (EOFError, KeyboardInterrupt):"
      ],
      "change_type": "modify"
    },
    {
      "file": "scripts/search_codebase.py",
      "function": "def interactive_mode(processor: CorticalTextProcessor):",
      "start_line": 200,
      "lines_added": [
        "                print(\"Commands: /expand, /code, /concepts, /stats, /quit\")",
        "            elif cmd == '/code' and len(cmd_parts) > 1:",
        "                expand_query_display(processor, cmd_parts[1], use_code=True)"
      ],
      "lines_removed": [
        "                print(\"Commands: /expand, /concepts, /stats, /quit\")"
      ],
      "context_before": [
        "            continue",
        "",
        "        if query.startswith('/'):",
        "            cmd_parts = query.split(maxsplit=1)",
        "            cmd = cmd_parts[0].lower()",
        "",
        "            if cmd == '/quit' or cmd == '/exit':",
        "                print(\"Goodbye!\")",
        "                break",
        "            elif cmd == '/help':"
      ],
      "context_after": [
        "            elif cmd == '/stats':",
        "                print(f\"\\nCorpus Statistics:\")",
        "                print(f\"  Documents: {len(processor.documents)}\")",
        "                print(f\"  Tokens: {processor.layers[0].column_count()}\")",
        "                print(f\"  Bigrams: {processor.layers[1].column_count()}\")",
        "                print(f\"  Concepts: {processor.layers[2].column_count()}\")",
        "                print(f\"  Relations: {len(processor.semantic_relations)}\")",
        "            elif cmd == '/expand' and len(cmd_parts) > 1:",
        "                expand_query_display(processor, cmd_parts[1])",
        "            elif cmd == '/concepts':",
        "                layer2 = processor.layers[2]",
        "                concepts = list(layer2.minicolumns.values())[:10]",
        "                print(f\"\\nTop concepts ({layer2.column_count()} total):\")",
        "                for c in concepts:",
        "                    print(f\"  {c.content[:50]}\")",
        "            else:",
        "                print(f\"Unknown command: {cmd}\")",
        "        else:",
        "            results = search_codebase(processor, query, top_n=5)"
      ],
      "change_type": "modify"
    },
    {
      "file": "scripts/search_codebase.py",
      "function": "def interactive_mode(processor: CorticalTextProcessor):",
      "start_line": 232,
      "lines_added": [
        "  %(prog)s \"fetch data\" --code            # Code-aware (also finds get/load/retrieve)",
        "  %(prog)s \"auth\" --code --expand         # Show programming synonyms",
        "    parser.add_argument('--code', action='store_true',",
        "                        help='Use code-aware query expansion (get‚Üífetch/load/retrieve)')",
        "            expand_query_display(processor, args.query, use_code=args.code)"
      ],
      "lines_removed": [
        "            expand_query_display(processor, args.query)"
      ],
      "context_before": [
        "",
        "def main():",
        "    parser = argparse.ArgumentParser(",
        "        description='Search the indexed codebase',",
        "        epilog=\"\"\"",
        "Examples:",
        "  %(prog)s \"PageRank algorithm\"           # Search with auto-boost",
        "  %(prog)s \"what is PageRank\" --prefer-docs  # Always boost docs",
        "  %(prog)s \"compute pagerank\" --no-boost  # Disable boosting",
        "  %(prog)s \"architecture\" --fast          # Fast document-level search"
      ],
      "context_after": [
        "        \"\"\"",
        "    )",
        "    parser.add_argument('query', nargs='?', help='Search query')",
        "    parser.add_argument('--corpus', '-c', default='corpus_dev.pkl',",
        "                        help='Corpus file path (default: corpus_dev.pkl)')",
        "    parser.add_argument('--top', '-n', type=int, default=5,",
        "                        help='Number of results (default: 5)')",
        "    parser.add_argument('--verbose', '-v', action='store_true',",
        "                        help='Show full passage text')",
        "    parser.add_argument('--expand', '-e', action='store_true',",
        "                        help='Show query expansion')",
        "    parser.add_argument('--interactive', '-i', action='store_true',",
        "                        help='Interactive search mode')",
        "    parser.add_argument('--fast', '-f', action='store_true',",
        "                        help='Fast search mode (document-level, ~2-3x faster)')",
        "    parser.add_argument('--prefer-docs', '-d', action='store_true',",
        "                        help='Always boost documentation files in results')",
        "    parser.add_argument('--no-boost', action='store_true',",
        "                        help='Disable document type boosting (raw TF-IDF)')",
        "    args = parser.parse_args()",
        "",
        "    base_path = Path(__file__).parent.parent",
        "    corpus_path = base_path / args.corpus",
        "",
        "    # Check if corpus exists",
        "    if not corpus_path.exists():",
        "        print(f\"Error: Corpus file not found: {corpus_path}\")",
        "        print(\"Run 'python scripts/index_codebase.py' first to create it.\")",
        "        sys.exit(1)",
        "",
        "    # Load corpus",
        "    print(f\"Loading corpus from {corpus_path}...\")",
        "    processor = CorticalTextProcessor.load(str(corpus_path))",
        "    print(f\"Loaded {len(processor.documents)} documents\\n\")",
        "",
        "    if args.interactive:",
        "        interactive_mode(processor)",
        "    elif args.query:",
        "        if args.expand:",
        "            print()",
        "",
        "        # Show query intent detection",
        "        is_conceptual = processor.is_conceptual_query(args.query)",
        "        if not args.no_boost:",
        "            intent_str = \"conceptual\" if is_conceptual else \"implementation\"",
        "            print(f\"(Query type: {intent_str})\")",
        "",
        "        results = search_codebase(",
        "            processor,"
      ],
      "change_type": "modify"
    },
    {
      "file": "showcase.py",
      "function": "class CorticalShowcase:",
      "start_line": 52,
      "lines_added": [
        "        self.demonstrate_code_features()"
      ],
      "lines_removed": [],
      "context_before": [
        "            print(\"No documents found!\")",
        "            return",
        "",
        "        self.analyze_hierarchy()",
        "        self.discover_key_concepts()",
        "        self.analyze_tfidf()",
        "        self.find_concept_associations()",
        "        self.analyze_document_relationships()",
        "        self.demonstrate_queries()",
        "        self.demonstrate_polysemy()"
      ],
      "context_after": [
        "        self.demonstrate_gap_analysis()",
        "        self.demonstrate_embeddings()",
        "        self.print_insights()",
        "    ",
        "    def print_intro(self):",
        "        \"\"\"Print introduction.\"\"\"",
        "        print(\"\"\"",
        "    ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó",
        "    ‚ïë                                                                      ‚ïë",
        "    ‚ïë            üß†  CORTICAL TEXT PROCESSOR SHOWCASE  üß†                  ‚ïë"
      ],
      "change_type": "add"
    },
    {
      "file": "showcase.py",
      "function": "class CorticalShowcase:",
      "start_line": 202,
      "lines_added": [
        "                    # O(1) lookup using _id_index",
        "                    neighbor = layer0.get_by_id(neighbor_id)",
        "                    if neighbor:",
        "                        bar_len = int(min(weight, 10) * 3)",
        "                        bar = \"‚îÄ\" * bar_len + \">\"",
        "                        print(f\"    {bar} {neighbor.content} (weight: {weight:.2f})\")"
      ],
      "lines_removed": [
        "                    # Find neighbor content",
        "                    for c in layer0.minicolumns.values():",
        "                        if c.id == neighbor_id:",
        "                            bar_len = int(min(weight, 10) * 3)",
        "                            bar = \"‚îÄ\" * bar_len + \">\"",
        "                            print(f\"    {bar} {c.content} (weight: {weight:.2f})\")",
        "                            break"
      ],
      "context_before": [
        "        for concept in test_concepts:",
        "            col = layer0.get_minicolumn(concept)",
        "            if col and col.lateral_connections:",
        "                print_subheader(f\"üîó '{concept}' connects to:\")",
        "                ",
        "                # Get top connections",
        "                sorted_conns = sorted(col.lateral_connections.items(), ",
        "                                     key=lambda x: x[1], reverse=True)[:6]",
        "                ",
        "                for neighbor_id, weight in sorted_conns:"
      ],
      "context_after": [
        "                print()",
        "    ",
        "    def analyze_document_relationships(self):",
        "        \"\"\"Show document-level relationships.\"\"\"",
        "        print_header(\"DOCUMENT RELATIONSHIPS\", \"‚ïê\")",
        "        ",
        "        print(\"Documents connect based on shared concepts and term overlap:\\n\")",
        "        ",
        "        layer3 = self.processor.get_layer(CorticalLayer.DOCUMENTS)",
        "        "
      ],
      "change_type": "modify"
    },
    {
      "file": "showcase.py",
      "function": "class CorticalShowcase:",
      "start_line": 306,
      "lines_added": [
        "    def demonstrate_code_features(self):",
        "        \"\"\"Demonstrate code-specific search capabilities.\"\"\"",
        "        print_header(\"CODE SEARCH FEATURES\", \"‚ïê\")",
        "",
        "        print(\"Features optimized for searching code and technical content:\\n\")",
        "",
        "        # 1. Query intent detection",
        "        print_subheader(\"üéØ Query Intent Detection\")",
        "        print(\"    The system detects whether queries are conceptual or implementation-focused:\\n\")",
        "",
        "        test_queries = [",
        "            (\"what is PageRank\", True),",
        "            (\"compute pagerank damping\", False),",
        "            (\"how does TF-IDF work\", True),",
        "            (\"find documents tfidf\", False),",
        "        ]",
        "",
        "        for query, expected_conceptual in test_queries:",
        "            is_conceptual = self.processor.is_conceptual_query(query)",
        "            intent = \"conceptual\" if is_conceptual else \"implementation\"",
        "            marker = \"üìñ\" if is_conceptual else \"üíª\"",
        "            print(f\"    {marker} \\\"{query}\\\" ‚Üí {intent}\")",
        "",
        "        print(\"\\n    üí° Use case: Boost documentation for conceptual queries,\")",
        "        print(\"                 boost code files for implementation queries.\")",
        "",
        "        # 2. Code-aware query expansion",
        "        print_subheader(\"\\nüîß Code-Aware Query Expansion\")",
        "        print(\"    Programming synonyms expand queries for better code search:\\n\")",
        "",
        "        code_queries = [\"fetch data\", \"get results\", \"process input\"]",
        "",
        "        for query in code_queries:",
        "            # Regular expansion",
        "            regular = self.processor.expand_query(query, max_expansions=5)",
        "            # Code-aware expansion",
        "            code_exp = self.processor.expand_query_for_code(query, max_expansions=8)",
        "",
        "            # Find terms only in code expansion",
        "            regular_terms = set(regular.keys())",
        "            code_terms = set(code_exp.keys())",
        "            new_terms = code_terms - regular_terms",
        "",
        "            print(f\"    Query: \\\"{query}\\\"\")",
        "            if new_terms:",
        "                new_list = sorted(new_terms, key=lambda t: -code_exp.get(t, 0))[:4]",
        "                print(f\"      + Code terms: {', '.join(new_list)}\")",
        "            else:",
        "                print(f\"      (corpus lacks programming synonyms for this query)\")",
        "            print()",
        "",
        "        # 3. Semantic fingerprinting",
        "        print_subheader(\"üîç Semantic Fingerprinting\")",
        "        print(\"    Compare text similarity using semantic fingerprints:\\n\")",
        "",
        "        # Get two related documents",
        "        if len(self.loaded_files) >= 2:",
        "            doc1_id = \"neural_pagerank\" if \"neural_pagerank\" in self.processor.documents else self.loaded_files[0][0]",
        "            doc2_id = \"pagerank_fundamentals\" if \"pagerank_fundamentals\" in self.processor.documents else self.loaded_files[1][0]",
        "",
        "            doc1_content = self.processor.documents.get(doc1_id, \"\")[:500]",
        "            doc2_content = self.processor.documents.get(doc2_id, \"\")[:500]",
        "",
        "            if doc1_content and doc2_content:",
        "                fp1 = self.processor.get_fingerprint(doc1_content, top_n=10)",
        "                fp2 = self.processor.get_fingerprint(doc2_content, top_n=10)",
        "",
        "                comparison = self.processor.compare_fingerprints(fp1, fp2)",
        "",
        "                print(f\"    Comparing: '{doc1_id}' vs '{doc2_id}'\")",
        "                print(f\"      Similarity: {comparison['overall_similarity']:.1%}\")",
        "                print(f\"      Shared concepts: {len(comparison.get('shared_concepts', []))}\")",
        "",
        "                if comparison['shared_terms']:",
        "                    shared = list(comparison['shared_terms'])[:5]",
        "                    print(f\"      Common terms: {', '.join(shared)}\")",
        "",
        "        print(\"\\n    üí° Use case: Find similar code, detect duplicates,\")",
        "        print(\"                 suggest related files when editing.\")",
        "        print()",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "            print(f\"\\n    'sticks' appears in {len(col.document_ids)} documents:\")",
        "            for doc_id in col.document_ids:",
        "                print(f\"      ‚Ä¢ {doc_id}\")",
        "",
        "        print(\"\\n    üí° Potential improvements:\")",
        "        print(\"      ‚Ä¢ Weight adjacent term matches higher (bigram boost)\")",
        "        print(\"      ‚Ä¢ Use document context for disambiguation\")",
        "        print(\"      ‚Ä¢ Implement word sense disambiguation\")",
        "        print()",
        ""
      ],
      "context_after": [
        "    def demonstrate_gap_analysis(self):",
        "        \"\"\"Show knowledge gap detection.\"\"\"",
        "        print_header(\"KNOWLEDGE GAP ANALYSIS\", \"‚ïê\")",
        "        ",
        "        print(\"Detecting gaps and anomalies in the corpus:\\n\")",
        "        ",
        "        gaps = self.processor.analyze_knowledge_gaps()",
        "        ",
        "        print(f\"  Coverage Score: {gaps['coverage_score']:.1%}\")",
        "        print(f\"  Connectivity Score: {gaps['connectivity_score']:.4f}\")"
      ],
      "change_type": "add"
    },
    {
      "file": "showcase.py",
      "function": "class CorticalShowcase:",
      "start_line": 385,
      "lines_added": [
        "        print(\"  ‚úì Demonstrated code search features\")"
      ],
      "lines_removed": [],
      "context_before": [
        "            top_doc = max(layer3.minicolumns.values(), key=lambda c: c.connection_count())",
        "            print(f\"  Most connected document: '{top_doc.content}'\")",
        "        ",
        "        print(\"\\n\" + \"‚ïê\" * 70)",
        "        print(\"Demo complete! The cortical text processor successfully:\")",
        "        print(\"  ‚úì Built hierarchical representations (Layers 0-3)\")",
        "        print(\"  ‚úì Discovered key concepts via PageRank\")",
        "        print(\"  ‚úì Computed TF-IDF for discriminative analysis\")",
        "        print(\"  ‚úì Found associations through lateral connections\")",
        "        print(\"  ‚úì Identified document relationships\")"
      ],
      "context_after": [
        "        print(\"  ‚úì Detected knowledge gaps and anomalies\")",
        "        print(\"  ‚úì Computed graph embeddings\")",
        "        print(\"  ‚úì Enabled semantic queries with expansion\")",
        "        print(\"‚ïê\" * 70 + \"\\n\")",
        "",
        "",
        "if __name__ == \"__main__\":",
        "    showcase = CorticalShowcase(samples_dir=\"samples\")",
        "    showcase.run()"
      ],
      "change_type": "add"
    }
  ],
  "hour_of_day": 1,
  "day_of_week": "Thursday",
  "seconds_since_last_commit": -388758,
  "is_merge": false,
  "is_initial": false,
  "parent_count": 1,
  "session_id": null,
  "related_chats": [],
  "ci_result": null,
  "reverted": false,
  "amended": false
}