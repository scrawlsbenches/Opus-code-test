{
  "hash": "e85c299016c3bfabdaadc56eeadd22169ed51187",
  "message": "Merge pull request #34 from scrawlsbenches/claude/louvain-community-detection-01FsvWk3GKjFLpEiPwQT4sBc",
  "author": "scrawlsbenches",
  "timestamp": "2025-12-11 10:17:47 -0500",
  "branch": "claude/multi-index-design-DvifZ",
  "files_changed": [
    "TASK_LIST.md",
    "cortical/analysis.py",
    "cortical/processor.py",
    "samples/ai_assisted_development.txt",
    "samples/code_review_practices.txt",
    "samples/debugging_strategies.txt",
    "samples/incremental_development.txt",
    "samples/refactoring_patterns.txt",
    "samples/task_decomposition.txt",
    "samples/technical_debt_management.txt",
    "samples/test_driven_development.txt",
    "tests/test_analysis.py"
  ],
  "insertions": 1110,
  "deletions": 57,
  "hunks": [
    {
      "file": "TASK_LIST.md",
      "function": null,
      "start_line": 1,
      "lines_added": [
        "**Completed Tasks:** 87+ (see archive)",
        "| 125 | Add clustering quality metrics (modularity, silhouette) | DevEx | - | Medium |",
        "| 126 | Investigate optimal Louvain resolution for sample corpus | Research | 123 | Medium |"
      ],
      "lines_removed": [
        "**Completed Tasks:** 86+ (see archive)",
        "| 123 | Replace label propagation with Louvain community detection | BugFix | - | Large |",
        "| 125 | Add clustering quality metrics (modularity, silhouette) | DevEx | 123 | Medium |"
      ],
      "context_before": [
        "# Task List: Cortical Text Processor",
        "",
        "Active backlog for the Cortical Text Processor project. Completed tasks are archived in [TASK_ARCHIVE.md](TASK_ARCHIVE.md).",
        "",
        "**Last Updated:** 2025-12-11",
        "**Pending Tasks:** 28"
      ],
      "context_after": [
        "",
        "---",
        "",
        "## Active Backlog",
        "",
        "<!-- Machine-parseable format for automation -->",
        "",
        "### ðŸ”´ Critical (Do Now)",
        "",
        "| # | Task | Category | Depends | Effort |",
        "|---|------|----------|---------|--------|",
        "| 124 | Add minimum cluster count regression tests | Testing | - | Medium |",
        "",
        "### ðŸŸ  High (Do This Week)",
        "",
        "| # | Task | Category | Depends | Effort |",
        "|---|------|----------|---------|--------|",
        "| 94 | Split query.py into focused modules | Arch | - | Large |",
        "| 97 | Integrate CorticalConfig into processor | Arch | - | Medium |",
        "",
        "### ðŸŸ¡ Medium (Do This Month)",
        "",
        "| # | Task | Category | Depends | Effort |",
        "|---|------|----------|---------|--------|",
        "| 91 | Create docs/README.md index | Docs | - | Small |",
        "| 92 | Add badges to README.md | DevEx | - | Small |",
        "| 93 | Update README with docs references | Docs | 91 | Small |"
      ],
      "change_type": "modify"
    },
    {
      "file": "TASK_LIST.md",
      "function": "Active backlog for the Cortical Text Processor project. Completed tasks are arch",
      "start_line": 80,
      "lines_added": [
        "| 123 | Replace label propagation with Louvain community detection | 2025-12-11 | Implemented Louvain algorithm, 34 clusters for 92 docs |",
        "### 123. Replace Label Propagation with Louvain Community Detection âœ…",
        "**Meta:** `status:completed` `priority:critical` `category:bugfix`",
        "**Files:** `cortical/analysis.py`, `cortical/processor.py`, `tests/test_analysis.py`",
        "**Completed:** 2025-12-11",
        "**Solution Applied:**",
        "1. Implemented `cluster_by_louvain()` in `cortical/analysis.py` (300+ lines)",
        "   - Phase 1: Local modularity optimization with cached sigma_tot for O(1) lookups",
        "   - Phase 2: Network aggregation into super-nodes",
        "   - Resolution parameter for controlling cluster granularity",
        "2. Added `clustering_method` parameter to `processor.build_concept_clusters()`",
        "   - Default: 'louvain' (recommended)",
        "   - Alternative: 'label_propagation' (backward compatibility)",
        "3. Enabled previously skipped regression test `test_no_single_cluster_dominates`",
        "",
        "**Results:**",
        "- 34 concept clusters for 92-document corpus (6518 tokens)",
        "- Largest cluster: 10.2% of tokens (well under 50% threshold)",
        "- All 823 tests pass",
        "- compute_all() takes ~12s for full corpus",
        "- [x] Louvain algorithm implemented without external dependencies",
        "- [x] 34 clusters for 92-document showcase corpus (exceeds 10+)",
        "- [x] All 823 existing tests pass",
        "- [x] Regression test `test_no_single_cluster_dominates` enabled and passing",
        "- [x] showcase.py demonstrates improved clustering"
      ],
      "lines_removed": [
        "### 123. Replace Label Propagation with Louvain Community Detection ðŸ”´",
        "**Meta:** `status:pending` `priority:critical` `category:bugfix`",
        "**Files:** `cortical/analysis.py`",
        "**Solution:** Replace with Louvain community detection algorithm:",
        "- Louvain optimizes modularity (internal density vs external sparsity)",
        "- Naturally handles dense graphs by finding natural community boundaries",
        "- Widely used in graph analysis (NetworkX, igraph, etc.)",
        "- Zero external dependencies (we can implement the algorithm ourselves)",
        "",
        "**Implementation Steps:**",
        "1. Implement Louvain algorithm in `analysis.py`",
        "   - Phase 1: Local modularity optimization",
        "   - Phase 2: Network aggregation",
        "   - Repeat until no improvement",
        "2. Add `clustering_method` parameter ('louvain', 'label_propagation')",
        "3. Default to 'louvain' for better results",
        "4. Keep label propagation for backward compatibility",
        "5. Update showcase.py to use new method",
        "",
        "**Expected Results:**",
        "- 10-20+ meaningful concept clusters for 95-doc corpus",
        "- Clusters that represent actual topic boundaries",
        "- Semantic coherence within clusters",
        "- [ ] Louvain algorithm implemented without external dependencies",
        "- [ ] 10+ clusters for 95-document showcase corpus",
        "- [ ] Existing tests pass",
        "- [ ] New tests verify cluster quality",
        "- [ ] showcase.py demonstrates improved clustering"
      ],
      "context_before": [
        "| # | Task | Started | Notes |",
        "|---|------|---------|-------|",
        "| 87 | Add Python code samples and showcase | 2025-12-11 | samples/*.py created |",
        "",
        "---",
        "",
        "## Recently Completed (Last 7 Days)",
        "",
        "| # | Task | Completed | Notes |",
        "|---|------|-----------|-------|"
      ],
      "context_after": [
        "| 122 | Investigate Concept Layer & Embeddings regressions | 2025-12-11 | Fixed inverted strictness, improved embeddings |",
        "| 119 | Create AI metadata generator script | 2025-12-11 | scripts/generate_ai_metadata.py with tests |",
        "| 120 | Add AI metadata loader to Claude skills | 2025-12-11 | ai-metadata skill created |",
        "| 121 | Auto-regenerate AI metadata on changes | 2025-12-11 | Documented in CLAUDE.md, skills |",
        "| 88 | Create package installation files | 2025-12-11 | pyproject.toml, requirements.txt |",
        "| 89 | Create CONTRIBUTING.md | 2025-12-11 | Contribution guide |",
        "| 90 | Create docs/quickstart.md | 2025-12-11 | 5-minute tutorial |",
        "| 103 | Add Priority Backlog Summary | 2025-12-11 | TASK_LIST.md restructure |",
        "| 104 | Create TASK_ARCHIVE.md | 2025-12-11 | 75+ tasks archived |",
        "| 105 | Standardize task format | 2025-12-11 | Meta tags, effort estimates |",
        "| 109 | Add Recently Completed section | 2025-12-11 | Session context |",
        "| 86 | Add semantic chunk boundaries for code | 2025-12-11 | In query.py |",
        "| 85 | Improve test vs source ranking | 2025-12-11 | DOC_TYPE_BOOSTS |",
        "",
        "*Full details in [TASK_ARCHIVE.md](TASK_ARCHIVE.md)*",
        "",
        "---",
        "",
        "## Pending Task Details",
        "",
        "",
        "**Effort:** Large",
        "",
        "**Problem:** Label propagation clustering fails catastrophically on densely connected graphs:",
        "- 95 documents produce only 3 concept clusters",
        "- One mega-cluster contains 99.8% of tokens (6,667 of 6,679)",
        "- The algorithm converges to minimal clusters regardless of strictness parameters",
        "- This renders the concept layer (Layer 2) essentially useless",
        "",
        "**Root Cause Analysis:**",
        "Label propagation works by having each node adopt the most common label among neighbors. On a densely connected graph (avg 18.2 connections per token), information propagates everywhere, causing nearly all nodes to converge to a single label.",
        "",
        "This is NOT a parameter tuning problem - it's a fundamental algorithmic limitation. The `cluster_strictness` parameter only delays convergence, it cannot prevent it.",
        "",
        "",
        "**Acceptance Criteria:**",
        "",
        "---",
        "",
        "### 124. Add Minimum Cluster Count Regression Tests ðŸ”´",
        "",
        "**Meta:** `status:pending` `priority:critical` `category:testing`",
        "**Files:** `tests/test_analysis.py`, `tests/test_processor.py`",
        "**Effort:** Medium",
        "",
        "**Problem:** We had NO tests that would catch clustering failures:"
      ],
      "change_type": "modify"
    },
    {
      "file": "TASK_LIST.md",
      "function": "Layer 2: Concept Layer (V4)",
      "start_line": 264,
      "lines_added": [
        "### 126. Investigate Optimal Louvain Resolution for Sample Corpus",
        "",
        "**Meta:** `status:pending` `priority:high` `category:research`",
        "**Files:** `cortical/analysis.py`, `cortical/processor.py`, `showcase.py`",
        "**Effort:** Medium",
        "**Depends:** 123",
        "",
        "**Problem:** The Louvain algorithm's `resolution` parameter significantly affects cluster count:",
        "- Resolution 0.5 â†’ ~19 clusters (coarse)",
        "- Resolution 1.0 â†’ ~32 clusters (default)",
        "- Resolution 2.0 â†’ ~66 clusters (fine)",
        "- Resolution 3.0 â†’ ~93 clusters (very fine)",
        "",
        "The default value of 1.0 produces 34 clusters for our 103-document corpus, but we haven't determined if this is optimal for the semantic structure of our sample documents.",
        "",
        "**Investigation Goals:**",
        "1. Analyze cluster quality at different resolution values using modularity score",
        "2. Evaluate semantic coherence within clusters (do related terms cluster together?)",
        "3. Assess cluster utility for downstream tasks (query expansion, concept retrieval)",
        "4. Consider whether different document types benefit from different resolutions",
        "5. Determine if auto-tuning resolution based on corpus characteristics is feasible",
        "",
        "**Proposed Approach:**",
        "```python",
        "# Test different resolutions and evaluate cluster quality",
        "for resolution in [0.5, 0.75, 1.0, 1.25, 1.5, 2.0, 3.0]:",
        "    clusters = processor.build_concept_clusters(resolution=resolution)",
        "    modularity = compute_modularity(processor.layers)",
        "    coherence = evaluate_semantic_coherence(clusters)",
        "    print(f\"Resolution {resolution}: {len(clusters)} clusters, Q={modularity:.3f}\")",
        "```",
        "",
        "**Questions to Answer:**",
        "- What resolution maximizes modularity for our corpus?",
        "- Do development-focused documents cluster better at certain resolutions?",
        "- Should we expose resolution as a user-configurable parameter in CorticalConfig?",
        "- Is there a heuristic for auto-selecting resolution based on corpus size/density?",
        "",
        "**Deliverables:**",
        "- [ ] Analysis script comparing resolution values",
        "- [ ] Recommended default resolution (with justification)",
        "- [ ] Documentation of resolution parameter effects",
        "- [ ] Optional: Auto-tuning heuristic if feasible",
        "",
        "---",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "",
        "**Acceptance Criteria:**",
        "- [ ] Modularity score implemented",
        "- [ ] Silhouette score implemented",
        "- [ ] Balance metric implemented",
        "- [ ] Metrics displayed in showcase.py",
        "- [ ] Quality thresholds documented",
        "",
        "---",
        ""
      ],
      "context_after": [
        "### 7. Document Magic Numbers",
        "",
        "**Meta:** `status:deferred` `priority:low` `category:docs`",
        "**Files:** `cortical/gaps.py:62,76,99`",
        "**Effort:** Small",
        "",
        "**Problem:** Magic numbers in gap detection lack documentation.",
        "",
        "**Note:** Deferred - functional as-is, low priority.",
        ""
      ],
      "change_type": "add"
    },
    {
      "file": "cortical/analysis.py",
      "function": null,
      "start_line": 1,
      "lines_added": [
        "- Louvain community detection for clustering (recommended)",
        "- Label propagation for clustering (legacy, for backward compatibility)"
      ],
      "lines_removed": [
        "- Label propagation for clustering"
      ],
      "context_before": [
        "\"\"\"",
        "Analysis Module",
        "===============",
        "",
        "Graph analysis algorithms for the cortical network.",
        "",
        "Contains implementations of:",
        "- PageRank for importance scoring",
        "- TF-IDF for term weighting"
      ],
      "context_after": [
        "- Activation propagation for information flow",
        "\"\"\"",
        "",
        "import math",
        "from typing import Dict, List, Tuple, Set, Optional, Any",
        "from collections import defaultdict",
        "",
        "from .layers import CorticalLayer, HierarchicalLayer",
        "from .minicolumn import Minicolumn",
        ""
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/analysis.py",
      "function": "def cluster_by_label_propagation(",
      "start_line": 630,
      "lines_added": [
        "def cluster_by_louvain(",
        "    layer: HierarchicalLayer,",
        "    min_cluster_size: int = 3,",
        "    resolution: float = 1.0,",
        "    max_iterations: int = 10",
        ") -> Dict[int, List[str]]:",
        "    \"\"\"",
        "    Cluster minicolumns using Louvain community detection.",
        "",
        "    Louvain is a modularity optimization algorithm that finds communities",
        "    by iteratively improving modularity. Unlike label propagation, it",
        "    handles dense graphs well and produces meaningful clusters.",
        "",
        "    The algorithm works in two phases:",
        "    1. Local optimization: Move nodes to communities that maximize modularity",
        "    2. Network aggregation: Merge communities into super-nodes and repeat",
        "",
        "    Args:",
        "        layer: Layer to cluster",
        "        min_cluster_size: Minimum nodes per cluster (clusters below this",
        "            size are filtered from the result)",
        "        resolution: Resolution parameter for modularity (default 1.0).",
        "            - Higher values (>1.0): More, smaller clusters",
        "            - Lower values (<1.0): Fewer, larger clusters",
        "        max_iterations: Maximum number of optimization passes (default 10)",
        "",
        "    Returns:",
        "        Dictionary mapping cluster_id to list of column contents",
        "",
        "    Example:",
        "        >>> clusters = cluster_by_louvain(layer0, min_cluster_size=3)",
        "        >>> print(f\"Found {len(clusters)} clusters\")",
        "",
        "    Note:",
        "        This is a zero-dependency implementation of the Louvain algorithm.",
        "        For very large graphs (>100k nodes), consider using optimized",
        "        implementations from networkx or igraph.",
        "    \"\"\"",
        "    columns = list(layer.minicolumns.keys())",
        "    n = len(columns)",
        "",
        "    if n == 0:",
        "        return {}",
        "",
        "    # Build adjacency structure from layer connections",
        "    # content -> {neighbor_content: weight}",
        "    adjacency: Dict[str, Dict[str, float]] = {c: {} for c in columns}",
        "    total_weight = 0.0",
        "",
        "    for content in columns:",
        "        col = layer.minicolumns[content]",
        "        for neighbor_id, weight in col.lateral_connections.items():",
        "            neighbor = layer.get_by_id(neighbor_id)",
        "            if neighbor and neighbor.content in adjacency:",
        "                adjacency[content][neighbor.content] = weight",
        "                total_weight += weight",
        "",
        "    # m = total edge weight (each edge counted once)",
        "    # Since adjacency is bidirectional, total_weight counts each edge twice",
        "    m = total_weight / 2.0",
        "",
        "    if m == 0:",
        "        # No connections - each node is its own cluster",
        "        clusters = {i: [content] for i, content in enumerate(columns)}",
        "        return {k: v for k, v in clusters.items() if len(v) >= min_cluster_size}",
        "",
        "    # Initialize: each node in its own community",
        "    # community[content] = community_id",
        "    community: Dict[str, int] = {content: i for i, content in enumerate(columns)}",
        "",
        "    # Precompute node degrees (sum of edge weights)",
        "    # k[content] = sum of weights attached to content",
        "    k: Dict[str, float] = {}",
        "    for content in columns:",
        "        k[content] = sum(adjacency[content].values())",
        "",
        "    # Cache community degree sums for O(1) lookup instead of O(n) per node",
        "    # sigma_tot[community_id] = sum of degrees of nodes in that community",
        "    sigma_tot: Dict[int, float] = {i: k[content] for i, content in enumerate(columns)}",
        "",
        "    def compute_modularity_gain(",
        "        node: str,",
        "        target_community: int,",
        "        node_community_weights: Dict[int, float]",
        "    ) -> float:",
        "        \"\"\"",
        "        Compute modularity gain from moving node to target_community.",
        "",
        "        Uses the formula:",
        "        Î”Q = [k_i,in / m - resolution * k_i * Î£_tot / (2mÂ²)]",
        "",
        "        where:",
        "        - k_i = degree of node i",
        "        - k_i,in = sum of edge weights from node i to nodes in target community",
        "        - Î£_tot = sum of degrees of all nodes in target community",
        "        \"\"\"",
        "        k_i = k[node]",
        "        k_i_in = node_community_weights.get(target_community, 0.0)",
        "",
        "        # Use cached sigma_tot value (O(1) instead of O(n))",
        "        target_sigma_tot = sigma_tot.get(target_community, 0.0)",
        "",
        "        # If node is already in target_community, exclude its contribution",
        "        if community[node] == target_community:",
        "            target_sigma_tot -= k_i",
        "",
        "        if m == 0:",
        "            return 0.0",
        "",
        "        # Modularity gain with resolution parameter",
        "        gain = k_i_in / m - resolution * k_i * target_sigma_tot / (2 * m * m)",
        "        return gain",
        "",
        "    def phase1() -> bool:",
        "        \"\"\"",
        "        Local optimization phase.",
        "",
        "        For each node, try moving it to each neighbor's community.",
        "        Move to the community that gives maximum modularity gain.",
        "",
        "        Returns:",
        "            True if any node was moved, False if converged",
        "        \"\"\"",
        "        nonlocal sigma_tot  # Allow updating the cached sigma_tot",
        "",
        "        improved = True",
        "        any_moved = False",
        "",
        "        while improved:",
        "            improved = False",
        "",
        "            for node in columns:",
        "                current_comm = community[node]",
        "",
        "                # Compute weights to each neighboring community",
        "                # comm_weights[community_id] = sum of edge weights to that community",
        "                comm_weights: Dict[int, float] = {}",
        "                for neighbor, weight in adjacency[node].items():",
        "                    neighbor_comm = community[neighbor]",
        "                    comm_weights[neighbor_comm] = comm_weights.get(neighbor_comm, 0.0) + weight",
        "",
        "                # Find best community to move to",
        "                best_comm = current_comm",
        "                best_gain = 0.0",
        "",
        "                # Check current community first (to stay if no improvement)",
        "                for target_comm in comm_weights:",
        "                    if target_comm == current_comm:",
        "                        continue",
        "",
        "                    gain = compute_modularity_gain(node, target_comm, comm_weights)",
        "                    # Also compute \"loss\" from leaving current community",
        "                    loss = compute_modularity_gain(node, current_comm, comm_weights)",
        "                    net_gain = gain - loss",
        "",
        "                    if net_gain > best_gain:",
        "                        best_gain = net_gain",
        "                        best_comm = target_comm",
        "",
        "                # Move node if there's improvement",
        "                if best_comm != current_comm:",
        "                    # Update sigma_tot cache: remove from old, add to new",
        "                    k_node = k[node]",
        "                    sigma_tot[current_comm] = sigma_tot.get(current_comm, 0.0) - k_node",
        "                    sigma_tot[best_comm] = sigma_tot.get(best_comm, 0.0) + k_node",
        "",
        "                    community[node] = best_comm",
        "                    improved = True",
        "                    any_moved = True",
        "",
        "        return any_moved",
        "",
        "    def phase2() -> Tuple[",
        "        Dict[str, Dict[str, float]],  # new adjacency",
        "        Dict[str, int],  # new community mapping",
        "        Dict[str, float],  # new k values",
        "        Dict[int, float],  # new sigma_tot",
        "        float,  # new m value",
        "        Dict[int, List[str]]  # community -> original nodes",
        "    ]:",
        "        \"\"\"",
        "        Network aggregation phase.",
        "",
        "        Merge nodes in the same community into super-nodes.",
        "        Create new graph where edge weight between super-nodes is",
        "        sum of edges between their constituent nodes.",
        "",
        "        Returns:",
        "            New adjacency, community mapping, k values, sigma_tot, m, and community members",
        "        \"\"\"",
        "        # Get unique communities",
        "        unique_comms = set(community.values())",
        "",
        "        # Map old community IDs to new sequential IDs",
        "        comm_map = {old_id: new_id for new_id, old_id in enumerate(sorted(unique_comms))}",
        "",
        "        # Track which original nodes belong to each super-node",
        "        comm_members: Dict[int, List[str]] = {i: [] for i in range(len(unique_comms))}",
        "        for node, comm in community.items():",
        "            new_comm = comm_map[comm]",
        "            comm_members[new_comm].append(node)",
        "",
        "        # Build new adjacency between super-nodes",
        "        new_adj: Dict[str, Dict[str, float]] = {}",
        "        new_m = 0.0",
        "",
        "        for new_comm in range(len(unique_comms)):",
        "            new_adj[str(new_comm)] = {}",
        "",
        "        for node, neighbors in adjacency.items():",
        "            node_new_comm = comm_map[community[node]]",
        "            for neighbor, weight in neighbors.items():",
        "                neighbor_new_comm = comm_map[community[neighbor]]",
        "                if node_new_comm != neighbor_new_comm:",
        "                    # Edge between different communities",
        "                    key = str(neighbor_new_comm)",
        "                    node_key = str(node_new_comm)",
        "                    new_adj[node_key][key] = new_adj[node_key].get(key, 0.0) + weight",
        "                    new_m += weight",
        "",
        "        new_m /= 2.0  # Each edge counted twice",
        "",
        "        # New community mapping (each super-node starts in its own community)",
        "        new_community = {str(i): i for i in range(len(unique_comms))}",
        "",
        "        # New k values (degree of each super-node)",
        "        new_k: Dict[str, float] = {}",
        "        for new_comm in range(len(unique_comms)):",
        "            # Sum of all degrees of constituent nodes",
        "            new_k[str(new_comm)] = sum(k[node] for node in comm_members[new_comm])",
        "",
        "        # New sigma_tot (each super-node starts in its own community, so sigma_tot = k)",
        "        new_sigma_tot: Dict[int, float] = {i: new_k[str(i)] for i in range(len(unique_comms))}",
        "",
        "        return new_adj, new_community, new_k, new_sigma_tot, new_m, comm_members",
        "",
        "    # Main Louvain loop",
        "    # Track the hierarchy of community memberships",
        "    community_hierarchy: List[Dict[int, List[str]]] = []",
        "",
        "    for iteration in range(max_iterations):",
        "        # Phase 1: Local optimization",
        "        moved = phase1()",
        "",
        "        if not moved and iteration > 0:",
        "            # Converged",
        "            break",
        "",
        "        # Check if we've reduced to a single community",
        "        unique_comms = set(community.values())",
        "        if len(unique_comms) <= 1:",
        "            break",
        "",
        "        # Phase 2: Network aggregation",
        "        adjacency, new_community, k, sigma_tot, m, members = phase2()",
        "        community_hierarchy.append(members)",
        "",
        "        # Update columns list for new super-nodes",
        "        columns = list(adjacency.keys())",
        "        community = new_community",
        "",
        "        if m == 0:",
        "            break",
        "",
        "    # Reconstruct final communities by unwinding the hierarchy",
        "    # Start with the final community assignment",
        "    final_communities: Dict[int, Set[str]] = {}",
        "",
        "    if community_hierarchy:",
        "        # We have hierarchy - unwind it",
        "        # Start with last level",
        "        for super_node, comm in community.items():",
        "            if comm not in final_communities:",
        "                final_communities[comm] = set()",
        "",
        "            # Trace back through hierarchy to get original nodes",
        "            current_members = {super_node}",
        "",
        "            for level_members in reversed(community_hierarchy):",
        "                new_members: Set[str] = set()",
        "                for member in current_members:",
        "                    member_int = int(member)",
        "                    if member_int in level_members:",
        "                        new_members.update(level_members[member_int])",
        "                    else:",
        "                        # Member is an original node",
        "                        new_members.add(member)",
        "                current_members = new_members",
        "",
        "            final_communities[comm].update(current_members)",
        "    else:",
        "        # No hierarchy - use direct community assignment",
        "        for node, comm in community.items():",
        "            if comm not in final_communities:",
        "                final_communities[comm] = set()",
        "            final_communities[comm].add(node)",
        "",
        "    # Convert to expected format and filter by size",
        "    result: Dict[int, List[str]] = {}",
        "    cluster_id = 0",
        "    for comm, members in final_communities.items():",
        "        # Filter out numeric super-node IDs, keep only original content strings",
        "        original_members = [m for m in members if m in layer.minicolumns]",
        "        if len(original_members) >= min_cluster_size:",
        "            result[cluster_id] = original_members",
        "            cluster_id += 1",
        "",
        "    # Update cluster_id on minicolumns",
        "    for label, members in result.items():",
        "        for content in members:",
        "            if content in layer.minicolumns:",
        "                layer.minicolumns[content].cluster_id = label",
        "",
        "    return result",
        "",
        ""
      ],
      "lines_removed": [],
      "context_before": [
        "",
        "    # Update cluster_id on minicolumns",
        "    for label, members in filtered.items():",
        "        for content in members:",
        "            if content in layer.minicolumns:",
        "                layer.minicolumns[content].cluster_id = label",
        "",
        "    return filtered",
        "",
        ""
      ],
      "context_after": [
        "def build_concept_clusters(",
        "    layers: Dict[CorticalLayer, HierarchicalLayer],",
        "    clusters: Dict[int, List[str]]",
        ") -> None:",
        "    \"\"\"",
        "    Build concept layer from token clusters.",
        "    ",
        "    Creates Layer 2 (Concepts) minicolumns from clustered tokens.",
        "    Each concept is named after its most important members.",
        "    "
      ],
      "change_type": "add"
    },
    {
      "file": "cortical/processor.py",
      "function": "class CorticalTextProcessor:",
      "start_line": 934,
      "lines_added": [
        "        clustering_method: str = 'louvain',",
        "        resolution: float = 1.0,",
        "        Build concept clusters from token layer.",
        "            clustering_method: Algorithm to use for clustering.",
        "                - 'louvain' (default): Louvain community detection.",
        "                  Recommended for dense graphs. Produces meaningful clusters",
        "                  by optimizing modularity.",
        "                - 'label_propagation': Legacy label propagation algorithm.",
        "                  May produce mega-clusters on dense graphs (not recommended).",
        "            cluster_strictness: For label_propagation only. Controls clustering",
        "                aggressiveness (0.0-1.0).",
        "            bridge_weight: For label_propagation only. Weight for synthetic",
        "                inter-document connections (0.0-1.0).",
        "            resolution: For louvain only. Resolution parameter for modularity.",
        "                - Higher values (>1.0): More, smaller clusters",
        "                - Lower values (<1.0): Fewer, larger clusters",
        "                - 1.0 (default): Standard modularity",
        "            >>> # Default Louvain clustering (recommended)",
        "            >>> # Louvain with higher resolution for more clusters",
        "            ...     clustering_method='louvain',",
        "            ...     resolution=1.5",
        "            ... )",
        "            >>>",
        "            >>> # Legacy label propagation (backward compatibility)",
        "            >>> clusters = processor.build_concept_clusters(",
        "            ...     clustering_method='label_propagation',",
        "            ...     cluster_strictness=0.5",
        "        if clustering_method == 'louvain':",
        "            clusters = analysis.cluster_by_louvain(",
        "                self.layers[CorticalLayer.TOKENS],",
        "                min_cluster_size=min_cluster_size,",
        "                resolution=resolution",
        "            )",
        "        elif clustering_method == 'label_propagation':",
        "            clusters = analysis.cluster_by_label_propagation(",
        "                self.layers[CorticalLayer.TOKENS],",
        "                min_cluster_size=min_cluster_size,",
        "                cluster_strictness=cluster_strictness,",
        "                bridge_weight=bridge_weight",
        "            )",
        "        else:",
        "            raise ValueError(",
        "                f\"Unknown clustering_method: {clustering_method}. \"",
        "                f\"Use 'louvain' or 'label_propagation'.\"",
        "            )",
        "",
        "            print(f\"Built {len(clusters)} concept clusters using {clustering_method}\")"
      ],
      "lines_removed": [
        "        Build concept clusters from token layer using label propagation.",
        "            cluster_strictness: Controls clustering aggressiveness (0.0-1.0).",
        "                Lower values create fewer, larger clusters with more connections.",
        "            bridge_weight: Weight for synthetic inter-document connections (0.0-1.0).",
        "                When > 0, adds weak connections between tokens from different",
        "                documents, helping bridge topic-isolated clusters.",
        "                - 0.0 (default): No bridging",
        "                - 0.3: Light bridging",
        "                - 0.7: Strong bridging",
        "            >>> # Default strict clustering",
        "            >>> # Looser clustering for more cross-topic connections",
        "            ...     cluster_strictness=0.5,",
        "            ...     bridge_weight=0.3",
        "        clusters = analysis.cluster_by_label_propagation(",
        "            self.layers[CorticalLayer.TOKENS],",
        "            min_cluster_size=min_cluster_size,",
        "            cluster_strictness=cluster_strictness,",
        "            bridge_weight=bridge_weight",
        "        )",
        "            print(f\"Built {len(clusters)} concept clusters\")"
      ],
      "context_before": [
        "        if verbose:",
        "            print(f\"Created {stats['connections_created']} bigram connections \"",
        "                  f\"(component: {stats['component_connections']}, \"",
        "                  f\"chain: {stats['chain_connections']}, \"",
        "                  f\"cooccur: {stats['cooccurrence_connections']})\")",
        "        return stats",
        "",
        "    def build_concept_clusters(",
        "        self,",
        "        min_cluster_size: int = 3,"
      ],
      "context_after": [
        "        cluster_strictness: float = 1.0,",
        "        bridge_weight: float = 0.0,",
        "        verbose: bool = True",
        "    ) -> Dict[int, List[str]]:",
        "        \"\"\"",
        "",
        "        Args:",
        "            min_cluster_size: Minimum tokens per cluster (default 3)",
        "                - 1.0 (default): Strict clustering, topics stay separate",
        "                - 0.5: Moderate mixing, allows some cross-topic clustering",
        "                - 0.0: Minimal clustering, most tokens group together",
        "            verbose: Print progress messages",
        "",
        "        Returns:",
        "            Dictionary mapping cluster_id to list of token contents",
        "",
        "        Example:",
        "            >>> clusters = processor.build_concept_clusters()",
        "            >>>",
        "            >>> clusters = processor.build_concept_clusters(",
        "            ... )",
        "        \"\"\"",
        "        analysis.build_concept_clusters(self.layers, clusters)",
        "        if verbose:",
        "        return clusters",
        "",
        "    def compute_concept_connections(",
        "        self,",
        "        use_semantics: bool = True,",
        "        min_shared_docs: int = 1,",
        "        min_jaccard: float = 0.1,",
        "        use_member_semantics: bool = False,",
        "        use_embedding_similarity: bool = False,",
        "        embedding_threshold: float = 0.3,"
      ],
      "change_type": "modify"
    },
    {
      "file": "samples/ai_assisted_development.txt",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "AI-Assisted Software Development: Principles for Intelligent Code Generation",
        "",
        "AI assistance in software development combines machine capability with human",
        "judgment. The most effective collaboration leverages AI strengths in pattern",
        "recognition, code generation, and search while preserving human oversight for",
        "design decisions, requirement interpretation, and quality assurance.",
        "",
        "Understanding before implementation distinguishes effective AI assistance.",
        "Reading existing code establishes patterns, conventions, and architectural",
        "context. Examining test files reveals expected behaviors. Reviewing documentation",
        "clarifies intent. This foundation enables changes that integrate naturally",
        "rather than introducing foreign patterns.",
        "",
        "Incremental changes minimize risk. Large modifications introduce many potential",
        "failure points simultaneously. Small, focused changes enable verification at",
        "each step. If tests fail after a small change, the cause is obvious. If tests",
        "fail after extensive changes, diagnosis becomes archaeology.",
        "",
        "Verification accompanies every change. Running tests after modifications",
        "confirms behavior preservation. Checking for linter errors catches style",
        "violations. Verifying builds succeed ensures syntactic correctness. Each",
        "verification provides confidence before proceeding to subsequent changes.",
        "",
        "Explicit reasoning supports complex decisions. When multiple approaches exist,",
        "enumerate options and evaluate tradeoffs. Document why one approach was chosen",
        "over alternatives. This transparency enables review and correction when",
        "reasoning contains errors.",
        "",
        "Conservative modification preserves working systems. When uncertain whether",
        "a change is safe, favor caution. Avoid modifying code outside the immediate",
        "scope of the task. Side effects in unrelated code indicate insufficient",
        "understanding or inappropriate coupling.",
        "",
        "Pattern consistency maintains codebase coherence. New code should resemble",
        "existing code in style, structure, and convention. Introducing novel patterns",
        "requires justification. Consistency reduces cognitive load for developers who",
        "must understand and maintain the code.",
        "",
        "Test coverage accompanies new functionality. Untested code is unverified code.",
        "Tests document expected behavior, prevent regression, and enable confident",
        "refactoring. The investment in tests pays dividends throughout the code's",
        "lifetime.",
        "",
        "Error handling deserves explicit attention. Consider what can fail and how",
        "failures should be handled. Graceful degradation prevents cascading failures.",
        "Meaningful error messages enable diagnosis. Robust systems anticipate and",
        "handle exceptional conditions.",
        "",
        "Documentation captures important decisions. Comments explain why, not what.",
        "README updates reflect new capabilities. API documentation stays synchronized",
        "with implementation. Future developers, including AI assistants, benefit from",
        "clear documentation.",
        "",
        "Security considerations apply throughout development. Input validation prevents",
        "injection attacks. Authentication verifies identity. Authorization enforces",
        "permissions. Sensitive data requires protection. Security cannot be added later;",
        "it must be built in from the beginning.",
        "",
        "Performance awareness prevents regressions. Changes to hot paths deserve extra",
        "scrutiny. Algorithm complexity matters for large inputs. Database queries can",
        "become bottlenecks. Profiling identifies actual rather than assumed problems.",
        "",
        "Feedback integration improves over time. When modifications fail tests or",
        "introduce bugs, analyze what went wrong. Pattern recognition across failures",
        "reveals systematic improvements. Learning from mistakes prevents repetition.",
        "",
        "Communication clarity supports collaboration. Explain what changes were made",
        "and why. Summarize test results and verification steps. Highlight areas of",
        "uncertainty or risk. Clear communication enables effective human oversight.",
        "",
        "Task tracking maintains focus. Complex tasks decompose into manageable steps.",
        "Progress tracking provides visibility. Completing items before starting new",
        "ones prevents abandoned partial work. Explicit tracking supports accountability.",
        "",
        "The goal is reliable, high-quality code that solves real problems. AI assistance",
        "accelerates development while maintaining the standards that enable sustainable",
        "software. Speed without quality creates technical debt; quality without speed",
        "frustrates users. Effective AI assistance achieves both.",
        "",
        "Humility acknowledges limitations. AI systems can be wrong. Humans provide",
        "essential judgment, domain knowledge, and accountability. The best outcomes",
        "emerge from human-AI collaboration that leverages each party's strengths while",
        "mitigating their weaknesses.",
        "",
        "Continuous improvement characterizes effective AI assistance. Each task provides",
        "learning opportunities. Feedback refines approaches. Better understanding of",
        "codebases improves suggestions. The partnership between human developers and",
        "AI assistants grows more effective over time through deliberate reflection",
        "and adaptation."
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "samples/code_review_practices.txt",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "Code Review Practices and Principles",
        "",
        "Code review is a systematic examination of source code by peers to identify defects,",
        "improve code quality, and share knowledge across the development team. Effective",
        "code review balances thoroughness with efficiency, focusing on meaningful feedback",
        "rather than stylistic nitpicks.",
        "",
        "The primary goals of code review include detecting bugs before they reach production,",
        "ensuring code follows established patterns and conventions, verifying that changes",
        "align with architectural decisions, and facilitating knowledge transfer between",
        "team members. Reviews also serve as documentation of design decisions and rationale.",
        "",
        "When reviewing code, prioritize correctness over style. Ask whether the code does",
        "what it claims to do, handles edge cases appropriately, and fails gracefully when",
        "given unexpected input. Security considerations deserve special attention: check",
        "for input validation, proper authentication, authorization boundaries, and potential",
        "injection vulnerabilities.",
        "",
        "Effective reviewers focus on the \"why\" rather than just the \"what\". Understanding",
        "the intent behind changes helps identify logical errors and suggests better approaches.",
        "Questions like \"what happens if this fails?\" or \"how does this interact with existing",
        "functionality?\" reveal implicit assumptions that may not hold.",
        "",
        "The size of changes matters significantly. Small, focused pull requests receive",
        "better reviews than large, sprawling changes. Aim for changes under 400 lines when",
        "possible. Large changes should be split into logical commits that can be reviewed",
        "independently while still maintaining a coherent narrative.",
        "",
        "Constructive feedback distinguishes excellent reviewers. Frame suggestions as",
        "questions or observations rather than commands. \"Have you considered...\" invites",
        "discussion while \"You should...\" creates defensiveness. Acknowledge good decisions",
        "and clever solutions alongside pointing out problems.",
        "",
        "Automation complements human review. Linters catch style issues, type checkers",
        "find type errors, and test suites verify behavior. These automated checks free",
        "reviewers to focus on design, logic, and maintainability concerns that require",
        "human judgment.",
        "",
        "Review comments should be actionable. Vague feedback like \"this is confusing\"",
        "provides less value than specific observations: \"the variable name 'data' doesn't",
        "convey what this contains; consider 'user_preferences' instead.\" Include examples",
        "or links to documentation when suggesting alternatives.",
        "",
        "The reviewer-author relationship should be collaborative, not adversarial. Both",
        "parties share the goal of shipping quality code. Disagreements should focus on",
        "technical merit, not personal preferences. When consensus proves elusive, escalate",
        "to team leads or defer to established conventions.",
        "",
        "Timeliness affects code review effectiveness. Stale reviews lose context as the",
        "codebase evolves. Aim to review changes within one business day. If a thorough",
        "review requires more time, leave initial comments and communicate the timeline.",
        "",
        "Self-review before requesting peer review catches obvious issues. Reading your",
        "own diff with fresh eyes reveals overlooked problems: missing error handling,",
        "inadequate tests, unclear naming, or unnecessary complexity.",
        "",
        "Documentation deserves the same scrutiny as code. Verify that comments explain",
        "why, not what. Check that README updates reflect new functionality. Ensure API",
        "documentation stays synchronized with implementation.",
        "",
        "Testing coverage should accompany code changes. New functionality needs tests",
        "demonstrating correct behavior. Bug fixes require tests that would have caught",
        "the bug. Refactoring should preserve existing test coverage while potentially",
        "adding tests for previously untested paths.",
        "",
        "Performance implications warrant consideration during review. Changes to hot paths,",
        "database queries, or algorithm complexity can impact system behavior significantly.",
        "Ask about benchmarks or profiling results for performance-sensitive changes."
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "samples/debugging_strategies.txt",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "Debugging Strategies: Systematic Approaches to Finding and Fixing Defects",
        "",
        "Debugging transforms mysterious misbehavior into understood and corrected code.",
        "Effective debugging combines systematic investigation with creative hypothesis",
        "generation. The goal extends beyond fixing the immediate symptom to understanding",
        "root causes and preventing similar issues.",
        "",
        "Reproduction provides the foundation for all debugging. Until you can reliably",
        "trigger the bug, you cannot verify fixes. Document the exact steps, inputs,",
        "environment, and timing that produce the failure. Intermittent bugs require",
        "identifying the conditions that make them reproducible.",
        "",
        "Minimization isolates the essential trigger. Remove everything unnecessary from",
        "the reproduction case. Simpler reproduction speeds investigation, eliminates",
        "confounding factors, and often reveals the bug's nature through what remains",
        "essential. A minimal test case may already suggest the solution.",
        "",
        "Binary search locates problems efficiently. When a large change introduces a bug,",
        "test the midpoint. If the bug exists, it was introduced in the first half;",
        "otherwise, the second half. Each test halves the search space, finding the",
        "culprit in logarithmic time. Git bisect automates this for commit histories.",
        "",
        "The scientific method structures investigation. Form a hypothesis about the",
        "cause. Design an experiment to test it. Execute the experiment and observe",
        "results. If the hypothesis is wrong, what did you learn? Each experiment should",
        "distinguish between possible causes, narrowing the search.",
        "",
        "Print debugging, despite its simplicity, remains effective. Strategic output",
        "statements reveal program state at key points. Compare actual values to expected",
        "values. The divergence point indicates where reasoning about the code differs",
        "from its actual behavior. Remove debugging output after resolving the issue.",
        "",
        "Debuggers provide interactive investigation. Breakpoints pause execution at",
        "specific locations. Step commands advance through code line by line. Variable",
        "inspection reveals current state. Conditional breakpoints trigger only when",
        "specific conditions hold, catching rare circumstances.",
        "",
        "Rubber duck debugging externalizes reasoning. Explaining the problem aloud,",
        "even to an inanimate object, forces articulation of assumptions and logic.",
        "The act of explanation often reveals gaps in understanding where bugs hide.",
        "Many bugs become obvious when you try to describe expected versus actual behavior.",
        "",
        "Log analysis traces execution history. Production logs capture events leading",
        "to failures. Structured logging with consistent formatting enables automated",
        "analysis. Correlation IDs track requests across distributed systems. Effective",
        "logging anticipates debugging needs before problems occur.",
        "",
        "Stack traces identify execution paths. When exceptions occur, the stack trace",
        "shows the call chain leading to failure. Read traces bottom-up: the immediate",
        "cause appears at the top, but root causes often lurk in earlier frames where",
        "incorrect state originated.",
        "",
        "State inspection examines data at rest. Database queries reveal stored values.",
        "API responses show what services return. Configuration files define runtime",
        "behavior. Discrepancies between expected and actual state explain unexpected",
        "behavior.",
        "",
        "Diff analysis compares working and broken states. What changed between the",
        "last working version and the current broken version? Code diffs, configuration",
        "changes, dependency updates, and infrastructure modifications all potentially",
        "introduce bugs. Reverting changes confirms or eliminates suspects.",
        "",
        "Memory analysis debugs resource issues. Memory profilers identify leaks and",
        "excessive allocation. Heap dumps capture memory state for offline analysis.",
        "Reference tracking reveals which objects retain memory that should be freed.",
        "",
        "Concurrency bugs require specialized techniques. Race conditions depend on timing,",
        "making reproduction difficult. Thread dumps capture execution state across all",
        "threads. Lock analysis identifies deadlocks and contention. Stress testing with",
        "randomized timing increases the probability of triggering races.",
        "",
        "Input validation catches data-related bugs. What happens with empty input? Null",
        "values? Extremely large values? Malformed formats? Unicode edge cases? Special",
        "characters? Boundary conditions? Systematic exploration of input space reveals",
        "assumptions violated by unexpected data.",
        "",
        "Error message analysis extracts maximum information. Exact error text enables",
        "searching documentation, issue trackers, and community forums. Error codes",
        "identify specific failure modes. Timestamps correlate events across system",
        "components.",
        "",
        "Root cause analysis prevents recurrence. After fixing the immediate bug, ask",
        "why it occurred. Was it a misunderstanding of requirements? A gap in testing?",
        "A communication failure? An unclear API? Addressing root causes prevents similar",
        "bugs rather than just fixing symptoms.",
        "",
        "Documentation captures debugging knowledge. Record what you tried, what you",
        "learned, and how you fixed the issue. Future debuggers, including your future",
        "self, benefit from this institutional memory. Good bug reports become valuable",
        "references for similar problems."
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "samples/incremental_development.txt",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "Incremental Development: Building Software Through Progressive Enhancement",
        "",
        "Incremental development constructs systems through successive additions of",
        "working functionality. Each increment extends the system while maintaining",
        "stability. This approach provides early feedback, reduces risk, and enables",
        "continuous adaptation to evolving requirements.",
        "",
        "The working system serves as the foundation. At every point, the software should",
        "be deployable and functional, even if feature-incomplete. Partially implemented",
        "features remain disabled until ready. The ability to ship at any time provides",
        "flexibility and reduces pressure from arbitrary deadlines.",
        "",
        "Feature flags enable incremental deployment. New functionality hides behind",
        "toggles that control visibility. Developers work on features in production",
        "without exposing them to users. Gradual rollout reveals problems to small",
        "audiences before full release. Flags also enable instant rollback if issues",
        "emerge.",
        "",
        "Vertical slices deliver end-to-end value. Rather than building complete layers",
        "sequentially, implement thin slices through all layers. A minimal but complete",
        "feature provides user value and validates architectural decisions. Subsequent",
        "iterations enhance and expand the initial slice.",
        "",
        "The walking skeleton establishes architecture early. Implement the minimum system",
        "that exercises all major components and their connections. This skeleton proves",
        "the architecture works before investing in detailed functionality. Flesh grows",
        "on proven bones rather than discovering structural problems late.",
        "",
        "Continuous integration validates each increment. Every change merges to the main",
        "branch frequently, triggering automated builds and tests. Integration problems",
        "surface immediately while changes remain small and fresh. Large batches of",
        "changes create large integration efforts; small changes integrate smoothly.",
        "",
        "Continuous deployment extends integration to production. Automated pipelines",
        "promote validated changes through environments to production. Manual deployment",
        "gates introduce delays and batch changes, sacrificing incremental benefits.",
        "Trusted automation enables rapid, safe delivery.",
        "",
        "Backward compatibility protects existing users during transitions. New versions",
        "accept old formats. APIs evolve additively without breaking existing clients.",
        "Database migrations transform data gradually. Careful compatibility enables",
        "incremental change without flag days that risk everything at once.",
        "",
        "The strangler fig pattern replaces legacy systems incrementally. New functionality",
        "routes through improved implementations while legacy code continues serving",
        "existing paths. Gradually, traffic migrates to the replacement. The legacy system",
        "withers as the new system grows, eventually becoming completely obsolete.",
        "",
        "Branch by abstraction refactors incrementally. Introduce an abstraction layer",
        "over the code requiring change. Migrate callers to use the abstraction. Implement",
        "the new version behind the same abstraction. Switch the abstraction to use the",
        "new implementation. Remove the old implementation. Each step is small and safe.",
        "",
        "Expand and contract manages API evolution. First expand: add new capability while",
        "preserving old interfaces. Allow clients to migrate at their pace. Then contract:",
        "remove deprecated interfaces after clients complete migration. This phased",
        "approach prevents breaking changes while enabling evolution.",
        "",
        "Monitoring validates incremental changes. Observability reveals whether new code",
        "behaves correctly in production. Key metrics, error rates, and user behavior",
        "confirm successful changes or signal problems requiring rollback. Without",
        "monitoring, incremental deployment operates blind.",
        "",
        "Small batches reduce risk and accelerate feedback. Large changes aggregate risk:",
        "more can go wrong, and problems are harder to diagnose. Small changes limit",
        "blast radius when issues occur and simplify root cause identification. The",
        "discipline of small changes prevents accumulation of unvalidated work.",
        "",
        "Definition of done ensures increments are truly complete. Code written but not",
        "tested, tested but not documented, or documented but not deployed does not",
        "constitute a complete increment. Done means releasable, including all necessary",
        "supporting work.",
        "",
        "Technical debt awareness accompanies incremental development. Fast increments",
        "may accumulate shortcuts that slow future work. Allocate time to address debt",
        "alongside feature development. Sustainable pace requires balancing delivery",
        "speed with code health.",
        "",
        "User feedback guides subsequent increments. Real usage reveals which features",
        "matter and how they should evolve. Early delivery enables early learning.",
        "Building incrementally with feedback loops creates software that serves actual",
        "rather than imagined needs.",
        "",
        "Iteration planning sequences increments strategically. High-value, low-risk",
        "increments build momentum and validate approaches. Risky increments surface",
        "early to preserve time for adaptation. Dependencies determine order when",
        "increments build on previous work.",
        "",
        "The goal is not incremental development for its own sake but the benefits it",
        "enables: reduced risk through early validation, faster feedback through early",
        "delivery, sustained flexibility through maintained deployability, and continuous",
        "improvement through learning from production experience."
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "samples/refactoring_patterns.txt",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "Refactoring Patterns: Systematic Code Improvement",
        "",
        "Refactoring restructures existing code without changing its external behavior.",
        "The goal is improved code quality: better readability, reduced complexity,",
        "enhanced maintainability, and clearer expression of intent. Safe refactoring",
        "requires tests that verify behavior preservation throughout the transformation.",
        "",
        "Extract Method addresses long functions by isolating cohesive code blocks into",
        "named functions. The new function name documents the purpose of that code segment.",
        "Extraction reveals implicit parameters and return values, clarifying data flow",
        "and reducing cognitive load when reading the original function.",
        "",
        "Rename Variable replaces unclear names with intention-revealing alternatives.",
        "Names like data, temp, or result convey nothing about purpose. Names like",
        "user_preferences, cached_calculation, or validation_result explain what values",
        "represent. Good names eliminate the need for explanatory comments.",
        "",
        "Extract Class separates distinct responsibilities into dedicated classes.",
        "A class handling both database access and business logic violates single",
        "responsibility. Extracting the database logic into a repository class improves",
        "cohesion and enables independent testing of each responsibility.",
        "",
        "Inline Method reverses extraction when a function's body is as clear as its name.",
        "Small indirection layers that add no abstraction benefit should be collapsed.",
        "However, preserve methods that serve as extension points or represent meaningful",
        "domain concepts even when currently simple.",
        "",
        "Replace Conditional with Polymorphism eliminates type-checking conditionals.",
        "Instead of switching on type to determine behavior, define a method on each type",
        "that implements its specific behavior. The conditional disappears, and adding",
        "new types requires no modification of existing code.",
        "",
        "Introduce Parameter Object groups related parameters into a cohesive structure.",
        "Functions taking many parameters become hard to call correctly. A parameter",
        "object makes relationships explicit, enables meaningful naming, and provides",
        "a home for behavior that operates on those related values.",
        "",
        "Replace Magic Number with Named Constant improves readability and maintainability.",
        "The number 86400 means nothing; SECONDS_PER_DAY documents intent and centralizes",
        "the value for consistent use. Constants also enable IDE support for finding",
        "all usages when values need to change.",
        "",
        "Decompose Conditional separates condition logic from branch bodies. Extract",
        "the condition into a well-named predicate function. Extract each branch into",
        "a function explaining what happens in that case. The restructured code reads",
        "like prose: if user_is_premium() then apply_discount() else charge_full_price().",
        "",
        "Replace Nested Conditionals with Guard Clauses flattens deeply nested code.",
        "Handle exceptional cases early with immediate returns, leaving the main logic",
        "path at the top level. Guard clauses eliminate arrow-shaped code that requires",
        "tracking multiple conditions simultaneously.",
        "",
        "Move Method relocates behavior to the class that owns the data it uses.",
        "Feature envy, where a method accesses another class's data extensively, signals",
        "misplaced responsibility. Moving the method near its data improves cohesion",
        "and reduces coupling between classes.",
        "",
        "Extract Interface defines abstraction boundaries for concrete implementations.",
        "When multiple classes share similar behavior, extract an interface they all",
        "implement. Code depending on the interface rather than concrete classes gains",
        "flexibility to substitute implementations.",
        "",
        "Replace Temp with Query converts temporary variables into method calls.",
        "Long methods often accumulate temporary variables that obscure data flow.",
        "Extracting calculations into methods reveals their purpose and enables reuse.",
        "The performance cost rarely matters; clarity usually trumps micro-optimization.",
        "",
        "Introduce Null Object eliminates null checks by providing a do-nothing implementation.",
        "Rather than checking if logger is null before logging, use a NullLogger that",
        "silently accepts and ignores log calls. Code becomes simpler when null is never",
        "a valid value to handle.",
        "",
        "Split Loop separates loops that do multiple things into focused iterations.",
        "A loop calculating both sum and maximum should become two loops, each doing one",
        "thing. The performance cost is negligible, but the clarity gain is substantial.",
        "Each loop can then be extracted into its own named method.",
        "",
        "Consolidate Duplicate Conditional Fragments moves common code outside conditionals.",
        "If both branches of a conditional end with the same statement, move that statement",
        "after the conditional. This reveals the essential difference between branches",
        "and eliminates subtle maintenance errors when common code drifts.",
        "",
        "The refactoring mindset prioritizes small, safe steps. Large refactorings emerge",
        "from sequences of small transformations, each keeping tests green. This discipline",
        "catches errors immediately while they are easy to diagnose and revert, rather",
        "than discovering problems after extensive changes obscure their source."
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "samples/task_decomposition.txt",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "Task Decomposition: Breaking Complex Work into Manageable Steps",
        "",
        "Task decomposition transforms overwhelming challenges into achievable increments.",
        "Large, ambiguous tasks paralyze progress; small, concrete tasks enable momentum.",
        "Effective decomposition reveals hidden complexity, exposes dependencies, and",
        "provides clear milestones that demonstrate progress toward larger goals.",
        "",
        "The first decomposition step clarifies the end state. What specific, observable",
        "outcome indicates completion? Vague goals like \"improve performance\" resist",
        "decomposition. Concrete goals like \"reduce API latency to under 200ms for the",
        "95th percentile\" enable identification of specific steps toward that outcome.",
        "",
        "Vertical slicing divides features into thin, end-to-end implementations rather",
        "than horizontal layers. Instead of building all backend, then all frontend,",
        "implement one complete user flow at a time. Each slice delivers value and",
        "provides feedback before investing in the next slice.",
        "",
        "Dependency mapping identifies which tasks block others. Tasks without dependencies",
        "can proceed in parallel. Tasks with many dependents deserve priority; their",
        "completion unblocks substantial downstream work. Visualizing dependencies reveals",
        "the critical path through a project.",
        "",
        "Size estimation improves decomposition quality. Tasks estimated at more than",
        "a few hours likely hide unrecognized complexity. Large estimates signal the",
        "need for further breakdown. Small, consistently-sized tasks enable predictable",
        "progress and accurate project forecasting.",
        "",
        "The spike pattern handles uncertainty. When a task's scope depends on unknown",
        "factors, time-box an investigation spike. The spike's goal is answering questions",
        "that enable decomposition, not delivering functionality. Spikes trade short-term",
        "efficiency for reduced long-term risk.",
        "",
        "Acceptance criteria define done for each task. What must be true when this task",
        "completes? Criteria might include tests passing, documentation updated, code",
        "reviewed, and functionality deployed. Clear criteria prevent scope creep and",
        "enable objective completion assessment.",
        "",
        "Progressive refinement recognizes that early decomposition relies on incomplete",
        "information. Initial task breakdowns reflect current understanding. As work",
        "progresses and understanding deepens, refine remaining tasks based on new",
        "knowledge. Premature detailed planning wastes effort on invalidated assumptions.",
        "",
        "The minimum viable approach identifies the simplest version that provides value.",
        "What is the smallest implementation worth deploying? Starting minimal enables",
        "faster feedback, earlier value delivery, and more informed decisions about",
        "subsequent elaboration. Complexity should grow only as needed.",
        "",
        "Risk-first ordering addresses uncertainties early. Tasks with unknown outcomes,",
        "unfamiliar technologies, or external dependencies should be tackled first.",
        "Early discovery of blockers preserves time for adaptation. Deferring risks",
        "compresses response time when problems inevitably emerge.",
        "",
        "Interface-first design enables parallel work. Define contracts between components",
        "before implementing either side. With stable interfaces, teams can work",
        "independently, integrating later. Interface stability matters more than initial",
        "perfection; iterate interfaces before committing to implementations.",
        "",
        "Test tasks explicitly in the decomposition. \"Write tests for user authentication\"",
        "should appear as distinct tasks, not assumed parts of implementation. Explicit",
        "test tasks ensure coverage, enable progress visibility, and prevent testing",
        "from being squeezed when schedules tighten.",
        "",
        "Documentation tasks similarly deserve explicit inclusion. API documentation,",
        "architecture decision records, runbooks, and user guides all require dedicated",
        "time. Making documentation visible prevents it from becoming an afterthought",
        "that never happens.",
        "",
        "Review checkpoints provide natural decomposition boundaries. Tasks should be",
        "scoped to enable meaningful code review. Massive changes overwhelm reviewers;",
        "small, focused changes receive better feedback. Structure tasks around reviewable",
        "units of work.",
        "",
        "Rollback planning accompanies deployment tasks. For each deployment, identify",
        "what could go wrong and how to recover. Decompose rollback procedures into",
        "their own verified steps. Practiced rollback enables confident deployment and",
        "rapid recovery from production issues.",
        "",
        "Communication tasks recognize that engineering includes more than code. Stakeholder",
        "updates, team synchronization, and cross-team coordination all require time and",
        "attention. Including communication explicitly prevents it from becoming invisible",
        "work that delays other tasks.",
        "",
        "The decomposition process itself benefits from iteration. Initial breakdowns",
        "often miss tasks discovered during implementation. Retrospectives identifying",
        "overlooked work improve future decomposition. Treat decomposition as a skill",
        "that improves with deliberate practice and reflection."
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "samples/technical_debt_management.txt",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "Technical Debt: Recognition, Measurement, and Strategic Management",
        "",
        "Technical debt represents the implied cost of future rework caused by choosing",
        "expedient solutions over better approaches that would take longer. Like financial",
        "debt, technical debt accumulates interest: the longer it remains, the more expensive",
        "remediation becomes as the codebase grows around the suboptimal implementation.",
        "",
        "Recognizing technical debt requires distinguishing between intentional and",
        "unintentional debt. Intentional debt results from conscious decisions to ship",
        "faster, accepting known limitations. Unintentional debt emerges from inexperience,",
        "changing requirements, or evolving understanding of the problem domain.",
        "",
        "Common manifestations of technical debt include duplicated code that should be",
        "abstracted, tightly coupled components that resist modification, missing tests",
        "that make changes risky, outdated dependencies with security vulnerabilities,",
        "and documentation that no longer reflects system behavior.",
        "",
        "The debt metaphor extends to interest payments. Code with high technical debt",
        "requires more time for each change: developers must work around limitations,",
        "debug obscure interactions, and manually verify behavior that tests should catch.",
        "This compound interest effect means early debt proves most expensive over time.",
        "",
        "Measuring technical debt combines quantitative and qualitative assessment.",
        "Static analysis tools measure code complexity, duplication, and dependency",
        "health. Developer surveys capture perceived friction and maintenance burden.",
        "Time tracking reveals which components consume disproportionate effort.",
        "",
        "Not all technical debt requires immediate payment. Strategic debt management",
        "prioritizes remediation based on business impact, change frequency, and risk.",
        "Debt in rarely-modified code costs little interest. Debt in core functionality",
        "touched by every feature deserves urgent attention.",
        "",
        "The debt quadrant model categorizes debt by recklessness and deliberateness.",
        "Deliberate, prudent debt accepts known shortcuts to meet deadlines. Inadvertent,",
        "prudent debt emerges as understanding deepens: \"now we know how we should have",
        "built it.\" Reckless debt, whether deliberate or inadvertent, reflects poor",
        "engineering practices that should be addressed through education and process.",
        "",
        "Paying down technical debt requires dedicated allocation. The boy scout rule",
        "suggests leaving code better than you found it, making incremental improvements",
        "alongside feature work. Dedicated refactoring sprints address larger structural",
        "issues that incremental improvement cannot fix.",
        "",
        "Documentation of technical debt ensures organizational memory. Track known",
        "limitations, their business impact, estimated remediation cost, and proposed",
        "solutions in a task management system. This visibility enables informed",
        "prioritization and prevents rediscovery of known issues.",
        "",
        "Prevention proves more cost-effective than remediation. Code review catches",
        "debt before it merges. Architecture review prevents systemic debt. Automated",
        "quality gates enforce minimum standards. Investment in developer education",
        "reduces inadvertent debt from inexperience.",
        "",
        "Refactoring represents the primary mechanism for debt payment. Safe refactoring",
        "requires comprehensive tests that verify behavior preservation. Without tests,",
        "refactoring becomes modification: changes that might alter behavior rather than",
        "merely restructuring code. Building test coverage often precedes refactoring.",
        "",
        "The strangler pattern enables gradual system replacement. Rather than big-bang",
        "rewrites that often fail, new functionality routes through improved implementations",
        "while legacy code continues serving existing paths. Over time, the legacy system",
        "withers as traffic migrates to the replacement.",
        "",
        "Communicating technical debt to stakeholders requires translation to business",
        "terms. Explain how debt slows feature delivery, increases defect rates, and",
        "raises operational risk. Quantify where possible: \"this refactoring would reduce",
        "our bug fix time from days to hours.\"",
        "",
        "Accepting debt strategically enables business agility. Sometimes shipping",
        "imperfect code that captures market opportunity justifies the future cost.",
        "The key is conscious decision-making with clear plans for eventual remediation,",
        "not accidental accumulation through negligence or ignorance."
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "samples/test_driven_development.txt",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "Test-Driven Development: Principles, Practice, and Patterns",
        "",
        "Test-driven development inverts the traditional coding sequence: write a failing",
        "test first, then implement code to make it pass, finally refactor while keeping",
        "tests green. This red-green-refactor cycle produces code that is testable by",
        "design and documented by executable specifications.",
        "",
        "The first step writes a test for functionality that does not yet exist. This",
        "test must fail, confirming it actually tests something meaningful. A test that",
        "passes immediately either tests existing behavior or tests nothing at all.",
        "The failing test defines the goal: make this assertion true.",
        "",
        "Writing the test first forces clarity about requirements. What exactly should",
        "this function return? What inputs does it accept? How should it handle errors?",
        "These questions, often deferred until implementation reveals ambiguity, must be",
        "answered before any production code exists.",
        "",
        "The second step implements the minimum code necessary to pass the test. Resist",
        "the temptation to implement more than the test requires. This discipline reveals",
        "missing tests: if a case matters, it deserves a test. Code without corresponding",
        "tests represents untested, potentially incorrect behavior.",
        "",
        "The third step refactors both test and implementation. With a passing test as",
        "a safety net, restructure code for clarity, eliminate duplication, and improve",
        "naming. The test suite verifies that refactoring preserves behavior. Refactor",
        "ruthlessly; the tests protect you.",
        "",
        "Test isolation ensures each test verifies one specific behavior. Isolated tests",
        "fail for one reason only, making failures easy to diagnose. Tests that verify",
        "multiple behaviors conflate distinct responsibilities and obscure the source of",
        "failures when multiple assertions fail together.",
        "",
        "Mock objects enable testing components in isolation. When testing a function that",
        "calls a database, mock the database to control its responses. This isolation",
        "speeds tests, eliminates environmental dependencies, and enables testing error",
        "handling by simulating failures.",
        "",
        "The testing pyramid suggests balance between test types. Unit tests form the",
        "base: fast, numerous, and isolated. Integration tests verify component interaction.",
        "End-to-end tests validate complete workflows. More tests at the base means faster",
        "feedback and easier debugging when tests fail.",
        "",
        "Arrange-Act-Assert structures tests clearly. Arrange sets up preconditions: create",
        "objects, configure mocks, establish state. Act invokes the behavior under test.",
        "Assert verifies the result. This structure makes tests readable and consistent.",
        "",
        "Descriptive test names document behavior. Rather than \"test_calculate\", write",
        "\"test_calculate_returns_zero_for_empty_input\". The name explains what the test",
        "verifies without reading the implementation. Test names serve as living",
        "documentation of system behavior.",
        "",
        "Edge cases deserve dedicated tests. Empty inputs, boundary values, null references,",
        "maximum sizes, and error conditions reveal implementation assumptions. A robust",
        "test suite exercises the boundaries where bugs commonly lurk.",
        "",
        "Test coverage measures which code paths tests execute. While 100% coverage does",
        "not guarantee correctness, low coverage indicates untested code that might contain",
        "bugs. Coverage reports guide test writing toward overlooked areas.",
        "",
        "Regression tests prevent bug recurrence. When fixing a bug, first write a test",
        "that reproduces it. The failing test proves the bug exists. After fixing, the",
        "passing test prevents the bug from returning. Bug fixes without tests risk",
        "regression.",
        "",
        "Property-based testing complements example-based tests. Rather than testing",
        "specific inputs, property-based tests generate random inputs and verify invariants",
        "hold. This approach discovers edge cases humans overlook while providing stronger",
        "guarantees about behavior.",
        "",
        "Test maintenance matters as much as test creation. Brittle tests that break with",
        "irrelevant changes waste time. Tests coupled to implementation details rather",
        "than behavior require constant updating. Invest in test quality to maintain",
        "velocity over time.",
        "",
        "The transformation priority premise guides implementation. Start with the simplest",
        "transformation: return a constant. Progress through increasingly powerful",
        "transformations: return input, add conditional, iterate, recurse. This discipline",
        "prevents over-engineering and ensures tests drive design.",
        "",
        "Behavior-driven development extends TDD with natural language specifications.",
        "Given-When-Then format structures tests as scenarios stakeholders understand.",
        "This shared language bridges communication between developers and domain experts",
        "while maintaining executable documentation."
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "tests/test_analysis.py",
      "function": "class TestClusteringQualityRegression(unittest.TestCase):",
      "start_line": 406,
      "lines_added": [
        "        With Louvain community detection (Task #123), this test should pass.",
        "        The Louvain algorithm optimizes modularity and produces well-balanced",
        "        clusters even on dense graphs.",
        "        Previously with label propagation:",
        "        Louvain avoids this by optimizing for modularity instead of propagating labels."
      ],
      "lines_removed": [
        "    @unittest.skip(\"KNOWN FAILURE: Label propagation creates mega-clusters on dense graphs. Enable after Task #123 (Louvain).\")",
        "        This test FAILS with current label propagation which puts 99%+ of",
        "        tokens into a single mega-cluster on larger corpora.",
        "        The issue:",
        "        This is a fundamental algorithmic limitation requiring Louvain (Task #123)."
      ],
      "context_before": [
        "        layer2 = self.processor.layers[CorticalLayer.CONCEPTS]",
        "",
        "        # 4 distinct topics should produce at least 2 clusters",
        "        # (relaxed from 4 because small corpora may have less separation)",
        "        self.assertGreaterEqual(",
        "            layer2.column_count(), 2,",
        "            f\"8 docs on 4 distinct topics should produce at least 2 clusters, \"",
        "            f\"got {layer2.column_count()}\"",
        "        )",
        ""
      ],
      "context_after": [
        "    def test_no_single_cluster_dominates(self):",
        "        \"\"\"Regression test: No single cluster should contain >50% of tokens.",
        "",
        "",
        "        - With 8 small docs (43 tokens): Largest cluster = 25% (OK)",
        "        - With 95 docs (6679 tokens): Largest cluster = 99.3% (BROKEN)",
        "",
        "        Label propagation converges to fewer clusters as graph density increases.",
        "        \"\"\"",
        "        layer0 = self.processor.layers[CorticalLayer.TOKENS]",
        "        layer2 = self.processor.layers[CorticalLayer.CONCEPTS]",
        "",
        "        if layer2.column_count() == 0:",
        "            self.fail(\"No concept clusters created at all\")",
        "",
        "        total_tokens = layer0.column_count()",
        "        max_cluster_size = max(",
        "            len(c.feedforward_connections)"
      ],
      "change_type": "modify"
    }
  ],
  "hour_of_day": 15,
  "day_of_week": "Thursday",
  "seconds_since_last_commit": -340021,
  "is_merge": true,
  "is_initial": false,
  "parent_count": 2,
  "session_id": null,
  "related_chats": [],
  "ci_result": null,
  "reverted": false,
  "amended": false
}