{
  "hash": "3a97db8ed52fd928e82e795d098cd3788ca860a0",
  "message": "refactor(#134): Add facade around protobuf, move imports to top",
  "author": "Claude",
  "timestamp": "2025-12-14 09:35:52 +0000",
  "branch": "claude/multi-index-design-DvifZ",
  "files_changed": [
    "cortical/persistence.py",
    "cortical/proto/__init__.py"
  ],
  "insertions": 103,
  "deletions": 42,
  "hunks": [
    {
      "file": "cortical/persistence.py",
      "function": "Supports:",
      "start_line": 11,
      "lines_added": [
        "from .proto import PROTOBUF_AVAILABLE, serialize_state, deserialize_state"
      ],
      "lines_removed": [],
      "context_before": [
        "\"\"\"",
        "",
        "import pickle",
        "import json",
        "import os",
        "import logging",
        "from typing import Dict, Optional, Any",
        "",
        "from .layers import CorticalLayer, HierarchicalLayer",
        "from .minicolumn import Minicolumn"
      ],
      "context_after": [
        "",
        "logger = logging.getLogger(__name__)",
        "",
        "",
        "def save_processor(",
        "    filepath: str,",
        "    layers: Dict[CorticalLayer, HierarchicalLayer],",
        "    documents: Dict[str, str],",
        "    document_metadata: Optional[Dict[str, Dict[str, Any]]] = None,",
        "    embeddings: Optional[Dict[str, list]] = None,"
      ],
      "change_type": "add"
    },
    {
      "file": "cortical/persistence.py",
      "function": "def save_processor(",
      "start_line": 68,
      "lines_added": [
        "        if not PROTOBUF_AVAILABLE:",
        "            )",
        "        text_output = serialize_state(",
        "            f.write(text_output)"
      ],
      "lines_removed": [
        "        try:",
        "            from .proto.serialization import to_proto",
        "            from google.protobuf import text_format",
        "        except ImportError as e:",
        "            ) from e",
        "        proto_state = to_proto(",
        "        # Use text format for human-readable, git-friendly output",
        "            f.write(text_format.MessageToString(proto_state))"
      ],
      "context_before": [
        "",
        "        # Serialize layers",
        "        for layer_enum, layer in layers.items():",
        "            state['layers'][layer_enum.value] = layer.to_dict()",
        "",
        "        with open(filepath, 'wb') as f:",
        "            pickle.dump(state, f, protocol=pickle.HIGHEST_PROTOCOL)",
        "",
        "    elif format == 'protobuf':",
        "        # Protocol Buffers serialization (text format for git-friendliness)"
      ],
      "context_after": [
        "            raise ImportError(",
        "                \"protobuf package is required for Protocol Buffers serialization. \"",
        "                \"Install it with: pip install protobuf\"",
        "",
        "            layers, documents, document_metadata,",
        "            embeddings, semantic_relations, metadata",
        "        )",
        "",
        "        with open(filepath, 'w', encoding='utf-8') as f:",
        "",
        "    if verbose:",
        "        total_cols = sum(len(layer.minicolumns) for layer in layers.values())",
        "        total_conns = sum(layer.total_connections() for layer in layers.values())",
        "        logger.info(f\"✓ Saved processor to {filepath} (format: {format})\")",
        "        logger.info(f\"  - {len(documents)} documents\")",
        "        logger.info(f\"  - {total_cols} minicolumns\")",
        "        logger.info(f\"  - {total_conns} connections\")",
        "        if embeddings:",
        "            logger.info(f\"  - {len(embeddings)} embeddings\")"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/persistence.py",
      "function": "def load_processor(",
      "start_line": 172,
      "lines_added": [
        "        if not PROTOBUF_AVAILABLE:",
        "            )",
        "        layers, documents, document_metadata, embeddings, semantic_relations, metadata = deserialize_state(proto_text)"
      ],
      "lines_removed": [
        "        try:",
        "            from .proto.serialization import from_proto",
        "            from .proto import schema_pb2",
        "            from google.protobuf import text_format",
        "        except ImportError as e:",
        "            ) from e",
        "        proto_state = schema_pb2.ProcessorState()",
        "        text_format.Parse(proto_text, proto_state)",
        "",
        "        layers, documents, document_metadata, embeddings, semantic_relations, metadata = from_proto(proto_state)"
      ],
      "context_before": [
        "            layers[CorticalLayer(level_int)] = layer",
        "",
        "        documents = state.get('documents', {})",
        "        document_metadata = state.get('document_metadata', {})",
        "        embeddings = state.get('embeddings', {})",
        "        semantic_relations = state.get('semantic_relations', [])",
        "        metadata = state.get('metadata', {})",
        "",
        "    elif format == 'protobuf':",
        "        # Protocol Buffers deserialization (text format)"
      ],
      "context_after": [
        "            raise ImportError(",
        "                \"protobuf package is required for Protocol Buffers deserialization. \"",
        "                \"Install it with: pip install protobuf\"",
        "",
        "        with open(filepath, 'r', encoding='utf-8') as f:",
        "            proto_text = f.read()",
        "",
        "",
        "    if verbose:",
        "        total_cols = sum(len(layer.minicolumns) for layer in layers.values())",
        "        total_conns = sum(layer.total_connections() for layer in layers.values())",
        "        logger.info(f\"✓ Loaded processor from {filepath} (format: {format})\")",
        "        logger.info(f\"  - {len(documents)} documents\")",
        "        logger.info(f\"  - {total_cols} minicolumns\")",
        "        logger.info(f\"  - {total_conns} connections\")",
        "        if embeddings:",
        "            logger.info(f\"  - {len(embeddings)} embeddings\")"
      ],
      "change_type": "modify"
    },
    {
      "file": "cortical/proto/__init__.py",
      "function": null,
      "start_line": 1,
      "lines_added": [
        "Protocol Buffers Serialization Facade",
        "Provides a clean API for Protocol Buffers text-format serialization.",
        "This module hides protobuf implementation details and provides simple",
        "functions for serializing/deserializing processor state to git-friendly",
        "text format.",
        "    from cortical.proto import serialize_state, deserialize_state, PROTOBUF_AVAILABLE",
        "    if PROTOBUF_AVAILABLE:",
        "        # Serialize state to text",
        "        text = serialize_state(layers, documents, metadata, ...)",
        "        # Deserialize text back to state",
        "        layers, docs, meta, emb, rels, info = deserialize_state(text)",
        "\"\"\"",
        "from typing import Dict, List, Optional, Any, Tuple",
        "# Check if protobuf is available",
        "PROTOBUF_AVAILABLE = False",
        "_text_format = None",
        "_to_proto = None",
        "_from_proto = None",
        "_schema_pb2 = None",
        "    from google.protobuf import text_format as _text_format",
        "    from .serialization import to_proto as _to_proto, from_proto as _from_proto",
        "    from . import schema_pb2 as _schema_pb2",
        "    PROTOBUF_AVAILABLE = True",
        "    pass",
        "",
        "",
        "def serialize_state(",
        "    layers: Dict,",
        "    documents: Dict[str, str],",
        "    document_metadata: Optional[Dict[str, Dict[str, Any]]] = None,",
        "    embeddings: Optional[Dict[str, list]] = None,",
        "    semantic_relations: Optional[list] = None,",
        "    metadata: Optional[Dict] = None",
        ") -> str:",
        "    \"\"\"",
        "    Serialize processor state to protobuf text format.",
        "",
        "    Args:",
        "        layers: Dictionary of CorticalLayer -> HierarchicalLayer",
        "        documents: Document collection {doc_id: content}",
        "        document_metadata: Per-document metadata",
        "        embeddings: Graph embeddings for terms",
        "        semantic_relations: Extracted semantic relations",
        "        metadata: Processor metadata (version, settings)",
        "",
        "    Returns:",
        "        Human-readable protobuf text format string",
        "",
        "    Raises:",
        "        ImportError: If protobuf package is not installed",
        "    \"\"\"",
        "    if not PROTOBUF_AVAILABLE:",
        "        raise ImportError(",
        "            \"protobuf package required for serialization. \"",
        "            \"Install with: pip install protobuf\"",
        "        )",
        "",
        "    proto_state = _to_proto(",
        "        layers, documents, document_metadata,",
        "        embeddings, semantic_relations, metadata",
        "    )",
        "    return _text_format.MessageToString(proto_state)",
        "",
        "",
        "def deserialize_state(text: str) -> Tuple:",
        "    \"\"\"",
        "    Deserialize protobuf text format to processor state.",
        "",
        "    Args:",
        "        text: Protobuf text format string",
        "",
        "    Returns:",
        "        Tuple of (layers, documents, document_metadata,",
        "                  embeddings, semantic_relations, metadata)",
        "",
        "    Raises:",
        "        ImportError: If protobuf package is not installed",
        "    \"\"\"",
        "    if not PROTOBUF_AVAILABLE:",
        "        raise ImportError(",
        "            \"protobuf package required for deserialization. \"",
        "            \"Install with: pip install protobuf\"",
        "        )",
        "",
        "    proto_state = _schema_pb2.ProcessorState()",
        "    _text_format.Parse(text, proto_state)",
        "    return _from_proto(proto_state)",
        "",
        "",
        "# Export public API",
        "__all__ = [",
        "    'PROTOBUF_AVAILABLE',",
        "    'serialize_state',",
        "    'deserialize_state',",
        "]"
      ],
      "lines_removed": [
        "Protocol Buffers Serialization Module",
        "Provides Protocol Buffers serialization for cross-language corpus sharing.",
        "This module enables the Cortical Text Processor to serialize and deserialize",
        "its state using Protocol Buffers TEXT FORMAT, allowing corpus data to be:",
        "- Human-readable and diffable in git",
        "- Shared across different programming languages",
        "- Reviewed in pull requests",
        "    from cortical.proto.serialization import to_proto, from_proto",
        "    from google.protobuf import text_format",
        "    # Convert processor state to protobuf",
        "    proto_state = to_proto(layers, documents, document_metadata,",
        "                           embeddings, semantic_relations, metadata)",
        "    # Serialize to text format (git-friendly)",
        "    text_output = text_format.MessageToString(proto_state)",
        "    # Deserialize from text format",
        "    proto_state = ProcessorState()",
        "    text_format.Parse(text_input, proto_state)",
        "    # Convert back to Python objects",
        "    state = from_proto(proto_state)",
        "\"\"\"",
        "    from .serialization import to_proto, from_proto",
        "    __all__ = ['to_proto', 'from_proto']",
        "    # Protobuf not installed - this is OK for core library functionality",
        "    __all__ = []"
      ],
      "context_before": [
        "\"\"\""
      ],
      "context_after": [
        "=====================================",
        "",
        "",
        "",
        "Usage:",
        "",
        "",
        "",
        "",
        "",
        "try:",
        "except ImportError:"
      ],
      "change_type": "modify"
    }
  ],
  "hour_of_day": 9,
  "day_of_week": "Sunday",
  "seconds_since_last_commit": -101336,
  "is_merge": false,
  "is_initial": false,
  "parent_count": 1,
  "session_id": null,
  "related_chats": [],
  "ci_result": null,
  "reverted": false,
  "amended": false
}