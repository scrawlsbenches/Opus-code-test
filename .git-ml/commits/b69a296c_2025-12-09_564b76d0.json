{
  "hash": "b69a296ca66564a4c1a07a7391d99562f4c78995",
  "message": "Add 7 new sample documents for diverse domain coverage",
  "author": "Claude",
  "timestamp": "2025-12-09 19:02:23 +0000",
  "branch": "claude/multi-index-design-DvifZ",
  "files_changed": [
    "samples/candlestick_patterns.txt",
    "samples/compilers.txt",
    "samples/computational_theory.txt",
    "samples/data_structures.txt",
    "samples/elliot_wave_theory.txt",
    "samples/financial_analysis.txt",
    "samples/neocortex.txt"
  ],
  "insertions": 79,
  "deletions": 0,
  "hunks": [
    {
      "file": "samples/candlestick_patterns.txt",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "Japanese Candlestick Pattern Analysis",
        "",
        "Candlestick charts originated in Japanese rice trading during the 18th century. Each candlestick displays four price points: open, high, low, and close. The body represents the range between open and close, while wicks or shadows extend to session extremes. Hollow or green bodies indicate closes above opens; filled or red bodies show closes below opens.",
        "",
        "Single candle patterns signal potential reversals. Doji candles with equal opens and closes indicate indecision. Hammer patterns at downtrend bottoms show rejection of lower prices with small bodies and long lower shadows. Shooting stars at uptrend tops mirror hammers inverted, signaling selling pressure. Marubozu candles with no shadows demonstrate strong conviction.",
        "",
        "Two-candle patterns provide stronger signals. Bullish engulfing occurs when a large green candle completely engulfs the prior red candle's body at support levels. Bearish engulfing reverses this at resistance. Piercing patterns show strong buying when a green candle opens below and closes above the prior red candle's midpoint. Dark cloud cover represents the bearish equivalent.",
        "",
        "Three-candle formations offer high-probability setups. Morning star bottoming patterns begin with a red candle, followed by a small-bodied star candle, then a strong green candle closing into the first candle's body. Evening stars invert this sequence at tops. Three white soldiers show consecutive green candles with higher closes signaling bullish momentum. Three black crows indicate the opposite bearish pressure.",
        "",
        "Context enhances pattern reliability. Volume confirmation validates breakouts and reversals. Support and resistance levels provide structural context for pattern interpretation. Trend direction determines whether patterns signal continuation or reversal. Multiple timeframe analysis confirms setups across different chart periods. Risk management through stop-loss placement beneath pattern lows protects against failed signals."
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "samples/compilers.txt",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "Compiler Design and Implementation",
        "",
        "Compilers transform source code into executable machine instructions through multiple processing phases. Lexical analysis tokenizes input streams, recognizing keywords, identifiers, literals, and operators using finite automata derived from regular expressions. Scanner generators like lex automate lexer construction from token specifications.",
        "",
        "Parsing constructs abstract syntax trees from token sequences according to context-free grammars. Top-down recursive descent parsers implement grammar productions as mutually recursive procedures. Bottom-up parsers like LR and LALR use shift-reduce actions guided by parsing tables. Parser generators including yacc and ANTLR produce parsers from grammar specifications.",
        "",
        "Semantic analysis decorates syntax trees with type information and enforces language rules. Symbol tables track identifier declarations and scopes. Type checking verifies operator-operand compatibility and enforces type system constraints. Type inference algorithms deduce types without explicit annotations through unification and constraint solving.",
        "",
        "Intermediate representations decouple front-end language processing from back-end code generation. Three-address code linearizes expressions into simple operations. Static single assignment form simplifies dataflow analysis by ensuring each variable receives exactly one definition. Control flow graphs represent program structure for optimization.",
        "",
        "Optimization transforms improve code efficiency while preserving semantics. Local optimizations like constant folding and strength reduction operate within basic blocks. Global optimizations including common subexpression elimination and dead code elimination require dataflow analysis. Loop optimizations—invariant code motion, induction variable elimination, and unrolling—target performance-critical regions. Register allocation maps virtual registers to limited physical registers using graph coloring algorithms.",
        "",
        "Code generation translates intermediate representations to target architecture instructions. Instruction selection matches IR patterns to machine operations. Instruction scheduling reorders operations to exploit pipeline parallelism and avoid hazards."
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "samples/computational_theory.txt",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "Computational Theory and Complexity",
        "",
        "Computational theory establishes fundamental limits of computing through mathematical formalism. Turing machines provide the foundational model—an infinite tape, read-write head, and finite state control. Church-Turing thesis posits that any effectively calculable function is Turing computable. Universal Turing machines simulate arbitrary Turing machines, establishing programmable computation.",
        "",
        "Decidability classifies problems by algorithmic solvability. The halting problem proves undecidable—no algorithm determines whether arbitrary programs terminate. Rice's theorem extends undecidability to all non-trivial semantic properties of programs. Reductions demonstrate undecidability by transforming known undecidable problems into new ones.",
        "",
        "Complexity theory measures computational resource requirements. Time complexity counts primitive operations as functions of input size. Space complexity measures memory usage. Asymptotic analysis using big-O notation captures growth rates, abstracting constant factors and lower-order terms.",
        "",
        "Complexity classes categorize problems by resource bounds. P contains problems solvable in polynomial time. NP encompasses problems with polynomial-time verifiable solutions. The P versus NP question—whether these classes differ—remains open. NP-complete problems, including SAT, traveling salesman, and graph coloring, are hardest within NP; solving any in polynomial time would prove P equals NP.",
        "",
        "Beyond NP, PSPACE includes problems solvable with polynomial space. EXPTIME requires exponential time. The polynomial hierarchy stratifies problems between P and PSPACE. Randomized complexity classes like BPP capture probabilistic algorithms. Circuit complexity measures non-uniform computation. Quantum complexity explores computational power of quantum mechanical systems, with BQP capturing efficient quantum computation."
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "samples/data_structures.txt",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "Fundamental Data Structures in Computer Science",
        "",
        "Data structures organize and store information for efficient access and modification. Arrays provide contiguous memory allocation enabling O(1) random access through index arithmetic. Dynamic arrays resize automatically, amortizing insertion costs across operations. Linked lists trade random access for O(1) insertions and deletions through pointer manipulation.",
        "",
        "Trees introduce hierarchical organization. Binary search trees maintain sorted order with O(log n) average operations. Self-balancing variants like AVL trees and red-black trees guarantee logarithmic worst-case complexity through rotation operations. B-trees optimize for disk access patterns, supporting databases with high branching factors minimizing tree height.",
        "",
        "Hash tables achieve O(1) average-case lookup, insertion, and deletion through hash functions mapping keys to array indices. Collision resolution strategies include chaining with linked lists and open addressing with linear or quadratic probing. Load factor monitoring triggers resizing to maintain performance guarantees.",
        "",
        "Heaps implement priority queues with O(log n) insertion and extraction. Binary heaps use array representation with parent-child relationships defined by index arithmetic. Fibonacci heaps achieve O(1) amortized insertion supporting efficient decrease-key operations for graph algorithms.",
        "",
        "Graphs represent relationships between entities through vertices and edges. Adjacency matrices enable O(1) edge queries at O(V^2) space cost. Adjacency lists reduce space complexity to O(V+E) for sparse graphs. Graph traversals include breadth-first search using queues and depth-first search using recursion or explicit stacks. Specialized structures like tries optimize string operations, while bloom filters provide probabilistic set membership testing with space efficiency."
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "samples/elliot_wave_theory.txt",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "Elliott Wave Theory in Market Analysis",
        "",
        "Elliott Wave Theory posits that financial markets move in predictable wave patterns reflecting collective investor psychology. Ralph Nelson Elliott discovered that market prices unfold in specific structures—five waves in the direction of the main trend followed by three corrective waves. This 5-3 pattern repeats at multiple time scales creating fractal market structure.",
        "",
        "Impulse waves contain five sub-waves labeled 1-2-3-4-5. Wave 1 initiates the trend, wave 2 corrects but never retraces beyond wave 1's origin. Wave 3 typically extends furthest and cannot be the shortest impulse wave. Wave 4 corrects wave 3 without overlapping wave 1 territory. Wave 5 completes the impulse sequence with diminishing momentum.",
        "",
        "Corrective waves follow impulse completions in A-B-C patterns. Zigzags form sharp corrections with wave C exceeding wave A. Flats show sideways consolidation where waves A, B, and C terminate near similar price levels. Triangles contract through five waves before breakout continuation. Complex corrections combine multiple simple patterns through connecting X waves.",
        "",
        "Fibonacci relationships pervade Elliott patterns. Retracements commonly reach 38.2%, 50%, or 61.8% of prior waves. Extensions project wave lengths using 1.618 and 2.618 multipliers. Wave equality and alternation guidelines help forecast likely wave termination zones. Time relationships between waves also exhibit Fibonacci proportions.",
        "",
        "Wave counting requires identifying the current position within the fractal hierarchy. Higher degree waves contain lower degree sub-waves—supercycles decompose to cycles, cycles to primary waves, continuing through intermediate, minor, minute, and minuette degrees. Proper wave labeling enables forecasting subsequent movements and identifying high-probability trade setups."
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "samples/financial_analysis.txt",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "Financial Analysis Fundamentals",
        "",
        "Financial analysis evaluates business performance through examination of financial statements, ratios, and market data. Fundamental analysis assesses intrinsic value by analyzing revenue growth, profit margins, debt levels, and cash flow generation. Analysts construct discounted cash flow models projecting future earnings and discounting them to present value using weighted average cost of capital.",
        "",
        "Ratio analysis reveals operational efficiency and financial health. Liquidity ratios like current ratio and quick ratio measure short-term solvency. Profitability ratios including return on equity, return on assets, and net profit margin indicate management effectiveness. Leverage ratios such as debt-to-equity and interest coverage assess capital structure risk.",
        "",
        "Income statement analysis tracks revenue recognition, cost of goods sold, operating expenses, and earnings per share trends. Balance sheet examination evaluates asset quality, working capital management, and liability structure. Cash flow statement analysis distinguishes operating, investing, and financing activities, revealing actual cash generation versus accrual accounting earnings.",
        "",
        "Comparative analysis benchmarks performance against industry peers and historical trends. Common-size statements normalize figures as percentages for cross-company comparison. Trend analysis identifies growth patterns and cyclical behavior. DuPont analysis decomposes return on equity into profit margin, asset turnover, and financial leverage components.",
        "",
        "Valuation metrics guide investment decisions. Price-to-earnings ratios compare market price to profitability. Enterprise value to EBITDA accounts for capital structure differences. Price-to-book ratios assess market premium over accounting value. Dividend yield and payout ratios evaluate income characteristics. Terminal value calculations in DCF models significantly impact fair value estimates."
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    },
    {
      "file": "samples/neocortex.txt",
      "function": null,
      "start_line": 0,
      "lines_added": [
        "The Neocortex: Architecture of Intelligence",
        "",
        "The neocortex comprises approximately 80% of human brain volume and underlies higher cognitive functions including perception, reasoning, language, and planning. This thin layered sheet, just 2-4 millimeters thick, folds extensively to maximize surface area within the skull. Humans possess roughly 16 billion neocortical neurons organized into stereotyped circuits repeated across the entire structure.",
        "",
        "Six distinct layers characterize neocortical organization. Layer 4 receives thalamic input carrying sensory information. Layers 2 and 3 process and integrate information through extensive lateral connections. Layer 5 pyramidal neurons project to subcortical structures and motor systems. Layer 6 provides feedback to the thalamus, modulating incoming signals. This layered architecture suggests a canonical microcircuit implementing universal cortical algorithms.",
        "",
        "Cortical columns span all layers vertically, functioning as fundamental processing units. Minicolumns containing roughly 100 neurons form the finest columnar structure. Macrocolumns aggregate hundreds of minicolumns sharing similar response properties. This columnar organization appears throughout sensory, motor, and association cortex, suggesting common computational principles across diverse functions.",
        "",
        "Hierarchical processing flows through cortical regions. Primary sensory areas extract basic features—edges in visual cortex, frequencies in auditory cortex. Successive regions construct increasingly abstract representations. Temporal cortex recognizes objects and faces. Prefrontal cortex maintains working memory and executes cognitive control. Feedforward connections carry information up the hierarchy while feedback connections provide contextual modulation.",
        "",
        "Synaptic plasticity enables learning through experience-dependent modification of connection strengths. Long-term potentiation strengthens frequently co-activated synapses. Spike-timing-dependent plasticity refines temporal relationships. Sleep consolidates memories by replaying neural patterns. Understanding neocortical computation could illuminate principles for artificial intelligence systems exhibiting human-level cognition."
      ],
      "lines_removed": [],
      "context_before": [],
      "context_after": [],
      "change_type": "add"
    }
  ],
  "hour_of_day": 19,
  "day_of_week": "Tuesday",
  "seconds_since_last_commit": -499345,
  "is_merge": false,
  "is_initial": false,
  "parent_count": 1,
  "session_id": null,
  "related_chats": [],
  "ci_result": null,
  "reverted": false,
  "amended": false
}