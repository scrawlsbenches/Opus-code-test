# Task List: Cortical Text Processor

Active backlog for the Cortical Text Processor project. Completed tasks are archived in [TASK_ARCHIVE.md](TASK_ARCHIVE.md).

**Last Updated:** 2025-12-12
**Pending Tasks:** 35
**Completed Tasks:** 133+ (see archive and Recently Completed)

---

## Active Backlog

<!-- Machine-parseable format for automation -->

### ðŸ”´ Critical (Do Now)

*All critical tasks completed!*

### ðŸŸ  High (Do This Week)

*All high priority tasks completed!*

### ðŸŸ¡ Medium (Do This Month)

| # | Task | Category | Depends | Effort |
|---|------|----------|---------|--------|
| 138 | Use sparse matrix multiplication for bigram connections | Perf | - | Medium |
| 133 | Implement WAL + snapshot persistence (fault-tolerant rebuild) | Arch | 132 | Large |
| 134 | Implement protobuf serialization for corpus | Arch | 132 | Medium |
| 135 | Implement chunked parallel processing for full-analysis | Arch | 132 | Large |
| 95 | Split processor.py into modules | Arch | 97 | Large |
| 98 | Replace print() with logging | CodeQual | - | Medium |
| 99 | Add input validation to public methods | CodeQual | - | Medium |
| 102 | Add tests for edge cases | Testing | - | Medium |
| 107 | Add Quick Context to tasks | TaskMgmt | - | Medium |
| 115 | Create component interaction diagram | AINav | - | Medium |

### ðŸŸ¢ Low (Backlog)

| # | Task | Category | Depends | Effort |
|---|------|----------|---------|--------|
| 73 | Add "Find Similar Code" command | DevEx | - | Medium |
| 74 | Add "Explain This Code" command | DevEx | - | Medium |
| 75 | Add "What Changed?" semantic diff | DevEx | - | Large |
| 76 | Add "Suggest Related Files" feature | DevEx | - | Medium |
| 78 | Add code pattern detection | DevEx | - | Large |
| 79 | Add corpus health dashboard | DevEx | - | Medium |
| 80 | Add "Learning Mode" for contributors | DevEx | - | Large |
| 100 | Implement plugin/extension registry | Arch | - | Large |
| 101 | Automate staleness tracking | Arch | - | Medium |
| 106 | Add task dependency graph | TaskMgmt | - | Small |
| 108 | Create task selection script | TaskMgmt | - | Medium |
| 117 | Create debugging cookbook | AINav | - | Medium |
| 118 | Add function complexity annotations | AINav | - | Small |
| 140 | Analyze customer service cluster quality | Research | 127 | Small |
| 129 | Test customer service retrieval quality | Testing | - | Small |
| 130 | Expand customer service sample cluster | Samples | - | Medium |
| 131 | Investigate cross-domain semantic bridges | Research | - | Medium |

### â¸ï¸ Deferred

| # | Task | Reason |
|---|------|--------|
| 110 | Add section markers to large files | Superseded by #119 (AI metadata generator) |
| 111 | Add "See Also" cross-references | Superseded by #119 (AI metadata generator) |
| 112 | Add docstring examples | Superseded by #119 (AI metadata generator) |
| 7 | Document magic numbers in gaps.py | Low priority, functional as-is |
| 42 | Add simple query language | Nice-to-have, not blocking |
| 44 | Remove deprecated feedforward_sources | Cleanup, low impact |
| 46 | Standardize return types with dataclasses | Nice-to-have |

### ðŸ”„ In Progress

| # | Task | Started | Notes |
|---|------|---------|-------|
| 87 | Add Python code samples and showcase | 2025-12-11 | samples/*.py created |

---

## Recently Completed (Last 7 Days)

| # | Task | Completed | Notes |
|---|------|-----------|-------|
| 139 | Batch bigram connection updates to reduce dict overhead | 2025-12-12 | add_lateral_connections_batch() method in minicolumn.py |
| 137 | Cap bigram connections to top-K per bigram | 2025-12-12 | max_connections_per_bigram parameter (default 50) in analysis.py |
| 116 | Document return value semantics | 2025-12-12 | Edge cases, score ranges, None vs exceptions, default parameters |
| 114 | Add type aliases for complex types | 2025-12-12 | cortical/types.py with 20+ aliases: DocumentScore, PassageResult, SemanticRelation, etc. |
| 113 | Document staleness tracking system | 2025-12-12 | Comprehensive docs in CLAUDE.md: computation types, API, incremental updates |
| 96 | Centralize duplicate constants | 2025-12-12 | cortical/constants.py with RELATION_WEIGHTS, DOC_TYPE_BOOSTS, query keywords |
| 91 | Create docs/README.md index | 2025-12-12 | Navigation by audience, reading paths, categorized docs |
| 92 | Add badges to README.md | 2025-12-12 | Python, License, Tests, Coverage, Zero Dependencies badges |
| 93 | Update README with docs references | 2025-12-12 | Documentation section with table linking to docs/*.md |
| 146 | Create behavioral tests for core user workflows | 2025-12-12 | 11 tests across 4 categories: Search, Performance, Quality, Robustness |
| 145 | Improve graph embedding quality for common terms | 2025-12-12 | Added 'tfidf' method, IDF weighting to 'fast' method |
| 143 | Investigate negative silhouette score in clustering | 2025-12-12 | Expected behavior: modularity â‰  silhouette (graph vs doc similarity) |
| 142 | Investigate 74s compute_all() performance regression | 2025-12-12 | 5.2x speedup via fast embeddings + sampling (74s â†’ 14s) |
| 144 | Boost exact document name matches in search | 2025-12-12 | doc_name_boost parameter in search functions |
| 141 | Filter Python keywords/artifacts from analysis | 2025-12-12 | CODE_NOISE_TOKENS + filter_code_noise tokenizer option |
| 94 | Split query.py into focused modules | 2025-12-12 | 8 modules: expansion, search, passages, chunking, intent, definitions, ranking, analogy |
| 97 | Integrate CorticalConfig into processor | 2025-12-11 | Config stored on processor, used in method defaults, saved/loaded |
| 127 | Create cluster coverage evaluation script | 2025-12-11 | scripts/evaluate_cluster.py with 24 tests |
| 125 | Add clustering quality metrics (modularity, silhouette) | 2025-12-11 | compute_clustering_quality() in analysis.py, showcase display |
| 124 | Add minimum cluster count regression tests | 2025-12-11 | 4 new tests: coherence, showcase count, mega-cluster, distribution |
| 128 | Fix definition boost that favors test mocks over real implementations | 2025-12-11 | Added is_test_file() and test file penalty |
| 132 | Profile full-analysis bottleneck (bigram, semantics O(nÂ²)) | 2025-12-11 | Created profile_full_analysis.py, fixed bottlenecks |
| 136 | Optimize semantics O(nÂ²) similarity with early termination | 2025-12-11 | Added max_similarity_pairs, min_context_keys |
| 126 | Investigate optimal Louvain resolution for sample corpus | 2025-12-11 | Research confirms default 1.0 is optimal |
| 123 | Replace label propagation with Louvain community detection | 2025-12-11 | Implemented Louvain algorithm, 34 clusters for 92 docs |
| 122 | Investigate Concept Layer & Embeddings regressions | 2025-12-11 | Fixed inverted strictness, improved embeddings |
| 119 | Create AI metadata generator script | 2025-12-11 | scripts/generate_ai_metadata.py with tests |
| 120 | Add AI metadata loader to Claude skills | 2025-12-11 | ai-metadata skill created |
| 121 | Auto-regenerate AI metadata on changes | 2025-12-11 | Documented in CLAUDE.md, skills |
| 88 | Create package installation files | 2025-12-11 | pyproject.toml, requirements.txt |
| 89 | Create CONTRIBUTING.md | 2025-12-11 | Contribution guide |
| 90 | Create docs/quickstart.md | 2025-12-11 | 5-minute tutorial |
| 103 | Add Priority Backlog Summary | 2025-12-11 | TASK_LIST.md restructure |
| 104 | Create TASK_ARCHIVE.md | 2025-12-11 | 75+ tasks archived |
| 105 | Standardize task format | 2025-12-11 | Meta tags, effort estimates |
| 109 | Add Recently Completed section | 2025-12-11 | Session context |
| 86 | Add semantic chunk boundaries for code | 2025-12-11 | In query.py |
| 85 | Improve test vs source ranking | 2025-12-11 | DOC_TYPE_BOOSTS |

*Full details in [TASK_ARCHIVE.md](TASK_ARCHIVE.md)*

---

## Pending Task Details

### 123. Replace Label Propagation with Louvain Community Detection âœ…

**Meta:** `status:completed` `priority:critical` `category:bugfix`
**Files:** `cortical/analysis.py`, `cortical/processor.py`, `tests/test_analysis.py`
**Effort:** Large
**Completed:** 2025-12-11

**Problem:** Label propagation clustering fails catastrophically on densely connected graphs:
- 95 documents produce only 3 concept clusters
- One mega-cluster contains 99.8% of tokens (6,667 of 6,679)
- The algorithm converges to minimal clusters regardless of strictness parameters
- This renders the concept layer (Layer 2) essentially useless

**Root Cause Analysis:**
Label propagation works by having each node adopt the most common label among neighbors. On a densely connected graph (avg 18.2 connections per token), information propagates everywhere, causing nearly all nodes to converge to a single label.

This is NOT a parameter tuning problem - it's a fundamental algorithmic limitation. The `cluster_strictness` parameter only delays convergence, it cannot prevent it.

**Solution Applied:**
1. Implemented `cluster_by_louvain()` in `cortical/analysis.py` (300+ lines)
   - Phase 1: Local modularity optimization with cached sigma_tot for O(1) lookups
   - Phase 2: Network aggregation into super-nodes
   - Resolution parameter for controlling cluster granularity
2. Added `clustering_method` parameter to `processor.build_concept_clusters()`
   - Default: 'louvain' (recommended)
   - Alternative: 'label_propagation' (backward compatibility)
3. Enabled previously skipped regression test `test_no_single_cluster_dominates`

**Results:**
- 34 concept clusters for 92-document corpus (6518 tokens)
- Largest cluster: 10.2% of tokens (well under 50% threshold)
- All 823 tests pass
- compute_all() takes ~12s for full corpus

**Acceptance Criteria:**
- [x] Louvain algorithm implemented without external dependencies
- [x] 34 clusters for 92-document showcase corpus (exceeds 10+)
- [x] All 823 existing tests pass
- [x] Regression test `test_no_single_cluster_dominates` enabled and passing
- [x] showcase.py demonstrates improved clustering

---

### 124. Add Minimum Cluster Count Regression Tests âœ…

**Meta:** `status:completed` `priority:critical` `category:testing`
**Files:** `tests/test_analysis.py`
**Effort:** Medium
**Completed:** 2025-12-11

**Problem:** We had NO tests that would catch clustering failures:
- Tests only checked that clustering returns valid dictionaries
- No baseline for expected cluster counts
- No quality thresholds for diverse corpora
- The regression went undetected until manual inspection

**Solution Applied:**
Added comprehensive regression tests in two test classes:

1. **TestClusteringQualityRegression** (existing, extended):
   - `test_cluster_semantic_coherence` - verifies tokens in same cluster have lateral connections

2. **TestShowcaseCorpusRegression** (new):
   - `test_showcase_produces_expected_cluster_count` - 100+ docs â†’ 15+ clusters
   - `test_showcase_no_mega_cluster` - no cluster >20% of tokens
   - `test_showcase_cluster_distribution` - at least 5 substantial clusters, varied sizes

**Test Results:**
- 994 total tests (up from 990)
- All showcase tests pass with 37 clusters, max 14.8% ratio
- Semantic coherence >50% of clusters have internal connections

**Acceptance Criteria:**
- [x] 4+ new regression tests for clustering quality
- [x] Tests pass after Louvain implementation (Task #123)

---

### 125. Add Clustering Quality Metrics (Modularity, Silhouette) âœ…

**Meta:** `status:completed` `priority:critical` `category:devex` `depends:123`
**Files:** `cortical/analysis.py`, `cortical/processor.py`, `showcase.py`, `tests/test_analysis.py`
**Effort:** Medium
**Completed:** 2025-12-11

**Problem:** We have no way to measure if clustering is good or bad:
- No modularity score to measure community quality
- No silhouette score to measure cluster separation
- No metrics in showcase output
- No way to compare algorithm performance

**Solution Applied:**

Added `compute_clustering_quality()` to `cortical/analysis.py` with:

1. **Modularity Score** (-1 to 1):
   - Q > 0.3: Good community structure
   - Q > 0.5: Strong community structure
   - Implementation uses standard modularity formula

2. **Silhouette Score** (-1 to 1):
   - Uses graph-based distance (1 - connection similarity)
   - Samples tokens for O(nÂ²) tractability
   - s > 0.25: Reasonable structure

3. **Balance Metric** (Gini coefficient):
   - 0 = perfectly balanced
   - 1 = all in one cluster

4. **Quality Assessment**: Human-readable interpretation string

**Example Output:**
```
Layer 2: Concept Layer (V4)
       37 minicolumns, 686 connections
       Quality: modularity=0.40, silhouette=0.15, balance=0.50
```

**Test Results:**
- 1001 tests pass (7 new tests for quality metrics)
- Showcase displays metrics in hierarchical structure section

**Acceptance Criteria:**
- [x] Modularity score implemented
- [x] Silhouette score implemented
- [x] Balance metric implemented
- [x] Metrics displayed in showcase.py
- [x] Quality thresholds documented

---

### 126. Investigate Optimal Louvain Resolution for Sample Corpus âœ…

**Meta:** `status:completed` `priority:high` `category:research`
**Files:** `scripts/analyze_louvain_resolution.py`, `docs/louvain_resolution_analysis.md`
**Effort:** Medium
**Depends:** 123
**Completed:** 2025-12-11

**Problem:** The Louvain algorithm's `resolution` parameter significantly affects cluster count:
- Resolution 0.5 â†’ 38 clusters (coarse, 64% mega-cluster)
- Resolution 1.0 â†’ 32 clusters (default, good balance)
- Resolution 2.0 â†’ 79 clusters (fine)
- Resolution 3.0 â†’ 125 clusters (very fine)

**Research Findings:**

1. **Tested 11 resolution values** (0.5 to 3.0) on 103-document corpus (7,102 tokens)

2. **Key metric results at resolution 1.0 (default):**
   - Modularity: 0.4036 (good, exceeds 0.3 threshold)
   - Max cluster: 9.5% of tokens (no mega-clusters)
   - Balance (Gini): 0.386 (reasonable distribution)

3. **All resolutions maintain modularity > 0.3** (good community structure)

4. **Low resolution (0.5) creates mega-clusters** (64% in one cluster) despite highest modularity (0.78)

5. **Default 1.0 is the inflection point** where max cluster drops below 10%

**Recommendation:** Keep default at 1.0, which provides:
- Good modularity (0.40)
- No mega-clusters (<10%)
- Semantically coherent groupings
- Standard Louvain interpretation

**Deliverables:**
- [x] Analysis script: `scripts/analyze_louvain_resolution.py`
- [x] Research report: `docs/louvain_resolution_analysis.md`
- [x] Recommendation: Keep default at 1.0 (already well-chosen)
- [x] Use case guidelines documented for resolution tuning

---

### 7. Document Magic Numbers

**Meta:** `status:deferred` `priority:low` `category:docs`
**Files:** `cortical/gaps.py:62,76,99`
**Effort:** Small

**Problem:** Magic numbers in gap detection lack documentation.

**Note:** Deferred - functional as-is, low priority.

---

### 42. Add Simple Query Language Support

**Meta:** `status:deferred` `priority:low` `category:feature`
**Files:** `cortical/query.py`
**Effort:** Large

**Problem:** Users must construct queries programmatically.

**Solution:** Add simple query syntax like `"neural AND networks NOT deep"`.

---

### 44. Remove Deprecated feedforward_sources

**Meta:** `status:deferred` `priority:low` `category:cleanup`
**Files:** `cortical/minicolumn.py:117`, `analysis.py:457`, `query.py:105`
**Effort:** Small

**Problem:** Deprecated field still exists in codebase.

---

### 46. Standardize Return Types with Dataclasses

**Meta:** `status:deferred` `priority:low` `category:code-quality`
**Files:** `cortical/query.py`
**Effort:** Medium

**Problem:** Query functions return tuples instead of typed dataclasses.

---

### 73. Add "Find Similar Code" Command

**Meta:** `status:pending` `priority:low` `category:devex`
**Files:** `scripts/similar_code.py` (new)
**Effort:** Medium

**Problem:** No easy way to find similar code blocks.

**Solution:** Create script using semantic fingerprinting.

---

### 74. Add "Explain This Code" Command

**Meta:** `status:pending` `priority:low` `category:devex`
**Files:** `scripts/explain_code.py` (new)
**Effort:** Medium

**Problem:** New contributors need help understanding code.

**Solution:** Script that retrieves relevant docs/comments for a code block.

---

### 75. Add "What Changed?" Semantic Diff

**Meta:** `status:pending` `priority:low` `category:devex`
**Files:** `scripts/what_changed.py` (new)
**Effort:** Large

**Problem:** Hard to understand semantic impact of changes.

**Solution:** Show how fingerprints changed between commits.

---

### 76. Add "Suggest Related Files" Feature

**Meta:** `status:pending` `priority:low` `category:devex`
**Files:** `scripts/related_files.py` (new)
**Effort:** Medium

**Problem:** When editing a file, don't know what else might need changes.

---

### 78. Add Code Pattern Detection

**Meta:** `status:pending` `priority:low` `category:devex`
**Files:** `cortical/patterns.py` (new), `scripts/find_patterns.py` (new)
**Effort:** Large

**Problem:** Hard to find common patterns (factory, singleton, etc.) in codebase.

---

### 79. Add Corpus Health Dashboard

**Meta:** `status:pending` `priority:low` `category:devex`
**Files:** `scripts/corpus_health.py` (new)
**Effort:** Medium

**Problem:** No quick overview of corpus health metrics.

---

### 80. Add "Learning Mode" for New Contributors

**Meta:** `status:pending` `priority:low` `category:devex`
**Files:** `scripts/learn_codebase.py` (new)
**Effort:** Large

**Problem:** Steep learning curve for new contributors.

**Solution:** Guided tour of codebase using semantic search.

---

### 87. Add Python Code Samples and Update Showcase

**Meta:** `status:in-progress` `priority:medium` `category:showcase`
**Files:** `showcase.py`, `samples/data_processor.py`, `samples/search_engine.py`, `samples/test_data_processor.py`
**Effort:** Medium

**Started:** 2025-12-11

**Progress:** Sample files created, showcase updates in progress.

---

### 88. Create Package Installation Files âœ“

**Meta:** `status:completed` `priority:critical` `category:devex`
**Files:** `pyproject.toml`, `requirements.txt`, `cortical/py.typed`
**Completed:** 2025-12-11

**Solution Applied:**
- Created `pyproject.toml` with modern Python packaging
- Created `requirements.txt` documenting dev dependencies
- Added `py.typed` marker for PEP 561 compliance
- Tested: `pip install -e .` and `pip install -e ".[dev]"` both work

---

### 89. Create CONTRIBUTING.md âœ“

**Meta:** `status:completed` `priority:high` `category:devex`
**Files:** `CONTRIBUTING.md`
**Completed:** 2025-12-11

**Solution Applied:**
- Created comprehensive CONTRIBUTING.md with:
  - Quick start (8 steps from fork to PR)
  - Development setup instructions
  - Test running commands
  - Code style guidelines with examples
  - Project structure overview
  - Links to code-of-ethics.md and definition-of-done.md

---

### 90. Create docs/quickstart.md Tutorial âœ“

**Meta:** `status:completed` `priority:high` `category:docs`
**Files:** `docs/quickstart.md`
**Completed:** 2025-12-11

**Solution Applied:**
- Created 5-minute quickstart tutorial with:
  - Installation instructions
  - "Your First 10 Lines" minimal example
  - Understanding results section
  - Query expansion demo
  - Passage retrieval for RAG
  - Key concepts table
  - Save/load instructions
  - Common patterns (batch, incremental, metadata)
  - Troubleshooting section

---

### 91. Create docs/README.md Index âœ…

**Meta:** `status:completed` `priority:medium` `category:docs`
**Files:** `docs/README.md`
**Effort:** Small
**Completed:** 2025-12-12

**Problem:** 11 docs files with no navigation.

**Solution Applied:** Created `docs/README.md` with navigation by audience (New Users, Developers, AI Agents), reading paths, and categorized documentation links.

---

### 92. Add Badges to README.md âœ…

**Meta:** `status:completed` `priority:medium` `category:devex`
**Files:** `README.md`
**Effort:** Small
**Completed:** 2025-12-12

**Problem:** No visual project health indicators.

**Solution Applied:** Added 5 badges to README.md: Python 3.8+, MIT License, Tests (1121 passing), Coverage (>89%), Zero Dependencies.

---

### 93. Update README with Documentation References âœ…

**Meta:** `status:completed` `priority:medium` `category:docs`
**Files:** `README.md`
**Effort:** Small
**Depends:** 91
**Completed:** 2025-12-12

**Problem:** README doesn't mention docs/ folder.

**Solution Applied:** Added "Documentation" section to README.md with table linking to all docs/*.md files including quickstart, architecture, algorithms, query-guide, cookbook, and glossary.

---

### 94. Split query.py into Focused Modules âœ…

**Meta:** `status:completed` `priority:high` `category:architecture`
**Files:** `cortical/query.py` â†’ `cortical/query/` package (8 modules)
**Effort:** Large
**Completed:** 2025-12-12

**Problem:** Single 2,719-line file violated Single Responsibility Principle.

**Solution Applied:**
```
cortical/query/
â”œâ”€â”€ __init__.py       # Re-export public API (backward compatible)
â”œâ”€â”€ expansion.py      # expand_query, expand_query_semantic
â”œâ”€â”€ search.py         # find_documents_for_query, fast_find_documents
â”œâ”€â”€ passages.py       # find_passages_for_query
â”œâ”€â”€ chunking.py       # Chunk-based text processing
â”œâ”€â”€ intent.py         # parse_intent_query, search_by_intent
â”œâ”€â”€ definitions.py    # is_definition_query, find_definition_passages
â”œâ”€â”€ ranking.py        # multi_stage_rank
â””â”€â”€ analogy.py        # complete_analogy
```

**Acceptance Criteria:**
- [x] No file >500 lines
- [x] All existing tests pass
- [x] Backward-compatible imports from `cortical.query`

---

### 95. Split processor.py into Focused Modules

**Meta:** `status:pending` `priority:medium` `category:architecture`
**Files:** `cortical/processor.py` (2,301 lines)
**Effort:** Large
**Depends:** 97

**Problem:** God Object with 100+ methods.

**Solution:** Extract DocumentManager, ComputationManager as internal classes.

---

### 96. Centralize Duplicate Constants âœ…

**Meta:** `status:completed` `priority:medium` `category:code-quality`
**Files:** `cortical/constants.py`
**Effort:** Small
**Completed:** 2025-12-12

**Problem:** `RELATION_WEIGHTS`, `DOC_TYPE_BOOSTS` defined in multiple places.

**Solution Applied:** Created `cortical/constants.py` as single source of truth containing RELATION_WEIGHTS, DOC_TYPE_BOOSTS, query keywords, and other shared constants. Updated imports across modules.

---

### 97. Integrate CorticalConfig into CorticalTextProcessor âœ…

**Meta:** `status:completed` `priority:high` `category:architecture`
**Files:** `cortical/processor.py`, `cortical/config.py`
**Effort:** Medium
**Completed:** 2025-12-11

**Problem:** Config exists but isn't used by processor.

**Solution Applied:**
- Added `config` parameter to `CorticalTextProcessor.__init__()`
- Config stored on processor instance (`self.config`)
- Methods use config defaults for parameters like `damping`, `resolution`, etc.
- Config saved/loaded with processor state via persistence
- Per-call override still supported

**Acceptance Criteria:**
- [x] Config parameter accepted
- [x] Methods use config defaults
- [x] Override still possible per-call

---

### 98. Replace print() with Logging Module

**Meta:** `status:pending` `priority:medium` `category:code-quality`
**Files:** `cortical/processor.py`, other modules
**Effort:** Medium

**Problem:** Uses print() for progress - can't configure.

**Solution:** Use `logging.getLogger(__name__)`.

---

### 99. Add Input Validation to Public Methods

**Meta:** `status:pending` `priority:medium` `category:code-quality`
**Files:** `cortical/processor.py`, `cortical/query.py`
**Effort:** Medium

**Problem:** Some methods don't validate inputs.

**Solution:** Add validation at API boundary.

---

### 100. Implement Plugin/Extension Registry

**Meta:** `status:pending` `priority:low` `category:architecture`
**Files:** `cortical/registry.py` (new), `cortical/processor.py`
**Effort:** Large

**Problem:** Can't add algorithms without modifying core.

**Solution:** Registry pattern for PageRank, clustering, expansion strategies.

---

### 101. Automate Staleness Tracking with Decorators

**Meta:** `status:pending` `priority:low` `category:architecture`
**Files:** `cortical/processor.py`
**Effort:** Medium

**Problem:** Manual `_mark_stale()` calls are error-prone.

**Solution:** `@invalidates('tfidf', 'pagerank')` decorator.

---

### 102. Add Tests for Edge Cases

**Meta:** `status:pending` `priority:medium` `category:testing`
**Files:** `tests/test_edge_cases.py` (new)
**Effort:** Medium

**Problem:** Missing tests for Unicode, large docs, malformed inputs.

---

### 103. Add Priority Backlog Summary to Top of TASK_LIST.md âœ“

**Meta:** `status:completed` `priority:high` `category:task-mgmt`
**Files:** `TASK_LIST.md`
**Completed:** 2025-12-11

**Solution Applied:** Added machine-parseable backlog with priority tiers (Critical/High/Medium/Low/Deferred) at top of file.

---

### 104. Create TASK_ARCHIVE.md for Completed Tasks âœ“

**Meta:** `status:completed` `priority:high` `category:task-mgmt`
**Files:** `TASK_ARCHIVE.md`, `TASK_LIST.md`
**Completed:** 2025-12-11

**Solution Applied:** Created archive with 75+ completed tasks, reduced TASK_LIST.md from 3,565 to 604 lines (83% reduction).

---

### 105. Standardize Task Format with Machine-Parseable Metadata âœ“

**Meta:** `status:completed` `priority:medium` `category:task-mgmt`
**Files:** `TASK_LIST.md`
**Completed:** 2025-12-11

**Solution Applied:** All tasks now use `**Meta:** \`status:...\` \`priority:...\`` format with effort estimates.

---

### 106. Add Task Dependency Graph

**Meta:** `status:pending` `priority:low` `category:task-mgmt`
**Files:** Section in TASK_LIST.md
**Effort:** Small
**Depends:** 105

**Problem:** Dependencies not visualized.

---

### 107. Add "Quick Context" Section to Each Task

**Meta:** `status:pending` `priority:medium` `category:task-mgmt`
**Files:** `TASK_LIST.md`
**Effort:** Medium
**Depends:** 105

**Problem:** Must search codebase to understand task context.

---

### 108. Create Task Selection Helper Script

**Meta:** `status:pending` `priority:low` `category:task-mgmt`
**Files:** `scripts/next_task.py` (new)
**Effort:** Medium
**Depends:** 103, 105

**Problem:** Selecting next task is manual work.

---

### 109. Add "Recently Completed" Section for Context âœ“

**Meta:** `status:completed` `priority:low` `category:task-mgmt`
**Files:** `TASK_LIST.md`
**Completed:** 2025-12-11

**Solution Applied:** Added "Recently Completed (Last 7 Days)" section with table format showing task, date, and notes.

---

### 110. Add Section Markers to Large Files

**Meta:** `status:pending` `priority:high` `category:ai-nav`
**Files:** `cortical/processor.py`, `cortical/query.py`
**Effort:** Small

**Problem:** Large files (processor.py: 2,301 lines, query.py: 2,719 lines) are hard to navigate. AI assistants must scan large portions to find relevant sections.

**Solution:** Add clear section markers like:
```python
# =============================================================================
# DOCUMENT MANAGEMENT
# =============================================================================

# =============================================================================
# COMPUTATION METHODS
# =============================================================================
```

**Acceptance Criteria:**
- [ ] processor.py has 5-7 logical sections marked
- [ ] query.py has 5-7 logical sections marked
- [ ] Section names match CLAUDE.md terminology

---

### 111. Add "See Also" Cross-References to Docstrings

**Meta:** `status:pending` `priority:high` `category:ai-nav`
**Files:** `cortical/processor.py`, `cortical/query.py`, `cortical/analysis.py`
**Effort:** Medium

**Problem:** When reading a function, AI assistants don't know about related functions without searching.

**Solution:** Add "See Also" sections to docstrings:
```python
def find_documents_for_query(self, query: str, top_n: int = 5):
    """
    Find documents matching query.

    See Also:
        fast_find_documents: Faster search, document-level only
        find_passages_for_query: Chunk-level retrieval for RAG
        expand_query: Get expanded terms before searching
    """
```

**Target Functions:** Top 20 most-used public methods.

---

### 112. Add Docstring Examples for Complex Functions

**Meta:** `status:pending` `priority:high` `category:ai-nav`
**Files:** `cortical/query.py`, `cortical/analysis.py`, `cortical/processor.py`
**Effort:** Medium

**Problem:** Complex functions lack examples showing expected input/output.

**Solution:** Add Examples section to docstrings:
```python
def expand_query(self, query: str, max_expansions: int = 10):
    """
    Expand query with related terms.

    Example:
        >>> processor.expand_query("neural networks")
        {'neural': 1.0, 'networks': 1.0, 'network': 0.85,
         'learning': 0.72, 'deep': 0.68}
    """
```

**Target Functions:**
- `expand_query()`, `expand_query_semantic()`, `expand_query_multihop()`
- `find_documents_for_query()`, `find_passages_for_query()`
- `parse_intent_query()`, `search_by_intent()`
- `complete_analogy()`
- `compute_pagerank()`, `compute_tfidf()`

---

### 113. Document Staleness Tracking System âœ…

**Meta:** `status:completed` `priority:medium` `category:ai-nav`
**Files:** `CLAUDE.md`
**Effort:** Small
**Completed:** 2025-12-12

**Problem:** The staleness tracking system was powerful but not documented.

**Solution Applied:** Added comprehensive "Staleness Tracking System" section to CLAUDE.md including:
- Table of all `COMP_*` constants and what they track
- How staleness works (all start stale, computed marks fresh)
- API methods (`is_stale()`, `get_stale_computations()`)
- Incremental update behavior
- When to check staleness before reading computed values

---

### 114. Add Type Aliases for Complex Types âœ…

**Meta:** `status:completed` `priority:medium` `category:ai-nav`
**Files:** `cortical/types.py`
**Effort:** Small
**Completed:** 2025-12-12

**Problem:** Complex return types like `List[Tuple[str, float, Dict[str, Any]]]` are hard to understand at a glance.

**Solution Applied:** Created `cortical/types.py` with 20+ type aliases organized by category:
- **Score types:** `DocumentScore`, `TermScore`, `DocumentResults`, `TermResults`
- **Passage types:** `PassageResult`, `PassageResults`, `PassageWithPosition`
- **Semantic types:** `SemanticRelation`, `SemanticRelations`
- **Graph types:** `ConnectionMap`, `LayerDict`, `ClusterAssignment`
- **Expansion types:** `ExpansionTerms`, `IntentResult`
- **Fingerprint types:** `Fingerprint`, `FingerprintComparison`

All types include docstrings explaining their structure.

---

### 115. Create Component Interaction Diagram

**Meta:** `status:pending` `priority:medium` `category:ai-nav`
**Files:** `docs/architecture.md` or `CLAUDE.md`
**Effort:** Medium

**Problem:** Understanding how modules call each other requires tracing imports and function calls.

**Solution:** Add ASCII or Mermaid diagram showing:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ processor.pyâ”‚ â† Public API entry point
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ calls
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ analysis.py â”‚ query.py â”‚ semantics.py â”‚ ...  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ operates on
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      layers.py  â†’  minicolumn.py             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

Include which module calls which, and data flow direction.

---

### 116. Document Return Value Semantics âœ…

**Meta:** `status:completed` `priority:medium` `category:ai-nav`
**Files:** `CLAUDE.md`
**Effort:** Medium
**Completed:** 2025-12-12

**Problem:** Inconsistent understanding of what functions return in edge cases.

**Solution Applied:** Added "Return Value Semantics" section to CLAUDE.md documenting:
- **Edge Case Returns:** Table showing what each scenario returns (empty corpus â†’ `[]`, unknown doc_id â†’ `{}`, etc.)
- **Score Ranges:** Relevance (unbounded), PageRank (0-1), TF-IDF (unbounded), Similarity (0-1), Confidence (0-1)
- **Lookup Functions:** `None` for missing items vs `KeyError` for invalid structure
- **Default Parameters:** Table of key defaults (`top_n=5`, `damping=0.85`, `resolution=1.0`, etc.)

---

### 117. Create Debugging Cookbook

**Meta:** `status:pending` `priority:low` `category:ai-nav`
**Files:** `docs/debugging.md` (new)
**Effort:** Medium

**Problem:** Common debugging scenarios require discovering patterns through trial and error.

**Solution:** Create cookbook with scenarios:

1. **"Why is my query returning no results?"**
   - Check if corpus has documents
   - Check if terms exist in corpus
   - Use `--expand` to see what's being searched

2. **"Why are PageRank values all zero?"**
   - Check if `compute_all()` was called
   - Check staleness with `is_stale()`

3. **"Why is search slow?"**
   - Use `fast_find_documents()` for document-level
   - Pre-build index with `build_search_index()`

4. **"Why are bigrams not connecting?"**
   - Verify space separator (not underscore)
   - Check `compute_bigram_connections()` was called

---

### 118. Add Function Complexity Annotations

**Meta:** `status:pending` `priority:low` `category:ai-nav`
**Files:** `cortical/processor.py`, `cortical/analysis.py`
**Effort:** Small

**Problem:** AI assistants don't know which functions are expensive to call.

**Solution:** Add complexity notes to expensive functions:
```python
def compute_all(self, verbose: bool = True):
    """
    Compute all network properties.

    Complexity: O(nÂ²) where n = total minicolumns across all layers.
    Typical time: 2-5 seconds for 10K documents.

    Note: For incremental updates, prefer add_document_incremental()
    which is O(m) where m = tokens in new document.
    """
```

**Target Functions:**
- `compute_all()` - O(nÂ²)
- `compute_pagerank()` - O(iterations Ã— edges)
- `build_concept_clusters()` - O(n Ã— iterations)
- `find_passages_for_query()` - O(docs Ã— chunks)

---

### 119. Create AI Metadata Generator Script âœ…

**Meta:** `status:completed` `priority:high` `category:ai-nav`
**Files:** `scripts/generate_ai_metadata.py`, `tests/test_generate_ai_metadata.py`
**Effort:** Medium
**Completed:** 2025-12-11

**Problem:** AI navigation tasks (110-118) require modifying code files directly, cluttering them for human readers. We need a way to provide rich AI navigation aids without polluting the source code.

**Solution:** Generate companion `.ai_meta` files (YAML) that provide:
- Section markers with line ranges
- Function cross-references ("See Also")
- Complexity annotations
- Return value semantics
- Docstring examples (extracted or generated)
- Import/dependency information

**Output format:**
```yaml
# processor.py.ai_meta - Auto-generated
file: cortical/processor.py
lines: 2301
generated: 2025-12-11T14:30:00

sections:
  - name: "Document Management"
    lines: [54, 520]
    functions: [process_document, add_document_incremental, remove_document]
  - name: "Computation Methods"
    lines: [613, 1140]
    functions: [compute_all, compute_importance, compute_tfidf]

functions:
  find_documents_for_query:
    line: 1596
    signature: "(query: str, top_n: int = 5) -> List[Tuple[str, float]]"
    see_also:
      fast_find_documents: "~2-3x faster, document-level only"
      find_passages_for_query: "Chunk-level retrieval for RAG"
    complexity: "O(terms Ã— documents)"
    returns:
      on_empty_corpus: "[]"
      on_no_matches: "[]"

dependencies:
  imports: [analysis, query, semantics]
  imported_by: [__init__]
```

**Acceptance Criteria:**
- [ ] Script generates .ai_meta for all cortical/*.py files
- [ ] *.ai_meta added to .gitignore
- [ ] Output is valid YAML (AI-parseable)
- [ ] Extracts sections by analyzing class/function groupings
- [ ] Extracts function signatures and line numbers
- [ ] Identifies related functions by naming patterns
- [ ] Can be run incrementally (only changed files)

---

### 120. Add AI Metadata Loader to Claude Skills âœ…

**Meta:** `status:completed` `priority:high` `category:ai-nav`
**Files:** `.claude/skills/ai-metadata/SKILL.md`, `.claude/skills/corpus-indexer/SKILL.md`
**Effort:** Small
**Depends:** 119
**Completed:** 2025-12-11

**Problem:** Generated .ai_meta files need to be used by AI assistants during code navigation.

**Solution:** Created new `ai-metadata` skill that:
1. Provides structured documentation for using .ai_meta files
2. Explains metadata fields (sections, see_also, complexity hints)
3. Shows best practices for AI agent navigation
4. Updated corpus-indexer skill to include metadata generation commands

---

### 121. Auto-regenerate AI Metadata on File Changes âœ…

**Meta:** `status:completed` `priority:high` `category:ai-nav`
**Files:** `CLAUDE.md`, `.claude/skills/corpus-indexer/SKILL.md`
**Effort:** Medium
**Depends:** 119
**Completed:** 2025-12-11

**Problem:** .ai_meta files become stale when source files change.

**Solution implemented:**
1. **Documented in CLAUDE.md** - AI Agent Onboarding section with startup command
2. **Incremental mode** - `python scripts/generate_ai_metadata.py --incremental` only updates changed files
3. **Combined workflow** - `python scripts/index_codebase.py --incremental && python scripts/generate_ai_metadata.py --incremental`
4. **Skills documentation** - corpus-indexer skill explains metadata regeneration

**Recommended workflow for new agents:**
```bash
# On arrival, check if metadata exists and regenerate if needed
ls cortical/*.ai_meta || python scripts/generate_ai_metadata.py
```

---

### 122. Investigate Concept Layer & Embeddings Regressions âœ…

**Meta:** `status:completed` `priority:critical` `category:bugfix`
**Files:** `cortical/analysis.py`, `cortical/embeddings.py`, `showcase.py`
**Effort:** Medium
**Completed:** 2025-12-11

**Problem:** Showcase output reveals potential regressions or bugs:

1. **Concept Layer has only 3 clusters** for 95 documents
2. **Graph embeddings show nonsensical similarities** - "neural" similar to "blockchain"

**Root Cause Analysis:**

1. **Clustering strictness logic was inverted** (introduced in Task #4):
   - Bug: `change_threshold = (1.0 - cluster_strictness) * 0.3` meant high strictness â†’ easy label changes
   - Fix: Changed to `change_threshold = cluster_strictness * 0.3` so high strictness â†’ resist changes
   - Also fixed bonus logic that had the same inversion

2. **Adjacency embeddings were sparse** for large graphs:
   - Bug: Only captured direct connections to landmark nodes, resulting in mostly-zero vectors
   - Fix: Added multi-hop propagation to reach landmarks through neighbors
   - Also changed showcase.py to use `random_walk` method (better semantic results)

3. **Concept cluster count** (3-5 for 95 docs) is actually correct behavior:
   - The corpus is highly connected (avg 18.2 connections per token)
   - Label propagation correctly merges connected tokens into large clusters
   - One giant cluster (6656 tokens) with a few small clusters is expected

**Solution Applied:**

1. Fixed `cluster_strictness` logic in `analysis.py:585` and `analysis.py:601-603`
2. Improved `_adjacency_embeddings()` with multi-hop propagation in `embeddings.py`
3. Changed showcase.py to use `method='random_walk'` for embeddings
4. Added 3 regression tests:
   - `test_cluster_strictness_direction` - ensures higher strictness â†’ more clusters
   - `test_random_walk_semantic_similarity` - ensures "neural" similar to "networks"
   - `test_adjacency_produces_nonzero_embeddings` - ensures dense embeddings

**Results After Fix:**
- Embeddings now show "neural" similar to "networks" (0.938), "learn" (0.928) âœ…
- Clustering direction now matches documentation âœ…
- All 820 tests pass âœ…

**Acceptance Criteria:**
- [x] Root cause identified via git history
- [x] Embedding similarities semantically meaningful
- [x] Regression test added to prevent recurrence
- [~] Concept clusters > 10: Not achievable due to highly connected corpus (correct behavior)

---

### 127. Create Cluster Coverage Evaluation Script âœ…

**Meta:** `status:completed` `priority:high` `category:devex`
**Files:** `scripts/evaluate_cluster.py`, `tests/test_evaluate_cluster.py`
**Effort:** Medium
**Depends:** 125
**Completed:** 2025-12-11

**Problem:** When adding sample documents to create topic clusters (e.g., customer service), there's no automated way to determine if the cluster has sufficient coverage or needs more documents.

**Solution Applied:** Created `scripts/evaluate_cluster.py` with:

**Usage:**
```bash
# Find and evaluate documents by topic search
python scripts/evaluate_cluster.py --topic "customer service"

# Evaluate specific documents
python scripts/evaluate_cluster.py --documents doc1,doc2,doc3

# Find documents by keywords
python scripts/evaluate_cluster.py --keywords customer,ticket,escalation --min-keywords 2

# Show expansion suggestions
python scripts/evaluate_cluster.py --topic "machine learning" --suggest --verbose
```

**Features Implemented:**
1. **Three cluster detection modes:**
   - `--topic`: Semantic search for related documents
   - `--documents`: Explicit document list
   - `--keywords`: Documents containing specified terms

2. **Coverage Metrics:**
   - Internal Cohesion: Weighted Jaccard similarity within cluster
   - External Separation: 1 - similarity to outside documents
   - Concept Coverage: Number of concepts covered
   - Term Diversity: Unique terms / total occurrences
   - Hub Document: Most centrally connected document

3. **Coverage Assessment:**
   - STRONG: High cohesion + separation + coverage
   - ADEQUATE: Usable but could improve
   - NEEDS EXPANSION: Low metrics

4. **Expansion Suggestions:** Related terms not well-covered by cluster

**Example Output:**
```
Cluster Analysis: Keywords: customer, ticket, support (4 documents)
===================================================================
Documents:
  * complaint_resolution
  * customer_satisfaction_metrics
  * customer_support_fundamentals (hub)
  * ticket_escalation_procedures

Metrics:
  Internal Cohesion:    0.08 (weak)
  External Separation:  0.98 (good)
  Concept Coverage:     35 concepts
  Term Diversity:       0.80

Coverage Assessment: ADEQUATE [~]
  Cluster is usable but could improve: weak internal connectivity.
```

**Test Results:**
- 24 new tests in tests/test_evaluate_cluster.py
- 1025 total tests pass

**Acceptance Criteria:**
- [x] Script identifies document clusters by topic/keywords
- [x] Computes cohesion and separation metrics
- [x] Provides clear coverage assessment (adequate/needs expansion)
- [x] Suggests specific expansion topics when coverage is low
- [x] Works with existing corpus or standalone document set

---

### 140. Analyze Customer Service Cluster Quality

**Meta:** `status:pending` `priority:low` `category:research`
**Files:** Analysis output
**Effort:** Small
**Depends:** 127

**Problem:** The customer service cluster was added but not deeply analyzed.

**Tasks:**
1. Run cluster evaluation script on customer service documents
2. Check if cluster forms a distinct concept group in Layer 2
3. Verify semantic coherence of the cluster
4. Document findings

**Expected Insights:**
- Whether 6 documents is sufficient for a coherent cluster
- How customer service concepts connect to other domains
- Quality of the "ticket â†’ learning" semantic bridge observed in embeddings

---

### 129. Test Customer Service Retrieval Quality

**Meta:** `status:pending` `priority:low` `category:testing`
**Files:** `tests/test_customer_service_retrieval.py` (new, optional)
**Effort:** Small

**Problem:** No systematic testing of retrieval quality for customer service queries.

**Test Queries:**
```python
queries = [
    ("how to handle angry customer", ["complaint_resolution", "customer_support_fundamentals"]),
    ("ticket escalation process", ["ticket_escalation_procedures"]),
    ("measure customer satisfaction", ["customer_satisfaction_metrics"]),
    ("reduce customer churn", ["customer_retention_strategies"]),
    ("call center workforce management", ["call_center_operations"]),
]
```

**Evaluation:**
- Precision@1: Does the top result match expected?
- Precision@3: Are expected docs in top 3?
- Compare with/without customer service docs in corpus

---

### 130. Expand Customer Service Sample Cluster

**Meta:** `status:pending` `priority:low` `category:samples`
**Files:** `samples/*.txt` (new documents)
**Effort:** Medium

**Problem:** The current 6 customer service documents may benefit from expansion.

**Potential New Documents:**
1. `live_chat_support.txt` - Chat-specific support strategies
2. `sla_management.txt` - Service level agreements and monitoring
3. `crm_integration.txt` - CRM systems and helpdesk software
4. `customer_journey_mapping.txt` - Journey analysis and touchpoints
5. `support_automation.txt` - Chatbots, auto-responses, AI in support
6. `multilingual_support.txt` - International customer service

**Depends On:** Results from Task #128 (cluster quality analysis)

---

### 131. Investigate Cross-Domain Semantic Bridges

**Meta:** `status:pending` `priority:low` `category:research`
**Files:** Analysis output, potentially `docs/semantic_bridges.md`
**Effort:** Medium

**Problem:** Interesting semantic connections exist between seemingly unrelated domains. The graph embeddings showed "ticket" similar to "learning" (0.937) - understanding these bridges could reveal insights about the corpus structure.

**Research Questions:**
1. What concepts bridge customer service to other domains?
2. Why does "ticket" connect to "learning"? (ticket systems in education? ticketing in other contexts?)
3. Are there other unexpected cross-domain connections?
4. Can these bridges improve cross-domain search?

**Approach:**
1. Extract embedding neighbors for customer service terms
2. Identify terms that appear in multiple domain clusters
3. Trace connection paths in the concept graph
4. Document interesting findings

**Potential Applications:**
- Improve query expansion with cross-domain terms
- Suggest related documents from different domains
- Identify knowledge gaps at domain boundaries

---

### 141. Filter Python Keywords/Artifacts from Analysis âœ…

**Meta:** `status:completed` `priority:high` `category:quality`
**Files:** `cortical/tokenizer.py`, `showcase.py`
**Effort:** Medium
**Completed:** 2025-12-12

**Problem (Dog-fooding 2025-12-12):** When Python code files are added to the corpus (samples/data_processor.py, etc.), Python keywords pollute the analysis:
- "self" is #2 in PageRank (0.0038) - meaningless for content analysis
- TF-IDF top terms: `assertequal`, `def`, `mockdatarecord`, `str`, `doc_id`
- These artifacts drown out meaningful content terms

**Solution Applied:**
1. Added `CODE_NOISE_TOKENS` constant with common Python/test tokens
2. Added `filter_code_noise` parameter to Tokenizer
3. showcase.py now uses `filter_code_noise=True` by default

**Acceptance Criteria:**
- [x] "self", "def", "str" not in top 20 PageRank terms when code files present
- [x] TF-IDF surfaces meaningful terms, not syntax
- [x] Existing code search features still work (1121 tests pass)

---

### 142. Investigate 74s compute_all() Performance Regression âœ…

**Meta:** `status:completed` `priority:high` `category:perf`
**Files:** `cortical/processor.py`, `cortical/embeddings.py`
**Effort:** Medium
**Completed:** 2025-12-12

**Problem (Dog-fooding 2025-12-12):** compute_all() took 74.28s for 109 documents.

**Root Cause Found:** `compute_graph_embeddings()` took 58s+ using adjacency method with multi-hop propagation on 7268 tokens.

**Solution Applied:**
1. Added `fast` embedding method (direct adjacency, no multi-hop)
2. Added `max_terms` parameter for sampling top-N by PageRank
3. Auto-selects sampling: <2000 tokens = all, 2000-5000 = 1500, >5000 = 1000
4. Changed default method from 'adjacency' to 'fast'

**Results:**
- compute_graph_embeddings: 58s â†’ 0.01s
- compute_all: 74.28s â†’ 14.21s (5.2x speedup)

**Acceptance Criteria:**
- [x] Identify root cause with profiling data
- [x] compute_all() under 30s for 109 documents (achieved 14.21s)
- [x] Document findings

---

### 143. Investigate Negative Silhouette Score in Clustering âœ…

**Meta:** `status:completed` `priority:high` `category:quality`
**Files:** `cortical/analysis.py`, `cortical/processor.py`
**Effort:** Medium
**Completed:** 2025-12-12

**Problem (Dog-fooding 2025-12-12):** Clustering quality metrics show:
- silhouette = -0.00 (negative indicates poor cluster separation)
- Only 33 clusters for 109 documents
- modularity = 0.40 (acceptable but not great)

**Investigation Findings:**
The negative silhouette score is **expected behavior**, not a bug. Key insights:

1. **Modularity and silhouette measure different things:**
   - Modularity: Graph edge density within clusters (what Louvain optimizes)
   - Silhouette: Document co-occurrence similarity (semantic coherence)

2. **Tokens that co-occur in sentences don't necessarily share documents:**
   - Louvain groups tokens that appear together in sentences
   - Silhouette measures if tokens appear in the same documents
   - These are fundamentally different similarity metrics

3. **Higher Louvain resolution makes silhouette worse:**
   - Resolution 1.0: silhouette=-0.03
   - Resolution 3.0: silhouette=-0.20
   - More clusters = less document overlap per cluster

**Solution Applied:**
1. Changed silhouette to use document co-occurrence (Jaccard on document sets)
2. Updated quality assessment to explain "typical graph clustering" for silhouette -0.1 to 0.1
3. Updated docstrings to explain metric interpretation
4. Added `_doc_similarity()` helper function

**Results:**
```
34 clusters with Good community structure (modularity 0.37),
typical graph clustering (silhouette -0.06), moderately balanced sizes
```

**Key Lesson:** Don't assume metrics must be positive. Understand what each metric measures before setting acceptance criteria.

---

### 144. Boost Exact Document Name Matches in Search âœ…

**Meta:** `status:completed` `priority:high` `category:quality`
**Files:** `cortical/query/search.py`
**Effort:** Small
**Completed:** 2025-12-12

**Problem (Dog-fooding 2025-12-12):** Query "distributed systems" returns:
1. unix_evolution (score: 19.804)
2. database_design_patterns (score: 16.583)
3. comprehensive_machine_learning (score: 13.244)

But `distributed_systems` document exists and should rank #1 for this query.

**Solution Applied:**
Added `doc_name_boost` parameter (default 2.0) to `find_documents_for_query()` and `fast_find_documents()`. Boost is proportional to query term overlap with document ID.

**Results:**
- "distributed systems" â†’ `distributed_systems` now #1
- "fermentation" â†’ `fermentation_science` still #1
- "quantum computing" â†’ `quantum_computing_basics` now #1

**Acceptance Criteria:**
- [x] Query "distributed systems" returns `distributed_systems` in top 2
- [x] Query "fermentation" keeps `fermentation_science` at #1
- [x] No regression on other queries (1121 tests pass)

---

### 145. Improve Graph Embedding Quality for Common Terms âœ…

**Meta:** `status:completed` `priority:high` `category:quality`
**Files:** `cortical/embeddings.py`, `cortical/processor.py`
**Effort:** Medium
**Completed:** 2025-12-12

**Problem (Dog-fooding 2025-12-12):** Graph embeddings show semantically odd similarities:
- 'data' â†’ cognitive (0.968), alternatives (0.967), server (0.958)
- 'machine' â†’ rate, experience, gradients (not semantically related)

**Root Cause Analysis:**
Graph-based embeddings use connection patterns to landmarks. Sparse connections
(only 8-13 out of 64 landmarks) cause misleading high similarity when vectors
are normalized. Terms sharing large documents have similar patterns regardless
of semantic meaning.

**Solution Applied:**
1. Added 'tfidf' embedding method that uses document distribution as feature space
2. Added IDF weighting to 'fast' method to down-weight common terms
3. Updated docstrings to recommend 'tfidf' for semantic similarity tasks

**Results:**
```
FAST (graph):   'machine' â†’ rate (0.82), gradients (0.77), experience (0.73)
TF-IDF:         'machine' â†’ representations (0.71), learning (0.71), image (0.70)

FAST (graph):   'learning' â†’ training (0.70), approaches (0.64)
TF-IDF:         'learning' â†’ neural (0.82), networks (0.81), training (0.77)
```

TF-IDF embeddings capture semantic similarity much better because terms appearing
in similar documents are usually semantically related.

**Acceptance Criteria:**
- [x] 'data' filtered out by CODE_NOISE_TOKENS (Task #141)
- [x] High-frequency terms don't dominate (TF-IDF naturally down-weights)
- [x] Existing good similarities preserved and improved
- [x] New 'tfidf' method available for semantic similarity tasks

---

### 146. Create Behavioral Tests for Core User Workflows âœ…

**Meta:** `status:completed` `priority:high` `category:testing`
**Files:** `tests/test_behavioral.py`
**Effort:** Medium
**Completed:** 2025-12-12

**Purpose:** Created acceptance/behavioral tests that verify the system delivers expected
user outcomes, catching the kinds of issues discovered during dog-fooding.

**Solution Applied:** Created `tests/test_behavioral.py` with 11 tests across 4 categories:

1. **TestSearchBehavior** (3 tests)
   - `test_document_name_matches_rank_highly` - "distributed systems" â†’ distributed_systems in top 2
   - `test_query_expansion_improves_recall` - Expanded queries find more relevant docs
   - `test_code_search_finds_implementations_not_tests` - Real code preferred over test files

2. **TestPerformanceBehavior** (2 tests)
   - `test_compute_all_under_threshold` - Full analysis < 30s for 100+ docs
   - `test_search_is_fast` - Single query < 100ms

3. **TestQualityBehavior** (4 tests)
   - `test_pagerank_surfaces_meaningful_terms` - No 'self', 'def' in top 20
   - `test_clustering_produces_coherent_groups` - modularity > 0.3
   - `test_tfidf_embeddings_capture_semantic_similarity` - 'learning' similar to 'neural'
   - `test_no_mega_cluster` - No cluster > 25% of tokens

4. **TestRobustnessBehavior** (2 tests)
   - `test_empty_query_raises_error` - ValueError for empty queries
   - `test_unknown_terms_handled_gracefully` - Nonsense queries return empty results

**Acceptance Criteria:**
- [x] TestSearchBehavior class with 3+ tests
- [x] TestPerformanceBehavior class with 2+ tests
- [x] TestQualityBehavior class with 3+ tests (4 implemented)
- [x] TestRobustnessBehavior class with 2+ tests
- [x] All tests pass on current codebase
- [x] Tests catch regressions from Tasks #141-145 if reverted

---

## Category Index

| Category | Pending | Description |
|----------|---------|-------------|
| Perf | 1 | Performance improvements (#138) |
| Arch | 6 | Architecture refactoring (#133, 134, 135, 95, 100, 101) |
| CodeQual | 2 | Code quality improvements (#98, 99) |
| Testing | 2 | Test coverage (#102, 129) |
| TaskMgmt | 3 | Task management system (#107, 106, 108) |
| AINav | 3 | AI assistant navigation (#115, 117, 118) |
| DevEx | 7 | Developer experience, scripts (#73-80) |
| Research | 2 | Research and analysis (#140, 131) |
| Samples | 1 | Sample document improvements (#130) |
| Showcase | 1 | In progress (#87) |
| Deferred | 7 | Low priority or superseded |

*Updated 2025-12-12 - Fixed staleness issues, renumbered Task #128 â†’ #140*

---

## Notes

- **Effort estimates:** Small (<1 hour), Medium (1-4 hours), Large (1+ days)
- **Dependencies:** Complete dependent tasks first
- **Quick Context:** Key info to start task without searching
- **Archive:** Full history in [TASK_ARCHIVE.md](TASK_ARCHIVE.md)

---

*Last restructured: 2025-12-11 (Tasks 88, 89, 90, 103, 104, 105, 109 implemented)*
