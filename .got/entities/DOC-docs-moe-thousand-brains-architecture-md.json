{
  "_checksum": "d2f17511e9067bc5",
  "_written_at": "2025-12-23T20:26:13.335092+00:00",
  "data": {
    "category": "docs",
    "content_hash": "153d71f65f0efaa5",
    "created_at": "2025-12-23T20:26:13.246632+00:00",
    "doc_type": "architecture",
    "entity_type": "document",
    "id": "DOC-docs-moe-thousand-brains-architecture-md",
    "is_stale": false,
    "last_file_modified": "2025-12-22T21:12:26+00:00",
    "last_verified": "2025-12-23T20:26:13.328213+00:00",
    "line_count": 519,
    "linked_decision_ids": [],
    "linked_task_ids": [],
    "metadata": {},
    "modified_at": "2025-12-23T20:26:13.335054+00:00",
    "path": "docs/moe-thousand-brains-architecture.md",
    "properties": {},
    "tags": [],
    "title": "Micro-Model Mixture of Experts Architecture",
    "version": 4,
    "word_count": 2013
  }
}